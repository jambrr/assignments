<doc id="12240" url="https://en.wikipedia.org/wiki?curid=12240" title="Gold">
Gold

Gold is a chemical element with symbol Au (from ) and atomic number 79. In its purest form, it is a bright, slightly reddish yellow, dense, soft, malleable and ductile metal. Chemically, gold is a transition metal and a group 11 element. It is one of the least reactive chemical elements, and is solid under standard conditions. The metal therefore occurs often in free elemental (native) form, as nuggets or grains, in rocks, in veins and in alluvial deposits. It occurs in a solid solution series with the native element silver (as electrum) and also naturally alloyed with copper and palladium. Less commonly, it occurs in minerals as gold compounds, often with tellurium (gold tellurides).
Gold's atomic number of 79 makes it one of the higher atomic number elements that occur naturally in the universe. It is thought to have been produced in supernova nucleosynthesis and from the collision of neutron stars and to have been present in the dust from which the Solar System formed. Because the Earth was molten when it was just formed, almost all of the gold present in the early Earth probably sank into the planetary core. Therefore, most of the gold that is present today in the Earth's crust and mantle is thought to have been delivered to Earth later, by asteroid impacts during the late heavy bombardment, about 4 billion years ago.
Gold resists attack by individual acids, but aqua regia (literally "royal water", a mixture of nitric acid and hydrochloric acid) can dissolve it. The acid mixture causes the formation of a soluble gold tetrachloride anion. It is insoluble in nitric acid, which dissolves silver and base metals, a property that has long been used to refine gold and to confirm the presence of gold in metallic objects, giving rise to the term "acid test". Gold also dissolves in alkaline solutions of cyanide, which are used in mining and electroplating. Gold dissolves in mercury, forming amalgam alloys, but this is not a chemical reaction.
Gold is a precious metal used for coinage, jewelry, and other arts throughout recorded history. In the past, a gold standard was often implemented as a monetary policy within and between nations, but gold coins ceased to be minted as a circulating currency in the 1930s, and the world gold standard was abandoned for a fiat currency system after 1976. The historical value of gold was rooted in its relative rarity, easy handling and minting, easy smelting and fabrication, resistance to corrosion and other chemical reactions (nobility), and distinctive color.
A total of 183,600 tonnes of gold is in existence above ground, as of 2014. This is equivalent to 9513 m3 of gold. The world consumption of new gold produced is about 50% in jewelry, 40% in investments, and 10% in industry. Gold’s high malleability, ductility, resistance to corrosion and most other chemical reactions, and conductivity of electricity have led to its continued use in corrosion resistant electrical connectors in all types of computerized devices (its chief industrial use). Gold is also used in infrared shielding, colored-glass production, gold leafing, and tooth restoration. Certain gold salts are still used as anti-inflammatories in medicine.
Etymology.
"Gold" is cognate with similar words in many Germanic languages, deriving via Proto-Germanic *"gulþą" from Proto-Indo-European *"ǵʰelh₃-" ("to shine, to gleam; to be yellow or green").
The symbol "Au" is from the , the Latin word for "gold". The Proto-Indo-European ancestor of "aurum" was "*h₂é-h₂us-o-", meaning "glow". This word is derived from the same root (Proto-Indo-European "*h₂u̯es-" "to dawn") as "*h₂éu̯sōs", the ancestor of the Latin word Aurora, "dawn". This etymological relationship is presumably behind the frequent claim in scientific publications that "aurum" meant "shining dawn".
Characteristics.
Gold is the most malleable of all metals; a single gram can be beaten into a sheet of 1 square meter, and an ounce into 300 square feet. Gold leaf can be beaten thin enough to become transparent. The transmitted light appears greenish blue, because gold strongly reflects yellow and red. Such semi-transparent sheets also strongly reflect infrared light, making them useful as infrared (radiant heat) shields in visors of heat-resistant suits, and in sun-visors for spacesuits. Gold is a good conductor of heat and electricity and reflects infrared radiation strongly.
In addition, gold is very dense: it has a density of 19,300 kg/m3. By comparison, the density of lead is 11,340 kg/m3, and that of the densest element, osmium, is 22,588 ± 15 kg/m3.
Chemistry.
Although gold is the most noble of the noble metals, it still forms many diverse compounds. The oxidation state of gold in its compounds ranges from −1 to +5, but Au(I) and Au(III) dominate its chemistry. Au(I), referred to as the aurous ion, is the most common oxidation state with soft ligands such as thioethers, thiolates, and tertiary phosphines. Au(I) compounds are typically linear. A good example is Au(CN)2−, which is the soluble form of gold encountered in mining. The binary gold halides, such as AuCl, form zigzag polymeric chains, again featuring linear coordination at Au. Most drugs based on gold are Au(I) derivatives.
Au(III) (auric) is a common oxidation state, and is illustrated by gold(III) chloride, Au2Cl6. The gold atom centers in Au(III) complexes, like other d8 compounds, are typically square planar, with chemical bonds that have both covalent and ionic character.
Gold does not react with oxygen at any temperature; similarly, it does not react with ozone. Gold is strongly attacked by fluorine at dull-red heat to form gold(III) fluoride.
Some free halogens react with gold. Powdered gold reacts with chlorine at 180 °C to form AuCl3. Gold reacts with bromine at 140 °C to form gold(III) bromide, but reacts only very slowly with iodine to form the monoiodide.
Gold does not react with sulfur directly, but gold(III) sulfide can be made by passing hydrogen sulfide through a dilute solution of gold(III) chloride or chlorauric acid.
Gold readily dissolves in mercury at room temperature to form an amalgam, and forms alloys with many other metals at higher temperatures. These alloys can be produced to modify the hardness and other metallurgical properties, to control melting point or to create exotic colors.
Gold reacts with potassium, rubidium, caesium, or tetramethylammonium, to form the respective auride salts, containing the Au− ion. Caesium auride is perhaps the most famous.
Gold is unaffected by most acids. It does not react with hydrofluoric, hydrochloric, hydrobromic, hydriodic, sulfuric, or nitric acid. It does react with aqua regia, a mixture of nitric and hydrochloric acids, and with selenic acid. Aqua regia, a 1:3 mixture of nitric acid and hydrochloric acid, dissolves gold. Nitric acid oxidizes the metal to +3 ions, but only in minute amounts, typically undetectable in the pure acid because of the chemical equilibrium of the reaction. However, the ions are removed from the equilibrium by hydrochloric acid, forming AuCl4− ions, or chloroauric acid, thereby enabling further oxidation.
Gold is similarly unaffected by most bases. It does not react with aqueous, solid, or molten sodium or potassium hydroxide. It does however, react with sodium or potassium cyanide under alkaline conditions when oxygen is present to form soluble complexes.
Common oxidation states of gold include +1 (gold(I) or aurous compounds) and +3 (gold(III) or auric compounds). Gold ions in solution are readily reduced and precipitated as metal by adding any other metal as the reducing agent. The added metal is oxidized and dissolves, allowing the gold to be displaced from solution and be recovered as a solid precipitate.
Less common oxidation states.
Less common oxidation states of gold include −1, +2, and +5.
The −1 oxidation state occurs in compounds containing the Au− anion, called aurides. Caesium auride (CsAu), for example, crystallizes in the caesium chloride motif. Other aurides include those of Rb+, K+, and tetramethylammonium (CH3)4N+. Gold has the highest Pauling electronegativity of any metal, with a value of 2.54, making the auride anion relatively stable.
Gold(II) compounds are usually diamagnetic with Au–Au bonds such as The evaporation of a solution of in concentrated produces red crystals of gold(II) sulfate, Au2(SO4)2. Originally thought to be a mixed-valence compound, it has been shown to contain cations, analogous to the better-known mercury(I) ion, . A gold(II) complex, the tetraxenonogold(II) cation, which contains xenon as a ligand, occurs in [AuXe4(Sb2F11)2.
Gold pentafluoride, along with its derivative anion, , and its difluorine complex, gold heptafluoride, is the sole example of gold(V), the highest verified oxidation state.
Some gold compounds exhibit "aurophilic bonding", which describes the tendency of gold ions to interact at distances that are too long to be a conventional Au–Au bond but shorter than van der Waals bonding. The interaction is estimated to be comparable in strength to that of a hydrogen bond.
Mixed valence compounds.
Well-defined cluster compounds are numerous. In such cases, gold has a fractional oxidation state. A representative example is the octahedral species {Au(P(C6H5)3)}62+. Gold chalcogenides, such as gold sulfide, feature equal amounts of Au(I) and Au(III).
Color.
Whereas most other pure metals are gray or silvery white, gold is slightly reddish yellow. This color is determined by the density of loosely bound (valence) electrons; those electrons oscillate as a collective "plasma" medium described in terms of a quasiparticle called a plasmon. The frequency of these oscillations lies in the ultraviolet range for most metals, but it falls into the visible range for gold due to subtle relativistic effects that affect the orbitals around gold atoms. Similar effects impart a golden hue to metallic caesium.
Common colored gold alloys such as rose gold can be created by the addition of various amounts of copper and silver, as indicated in the triangular diagram to the left. Alloys containing palladium or nickel are also important in commercial jewelry as these produce white gold alloys. Less commonly, addition of manganese, aluminium, iron, indium and other elements can produce more unusual colors of gold for various applications.
Isotopes.
Gold has only one stable isotope, , which is also its only naturally occurring isotope, so gold is both a mononuclidic and monoisotopic element. Thirty-six radioisotopes have been synthesized ranging in atomic mass from 169 to 205. The most stable of these is with a half-life of 186.1 days. The least stable is , which decays by proton emission with a half-life of 30 µs. Most of gold's radioisotopes with atomic masses below 197 decay by some combination of proton emission, α decay, and β+ decay. The exceptions are , which decays by electron capture, and , which decays most often by electron capture (93%) with a minor β− decay path (7%). All of gold's radioisotopes with atomic masses above 197 decay by β− decay.
At least 32 nuclear isomers have also been characterized, ranging in atomic mass from 170 to 200. Within that range, only , , , , and do not have isomers. Gold's most stable isomer is with a half-life of 2.27 days. Gold's least stable isomer is with a half-life of only 7 ns. has three decay paths: β+ decay, isomeric transition, and alpha decay. No other isomer or isotope of gold has three decay paths.
Modern applications.
The world consumption of new gold produced is about 50% in jewelry, 40% in investments, and 10% in industry.
Jewelry.
Because of the softness of pure (24k) gold, it is usually alloyed with base metals for use in jewelry, altering its hardness and ductility, melting point, color and other properties. Alloys with lower karat rating, typically 22k, 18k, 14k or 10k, contain higher percentages of copper or other base metals or silver or palladium in the alloy. Copper is the most commonly used base metal, yielding a redder color.
Eighteen-karat gold containing 25% copper is found in antique and Russian jewelry and has a distinct, though not dominant, copper cast, creating rose gold. Fourteen-karat gold-copper alloy is nearly identical in color to certain bronze alloys, and both may be used to produce police and other badges. Blue gold can be made by alloying with iron and purple gold can be made by alloying with aluminium, although rarely done except in specialized jewelry. Blue gold is more brittle and therefore more difficult to work with when making jewelry.
Fourteen- and eighteen-karat gold alloys with silver alone appear greenish-yellow and are referred to as green gold. White gold alloys can be made with palladium or nickel. White 18-karat gold containing 17.3% nickel, 5.5% zinc and 2.2% copper is silvery in appearance. Nickel is toxic, however, and its release from nickel white gold is controlled by legislation in Europe.
Alternative white gold alloys are available based on palladium, silver and other white metals, but the palladium alloys are more expensive than those using nickel. High-karat white gold alloys are far more resistant to corrosion than are either pure silver or sterling silver. The Japanese craft of Mokume-gane exploits the color contrasts between laminated colored gold alloys to produce decorative wood-grain effects.
By 2014 the gold jewelry industry was escalating despite a dip in gold prices. Demand in the first quarter of 2014 pushed turnover to $23.7 billion according to a World Gold Council report.
Investment.
Many holders of gold store it in form of bullion coins or bars as a hedge against inflation or other economic disruptions. However, economist Martin Feldstein does not believe gold serves as a hedge against inflation or currency depreciation.
The ISO 4217 currency code of gold is XAU.
Modern bullion coins for investment or collector purposes do not require good mechanical wear properties; they are typically fine gold at 24k, although the American Gold Eagle and the British gold sovereign continue to be minted in 22k (0.92) metal in historical tradition, and the South African Krugerrand, first released in 1967, is also 22k (0.92). The "special issue" Canadian Gold Maple Leaf coin contains the highest purity gold of any bullion coin, at 99.999% or 0.99999, while the "popular issue" Canadian Gold Maple Leaf coin has a purity of 99.99%.
Several other 99.99% pure gold coins are available. In 2006, the United States Mint began producing the American Buffalo gold bullion coin with a purity of 99.99%. The Australian Gold Kangaroos were first coined in 1986 as the Australian Gold Nugget but changed the reverse design in 1989. Other modern coins include the Austrian Vienna Philharmonic bullion coin and the Chinese Gold Panda.
Electronics connectors.
Only 10% of the world consumption of new gold produced goes to industry, but by far the most important industrial use for new gold is in fabrication of corrosion-free electrical connectors in computers and other electrical devices. For example, according to the World Gold council, a typical cell phone may contain 50 mg of gold, worth about 50 cents. But since nearly one billion cell phones are produced each year, a gold value of 50 cents in each phone adds to $500 million in gold from just this application.
Though gold is attacked by free chlorine, its good conductivity and general resistance to oxidation and corrosion in other environments (including resistance to non-chlorinated acids) has led to its widespread industrial use in the electronic era as a thin-layer coating on electrical connectors, thereby ensuring good connection. For example, gold is used in the connectors of the more expensive electronics cables, such as audio, video and USB cables. The benefit of using gold over other connector metals such as tin in these applications has been debated; gold connectors are often criticized by audio-visual experts as unnecessary for most consumers and seen as simply a marketing ploy. However, the use of gold in other applications in electronic sliding contacts in highly humid or corrosive atmospheres, and in use for contacts with a very high failure cost (certain computers, communications equipment, spacecraft, jet aircraft engines) remains very common.
Besides sliding electrical contacts, gold is also used in electrical contacts because of its resistance to corrosion, electrical conductivity, ductility and lack of toxicity. Switch contacts are generally subjected to more intense corrosion stress than are sliding contacts. Fine gold wires are used to connect semiconductor devices to their packages through a process known as wire bonding.
The concentration of free electrons in gold metal is 5.90×1022 cm−3. Gold is highly conductive to electricity, and has been used for electrical wiring in some high-energy applications (only silver and copper are more conductive per volume, but gold has the advantage of corrosion resistance). For example, gold electrical wires were used during some of the Manhattan Project's atomic experiments, but large high-current silver wires were used in the calutron isotope separator magnets in the project.
Commercial chemistry.
Gold is attacked by and dissolves in alkaline solutions of potassium or sodium cyanide, to form the salt gold cyanide—a technique that has been used in extracting metallic gold from ores in the cyanide process. Gold cyanide is the electrolyte used in commercial electroplating of gold onto base metals and electroforming.
Gold chloride (chloroauric acid) solutions are used to make colloidal gold by reduction with citrate or ascorbate ions. Gold chloride and gold oxide are used to make cranberry or red-colored glass, which, like colloidal gold suspensions, contains evenly sized spherical gold nanoparticles.
Medicine.
Metallic and gold compounds have been used for medicinal purposes historically and are still in use. The apparent paradox of the actual toxicology of the substance suggests the possibility of serious gaps in the understanding of the action of gold in physiology.
Gold (usually as the metal) is perhaps the most anciently administered medicine (apparently by shamanic practitioners) and known to Dioscorides. In medieval times, gold was often seen as beneficial for the health, in the belief that something so rare and beautiful could not be anything but healthy. Even some modern esotericists and forms of alternative medicine assign metallic gold a healing power.
In the 19th century gold had a reputation as a "nervine," a therapy for nervous disorders. Depression, epilepsy, migraine, and glandular problems such as amenorrhea and impotence were treated, and most notably alcoholism (Keeley, 1897).
Only salts and radioisotopes of gold are of pharmacological value, since elemental (metallic) gold is inert to all chemicals it encounters inside the body (i.e., ingested gold cannot be attacked by stomach acid). Some gold salts do have anti-inflammatory properties and at present two are still used as pharmaceuticals in the treatment of arthritis and other similar conditions in the US (sodium aurothiomalate and auranofin). These drugs have been explored as a means to help to reduce the pain and swelling of rheumatoid arthritis, and also (historically) against tuberculosis and some parasites.
Gold alloys are used in restorative dentistry, especially in tooth restorations, such as crowns and permanent bridges. The gold alloys' slight malleability facilitates the creation of a superior molar mating surface with other teeth and produces results that are generally more satisfactory than those produced by the creation of porcelain crowns. The use of gold crowns in more prominent teeth such as incisors is favored in some cultures and discouraged in others.
Colloidal gold preparations (suspensions of gold nanoparticles) in water are intensely red-colored, and can be made with tightly controlled particle sizes up to a few tens of nanometers across by reduction of gold chloride with citrate or ascorbate ions. Colloidal gold is used in research applications in medicine, biology and materials science. The technique of immunogold labeling exploits the ability of the gold particles to adsorb protein molecules onto their surfaces. Colloidal gold particles coated with specific antibodies can be used as probes for the presence and position of antigens on the surfaces of cells. In ultrathin sections of tissues viewed by electron microscopy, the immunogold labels appear as extremely dense round spots at the position of the antigen.
Gold, or alloys of gold and palladium, are applied as conductive coating to biological specimens and other non-conducting materials such as plastics and glass to be viewed in a scanning electron microscope. The coating, which is usually applied by sputtering with an argon plasma, has a triple role in this application. Gold's very high electrical conductivity drains electrical charge to earth, and its very high density provides stopping power for electrons in the electron beam, helping to limit the depth to which the electron beam penetrates the specimen. This improves definition of the position and topography of the specimen surface and increases the spatial resolution of the image. Gold also produces a high output of secondary electrons when irradiated by an electron beam, and these low-energy electrons are the most commonly used signal source used in the scanning electron microscope.
The isotope gold-198 (half-life 2.7 days) is used, in nuclear medicine, in some cancer treatments and for treating other diseases.
Monetary exchange (historical).
Gold has been widely used throughout the world as money, for efficient indirect exchange (versus barter), and to store wealth in hoards. For exchange purposes, mints produce standardized gold bullion coins, bars and other units of fixed weight and purity.
The first known coins containing gold were struck in Lydia, Asia Minor, around 600 BC. The "talent" coin of gold in use during the periods of Grecian history both before and during the time of the life of Homer weighed between 8.42 and 8.75 grams. From an earlier preference in using silver, European economies re-established the minting of gold as coinage during the thirteenth and fourteenth centuries.
Bills (that mature into gold coin) and gold certificates (convertible into gold coin at the issuing bank) added to the circulating stock of gold standard money in most 19th century industrial economies.
In preparation for World War I the warring nations moved to fractional gold standards, inflating their currencies to finance the war effort. 
Post-war, the victorious countries, most notably Britain, gradually restored gold-convertibility, but international flows of gold via bills of exchange remained embargoed; international shipments were made exclusively for bilateral trades or to pay war reparations.
After World War II gold was replaced by a system of nominally convertible currencies related by fixed exchange rates following the Bretton Woods system. Gold standards and the direct convertibility of currencies to gold have been abandoned by world governments, led in 1971 by the United States' refusal to redeem its dollars in gold. Fiat currency now fills most monetary roles. Switzerland was the last country to tie its currency to gold; it backed 40% of its value until the Swiss joined the International Monetary Fund in 1999.
Central banks continue to keep a portion of their liquid reserves as gold in some form, and metals exchanges such as the London Bullion Market Association still clear transactions denominated in gold, including future delivery contracts.
Today, gold mining output is declining. 
With the sharp growth of economies in the 20th century, and increasing foreign exchange, the world's gold reserves and their trading market have become a small fraction of all markets and fixed exchange rates of currencies to gold have been replaced by floating prices for gold and gold future contract.
Though the gold stock grows by only 1 or 2% per year, very little metal is irretrievably consumed. Inventory above ground would satisfy many decades of industrial and even artisan uses at current prices.
The gold content of alloys is measured in carats (k). Pure gold is designated as 24k. English gold coins intended for circulation from 1526 into the 1930s were typically a standard 22k alloy called crown gold, for hardness (American gold coins for circulation after 1837 contained the slightly lower amount of 0.900 fine gold, or 21.6 kt).
Although the prices of some platinum group metals can be much higher, gold has long been considered the most desirable of precious metals, and its value has been used as the standard for many currencies. Gold has been used as a symbol for purity, value, royalty, and particularly roles that combine these properties. Gold as a sign of wealth and prestige was ridiculed by Thomas More in his treatise "Utopia". On that imaginary island, gold is so abundant that it is used to make chains for slaves, tableware, and lavatory seats. When ambassadors from other countries arrive, dressed in ostentatious gold jewels and badges, the Utopians mistake them for menial servants, paying homage instead to the most modestly dressed of their party.
Cultural history.
Gold artifacts found at the Nahal Kana cave cemetery dated during the 1980s, showed these to be from within the Chalcolithic, and considered the earliest find from the Levant (Gopher "et al." 1990). Gold artifacts in the Balkans also appear from the 4th millennium BC, such as those found in the Varna Necropolis near Lake Varna in Bulgaria, thought by one source (La Niece 2009) to be the earliest "well-dated" find of gold artifacts. Gold artifacts such as the golden hats and the Nebra disk appeared in Central Europe from the 2nd millennium BC Bronze Age.
Egyptian hieroglyphs from as early as 2600 BC describe gold, which King Tushratta of the Mitanni claimed was "more plentiful than dirt" in Egypt. Egypt and especially Nubia had the resources to make them major gold-producing areas for much of history. One of the earliest known maps, known as the Turin Papyrus Map, shows the plan of a gold mine in Nubia together with indications of the local geology. The primitive working methods are described by both Strabo and Diodorus Siculus, and included fire-setting. Large mines were also present across the Red Sea in what is now Saudi Arabia.
The legend of the golden fleece may refer to the use of fleeces to trap gold dust from placer deposits in the ancient world. Gold is mentioned frequently in the Old Testament, starting with Genesis 2:11 (at Havilah), the story of The Golden Calf and many parts of the temple including the Menorah and the golden altar. In the New Testament, it is included with the gifts of the magi in the first chapters of Matthew. The Book of Revelation 21:21 describes the city of New Jerusalem as having streets "made of pure gold, clear as crystal". Exploitation of gold in the south-east corner of the Black Sea is said to date from the time of Midas, and this gold was important in the establishment of what is probably the world's earliest coinage in Lydia around 610 BC. From the 6th or 5th century BC, the Chu (state) circulated the Ying Yuan, one kind of square gold coin.
In Roman metallurgy, new methods for extracting gold on a large scale were developed by introducing hydraulic mining methods, especially in Hispania from 25 BC onwards and in Dacia from 106 AD onwards. One of their largest mines was at Las Medulas in León (Spain), where seven long aqueducts enabled them to sluice most of a large alluvial deposit. The mines at Roşia Montană in Transylvania were also very large, and until very recently, still mined by opencast methods. They also exploited smaller deposits in Britain, such as placer and hard-rock deposits at Dolaucothi. The various methods they used are well described by Pliny the Elder in his encyclopedia Naturalis Historia written towards the end of the first century AD.
During Mansa Musa's (ruler of the Mali Empire from 1312 to 1337) hajj to Mecca in 1324, he passed through Cairo in July 1324, and was reportedly accompanied by a camel train that included thousands of people and nearly a hundred camels where he gave away so much gold that it depressed the price in Egypt for over a decade. A contemporary Arab historian remarked:
The European exploration of the Americas was fueled in no small part by reports of the gold ornaments displayed in great profusion by Native American peoples, especially in Mesoamerica, Peru, Ecuador and Colombia. The Aztecs regarded gold as the product of the gods, calling it literally "god excrement" ("teocuitlatl" in Nahuatl), and after Moctezuma II was killed, most of this gold was shipped to Spain. However, for the indigenous peoples of North America gold was considered useless and they saw much greater value in other minerals which were directly related to their utility, such as obsidian, flint, and slate. Rumors of cities filled with gold fueled legends of El Dorado.
Gold played a role in western culture, as a cause for desire and of corruption, as told in children's fables such as Rumpelstiltskin, where the peasant's daughter turns hay into gold, in return for giving up her child when she becomes a princess; and the stealing of the hen that lays golden eggs in Jack and the Beanstalk.
The top prize at the Olympic games is the gold medal.
75% of all gold ever produced has been extracted since 1910. It has been estimated that all the gold ever refined would form a single cube 20 m (66 ft) on a side (equivalent to 8,000 m3).
One main goal of the alchemists was to produce gold from other substances, such as lead — presumably by the interaction with a mythical substance called the philosopher's stone. Although they never succeeded in this attempt, the alchemists did promote an interest in systematically finding out what can be done with substances, and this laid the foundation for today's chemistry. Their symbol for gold was the circle with a point at its center (☉), which was also the astrological symbol and the ancient Chinese character for the Sun.
Golden treasures have been rumored to be found at various locations, following tragedies such as the Jewish temple treasures in the Vatican, following the temple's destruction in 70 AD, a gold stash on the Titanic, the Nazi gold train – following World War II.
The Dome of the Rock on the Jerusalem temple site is covered with an ultra-thin golden glasure. The Sikh Golden temple, the Harmandir Sahib, is a building covered with gold. Similarly the Wat Phra Kaew emerald Buddhist temple (wat) in Thailand has ornamental gold statues walls and roofs. Some European king and queen's crowns were made of gold, and gold was used for the bridal crown since antiquity. An ancient Talmudic text circa 100 AD describes Rachel, Rabbi Akiba's wife asking for a "Jerusalem of Gold" (crown). A Greek burial crown made of gold was found in a grave circa 370 BC.
Occurrence.
Gold's atomic number of 79 makes it one of the higher atomic number elements that occur naturally. Traditionally, gold is thought to have formed by the R-process in supernova nucleosynthesis, but a relatively recent paper suggests that gold and other elements heavier than iron may also be produced in quantity by the collision of neutron stars. In both cases, satellite spectrometers only indirectly detect the resulting gold: "we have no spectroscopic evidence that elements have truly been produced."
These gold nucleogenesis theories hold that the resulting explosions scattered metal-containing dusts (including heavy elements such as gold) into the region of space in which they later condensed into our solar system and the Earth. Because the Earth was molten when it was just formed, almost all of the gold present on Earth sank into the core. Most of the gold that is present today in the Earth's crust and mantle is thought to have been delivered to Earth later, by asteroid impacts during the Late Heavy Bombardment.
The asteroid that formed Vredefort crater 2.020 billion years ago is often credited with seeding the Witwatersrand basin in South Africa with the richest gold deposits on earth. However, the gold-bearing Witwatersrand rocks were laid down between 700 and 950 million years before the Vredefort impact. These gold-bearing rocks had furthermore been covered by a thick layer of Ventersdorp lavas and the Transvaal Supergroup of rocks before the meteor struck. What the Vredefort impact achieved, however, was to distort the Witwatersrand basin in such a way that the gold-bearing rocks were brought to the present erosion surface in Johannesburg, on the Witwatersrand, just inside the rim of the original 300 km diameter crater caused by the meteor strike. The discovery of the deposit in 1886 launched the Witwatersrand Gold Rush. Nearly 50% of all the gold ever mined on earth has been extracted from these Witwatersrand rocks.
On Earth, gold is found in ores in rock formed from the Precambrian time onward. It most often occurs as a native metal, typically in a metal solid solution with silver (i.e. as a gold silver alloy). Such alloys usually have a silver content of 8–10%. Electrum is elemental gold with more than 20% silver. Electrum's color runs from golden-silvery to silvery, dependent upon the silver content. The more silver, the lower the specific gravity.
Native gold occurs as very small to microscopic particles embedded in rock, often together with quartz or sulfide minerals such as "Fool's Gold", which is a pyrite. These are called lode deposits. The metal in a native state is also found in the form of free flakes, grains or larger nuggets that have been eroded from rocks and end up in alluvial deposits called placer deposits. Such free gold is always richer at the surface of gold-bearing veins owing to the oxidation of accompanying minerals followed by weathering, and washing of the dust into streams and rivers, where it collects and can be welded by water action to form nuggets.
Gold sometimes occurs combined with tellurium as the minerals calaverite, krennerite, nagyagite, petzite and sylvanite (see telluride minerals), and as the rare bismuthide maldonite (Au2Bi) and antimonide aurostibite (AuSb2). Gold also occurs in rare alloys with copper, lead, and mercury: the minerals auricupride (Cu3Au), novodneprite (AuPb3) and weishanite ((Au, Ag)3Hg2).
Recent research suggests that microbes can sometimes play an important role in forming gold deposits, transporting and precipitating gold to form grains and nuggets that collect in alluvial deposits.
Another recent study has claimed water in faults vaporizes during an earthquake, depositing gold. When an earthquake strikes, it moves along a fault. Water often lubricates faults, filling in fractures and jogs. About 6 miles (10 kilometers) below the surface, under incredible temperatures and pressures, the water carries high concentrations of carbon dioxide, silica, and gold. During an earthquake, the fault jog suddenly opens wider. The water inside the void instantly vaporizes, flashing to steam and forcing silica, which forms the mineral quartz, and gold out of the fluids and onto nearby surfaces.
Seawater.
The world's oceans contain gold. Measured concentrations of gold in the Atlantic and Northeast Pacific are 50–150 femtomol/L or 10–30 parts per quadrillion (about 10–30 g/km3). In general, gold concentrations for south Atlantic and central Pacific samples are the same (~50 femtomol/L) but less certain. Mediterranean deep waters contain slightly higher concentrations of gold (100–150 femtomol/L) attributed to wind-blown dust and/or rivers. At 10 parts per quadrillion the Earth's oceans would hold 15,000 tonnes of gold. These figures are three orders of magnitude less than reported in the literature prior to 1988, indicating contamination problems with the earlier data.
A number of people have claimed to be able to economically recover gold from sea water, but so far they have all been either mistaken or acted in an intentional deception. Prescott Jernegan ran a gold-from-seawater swindle in the United States in the 1890s. A British fraudster ran the same scam in England in the early 1900s. Fritz Haber (the German inventor of the Haber process) did research on the extraction of gold from sea water in an effort to help pay Germany's reparations following World War I. Based on the published values of 2 to 64 ppb of gold in seawater a commercially successful extraction seemed possible. After analysis of 4,000 water samples yielding an average of 0.004 ppb it became clear that the extraction would not be possible and he stopped the project. No commercially viable mechanism for performing gold extraction from sea water has yet been identified. Gold synthesis is not economically viable and is unlikely to become so in the foreseeable future.
Production.
The World Gold Council states that as of the end of 2014, "there were 183,600 tonnes of stocks in existence above ground". This can be represented by a cube with an edge length of about 21 meters. At $1,075 per troy ounce, 183,600 metric tonnes of gold would have a value of $6.3 trillion.
World production for 2014 totaled 2,990 tonnes, compared to 2,300 tonnes for 2008.
Mining.
Since the 1880s, South Africa has been the source for a large proportion of the world's gold supply, with about 50% of all gold ever produced having come from South Africa. Production in 1970 accounted for 79% of the world supply, producing about 1,480 tonnes. In 2007 China (with 276 tonnes) overtook South Africa as the world's largest gold producer, the first time since 1905 that South Africa has not been the largest.
As of 2013, China was the world's leading gold-mining country, followed in order by Australia, the United States, Russia, and Peru. South Africa, which had dominated world gold production for most of the 20th Century, had declined to sixth place. Other major producers are the Ghana, Burkina Faso, Mali, Indonesia and Uzbekistan.
In South America, the controversial project Pascua Lama aims at exploitation of rich fields in the high mountains of Atacama Desert, at the border between Chile and Argentina.
Today about one-quarter of the world gold output is estimated to originate from artisanal or small scale mining.
The city of Johannesburg located in South Africa was founded as a result of the Witwatersrand Gold Rush which resulted in the discovery of some of the largest gold deposits the world has ever seen. The gold fields are confined to the northern and north-western edges of the Witwatersrand basin, which is a 5–7 km thick layer of archean rocks located, in most places, deep under the Free State, Gauteng and surrounding provinces. These Witwatersrand rocks are exposed at the surface on the Witwatersrand, in and around Johannesburg, but also in isolated patches to the south-east and south-west of Johannesburg, as well as in an arc around the Vredefort Dome which lies close to the center of the Witwatersrand basin. From these surface exposures the basin dips extensively, requiring some of the mining to occur at depths of nearly 4000 m, making them, especially the Savuka and TauTona mines to the south-west of Johannesburg, the deepest mines on earth. The gold is found only in six areas where archean rivers from the north and north-west formed extensive pebbly braided river deltas before draining into the "Witwatersrand sea" where the rest of the Witwatersrand sediments were deposited.
The Second Boer War of 1899–1901 between the British Empire and the Afrikaner Boers was at least partly over the rights of miners and possession of the gold wealth in South Africa.
Prospecting.
During the 19th century, gold rushes occurred whenever large gold deposits were discovered. The first documented discovery of gold in the United States was at the Reed Gold Mine near Georgeville, North Carolina in 1803. The first major gold strike in the United States occurred in a small north Georgia town called Dahlonega. Further gold rushes occurred in California, Colorado, the Black Hills, Otago in New Zealand, Australia, Witwatersrand in South Africa, and the Klondike in Canada.
Bioremediation.
A sample of the fungus "Aspergillus niger" was found growing from gold mining solution; and was found to contain cyano metal complexes; such as gold, silver, copper iron and zinc. The fungus also plays a role in the solubilization of heavy metal sulfides.
Extraction.
Gold extraction is most economical in large, easily mined deposits. Ore grades as little as 0.5 mg/kg (0.5 parts per million, ppm) can be economical. Typical ore grades in open-pit mines are 1–5 mg/kg (1–5 ppm); ore grades in underground or hard rock mines are usually at least 3 mg/kg (3 ppm). Because ore grades of 30 mg/kg (30 ppm) are usually needed before gold is visible to the naked eye, in most gold mines the gold is invisible.
The average gold mining and extraction costs were about US$317/oz in 2007, but these can vary widely depending on mining type and ore quality; global mine production amounted to 2,471.1 tonnes.
Refining.
After initial production, gold is often subsequently refined industrially by the Wohlwill process which is based on electrolysis or by the Miller process, that is chlorination in the melt. The Wohlwill process results in higher purity, but is more complex and is only applied in small-scale installations. Other methods of assaying and purifying smaller amounts of gold include parting and inquartation as well as cupellation, or refining methods based on the dissolution of gold in aqua regia.
Synthesis from other elements.
Gold was synthesized from mercury by neutron bombardment in 1941, but the isotopes of gold produced were all radioactive. In 1924, a Japanese physicist, Hantaro Nagaoka, accomplished the same feat.
Gold can currently be manufactured in a nuclear reactor by irradiation either of platinum or mercury.
Only the mercury isotope 196Hg, which occurs with a frequency of 0.15% in natural mercury, can be converted to gold by neutron capture, and following electron capture-decay into 197Au with slow neutrons. Other mercury isotopes are converted when irradiated with slow neutrons into one another, or formed mercury isotopes which beta decay into thallium.
Using fast neutrons, the mercury isotope 198Hg, which composes 9.97% of natural mercury, can be converted by splitting off a neutron and becoming 197Hg, which then disintegrates to stable gold. This reaction, however, possesses a smaller activation cross-section and is feasible only with un-moderated reactors.
It is also possible to eject several neutrons with very high energy into the other mercury isotopes in order to form 197Hg. However such high-energy neutrons can be produced only by particle accelerators.
Consumption.
The consumption of gold produced in the world is about 50% in jewelry, 40% in investments, and 10% in industry.
According to World Gold Council, China is the world's largest single consumer of gold in 2013 and toppled India for the first time with Chinese consumption increasing by 32 percent in a year, while that of India only rose by 13 percent and world consumption rose by 21 percent. Unlike India where gold is used for mainly for jewellery, China uses gold for manufacturing and retail.
Pollution.
Gold production is associated with contribution to hazardous pollution.
Low-grade gold ore may contain less than one ppm gold metal; such ore is ground and mixed with sodium cyanide dissolve the gold. Cyanide is a highly poisonous chemical, which can kill living creatures when exposed in minute quantities. Many cyanide spills from gold mines have occurred in both developed and developing countries which killed aquatic life in long stretches of affected rivers. Environmentalists consider these events major environmental disasters. Thirty tons of used ore is dumped as waste for producing one troy ounce of gold. Gold ore dumps are the source of many heavy elements such as cadmium, lead, zinc, copper, arsenic, selenium and mercury. When sulfide bearing minerals in these ore dumps are exposed to air and water, the sulfide transforms into sulfuric acid which in turn dissolves these heavy metals facilitating their passage into surface water and ground water. This process is called acid mine drainage. These gold ore dumps are long term, highly hazardous wastes second only to nuclear waste dumps.
It was once common to use mercury to recover gold from ore, but today the use of mercury is largely limited to small-scale individual miners. Minute quantities of mercury compounds can reach water bodies, causing heavy metal contamination. Mercury can then enter into the human food chain in the form of methylmercury. Mercury poisoning in humans causes incurable brain function damage and severe retardation.
Gold extraction is also a highly energy intensive industry, extracting ore from deep mines and grinding the large quantity of ore for further chemical extraction requires nearly 25 kW·h of electricity per gram of gold produced.
Toxicity.
Pure metallic (elemental) gold is non-toxic and non-irritating when ingested and is sometimes used as a food decoration in the form of gold leaf. Metallic gold is also a component of the alcoholic drinks Goldschläger, Gold Strike, and Goldwasser. Metallic gold is approved as a food additive in the EU (E175 in the Codex Alimentarius). Although the gold ion is toxic, the acceptance of metallic gold as a food additive is due to its relative chemical inertness, and resistance to being corroded or transformed into soluble salts (gold compounds) by any known chemical process which would be encountered in the human body.
Soluble compounds (gold salts) such as gold chloride are toxic to the liver and kidneys. Common cyanide salts of gold such as potassium gold cyanide, used in gold electroplating, are toxic by virtue of both their cyanide and gold content. There are rare cases of lethal gold poisoning from potassium gold cyanide. Gold toxicity can be ameliorated with chelation therapy with an agent such as dimercaprol.
Gold metal was voted Allergen of the Year in 2001 by the American Contact Dermatitis Society. Gold contact allergies affect mostly women. Despite this, gold is a relatively non-potent contact allergen, in comparison with metals like nickel.
Price.
As at December 2015, gold is valued at around US$39,000 per kilogram (US$1,200 per troy ounce).
Like other precious metals, gold is measured by troy weight and by grams. When it is alloyed with other metals the term "carat" or "karat" is used to indicate the purity of gold present, with 24 carats being pure gold and lower ratings proportionally less. The purity of a gold bar or coin can also be expressed as a decimal figure ranging from 0 to 1, known as the millesimal fineness, such as 0.995 being very pure.
History.
The price of gold is determined through trading in the gold and derivatives markets, but a procedure known as the Gold Fixing in London, originating in September 1919, provides a daily benchmark price to the industry. The afternoon fixing was introduced in 1968 to provide a price when US markets are open.
Historically gold coinage was widely used as currency; when paper money was introduced, it typically was a receipt redeemable for gold coin or bullion. In a monetary system known as the gold standard, a certain weight of gold was given the name of a unit of currency. For a long period, the United States government set the value of the US dollar so that one troy ounce was equal to $20.67 ($664.56/kg), but in 1934 the dollar was devalued to $35.00 per troy ounce ($1125.27/kg). By 1961, it was becoming hard to maintain this price, and a pool of US and European banks agreed to manipulate the market to prevent further currency devaluation against increased gold demand.
On 17 March 1968, economic circumstances caused the collapse of the gold pool, and a two-tiered pricing scheme was established whereby gold was still used to settle international accounts at the old $35.00 per troy ounce ($1.13/g) but the price of gold on the private market was allowed to fluctuate; this two-tiered pricing system was abandoned in 1975 when the price of gold was left to find its free-market level. Central banks still hold historical gold reserves as a store of value although the level has generally been declining. The largest gold depository in the world is that of the U.S. Federal Reserve Bank in New York, which holds about 3% of the gold ever mined, as does the similarly laden U.S. Bullion Depository at Fort Knox.
In 2005 the World Gold Council estimated total global gold supply to be 3,859 tonnes and demand to be 3,754 tonnes, giving a surplus of 105 tonnes.
Sometime around 1970 the price began in trend to greatly increase, and between 1968 and 2000 the price of gold ranged widely, from a high of $850/oz ($27,300/kg) on 21 January 1980, to a low of $252.90/oz ($8,131/kg) on 21 June 1999 (London Gold Fixing). Prices increased rapidly from 2001, but the 1980 high was not exceeded until 3 January 2008 when a new maximum of $865.35 per troy ounce was set. Another record price was set on 17 March 2008 at $1023.50/oz ($32,900/kg).
In late 2009, gold markets experienced renewed momentum upwards due to increased demand and a weakening US dollar. On 2 December 2009, Gold reached a new high closing at $1,217.23. Gold further rallied hitting new highs in May 2010 after the European Union debt crisis prompted further purchase of gold as a safe asset. On 1 March 2011, gold hit a new all-time high of $1432.57, based on investor concerns regarding ongoing unrest in North Africa as well as in the Middle East.
From April 2001 to August 2011, spot gold prices more than quintupled in value against the US dollar, hitting a new all-time high of $1,913.50 on 23 August 2011, prompting speculation that the long secular bear market had ended and a bull market had returned. However, the price then began a slow decline to the $1200-per-ounce range in late 2014 and 2015.
Symbolism.
Great human achievements are frequently rewarded with gold, in the form of gold medals, golden trophies and other decorations. Winners of athletic events and other graded competitions are usually awarded a gold medal. Many awards such as the Nobel Prize are made from gold as well. Other award statues and prizes are depicted in gold or are gold plated (such as the Academy Awards, the Golden Globe Awards, the Emmy Awards, the Palme d'Or, and the British Academy Film Awards).
Aristotle in his ethics used gold symbolism when referring to what is now commonly known as the golden mean. Similarly, gold is associated with perfect or divine principles, such as in the case of the golden ratio and the golden rule.
Gold is further associated with the wisdom of aging and fruition. The fiftieth wedding anniversary is golden. Our most valued or most successful latter years are sometimes considered "golden years". The height of a civilization is referred to as a "golden age".
In some forms of Christianity and Judaism, gold has been associated both with holiness and evil. In the Book of Exodus, the Golden Calf is a symbol of idolatry, while in the Book of Genesis, Abraham was said to be rich in gold and silver, and Moses was instructed to cover the Mercy Seat of the Ark of the Covenant with pure gold. In Byzantine iconography the halos of Christ, Mary and the Christian saints are often golden.
According to Christopher Columbus, those who had something of gold were in possession of something of great value on Earth and a substance to even help souls to paradise.
Wedding rings have long been made of gold. It is long lasting and unaffected by the passage of time and may aid in the ring symbolism of eternal vows before God and the perfection the marriage signifies. In Orthodox Christian wedding ceremonies, the wedded couple is adorned with a golden crown (though some opt for wreaths, instead) during the ceremony, an amalgamation of symbolic rites.
In popular culture gold has many connotations but is most generally connected to terms such as good or great, such as in the phrases: "has a heart of gold", "that's golden!", "golden moment", "then you're golden!" and "golden boy". It remains a cultural symbol of wealth and through that, in many societies, success.

</doc>
<doc id="12241" url="https://en.wikipedia.org/wiki?curid=12241" title="Gallium">
Gallium

Gallium is a chemical element with symbol Ga and atomic number 31. Elemental gallium does not occur in free form in nature, but as the gallium(III) compounds that are in trace amounts in zinc ores and in bauxite. Gallium is a soft, silvery metal, and elemental gallium is a brittle solid at low temperatures, and melts at (slightly above room temperature). The melting point of gallium is used as a temperature reference point. The alloy galinstan (68.5% gallium, 21.5% indium, and 10% tin) has an even lower melting point of , well below the freezing point of water. Since its discovery in 1875, gallium has been used as an agent to make alloys that melt at low temperatures. It has also been useful in semiconductors, including as a dopant.
Gallium is predominantly used in electronics. Gallium arsenide, the primary chemical compound of gallium in electronics, is used in microwave circuits, high-speed switching circuits, and infrared circuits. Semiconductive gallium nitride and indium gallium nitride produce blue and violet light-emitting diodes (LEDs) and diode lasers. Gallium is also used in the production of artificial gadolinium gallium garnet for jewelry.
Gallium has no known natural role in biology. Gallium(III) behaves in a similar manner to ferric salts in biological systems and has been used in some medical applications, including pharmaceuticals and radiopharmaceuticals. Gallium thermometers are manufactured as an eco-friendly alternative to mercury thermometers.
Physical properties.
Elemental gallium is not found in nature, but it is easily obtained by smelting. Very pure gallium metal has a silvery color and its solid metal fractures conchoidally like glass. Gallium metal expands by 3.1% when it solidifies, and therefore storage in either glass or metal containers is avoided, due to the possibility of container rupture with freezing. Gallium shares the higher-density liquid state with only a few materials, like water, silicon, germanium, bismuth, and plutonium.
Gallium attacks most other metals by diffusing into their metal lattice. Gallium, for example, diffuses into the grain boundaries of aluminium-zinc alloys or steel, making them very brittle. Gallium easily alloys with many metals, and is used in small quantities as a plutonium-gallium alloy in the plutonium cores of nuclear bombs, to help stabilize the plutonium crystal structure.
The melting point of gallium, at 302.9146 K (29.7646 °C, 85.5763 °F), is just above room temperature, and is approximately the same as the average summer daytime temperatures in Earth's mid-latitudes. Gallium's melting point (mp) is one of the formal temperature reference points in the International Temperature Scale of 1990 (ITS-90) established by the BIPM. The triple point of gallium, at 302.9166 K (29.7666 °C, 85.5799 °F), is being used by NIST in preference to gallium's melting point.
The unique melting point of gallium allows it to melt in one's hand, and then refreeze if removed. This metal has a strong tendency to supercool below its melting point/freezing point. Seeding with a crystal helps to initiate freezing. Gallium is one of the metals (with caesium, rubidium, mercury, and likely francium) that are liquid at or near-normal room temperature, and can therefore be used in metal-in-glass high-temperature thermometers. It is also notable for having one of the largest liquid ranges for a metal, and for having (unlike mercury) a low vapor pressure at high temperatures. Gallium's boiling point, 2673 K, is more than eight times higher than its melting point on the absolute scale, making it the greatest ratio between melting point and boiling point of any element. Unlike mercury, liquid gallium metal wets glass and skin, making it mechanically more difficult to handle (even though it is substantially less toxic and requires far fewer precautions). For this reason as well as the metal contamination and freezing-expansion problems, samples of gallium metal are usually supplied in polyethylene packets within other containers.
Gallium does not crystallize in any of the simple crystal structures. The stable phase under normal conditions is orthorhombic with 8 atoms in the conventional unit cell. Within a unit cell, each atom has only one nearest neighbor (at a distance of 244 pm). The remaining six unit cell neighbors are spaced 27, 30 and 39 pm farther away, and they are grouped in pairs with the same distance. Many stable and metastable phases are found as function of temperature and pressure.
The bonding between the two nearest neighbors is covalent, hence Ga2 dimers are seen as the fundamental building blocks of the crystal. This explains the drop of the melting point compared to its neighbor elements aluminium and indium.
The physical properties of gallium are highly anisotropic, i.e. have different values along the three major crystallographical axes "a", "b", and "c" (see table); for this reason, there is a significant difference between the linear (α) and volume thermal expansion coefficients. The properties of gallium are also strongly temperature-dependent, especially near the melting point. For example, the thermal expansion coefficient increases by several hundred percent upon melting.
Chemical properties.
Gallium is found primarily in the +3 oxidation state. The +1 oxidation is also attested in some compounds. For example, the very stable GaCl2 contains both gallium(I) and gallium(III) and can be formulated as GaIGaIIICl4, in contrast the monochloride is unstable above 0 °C disproportionating into elemental gallium and gallium(III) chloride. Compounds containing gallium-gallium bonds, are true gallium(II) compounds for example GaS can be formulated Ga24+(S2−)2, and the dioxan complex Ga2Cl4(C4H8O2)2 contains a Ga-Ga bond.
Chalcogen compounds.
Gallium reacts with chalcogen elements (sometimes called the "oxygen family") only at relatively high temperatures. At room temperature, gallium metal is unreactive towards air and water due to the formation of a passive, protective oxide layer. At higher temperatures, however, it reacts with oxygen in the air to form gallium(III) oxide, .
Reducing with elemental gallium in vacuum at 500 °C to 700 °C yields the dark brown gallium(I) oxide, . is a very strong reducing agent, capable of reducing sulfuric acid to hydrogen sulfide. It disproportionates at 800 °C back to gallium and .
Gallium(III) sulfide, , has 3 possible crystal modifications. It can be made by the reaction of gallium with hydrogen sulfide () at 950 °C. Alternatively, can also be used at 747 °C:
Reacting a mixture of alkali metal carbonates and with leads to the formation of "thiogallates" containing the anion. Strong acids decompose these salts, releasing in the process. The mercury salt, , can be used as a phosphor.
Gallium also forms sulfides in lower oxidation states, such as gallium(II) sulfide and the green gallium(I) sulfide, the latter of which is produced from the former by heating to 1000 °C under a stream of nitrogen.
The other binary chalcogenides, and , have zincblende structure. They are all semiconductors, but are easily hydrolysed, limiting their usefulness.
Aqueous chemistry.
Strong acids dissolve gallium, forming gallium(III) salts such as gallium sulfate (gallium sulfate) and gallium(III) nitrate (gallium nitrate). Aqueous solutions of gallium(III) salts contain the hydrated gallium ion, . Gallium(III) hydroxide, , may be precipitated from gallium(III) solutions by adding ammonia. Dehydrating at 100 °C produces gallium oxide hydroxide, GaO(OH).
Alkaline hydroxide solutions dissolve gallium, forming "gallate" salts containing the anion. Gallium hydroxide, which is amphoteric, also dissolves in alkali to form gallate salts. Although earlier work suggested as another possible gallate anion, this species was not found in later work.
Pnictogen compounds.
Gallium reacts with ammonia at 1050 °C to form gallium nitride, GaN. Gallium also forms binary compounds with phosphorus, arsenic, and antimony: gallium phosphide (GaP), gallium arsenide (GaAs), and gallium antimonide (GaSb). These compounds have the same structure as ZnS, and have important semiconducting properties. GaP, GaAs, and GaSb can be synthesized by the direct reaction of gallium with elemental phosphorus, arsenic, or antimony. They exhibit higher electrical conductivity than GaN. GaP can also be synthesized by the reaction of with phosphorus at low temperatures.
Gallium also forms "ternary nitrides"; for example:
Similar compounds with phosphorus and arsenic also exist: and . These compounds are easily hydrolyzed by dilute acids and water.
Halides.
Gallium(III) oxide reacts with fluorinating agents such as HF or fluorine to form gallium(III) fluoride, . It is an ionic compound strongly insoluble in water. However, it does dissolve in hydrofluoric acid, in which it forms an adduct with water, . Attempting to dehydrate this adduct instead forms . The adduct reacts with ammonia to form , which can then be heated to form anhydrous .
Gallium trichloride is formed by the reaction of gallium metal with chlorine gas. Unlike the trifluoride, gallium(III) chloride exists as dimeric molecules, , with a melting point of 78 °C. This is also the case for the bromide and iodide, gallium(III) bromide and gallium(III) iodide.
Like the other group 13 trihalides, gallium(III) halides are Lewis acids, reacting as halide acceptors with alkali metal halides to form salts containing anions, where X is a halogen. They also react with alkyl halides to form carbocations and .
When heated to a high temperature, gallium(III) halides react with elemental gallium to form the respective gallium(I) halides. For example, reacts with Ga to form :
At lower temperatures, the equilibrium shifts toward the left and GaCl disproportionates back to elemental gallium and . GaCl can also be made by the reaction of Ga with HCl at 950 °C; it can then be condensed as red solid.
Gallium(I) compounds can be stabilized by forming adducts with Lewis acids. For example:
The so-called "gallium(II) halides", , are actually adducts of gallium(I) halides with the respective gallium(III) halides, having the structure . For example:
Hydrogen compounds.
Like aluminium, gallium also forms a hydride, , known as "gallane", which may be obtained by the reaction of lithium gallanate () with gallium(III) chloride at −30 °C:
In the presence of dimethyl ether as solvent, polymerizes to . If no solvent is used, the dimer ("digallane") is formed as a gas. Its structure is similar to diborane, having two hydrogen atoms bridging the two gallium centers, unlike α-aluminium hydride in which aluminium has a coordination number of 6.
Gallane is unstable above −10 °C, decomposing to elemental gallium and hydrogen.
History.
In 1871, existence of gallium was first predicted by Russian chemist Dmitri Mendeleev, who named it "eka-aluminium" on the basis of its position in his periodic table. He also predicted several properties of the element, which correspond closely to real gallium properties, such as density, melting point, oxide character and bonding in chloride.
Gallium was discovered spectroscopically by French chemist Paul Emile Lecoq de Boisbaudran in 1875 by its characteristic spectrum (two violet lines) in an examination of a sphalerite sample. Later that year, Lecoq obtained the free metal by electrolysis of its hydroxide in potassium hydroxide solution. He named the element "gallia", from Latin "Gallia" meaning Gaul, after his native land of France. It was later claimed that, in one of those multilingual puns so beloved of men of science in the 19th century, he had also named gallium after himself, as his name, "Le coq", is the French for "the rooster", and the Latin for "rooster" is ""gallus""; however, in an 1877 article Lecoq denied this supposition. (Cf. the naming of the J/ψ meson and the dwarf planet Pluto.)
From its discovery in 1875 up to the era of semiconductors, its primary uses were in high-temperature thermometric applications and in preparation of metal alloys with unusual properties of stability, or ease of melting; some being liquid at room temperature or below. The development of gallium arsenide as a direct band gap semiconductor in the 1960s ushered in the most important stage in the applications of gallium.
Occurrence.
Gallium does not exist in free form in nature, and the few high-gallium minerals such as gallite (CuGaS2) are too rare to serve as a primary source of the element or its compounds. Its abundance in the Earth's crust is approximately 16.9 ppm. Gallium is found and extracted as a trace component in bauxite and to a small extent from sphalerite. The amount extracted from coal, diaspore and germanite in which gallium is also present is negligible. The United States Geological Survey (USGS) estimates gallium reserves to exceed 1 million tonnes, based on 50 ppm by weight concentration in known reserves of bauxite and zinc ores. Some flue dusts from burning coal have been shown to contain small quantities of gallium, typically less than 1% by weight.
Production.
Gallium is a byproduct of the production of aluminium and zinc, whereas the sphalerite for zinc production is the minor source. Most gallium is extracted from the crude aluminium hydroxide solution of the Bayer process for producing alumina and aluminium. A mercury cell electrolysis and hydrolysis of the amalgam with sodium hydroxide leads to sodium gallate. Electrolysis then gives gallium metal. For semiconductor use, further purification is carried out using zone melting, or else single crystal extraction from a melt (Czochralski process). Purities of 99.9999% are routinely achieved and commercially widely available.
In 1986, the production was estimated at 40 tons. In 2007 the production of gallium was 184 tonnes with less than 100 tonnes from mining and the rest from scrap recycling. By 2012 world production of gallium was an estimated 273 metric tons. In January 2013, the market price of gallium lay at $580USD per kg, while by autumn that year, it had dropped to around $280USD per kg, or just under $9USD per ounce.
Applications.
The semiconductor applications dominate the commercial use of gallium, accounting for 98% of applications. The next major application is for gadolinium gallium garnets.
Semiconductors.
Because of this application, extremely high-purity (99.9999+%) gallium is commercially available. Gallium arsenide (GaAs) and gallium nitride (GaN) used in electronic components represented about 98% of the gallium consumption in the United States in 2007. About 66% of semiconductor gallium is used in the U.S. in integrated circuits (mostly gallium arsenide), such as the manufacture of ultra-high speed logic chips and MESFETs for low-noise microwave preamplifiers in cell phones. About 20% is used in optoelectronics. Worldwide, gallium arsenide makes up 95% of the annual global gallium consumption.
Gallium arsenide is used in optoelectronics in a variety of infrared applications. Aluminium gallium arsenide (AlGaAs) is used in high-powered infrared laser diodes. As a component of the semiconductors indium gallium nitride and gallium nitride, gallium is used to produce blue and violet optoelectronic devices, mostly laser diodes and light-emitting diodes. For example, gallium nitride 405 nm diode lasers are used as a violet light source for higher-density compact disc data storage, in the Blu-ray Disc standard.
Multijunction photovoltaic cells, developed for satellite power applications, are made by molecular beam epitaxy or metalorganic vapour phase epitaxy of thin films of gallium arsenide, indium gallium phosphide or indium gallium arsenide.The Mars Exploration Rovers and several satellites use triple junction gallium arsenide on germanium cells. Gallium is also a component in photovoltaic compounds (such as copper indium gallium selenium sulfide or Cu(In,Ga)(Se,S)2) for use in solar panels as a cost-efficient alternative to crystalline silicon.
Galinstan and other alloys.
Gallium readily alloys with most metals, and has been used as a component in low-melting alloys. A nearly eutectic alloy of gallium, indium, and tin is a room temperature liquid that is available in medical thermometers. This alloy, with the trade-name "Galinstan" (with the "-stan" referring to the tin), has a low freezing point of −19 °C (−2.2 °F). It has been suggested that this family of alloys could also be used to cool computer chips in place of water. Gallium alloys have been evaluated as substitutes for mercury dental amalgams, but these materials have yet to see wide acceptance.
Because gallium wets glass or porcelain, gallium can be used to create brilliant mirrors. When the wetting action of gallium-alloys is not desired (as in Galinstan glass thermometers), the glass must be protected with a transparent layer of gallium(III) oxide.
The plutonium used in nuclear weapon pits is machined by alloying with gallium to stabilize its δ phase.
Gallium added in quantities up to 2% in common solders can aid wetting and flow characteristics.
Alloys of Al and Ga have been evaluated for hydrogen production.
It is used as the alloying element in the magnetic shape-memory alloy Ni-Mn-Ga.
Biomedical applications.
Although gallium has no natural function in biology, gallium ions interact with processes in the body in a similar manner to iron(III). As these processes include inflammation, which is a marker for many disease states, several gallium salts are used, or are in development, as both pharmaceuticals and radiopharmaceuticals in medicine. When gallium ions are mistakenly taken up by bacteria such as "Pseudomonas", the bacteria's ability to respire is interfered with and the bacteria die. The mechanism behind this is that iron is redox active, which allows for the transfer of electrons during respiration, but gallium is redox inactive. 
Gallium nitrate (brand name Ganite) has been used as an intravenous pharmaceutical to treat hypercalcemia associated with tumor metastasis to bones. Gallium is thought to interfere with osteoclast function. It may be effective when other treatments for maligancy-associated hypercalcemia are not.
Gallium maltolate, a highly orally absorbable form of gallium(III) ion, is an anti-proliferative to pathologically proliferating cells, particularly cancer cells and some bacteria, due primarily to its ability to mimic ferric iron (Fe3+). It is undergoing clinical and preclinical trials as a potential treatment for a number of types of cancer, infectious disease, and inflammatory disease.
A complex amine-phenol Ga(III) compound MR045 was found to be selectively toxic to parasites that have developed resistance to chloroquine, a common drug against malaria. Both the Ga(III) complex and chloroquine act by inhibiting crystallization of hemozoin, a disposal product formed from the digestion of blood by the parasites.
Radiogallium salts.
Gallium-67 salts such as gallium citrate and gallium nitrate are used as radiopharmaceutical agents in a nuclear medicine imaging procedure commonly referred to as a gallium scan. The form or salt of gallium is unimportant. For these applications, the radioactive isotope 67Ga is used. The body handles Ga3+ in many ways as though it were iron, and thus it is bound (and concentrates) in areas of inflammation, such as infection, and also areas of rapid cell division. This allows such sites to be imaged by nuclear scan techniques. This use has largely been replaced by fluorodeoxyglucose (FDG) for positron emission tomography, "PET" scan and indium-111 labelled leukocyte scans. However, the localization of gallium in the body has some properties which make it unique in some circumstances from competing modalities using other radioisotopes.
Gallium-68, a positron emitter with a half life of 68 min, is now used as a diagnostic radionuclide in PET-CT when linked to pharmaceutical preparations such as DOTATOC, a somatostatin analogue used for neuroendocrine tumors investigation, and DOTA-TATE, a newer one, used for neuroendocrine metastasis and lung neuroendocrine cancer, such as certain types of "microcytoma". Gallium-68's preparation as a pharmaceutical is chemical and the radionuclide is extracted by elution from germanium-68, a synthetic radioisotope of germanium, in gallium-68 generators.
Precautions.
The Ga(III) ion of soluble gallium salts tends to form the insoluble hydroxide when injected in large amounts, and in animals precipitation of this has resulted in renal toxicity. In lower doses, soluble gallium is tolerated well, and does not accumulate as a poison.
While metallic gallium is not considered toxic, the data is inconclusive. Some sources suggest that it may cause dermatitis from prolonged exposure; other tests have not caused a positive reaction. Like most metals, finely divided gallium loses its luster and powdered gallium appears gray. Thus, when gallium is handled with bare hands, the extremely fine dispersion of liquid gallium droplets, which results from wetting skin with the metal, may appear as a gray skin stain.

</doc>
<doc id="12242" url="https://en.wikipedia.org/wiki?curid=12242" title="Germanium">
Germanium

Germanium is a chemical element with symbol Ge and atomic number 32. It is a lustrous, hard, grayish-white metalloid in the carbon group, chemically similar to its group neighbors tin and silicon. Purified germanium is a semiconductor, with an appearance most similar to elemental silicon. Like silicon, germanium naturally reacts and forms complexes with oxygen in nature. Unlike silicon, it is too reactive to be found naturally on Earth in the free (native) state.
Because very few minerals contain it in high concentration, germanium was discovered comparatively late in the history of chemistry. Germanium ranks near fiftieth in relative abundance of the elements in the Earth's crust. In 1869, Dmitri Mendeleev predicted its existence and some of its properties based on its position on his periodic table and called the element ekasilicon. Nearly two decades later, in 1886, Clemens Winkler found the new element along with silver and sulfur, in a rare mineral called argyrodite. Although the new element somewhat resembled arsenic and antimony in appearance, its combining ratios in the new element's compounds agreed with Mendeleev's predictions for a relative of silicon. Winkler named the element after his country, Germany. Today, germanium is mined primarily from sphalerite (the primary ore of zinc), though germanium is also recovered commercially from silver, lead, and copper ores.
Germanium "metal" (isolated germanium) is used as a semiconductor in transistors and various other electronic devices. Historically the first decade of semiconductor electronics was based entirely on germanium. Today, however, its production for use in semiconductor electronics is a small fraction (2%) of that of ultra-high purity silicon, which has largely replaced it. Presently, germanium's major end uses are in fibre-optic systems, infrared optics and in solar cell applications. Germanium compounds are also used for polymerization catalysts and have most recently found use in the production of nanowires. This element forms a large number of organometallic compounds, such as tetraethylgermane, which are useful in organometallic chemistry.
Germanium is not thought to be an essential element for any living organism. Some complexed organic germanium compounds are being investigated as possible pharmaceuticals, though none have yet proven successful. Similar to silicon and aluminum, natural germanium compounds tend to be insoluble in water, and thus have little oral toxicity. However, synthetic soluble germanium salts are nephrotoxic, and synthetic chemically reactive germanium compounds with halogens and hydrogen are irritants and toxins.
History.
In his report on "The Periodic Law of the Chemical Elements", in 1869, the Russian chemist Dmitri Ivanovich Mendeleev predicted the existence of several unknown chemical elements, including one that would fill a gap in the carbon family in his Periodic Table of the Elements, located between silicon and tin. Because of its position in his Periodic Table, Mendeleev called it "ekasilicon (Es)", and he estimated its atomic weight as about 72.0.
In mid-1885, at a mine near Freiberg, Saxony, a new mineral was discovered and named "argyrodite", because of its high silver content. The chemist Clemens Winkler analyzed this new mineral, which proved to be a combination of silver, sulfur, and a new element. Winkler was able to isolate this new element and found it somewhat similar to antimony, in 1886. Before Winkler published his results on the new element, he decided that he would name his element "neptunium", since the recent discovery of planet Neptune in 1846 had been preceded by mathematical predictions of its existence. However, the name "neptunium" had already been given to another proposed chemical element (though not the element that today bears the name neptunium, which was discovered in 1940), so instead, Winkler named the new element "germanium" (from the Latin word, "Germania", for Germany) in honor of his homeland. Argyrodite proved empirically to be Ag8GeS6.
Because this new element showed some similarities with the elements arsenic and antimony, its proper place in the periodic table was under consideration, but its similarities with Dmitri Mendeleev's predicted element "ekasilicon" confirmed that it belonged in this place on the periodic table. With further material from 500 kg of ore from the mines in Saxony, Winkler confirmed the chemical properties of the new element in 1887. He also determined an atomic weight of 72.32 by analyzing pure germanium tetrachloride (), while Lecoq de Boisbaudran deduced 72.3 by a comparison of the lines in the spark spectrum of the element.
Winkler was able to prepare several new compounds of germanium, including its fluorides, chlorides, sulfides, germanium dioxide, and tetraethylgermane (Ge(C2H5)4), the first organogermane. The physical data from these compounds — which corresponded well with Mendeleev's predictions — made the discovery an important confirmation of Mendeleev's idea of element periodicity. Here is a comparison between the prediction and Winkler's data:
Until the late 1930s, germanium was thought to be a poorly conducting metal. Germanium did not become economically significant until after 1945, when its properties as a semiconductor were recognized as being useful in electronics. During World War II, small amounts of germanium had begun to be used in some special electronic devices, mostly diodes. Its first major use was the point-contact Schottky diodes for radar pulse detection during the War. The first silicon-germanium alloys were obtained in 1955. Before 1945, only a few hundred kilograms of germanium were produced in smelters each year, but by the end of the 1950s, the annual worldwide production had reached 40 metric tons.
The development of the germanium transistor in 1948 opened the door to countless applications of solid state electronics. From 1950 through the early 1970s, this area provided an increasing market for germanium, but then high-purity silicon began replacing germanium in transistors, diodes, and rectifiers. For example, the company that became Fairchild Semiconductor was founded in 1957 with the express purpose of producing silicon transistors. Silicon has superior electrical properties, but it requires much greater purity, which could not be commercially achieved in the early years of semiconductor electronics.
Meanwhile, the demand for germanium for use in fiber optics communication networks, infrared night vision systems, and polymerization catalysts increased dramatically. These end uses represented 85% of worldwide germanium consumption in 2000. The US government even designated germanium as a strategic and critical material, calling for a 146 ton (132 t) supply in the national defense stockpile in 1987.
Germanium differs from silicon in that the supply for germanium is limited by the availability of exploitable sources, while the supply of silicon is only limited by production capacity since silicon comes from ordinary sand or quartz. As a result, while silicon could be bought in 1998 for less than $10 per kg, the price of 1 kg of germanium was then almost $800.
Characteristics.
Under standard conditions germanium is a brittle, silvery-white, semi-metallic element. This form constitutes an allotrope known as "α-germanium", which has a metallic luster and a diamond cubic crystal structure, the same as diamond. At pressures above 120 kbar, a different allotrope known as "β-germanium" forms, which has the same structure as β-tin. Along with silicon, gallium, bismuth, antimony, and water, it is one of the few substances that expands as it solidifies (i.e. freezes) from its molten state.
Germanium is a semiconductor. Zone refining techniques have led to the production of crystalline germanium for semiconductors that has an impurity of only one part in 1010, making it one of the purest materials ever obtained. The first metallic material discovered (in 2005) to become a superconductor in the presence of an extremely strong electromagnetic field was an alloy of germanium with uranium and rhodium.
Pure germanium is known to spontaneously extrude very long screw dislocations. They are one of the primary reasons for the failure of older diodes and transistors made from germanium; depending on what they eventually touch, they may lead to an electrical short.
Chemistry.
Elemental germanium oxidizes slowly to GeO2 at 250 °C. Germanium is insoluble in dilute acids and alkalis but dissolves slowly in hot concentrated sulfuric and nitric acids and reacts violently with molten alkalis to produce germanates (). Germanium occurs mostly in the oxidation state +4 although many compounds are known with the oxidation state of +2. Other oxidation states are rare, such as +3 found in compounds such as Ge2Cl6, and +3 and +1 observed on the surface of oxides, or negative oxidation states in germanes, such as −4 in . Germanium cluster anions (Zintl ions) such as Ge42−, Ge94−, Ge92−, [(Ge9)2]6− have been prepared by the extraction from alloys containing alkali metals and germanium in liquid ammonia in the presence of ethylenediamine or a cryptand. The oxidation states of the element in these ions are not integers—similar to the ozonides O3−.
Two oxides of germanium are known: germanium dioxide (, germania) and germanium monoxide, (). The dioxide, GeO2 can be obtained by roasting germanium disulfide (), and is a white powder that is only slightly soluble in water but reacts with alkalis to form germanates. The monoxide, germanous oxide, can be obtained by the high temperature reaction of GeO2 with Ge metal. The dioxide (and the related oxides and germanates) exhibits the unusual property of having a high refractive index for visible light, but transparency to infrared light. Bismuth germanate, Bi4Ge3O12, (BGO) is used as a scintillator.
Binary compounds with other chalcogens are also known, such as the disulfide (), diselenide (), and the monosulfide (GeS), selenide (GeSe), and telluride (GeTe). GeS2 forms as a white precipitate when hydrogen sulfide is passed through strongly acid solutions containing Ge(IV). The disulfide is appreciably soluble in water and in solutions of caustic alkalis or alkaline sulfides. Nevertheless, it is not soluble in acidic water, which allowed Winkler to discover the element. By heating the disulfide in a current of hydrogen, the monosulfide (GeS) is formed, which sublimes in thin plates of a dark color and metallic luster, and is soluble in solutions of the caustic alkalis. Upon melting with alkaline carbonates and sulfur, germanium compounds form salts known as thiogermanates.
Four tetrahalides are known. Under normal conditions GeI4 is a solid, GeF4 a gas and the others volatile liquids. For example, germanium tetrachloride, GeCl4, is obtained as a colorless fuming liquid boiling at 83.1 °C by heating the metal with chlorine. All the tetrahalides are readily hydrolyzed to hydrated germanium dioxide. GeCl4 is used in the production of organogermanium compounds. All four dihalides are known and in contrast to the tetrahalides are polymeric solids. Additionally Ge2Cl6 and some higher compounds of formula Ge"n"Cl2"n"+2 are known. The unusual compound Ge6Cl16 has been prepared that contains the Ge5Cl12 unit with a neopentane structure.
Germane (GeH4) is a compound similar in structure to methane. Polygermanes—compounds that are similar to alkanes—with formula Ge"n"H2"n"+2 containing up to five germanium atoms are known. The germanes are less volatile and less reactive than their corresponding silicon analogues. GeH4 reacts with alkali metals in liquid ammonia to form white crystalline MGeH3 which contain the GeH3− anion. The germanium hydrohalides with one, two and three halogen atoms are colorless reactive liquids.
The first organogermanium compound was synthesized by Winkler in 1887; the reaction of germanium tetrachloride with diethylzinc yielded tetraethylgermane (). Organogermanes of the type R4Ge (where R is an alkyl) such as tetramethylgermane () and tetraethylgermane are accessed through the cheapest available germanium precursor germanium tetrachloride and alkyl nucleophiles. Organic germanium hydrides such as isobutylgermane () were found to be less hazardous and may be used as a liquid substitute for toxic germane gas in semiconductor applications. Many germanium reactive intermediates are known: germyl free radicals, germylenes (similar to carbenes), and germynes (similar to carbynes). The organogermanium compound 2-carboxyethylgermasesquioxane was first reported in the 1970s, and for a while was used as a dietary supplement and thought to possibly have anti-tumor qualities.
Using a ligand called Eind (1,1,3,3,5,5,7,7-octaethyl-s-hydrindacen-4-yl) germanium is able to form a double bond with oxygen (germanone).
Isotopes.
Germanium has five naturally occurring isotopes, , , , , . Of these, is very slightly radioactive, decaying by double beta decay with a half-life of . is the most common isotope, having a natural abundance of approximately 36%. is the least common with a natural abundance of approximately 7%. When bombarded with alpha particles, the isotope will generate stable , releasing high energy electrons in the process. Because of this, it is used in combination with radon for nuclear batteries.
At least 27 radioisotopes have also been synthesized ranging in atomic mass from 58 to 89. The most stable of these is , decaying by electron capture with a half-life of . The least stable is with a half-life of . While most of germanium's radioisotopes decay by beta decay, and decay by Positron emission delayed proton emission. through isotopes also exhibit minor Beta decay delayed neutron emission decay paths.
Occurrence.
Germanium is created through stellar nucleosynthesis, mostly by the s-process in asymptotic giant branch stars. The s-process is a slow neutron capture of lighter elements inside pulsating red giant stars. Germanium has been detected in the atmosphere of Jupiter and in some of the most distant stars. Its abundance in the Earth's crust is approximately 1.6 ppm. There are only a few minerals like argyrodite, briartite, germanite, and renierite that contain appreciable amounts of germanium, but no mineable deposits exist for any of them. Some zinc-copper-lead ore bodies contain enough germanium that it can be extracted from the final ore concentrate.
An unusual enrichment process causes a high content of germanium in some coal seams, which was discovered by Victor Moritz Goldschmidt during a broad survey for germanium deposits. The highest concentration ever found was in the Hartley coal ash with up to 1.6% of germanium. The coal deposits near Xilinhaote, Inner Mongolia, contain an estimated 1600 tonnes of germanium.
Production.
About 118 tonnes of germanium was produced in 2011 worldwide, mostly in China (80 t), Russia (5 t) and United States (3 t). Germanium is recovered as a by-product from sphalerite zinc ores where it is concentrated in amounts of up to 0.3%, especially from low-temperature sediment-hosted, massive Zn–Pb–Cu(–Ba) deposits and carbonate-hosted Zn–Pb deposits. A recent study found that at least 10,000 t of extractable germanium is contained in known zinc reserves, particularly those hosted by Mississippi-Valley type deposits, while at least 112,000 t is contained in coal reserves. In 2007 35% of the demand was met by recycled germanium.
While it is produced mainly from sphalerite, it is also found in silver, lead, and copper ores. Another source of germanium is fly ash of coal power plants which use coal from some coal deposits with a large concentration of germanium. Russia and China used this as a source for germanium. Russia's deposits are located in the far east of the country on Sakhalin Island. The coal mines northeast of Vladivostok have also been used as a germanium source. The deposits in China are mainly located in the lignite mines near Lincang, Yunnan; coal mines near Xilinhaote, Inner Mongolia are also used.
The ore concentrates are mostly sulfidic; they are converted to the oxides by heating under air, in a process known as roasting:
Part of the germanium ends up in the dust produced during this process, while the rest is converted to germanates which are leached together with the zinc from the cinder by sulfuric acid. After neutralization only the zinc stays in solution and the precipitate contains the germanium and other metals. After reducing the amount of zinc in the precipitate by the Waelz process, the residing Waelz oxide is leached a second time. The dioxide is obtained as precipitate and converted with chlorine gas or hydrochloric acid to germanium tetrachloride, which has a low boiling point and can be distilled off:
Germanium tetrachloride is either hydrolyzed to the oxide (GeO2) or purified by fractional distillation and then hydrolyzed.
The highly pure GeO2 is now suitable for the production of germanium glass. The pure germanium oxide is reduced by the reaction with hydrogen to obtain germanium suitable for the infrared optics or semiconductor industry:
The germanium for steel production and other industrial processes is normally reduced using carbon:
Applications.
[[File:Singlemode fibre structure.svg|thumb|right|A typical single-mode optical fiber. Germanium oxide is a dopant of the core silica (Item 1).
1. Core 8 µm
2. Cladding 125 µm
3. Buffer 250 µm
4. Jacket 400 µm|alt=A drawing of four concentric cylinders.]]
The major end uses for germanium in 2007, worldwide, were estimated to be: 35% for fiber-optic systems, 30% infrared optics, 15% for polymerization catalysts, and 15% for electronics and solar electric applications. The remaining 5% went into other uses such as phosphors, metallurgy, and chemotherapy.
Optics.
The most notable physical characteristics of germania (GeO2) are its high index of refraction and its low optical dispersion. These make it especially useful for wide-angle camera lenses, microscopy, and for the core part of optical fibers. It also replaced titania as the silica dopant for silica fiber, eliminating the need for subsequent heat treatment, which made the fibers brittle. At the end of 2002 the fiber optics industry accounted for 60% of the annual germanium use in the United States, but this use accounts for less than 10% of worldwide consumption. GeSbTe is a phase change material used for its optic properties, such as in rewritable DVDs.
Because germanium is transparent in the infrared it is a very important infrared optical material, that can be readily cut and polished into lenses and windows. It is especially used as the front optic in thermal imaging cameras working in the 8 to 14 micron wavelength range for passive thermal imaging and for hot-spot detection in military, night vision system in cars, and fire fighting applications. It is therefore used in infrared spectroscopes and other optical equipment which require extremely sensitive infrared detectors. The material has a very high refractive index (4.0) and so needs to be anti-reflection coated. Particularly, a very hard special antireflection coating of diamond-like carbon (DLC), refractive index 2.0, is a good match and produces a diamond-hard surface that can withstand much environmental rough treatment.
Electronics.
Silicon-germanium alloys are rapidly becoming an important semiconductor material, for use in high-speed integrated circuits. Circuits utilizing the properties of Si-SiGe junctions can be much faster than those using silicon alone. Silicon-germanium is beginning to replace gallium arsenide (GaAs) in wireless communications devices. The SiGe chips, with high-speed properties, can be made with low-cost, well-established production techniques of the silicon chip industry.
The recent rise in energy cost has improved the economics of solar panels, a potential major new use of germanium. Germanium is the substrate of the wafers for high-efficiency multijunction photovoltaic cells for space applications.
Because germanium and gallium arsenide have very similar lattice constants, germanium substrates can be used to make gallium arsenide solar cells. The Mars Exploration Rovers and several satellites use triple junction gallium arsenide on germanium cells.
Germanium-on-insulator substrates are seen as a potential replacement for silicon on miniaturized chips. Other uses in electronics include phosphors in fluorescent lamps, and germanium-base solid-state light-emitting diodes (LEDs). Germanium transistors are still used in some effects pedals by musicians who wish to reproduce the distinctive tonal character of the "fuzz"-tone from the early rock and roll era, most notably the Dallas Arbiter Fuzz Face.
Other uses.
Germanium dioxide is also used in catalysts for polymerization in the production of polyethylene terephthalate (PET). The high brilliance of the produced polyester is especially used for PET bottles marketed in Japan. However, in the United States, no germanium is used for polymerization catalysts. Due to the similarity between silica (SiO2) and germanium dioxide (GeO2), the silica stationary phase in some gas chromatography columns can be replaced by GeO2.
In recent years germanium has seen increasing use in precious metal alloys. In sterling silver alloys, for instance, it has been found to reduce firescale, increase tarnish resistance, and increase the alloy's response to precipitation hardening. A tarnish-proof sterling silver alloy, trademarked Argentium, contains 1.2% germanium.
High purity germanium single crystal semiconductor detectors can precisely identify radiation sources—for example in airport security. Germanium is useful for monochromators for beamlines used in single crystal neutron scattering and synchrotron X-ray diffraction. The reflectivity has advantages over silicon in neutron and high energy X-ray applications. Crystals of high purity germanium are used in detectors for gamma spectroscopy and the search for dark matter.
Germanium crystals are also used in X-ray spectrometers for the determination of phosphorus, chlorine and sulfur.
Inorganic germanium and medical aspects.
Inorganic germanium and organic germanium are different chemical compounds of germanium and their properties are different.
Germanium is not thought to be essential to the health of plants or animals. Germanium in the environment has little or no health impact. This is primarily because it usually occurs only as a trace element in ores and carbonaceous materials, and is used in very small quantities that are not likely to be ingested, in its various industrial and electronic applications. For similar reasons, germanium in end-uses has little impact on the environment as a biohazard. Some reactive intermediate compounds of germanium are poisonous (see precautions, below).
Germanium supplements, made from both organic and inorganic germanium, have been marketed as an alternative medicine capable of treating leukemia and lung cancer. There is however no medical evidence of benefit, and instead some evidence that such supplements are actively harmful.
Other germanium compounds have been administered by alternative medical practitioners as non-FDA-allowed injectable solutions. Soluble inorganic forms of germanium used at first, notably the citrate-lactate salt, led to a number of cases of renal dysfunction, hepatic steatosis and peripheral neuropathy in individuals using them on a chronic basis. Plasma and urine germanium concentrations in these individuals, several of whom died, were several orders of magnitude greater than endogenous levels. A more recent organic form, beta-carboxyethylgermanium sesquioxide (propagermanium), has not exhibited the same spectrum of toxic effects.
U.S. Food and Drug Administration research has concluded that inorganic germanium, when used as a nutritional supplement, "presents potential human health hazard".
Certain compounds of germanium have low toxicity to mammals, but have toxic effects against certain bacteria.
Precautions for chemically reactive germanium compounds.
Some of germanium's artificially-produced compounds are quite reactive and present an immediate hazard to human health on exposure. For example, germanium chloride and germane (GeH4) are a liquid and gas, respectively, that can be very irritating to the eyes, skin, lungs, and throat.

</doc>
<doc id="12243" url="https://en.wikipedia.org/wiki?curid=12243" title="Gadolinium">
Gadolinium

Gadolinium is a chemical element with symbol Gd and atomic number 64. It is a silvery-white, malleable and ductile rare-earth metal. It is found in nature only in combined (salt) form. Gadolinium was first detected spectroscopically in 1880 by de Marignac who separated its oxide and is credited with its discovery. It is named for gadolinite, one of the minerals in which it was found, in turn named for chemist Johan Gadolin. The metal was isolated by Paul Emile Lecoq de Boisbaudran in 1886.
Gadolinium metal possesses unusual metallurgic properties, to the extent that as little as 1% gadolinium can significantly improve the workability and resistance to high temperature oxidation of iron, chromium, and related alloys. Gadolinium as a metal or salt has exceptionally high absorption of neutrons and therefore is used for shielding in neutron radiography and in nuclear reactors. Like most rare earths, gadolinium forms trivalent ions which have fluorescent properties. Gadolinium(III) salts have therefore been used as green phosphors in various applications.
The gadolinium(III) ion occurring in water-soluble salts is quite toxic to mammals. However, chelated gadolinium(III) compounds are far less toxic because they carry gadolinium(III) through the kidneys and out of the body before the free ion can be released into tissue. Because of its paramagnetic properties, solutions of chelated organic gadolinium complexes are used as intravenously administered gadolinium-based MRI contrast agents in medical magnetic resonance imaging. However, in a small minority of patients with renal failure, at least four such agents have been associated with development of the rare nodular inflammatory disease nephrogenic systemic fibrosis. This is thought to be due to the gadolinium ion itself, since gadolinium(III) carrier molecules associated with the disease differ.
Characteristics.
Physical properties.
Gadolinium is a silvery-white malleable and ductile rare-earth metal. It crystallizes in hexagonal, close-packed α-form at room temperature, but, when heated to temperatures above 1235 °C, it transforms into its β-form, which has a body-centered cubic structure.
Gadolinium-157 has the highest thermal neutron capture cross-section among any stable nuclides: 259,000 barns. Only xenon-135 has a higher cross section, 2 million barns, but that isotope is unstable.
Gadolinium is generally believed to be ferromagnetic at temperatures below and is strongly paramagnetic above this temperature. There is some evidence that gadolinium may be a helical antiferromagnet, rather than a ferromagnet, below . Gadolinium demonstrates a magnetocaloric effect whereby its temperature increases when it enters a magnetic field and decreases when it leaves the magnetic field. The temperature is lowered to for the gadolinium alloy Gd85Er15, and the effect is considerably stronger for the alloy Gd5(Si2Ge2), but at a much lower temperature (<). A significant magnetocaloric effect is observed at higher temperatures, up to 300 K, in the Gd5(SixGe1-x)4 compounds.
Individual gadolinium atoms have been isolated by encapsulating them into fullerene molecules and visualized with transmission electron microscope. Individual Gd atoms and small Gd clusters have also been incorporated into carbon nanotubes.
Chemical properties.
Gadolinium combines with most elements to form Gd(III) derivatives. It also combines with nitrogen, carbon, sulfur, phosphorus, boron, selenium, silicon and arsenic at elevated temperatures, forming binary compounds.
Unlike other rare earth elements, metallic gadolinium is relatively stable in dry air. However, it tarnishes quickly in moist air, forming a loosely adhering gadolinium(III) oxide (Gd2O3), which spalls off, exposing more surface to oxidation.
Gadolinium is a strong reducing agent, which reduces oxides of several metals into their elements. Gadolinium is quite electropositive and reacts slowly with cold water and quite quickly with hot water to form gadolinium hydroxide:
Gadolinium metal is attacked readily by dilute sulfuric acid to form solutions containing the colorless Gd(III) ions, which exist as [Gd(H2O)9]3+ complexes:
Gadolinium metal reacts with the halogens (X2) at temperature about 200 °C:
Chemical compounds.
In the great majority of its compounds, Gd adopts the oxidation state +3. All four trihalides are known. All are white except for the iodide, which is yellow. Most commonly encountered of the halides is gadolinium(III) chloride (GdCl3). The oxide dissolves in acids to give the salts, such as gadolinium(III) nitrate.
Gadolinium(III), like most lanthanide ions, forms complexes with high coordination numbers. This tendency is illustrated by the use of the chelating agent DOTA, an octadentate ligand. Salts of [Gd(DOTA)]− are useful in magnetic resonance imaging. A variety of related chelate complexes have been developed, including gadodiamide.
Reduced gadolinium compounds are known, especially in the solid state. Gadolinium(II) halides are obtained by heating Gd(III) halides in presence of metallic Gd in tantalum containers. Gadolinium also form sesquichloride Gd2Cl3, which can be further reduced to GdCl by annealing at 800 °C. This gadolinium(I) chloride forms platelets with layered graphite-like structure.
Isotopes.
Naturally occurring gadolinium is composed of 6 stable isotopes, 154Gd, 155Gd, 156Gd, 157Gd, 158Gd and 160Gd, and 1 radioisotope, 152Gd, with 158Gd being the most abundant (24.84% natural abundance). The predicted double beta decay of 160Gd has never been observed (the only lower limit on its half-life of more than 1.3×1021 years has been set experimentally).
Twenty-nine radioisotopes have been characterized, with the most stable being alpha-decaying 152Gd (naturally occurring) with a half-life of 1.08×1014 years, and 150Gd with a half-life of 1.79×106 years. All of the remaining radioactive isotopes have half-lives of less than 74.7 years. The majority of these have half-lives of less than 24.6 seconds. Gadolinium isotopes have 4 metastable isomers, with the most stable being 143mGd (t1/2=110 seconds), 145mGd (t1/2=85 seconds) and 141mGd (t1/2=24.5 seconds).
Isotopes with atomic masses lower than the most abundant stable isotope, 158Gd, primarily decay via electron capture to Eu (europium) isotopes. At higher atomic masses, the primary decay mode is beta decay, and the primary products are Tb (terbium) isotopes.
History.
Gadolinium is named from the mineral gadolinite, in turn named for Finnish chemist and geologist Johan Gadolin.
In 1880, Swiss chemist Jean Charles Galissard de Marignac observed spectroscopic lines due to gadolinium in samples of gadolinite (which actually contains relatively little gadolinium, but enough to show a spectrum), and in the separate mineral cerite. The latter mineral proved to contain far more of the element with the new spectral line, and Jean Charles Galissard de Marignac eventually separated a mineral oxide from cerite which he realized was the oxide of this new element. He named the oxide "gadolinia." Because he realized that "gadolinia" was the oxide of a new element, he is credited with discovery of gadolinium. French chemist Paul Émile Lecoq de Boisbaudran actually carried out the separation of gadolinium metal from gadolinia, in 1886.
Occurrence.
Gadolinium is a constituent in many minerals such as monazite and bastnäsite, which are oxides. The metal is too reactive to exist naturally. Ironically, as noted above, the mineral gadolinite actually contains only traces of Gd. The abundance in the earth crust is about 6.2 mg/kg. The main mining areas are China, USA, Brazil, Sri Lanka, India and Australia with reserves expected to exceed one million tonnes. World production of pure gadolinium is about 400 tonnes per year. The only known mineral with essential gadolinium, lepersonnite-(Gd), is very rare.
Production.
Gadolinium is produced both from monazite and bastnäsite.
Gadolinium metal is obtained from its oxide or salts by heating with calcium at 1450 °C under argon atmosphere. Sponge gadolinium can be produced by reducing molten GdCl3 with an appropriate metal at temperatures below 1312 °C (melting point of Gd) in a reduced pressure.
Applications.
Gadolinium has no large-scale applications but has a variety of specialized uses.
Gadolinium has the highest neutron cross-section among any stable nuclides: 61,000 barns for 155Gd and 259,000 barns for 157Gd. 157Gd has been used to target tumors in neutron therapy. This element is very effective for use with neutron radiography and in shielding of nuclear reactors. It is used as a secondary, emergency shut-down measure in some nuclear reactors, particularly of the CANDU type. Gadolinium is also used in nuclear marine propulsion systems as a burnable poison.
Gadolinium also possesses unusual metallurgic properties, with as little as 1% of gadolinium improving the workability and resistance of iron, chromium, and related alloys to high temperatures and oxidation.
Gadolinium is paramagnetic at room temperature, with a ferromagnetic Curie point of 20 °C. Paramagnetic ions, such as gadolinium, enhance nuclear relaxation rates. This trait makes gadolinium useful for magnetic resonance imaging (MRI). Solutions of organic gadolinium complexes and gadolinium compounds are used as intravenous MRI contrast agent to enhance images in medical magnetic resonance imaging and magnetic resonance angiography (MRA) procedures. Magnevist is the most widespread example. Nanotubes packed with gadolinium, dubbed "gadonanotubes", are 40 times more effective than this traditional gadolinium contrast agent. Once injected, gadolinium-based contrast agents accumulate in abnormal tissues of the brain and body. This accumulation provides a greater contrast between normal and abnormal tissues, allowing doctors to better locate uncommon cell growths and tumors.
Gadolinium as a phosphor is also used in other imaging. In X-ray systems, gadolinium is contained in the phosphor layer, suspended in a polymer matrix at the detector. Terbium-doped gadolinium oxysulfide (Gd2O2S: Tb) at the phosphor layer converts the X-rays released from the source into light. This material emits green light at 540 nm due to the presence of Tb3+, which is very useful for enhancing the imaging quality. The energy conversion of Gd is up to 20%, which means that one-fifth of the X-rays striking the phosphor layer can be converted into light photons. Gadolinium oxyorthosilicate (Gd2SiO5, GSO; usually doped by 0.1–1% of Ce) is a single crystal that is used as a scintillator in medical imaging such as positron emission tomography or for detecting neutrons.
Gadolinium compounds are also used for making green phosphors for color TV tubes.
Gadolinium-153 is produced in a nuclear reactor from elemental europium or enriched gadolinium targets. It has a half-life of 240±10 days and emits gamma radiation with strong peaks at 41 keV and 102 keV. It is used in many quality assurance applications, such as line sources and calibration phantoms, to ensure that nuclear medicine imaging systems operate correctly and produce useful images of radioisotope distribution inside the patient. It is also used as a gamma ray source in X-ray absorption measurements or in bone density gauges for osteoporosis screening, as well as in the Lixiscope portable X-ray imaging system.
Gadolinium is used for making gadolinium yttrium garnet (Gd:Y3Al5O12); it has microwave applications and is used in fabrication of various optical components and as substrate material for magneto-optical films.
Gadolinium gallium garnet (GGG, Gd3Ga5O12) was used for imitation diamonds and for computer bubble memory.
Gadolinium can also serve as an electrolyte in solid oxide fuel cells (SOFCs). Using gadolinium as a dopant for materials like cerium oxide (in the form of gadolinium doped ceria) creates an electrolyte with both high ionic conductivity and low operating temperatures that are optimal for cost-effective production of fuel cells.
Research is being conducted on magnetic refrigeration near room temperature, which could provide significant efficiency and environmental advantages over conventional refrigeration methods. Gadolinium-based materials, such as Gd5(SixGe1-x)4, are currently the most promising materials owing to their high Curie temperature and giant magnetocaloric effect. Pure Gd itself exhibits a large magnetocaloric effect near its Curie temperature of 20 °C, and this has sparked great interest into producing Gd alloys with a larger effect and tunable Curie temperature. In Gd5(SixGe1-x)4, Si and Ge compositions can be varied to adjust the Curie temperature. This technology is still very early in development and significant material improvements still need to be made before it is commercially viable.
Biological role.
Gadolinium has no known native biological role, but its compounds are used as research tools in biomedicine. Gd3+ compounds are components of MRI contrast agents. It is used in various ion channel electrophysiology experiments to block sodium leak channels and stretch activated ion channels.
Safety.
As a free ion, gadolinium is reported often to be highly toxic, but MRI contrast agents are chelated compounds and are considered safe enough to be used in most persons. The toxicity of free gadolinium ions in animals is due to interference with a number of calcium-ion channel dependent processes. The 50% lethal dose is about 100–200 mg/kg. Toxicities have not been reported following low dose exposure to gadolinium ions. Toxicity studies in rodents, however show that chelation of gadolinium (which also improves its solubility) decreases its toxicity with regard to the free ion by at least a factor of 100 (i.e., the lethal dose for the Gd-chelate increases by 100 times). It is believed therefore that clinical toxicity of Gd contrast agents in humans will depend on the strength of the chelating agent; however this research is still not complete. About a dozen different Gd-chelated agents have been approved as MRI contrast agents around the world.
Gadolinium MRI contrast agents have proved safer than the iodinated contrast agents used in X-ray radiography or computed tomography. Anaphylactoid reactions are rare, occurring in approximately 0.03–0.1%.
Although gadolinium agents have proved useful for patients with renal impairment, in patients with severe renal failure requiring dialysis, there is a risk of a rare but serious illnesses, called nephrogenic systemic fibrosis (NSF) or nephrogenic fibrosing dermopathy, that has been linked to the use of four gadolinium-containing MRI contrast agents. The disease resembles scleromyxedema and to some extent scleroderma. It may occur months after contrast has been injected. Its association with gadolinium and not the carrier molecule is confirmed by its occurrence in from contrast materials in which gadolinium is carried by very different carrier molecules. UNC Radiologist Richard Semelka has coined the term Gadolinium Deposition Disease (GDD), a disease process observed in subjects with normal or near normal renal function who develop persistent symptoms that arise within hours to 2 months following the administration of gadolinium based contrast agents.
Current guidelines in the United States are that dialysis patients should only receive gadolinium agents where essential and to consider performing an iodinated contrast enhanced CT when feasible. If a contrast enhanced MRI must be performed on a dialysis patient, it is recommended that certain high-risk contrast agents be avoided and that a lower dose be considered. The American College of Radiology recommends that contrast enhanced MRI examinations be performed as closely before dialysis as possible as a precautionary measure, although this has not been proven to reduce the likelihood of developing NSF.

</doc>
<doc id="12244" url="https://en.wikipedia.org/wiki?curid=12244" title="German Unity Day">
German Unity Day

The Day of German Unity () is the national day of Germany, celebrated on 3 October as a public holiday. It commemorates the anniversary of German reunification in 1990, when the goal of a united Germany that originated in the middle of the 19th century, was fulfilled again. Therefore, the name addresses neither the re-union nor the union, but the unity of Germany. The Day of German Unity on 3 October has been the German national holiday since 1990, when the reunification was formally completed. It is a legal holiday for the Federal Republic of Germany.
An alternative choice to commemorate the reunification could have been the day the Berlin Wall came down: 9 November 1989, which coincided with the anniversary of the proclamation of the German Republic in 1918, and the defeat of Hitler's first coup in 1923. However, 9 November was also the anniversary of the first large-scale Nazi-led pogroms against Jews in 1938 ("Kristallnacht"), so the day was considered inappropriate as a national holiday. (See November 9 in German history.) Therefore, 3 October 1990, the day of the formal reunification, was chosen instead and replaced the "Day of German Unity" on 17 June, the national holiday of the Federal Republic of Germany from 1954.
Imperial Germany.
Before 1871, in the area where the single state of Germany now exists, different kingdoms and principalities existed. After the unification of Germany, and the Founding of the Empire 1871, there was still no common national holiday. The Sedantag was, however, celebrated every year on 2 September, recalling the decisive victory in the Franco-Prussian War on 2 September 1870.
After the founding of the Empire in 1871, there were calls for a national holiday, and there were three suggestions. No decision was made. Until 1873, the Sedantag was moved to 18 January or the day of the Frankfurt Treaty (10 May 1871). The Sedantag would soon also be celebrated at the universities and in many German cities. It never occurred to them to think about "Empire Parade" or "Emperor's Birthday". Some Culture Ministers of the states, especially in Prussia, decided that the Sedantag would be an official festival in schools. Upon many suggestions, the date of the Emperor's proclamation on 18 January would be established as day of remembrance. Emperor Wilhelm I declined this: "This was also the day of the first Prussian coronation of the king, which should not fall into the shadow of a united German holiday."
Weimar Republic.
On 31 July 1919, the Weimar Constitution would be accepted in its form by the Weimar National Congress. In memorial of this "Hour of birth of democracy", the 11 August was created as Constitution Day, because the President of the Empire, Friedrich Ebert, signed the constitution on this day.
National Socialism.
Shortly after the Nazis took power in 1933 (the so-called Machtergreifung), May Day (1 May) was established as a national holiday in the "German Reich". It was already celebrated as a "Day of the Labor Movement" since 1890, and also was part of the tradition for the May dance commemorating the Walpurgis Night. Immediately after the establishment of the holiday in 1933, the Nazis banned trade unions on 2 May 1933 and occupied their buildings as offices for the Nazi Movement. On 1 March 1939, Hitler declared 9 November as the "Memorial Day for the movement" as the national holiday.
Federal Republic of Germany.
From 1954 to 1990, 17 June was an official holiday in the Federal Republic of Germany to commemorate the Uprising of 1953 in East Germany, even with the name "Day of the German Unity". Since 1963, it was proclaimed by the President of the Federal Republic as "National Day of Memorial of the German People". Therefore, in the year 1990, the "Day of German Unity" was celebrated twice.
German Democratic Republic.
In East Germany, the Founding Day in 1949 would be celebrated on 7 October as Day of the Republic, until the 40th anniversary in 1989.
Decision for GDR's unity with the Federal Republic.
The motive for setting the date of 3 October as the possible Day of Unity was decided by the Volkskammer (GDR Congress) on the impending economical and political collapse of the GDR. The Helsinki Conference was set for 2 October, at which the foreign ministers would be informed of the results of the Two-plus-Four talks.
At the beginning of July, the governments of both German states decided on the schedule: elections in the GDR would be held on 14 October, and a common election for the entire country on 2 December.
The decision on the date was finally made on 22 August by the GDR's Minister-President, Lothar de Maizière, at a special session of the Volkskammer, which began at 9 pm. After a heated debate, the President of the Volkskammer, Sabine Bergmann-Pohl, announced the results at 2:30 am on 23 August:
"The Volkskammer decides on the accession of the GDR to the Constitution of the Federal Republic of Germany according to Article 23 of the Basic Laws effective as of 3 October 1990. In the matter Nr. 201 there have been 363 votes. There were no invalid votes. 294 deputies have voted 'yes.' (Strong applause from CDU/DA, DSU, FDP, partly SPD and the deputies standing up in their seats.)
"62 deputies have voted 'no', and 7 people abstained. This is a historic event. Ladies and Gentlemen, I believe that we have not made an easy decision, but today we have acted within our responsibilities of the voting rights of the citizens of the GDR. I thank everybody that this result was made possible by a consensus across party lines."
Gregor Gysi, Chairman of the SED-PDS, was visibly moved and made a personal statement: "Madame President! The Parliament has no more and no less decided on the downfall of the German Democratic Republic as of 3 October 1990". (Jubilant cheers from the CDU/DA, DSU and SPD.)
Attempt to change the date of national holiday.
On 3 November 2004, the Federal Chancellor, Gerhard Schröder, suggested that the "Day of the German Unity" be celebrated on a Sunday, for economic reasons. Instead of 3 October, the National Reunification should be celebrated on the first Sunday of October. This suggestion received a lot of criticism from many sides, amongst them from Federal President Horst Köhler as well as the President of the Bundestag, Wolfgang Thierse. The demand worried a part of the population because of discontent for an increased working hours would be seen as a provocation and devaluing the national holiday. In addition, fixing the Unity Day on the first Sunday of October would have meant that it would sometimes fall on 7 October, which happens to have been the national day of East Germany; this date would thus have been seen as commemorating the division of Germany rather than the reunification. The idea was dropped after a short but angry debate.
Celebrations.
The Day of German Unity is celebrated each year with a ceremonial act and a citizens' festival ("").
The celebrations are hosted habitually in the state capital of the German state presiding over the Bundesrat in the respective year (a sequence determined by the "Königstein Agreement"). After Bonn in 2011, Frankfurt am Main is going to be the second non-state capital to host the celebrations in 2015:
In addition, various celebrations are held in the federal capital Berlin, mainly based on the Straße des 17. Juni and around the Brandenburg Gate. State capitals and also other cities often have additional festivities. Furthermore, the Oktoberfest beer festival in Munich, which traditionally runs until the first Sunday in October, now runs until 3 October, if the Sunday in question falls on the first or second day of October. The celebrations in the host city always includes a festival and fireworks show.
Zipfelbund: compass communities.
At the 1999 Day of German Unity celebration in Wiesbaden the (Compass Confederation) was formalised. The Zipfelbund are the four communities at the cardinal compass points of Germany: North – List on the island of Sylt, West – Selfkant, South – Oberstdorf and East – Görlitz. Together, they always participate in the respective annual celebration to represent the modern borders of Germany.

</doc>
<doc id="12246" url="https://en.wikipedia.org/wiki?curid=12246" title="Alliance '90/The Greens">
Alliance '90/The Greens

Alliance '90/The Greens () is a green political party in Germany, formed from the merger of the German Green Party (founded in West Germany in 1980) and Alliance 90 (founded during the Revolution of 1989–1990 in East Germany) in 1993. Its leaders are Simone Peter and Cem Özdemir. In the 2013 federal elections, the party won 8.4% of the votes and 63 out of 630 seats in the Bundestag, making them the smallest party with representation in the national parliament.
Former names and variants in the states.
The party used to be called The Greens since its foundation in 1980 until its unification with Alliance '90 in 1993.
"Grüne Liste Umweltschutz" (green list for environmental protection) were the names of some branches in Lower Saxony and other states in the Federal Republic of Germany. These groups were founded in 1977 and took part in several elections. Most of them merged with The Greens in 1980.
The West Berlin state branch of The Greens was founded as "Alternative Liste", or precisely, "Alternative Liste für Demokratie und Umweltschutz" (AL; alternative list for democracy and environmental protection) in 1978 and became the official West Berlin branch of The Greens in 1980. In 1993 it renamed to Alliance '90/The Greens Berlin after the merger with East Berlin's Greens and Alliance '90.
The Hamburg state branch of the Green Party was called "Grün-Alternative Liste Hamburg" (GAL; green-alternative list) from its foundation in 1982 until 2012. In 1984 it became the official Hamburg branch of The Greens.
History.
January 13, 1980 - Foundation congress.
In the 1970s, environmentalists and peace activists politically organised amongst thousands of action groups. The political party The Greens () was founded January 13, 1980 in Karlsruhe to give this movement political and parliamentary representation. Opposition to pollution, use of nuclear power, NATO military action, and certain aspects of industrialised society were principal campaign issues. The Greens originated from civil initiatives, new social movements of the protests of 1968, but also from the conservative spectrum. Important figures in the first years were – among others – Petra Kelly, Gert Bastian, Lukas Beckmann, Rudolf Bahro, Joseph Beuys, Antje Vollmer, Joschka Fischer, Herbert Gruhl, August Haußleiter and Baldur Springmann.
It was at this congress, that the Greens lay their ideological foundations, proclaiming the famous Four Pillars of the Green Party:
1980s: Parliamentary representation on the federal level.
In 1982, the conservative factions of the Greens broke away to form the Ecological Democratic Party (ÖDP). Those who remained in the Green party were more strongly pacifist and against restrictions on immigration and reproductive rights, while supporting the legalisation of cannabis use, placing a higher priority on working for LGBT rights, and tending to advocate what they described as "anti-authoritarian" concepts of education and child-rearing. They also tended to identify more closely with a culture of protest and civil disobedience, frequently clashing with police at demonstrations against nuclear weapons, nuclear energy, and the construction of a new runway ("Startbahn West") at Frankfurt airport. Those who left the party at the time might have felt similarly about some of these issues, but did not identify with the forms of protest that Green party members took part in.
After some success at state-level elections, the party won 27 seats with 5.7% of the vote in the Bundestag, the lower house of the German parliament, in the 1983 federal election. Among the important political issues at the time was the deployment of Pershing II IRBMs and nuclear-tipped cruise missiles by the U.S. and NATO on West German soil, generating strong opposition in the general population that found an outlet in mass demonstrations. The newly formed party was able to draw on this popular movement to recruit support. Partly due to the impact of the Chernobyl disaster in 1986, and to growing awareness of the threat of air pollution and acid rain to German forests ("Waldsterben"), the Greens increased their share of the vote to 8.3% in the 1987 federal election. Around this time, Joschka Fischer emerged as the unofficial leader of the party, which he remained until resigning all leadership posts following the 2005 federal election.
Until 1987, the Greens comprised a faction involved in pedophile activism, the "SchwuP" short for "Arbeitsgemeinschaft "Schwule, Päderasten und Transsexuelle"" (approx. "working group "Gays, Pederasts and Transsexuals""). This faction campaigned for repealing § 176 of the German penal code, dealing with child sexual abuse. This group was controversial within the party itself, and was seen as partly responsible for the poor election result of 1985. This controversy re-surfaced in 2013 in the context of the pedophilia activism of Daniel Cohn-Bendit, and chairwoman Claudia Roth stated she welcomed an independent scientific investigation on the extent of influence pedophile activists had on the party in the mid 1980s. In November 2014 the political scientist Franz Walter presented the final report about his research on pedophilia ideology inside the antecedent greens on an press conference.
1990s: German reunification, fall out of parliament.
In the 1990 federal elections, taking place post-reunified Germany, the Greens in the West did not pass the 5% limit required to win seats in the Bundestag. It was only due to a temporary modification of German election law, applying the five-percent "hurdle" separately in East and West Germany, that the Greens acquired any parliamentary seats at all. This happened because in the new states of Germany, the Greens, in a joint effort with Alliance 90, a heterogenous grouping of civil rights activists, were able to gain more than 5% of the vote. Some critics attribute this poor performance to the reluctance of the campaign to cater to the prevalent mood of nationalism, instead focusing on subjects such as global warming. A campaign poster at the time proudly stated, "Everyone is talking about Germany; we're talking about the weather!", paraphrasing a popular slogan of Deutsche Bundesbahn, the German national railway. After the 1994 federal election, however, the merged party returned to the Bundestag, and the Greens received 7.3% of the vote nationwide and 49 seats.
1998–2002: Greens as governing party, first term.
In the 1998 federal election, despite a slight fall in their percentage of the vote (6.7%), the Greens retained 47 seats and joined the federal government for the first time in 'Red-Green' coalition government with the Social Democratic Party of Germany (SPD). Joschka Fischer became Vice-Chancellor of Germany and foreign minister in the new government, which had two other Green ministers (Andrea Fischer, later Renate Künast, and Jürgen Trittin).
Almost immediately the party was plunged into a crisis by the question of German participation in the NATO actions in Kosovo. Numerous anti-war party members resigned their party membership when the first post-war deployment of German troops in a military conflict abroad occurred under a Red-Green government, and the party began to experience a long string of defeats in local and state-level elections. Disappointment with the Green participation in government increased when anti-nuclear power activists realised that shutting down the nation's nuclear power stations would not happen as quickly as they wished, and numerous pro-business SPD members of the federal cabinet opposed the environmentalist agenda of the Greens, calling for tacit compromises.
In 2001, the party experienced a further crisis as some Green Members of Parliament refused to back the government's plan of sending military personnel to help with the 2001 invasion of Afghanistan. Chancellor Gerhard Schröder called a vote of confidence, tying it to his strategy on the war. Four Green MPs and one Social Democrat voted against the government, but Schröder was still able to command a majority.
On the other hand, the Greens achieved a major success as a governing party through the 2000 decision to phase out the use of nuclear energy. Minister of Environment, Nature Conservation and Nuclear Safety Jürgen Trittin reached an agreement with energy companies on the gradual phasing out of the country's nineteen nuclear power plants and a cessation of civil usage of nuclear power by 2020. This was authorised through the Nuclear Exit Law. Based on an estimate of 32 years as the normal period of operation for a nuclear power plant, the agreement defines precisely how much energy a power plant is allowed to produce before being shut down. This law has since been overturned.
2002–2005: Greens as governing party, second term.
Despite the crises of the preceding electoral period, in the 2002 federal election, the Greens increased their total to 55 seats (in a smaller parliament) and 8.6%. This was partly due to the perception that the internal debate over the war in Afghanistan had been more honest and open than in other parties, and one of the MPs who had voted against the Afghanistan deployment, Hans-Christian Ströbele, was directly elected to the Bundestag as a district representative for the Friedrichshain-Kreuzberg - Prenzlauer Berg East constituency in Berlin, becoming the first Green to ever gain a first-past-the-post seat in Germany. The Greens benefited from increased inroads among traditionally left-wing demographics which had benefited from Green-initiated legislation in the 1998-2002 term, such as environmentalists (Renewable Energies Act) and LGBT groups (Registered Partnership Law). Perhaps most important for determining the success of both the Greens and the SPD was the increasing threat of war in Iraq, which was highly unpopular with the German public, and helped gather votes for the parties which had taken a stand against participation in this war. Despite losses for the SPD, the Red-Green coalition government commanded a very slight majority in the Bundestag and was renewed, with Joschka Fischer as foreign minister, Renate Künast as minister for consumer protection, nutrition and agriculture, and Jürgen Trittin as minister for the environment.
One internal issue in 2002 was the failed attempt to settle a long-standing discussion about the question of whether members of parliament should be allowed to become members of the party executive. Two party conventions declined to change the party statute. The necessary majority of two thirds was missed by a small margin. As a result, former party chairpersons Fritz Kuhn and Claudia Roth (who had been elected to parliament that year) were no longer able to continue in their executive function and were replaced by former party secretary general Reinhard Bütikofer and former Bundestag member Angelika Beer. The party then held a member referendum on this question in the spring of 2003 which changed the party statute. Now members of parliament may be elected for two of the six seats of the party executive, as long as they are not ministers or caucus leaders. 57% of all party members voted in the member referendum, with 67% voting in favor of the change. The referendum was only the second in the history of Alliance 90/The Greens, the first having been held about the merger of the Greens and Alliance 90. In 2004, after Angelika Beer was elected to the European parliament, Claudia Roth was elected to replace her as party chair.
The only party convention in 2003 was planned for November 2003, but about 20% of the local organisations forced the federal party to hold a special party convention in Cottbus early to discuss the party position regarding "Agenda 2010", a major reform of the German welfare programmes planned by Chancellor Schröder.
The November 2003 party convention was held in Dresden and decided the election platform for the 2004 European Parliament elections. The German Green list for these elections was headed by Rebecca Harms (then leader of the Green party in Lower Saxony) and Daniel Cohn-Bendit, previously Member of the European Parliament for the The Greens of France. The November 2003 convention is also noteworthy because it was the first convention of a German political party ever to use an electronic voting system.
The Greens gained a record 13 of Germany's 99 seats in these elections, mainly due to the perceived competence of Green ministers in the federal government and the unpopularity of the Social Democratic Party.
In early 2005, the Greens were the target of the German Visa Affair 2005, instigated in the media by the Christian Democratic Union (CDU). At the end of April 2005, they celebrated the decommissioning of the Obrigheim nuclear power station. They also continue to support a bill for an Anti-Discrimination Law (:de: Allgemeines Gleichbehandlungsgesetz) in the Bundestag.
In May 2005, the only remaining state-level red-green coalition government lost the vote in the North Rhine-Westphalia state election, leaving only the federal government with participation of the Greens (apart from local governments). In the early 2005 federal election the party incurred very small losses and achieved 8.1% of the vote and 51 seats. However, due to larger losses of the SPD, the previous coalition no longer had a majority in the Bundestag.
2005–present: Greens back in opposition.
For almost two years after the federal elections in 2005, the Greens were not part of any government at the state or federal level. In June 2007, the Greens in Bremen entered into a coalition with the Social Democratic Party (SPD) following the 2007 Bremen state election.
In April 2008, following the 2008 Hamburg state election, the Green-Alternative List (GAL) in Hamburg entered into a coalition with the Christian Democratic Union (CDU), the first such state-level coalition in Germany. Although the GAL had to agree to the deepening of the Elbe River, the construction of a new coal-fired power station and two road projects they had opposed, they also received some significant concessions from the CDU. These included reforming state schools by increasing the number of primary school educational stages, the restoration of trams as public transportation in the city-state, and more pedestrian-friendly real estate development. In 2010, the coalition collapsed, resulting in new elections that were won by SPD.
Following the Saarland state election of August 2009, The Greens held the balance of power after a close election where no two-party coalitions could create a stable majority government. After negotiations, the Saarland Greens rejected the option of a left-wing 'red-red-green' coalition with the SPD and The Left ("Die Linke") in order to form a centre-right state government with the CDU and Free Democratic Party (FDP), a historical first time that a Jamaica coalition has formed in German politics.
In June 2010, in the first state election following the victory of the CDU/CSU and FDP in the 2009 federal election, the "black-yellow" CDU-FDP coalition in North Rhine-Westphalia under Jürgen Rüttgers lost its majority. The Greens and the SPD came one seat short of a governing majority, but after multiple negotiations about coalitions of SPD and Greens with either the FDP or The Left, the SPD and Greens decided to form a minority government, which was possible because under the constitution of North Rhine-Westphalia a plurality of seats is sufficient to elect a minister-president. So a red-green government in a state where it was defeated under Peer Steinbrück in 2005 came into office again on June 14, 2010 with the election of Hannelore Kraft as minister-president.
The Greens founded the first international chapter of a German political party in the U.S. on April 13, 2008 at the Goethe-Institut in Washington D.C. Its main goal is "to provide a platform for politically active and green-oriented German citizens, in and beyond Washington D.C., to discuss and actively participate in German Green politics. [...] to foster professional and personal exchange, channeling the outcomes towards the political discourse in Germany."
In 2011, the Greens made large gains in Rhineland-Palatinate and in Baden-Württemberg. In Baden-Württemberg they became the senior partner in a governing coalition for the first time. Winfried Kretschmann is now the first Green to serve as Minister-President of a German State. Polling data from August 2011 indicated that one in five Germans support the Greens. Since 4 October 2011 the party has been represented in all state parliaments.
Like the Social Democrats, the Greens backed Chancellor Angela Merkel on most bailout votes in the German parliament during her second term, saying their pro-European stances overrode party politics. Shortly before the elections, the party plummeted to a four-year low in the polls, undermining efforts by Peer Steinbrück’s Social Democrats to unseat Merkel.
Policy.
Energy and nuclear power.
From the inception of the party, they have been concerned with the immediate halt of construction or operation of all nuclear power stations. As an alternative, they promote a shift to alternative energy and a comprehensive program of energy conservation.
In 1986, large parts of Germany were covered with radioactive contamination from the Chernobyl disaster and Germans went to great lengths to deal with the contamination. Germany's anti-nuclear stance was strengthened. From the mid-1990s onwards, anti-nuclear protests were primarily directed against transports of radioactive waste in "CASTOR" containers.
After the Chernobyl disaster, the Greens became more radicalised and resisted compromise on the nuclear issue. During the 1990s, a re-orientation towards a moderate program occurred, with concern about global warming and ozone depletion taking a more prominent role. During the federal red-green government (1998–2005) many people became disappointed with what they saw as excessive compromise on key Greens policies.
Energy policy is still the most important cross-cutting issue in climate and economic policies. Implementation of Green Policy would see electricity generation from 100 percent renewable sources as early as 2040. The development of renewable energy and combined heat and power is also a great opportunity for technical and economic innovation. Solar industry and environmental technologies are already a significant part of key industries providing jobs which need to be developed and promoted vigorously. In addition, a priority of green energy policy is increasing the thermal insulation and energy efficiency of homes, the phaseout of all nuclear energy generation with possible high-efficiency gas-fired power plants operational during the transition phase.
Environment and climate policy.
The central idea of green politics is sustainable development. The concept of environmental protection is the cornerstone of Alliance 90/The Greens policy. In particular, the economic, energy and transport policy claims are in close interaction with environmental considerations. The Greens acknowledge the natural environment as a high priority and animal protection should be enshrined as a national objective in constitutional law. An effective environmental policy would be based on a common environmental code, with the urgent integration of a climate change bill. During the red-green coalition (1998–2005) a policy of agricultural change was launched labeled as a paradigm shift in agricultural policy towards a more ecological friendly agriculture, which needs to continue.
Climate change is at the center of all policy considerations. This includes environmental policy and safety and social aspects. The plans of the Alliance 90/The Greens provide a climate change bill laying down binding reductions to greenhouse gas emissions in Germany by 2020 restricting emissions to minus 40 percent compared to 1990.
Transport.
A similarly high priority is given to transport policy. The switch from a traveling allowance to a mobility allowance, which is paid regardless of income to all employees, replacing company car privileges. The truck toll will act as a climate protection instrument internalizing the external costs of transport. Railway should be promoted in order to achieve the desired environmental objectives and the comprehensive care of customers. The railway infrastructure is to remain permanently in the public sector, allowing a reduction in expenditure on road construction infrastructure. The Greens want to control privileges on kerosene and for international flights, introduce an air ticket levy. Restrict speeds nationwide on the highways to 120 km/h and country roads to 80 km/h. The Greens want to create a market incentive and research program of €500 million annually to ensure that by 2020 there are at least two million electric cars on German roads.
Electorate.
The Infratest Dimap political research company has suggested the Green voter demographic includes those on higher incomes (e.g. above €2000/month) and the party's support is less among households with lower incomes. The same polling research also concluded that the Greens received fewer votes from the unemployed and general working population, with business people favouring the party as well as the centre-right liberal Free Democratic Party. According to Infratest Dimap the Greens received more voters from the age group 34-42 than any other age group and that the young were generally more supportive of the party than the old. (Source: Intrafest Dimap political research company for the ARD.)
The Greens have a higher voter demographic in urban areas than rural areas, except for a small number of rural areas with pressing local environmental concerns, such as strip mining or radioactive waste deposits. The cities of Bonn, Cologne, Stuttgart, Berlin, Hamburg, Frankfurt and Munich have among the highest per cent Green voters in the country. The smaller towns of Freiburg im Breisgau, Tübingen, Konstanz, Oldenburg, Darmstadt, Heidelberg and Göttingen, have a strong share of Green votes, with Freiburg, Darmstadt, Tübingen and Konstanz even having Green mayors. The party has a lower level of support in the states of the former German Democratic Republic (East Germany). However, in 2011, the Greens are represented in the parliament of all German states.

</doc>
<doc id="12248" url="https://en.wikipedia.org/wiki?curid=12248" title="Gheorghe Zamfir">
Gheorghe Zamfir

Gheorghe Zamfir (; born April 6, 1941) is a Romanian pan flute musician.
Zamfir is known for playing an expanded version of the traditional Romanian-style pan flute (nai) of 20 pipes to 22, 25, 28 and 30 pipes to increase its range, and obtaining as many as eight overtones (additionally to the fundamental tone) from each pipe by changing the embouchure.
He is arguably known as "The Master of the Pan Flute". It is said he has sold more records worldwide than the Beatles and Elvis Presley.
Career.
Zamfir came to the public eye when he was approached by Swiss ethnomusicologist Marcel Cellier, who extensively researched Romanian folk music in the 1960s. The composer Vladimir Cosma brought Zamfir with his pan flute to Western European countries for the first time in 1972 as the soloist in Cosma's original music for the movie "Le grand blond avec une chaussure noire". This was very successful, and since then, he has been used as soloist in movie soundtracks by composers Francis Lai, Ennio Morricone and many others. Largely through television commercials where he was billed as "Zamfir, Master of the pan Flute", he introduced the folk instrument to a modern audience and revived it from obscurity.
In 1966, Zamfir was appointed conductor of the "Ciocîrlia Orchestra", one of the most prestigious state ensembles of Romania, destined for concert tours abroad. This created the opportunity for composition and arranging. In 1969, he left Ciocîrlia and started his own small band (taraf) and in 1970 he had his first longer term contract in Paris. Zamfir discovered the much greater freedom for artistic adventure. His taraf consisted of: Ion Drăgoi (violin), Ion Lăceanu (flutes), Dumitru Fărcaș (tarogato), Petre Vidrean (double bass) and Tony Iordache (cymbalum) all number 1 soloists in their country. This taraf made some excellent recordings (CD Zamfir a Paris). He changed the composition of the band soon after: Efta Botoca (violin), Marin Chisar (flutes), Dorin Ciobaru and Pavel Cebzan (clarinet and tarogato), Petre Vidrean (bass) and Pantelimon Stînga (cymbalum). It is said that this change was made to increase the command of Zamfir and have more artistic freedom. A turning point was the recording of Zamfir's composition "Messe pour la Paix" (Philips). His taraf joined a choir and a symphonic orchestra. This was evidence of the growing ambition. While the Philips recordings of that time were rather conservative, Zamfir preached revolution in the concert halls with daring performances. Some say that this short period was the highlight of his career. In 1979, he recorded "The Lonely Shepherd" with James Last. Zamfir put himself on the world map and since then his career became highly varied, hovering over classical repertoire, easy listening and pop music.
Zamfir's big break in the English-speaking world came when the BBC religious television programme "The Light of Experience" adopted his recording of "Doina De Jale", a traditional Romanian funeral song, as its theme. Popular demand forced Epic Records to release the tune as a single in 1976, and it climbed to number four on the UK charts. It would prove to be his only UK hit single, but it helped pave the way for a consistent stream of album sales in Britain. His song "Summer Love" reached number 9 in South Africa in November 1976. In 1983, he scored a No. 3 hit on the Canadian Adult Contemporary chart with "Blue Navajo," and several of his albums (including 1982's "Romance" and 1983's "Childhood Dreams") have charted in Canada as well. 
After nearly a decade-long absence, Zamfir returned to Canada in January 2006 for a seven-city tour with the Traffic Strings quintet. The program included a world premiere of Vivaldi's Four Seasons for PanFlute and string quintet arranged by Lucian Moraru, jazz standards, and well-known favourites. 
In 2009, Zamfir was sampled by Animal Collective in the song "Graze" on their EP Fall Be Kind.
In 2012, Zamfir performed at the opening ceremony of the 11th Conference of Parties to the Ramsar Convention at the Palace of the Parliament in Bucharest, Romania.
Soundtracks.
His first appearance as soloist interpreter in a movie soundtrack was in Vladimir Cosma's 1972 "Le grand blond avec une chaussure noire", whose soundtrack became a worldwide hit.
Another of his notable contributions was to the soundtrack of the classic 1975 Australian film "Picnic at Hanging Rock".
He was asked by Ennio Morricone to perform the pieces "Childhood Memories" and "Cockeye's Song" for the soundtrack of Sergio Leone's 1984 gangster film "Once Upon a Time in America". His performance can also be heard throughout the 1984 film "The Karate Kid".
One of Zamfir's most famous pieces is "The Lonely Shepherd", which was written by James Last and recorded with the James Last Orchestra, and first included on Zamfir's 1977 album "Memories from Russia"; it was also released as a single. "The Lonely Shepherd" was used as the theme for the 1979 Australian miniseries "Golden Soak". It was also featured in Quentin Tarantino's 2003 film "".
His song "Frunzuliță Lemn Adus Cântec De Nuntă" ("Fluttering Green Leaves Wedding Song") appears in the1991 Studio Ghibli film "Only Yesterday".
Personal life.
Zamfir was born in Găeşti, Romania, on April 6, 1941. Although initially interested in becoming an accordionist, at the age of 14 he began his pan flute studies with Fănică Luca at the Special Musical School no. 1 in Bucharest. Later he attended the Ciprian Porumbescu Conservatory. He currently resides and teaches pan flute in Bucharest. His son, Emmanuel Teodor (who resides in Montreal, Canada), is also a drummer/musician.
Bibliography.
Zamfir has written an instructional book, "Traitė Du Naï Roumain: méthode de flûte de pan", Paris: Chappell S.A., 1975, ISBN 88-8291-286-8, and an autobiography "Binecuvântare şi blestem" ("Blessing and Curse"), Arad: Mirador, 2000, ISBN 973-9284-56-6.

</doc>
<doc id="12250" url="https://en.wikipedia.org/wiki?curid=12250" title="Georg Henrik von Wright">
Georg Henrik von Wright

Georg Henrik von Wright (, 14 June 1916 – 16 June 2003) was a Finnish philosopher, who succeeded Ludwig Wittgenstein as professor at the University of Cambridge. He published in English, Finnish, German, and Swedish, having belonged to the Swedish-speaking minority of Finland. Von Wright was of both Finnish and 17th-century Scottish ancestry.
Work.
Von Wright's writings come under two broad categories. The first is analytic philosophy and philosophical logic in the Anglo-American vein. His 1951 books, "An Essay in Modal Logic" and "Deontic Logic", were landmarks in the postwar rise of formal modal logic and its deontic version. He was an authority on Wittgenstein, editing his later works. He was the leading figure in the Finnish philosophy of his time, specializing in philosophical logic, philosophical analysis, philosophy of action, philosophy of language, philosophy of mind, and the close study of Charles Sanders Peirce.
The other vein in von Wright's writings is moralist and pessimist. During the last twenty years of his life, under the influence of Oswald Spengler, Jürgen Habermas and the Frankfurt School's reflections about modern Rationality, he wrote prolifically. His best known article from this period is entitled "The Myth of Progress", and it questions whether our apparent material and technological progress can really be considered "progress".
In the last year of his life, among his other honorary degrees, he held an honorary degree at the University of Bergen.
Bibliography.
Von Wright edited posthumous publications by Wittgenstein, which were published by Blackwell (unless otherwise stated):

</doc>
<doc id="12253" url="https://en.wikipedia.org/wiki?curid=12253" title="Gaudy Night">
Gaudy Night

Gaudy Night (1935) is a mystery novel by Dorothy L. Sayers, the tenth in her popular series about aristocratic sleuth Lord Peter Wimsey, and the third featuring crime writer Harriet Vane.
The dons of Harriet Vane's alma mater, the all-female Shrewsbury College, Oxford (a thinly veiled take on Sayers' own Somerville College), have invited her back to attend the much anticipated annual 'Gaudy' celebrations. However, the mood turns sour when someone begins a series of malicious pranks including poison-pen messages, obscene graffiti, the destruction of a set of proofs and crafting vile effigies. Desperate to avoid a possible murder in college, Harriet eventually asks her old friend Wimsey to investigate.
Explanation of the novel's title.
"Gaudy" derives from the Latin "gaudium" and Old French "gaudie", meaning "merry-making" or "enjoyment". A college gaudy at Oxford is a meeting for former members. The phrase "gaudy night" is taken from Shakespeare's "Antony and Cleopatra":
Plot summary.
Harriet Vane returns with trepidation to Shrewsbury College, Oxford to attend the Gaudy dinner. Expecting hostility because of her notoriety, she is surprised to be welcomed warmly by the dons, and rediscovers her old love of the academic life.
Some time later the Dean of Shrewsbury writes to ask for help. There has been an outbreak of anonymous letters, vandalism and threats, apparently from someone within the college, and a scandal is feared. Harriet, herself a victim of poison-pen letters since her trial, reluctantly agrees to help, and spends much of the next few months in residence at the college, ostensibly to do research on Sheridan Le Fanu and to assist a don with her book.
As she wrestles with the case, trying to narrow down the list of suspects and avert a major scandal, Harriet is forced to examine her ambivalent feelings about love and marriage, along with her attraction to academia as an intellectual (and emotional) refuge. Her personal dilemma becomes entangled with darkly hinted suspicions and prejudices raised by the crimes at the college, which appear to have been committed by a sexually frustrated female don. Harriet is forced to re-examine her relationship with Wimsey in the light of what she has discovered about herself. Wimsey eventually arrives in Oxford to help her, and she gains a new perspective on him from those who know him, including his nephew, an undergraduate at the university.
The attacks build to a crisis, and the college community of students, dons and servants is almost torn apart by suspicion and fear. There is an attempt to drive a vulnerable student to suicide and a physical assault on Harriet that almost kills her. The perpetrator is finally unmasked by Wimsey as one of the college servants, revealed to be the widow of a disgraced academic at a northern university. Her husband's academic fraud had been exposed by one of his fellow dons there, destroying his career and driving him to suicide. The don has since moved to Shrewsbury College, and the campaign has been the widow's revenge against intellectual women who move outside their "proper" domestic sphere.
At the end of the book, Harriet Vane finally accepts Wimsey's proposal of marriage. (Their wedding and honeymoon—interrupted by another murder mystery—are depicted in "Busman's Honeymoon".)
Literary significance and criticism.
Although no murder occurs in "Gaudy Night", it includes a great deal of suspense and psychological thrills. The narrative is interwoven with a love story and an examination of women's struggles to enlarge their roles and achieve some independence within the social climate of 1930s England, and the novel has been described as "the first feminist mystery novel".
Jacques Barzun: ""Gaudy Night" is a remarkable achievement. Harriet Vane and Saint-George, the undergraduate nephew of Lord Peter, help give variety, and the college setting justifies good intellectual debate. The motive is magnificently orated on by the culprit in a scene that is a striking set-piece. And though the Shrewsbury dons are sometimes hard to distinguish one from another, the College architecture is very good. Note a reference to C. P. Snow's "The Search", and sound views on counterpoint versus harmony."
"Gaudy Night" deals with a number of philosophical themes, such as the right relation between love and independence or between principles and personal loyalties. Susan Haack has an essay on "Gaudy Night" as a philosophical novel.
Women's education.
The issue of women's right to academic education is central to the book's plot. The lecturers of Shrewsbury College are veterans of the prolonged struggle for academic degrees for women, which Oxford granted only reluctantly (Sayers herself took part in this struggle). The lecturers are surprised and a bit dismayed at the attitude of their students, who take for granted this right for which such a hard struggle had to be waged.
And in fact, the struggle was not yet completely won. Some of the male lecturers in Oxford were still not happy with women getting degrees; the number of women in the University was restricted by statute to no more than 25% (a restriction which would only be removed in the 1970s); women were segregated in special women's colleges such as Shrewsbury, while the prestigious historic colleges remained exclusively male; women's colleges were starved of funds and run on a shoestring.
Publication of such going-ons as happen in the book (poison-pen letters, vandalism, the near-suicide of a student and near-murder of a lecturer) would discredit and severely damage Shrewsbury College in particular and the cause of women's education in general. Therefore, all this must be kept secret – which rules out any approach to the police or other outside agency.
For most of the book, it is assumed that the perpetrator is mentally deranged and that this is a sufficient motive. But as it turned out, in fact all these acts were carried out by deliberate design, with the conscious intention of causing just such a discrediting of women's education. Ironically the perpetrator turns out to be a strong, assertive woman, capable of taking bold initiatives and setting the agenda for everybody else – and using all this to aggressively promote a violently anti-feminist agenda.
International background.
A subplot in the book is Peter Wimsey's role as an informal envoy of the British Foreign Ministry, called upon to help defuse international crises when more conventional diplomats have failed. For much of the book he is in Italy, dealing with a major crisis which for a time seemed to threaten the outbreak of a new European war (as he tells Bunter). Though not explicitly named, this was clearly the Abyssinia Crisis, and the reference would probably have been clear to readers at the time. The book reflects the mindset at the time of writing, when the outbreak of the Second World War had not yet come to seem inevitable.
In the frame of the book's plot, Wimsey's diplomatic obligations serve as a plot device to keep him away from Britain, and leave Harriet on her own for most of the book, to try to resolve the mystery at Oxford without his help.
Film, TV or theatrical adaptations.
The book was adapted for television in 1987 as part of a series starring Edward Petherbridge as Lord Peter and Harriet Walter as Harriet Vane.
In 2005, an adaptation of the novel was released on CD by the BBC Radio Collection to finally complete the run of Wimsey adaptations begun with "Whose Body?" in 1973; the role of Harriet was played by Joanna David, and Wimsey by Ian Carmichael.
In 2006, a theatrical adaptation was created by Frances Limoncelli and directed by Dorothy Milne at Lifeline Theatre in Chicago.
The plot of "Gaudy Night" was adapted to become the two-part "Out of the Past" episode (#155 & #156) of the American television mystery series "" starring Dick van Dyke as Dr. Mark Sloan. The episode first aired on 11 May 2000.

</doc>
<doc id="12255" url="https://en.wikipedia.org/wiki?curid=12255" title="G">
G

G (named "gee" ) is the 7th letter in the ISO basic Latin alphabet.
History.
The letter 'G' was introduced in the Old Latin period as a variant of 'C' to distinguish voiced from voiceless . The recorded originator of 'G' is freedman Spurius Carvilius Ruga, the first Roman to open a fee-paying school, who taught around 230 BC. At this time, 'K' had fallen out of favor, and 'C', which had formerly represented both and before open vowels, had come to express in all environments.
Ruga's positioning of 'G' shows that alphabetic order related to the letters' values as Greek numerals was a concern even in the 3rd century BC. According to some records, the original seventh letter, 'Z', had been purged from the Latin alphabet somewhat earlier in the 3rd century BC by the Roman censor Appius Claudius, who found it distasteful and foreign. Sampson (1985) suggests that: "Evidently the order of the alphabet was felt to be such a concrete thing that a new letter could be added in the middle only if a 'space' was created by the dropping of an old letter." 
Hempl (1899) proposes that there never was such a "space" in the alphabet and that in fact 'G' was a direct descendant of zeta. Zeta took shapes like ⊏ in some of the Old Italic scripts; the development of the monumental form 'G' from this shape would be exactly parallel to the development of 'C' from gamma. He suggests that the pronunciation > was due to contamination from the also similar-looking 'K'.
Eventually, both velar consonants and developed palatalized allophones before front vowels; consequently in today's Romance languages, and have different sound values depending on context (known as hard and soft C and hard and soft G). Because of French influence, English orthography shares this feature.
Typographic variants.
The modern lowercase 'g' has two typographic variants: the single-story (sometimes "opentail") " and the double-story (sometimes "looptail") ". The single-story form derives from the majuscule (uppercase) form by raising the serif that distinguishes it from 'c' to the top of the loop, thus closing the loop, and extending the vertical stroke downward and to the left. The double-story form (g) had developed similarly, except that some ornate forms then extended the tail back to the right, and to the left again, forming a closed bowl or loop. The initial extension to the right was absorbed into the upper closed bowl. The double-story version became popular when printing switched to "Roman type" because the tail was effectively shorter, making it possible to put more lines on a page. In the double-story version, a small top stroke in the upper-right, often terminating in an orb shape, is called an "ear".
Generally, the two forms are complementary, but occasionally the difference has been exploited to provide contrast. The 1949 "Principles of the International Phonetic Association" recommends using for advanced voiced velar plosives (denoted by Latin small letter script G) and for regular ones where the two are contrasted, but this suggestion was never accepted by phoneticians in general, and today " is the symbol used in the , with " acknowledged as an acceptable variant and more often used in printed materials.
Use in writing systems.
English.
In English, the letter appears either alone or in some digraphs. Alone, it represents
In words of Romance origin, is mainly soft before (including the digraphs and ), , and and hard otherwise.
There are many English words of non-Romance origin where is hard though followed by or (e.g. "get", "gift"), and a few in which is soft though followed by such as "gaol" or "margarine".
The digraph represents
The digraph represents either 
The digraph (which mostly came about when the letter Yogh, which took various values including , , and , was removed from the alphabet) now represents a great variety of values, including 
The digraph may represent 
Other languages.
Most Romance languages and some Nordic languages also have two main pronunciations for , hard and soft. While the soft value of varies in different Romance languages ( in French and Portuguese, in Catalan, in Italian and Romanian, and in most dialects of Spanish), in all except Romanian and Italian, soft has the same pronunciation as the .
In Italian and Romanian, is used to represent before front vowels where would otherwise represent a soft value. In Italian and French, is used to represent the palatal nasal , a sound somewhat similar to the in English "canyon". In Italian, the trigraph , when appearing before a vowel or as the article and pronoun "gli", represents the palatal lateral approximant .
Other languages typically use to represent regardless of position.
Amongst European languages Dutch is an exception as it does not have in its native words, and instead represents a voiced velar fricative , a sound that does not occur in modern English, but there is a dialectal variation: many Netherlandic dialects use a voiceless fricative ( or ) instead, and in southern dialects it may be palatalized to . Nevertheless, word-finally it is always voiceless in all dialects, including the standard Dutch of Belgium and the Netherlands. On the other hand, some dialects (like Amelands), may have a phonemic .
Faroese uses to represent , in addition to , and also uses it to indicate a glide.
In Maori (Te Reo Māori), is used in the digraph which represents the velar nasal and is pronounced like the in "singer".
In older Czech and Slovak orthographies, was used to represent , while was written as ( with caron).

</doc>
<doc id="12257" url="https://en.wikipedia.org/wiki?curid=12257" title="Gamma">
Gamma

Gamma (uppercase ', lowercase '; "Gámma") is the third letter of the Greek alphabet. In the system of Greek numerals it has a value of 3. In Ancient Greek, gamma represented a voiced velar stop . In Modern Greek, it represents a voiced fricative.
Based on its phonetic value in Modern Greek, gamma has also been introduced in 
a number of modern Latin-alphabet based phonetic notations (Latin (or latinized) gamma Ɣ).
History.
The Greek letter Gamma Γ was derived from the Phoenician letter for the /g/ phoneme (𐤂 "gīml"), 
and as such is cognate with Hebrew "gimel" ג. 
Based on its name, the letter has been interpreted as an abstract representation of a camel's neck, but this has been criticized as contrived, and it is more likely that the letter is derived from an Egyptian hieroglyph representing a club or throwing stick.
In the archaic period, the shape of gamma was closer to a classical lambda (Λ), while lambda retained the Phoenician L-shape (𐌋).
Letters that arose from the Greek gamma include Etruscan (Old Italic) 𐌂, Roman C and G, Runic "kaunan" ᚲ, Gothic "geuua" 𐌲,
the Coptic Ⲅ, 
and the Cyrillic letters Г and Ґ.
Greek phoneme.
The Ancient Greek /g/ phoneme was the voiced velar stop, continuing the PIE voiced velar and palatal stops "*g", "*ǵ".
The modern Greek phoneme represented by gamma is realized either as a palatal (before a front vowel, /e, i/), or as a velar (in all other environments). Both in Ancient and in Modern Greek, before other velars (κ, χ, ξ "k, kh, ks"), gamma represents a velar nasal . A double gamma γγ represents the sequence (phonetically varying ) or .
Phonetic transcription.
Lowercase Greek gamma is used in the Americanist phonetic notation and Uralic Phonetic Alphabet to indicate voiced consonants. In International Phonetic Alphabet, it represents the voiced velar fricative.
In these contexts, gamma is considered as an addition to Latin alphabet, the so-called Latin or latinized gamma ɣ (with an upper-case variant Ɣ based on the minuscule).
Latin gamma is used to represent a voiced velar fricative, both in the International Phonetic Alphabet, and in the alphabets of African languages such as Dagbani, Dinka, Kabiyé, and Ewe, 
some Berber languages using the Berber Latin alphabet, and sometimes in the romanization of Pashto.
Lowercase Latin gamma is used in the International Phonetic Alphabet to represent the voiced velar fricative. A lowercase Latin gamma that lies above the baseline rather than crossing it () represents the close-mid back unrounded vowel. In certain nonstandard variations of the IPA the uppercase form is used.
Mathematics and science.
Lower case.
The lower-case letter formula_1 is used as a symbol for:
The lowercase Latin gamma ɣ can also be used in contexts (such as chemical or molecule nomenclature) where gamma must not be confused with the letter y, which can occur in some computer typefaces.
Upper case.
The upper-case letter formula_3 is used as a symbol for:
Encoding.
HTML.
The HTML entities for uppercase and lowercase gamma are codice_1 and codice_2.
Unicode.
These characters are used only as mathematical symbols. Stylized Greek text should be encoded using the normal Greek letters, with markup and formatting to indicate text style.

</doc>
<doc id="12265" url="https://en.wikipedia.org/wiki?curid=12265" title="Goitre">
Goitre

A goitre (British English) or goiter (American English) (from the Latin "gutteria", struma) is a swelling of the neck or larynx resulting from enlargement of the thyroid gland (thyromegaly), associated with a thyroid gland that is not functioning properly.
Worldwide, over 90.54% cases of goitre are caused by iodine deficiency.
Signs and symptoms.
Goitre which is associated with hypothyroidism or hyperthyroidism may be present with symptoms of the underlying disorder. For hyperthyroidism, the most common symptoms are associated with adrenergic stimulation: tachycardia, palpitations, nervousness, tremor, increased blood pressure and heat intolerance. Clinical manifestations are often related to hypermetabolism, including increased metabolism, excessive thyroid hormone, an increase in oxygen consumption, metabolic changes in protein metabolism, immunologic stimulation of diffuse goitre, and ocular changes (exophthalmos). Hypothyroid individuals may have weight gain despite poor appetite, cold intolerance, constipation and lethargy. However, these symptoms are often nonspecific and hard to diagnose.
Morphology.
Regarding morphology, goitres may be classified either as the growth pattern or as the size of the growth:
Causes.
Worldwide, the most common cause for goitre is iodine deficiency, usually seen in countries that do not use iodized salt. Selenium deficiency is also considered a contributing factor. In countries that use iodized salt, Hashimoto's thyroiditis is the most common cause. Goitre can also result from cyanide poisoning; this is particularly common in tropical countries where people eat the cyanide-rich cassava root as the staple food.
Treatment.
Goitre is treated according to the cause. If the thyroid gland is producing too much T3 and T4, radioactive iodine is given to the patient to shrink the gland. If goitre is caused by iodine deficiency, small doses of iodide in the form of Lugol's Iodine or KI solution are given. If the goitre is associated with an underactive thyroid, thyroid supplements are used as treatment. In extreme cases, a partial or complete thyroidectomy is required.
Epidemiology.
[[File:Iodine deficiency world map - DALY - WHO2002.svg|thumb|upright=1.15|Disability-adjusted life year for iodine deficiency per 100,000 inhabitants in 2002.
Goitre is more common among women, but this includes the many types of goitre caused by autoimmune problems, and not only those caused by simple lack of iodine.
History.
Chinese physicians of the Tang Dynasty (618–907) were the first to successfully treat patients with goitre by using the iodine-rich thyroid gland of animals such as sheep and pigs—in raw, pill, or powdered form. This was outlined in Zhen Quan's (d. 643 AD) book, as well as several others. One Chinese book, "The Pharmacopoeia of the Heavenly Husbandman", asserted that iodine-rich sargassum was used to treat goitre patients by the 1st century BC, but this book was written much later.
In the 12th century, Zayn al-Din al-Jurjani, a Persian physician, provided the first description of Graves' disease after noting the association of goitre and exophthalmos in his "Thesaurus of the Shah of Khwarazm", the major medical dictionary of its time. Al-Jurjani also established an association between goitre and palpitation. The disease was later named after Irish doctor Robert James Graves, who described a case of goitre with exophthalmos in 1835. The German Karl Adolph von Basedow also independently reported the same constellation of symptoms in 1840, while earlier reports of the disease were also published by the Italians Giuseppe Flajani and Antonio Giuseppe Testa, in 1802 and 1810 respectively, and by the English physician Caleb Hillier Parry (a friend of Edward Jenner) in the late 18th century.
Paracelsus (1493–1541) was the first person to propose a relationship between goitre and minerals (particularly lead) in drinking water. Iodine was later discovered by Bernard Courtois in 1811 from seaweed ash.
Goitre was previously common in many areas that were deficient in iodine in the soil. For example, in the English Midlands, the condition was known as Derbyshire Neck. In the United States, goitre was found in the Great Lakes, Midwest, and Intermountain regions. The condition now is practically absent in affluent nations, where table salt is supplemented with iodine. However, it is still prevalent in India, China, Central Asia, and Central Africa.
Goitre had been prevalent in the alpine countries for a long time. Switzerland reduced the condition by introducing iodised salt in 1922. The Bavarian tracht in the Miesbach and Salzburg regions, which appeared in the 19th century, includes a choker, dubbed "Kropfband" (struma band) which was used to hide either the goitre or the remnants of goitre surgery.
Society and culture.
In the 1920s wearing bottles of iodine around the neck was believed to prevent goitre.
Heraldry.
The coat of arms and crest of Die Kröpfner, of Tyrol showed a man "afflicted with a large goitre", an apparent pun on the German for the word ("Kropf").

</doc>
<doc id="12266" url="https://en.wikipedia.org/wiki?curid=12266" title="Genetics">
Genetics

Genetics is the study of genes, genetic variation, and heredity in living organisms. It is generally considered a field of biology, but it intersects frequently with many of the life sciences and is strongly linked with the study of information systems.
The father of genetics is Gregor Mendel, a late 19th-century scientist and Augustinian friar. Mendel studied 'trait inheritance', patterns in the way traits were handed down from parents to offspring. He observed that organisms (pea plants) inherit traits by way of discrete "units of inheritance". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.
Trait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded beyond inheritance to studying the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance) and within the context of a population. Genetics has given rise to a number of sub-fields including epigenetics and population genetics. Organisms studied within the broad field span the domain of life, including bacteria, plants, animals, and humans.
Genetic processes work in combination with an organism's environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intra- or extra-cellular environment of a cell or organism may switch gene transcription on or off. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate. While the average height of the two corn stalks may be genetically determined to be equal, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment.
Etymology.
The word genetics stems from the Ancient Greek ' meaning "genitive"/"generative", which in turn derives from ' meaning "origin".
The gene.
The modern working definition of a gene is a portion (or sequence) of DNA that codes for a known cellular function or process (e.g. the function "make melanin molecules"). A single 'gene' is most similar to a single 'word' in the English language. The nucleotides (molecules) that make up genes can be seen as 'letters' in the English language. Nucleotides are named according to which of the four nitrogenous bases they contain. The four bases are cytosine, guanine, adenine, and thymine. A single gene may have a small number of nucleotides or a large number of nucleotides, in the same way that a word may be small or large (e.g. 'cell' vs. 'electrophysiology'). A single gene often interacts with neighboring genes to produce a cellular function and can even be ineffectual without those neighboring genes. This can be seen in the same way that a 'word' may have meaning only in the context of a 'sentence.' A series of nucleotides can be put together without forming a gene (non coding regions of DNA), like a string of letters can be put together without forming a word (e.g. udkslk). Nonetheless, all words have letters, like all genes must have nucleotides.
A quick heuristic that is often used (but not always true) is "one gene, one protein" meaning a singular gene codes for a singular protein type in a cell (enzyme, transcription factor, etc.).
The sequence of nucleotides in a gene is read and translated by a cell to produce a chain of amino acids which in turn folds into a protein. The order of amino acids in a protein corresponds to the order of nucleotides in the gene. This relationship between nucleotide sequence and amino acid sequence is known as the genetic code. The amino acids in a protein determine how it folds into its unique three-dimensional shape, a structure that is ultimately responsible for the protein's function. Proteins carry out many of the functions needed for cells to live. A change to the DNA in a gene can alter a protein's amino acid sequence, thereby changing its shape and function and rendering the protein ineffective or even malignant (e.g. sickle cell anemia). Changes to genes are called mutations.
History.
The observation that living things inherit traits from their parents has been used since prehistoric times to improve crop plants and animals through selective breeding. The modern science of genetics, seeking to understand this process, began with the work of Gregor Mendel in the mid-19th century.
Prior to Mendel, Imre Festetics, a Hungarian noble, who lived in Kőszeg before Gregor Mendel, was the first who used the word "genetics". He described several rules of genetic inheritance in his work "The genetic law of the Nature" (Die genetische Gesätze der Natur, 1819). His second law is the same as what Mendel published. In his third law, he developed the basic principles of mutation (he can be considered a forerunner of Hugo de Vries.)
Other theories of inheritance preceded his work. A popular theory during Mendel's time was the concept of blending inheritance: the idea that individuals inherit a smooth blend of traits from their parents. Mendel's work provided examples where traits were definitely not blended after hybridization, showing that traits are produced by combinations of distinct genes rather than a continuous blend. Blending of traits in the progeny is now explained by the action of multiple genes with quantitative effects. Another theory that had some support at that time was the inheritance of acquired characteristics: the belief that individuals inherit traits strengthened by their parents. This theory (commonly associated with Jean-Baptiste Lamarck) is now known to be wrong—the experiences of individuals do not affect the genes they pass to their children, although evidence in the field of epigenetics has revived some aspects of Lamarck's theory. Other theories included the pangenesis of Charles Darwin (which had both acquired and inherited aspects) and Francis Galton's reformulation of pangenesis as both particulate and inherited.
Mendelian and classical genetics.
Modern genetics started with Gregor Johann Mendel, a scientist and Augustinian friar who studied the nature of inheritance in plants. In his paper ""Versuche über Pflanzenhybriden"" ("Experiments on Plant Hybridization"), presented in 1865 to the "Naturforschender Verein" (Society for Research in Nature) in Brünn, Mendel traced the inheritance patterns of certain traits in pea plants and described them mathematically. Although this pattern of inheritance could only be observed for a few traits, Mendel's work suggested that heredity was particulate, not acquired, and that the inheritance patterns of many traits could be explained through simple rules and ratios.
The importance of Mendel's work did not gain wide understanding until the 1890s, after his death, when other scientists working on similar problems re-discovered his research. William Bateson, a proponent of Mendel's work, coined the word "genetics" in 1905. (The adjective "genetic", derived from the Greek word "genesis"—γένεσις, "origin", predates the noun and was first used in a biological sense in 1860.) Bateson both acted as a mentor and was aided significantly by the work of women scientists from Newnham College at Cambridge, specifically the work of Becky Saunders, Nora Darwin Barlow, and Muriel Wheldale Onslow. Bateson popularized the usage of the word "genetics" to describe the study of inheritance in his inaugural address to the Third International Conference on Plant Hybridization in London, England, in 1906.
After the rediscovery of Mendel's work, scientists tried to determine which molecules in the cell were responsible for inheritance. In 1911, Thomas Hunt Morgan argued that genes are on chromosomes, based on observations of a sex-linked white eye mutation in fruit flies. In 1913, his student Alfred Sturtevant used the phenomenon of genetic linkage to show that genes are arranged linearly on the chromosome.
Molecular genetics.
Although genes were known to exist on chromosomes, chromosomes are composed of both protein and DNA, and scientists did not know which of the two is responsible for inheritance. In 1928, Frederick Griffith discovered the phenomenon of transformation (see Griffith's experiment): dead bacteria could transfer genetic material to "transform" other still-living bacteria. Sixteen years later, in 1944, the Avery–MacLeod–McCarty experiment identified DNA as the molecule responsible for transformation. The role of the nucleus as the repository of genetic information in eukaryotes had been established by Hämmerling in 1943 in his work on the single celled alga "Acetabularia". The Hershey–Chase experiment in 1952 confirmed that DNA (rather than protein) is the genetic material of the viruses that infect bacteria, providing further evidence that DNA is the molecule responsible for inheritance.
James Watson and Francis Crick determined the structure of DNA in 1953, using the X-ray crystallography work of Rosalind Franklin and Maurice Wilkins that indicated DNA had a helical structure (i.e., shaped like a corkscrew). Their double-helix model had two strands of DNA with the nucleotides pointing inward, each matching a complementary nucleotide on the other strand to form what looks like rungs on a twisted ladder. This structure showed that genetic information exists in the sequence of nucleotides on each strand of DNA. The structure also suggested a simple method for replication: if the strands are separated, new partner strands can be reconstructed for each based on the sequence of the old strand. This property is what gives DNA its semi-conservative nature where one strand of new DNA is from an original parent strand.
Although the structure of DNA showed how inheritance works, it was still not known how DNA influences the behavior of cells. In the following years, scientists tried to understand how DNA controls the process of protein production. It was discovered that the cell uses DNA as a template to create matching messenger RNA, molecules with nucleotides very similar to DNA. The nucleotide sequence of a messenger RNA is used to create an amino acid sequence in protein; this translation between nucleotide sequences and amino acid sequences is known as the genetic code.
With the newfound molecular understanding of inheritance came an explosion of research. A notable theory arose from Tomoko Ohta in 1973 with her amendment to the neutral theory of molecular evolution through publishing the nearly neutral theory of molecular evolution. In this theory, Ohta stressed the importance of natural selection and the environment to the rate at which genetic evolution occurs. One important development was chain-termination DNA sequencing in 1977 by Frederick Sanger. This technology allows scientists to read the nucleotide sequence of a DNA molecule. In 1983, Kary Banks Mullis developed the polymerase chain reaction, providing a quick way to isolate and amplify a specific section of DNA from a mixture. The efforts of the Human Genome Project, Department of Energy, NIH, and parallel private efforts by Celera Genomics led to the sequencing of the human genome in 2003.
Features of inheritance.
Discrete inheritance and Mendel's laws.
At its most fundamental level, inheritance in organisms occurs by passing discrete heritable units, called genes, from parents to progeny. This property was first observed by Gregor Mendel, who studied the segregation of heritable traits in pea plants. In his experiments studying the trait for flower color, Mendel observed that the flowers of each pea plant were either purple or white—but never an intermediate between the two colors. These different, discrete versions of the same gene are called alleles.
In the case of the pea, which is a diploid species, each individual plant has two copies of each gene, one copy inherited from each parent. Many species, including humans, have this pattern of inheritance. Diploid organisms with two copies of the same allele of a given gene are called homozygous at that gene locus, while organisms with two different alleles of a given gene are called heterozygous.
The set of alleles for a given organism is called its genotype, while the observable traits of the organism are called its phenotype. When organisms are heterozygous at a gene, often one allele is called dominant as its qualities dominate the phenotype of the organism, while the other allele is called recessive as its qualities recede and are not observed. Some alleles do not have complete dominance and instead have incomplete dominance by expressing an intermediate phenotype, or codominance by expressing both alleles at once.
When a pair of organisms reproduce sexually, their offspring randomly inherit one of the two alleles from each parent. These observations of discrete inheritance and the segregation of alleles are collectively known as Mendel's first law or the Law of Segregation.
Notation and diagrams.
Geneticists use diagrams and symbols to describe inheritance. A gene is represented by one or a few letters. Often a "+" symbol is used to mark the usual, non-mutant allele for a gene.
In fertilization and breeding experiments (and especially when discussing Mendel's laws) the parents are referred to as the "P" generation and the offspring as the "F1" (first filial) generation. When the F1 offspring mate with each other, the offspring are called the "F2" (second filial) generation. One of the common diagrams used to predict the result of cross-breeding is the Punnett square.
When studying human genetic diseases, geneticists often use pedigree charts to represent the inheritance of traits. These charts map the inheritance of a trait in a family tree.
Multiple gene interactions.
Organisms have thousands of genes, and in sexually reproducing organisms these genes generally assort independently of each other. This means that the inheritance of an allele for yellow or green pea color is unrelated to the inheritance of alleles for white or purple flowers. This phenomenon, known as "Mendel's second law" or the "Law of independent assortment", means that the alleles of different genes get shuffled between parents to form offspring with many different combinations. (Some genes do not assort independently, demonstrating genetic linkage, a topic discussed later in this article.)
Often different genes can interact in a way that influences the same trait. In the Blue-eyed Mary ("Omphalodes verna"), for example, there exists a gene with alleles that determine the color of flowers: blue or magenta. Another gene, however, controls whether the flowers have color at all or are white. When a plant has two copies of this white allele, its flowers are white—regardless of whether the first gene has blue or magenta alleles. This interaction between genes is called epistasis, with the second gene epistatic to the first.
Many traits are not discrete features (e.g. purple or white flowers) but are instead continuous features (e.g. human height and skin color). These complex traits are products of many genes. The influence of these genes is mediated, to varying degrees, by the environment an organism has experienced. The degree to which an organism's genes contribute to a complex trait is called heritability. Measurement of the heritability of a trait is relative—in a more variable environment, the environment has a bigger influence on the total variation of the trait. For example, human height is a trait with complex causes. It has a heritability of 89% in the United States. In Nigeria, however, where people experience a more variable access to good nutrition and health care, height has a heritability of only 62%.
Molecular basis for inheritance.
DNA and chromosomes.
The molecular basis for genes is deoxyribonucleic acid (DNA). DNA is composed of a chain of nucleotides, of which there are four types: adenine (A), cytosine (C), guanine (G), and thymine (T). Genetic information exists in the sequence of these nucleotides, and genes exist as stretches of sequence along the DNA chain. Viruses are the only exception to this rule—sometimes viruses use the very similar molecule, RNA, instead of DNA as their genetic material. Viruses cannot reproduce without a host and are unaffected by many genetic processes, so tend not to be considered living organisms.
DNA normally exists as a double-stranded molecule, coiled into the shape of a double helix. Each nucleotide in DNA preferentially pairs with its partner nucleotide on the opposite strand: A pairs with T, and C pairs with G. Thus, in its two-stranded form, each strand effectively contains all necessary information, redundant with its partner strand. This structure of DNA is the physical basis for inheritance: DNA replication duplicates the genetic information by splitting the strands and using each strand as a template for synthesis of a new partner strand.
Genes are arranged linearly along long chains of DNA base-pair sequences. In bacteria, each cell usually contains a single circular genophore, while eukaryotic organisms (such as plants and animals) have their DNA arranged in multiple linear chromosomes. These DNA strands are often extremely long; the largest human chromosome, for example, is about 247 million base pairs in length. The DNA of a chromosome is associated with structural proteins that organize, compact and control access to the DNA, forming a material called chromatin; in eukaryotes, chromatin is usually composed of nucleosomes, segments of DNA wound around cores of histone proteins. The full set of hereditary material in an organism (usually the combined DNA sequences of all chromosomes) is called the genome.
While haploid organisms have only one copy of each chromosome, most animals and many plants are diploid, containing two of each chromosome and thus two copies of every gene. The two alleles for a gene are located on identical loci of the two homologous chromosomes, each allele inherited from a different parent.
Many species have so-called sex chromosomes that determine the gender of each organism. In humans and many other animals, the Y chromosome contains the gene that triggers the development of the specifically male characteristics. In evolution, this chromosome has lost most of its content and also most of its genes, while the X chromosome is similar to the other chromosomes and contains many genes. The X and Y chromosomes form a strongly heterogeneous pair.
Reproduction.
When cells divide, their full genome is copied and each daughter cell inherits one copy. This process, called mitosis, is the simplest form of reproduction and is the basis for asexual reproduction. Asexual reproduction can also occur in multicellular organisms, producing offspring that inherit their genome from a single parent. Offspring that are genetically identical to their parents are called clones.
Eukaryotic organisms often use sexual reproduction to generate offspring that contain a mixture of genetic material inherited from two different parents. The process of sexual reproduction alternates between forms that contain single copies of the genome (haploid) and double copies (diploid). Haploid cells fuse and combine genetic material to create a diploid cell with paired chromosomes. Diploid organisms form haploids by dividing, without replicating their DNA, to create daughter cells that randomly inherit one of each pair of chromosomes. Most animals and many plants are diploid for most of their lifespan, with the haploid form reduced to single cell gametes such as sperm or eggs.
Although they do not use the haploid/diploid method of sexual reproduction, bacteria have many methods of acquiring new genetic information. Some bacteria can undergo conjugation, transferring a small circular piece of DNA to another bacterium. Bacteria can also take up raw DNA fragments found in the environment and integrate them into their genomes, a phenomenon known as transformation. These processes result in horizontal gene transfer, transmitting fragments of genetic information between organisms that would be otherwise unrelated.
Recombination and genetic linkage.
The diploid nature of chromosomes allows for genes on different chromosomes to assort independently or be separated from their homologous pair during sexual reproduction wherein haploid gametes are formed. In this way new combinations of genes can occur in the offspring of a mating pair. Genes on the same chromosome would theoretically never recombine. However, they do via the cellular process of chromosomal crossover. During crossover, chromosomes exchange stretches of DNA, effectively shuffling the gene alleles between the chromosomes. This process of chromosomal crossover generally occurs during meiosis, a series of cell divisions that creates haploid cells.
The first cytological demonstration of crossing over was performed by Harriet Creighton and Barbara McClintock in 1931. Their research and experiments on corn provided cytological evidence for the genetic theory that linked genes on paired chromosomes do in fact exchange places from one homolog to the other.
The probability of chromosomal crossover occurring between two given points on the chromosome is related to the distance between the points. For an arbitrarily long distance, the probability of crossover is high enough that the inheritance of the genes is effectively uncorrelated. For genes that are closer together, however, the lower probability of crossover means that the genes demonstrate genetic linkage; alleles for the two genes tend to be inherited together. The amounts of linkage between a series of genes can be combined to form a linear linkage map that roughly describes the arrangement of the genes along the chromosome.
Gene expression.
Genetic code.
Genes generally express their functional effect through the production of proteins, which are complex molecules responsible for most functions in the cell. Proteins are made up of one or more polypeptide chains, each of which is composed of a sequence of amino acids, and the DNA sequence of a gene (through an RNA intermediate) is used to produce a specific amino acid sequence. This process begins with the production of an RNA molecule with a sequence matching the gene's DNA sequence, a process called transcription.
This messenger RNA molecule is then used to produce a corresponding amino acid sequence through a process called translation. Each group of three nucleotides in the sequence, called a codon, corresponds either to one of the twenty possible amino acids in a protein or an instruction to end the amino acid sequence; this correspondence is called the genetic code. The flow of information is unidirectional: information is transferred from nucleotide sequences into the amino acid sequence of proteins, but it never transfers from protein back into the sequence of DNA—a phenomenon Francis Crick called the central dogma of molecular biology.
The specific sequence of amino acids results in a unique three-dimensional structure for that protein, and the three-dimensional structures of proteins are related to their functions. Some are simple structural molecules, like the fibers formed by the protein collagen. Proteins can bind to other proteins and simple molecules, sometimes acting as enzymes by facilitating chemical reactions within the bound molecules (without changing the structure of the protein itself). Protein structure is dynamic; the protein hemoglobin bends into slightly different forms as it facilitates the capture, transport, and release of oxygen molecules within mammalian blood.
A single nucleotide difference within DNA can cause a change in the amino acid sequence of a protein. Because protein structures are the result of their amino acid sequences, some changes can dramatically change the properties of a protein by destabilizing the structure or changing the surface of the protein in a way that changes its interaction with other proteins and molecules. For example, sickle-cell anemia is a human genetic disease that results from a single base difference within the coding region for the β-globin section of hemoglobin, causing a single amino acid change that changes hemoglobin's physical properties. Sickle-cell versions of hemoglobin stick to themselves, stacking to form fibers that distort the shape of red blood cells carrying the protein. These sickle-shaped cells no longer flow smoothly through blood vessels, having a tendency to clog or degrade, causing the medical problems associated with this disease.
Some DNA sequences are transcribed into RNA but are not translated into protein products—such RNA molecules are called non-coding RNA. In some cases, these products fold into structures which are involved in critical cell functions (e.g. ribosomal RNA and transfer RNA). RNA can also have regulatory effects through hybridization interactions with other RNA molecules (e.g. microRNA).
Nature and nurture.
Although genes contain all the information an organism uses to function, the environment plays an important role in determining the ultimate phenotypes an organism displays. This is the complementary relationship often referred to as "nature and nurture". The phenotype of an organism depends on the interaction of genes and the environment. An interesting example is the coat coloration of the Siamese cat. In this case, the body temperature of the cat plays the role of the environment. The cat's genes code for dark hair, thus the hair-producing cells in the cat make cellular proteins resulting in dark hair. But these dark hair-producing proteins are sensitive to temperature (i.e. have a mutation causing temperature-sensitivity) and denature in higher-temperature environments, failing to produce dark-hair pigment in areas where the cat has a higher body temperature. In a low-temperature environment, however, the protein's structure is stable and produces dark-hair pigment normally. The protein remains functional in areas of skin that are coldersuch as its legs, ears, tail and faceso the cat has dark-hair at its extremities.
Environment plays a major role in effects of the human genetic disease phenylketonuria. The mutation that causes phenylketonuria disrupts the ability of the body to break down the amino acid phenylalanine, causing a toxic build-up of an intermediate molecule that, in turn, causes severe symptoms of progressive mental retardation and seizures. However, if someone with the phenylketonuria mutation follows a strict diet that avoids this amino acid, they remain normal and healthy.
A popular method in determining how genes and environment ("nature and nurture") contribute to a phenotype is by studying identical and fraternal twins or siblings of multiple births. Because identical siblings come from the same zygote, they are genetically the same. Fraternal twins are as genetically different from one another as normal siblings. By analyzing statistics on how often a twin of a set has a certain disorder compared to other sets of twins, scientists can determine whether that disorder is caused by genetic or environmental factors (i.e. whether it has 'nature' or 'nurture' causes). One famous example is the multiple birth study of the Genain quadruplets, who were identical quadruplets all diagnosed with schizophrenia.
Gene regulation.
The genome of a given organism contains thousands of genes, but not all these genes need to be active at any given moment. A gene is expressed when it is being transcribed into mRNA and there exist many cellular methods of controlling the expression of genes such that proteins are produced only when needed by the cell. Transcription factors are regulatory proteins that bind to DNA, either promoting or inhibiting the transcription of a gene. Within the genome of "Escherichia coli" bacteria, for example, there exists a series of genes necessary for the synthesis of the amino acid tryptophan. However, when tryptophan is already available to the cell, these genes for tryptophan synthesis are no longer needed. The presence of tryptophan directly affects the activity of the genes—tryptophan molecules bind to the tryptophan repressor (a transcription factor), changing the repressor's structure such that the repressor binds to the genes. The tryptophan repressor blocks the transcription and expression of the genes, thereby creating negative feedback regulation of the tryptophan synthesis process.
Differences in gene expression are especially clear within multicellular organisms, where cells all contain the same genome but have very different structures and behaviors due to the expression of different sets of genes. All the cells in a multicellular organism derive from a single cell, differentiating into variant cell types in response to external and intercellular signals and gradually establishing different patterns of gene expression to create different behaviors. As no single gene is responsible for the development of structures within multicellular organisms, these patterns arise from the complex interactions between many cells.
Within eukaryotes, there exist structural features of chromatin that influence the transcription of genes, often in the form of modifications to DNA and chromatin that are stably inherited by daughter cells. These features are called "epigenetic" because they exist "on top" of the DNA sequence and retain inheritance from one cell generation to the next. Because of epigenetic features, different cell types grown within the same medium can retain very different properties. Although epigenetic features are generally dynamic over the course of development, some, like the phenomenon of paramutation, have multigenerational inheritance and exist as rare exceptions to the general rule of DNA as the basis for inheritance.
Genetic change.
Mutations.
During the process of DNA replication, errors occasionally occur in the polymerization of the second strand. These errors, called mutations, can have an impact on the phenotype of an organism, especially if they occur within the protein coding sequence of a gene. Error rates are usually very low—1 error in every 10–100 million bases—due to the "proofreading" ability of DNA polymerases. Processes that increase the rate of changes in DNA are called mutagenic: mutagenic chemicals promote errors in DNA replication, often by interfering with the structure of base-pairing, while UV radiation induces mutations by causing damage to the DNA structure. Chemical damage to DNA occurs naturally as well and cells use DNA repair mechanisms to repair mismatches and breaks. The repair does not, however, always restore the original sequence.
In organisms that use chromosomal crossover to exchange DNA and recombine genes, errors in alignment during meiosis can also cause mutations. Errors in crossover are especially likely when similar sequences cause partner chromosomes to adopt a mistaken alignment; this makes some regions in genomes more prone to mutating in this way. These errors create large structural changes in DNA sequenceduplications, inversions, deletions of entire regionsor the accidental exchange of whole parts of sequences between different chromosomes (chromosomal translocation).
Natural selection and evolution.
Mutations alter an organism's genotype and occasionally this causes different phenotypes to appear. Most mutations have little effect on an organism's phenotype, health, or reproductive fitness. Mutations that do have an effect are usually deleterious, but occasionally some can be beneficial. Studies in the fly "Drosophila melanogaster" suggest that if a mutation changes a protein produced by a gene, about 70 percent of these mutations will be harmful with the remainder being either neutral or weakly beneficial.
Population genetics studies the distribution of genetic differences within populations and how these distributions change over time. Changes in the frequency of an allele in a population are mainly influenced by natural selection, where a given allele provides a selective or reproductive advantage to the organism, as well as other factors such as mutation, genetic drift, genetic draft, artificial selection and migration.
Over many generations, the genomes of organisms can change significantly, resulting in evolution. In the process called adaptation, selection for beneficial mutations can cause a species to evolve into forms better able to survive in their environment.Earlier related ideas were acknowledged in </ref> New species are formed through the process of speciation, often caused by geographical separations that prevent populations from exchanging genes with each other. The application of genetic principles to the study of population biology and evolution is known as the "modern synthesis".
By comparing the homology between different species' genomes, it is possible to calculate the evolutionary distance between them and when they may have diverged. Genetic comparisons are generally considered a more accurate method of characterizing the relatedness between species than the comparison of phenotypic characteristics. The evolutionary distances between species can be used to form evolutionary trees; these trees represent the common descent and divergence of species over time, although they do not show the transfer of genetic material between unrelated species (known as horizontal gene transfer and most common in bacteria).
Model organisms.
Although geneticists originally studied inheritance in a wide range of organisms, researchers began to specialize in studying the genetics of a particular subset of organisms. The fact that significant research already existed for a given organism would encourage new researchers to choose it for further study, and so eventually a few model organisms became the basis for most genetics research. Common research topics in model organism genetics include the study of gene regulation and the involvement of genes in development and cancer.
Organisms were chosen, in part, for convenience—short generation times and easy genetic manipulation made some organisms popular genetics research tools. Widely used model organisms include the gut bacterium "Escherichia coli", the plant "Arabidopsis thaliana", baker's yeast ("Saccharomyces cerevisiae"), the nematode "Caenorhabditis elegans", the common fruit fly ("Drosophila melanogaster"), and the common house mouse ("Mus musculus").
Medicine.
Medical genetics seeks to understand how genetic variation relates to human health and disease. When searching for an unknown gene that may be involved in a disease, researchers commonly use genetic linkage and genetic pedigree charts to find the location on the genome associated with the disease. At the population level, researchers take advantage of Mendelian randomization to look for locations in the genome that are associated with diseases, a method especially useful for multigenic traits not clearly defined by a single gene. Once a candidate gene is found, further research is often done on the corresponding genethe orthologous genein model organisms. In addition to studying genetic diseases, the increased availability of genotyping methods has led to the field of pharmacogenetics: the study of how genotype can affect drug responses.
Individuals differ in their inherited tendency to develop cancer, and cancer is a genetic disease. The process of cancer development in the body is a combination of events. Mutations occasionally occur within cells in the body as they divide. Although these mutations will not be inherited by any offspring, they can affect the behavior of cells, sometimes causing them to grow and divide more frequently. There are biological mechanisms that attempt to stop this process; signals are given to inappropriately dividing cells that should trigger cell death, but sometimes additional mutations occur that cause cells to ignore these messages. An internal process of natural selection occurs within the body and eventually mutations accumulate within cells to promote their own growth, creating a cancerous tumor that grows and invades various tissues of the body.
Normally, a cell divides only in response to signals called growth factors and stops growing once in contact with surrounding cells and in response to growth-inhibitory signals. It usually then divides a limited number of times and dies, staying within the epithelium where it is unable to migrate to other organs. To become a cancer cell, a cell has to accumulate mutations in a number of genes (3–7) that allow it to bypass this regulation: it no longer needs growth factors to divide, it continues growing when making contact to neighbor cells, and ignores inhibitory signals, it will keep growing indefinitely and is immortal, it will escape from the epithelium and ultimately may be able to escape from the primary tumor, cross the endothelium of a blood vessel, be transported by the bloodstream and will colonize a new organ, forming deadly metastasis. Although there are some genetic predispositions in a small fraction of cancers, the major fraction is due to a set of new genetic mutations that originally appear and accumulate in one or a small number of cells that will divide to form the tumor and are not transmitted to the progeny (somatic mutations). The most frequent mutations are a loss of function of p53 protein, a tumor suppressor, or in the p53 pathway, and gain of function mutations in the ras proteins, or in other oncogenes.
Research methods.
DNA can be manipulated in the laboratory. Restriction enzymes are commonly used enzymes that cut DNA at specific sequences, producing predictable fragments of DNA. DNA fragments can be visualized through use of gel electrophoresis, which separates fragments according to their length.
The use of ligation enzymes allows DNA fragments to be connected. By binding ("ligating") fragments of DNA together from different sources, researchers can create recombinant DNA, the DNA often associated with genetically modified organisms. Recombinant DNA is commonly used in the context of plasmids: short circular DNA molecules with a few genes on them. In the process known as molecular cloning, researchers can amplify the DNA fragments by inserting plasmids into bacteria and then culturing them on plates of agar (to isolate clones of bacteria cells). ("Cloning" can also refer to the various means of creating cloned ("clonal") organisms.)
DNA can also be amplified using a procedure called the polymerase chain reaction (PCR). By using specific short sequences of DNA, PCR can isolate and exponentially amplify a targeted region of DNA. Because it can amplify from extremely small amounts of DNA, PCR is also often used to detect the presence of specific DNA sequences.
DNA sequencing and genomics.
DNA sequencing, one of the most fundamental technologies developed to study genetics, allows researchers to determine the sequence of nucleotides in DNA fragments. The technique of chain-termination sequencing, developed in 1977 by a team led by Frederick Sanger, is still routinely used to sequence DNA fragments. Using this technology, researchers have been able to study the molecular sequences associated with many human diseases.
As sequencing has become less expensive, researchers have sequenced the genomes of many organisms, using a process called genome assembly, which utilizes computational tools to stitch together sequences from many different fragments. These technologies were used to sequence the human genome in the Human Genome Project completed in 2003. New high-throughput sequencing technologies are dramatically lowering the cost of DNA sequencing, with many researchers hoping to bring the cost of resequencing a human genome down to a thousand dollars.
Next generation sequencing (or high-throughput sequencing) came about due to the ever-increasing demand for low-cost sequencing. These sequencing technologies allow the production of potentially millions of sequences concurrently. The large amount of sequence data available has created the field of genomics, research that uses computational tools to search for and analyze patterns in the full genomes of organisms. Genomics can also be considered a subfield of bioinformatics, which uses computational approaches to analyze large sets of biological data. A common problem to these fields of research is how to manage and share data that deals with human subject and personally identifiable information. See also genomics data sharing.
Society and culture.
On 19 March 2015, a leading group of biologists urged a worldwide ban on clinical use of methods, particularly the use of CRISPR and zinc finger, to edit the human genome in a way that can be inherited. In April 2015, Chinese researchers reported results of basic research to edit the DNA of non-viable human embryos using CRISPR.

</doc>
<doc id="12272" url="https://en.wikipedia.org/wiki?curid=12272" title="George Pappas">
George Pappas

George Sotiros Pappas (born 1942) is a professor of philosophy at Ohio State University. Pappas specializes in epistemology, the history of early modern philosophy, philosophy of religion, and metaphysics. He is of Greek and English origin.
He is the author of the Stanford Encyclopedia of Philosophy entry on Internalist versus Externalist conceptions of epistemic justification.
He was co-editor (with Marshall Swain) of "Essays on Knowledge and Justification" (1978), regarded as a key anthology of essays relating to the Gettier problem and used as a core text in undergraduate epistemology courses.
George Pappas is an editorial consultant of Berkeley Studies.
Studies in Berkeley's philosophy.
George S. Pappas is known to be a leading Berkeley scholar; his essay “Berkeley and Scepticism” was in 1993 awarded the International Berkeley Prize. Professor Pappas is a regular participant of International Berkeley Conferences. At one such conference, celebrating the 300th anniversary of George Berkeley’s birth, Mr. Pappas propounded a new approach to the relationship between Berkeley’s anti-abstractionism and "esse est percipi" principle. On Pappas reading, Berkeley’s two theses — that there are no abstract ideas and that sensible objects must be perceived in order to exist — entail one another.
Pappas' interpretation of Berkeley's ‘"esse" is "percipi"’ thesis has sparked much discussion. In 1989, the Garland Publishing Company brought out a 15-volume collection of major works on Berkeley; Pappas' paper “Abstract ideas and the 'esse is percipi' thesis” was included in the third volume, as it was considered to be a significant contribution to Berkeley scholarship.
Pappas developed his treatment of Berkeley’s “esse est percipi” principle to repudiate the "inherence interpretation of Berkeley", upon which Edwin E. Allaire, among others, elaborated
“That account is put forward to answer an extremely perplexing question in the history of philosophy: Why did Berkeley embrace idealism, i. e., why did he hold that esse est percipi, that to be is to be perceived? 
After emerging in the early 1960s, the “inherence account” attracted numerous proponents and became an influential element of contemporary Berkeley scholarship. In his paper “Ideas, minds, and Berkeley” Pappas revealed some discrepancies between fountain-head evidences and Allaire’s approach to a reconstruction of Berkeley’s idealism. Pappas' critical examination of the “inherence account” is greatly appreciated by Berkeley scholars. Pappas’ penetrating remarks compelled Edwin B. Allaire to revise and improve his conception. Even those who share Allaire’s account of Berkeley’s idealism acknowledge Pappas’ article to be “an excellent review and critique of the IA account.”
In 2000 George Pappas published his monograph "Berkeley's thought" in which some parts were based on earlier papers of his. While writings by A. A. Luce or Geoffrey Warnock are long out dated, the book "Berkeley's thought" written by Dr Pappas is often included in lists of recommended literature on Berkeley’s philosophy.

</doc>
<doc id="12274" url="https://en.wikipedia.org/wiki?curid=12274" title="Guy de Maupassant">
Guy de Maupassant

Henri René Albert Guy de Maupassant (; ; 5 August 1850 – 6 July 1893) was a French writer, remembered as a master of the short story form, and as a representative of the naturalist school of writers, who depicted human lives and destinies and social forces in disillusioned and often pessimistic terms.
Maupassant was a protégé of Flaubert and his stories are characterized by economy of style and efficient, effortless "dénouements" (outcomes). Many are set during the Franco-Prussian War of the 1870s, describing the futility of war and the innocent civilians who, caught up in events beyond their control, are permanently changed by their experiences. He wrote some 300 short stories, six novels, three travel books, and one volume of verse. His first published story, "Boule de Suif" ("Ball of Fat", 1880), is often considered his masterpiece.
Biography.
Henri-René-Albert-Guy de Maupassant was born 5 August 1850 at the "Château de Miromesnil" (Castle Miromesnil, near Dieppe in the Seine-Inférieure (now Seine-Maritime) department in France. He was the first son of Laure Le Poittevin and Gustave de Maupassant, both from prosperous bourgeois families. When Maupassant was 11 and his brother Hervé was five, his mother, an independent-minded woman, risked social disgrace to obtain a legal separation from her husband.
After the separation, Laure Le Poittevin kept her two sons. With the father's absence, Maupassant's mother became the most influential figure in the young boy's life. She was an exceptionally well read woman and was very fond of classical literature, particularly Shakespeare. Until the age of thirteen, Guy happily lived with his mother, at Étretat, in the Villa des Verguies, where, between the sea and the luxuriant countryside, he grew very fond of fishing and outdoor activities. At age thirteen, his mother next placed her two sons as day boarders in a private school, the Institution Leroy-Petit, in Rouen—the "Institution Robineau" of Maupassant's story "La Question du Latin"—for classical studies. From his early education he retained a marked hostility to religion, and to judge from verses composed around this time he deplored the ecclesiastical atmosphere, its ritual and discipline. Finding the place to be unbearable, he finally got himself expelled in his next-to-last year.
In 1867, as he entered junior high school, Maupassant made acquaintance with Gustave Flaubert at Croisset at the insistence of his mother. Next year, in autumn, he was sent to the "Lycée Pierre-Corneille" in Rouen where he proved a good scholar indulging in poetry and taking a prominent part in theatricals. In October 1868, at the age of 18, he saved the famous poet Algernon Charles Swinburne from drowning off the coast of Étretat.
The Franco-Prussian War broke out soon after his graduation from college in 1870; he enlisted as a volunteer. In 1871, he left Normandy and moved to Paris where he spent ten years as a clerk in the Navy Department. During this time his only recreation and relaxation was boating on the Seine on Sundays and holidays.
Gustave Flaubert took him under his protection and acted as a kind of literary guardian to him, guiding his debut in journalism and literature. At Flaubert's home he met Émile Zola and the Russian novelist Ivan Turgenev, as well as many of the proponents of the realist and naturalist schools. He wrote and played himself in a comedy in 1875 (with the benediction of Flaubert), "À la feuille de rose, maison turque".
In 1878, he was transferred to the Ministry of Public Instruction and became a contributing editor to several leading newspapers such as "Le Figaro", "Gil Blas", "Le Gaulois" and "l'Écho de Paris". He devoted his spare time to writing novels and short stories.
In 1880 he published what is considered his first masterpiece, "Boule de Suif", which met with instant and tremendous success. Flaubert characterized it as "a masterpiece that will endure." This was Maupassant's first piece of short fiction set during the Franco-Prussian War, and was followed by short stories such as "Deux Amis", "Mother Savage", and "Mademoiselle Fifi".
The decade from 1880 to 1891 was the most fertile period of Maupassant's life. Made famous by his first short story, he worked methodically and produced two or sometimes four volumes annually. His talent and practical business sense made him wealthy.
In 1881 he published his first volume of short stories under the title of "La Maison Tellier"; it reached its twelfth edition within two years. In 1883 he finished his first novel, "Une Vie" (translated into English as "A Woman's Life"), 25,000 copies of which were sold in less than a year. His second novel "Bel Ami", which came out in 1885, had thirty-seven printings in four months.
His editor, Havard, commissioned him to write more stories, and Maupassant continued to produce them efficiently and frequently. At this time he wrote what many consider to be his greatest novel, "Pierre et Jean".
With a natural aversion to society, he loved retirement, solitude, and meditation. He traveled extensively in Algeria, Italy, England, Brittany, Sicily, Auvergne, and from each voyage brought back a new volume. He cruised on his private yacht "Bel-Ami", named after his novel. This life did not prevent him from making friends among the literary celebrities of his day: Alexandre Dumas, fils had a paternal affection for him; at Aix-les-Bains he met Hippolyte Taine and became devoted to the philosopher-historian.
Flaubert continued to act as his literary godfather. His friendship with the Goncourts was of short duration; his frank and practical nature reacted against the ambiance of gossip, scandal, duplicity, and invidious criticism that the two brothers had created around them in the guise of an 18th-century style salon.
Maupassant was one of a fair number of 19th-century Parisians, including Charles Gounod, Alexandre Dumas, fils, and Charles Garnier, who did not care for the Eiffel Tower. He often ate lunch in the restaurant at its base, not out of preference for the food but because it was only there that he could avoid seeing its otherwise unavoidable profile. He and forty-six other Parisian literary and artistic notables attached their names to an elaborately irate letter of protest against the tower's construction, written to the Minister of Public Works.
Maupassant also wrote under several pseudonyms such as Joseph Prunier, Guy de Valmont, and Maufrigneuse (which he used from 1881 to 1885).
In his later years he developed a constant desire for solitude, an obsession for self-preservation, and a fear of death and paranoia of persecution caused by the syphilis he had contracted in his youth. On 2 January 1892, Maupassant tried to commit suicide by cutting his throat, and was committed to the private asylum of Esprit Blanche at Passy, in Paris, where he died 6 July 1893.
Guy De Maupassant penned his own epitaph: "I have coveted everything and taken pleasure in nothing." He is buried in Section 26 of the Montparnasse Cemetery, Paris.
Significance.
Maupassant is considered one of the fathers of the modern short story. He delighted in clever plotting, and served as a model for Somerset Maugham and O. Henry in this respect. One of his famous short stories, "The Necklace", was imitated with a twist by both Maugham ("Mr Know-All", "A String of Beads") and Henry James ("Paste").
Taking his cue from Balzac, Maupassant wrote comfortably in both the high-Realist and fantastic modes; stories and novels such as "L'Héritage" and "Bel-Ami" aim to recreate Third Republic France in a realistic way, whereas many of the short stories (notably "Le Horla" and "Qui sait?") describe apparently supernatural phenomena.
The supernatural in Maupassant, however, is often implicitly a symptom of the protagonists' troubled minds; Maupassant was fascinated by the burgeoning discipline of psychiatry, and attended the public lectures of Jean-Martin Charcot between 1885 and 1886.
Legacy.
Leo Tolstoy used Maupassant as the subject for one of his essays on art: "". His stories are only second to Shakespeare in their inspiration of movie adaptations with films ranging from "Stagecoach", "Citizen Kane", "Oyuki the Virgin" and "Masculine Feminine".
Friedrich Nietzsche's autobiography mentions him in the following text:
"I cannot at all conceive in which century of history one could haul together such inquisitive and at the same time delicate psychologists as one can in contemporary Paris: I can name as a sample – for their number is by no means small, ... or to pick out one of the stronger race, a genuine Latin to whom I am particularly attached, Guy de Maupassant."
Gene Roddenberry, in an early draft for the "The Questor Tapes", wrote a scene in which the android "Questor" employs Maupassant's theory that, "the human female will open her mind to a man to whom she has opened other channels of communications." In the script Questor copulates with a woman to obtain information that she is reluctant to impart. Due to complaints from NBC executives, this part of the script was never filmed.
Michel Drach directed and co-wrote a 1982 French biographical film: "Guy de Maupassant". Claude Brasseur stars as the titular character.

</doc>
<doc id="12276" url="https://en.wikipedia.org/wiki?curid=12276" title="Gheorghe Hagi">
Gheorghe Hagi

Gheorghe Hagi () is a Romanian former footballer, considered one of the best attacking midfielders in Europe during the 1980s and 1990s and the greatest Romanian footballer of all time. Galatasaray fans called him 'Comandante' (The Commander) and Romanians called him 'Regele' (The King).
Nicknamed "The Maradona of the Carpathians", Hagi is considered a hero in his homeland. He was named Romanian Footballer of the Year seven times, and is regarded as one of the best football players of his generation. As a creative advanced playmaker, he was renowned for his dribbling, technique, vision, passing and finishing.
Hagi played for the Romanian national team in three World Cups in 1990, 1994 (where he was named in the World Cup All-Star Team) and 1998, as well as in three European Football Championships in 1984, 1996 and 2000. He won a total of 125 caps for Romania, ranked second after Dorinel Munteanu, and is the joint leading goalscorer (alongside Adrian Mutu) with 35 goals.
In November 2003, to celebrate UEFA's Jubilee, he was selected as the Golden Player of Romania by the Romanian Football Federation as their most outstanding player of the past 50 years. In 2004, he was named by Pelé as one of the 125 Greatest Living Footballers at a FIFA Awards Ceremony. Hagi is one of the few footballers to have played for both Spanish rival clubs Real Madrid and Barcelona.
In 2009, Hagi founded Romanian club Viitorul Constanța. He is currently both owner and chairman of the club. Hagi also established the Gheorghe Hagi football academy, one of the biggest football academies in Southeastern Europe.
His son Ianis is also a footballer.
Early life and career.
Hagi was born in Săcele, Romania, from an ethnic Aromanian family. He started his career playing for the youth teams of Farul Constanța in the 1970s, before being selected by the Romanian Football Federation to join the squad of Luceafărul București in 1980 for two years. In 1982 he returned to Constanța, but one year later, aged 18, he was prepared to make the step to a top team. He was originally directed to Universitatea Craiova, but chose Sportul Studențesc of Bucharest instead.
In late 1987 Hagi transferred to the Romanian giants Steaua București as the team prepared for their European Super Cup final against Dynamo Kyiv. The original contract was for one game only, the final. However, after winning the trophy, where Hagi scored the only goal of the game, Steaua did not want to release him back to Sportul Studențesc and retained him. During his Steaua years (1987–1990), Hagi played 97 Liga I games, scoring 76 goals. He and the team reached the European Cup semi-final in 1988 and the final in the following year. Hagi and Steaua were the champions of Romania in 1987, 1988 and 1989 and as well as winning the Cupa României in 1987, 1988 and 1989. His strong performances had him linked with Arrigo Sacchi's AC Milan and FC Bayern Munich but Nicolae Ceaușescu's communist government rejected any offer.
After the 1990 World Cup, he was signed by Real Madrid. The La Liga side paid $4.3 million to Steaua București for him. Hagi played two seasons with Real Madrid and then was sold to Brescia.
Hagi started the season 1992–1993 with Brescia but in the first season the club was relegated to Serie B; in the next season Hagi helped Brescia Calcio win Serie B and get promoted to Serie A. After performing memorably during the 1994 World Cup, Hagi was signed by Barcelona.
After two years at FC Barcelona, Hagi signed for Galatasaray. At Galatasaray, he was both successful and highly popular among the Turkish supporters. Hagi was an important member of a team that would win four consecutive league titles. In 2000, at the age of 35, Hagi had the best days of his career winning every possible trophy with Galatasaray. Gala won the UEFA Cup after defeating Arsenal in the final, a match in which Hagi was sent off for punching Arsenal captain Tony Adams. This was followed by the capture of the European Super Cup with a historic win against Hagi's former club Real Madrid. Both feats were firsts, and remain unmatched in Turkish football history. The mass hysteria caused by these wins in Istanbul raised Hagi's popularity even further with the fans and made French ex-international Luis Fernández to say that "Hagi is like wine, the older it gets, the better he is". When he retired in 2001, he remained one of the most beloved players in the Turkish and Romanian championships. Hagi is highly praised by the Galatasaray supporters. The classic chant "I Love You Hagi" was adopted by Gala fans since his arrival at Galatasaray SK.
International career.
Hagi made his debut for the Romania national team at the age of 18 in 1983 in a game against Norway played in Oslo. He was part of the Romanian team until 2000.
Hagi led the Romanian team to its best ever international performance at the 1994 World Cup, where the team reached the quarterfinals before Sweden ended their run after winning the penalty shoot-out. Hagi scored three times in the tournament, including a memorable goal in their 3–2 surprise defeat of South American powerhouse and previous runners-up Argentina. In the first of Romania's group stage matches, against Colombia, Hagi scored one of the most memorable goals of that tournament, curling in a 40-yard lob over Colombian goalkeeper Oscar Córdoba who was caught out of position. He was named in the Team of the Tournament.
Four years later, after the 1998 World Cup, Hagi decided to retire from the national team, only to change his mind after a few months and play at the 2000 European Football Championship, during which he was sent off in the quarter-final loss against Italy.
Hagi retired from professional football in 2001, age 36, in a game called "Gala Hagi" on 24 April. He still holds the record as Romanian national team top scorer.
Career as coach.
Romania national team.
In 2001 Hagi was named the manager of Romania, replacing Ladislau Bölöni, who left the squad to coach Sporting Clube de Portugal. However, after failing to qualify the team for the World Cup, Hagi was sacked. His only notable achievement during the six months as Romania's manager was the win in Budapest against Hungary.
Bursaspor.
In 2003, Hagi took over as coach of Turkish first division side Bursaspor, but left the club after a disappointing start to the season.
Galatasaray.
He then became manager of Galatasaray in 2004, leading the team to the Turkish Cup in 2005 final with 5–1 as a score against their rivals Fenerbahçe SK. However, his contract was not renewed since his team was not able to win the Turkish League title against Fenerbahçe SK at the centennial of the club.
Politehnica Timișoara.
Romanian team Steaua București wanted to hire him in the summer of 2005, but Hagi's requested wage could not be met by the Romanian champions. Hagi became manager of FC Politehnica Timișoara instead, and after a string of bad results and disagreements with the management, he left the club after a few months. Constanța's main stadium used to bear his name, but the name was changed after Hagi signed with FC Politehnica Timișoara.
Steaua București.
From June to September 2007, Hagi coached Steaua București, had a mediocre start in the internal championship mainly due to the large number of unavailable injured players, managed to qualify the team for the second time in line to Champions League Groups passing two qualifying rounds. He resigned due to a long series of conflicts with the team's owner Gigi Becali, which also happens to be his godson. The main reason for resigning was the owner's policy of imposing players, making the team's strategy and threats. Hagi's resignation happened just a few hours after Steaua's first Champions League game in the actual season with Slavia Prague in Prague, Czech Republic, lost with 2–1
Galatasaray.
After Frank Rijkaard was sacked as coach, Hagi signed a one and a half year contract with Galatasaray on 21 October 2010. The official presentation was held on 22 October. His former team mate from Galatasaray Tugay Kerimoğlu assisted him in Istanbul. He was sacked after a series of poor results in the league on 22 March 2011.
Style of play.
A talented left-footed attacking midfielder, Hagi's playing style was frequently compared with Maradona's throughout his career, due to his technical ability as well as his temperamental character and leadership; as a youth, he was mainly inspired by compatriots Anghel Iordănescu and Ion Dumitru. A creative, quick, and mobile advanced playmaker, who was also tactically versatile and capable of playing in several offensive positions on either wing or through the middle, he was known for both scoring and assisting goals, and was renowned in particular for his dribbling ability, vision, and passing accuracy. He was also an accurate finisher and set-piece taker, and possessed notable upper body strength, despite his small stature, which, along with his control, aided him in protecting the ball from opponents. Despite his skill and his reputation as one of the greatest number 10s of his generation, his career was marked by inconsistency at times, and he was also considered to be a controversial player, due to his rebellious and arrogant attitude, as well as his low work-rate, which led to several disagreements with his managers.
Career statistics.
International goals.
"Scores and results list Romania's goal tally first"

</doc>
<doc id="12277" url="https://en.wikipedia.org/wiki?curid=12277" title="Gordon Banks">
Gordon Banks

He joined Chesterfield in March 1953, and played for the youth team in the 1956 FA Youth Cup final. He made his first team debut in November 1958, and was sold to Leicester City for £7,000 in July 1959. He played in four cup finals for the club, as they were beaten in the 1961 and 1963 FA Cup finals, before winning the League Cup in 1964 and finishing as finalists in 1965. During this time he established himself as England's number one goalkeeper, and played every game of the nation's 1966 World Cup victory. Despite this success, he was dropped by Leicester and sold on to Stoke City for £50,000 in April 1967. He made one of the game's great saves to prevent a Pelé goal in the 1970 World Cup, but was absent due to illness as England were beaten by West Germany at the quarter-final stage.
He was Stoke's goalkeeper in the 1972 League Cup win – the club's only major honour. He was still Stoke and England's number one when a car crash in October 1972 cost him both the sight in one eye and his professional career. He did though play in the United States for the Fort Lauderdale Strikers in 1977 and 1978. He briefly entered management with Telford United, but left the game after he was sacked in December 1980.
Club career.
Chesterfield.
Banks was born in Sheffield, West Riding of Yorkshire, and brought up in the working class area of Tinsley. The family later moved to the village of Catcliffe after his father set up a (then illegal) betting shop. This brought greater prosperity but also misery, as one day Banks's disabled brother was mugged for the shop's daily takings, and died of his injuries some weeks later. He left school in December 1952, and took up employment as a bagger with a local coal merchant, which helped to build up his upper body strength. He spent a season playing for amateur side Millspaugh F.C. after their regular goalkeeper failed to turn up for a match; the club's trainer spotted Banks amongst the spectators and invited him to play in goal as he was aware that Banks had previously played for Sheffield Schoolboys. His performances there earned him a game in the Yorkshire League for Rawmarsh Welfare, however a 12–2 defeat to Stocksbridge Works on his debut was followed by a 3–1 home defeat, and he was dropped by Rawmarsh and returned to Millspaugh. Still aged 15, he then switched jobs to become a hod carrier.
He was scouted by Chesterfield whilst playing for Millspaugh, and offered a six-game trial in the youth team in March 1953. He impressed enough in these games to be offered a part-time £3 a week contract by manager Teddy Davison in July 1953. The reserve team were placed in the Central League on account of a powerful club director rather than on merit, and Banks conceded 122 goals in the 1954–55 season as the "Spireites" finished in last place with only three victories. Banks was posted to Germany with the Royal Signals on national service, and won the Rhine Cup with his regimental team. He recovered from a fractured elbow to help the Chesterfield youth team to the 1956 final of the FA Youth Cup. There they were beaten 4–3 on aggregate by Manchester United's famous "Busby Babes" – a team that included both Wilf McGuinness and Bobby Charlton.
Banks was given his first team debut by manager Doug Livingstone, at the expense of long-serving Ron Powell, in a Third Division game against Colchester United at Saltergate in November 1958. The game ended 2–2, and Banks kept his place against Norwich City in the following match, and he missed only three games by the end of the 1958–59 season due to injury. With no goalkeeping coaching to speak of, Banks had to learn from his mistakes on the pitch, and he soon developed into a modern vocal goalkeeper who ordered the players in front of him into a more effective defence. Having just 23 league and three cup appearances to his name, it came as a surprise to Banks when Matt Gillies, manager of First Division club Leicester City, bought him from Chesterfield for £7,000 in July 1959; this also meant a wage increase to £15 a week.
Leicester City.
Banks faced competition from five other goalkeepers, including 30-year-old Scotland international John Anderson and 25-year-old Dave MacLaren. He started the 1959–60 season as the reserve team's goalkeeper, in effect making him the club's second choice ahead of four of his rivals but behind first team choice MacLaren. He had played four reserve team games when MacLaren picked up an injury and manager Matt Gillies selected Banks for his Leicester debut against Blackpool at Filbert Street on 9 September. The match finished 1–1, with Jackie Mudie's strike cancelling out Ken Leek's opener. Banks retained his place for the 2–0 defeat against Newcastle United at St James' Park three days later. With McLaren fit again, Banks was sent back to the reserves but, after the first team conceded 14 goals in the next five games, he was recalled and became the first-choice goalkeeper for the remainder of the season. The defensive record did not improve at first, with Banks conceding six in a heavy defeat to Everton at Goodison Park, but he improved in each match and the "Foxes" settled for a comfortable 12th-place finish. In training, he worked extensively on improving his weaknesses, such as coming for crosses. He put in extra hours during training and came up with practice sessions to improve his skills – this was largely unique in an era where there were no specialized goalkeeping coaches. In the summer, both Anderson and MacLaren departed, leaving Banks as the club's undisputed number one ahead of a group of understudies.
Leicester finished sixth in 1960–61, and managed to beat champions Tottenham Hotspur at White Hart Lane. Yet their greatest accomplishment was in reaching the final of the FA Cup, with Banks conceding only five goals in their nine games on route to the final and keeping three clean sheets in the semi-final and two replays against Sheffield United. Goals from Jimmy Walsh and Ken Leek finally broke the deadlock in the second replay at St Andrew's. Their opponents in the final at Wembley were Tottenham, who had already won the First Division title by an eight-point margin. Right-back Len Chalmers picked up a severe injury early in the match, and with Ken Leek dropped for disciplinary reasons in rookie Hughie McIlmoyle's favour, City were effectively playing with ten men and offered little threat going forward. Bobby Smith and Terry Dyson gave "Spurs" a 2–0 win and the first "double" of the 20th century, with Banks unable to prevent either goal.
The 1961–62 season proved to be highly disappointing, as Leicester finished 14th in the league and exited the FA Cup at the hands of Stoke City. The only highlight was the club's participation in the European Cup Winners' Cup, which actually put Banks in the difficult position of choosing to play for his club against Spanish club Atlético Madrid or choosing to attend the England versus Portugal match as a non-playing squad member. He elected to attend both games, leaving London at full-time to reach Leicester 30 minutes before the kick-off against Madrid. A last minute goal earned the Spaniards a 1–1 draw at Filbert Street. In the return leg, Banks saved an Enrique Collar penalty, but Madrid were awarded a second penalty which Collar converted, and Leicester lost the game 2–0 (losing the tie 3–1 on aggregate).
Banks broke his nose at Craven Cottage, on the opening day of the 1962–63 season, in a 2–1 defeat to Fulham. Leicester went to chase a possible double, reaching the FA Cup semi-finals whilst lying top of the table in April. City beat Liverpool 1–0 at Hillsborough to reach the final, with Banks keeping a clean sheet despite his goal being under a near-constant siege from the Merseyside club. The "News of the World" reported that Liverpool had had 34 attempts on goal to Leicester's one, and Banks later stated that it was his finest performance at club level. Yet Banks then broke a finger in a 2–1 defeat to West Bromwich Albion at The Hawthorns, and was out injured as Leicester lost their final three league games to end the season in a disappointing fourth place. In the 1963 FA Cup Final, Banks and the rest of the team underperformed, and lost the game 3–1 to Manchester United.
City ended the 1963–64 season in 11th place, having been inconsistent all season. Success instead came through the League Cup, as they beat West Ham United 6–3 over two legs in the semi-finals to reach the final against Stoke City. The opening tie at the Victoria Ground finished 1–1 in extremely muddy conditions as Banks spilled a shot from Bill Asprey, with Keith Bebbington pouncing on the rebound. Back at Filbert Street, goals from Mike Stringfellow, Dave Gibson and Howard Riley won the game for Leicester 3–2 and settled the tie at 4–3.
Banks started the 1964–65 season on wages of £40 a week, and the club only agreed to pay £60 a week in December. These miserly wages made it difficult for the club to spend the £80,000 it received for the sale of Frank McLintock – he had put in a transfer request over dissatisfaction with his pay and quality replacements were reluctant to join a club that paid full internationals like Banks and McLintock no more than the base rate that rival clubs paid to average players. Leicester finished 18th in the league and were knocked out of the FA Cup by Liverpool at Anfield in the Sixth Round. In the League Cup, City struggled to get past Peterborough United (in a replay), Grimsby Town and Crystal Palace (in a replay), before they recorded an 8–1 victory over Coventry City at Highfield Road. After easing past Plymouth Argyle in the semi-finals, Banks found himself playing in another League Cup final. However Chelsea won the final after successfully defending their 3–2 win at Stamford Bridge with a goalless draw at Filbert Street.
He missed the first nine games of the 1965–66 season after breaking his wrist diving at the feet of Northampton Town's Joe Kiernan in a pre-season friendly. Leicester finished the season in seventh spot, and exited both cup competitions at the hands of Manchester City.
Despite being a World Cup winner in the previous summer, Banks was dropped towards the end of the 1966–67 season for highly promising teenage reserve Peter Shilton. Manager Matt Gillies was blunt, telling Banks "we and the club's directors think your best days are behind you, and you should move on". Teammate Richie Norman told Banks that Gillies was pressured into the decision after Shilton told the board he would leave the club unless he was given first team football. Banks was transfer listed at £50,000, the same price the club received for Derek Dougan in March 1967. However many of the big clubs were unwilling to outlay such an expense on a goalkeeper. Liverpool manager Bill Shankly showed strong interest, but could not convince the club's board of directors to agree to such a large fee for a goalkeeper. West Ham United manager Ron Greenwood was prepared to match the fee, but instead signed Kilmarnock's Bobby Ferguson for £65,000 as he had already agreed terms with Kilmarnock and did not want to go back on his word. Terms were instead agreed with Stoke City, a mid-table First Division side.
Stoke City.
On leaving Filbert Street, Banks requested a loyalty bonus from Leicester, and was told by Matt Gillies "We've decided not to pay you a penny. There's to be no compensation payment and that's final." Banks then refused the move until Stoke boss Tony Waddington seemingly negotiated a £2,000 payment out of Leicester. It was only some years later that Banks was informed that Stoke had actually made the payment, not Leicester. Waddington valued good goalkeepers highly, and the two built up a close relationship. During this time, Banks moved to Madeley, Staffordshire. He replaced John Farmer's as the club's number one, and kept goal in the last four games of the 1966–67 season, making his home debut at the Victoria Ground in a 3–1 win over former club Leicester.
Banks fitted in well at Stoke, as Waddington built a team of veteran players who were judged by some to be past their best. The "Potters" struggled near the foot of the First Division table in the 1967–68 and 1968–69 campaigns, before rising to ninth place in the 1969–70 season. Banks remained a reliable stopper for the club, though on 1 March 1969 he was knocked unconscious at Roker Park by Sunderland's Malcolm Moore, and his replacement David Herd conceded four goals in a 4–1 defeat. Banks also played a season for the Cleveland Stokers of the American United Soccer Association in the summer of 1968; he played seven of the short-lived club's 12 games in Cleveland, Ohio.
Banks made what he believed to be three of the best saves of his career in a Stoke shirt. In the first instance he saved and caught a powerful and well placed header from Manchester City's Wyn Davies from just eight yards out, in the second case he saved a Francis Lee header at Maine Road, and he made his third great save for the club by catching a volley from Tottenham Hotspur's Alan Gilzean that had been hit from just six yards out at White Hart Lane.
Stoke began to compete for honours in the 1970–71 season, though despite impressive victories against the top two clubs – Arsenal and Leeds United – City ended the season in mid-table obscurity. The club's great achievement was in reaching the semi-finals of the FA Cup, beating Millwall, Huddersfield Town, Ipswich Town and Hull City en route. Facing Arsenal at Hillsborough in the semi-finals, they lost a two-goal lead to draw 2–2, and were then beaten 2–0 in the replay at Villa Park.
Despite another mid-table finish in 1971–72, Stoke beat Chesterfield, Tranmere Rovers, Hull City and Manchester United to reach another FA Cup semi-final. They again faced Arsenal, and once more a draw at Hillsborough meant a replay at Goodison Park. The "Gunners" goals in a 2–1 victory came from a disputed Frank McLintock penalty and a John Radford goal that television replays showed was clearly offside. In a May 2011 interview, he said that he still felt "cheated" out of a chance to play for the club in an FA Cup final. Stoke and Banks found solace in the League Cup, though it took them 11 matches to reach the final after overcoming Southport, then Oxford United in a replay, Manchester United in a second replay, Bristol Rovers, and then West Ham United in a second replay following an aggregate draw after two legs. In extra-time of the second leg with West Ham, Banks fouled Harry Redknapp to give up a penalty, and then saved Geoff Hurst's powerful spot-kick to keep City in the competition. They then faced Chelsea in the final at Wembley. Peter Osgood beat Banks with a hooked shot just before half-time, but goals from Terry Conroy and George Eastham won Stoke the game at 2–1. At the end of the season Banks was named as the FWA Footballer of the Year, becoming the first goalkeeper to receive the honour since Bert Trautmann in 1956.
On 22 October 1972, while driving home from a session on his injured shoulder with the Stoke physiotherapist, Banks lost control of his new Ford Consul (a re-badged Ford Granada Mk 1) car which ended up in a ditch. He had attempted to overtake a car on a sharp turn and collided with an oncoming Austin A60 van. He was taken to the North Staffordshire Hospital and during an operation received 200 stitches in his face and over 100 micro-stitches inside the socket of his right eye, and was told the chances of saving the sight in his eye were 50–50. His sight never returned, and as the loss of binocular vision severely limited his abilities as a goalkeeper, he retired from professional football the following summer.
Fort Lauderdale Strikers.
In April 1977 he went to play as a named superstar in the North American Soccer League (NASL) for Fort Lauderdale Strikers. The Strikers won their division in 1977, and Banks was named NASL Goalkeeper of the Year after he conceded only 29 goals in 26 games – the best defensive record in the NASL. He also played one League of Ireland game for St Patrick's Athletic; keeping a clean sheet in a 1–0 win over Shamrock Rovers at Richmond Park on 2 October 1977. He returned to Fort Lauderdale and played 11 games in the 1978 season.
International career.
Banks was capped twice for the England under-23 side in matches against Wales and Scotland in 1961.
Ron Springett was the goalkeeper for England as Banks rose to prominence, but after the 1962 World Cup in Chile, a new coach was appointed in former England right-back Alf Ramsey. Ramsey demanded sole control of the team and began looking towards the next World Cup. Banks won his first cap on 6 April 1963 against Scotland at Wembley, after Springett was dropped following a poor performance. England lost 2–1, though Banks was blameless as Scotland's goals came firstly from an error by Jimmy Armfield and then secondly from the penalty spot. He was picked for the next match against Brazil, which ended in a credible 1–1 draw after Bryan Douglas cancelled out Pepe's opener. Banks continued to play consistently to become established as England's first-choice goalkeeper. In 1963, he was picked to play against the Rest of the World, in a celebration match to mark 100 years of The Football Association.
Banks also played in two of England's three games at the "Little World Cup" in Brazil in the summer of 1964, a 1–1 draw with Portugal and a 1–0 defeat to Argentina. Blackpool's Tony Waiters won five caps in the England goal in 1964, but found that his challenge to Banks' first team place came to an end after he conceded five goals to Brazil. During England's summer of 1965 tour he built up a solid understanding with his defenders – George Cohen, Jack Charlton, Bobby Moore, and Ray Wilson – as he only conceded two goals in four matches against Hungary, Yugoslavia, West Germany, and Sweden. They then played seven friendlies in 1966 in the build-up to the World Cup, though the team passed their biggest test of character in the British Home Championship, beating Scotland 4–3 in front of a crowd of over 130,000 at Hampden Park. Going into the competition, the only defeat in 21 matches since the "Little World Cup" came against Austria, in a game that Banks missed due to injury.
1966 World Cup.
Banks entered the 1966 FIFA World Cup as England's first choice goalkeeper, and his understudies Ron Springett and Peter Bonetti never took to the field during the tournament. England opened the tournament with a goalless draw against Uruguay, with Banks a virtual spectator as the highly defensive Uruguayans rarely ventured out of their own half. They then defeated Mexico 2–0, with Banks again rarely troubled throughout. A 2–0 win over France then took England through the group stage without Banks conceding a goal.
England beat Argentina 1–0 in the last eight, with Geoff Hurst scoring with a header; the match was sullied by the first-half sending off of Argentinian midfielder Antonio Rattín, who refused to leave the pitch after being dismissed for dissent. In contrast to the previous games, the semi-final against Portugal proved to be a fair contest between two sides of talented players eager to attack from the start of the match. Yet there was panic in the buildup to the game as trainer Harold Shepherdson forgot to buy chewing gum, which Banks used to make his hands stickier and better able to handle the ball, and so Shepherdson had to run to a nearby newsagents to purchase gum as the teams were in the tunnel. Bobby Charlton scored two goals, but Portugal made a strong finish and won a penalty on 82 minutes after Jack Charlton handled the ball in the penalty area. Eusébio converted the penalty after sending Banks the wrong way, and the game finished 2–1 in England's favour. This was the first goal Banks had conceded for England in 721 minutes of regular play, since giving up Scotland's last goal after 81 minutes of the Home International clash in April. This remains a record for an England goalkeeper.
England's opponents in the final were West Germany. It was England who dominated the final but it was Banks who was beaten first. A weak header from Ray Wilson handed a chance to Helmut Haller, who sent an accurate but relatively weak shot into the corner of the net; Banks had been unsighted by Jack Charlton, and he failed to adjust his position in time to reach the ball. England equalised through a Geoff Hurst header within six minutes and went ahead late in the second half through Martin Peters. With seconds left in the game, Lothar Emmerich sent a free kick into the England penalty area, and the ball fell to Wolfgang Weber, who guided the ball over a lunging Ray Wilson and an outstretched Banks into the net to take the game into extra-time. In extra-time, the Germans sent shots in at the England goal which Banks managed to catch and control without any great danger. Hurst scored two goals to complete his hat-trick, and though many claimed his second goal never crossed the line Banks always maintained his belief that the officials called the decision correctly. Between these goals Banks had to deal with a fiery shot from Sigfried Held, and was later exposed only for Uwe Seeler to come just centimetres away from connecting with the ball.
Euro 1968.
Scotland were the first team to beat the world champions, as goals from Denis Law, Bobby Lennox and Jim McCalliog secured a 3–2 victory at Wembley on 15 April 1967. Despite this set-back, England qualified for UEFA Euro 1968, which consisted of just four teams: England, Italy (hosts), the Soviet Union, and Yugoslavia. England played just two games at the tournament, losing 1–0 to Yugoslavia, and then beating the Soviets 2–0 in the third-place play-off.
1970 World Cup.
Banks went into the 1970 World Cup as England's number one with 59 caps to his name, and had Peter Bonetti (one cap) and Alex Stepney (six caps) as his understudies. However he soon found that the heat and altitude at Guadalajara, Mexico difficult to cope with. The team's efforts at acclimatization were not helped when Bobby Moore was falsely accused of stealing the infamous "Bogotá Bracelet". Despite this, a Geoff Hurst goal was enough to beat their first opponents, Romania. A far tougher test awaited on 7 June, when England faced Brazil. The day before the match Banks was informed that he had been awarded an OBE.
Playing at pace, Brazil were putting England under enormous pressure and an attack was begun by captain Carlos Alberto who sent a low ball down the right flank for the speedy Jairzinho to latch on to. The Brazilian winger sped past left-back Terry Cooper and crossed the ball into the six-yard box, where Pelé connected with a powerful header to send the ball low towards the right-hand corner of the goal. In the knowledge that his header was placed to perfection, Pelé immediately shouted "Gol!" (Brazilian Portuguese for goal).
The split-second incident only allowed Banks time for one conscious thought – that the shot was impossible to catch, and the only way to prevent Pelé from following up on the rebound would be to parry the ball over the bar. The ball bounced two yards in front of the goal-line, and Banks managed to make contact with the ball with the fingers of his right hand, and rolled his hand slightly to angle to ball over the crossbar. He landed in the inner netting of the goal, and knew he had saved the ball after witnessing Pelé's reaction. Banks then rose to his feet to defend the corner, and broke into laughter after the following exchange:
Pelé, and numerous journalists and pundits, would later describe the save as the greatest in the history of the game. Banks would later say "They won't remember me for winning the World Cup, it'll be for that save. That's how big a thing it is. People just want to talk about that save." In 2002 the UK public voted the save No. 41 in the list of the 100 Greatest Sporting Moments.
Brazil still won the game 1–0 – Jairzinho guided a shot past Banks in the second half. England ultimately joined Brazil in the last eight after a win in the final group game against Czechoslovakia. The reward was a rematch of the 1966 final against West Germany.
The day before the game Banks and England's hopes of making further inroads into the World Cup were dented when he started to complain of an upset stomach. He became affected by violent stomach cramps and aching limbs, and spent his time in the bathroom sweating, shivering and vomiting. He passed an extremely undemanding fitness test but suffered a relapse shortly before the game and Ramsey was forced to rest him in place of Peter Bonetti. Ramsey remarked that "of all the players to lose, we had to lose him." Banks watched the game on television at the hotel as England lost a two-goal lead to be eliminated 3–2 after extra time; due to a time delay on the broadcast he switched the television off with England 2–0 in the lead as Bobby Moore returned to the hotel to break the news of the defeat. Conspiracies later surfaced that Banks had been poisoned to take him out of the game, but with no evidence to support them Banks never believed in these conspiracies.
Final years.
Only four teams competed in UEFA Euro 1972: Belgium (hosts), Hungary, the Soviet Union, and West Germany. England came close to qualifying, but lost 3–1 to West Germany in the final round of qualifying.
On 15 May 1971, Banks was involved in a notorious incident with George Best who, while playing against England for Northern Ireland, flicked the ball out of Banks' hands and headed it into the net. The move was audacious, but was disallowed by the referee, who judged it to be dangerous play. Banks played his 73rd and final game for England on 27 May 1972, in a 1–0 win over Scotland at Hampden Park. During his 73 international games he kept 35 clean sheets and lost just nine games.
Coaching career.
In December 1977 he was appointed as a coach at Port Vale by manager Dennis Butler, being demoted to reserve coach in October 1978 by new boss Alan Bloor. Banks enjoyed coaching but soon resigned his position, feeling that players such as Bernie Wright refused to take his advice on board. He applied for the vacant management positions at Lincoln City and Rotherham United, but was rejected. He instead accepted the role as manager of Alliance Premier League part-time club Telford United. He signed a goalkeeper, centre-half and centre-forward from Bangor City for £1,500, as well as former Stoke striker John Ruggiero. The "Bucks" finished in 13th place in 1979–80. In November 1980, he left Jackie Mudie in temporary charge of team affairs whilst he underwent surgery, who led the club to defeat in the FA Trophy at the hands of a lower league club. On his return to the club Banks was sacked. He was offered the position of raffle-ticket seller, and accepted the post in the belief that it would entitle him to the money owed to him in the terms of his management contract; he eventually had to settle for 50% of his contract. He later stated that "It broke my heart ... I did not want to stay in the game."
Personal life.
Banks first met his wife Ursula during his National service in Germany in 1955. They had three children: Robert (born July 1958), Wendy (1963), and Julie (1969). He separated from Ursula during his time in America, but the couple reunited when Banks returned to England. Banks' nephew is Nick Banks, drummer of the band Pulp.
Shortly after his retirement, Banks was surprised by Eamonn Andrews for an episode of "This Is Your Life". He later fronted a Leicester based hospitality company. He lost a significant sum of money when the business failed, but was helped out by Leicester City, who offered him a belated testimonial match. He was appointed as Stoke City's president following the death of Stanley Matthews. Since the 1980s he has been a member of the three man pool's panel.
In 2001, he sold his World Cup winners medal at Christie's for £124,750, and his international cap from the final was also sold at £27,025.
Banks was an Inaugural Inductee to the English Football Hall of Fame in 2002. In March 2004, he was named by Pelé as one of the world's 125 greatest living footballers. He was awarded an honorary doctorate from Keele University in February 2006. In May 2006, Banks was the first "legend" to be inducted into a new Walk of Fame, by having a plaque installed in the pavement in front of Sheffield Town Hall. In July 2008, Pelé unveiled a statue of Banks making his famous 1970 World Cup save outside the Britannia Stadium. In March 2011, he was also inducted into the City of Stoke-on-Trent Hall of Fame, along with Roy Sproson.
In 1980 Banks published his first autobiography, "Banks of England". He published a more comprehensive autobiography in 2002: "Banksy: My Autobiography". Irish investigative author, Don Mullan, published a boyhood memoir in 2006 called "GORDON BANKS: A Hero Who Could Fly" in which he wrote about the influence of the England goalkeeper on his life.
In December 2015, it was announced he was receiving treatment for kidney cancer.
Career statistics.
Sources: 
Source:

</doc>
<doc id="12278" url="https://en.wikipedia.org/wiki?curid=12278" title="Ganglion">
Ganglion

In anatomy, a ganglion ( ; plural ganglia) is a nerve cell cluster or a group of nerve cell bodies located in the autonomic nervous system. Ganglia house the cell bodies of afferent nerves.
Neurology.
In a neurological context, ganglia are composed mainly of somata and dendritic structures which are bundled or connected. Ganglia often interconnect with other ganglia to form a complex system of ganglia known as a plexus. Ganglia provide relay points and intermediary connections between different neurological structures in the body, such as the peripheral and central nervous systems.
Among vertebrates there are three major groups of ganglia:
In the autonomic nervous system, fibers from the central nervous system to the ganglia are known as preganglionic fibers, while those from the ganglia to the effector organ are called postganglionic fibers.
Basal ganglia.
The term "ganglion" refers to the peripheral nervous system.
However, in the brain (part of the central nervous system), the "basal ganglia", or "basal nuclei," is a group of nuclei interconnected with the cerebral cortex, thalamus and brainstem, associated with a variety of functions: motor control, cognition, emotions, and learning.
Partly due to this ambiguity, the "Terminologia Anatomica" recommends using the term "basal nuclei" instead of "basal ganglia"; however, this usage has not been generally adopted.
Pseudoganglion.
A pseudoganglion is a localized thickening of the main part or "trunk" of a nerve that has the appearance of a ganglion but has only nerve fibres and no nerve cell bodies.
Pseudoganglia are found in the teres minor muscle and radial nerve.

</doc>
<doc id="12281" url="https://en.wikipedia.org/wiki?curid=12281" title="Gottfried Wilhelm Leibniz">
Gottfried Wilhelm Leibniz

Gottfried Wilhelm (von) Leibniz (; or ; ; – November 14, 1716) was a German polymath and philosopher who occupies a prominent place in the history of mathematics and the history of philosophy. Scholars including Bertrand Russell believe Leibniz developed calculus independently of Isaac Newton, and Leibniz's notation has been widely used ever since it was published. It was only in the 20th century that his Law of Continuity and Transcendental Law of Homogeneity found mathematical implementation (by means of non-standard analysis). He became one of the most prolific inventors in the field of mechanical calculators. While working on adding automatic multiplication and division to Pascal's calculator, he was the first to describe a pinwheel calculator in 1685 and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, which is the foundation of virtually all digital computers.
In philosophy, Leibniz is most noted for his optimism, i.e. his conclusion that our Universe is, in a restricted sense, the best possible one that God could have created, an idea that was often lampooned by others such as Voltaire. Leibniz, along with René Descartes and Baruch Spinoza, was one of the three great 17th-century advocates of rationalism. The work of Leibniz anticipated modern logic and analytic philosophy, but his philosophy also looks back to the scholastic tradition, in which conclusions are produced by applying reason of first principles or prior definitions rather than to empirical evidence.
Leibniz made major contributions to physics and technology, and anticipated notions that surfaced much later in philosophy, probability theory, biology, medicine, geology, psychology, linguistics, and computer science. He wrote works on philosophy, politics, law, ethics, theology, history, and philology. Leibniz's contributions to this vast array of subjects were scattered in various learned journals, in tens of thousands of letters, and in unpublished manuscripts. He wrote in several languages, but primarily in Latin, French, and German. There is no complete gathering of the writings of Leibniz.
Biography.
Early life.
Gottfried Leibniz was born on July 1, 1646, toward the end of the Thirty Years' War, in Leipzig, Saxony, to Friedrich Leibniz and Catharina Schmuck. Friedrich noted in his family journal:
In English:
Leibniz was baptized on July 3 of that year at St. Nicholas Church, Leipzig; his godfather was the Lutheran theologian . His father died when he was six and a half years old, and from that point on he was raised by his mother. Her teachings influenced Leibniz's philosophical thoughts in his later life.
Leibniz's father had been a Professor of Moral Philosophy at the University of Leipzig, and the boy later inherited his father's personal library. He was given free access to it from the age of seven. While Leibniz's schoolwork was largely confined to the study of a small canon of authorities, his father's library enabled him to study a wide variety of advanced philosophical and theological works—ones that he would not have otherwise been able to read until his college years. Access to his father's library, largely written in Latin, also led to his proficiency in the Latin language, which he achieved by the age of 12. He also composed 300 hexameters of Latin verse, in a single morning, for a special event at school at the age of 13.
In April 1661 he enrolled in his father's former university at age 15, and completed his bachelor's degree in Philosophy in December 1662. He defended his "Disputatio Metaphysica de Principio Individui" ("Metaphysical Disputation on the Principle of Individuation"), which addressed the principle of individuation, on June 9, 1663. Leibniz earned his master's degree in Philosophy on February 7, 1664. He published and defended a dissertation "Specimen Quaestionum Philosophicarum ex Jure collectarum" ("An Essay of Collected Philosophical Problems of Right"), arguing for both a theoretical and a pedagogical relationship between philosophy and law, in December 1664. After one year of legal studies, he was awarded his bachelor's degree in Law on September 28, 1665. His dissertation was titled "De conditionibus" ("On Conditions").
In early 1666, at age 19, Leibniz wrote his first book, "De Arte Combinatoria" ("On the Combinatorial Art"), the first part of which was also his habilitation thesis in Philosophy, which he defended in March 1666. His next goal was to earn his license and Doctorate in Law, which normally required three years of study. In 1666, the University of Leipzig turned down Leibniz's doctoral application and refused to grant him a Doctorate in Law, most likely due to his relative youth. Leibniz subsequently left Leipzig.
Leibniz then enrolled in the University of Altdorf and quickly submitted a thesis, which he had probably been working on earlier in Leipzig. The title of his thesis was "Disputatio Inauguralis de Casibus Perplexis in Jure" ("Inaugural Disputation on Ambiguous Legal Cases"). Leibniz earned his license to practice law and his Doctorate in Law in November 1666. He next declined the offer of an academic appointment at Altdorf, saying that "my thoughts were turned in an entirely different direction".
As an adult, Leibniz often introduced himself as "Gottfried von Leibniz". Many posthumously published editions of his writings presented his name on the title page as "Freiherr G. W. von Leibniz." However, no document has ever been found from any contemporary government that stated his appointment to any form of nobility.
1666–1674.
's first position was as a salaried secretary to an alchemical society in Nuremberg. He knew fairly little about the subject at that time but presented himself as deeply learned. He soon met Johann Christian von Boyneburg (1622–1672), the dismissed chief minister of the Elector of Mainz, Johann Philipp von Schönborn. Von Boyneburg hired Leibniz as an assistant, and shortly thereafter reconciled with the Elector and introduced Leibniz to him. Leibniz then dedicated an essay on law to the Elector in the hope of obtaining employment. The stratagem worked; the Elector asked Leibniz to assist with the redrafting of the legal code for the Electorate. In 1669, Leibniz was appointed assessor in the Court of Appeal. Although von Boyneburg died late in 1672, Leibniz remained under the employment of his widow until she dismissed him in 1674.
Von Boyneburg did much to promote Leibniz's reputation, and the latter's memoranda and letters began to attract favorable notice. After Leibniz's service to the Elector there soon followed a diplomatic role. He published an essay, under the pseudonym of a fictitious Polish nobleman, arguing (unsuccessfully) for the German candidate for the Polish crown. The main force in European geopolitics during Leibniz's adult life was the ambition of Louis XIV of France, backed by French military and economic might. Meanwhile, the Thirty Years' War had left German-speaking Europe exhausted, fragmented, and economically backward. Leibniz proposed to protect German-speaking Europe by distracting Louis as follows. France would be invited to take Egypt as a stepping stone towards an eventual conquest of the Dutch East Indies. In return, France would agree to leave Germany and the Netherlands undisturbed. This plan obtained the Elector's cautious support. In 1672, the French government invited Leibniz to Paris for discussion, but the plan was soon overtaken by the outbreak of the Franco-Dutch War and became irrelevant. Napoleon's failed invasion of Egypt in 1798 can be seen as an unwitting, late implementation of Leibniz's plan, after the Eastern hemisphere colonial supremacy in Europe had already passed from the Dutch to the British.
Thus Leibniz began several years in Paris. Soon after arriving, he met Dutch physicist and mathematician Christiaan Huygens and realised that his own knowledge of mathematics and physics was patchy. With Huygens as his mentor, he began a program of self-study that soon pushed him to making major contributions to both subjects, including discovering his version of the differential and integral calculus. He met Nicolas Malebranche and Antoine Arnauld, the leading French philosophers of the day, and studied the writings of Descartes and Pascal, unpublished as well as published. He befriended a German mathematician, Ehrenfried Walther von Tschirnhaus; they corresponded for the rest of their lives. In 1675 he would be admitted by the French Academy of Sciences as a foreign honorary member, despite his lack of attention to the academy.
When it became clear that France would not implement its part of Leibniz's Egyptian plan, the Elector sent his nephew, escorted by Leibniz, on a related mission to the English government in London, early in 1673. There Leibniz came into acquaintance of Henry Oldenburg and John Collins. He met with the Royal Society where he demonstrated a calculating machine that he had designed and had been building since 1670. The machine was able to execute all four basic operations (adding, subtracting, multiplying, and dividing), and the society quickly made him an external member. 
The mission ended abruptly when news of the Elector's death (12 February 1673) reached them. Leibniz promptly returned to Paris and not, as had been planned, to Mainz. The sudden deaths of his two patrons in the same winter meant that Leibniz had to find a new basis for his career. 
In this regard, a 1669 invitation from the John Frederick of Brunswick to visit Hanover proved to have been fateful. Leibniz had declined the invitation, but had begun corresponding with the duke in 1671. In 1673, the duke offered Leibniz the post of counsellor. Leibniz very reluctantly accepted the position two years later, only after it became clear that no employment in Paris, whose intellectual stimulation he relished, or with the Habsburg imperial court, was forthcoming.
House of Hanover, 1676–1716.
Leibniz managed to delay his arrival in Hanover until the end of 1676 after making one more short journey to London, where Newton accused him of having seen Newton's unpublished work on calculus in advance. This was alleged to be evidence supporting the accusation, made decades later, that he had stolen calculus from Newton. On the journey from London to Hanover, Leibniz stopped in The Hague where he met van Leeuwenhoek, the discoverer of microorganisms. He also spent several days in intense discussion with Spinoza, who had just completed his masterwork, the "Ethics".
In 1677, he was promoted, at his request, to Privy Counselor of Justice, a post he held for the rest of his life. Leibniz served three consecutive rulers of the House of Brunswick as historian, political adviser, and most consequentially, as librarian of the ducal library. He thenceforth employed his pen on all the various political, historical, and theological matters involving the House of Brunswick; the resulting documents form a valuable part of the historical record for the period.
Among the few people in north Germany to accept Leibniz were the Electress Sophia of Hanover (1630–1714), her daughter Sophia Charlotte of Hanover (1668–1705), the Queen of Prussia and his avowed disciple, and Caroline of Ansbach, the consort of her grandson, the future George II. To each of these women he was correspondent, adviser, and friend. In turn, they all approved of Leibniz more than did their spouses and the future king George I of Great Britain.
The population of Hanover was only about 10,000, and its provinciality eventually grated on Leibniz. Nevertheless, to be a major courtier to the House of Brunswick was quite an honor, especially in light of the meteoric rise in the prestige of that House during Leibniz's association with it. In 1692, the Duke of Brunswick became a hereditary Elector of the Holy Roman Empire. The British Act of Settlement 1701 designated the Electress Sophia and her descent as the royal family of England, once both King William III and his sister-in-law and successor, Queen Anne, were dead. Leibniz played a role in the initiatives and negotiations leading up to that Act, but not always an effective one. For example, something he published anonymously in England, thinking to promote the Brunswick cause, was formally censured by the British Parliament.
The Brunswicks tolerated the enormous effort Leibniz devoted to intellectual pursuits unrelated to his duties as a courtier, pursuits such as perfecting calculus, writing about other mathematics, logic, physics, and philosophy, and keeping up a vast correspondence. He began working on calculus in 1674; the earliest evidence of its use in his surviving notebooks is 1675. By 1677 he had a coherent system in hand, but did not publish it until 1684. Leibniz's most important mathematical papers were published between 1682 and 1692, usually in a journal which he and Otto Mencke founded in 1682, the "Acta Eruditorum". That journal played a key role in advancing his mathematical and scientific reputation, which in turn enhanced his eminence in diplomacy, history, theology, and philosophy.
The Elector Ernest Augustus commissioned Leibniz to write a history of the House of Brunswick, going back to the time of Charlemagne or earlier, hoping that the resulting book would advance his dynastic ambitions. From 1687 to 1690, Leibniz traveled extensively in Germany, Austria, and Italy, seeking and finding archival materials bearing on this project. Decades went by but no history appeared; the next Elector became quite annoyed at Leibniz's apparent dilatoriness. Leibniz never finished the project, in part because of his huge output on many other fronts, but also because he insisted on writing a meticulously researched and erudite book based on archival sources, when his patrons would have been quite happy with a short popular book, one perhaps little more than a genealogy with commentary, to be completed in three years or less. They never knew that he had in fact carried out a fair part of his assigned task: when the material Leibniz had written and collected for his history of the House of Brunswick was finally published in the 19th century, it filled three volumes.
In 1708, John Keill, writing in the journal of the Royal Society and with Newton's presumed blessing, accused Leibniz of having plagiarised Newton's calculus. Thus began the calculus priority dispute which darkened the remainder of Leibniz's life. A formal investigation by the Royal Society (in which Newton was an unacknowledged participant), undertaken in response to Leibniz's demand for a retraction, upheld Keill's charge. Historians of mathematics writing since 1900 or so have tended to acquit Leibniz, pointing to important differences between Leibniz's and Newton's versions of calculus.
In 1711, while traveling in northern Europe, the Russian Tsar Peter the Great stopped in Hanover and met Leibniz, who then took some interest in Russian matters for the rest of his life. In 1712, Leibniz began a two-year residence in Vienna, where he was appointed Imperial Court Councillor to the Habsburgs. On the death of Queen Anne in 1714, Elector George Louis became King George I of Great Britain, under the terms of the 1701 Act of Settlement. Even though Leibniz had done much to bring about this happy event, it was not to be his hour of glory. Despite the intercession of the Princess of Wales, Caroline of Ansbach, George I forbade Leibniz to join him in London until he completed at least one volume of the history of the Brunswick family his father had commissioned nearly 30 years earlier. Moreover, for George I to include Leibniz in his London court would have been deemed insulting to Newton, who was seen as having won the calculus priority dispute and whose standing in British official circles could not have been higher. Finally, his dear friend and defender, the Dowager Electress Sophia, died in 1714.
Death.
Leibniz died in Hanover in 1716: at the time, he was so out of favor that neither George I (who happened to be near Hanover at that time) nor any fellow courtier other than his personal secretary attended the funeral. Even though Leibniz was a life member of the Royal Society and the Berlin Academy of Sciences, neither organization saw fit to honor his passing. His grave went unmarked for more than 50 years. Leibniz was eulogized by Fontenelle, before the Academie des Sciences in Paris, which had admitted him as a foreign member in 1700. The eulogy was composed at the behest of the Duchess of Orleans, a niece of the Electress Sophia.
Personal life.
Leibniz never married. He complained on occasion about money, but the fair sum he left to his sole heir, his sister's stepson, proved that the Brunswicks had, by and large, paid him well. In his diplomatic endeavors, he at times verged on the unscrupulous, as was all too often the case with professional diplomats of his day. On several occasions, Leibniz backdated and altered personal manuscripts, actions which put him in a bad light during the calculus controversy. On the other hand, he was charming, well-mannered, and not without humor and imagination. He had many friends and admirers all over Europe. On Leibniz's religious views, although he is considered by some biographers as a deist, he has also been claimed as a philosophical theist.
Philosopher.
Leibniz's philosophical thinking appears fragmented, because his philosophical writings consist mainly of a multitude of short pieces: journal articles, manuscripts published long after his death, and many letters to many correspondents. He wrote only two book-length philosophical treatises, of which only the "Théodicée" of 1710 was published in his lifetime.
Leibniz dated his beginning as a philosopher to his "Discourse on Metaphysics", which he composed in 1686 as a commentary on a running dispute between Nicolas Malebranche and Antoine Arnauld. This led to an extensive and valuable correspondence with Arnauld; it and the "Discourse" were not published until the 19th century. In 1695, Leibniz made his public entrée into European philosophy with a journal article titled "New System of the Nature and Communication of Substances". Between 1695 and 1705, he composed his "New Essays on Human Understanding", a lengthy commentary on John Locke's 1690 "An Essay Concerning Human Understanding", but upon learning of Locke's 1704 death, lost the desire to publish it, so that the "New Essays" were not published until 1765. The "Monadologie", composed in 1714 and published posthumously, consists of 90 aphorisms.
Leibniz met Spinoza in 1676, read some of his unpublished writings, and has since been suspected of appropriating some of Spinoza's ideas. While Leibniz admired Spinoza's powerful intellect, he was also forthrightly dismayed by Spinoza's conclusions, especially when these were inconsistent with Christian orthodoxy.
Unlike Descartes and Spinoza, Leibniz had a thorough university education in philosophy. He was influenced by his Leipzig professor Jakob Thomasius, who also supervised his BA thesis in philosophy. Leibniz also eagerly read Francisco Suárez, a Spanish Jesuit respected even in Lutheran universities. Leibniz was deeply interested in the new methods and conclusions of Descartes, Huygens, Newton, and Boyle, but viewed their work through a lens heavily tinted by scholastic notions. Yet it remains the case that Leibniz's methods and concerns often anticipate the logic, and analytic and linguistic philosophy of the 20th century.
The Principles.
Leibniz variously invoked one or another of seven fundamental philosophical Principles:
Leibniz would on occasion give a rational defense of a specific principle, but more often took them for granted.
The monads.
Leibniz's best known contribution to metaphysics is his theory of monads, as exposited in "Monadologie". According to Leibniz, monads are elementary particles with blurred perceptions of one another. Monads can also be compared to the corpuscles of the Mechanical Philosophy of René Descartes and others. Monads are the ultimate elements of the universe. The monads are "substantial forms of being" with the following properties: they are eternal, indecomposable, individual, subject to their own laws, un-interacting, and each reflecting the entire universe in a pre-established harmony (a historically important example of panpsychism). Monads are centers of force; substance is force, while space, matter, and motion are merely phenomenal.
The ontological essence of a monad is its irreducible simplicity. Unlike atoms, monads possess no material or spatial character. They also differ from atoms by their complete mutual independence, so that interactions among monads are only apparent. Instead, by virtue of the principle of pre-established harmony, each monad follows a preprogrammed set of "instructions" peculiar to itself, so that a monad "knows" what to do at each moment. By virtue of these intrinsic instructions, each monad is like a little mirror of the universe. Monads need not be "small"; e.g., each human being constitutes a monad, in which case free will is problematic.
Monads are purported to have gotten rid of the problematic:
Theodicy and optimism.
The word "optimism" is used in the classic sense of optimal, not optimistic.
The "Theodicy" tries to justify the apparent imperfections of the world by claiming that it is optimal among all possible worlds. It must be the best possible and most balanced world, because it was created by an all powerful and all knowing God, who would not choose to create an imperfect world if a better world could be known to him or possible to exist. In effect, apparent flaws that can be identified in this world must exist in every possible world, because otherwise God would have chosen to create the world that excluded those flaws.
Leibniz asserted that the truths of theology (religion) and philosophy cannot contradict each other, since reason and faith are both "gifts of God" so that their conflict would imply God contending against himself. The "Theodicy" is Leibniz's attempt to reconcile his personal philosophical system with his interpretation of the tenets of Christianity. This project was motivated in part by Leibniz's belief, shared by many conservative philosophers and theologians during the Enlightenment, in the rational and enlightened nature of the Christian religion as compared to its purportedly less advanced non-Western counterparts. It was also shaped by Leibniz's belief in the perfectibility of human nature (if humanity relied on correct philosophy and religion as a guide), and by his belief that metaphysical necessity must have a rational or logical foundation, even if this metaphysical causality seemed inexplicable in terms of physical necessity (the natural laws identified by science).
Because reason and faith must be entirely reconciled, any tenet of faith which could not be defended by reason must be rejected. Leibniz then approached one of the central criticisms of Christian theism: if God is all good, all wise and all powerful, how did evil come into the world? The answer (according to Leibniz) is that, while God is indeed unlimited in wisdom and power, his human creations, as creations, are limited both in their wisdom and in their will (power to act). This predisposes humans to false beliefs, wrong decisions and ineffective actions in the exercise of their free will. God does not arbitrarily inflict pain and suffering on humans; rather he permits both "moral evil" (sin) and "physical evil" (pain and suffering) as the necessary consequences of "metaphysical evil" (imperfection), as a means by which humans can identify and correct their erroneous decisions, and as a contrast to true good.
Further, although human actions flow from prior causes that ultimately arise in God, and therefore are known as a metaphysical certainty to God, an individual's free will is exercised within natural laws, where choices are merely contingently necessary, to be decided in the event by a "wonderful spontaneity" that provides individuals an escape from rigorous predestination.
Symbolic thought.
Leibniz believed that much of human reasoning could be reduced to calculations of a sort, and that such calculations could resolve many differences of opinion:
The only way to rectify our reasonings is to make them as tangible as those of the Mathematicians, so that we can find our error at a glance, and when there are disputes among persons, we can simply say: Let us calculate ["calculemus"], without further ado, to see who is right.
Leibniz's calculus ratiocinator, which resembles symbolic logic, can be viewed as a way of making such calculations feasible. Leibniz wrote memoranda that can now be read as groping attempts to get symbolic logic—and thus his "calculus"—off the ground. But Gerhard and Couturat did not publish these writings until modern formal logic had emerged in Frege's "Begriffsschrift" and in writings by Charles Sanders Peirce and his students in the 1880s, and hence well after Boole and De Morgan began that logic in 1847.
Leibniz thought symbols were important for human understanding. He attached so much importance to the development of good notations that he attributed all his discoveries in mathematics to this. His notation for calculus is an example of his skill in this regard. C.S. Peirce, a 19th-century pioneer of semiotics, shared Leibniz's passion for symbols and notation, and his belief that these are essential to a well-running logic and mathematics.
But Leibniz took his speculations much further. Defining a character as any written sign, he then defined a "real" character as one that represents an idea directly and not simply as the word embodying the idea. Some real characters, such as the notation of logic, serve only to facilitate reasoning. Many characters well known in his day, including Egyptian hieroglyphics, Chinese characters, and the symbols of astronomy and chemistry, he deemed not real. Instead, he proposed the creation of a "characteristica universalis" or "universal characteristic", built on an alphabet of human thought in which each fundamental concept would be represented by a unique "real" character:
It is obvious that if we could find characters or signs suited for expressing all our thoughts as clearly and as exactly as arithmetic expresses numbers or geometry expresses lines, we could do in all matters "insofar as they are subject to reasoning" all that we can do in arithmetic and geometry. For all investigations which depend on reasoning would be carried out by transposing these characters and by a species of calculus.
Complex thoughts would be represented by combining characters for simpler thoughts. Leibniz saw that the uniqueness of prime factorization suggests a central role for prime numbers in the universal characteristic, a striking anticipation of Gödel numbering. Granted, there is no intuitive or mnemonic way to number any set of elementary concepts using the prime numbers. Leibniz's idea of reasoning through a universal language of symbols and calculations however remarkably foreshadows great 20th century developments in formal systems, such as Turing completeness, where computation was used to define equivalent universal languages (see Turing degree).
Because Leibniz was a mathematical novice when he first wrote about the "characteristic", at first he did not conceive it as an algebra but rather as a universal language or script. Only in 1676 did he conceive of a kind of "algebra of thought", modeled on and including conventional algebra and its notation. The resulting "characteristic" included a logical calculus, some combinatorics, algebra, his "analysis situs" (geometry of situation), a universal concept language, and more.
What Leibniz actually intended by his "characteristica universalis" and calculus ratiocinator, and the extent to which modern formal logic does justice to calculus, may never be established.
Formal logic.
Leibniz is one of the most important logicians between Aristotle and 1847, when George Boole and Augustus De Morgan each published books that began modern formal logic. Leibniz enunciated the principal properties of what we now call conjunction, disjunction, negation, identity, set inclusion, and the empty set. The principles of Leibniz's logic and, arguably, of his whole philosophy, reduce to two:
The formal logic that emerged early in the 20th century also requires, at minimum, unary negation and quantified variables ranging over some universe of discourse.
Leibniz published nothing on formal logic in his lifetime; most of what he wrote on the subject consists of working drafts. In his book "History of Western Philosophy", Bertrand Russell went so far as to claim that Leibniz had developed logic in his unpublished writings to a level which was reached only 200 years later.
Mathematician.
Although the mathematical notion of function was implicit in trigonometric and logarithmic tables, which existed in his day, Leibniz was the first, in 1692 and 1694, to employ it explicitly, to denote any of several geometric concepts derived from a curve, such as abscissa, ordinate, tangent, chord, and the perpendicular. In the 18th century, "function" lost these geometrical associations.
Leibniz was the first to see that the coefficients of a system of linear equations could be arranged into an array, now called a matrix, which can be manipulated to find the solution of the system, if any. This method was later called Gaussian elimination. Leibniz's discoveries of Boolean algebra and of symbolic logic, also relevant to mathematics, are discussed in the preceding section. The best overview of Leibniz's writings on calculus may be found in Bos (1974).
Calculus.
Leibniz is credited, along with Sir Isaac Newton, with the discovery of calculus (differential and integral calculus). According to Leibniz's notebooks, a critical breakthrough occurred on November 11, 1675, when he employed integral calculus for the first time to find the area under the graph of a function "y" = "ƒ"("x"). He introduced several notations used to this day, for instance the integral sign ∫ representing an elongated S, from the Latin word "summa" and the "d" used for differentials, from the Latin word "differentia". This cleverly suggestive notation for calculus is probably his most enduring mathematical legacy. Leibniz did not publish anything about his calculus until 1684. The product rule of differential calculus is still called "Leibniz's law". In addition, the theorem that tells how and when to differentiate under the integral sign is called the Leibniz integral rule.
Leibniz exploited infinitesimals in developing calculus, manipulating them in ways suggesting that they had paradoxical algebraic properties. George Berkeley, in a tract called "The Analyst" and also in "De Motu", criticized these. A recent study argues that Leibnizian calculus was free of contradictions, and was better grounded than Berkeley's empiricist criticisms.
From 1711 until his death, Leibniz was engaged in a dispute with John Keill, Newton and others, over whether Leibniz had invented calculus independently of Newton. This subject is treated at length in the article Leibniz-Newton controversy.
The use of infinitesimals in mathematics was frowned upon by followers of Karl Weierstrass, but survived in science and engineering, and even in rigorous mathematics, via the fundamental computational device known as the differential. Beginning in 1960, Abraham Robinson worked out a rigorous foundation for Leibniz's infinitesimals, using model theory, in the context of a field of hyperreal numbers. The resulting non-standard analysis can be seen as a belated vindication of Leibniz's mathematical reasoning. Robinson's transfer principle is a mathematical implementation of Leibniz's heuristic law of continuity, while the standard part function implements the Leibnizian transcendental law of homogeneity.
Topology.
Leibniz was the first to use the term "analysis situs", later used in the 19th century to refer to what is now known as topology. There are two takes on this situation. On the one hand, Mates, citing a 1954 paper in German by Jacob Freudenthal, argues:
Although for Leibniz the situs of a sequence of points is completely determined by the distance between them and is altered if those distances are altered, his admirer Euler, in the famous 1736 paper solving the Königsberg Bridge Problem and its generalizations, used the term "geometria situs" in such a sense that the situs remains unchanged under topological deformations. He mistakenly credits Leibniz with originating this concept. ...it is sometimes not realized that Leibniz used the term in an entirely different sense and hence can hardly be considered the founder of that part of mathematics.
But Hideaki Hirano argues differently, quoting Mandelbrot:
To sample Leibniz' scientific works is a sobering experience. Next to calculus, and to other thoughts that have been carried out to completion, the number and variety of premonitory thrusts is overwhelming. We saw examples in 'packing,'... My Leibniz mania is further reinforced by finding that for one moment its hero attached importance to geometric scaling. In "Euclidis Prota"..., which is an attempt to tighten Euclid's axioms, he states...: 'I have diverse definitions for the straight line. The straight line is a curve, any part of which is similar to the whole, and it alone has this property, not only among curves but among sets.' This claim can be proved today.
Thus the fractal geometry promoted by Mandelbrot drew on Leibniz's notions of self-similarity and the principle of continuity: "natura non saltum facit". We also see that when Leibniz wrote, in a metaphysical vein, that "the straight line is a curve, any part of which is similar to the whole", he was anticipating topology by more than two centuries. As for "packing", Leibniz told to his friend and correspondent Des Bosses to imagine a circle, then to inscribe within it three congruent circles with maximum radius; the latter smaller circles could be filled with three even smaller circles by the same procedure. This process can be continued infinitely, from which arises a good idea of self-similarity. Leibniz's improvement of Euclid's axiom contains the same concept.
Scientist and engineer.
Leibniz's writings are currently discussed, not only for their anticipations and possible discoveries not yet recognized, but as ways of advancing present knowledge. Much of his writing on physics is included in Gerhardt's "Mathematical Writings".
Physics.
Leibniz contributed a fair amount to the statics and dynamics emerging around him, often disagreeing with Descartes and Newton. He devised a new theory of motion (dynamics) based on kinetic energy and potential energy, which posited space as relative, whereas Newton was thoroughly convinced that space was absolute. An important example of Leibniz's mature physical thinking is his "Specimen Dynamicum" of 1695.
Until the discovery of subatomic particles and the quantum mechanics governing them, many of Leibniz's speculative ideas about aspects of nature not reducible to statics and dynamics made little sense. For instance, he anticipated Albert Einstein by arguing, against Newton, that space, time and motion are relative, not absolute: "As for my own opinion, I have said more than once, that I hold space to be something merely relative, as time is, that I hold it to be an order of coexistences, as time is an order of successions."
Leibniz's relationist notion of space and time as against Newton's substantivalist views. According to Newton's substantivalism, space and time are entities in their own right, existing independently of things. Leibniz's relationism, on the other hand, describes space and time as systems of relations that exist between objects. The rise of general relativity and subsequent work in the history of physics has put Leibniz's stance in a more favorable light.
One of Leibniz's projects was to recast Newton's theory as a vortex theory. However, his project went beyond vortex theory, since at its heart there was an attempt to explain one of the most difficult problems in physics, that of the origin of the cohesion of matter.
Leibniz's rule for the derivatives of products is an important, if often overlooked, step in many proofs in diverse fields of physics. The principle of sufficient reason has been invoked in recent cosmology, and his identity of indiscernibles in quantum mechanics, a field some even credit him with having anticipated in some sense. Those who advocate digital philosophy, a recent direction in cosmology, claim Leibniz as a precursor.
The "vis viva".
Leibniz's "vis viva" (Latin for "living force") is "mv"2, twice the modern kinetic energy. He realized that the total energy would be conserved in certain mechanical systems, so he considered it an innate motive characteristic of matter. Here too his thinking gave rise to another regrettable nationalistic dispute. His "vis viva" was seen as rivaling the conservation of momentum championed by Newton in England and by Descartes in France; hence academics in those countries tended to neglect Leibniz's idea. In reality, both energy and momentum are conserved, so the two approaches are equally valid.
Other natural science.
By proposing that the earth has a molten core, he anticipated modern geology. In embryology, he was a preformationist, but also proposed that organisms are the outcome of a combination of an infinite number of possible microstructures and of their powers. In the life sciences and paleontology, he revealed an amazing transformist intuition, fueled by his study of comparative anatomy and fossils. One of his principal works on this subject, "Protogaea", unpublished in his lifetime, has recently been published in English for the first time. He worked out a primal organismic theory. In medicine, he exhorted the physicians of his time—with some results—to ground their theories in detailed comparative observations and verified experiments, and to distinguish firmly scientific and metaphysical points of view.
Social science.
Much of Leibniz's work went on to have a great impact on the field of psychology. His theory regarding consciousness in relation to the principle of continuity can be seen as an early theory regarding the stages of sleep. He believed that by the principle that phenomena found in nature were continuous by default, it was likely that the transition between conscious and unconscious states had intermediary steps. Though Leibniz's ideas regarding pre-established harmony were rejected by many, psychologists embraced his ideas of psychophysical parallelism. This idea refers to the mind–body problem, stating that the mind and brain do not act upon each other, but act alongside each other separately but in harmony.
Leibniz believed that the mind had a very active role in perception, and plays a much larger role in sensory input. He focused heavily on perception, distinguishing between the type of perception where we are conscious of a stimulus, and the other which is being aware of a distinct perception. He thought that there are many "petites perceptions", or small perceptions of which we perceive but of which we are unaware. For example, when a bag of rice is spilled, we see the rice but are not necessarily aware of how many grains are in the pile. With this principle, there are an infinite number of perceptions within us at any given time of which we are unaware. For this to be true, there must also be a portion of the mind of which we are unaware at any given time. In this way, Leibniz's theory of perception can be viewed as one of many theories leading up to the idea of the unconscious. Additionally, the idea of subliminal stimuli can be traced back to his theory of small perceptions. Leibniz was a direct influence on Ernst Platner, who is credited with originally coining the term "Unbewußtseyn" (unconscious).
Leibniz's ideas regarding music and tonal perception went on to influence the laboratory studies of Wilhelm Wundt.
In public health, he advocated establishing a medical administrative authority, with powers over epidemiology and veterinary medicine. He worked to set up a coherent medical training program, oriented towards public health and preventive measures. In economic policy, he proposed tax reforms and a national insurance program, and discussed the balance of trade. He even proposed something akin to what much later emerged as game theory. In sociology he laid the ground for communication theory.
Technology.
In 1906, Garland published a volume of Leibniz's writings bearing on his many practical inventions and engineering work. To date, few of these writings have been translated into English. Nevertheless, it is well understood that Leibniz was a serious inventor, engineer, and applied scientist, with great respect for practical life. Following the motto "theoria cum praxi", he urged that theory be combined with practical application, and thus has been claimed as the father of applied science. He designed wind-driven propellers and water pumps, mining machines to extract ore, hydraulic presses, lamps, submarines, clocks, etc. With Denis Papin, he invented a steam engine. He even proposed a method for desalinating water. From 1680 to 1685, he struggled to overcome the chronic flooding that afflicted the ducal silver mines in the Harz Mountains, but did not succeed.
Computation.
Leibniz may have been the first computer scientist and information theorist. Early in life, he documented the binary numeral system (base 2), then revisited that system throughout his career. He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.
In 1671, Leibniz began to invent a machine that could execute all four arithmetic operations, gradually improving it over a number of years. This "stepped reckoner" attracted fair attention and was the basis of his election to the Royal Society in 1673. A number of such machines were made during his years in Hanover by a craftsman working under his supervision. They were not an unambiguous success because they did not fully mechanize the carry operation. Couturat reported finding an unpublished note by Leibniz, dated 1674, describing a machine capable of performing some algebraic operations. Leibniz also devised a (now reproduced) cipher machine, recovered by Nicholas Rescher in 2010. In 1693, Leibniz released to the public a design of a machine which could, in theory, integrate differential equations.
Leibniz was groping towards hardware and software concepts worked out much later by Charles Babbage and Ada Lovelace. In 1679, while mulling over his binary arithmetic, Leibniz imagined a machine in which binary numbers were represented by marbles, governed by a rudimentary sort of punched cards. Modern electronic digital computers replace Leibniz's marbles moving by gravity with shift registers, voltage gradients, and pulses of electrons, but otherwise they run roughly as Leibniz envisioned in 1679.
Librarian.
While serving as librarian of the ducal libraries in Hanover and Wolfenbuettel, Leibniz effectively became one of the founders of library science. The latter library was enormous for its day, as it contained more than 100,000 volumes, and Leibniz helped design a new building for it, believed to be the first building explicitly designed to be a library. He also designed a book indexing system in ignorance of the only other such system then extant, that of the Bodleian Library at Oxford University. He also called on publishers to distribute abstracts of all new titles they produced each year, in a standard form that would facilitate indexing. He hoped that this abstracting project would eventually include everything printed from his day back to Gutenberg. Neither proposal met with success at the time, but something like them became standard practice among English language publishers during the 20th century, under the aegis of the Library of Congress and the British Library.
He called for the creation of an empirical database as a way to further all sciences. His "characteristica universalis", calculus ratiocinator, and a "community of minds"—intended, among other things, to bring political and religious unity to Europe—can be seen as distant unwitting anticipations of artificial languages (e.g., Esperanto and its rivals), symbolic logic, even the World Wide Web.
Advocate of scientific societies.
Leibniz emphasized that research was a collaborative endeavor. Hence he warmly advocated the formation of national scientific societies along the lines of the British Royal Society and the French Academie Royale des Sciences. More specifically, in his correspondence and travels he urged the creation of such societies in Dresden, Saint Petersburg, Vienna, and Berlin. Only one such project came to fruition; in 1700, the Berlin Academy of Sciences was created. Leibniz drew up its first statutes, and served as its first President for the remainder of his life. That Academy evolved into the German Academy of Sciences, the publisher of the ongoing critical edition of his works.
Lawyer, moralist.
With the possible exception of Marcus Aurelius, no philosopher has ever had as much experience with practical affairs of state as Leibniz. Leibniz's writings on law, ethics, and politics were long overlooked by English-speaking scholars, but this has changed of late.
While Leibniz was no apologist for absolute monarchy like Hobbes, or for tyranny in any form, neither did he echo the political and constitutional views of his contemporary John Locke, views invoked in support of democracy, in 18th-century America and later elsewhere. The following excerpt from a 1695 letter to Baron J. C. Boyneburg's son Philipp is very revealing of Leibniz's political sentiments:
As for... the great question of the power of sovereigns and the obedience their peoples owe them, I usually say that it would be good for princes to be persuaded that their people have the right to resist them, and for the people, on the other hand, to be persuaded to obey them passively. I am, however, quite of the opinion of Grotius, that one ought to obey as a rule, the evil of revolution being greater beyond comparison than the evils causing it. Yet I recognize that a prince can go to such excess, and place the well-being of the state in such danger, that the obligation to endure ceases. This is most rare, however, and the theologian who authorizes violence under this pretext should take care against excess; excess being infinitely more dangerous than deficiency.
In 1677, Leibniz called for a European confederation, governed by a council or senate, whose members would represent entire nations and would be free to vote their consciences; this is sometimes considered an anticipation of the European Union. He believed that Europe would adopt a uniform religion. He reiterated these proposals in 1715.
But at the same time, he arrived to propose an interreligious and multicultural project to create a universal system of justice, which required from him a broad interdisciplinary perspective. In order to it, he combined linguistics, especially sinology, moral and law philosophy, management, economics, politics.
Ecumenism.
Leibniz devoted considerable intellectual and diplomatic effort to what would now be called ecumenical endeavor, seeking to reconcile first the Roman Catholic and Lutheran churches, later the Lutheran and Reformed churches. In this respect, he followed the example of his early patrons, Baron von Boyneburg and the Duke John Frederick—both cradle Lutherans who converted to Catholicism as adults—who did what they could to encourage the reunion of the two faiths, and who warmly welcomed such endeavors by others. (The House of Brunswick remained Lutheran because the Duke's children did not follow their father.) These efforts included corresponding with the French bishop Jacques-Bénigne Bossuet, and involved Leibniz in a fair bit of theological controversy. He evidently thought that the thoroughgoing application of reason would suffice to heal the breach caused by the Reformation.
Philologist.
Leibniz the philologist was an avid student of languages, eagerly latching on to any information about vocabulary and grammar that came his way. He refuted the belief, widely held by Christian scholars in his day, that Hebrew was the primeval language of the human race. He also refuted the argument, advanced by Swedish scholars in his day, that a form of proto-Swedish was the ancestor of the Germanic languages. He puzzled over the origins of the Slavic languages, was aware of the existence of Sanskrit, and was fascinated by classical Chinese.
He published the "princeps editio" (first modern edition) of the late medieval "Chronicon Holtzatiae", a Latin chronicle of the County of Holstein.
Sinophile.
Leibniz was perhaps the first major European intellect to take a close interest in Chinese civilization, which he knew by corresponding with, and reading other works by, European Christian missionaries posted in China. Having read "Confucius Sinarum Philosophus" on the first year of its publication, he concluded that Europeans could learn much from the Confucian ethical tradition. He mulled over the possibility that the Chinese characters were an unwitting form of his universal characteristic. He noted with fascination how the "I Ching" hexagrams correspond to the binary numbers from 000000 to 111111, and concluded that this mapping was evidence of major Chinese accomplishments in the sort of philosophical mathematics he admired.
Leibniz's attraction to Chinese philosophy originates from his perception that Chinese philosophy was similar to his own. The historian E.R. Hughes suggests that Leibniz's ideas of "simple substance" and "pre-established harmony" were directly influenced by Confucianism, pointing to the fact that they were conceived during the period that he was reading "Confucius Sinarum Philosophus".
As polymath.
While making his grand tour of European archives to research the Brunswick family history that he never completed, Leibniz stopped in Vienna between May 1688 and February 1689, where he did much legal and diplomatic work for the Brunswicks. He visited mines, talked with mine engineers, and tried to negotiate export contracts for lead from the ducal mines in the Harz mountains. His proposal that the streets of Vienna be lit with lamps burning rapeseed oil was implemented. During a formal audience with the Austrian Emperor and in subsequent memoranda, he advocated reorganizing the Austrian economy, reforming the coinage of much of central Europe, negotiating a Concordat between the Habsburgs and the Vatican, and creating an imperial research library, official archive, and public insurance fund. He wrote and published an important paper on mechanics.
Leibniz also wrote a short paper, "Primae veritates", first published by Louis Couturat in 1903 (pp. 518–523) summarizing his views on metaphysics. The paper is undated; that he wrote it while in Vienna in 1689 was determined only in 1999, when the ongoing critical edition finally published Leibniz's philosophical writings for the period 1677–90. Couturat's reading of this paper was the launching point for much 20th-century thinking about Leibniz, especially among analytic philosophers. But after a meticulous study of all of Leibniz's philosophical writings up to 1688—a study the 1999 additions to the critical edition made possible—Mercer (2001) begged to differ with Couturat's reading; the jury is still out.
Posthumous reputation.
As a mathematician and philosopher.
When Leibniz died, his reputation was in decline. He was remembered for only one book, the "Théodicée", whose supposed central argument Voltaire lampooned in his popular book "Candide", which concludes with the character Candide saying, ""Non liquet"" (it is not clear), a term that was applied during the Roman Republic to a legal verdict of "not proven". Voltaire's depiction of Leibniz's ideas was so influential that many believed it to be an accurate description. Thus Voltaire and his "Candide" bear some of the blame for the lingering failure to appreciate and understand Leibniz's ideas. Leibniz had an ardent disciple, Christian Wolff, whose dogmatic and facile outlook did Leibniz's reputation much harm. He also influenced David Hume who read his "Théodicée" and used some of his ideas. In any event, philosophical fashion was moving away from the rationalism and system building of the 17th century, of which Leibniz had been such an ardent proponent. His work on law, diplomacy, and history was seen as of ephemeral interest. The vastness and richness of his correspondence went unrecognized.
Much of Europe came to doubt that Leibniz had discovered calculus independently of Newton, and hence his whole work in mathematics and physics was neglected. Voltaire, an admirer of Newton, also wrote "Candide" at least in part to discredit Leibniz's claim to having discovered calculus and Leibniz's charge that Newton's theory of universal gravitation was incorrect.
Leibniz's long march to his present glory began with the 1765 publication of the "Nouveaux Essais", which Kant read closely. In 1768, Louis Dutens edited the first multi-volume edition of Leibniz's writings, followed in the 19th century by a number of editions, including those edited by Erdmann, Foucher de Careil, Gerhardt, Gerland, Klopp, and Mollat. Publication of Leibniz's correspondence with notables such as Antoine Arnauld, Samuel Clarke, Sophia of Hanover, and her daughter Sophia Charlotte of Hanover, began.
In 1900, Bertrand Russell published a critical study of Leibniz's metaphysics. Shortly thereafter, Louis Couturat published an important study of Leibniz, and edited a volume of Leibniz's heretofore unpublished writings, mainly on logic. They made Leibniz somewhat respectable among 20th-century analytical and linguistic philosophers in the English-speaking world (Leibniz had already been of great influence to many Germans such as Bernhard Riemann). For example, Leibniz's phrase "salva veritate", meaning interchangeability without loss of or compromising the truth, recurs in Willard Quine's writings. Nevertheless, the secondary literature on Leibniz did not really blossom until after World War II. This is especially true of English speaking countries; in Gregory Brown's bibliography fewer than 30 of the English language entries were published before 1946. American Leibniz studies owe much to Leroy Loemker (1904–1985) through his translations and his interpretive essays in LeClerc (1973).
Nicholas Jolley has surmised that Leibniz's reputation as a philosopher is now perhaps higher than at any time since he was alive. Analytic and contemporary philosophy continue to invoke his notions of identity, individuation, and possible worlds. Work in the history of 17th- and 18th-century ideas has revealed more clearly the 17th-century "Intellectual Revolution" that preceded the better-known Industrial and commercial revolutions of the 18th and 19th centuries.
In 1985, the German government created the Leibniz Prize, offering an annual award of 1.55 million euros for experimental results and 770,000 euros for theoretical ones. It is the world's largest prize for scientific achievement.
The collection of manuscript papers of Leibniz at the Gottfried Wilhelm Leibniz Bibliothek – Niedersächische Landesbibliothek were inscribed on UNESCO's Memory of the World Register in 2007.
Writings and edition.
Leibniz mainly wrote in three languages: scholastic Latin, French and German. During his lifetime, he published many pamphlets and scholarly articles, but only two "philosophical" books, the "Combinatorial Art" and the "Théodicée". (He published numerous pamphlets, often anonymous, on behalf of the House of Brunswick-Lüneburg, most notably the "De jure suprematum" a major consideration of the nature of sovereignty.) One substantial book appeared posthumously, his "Nouveaux essais sur l'entendement humain", which Leibniz had withheld from publication after the death of John Locke. Only in 1895, when Bodemann completed his catalogue of Leibniz's manuscripts and correspondence, did the enormous extent of Leibniz's "Nachlass" become clear: about 15,000 letters to more than 1000 recipients plus more than 40,000 other items. Moreover, quite a few of these letters are of essay length. Much of his vast correspondence, especially the letters dated after 1700, remains unpublished, and much of what is published has been so only in recent decades. The amount, variety, and disorder of Leibniz's writings are a predictable result of a situation he described in a letter as follows:
I cannot tell you how extraordinarily distracted and spread out I am. I am trying to find various things in the archives; I look at old papers and hunt up unpublished documents. From these I hope to shed some light on the history of the of Brunswick. I receive and answer a huge number of letters. At the same time, I have so many mathematical results, philosophical thoughts, and other literary innovations that should not be allowed to vanish that I often do not know where to begin.
The extant parts of the critical edition of Leibniz's writings are organized as follows:
The systematic cataloguing of all of Leibniz's "Nachlass" began in 1901. It was hampered by two world wars and decades of German division in two states with the cold war's "iron curtain" in between, separating scholars, and also scattering portions of his literary estates. The ambitious project has had to deal with seven languages contained in some 200,000 pages of written and printed paper. In 1985 it was reorganized and included in a joint program of German federal and state ("Länder") academies. Since then the branches in Potsdam, Münster, Hanover and Berlin have jointly published 57 volumes of the critical edition, with an average of 870 pages, and prepared index and concordance works.
Selected works.
The year given is usually that in which the work was completed, not of its eventual publication.
Collections.
Six important collections of English translations are Wiener (1951), Parkinson (1966), Loemker (1969), Ariew and Garber (1989), Woolhouse and Francks (1998), and Strickland (2006). The ongoing critical edition of all of Leibniz's writings is "Sämtliche Schriften und Briefe".
References.
Bibliographies.
An updated bibliography of more than 25.000 titles is available at Leibniz Bibliographie.

</doc>
<doc id="12283" url="https://en.wikipedia.org/wiki?curid=12283" title="Gamma World">
Gamma World

Gamma World is a science fantasy role-playing game, originally designed by James M. Ward and Gary Jaquet, and first published by TSR in 1978. It borrowed heavily from Ward's earlier product, "Metamorphosis Alpha".
Setting.
"Gamma World" takes place in the mid-25th century, more than a century after a second nuclear war had destroyed human civilization. The war that destroyed civilization in "Gamma World" is only vaguely described in most editions of the game, and what details are provided change from version to version. The first two editions place a first nuclear war near the end of the 21st century, with the final war in the years AD 2309-2322, and ascribe the final annihilation to a terrorist group called "The Apocalypse" and the ensuing retaliation by surviving factions. Later versions would radically alter the reason for the collapse. The 2000 Alternity version is due to alien arrival and nuclear response. The 2003 d20 Modern iteration is due to rampant use of nanotechnology and AI. The 2010 edition introduces a radically different comedy backstory that attributes the destruction of civilization to the activation of the Large Hadron Collider, which caused multiple realities to exchange features in an event known as "The Big Mistake". All editions, however, agree that the cataclysm destroyed all government and society beyond a village scale, plunging the world into a Dark Age. In many editions of the game technology is at best quasi-medieval (in the first edition, the crossbow is described as "the ultimate weapon" for most Gamma World societies). Some, such as the 2003 and, to a lesser degree the 2010 edition, feature advanced technology that is well known and often easily available. Whereas earlier editions had ancient super-science artifacts as risky to use due to the average Gamma World character not knowing how to properly operate such devices. Or not even knowing what the device is at all. The post-apocalyptic inhabitants of Earth now refer to their planet as "Gamma World" (or "Gamma Terra" in later editions).
Gamma World is a chaotic, dangerous environment that little resembles pre-apocalyptic Earth. The weapons unleashed during the final war were strong enough to alter coastlines, level cities, and leave large areas of land lethally radioactive. These future weapons bathed the surviving life of Earth in unspecified forms of radiation and biochemical agents, producing widespread, permanent mutations among humans, animals, and plants. As a result, fantastic mutations such as multiple limbs, super strength, and psychic powers are relatively common. (Random tables of such improbable mutations are a hallmark of every edition of "Gamma World".) Many animals and plants are sentient, semi-civilized species competing with surviving humans. Both humans and non-humans have lost most knowledge of the pre-war humans, whom Gamma World's inhabitants refer to as "the Ancients". The only group with significant knowledge of the Ancients are isolated robots and other artificial intelligences that survived the war—though these machines tend to be damaged, in ill-repair, or hostile to organic beings.
"Gamma World" player characters include unmutated humans (referred to as "Pure Strain Humans" in most editions), mutated humans, sentient animals or plants, and androids. Characters explore Ancient ruins and strange post-apocalyptic societies to gain knowledge of the Ancients and social status for themselves. Common adventure themes involve protecting fragile post-apocalypse societies, retrieving Ancient "artifacts" (science fiction gadgetry such as power armor, laser pistols, and anti-grav sleds), or mere survival against the multifarious dangers of the future (such as gun-toting mutant rabbits, rampaging ancient death machines, or other Gamma Worlders bent on mayhem).
A recurrent source of conflict on Gamma World is the rivalry among the "Cryptic Alliances", semi-secret societies whose ideological agendas—usually verging on monomania—often bring them into conflict with the rest of the Gamma World. For example, the Pure Strain Human "Knights of Genetic Purity" seek to exterminate all mutants, while the all-mutant "Iron Society" wants to eliminate unmutated humans. Other rivalries involve attitudes towards Ancient technology, with some Alliances (such as "The Restorationists") seeking to rebuild Ancient society, while others (such as "The Seekers") want to destroy remaining artifacts.
System.
Throughout the game's many editions, "Gamma World" has almost always remained strongly compatible with the then-current edition of "Dungeons & Dragons" ("D&D"). Attribute generation is much the same for instance with a range of 3 to 18, randomly generated by rolling three six-sided dice. The attributes themselves are the same, but with occasional name changes such as Physical Strength instead of Strength and Mental Strength instead of Wisdom. This allows Gamma World and D&D characters to potentially cross over genres.
Character generation is mostly random, and features one of the game's most distinctive mechanics, the mutation tables. Players who choose to play mutants roll dice to randomly determine their characters' mutations. All versions of "Gamma World" eschew a realistic portrayal of genetic mutation to one degree or another, instead giving characters fantastic abilities like psychic powers, laser beams, force fields, life draining and others. While other mutations are extensions or extremes of naturally existing features transposed from different species such as electrical generation, infravision, quills, multiple limbs, dual brains, carapaces, gills, etc.
This was offset with defects that also ranged from the fantastical; such as skin that dissolves in water, attracting monsters - to mundane; such as seizures, madness and phobias.
Characters in most versions of "Gamma World" earn experience points during their adventures, which cause the character's Rank (in some editions, Level) to increase. Unlike "D&D", however, the first two editions of "Gamma World" do not use a concept of true level or character class, and increases in Rank do not affect the character's skills or combat abilities. In fact, in the first three editions of the game, character rank is primarily a measure of the character's social prestige.
The game mechanics used for resolving character actions, on the other hand, greatly varied between "Gamma World" editions. The first two editions, like the early editions of "D&D", depend heavily on matrix-based mechanics, where two factors (one representing the actor or attacker, and one representing the opponent) are cross-referenced on a chart. For some actions, such as attacks, the number located on the matrix represents a number the acting player must roll. For other actions (such as determining the result of radiation exposure), the matrix result indicates a non-negotiable result. "Gamma World"'s first two editions had a variety of specialized matrices for different situations (again, closely resembling "D&D").
The third edition rules replace specialized matrices with the Action Control Table (ACT), a single, color-coded chart that allowed players to determine whether a character action succeeded, and the degree of success, with a single roll. (The ACT concept is drawn from the "Marvel Super Heroes" game published by TSR shortly before development of "Gamma World"'s third edition.) The ACT requires the referee to cross-reference the difficulty of a character action with the ability score used to complete that action, determining which column of the ACT is used for that action. The character's player then rolls percentile dice; the result is compared to appropriate column, determining a degree of success or failure and eliminating the need for second result roll (e.g. the damage roll that many games require after a successful combat action).
The fourth edition was directly compatible with 2nd Edition AD&D with some minor differences in mechanics. The fifth and sixth editions though would relegate Gamma World to that of a Campaign Setting and require the core books to play. 5th uses the Alternity system which is mostly represented in the book but required the core rules in order to resolve some factors. 6th Edition though was fully incomplete on its own and required the d20 Modern rulebook in order to play the game.
The seventh version uses a streamlined version of "D&D 4th edition" mechanics. Character generation choice though was nearly fully removed. Instead of choosing a character class, a player had to roll a twenty-sided die two times and consult an accompanying character origin table. For example, a player might obtain the result "Radioactive Yeti" and gain the powers associated with the "Radioactive" and "Yeti" origins. Attributes, mutations, and skills were also randomly assigned. Two decks of cards comprising the core of a "Collectible Card Game" are included with the game. One deck represented random Alpha Mutations, which could be drawn to gain temporary powers, and the other contained various Omega Tech, powerful technological devices that could possibly backfire on those that used them. Some 4th edition rules enhancements for the setting include new damage types such as "Radiation", Gamma World-specific skills, and increased lethality. Despite these differences, it is possible to use characters and monsters from a "D&D" game in "Gamma World" and vice versa.
History.
First Edition (1978).
The original "Gamma World" boxed set (containing a 56-page rulebook, a map of a devastated North America, and dice) was released in 1978. TSR went on to publish three accessories for the 1st edition of the game:
Grenadier Miniatures also supported the game, with a line of licensed miniatures.
At least one other TSR product was announced -- "Metamorphosis Alpha to Omega", an adaptation of "Metamorphosis Alpha's" campaign setting to "Gamma World's" rules (Anon 1981). Work on the adaptation was halted when a 2nd edition of "Gamma World" was announced. This was later released as "Metamorphosis Alpha to Omega" using the "Amazing Engine" Rules.
Second Edition (1983).
The second edition "Gamma World" boxed set (with rules designed by Ward, Jaquet, and David James Ritchie) was released in 1983. Two modules and two accessories accessories were released for this version:
TSR also produced four packs of "Gamma World" miniatures. TSR started production on a third adventure module, which was to be assigned the identification code GW5 and had the working title "Rapture of the Deep". This module was not published. However, a 'ghost' Second Edition GW5 "Rapture of the Deep" module was produced in 2007.
Third Edition (1985).
The 3rd edition of "Gamma World" was another boxed set, credited to James M. Ward and published in September 1985. It introduced the Action Control Table, a color-coded table used to resolve nearly all actions in the game. (Color-coded tables were something of a trend at TSR in mid-1980s. After 1984's "Marvel Super Heroes" proved the viability of the concept, TSR revised "Gamma World", "Star Frontiers", and "Top Secret" to use similar tables.) Unfortunately for TSR, this version of the rules became notorious for the number of editorial mistakes, including cross-references to rules that didn't appear in the boxed set. The errors were serious enough that TSR published a "Gamma World Rules Supplement" containing the "missing" rules. The "Rules Supplement" was sent to gamers who requested it by mail, and included in reprintings of the boxed set.
The five modules TSR published for "Gamma World's" 3rd edition introduced the setting's first multi-module metaplot, which involved rebuilding an ancient 'sky chariot':
TSR dropped the 3rd edition of "Gamma World" from its product line before the multi-module storyline could be completed. In 2003 an unofficial conclusion to the series was published under the title GW11 "Omega Project".
Despite its editorial issues, the 3rd edition rules were well-received enough to win the 1986/1987 Gamer's Choice Award for "Best Science-Fiction Roleplaying Game".
Fourth Edition (1992).
The 4th edition of "Gamma World" was a 192-page softcover book, written by Bruce Nesmith and James M. Ward, published in May 1992 by TSR. This version of the game abandoned the 3rd edition's Action Control Table for mechanics resembling "2nd Edition Advanced Dungeons & Dragons". TSR published five accessories for the 4th edition:
TSR's "Gamma World" development team announced at Gen Con 1993 that no further products would be released for the 4th edition. They also announced that TSR had restarted development of "Metamorphosis Alpha to Omega", but that the manuscript would be completed using the "Amazing Engine" rules.
Fifth edition (2000).
The 5th version of "Gamma World" was a supplement for the science-fiction game "Alternity". (In a nod to "Gamma World"'s reputation for being repeatedly revised, the book's back cover states "That's right, it's the return of the "Gamma World"".) The "Gamma World Campaign Setting" (ISBN 0-7869-1629-X) was a 192-page softcover book written by Andy Collins and Jeff Grubb, published in 2000 by Wizards of the Coast (WOTC), only a month after WOTC announced its cancellation of the "Alternity" line. This version of "Gamma World" is unique as the only one not to have accessories or supplements.
Omega World (2002).
In September 2002, "Omega World", a "d20 System" mini-game based on "Gamma World" and written by Jonathan Tweet, was published in "Dungeon" 94/"Polyhedron" 153. Tweet does not plan any expansions for the game, although it received a warm reception from "Gamma World" fans and players new to the concept alike.
Sixth Edition (2003).
In November 2002, Sword & Sorcery Studios (SSS) announced that it had licensed the "Gamma World" setting from WOTC in order to produce a sixth version of the game. SSS's version of the game, which reached the market in 2003, used the "d20 Modern" system, and mimicked "D&D"'s "three core book" model with three hardcover manuals:
Sword & Sorcery Studios also published three paperback supplements for the d20 version of "Gamma World":
This new version of the game presented a more sober and serious approach to the concept of a post-nuclear world, at odds with the more light-hearted and adventurous approach taken by previous editions; it was also the first edition of the game to include fantastical nanotechnology on a large scale. In August 2005, White Wolf announced that it was reverting the rights to publish "Gamma World" products back to Wizards of the Coast, putting the game out of print again. Several critics and fans considered Tweet's "Omega World" to be a superior d20 System treatment of the "Gamma World" concept.
Seventh Edition (2010).
At the Dungeons & Dragons Experience fan convention in early 2010, Wizards of the Coast announced a new version of "Gamma World", eventually released in October of that year. The game is compatible with the D&D 4th Edition rules and the System Reference Document, but is not considered a separate D&D setting.
The basic box included 80 non-random cards. In addition, random "boosters" of "Alpha Mutation" and "Omega Tech" cards for players are sold separately in packs of eight.
This edition of "Gamma World" includes the following three boxed sets (one core set and two expansion kits):
Additional content was released for the 7th Edition of "Gamma World" at Game Day 2010 and at the 2010 Penny Arcade Expo, Trouble in Freesboro and Pax Extraterrestria respectively. In addition, a Christmas themed adventure and vehicular combat rules were released online.

</doc>
<doc id="12284" url="https://en.wikipedia.org/wiki?curid=12284" title="Grimoire">
Grimoire

A grimoire ( ) is a textbook of magic, typically including instructions on how to create magical objects like talismans and amulets; how to perform magical spells; charms and divination; and how to summon or invoke supernatural entities such as angels, spirits, and demons. In many cases, the books themselves are believed to be imbued with magical powers, though in many cultures, other sacred texts that are not grimoires (such as the Bible) have been believed to have supernatural properties intrinsically. In this manner while all "books on magic" could be thought of as grimoires, not all "magical books" should.
While the term "grimoire" is originally European and many Europeans throughout history, particularly ceremonial magicians and cunning folk, have made use of grimoires, the historian Owen Davies noted that similar books can be found all across the world, ranging from Jamaica to Sumatra. He also noted that the first grimoires could be found in Europe and the Ancient Near East.
Etymology.
It is most commonly believed that the term "grimoire" originated from the Old French word "grammaire", which had initially been used to refer to all books written in Latin. By the 18th century, the term had gained its now common usage in France, and had begun to be used to refer purely to books of magic. Owen Davies presumed this was because "many of them continued to circulate in Latin manuscripts".
However, the term "grimoire" later developed into a figure of speech amongst the French indicating something that was hard to understand. In the 19th century, with the increasing interest in occultism amongst the British following the publication of Francis Barrett's "The Magus" (1801), the term entered the English language in reference to books of magic.
History.
Ancient period.
The earliest known written magical incantations come from ancient Mesopotamia (modern Iraq), where they have been found inscribed on cuneiform clay tablets excavated by archaeologists from the city of Uruk and dated to between the 5th and 4th centuries BC. The ancient Egyptians also employed magical incantations, which have been found inscribed on amulets and other items. The Egyptian magical system, known as heka, was greatly altered and enhanced after the Macedonians, led by Alexander the Great, invaded Egypt in 332 BC. Under the next three centuries of Hellenistic Egypt, the Coptic writing system evolved, and the Library of Alexandria was opened. This likely had an influence upon books of magic, with the trend on known incantations switching from simple health and protection charms to more specific things, such as financial success and sexual fulfillment. Around this time the legendary figure of Hermes Trismegistus developed as a conflation of the Egyptian god Thoth and the Greek Hermes; this figure was associated with writing and magic and, therefore, of books on magic.
The ancient Greeks and Romans believed that books on magic were invented by the Persians. The 1st-century AD writer Pliny the Elder stated that magic had been first discovered by the ancient philosopher Zoroaster around the year 6347 BC but that it was only written down in the 5th century BC by the magician Osthanes. His claims are not, however, supported by modern historians.
The ancient Jewish people were often viewed as being knowledgeable in magic, which, according to legend, they had learned from Moses, who had learned it in Egypt. Among many ancient writers, Moses was seen as an Egyptian rather than a Jew. Two manuscripts likely dating to the 4th century, both of which purport to be the legendary eighth Book of Moses (the first five being the initial books in the Biblical Old Testament), present him as a polytheist who explained how to conjure gods and subdue demons.
Meanwhile, there is definite evidence of grimoires being used by certain, particularly Gnostic, sects of early Christianity. In the "Book of Enoch" found within the Dead Sea Scrolls, for instance, there is information on astrology and the angels. In possible connection with the "Book of Enoch", the idea of Enoch and his great-grandson Noah having some involvement with books of magic given to them by angels continued through to the medieval period.
Israelite King Solomon was a Biblical figure associated with magic and sorcery in the ancient world. The 1st-century Romano-Jewish historian Josephus mentioned a book circulating under the name of Solomon that contained incantations for summoning demons and described how a Jew called Eleazar used it to cure cases of possession. The book may have been the "Testament of Solomon" but was more probably a different work. The pseudepigraphic "Testament of Solomon" is one of the oldest magical texts. It is a Greek manuscript attributed to Solomon and likely written in either Babylonia or Egypt sometime in the first five centuries AD, over 1,000 years after Solomon's death.
The work tells of the building of The Temple and relates that construction was hampered by demons until the angel Michael gave the king a magical ring. The ring, engraved with the Seal of Solomon, had the power to bind demons from doing harm. Solomon used it to lock demons in jars and commanded others to do his bidding, although eventually, according to the "Testament", he was tempted into worshiping "false gods", such as Moloch, Baal, and Rapha. Subsequently, after losing favour with God, King Solomon wrote the work as a warning and a guide to the reader.
When Christianity became the dominant faith of the Roman Empire, the early Church frowned upon the propagation of books on magic, connecting it with paganism, and burned books of magic. The New Testament records that St. Paul called for the burning of magic and pagan books in the city of Ephesus; this advice was adopted on a large scale after the Christian ascent to power. Even before Christianisation, the Imperial Roman government had suppressed many pagan, Christian, philosophical, and divinatory texts that it viewed as threats to Roman authority, including those of the Greek mystic and mathematician Pythagoras.
Medieval period.
In the Medieval period, the production of grimoires continued in Christendom, as well as amongst Jews and the followers of the newly founded Islamic faith. As the historian Owen Davies noted, "while the Church was ultimately successful in defeating pagan worship it never managed to demarcate clearly and maintain a line of practice between religious devotion and magic." The use of such books on magic continued. In Christianised Europe, the Church divided books of magic into two kinds: those that dealt with "natural magic" and those that dealt in "demonic magic". The former was acceptable, because it was viewed as merely taking note of the powers in nature that were created by God; for instance, the Anglo-Saxon leechbooks, which contained simple spells for medicinal purposes, were tolerated. However, demonic magic was not acceptable, because it was believed that such magic did not come from God, but from the Devil and his demons. These grimoires dealt in such topics as necromancy, divination and demonology. Despite this, "there is ample evidence that the mediaeval clergy were the main practitioners of magic and therefore the owners, transcribers, and circulators of grimoires," while several grimoires were attributed to Popes.
One such Arabic grimoire devoted to astral magic, the 12th-century "Ghâyat al-Hakîm fi'l-sihr", was later translated into Latin and circulated in Europe during the 13th century under the name of the "Picatrix". However, not all such grimoires of this era were based upon Arabic sources. The 13th-century "Sworn Book of Honorius", for instance, was (like the ancient "Testament of Solomon" before it) largely based on the supposed teachings of the Biblical king Solomon and included ideas such as prayers and a ritual circle, with the mystical purpose of having visions of God, Hell, and Purgatory and gaining much wisdom and knowledge as a result. Another was the Hebrew "Sefer Raziel Ha-Malakh", translated in Europe as the "Liber Razielis Archangeli".
A later book also claiming to have been written by Solomon was originally written in Greek during the 15th century, where it was known as the "Magical Treatise of Solomon" or the "Little Key of the Whole Art of Hygromancy, Found by Several Craftmen and by the Holy Prophet Solomon". In the 16th century, this work had been translated into Latin and Italian, being renamed the "Clavicula Salomonis", or the "Key of Solomon". In Christendom during the mediaeval age, grimoires were written that were attributed to other ancient figures, thereby supposedly giving them a sense of authenticity because of their antiquity. The German abbot and occultist Trithemius (1462–1516) supposedly had a "Book of Simon the Magician", based upon the New Testament figure of Simon Magus. Magus had been a contemporary of Jesus Christ's and, like the Biblical Jesus, had supposedly performed miracles, but had been demonised by the Medieval Church as a devil worshiper and evil individual. Similarly, it was commonly believed by mediaeval people that other ancient figures, such as the poet Virgil, astronomer Ptolemy and philosopher Aristotle, had been involved in magic, and grimoires claiming to have been written by them were circulated. However, there were those who did not believe this; for instance, the Franciscan friar Roger Bacon (c. 1214–94) stated that books falsely claiming to be by ancient authors "ought to be prohibited by law."
Early modern period.
As the early modern period commenced in the late 15th century, many changes began to shock Europe that would have an effect on the production of grimoires. Historian Owen Davies classed the most important of these as the Protestant Reformation and subsequent Catholic Counter-Reformation, the witch-hunts and the advent of printing. The Renaissance saw the continuation of interest in magic that had been found in the Mediaeval period, and in this period, there was an increased interest in Hermeticism among occultists and ceremonial magicians in Europe, largely fueled by the 1471 translation of the ancient "Corpus hermeticum" into Latin by Marsilio Ficino (1433–99).
Alongside this, there was a rise in interest the Jewish mysticism known as the Kabbalah, which was spread across the continent by Pico della Mirandola and Johannes Reuchlin. The most important magician of the Renaissance was Heinrich Cornelius Agrippa (1486–1535), who widely studied occult topics and earlier grimoires and eventually published his own, the "Three Books of Occult Philosophy", in 1533. A similar figure was the Swiss magician known as Paracelsus (1493–1541), who published "Of the Supreme Mysteries of Nature", in which he emphasised the distinction between good and bad magic. A third such individual was Johann Georg Faust, upon whom several pieces of later literature were written, such as Christopher Marlowe's "Doctor Faustus", that portrayed him as consulting with demons. The idea of demonology had remained strong in the Renaissance, and several demonological grimoires were published, including "The Fourth Book of Occult Philosophy", which falsely claimed to having been authored by Agrippa, and the "Pseudomonarchia Daemonum", which listed 69 demons. To counter this, the Roman Catholic Church authorised the production of many works of exorcism, the rituals of which were often very similar to those of demonic conjuration. Alongside these demonological works, grimoires on natural magic continued to be produced, including "Magia naturalis", written by Giambattista Della Porta (1535–1615).
The advent of printing in Europe meant that books could be mass-produced for the first time and could reach an ever-growing literate audience. Among the earliest books to be printed were magical texts. The "nóminas" were one example, consisting of prayers to the saints used as talismans. It was particularly in Protestant countries, such as Switzerland and the German states, which were not under the domination of the Roman Catholic Church, where such grimoires were published.
Despite the advent of print, however, handwritten grimoires remained highly valued, as they were believed to contain inherent magical powers, and they continued to be produced. With increasing availability, people lower down the social scale and women began to have access to books on magic; this was often incorporated into the popular folk magic of the average people and, in particular, that of the cunning folk, who were professionally involved in folk magic. These works left Europe and were imported to the parts of Latin America controlled by the Spanish and Portuguese empires and the parts of North America controlled by the British and French empires.
Throughout this period, the Inquisition, a Roman Catholic organisation, had organised the mass suppression of peoples and beliefs that they considered heretical. In many cases, grimoires were found in the heretics' possessions and destroyed. In 1599, the church published the "Indexes of Prohibited Books", in which many grimoires were listed as forbidden, including several mediaeval ones, such as the "Key of Solomon", which were still popular. In Christendom, there also began to develop a widespread fear of witchcraft, which was believed to be Satanic in nature. The subsequent hysteria, known as the Witch Hunt, caused the death of around 40,000 people, most of whom were women. Sometimes, those found with grimoires, particularly demonological ones, were prosecuted and dealt with as witches but, in most cases, those accused had no access to such books. The European nation that proved the exception to this, however, was the highly literate Iceland, where a third of the 134 witch trials held involved people who had owned grimoires. By the end of the Early Modern period and the beginning of the Enlightenment, many European governments brought in laws prohibiting many superstitious beliefs in an attempt to bring an end to the Witch Hunt; this would invariably affect the release of grimoires.
Meanwhile, Hermeticism and the Kabbalah would influence the creation of a mystical philosophy known as Rosicrucianism, which first appeared in the early 17th century, when two pamphlets detailing the existence of the mysterious Rosicrucian group were published in Germany. These claimed that Rosicrucianism had originated with a Medieval figure known as Christian Rosenkreuz, who had founded the Brotherhood of the Rosy Cross; however, there was no evidence for the existence of Rosenkreuz or the Brotherhood.
18th and 19th centuries.
The 18th century saw the rise of the Enlightenment, a movement devoted to science and rationalism, predominantly amongst the ruling classes. However, amongst much of Europe, belief in magic and witchcraft persisted, as did the witch trials in certain areas. Governments tried to crack down on magicians and fortune tellers, particularly in France, where the police viewed them as social pests who took money from the gullible, often in a search for treasure. In doing so, they confiscated many grimoires. However, in France a new form of printing developed, the "Bibliothèque bleue". Many grimoires published through this circulated among an ever-growing percentage of the populace, in particular the "Grand Albert", the "Petit Albert" (1782), the "Grimoire du Pape Honorious" and the "Enchiridion Leonis Papae". The "Petit Albert" contained a wide variety of forms of magic, for instance, dealing in simple charms for ailments along with more complex things such as the instructions for making a Hand of Glory.
In the late 18th and early 19th centuries, following the French Revolution of 1789, a hugely influential grimoire was published under the title of the "Grand Grimoire", which was considered particularly powerful, because it involved conjuring and making a pact with the devil's chief minister, Lucifugé Rofocale, to gain wealth from him. A new version of this grimoire was later published under the title of the "Dragon rouge" and was available for sale in many Parisian bookstores. Similar books published in France at the time included the "Black Pullet" and the "Grimoirium Verum". The "Black Pullet", probably authored in late-18th-century Rome or France, differs from the typical grimoires in that it does not claim to be a manuscript from antiquity but told by a man who was a member of Napoleon's armed expeditionary forces in Egypt.
The widespread availability of printed grimoires in France—despite the opposition of both the rationalists and the church—soon spread to neighbouring countries such as Spain and Germany. In Switzerland, Geneva was commonly associated with the occult at the time, particularly by Catholics, because it had been a stronghold of Protestantism. Many of those interested in the esoteric traveled from Roman Catholic nations to Switzerland to purchase grimoires or to study with occultists. Soon, grimoires appeared that involved Catholic saints; one example that appeared during the 19th century that became relatively popular, particularly in Spain, was the "Libro de San Cipriano", or "The Book of St. Ciprian", which falsely claimed to date from c. 1000. Like most grimoires of this period, it dealt with (among other things) how to discover treasure.
In Germany, with the increased interest in folklore during the 19th century, many historians took an interest in magic and in grimoires. Several published extracts of such grimoires in their own books on the history of magic, thereby helping to further propagate them. Perhaps the most notable of these was the Protestant pastor Georg Conrad Horst (1779–1832), who from 1821 to 1826, published a six-volume collection of magical texts in which he studied grimoires as a peculiarity of the Mediaeval mindset. Another scholar of the time interested in grimoires, the antiquarian bookseller Johann Scheible, first published the "Sixth and Seventh Books of Moses", two influential magical texts that claimed to have been written by the ancient Jewish figure Moses. "The Sixth and Seventh Books of Moses" were among the works that later spread to the countries of Scandinavia, where, in Danish and Swedish, grimoires were known as black books and were commonly found among members of the army.
In Britain, new grimoires continued to be produced throughout the 18th century, such as Ebenezer Sibly's "A New and Complete Illustration of the Celestial Science of Astrology". In the last decades of that century, London experienced a revival of interest in the occult that was further propagated when Francis Barrett published "The Magus" in 1801. "The Magus" contained many things taken from older grimoires, particularly those of Cornelius Agrippa, and while not achieving initial popularity upon release, gradually became an influential text. One of Barrett's pupils, John Parkin, created his own handwritten grimoire, "The Grand Oracle of Heaven, or, The Art of Divine Magic", although it was never published, largely because Britain was at war with France, and grimoires were commonly associated with the French. The only writer to publish British grimoires widely in the early 19th century, Robert Cross Smith, released "The Philosophical Merlin" (1822) and "The Astrologer of the Nineteenth Century" (1825), but neither sold well.
In the late 19th century, several of these texts (including the Abra-Melin text and the "Key of Solomon") were reclaimed by para-Masonic magical organisations, such as the Hermetic Order of the Golden Dawn and the Ordo Templi Orientis.
20th and 21st centuries.
"The Secret Grimoire of Turiel" claims to have been written in the 16th century, but no copy older than 1927 has been produced.
A modern grimoire, the "Simon Necronomicon", takes its name from a fictional book of magic in the stories of H. P. Lovecraft, inspired by Babylonian mythology and by the "Ars Goetia", a section in the "Lesser Key of Solomon" that concerns the summoning of demons. The "Azoëtia" of Andrew D. Chumbley has been described as a modern grimoire.
The neopagan religion of Wicca publicly appeared in the 1940s, and Gerald Gardner introduced the "Book of Shadows" as a Wiccan grimoire.
Oberon Zell-Ravenheart, headmaster of the Grey School of Wizardry, wrote and compiled the school's "Grimoire for the Apprentice Wizard" (New Page, 2004) and the sequel "Companion for the Apprentice Wizard" (New Page 2006).
In popular culture.
The term grimoire commonly serves as an alternative name for a spell book or tome of magical knowledge in fantasy fiction and role-playing games. The most famous fictional grimoire is the "Necronomicon", a creation of H. P. Lovecraft.
Grimoires are a prominent focus of the Japanese light novel, manga, and anime series A Certain Magical Index. One of the primary characters, Index (named after the "Index Librorum Prohibitorum"), stores the indestructible spiritual copies of 103,000 grimoires in her head.
In the television series "Charmed", the Grimoire is the evil equivalent of the Halliwell sisters' "Book of Shadows".
In the television series "The Vampire Diaries", there are many different grimoires written by many different witches. They are an individual witches spell book and when possessed by another witch the spells inside can be used.
The central book of spells in the Disney animated fantasy adventure series "Gargoyles", the "Grimorum Arcanorum", is an ancient book of magic used by Demona, David Xanatos and the Archmage in schemes throughout the series' storyline.
Rose Lalonde from Homestuck had a grimoire for "Summoning the Zoologically Dubious."
In the video game "Nier", one of the main characters is a talking grimoire who is usually referred to as Weiss, despite his protests that his companions should use his full title, Grimoire Weiss.
The video game Eternal Darkness: Sanity's Requim features a grimoire titled the "Tome of Eternal Darkness".
The videogame Scooby-Doo! Mystery Mayhem features a grimoire named "The Tome of Doom".
The film "The Sorcerer" features a grimoire called the "Incantus".
In Gregory Maguire's series of books set in the Land of Oz: "Wicked: The Life and Times of the Wicked Witch of the West", "Son of a Witch", "A Lion Among Men", and "Out of Oz" (referred to together as "The Wicked Years"), there is a book of spells and other magical arcana called the "Grimmerie".
The anime show, Dance with Devils centers around the conflict between devils and vampires over the location of the forbidden grimoire and the key to its location is the main heroine, Ritsuka Tachibana.

</doc>
<doc id="12285" url="https://en.wikipedia.org/wiki?curid=12285" title="Grand Guignol">
Grand Guignol

Le Théâtre du Grand-Guignol (: "The Theatre of the Great Puppet")—known as the Grand Guignol—was a theatre in the Pigalle area of Paris (at 20 bis, ). From its opening in 1897 until its closing in 1962, it specialised in naturalistic horror shows. Its name is often used as a general term for graphic, amoral horror entertainment, a genre popular from Elizabethan and Jacobean theatre (for instance Shakespeare's "Titus Andronicus", and Webster's "The Duchess of Malfi" and "The White Devil"), to today's splatter films.
Theatre.
"Le Théâtre du Grand-Guignol" was founded in 1894 by Oscar Méténier, who planned it as a space for naturalist performance. With 293 seats, the venue was the smallest in Paris.
A former chapel, the theatre's previous life was evident in the boxes—which looked like confessionals—and in the angels over the orchestra. Although the architecture created frustrating obstacles, the design that was initially a predicament ultimately became beneficial to the marketing of the theatre. The opaque furniture and gothic structures placed sporadically on the walls of the building exude a feeling of eeriness from the moment of entrance. People came to this theatre for an experience, not only to see a show. The audience at "Le Théâtre du Grand-Guignol" endured the terror of the shows because they wanted to be filled with strong “feelings” of something. Many attended the shows to get a feeling of arousal. Underneath the balcony were boxes (originally built for nuns to watch church services) that were available for theatre-goers to rent during performances because they would get so aroused by the action happening on stage. It has been said that audience members would get so boisterous in the boxes, that actors would sometimes break character and yell something such as, “keep it down in there!” Conversely, there were audience members who could not physically handle the brutality of the actions taking place on stage. Frequently, the “special effects” would be too realistic and often an audience member would faint or vomit during performances.
The theatre owed its name to Guignol, which was a traditional Lyonnaise puppet character, joining political commentary with the style of Punch and Judy.
The theatre's peak was between World War I and World War II, when it was frequented by royalty and celebrities in evening dress.
Important people.
Oscar Méténier was the Grand Guignol's founder and original director. Under his direction, the theatre produced plays about a class of people who were not considered appropriate subjects in other venues: prostitutes, criminals, street urchins, and others at the lower end of Paris's social echelon.
Max Maurey served as director from 1898 to 1914. Maurey shifted the theatre's emphasis to the horror plays it would become famous for and judged the success of a performance by the number of patrons who passed out from shock; the average was two faintings each evening. Maurey discovered André de Lorde, who would become the most important playwright for the theatre.
André de Lorde was the theatre's principal playwright from 1901 to 1926. He wrote at least 100 plays for the Grand Guignol and collaborated with experimental psychologist Alfred Binet to create plays about insanity, one of the theatre's frequently recurring themes.
Camille Choisy served as director from 1914 to 1930. He contributed his expertise in special effects and scenery to the theatre's distinctive style.
Jack Jouvin served as director from 1930 to 1937. He shifted the theatre's subject matter, focusing performances not on gory horror but psychological drama. Under his leadership the theatre's popularity waned; and after World War II, it was not well-attended.
Charles Nonon was the theatre's last director.
Plays.
At the Grand Guignol, patrons would see five or six plays, all in a style that attempted to be brutally true to the theatre's naturalistic ideals. The plays were in a variety of styles, but the most popular and best known were the horror plays, featuring a distinctly bleak worldview as well as notably gory special effects in their notoriously bloody climaxes. The horrors depicted at Grand Guignol were generally not supernatural : these plays often explored the altered states, like insanity, hypnosis, panic, under which uncontrolled horror could happen. To heighten the effect, the horror plays were often alternated with comedies.
"Le Laboratoire des Hallucinations," by André de Lorde: When a doctor finds his wife's lover in his operating room, he performs a graphic brain surgery rendering the adulterer a hallucinating semi-zombie. Now insane, the lover/patient hammers a chisel into the doctor's brain.
"Un Crime dans une Maison de Fous," by André de Lorde: Two hags in an insane asylum use scissors to blind a young, pretty fellow inmate out of jealousy.
"L'Horrible Passion," by André de Lorde: A nanny strangles the children in her care.
"Le Baiser dans la nuit" by Maurice Level: A young woman visits the man whose face she horribly disfigured with acid, where he obtains his revenge.
Theatre closing.
Audiences waned in the years following World War II, and the Grand Guignol closed its doors in 1962. Management attributed the closure in part to the fact that the theatre's faux horrors had been eclipsed by the actual events of the Holocaust two decades earlier. "We could never equal Buchenwald," said its final director, Charles Nonon. "Before the war, everyone felt that what was happening onstage was impossible. Now we know that these things, and worse, are possible in reality."
The Grand Guignol building still exists. It is occupied by , a company devoted to presenting plays in sign language.
Legacy.
Grand Guignol flourished briefly in London in the early 1920s under the direction of Jose Levy, where it attracted the talents of Sybil Thorndike and Noël Coward, and a series of short English "Grand Guignol" films (using original screenplays, not play adaptations) was made at the same time, directed by Fred Paul. Several of the films exist at the BFI National Archive.
The Grand Guignol was revived once again in London in 1945, under the direction of Frederick Witney, where it ran for two seasons at the Granville Theatre. These included premiers of Witney's own work as well as adaptations of French originals.
In recent years, English director-writer, Richard Mazda, has re-introduced New York audiences to the Grand Guignol. His acting troupe, The Queens Players, have produced 6 mainstage productions of Grand Guignol plays, and Mazda is writing new plays in the classic Guignol style. The sixth production, "Theatre of Fear", included De Lorde's famous adaptation of Poe's "The System of Doctor Tarr and Professor Fether" ("Le Systéme du Dr Goudron et Pr Plume") as well as two original plays, "Double Crossed" and "The Good Death" alongside "The Tell Tale Heart".
The 1963 mondo film "Ecco" includes a scene which may have been filmed at the Grand Guignol theatre during its final years.
American avant-garde composer John Zorn released an album called "Grand Guignol" by Naked City in 1992, in a reference to "the darker side of our existence which has always been with us and always will be".
Washington, D.C.-based Molotov Theatre Group, established in 2007, is dedicated to preserving and exploring the aesthetic of the Grand Guignol. They have entered two plays into the Capital Fringe Festival in Washington, D.C. Their 2007 show, "For Boston", won "Best Comedy", and their second show, "The Sticking Place", won "Best Overall" in 2008.
The Swiss theatre company, Compagnie Pied de Biche revisits the Grand Guignol genre in contemporary contexts since 2008. The company staged in 2010 a diptych "Impact & Dr. Incubis", based on original texts by Nicolas Yazgi and directed by Frédéric Ozier. More than literal adaptations, the plays address violence, death, crime and fear in contemporary contexts, while revisiting many trope of the original Grand Guignol corpus, often with humour.
Recently formed London-based Grand Guignol company Theatre of the Damned, brought their first production to the Camden Fringe in 2010 and produced the award nominated "Grand Guignol" in November of that year. In 2011, they staged "Revenge of the Grand Guignol" at the Courtyard Theatre, London, as part of the London Horror Festival.
Also based in London, Le Nouveau Guignol form the UK's only permanent repertory Grand Guignol company; plays within their current repertoire include French Guignol classics such as "The Final Kiss", "Tics... Or Doing the Deed", "The Lighthouse Keepers", "Private Room Number Six" and "The Kiss of Blood". However, as their company remit also includes encouraging new writing, they have also staged several new plays in the Grand-Guignol style, including "Eating For Two", "Penalty" and "Ways and Means". Le Nouveau Guignol took part in the London Horror Festival alongside Theatre of the Damned at Courtyard Theatre in November 2011.
The Grand Guignolers (http://www.grandguignolers.com) in Los Angeles, California, established in 2007, create and perform traditional Grand Guignol and original works as a 1920s Parisian theatre troupe. Shows are staged as event and draw on multiple forms of traditional physical theatrical genres in new ways including Commedia dell'arte, melodrama, mime, mask, clown, dance, vaudeville, magic, and puppetry. Many shows play with the literal 'big puppet' theme utilizing puppetry at various levels including the Petits Guignolers, a French existentially lewd finger puppet show with Grand Guignol effects. The company paid homage to its namesake by staging a surprise 200th birthday party for Guignol with "A Grand Guignol Children's Show* (*NOT for Children)." In addition to evenings of classic Grand Guignol, original productions have included "A Very Grand Guignol Christmas" and "Absinthe, Opium and Magic, 1920s Shanghai." The company has performed at ArtWorks, the Actors' Gang and the Pasadena Playhouse. The company offers workshops in melodrama and Grand Guignol led by artistic director, Debbie McMahon.
August–October, 2013 The Xoregos Performing Company presented "Danse Macabre", a contemporary tribute to Grand Guignol at Theater for the New City in New York City. "Danse Macabre" was a program of four plays of psychological and physical terror, one humorous work and a dance, in keeping with Grand Guignol's programming history. The run ended October 27 at the Bronx Museum of the Arts. The playwrights were Dave deChristopher, Jack Feldstein, Dylan Guy, Pamela Scott and Joel Trinidad. Choreographer and Director, Shela Xoregos.
In November 2014, 86 years after the last show of Alfredo Sainati's "La Compagnia del Grand-Guignol", founded in 1908 and which had been the only example of Grand Guignol in Italy, the Convivio d'Arte Company (http://www.conviviodarte.it) presented in Milan "Grand Guignol de Milan: Le Cabaret des Vampires". 
The show was an original tribute to Grand Guignol, a horror vaudeville with various horror and grotesque performances such as monologues, live music and burlesque, with a satirical black humour conduction.
The Japanese music group ALI Project created the song "Gesshoku Grand Guignol" as the opening for the Bee Train anime "Avenger", while British rock band Duels also named an instrumental track after the theatre.
The second album of Austrian horror punk band Bloodsucking Zombies From Outer Space is called "A Night at Grand Guignol" (2005). "Grand Guignol" is also the name of the first track in Bajofondo's second album "Mar Dulce".
While the original Grand Guignol attempted to present naturalistic horror, the performances would seem melodramatic and heightened to today's audience. For this reason, the term is often applied to films and plays of a stylised nature with heightened acting, melodrama and theatrical effects such as "", "Sleepy Hollow", "Quills", and the Hammer Horror films that went before them.
"What Ever Happened to Baby Jane?"; "Hush... Hush, Sweet Charlotte"; "What Ever Happened to Aunt Alice?"; "What's the Matter with Helen?"; "Night Watch" and "Whoever Slew Auntie Roo?" form a sub-branch of the genre called Grande Dame Guignol for its use of aging A-list actresses in sensational horror films.
Kaori Yuki's manga Grand Guignol Orchestra is named for the locale.
In October 2002, synth group Soft Cell released the album "Beauty Without Cruelty" featuring the track "Le Grand Guignol".
A London version of the theatre is featured prominently throughout Season 1 of the Showtime series Penny Dreadful (TV series), which premiered in 2014. The series' soundtrack features a song for the theatre title "Welcome to the Grand Guignol".
In Brazil, the group Vigor Mortis has been producing theatre gory plays, as well as movies and comics of horror and horror comedy.

</doc>
<doc id="12286" url="https://en.wikipedia.org/wiki?curid=12286" title="Great Plague of London">
Great Plague of London

The Great Plague, lasting from 1665 to 1666, was the last major epidemic of the bubonic plague to occur in England. It happened within the centuries-long time period of the Second Pandemic, an extended period of intermittent bubonic plague epidemics which began in Europe in 1347, the first year of the Black Death, an outbreak which included other forms such as pneumonic plague, and lasted until 1750.
The Great Plague killed an estimated 100,000 people, almost a quarter of London's population. Plague is caused by the "Yersinia pestis" bacterium, which is usually transmitted through the bite of an infected rat flea.
The 1665–66 epidemic was on a far smaller scale than the earlier Black Death pandemic; it was remembered afterwards as the "great" plague mainly because it was the last widespread outbreak of bubonic plague in England during the 400-year timespan of the Second Pandemic.
London in 1665.
The plague had been a recurring problem in 17th century London. There had been 30,000 deaths due to the plague in 1603, 35,000 in 1625, and 10,000, as well as smaller numbers in other years.
During the winter of 1664, a bright comet was to be seen in the sky and the people of London were fearful, wondering what evil event it portended. London at that time consisted of a city of about 448 acres surrounded by a city wall, which had originally been built to keep out raiding bands. There were gates at Ludgate, Newgate, Aldersgate, Cripplegate, Moorgate and Bishopsgate and to the south lay the River Thames and London Bridge. In the poorer parts of the city, hygiene was impossible to maintain in the overcrowded tenements and garrets. There was no sanitation, and open drains flowed along the centre of winding streets. The cobbles were slippery with animal dung, rubbish and the slops thrown out of the houses, muddy and buzzing with flies in summer and awash with sewage in winter. The City Corporation employed "rakers" to remove the worst of the filth and it was transported to mounds outside the walls where it accumulated and continued to decompose. The stench was overwhelming and people walked around with handkerchiefs or nosegays pressed against their nostrils.
Some of the city's necessities such as coal arrived by barge, but most came by road. Carts, carriages, horses and pedestrians were crowded together and the gateways in the wall formed bottlenecks through which it was difficult to progress. The nineteen-arch London Bridge was even more congested. The better-off used hackney carriages and sedan chairs to get to their destinations without getting filthy. The poor walked, and might be splashed by the wheeled vehicles and drenched by slops being thrown out and water falling from the overhanging roofs. Another hazard was the choking black smoke belching forth from factories which made soap, from breweries and iron smelters and from about 15,000 houses burning coal.
Outside the city walls, suburbs had sprung up providing homes for the craftsmen and tradespeople who flocked to the already overcrowded city. These were shanty towns with wooden shacks and no sanitation. The government had tried to control this development but had failed and over a quarter of a million people lived here. Other immigrants had taken over fine town houses, vacated by Royalists who had fled the country during the Commonwealth, converting them into tenements with different families in every room. These properties were soon vandalised and became rat-infested slums.
Administration of the City of London was organised by the Lord Mayor, Aldermen and common councillors, but not all of the inhabited area generally comprising London was legally part of the City. Both inside the City and outside its boundaries there were also Liberties, which were areas of varying sizes which historically had been granted rights to self-government. Many had been associated with religious institutions, and when these were abolished in the Dissolution of the Monasteries, their historic rights were transferred along with their property to new owners. The walled City was surrounded by a ring of Liberties which had come under its authority, contemporarily called 'the City and Liberties', but these were surrounded by further suburbs with varying administrations. Westminster was an independent town with its own liberties, although it was joined to London by urban development. The Tower of London was an independent liberty, as were others. Areas north of the river not part of one of these administrations came under the authority of the county of Middlesex, and south of the river under Surrey.
At that time, bubonic plague was a much feared disease but its cause was not understood. The credulous blamed emanations from the earth, "pestilential effluviums", unusual weather, sickness in livestock, abnormal behaviour of animals or an increase in the numbers of moles, frogs, mice or flies. It was not until 1894 that the identification by Alexandre Yersin of its causal agent "Yersinia pestis" was made and the transmission of the bacterium by rat fleas became known.
The recording of deaths.
In order to judge the severity of an epidemic, it is first necessary to know how big the population was in which it occurred. There was no official census of the population to provide this figure, and the best contemporary count comes from the work of John Graunt (1620–1674), who was one of the earliest Fellows of the Royal Society and one of the first demographers, bringing a scientific approach to the collection of statistics. In 1662, he estimated that 384,000 people lived in the City of London, the Liberties, Westminster and the out-parishes, based on figures in the bills of mortality published each week in the capital. These different districts with different administrations constituted the officially recognised extent of London as a whole. In 1665, he revised his estimate to 'not above 460,000'. Other contemporaries put the figure higher, (the French Ambassador, for example, suggested 600,000) but with no mathematical basis to support their estimates. The next largest city in the kingdom was Norwich, with of population of 30,000.
There was no duty to report a death to anyone in authority. Instead, each parish appointed two or more 'searchers of the dead', whose duty was to inspect a corpse and determine the cause of death. A searcher was entitled to charge a small fee from relatives for each death they reported, and so habitually the parish would appoint someone to the post who would otherwise be destitute and would be receiving support from the parish poor rate. Typically, this meant searchers would be old women who were illiterate, might know little about identifying diseases and who would be open to dishonesty. Searchers would typically learn about a death either from the local sexton who had been asked to dig a grave, or from the tolling of a church bell. Anyone who did not report a death to their local church, such as Quakers, Anabaptists, other non-Anglican Christians or Jews, frequently did not get included in the official records. Searchers during times of plague were required to live apart from the community and stay indoors except when performing their duties, for fear of spreading the diseases. Outside they should avoid other people and always carry a white stick to warn of their occupation. Searchers reported to the Parish Clerk, who made a return each week to the Company of Parish Clerks in Brode Lane. Figures were then passed to the Lord Mayor and then to the Minister of State once plague became a matter of national concern. The reported figures were used to compile the Bills of Mortality, which listed total deaths in each parish and whether by plague. The system of Searchers to report the cause of death continued until 1836.
Graunt recorded the incompetence of the Searchers at identifying true causes of death, remarking on the frequent recording of 'consumption' rather than other diseases which were recognised then by physicians. He suggested a cup of ale and a doubling of their fee to two groats rather than one was sufficient for Searchers to change the cause of death to one more convenient for the householders. No one wished to be known as having had a death by plague in their household, and Parish Clerks, too, connived in covering up cases of plague in their official returns. Analysis of the Bills of Mortality during the months plague took hold shows a rise in deaths other than by plague well above the average death rate, which has been attributed to misrepresentation of the true cause of death. As plague spread, a system of quarantine was introduced, whereby any house where someone had died from plague would be locked up and no one allowed to enter or leave for 40 days. This frequently led to the deaths of the other inhabitants, by neglect if not from plague, and provided ample incentive not to report the disease. The official returns record 68,596 cases of plague, but a reasonable estimate suggests this figure is 30,000 short of the true total. A plague house was marked with a red cross on the door with the words "Lord have mercy upon us", and a watchman stood guard outside.
Preventative measures.
Reports of plague around Europe began to reach England in the 1660s, causing the Privy Council to consider what steps might be taken to prevent it crossing to England. Quarantining of ships had been used during previous outbreaks and was again introduced for ships coming to London in November 1663, following outbreaks in Amsterdam and Hamburg. Two naval ships were assigned to intercept any vessels entering the Thames estuary. Ships from infected ports were required to moor at Hole Haven on Canvey Island for a period of 30 days before being allowed to travel upriver. Ships from ports free of plague or completing their quarantine were given a certificate of health and allowed to travel on. A second inspection line was established between the forts on opposite banks of the Thames at Tilbury and Gravesend with instructions only to pass ships with a certificate.
The duration of quarantine was increased to forty days in May 1664 as the continental plague worsened, and the areas subject to quarantine changed with the news of the spread of plague to include all of Holland, Zeeland and Friesland (all regions of the Dutch Republic), although restrictions on Hamburg were removed in November. Quarantine measures against ships coming from the Dutch Republic were put in place in 29 other ports from May, commencing with Great Yarmouth. The Dutch ambassador objected at the constraint of trade with his country, but England responded that it had been one of the last countries introducing such restrictions. Regulations were enforced quite strictly, so that people or houses where voyagers had come ashore without serving their quarantine were also subjected to 40 days quarantine.
Outbreak.
Plague had been one of the hazards of life in Britain ever since its dramatic appearance in 1347 with the Black Death. The Bills of Mortality began to be published regularly in 1603, in which year 33,347 deaths were recorded from plague. Between then and 1665, only four years had no recorded cases. In 1563, a thousand people were reportedly dying in London each week. In 1593, there were 15,003 deaths, 1625 saw 41,313 dead, between 1640 and 1646 came 11,000 deaths, culminating in 3,597 for 1647. The 1625 outbreak was recorded at the time as the 'Great Plague', until 1665 surpassed it. These official figures are likely to under-report actual numbers.
Early days.
Although plague was known, it was still sufficiently uncommon that medical practitioners might have had no personal experience of seeing the disease; medical training varied from those who had attended the college of physicians, to apothecaries who also acted as modern doctors, to simple charlatans. Other diseases abounded, such as an outbreak of smallpox the year before, and these uncertainties all added to difficulties identifying the true start of the epidemic. Contemporary accounts suggest cases of plague occurred through the winter of 1664/5, some of which were fatal but a number of which did not display the virulence of the later epidemic. The winter was cold, the ground frozen from December to March, river traffic on the Thames twice blocked by ice, and it may be that the cold weather held back its spread.
This outbreak of bubonic plague in England is thought to have spread from the Netherlands, where the disease had been occurring intermittently since 1599. It is unclear exactly where the disease first struck but the initial contagion may have arrived with Dutch trading ships carrying bales of cotton from Amsterdam, which was ravaged by the disease in 1663–1664, with a mortality given of 50,000. The first areas to be struck are believed to be the dock areas just outside London, and the parish of St Giles in the Fields. In both of these localities, poor workers were crowded into ill-kept structures. Two suspicious deaths were recorded in St. Giles parish in December 1664 and another in February 1665. These did not appear as plague deaths on the Bills of Mortality, so no control measures were taken by the authorities, but the total number of people dying in London during the first four months of 1665 showed a marked increase. By the end of April, only four plague deaths had been recorded, two in the parish of St. Giles, but total deaths per week had risen from around 290 to 398.
Although there had been only three official cases in April, which level of plague in earlier years had not induced any official response, the Privy Council now acted to introduce household quarantine. Justices of the Peace in Middlesex were instructed to investigate any suspected cases and to shut up the house if it was confirmed. Shortly after, a similar order was issued by the King's Bench to the City and Liberties. A riot broke out in St. Giles when the first house was sealed up; the crowd broke down the door and released the inhabitants. Rioters caught were punished severely. Instructions were given to build pest-houses, which were essentially isolation hospitals built away from other people where the sick could be cared for (or stay until they died). This official activity suggests that despite the few recorded cases, the government was already aware that this was a serious outbreak of plague.
With the arrival of warmer weather, the disease began to take a firmer hold. In the week 2–9 May, there were three recorded deaths in the parish of St Giles, four in neighbouring St Clement Danes and one each in St Andrew, Holborn and St Mary Woolchurch Haw. Only the last was actually inside the city walls. A Privy Council committee was formed to investigate methods to best prevent the spread of plague, and measures were introduced to close some of the ale houses in affected areas and limit the number of lodgers allowed in a household. In the city, the Lord Mayor issued a proclamation that all householders must diligently clean the streets outside their property, which was a householder's responsibility, not a state one (the city employed scavengers and rakers to remove the worst of the mess). Matters just became worse, and Aldermen were instructed to find and punish those failing their duty. As cases in St. Giles began to rise, an attempt was made to quarantine the area and constables were instructed to inspect everyone wishing to travel and contain inside vagrants or suspect persons.
People began to be alarmed. Samuel Pepys, who had an important position at the Admiralty, stayed in London and provided a contemporary account of the plague through his diary. On 30 April he wrote: "Great fears of the sickness here in the City it being said that two or three houses are already shut up. God preserve us all!" Another source of information on the time is a fictional account, "A Journal of the Plague Year", which was written by Daniel Defoe and published in 1722. He was only six when the plague struck but made use of his family's recollections (his uncle was a saddler in East London and his father a butcher in Cripplegate), interviews with survivors and sight of such official records as were available.
Exodus from the city.
By July 1665, plague was rampant in the City of London. King Charles II of England, his family and his court left the city for Salisbury, moving on to Oxford in September when some cases of plague occurred in Salisbury. The aldermen and most of the other city authorities opted to stay at their posts. The Lord Mayor of London, Sir John Lawrence, also decided to stay in the city. Businesses were closed when merchants and professionals fled. Defoe wrote "Nothing was to be seen but wagons and carts, with goods, women, servants, children, coaches filled with people of the better sort, and horsemen attending them, and all hurrying away". As the plague raged throughout the summer, only a small number of clergymen, physicians and apothecaries remained to cope with an increasingly large number of victims. Edward Cotes, author of "London's Dreadful Visitation", expressed the hope that "Neither the Physicians of our Souls or Bodies may hereafter in such great numbers forsake us".
The poorer people were also alarmed by the contagion and some left the city, but it was not easy for them to abandon their accommodation and livelihoods for an uncertain future elsewhere. Before exiting through the city gates, they were required to possess a certificate of good health signed by the Lord Mayor and these became increasingly difficult to obtain. As time went by and the numbers of plague victims rose, people living in the villages outside London began to resent this exodus and were no longer prepared to accept townsfolk from London, with or without a certificate. The refugees were turned back, were not allowed to pass through towns and had to travel across country, and were forced to live rough on what they could steal or scavenge from the fields. Many died in wretched circumstances of starvation and thirst in the hot summer that was to follow.
Height of the epidemic.
In the last week of July, the London Bill of Mortality showed 3,014 deaths, of which 2,020 had died from the plague. The number of deaths as a result of plague may have been underestimated, as deaths in other years in the same period were much lower, at around 300. As the number of victims affected mounted up, burial grounds became overfull, and pits were dug to accommodate the dead. Drivers of dead-carts travelled the streets calling "Bring out your dead" and carted away piles of bodies. The authorities became concerned that the number of deaths might cause public alarm and ordered that body removal and interment should take place only at night. As time went on, there were too many victims, and too few drivers, to remove the bodies which began to be stacked up against the walls of houses. Daytime collection was resumed and the plague pits became mounds of decomposing corpses. In the parish of Aldgate, a great hole was dug near the churchyard, fifty feet long and twenty feet wide. Digging was continued by labourers at one end while the dead-carts tipped in corpses at the other. When there was no room for further extension it was dug deeper until ground water was reached at twenty feet. When finally covered with earth it housed 1,114 corpses.
Plague doctors traversed the streets diagnosing victims, although many of them had no formal medical training. Several public health efforts were attempted. Physicians were hired by city officials and burial details were carefully organized, but panic spread through the city and, out of the fear of contagion, people were hastily buried in overcrowded pits. The means of transmission of the disease were not known but thinking they might be linked to the animals, the City Corporation ordered a cull of dogs and cats. This decision may have affected the length of the epidemic since those animals could have helped keep in check the rat population carrying the fleas which transmitted the disease. Thinking bad air was involved in transmission, the authorities ordered giant bonfires to be burned in the streets and house fires to be kept burning night and day, in hopes that the air would be cleansed. Tobacco was thought to be a prophylactic and it was later said that no London tobacconist had died from the plague during the epidemic.
Trade and business had completely dried up, and the streets were empty of people except for the dead-carts and the desperate dying victims. That people did not starve was down to the foresight of Sir John Lawrence and the Corporation of London who arranged for a commission of one farthing to be paid above the normal price for every quarter of corn landed in the Port of London. Another food source was the villages around London which, denied of their usual sales in the capital, left vegetables in specified market areas, negotiated their sale by shouting, and collected their payment after the money had been left submerged in a bucket of water to "disinfect" the coins.
Records state that plague deaths in London and the suburbs crept up over the summer from 2,000 people per week to over 7,000 per week in September. These figures are likely to be a considerable underestimate. Many of the sextons and parish clerks who kept the records themselves died. Quakers refused to co-operate and many of the poor were just dumped into mass graves unrecorded. It is not clear how many people caught the disease and made a recovery because only deaths were recorded and many records were destroyed in the Great Fire of London the following year. In the few districts where intact records remain, plague deaths varied between 30% and over 50% of the total population.
Though concentrated in London, the outbreak affected other areas of the country as well. Perhaps the most famous example was the village of Eyam in Derbyshire. The plague allegedly arrived with a merchant carrying a parcel of cloth sent from London, although this is a disputed point. The villagers imposed a quarantine on themselves to stop the further spread of the disease. This prevented the disease from moving into surrounding areas but the cost to the village was the death of around 80% of its inhabitants over a period of fourteen months.
Aftermath.
By late autumn, the death toll in London and the suburbs began to slow until, in February 1666, it was considered safe enough for the King and his entourage to come back to the city. With the return of the monarch, others began to return: The gentry returned in their carriages accompanied by carts piled high with their belongings. The judges moved back from Windsor to sit in Westminster Hall, although Parliament, which had been prorogued in April 1665, did not reconvene until September 1666. Trade recommenced and businesses and workshops opened up. London was the goal of a new wave of people who flocked to the city in expectation of making their fortunes. Writing at the end of March 1666, Lord Clarendon, the Lord Chancellor, stated "... the streets were as full, the Exchange as much crowded, the people in all places as numerous as they had ever been seen ...".
Plague cases continued to occur sporadically at a modest rate until the summer of 1666. On the second and third of September that year, the Great Fire of London destroyed much of the City of London, and some people believed that the fire put an end to the epidemic. However, it is now thought that the plague had largely subsided before the fire took place. In fact, most of the later cases of plague were found in the suburbs, and it was the City of London itself that was destroyed by the Fire.
According to the Bills of Mortality, there were in total 68,596 deaths in London from the plague in 1665. Lord Clarendon estimated that the true number of mortalities was probably twice that figure. The next year, 1666, saw further deaths in other cities but on a lesser scale. Dr Thomas Gumble, chaplain to the Duke of Albermarle, both of whom had stayed in London for the whole of the epidemic, estimated that the total death count for the country from plague during 1665 and 1666 was about 200,000.
The Great Plague of 1665/1666 was the last major outbreak of bubonic plague in Great Britain. The last recorded death from plague came in 1679, and it was removed as a specific category in the Bills of Mortality after 1703. It spread to other towns in East Anglia and the southeast of England but fewer than ten percent of parishes outside London had a higher than average death rate during those years. Urban areas were more affected than rural ones; Norwich, Ipswich, Colchester, Southampton and Winchester were badly affected, while the West of England and areas of the Midlands escaped altogether.
The population of England in 1650 was approximately 5.25 million, which declined to about 4.9 million by 1680, recovering to just over 5 million by 1700. Other diseases, such as smallpox, took a high toll on the population even without the contribution by plague. The higher death rate in cities, both generally and specifically from the plague, was made up by continuous immigration, from small towns to larger ones and from the countryside to the town.
There were no contemporary censuses of London's population, but available records suggest that the population returned to its previous level within a couple of years. Burials in 1667 had returned to 1663 levels, Hearth Tax returns had recovered, John Graunt contemporarily analysed baptism records and concluded they represented a recovered population. Part of this could be accounted for by the return of wealthy households, merchants and manufacturing industries, all of which needed to replace losses amongst their staff and took steps to bring in necessary people. Colchester had suffered more severe depopulation, but manufacturing records for cloth suggested that production had recovered or even increased by 1669, and the total population had nearly returned to pre-plague levels by 1674. Other towns did less well: Ipswich was affected less than Colchester, but in 1674, its population had dropped by 18%, more than could be accounted for by the plague deaths alone.
As a proportion of the population who died, the London death toll was less severe than in a number of other towns. The total of deaths in London was greater than in any previous outbreak for 100 years, though as a proportion of the population, the epidemics in 1563, 1603 and 1625 were comparable or greater. Perhaps around 2.5% of the English population died.
Impact.
The plague in London largely affected the poor, as the rich were able to leave the city by either retiring to their country estates or residing with kin in other parts of the country. The subsequent Great Fire of London, however, ruined many city merchants and property owners. As a result of these events, London was largely rebuilt and Parliament enacted the Rebuilding of London Act 1666. Although the street plan of the capital remained relatively unchanged, some improvements were made: streets were widened, pavements were created, open sewers abolished, wooden buildings and overhanging gables forbidden, and the design and construction of buildings controlled. The use of brick or stone was mandatory and many gracious buildings were constructed. Not only was the capital rejuvenated, but it became a healthier environment in which to live. Londoners had a greater sense of community after they had overcome the great adversities of 1665 and 1666.
Rebuilding took over ten years and was supervised by Robert Hooke as Surveyor of London. The architect Sir Christopher Wren was involved in the rebuilding of St Paul's Cathedral and more than fifty London churches. King Charles ll did much to foster the rebuilding work. He was a patron of the arts and sciences and founded the Royal Observatory and supported the Royal Society, a scientific group whose early members included Robert Hooke, Robert Boyle and Sir Isaac Newton. In fact, out of the fire and pestilence flowed a renaissance in the arts and sciences in England.

</doc>
<doc id="12293" url="https://en.wikipedia.org/wiki?curid=12293" title="Graphical user interface">
Graphical user interface

In computer science, a graphical user interface or GUI, pronounced ("gooey") is a type of interface that allows users to interact with electronic devices through graphical icons and visual indicators such as secondary notation, as opposed to text-based interfaces, typed command labels or text navigation. GUIs were introduced in reaction to the perceived steep learning curve of command-line interfaces (CLIs), which require commands to be typed on the keyboard.
The actions in a GUI are usually performed through direct manipulation of the graphical elements. In addition to computers, GUIs can be found in hand-held devices such as MP3 players, portable media players, gaming devices, smartphones and smaller household, office and industrial equipment. The term "GUI" tends not to be applied to other low-resolution types of interfaces with display resolutions, such as video games (where HUD is preferred), or not restricted to flat screens, like volumetric displays because the term is restricted to the scope of two-dimensional display screens able to describe generic information, in the tradition of the computer science research at the PARC (Palo Alto Research Center).
User interface and interaction design.
Designing the visual composition and temporal behavior of a GUI is an important part of software application programming in the area of human-computer interaction. Its goal is to enhance the efficiency and ease of use for the underlying logical design of a stored program, a design discipline known as usability. Methods of user-centered design are used to ensure that the visual language introduced in the design is well tailored to the tasks.
The visible graphical interface features of an application are sometimes referred to as "chrome" or "GUI" ("gooey"). Typically, the user interacts with information by manipulating visual widgets that allow for interactions appropriate to the kind of data they hold. The widgets of a well-designed interface are selected to support the actions necessary to achieve the goals of the user. A model-view-controller allows for a flexible structure in which the interface is independent from and indirectly linked to application functionality, so the GUI can be easily customized. This allows the user to select or design a different "skin" at will, and eases the designer's work to change the interface as the user needs evolve. Good user interface design relates to the user, not the system architecture.
Large widgets, such as windows, usually provide a frame or container for the main presentation content such as a web page, email message or drawing. Smaller ones usually act as a user-input tool.
A GUI may be designed for the requirements of a vertical market as application-specific graphical user interfaces. Examples of application-specific GUIs include automated teller machines (ATM), point-of-sale touchscreens at restaurants, self-service checkouts used in a retail store, airline self-ticketing and check-in, information kiosks in a public space, like a train station or a museum, and monitors or control screens in an embedded industrial application which employ a real time operating system (RTOS).
The latest cell phones and handheld game systems also employ application specific touchscreen GUIs. Newer automobiles use GUIs in their navigation systems and touch screen multimedia centers.
Components.
A GUI uses a combination of technologies and devices to provide a platform that the user can interact with, for the tasks of gathering and producing information.
A series of elements conforming a visual language have evolved to represent information stored in computers. This makes it easier for people with few computer skills to work with and use computer software. The most common combination of such elements in GUIs is the WIMP ("window, icon, menu, pointing device") paradigm, especially in personal computers.
The WIMP style of interaction uses a virtual input device to control the position of a pointer, most often a mouse, and presents information organized in windows and represented with icons. Available commands are compiled together in menus, and actions are performed making gestures with the pointing device. A window manager facilitates the interactions between windows, applications, and the windowing system. The windowing system handles hardware devices such as pointing devices and graphics hardware, as well as the positioning of the pointer.
In personal computers, all these elements are modeled through a desktop metaphor to produce a simulation called a desktop environment in which the display represents a desktop, upon which documents and folders of documents can be placed. Window managers and other software combine to simulate the desktop environment with varying degrees of realism.
Post-WIMP interfaces.
Smaller mobile devices such as PDAs and smartphones typically use the WIMP elements with different unifying metaphors, due to constraints in space and available input devices. Applications for which WIMP is not well suited may use newer interaction techniques, collectively named as post-WIMP user interfaces.
As of 2011, some touchscreen-based operating systems such as Apple's iOS (iPhone) and Android use the class of GUIs named post-WIMP. These support styles of interaction using more than one finger in contact with a display, which allows actions such as pinching and rotating, which are unsupported by one pointer and mouse.
Interaction.
Human interface devices, for the efficient interaction with a GUI include a computer keyboard, especially used in conjunction with keyboard shortcuts, pointing devices for the cursor (or rather pointer) control: mouse, pointing stick, touchpad, trackball, joystick, etc., virtual keyboards, and head-up displays (translucent information devices at the eye level).
There are also actions performed by programs that affect the GUI. For example, there are components like inotify or D-Bus to facilitate the communication of computer programs with each other.
History.
Precursors.
A precursor to GUIs was invented by researchers at the Stanford Research Institute, led by Douglas Engelbart. They developed the use of text-based hyperlinks manipulated with a mouse for the On-Line System (NLS). The concept of hyperlinks was further refined and extended to graphics by researchers at Xerox PARC and specifically Alan Kay, who went beyond text-based hyperlinks and used a GUI as the primary interface for the Xerox Alto computer, released in 1973. Most modern general-purpose GUIs are derived from this system.
Ivan Sutherland developed a pointer-based system called the Sketchpad in 1963. It used a light-pen to guide the creation and manipulation of objects in engineering drawings.
PARC user interface.
The PARC user interface consisted of graphical elements such as windows, menus, radio buttons, and check boxes. The concept of icons was later introduced by David Smith, who had written a thesis on the subject under the guidance of Kay. The PARC user interface employs a pointing device in addition to a keyboard. These aspects can be emphasized by using the alternative acronym WIMP, which stands for "windows", "icons", "menus" and "pointing device".
Evolution.
Following PARC the first GUI-centric computer operating model was the Xerox 8010 Star Information System in 1981, followed by the Apple Lisa (which presented the concept of menu bar as well as window controls) in 1983, the Apple Macintosh 128K in 1984, and the Atari ST and Commodore Amiga in 1985.
Visi On was released in 1983 for the IBM PC compatible computers, but didn't became popular due to its high hardware demands. Nevertheless, it was a crucial influence on the contemporary development of Microsoft Windows.
Apple, IBM and Microsoft used many of Xerox's ideas to develop products, and IBM's Common User Access specifications formed the basis of the user interface found in Microsoft Windows, IBM OS/2 Presentation Manager, and the Unix Motif toolkit and window manager. These ideas evolved to create the interface found in current versions of Microsoft Windows, as well as in various desktop environments for Unix-like operating systems, such as Mac OS X and Linux. Thus most current GUIs have largely common idioms.
Popularization.
GUIs were a hot topic in the early 1980s. The Apple Lisa was released in 1983 and various windowing systems existed for MS-DOS. Individual applications for a number of platforms presented their own take on the GUI. Despite the GUIs advantages, many reviewers questioned the value of the entire concept, citing hardware limitations as well as the difficulty in finding compatible software.
In 1984, Apple released a television commercial which introduced the Apple Macintosh during the telecast of Super Bowl XVIII by CBS, with allusions to George Orwell's noted novel, "Nineteen Eighty-Four". The commercial was aimed at making people think about computers, identifying the user-friendly interface as a personal computer which departed from previous business-oriented systems, and becoming a signature representation of Apple products.
Accompanied by an extensive marketing campaign, Windows 95 was a major success in the marketplace at launch and shortly became the most popular desktop operating system.
In 2007, with the iPhone and later in 2010 with the introduction of the iPad, Apple popularized the post-WIMP style of interaction for multi-touch screens, and those devices were considered to be milestones in the development of mobile devices.
The GUIs familiar to most people as of the mid-2010s are Microsoft Windows, Mac OS X, and the X Window System interfaces for desktop and laptop computers, and Apple's iOS, Android, Symbian, BlackBerry OS, Windows Phone, Palm OSWeb OS, and Firefox OS for handheld ("smartphone") devices.
Comparison to other interfaces.
Command-line interfaces.
Since the commands available in command line interfaces can be numerous, complicated operations can be completed using a short sequence of words and symbols. This allows for greater efficiency and productivity once many commands are learned, but reaching this level takes some time because the command words may not be easily discoverable or mnemonic. In addition, using the command line can become slow and error-prone when the user needs to enter very long commands comprising many parameters and/or several different filenames at once. WIMPs ("window, icon, menu, pointing device"), on the other hand, present the user with numerous widgets that represent and can trigger some of the system's available commands.
On the other hand, GUIs can be made quite hard by burying dialogs deep in the system, or moving dialogs from place to place. Also, dialog boxes are considerably harder for the user to script.
WIMPs extensively use modes as the meaning of all keys and clicks on specific positions on the screen are redefined all the time. Command line interfaces use modes only in limited forms, such as the current directory and environment variables.
Most modern operating systems provide both a GUI and some level of a CLI, although the GUIs usually receive more attention. The GUI is usually WIMP-based, although occasionally other metaphors surface, such as those used in Microsoft Bob, 3dwm or File System Visualizer (FSV).
GUI wrappers.
Graphical user interface (GUI) wrappers circumvent the command-line interface versions (CLI) of (typically) Linux and Unix-like software applications and their text-based interfaces or typed command labels. While command-line or text-based application allow users to run the program non-interactively, GUIs wrappers on top of them avoid the steep learning curve of the command-line, which requires commands to be typed on the keyboard. By starting a GUI wrapper, users can intuitively interact with polipo, start, stop, and change its working parameters, through graphical icons and visual indicators of a desktop environment, for example.
Applications may also provide both interfaces, and when they do the GUI is usually a WIMP wrapper around the command-line version. This is especially common with applications designed for Unix-like operating systems. The latter used to be implemented first because it allowed the developers to focus exclusively on their product's functionality without bothering about interface details such as designing icons and placing buttons. Designing programs this way also allows users to run the program in a shell script. An example of this basic design could be the specialized polipo command-line web proxy server, which has some connected GUI wrapper projects, e.g. for Windows OS ("solipo"), Mac OS X ("dolipo"), and Android ("polipoid").
Three-dimensional user interfaces.
For typical computer displays, "three-dimensional" is a misnomer—their displays are two-dimensional. Semantically, however, most graphical user interfaces use three dimensions – in addition to height and width, they offer a third dimension of layering or stacking screen elements over one another. This may be represented visually on screen through an illusionary transparent effect, which offers the advantage that information in background windows may still be read, if not interacted with. Or the environment may simply hide the background information, possibly making the distinction apparent by drawing a drop shadow effect over it.
Some environments use the methods of 3D graphics to project virtual three dimensional user interface objects onto the screen. These are often shown in use in sci-fi films (see below for examples). As the processing power of computer graphics hardware increases, this becomes less of an obstacle to a smooth user experience.
Three-dimensional graphics are currently mostly used in computer games, art and computer-aided design (CAD). A three-dimensional computing environment could also be useful in other scenarios, like molecular graphics and aircraft design.
Several attempts have been made to create a multi-user three-dimensional environment, including the Croquet Project and Sun's Project Looking Glass.
Technologies.
The use of three-dimensional graphics has become increasingly common in mainstream operating systems, from creating attractive interfaces—eye candy— to functional purposes only possible using three dimensions. For example, user switching is represented by rotating a cube whose faces are each user's workspace, and window management is represented via a Rolodex-style flipping mechanism in Windows Vista (see Windows Flip 3D). In both cases, the operating system transforms windows on-the-fly while continuing to update the content of those windows.
Interfaces for the X Window System have also implemented advanced three-dimensional user interfaces through compositing window managers such as Beryl, Compiz and KWin using the AIGLX or XGL architectures, allowing for the usage of OpenGL to animate the user's interactions with the desktop.
Another branch in the three-dimensional desktop environment is the three-dimensional GUIs that take the desktop metaphor a step further, like the BumpTop, where a user can manipulate documents and windows as if they were "real world" documents, with realistic movement and physics.
The Zooming User Interface (ZUI) is a related technology that promises to deliver the representation benefits of 3D environments without their usability drawbacks of orientation problems and hidden objects. It is a logical advancement on the GUI, blending some three-dimensional movement with two-dimensional or "2.5D" vector objects. In 2006, Hillcrest Labs introduced the first zooming user interface for television.
In science fiction.
Three-dimensional GUIs appeared in science fiction literature and movies before they were technically feasible or in common use. For example; the 1993 American film "Jurassic Park" features Silicon Graphics' three-dimensional file manager File System Navigator, a real-life file manager for Unix operating systems. The film Minority Report has scenes of police officers using specialized 3d data systems. In prose fiction, three-dimensional user interfaces have been displayed as immersible environments like William Gibson's Cyberspace or Neal Stephenson's Metaverse. Many futuristic imaginings of user interfaces rely heavily on object-oriented user interface (OOUI) style and especially object-oriented graphical user interface (OOGUI) style.

</doc>
<doc id="12295" url="https://en.wikipedia.org/wiki?curid=12295" title="Gamete">
Gamete

A gamete (from Ancient Greek γαμετή "gamete" from gamein "to marry") is a cell that fuses with another cell during fertilization (conception) in organisms that sexually reproduce. In species that produce two morphologically distinct types of gametes, and in which each individual produces only one type, a female is any individual that produces the larger type of gamete—called an ovum (or egg)—and a male produces the smaller tadpole-like type—called a sperm. This is an example of anisogamy or heterogamy, the condition in which females and males produce gametes of different sizes (this is the case in humans; the human ovum has approximately 100,000 times the volume of a single human sperm cell). In contrast, isogamy is the state of gametes from both sexes being the same size and shape, and given arbitrary designators for mating type. The name gamete was introduced by the Austrian biologist Gregor Mendel. Gametes carry half the genetic information of an individual, one ploidy of each type, and are created through meiosis.
Dissimilarity.
In contrast to a gamete, the diploid somatic cells of an individual contain one copy of the chromosome set from the sperm and one copy of the chromosome set from the egg cell; that is, the cells of the offspring have genes expressing characteristics of both the "father" and the "mother". A gamete's chromosomes are not exact duplicates of either of the sets of chromosomes carried in the diploid chromosomes, and often undergo random mutations resulting in modified DNA (and subsequently, new proteins and phenotypes).
Gender determination in humans and birds.
In humans, a normal ovum can carry only an X chromosome (of the X and Y chromosomes), whereas a sperm may carry either an X or a Y (a non-normal ovum can end up carrying two or no X chromosomes, as a result of a mistake at either of the two stages of meiosis, while a non-normal sperm cell can end up carrying either no sex-defining chromosomes,an XY pair, an XX pair as a result of the forementioned reason); ergo the male sperm can play a role in determining the gender of any resulting zygote, if the zygote has two X chromosomes it may develop into a female, if it has an X and a Y chromosome, it may develop into a male. For birds, the female ovum determines the sex of the offspring, through the ZW sex-determination system.
Artificial gametes.
Artificial gametes, also known as In vitro derived gametes (IVD), stem cell-derived gametes (SCDGs), and In vitro generated gametes (IVG), are gametes derived from stem cells. Research shows that artificial gametes may be a reproductive technique for same-sex male couples, although a surrogate mother would still be required for the gestation period. Women who have passed menopause may be able to produce eggs and bear genetically related children with artificial gametes. Robert Sparrow wrote, in the Journal of Medical Ethics, that embryos derived from artificial gametes could be used to derive new gametes and this process could be repeated to create multiple human generations in the laboratory. This technique could be used to create cell lines for medical applications and for studying the heredity of genetic disorders. Additionally, this technique could be used for human enhancement by selectively breeding for a desired genome or by using recombinant DNA technology to create enhancements that have not arisen in nature.
Plants.
Plants which reproduce sexually also have gametes. However, since plants have an alternation of diploid and haploid generations some differences exist. In flowering plants the flowers use meiosis to produce a haploid generation which produce gametes through mitosis. The female haploid is called the ovule and is produced by the ovary of the flower. When mature the haploid ovule produces the female gamete which are ready for fertilization. The male haploid is pollen and is produced by the anther, when pollen lands on a mature stigma of a flower it grows a pollen tube down into the flower. The haploid pollen then produces sperm by mitosis and releases them for fertilization.

</doc>
<doc id="12296" url="https://en.wikipedia.org/wiki?curid=12296" title="List of German proverbs">
List of German proverbs


</doc>
<doc id="12298" url="https://en.wikipedia.org/wiki?curid=12298" title="Godiva (programming language)">
Godiva (programming language)

Godiva (GOal-DIrected JaVA) is an extension to the Java programming language supporting goal-directed evaluation of expressions.

</doc>
<doc id="12300" url="https://en.wikipedia.org/wiki?curid=12300" title="George R. R. Martin">
George R. R. Martin

George Raymond Richard Martin (born George Raymond Martin; September 20, 1948), often referred to as GRRM, is an American novelist and short story writer in the fantasy, horror, and science fiction genres, a screenwriter, and television producer. He is best known for his international bestselling series of epic fantasy novels, "A Song of Ice and Fire", which was adapted into the HBO dramatic TV series "Game of Thrones".
Martin serves as the series' co-executive producer, and also scripted four episodes of the series. In 2005, Lev Grossman of "Time" called Martin "the American Tolkien", and the magazine later named him one of the "2011 Time 100," a list of the "most influential people in the world."
Early life.
George Raymond Martin (he later adopted the confirmation name Richard, at thirteen years old) was born on September 20, 1948, in Bayonne, New Jersey, the son of longshoreman Raymond Collins Martin and his wife Margaret Brady Martin. He has two younger sisters, Darleen and Janet. Martin's father was of half Italian descent, while his mother was of half Irish ancestry; he also has German, English, and French roots.
The family first lived in a house on Broadway, belonging to Martin's great-grandmother. In 1953, they moved to a federal housing project near the Bayonne docks. During Martin's childhood, his world consisted predominantly of "First Street to Fifth Street", between his grade school and his home; this limited world made him want to travel and experience other places, but the only way of doing so was through his imagination, so he became a voracious reader.
When Martin's family moved to a larger apartment after his sister was born, he also had a view of the waters of the Kill van Kull, where freighters and oil tankers flying flags from distant countries were entering and leaving Port Newark. He had an encyclopedia with a list of flags, and when using it to figure out where the ships came from, he would find himself dreaming of traveling to these remote locations. After the sun went down, the lights from Staten Island would shine across the water, which in his imagination was Shangri-La and "Shanghai and Paris, Timbuctoo and Kalamazoo, Marsport and Trantor, and all the other places that I'd never been and could never hope to go".
The young Martin began writing and selling monster stories for pennies to other neighborhood children, dramatic readings included. He also wrote stories about a mythical kingdom populated by his pet turtles; the turtles died frequently in their toy castle, so he finally decided they were killing each other off in "sinister plots".
Martin attended Mary Jane Donohoe School and then later Marist High School. While there he became an avid comic-book fan, developing a strong interest in the innovative superheroes being published by Marvel Comics. A letter Martin wrote to the editor of "Fantastic Four" was printed in issue No. 20 (Nov 1963); it was the first of many sent, e.g., "FF" #32, #34, and others from his family's home at 35 E. First Street, Bayonne, NJ. Fans who read his letters then wrote him letters in turn, and through such contacts, Martin joined the fledgling comics fandom of the era, writing fiction for various fanzines; he was the first to register for an early comic book convention held in New York in 1964. In 1965, Martin won comic fandom's Alley Award for his prose superhero story "Powerman vs. The Blue Barrier", the first of many awards he would go on to win for his fiction.
In 1970, Martin earned a B.S. in Journalism from Northwestern University, Evanston, Illinois, graduating "summa cum laude"; he went on to complete his M.S. in Journalism in 1971, also from Northwestern. Eligible for the draft during the Vietnam War, to which he objected, Martin applied for and obtained conscientious-objector status; he instead did alternative service work for two years (1972–1974) as a VISTA volunteer, attached to the Cook County Legal Assistance Foundation. An expert chess player, he also directed chess tournaments for the Continental Chess Association from 1973 to 1976.
Teaching.
In the mid-1970s, Martin met English professor George Guthridge from Dubuque, Iowa, at a science fiction convention in Milwaukee. Martin persuaded Guthridge (who confesses that at that time he despised science fiction and fantasy) not only to give speculative fiction a second look, but to write in the field himself. (Guthridge has since been a finalist for the Hugo Award and twice for the Nebula Award for science fiction and fantasy. In 1998, he won a Bram Stoker Award for best horror novel.)
In turn, Guthridge helped Martin find a job at Clarke University (then Clarke College). Martin "wasn't making enough money to stay alive", from writing and the chess tournaments, says Guthridge. From 1976 to 1978, Martin was an English and journalism instructor at Clarke, and he became Writer In Residence at the college from 1978 to 1979.
While he enjoyed teaching, the sudden death of friend and fellow author Tom Reamy in late 1977 made Martin reevaluate his own life, and he eventually decided to try to become a full-time writer. He resigned from his job, and being tired of the hard winters in Dubuque, he moved to Santa Fe in 1979.
Writing career.
Martin began selling science fiction short stories professionally in 1970, at age 21. His first sale was "The Hero", sold to "Galaxy" magazine and published in its February 1971 issue; other sales soon followed. His first story to be nominated for the Hugo Award and Nebula Awards was "With Morning Comes Mistfall", published in 1973 in "Analog" magazine. A member of the Science Fiction and Fantasy Writers of America (SFWA), Martin became the organization's Southwest Regional Director from 1977 to 1979; he served as its vice-president from 1996 to 1998.
In 1976, for Kansas City's MidAmeriCon, the 34th World Science Fiction Convention (Worldcon), Martin and his friend and fellow writer-editor Gardner Dozois conceived of and organized the first Hugo Losers' Party for the benefit of all past and present Hugo-losing writers, their friends and families, the evening following the convention's Hugo Awards ceremony. Martin was nominated for two Hugos that year but lost both awards, for the novelette "...and Seven Times Never Kill Man" and the novella "The Storms of Windhaven," co-written with Lisa Tuttle. The Hugo Losers' Party became an annual Worldcon event thereafter, and its formal title eventually changed to something a bit more politically correct as both its size and prestige grew.
Although Martin often writes fantasy or horror, a number of his earlier works are science fiction tales occurring in a loosely defined future history, known informally as "The Thousand Worlds" or "The Manrealm". He has also written at least one piece of political-military fiction, "Night of the Vampyres", collected in Harry Turtledove's anthology "The Best Military Science Fiction of the 20th Century" (2001).
The unexpected commercial failure of Martin's fourth book, "The Armageddon Rag" (1983), "essentially destroyed my career as a novelist at the time", he recalled. However, that failure led him to seek a career in television after a Hollywood option on that novel led to his being hired, first as a staff writer and then as an Executive Story Consultant, for the revival of the "Twilight Zone". After the CBS series was cancelled, Martin migrated over to the already-underway satirical science fiction series "Max Headroom". He worked on scripts and created the show's "Ped Xing" character (the president of the Zic Zak corporation, Network 23's primary sponsor). Before his scripts could go into production, however, the ABC show was cancelled in the middle of its second season. Martin was then hired as a writer-producer on the new dramatic fantasy series "Beauty and the Beast"; in 1989 he became the show's co-supervising producer and wrote 14 of its episodes.
During this same period, Martin continued working in print media as a book-series editor, this time overseeing the development of the multi-author "Wild Cards" book series, which takes place in a shared universe in which a small slice of post–World War II humanity gains superpowers after the release of an alien-engineered virus; new titles are still being published in the ongoing series from Tor Books. In "Second Person" Martin "gives a personal account of the close-knit role-playing game (RPG) culture that gave rise to his "Wild Cards" shared-world anthologies". An important element in the creation of the multiple author series was a campaign of Chaosium's role-playing game "Superworld" (1983), that Martin ran in Albuquerque. Martin's own contributions to "Wild Cards" have included Thomas Tudbury, "The Great and Powerful Turtle", a powerful psychokinetic whose flying "shell" consisted of an armored VW Beetle. As of June 2011, 21 "Wild Cards" volumes had been published in the series; earlier that same year, Martin signed the contract for the 22nd volume, "Low Ball" (2014), published by Tor Books. In early 2012, Martin signed another Tor contract for the 23rd "Wild Cards" volume, "High Stakes".
While he was making a satisfactory living in Hollywood, he did not feel fulfilled given that so few of the projects he worked on ever went into production; "No amount of money can really take the place of... you want your stuff to be read. You want an audience and four guys in an executive office suite at ABC or Columbia is not adequate."
Martin's novella, "Nightflyers" (1980), was adapted into an eponymous 1987 feature film; he was unhappy about having to cut plot elements for the screenplay's scenario in order to accommodate the film's small budget.
"A Song of Ice and Fire".
In 1991, Martin briefly returned to writing novels and began what would eventually turn into his epic fantasy series: "A Song of Ice and Fire," which was inspired by the Wars of the Roses and "Ivanhoe". It is currently intended to comprise seven volumes. The first, "A Game of Thrones", was published in 1996. In November 2005, "A Feast for Crows", the fourth novel in this series, became "The New York Times" No. 1 Bestseller and also achieved No. 1 ranking on "The Wall Street Journal" bestseller list. In addition, in September 2006, "A Feast for Crows" was nominated for both a Quill Award and the British Fantasy Award. The fifth book, "A Dance with Dragons", was published July 12, 2011, and quickly became an international bestseller, including achieving a No. 1 spot on the "New York Times" Bestseller List and many others; it remained on the "New York Times" list for 88 weeks. The series has received praise from authors, readers, and critics alike. In 2012, "A Dance With Dragons" made the final ballot for science fiction and fantasy's Hugo Award, World Fantasy Award, Locus Poll Award, and the British Fantasy Award; the novel went on to win the Locus Poll Award for Best Fantasy Novel. Two more novels are planned and still being written in the Ice and Fire series: "The Winds of Winter" and "A Dream of Spring".
HBO adaptation.
HBO Productions purchased the television rights for the entire "A Song of Ice and Fire" series in 2007 and began airing the fantasy series on their U. S. premium cable channel April 17, 2011. Titled "Game of Thrones", it ran weekly for ten episodes, each approximately an hour long. Although busy completing "A Dance With Dragons" and other projects, George R. R. Martin was heavily involved in the production of the television series adaptation of his books. Martin's involvement included the selection of a production team and participation in scriptwriting; the opening credits list him as a co-executive producer of the series. The series was renewed shortly after the first episode aired.
The first season was nominated for 13 Emmy Awards, ultimately winning two:
The first season was also nominated for a 2012 Hugo Award, fantasy and science fiction's oldest award, presented by the World Science Fiction Society each year at the annual Worldcon; the show went on to win the 2012 Hugo for "Best Dramatic Presentation, Long Form", at Chicon 7, the 70th World Science Fiction Convention, in Chicago, IL; Martin took home one of the three Hugo Award trophies awarded in that collaborative category, the other two going to "Game of Thrones" showrunners David Benioff and D.B. Weiss.
The second season, based on the second Ice and Fire novel "A Clash of Kings", began airing on HBO in the U.S. April 1, 2012; the second season was nominated for 12 Emmy Awards, including another Supporting Actor nomination for Dinklage. It went on to win six of those Emmys in the Technical Arts categories, which were awarded the week before the regular televised 2012 awards show. The second season episode "Blackwater", written by George R.R. Martin, was nominated the following year for the 2013 Hugo Award in the "Best Dramatic Presentation, Short Form" category; that episode went on to win the Hugo Award at LoneStarCon 3, the 71st World Science Fiction Convention, in San Antonio, Texas. In addition to Martin, showrunners Benioff and Weiss (who contributed several scenes to the final screenplay) and episode director Neil Marshal (who expanded the scope of the episode on set) received Hugo statuettes.
Themes.
Martin's work has been described by the "Los Angeles Times" as having "complex story lines, fascinating characters, great dialogue, perfect pacing". While the "New York Times" sees it as "fantasy for grown ups", others feel it is dark and cynical. His first novel, "Dying of the Light", set the tone for some of his future work; it unfolds on a mostly abandoned planet that is slowly becoming uninhabitable as it moves away from its sun. This story has a strong sense of melancholy. His characters are often unhappy or, at least, unsatisfied, in many cases holding on to idealisms in spite of an otherwise chaotic and ruthless world, in many cases troubled by their own self-seeking or violent actions, even as they undertake them. Many have elements of tragic heroes or antiheroes in them; reviewer T. M. Wagner writes, "Let it never be said Martin doesn't share Shakespeare's fondness for the senselessly tragic."
The overall gloominess of "A Song of Ice and Fire" can be an obstacle for some readers; the Inchoatus Group writes, "If this absence of joy is going to trouble you, or you're looking for something more affirming, then you should probably seek elsewhere." For many fans, however, it is precisely this level of "realness" and "completeness", including many characters' imperfections, moral and ethical ambiguity, and consequential plot twists (often sudden), that is endearing about Martin's work and keeps the series' story arcs compelling enough to keep following despite its sheer brutality and intricately messy/interwoven plotlines; as TM Wagner points out, "There's great tragedy here, but there's also excitement, humor, heroism even in weaklings, nobility even in villains, and, now and then, a taste of justice after all. It's a rare gift when a writer can invest his story with that much humanity."
Martin's characters are multifaceted, each with intricate pasts, aspirations, and ambitions. "Publishers Weekly" writes of his ongoing epic fantasy "A Song of Ice and Fire", "The complexity of characters such as Daenerys, Arya and the Kingslayer will keep readers turning even the vast number of pages contained in this volume, for the author, like Tolkien or Jordan, makes us care about their fates." Misfortune, injury, and death (including false death and reanimation) often befall major or minor characters, no matter how attached the reader has become. Martin has described his penchant for killing off important characters as being necessary for the story's depth: "when my characters are in danger, I want you to be afraid to turn the page, (so) you need to show right from the beginning that you're playing for keeps".
In distinguishing his work from others, Martin makes a point of emphasizing realism and plausible social dynamics above an over-reliance on magic and a simplistic "good versus evil" dichotomy, which contemporary fantasy writing is often criticized for. Notably, Martin's work makes a sharp departure from the prevalent "heroic knights and chivalry" schema that has become a mainstay in fantasy as derived from the "Lord of the Rings" series of J.R.R. Tolkien. He specifically critiques the oversimplification of Tolkien's themes and devices by imitators in ways that he has humorously described as "Disneyland Middle Ages" that gloss over or even ignore major differences between medieval and modern societies, particularly social structures, ways of living, and political arrangements. Martin has been described as "the American Tolkien" by literary critics. While Martin finds inspiration in Tolkien's legacy, he aims to go beyond what he sees as Tolkien's "medieval philosophy" of "if the king was a good man, the land would prosper" to delve into the complexities, ambiguities, and vagaries of real-life power: "We look at real history and it's not that simple ... Just having good intentions doesn't make you a wise king."
In fact, the author makes a point of grounding his work on a foundation of historical fiction, which he channels to evoke important social and political elements of primarily the European medieval era that differ markedly from elements of modern times, including the multigenerational, rigid, and often brutally consequential nature of the hierarchical class system of feudal societies that is in many cases overlooked in fantasy writing. Even as "A Song of Ice and Fire" is a fantasy series that employs magic and the surreal as central to the genre, Martin is keen to ensure that magic is merely one element of many that moves his work forward, not a generic deus ex machina that is itself the focus of his stories, something he has been very conscious about since reading Tolkien; "If you look at The Lord of the Rings, what strikes you, it certainly struck me, is that although the world is infused with this great sense of magic, there is very little onstage magic. So you have a sense of magic, but it's kept under very tight control, and I really took that to heart when I was starting my own series." Martin's ultimate aim is an exploration of the internal conflicts that define the human condition, which, in deriving inspiration from William Faulkner, he ultimately describes as the only reason to read "any" literature, regardless of genre.
This nuanced, multi-layered, all-encompassing nature of Martin's work has consistently received accolades – his work has "captured the imaginations of millions for the same reason the archetypal dramas of Homer, Sophocles or Shakespeare have lasted for millennia. They show us the conflict between self-sacrifice and self-interest, between the human spirit and the human ego, between good and evil. And when we look up from the page we recognise those same conflicts in the world around us and in ourselves."
Relationship with fans.
Blog.
Martin actively contributes to his blog, Not a Blog. He still does all his writing on an old DOS machine running Wordstar 4.0.
Conventions.
Martin is known for his regular attendance through the decades at science fiction conventions and comics conventions, and his accessibility to fans. In the early 1980s, critic and writer Thomas Disch identified Martin as a member of the "Labor Day Group", writers who regularly congregated at the annual Worldcon, usually held on or around the Labor Day weekend. Since the early 1970s he has also attended regional science fiction conventions, and since 1986 Martin has participated annually in Albuquerque's smaller regional convention Bubonicon, near his New Mexico home. He was invited to be Guest of Honor at the 61st World Science Fiction Convention in Toronto, held in 2003.
Fan club.
Martin's official fan club is the "Brotherhood Without Banners", who have a regular posting board at the Forum of the website westeros.org, which is focused on his "Song of Ice and Fire" fantasy series. At the annual World Science Fiction Convention every year, the "BWB" hosts a large, on-going hospitality suite that is open to all members of the Worldcon; their suite frequently wins by popular vote the convention's best party award.
Fan criticism and response.
Martin has been criticized by some of his readers for the long periods between books in the "Ice and Fire" series, notably the six-year gap between the fourth volume, "A Feast for Crows" (2005), and the fifth volume, "A Dance with Dragons" (2011). The previous year, in 2010, Martin had responded to fan criticisms by saying he was unwilling to write only his "Ice and Fire" series, noting that working on other prose and compiling and editing different book projects has always been part of his working process. Writer Neil Gaiman famously wrote on his blog in 2009 to a critic of Martin's pace, "George R. R. Martin is not your bitch." Gaiman later went on to state that writers aren't machines and that they have every right to work on other projects if they want to.
Fan fiction.
Martin is opposed to fan fiction, which he views as copyright infringement and a bad exercise for aspiring writers in terms of developing skills in world-building and character development.
Personal life.
In the early 1970s, Martin was in a relationship with fellow science-fiction/fantasy author Lisa Tuttle, with whom he co-wrote "Windhaven".
While attending an East Coast science fiction convention he met his first wife, Gale Burnick; they were married in 1975, but the marriage ended in divorce, without issue, in 1979. On February 15, 2011, Martin married his longtime partner Parris McBride during a small ceremony at their Santa Fe home. On August 19, 2011, they held a larger wedding ceremony and reception at Renovation, the 69th World Science Fiction Convention, in Reno, Nevada.
He and his wife Parris are supporters of the Wild Spirit Wolf Sanctuary in New Mexico. In early 2013 he purchased Santa Fe's Jean Cocteau Cinema and Coffee House, which had been closed since 2006. He had the property completely restored, including both its original 35 mm capability to which was added digital projection and sound; the Cocteau officially reopened for business on August 9, 2013. Martin has also supported Meow Wolf, an arts collective in Santa Fe, having pledged $2.7 million towards a new art-space in January 2015.
In response to a question on his religious views, Martin replied: "I suppose I'm a lapsed Catholic. You would consider me an atheist or agnostic. I find religion and spirituality fascinating. I would like to believe this isn't the end and there's something more, but I can't convince the rational part of me that makes any sense whatsoever."
Martin is a fan of the New York Jets and the New York Mets.
Martin made a guest appearance as himself in an episode, "El Skeletorito", of an Adult Swim show, "Robot Chicken".
He also appeared in SyFy's "Z Nation" as a zombie version of himself in season two's "The Collector," where he is still signing copies of his new novel.
Philanthropy.
In 2014, Martin launched a high-profile campaign on Prizeo to raise funds for two charities: Wild Spirit Wolf Sanctuary and the Food Depot of Santa Fe. As part of the campaign, Martin offered one donor the chance to accompany him on a trip to the wolf sanctuary, including a helicopter ride and dinner. Martin also offered those donating $20,000 or more the opportunity to have a character named after them in an upcoming A Song Of Ice And Fire novel and "killed off". The campaign garnered significant media attention and raised a total of $502,549.
Politics.
Growing up, Martin avoided the draft to the Vietnam War by being a conscientious objector and did two years of alternative service. He generally opposed the war and thought it was a "terrible mistake for America." He also opposes the idea of the glory of war and tries to realistically describe war in his books.
In 2014, Martin endorsed Senator Tom Udall.
In the midst of pressure to pull the 2014 feature film "The Interview" from theatres, the Jean Cocteau Theatre in Santa Fe, New Mexico, which has been owned by Martin since 2013, decided to show the film. "As a movie theater, we are not just involved in the entertainment business. We are involved in the First Amendment business, protecting our freedoms," theatre manager Jon Bowman told the "Santa Fe New Mexican".
On 20 November 2015, writing on his LiveJournal, Martin advocated for allowing Syrian refugees into the United States.

</doc>
