<doc id="10902" url="https://en.wikipedia.org/wiki?curid=10902" title="Force">
Force

In physics, a force is any interaction that, when unopposed, will change the motion of an object. In other words, a force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described by intuitive concepts such as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F.
The original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object
Related concepts to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the so-called mechanical stress. Pressure is a simple type of stress. Stress usually causes deformation of solid materials, or flow in fluids.
Development of the concept.
Philosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Galileo Galilei and Sir Isaac Newton. With his mathematical insight, Sir Isaac Newton formulated laws of motion that were not improved-on for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia.
With modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.
Pre-Newtonian concepts.
Since antiquity the concept of force has been recognized as integral to the functioning of each of the simple machines. The mechanical advantage given by a simple machine allowed for less force to be used in exchange for that force acting over a greater distance for the same amount of work. Analysis of the characteristics of forces ultimately culminated in the work of Archimedes who was especially famous for formulating a treatment of buoyant forces inherent in fluids.
Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different "natural places" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their "natural place" (e.g., for heavy bodies to fall), which led to "natural motion", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.
Aristotelian physics began facing criticism in Medieval science, first by John Philoponus in the 6th century.
The shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late Medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion early in the 17th century. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction.
Newtonian mechanics.
Sir Isaac Newton sought to describe the motion of all objects using the concepts of inertia and force, and in doing so he found that they obey certain conservation laws. In 1687, Newton went on to publish his thesis "Philosophiæ Naturalis Principia Mathematica". In this work Newton set out three laws of motion that to this day are the way forces are described in physics.
First law.
Newton's First Law of Motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force or "resultant force". This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium "natural state" in place of the Aristotelian idea of the "natural state of rest". That is, the first law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making "rest" physically indistinguishable from "non-zero constant velocity", Newton's First Law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is "in motion" and which object is "at rest". In other words, to phrase matters more technically, the laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation.
For instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change from being at rest. A person can throw a ball straight up in the air and catch it as it falls down without worrying about applying a force in the direction the vehicle is moving. This is true even though another person who is observing the moving vehicle pass by also observes the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest.
The concept of inertia can be further generalized to explain the tendency of objects to continue in many different forms of constant motion, even those that are not strictly constant velocity. The rotational inertia of planet Earth is what fixes the constancy of the length of a day and the length of a year. Albert Einstein extended the principle of inertia further when he explained that reference frames subject to constant acceleration, such as those free-falling toward a gravitating object, were physically equivalent to inertial reference frames. This is why, for example, astronauts experience weightlessness when in free-fall orbit around the Earth, and why Newton's Laws of Motion are more easily discernible in such environments. If an astronaut places an object with mass in mid-air next to himself, it will remain stationary with respect to the astronaut due to its inertia. This is the same thing that would occur if the astronaut and the object were in intergalactic space with no net force of gravity acting on their shared reference frame. This principle of equivalence was one of the foundational underpinnings for the development of the general theory of relativity.
[[File:GodfreyKneller-IsaacNewton-1689.jpg|right|thumb|Though Sir Isaac Newton's most famous equation is<br>
formula_1, he actually wrote down a different form for his second law of motion that did not use differential calculus.]]
Second law.
A modern statement of Newton's Second Law is a vector equation:
where formula_3 is the momentum of the system, and formula_4 is the net (vector sum) force. In equilibrium, there is zero "net" force by definition, but (balanced) forces may be present nevertheless. In contrast, the second law states an "unbalanced" force acting on an object will result in the object's momentum changing over time.
By the definition of momentum,
where "m" is the mass and formula_6 is the velocity.
Newton's second law applies only to a system of constant mass, and hence "m" may be moved outside the derivative operator. The equation then becomes
By substituting the definition of acceleration, the algebraic version of Newton's Second Law is derived:
Newton never explicitly stated the formula in the reduced form above.
Newton's Second Law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of "mass" by writing the law as an equality; the relative units of force and mass then are fixed.
The use of Newton's Second Law as a "definition" of force has been disparaged in some of the more rigorous textbooks, because it is essentially a mathematical truism. Notable physicists, philosophers and mathematicians who have sought a more explicit definition of the concept of force include Ernst Mach, Clifford Truesdell and Walter Noll.
Newton's Second Law can be used to measure the strength of forces. For instance, knowledge of the masses of planets along with the accelerations of their orbits allows scientists to calculate the gravitational forces on planets.
Third law.
Newton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are "interactions" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body. Whenever a first body exerts a force F on a second body, the second body exerts a force −F on the first body. F and −F are equal in magnitude and opposite in direction. This law is sometimes referred to as the "action-reaction law", with F called the "action" and −F the "reaction". The action and the reaction are simultaneous:
If object 1 and object 2 are considered to be in the same system, then the net force on the system due to the interactions between objects 1 and 2 is zero since
This means that in a closed system of particles, there are no internal forces that are unbalanced. That is, the action-reaction force shared between any two objects in a closed system will not cause the center of mass of the system to accelerate. The constituent objects only accelerate with respect to each other, the system itself remains unaccelerated. Alternatively, if an external force acts on the system, then the center of mass will experience an acceleration proportional to the magnitude of the external force divided by the mass of the system.
Combining Newton's Second and Third Laws, it is possible to show that the linear momentum of a system is conserved. Using
and integrating with respect to time, the equation:
is obtained. For a system that includes objects 1 and 2,
which is the conservation of linear momentum. Using the similar arguments, it is possible to generalize this to a system of an arbitrary number of particles. This shows that exchanging momentum between constituent objects will not affect the net momentum of a system. In general, as long as all forces are due to the interaction of objects with mass, it is possible to define a system such that net momentum is never lost nor gained.
Special theory of relativity.
In the special theory of relativity, mass and energy are equivalent (as can be seen by calculating the work required to accelerate an object). When an object's velocity increases, so does its energy and hence its mass equivalent (inertia). It thus requires more force to accelerate it the same amount than it did at a lower velocity. Newton's Second Law
remains valid because it is a mathematical definition. But in order to be conserved, relativistic momentum must be redefined as:
where
The relativistic expression relating force and acceleration for a particle with constant non-zero rest mass formula_20 moving in the formula_21 direction is:
where the Lorentz factor
In the early history of relativity, the expressions formula_26 and formula_27 were called longitudinal and transverse mass. Relativistic force does not produce a constant acceleration, but an ever decreasing acceleration as the object approaches the speed of light. Note that formula_28 is undefined for an object with a non-zero rest mass at the speed of light, and the theory yields no prediction at that speed.
If formula_17 is very small compared to formula_18, then formula_31 is very close to 1 and 
is a close approximation. Even for use in relativity, however, one can restore the form of
through the use of four-vectors. This relation is correct in relativity when formula_34 is the four-force, formula_20 is the invariant mass, and formula_36 is the four-acceleration.
Descriptions.
Since forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics.
Forces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as "vector quantities". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems.
Historically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the "resultant" (also called the "net force"), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body.
Free-body diagrams can be used as a convenient way to keep track of forces acting on a system. Ideally, these diagrams are drawn with the angles and relative magnitudes of the force vectors preserved so that graphical vector addition can be done to determine the net force.
As well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two.
Equilibrium.
Equilibrium occurs when the resultant force acting on a point particle is zero (that is, the vector sum of all forces is zero). When dealing with an extended body, it is also necessary that the net torque in it is 0.
There are two kinds of equilibrium: static equilibrium and dynamic equilibrium.
Static.
Static equilibrium was understood well before the invention of classical mechanics. Objects that are at rest have zero net force acting on them.
The simplest case of static equilibrium occurs when two forces are equal in magnitude but opposite in direction. For example, an object on a level surface is pulled (attracted) downward toward the center of the Earth by the force of gravity. At the same time, surface forces resist the downward force with equal upward force (called the normal force). The situation is one of zero net force and no acceleration.
Pushing against an object on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force "exactly" balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.
A static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the "spring reaction force", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion.
Dynamic.
Dynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an "absolute rest frame" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a "natural state" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity.
Moreover, any object traveling at a constant velocity must be subject to zero net force (resultant force). This is the definition of dynamic equilibrium: when all the forces on an object balance but it still moves at a constant velocity.
A simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion.
Forces in Quantum Mechanics.
The notion "force" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrödinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes "quantized", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of "forces". However, the potentials V(x,y,z) or fields, from which the forces generally can be derived, are treated similar to classical position variables, i.e., formula_37.
This becomes different only in the framework of quantum field theory, where these fields are also quantized.
However, already in quantum mechanics there is one "caveat", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the "spin", and there is the Pauli principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a "symmetric" spin function (e.g. parallel spins) the spatial variables must be "antisymmetric" (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel "spins" the "position variables" must be symmetric (i.e. the apparent force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.
Thus the notion "force" loses already part of its meaning.
Feynman diagrams.
In modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be "fundamental interactions". When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex.
The utility of Feynman diagrams is that other types of physical phenomena that are part of the general picture of fundamental interactions but are conceptually separate from forces can also be described using the same rules. For example, a Feynman diagram can describe in succinct detail how a neutron decays into an electron, proton, and neutrino, an interaction mediated by the same gauge boson that is responsible for the weak nuclear force.
Fundamental forces.
All of the forces in the universe are based on four fundamental interactions. The strong and weak forces are nuclear forces that act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between the atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Exclusion Principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference.
The development of fundamental theories for forces proceeded along the lines of unification of disparate ideas. For example, Isaac Newton unified the force responsible for objects falling at the surface of the Earth with the force responsible for the orbits of celestial mechanics in his universal theory of gravitation. Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through one consistent theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This standard model of particle physics posits a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory subsequently confirmed by observation. The complete formulation of the standard model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations indicate that the standard model is incomplete. A Grand Unified Theory allowing for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.
Gravitational.
What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as formula_38 and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of formula_20 will experience a force:
In free-fall, this force is unopposed and therefore the net force on the object is its weight. For objects not in free-fall, the force of gravity is opposed by the reactions of their supports. For example, a person standing on the ground experiences zero net force, since his weight is balanced by a normal force exerted by the ground.
Newton's contribution to gravitational theory was to unify the motions of heavenly bodies, which Aristotle had assumed were in a natural state of constant motion, with falling motion observed on the Earth. He proposed a law of gravity that could account for the celestial motions that had been described earlier using Kepler's laws of planetary motion.
Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass (formula_41) and the radius (formula_42) of the Earth to the gravitational acceleration:
where formula_44 is the distance between the two objects' centers of mass and formula_45 is the unit vector pointed in the direction away from the center of the first object toward the center of the second object.
This formula was powerful enough to stand as the basis for all subsequent descriptions of motion within the solar system until the 20th century. During that time, sophisticated methods of perturbation analysis were invented to calculate the deviations of orbits due to the influence of multiple bodies on a planet, moon, comet, or asteroid. The formalism was exact enough to allow mathematicians to predict the existence of the planet Neptune before it was observed.
It was only the orbit of the planet Mercury that Newton's Law of Gravitation seemed not to fully explain. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however, despite some early indications, no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be less correct than an alternative.
Since then, and so far, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time – defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the "ballistic trajectory" of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory (when the extra ct dimension is added) is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as "gravitational force".
Electromagnetic.
The electrostatic force was first described in 1784 by Coulomb as a force that existed intrinsically between two charges. The properties of the electrostatic force were that it varied as an inverse square law directed in the radial direction, was both attractive and repulsive (there was intrinsic polarity), was independent of the mass of the charged objects, and followed the superposition principle. Coulomb's law unifies all these observations into one succinct statement.
Subsequent mathematicians and physicists found the construct of the "electric field" to be useful for determining the electrostatic force on an electric charge at any point in space. The electric field was based on using a hypothetical "test charge" anywhere in space and then using Coulomb's Law to determine the electrostatic force. Thus the electric field anywhere in space is defined as
where formula_47 is the magnitude of the hypothetical test charge.
Meanwhile, the Lorentz force of magnetism was discovered to exist between two electric currents. It has the same mathematical character as Coulomb's Law with the proviso that like currents attract and unlike currents repel. Similar to the electric field, the magnetic field can be used to determine the magnetic force on an electric current at any point in space. In this case, the magnitude of the magnetic field was determined to be
where formula_49 is the magnitude of the hypothetical test current and formula_50 is the length of hypothetical wire through which the test current flows. The magnetic field exerts a force on all magnets including, for example, those used in compasses. The fact that the Earth's magnetic field is aligned closely with the orientation of the Earth's axis causes compass magnets to become oriented because of the magnetic force pulling on the needle.
Through combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified "electromagnetic force" that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law:
where formula_4 is the electromagnetic force, formula_47 is the magnitude of the charge of the particle, formula_54 is the electric field, formula_6 is the velocity of the particle that is crossed with the magnetic field (formula_56).
The origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These "Maxwell Equations" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be "self-generating" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum.
However, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave–particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force.
It is a common misconception to ascribe the stiffness and rigidity of solid matter to the repulsion of like charges under the influence of the electromagnetic force. However, these characteristics actually result from the Pauli exclusion principle. Since electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons. When the electrons in a material are densely packed together, there are not enough lower energy quantum mechanical states for them all, so some of them must be in higher energy states. This means that it takes energy to pack them together. While this effect is manifested macroscopically as a structural force, it is technically only the result of the existence of a finite set of electron states.
Strong nuclear.
There are two "nuclear forces", which today are usually described as interactions that take place in quantum theories of particle physics. The strong nuclear force is the force responsible for the structural integrity of atomic nuclei while the weak nuclear force is responsible for the decay of certain nucleons into leptons and other types of hadrons.
The strong force is today understood to represent the interactions between quarks and gluons as detailed by the theory of quantum chromodynamics (QCD). The strong force is the fundamental force mediated by gluons, acting upon quarks, antiquarks, and the gluons themselves. The (aptly named) strong interaction is the "strongest" of the four fundamental forces.
The strong force only acts "directly" upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement.
Weak nuclear.
The weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. The word "weak" derives from the fact that the field strength is some 1013 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 1015 kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang.
Non-fundamental forces.
Some forces are consequences of the fundamental ones. In such situations, idealized models can be utilized to gain physical insight.
Normal force.
The normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects. The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface.
Friction.
Friction is a surface force that opposes relative motion. The frictional force is directly related to the normal force that acts to keep two solid objects separated at the point of contact. There are two broad classifications of frictional forces: static friction and kinetic friction.
The static friction force (formula_57) will exactly oppose forces applied to an object parallel to a surface contact up to the limit specified by the coefficient of static friction (formula_58) multiplied by the normal force (formula_59). In other words, the magnitude of the static friction force satisfies the inequality:
The kinetic friction force (formula_61) is independent of both the forces applied and the movement of the object. Thus, the magnitude of the force equals:
where formula_63 is the coefficient of kinetic friction. For most surface interfaces, the coefficient of kinetic friction is less than the coefficient of static friction.
Tension.
Tension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine.
Elastic force.
An elastic force acts to return a spring to its natural length. An ideal spring is taken to be massless, frictionless, unbreakable, and infinitely stretchable. Such springs exert forces that push when contracted, or pull when extended, in proportion to the displacement of the spring from its equilibrium position. This linear relationship was described by Robert Hooke in 1676, for whom Hooke's law is named. If formula_64 is the displacement, the force exerted by an ideal spring equals:
where formula_66 is the spring constant (or force constant), which is particular to the spring. The minus sign accounts for the tendency of the force to act in opposition to the applied load.
Continuum mechanics.
Newton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows:
where formula_68 is the volume of the object in the fluid and formula_69 is the scalar function that describes the pressure at all locations in space. Pressure gradients and differentials result in the buoyant force for fluids suspended in gravitational fields, winds in atmospheric science, and the lift associated with aerodynamics and flight.
A specific instance of such a force that is associated with dynamic pressure is fluid resistance: a body force that resists the motion of an object through a fluid due to viscosity. For so-called "Stokes' drag" the force is approximately proportional to the velocity, but opposite in direction:
where:
More formally, forces in continuum mechanics are fully described by a stress–tensor with terms that are roughly defined as
where formula_74 is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.
Fictitious forces.
There are forces that are frame dependent, meaning that they appear due to the adoption of non-Newtonian (that is, non-inertial) reference frames. Such forces include the centrifugal force and the Coriolis force. These forces are considered fictitious because they do not exist in frames of reference that are not accelerating. Because these forces are not genuine they are also referred to as "pseudo forces".
In general relativity, gravity becomes a fictitious force that arises in situations where spacetime deviates from a flat geometry. As an extension, Kaluza–Klein theory and string theory ascribe electromagnetism and the other fundamental forces respectively to the curvature of differently scaled dimensions, which would ultimately imply that all forces are fictitious.
Rotations and torque.
Forces that cause extended objects to rotate are associated with torques. Mathematically, the torque of a force formula_4 is defined relative to an arbitrary reference point as the cross-product:
where
Torque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body:
where
This provides a definition for the moment of inertia, which is the rotational equivalent for mass. In more advanced treatments of mechanics, where the rotation over a time interval is described, the moment of inertia must be substituted by the tensor that, when properly analyzed, fully determines the characteristics of rotations including precession and nutation.
Equivalently, the differential form of Newton's Second Law provides an alternative definition of torque:
Newton's Third Law of Motion requires that all objects exerting torques themselves experience equal and opposite torques, and therefore also directly implies the conservation of angular momentum for closed systems that experience rotations and revolutions through the action of internal torques.
Centripetal force.
For an object accelerating in circular motion, the unbalanced force acting on the object equals:
where formula_20 is the mass of the object, formula_17 is the velocity of the object and formula_44 is the distance to the center of the circular path and formula_45 is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction.
Kinematic integrals.
Forces can be used to define a number of physical concepts by integrating with respect to kinematic variables. For example, integrating with respect to time gives the definition of impulse:
which by Newton's Second Law must be equivalent to the change in momentum (yielding the Impulse momentum theorem).
Similarly, integrating with respect to position gives a definition for the work done by a force:
which is equivalent to changes in kinetic energy (yielding the work energy theorem).
Power "P" is the rate of change d"W"/d"t" of the work "W", as the trajectory is extended by a position change formula_90 in a time interval d"t":
with formula_92 the velocity.
Potential energy.
Instead of a force, often the mathematically related concept of a potential energy field can be used for convenience. For instance, the gravitational force acting upon an object can be seen as the action of the gravitational field that is present at the object's location. Restating mathematically the definition of energy (via the definition of work), a potential scalar field formula_93 is defined as that field whose gradient is equal and opposite to the force produced at every point:
Forces can be classified as conservative or nonconservative. Conservative forces are equivalent to the gradient of a potential while nonconservative forces are not.
Conservative forces.
A conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area.
Conservative forces include gravity, the electromagnetic force, and the spring force. Each of these forces has models that are dependent on a position often given as a radial vector formula_77 emanating from spherically symmetric potentials. Examples of this follow:
For gravity:
where formula_97 is the gravitational constant, and formula_98 is the mass of object "n".
For electrostatic forces:
where formula_100 is electric permittivity of free space, and formula_101 is the electric charge of object "n".
For spring forces:
where formula_66 is the spring constant.
Nonconservative forces.
For certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials.
The connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.
Units of measurement.
The SI unit of force is the newton (symbol N), which is the force required to accelerate a one kilogram mass at a rate of one meter per second squared, or . The corresponding CGS unit is the dyne, the force required to accelerate a one gram mass by one centimeter per second squared, or . A newton is thus equal to 100,000 dynes.
The gravitational foot-pound-second English unit of force is the pound-force (lbf), defined as the force exerted by gravity on a pound-mass in the standard gravitational field of . The pound-force provides an alternative unit of mass: one slug is the mass that will accelerate by one foot per second squared when acted on by one pound-force.
An alternative unit of force in a different foot-pound-second system, the absolute fps system, is the poundal, defined as the force required to accelerate a one-pound mass at a rate of one foot per second squared. The units of slug and poundal are designed to avoid a constant of proportionality in Newton's Second Law.
The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.
See also Ton-force.
Force measurement.
See force gauge, spring scale, load cell

</doc>
<doc id="10905" url="https://en.wikipedia.org/wiki?curid=10905" title="Family law">
Family law

Family law (also called matrimonial law) is an area of the law that deals with family matters and domestic relations, including:
This list is not exhaustive and varies depending on jurisdiction. In many jurisdictions in the United States, the family courts see the most crowded dockets. Litigants representative of all social and economic classes are parties within the system.
For the conflict of laws elements dealing with transnational and interstate issues, see marriage (conflict), divorce (conflict) and nullity (conflict).

</doc>
<doc id="10909" url="https://en.wikipedia.org/wiki?curid=10909" title="Foonly">
Foonly

Foonly was a short-lived American computer company formed by Dave Poole, one of the principal Super Foonly designers as well as one of hackerdom's more colourful personalities. The company produced a series of DEC PDP-10 compatible computers, first the high-performance F-1, and later a series of smaller and less expensive designs. The first Foonly machine, the F-1, was the computational engine used to create some of the graphics in the 1982 film "Tron".
PDP-10 successor.
The PDP-10 successor was to have been built by the Super Foonly project at the Stanford Artificial Intelligence Laboratory (SAIL) along with a new operating system. The intention was to leapfrog from the old DEC timesharing system which SAIL was then running to a new generation, bypassing TENEX – at that time the ARPANET standard. The F-1 was the fastest PDP-10 architecture machine ever built, with a clock rate of 90-100 ns per cycle, but only one was ever made. ARPA funding for both the Super Foonly and the new operating system was cut in 1974. The design for Foonly contributed greatly to the design of the PDP-10 model KL10.
History.
The following few paragraphs are a personal account of the events, by Dave Dyer:
Foonly Inc. did not acquire any financial resources as a result of building the F-1. They turned to the market for low-end machines, producing a series of smaller, slower, and much less expensive DEC-10 clones that ran a TENEX variant called Foonex; this seriously limited their market. Also, the machines shipped as wire-wrapped engineering prototypes requiring individual attention from more than usually competent site personnel, and thus had significant reliability problems. Poole's legendary temper and unwillingness to suffer fools gladly did not help matters. By the time of DEC's Jupiter project cancellation in 1983, Foonly's proposal to build another F-1 was eclipsed by another DEC-10 clone, the Mars computer, and the company never quite recovered.
Added by Phil Petit, (one of the above-mentioned Foonly designers):
Added by Dan Martin - Principal Engineer for Tymshare Inc.
The main application for Tymshare's version of the F4 was a version of Doug Englebart's NLS system, developed when his team moved to Tymshare from SRI, called "Augment". The machine, called the 26KL, was marketed as the "Augment Engine" when running Augment.
External links.
October 6, 2015
Added by Paul Milleson - Principal Manager, Foonly Inc.

</doc>
<doc id="10911" url="https://en.wikipedia.org/wiki?curid=10911" title="Functional group">
Functional group

In organic chemistry, functional groups are specific groups (moieties) of atoms or bonds within molecules that are responsible for the characteristic chemical reactions of those molecules. The same functional group will undergo the same or similar chemical reaction(s) regardless of the size of the molecule it is a part of. However, its relative reactivity can be modified by other functional groups nearby. The atoms of functional groups are linked to each other and to the rest of the molecule by covalent bonds. When the group of covalently bound atoms bears a net charge, the group is referred to more properly as a polyatomic ion or a complex ion. Any subgroup of atoms of a compound also may be called a radical, and if a covalent bond is broken homolytically, the resulting fragment radicals are referred as free radicals.
Combining the names of functional groups with the names of the parent alkanes generates what is termed a systematic nomenclature for naming organic compounds. The first carbon atom after the carbon that attaches to the functional group is called the alpha carbon; the second, beta carbon, the third, gamma carbon, etc. If there is another functional group at a carbon, it may be named with the Greek letter, e.g., the gamma-amine in gamma-aminobutanoic acid is on the third carbon of the carbon chain attached to the carboxylic acid group.
Synthetic chemistry.
Organic reactions are facilitated and controlled by the functional groups of the reactants. In general, alkyls are unreactive and difficult to get to react selectively at the desired positions, with few exceptions. In contrast, unsaturated carbon functional groups, and carbon-oxygen and carbon-nitrogen functional groups have a more diverse array of reactions that are also selective. It may be necessary to create a functional group in the molecule to make it react. For example, to synthesize iso-octane (the 8-carbon ideal gasoline) from the unfunctionalized alkane isobutane (a 4-carbon gas), isobutane is first dehydrogenated into isobutene. This contains the alkene functional group and can now dimerize with another isobutene to give iso-octene, which is then catalytically hydrogenated to iso-octane using pressured hydrogen gas.
Functionalization.
Functionalization is the addition of functional groups onto the surface of a material by chemical synthesis methods. The functional group added can be subjected to ordinary synthesis methods to attach virtually any kind of organic compound onto the surface. Functionalization is employed for surface modification of industrial materials in order to achieve desired surface properties such as water repellent coatings for automobile windshields and non-biofouling, hydrophilic coatings for contact lenses. In addition, functional groups are used to covalently link functional molecules to the surface of chemical and biochemical devices such as microarrays and microelectromechanical systems. Catalysts can be attached to a material that has been functionalized. For example, silica is functionalized with an alkyl silicone, wherein the alkyl contains an amine functional group. A ligand such as an EDTA fragment is synthesized onto the amine, and a metal cation is complexed into the EDTA fragment. The EDTA is "not" adsorbed onto the surface, but connected by a permanent chemical bond. Functional groups are also used to covalently link molecules such as fluorescent dyes, nanoparticles, proteins, DNA, and other compounds of interest for a variety of applications such as sensing and basic chemical research.
Table of common functional groups.
The following is a list of common functional groups. In the formulas, the symbols R and R' usually denote an attached hydrogen, or a hydrocarbon side chain of any length, but may sometimes refer to any group of atoms.
Hydrocarbons.
Functional groups, called hydrocarbyl, that contain only carbon and hydrogen, but vary in the number and order of double bonds. Each one differs in type (and scope) of reactivity.
There are also a large number of branched or ring alkanes that have specific names, e.g., tert-butyl, bornyl, cyclohexyl, etc. Hydrocarbons may form charged structures: positively charged carbocations or negative carbanions. Carbocations are often named "-um". Examples are tropylium and triphenylmethyl cations and the cyclopentadienyl anion.
Groups containing halogens.
Haloalkanes are a class of molecule that is defined by a carbon–halogen bond. This bond can be relatively weak (in the case of an iodoalkane) or quite stable (as in the case of a fluoroalkane). In general, with the exception of fluorinated compounds, haloalkanes readily undergo nucleophilic substitution reactions or elimination reactions. The substitution on the carbon, the acidity of an adjacent proton, the solvent conditions, etc. all can influence the outcome of the reactivity.
Groups containing oxygen.
Compounds that contain C-O bonds each possess differing reactivity based upon the location and hybridization of the C-O bond, owing to the electron-withdrawing effect of sp-hybridized oxygen (carbonyl groups) and the donating effects of sp2-hybridized oxygen (alcohol groups).
Groups containing nitrogen.
Compounds that contain nitrogen in this category may contain C-O bonds, such as in the case of amides.
Groups containing sulfur.
Compounds that contain sulfur exhibit unique chemistry due to their ability to form more bonds than oxygen, their lighter analogue on the periodic table. Substitutive nomenclature (marked as prefix in table) is preferred over functional class nomenclature (marked as suffix in table) for sulfides, disulfides, sulfoxides and sulfones.
Groups containing phosphorus.
Compounds that contain phosphorus exhibit unique chemistry due to their ability to form more bonds than nitrogen, their lighter analogues on the periodic table.
Groups containing boron.
Compounds containing boron exhibit unique chemistry due to their having partially filled octets and therefore acting as Lewis acids.
Names of radicals or moieties.
These names are used to refer to the moieties themselves or to radical species, and also to form the names of halides and substituents in larger molecules.
When the parent hydrocarbon is unsaturated, the suffix ("-yl", "-ylidene", or "-ylidyne") replaces "-ane" (e.g. "ethane" becomes "ethyl"); otherwise, the suffix replaces only the final "-e" (e.g. "ethyne" becomes "ethynyl").
Note that when used to refer to moieties, multiple single bonds differ from a single multiple bond. For example, a methylene bridge (methanediyl) has two single bonds, whereas a methylene group (methylidene) has one double bond. Suffixes can be combined, as in methylidyne (triple bond) vs. methylylidene (single bond and double bond) vs. methanetriyl (three single bonds).
There are some retained names, such as methylene for methanediyl, 1,x-phenylene for phenyl-1,x-diyl (where x is 2, 3, or 4), carbyne for methylidyne, and trityl for triphenylmethyl.

</doc>
<doc id="10913" url="https://en.wikipedia.org/wiki?curid=10913" title="Fractal">
Fractal

A fractal is a natural phenomenon or a mathematical set that exhibits a repeating pattern that displays at every scale. It is also known as expanding symmetry or evolving symmetry. If the replication is exactly the same at every scale, it is called a self-similar pattern. An example of this is the Menger Sponge. Fractals can also be nearly the same at different levels. This latter pattern is illustrated in the magnifications of the Mandelbrot set. Fractals also include the idea of a detailed pattern that repeats itself.
Fractals are different from other geometric figures because of the way in which they scale. Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). Likewise, if the radius of a sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the dimension that the sphere resides in). But if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer. This power is called the fractal dimension of the fractal, and it usually exceeds the fractal's topological dimension.
As mathematical equations, fractals are usually nowhere differentiable. An infinite fractal curve can be conceived of as winding through space differently from an ordinary line, still being a 1-dimensional line yet having a fractal dimension indicating it also resembles a surface.
The mathematical roots of the idea of fractals have been traced throughout the years as a formal path of published works, starting in the 17th century with notions of recursion, then moving through increasingly rigorous mathematical treatment of the concept to the study of continuous but not differentiable functions in the 19th century by the seminal work of Bernard Bolzano, Bernhard Riemann, and Karl Weierstrass, and on to the coining of the word "fractal" in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 20th century. The term "fractal" was first used by mathematician Benoît Mandelbrot in 1975. Mandelbrot based it on the Latin "frāctus" meaning "broken" or "fractured", and used it to extend the concept of theoretical fractional dimensions to geometric patterns in nature.
There is some disagreement amongst authorities about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as "beautiful, damn hard, increasingly useful. That's fractals." The general consensus is that theoretical fractals are infinitely self-similar, iterated, and detailed mathematical constructs having fractal dimensions, of which many examples have been formulated and studied in great depth. Fractals are not limited to geometric patterns, but can also describe processes in time. Fractal patterns with various degrees of self-similarity have been rendered or studied in images, structures and sounds and found in nature, technology, art, and law.
Introduction.
The word "fractal" often has different connotations for laypeople than for mathematicians, where the layperson is more likely to be familiar with fractal art than a mathematical conception. The mathematical concept is difficult to define formally even for mathematicians, but key features can be understood with little mathematical background.
The feature of "self-similarity", for instance, is easily understood by analogy to zooming in with a lens or other device that zooms in on digital images to uncover finer, previously invisible, new structure. If this is done on fractals, however, no new detail appears; nothing changes and the same pattern repeats over and over, or for some fractals, nearly the same pattern reappears over and over. Self-similarity itself is not necessarily counter-intuitive (e.g., people have pondered self-similarity informally such as in the infinite regress in parallel mirrors or the homunculus, the little man inside the head of the little man inside the head...). The difference for fractals is that the pattern reproduced must be detailed.
This idea of being detailed relates to another feature that can be understood without mathematical background: Having a fractional or fractal dimension greater than its topological dimension, for instance, refers to how a fractal scales compared to how geometric shapes are usually perceived. A regular line, for instance, is conventionally understood to be 1-dimensional; if such a curve is divided into pieces each 1/3 the length of the original, there are always 3 equal pieces. In contrast, consider the Koch snowflake. It is also 1-dimensional for the same reason as the ordinary line, but it has, in addition, a fractal dimension greater than 1 because of how its detail can be measured. The fractal curve divided into parts 1/3 the length of the original line becomes 4 pieces rearranged to repeat the original detail, and this unusual relationship is the basis of its fractal dimension.
This also leads to understanding a third feature, that fractals as mathematical equations are "nowhere differentiable". In a concrete sense, this means fractals cannot be measured in traditional ways. To elaborate, in trying to find the length of a wavy non-fractal curve, one could find straight segments of some measuring tool small enough to lay end to end over the waves, where the pieces could get small enough to be considered to conform to the curve in the normal manner of measuring with a tape measure. But in measuring a wavy fractal curve such as the Koch snowflake, one would never find a small enough straight segment to conform to the curve, because the wavy pattern would always re-appear, albeit at a smaller size, essentially pulling a little more of the tape measure into the total length measured each time one attempted to fit it tighter and tighter to the curve.
History.
The history of fractals traces a path from chiefly theoretical studies to modern applications in computer graphics, with several notable people contributing canonical fractal forms along the way. According to Pickover, the mathematics behind fractals began to take shape in the 17th century when the mathematician and philosopher Gottfried Leibniz pondered recursive self-similarity (although he made the mistake of thinking that only the straight line was self-similar in this sense). In his writings, Leibniz used the term "fractional exponents", but lamented that "Geometry" did not yet know of them. Indeed, according to various historical accounts, after that point few mathematicians tackled the issues and the work of those who did remained obscured largely because of resistance to such unfamiliar emerging concepts, which were sometimes referred to as mathematical "monsters". Thus, it was not until two centuries had passed that in 1872 Karl Weierstrass presented the first definition of a function with a graph that would today be considered fractal, having the non-intuitive property of being everywhere continuous but nowhere differentiable. Not long after that, in 1883, Georg Cantor, who attended lectures by Weierstrass, published examples of subsets of the real line known as Cantor sets, which had unusual properties and are now recognized as fractals. Also in the last part of that century, Felix Klein and Henri Poincaré introduced a category of fractal that has come to be called "self-inverse" fractals.
One of the next milestones came in 1904, when Helge von Koch, extending ideas of Poincaré and dissatisfied with Weierstrass's abstract and analytic definition, gave a more geometric definition including hand drawn images of a similar function, which is now called the Koch snowflake. Another milestone came a decade later in 1915, when Wacław Sierpiński constructed his famous triangle then, one year later, his carpet. By 1918, two French mathematicians, Pierre Fatou and Gaston Julia, though working independently, arrived essentially simultaneously at results describing what are now seen as fractal behaviour associated with mapping complex numbers and iterative functions and leading to further ideas about attractors and repellors (i.e., points that attract or repel other points), which have become very important in the study of fractals. Very shortly after that work was submitted, by March 1918, Felix Hausdorff expanded the definition of "dimension", significantly for the evolution of the definition of fractals, to allow for sets to have noninteger dimensions. The idea of self-similar curves was taken further by Paul Lévy, who, in his 1938 paper "Plane or Space Curves and Surfaces Consisting of Parts Similar to the Whole" described a new fractal curve, the Lévy C curve.
Different researchers have postulated that without the aid of modern computer graphics, early investigators were limited to what they could depict in manual drawings, so lacked the means to visualize the beauty and appreciate some of the implications of many of the patterns they had discovered (the Julia set, for instance, could only be visualized through a few iterations as very simple drawings]). That changed, however, in the 1960s, when Benoît Mandelbrot started writing about self-similarity in papers such as "How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension", which built on earlier work by Lewis Fry Richardson. In 1975 Mandelbrot solidified hundreds of years of thought and mathematical development in coining the word "fractal" and illustrated his mathematical definition with striking computer-constructed visualizations. These images, such as of his canonical Mandelbrot set, captured the popular imagination; many of them were based on recursion, leading to the popular meaning of the term "fractal". Currently, fractal studies are essentially exclusively computer-based.
In 1980, Loren Carpenter gave a presentation at the SIGGRAPH where he introduced his software for generating and rendering fractally generated landscapes.
Characteristics.
One often cited description that Mandelbrot published to describe geometric fractals is "a rough or fragmented geometric shape that can be split into parts, each of which is (at least approximately) a reduced-size copy of the whole"; this is generally helpful but limited. Authors disagree on the exact definition of "fractal", but most usually elaborate on the basic ideas of self-similarity and an unusual relationship with the space a fractal is embedded in. One point agreed on is that fractal patterns are characterized by fractal dimensions, but whereas these numbers quantify complexity (i.e., changing detail with changing scale), they neither uniquely describe nor specify details of how to construct particular fractal patterns. In 1975 when Mandelbrot coined the word "fractal", he did so to denote an object whose Hausdorff–Besicovitch dimension is greater than its topological dimension. It has been noted that this dimensional requirement is not met by fractal space-filling curves such as the Hilbert curve.
According to Falconer, rather than being strictly defined, fractals should, in addition to being nowhere differentiable and able to have a fractal dimension, be generally characterized by a gestalt of the following features;
As a group, these criteria form guidelines for excluding certain cases, such as those that may be self-similar without having other typically fractal features. A straight line, for instance, is self-similar but not fractal because it lacks detail, is easily described in Euclidean language, has the same Hausdorff dimension as topological dimension, and is fully defined without a need for recursion.
Brownian motion.
A path generated by a one dimensional Wiener process is a fractal curve of dimension 1.5, and Brownian motion is a finite version of this.
Common techniques for generating fractals.
Images of fractals can be created by fractal generating programs.
Simulated fractals.
Fractal patterns have been modeled extensively, albeit within a range of scales rather than infinitely, owing to the practical limits of physical time and space. Models may simulate theoretical fractals or natural phenomena with fractal features. The outputs of the modelling process may be highly artistic renderings, outputs for investigation, or benchmarks for fractal analysis. Some specific applications of fractals to technology are listed elsewhere. Images and other outputs of modelling are normally referred to as being "fractals" even if they do not have strictly fractal characteristics, such as when it is possible to zoom into a region of the fractal image that does not exhibit any fractal properties. Also, these may include calculation or display artifacts which are not characteristics of true fractals.
Modeled fractals may be sounds, digital images, electrochemical patterns, circadian rhythms, etc.
Fractal patterns have been reconstructed in physical 3-dimensional space and virtually, often called "in silico" modeling. Models of fractals are generally created using fractal-generating software that implements techniques such as those outlined above. As one illustration, trees, ferns, cells of the nervous system, blood and lung vasculature, and other branching patterns in nature can be modeled on a computer by using recursive algorithms and L-systems techniques. The recursive nature of some patterns is obvious in certain examples—a branch from a tree or a frond from a fern is a miniature replica of the whole: not identical, but similar in nature. Similarly, random fractals have been used to describe/create many highly irregular real-world objects. A limitation of modeling fractals is that resemblance of a fractal model to a natural phenomenon does not prove that the phenomenon being modeled is formed by a process similar to the modeling algorithms.
Natural phenomena with fractal features.
Approximate fractals found in nature display self-similarity over extended, but finite, scale ranges. The connection between fractals and leaves, for instance, is currently being used to determine how much carbon is contained in trees. Phenomena known to have fractal features include: 
<br>
In creative works.
The paintings of American artist Jackson Pollock have a definite fractal dimension. While Pollock's paintings appear to be composed of chaotic dripping and splattering, computer analysis demonstrates a degree of self-similarity at different scales (levels of detail) in his work.
Decalcomania, a technique used by artists such as Max Ernst, can produce fractal-like patterns. It involves pressing paint between two surfaces and pulling them apart.
Cyberneticist Ron Eglash has suggested that fractal geometry and mathematics are prevalent in African art, games, divination, trade, and architecture. Circular houses appear in circles of circles, rectangular houses in rectangles of rectangles, and so on. Such scaling patterns can also be found in African textiles, sculpture, and even cornrow hairstyles.
In a 1996 interview with Michael Silverblatt, David Foster Wallace admitted that the structure of the first draft of "Infinite Jest" he gave to his editor Michael Pietsch was inspired by fractals, specifically the Sierpinski triangle (a.k.a. Sierpinski gasket) but that the edited novel is "more like a lopsided Sierpinsky Gasket".

</doc>
<doc id="10915" url="https://en.wikipedia.org/wiki?curid=10915" title="Fluid">
Fluid

In physics, a fluid is a substance that continually deforms (flows) under an applied shear stress. Fluids are a subset of the phases of matter and include liquids, gases, plasmas and, to some extent, plastic solids. Fluids can be defined as substances that have zero shear modulus or in simpler terms a fluid is a substance which cannot resist any shear force applied to it.
Although the term "fluid" includes both the liquid and gas phases, in common usage, "fluid" is often used as a synonym for "liquid", with no implication that gas could also be present. For example, "brake fluid" is hydraulic oil and will not perform its required incompressible function if there is gas in it. This colloquial usage of the term is also common in medicine and in nutrition ("take plenty of fluids").
Liquids form a free surface (that is, a surface not created by the container) while gases do not. The distinction between solids and fluid is not entirely obvious. The distinction is made by evaluating the viscosity of the substance. Silly Putty can be considered to behave like a solid or a fluid, depending on the time period over which it is observed. It is best described as a viscoelastic fluid. There are many examples of substances proving difficult to classify. A particularly interesting one is pitch, as demonstrated in the pitch drop experiment currently running at the University of Queensland.
Physics.
Fluids display properties such as:
These properties are typically a function of their inability to support a shear stress in static equilibrium.
Solids can be subjected to shear stresses, and to normal stresses—both compressive and tensile. In contrast, ideal fluids can only be subjected to normal, compressive stress which is called pressure. Real fluids display viscosity and so are capable of being subjected to low levels of shear stress.
Modelling.
In a solid, shear stress is a function of strain, but in a fluid, shear stress is a function of strain rate. A consequence of this behavior is Pascal's law which describes the role of pressure in characterizing a fluid's state. 
Depending on the relationship between shear stress, and the rate of strain and its derivatives, fluids can be characterized as one of the following:
The behavior of fluids can be described by the Navier–Stokes equations—a set of partial differential equations which are based on:
The study of fluids is fluid mechanics, which is subdivided into fluid dynamics and fluid statics depending on whether the fluid is in motion.

</doc>
<doc id="10916" url="https://en.wikipedia.org/wiki?curid=10916" title="FAQ">
FAQ

Frequently asked questions (FAQ) or Questions and Answers (Q&A), are listed questions and answers, all supposed to be commonly asked in some context, and pertaining to a particular topic. The format is commonly used on email mailing lists and other online forums, where certain common questions tend to recur.
"FAQ" is pronounced as either an initialism (F-A-Q) or an acronym. Since the acronym "FAQ" originated in textual media, its pronunciation varies; "F-A-Q",and "fack", are commonly heard. Depending on usage, the term may refer specifically to a single frequently asked question, or to an assembled list of many questions and their answers. Web page designers often label a single list of questions as a "FAQ", such as on Google.com, while using "FAQs" to denote multiple lists of questions such as on United States Treasury sites.
Origins.
While the name may be recent, the FAQ format itself is quite old. For instance, Matthew Hopkins wrote "The Discovery of Witches" in 1647 as a list of questions and answers, introduced as "Certaine Queries answered". Many old catechisms are in a question-and-answer (Q&A) format. Summa Theologica, written by Thomas Aquinas in the second half of the 13th century, is a series of common questions about Christianity to which he wrote a series of replies. Plato's dialogues are even older.
The "FAQ" is an Internet textual tradition originating from the technical limitations of early mailing lists from NASA in the early 1980s. The first FAQ developed over several pre-Web years starting from 1982 when storage was expensive. On ARPAnet's SPACE mailing list, the presumption was that new users would download archived past messages through ftp. In practice, this rarely happened and the users tended to post questions to the mailing list instead of searching its archives. Repeating the "right" answers becomes tedious, and went against developing netiquette. A series of different measures were set up by loosely affiliated groups of computer system administrators, from regularly posted messages to netlib-like query email daemons. The acronym "FAQ" was developed between 1982 and 1985 by Eugene Miya of NASA for the SPACE mailing list. The format was then picked up on other mailing lists and Usenet news groups. Posting frequency changed to monthly, and finally weekly and daily across a variety of mailing lists and newsgroups. The first person to post a weekly FAQ was Jef Poskanzer to the Usenet net.graphics/comp.graphics newsgroups. Eugene Miya experimented with the first daily FAQ.
Meanwhile, on Usenet, Mark Horton had started a series of "Periodic Posts" (PP) which attempted to answer trivial questions with appropriate answers. Periodic summary messages posted to Usenet newsgroups attempted to reduce the continual reposting of the same basic questions and associated wrong answers. On Usenet, posting questions which are covered in a group's FAQ came to be considered poor netiquette, as it showed that the poster has not done the expected background reading before asking others to provide answers. Some groups may have multiple FAQ on related topics, or even two or more competing FAQs explaining a topic from different points of view.
Another factor on early ARPANET mailing lists was people asking questions promising to 'summarize' received answers, then either neglecting to do this or else posting simple concatenations of received replies with little to no quality checking.
Modern developments.
Originally the term "FAQ" referred to the Frequently Asked Question itself, and the compilation of questions and answers was known as a "FAQ list" or some similar expression. The term became more frequently used to refer to the list, and a text consisting of questions and their answers is often called a FAQ regardless of whether the questions are actually "frequently" asked, if they are asked at all, or if there is even any way of asking questions.
In some cases informative documents not in the traditional FAQ style have also been described as FAQs, particularly the video game FAQ, which is often a detailed descriptions of gameplay, including tips, secrets, and beginning-to-end guidance. Rarely are videogame FAQs in a question-and-answer format, although they may contain a short section of questions and answers.
Over time, the accumulated FAQs across all USENET news groups sparked the creation of the "*.answers" moderated newsgroups such as comp.answers, misc.answers and sci.answers for crossposting and collecting FAQ across respective comp.*, misc.*, sci.* newsgroups.

</doc>
<doc id="10918" url="https://en.wikipedia.org/wiki?curid=10918" title="Fibonacci number">
Fibonacci number

In mathematics, the Fibonacci numbers or Fibonacci sequence are the numbers in the following integer sequence:
or (often, in modern usage):
By definition, the first two numbers in the Fibonacci sequence are either 1 and 1, or 0 and 1, depending on the chosen starting point of the sequence, and each subsequent number is the sum of the previous two.
In mathematical terms, the sequence "Fn" of Fibonacci numbers is defined by the recurrence relation
with seed values
or
The Fibonacci sequence is named after Italian mathematician Leonardo of Pisa, known as Fibonacci. His 1202 book "Liber Abaci" introduced the sequence to Western European mathematics, although the sequence had been described earlier as Virahanka numbers in Indian mathematics. By modern convention, the sequence begins either with "F"0 = 0 or with "F"1 = 1. The sequence described in "Liber Abaci" began with "F"1 = 1.
Fibonacci numbers are closely related to Lucas numbers formula_6 in that they form a complementary pair of Lucas sequences formula_7 and formula_8. They are intimately connected with the golden ratio; for example, the closest rational approximations to the ratio are 2/1, 3/2, 5/3, 8/5, ... .
Fibonacci numbers appear unexpectedly often in mathematics, so much so that there is an entire journal dedicated to their study, the Fibonacci Quarterly. Applications of Fibonacci numbers include computer algorithms such as the Fibonacci search technique and the Fibonacci heap data structure, and graphs called Fibonacci cubes used for interconnecting parallel and distributed systems. They also appear in biological settings, such as branching in trees, phyllotaxis (the arrangement of leaves on a stem), the fruit sprouts of a pineapple, the flowering of an artichoke, an uncurling fern and the arrangement of a pine cone's bracts.
Origins.
The Fibonacci sequence appears in Indian mathematics, in connection with Sanskrit prosody. In the Sanskrit tradition of prosody, there was interest in enumerating all patterns of long (L) syllables that are 2 units of duration, and short (S) syllables that are 1 unit of duration. Counting the different patterns of L and S of a given duration results in the Fibonacci numbers: the number of patterns that are "m" short syllables long is the Fibonacci number "F""m" + 1.
Susantha Goonatilake writes that the development of the Fibonacci sequence "is attributed in part to Pingala (200 BC), later being associated with Virahanka (c. 700 AD), Gopāla (c. 1135), and Hemachandra (c. 1150)". Parmanand Singh cites Pingala's cryptic formula "misrau cha" ("the two are mixed") and cites scholars who interpret it in context as saying that the cases for "m" beats ("F""m+1") is obtained by adding a to "F""m" cases and [L to the "F""m"−1 cases. He dates Pingala before 450 BC.
However, the clearest exposition of the sequence arises in the work of Virahanka (c. 700 AD), whose own work is lost, but is available in a quotation by Gopala (c. 1135):
The sequence is also discussed by Gopala (before 1135 AD) and by the Jain scholar Hemachandra (c. 1150).
Outside of India, the Fibonacci sequence first appears in the book "Liber Abaci" (1202) by Fibonacci. Fibonacci considers the growth of an idealized (biologically unrealistic) rabbit population, assuming that: a newly born pair of rabbits, one male, one female, are put in a field; rabbits are able to mate at the age of one month so that at the end of its second month a female can produce another pair of rabbits; rabbits never die and a mating pair always produces one new pair (one male, one female) every month from the second month on. The puzzle that Fibonacci posed was: how many pairs will there be in one year?
At the end of the "n"th month, the number of pairs of rabbits is equal to the number of new pairs (which is the number of pairs in month "n" − 2) plus the number of pairs alive last month ("n" − 1). This is the "n"th Fibonacci number.
The name "Fibonacci sequence" was first used by the 19th-century number theorist Édouard Lucas.
List of Fibonacci numbers.
The first 21 Fibonacci numbers "Fn" for "n" = 0, 1, 2, …, 20 are:
The sequence can also be extended to negative index "n" using the re-arranged recurrence relation
Thus the bidirectional sequence is
Use in mathematics.
The Fibonacci numbers occur in the sums of "shallow" diagonals in Pascal's triangle (see Binomial coefficient).
These numbers also give the solution to certain enumerative problems. The most common such problem is that of counting the number of compositions of 1s and 2s that sum to a given total "n": there are "F""n"+1 ways to do this.
For example, if "n" = 5, then "F""n"+1 = "F"6 = 8 counts the eight compositions:
1+1+1+1+1 = 1+1+1+2 = 1+1+2+1 = 1+2+1+1 = 2+1+1+1 = 2+2+1 = 2+1+2 = 1+2+2,
all of which sum to "n" = 5 = 6−1.
The Fibonacci numbers can be found in different ways among the set of binary strings, or equivalently, among the subsets of a given set.
Relation to the golden ratio.
Closed-form expression.
Like every sequence defined by a linear recurrence with constant coefficients, the Fibonacci numbers have a closed-form solution. It has become known as "Binet's formula", even though it was already known by Abraham de Moivre:
where
is the golden ratio (), and
Since formula_15, this formula can also be written as
formula_16
To see this, note that φ and ψ are both solutions of the equations
so the powers of φ and ψ satisfy the Fibonacci recursion. In other words,
and
It follows that for any values "a" and "b", the sequence defined by
satisfies the same recurrence
If "a" and "b" are chosen so that "U"0 = 0 and "U"1 = 1 then the resulting sequence "U""n" must be the Fibonacci sequence. This is the same as requiring "a" and "b" satisfy the system of equations:
which has solution
producing the required formula.
Through algebraic manipulation, the equation can be rewritten to form a solution for any sequence formula_24:
formula_25
Computation by rounding.
Since
for all "n" ≥ 0, the number "F""n" is the closest integer to formula_27. Therefore, it can be found by rounding, that is by the use of the nearest integer function:
or in terms of the floor function:
Similarly, if we already know that the number "F" > 1 is a Fibonacci number, we can determine its index within the sequence by
Limit of consecutive quotients.
Johannes Kepler observed that the ratio of consecutive Fibonacci numbers converges. He wrote that "as 5 is to 8 so is 8 to 13, practically, and as 8 is to 13, so is 13 to 21 almost", and concluded that the limit approaches the golden ratio formula_31.
This convergence holds regardless of the starting values, excluding 0, 0. This can be derived from Binet's formula.
For example, the initial values 3 and 2 generate the sequence 3, 2, 5, 7, 12, 19, 31, 50, 81, 131, 212, 343, 555, …, etc. The ratio of consecutive terms in this sequence shows the same convergence towards the golden ratio.
Another consequence is that the limit of the ratio of two Fibonacci numbers offset by a particular finite deviation in index corresponds to the golden ratio raised by that deviation. Or, in other words:
Decomposition of powers of the golden ratio.
Since the golden ratio satisfies the equation
this expression can be used to decompose higher powers formula_35 as a linear function of lower powers, which in turn can be decomposed all the way down to a linear combination of formula_31 and 1. The resulting recurrence relationships yield Fibonacci numbers as the linear coefficients:
This equation can be proved by induction on "n".
This expression is also true for "n" < 1 if the Fibonacci sequence "Fn" is extended to negative integers using the Fibonacci rule formula_38
Matrix form.
A 2-dimensional system of linear difference equations that describes the Fibonacci sequence is
which yields formula_40. As the eigenvalues of the matrix A are formula_41 and formula_42, for the respective eigenvectors formula_43 and formula_44,
and the initial value formula_45, the th term is
from which the th element in the Fibonacci series
as an analytic function of is now read off directly:
Equivalently, the same computation is performed by diagonalization of A through use of its eigendecomposition:
where formula_49 and formula_50 .
The closed-form expression for the th element in the Fibonacci series is therefore given by
which again yields
The matrix A has a determinant of −1, and thus it is a 2×2 unimodular matrix.
This property can be understood in terms of the continued fraction representation for the golden ratio:
The Fibonacci numbers occur as the ratio of successive convergents of the continued fraction for , and the matrix formed from successive convergents of any continued fraction has a determinant of +1 or −1. The matrix representation gives the following closed expression for the Fibonacci numbers:
Taking the determinant of both sides of this equation yields Cassini's identity,
Moreover, since for any square matrix A, the following identities can be derived (they are obtained form two different coefficients of the matrix product, and one may easily deduce the second one from the first one by changing into ),
In particular, with "m" = "n",
These last two identities provide a way to compute Fibonacci numbers recursively in arithmetic operations and in time , where is the time for the multiplication of two numbers of "n" digits. This matches the time for computing the "n"th Fibonacci number from the closed-form matrix formula, but with fewer redundant steps if one avoids recomputing an already computed Fibonacci number (recursion with memoization).
Recognizing Fibonacci numbers.
The question may arise whether a positive integer "x" is a Fibonacci number. This is true if and only if one or both of formula_58 or formula_59 is a perfect square. This is because Binet's formula above can be rearranged to give
which allows one to find the position in the sequence of a given Fibonacci number.
This formula must return an integer for all "n", so the radical expression must be an integer (otherwise the logarithm does not even return a rational number).
Combinatorial identities.
Most identities involving Fibonacci numbers can be proved using combinatorial arguments using the fact that "F""n" can be interpreted as the number of sequences of 1s and 2s that sum to "n" − 1. This can be taken as the definition of "F""n", with the convention that "F"0 = 0, meaning no sum adds up to −1, and that "F"1 = 1, meaning the empty sum "adds up" to 0. Here, the order of the summand matters. For example, 1 + 2 and 2 + 1 are considered two different sums.
For example, the recurrence relation
or in words, the "n"th Fibonacci number is the sum of the previous two Fibonacci numbers, may be shown by dividing the "F""n" sums of 1s and 2s that add to "n" − 1 into two non-overlapping groups. One group contains those sums whose first term is 1 and the other those sums whose first term is 2. In the first group the remaining terms add to "n" − 2, so it has "F""n"-1 sums, and in the second group the remaining terms add to "n" − 3, so there are "F""n"−2 sums. So there are a total of "F""n"−1 + "F""n"−2 sums altogether, showing this is equal to "F""n".
Similarly, it may be shown that the sum of the first Fibonacci numbers up to the "n"th is equal to the ("n" + 2)-nd Fibonacci number minus 1. In symbols:
This is done by dividing the sums adding to "n" + 1 in a different way, this time by the location of the first 2. Specifically, the first group consists of those sums that start with 2, the second group those that start 1 + 2, the third 1 + 1 + 2, and so on, until the last group, which consists of the single sum where only 1's are used. The number of sums in the first group is "F"("n"), "F"("n" − 1) in the second group, and so on, with 1 sum in the last group. So the total number of sums is "F"("n") + "F"("n" − 1) + ... + "F"(1) + 1 and therefore this quantity is equal to "F"("n" + 2).
A similar argument, grouping the sums by the position of the first 1 rather than the first 2, gives two more identities:
and
In words, the sum of the first Fibonacci numbers with odd index up to "F"2"n"−1 is the (2"n")th Fibonacci number, and the sum of the first Fibonacci numbers with even index up to "F"2"n" is the (2"n" + 1)th Fibonacci number minus 1.
A different trick may be used to prove
or in words, the sum of the squares of the first Fibonacci numbers up to "F""n" is the product of the "n"th and ("n" + 1)th Fibonacci numbers. In this case note that Fibonacci rectangle of size "F""n" by "F"("n" + 1) can be decomposed into squares of size "F""n", "F""n"−1, and so on to "F"1 = 1, from which the identity follows by comparing areas.
Other identities.
Numerous other identities can be derived using various methods. Some of the most noteworthy are:
Cassini and Catalan's identities.
Cassini's identity states that
Catalan's identity is a generalization:
d'Ocagne's identity.
where "L""n" is the "n"'th Lucas number. The last is an identity for doubling "n"; other identities of this type are
by Cassini's identity.
These can be found experimentally using lattice reduction, and are useful in setting up the special number field sieve to factorize a Fibonacci number.
More generally,
Putting in this formula, one gets again the formulas of the end of above section Matrix form.
Power series.
The generating function of the Fibonacci sequence is the power series
This series is convergent for formula_76 and its sum has a simple closed-form:
This can be proved by using the Fibonacci recurrence to expand each coefficient in the infinite sum:
Solving the equation
for "s"("x") results in the above closed form.
If is the reciprocal of an integer "k" that is greater than 1, the closed form of the series becomes
In particular,
for all positive integers "m".
Some math puzzle-books present as curious the particular value that comes from "m"=1, which is formula_82 Similarly, "m"=2 gives formula_83
Reciprocal sums.
Infinite sums over reciprocal Fibonacci numbers can sometimes be evaluated in terms of theta functions. For example, we can write the sum of every odd-indexed reciprocal Fibonacci number as
and the sum of squared reciprocal Fibonacci numbers as
If we add 1 to each Fibonacci number in the first sum, there is also the closed form
and there is a "nested" sum of squared Fibonacci numbers giving the reciprocal of the golden ratio,
No closed formula for the reciprocal Fibonacci constant
is known, but the number has been proved irrational by Richard André-Jeannin.
The Millin series gives the identity
which follows from the closed form for its partial sums as "N" tends to infinity:
Primes and divisibility.
Divisibility properties.
Every 3rd number of the sequence is even and more generally, every "k"th number of the sequence is a multiple of "Fk". Thus the Fibonacci sequence is an example of a divisibility sequence. In fact, the Fibonacci sequence satisfies the stronger divisibility property
Any three consecutive Fibonacci numbers are pairwise coprime, which means that, for every "n",
Every prime number "p" divides a Fibonacci number that can be determined by the value of "p" modulo 5. If "p" is congruent to 1 or 4 (mod 5), then "p" divides "F""p" − 1, and if "p" is congruent to 2 or 3 (mod 5), then, "p" divides "F""p" + 1. The remaining case is that "p" = 5, and in this case "p" divides "F"p. These cases can be combined into a single formula, using the Legendre symbol:
Primality testing.
The above formula can be used as a primality test in the sense that if
When m is large—say a 500-bit number—then we can calculate Fm (mod n) efficiently using the matrix form. Thus
Here the matrix power Am is calculated using Modular exponentiation, which can be adapted to matrices--modular exponentiation for matrices
Fibonacci primes.
A "Fibonacci prime" is a Fibonacci number that is prime. The first few are:
Fibonacci primes with thousands of digits have been found, but it is not known whether there are infinitely many.
"F""kn" is divisible by "F""n", so, apart from "F"4 = 3, any Fibonacci prime must have a prime index. As there are arbitrarily long runs of composite numbers, there are therefore also arbitrarily long runs of composite Fibonacci numbers.
No Fibonacci number greater than "F"6 = 8 is one greater or one less than a prime number.
The only nontrivial
square Fibonacci number is 144. Attila Pethő proved in 2001 that there is only a finite number of perfect power Fibonacci numbers. In 2006, Y. Bugeaud, M. Mignotte, and S. Siksek proved that 8 and 144 are the only such non-trivial perfect powers.
Prime divisors of Fibonacci numbers.
With the exceptions of 1, 8 and 144 ("F"1 = "F"2, "F"6 and "F"12) every Fibonacci number has a prime factor that is not a factor of any smaller Fibonacci number (Carmichael's theorem). As a result, 8 and 144 ("F"6 and "F"12) are the only Fibonacci numbers that are the product of other Fibonacci numbers .
The divisibility of Fibonacci numbers by a prime "p" is related to the Legendre symbol formula_96 which is evaluated as follows:
If "p" is a prime number then
For example,
It is not known whether there exists a prime "p" such that
Such primes (if there are any) would be called Wall–Sun–Sun primes.
Also, if "p" ≠ 5 is an odd prime number then:
Example 1. "p" = 7, in this case "p" ≡ 3 (mod 4) and we have:
Example 2. "p" = 11, in this case "p" ≡ 3 (mod 4) and we have:
Example 3. "p" = 13, in this case "p" ≡ 1 (mod 4) and we have:
Example 4. "p" = 29, in this case "p" ≡ 1 (mod 4) and we have:
For odd "n", all odd prime divisors of "F""n" are congruent to 1 modulo 4, implying that all odd divisors of "F""n" (as the products of odd prime divisors) are congruent to 1 modulo 4.
For example,
All known factors of Fibonacci numbers "F"("i") for all "i" < 50000 are collected at the relevant repositories.
Periodicity modulo "n".
If the members of the Fibonacci sequence are taken mod "n", the resulting sequence is periodic with period at most "6n". The lengths of the periods for various "n" form the so-called Pisano periods . Determining a general formula for the Pisano periods is an open problem, which includes as a subproblem a special instance of the problem of finding the multiplicative order of a modular integer or of an element in a finite field. However, for any particular "n", the Pisano period may be found as an instance of cycle detection.
Right triangles.
Starting with 5, every second Fibonacci number is the length of the hypotenuse of a right triangle with integer sides, or in other words, the largest number in a Pythagorean triple. The length of the longer leg of this triangle is equal to the sum of the three sides of the preceding triangle in this series of triangles, and the shorter leg is equal to the difference between the preceding bypassed Fibonacci number and the shorter leg of the preceding triangle.
The first triangle in this series has sides of length 5, 4, and 3. Skipping 8, the next triangle has sides of length 13, 12 (5 + 4 + 3), and 5 (8 − 3). Skipping 21, the next triangle has sides of length 34, 30 (13 + 12 + 5), and 16 (21 − 5). This series continues indefinitely. The triangle sides "a", "b", "c" can be calculated directly:
These formulas satisfy formula_116 for all "n", but they only represent triangle sides when "n" > 2.
Any four consecutive Fibonacci numbers "F""n", "F""n"+1, "F""n"+2 and "F""n"+3 can also be used to generate a Pythagorean triple in a different way:
Example 1: let the Fibonacci numbers be 1, 2, 3 and 5. Then:
Magnitude.
Since "Fn" is asymptotic to formula_119, the number of digits in "F""n" is asymptotic to formula_120. As a consequence, for every integer "d" > 1 there are either 4 or 5 Fibonacci numbers with "d" decimal digits.
More generally, in the base "b" representation, the number of digits in "F""n" is asymptotic to formula_121.
Applications.
The Fibonacci numbers are important in the computational run-time analysis of Euclid's algorithm to determine the greatest common divisor of two integers: the worst case input for this algorithm is a pair of consecutive Fibonacci numbers.
Brasch et al. 2012 show how a generalised Fibonacci sequence also can be connected to the field of economics. In particular, it is shown how a generalised Fibonacci sequence enters the control function of ﬁnite-horizon dynamic optimisation problems with one state and one control variable. The procedure is illustrated in an example often referred to as the Brock–Mirman economic growth model.
Yuri Matiyasevich was able to show that the Fibonacci numbers can be defined by a Diophantine equation, which led to his original solution of Hilbert's tenth problem.
The Fibonacci numbers are also an example of a complete sequence. This means that every positive integer can be written as a sum of Fibonacci numbers, where any one number is used once at most.
Moreover, every positive integer can be written in a unique way as the sum of "one or more" distinct Fibonacci numbers in such a way that the sum does not include any two consecutive Fibonacci numbers. This is known as Zeckendorf's theorem, and a sum of Fibonacci numbers that satisfies these conditions is called a Zeckendorf representation. The Zeckendorf representation of a number can be used to derive its Fibonacci coding.
Fibonacci numbers are used by some pseudorandom number generators.
They are also used in planning poker, which is a step in estimating in software development projects that use the Scrum (software development) methodology.
Fibonacci numbers are used in a polyphase version of the merge sort algorithm in which an unsorted list is divided into two lists whose lengths correspond to sequential Fibonacci numbers – by dividing the list so that the two parts have lengths in the approximate proportion φ. A tape-drive implementation of the polyphase merge sort was described in "The Art of Computer Programming".
Fibonacci numbers arise in the analysis of the Fibonacci heap data structure.
The Fibonacci cube is an undirected graph with a Fibonacci number of nodes that has been proposed as a network topology for parallel computing.
A one-dimensional optimization method, called the Fibonacci search technique, uses Fibonacci numbers.
The Fibonacci number series is used for optional lossy compression in the IFF 8SVX audio file format used on Amiga computers. The number series compands the original audio wave similar to logarithmic methods such as µ-law.
Since the conversion factor 1.609344 for miles to kilometers is close to the golden ratio (denoted φ), the decomposition of distance in miles into a sum of Fibonacci numbers becomes nearly the kilometer sum when the Fibonacci numbers are replaced by their successors. This method amounts to a radix 2 number register in golden ratio base φ being shifted. To convert from kilometers to miles, shift the register down the Fibonacci sequence instead.
In nature.
Fibonacci sequences appear in biological settings, in two consecutive Fibonacci numbers, such as branching in trees, arrangement of leaves on a stem, the fruitlets of a pineapple, the flowering of artichoke, an uncurling fern and the arrangement of a pine cone, and the family tree of honeybees. However, numerous poorly substantiated claims of Fibonacci numbers or golden sections in nature are found in popular sources, e.g., relating to the breeding of rabbits in Fibonacci's own unrealistic example, the seeds on a sunflower, the spirals of shells, and the curve of waves.
Przemysław Prusinkiewicz advanced the idea that real instances can in part be understood as the expression of certain algebraic constraints on free groups, specifically as certain Lindenmayer grammars.
A model for the pattern of florets in the head of a sunflower was proposed by H. Vogel in 1979. This has the form
where "n" is the index number of the floret and "c" is a constant scaling factor; the florets thus lie on Fermat's spiral. The divergence angle, approximately 137.51°, is the golden angle, dividing the circle in the golden ratio. Because this ratio is irrational, no floret has a neighbor at exactly the same angle from the center, so the florets pack efficiently. Because the rational approximations to the golden ratio are of the form "F"("j"):"F"("j" + 1), the nearest neighbors of floret number "n" are those at "n" ± "F"("j") for some index "j", which depends on "r", the distance from the center. It is often said that sunflowers and similar arrangements have 55 spirals in one direction and 89 in the other (or some other pair of adjacent Fibonacci numbers), but this is true only of one range of radii, typically the outermost and thus most conspicuous.
The bee ancestry code.
Fibonacci numbers also appear in the pedigrees of idealized honeybees, according to the following rules:
Thus, a male bee always has one parent, and a female bee has two.
If one traces the pedigree of any male bee (1 bee), he has 1 parent (1 bee), 2 grandparents, 3 great-grandparents, 5 great-great-grandparents, and so on. This sequence of numbers of parents is the Fibonacci sequence. The number of ancestors at each level, "F""n", is the number of female ancestors, which is "F""n"−1, plus the number of male ancestors, which is "F""n"−2. This is under the unrealistic assumption that the ancestors at each level are otherwise unrelated.
Generalizations.
The Fibonacci sequence has been generalized in many ways. These include:

</doc>
<doc id="10923" url="https://en.wikipedia.org/wiki?curid=10923" title="Fontainebleau">
Fontainebleau

Fontainebleau (; ) is a commune in the metropolitan area of Paris, France. It is located south-southeast of the centre of Paris. Fontainebleau is a sub-prefecture of the Seine-et-Marne department, and it is the seat of the "arrondissement" of Fontainebleau. The commune has the largest land area in the Île-de-France region; it is the only one to cover a larger area than Paris itself.
Fontainebleau, together with the neighbouring commune of Avon and three other smaller communes, form an urban area of 39,713 inhabitants (according to the 2001 census). This urban area is a satellite of Paris.
Fontainebleau is renowned for the large and scenic forest of Fontainebleau, a favourite weekend getaway for Parisians, as well as for the historical Château de Fontainebleau, which once belonged to the kings of France. It is also the home of INSEAD, one of the world's most elite business schools; of the "École supérieure d'ingénieurs en informatique et génie des télécommunications" (ESIGETEL), one of France's "grandes écoles"; and of a branch of the "École nationale supérieure des mines de Paris", the Paris School of Mines, also one of the elite "grandes écoles".
Inhabitants of Fontainebleau are called "Bellifontains".
History.
Fontainebleau has been recorded in different Latinised forms, such as, "Fons Bleaudi", "Fons Bliaudi", "Fons Blaadi" in the 12th and 13th centuries, with "Fontem blahaud" being recorded in 1137. It became "Fons Bellaqueus" in the 17th century, which gave rise to the name of the inhabitants as "Bellifontains". The name originates as a medieval composite of two words: "Fontaine–" meaning spring, or fountainhead, followed by a person’s Germanic name "Blizwald".
This hamlet was endowed with a royal hunting lodge and a chapel by Louis VII in the middle of the twelfth century. A century later, Louis IX, also called Saint Louis, who held Fontainebleau in high esteem and referred to it as "his wilderness", had a country house and a hospital constructed there.
Philip the Fair was born there in 1268 and died there in 1314. In all, thirty-four sovereigns, from Louis VI, the Fat, (1081–1137) to Napoleon III (1808–1873), spent time at Fontainebleau.
The connection between the town of Fontainebleau and the French monarchy was reinforced with the transformation of the royal country house into a true royal palace, the Palace of Fontainebleau. This was accomplished by the great builder-king, Francis I (1494–1547), who, in the largest of his many construction projects, reconstructed, expanded, and transformed the royal château at Fontainebleau into a residence that became his favourite, as well as the residence of his mistress, Anne, duchess of Étampes.
From the sixteenth to the eighteenth century, every monarch, from Francis I to Louis XV, made important renovations at the Palace of Fontainebleau, including demolitions, reconstructions, additions, and embellishments of various descriptions, all of which endowed it with a character that is a bit heterogeneous, but harmonious nonetheless.
On 18 October 1685, Louis XIV signed the "Edict of Fontainebleau" there. Also known as the "Revocation of the Edict of Nantes", this royal fiat reversed the permission granted to the Huguenots in 1598 to worship publicly in specified locations and hold certain other privileges. The result was that a large number of Protestants were forced to convert to the Catholic faith, killed, or forced into exile, mainly in the Low Countries, Prussia and in England.
The 1762 Treaty of Fontainebleau, a secret agreement between France and Spain concerning the Louisiana territory in North America, was concluded here. Also, preliminary negotiations, held before the 1763 Treaty of Paris was signed, ending the Seven Years' War, were at Fontainebleau.
During the French Revolution, Fontainebleau was temporarily renamed Fontaine-la-Montagne, meaning "Fountain by the Mountain". (The mountain referred to is the series of rocky formations located in the forest of Fontainebleau.)
On 29 October 1807, Manuel Godoy, chancellor to the Spanish king, Charles IV and Napoleon signed the Treaty of Fontainebleau, which authorized the passage of French troops through Spanish territories so that they might invade Portugal.
On 20 June 1812, Pope Pius VII arrived at the château of Fontainebleau, after a secret transfer from Savona, accompanied by his personal physician, Balthazard Claraz. In poor health, the Pope was the prisoner of Napoleon, and he remained in his genteel prison at Fontainebleau for nineteen months. From June 1812 until 23 January 1814, the Pope never left his apartments.
On 20 April 1814, Napoleon Bonaparte, shortly before his first abdication, bid farewell to the Old Guard, the renowned "grognards" (gripers) who had served with him since his very first campaigns, in the "White Horse Courtyard" (la cour du Cheval Blanc) at the Palace of Fontainebleau. (The courtyard has since been renamed the "Courtyard of Goodbyes".) According to contemporary sources, the occasion was very moving. The 1814 Treaty of Fontainebleau stripped Napoleon of his powers (but not his title as Emperor of the French) and sent him into exile on Elba.
Until the 19th century, Fontainebleau was a village and a suburb of Avon. Later, it developed as an independent residential city.
For the 1924 Summer Olympics, the town played host to the riding portion of the modern pentathlon event. This event took place near a golf course.
In July and August 1946, the town hosted the Franco-Vietnamese Conference, intended to find a solution to the long-contested struggle for Vietnam’s independence from France, but the conference ended in failure.
Fontainebleau also hosted the general staff of the Allied Forces in Central Europe (Allied Forces Center or AFCENT) and the land forces command (LANDCENT); the air forces command (AIRCENT) was located nearby at Camp Guynemer. These facilities were in place from the inception of NATO until France’s partial withdrawal from NATO in 1967 when the United States returned those bases to French control. NATO moved AFCENT to Brunssum in the Netherlands and AIRCENT to Ramstein in West Germany. (Note that the Supreme Headquarters Allied Powers Europe, also known as SHAPE, was located at Rocquencourt, west of Paris, quite a distance from Fontainebleau).
Tourism.
Fontainebleau is a popular tourist destination; each year, 300,000 people visit the palace and about 11 million people visit the forest.
Fontainebleau forest.
The forest of Fontainebleau surrounds the town and dozens of nearby villages. It is protected by France's "Office National des Forêts", and it is recognised as a French national park. It is managed in order that its wild plants and trees, such as the rare service tree of Fontainebleau, and its populations of birds, mammals, and butterflies, can be conserved. It is a former royal hunting park often visited by hikers and horse riders. The forest is also well regarded for bouldering and is particularly popular among climbers, as the biggest developed area of that kind in the world.
Royal Château de Fontainebleau.
The Royal Château de Fontainebleau is a large palace where the kings of France took their ease. It is also the site where the French royal court, from 1528 onwards, entertained the body of new ideas that became known as the Renaissance.
INSEAD.
The European (and historical) campus of the INSEAD business school is located at the edge of Fontainebleau, by the Lycee Francois Couperin. INSEAD students live in local accommodations around the Fontainebleau area, and especially in the surrounding towns.
Other Notables.
The graves of G. I. Gurdjieff and Katherine Mansfield can be found in the cemetery at Avon.
Transport.
Fontainebleau is served by two stations on the Transilien Paris–Lyon rail line: Fontainebleau–Avon and Thomery. Fontainebleau–Avon station, the station closest to the centre of Fontainebleau, is located near the dividing-line between the commune of Fontainebleau and the commune of Avon, on the Avon side of the border.
Twinning.
Fontainebleau is twinned with the following cities:

</doc>
<doc id="10929" url="https://en.wikipedia.org/wiki?curid=10929" title="Fighter aircraft">
Fighter aircraft

A fighter aircraft is a military aircraft designed primarily for air-to-air combat against other aircraft, as opposed to bombers and attack aircraft, whose main mission is to attack ground targets. The hallmarks of a fighter are its speed, maneuverability, and small size relative to other combat aircraft.
Many fighters have secondary ground-attack capabilities, and some are designed as dual-purpose fighter-bombers; often aircraft that do not fulfill the standard definition are called fighters. This may be for political or national security reasons, for advertising purposes, or other reasons.
A fighter's main purpose is to establish air superiority over a battlefield. Since World War I, achieving and maintaining air superiority has been considered essential for victory in conventional warfare. The success or failure of a belligerent's efforts to gain air supremacy hinges on several factors including the skill of its pilots, the tactical soundness of its doctrine for deploying its fighters, and the numbers and performance of those fighters. Because of the importance of air superiority, since the dawn of aerial combat armed forces have constantly competed to develop technologically superior fighters and to deploy these fighters in greater numbers, and fielding a viable fighter fleet consumes a substantial proportion of the defense budgets of modern armed forces.
Terminology.
The word "fighter" did not become the official English-language term for such aircraft until after World War I. In the British Royal Flying Corps and Royal Air Force these aircraft were referred to as "scouts" into the early 1920s. The U.S. Army called their fighters "pursuit" aircraft from 1916 until the late 1940s. In most languages a fighter aircraft is known as a "hunter", or "hunting aircraft" ("avion de chasse, jagdflugzeuge, avión de caza" etc.). Exceptions include Russian, where a fighter is an "истребитель" (pronounced "istrebitel"), meaning "exterminator", and Hebrew where it is "matose krav" (literally "battle plane").
As a part of military nomenclature, a letter is often assigned to various types of aircraft to indicate their use, along with a number to indicate the specific aircraft. The letters used to designate a fighter differ in various countries — in the English-speaking world, "F" is now used to indicate a fighter (e.g. F-35 or Spitfire F.22), though when the pursuit designation was used in the US, they were "P" types (e.g. P-40). In Russia "I" was used (I-16), while the French continue to use "C" (Nieuport 17 C.1).
Although the term "fighter" specifies aircraft designed to shoot down other aircraft, such designs are often also useful as multirole fighter-bombers, strike fighters, and sometimes lighter, fighter-sized tactical ground-attack aircraft. This has always been the case, for instance the Sopwith Camel and other "fighting scouts" of World War I performed a great deal of ground-attack work. In World War II, the USAAF and RAF often favored fighters over dedicated light bombers or dive bombers, and types such as the P-47 Thunderbolt and Hawker Hurricane that were no longer competitive aerial combat fighters were relegated to ground attack. Several aircraft, such as the F-111 and F-117, have received fighter designations but had no fighter capability due to political or other reasons. The F-111B variant was originally intended for a fighter role with the U.S. Navy, but it was cancelled. This blurring follows the use of fighters from their earliest days for "attack" or "strike" operations against ground targets by means of strafing or dropping small bombs and incendiaries. Versatile multirole fighter-bombers such as the F/A-18 Hornet are a less expensive option than having a range of specialized aircraft types.
Some of the most expensive fighters such as the US F-14 Tomcat, F-15 Eagle, F-22 Raptor and Russian Su-27 were employed as all-weather interceptors as well as air superiority fighter aircraft, while commonly developing air-to-ground roles late in their careers. An interceptor is generally an aircraft intended to target (or intercept) bombers and so often trades maneuverability for climb rate.
Development overview.
Fighters were developed in World War I to deny enemy aircraft and dirigibles the ability to gather information by reconnaissance. Early fighters were very small and lightly armed by later standards, and most were biplanes built with a wooden frame, covered with fabric, and limited to about 100 mph. As control of the airspace over armies became increasingly important all of the major powers developed fighters to support their military operations. Between the wars, wood was largely replaced by steel tubing, then aluminium tubing, and finally aluminium stressed skin structures began to predominate.
By World War II, most fighters were all-metal monoplanes armed with batteries of machine guns or cannons and some were capable of speeds approaching 400 mph. Most fighters up to this point had one engine, but a number of twin-engine fighters were built; however they were found to be outmatched against single-engine fighters and were relegated to other tasks, such as night fighters equipped with primitive radar sets.
By the end of the war, turbojet engines were replacing piston engines as the means of propulsion, further increasing aircraft speed. Since the weight of the engine was so much less than on piston engined fighters, having two engines was no longer a handicap and one or two were used, depending on requirements. This in turn required the development of ejection seats so the pilot could escape and G-suits to counter the much greater forces being applied to the pilot during maneuvers.
In the 1950s, radar was fitted to day fighters, since pilots could no longer see far enough ahead to prepare for any opposition. Since then, radar capabilities have grown enormously and are now the primary method of target acquisition. Wings were made thinner and swept back to reduce transonic drag, which required new manufacturing methods to obtain sufficient strength. Skins were no longer sheet metal riveted to a structure, but milled from large slabs of alloy. The sound barrier was broken, and after a few false starts due to required changes in controls, speeds quickly reached Mach 2—past which aircraft can't maneuver to avoid attack.
Air-to-air missiles largely replaced guns and rockets in the early 1960s since both were believed unusable at the speeds being attained, however the Vietnam War showed that guns still had a role to play, and most fighters built since then are fitted with cannon (typically between 20 and 30 mm in caliber) in addition to missiles. Most modern combat aircraft can carry at least a pair of air-to-air missiles.
In the 1970s, turbofans replaced turbojets, improving fuel economy enough that the last piston engined support aircraft could be replaced with jets, making multi-role combat aircraft possible. Honeycomb structures began to replace milled structures, and the first composite components began to appear on components subjected to little stress.
With the steady improvements in computers, defensive systems have become increasingly efficient. To counter this, stealth technologies have been pursued by the United States, Russia, India and China. The first step was to find ways to reduce the aircraft's reflectivity to radar waves by burying the engines, eliminating sharp corners and diverting any reflections away from the radar sets of opposing forces. Various materials were found to absorb the energy from radar waves, and were incorporated into special finishes that have since found widespread application. Composite structures have become widespread, including major structural components, and have helped to counterbalance the steady increases in aircraft weight—most modern fighters are larger and heavier than World War II medium bombers.
Piston engine fighters.
World War I.
The word "fighter" was first used to describe a two-seater aircraft with sufficient lift to carry a machine gun and its operator as well as the pilot. Some of the first such "fighters" belonged to the "gunbus" series of experimental gun carriers of the British Vickers company that culminated in the Vickers F.B.5 Gunbus of 1914. The main drawback of this type of aircraft was its lack of speed. Planners quickly realized that an aircraft intended to destroy its kind in the air had to be fast enough to catch its quarry.
Another type of military aircraft was to form the basis for an effective "fighter" in the modern sense of the word. It was based on the small fast aircraft developed before the war for such air races as the Gordon Bennett Cup and Schneider Trophy. The military scout airplane was not expected to carry serious armament, but rather to rely on its speed to reach the scout or reconnoiter location and return quickly to report—essentially an aerial horse. British scout aircraft, in this sense, included the Sopwith Tabloid and Bristol Scout. French equivalents included the Morane-Saulnier N.
Soon after the commencement of the war, pilots armed themselves with pistols, carbines, grenades, and an assortment of improvised weapons. Many of these proved ineffective as the pilot had to fly his airplane while attempting to aim a handheld weapon and make a difficult deflection shot. The first step in finding a real solution was to mount the weapon on the aircraft, but the propeller remained a problem since the best direction to shoot is straight ahead. Numerous solutions were tried. A second crew member behind the pilot could aim and fire a swivel-mounted machine gun at enemy airplanes; however, this limited the area of coverage chiefly to the rear hemisphere, and effective coordination of the pilot's maneuvering with the gunner's aiming was difficult. This option was chiefly employed as a defensive measure on two-seater reconnaissance aircraft from 1915 on. Both the SPAD S.A and the Royal Aircraft Factory B.E.9 added a second crewman ahead of the engine in a pod but this was both hazardous to the second crewman and limited performance. The Sopwith L.R.T.Tr. similarly added a pod on the top wing with no better luck.
An alternative was to build a "pusher" scout such as the Airco DH.2, with the propeller mounted behind the pilot. The main drawback was that the high drag of a pusher type's tail structure made it slower than a similar "tractor" aircraft.
A better solution for a single seat scout was to mount the machine gun (rifles and pistols having been dispensed with) to fire forwards but outside the propeller arc. Wing guns were tried but the unreliable weapons available required frequent clearing of jammed rounds and misfires and remained impractical until after the war. Mounting the machine gun over the top wing worked well and was used long after the ideal solution was found. The Nieuport 11 of 1916 and Royal Aircraft Factory S.E.5 of 1918 both used this system with considerable success; however, this placement made aiming difficult and the location made it difficult for a pilot to both maneuver and have access to the gun's breech. The British Foster mounting was specifically designed for this kind of application, fitted with the Lewis Machine gun, which due to its design was unsuitable for synchronizing.
The need to arm a tractor scout with a forward-firing gun whose bullets passed through the propeller arc was evident even before the outbreak of war and inventors in both France and Germany devised mechanisms that could time the firing of the individual rounds to avoid hitting the propeller blades. Franz Schneider, a Swiss engineer, had patented such a device in Germany in 1913, but his original work was not followed up. French aircraft designer Raymond Saulnier patented a practical device in April 1914, but trials were unsuccessful because of the propensity of the machine gun employed to hang fire due to unreliable ammunition.
In December 1914, French aviator Roland Garros asked Saulnier to install his synchronization gear on Garros' Morane-Saulnier Type L. Unfortunately the gas-operated Hotchkiss machine gun he was provided had an erratic rate of fire and it was impossible to synchronize it with a spinning propeller. As an interim measure, the propeller blades were armored and fitted with metal wedges to protect the pilot from ricochets. Garros' modified monoplane was first flown in March 1915 and he began combat operations soon thereafter. Garros scored three victories in three weeks before he himself was downed on 18 April and his airplane, along with its synchronization gear and propeller was captured by the Germans.
Meanwhile, the synchronization gear (called the "Stangensteuerung" in German, for "pushrod control system") devised by the engineers of Anthony Fokker's firm was the first system to see production contracts, and would make the Fokker "Eindecker" monoplane a feared name over the Western Front, despite its being an adaptation of an obsolete pre-war French Morane-Saulnier racing airplane, with a mediocre performance and poor flight characteristics. The first victory for the "Eindecker" came on 1 July 1915, when "Leutnant" Kurt Wintgens, flying with the "Feldflieger Abteilung 6" unit on the Western Front, forced down a Morane-Saulnier Type L two-seat "parasol" monoplane just east of Luneville. Wintgens' aircraft, one of the five Fokker M.5K/MG production prototype examples of the "Eindecker", was armed with a synchronized, air-cooled aviation version of the Parabellum MG14 machine gun.
The success of the "Eindecker" kicked off a competitive cycle of improvement among the combatants, both sides striving to build ever more capable single-seat fighters. The Albatros D.I and Sopwith Pup of 1916 set the classic pattern followed by fighters for about twenty years. Most were biplanes and only rarely monoplanes or triplanes. The strong box structure of the biplane provided a rigid wing that allowed the accurate lateral control essential for dogfighting. They had a single operator, who flew the aircraft and also controlled its armament. They were armed with one or two Maxim or Vickers machine guns, which were easier to synchronize than other types, firing through the propeller arc. Gun breeches were directly in front of the pilot, with obvious implications in case of accidents, but jams could be cleared in flight, while aiming was simplified.
The use of metal aircraft structures was pioneered before World War I by Breguet but would find its biggest proponent in Anthony Fokker, who used chrome-molybdenum steel tubing for the fuselage structure of all his fighter designs, while the innovative German engineer Hugo Junkers developed two all-metal, single-seat fighter monoplane designs with cantilever wings: the strictly experimental Junkers J 2 private-venture aircraft, made with steel, and some forty examples of the Junkers D.I, made with corrugated duralumin, all based on his experience in creating the pioneering Junkers J 1 all-metal airframe technology demonstration aircraft of late 1915. While Fokker would pursue steel tube fuselages with wooden wings until the late 1930s, and Junkers would focus on corrugated sheet metal, Dornier was the first to build a fighter (The Dornier-Zeppelin D.I) made with pre-stressed sheet aluminium and having cantelevered wings, a form that would replace all others in the 1930s.
As collective combat experience grew, the more successful pilots such as Oswald Boelcke, Max Immelmann, and Edward Mannock developed innovative tactical formations and maneuvers to enhance their air units' combat effectiveness.
Allied and—before 1918—German pilots of World War I were not equipped with parachutes, so in-flight fires or structural failure were often fatal. Parachutes were well-developed by 1918 having previously been used by balloonists, and were adopted by the German flying services during the course of that year (the famous Manfred von Richthofen "Red Baron" was wearing one when he was killed), but the allied command continued to oppose their use on various grounds.
In April 1917, during a brief period of German aerial supremacy a British pilot's average life expectancy was 93 flying hours, or about three weeks of active service. More than 50,000 airmen from both sides died during the war.
Inter-war period (1919–38).
Fighter development stagnated between the wars, especially in the United States and the United Kingdom, where budgets were small. In France, Italy and Russia, where large budgets continued to allow major development, both monoplanes and all metal structures were common. By the end of the 1920s, however, those countries overspent themselves and were overtaken in the 1930s by those powers that hadn't been spending heavily, namely the British, the Americans and the Germans.
Given limited defense budgets, air forces tended to be conservative in their aircraft purchases, and biplanes remained popular with pilots because of their agility, and remained in service long after they had ceased to be competitive. Designs such as the Gloster Gladiator, Fiat CR.42, and Polikarpov I-15 were common even in the late 1930s, and many were still in service as late as 1942. Up until the mid-1930s, the majority of fighters in the US, the UK, Italy and Russia remained fabric-covered biplanes.
Fighter armament eventually began to be mounted inside the wings, outside the arc of the propeller, though most designs retained two synchronized machine guns directly ahead of the pilot, where they were more accurate (that being the strongest part of the structure, reducing the vibration to which the guns were subjected to). Shooting with this traditional arrangement was also easier for the further reason that the guns shot directly ahead in the direction of the aircraft's flight, up to the limit of the guns range; unlike wing-mounted guns which to be effective required to be harmonised, that is, preset to shoot at an angle by ground crews so that their bullets would converge on a target area a set distance ahead of the fighter. Rifle-caliber .30 and .303 in (7.62 mm) caliber guns remained the norm, with larger weapons either being too heavy and cumbersome or deemed unnecessary against such lightly built aircraft. It was not considered unreasonable to use World War I-style armament to counter enemy fighters as there was insufficient air-to-air combat during most of the period to disprove this notion.
The rotary engine, popular during World War I, quickly disappeared, its development having reached the point where rotational forces prevented more fuel and air from being delivered to the cylinders, which limited horsepower. They were replaced chiefly by the stationary radial engine though major advances led to inline engines, which gained ground with several exceptional engines—including the V-12 Curtiss D-12. Aircraft engines increased in power several-fold over the period, going from a typical in the 1918 Fokker D.VII to in the 1938 Curtiss P-36. The debate between the sleek in-line engines versus the more reliable radial models continued, with naval air forces preferring the radial engines, and land-based forces often choosing in-line units. Radial designs did not require a separate (and vulnerable) cooling system, but had increased drag. In-line engines often had a better power-to-weight ratio, but there were radial engines that kept working even after having suffered significant battle damage.
Some air forces experimented with "heavy fighters" (called "destroyers" by the Germans). These were larger, usually twin-engined aircraft, sometimes adaptations of light or medium bomber types. Such designs typically had greater internal fuel capacity (thus longer range) and heavier armament than their single-engine counterparts. In combat, they proved vulnerable to more agile single-engine fighters.
The primary driver of fighter innovation, right up to the period of rapid re-armament in the late 1930s, were not military budgets, but civilian aircraft racing. Aircraft designed for these races introduced innovations like streamlining and more powerful engines that would find their way into the fighters of World War II. The most significant of these was the Schneider Trophy races, where competition grew so fierce, only national governments could afford to enter.
At the very end of the inter-war period in Europe came the Spanish Civil War. This was just the opportunity the German "Luftwaffe", Italian "Regia Aeronautica", and the Soviet Union's Red Air Force needed to test their latest aircraft. Each party sent numerous aircraft types to support their sides in the conflict. In the dogfights over Spain, the latest Messerschmitt Bf 109 fighters did well, as did the Soviet Polikarpov I-16. The German design had considerably more room for development however and the lessons learned led to greatly improved models in World War II. The Russians, whose side lost, failed to keep up and despite newer models coming into service, I-16s were outfought by the improved Bf 109s in World War II, while remaining the most common Soviet front-line fighter into 1942. For their part, the Italians developed several monoplanes such as the Fiat G.50, but being short on funds, were forced to continue operating obsolete Fiat CR.42 biplanes.
From the early 1930s the Japanese had been at war against both the Chinese Nationalists and the Russians in China, and used the experience to improve both training and aircraft, replacing biplanes with modern cantilever monoplanes and creating a cadre of exceptional pilots for use in the Pacific War. In the United Kingdom, at the behest of Neville Chamberlain, (more famous for his 'peace in our time' speech) the entire British aviation industry was retooled, allowing it to change quickly from fabric covered metal framed biplanes to cantilever stressed skin monoplanes in time for the war with Germany.
The period of improving the same biplane design over and over was now coming to an end, and the Hawker Hurricane and Supermarine Spitfire finally started to supplant the Gloster Gladiator and Hawker Fury biplanes but many of the former remained in front-line service well past the start of World War II. While not a combatant themselves in Spain, they absorbed many of the lessons learned in time to use them.
The Spanish Civil War also provided an opportunity for updating fighter tactics. One of the innovations to result from the aerial warfare experience this conflict provided was the development of the "finger-four" formation by the German pilot Werner Mölders. Each fighter squadron (German: "Staffel") was divided into several flights ("Schwärme") of four aircraft. Each "Schwarm" was divided into two "Rotten", which was a pair of aircraft. Each "Rotte" was composed of a leader and a wingman. This flexible formation allowed the pilots to maintain greater situational awareness, and the two "Rotten" could split up at any time and attack on their own. The finger-four would become widely adopted as the fundamental tactical formation over the course of World War.
World War II.
World War II featured fighter combat on a larger scale than any other conflict to date. German Field Marshal Erwin Rommel noted the effect of airpower: "Anyone who has to fight, even with the most modern weapons, against an enemy in complete command of the air, fights like a savage against modern European troops, under the same handicaps and with the same chances of success." Throughout the war, fighters performed their conventional role in establishing air superiority through combat with other fighters and through bomber interception, and also often performed roles such as tactical air support and reconnaissance.
Fighter design varied widely among combatants. The Japanese and Italians favored lightly armed and armored but highly maneuverable designs such as the Japanese Nakajima Ki-27, Nakajima Ki-43 and Mitsubishi A6M Zero and Italy's Fiat G.50 and Macchi MC.200. In contrast, designers in Great Britain, Germany, the Soviet Union, and the United States believed that the increased speed of fighter aircraft would create "g"-forces unbearable to pilots who attempted maneuvering dogfights typical of the First World War, and their fighters were instead optimized for speed and firepower. In practice, while light, highly maneuverable aircraft did possess some advantages in fighter-versus-fighter combat, those could usually be overcome by sound tactical doctrine, and the design approach of the Italians and Japanese made their fighters ill-suited as interceptors or attack aircraft.
European theater.
During the invasion of Poland and the Battle of France, Luftwaffe fighters—primarily the Messerschmitt Bf 109—held air superiority, and the Luftwaffe played a major role in German victories in these campaigns. During the Battle of Britain, however, British Hurricanes and Spitfires proved roughly equal to Luftwaffe fighters. Additionally Britain's use of radar and the advantages of fighting above Britain's home territory allowed the RAF to deny Germany air superiority, saving Britain from possible German invasion and dealing the Axis a major defeat early in the Second World War.
On the Eastern Front, Soviet fighter forces were overwhelmed during the opening phases of Operation Barbarossa. This was a result of the tactical surprise at the outset of the campaign, the leadership vacuum within the Soviet military left by the Great Purge, and the general inferiority of Soviet designs at the time, such as the obsolescent I-15 biplane and the I-16. More modern Soviet designs, including the MiG-3, LaGG-3 and Yak-1, had not yet arrived in numbers and in any case were still inferior to the Messerschmitt Bf 109. As a result, during the early months of these campaigns, Axis air forces destroyed large numbers of Red Air Force aircraft on the ground and in one-sided dogfights.
In the later stages on the Eastern Front, Soviet training and leadership improved, as did their equipment. Late-war Soviet designs such as the Yakovlev Yak-3 and Lavochkin La-7 had performance comparable to the German Bf-109 and Focke-Wulf Fw 190. Also, significant numbers of British, and later U.S., fighter aircraft were supplied to aid the Soviet war effort as part of Lend-Lease, with the Bell P-39 Airacobra proving particularly effective in the lower-altitude combat typical of the Eastern Front. The Soviets were also helped indirectly by the American and British bombing campaigns, which forced the Luftwaffe to shift many of its fighters away from the Eastern Front in defense against these raids. The Soviets increasingly were able to challenge the Luftwaffe, and while the Luftwaffe maintained a qualitative edge over the Red Air Force for much of the war, the increasing numbers and efficacy of the Soviet Air Force were critical to the Red Army's efforts at turning back and eventually annihilating the Wehrmacht.
Meanwhile, air combat on the Western Front had a much different character. Much of this combat was centered around the strategic bombing campaigns of the RAF and the USAAF. Axis fighter aircraft focused on defending against Allied bombers while Allied fighters' main role was as bomber escorts. The RAF raided German cities at night, and both sides developed radar-equipped night fighters for these battles. The Americans, in contrast, flew daylight bombing raids into Germany. Unescorted Consolidated B-24 Liberators and Boeing B-17 Flying Fortress bombers, however, proved unable to fend off German interceptors (primarily Bf-109s and FW-190s). With the later arrival of long range fighters, particularly the North American P-51 Mustang, American fighters were able to escort daylight raids far into Germany and establish control of the skies over Western Europe.
By the time of Operation Overlord in June 1944, the Allies had gained near complete air superiority over the Western Front. This cleared the way both for intensified strategic bombing of German cities and industries, and for the tactical bombing of battlefield targets. With the Luftwaffe largely cleared from the skies, Allied fighters increasingly served as attack aircraft.
Allied fighters, by gaining air superiority over the European battlefield, played a crucial role in the eventual defeat of the Axis, which Reichmarshal Hermann Göring, commander of the German "Luftwaffe" summed up when he said: "When I saw Mustangs over Berlin, I knew the jig was up."
Pacific theater.
Major air combat during the war in the Pacific began with the entry of the Western Allies following Japan's attack against Pearl Harbor. The
Imperial Japanese Navy Air Service primarily operated the Mitsubishi A6M Zero, and the Imperial Japanese Army Air Service flew the Nakajima Ki-27 and the Nakajima Ki-43, initially enjoying great success, as these fighters generally had better range, maneuverability, speed and climb rates than their Allied counterparts. Additionally, Japanese pilots had received excellent training and many were combat veterans from Japan's campaigns in China. They quickly gained air superiority over the Allies, who at this stage of the war were often disorganized, under-trained and poorly equipped, and Japanese air power contributed significantly to their successes in the Philippines, Malaysia and Singapore, the Dutch East Indies and Burma.
By mid-1942, the Allies began to regroup and while some Allied aircraft such as the Brewster Buffalo and the P-39 were hopelessly outclassed by fighters like Japan's Zero, others such as the Army's P-40 and the Navy's Wildcat possessed attributes such as superior firepower, ruggedness and dive speed, and the Allies soon developed tactics (such as the Thach weave) to take advantage of these strengths. These changes soon paid dividends, as the Allied ability to deny Japan air superiority was critical to their victories at Coral Sea, Midway, Guadalcanal and New Guinea. In China, the Flying Tigers also used the same tactics with some success, although they were unable to stem the tide of Japanese advances there.
By 1943, the Allies began to gain the upper hand in the Pacific Campaign's air campaigns. Several factors contributed to this shift. First, second-generation Allied fighters such as the Hellcat and the P-38, and later the Corsair, the P-47 and the P-51, began arriving in numbers. These fighters outperformed Japanese fighters in all respects except maneuverability. Other problems with Japan's fighter aircraft also became apparent as the war progressed, such as their lack of armor and light armament, which made them inadequate as bomber interceptors or ground-attack planes—roles Allied fighters excelled at. Most importantly, Japan's training program failed to provide enough well-trained pilots to replace losses. In contrast, the Allies improved both the quantity and quality of pilots graduating from their training programs.
By mid-1944, Allied fighters had gained air superiority throughout the theater, which would not be contested again during the war. The extent of Allied quantitative and qualitative superiority by this point in the war was demonstrated during the Battle of the Philippine Sea, a lopsided Allied victory where Japanese fliers were downed in such numbers and with such ease that American fighter pilots likened it to a great turkey shoot.
Late in the war, Japan did begin to produce new fighters such as the Nakajima Ki-84 and the Kawanishi N1K to replace the venerable Zero, but these were produced only in small numbers, and in any case by that time Japan lacked trained pilots or sufficient fuel to mount a sustained challenge to Allied fighters. During the closing stages of the war, Japan's fighter arm could not seriously challenge raids over Japan by American B-29s, and was largely relegated to Kamikaze tactics.
Technological innovations.
Fighter technology advanced rapidly during the Second World War. Piston-engines, which powered the vast majority of World War II fighters, grew more powerful: at the beginning of the war fighters typically had engines producing between and , while by the end of the war many could produce over . For example, the Spitfire, one of the few fighters in continuous production throughout the war, was in 1939 powered by a Merlin II, while variants produced in 1945 were equipped with the Griffon 61. Nevertheless, these fighters could only achieve modest increases in top speed due to problems of compressibility created as aircraft and their propellers approached the sound barrier, and it was apparent that propeller-driven aircraft were approaching the limits of their performance. German jet and rocket powered fighters entered combat in 1944, although too late to impact the war's outcome. The same year the Allies' only operational jet fighter, the Gloster Meteor, also entered service.
World War II fighters also increasingly featured monocoque construction, which improved their aerodynamic efficiency while also adding structural strength. Laminar flow wings, which improved high speed performance, also came into use on fighters such as the P-51, while the Messerschmitt Me 262 and the Messerschmitt Me 163 featured swept wings that dramatically reduced drag at high subsonic speeds.
Armament also advanced during the war. The rifle-caliber machine guns that were common on prewar fighters could not easily down the more rugged warplanes of the era. Air forces began to replace or supplement them with cannons, which fired explosive shells that could blast a hole in an enemy aircraft—rather than relying on kinetic energy from a solid bullet striking a critical component of the aircraft, such as a fuel line or control cable, or the pilot. Cannons could bring down even heavy bombers with just a few hits, but their slower rate of fire made it difficult to hit fast moving fighters in a dogfight. Eventually, most fighters mounted cannons, sometimes in combination with machine guns.
The British epitomized this shift. Their standard early war fighters mounted eight calibre machine guns—whereas by mid-war they often featured a combination of machine guns and 20 mm cannons, and late in the war often only cannons. The Americans, in contrast, had problems designing a native cannon design, so instead placed multiple .50 caliber (12.7 mm) heavy machine guns on their fighters. Fighters were also increasingly fitted with bomb racks and air-to-surface ordnance such as bombs or rockets beneath their wings, and pressed into close air support roles as fighter-bombers. Although they carried less ordnance than light and medium bombers, and generally had a shorter range, they were cheaper to produce and maintain and their maneuverability made it easier for them to hit moving targets such as motorized vehicles. Moreover, if they encountered enemy fighters, their ordnance (which reduced lift and increased drag and therefore decreased performance) could be jettisoned and they could engage the enemy fighters, which eliminated the need for fighter escorts that bombers required. Heavily armed and sturdily constructed fighters such as Germany's Focke-Wulf Fw 190, Britain's Hawker Typhoon and Hawker Tempest, and America's P-40, Corsair, P-47 and P-38 all excelled as fighter-bombers, and since the Second World War ground attack has been an important secondary capability of many fighters.
World War II also saw the first use of airborne radar on fighters. The primary purpose of these radars was to help night fighters locate enemy bombers and fighters. Because of the bulkiness of these radar sets, they could not be carried on conventional single-engined fighters and instead were typically retrofitted to larger heavy fighters or light bombers such as Germany's Messerschmitt Bf 110 and Junkers Ju 88, Britain's Mosquito and Beaufighter, and America's A-20, which then served as night fighters. The Northrop P-61 Black Widow, a purpose-built night fighter, was the only fighter of the war that incorporated radar into its original design. Britain and America cooperated closely in the development of airborne radar, and Germany's radar technology generally lagged slightly behind Anglo-American efforts, while other combatants developed few radar-equipped fighters.
Post–World War II period.
Several prototype fighter programs begun early in 1945 continued on after the war and led to advanced piston-engine fighters that entered production and operational service in 1946. A typical example is the Lavochkin La-9 'Fritz', which was an evolution of the successful wartime Lavochkin La-7 'Fin'. Working through a series of prototypes, the La-120, La-126 and La-130, the Lavochkin design bureau sought to replace the La-7's wooden airframe with a metal one, as well as fit a laminar-flow wing to improve maneuver performance, and increased armament. The La-9 entered service in August 1946 and was produced until 1948; it also served as the basis for the development of a long-range escort fighter, the La-11 'Fang', of which nearly 1200 were produced 1947–1951. Over the course of the Korean War, however, it became obvious that the day of the piston-engined fighter was coming to a close and that the future would lie with the jet fighter.
This period also witnessed experimentation with jet-assisted piston engine aircraft. La-9 derivatives included examples fitted with two underwing auxiliary pulsejet engines (the La-9RD) and a similarly mounted pair of auxiliary ramjet engines (the La-138); however, neither of these entered service. One that did enter service – with the U.S. Navy in March 1945 – was the Ryan FR-1 Fireball; production was halted with the war's end on VJ-Day, with only 66 having been delivered, and the type was withdrawn from service in 1947. The USAAF had ordered its first 13 mixed turboprop-turbojet-powered pre-production prototypes of the Consolidated Vultee XP-81 fighter, but this program was also canceled by VJ Day, with 80% of the engineering work completed.
Rocket-powered fighters.
The first rocket-powered aircraft was the Lippisch Ente, which made a successful maiden flight in March 1928. The only pure rocket aircraft ever mass-produced was the Messerschmitt Me 163 in 1944, one of several German World War II projects aimed at developing rocket-powered aircraft. Later variants of the Me 262 (C-1a and C-2b) were also fitted with rocket powerplants, while earlier models were fitted with rocket boosters, but were not mass-produced with these modifications.
The USSR experimented with a rocket-powered interceptor in the years immediately following World War II, the Mikoyan-Gurevich I-270. Only two were built.
In the 1950s, the British developed mixed-power jet designs employing both rocket and jet engines to cover the performance gap that existed in turbojet designs. The rocket was the main engine for delivering the speed and height required for high-speed interception of high-level bombers and the turbojet gave increased fuel economy in other parts of flight, most notably to ensure the aircraft was able to make a powered landing rather than risking an unpredictable gliding return. The Saunders-Roe SR.53 was a successful design, and was planned for production when economics forced the British to curtail most aircraft programs in the late 1950s. Furthermore, rapid advancements in jet engine technology rendered mixed-power aircraft designs like Saunders-Roe's SR.53 (and its SR.177 maritime variant) obsolete. The American XF-91 Thunderceptor (the first U.S. fighter to exceed Mach 1 in level flight) met a similar fate for the same reason, and no hybrid rocket-and-jet-engine fighter design has ever been placed into service. The only operational implementation of mixed propulsion was Rocket-Assisted Take Off (RATO), a system rarely used in fighters.
Jet-powered fighters.
It has become common in the aviation community to classify jet fighters by "generations" for historical purposes. There are no official definitions of these generations; rather, they represent the notion that there are stages in the development of fighter design approaches, performance capabilities, and technological evolution. Also other authors have packed the fighters into different generations. For example, Richard P. Hallion of the Secretary of the Air Force's Action Group classified the F-16 as a sixth generation jet fighter.
The timeframes associated with each generation are inexact and are only indicative of the period during which their design philosophies and technology employment enjoyed a prevailing influence on fighter design and development. These timeframes also encompass the peak period of service entry for such aircraft.
First generation subsonic jet fighters (mid-1940s to mid-1950s).
The first generation of jet fighters comprised the initial, subsonic jet fighter designs introduced late in World War II and in the early post-war period. They differed little from their piston-engined counterparts in appearance, and many employed unswept wings. Guns remained the principal armament. The need to obtain a decisive advantage in maximum speed pushed the development of turbojet-powered aircraft forward. Top speeds for fighters rose steadily throughout World War II as more powerful piston engines were developed, and was approaching transonic flight speeds where the efficiency of propellers drops off, making further speed increases nearly impossible.
The first jets were developed during World War II and saw combat in the last two years of the war. Messerschmitt developed the first operational jet fighter, the Me 262. It was considerably faster than contemporary piston-driven aircraft, and in the hands of a competent pilot, was quite difficult for Allied pilots to defeat. The design was never deployed in numbers sufficient to stop the Allied air campaign, and a combination of fuel shortages, pilot losses, and technical difficulties with the engines kept the number of sorties low. Nevertheless, the Me 262 indicated the obsolescence of piston-driven aircraft. Spurred by reports of the German jets, Britain's Gloster Meteor entered production soon after and the two entered service around the same time in 1944. Meteors were commonly used to intercept the V-1 "buzz bomb", as they were faster than available piston-engined fighters at the low altitudes the flying bombs were flying. By the end of the war almost all work on piston-powered fighters had ended. A few designs combining piston and jet engines for propulsion – such as the Ryan FR Fireball – saw brief use, but by the end of the 1940s virtually all new fighters were jet-powered.
Despite their advantages, the early jet fighters were far from perfect. The operational lifespan of turbines were very short and engines were temperamental, while power could be adjusted only slowly and acceleration was poor (even if top speed was higher) compared to the final generation of piston fighters. Many squadrons of piston-engined fighters were retained until the early to mid-1950s, even in the air forces of the major powers (though the types retained were the best of the World War II designs). Innovations including ejection seats, air brakes and all-moving tailplanes became widespread in this period.
The Americans began using jet fighters operationally post-war, the wartime Bell P-59 having proven itself a failure. The Lockheed P-80 Shooting Star (soon re-designated F-80) was less elegant than the swept-wing Me 262, but had a cruise speed (]) as high as the maximum speed attainable by many piston-engined fighters. The British designed several new jets, including the distinctive twin boom de Havilland Vampire which was sold to the air forces of many nations.
The British transferred the technology of the Rolls-Royce Nene jet engine to the Soviets, who soon put it to use in their advanced Mikoyan-Gurevich MiG-15 fighter, which used fully swept wings that allowed flying closer to the speed of sound than straight-winged designs such as the F-80. Its top speed of proved quite a shock to the American F-80 pilots who encountered them over Korea, along with their armament of two 23 mm cannons and a single 37 mm cannon. Nevertheless, in the first jet-versus-jet dogfight, which occurred during the Korean War on 8 November 1950, an F-80 downed two North Korean MiG-15s.
The Americans responded by rushing their own swept-wing F-86 Sabre into battle against the MiGs, which had similar transsonic performance. The two aircraft had different strengths and weaknesses, but were similar enough that victory could go either way. While the Sabres were focused primarily on downing MiGs and scored favourably against those flown by the poorly trained North Koreans, the MiGs in turn decimated US bomber formations and forced the withdrawal of numerous American types from operational service.
The world's navies also transitioned to jets during this period, despite the need for catapult-launching of the new aircraft. Grumman's F9F Panther was adopted by the U.S. Navy as their primary jet fighter in the Korean War period, and it was one of the first jet fighters to employ an afterburner. The de Havilland Sea Vampire was the Royal Navy's first jet fighter. Radar was used on specialized night fighters such as the F3D Skyknight, which also downed MiGs over Korea, and later fitted to the F2H Banshee and swept wing F7U Cutlass and F3H Demon as all-weather / night fighters. Early versions of Infra-red (IR) air-to-air missiles (AAMs) such as the AIM-9 Sidewinder and radar guided missiles such as the AIM-7 Sparrow whose descendants are still in use, were first introduced on swept wing subsonic Demon and Cutlass naval fighters.
Second generation jet fighters (mid-1950s to early 1960s).
The development of second-generation fighters was shaped by technological breakthroughs, lessons learned from the aerial battles of the Korean War, and a focus on conducting operations in a nuclear warfare environment. Technological advances in aerodynamics, propulsion and aerospace building materials (primarily aluminium alloys) permitted designers to experiment with aeronautical innovations, such as swept wings, delta wings, and area-ruled fuselages. Widespread use of afterburning turbojet engines made these the first production aircraft to break the sound barrier, and the ability to sustain supersonic speeds in level flight became a common capability amongst fighters of this generation.
Fighter designs also took advantage of new electronics technologies that made effective radars small enough to carry aboard smaller aircraft. Onboard radars permitted detection of enemy aircraft beyond visual range, thereby improving the handoff of targets by longer-ranged ground-based warning and tracking radars. Similarly, advances in guided missile development allowed air-to-air missiles to begin supplementing the gun as the primary offensive weapon for the first time in fighter history. During this period, passive-homing infrared-guided (IR) missiles became commonplace, but early IR missile sensors had poor sensitivity and a very narrow field of view (typically no more than 30°), which limited their effective use to only close-range, tail-chase engagements. Radar-guided (RF) missiles were introduced as well, but early examples proved unreliable. These semi-active radar homing (SARH) missiles could track and intercept an enemy aircraft "painted" by the launching aircraft's onboard radar. Medium- and long-range RF air-to-air missiles promised to open up a new dimension of "beyond-visual-range" (BVR) combat, and much effort was placed in further development of this technology.
The prospect of a potential third world war featuring large mechanized armies and nuclear weapon strikes led to a degree of specialization along two design approaches: interceptors, such as the English Electric Lightning and Mikoyan-Gurevich MiG-21F; and fighter-bombers, such as the Republic F-105 Thunderchief and the Sukhoi Su-7B. Dogfighting, per se, was de-emphasized in both cases. The interceptor was an outgrowth of the vision that guided missiles would completely replace guns and combat would take place at beyond visual ranges. As a result, interceptors were designed with a large missile payload and a powerful radar, sacrificing agility in favor of high speed, altitude ceiling and rate of climb. With a primary air defense role, emphasis was placed on the ability to intercept strategic bombers flying at high altitudes. Specialized point-defense interceptors often had limited range and little, if any, ground-attack capabilities. Fighter-bombers could swing, between air superiority and ground-attack roles, and were often designed for a high-speed, low-altitude dash to deliver their ordnance. Television- and IR-guided air-to-surface missiles were introduced to augment traditional gravity bombs, and some were also equipped to deliver a nuclear bomb.
Third generation jet fighters (early 1960s to circa 1970).
The third generation witnessed continued maturation of second-generation innovations, but it is most marked by renewed emphases on maneuverability and traditional ground-attack capabilities. Over the course of the 1960s, increasing combat experience with guided missiles demonstrated that combat would devolve into close-in dogfights. Analog avionics began to appear, replacing older "steam-gauge" cockpit instrumentation. Enhancements to the aerodynamic performance of third-generation fighters included flight control surfaces such as canards, powered slats, and blown flaps. A number of technologies would be tried for Vertical/Short Takeoff and Landing, but thrust vectoring would be successful on the Harrier.
Growth in air combat capability focused on the introduction of improved air-to-air missiles, radar systems, and other avionics. While guns remained standard equipment (early models of F-4 being a notable exception), air-to-air missiles became the primary weapons for air superiority fighters, which employed more sophisticated radars and medium-range RF AAMs to achieve greater "stand-off" ranges, however, kill probabilities proved unexpectedly low for RF missiles due to poor reliability and improved electronic countermeasures (ECM) for spoofing radar seekers. Infrared-homing AAMs saw their fields of view expand to 45°, which strengthened their tactical usability. Nevertheless, the low dogfight loss-exchange ratios experienced by American fighters in the skies over Vietnam led the U.S. Navy to establish its famous "TOPGUN" fighter weapons school, which provided a graduate-level curriculum to train fleet fighter pilots in advanced Air Combat Maneuvering (ACM) and Dissimilar Air Combat Training (DACT) tactics and techniques.
This era also saw an expansion in ground-attack capabilities, principally in guided missiles, and witnessed the introduction of the first truly effective avionics for enhanced ground attack, including terrain-avoidance systems. Air-to-surface missiles (ASM) equipped with electro-optical (E-O) contrast seekers – such as the initial model of the widely used AGM-65 Maverick – became standard weapons, and laser-guided bombs (LGBs) became widespread in effort to improve precision-attack capabilities. Guidance for such precision-guided munitions (PGM) was provided by externally mounted targeting pods, which were introduced in the mid-1960s.
It also led to the development of new automatic-fire weapons, primarily chain-guns that use an electric motor to drive the mechanism of a cannon. This allowed a plane to carry a single multi-barrel weapon (such as the 20 mm Vulcan), and provided greater accuracy and rates of fire. Powerplant reliability increased and jet engines became "smokeless" to make it harder to sight aircraft at long distances.
Dedicated ground-attack aircraft (like the Grumman A-6 Intruder, SEPECAT Jaguar and LTV A-7 Corsair II) offered longer range, more sophisticated night attack systems or lower cost than supersonic fighters. With variable-geometry wings, the supersonic F-111 introduced the Pratt & Whitney TF30, the first turbofan equipped with afterburner. The ambitious project sought to create a versatile common fighter for many roles and services. It would serve well as an all-weather bomber, but lacked the performance to defeat other fighters. The McDonnell F-4 Phantom was designed around radar and missiles as an all-weather interceptor, but emerged as a versatile strike bomber nimble enough to prevail in air combat, adopted by the U.S. Navy, Air Force and Marine Corps. Despite numerous shortcomings that would be not be fully addressed until newer fighters, the Phantom claimed 280 aerial kills, more than any other U.S. fighter over Vietnam. With range and payload capabilities that rivaled that of World War II bombers such as B-24 Liberator, the Phantom would become a highly successful multirole aircraft.
Fourth generation jet fighters (circa 1970 to mid-1990s).
Fourth-generation fighters continued the trend towards multirole configurations, and were equipped with increasingly sophisticated avionics and weapon systems. Fighter designs were significantly influenced by the Energy-Maneuverability (E-M) theory developed by Colonel John Boyd and mathematician Thomas Christie, based upon Boyd's combat experience in the Korean War and as a fighter tactics instructor during the 1960s. E-M theory emphasized the value of aircraft specific energy maintenance as an advantage in fighter combat. Boyd perceived maneuverability as the primary means of getting "inside" an adversary's decision-making cycle, a process Boyd called the "OODA loop" (for "Observation-Orientation-Decision-Action"). This approach emphasized aircraft designs that were capable of performing "fast transients" – quick changes in speed, altitude, and direction – as opposed to relying chiefly on high speed alone.
E-M characteristics were first applied to the McDonnell Douglas F-15 Eagle, but Boyd and his supporters believed these performance parameters called for a small, lightweight aircraft with a larger, higher-lift wing. The small size would minimize drag and increase the thrust-to-weight ratio, while the larger wing would minimize wing loading; while the reduced wing loading tends to lower top speed and can cut range, it increases payload capacity and the range reduction can be compensated for by increased fuel in the larger wing. The efforts of Boyd's "Fighter Mafia" would result in the General Dynamics F-16 Fighting Falcon (now Lockheed Martin's).
The F-16's maneuverability was further enhanced by its slight aerodynamic instability. This technique, called "relaxed static stability" (RSS), was made possible by introduction of the "fly-by-wire" (FBW) flight control system (FLCS), which in turn was enabled by advances in computers and system integration techniques. Analog avionics, required to enable FBW operations, became a fundamental requirement and began to be replaced by digital flight control systems in the latter half of the 1980s. Likewise, Full Authority Digital Engine Controls (FADEC) to electronically manage powerplant performance was introduced with the Pratt & Whitney F100 turbofan. The F-16's sole reliance on electronics and wires to relay flight commands, instead of the usual cables and mechanical linkage controls, earned it the sobriquet of "the electric jet". Electronic FLCS and FADEC quickly became essential components of all subsequent fighter designs.
Other innovative technologies introduced in fourth-generation fighters include pulse-Doppler fire-control radars (providing a "look-down/shoot-down" capability), head-up displays (HUD), "hands on throttle-and-stick" (HOTAS) controls, and multi-function displays (MFD), all now essential equipment. Aircraft designers began to incorporate composite materials in the form of bonded aluminum honeycomb structural elements and graphite epoxy laminate skins to reduce weight. Infrared search-and-track (IRST) sensors became widespread for air-to-ground weapons delivery, and appeared for air-to-air combat as well. "All-aspect" IR AAM became standard air superiority weapons, which permitted engagement of enemy aircraft from any angle (although the field of view remained relatively limited). The first long-range active-radar-homing RF AAM entered service with the AIM-54 Phoenix, which solely equipped the Grumman F-14 Tomcat, one of the few variable-sweep-wing fighter designs to enter production. Even with the tremendous advancement of air-to-air missiles in this era, internal guns were standard equipment.
Another revolution came in the form of a stronger reliance on ease of maintenance, which led to standardisation of parts, reductions in the numbers of access panels and lubrication points, and overall parts reduction in more complicated equipment like the engines. Some early jet fighters required 50 man-hours of work by a ground crew for every hour the aircraft was in the air; later models substantially reduced this to allow faster turn-around times and more sorties in a day. Some modern military aircraft only require 10 man-hours of work per hour of flight time, and others are even more efficient.
Aerodynamic innovations included variable-camber wings and exploitation of the vortex lift effect to achieve higher angles of attack through the addition of leading-edge extension devices such as strakes.
Unlike interceptors of the previous eras, most fourth-generation air-superiority fighters were designed to be agile dogfighters (although the Mikoyan MiG-31 and Panavia Tornado ADV are notable exceptions). The continually rising cost of fighters, however, continued to emphasize the value of multirole fighters. The need for both types of fighters led to the "high/low mix" concept, which envisioned a high-capability and high-cost core of dedicated air-superiority fighters (like the F-15 and Su-27) supplemented by a larger contingent of lower-cost multi-role fighters (such as the F-16 and MiG-29).
Most fourth-generation fighters, such as the McDonnell Douglas F/A-18 Hornet and Dassault Mirage 2000, are true multirole warplanes, designed as such from the start. This was facilitated by multimode avionics that could switch seamlessly between air and ground modes. The earlier approaches of adding on strike capabilities or designing separate models specialized for different roles generally became "passé" (with the Panavia Tornado being an exception in this regard). Attack roles were generally assigned to dedicated ground-attack aircraft such as the Sukhoi Su-25 and the A-10 Thunderbolt II.
A typical US Air Force fighter wing of the period might contain a mix of one air superiority squadron (F-15C), one strike fighter squadron (F-15E), and two multirole fighter squadrons (F-16C).
Perhaps the most novel technology introduced for combat aircraft was "stealth", which involves the use of special "low-observable" (L-O) materials and design techniques to reduce the susceptibility of an aircraft to detection by the enemy's sensor systems, particularly radars. The first stealth aircraft introduced were the Lockheed F-117 Nighthawk attack aircraft (introduced in 1983) and the Northrop Grumman B-2 Spirit bomber (which first flew in 1989). Although no stealthy fighters per se appeared among the fourth generation, some radar-absorbent coatings and other L-O treatments developed for these programs are reported to have been subsequently applied to fourth-generation fighters.
4.5th generation jet fighters (1990s to 2005).
The end of the Cold War in 1991 led many governments to significantly decrease military spending as a "peace dividend". Air force inventories were cut. Research and development programs working on "fifth-generation" fighters took serious hits. Many programs were canceled during the first half of the 1990s, and those that survived were "stretched out". While the practice of slowing the pace of development reduces annual investment expenses, it comes at the penalty of increased overall program and unit costs over the long-term. In this instance, however, it also permitted designers to make use of the tremendous achievements being made in the fields of computers, avionics and other flight electronics, which had become possible largely due to the advances made in microchip and semiconductor technologies in the 1980s and 1990s. This opportunity enabled designers to develop fourth-generation designs – or redesigns – with significantly enhanced capabilities. These improved designs have become known as "Generation 4.5" fighters, recognizing their intermediate nature between the 4th and 5th generations, and their contribution in furthering development of individual fifth-generation technologies.
The primary characteristics of this sub-generation are the application of advanced digital avionics and aerospace materials, modest signature reduction (primarily RF "stealth"), and highly integrated systems and weapons. These fighters have been designed to operate in a "network-centric" battlefield environment and are principally multirole aircraft. Key weapons technologies introduced include beyond-visual-range (BVR) AAMs; Global Positioning System (GPS)-guided weapons, solid-state phased-array radars; helmet-mounted sights; and improved secure, jamming-resistant datalinks. Thrust vectoring to further improve transient maneuvering capabilities has also been adopted by many 4.5th generation fighters, and uprated powerplants have enabled some designs to achieve a degree of "supercruise" ability. Stealth characteristics are focused primarily on frontal-aspect radar cross section (RCS) signature-reduction techniques including radar-absorbent materials (RAM), L-O coatings and limited shaping techniques.
"Half-generation" designs are either based on existing airframes or are based on new airframes following similar design theory as previous iterations; however, these modifications have introduced the structural use of composite materials to reduce weight, greater fuel fractions to increase range, and signature reduction treatments to achieve lower RCS compared to their predecessors. Prime examples of such aircraft, which are based on new airframe designs making extensive use of carbon-fibre composites, include the Eurofighter Typhoon, Dassault Rafale, and Saab JAS 39 Gripen.
Apart from these fighter jets, most of the 4.5 generation aircraft are actually modified variants of existing airframes from the earlier fourth generation fighter jets. Such fighter jets are generally heavier and examples include the Boeing F/A-18E/F Super Hornet, which is an evolution of the 1970s F/A-18 Hornet design, the F-15E Strike Eagle, which is a ground-attack/multi-role variant of the F-15 Eagle, the Su-30MKI and Su-30MKK variants of the Sukhoi Su-30 and the MiG-29M, MiG-29K and MiG-35, upgraded versions of the Mikoyan MiG-29. The Su-30MKI and MiG-35 feature thrust vectoring engine nozzles to enhance maneuvering.
4.5 generation fighters first entered service in the early 1990s, and most of them are still being produced and evolved. It is quite possible that they may continue in production alongside fifth-generation fighters due to the expense of developing the advanced level of stealth technology needed to achieve aircraft designs featuring very low observables (VLO), which is one of the defining features of fifth-generation fighters. Of the 4.5th generation designs, the Strike Eagle, Super Hornet, Typhoon, Gripen, and Rafale have been used in combat.
The U.S. government has defined 4.5 generation fighter aircraft as those that "(1) have advanced capabilities, including— (A) AESA radar; (B) high capacity data-link; and (C) enhanced avionics; and (2) have the ability to deploy current and reasonably foreseeable advanced armaments."
Fifth generation jet fighters (2005 to the present).
The fifth generation was ushered in by the Lockheed Martin/Boeing F-22 Raptor in late 2005. Currently the cutting edge of fighter design, fifth-generation fighters are characterized by being designed from the start to operate in a network-centric combat environment, and to feature extremely low, all-aspect, multi-spectral signatures employing advanced materials and shaping techniques. They have multifunction AESA radars with high-bandwidth, low-probability of intercept (LPI) data transmission capabilities. The Infra-red search and track sensors incorporated for air-to-air combat as well as for air-to-ground weapons delivery in the 4.5th generation fighters are now fused in with other sensors for Situational Awareness IRST or SAIRST, which constantly tracks all targets of interest around the aircraft so the pilot need not guess when he glances. These sensors, along with advanced avionics, glass cockpits, helmet-mounted sights (not currently on F-22), and improved secure, jamming-resistant LPI datalinks are highly integrated to provide multi-platform, multi-sensor data fusion for vastly improved situational awareness while easing the pilot's workload. Avionics suites rely on extensive use of very high-speed integrated circuit (VHSIC) technology, common modules, and high-speed data buses. Overall, the integration of all these elements is claimed to provide fifth-generation fighters with a "first-look, first-shot, first-kill capability".
The AESA radar offers unique capabilities for fighters (and it is also quickly becoming essential for Generation 4.5 aircraft designs, as well as being retrofitted onto some fourth-generation aircraft). In addition to its high resistance to ECM and LPI features, it enables the fighter to function as a sort of "mini-AWACS," providing high-gain electronic support measures (ESM) and electronic warfare (EW) jamming functions.
Other technologies common to this latest generation of fighters includes integrated electronic warfare system (INEWS) technology, integrated communications, navigation, and identification (CNI) avionics technology, centralized "vehicle health monitoring" systems for ease of maintenance, fiber optics data transmission, stealth technology and even hovering capabilities. Maneuver performance remains important and is enhanced by thrust-vectoring, which also helps reduce takeoff and landing distances. Supercruise may or may not be featured; it permits flight at supersonic speeds without the use of the afterburner – a device that significantly increases IR signature when used in full military power.
A key attribute of fifth-generation fighters is a small radar cross-section. Great care has been taken in designing its layout and internal structure to minimize RCS over a broad bandwidth of detection and tracking radar frequencies; furthermore, to maintain its VLO signature during combat operations, primary weapons are carried in internal weapon bays that are only briefly opened to permit weapon launch. Furthermore, stealth technology has advanced to the point where it can be employed without a tradeoff with aerodynamics performance, in contrast to previous stealth efforts. Some attention has also been paid to reducing IR signatures, especially on the F-22. Detailed information on these signature-reduction techniques is classified, but in general includes special shaping approaches, thermoset and thermoplastic materials, extensive structural use of advanced composites, conformal sensors, heat-resistant coatings, low-observable wire meshes to cover intake and cooling vents, heat ablating tiles on the exhaust troughs (seen on the Northrop YF-23), and coating internal and external metal areas with radar-absorbent materials and paint (RAM/RAP).
Such aircraft are sophisticated and expensive. The U.S. Air Force originally planned to acquire 650 F-22s, but now only 187 will be built. As a result, its unit flyaway cost (FAC) is around US$150 million. To spread the development costs – and production base – more broadly, the Joint Strike Fighter (JSF) program enrolls eight other countries as cost- and risk-sharing partners. Altogether, the nine partner nations anticipate procuring over 3,000 Lockheed Martin F-35 Lightning II fighters at an anticipated average FAC of $80–85 million. The F-35, however, is designed to be a family of three aircraft, a conventional take-off and landing (CTOL) fighter, a short take-off and vertical landing (STOVL) fighter, and a Catapult Assisted Take Off But Arrested Recovery (CATOBAR) fighter, each of which has a different unit price and slightly varying specifications in terms of fuel capacity (and therefore range), size and payload.
Other countries have initiated fifth-generation fighter development projects, with Russia's Sukhoi PAK FA and Mikoyan LMFS. In October 2007, Russia and India signed an agreement for joint participation in a Fifth-Generation Fighter Aircraft Program (FGFA), which gives India responsibility for development of a two-seat model of the PAK-FA. India is also developing the Advanced Medium Combat Aircraft (AMCA). In December 2010, it was discovered that China is developing the 5th generation fighter Chengdu J-20. The J-20 took its maiden flight in January 2011 and is planned to be deployed in 2017–19 time frame. The Shenyang J-31 took its maiden flight on 31 October 2012. Japan is exploring its technical feasibility to produce fifth-generation fighters.
Sixth generation jet fighters.
A sixth generation jet fighter is a conceptual airplane expected to enter service in the United States Air Force and United States Navy in 2025–30 timeframe. The USAF seeks a new fighter for the 2030–50 period named the "Next Generation Tactical Aircraft"/"Next Gen TACAIR" The US Navy looks to replace its F/A-18E/F Super Hornets beginning in 2025 with the Next Generation Air Dominance air superiority fighter.

</doc>
<doc id="10930" url="https://en.wikipedia.org/wiki?curid=10930" title="February 25">
February 25


</doc>
<doc id="10931" url="https://en.wikipedia.org/wiki?curid=10931" title="Finite-state machine">
Finite-state machine

A finite-state machine (FSM) or finite-state automaton (FSA, plural: "automata"), or simply a state machine, is a mathematical model of computation used to design both computer programs and sequential logic circuits. It is conceived as an abstract machine that can be in one of a finite number of "states". The machine is in only one state at a time; the state it is in at any given time is called the "current state". It can change from one state to another when initiated by a triggering event or condition; this is called a "transition". A particular FSM is defined by a list of its states, and the triggering condition for each transition.
The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, which drop riders off at upper floors before going down, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of combination numbers in the proper order.
Finite-state machines can model a large number of problems, among which are electronic design automation, communication protocol design, language parsing and other engineering applications. In biology and artificial intelligence research, state machines or hierarchies of state machines have been used to describe neurological systems. In linguistics, they are used to describe simple parts of the grammars of natural languages.
Considered as an abstract model of computation, the finite state machine has less computational power than some other models of computation such as the Turing machine. That is, there are tasks that no FSM can do, but some Turing machines can. This is because the FSM memory is limited by the number of states.
FSMs are studied in the more general field of automata theory.
Example: coin-operated turnstile.
An example of a very simple mechanism that can be modeled by a state machine is a turnstile. A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway. Initially the arms are locked, blocking the entry, preventing patrons from passing through. Depositing a coin or token in a slot on the turnstile unlocks the arms, allowing a single customer to push through. After the customer passes through, the arms are locked again until another coin is inserted.
Considered as a state machine, the turnstile has two states: Locked and Unlocked. There are two inputs that affect its state: putting a coin in the slot (coin) and pushing the arm (push). In the locked state, pushing on the arm has no effect; no matter how many times the input push is given, it stays in the locked state. Putting a coin in – that is, giving the machine a coin input – shifts the state from Locked to Unlocked. In the unlocked state, putting additional coins in has no effect; that is, giving additional coin inputs does not change the state. However, a customer pushing through the arms, giving a push input, shifts the state back to Locked.
The turnstile state machine can be represented by a state transition table, showing for each state the new state and the output (action) resulting from each input
It can also be represented by a directed graph called a state diagram "(above)". Each of the states is represented by a node ("circle"). Edges ("arrows") show the transitions from one state to another. Each arrow is labeled with the input that triggers that transition. Inputs that don't cause a change of state (such as a coin input in the Unlocked state) are represented by a circular arrow returning to the original state. The arrow into the Locked node from the black dot indicates it is the initial state.
Concepts and terminology.
A "state" is a description of the status of a system that is waiting to execute a "transition". A transition is a set of actions to be executed when a condition is fulfilled or when an event is received.
For example, when using an audio system to listen to the radio (the system is in the "radio" state), receiving a "next" stimulus results in moving to the next station. When the system is in the "CD" state, the "next" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state.
In some finite-state machine representations, it is also possible to associate actions with a state:
Representations.
State/Event table.
Several state transition table types are used. The most common representation is shown below: the combination of current state (e.g. B) and input (e.g. Y) shows the next state (e.g. C). The complete action's information is not directly described in the table and can only be added using footnotes. A FSM definition including the full actions information is possible using state tables (see also virtual finite-state machine).
UML state machines.
The Unified Modeling Language has a notation for describing state machines. UML state machines overcome the limitations of traditional finite state machines while retaining their main benefits. UML state machines introduce the new concepts of hierarchically nested states and orthogonal regions, while extending the notion of actions. UML state machines have the characteristics of both Mealy machines and Moore machines. They support actions that depend on both the state of the system and the triggering event, as in Mealy machines, as well as entry and exit actions, which are associated with states rather than transitions, as in Moore machines.
SDL state machines.
The Specification and Description Language is a standard from ITU that includes graphical symbols to describe actions in the transition:
SDL embeds basic data types called Abstract Data Types, an action language, and an execution semantic in order to make the finite state machine executable.
Other state diagrams.
There are a large number of variants to represent an FSM such as the one in figure 3.
Usage.
In addition to their use in modeling reactive systems presented here, finite state automata are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, and logic. Finite state machines are a class of automata studied in automata theory and the theory of computation.
In computer science, finite state machines are widely used in modeling of application behavior, design of hardware digital systems, software engineering, compilers, network protocols, and the study of computation and languages.
Classification.
The state machines can be subdivided into Transducers, Acceptors, Classifiers and Sequencers.
Acceptors and recognizers.
Acceptors (also called recognizers and sequence detectors) produce binary output, indicating whether or not received input is accepted. Each state of an FSM is either "accepting" or "not accepting". Once all input has been received, if the current state is an accepting state, the input is accepted; otherwise it is rejected. As a rule, input is a series of symbols (characters); actions are not used. The example in figure 4 shows a finite state machine that accepts the string "nice". In this FSM, the only accepting state is state 7.
A machine could also be described as defining a language, that would contain every string accepted by the machine but none of the rejected ones; that language is "accepted" by the machine. By definition, the languages accepted by FSMs are the regular languages—; a language is regular if there is some FSM that accepts it.
The problem of determining the language accepted by a given FSA is an instance of the algebraic path problem—itself a generalization of the shortest path problem to graphs with edges weighted by the elements of an (arbitrary) semiring.
Start state.
The start state is usually shown drawn with an arrow "pointing at it from any where" (Sipser (2006) p. 34).
Accept (or final) states.
Accept states (also referred to as accepting or final states) are those at which the machine reports that the input string, as processed so far, is a member of the language it accepts. Accepting states are usually represented by double circles.
The start state can also be an accepting state, in which case the automaton accepts the empty string. If the start state is not an accepting state and there are no connecting edges to any of the accepting states, then the automaton is accepting nothing.
An example of an accepting state appears in Fig.5: a deterministic finite automaton (DFA) that detects whether the binary input string contains an even number of 0s.
"S"1 (which is also the start state) indicates the state at which an even number of 0s has been input. S1 is therefore an accepting state. This machine will finish in an accept state, if the binary string contains an even number of 0s (including any binary string containing no 0s). Examples of strings accepted by this DFA are ε (the empty string), 1, 11, 11…, 00, 010, 1010, 10110, etc…
Classifier is a generalization that, similar to acceptor, produces single output when terminates but has more than two terminal states.
Transducers.
Transducers generate output based on a given input and/or a state using actions. They are used for control applications and in the field of computational linguistics.
In control applications, two types are distinguished:
Generators.
The sequencers or generators are a subclass of aforementioned types that have a single-letter input alphabet. They produce only one sequence, which can be interpreted as output sequence of transducer or classifier outputs.
Determinism.
A further distinction is between deterministic (DFA) and non-deterministic (NFA, GNFA) automata. In deterministic automata, every state has exactly one transition for each possible input. In non-deterministic automata, an input can lead to one, more than one or no transition for a given state. This distinction is relevant in practice, but not in theory, as there exists an algorithm (the powerset construction) that can transform any NFA into a more complex DFA with identical functionality.
The FSM with only one state is called a combinatorial FSM and uses only input actions. This concept is useful in cases where a number of FSM are required to work together, and where it is convenient to consider a purely combinatorial part as a form of FSM to suit the design tools.
Alternative semantics.
There are other sets of semantics available to represent state machines. For example, there are tools for modeling and designing logic for embedded controllers. They combine hierarchical state machines, flow graphs, and truth tables into one language, resulting in a different formalism and set of semantics. Figure 8 illustrates this mix of state machines and flow graphs with a set of states to represent the state of a stopwatch and a flow graph to control the ticks of the watch. These charts, like Harel's original state machines, support hierarchically nested states, orthogonal regions, state actions, and transition actions.
FSM logic.
The next state and output of an FSM is a function of the input and of the current state. The FSM logic is shown in Figure 8.
Mathematical model.
In accordance with the general classification, the following formal definitions are found:
For both deterministic and non-deterministic FSMs, it is conventional to allow formula_6 to be a partial function, i.e. formula_13 does not have to be defined for every combination of formula_14 and formula_15. If an FSM formula_16 is in a state formula_17, the next symbol is formula_18 and formula_13 is not defined, then formula_16 can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions.
A finite-state machine is a restricted Turing machine where the head can only perform "read" operations, and always moves from left to right.
If the output function is a function of a state and input alphabet (formula_31) that definition corresponds to the Mealy model, and can be modelled as a Mealy machine. If the output function depends only on a state (formula_32) that definition corresponds to the Moore model, and can be modelled as a Moore machine. A finite-state machine with no output function at all is known as a semiautomaton or transition system.
If we disregard the first output symbol of a Moore machine, formula_33, then it can be readily converted to an output-equivalent Mealy machine by setting the output function of every Mealy transition (i.e. labeling every edge) with the output symbol given of the destination Moore state. The converse transformation is less straightforward because a Mealy machine state may have different output labels on its incoming transitions (edges). Every such state needs to be split in multiple Moore machine states, one for every incident output symbol.
Optimization.
Optimizing an FSM means finding the machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm. Other techniques include using an implication table, or the Moore reduction procedure. Additionally, acyclic FSAs can be minimized in linear time.
Implementation.
Hardware applications.
In a digital circuit, an FSM may be built using a programmable logic device, a programmable logic controller, logic gates and flip flops or relays. More specifically, a hardware implementation requires a register to store state variables, a block of combinational logic that determines the state transition, and a second block of combinational logic that determines the output of an FSM. One of the classic hardware implementations is the Richards controller.
A particular case of Moore FSM, when output is directly connected to the state flip-flops, that is when output function is simple identity, is known as Medvedev FSM. It is advised in chip design that no logic is placed between primary I/O and registers to minimize interchip delays, which are usually long and limit the FSM frequencies.
Through state encoding for low power state machines may be optimized to minimize power consumption.
Software applications.
The following concepts are commonly used to build software applications with finite state machines:
Finite automata and compilers.
Finite automata are often used in the frontend of programming language compilers. Such a frontend may comprise several finite state machines that implement a lexical analyzer and a parser.
Starting from a sequence of characters, the lexical analyzer builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The lexical analyzer and the parser handle the regular and context-free parts of the programming language's grammar.
Further reading.
Finite Markov chain processes.
Finite Markov-chain processes are also known as subshifts of finite type.

</doc>
<doc id="10933" url="https://en.wikipedia.org/wiki?curid=10933" title="Functional programming">
Functional programming

In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements. In functional code, the output value of a function depends only on the arguments that are input to the function, so calling a function "f" twice with the same value for an argument "x" will produce the same result "f(x)" each time. Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming.
Functional programming has its roots in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, "logic programming", is based on relations.
In contrast, imperative programming changes state with commands in the source language, the most simple example being assignment. Imperative programming does have functions—not in the mathematical sense—but in the sense of subroutines. They can have side effects that may change the value of program state. Functions without return values therefore make sense. Because of this, they lack referential transparency, i.e. the same language expression can result in different values at different times depending on the state of the executing program.
Functional programming languages, especially purely functional ones such as Hope, have largely been emphasized in academia rather than in commercial software development. However, prominent programming languages which support functional programming such as Common Lisp, Scheme, Clojure, Wolfram Language (also known as Mathematica), Racket, Erlang, OCaml, Haskell, and F# have been used in industrial and commercial applications by a wide variety of organizations. Functional programming is also supported in some domain-specific programming languages like R (statistics), J, K and Q from Kx Systems (financial analysis), XQuery/XSLT (XML), and Opal. Widespread domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in eschewing mutable values.
Programming in a functional style can also be accomplished in languages that are not specifically designed for functional programming. For example, the imperative Perl programming language has been the subject of a book describing how to apply functional programming concepts. This is also true of the PHP programming language. C++11, Java 8, and C# 3.0 all added constructs to facilitate the functional style. The Julia language also offers functional programming abilities. An interesting case is that of Scala – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.
History.
Lambda calculus provides a theoretical framework for describing functions and their evaluation. Although it is a mathematical abstraction rather than a programming language, it forms the basis of almost all functional programming languages today. An equivalent theoretical formulation, combinatory logic, is commonly perceived as more abstract than lambda calculus and preceded it in invention. Combinatory logic and lambda calculus were both originally developed to achieve a clearer approach to the foundations of mathematics.
An early functional-flavored language was Lisp, developed by John McCarthy while at Massachusetts Institute of Technology (MIT) for the IBM 700/7000 series scientific computers in the late 1950s. Lisp introduced many features now found in functional languages, though Lisp is technically a multi-paradigm language. Scheme and Dylan were later attempts to simplify and improve Lisp.
Information Processing Language (IPL) is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of "generator", which amounts to a function accepting a function as an argument, and, since it is an assembly-level language, code can be used as data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.
Kenneth E. Iverson developed APL in the early 1960s, described in his 1962 book "A Programming Language" (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.
John Backus presented FP in his 1977 Turing Award lecture "Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs". He defines functional programs as being built up in a hierarchical way by means of "combining forms" that allow an "algebra of programs"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style which has come to be associated with functional programming.
In the 1970s, ML was created by Robin Milner at the University of Edinburgh, and David Turner initially developed the language SASL at the University of St. Andrews and later the language Miranda at the University of Kent. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML. Meanwhile, the development of Scheme (a partly functional dialect of Lisp), as described in the influential Lambda Papers and the 1985 textbook "Structure and Interpretation of Computer Programs", brought awareness of the power of functional programming to the wider programming-languages community.
In the 1980s, Per Martin-Löf developed intuitionistic type theory (also called "constructive" type theory), which associated functional programs with constructive proofs of arbitrarily complex mathematical propositions expressed as dependent types. This led to powerful new approaches to interactive theorem proving and has influenced the development of many subsequent functional programming languages.
The Haskell language began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.
Concepts.
A number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages are often hybrids of several programming paradigms, so programmers using "mostly imperative" languages may have utilized some of these concepts.
First-class and higher-order functions.
Higher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator formula_1, which returns the derivative of a function formula_2.
Higher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: "higher-order" describes a mathematical concept of functions that operate on other functions, while "first-class" is a computer science term that describes programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).
Higher-order functions enable partial application or currying, a technique in which a function is applied to its arguments one at a time, with each application returning a new function that accepts the next argument. This allows one to succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.
Pure functions.
Purely functional functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:
While most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also allows functions to be designated "pure".
Recursion.
Iteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached. Though some recursion requires maintaining a stack, tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. The Scheme language standard requires implementations to recognize and optimize tail recursion. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.
Common patterns of recursion can be factored out using higher order functions, with catamorphisms and anamorphisms (or "folds" and "unfolds") being the most obvious examples. Such higher order functions play a role analogous to built-in control structures such as loops in imperative languages.
Most general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.
Strict versus non-strict evaluation.
Functional languages can be categorized by whether they use "strict (eager)" or "non-strict (lazy)" evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm will itself fail. For example, the expression:
will fail under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function will return the value 4 (i.e., the number of items in the list), since evaluating it will not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.
The usual implementation strategy for lazy evaluation in functional languages is graph reduction. Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.
Type systems.
Especially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), although they reject all invalid programs at runtime, when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.
Some research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which allows types to depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with. But dependent types can express arbitrary propositions in predicate logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.
A limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience. GADT's are available in the Glasgow Haskell Compiler, in OCaml (since version 4.00) and in Scala (as "case classes"), and have been proposed as additions to other languages including Java and C#.
Referential Transparency.
Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.
Consider C assignment statement codice_1, this changes the value assigned to the variable codice_2. Let us say that the initial value of codice_2 was codice_4, then two consecutive evaluations of the variable codice_2 will yield codice_6 and codice_7 respectively. Clearly, replacing codice_1 with either codice_6 or codice_7 gives a program with different meaning, and so the expression "is not" referentially transparent. In fact, assignment statements are never referentially transparent.
Now, consider another function such as codice_11 "is" transparent, as it will not implicitly change the input x and thus has no such side effects.
Functional programs exclusively use this type of function and are therefore referentially transparent.
Functional programming in non-functional languages.
It is possible to use a functional style of programming in languages that are not traditionally considered functional languages. For example, both D and Fortran 95 explicitly support pure functions.
JavaScript, Lua and Python had first class functions from their inception. Amrit Prem added support to Python for "lambda", "map", "reduce", and "filter" in 1994, as well as closures in Python 2.2, though Python 3 relegated "reduce" to the codice_12 standard library module. First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, and C++11.
In Java, anonymous classes can sometimes be used to simulate closures; however, anonymous classes are not always proper replacements to closures because they have more limited capabilities. Java 8 supports lambda expressions as a replacement for some anonymous classes. However, the presence of checked exceptions in Java can make functional programming inconvenient, because it can be necessary to catch checked exceptions and then rethrow them—a problem that does not occur in other JVM languages that do not have checked exceptions, such as Scala.
In C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.
Many object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.
Similarly, the idea of immutable data from functional programming is often included in imperative programming languages, for example the tuple in Python, which is an immutable array.
Comparison to imperative programming.
Functional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.
Higher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.
Simulating state.
There are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.
The pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).
Another way in which functional languages can simulate state is by passing around a data structure that represents the current state as a parameter to function calls. On each function call, a copy of this data structure is created with whatever differences are the result of the function. This is referred to as 'state-passing style'.
Impure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.
Alternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.
Efficiency issues.
Functional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal. This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware (which is a highly evolved Turing machine). Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer-chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree). However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C. For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.
Immutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.
Lazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993 discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan "et al." 2008 give some practical advice for analyzing and fixing them.
However, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles) .
Coding styles.
Imperative programs have the environment and a sequence of steps manipulating the environment. Functional programs have an expression that is successively substituted until it reaches normal form. A example illustrates this with different solutions to the same programming goal (calculating Fibonacci numbers).
Python.
Printing first 10 fibonacci numbers, iterative
Printing first 10 fibonacci numbers, functional expression style
Printing a list with first 10 fibonacci numbers, with generators
Printing a list with first 10 fibonacci numbers, functional expression style
Haskell.
Printing first 10 fibonacci numbers, functional expression style
Printing a list with first 10 fibonacci numbers, functional expression style
Printing the 11th fibonacci number, functional expression style
Printing the 11th fibonacci number, functional expression style, tail recursive
Printing the 11th fibonacci number, functional expression style with recursive lists
Printing the 11th fibonacci number, functional expression style with primitives for recursive lists
Printing the 11th fibonacci number, functional expression style with primitives for recursive lists, more concisely
Printing the 11th fibonacci number, functional declaration style, tail recursive
Perl 6.
As influenced by Haskell and others, Perl 6 has several functional and declarative approaches to problems. For example, you can declaratively build up a well-typed recursive version (the type constraints are optional) through signature pattern matching:
An alternative to this is to construct a lazy iterative sequence, which appears as an almost direct illustration of the sequence:
Erlang.
Erlang is a functional, concurrent, general-purpose programming language. A Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. Use other algorithms for fast performance):
Elixir.
Elixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM).
The Fibonacci function can be written in Elixir as follows:<syntaxhighlight lang="elixir">
defmodule Fibonacci do
end
</syntaxhighlight>
Lisp.
The Fibonacci function can be written in Common Lisp as follows:
The program can then be called as
D.
D has support for functional programming:
R.
R (programming language) is an environment for statistical computing and graphics. It is also a functional programming language.
The Fibonacci function can be written in R as a recursive function as follows:
Or it can be written as a singly recursive function:
Or it can be written as an iterative function:
The function can then be called as
Use in industry.
Functional programming has long been popular in academia, but with few industrial applications. However, recently several prominent functional programming languages have been used in commercial or industrial systems. For example, the Erlang programming language, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems. It has since become popular for building a range of applications at companies such as T-Mobile, Nortel, Facebook, Électricité de France and WhatsApp. The Scheme dialect of Lisp was used as the basis for several applications on early Apple Macintosh computers, and has more recently been applied to problems such as training simulation software and telescope control. OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis, driver verification, industrial robot programming, and static analysis of embedded software. Haskell, although initially intended as a research language, has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.
Other functional programming languages that have seen use in industry include Scala, F#, (both being functional-OO hybrids with support for both purely functional and imperative programming) Wolfram Language, Lisp, Standard ML, and Clojure.
In education.
Functional programming is being used as a method to teach problem solving, algebra and geometric concepts.
It has also been used as a tool to teach classical mechanics in Structure and Interpretation of Classical Mechanics.

</doc>
<doc id="10936" url="https://en.wikipedia.org/wiki?curid=10936" title="February 29">
February 29

February 29, also known as leap day or leap year day, is a date added to most years that are divisible by 4, such as 2008, 2012, 2016, 2020, and 2024. A leap day is added in various solar calendars (calendars based on the Earth's rotation around the Sun), including the Gregorian calendar standard in most of the world. Lunisolar calendars (calendars based on the rotation of the Moon) instead add a leap or intercalary month.
In the Gregorian calendar, years that are divisible by 100, but not by 400, do not contain a leap day. Thus, 1700, 1800, and 1900 did not contain a leap day, 2100, 2200, and 2300 will not contain a leap day, while 1600 and 2000 did, and 2400 will. Years containing a leap day are called leap years. February 29 is the 60th day of the Gregorian calendar in such a year, with 306 days remaining until the end of the year. In the Chinese calendar, this day will only occur in years of the monkey, dragon, and rat.
A leap day is observed because a complete revolution around the Sun takes approximately 6 hours longer than 365 days (8,760 hours). It compensates for this lag, realigning the calendar with the Earth's position in the Solar System; otherwise, seasons would occur in a different time than intended in the calendar year. The Gregorian calendar has the century rules to the leap year to ensure that Easter occurs near the vernal equinox, or spring in the northern hemisphere.
Leap years.
Although most modern calendar years have 365 days, a complete revolution around the Sun (one solar year) takes approximately 365 days and 6 hours. An extra 24 hours thus accumulates every four years, requiring that an extra calendar day be added to align the calendar with the Sun's apparent position. Without the added day, in future years the seasons would occur later in the calendar, eventually leading to confusion about when to undertake activities dependent on weather, ecology, or hours of daylight.
A solar year is actually slightly shorter than 365 days and 6 hours (365.25 days). As early as the 13th century it was recognized that the year is shorter than the 365.25 days assumed by the Julian calendar: the Earth's orbital period around the Sun was derived from the medieval Alfonsine tables as 365 days, 5 hours, 49 minutes, and 16 seconds (365.2425 days). The currently accepted modern figure is 365 days, 5 hours, 48 minutes, 45 seconds. Adding a calendar day every four years, therefore, results in an excess of around 44 minutes for those four years, or about 3 days every 400 years. To compensate for this, three days are removed every 400 years. The Gregorian calendar reform implements this adjustment by making an exception to the general rule that there is a leap year every four years. Instead, a year divisible by 100 is not a leap year unless that year was also exactly divisible by 400. This means that the years 1600, 2000, and 2400 are leap years, while the years 1700, 1800, 1900, 2100, 2200, 2300, and 2500 are common years.
Modern (Gregorian) calendar.
The Gregorian calendar repeats itself every 400 years, which is exactly 20,871 weeks including 97 leap days. Over this period, February 29 falls on Sunday, Tuesday, and Thursday 13 times each; 14 times each on Friday and Saturday; and 15 times each on Monday and Wednesday. The order of the leap days is: Thursday, Tuesday, Sunday, Friday, Wednesday, Monday, and Saturday.
Early Roman calendar (of Numa Pompilius).
The calendar of the Roman king Numa Pompilius had only 355 days (even though it was not a lunar calendar) which meant that it would quickly become unsynchronized with the solar year. An earlier Roman solution to this problem was to lengthen the calendar periodically by adding extra days to February, the last month of the year. February consisted of two parts, each with an odd number of days. The first part ended with the "Terminalia" on the 23rd, which was considered the end of the religious year, and the five remaining days formed the second part. To keep the calendar year roughly aligned with the solar year, a leap month, called "Mensis Intercalaris" ("intercalary month"), was added from time to time between these two parts of February. The (usual) second part of February was incorporated in the intercalary month as its last five days, with no change either in their dates or the festivals observed on them. This followed naturally, because the days after the Ides (13th) of February (in an ordinary year) or the Ides of Intercalaris (in an intercalary year) both counted down to the Kalends of March (i.e. they were known as "the "n"th day before the Kalends of March"). The Nones (5th) and Ides of Intercalaris occupied their normal positions.
The third-century writer Censorinus says:When it was thought necessary to add (every two years) an intercalary month of 22 or 23 days, so that the civil year should correspond to the natural (solar) year, this intercalation was in preference made in February, between Terminalia and Regifugium [24th.
Later Roman calendar (Julian).
The leap day was introduced in Rome as a part of the Julian reform in the 1st century BC. As before, the intercalation was made after February 23. The day following the Terminalia (February 23) was doubled, forming the ""bis sextum""—literally 'twice sixth', since February 24 was 'the sixth day before the Kalends of March' using Roman inclusive counting (March 1 was the Kalends of March and was also the first day of the calendar year). Although there were exceptions, the first day of the "bis sextum" (February 24) was usually regarded as the intercalated or "bissextile" day since the 3rd century AD. February 29 came to be regarded as the leap day when the Roman system of numbering days was replaced by sequential numbering in the late Middle Ages.
Leap second.
The concepts of the leap year and leap day are distinct from the leap second, which results from changes in the Earth's rotational speed. But the basic problem is the same: the quotient of the larger measure of time by the smaller is a non-integer. There is no way to perfectly fit a whole number of days/months into a year, nor is there a way to perfectly fit a whole number of seconds into a day. Leap seconds and leap years are used to correct the resulting drift.
Births.
A person who is born on February 29 may be called a "leapling" or a "leap-year baby". In non-leap years, some leaplings celebrate their birthday on either February 28 or March 1, while others only observe birthdays on the authentic intercalary date, February 29.
Legal status.
The effective legal date of a leapling's birthday in non-leap years varies between jurisdictions.
In the United Kingdom and Hong Kong, when a person born on February 29 turns 18, they are considered to have their birthday on March 1 in the relevant year. The same is also true in the United States of America.
In New Zealand, a person born on February 29 is deemed to have their birthday on February 28 in non-leap years, for the purposes of Driver Licensing under §2(2) of the Land Transport (Driver Licensing) Rule 1999. The net result is that for drivers aged 75, or over 80, their driver licence expires at the end of the last day of February, even though their birthday would otherwise fall on the first day in March in non-leap years. Otherwise, New Zealand legislation is silent on when a person born on 29 February has their birthday, although case law would suggest that age is computed based on the number of years elapsed, from the day after the date of birth, and that the person's birth day then occurs on the last day of the year period. This differs from English common law where a birthday is considered to be the start of the next year, the preceding year ending at midnight on the day preceding the birthday. While a person attains the same age on the same day, it also means that, in New Zealand, if something must be done by the time a person attains a certain age, that thing can be done on the birthday that they attain that age and still be lawful.
In Taiwan (Republic of China), the legal birthday of a leapling is February 28 in common years:
Thus, in England and Wales or in Hong Kong, a person born on February 29 will have legally reached 18 years old on March 1. If he or she was born in Taiwan he or she legally becomes 18 on February 28, a day earlier. In the United States, according to John Reitz, a professor of law at the University of Iowa, there is no "... statute or general rule that has anything to do with leap day." Reitz speculates that "March 1 would likely be considered the legal birthday in non-leap years of someone born on leap day," using the same reasoning as described for the United Kingdom and Hong Kong.
In fiction.
There are many instances in children's literature where a person's claim to be only a quarter of their actual age turns out to be based on counting their leap-year birthdays.
A similar device is used in the plot of Gilbert and Sullivan's 1879 comic opera "The Pirates of Penzance". As a child, Frederic was apprenticed to a band of pirates until his 21st birthday. Having passed his 21st year, he leaves the pirate band and falls in love. However, since he was born on February 29, his 21st "birthday" will not arrive until he is eighty-four, so he must leave his fiancée and return to the pirates.
This plot point was also used in a Sherlock Holmes story based on the Basil Rathbone era, where a friend of Dr. Watson's is a Baronet who is due to receive his inheritance on the New Year's Day of the year where his twenty-first birthday will be celebrated, only for the law to deprive him of the money as he was born on February 29; with the 84-year-old Baronet distraught at the news that 1900 is not a leap year, Holmes helps the Baronet fake his death long enough for his grandson — who is the appropriate age to receive the inheritance — to establish his claim and receive the money himself.
Notable 29 February births.
Notable persons born on February 29:
Folk traditions.
There is a popular tradition known as Bachelor's Day in some countries allowing a woman to propose marriage to a man on February 29. If the man refuses, he then is obliged to give the woman money or buy her a dress. In upper-class societies in Europe, if the man refuses marriage, he then must purchase 12 pairs of gloves for the woman, suggesting that the gloves are to hide the woman's embarrassment of not having an engagement ring. In Ireland, the tradition is supposed to originate from a deal that Saint Bridget struck with Saint Patrick.
In the town of Aurora, Illinois, single women are deputized and may arrest single men, subject to a four-dollar fine, every February 29.
In Greece, it is considered unlucky to marry on a leap day.

</doc>
<doc id="10937" url="https://en.wikipedia.org/wiki?curid=10937" title="Francis Scott Key">
Francis Scott Key

Francis Scott Key (August 1, 1779January 11, 1843) was an American lawyer, author, and amateur poet, from Georgetown, Washington, D.C. who wrote the lyrics to the United States' national anthem, "The Star-Spangled Banner".
Early life and family.
Francis Scott Key was born to Ann Phoebe Penn Dagworthy (Charlton) and Captain John Ross Key at the family plantation Terra Rubra in what was then part of Frederick County, now Carroll County, Maryland. His father was a lawyer, judge, and officer in the Continental Army. His great-grandparents on his father's side were Philip Key and Susanna Barton Gardiner, both of whom were born in London and immigrated to Maryland in 1726.
Key graduated from St.John's College, Annapolis, Maryland and also read law under his uncle Philip Barton Key. He married Mary Tayloe Lloyd on January 1, 1802.
"The Star-Spangled Banner".
During the War of 1812, Key, accompanied by the British Prisoner Exchange Agent Colonel John Stuart Skinner, dined aboard the British ship HMS "Tonnant", as the guests of three British officers: Vice Admiral Alexander Cochrane, Rear Admiral George Cockburn, and Major General Robert Ross. Skinner and Key were there to negotiate the release of prisoners, one of whom was Dr.William Beanes, a resident of Upper Marlboro, Maryland who had been arrested after jailing marauding British troops who were looting local farms. Skinner, Key, and Beanes were not allowed to return to their own sloop because they had become familiar with the strength and position of the British units and with the British intent to attack Baltimore. Thus, Key was unable to do anything but watch the bombarding of the American forces at Fort McHenry during the Battle of Baltimore on the night of September 1314,1814.
At dawn, Key was able to see an American flag still waving and reported this to the prisoners below deck. Back in Baltimore and inspired, Key wrote a poem about his experience, "Defence of Fort M'Henry", which was soon published in William Pechin's the "American and Commercial Daily Advertiser" on September21,1814. He took it to Thomas Carr, a music publisher, who adapted it to the rhythms of composer John Stafford Smith's "To Anacreon in Heaven", a popular tune Key had already used as a setting for his 1805 song "When the Warrior Returns," celebrating U.S. heroes of the First Barbary War. (Key used the "star spangled" flag imagery in the earlier song.) It has become better known as "The Star-Spangled Banner". Though somewhat difficult to sing, it became increasingly popular, competing with "Hail, Columbia" (1796) as the de facto national anthem by the Mexican–American War and American Civil War. More than a century after its first publication, the song was adopted as the American national anthem, first by an Executive Order from President Woodrow Wilson in1916 (which had little effect beyond requiring military bands to play what became known as the "Service Version") and then by a Congressional resolution in1931, signed by President Herbert Hoover.
Legal career.
Key was a leading attorney in Frederick, Maryland and Washington, D.C. for many years, with an extensive real estate as well as trial practice. He and his family settled in Georgetown in 1805 or 1806, near the new national capital. There the young Key assisted his uncle, the prominent lawyer Philip Barton Key, including in the sensational conspiracy trial of Aaron Burr and the expulsion of Senator John Smith of Ohio. Key made the first of his many arguments before the United States Supreme Court in 1807. In 1808 Key assisted President Thomas Jefferson's attorney general in "United Statesv.Peters."
A supporter of Andrew Jackson, Key, in 1829 assisted in the prosecution of Tobias Watkins, former U.S.Treasury auditor under former President John Quincy Adams for misappropriating public monies, and also handled the Petticoat affair concerning Secretary of War John Eaton who had married a widowed saloonkeeper. In 1832, Key served as the attorney for Sam Houston, then a former U.S. Representative and Governor of Tennessee, during his trial for assaulting Representative William Stanbery of Ohio.
President Jackson nominated Key for United States Attorney for the District of Columbia in 1833. After the U.S. Senate approved the nomination, Key served from 1833 to 1841, while also handling his own private legal cases. In 1835, in his most famous case, Key prosecuted Richard Lawrence for his unsuccessful attempt to assassinate President Andrew Jackson at the entrance doors and top steps of the Capitol, the first attempt to kill an American chief executive.
Slavery and American Colonization Society.
Key purchased his first slave in 1800 or 1801, and owned six slaves in 1820. Mostly in the 1830s, Key manumitted seven slaves, one of whom (Clem Johnson) continued to work for him for wages as his farm's foreman, supervising several slaves.
Key throughout his career also represented several slaves seeking their freedom in court (for free), as well as several masters seeking return of their runaway human property. Key, Judge William Leigh of Halifax and bishop William Meade were administrators of the will of their friend John Randolph of Roanoke, who died without children and left a will directing his executors to free his more than four hundred slaves. Over the next decade, beginning in 1833, the administrators fought to enforce the will and provide the freed slaves land to support themselves.
Key was considered a decent master, and publicly criticized slavery's cruelties, so much that after his death a newspaper editorial stated "So actively hostile was he to the peculiar institution that he was called 'The Nigger Lawyer' ... because he often volunteered to defend the downtrodden sons and daughters of Africa. Mr.Key convinced me that slavery was wrong--radically wrong."
Key was a founding member and active leader of the American Colonization Society, and its predecessor influential Maryland branch, the primary goal of which was to send free African-Americans back to Africa.
However, he was removed from the board in 1833 as its policies shifted toward abolitionist.
Anti-abolitionist.
A slave-owner himself, Key used his position as U.S. Attorney to suppress abolitionists. In 1833, Key secured a grand jury indictment against Benjamin Lundy, editor of the anti-slavery publication, the "Genius of Universal Emancipation", and his printer, William Greer, for libel after Lundy published an article that declared, "There is neither mercy nor justice for colored people in this district Columbia". Lundy's article, Key said in the indictment, "was intended to injure, oppress, aggrieve, and vilify the good name, fame, credit & reputation of the Magistrates and constables" of Washington. Lundy left town rather than face trial; Greer was acquitted.
In August 1836, Key agreed to prosecute botanist and doctor Reuben Crandall, brother of controversial Connecticut school teacher Prudence Crandall, who had recently moved to the national capital. Key secured an indictment for "seditious libel" after two marshals (who operated as slave catchers in their off hours) found Crandall had a trunk full of anti-slavery publications in his Georgetown residence, five days after the Snow Riot, caused by rumors that a mentally ill slave had attempted to kill an elderly white woman. In an April 1837 trial that attracted nationwide attention, Key charged that Crandall's actions instigated slaves to rebel. Crandall's attorneys acknowledged he opposed slavery, but denied any intent or actions to encourage rebellion. Key, in his final address to the jury said: A jury acquitted Crandall.
This defeat, as well as family tragedies in 1835, diminished Key's political ambition. He resigned as district attorney in 1840. He remained a staunch proponent of African colonization and a strong critic of the antislavery movement until his death.
Religion.
Key was a devout and prominent Episcopalian. In his youth, Key almost became an Episcopal priest rather than a lawyer, and throughout his life he sprinkled biblical references in his correspondence. Key was active in All Saints Parish in Frederick, Maryland, near his family's home. He also helped found or financially support several parishes in the new national capital, including St. John's Church in Georgetown and Christ Church in Alexandria.
From 1818 until his death in 1843, Key was associated with the American Bible Society. , he successfully opposed an abolitionist resolution presented to that group.
Key also helped found two Episcopal seminaries, one in Baltimore, as well as the Virginia Theological Seminary across the Potomac River in Alexandria, Virginia. Key also published a prose work called "The Power of Literature, and Its Connection with Religion" in 1834.
Death and legacy.
In 1843, Key died at the home of his daughter Elizabeth Howard in Baltimore from pleurisy and was initially interred in Old Saint Paul's Cemetery in the vault of John Eager Howard. In 1866, his body was moved to his family plot in Frederick at Mount Olivet Cemetery.
The Key Monument Association erected a memorial in 1898 and the remains of both Francis Scott Key and his wife, Mary Tayloe Lloyd, were placed in a crypt in the base of the monument.
Despite several efforts to preserve it, the Francis Scott Key residence was ultimately dismantled in1947. The residence had been located at 351618MStreet in Georgetown.
Though Key had written poetry from time to time, often with heavily religious themes, these works were not collected and published until 14years after his death. Two of Key's religious poems used as Christian hymns include "Before the Lord We Bow" and "Lord, with Glowing Heart I'd Praise Thee".
In1806, Key's sister, Anne Phoebe Charlton Key, married Roger B. Taney, who would later become Chief Justice of the United States. In 1846 one daughter, Alice, married U.S. Senator George H. Pendleton and another, Ellen Lloyd, married Simon F. Blunt. In1859 Key's son Philip Barton Key II was shot and killed by Daniel Sicklesa U.S.Representative from New York who would serve as a general in the American Civil Warafter he discovered that Philip Barton Key was having an affair with his wife. Sickles was acquitted in the first use of the temporary insanity defense. In1861 Key's grandson Francis Key Howard was imprisoned in Fort McHenry with the Mayor of Baltimore George William Brown and other locals deemed pro-South.
Key was a distant cousin and the namesake of F. Scott Fitzgerald, whose full name was Francis Scott Key Fitzgerald. His direct descendants include geneticist Thomas Hunt Morgan, guitarist Dana Key, and American fashion designer and socialite Pauline de Rothschild.

</doc>
<doc id="10938" url="https://en.wikipedia.org/wiki?curid=10938" title="FSU">
FSU

FSU may refer to:

</doc>
<doc id="10939" url="https://en.wikipedia.org/wiki?curid=10939" title="Formal language">
Formal language

In mathematics, computer science, and linguistics, a formal language is a set of strings of symbols that may be constrained by rules that are specific to it.
The alphabet of a formal language is the set of symbols, letters, or tokens from which the strings of the language may be formed; frequently it is required to be finite. The strings formed from this alphabet are called words, and the words that belong to a particular formal language are sometimes called "well-formed words" or "well-formed formulas". A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, also called its formation rule.
The field of formal language theory studies primarily the purely syntactical aspects of such languages—that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.
In computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.
History.
The first formal language is thought to be the one used by Gottlob Frege in his "Begriffsschrift" (1879), literally meaning "concept writing", and which Frege described as a "formal language of pure thought."
Axel Thue's early Semi-Thue system which can be used for rewriting strings was influential on formal grammars.
Words over an alphabet.
An alphabet, in the context of formal languages, can be any set, although it often makes sense to use an alphabet in the usual sense of the word, or more generally a character set such as ASCII or Unicode. Alphabets can also be infinite; e.g. first-order logic is often expressed using an alphabet which, besides symbols such as ∧, ¬, ∀ and parentheses, contains infinitely many elements "x"0, "x"1, "x"2, … that play the role of variables. The elements of an alphabet are called its letters.
A word over an alphabet can be any finite sequence, or string, of characters or letters, which sometimes may include spaces, and are separated by specified word separation characters. The set of all words over an alphabet Σ is usually denoted by Σ* (using the Kleene star). The length of a word is the number of characters or letters it is composed of. For any alphabet there is only one word of length 0, the "empty word", which is often denoted by e, ε or λ. By concatenation one can combine two words to form a new word, whose length is the sum of the lengths of the original words. The result of concatenating a word with the empty word is the original word.
In some applications, especially in logic, the alphabet is also known as the "vocabulary" and words are known as "formulas" or "sentences"; this breaks the letter/word metaphor and replaces it by a word/sentence metaphor.
Definition.
A formal language "L" over an alphabet Σ is a subset of Σ*, that is, a set of words over that alphabet. Sometimes the sets of words are grouped into expressions, whereas rules and constraints may be formulated for the creation of 'well-formed expressions'.
In computer science and mathematics, which do not usually deal with natural languages, the adjective "formal" is often omitted as redundant.
While formal language theory usually concerns itself with formal languages that are described by some syntactical rules, the actual definition of the concept "formal language" is only as above: a (possibly infinite) set of finite-length strings composed from a given alphabet, no more nor less. In practice, there are many languages that can be described by rules, such as regular languages or context-free languages. The notion of a formal grammar may be closer to the intuitive concept of a "language," one described by syntactic rules. By an abuse of the definition, a particular formal language is often thought of as being equipped with a formal grammar that describes it.
Examples.
The following rules describe a formal language  over the alphabet Σ = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, = }:
Under these rules, the string "23+4=555" is in , but the string "=234=+" is not. This formal language expresses natural numbers, well-formed addition statements, and well-formed addition equalities, but it expresses only what they look like (their syntax), not what they mean (semantics). For instance, nowhere in these rules is there any indication that "0" means the number zero, or that "+" means addition.
Constructions.
For finite languages one can explicitly enumerate all well-formed words. For example, we can describe a language  as just  = {"a", "b", "ab", "cba"}. The degenerate case of this construction is the empty language, which contains no words at all ( = ∅).
However, even over a finite (non-empty) alphabet such as Σ = {a, b} there are an infinite number of words that can potentially be expressed: "a", "abb", "ababba", "aaababbbbaab", …. Therefore, formal languages are typically infinite, and describing an infinite formal language is not as simple as writing "L" = {"a", "b", "ab", "cba"}. Here are some examples of formal languages:
Language-specification formalisms.
Formal languages are used as tools in multiple disciplines. However, formal language theory rarely concerns itself with particular languages (except as examples), but is mainly concerned with the study of various types of formalisms to describe languages. For instance, a language can be given as
Typical questions asked about such formalisms include:
Surprisingly often, the answer to these decision problems is "it cannot be done at all", or "it is extremely expensive" (with a characterization of how expensive). Therefore, formal language theory is a major application area of computability theory and complexity theory. Formal languages may be classified in the Chomsky hierarchy based on the expressive power of their generative grammar as well as the complexity of their recognizing automaton. Context-free grammars and regular grammars provide a good compromise between expressivity and ease of parsing, and are widely used in practical applications.
Operations on languages.
Certain operations on languages are common. This includes the standard set operations, such as union, intersection, and complement. Another class of operation is the element-wise application of string operations.
Examples: suppose "L"1 and "L"2 are languages over some common alphabet.
Such string operations are used to investigate closure properties of classes of languages. A class of languages is closed under a particular operation when the operation, applied to languages in the class, always produces a language in the same class again. For instance, the context-free languages are known to be closed under union, concatenation, and intersection with regular languages, but not closed under intersection or complement. The theory of trios and abstract families of languages studies the most common closure properties of language families in their own right.
Applications.
Programming languages.
A compiler usually has two distinct components. A lexical analyzer, generated by a tool like codice_1, identifies the tokens of the programming language grammar, e.g. identifiers or keywords, which are themselves expressed in a simpler formal language, usually by means of regular expressions. At the most basic conceptual level, a parser, usually generated by a parser generator like codice_2, attempts to decide if the source program is valid, that is if it belongs to the programming language for which the compiler was built.
Of course, compilers do more than just parse the source code — they usually translate it into some executable format. Because of this, a parser usually outputs more than a yes/no answer, typically an abstract syntax tree. This is used by subsequent stages of the compiler to eventually generate an executable containing machine code that runs directly on the hardware, or some intermediate code that requires a virtual machine to execute.
Formal theories, systems and proofs.
In mathematical logic, a "formal theory" is a set of sentences expressed in a formal language.
A "formal system" (also called a "logical calculus", or a "logical system") consists of a formal language together with a deductive apparatus (also called a "deductive system"). The deductive apparatus may consist of a set of transformation rules which may be interpreted as valid rules of inference or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems formula_1 and formula_2 may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).
A "formal proof" or "derivation" is a finite sequence of well-formed formulas (which may be interpreted as propositions) each of which is an axiom or follows from the preceding formulas in the sequence by a rule of inference. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.
Interpretations and models.
Formal languages are entirely syntactic in nature but may be given semantics that give meaning to the elements of the language. For instance, in mathematical logic, the set of possible formulas of a particular logic is a formal language, and an interpretation assigns a meaning to each of the formulas—usually, a truth value.
The study of interpretations of formal languages is called formal semantics. In mathematical logic, this is often done in terms of model theory. In model theory, the terms that occur in a formula are interpreted as mathematical structures, and fixed compositional interpretation rules determine how the truth value of the formula can be derived from the interpretation of its terms; a "model" for a formula is an interpretation of terms such that the formula becomes true.

</doc>
<doc id="10940" url="https://en.wikipedia.org/wiki?curid=10940" title="Free to Choose">
Free to Choose

Free to Choose (1980) is a book and a ten-part television series broadcast on public television by economists Milton and Rose D. Friedman that advocates free market principles. It was primarily a response to an earlier landmark book and television series: "The Age of Uncertainty", by the noted economist John Kenneth Galbraith. Milton Friedman won the Nobel Memorial Prize in Economics in 1976.
Overview.
"Free to Choose: A Personal Statement" maintains that the free market works best for all members of a society, provides examples of how the free market engenders prosperity, and maintains that it can solve problems where other approaches have failed. Published in January 1980, the 297 page book contains 10 chapters. The book was on the United States best sellers list for 5 weeks.
PBS telecast the series, beginning in January 1980. The general format was that of Dr. Friedman visiting and narrating a number of success and failure stories in history, which Dr. Friedman attributes to capitalism or the lack thereof (e.g. Hong Kong is commended for its free markets, while India is excoriated for relying on centralized planning especially for its protection of its traditional textile industry). Following the primary show, Dr. Friedman would engage in discussion moderated by Robert McKenzie with a number of selected debaters drawn from trade unions, academy and the business community, such as Donald Rumsfeld (then of G.D. Searle & Company) and Frances Fox Piven of City University of New York. The interlocutors would offer objections to or support for the proposals put forward by Friedman, who would in turn respond. After the final episode, Friedman sat down for an interview with Lawrence Spivak.
The series was rebroadcast in 1990 with Linda Chavez moderating the episodes. Arnold Schwarzenegger, Ronald Reagan, Steve Allen and others give personal introductions for each episode in the series. This time, after the documentary part, Friedman sits down with a single opponent to debate the issues raised in the episode.
Guest debaters.
Guest debaters included:
Positions advocated.
The Friedmans advocate "laissez-faire" economic policies, often criticizing interventionist government policies and their cost in personal freedoms and economic efficiency in the United States and abroad. The authors argue against government taxation on gas and tobacco and government regulation of the public school systems. The Friedmans argue that the Federal Reserve exacerbated the Great Depression by neglecting to prevent the decline of the money supply in the years leading up to it.
On the subject of welfare, the Friedmans argue that current welfare practices are creating "wards of the state" as opposed to "self-reliant individuals" and suggest a negative income tax as a less harmful alternative. The Friedmans also argue for abolishing the Food and Drug Administration, tighter control of Fed money supply, and the repeal of laws favoring labor unions.

</doc>
<doc id="10945" url="https://en.wikipedia.org/wiki?curid=10945" title="Melbourne Grand Prix Circuit">
Melbourne Grand Prix Circuit

The Melbourne Grand Prix Circuit is a street circuit around Albert Park Lake, only a few kilometres south of central Melbourne. It is used annually as a racetrack for the Formula One Australian Grand Prix, V8 Supercars Challenge and associated support races. The circuit has FIA Grade 1 license. In spite of being a circuit on public roads it has characteristics of a natural road course considering it being fast and flowing combined with extensive runoff in many corners.
Design.
The circuit uses everyday sections of road that circle Albert Park Lake, a small man-altered lake (originally a large lagoon formed as part of the ancient Yarra River course) just south of the Central Business District of Melbourne. The road sections that are used were rebuilt prior to the inaugural event in 1996 to ensure consistency and smoothness. As a result, compared to other circuits that are held on public roads, the Albert Park track has quite a smooth surface. Before 2007 there existed only a few other places on the Formula 1 calendar with a body of water close to the track. Many of the new tracks, such as Valencia, Singapore and Abu Dhabi have imitated that feature.
The course is considered to be quite fast and relatively easy to drive, drivers having commented that the consistent placement of corners allows them to easily learn the circuit and achieve competitive times. However, the flat terrain around the lake, coupled with a track design that features few true straights, means that the track is not conducive to overtaking or easy spectating unless in possession of a grandstand seat.
Each year, most of the trackside fencing, pedestrian overpasses, grandstands and other motorsport infrastructure are erected approximately two months prior to the Grand Prix weekend and removed within 6 weeks after the event. Land around the circuit (including a large aquatic centre, a golf course, a Lakeside Stadium, some restaurants and rowing boathouses) has restricted access during the grand prix weekend. Dissent is still prevalent among nearby local residents and users of those others facilities, and some still maintain a silent protest against the event. Nevertheless, the event is reasonably popular in Melbourne and Australia (with a large European population and a general interest in motorsport). Middle Park, the home of South Melbourne FC was demolished in 1994 due to expansion at Albert Park.
On 4 July 2008, the official F1 site reported that more than 300,000 people attended the four-day Melbourne Grand Prix, though actual ticket sales were later disputed by the local media. The Grand Prix will continue until at least 2020 after securing a new contract with Formula One Management. There has never been a night race at Albert Park, however, 2009’s event started at 5.00 p.m.
Everyday access.
During the nine months of the year when the track is not required for Grand Prix preparation or the race weekend, most of the track can be driven by ordinary street-registered vehicles either clockwise or anti-clockwise.
Only the sections between turns 3, 4 and 5, then 5 and 6, differ significantly from the race track configuration. Turn 4 is replaced by a car park access road running directly from turns 3 to 5. Between turns 5 and 6, the road is blocked. It is possible to drive from turn 5 on to Albert Road and back on to the track at turn 7 though two sets of lights control the flow of this option. The only set of lights on the actual track is half-way between turns 12 and 13, where drivers using Queens Road are catered for. The chicanes at turns 11 and 12 is considerably more open than that used in the grand prix, using the escape roads. Turn 9 is also a car park and traffic is directed down another escape road.
The speed limit is generally which is slower than an F1 car under pit lane speed restrictions. Some short sections have a speed limit of . The back of the track, turns 7 to 13 inclusive, is known as Lakeside Drive. Double lines separate the two-way traffic along most of Lakeside Drive with short road islands approximately every 50 metres. This means overtaking is illegal here.
Approximately 50% of the track edge is lined with short parkland-style chain-linked fencing leaving normal drivers less room for error than F1 drivers have during race weekend. There is however substantial shoulder room between the outside of each lane and the fencing.
A lap in a Formula One car.
Turn 1 is an incredibly challenging, medium speed corner that catches several drivers out. It comes at the end of the first DRS zone but despite this it is not a prime overtaking spot. You brake just after the 100 metre board and shift down into third gear, then you accelerate as soon as you hit the exit kerb, keeping flat out round turn 2. It is important to gain a good exit, as the second DRS zone begins down the following straight. Turn 3 is the best overtaking place as you brake roughly 100 metres before the apex for the second gear corner. It is reasonably easy to outbrake a competitor and you can use either the inside or outside; turn 4 is a left-hander than comes immediately after so the outside of turn 3 gives the inside for four. Turn five is a flat-out right hander with high g-force, and turns six, seven and eight make up a difficult complex at the back of the circuit. Six has a very challenging braking-zone due to trees' shadows obscuring the view of parts of the track. It is even more difficult in the wet as you are unable to see and puddles that will be sitting on the circuit, making it easy to spin off. Turn seven is similar to turn two; a flat-out left-hander after a tricky right, then turn eight is a long, flat-out right-hander which drivers now take with DRS. Turns 9 and 10 make up a slow chicane, some would say a pointless chicane, that leads onto a short straight. You brake about eighty metres from turn nine and you need good traction to carry as much speed as you can out of ten. Turns 11 and 12 make up a challenging, high-speed, left-right chicane where drivers ride the kerbs, however too much kerb can upset the car's balance. Turn 13 can be used for overtaking but offline it can be quite dirty and slippery. Slipstreaming a car out of 12 can get them alongside another car, then braking late can get you down the inside of the third-gear right-hander. Turn 14 is an exciting right-hander that is taken in fifth-gear and requires a lift off the throttle. Turn 15 is the slowest point on the circuit and is a second-gear left-hander, and it is important to get a good exit from the near-flat-out turn 16 so you gain speed all the way down the pit straight.
Albert Park Circuit (1953 to 1958).
Albert Park has the distinction of being the only venue to host the Australian Grand Prix in both World Championship and non-World Championship formats with an earlier configuration of the current circuit used for the race on two occasions during the 1950s. During this time racing was conducted in an anti-clockwise direction as opposed to the current circuit which runs clockwise.
Known as the Albert Park Circuit, the original 3.125 mile (5.03 kilometre) course hosted a total of six race meetings: 
Lap records.
As of 14 March 2015.

</doc>
<doc id="10946" url="https://en.wikipedia.org/wiki?curid=10946" title="Monaco Grand Prix">
Monaco Grand Prix

The Monaco Grand Prix () is a Formula One motor race held each year on the Circuit de Monaco. Run since 1929, it is widely considered to be one of the most important and prestigious automobile races in the world and, with the Indianapolis 500 and the 24 Hours of Le Mans, forms the Triple Crown of Motorsport. The circuit has been called "an exceptional location of glamour and prestige".
The race is held on a narrow course laid out in the streets of Monaco, with many elevation changes and tight corners as well as a tunnel, making it one of the most demanding tracks in Formula One. In spite of the relatively low average speeds, it is a dangerous place to race and often involves the intervention of a safety car. It is the only Grand Prix that does not adhere to the FIA's mandated minimum race distance.
The event was part of the pre-Second World War European Championship and was included in the first World Championship of Drivers in 1950. It was designated the European Grand Prix two times, 1955 and 1963, when this title was an honorary designation given each year to one Grand Prix race in Europe. Graham Hill was known as ""Mr. Monaco"" due to his five Monaco wins in the 1960s. Brazil's Ayrton Senna won the race more times than any other driver, with six victories, winning five races consecutively between 1989 and 1993. Fernando Alonso is the only driver to have won the race in consecutive years for different constructors, winning for Renault in 2006 and McLaren in 2007.
History.
Origins.
Like many European races, the Monaco Grand Prix predates the current World Championship. The principality's first Grand Prix was organised in 1929 by Antony Noghès, under the auspices of Prince Louis II, through the Automobile Club de Monaco (ACM). Alexandre Noghès, Anthony's father, was founding president of the ACM, originally named "Sport Vélocipédique Monégasque". The ACM made their first foray into motorsport by holding the Rallye Automobile Monte Carlo in 1911. In 1928 the club applied to the "Association Internationale des Automobiles Clubs Reconnus" (AIACR), the international governing body of motorsport, to be upgraded from a regional French club to full national status. Their application was refused due to the lack of a major motorsport event held wholly within Monaco's boundaries. The rally could not be considered as it mostly used the roads of other European countries.
In order to attain full national status, Noghès proposed the creation of an automobile Grand Prix in the streets of Monte Carlo. Noghès obtained the official support of Prince Louis II. Noghès also gained support for his plans from Monegasque Louis Chiron, a top-level driver in European Grand Prix racing. Chiron thought that the topography of the location would be well suited to setting up a race track.
The first race, held on 14 April 1929, was organised by cigarette magnate Antony Noghès under the auspices of the "Automobile Club de Monaco", and was won by William Grover-Williams driving a Bugatti.
The first Grand Prix Automobile de Monaco was an invitation-only event, but not all of those invited decided to attend. The leading Maserati and Alfa Romeo drivers decided not to compete, but Bugatti was well represented. Mercedes sent their leading driver Rudolf Caracciola to drive a Mercedes SSK. Caracciola drove a fighting race, bringing his SSK up to second position at the end of the race, despite starting in fifteenth. The race was won by "Williams" (pseudonym of William Grover-Williams) driving a Bugatti Type 35B painted dark green. Another driver who competed using a pseudonym was "Georges Philippe", the Baron Philippe de Rothschild. Chiron was unable to compete, having a prior commitment to compete in the Indianapolis 500 on the same day.
However, Chiron did compete the following year, when he was beaten by René Dreyfus and his Bugatti and finished second, and took victory in the 1931 race driving a Bugatti. , he remains the only native of Monaco to have won the event.
Pre-war.
The race quickly grew in importance. Because of the large number of races which were being termed 'Grands Prix', the AIACR formally recognised the most important race of each of its affiliated national automobile clubs as International Grands Prix, or "Grandes Épreuves", and in 1933 Monaco was ranked as such alongside the French, Belgian, Italian, and Spanish Grands Prix. That year's race was the first Grand Prix where grid positions were decided, as they are now, by practice time rather than the established method of balloting. The race saw Achille Varzi and Tazio Nuvolari exchange the lead many times before being settled in Varzi's favour on the final lap when Nuvolari's car caught fire. The race became a round of the new European Championship in 1936, when stormy weather and a broken oil line led to a series of crashes, eliminating the Mercedes-Benzes of Chiron, Fagioli, and von Brauchitsch, as well as Bernd Rosemeyer's "Typ C" for newcomer Auto Union; Rudolf Caracciola, proving the truth of his nickname, "Regenmeister" (Rainmaster), went on to win. In 1937, von Brauchitsch duelled Caracciola before coming out on top. It was the last prewar "Grand Prix" at Monaco, for in 1938, the demand for ₤500 (about US$2450) in appearance money "per" top entrant led AIACR to cancel the event, while looming war overtook it in 1939, and the Second World War ended organised racing in Europe until 1945.
Post-war Grand Prix.
Racing in Europe started again on 9 September 1945 at the Bois de Boulogne Park in the city of Paris, four months and one day after the end of the war in Europe. In 1946 a new premier racing category, Grand Prix, was defined by the Fédération Internationale de l'Automobile (FIA), the successor of the AIACR, based on the pre-war voiturette class. A Monaco Grand Prix was run to this formula in 1948, won by the future world champion Nino Farina in a Maserati 4CLT.
Formula One.
Early championship days.
The 1949 event was cancelled due to the death of Prince Louis II; it was included in the new Formula One World Drivers' Championship the following year. The race provided future five-time world champion Juan Manuel Fangio with his first win in a World Championship race, as well as third place for the 51-year-old Louis Chiron, his best result in the World Championship era. However, there was no race in 1951, and in 1952, a year in which the world drivers' championship was run for less powerful Formula Two cars, the race was run to sports car rules instead and did not form part of the World Championship.
Since 1955, the Monaco Grand Prix has continuously been part of the Formula One World Championship. That year, Maurice Trintignant won in Monte Carlo for the first time and Chiron again scored points and at 56 became the oldest driver to compete in a Formula One Grand Prix. It was not until 1957, when Fangio won again, that the Grand Prix saw a double winner. Between 1954 and 1961 Fangio's former Mercedes colleague, Stirling Moss, went one better, as did Trintignant, who won the race again in 1958 driving a Cooper. The 1961 race saw Moss fend off three works Ferrari 156s in a year-old privateer Rob Walker Racing Team Lotus 18, to take his third Monaco victory.
Graham Hill's era.
Britain's Graham Hill won the race five times in the 1960s and became known as ""King of Monaco"" and ""Mr. Monaco"". He first won in 1963, and then won the next two years. In the 1965 race he took pole position and led from the start, but went up an escape road on lap 25 to avoid hitting a slow backmarker. Re-joining in fifth place, Hill set several new lap records on the way to winning. The race was also notable for Jim Clark's absence (he was doing the Indianapolis 500), and for Paul Hawkins' Lotus ending up in the harbour. A similar incident was included in the 1966 film "Grand Prix". Hill's teammate, Briton Jackie Stewart, won in 1966 and New Zealander Denny Hulme won in 1967, but Hill won the next two years, the 1969 event being his final Formula One championship victory, by which time he was a double Formula One world champion.
Track alterations, safety, and increasing business interests.
By the start of the 1970s, efforts by Jackie Stewart saw a few events cancelled because of safety concerns. For the 1969 event, Armco barriers were placed at specific points for the first time in the circuit's history; before that, the circuit's conditions were (aside from the removal of people's production cars parked on the side of the road) virtually identical to everyday civilian use. If a driver went off, he would crash into whatever was next to the track (buildings, trees, lamp posts, glass windows, and even a train station); and in Alberto Ascari and Paul Hawkins' cases, the harbour water, because the concrete road the course used had no Armco to protect the drivers from going off the track and into the Mediterranean. The circuit gained more Armco in specific points for the next 2 races, and by 1972, the circuit was almost completely Armco-lined. And for the first time in its history, the Monaco circuit was altered that year; the pits were moved next to the waterfront straight between the chicane and Tabac and the chicane was moved further forward right before Tabac and was the junction point between the pits and the course. The course was changed again for the 1973 race; the Rainier III Nautical Stadium was constructed where the straight that went behind the pits was and the circuit introduced a twisty section that went around the new swimming pool. This created space for a whole new pit facility; and in 1976 the course was altered yet again; the Sainte Devote corner was made slower and a chicane was placed right before the pit straight.
For the next two races, By the early 1970s, as Brabham team owner Bernie Ecclestone started to marshal the collective bargaining power of the Formula One Constructors Association (FOCA), Monaco was prestigious enough to become an early bone of contention. Historically the number of cars permitted in a race was decided by the race organiser, in this case the ACM, which had always set a low number of around 16. In 1972 Ecclestone was starting to negotiate deals which relied on FOCA guaranteeing at least 18 entrants for every race. A stand-off over this issue left the 1972 race in jeopardy until the ACM gave in and agreed that 26 cars could participate – the same number permitted at most other circuits. Two years later, in 1974, the ACM managed to get the numbers back down to 18.
Because of its tight confines, slow average speeds and punishing nature, Monaco has often thrown up unexpected results. In the 1982 race René Arnoux led the first 15 laps, before retiring. Alain Prost then led until four laps from the end, when he spun off on the wet track, hit the barriers and lost a wheel, giving Riccardo Patrese the lead. Patrese himself spun with only a lap and a half to go, letting Didier Pironi through to the front, followed by Andrea de Cesaris. On the last lap, Pironi ran out of fuel in the tunnel, but De Cesaris also ran out of fuel before he could overtake. In the meantime Patrese had bump-started his car and went through to score his first Grand Prix win.
In 1983 the ACM became entangled in the disagreements between Fédération Internationale du Sport Automobile (FISA) and FOCA. The ACM, with the agreement of Bernie Ecclestone, negotiated an individual television rights deal with ABC in the United States. This broke an agreement enforced by FISA for a single central negotiation of television rights. Jean-Marie Balestre, president of FISA, announced that the Monaco Grand Prix would not form part of the Formula One world championship in 1985. The ACM fought their case in the French courts. They won the case and the race was eventually reinstated.
Prost/Senna era.
For the decade from 1984 to 1993 the race was won by only two drivers, arguably the two best drivers in Formula One at the time- Frenchman Prost and Brazilian Ayrton Senna. Prost, already a winner of the support race for Formula Three cars in 1979, took his first Monaco win at the 1984 race. The race started 45 minutes late after heavy rain. Prost led briefly before Nigel Mansell overtook him on lap 11. Mansell crashed out five laps later, letting Prost back into the lead. On lap 27, Prost led from Ayrton Senna's Toleman and Stefan Bellof's Tyrrell. Senna was catching Prost and Bellof was catching both of them. However, on lap 31, the race was controversially stopped with conditions deemed to be undriveable. Later, FISA fined the clerk of the course, Jacky Ickx, $6,000 and suspended his licence for not consulting the stewards before stopping the race. The drivers received only half of the points that would usually be awarded, as the race had been stopped before two thirds of the intended race distance had been completed.
Prost won 1985 after polesitter Senna retired with a blown Renault engine in his Lotus after overrevving it at the start, and Michele Alboreto in the Ferrari retook the lead twice, but he went off the track at Sainte-Devote, where Brazilian Nelson Piquet and Italian Riccardo Patrese had a huge accident only a few laps previously and oil and debris littered the track. Prost passed Alboreto, who retook the Frenchman, and then he punctured a tire after running over bodywork debris from the Piquet/Patrese accident, which dropped him to 4th. He was able to pass his Roman countrymen Andrea De Cesaris and Elio de Angelis, but finished 2nd behind Prost. The French Prost dominated 1986 after starting from pole position, a race where the Nouvelle Chicane had been changed on the grounds of safety.
Senna holds the record for the most victories in Monaco, with six, including five consecutive wins between 1989 and 1993, as well as eight podium finishes in ten starts. His 1987 win was the first time a car with an active suspension had won a Grand Prix. He managed to win this race after Briton Nigel Mansell in a Williams-Honda went out with a broken exhaust. His win was very popular with the people of Monaco, and when he was arrested on the Monday following the race, for riding a motorcycle without wearing a helmet, he was released by the officers after they realised who he was. Senna dominated 1988, and was able to get ahead of his teammate Prost while the Frenchman was held up for most of the race by Austrian Gerhard Berger in a Ferrari. By the time Prost got past Berger, he pushed as hard as he could and set a lap some 6 seconds faster than Senna's- at which the Brazilian panicked; he then set 2 fastest laps, and while pushing as hard as possible, he bit the barrier at the Portier corner and crashed into the Armco separating the road from the Mediterranean. Senna was so upset that he went back to his Monaco flat and was not heard from again; Prost went on to win for the fourth time. Senna dominated 1989 while Prost was stuck behind backmarker has-been Rene Arnoux and others; the Brazilian also dominated 1990 and 1991. At the 1992 event Nigel Mansell, who had won all five races held to that point in the season, took pole and dominated the race in his Williams FW14B-Renault. However, with seven laps remaining, Mansell suffered a loose wheel nut and was forced into the pits, emerging behind Ayrton Senna's McLaren-Honda, who was on worn tyres. Mansell, on fresh tyres, set a lap record almost two seconds quicker than Senna's and closed from 5.2 to 1.9 seconds in only two laps. The pair duelled around Monaco for the final four laps but Mansell could find no way past, finishing just two tenths of a second behind the Brazilian. Senna had a poor start to the 1993 event, he crashed in practice and qualified 3rd behind pole-sitter Prost and the German rising star Michael Schumacher. The Brazilian Senna was again fortuitous to win in 1993 after getting to the first corner in third behind Prost and Schumacher. Prost had to serve a time penalty for jumping the start and Schumacher retired with hydrualic active suspension problems, and Senna won in front of the late Graham Hill's son Damon. It was Senna's fifth win at Monaco, equalling Graham Hill's record. After Senna took his sixth win at the 1993 race, breaking Graham Hill's record for most wins at the Monaco Grand Prix, runner-up Damon Hill commented that "If my father was around now, he would be the first to congratulate Ayrton."
Modern times.
The 1994 race was an emotional and tragic affair; it came 2 weeks after the tragic race at Imola where Austrian Roland Ratzenberger and Senna both died from massive head injuries from on-track accidents on successive days. But during the Monaco event, German Karl Wendlinger had an appalling accident in his Sauber in the tunnel; he went into a coma and was to miss the rest of the season; some feared for his life. But the German Schumacher won the 1994 Monaco event easily. The 1996 race saw Michael Schumacher take pole position before crashing out on the first lap after being overtaken by Damon Hill. Hill led the first 40 laps before his engine expired in the tunnel. Jean Alesi took the lead but suffered suspension failure 20 laps later. Olivier Panis, who started in 14th place, moved into the lead and stayed there until the end of the race, being pushed all the way by David Coulthard. It was Panis' only win, and the last for his Ligier team. Only three cars crossed the finish line, but seven were classified.
Seven-time world champion Schumacher would eventually win the race five times, matching Graham Hill's record. Schumacher also holds the current lap record with a 1:14.439, set in 2004. In his appearance at the 2006 event, he attracted criticism when, while provisionally holding pole position and with the qualifying session drawing to a close, he stopped his car at the Rascasse hairpin, blocking the track and obliging competitors to slow down. Although Schumacher claimed it was the unintentional result of a genuine car failure, the FIA disagreed and he was sent to the back of the grid.
In July 2010, Bernie Ecclestone announced that a 10-year deal had been reached with the race organisers, keeping the race on the calendar until at least 2020.
Circuit.
The Circuit de Monaco consists of the city streets of Monte Carlo and La Condamine, which includes the famous harbour. It is unique in having been held on the same circuit every time it has been run over such a long period — only the Italian Grand Prix, which has been held at Autodromo Nazionale Monza during every Formula One regulated year except 1980, has a similarly lengthy and close relationship with a single circuit.
The race circuit has many elevation changes, tight corners, and a narrow course that makes it one of the most demanding tracks in Formula One racing. , two drivers have crashed and ended up in the harbour, the most famous being Alberto Ascari in
1955. Despite the fact that the course has had minor changes several times during its history, it is still considered the ultimate test of driving skills in Formula One, and if it were not already an existing Grand Prix, it would not be permitted to be added to the schedule for safety reasons. Even in 1929, 'La Vie Automobile' magazine offered the opinion that "Any respectable traffic system would have covered the track with Â«DangerÂ» sign posts left, right and centre".
Triple Formula One champion Nelson Piquet was fond of saying that racing at Monaco was "like trying to cycle round your living room", but added that "a win here was worth two anywhere else".
Notably, the course includes a tunnel. The contrast of daylight and gloom when entering/exiting the tunnel presents "challenges not faced elsewhere", as the drivers have to "adjust their vision as they emerge from the tunnel at the fastest point of the track and brake for the chicane in the daylight.". The fastest-ever lap was set by Kimi Räikkönen in qualifying for the 2006 Grand Prix, at 1m 13.532.
Viewing areas.
During the Grand Prix weekend spectators crowd around the Monaco Circuit. There are a number of temporary grandstands built around the circuit, mostly around the harbour area. The rich and famous arrive on their boats and the yachts in the harbour fill with spectators. Balconies around Monaco become viewing areas for the race too. Many hotels and residents cash in on the birds eye views of the race. Grand Prix organizers Automobile Club de Monaco officially voted the Ermanno Palace Penthouse the ‘Best view of the Monaco Grand Prix’.
Organization.
The Monaco Grand Prix is organized each year by the "Automobile Club de Monaco" which also runs the Monte Carlo Rally and the Junior Monaco Kart Cup.
It differs in several ways from other Grands Prix. The practice session for the race is held on the Thursday preceding the race instead of Friday. This allows the streets to be opened to the public again on Friday. Until the late 1990s the race started at 3:30 p.m. local time – an hour and a half later than other European Formula One races. In recent years the race has fallen in line with the other Formula One races for the convenience of television viewers. Also, earlier the event was traditionally held on the week of Ascension Day. It is now held on what is Memorial Day Weekend in the United States. For many years, the numbers of cars admitted to Grands Prix was at the discretion of the race organisers – Monaco had the smallest grids, ostensibly because of its narrow and twisting track. Only 18 cars were permitted to enter the 1975 Monaco Grand Prix, compared to 23 to 26 cars at all other rounds that year.
The erecting of the circuit takes six weeks, and the removal after the race takes three weeks. There is no podium as such at the race. Instead a section of the track is closed after the race to act as parc fermé, a place where the cars are held for official inspection. The first three drivers in the race leave their cars there and walk directly to the royal box where the 'podium' ceremony is held, which is considered a custom for the race. The trophies are handed out before the national anthems' of the driver and team are played, as opposed to other Grands Prix where the anthems are played first.
Fame.
The Monaco Grand Prix is widely considered to be one of the most important and prestigious automobile races in the world alongside the Indianapolis 500-Mile Race and the 24 Hours of Le Mans. These three races are considered to form a "Triple Crown" of the three most famous motor races in the world. Graham Hill is the only driver to have won the Triple Crown, by winning all three races. The practice session for Monaco overlaps with that for the Indianapolis 500, and the races themselves sometimes clash. As the two races take place on opposite sides of the Atlantic Ocean and form part of different championships, it is difficult for one driver to compete effectively in both during his career. Juan Pablo Montoya, who won the Monaco Grand Prix in 2003 and the Indianapolis 500 in 2000 and 2015, is the only driver still racing in 2015 who has won two of the three races and thus is the closest to completing the Triple Crown.
In awarding its first Gold medal for motor sport to Prince Rainier III, the Fédération Internationale de l'Automobile (FIA) characterised the Monaco Grand Prix as contributing "an exceptional location of glamour and prestige" to motor sport. It has been run under the patronage of three generations of Monaco's royal family: Louis II, Rainier III and Albert II, all of whom have taken a close interest in the race. A large part of the principality's income comes from tourists attracted by the warm climate and the famous casino, but it is also a tax haven and is home to many millionaires, including several Formula One drivers.
Monaco has produced only three native Formula One drivers, Louis Chiron, André Testut and Olivier Beretta, but its tax status has made it home to many drivers over the years, including Gilles Villeneuve and Ayrton Senna. Of the 2006 Formula One contenders, several have property in the principality, including Jenson Button and David Coulthard, who was part owner of a hotel there. Because of the small size of the town and the location of the circuit, drivers whose races end early can usually get back to their apartments in minutes. Ayrton Senna famously retired to his apartment after crashing out of the lead of the 1988 race.
Winners.
Multiple winners (drivers).
"Embolded drivers are still competing in the Formula One championship"
Multiple winners (constructors).
"A pink background indicates an event which was not part of the Formula One World Championship."
"A cream background indicates an event which was part of the pre-war European Championship."
"Embolded teams are competing in the Formula One championship in the current season."
By year.
"A pink background indicates an event which was not part of the Formula One World Championship."
"A cream background indicates an event which was part of the pre-war European Championship."

</doc>
<doc id="10947" url="https://en.wikipedia.org/wiki?curid=10947" title="Fission">
Fission

Fission, a splitting of something into two or more parts, may refer to:

</doc>
<doc id="10948" url="https://en.wikipedia.org/wiki?curid=10948" title="Fusion">
Fusion

Fusion or synthesis, the process of combining two or more distinct entities into a new whole, may refer to:

</doc>
<doc id="10949" url="https://en.wikipedia.org/wiki?curid=10949" title="Four color theorem">
Four color theorem

In mathematics, the four color theorem, or the four color map theorem, states that, given any separation of a plane into contiguous regions, producing a figure called a "map", no more than four colors are required to color the regions of the map so that no two adjacent regions have the same color. Two regions are called "adjacent" if they share a common boundary that is not a corner, where corners are the points shared by three or more regions.
Despite the motivation from coloring political maps of countries, the theorem is not of particular interest to mapmakers. According to an article by the math historian Kenneth May , “Maps utilizing only four colors are rare, and those that do usually require only three. Books on cartography and the history of mapmaking do not mention the four-color property.”
Three colors are adequate for simpler maps, but an additional fourth color is required for some maps, such as a map in which one region is surrounded by an odd number of other regions that touch each other in a cycle. The five color theorem, which has a short elementary proof, states that five colors suffice to color a map and was proven in the late 19th century ; however, proving that four colors suffice turned out to be significantly harder. A number of false proofs and false counterexamples have appeared since the first statement of the four color theorem in 1852.
The four color theorem was proven in 1976 by Kenneth Appel and Wolfgang Haken. It was the first major theorem to be proved using a computer. Appel and Haken's approach started by showing that there is a particular set of 1,936 maps, each of which cannot be part of a smallest-sized counterexample to the four color theorem. (If they did appear, you could make a smaller counter-example.) Appel and Haken used a special-purpose computer program to confirm that each of these maps had this property. Additionally, any map that could potentially be a counterexample must have a portion that looks like one of these 1,936 maps. Showing this required hundreds of pages of hand analysis. Appel and Haken concluded that no smallest counterexamples exist because any must contain, yet do not contain, one of these 1,936 maps. This contradiction means there are no counterexamples at all and that the theorem is therefore true. Initially, their proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand . Since then the proof has gained wider acceptance, although doubts remain .
To dispel remaining doubt about the Appel–Haken proof, a simpler proof using the same ideas and still relying on computers was published in 1997 by Robertson, Sanders, Seymour, and Thomas. Additionally, in 2005, the theorem was proven by Georges Gonthier with general purpose theorem proving software.
Precise formulation of the theorem.
The intuitive statement of the four color theorem, i.e. 'that given any separation of a plane into contiguous regions, called a map, the regions can be colored using at most four colors so that no two adjacent regions have the same color', needs to be interpreted appropriately to be correct.
First, all corners, points that belong to (technically, are in the closure of) three or more countries, must be ignored. In addition, bizarre maps (using regions of finite area but infinite perimeter) can require more than four colors.
Second, for the purpose of the theorem, every "country" has to be a connected region, or contiguous. In the real world, this is not true (e.g. the Upper and Lower Peninsula of Michigan, Nakhchivan as part of Azerbaijan, and Kaliningrad as part of Russia are not contiguous). Because all the territory of a particular country must be the same color, four colors may not be sufficient. For instance, consider a simplified map:
In this map, the two regions labeled "A" belong to the same country, and must be the same color. This map then requires five colors, since the two "A" regions together are contiguous with four other regions, each of which is contiguous with all the others. A similar construction also applies if a single color is used for all bodies of water, as is usual on real maps. For maps in which more than one country may have multiple disconnected regions, six or more colors might be required.
A simpler statement of the theorem uses graph theory. The set of regions of a map can be represented more abstractly as an undirected graph that has a vertex for each region and an edge for every pair of regions that share a boundary segment. This graph is planar (it is important to note that we are talking about the graphs that have some limitations according to the map they are transformed from only): it can be drawn in the plane without crossings by placing each vertex at an arbitrarily chosen location within the region to which it corresponds, and by drawing the edges as curves that lead without crossing within each region from the vertex location to each shared boundary point of the region. Conversely any planar graph can be formed from a map in this way. In graph-theoretic terminology, the four-color theorem states that the vertices of every planar graph can be colored with at most four colors so that no two adjacent vertices receive the same color, or for short, "every planar graph is four-colorable" (; ).
History.
Early proof attempts.
Möbius mentioned the problem in his lectures as early as 1840. The conjecture was first proposed on October 23, 1852 when Francis Guthrie, while trying to color the map of counties of England, noticed that only four different colors were needed. At the time, Guthrie's brother, Frederick, was a student of Augustus De Morgan (the former advisor of Francis) at University College London. Francis inquired with Frederick regarding it, who then took it to De Morgan (Francis Guthrie graduated later in 1852, and later became a professor of mathematics in South Africa). According to De Morgan:
"A student of mine asked me to day to give him a reason for a fact which I did not know was a fact—and do not yet. He says that if a figure be any how divided and the compartments differently colored so that figures with any portion of common boundary "line" are differently colored—four colors may be wanted but not more—the following is his case in which four colors "are" wanted. Query cannot a necessity for five or more be invented…" 
"F.G.", perhaps one of the two Guthries, published the question in "The Athenaeum" in 1854, and De Morgan posed the question again in the same magazine in 1860. Another early published reference by in turn credits the conjecture to De Morgan.
There were several early failed attempts at proving the theorem. De Morgan believed that it followed from a simple fact about four regions, though he didn't believe that fact could be derived from more elementary facts.
This arises in the following way. We never need four colors in a neighborhood unless there be four counties, each of which has boundary lines in common with each of the other three. Such a thing cannot happen with four areas unless one or more of them be inclosed by the rest; and the color used for the inclosed county is thus set free to go on with. Now this principle, that four areas cannot each have common boundary with all the other three without inclosure, is not, we fully believe, capable of demonstration upon anything more evident and more elementary; it must stand as a postulate.
One alleged proof was given by Alfred Kempe in 1879, which was widely acclaimed; another was given by Peter Guthrie Tait in 1880. It was not until 1890 that Kempe's proof was shown incorrect by Percy Heawood, and in 1891 Tait's proof was shown incorrect by Julius Petersen—each false proof stood unchallenged for 11 years .
In 1890, in addition to exposing the flaw in Kempe's proof, Heawood proved the five color theorem and generalized the four color conjecture to surfaces of arbitrary genus—see below.
Tait, in 1880, showed that the four color theorem is equivalent to the statement that a certain type of graph (called a snark in modern terminology) must be non-planar.
In 1943, Hugo Hadwiger formulated the Hadwiger conjecture , a far-reaching generalization of the four-color problem that still remains unsolved.
Proof by computer.
During the 1960s and 1970s German mathematician Heinrich Heesch developed methods of using computers to search for a proof. Notably he was the first to use discharging for proving the theorem, which turned out to be important in the unavoidability portion of the subsequent Appel-Haken proof. He also expanded on the concept of reducibility and, along with Ken Durre, developed a computer test for it. Unfortunately, at this critical juncture, he was unable to procure the necessary supercomputer time to continue his work .
Others took up his methods and his computer-assisted approach. While other teams of mathematicians were racing to complete proofs, Kenneth Appel and Wolfgang Haken at the University of Illinois announced, on June 21, 1976, that they had proven the theorem. They were assisted in some algorithmic work by John A. Koch .
If the four-color conjecture were false, there would be at least one map with the smallest possible number of regions that requires five colors. The proof showed that such a minimal counterexample cannot exist, through the use of two technical concepts (; ; ):
Using mathematical rules and procedures based on properties of reducible configurations, Appel and Haken found an unavoidable set of reducible configurations, thus proving that a minimal counterexample to the four-color conjecture could not exist. Their proof reduced the infinitude of possible maps to 1,936 reducible configurations (later reduced to 1,476) which had to be checked one by one by computer and took over a thousand hours. This reducibility part of the work was independently double checked with different programs and computers. However, the unavoidability part of the proof was verified in over 400 pages of microfiche, which had to be checked by hand .
Appel and Haken's announcement was widely reported by the news media around the world, and the math department at the University of Illinois used a postmark stating "Four colors suffice." At the same time the unusual nature of the proof—it was the first major theorem to be proven with extensive computer assistance—and the complexity of the human-verifiable portion, aroused considerable controversy .
In the early 1980s, rumors spread of a flaw in the Appel-Haken proof. Ulrich Schmidt at RWTH Aachen examined Appel and Haken's proof for his master's thesis . He had checked about 40% of the unavoidability portion and found a significant error in the discharging procedure . In 1986, Appel and Haken were asked by the editor of "Mathematical Intelligencer" to write an article addressing the rumors of flaws in their proof. They responded that the rumors were due to a "misinterpretation of [Schmidt's] results" and obliged with a detailed article . Their magnum opus, "Every Planar Map is Four-Colorable", a book claiming a complete and detailed proof (with a microfiche supplement of over 400 pages), appeared in 1989 and explained Schmidt's discovery and several further errors found by others .
Simplification and verification.
Since the proving of the theorem, efficient algorithms have been found for 4-coloring maps requiring only O("n"2) time, where "n" is the number of vertices. In 1996, Neil Robertson, Daniel P. Sanders, Paul Seymour, and Robin Thomas created a quadratic-time algorithm, improving on a quartic-time algorithm based on Appel and Haken’s proof (; ). This new proof is similar to Appel and Haken's but more efficient because it reduces the complexity of the problem and requires checking only 633 reducible configurations. Both the unavoidability and reducibility parts of this new proof must be executed by computer and are impractical to check by hand . In 2001, the same authors announced an alternative proof, by proving the snark theorem (; ).
In 2005, Benjamin Werner and Georges Gonthier formalized a proof of the theorem inside the Coq proof assistant. This removed the need to trust the various computer programs used to verify particular cases; it is only necessary to trust the Coq kernel .
Summary of proof ideas.
The following discussion is a summary based on the introduction to Appel and Haken's book "Every Planar Map is Four Colorable" . Although flawed, Kempe's original purported proof of the four color theorem provided some of the basic tools later used to prove it. The explanation here is reworded in terms of the modern graph theory formulation above.
Kempe's argument goes as follows. First, if planar regions separated by the graph are not "triangulated", i.e. do not have exactly three edges in their boundaries, we can add edges without introducing new vertices in order to make every region triangular, including the unbounded outer region. If this triangulated graph is colorable using four colors or fewer, so is the original graph since the same coloring is valid if edges are removed. So it suffices to prove the four color theorem for triangulated graphs to prove it for all planar graphs, and without loss of generality we assume the graph is triangulated.
Suppose "v", "e", and "f" are the number of vertices, edges, and regions (faces). Since each region is triangular and each edge is shared by two regions, we have that 2"e" = 3"f". This together with Euler's formula, "v" − "e" + "f" = 2, can be used to show that 6"v" − 2"e" = 12. Now, the "degree" of a vertex is the number of edges abutting it. If "v""n" is the number of vertices of degree "n" and "D" is the maximum degree of any vertex,
But since 12 > 0 and 6 − "i" ≤ 0 for all "i" ≥ 6, this demonstrates that there is at least one vertex of degree 5 or less.
If there is a graph requiring 5 colors, then there is a "minimal" such graph, where removing any vertex makes it four-colorable. Call this graph "G". Then "G" cannot have a vertex of degree 3 or less, because if "d"("v") ≤ 3, we can remove "v" from "G", four-color the smaller graph, then add back "v" and extend the four-coloring to it by choosing a color different from its neighbors.
Kempe also showed correctly that "G" can have no vertex of degree 4. As before we remove the vertex "v" and four-color the remaining vertices. If all four neighbors of "v" are different colors, say red, green, blue, and yellow in clockwise order, we look for an alternating path of vertices colored red and blue joining the red and blue neighbors. Such a path is called a Kempe chain. There may be a Kempe chain joining the red and blue neighbors, and there may be a Kempe chain joining the green and yellow neighbors, but not both, since these two paths would necessarily intersect, and the vertex where they intersect cannot be colored. Suppose it is the red and blue neighbors that are not chained together. Explore all vertices attached to the red neighbor by red-blue alternating paths, and then reverse the colors red and blue on all these vertices. The result is still a valid four-coloring, and "v" can now be added back and colored red.
This leaves only the case where "G" has a vertex of degree 5; but Kempe's argument was flawed for this case. Heawood noticed Kempe's mistake and also observed that if one was satisfied with proving only five colors are needed, one could run through the above argument (changing only that the minimal counterexample requires 6 colors) and use Kempe chains in the degree 5 situation to prove the five color theorem.
In any case, to deal with this degree 5 vertex case requires a more complicated notion than removing a vertex. Rather the form of the argument is generalized to considering "configurations", which are connected subgraphs of "G" with the degree of each vertex (in G) specified. For example, the case described in degree 4 vertex situation is the configuration consisting of a single vertex labelled as having degree 4 in "G". As above, it suffices to demonstrate that if the configuration is removed and the remaining graph four-colored, then the coloring can be modified in such a way that when the configuration is re-added, the four-coloring can be extended to it as well. A configuration for which this is possible is called a "reducible configuration". If at least one of a set of configurations must occur somewhere in G, that set is called "unavoidable". The argument above began by giving an unavoidable set of five configurations (a single vertex with degree 1, a single vertex with degree 2, ..., a single vertex with degree 5) and then proceeded to show that the first 4 are reducible; to exhibit an unavoidable set of configurations where every configuration in the set is reducible would prove the theorem.
Because "G" is triangular, the degree of each vertex in a configuration is known, and all edges internal to the configuration are known, the number of vertices in "G" adjacent to a given configuration is fixed, and they are joined in a cycle. These vertices form the "ring" of the configuration; a configuration with "k" vertices in its ring is a "k"-ring configuration, and the configuration together with its ring is called the "ringed configuration". As in the simple cases above, one may enumerate all distinct four-colorings of the ring; any coloring that can be extended without modification to a coloring of the configuration is called "initially good". For example, the single-vertex configuration above with 3 or less neighbors were initially good. In general, the surrounding graph must be systematically recolored to turn the ring's coloring into a good one, as was done in the case above where there were 4 neighbors; for a general configuration with a larger ring, this requires more complex techniques. Because of the large number of distinct four-colorings of the ring, this is the primary step requiring computer assistance.
Finally, it remains to identify an unavoidable set of configurations amenable to reduction by this procedure. The primary method used to discover such a set is the method of discharging. The intuitive idea underlying discharging is to consider the planar graph as an electrical network. Initially positive and negative "electrical charge" is distributed amongst the vertices so that the total is positive.
Recall the formula above:
Each vertex is assigned an initial charge of 6-deg("v"). Then one "flows" the charge by systematically redistributing the charge from a vertex to its neighboring vertices according to a set of rules, the "discharging procedure". Since charge is preserved, some vertices still have positive charge. The rules restrict the possibilities for configurations of positively charged vertices, so enumerating all such possible configurations gives an unavoidable set.
As long as some member of the unavoidable set is not reducible, the discharging procedure is modified to eliminate it (while introducing other configurations). Appel and Haken's final discharging procedure was extremely complex and, together with a description of the resulting unavoidable configuration set, filled a 400-page volume, but the configurations it generated could be checked mechanically to be reducible. Verifying the volume describing the unavoidable configuration set itself was done by peer review over a period of several years.
A technical detail not discussed here but required to complete the proof is "immersion reducibility".
False disproofs.
The four color theorem has been notorious for attracting a large number of false proofs and disproofs in its long history. At first, "The New York Times" refused as a matter of policy to report on the Appel–Haken proof, fearing that the proof would be shown false like the ones before it . Some alleged proofs, like Kempe's and Tait's mentioned above, stood under public scrutiny for over a decade before they were exposed. But many more, authored by amateurs, were never published at all.
Generally, the simplest, though invalid, counterexamples attempt to create one region which touches all other regions. This forces the remaining regions to be colored with only three colors. Because the four color theorem is true, this is always possible; however, because the person drawing the map is focused on the one large region, they fail to notice that the remaining regions can in fact be colored with three colors.
This trick can be generalized: there are many maps where if the colors of some regions are selected beforehand, it becomes impossible to color the remaining regions without exceeding four colors. A casual verifier of the counterexample may not think to change the colors of these regions, so that the counterexample will appear as though it is valid.
Perhaps one effect underlying this common misconception is the fact that the color restriction is not transitive: a region only has to be colored differently from regions it touches directly, not regions touching regions that it touches. If this were the restriction, planar graphs would require arbitrarily large numbers of colors.
Other false disproofs violate the assumptions of the theorem in unexpected ways, such as using a region that consists of multiple disconnected parts, or disallowing regions of the same color from touching at a point.
Three-coloring.
While every planar map can be colored with four colors, it is NP-complete in complexity to decide whether an arbitrary planar map can be colored with just three colors.
Generalizations.
The four-color theorem applies not only to finite planar graphs, but also to infinite graphs that can be drawn without crossings in the plane, and even more generally to infinite graphs (possibly with an uncountable number of vertices) for which every finite subgraph is planar. To prove this, one can combine a proof of the theorem for finite planar graphs with the De Bruijn–Erdős theorem stating that, if every finite subgraph of an infinite graph is "k"-colorable, then the whole graph is also "k"-colorable . This can also be seen as an immediate consequence of Kurt Gödel's compactness theorem for first-order logic, simply by expressing the colorability of an infinite graph with a set of logical formulae.
One can also consider the coloring problem on surfaces other than the plane (Weisstein). The problem on the sphere or cylinder is equivalent to that on the plane. For closed (orientable or non-orientable) surfaces with positive genus, the maximum number "p" of colors needed depends on the surface's Euler characteristic χ according to the formula
where the outermost brackets denote the floor function.
Alternatively, for an orientable surface the formula can be given in terms of the genus of a surface, "g":
This formula, the Heawood conjecture, was conjectured by P.J. Heawood in 1890 and proven by Gerhard Ringel and J. W. T. Youngs in 1968. The only exception to the formula is the Klein bottle, which has Euler characteristic 0 (hence the formula gives p = 7) and requires 6 colors, as shown by P. Franklin in 1934 (Weisstein).
For example, the torus has Euler characteristic χ = 0 (and genus "g" = 1) and thus "p" = 7, so no more than 7 colors are required to color any map on a torus. This upper bound of 7 is sharp: certain toroidal polyhedra such as the Szilassi polyhedron require seven colors.
A Möbius strip requires six colors as do 1-planar graphs (graphs drawn with at most one simple crossing per edge) . If both the vertices and the faces of a planar graph are colored, in such a way that no two adjacent vertices, faces, or vertex-face pair have the same color, then again at most six colors are needed .
There is no obvious extension of the coloring result to three-dimensional solid regions. By using a set of "n" flexible rods, one can arrange that every rod touches every other rod. The set would then require "n" colors, or "n"+1 if you consider the empty space that also touches every rod. The number "n" can be taken to be any integer, as large as desired. Such examples were known to Fredrick Guthrie in 1880 . Even for axis-parallel cuboids (considered to be adjacent when two cuboids share a two-dimensional boundary area) an unbounded number of colors may be necessary (; ).

</doc>
<doc id="10951" url="https://en.wikipedia.org/wiki?curid=10951" title="Fahrenheit 451">
Fahrenheit 451

Fahrenheit 451 is a dystopian novel by Ray Bradbury published in 1953. It is regarded as one of his best works. The novel presents a future American society where books are outlawed and "firemen" burn any that are found. The title refers to the temperature that Bradbury asserted to be the autoignition temperature of paper. (In reality, scientists place the autoignition temperature of paper anywhere from high 440 degrees Fahrenheit to some 30 degrees hotter, depending on the study and type of paper.)
The novel has been the subject of interpretations focusing on the historical role of book burning in suppressing dissenting ideas. In a 1956 radio interview, Bradbury stated that he wrote "Fahrenheit 451" because of his concerns at the time (during the McCarthy era) about the threat of book burning in the United States. In later years, he stated his motivation for writing the book in more general terms.
In 1954, "Fahrenheit 451" won the American Academy of Arts and Letters Award in Literature and the Commonwealth Club of California Gold Medal. It has since won the Prometheus "Hall of Fame" Award in 1984 and a 1954 "Retro" Hugo Award, one of only four Best Novel Retro Hugos ever given, in 2004. Bradbury was honored with a Spoken Word Grammy nomination for his 1976 audiobook version.
Adaptations include François Truffaut's film adaptation of the novel in 1966, and a BBC Radio dramatization was produced in 1982. Bradbury published a stage play version in 1979 and helped develop a 1984 interactive fiction computer game titled "Fahrenheit 451", released in 2010 with a collection of his short stories, "A Pleasure to Burn".
On December 21, 2015, the Internet Engineering Steering Group approved the publication of "An HTTP Status Code to Report Legal Obstacles", which specifies that websites forced to block resources for legal reasons should return a status code of 451 when users request those resources.
Plot summary.
"Fahrenheit 451" is set in an unspecified city (likely in the American Mid-West) at an unspecified time in the future after the year 1960.
The novel is divided into three parts: "The Hearth and the Salamander", "The Sieve and the Sand", and "Burning Bright".
"The Hearth and the Salamander".
Guy Montag is a "fireman" hired to burn the possessions of those who read outlawed books. One fall night while returning from work, he meets his new neighbor: a teenage girl named Clarisse McClellan, whose free-thinking ideals and liberating spirit cause him to question his life and his own perceived happiness. Montag returns home to find that his wife Mildred has overdosed on sleeping pills, and calls for medical attention. Mildred survives with no memory of what happened. Over the next days, Clarisse faithfully meets Montag as he walks home. She tells him about how her interests have made her an outcast at school. Montag looks forward to these meetings, and just as he begins to expect them, Clarisse goes absent. He senses something is wrong.
In the following days, while at work with the other firemen ransacking the book-filled house of an old woman before the inevitable burning, Montag steals a book before any of his coworkers notice. The woman refuses to leave her house and her books, choosing instead to light a match and burn herself alive. Montag returns home jarred by the woman's suicide. While getting ready for bed, he hides the stolen book under his pillow. Still shaken by the night's events, he attempts to make conversation with Mildred, conversation that only causes him to realize how little he knows her and how little they have in common. Montag asks his wife if she has seen Clarisse recently. Mildred mutters that she believes Clarisse died after getting struck by a speeding car and that her family has moved away. Dismayed by her failure to mention this, Montag uneasily tries to fall asleep. Outside he suspects the presence of "The Hound", an eight-legged robotic dog-like creature that resides in the firehouse and aids the firemen.
Montag awakens ill the next morning and stays home from work. He relates the story of the burned woman to an apathetic Mildred and mentions perhaps quitting his work. The possibility of becoming destitute over the loss of income provokes a strong reaction from her and she explains that the woman herself is to blame because she had books.
Captain Beatty, Montag's fire chief, personally visits Montag to see how he is doing. Sensing Montag's concerns, Beatty recounts how books lost their value and where the firemen fit in: Over the course of several decades, people embraced new media, sports, and a quickening pace of life. Books were ruthlessly abridged or degraded to accommodate a short attention span while minority groups protested over the controversial, outdated content perceived to be found in books. The government took advantage of this and the firemen were soon hired to burn books in the name of public happiness. Beatty adds casually that all firemen eventually steal a book out of curiosity; if the book is burned within 24 hours, the fireman and his family will not get in trouble.
After Beatty has left, Montag reveals to Mildred that over the last year he has accumulated a stash of books that he has kept hidden in their air-conditioning duct. In a panic, Mildred grabs a book and rushes to throw it in their kitchen incinerator; Montag subdues her and tells her that the two of them are going to read the books to see if they have value. If they do not, he promises the books will be burned and all will return to normal.
"The Sieve and the Sand".
While Montag and Mildred are perusing the stolen books, a sniffing occurs at their front door. Montag recognizes it as The Hound while Mildred passes it off as a random dog. They resume their discussion once the sound ceases. Montag laments Mildred's suicide attempt, the woman who burned herself, and the constant din of bombers flying over their house taking part in a looming war neither he, nor anybody else, knows much about. He states that maybe the books of the past have messages that can save society from its own destruction. The conversation is interrupted by a call from Mildred's friend Ann Bowles, and they set up a date to watch the "parlor walls" (large televisions lining the walls of her living room) that night at Mildred's house.
Montag meanwhile concedes that they will need help to understand the books. Montag remembers an old man named Faber he once met in a park a year ago, an English professor before books were banned. He telephones Faber with questions about books and Faber soon hangs up on him. Undeterred, Montag makes a subway trip to Faber's home along with a rare copy of the Bible, the book he stole at the woman's house. Montag forces the scared and reluctant Faber into helping him by methodically ripping pages from the Bible. Faber concedes and gives Montag a homemade ear-piece communicator so he can offer constant guidance.
After Montag returns home, Mildred's friends, Mrs. Bowles and Clara Phelps, arrive to watch the parlor walls. Not interested in the insipid entertainment they are watching, Montag turns off the walls and tries to engage the women in meaningful conversation, only to find them indifferent to all but the most trivial aspects of the upcoming war, friend's deaths, their families, and politics. Montag leaves momentarily and returns with a book of poetry. This confuses the women and alarms Faber who is listening remotely. He proceeds to recite the poem "Dover Beach", causing Mrs. Phelps to cry. At the behest of Faber in the ear-piece, Montag burns the book. Mildred's friends leave in disgust while Mildred locks herself in the bathroom and takes more sleeping pills.
In the aftermath of the parlor party, Montag hides his books in his backyard before returning to the firehouse late at night with just the stolen Bible. He finds Beatty playing cards with the other firemen. Montag hands him the book, which is unceremoniously tossed into the trash. Beatty tells Montag that he had a dream in which they fought endlessly by quoting books to each other. In describing the dream Beatty reveals that, despite his disillusionment, he was once an enthusiastic reader. A fire alarm sounds and Beatty picks up the address from the dispatcher system. They drive in the firetruck recklessly to the destination. Montag is stunned when the truck arrives at his house.
"Burning Bright".
Beatty orders Montag to destroy his own house, telling him that his wife and her friends were the ones who reported him. Montag tries to talk to Mildred as she quickly leaves the house. Mildred ignores him, gets inside a taxi, and vanishes down the street. Montag obeys the chief, destroying the home piece by piece with a flamethrower. As soon as he has incinerated the house, Beatty discovers Montag's ear-piece and plans to hunt down Faber. Montag threatens Beatty with the flamethrower and (after Beatty taunts him) burns his boss alive, and knocks his coworkers unconscious. As Montag escapes the scene, the firehouse's mechanical dog attacks him, managing to inject his leg with a tranquilizer. He destroys it with the flamethrower and limps away.
Montag runs through the city streets towards Faber's house. Faber urges him to make his way to the countryside and contact the exiled book-lovers who live there. He mentions he will be leaving on an early bus heading to St. Louis and that he and Montag can rendezvous there later. On Faber's television, they watch news reports of another mechanical hound being released, with news helicopters following it to create a public spectacle. Montag leaves Faber's house. After an extended manhunt, he escapes by wading into a river and floating downstream.
Montag leaves the river in the countryside, where he meets the exiled drifters, led by a man named Granger. They have each memorized books for an upcoming time when society is ready to rediscover them. While learning the philosophy of the exiles, Montag and the group watch helplessly as bombers fly overhead and attack the city with nuclear weapons, completely annihilating it. While Faber would have left on the early bus, Mildred along with everyone else in the city was surely killed. Montag and the group are injured and dirtied, but manage to survive the shock wave.
In the morning after, Granger teaches Montag and the others about the legendary phoenix and its endless cycle of long life, death in flames, and rebirth. He adds that the phoenix must have some relation to mankind, which constantly repeats its mistakes. Granger emphasizes that man has something the phoenix does not: mankind can remember the mistakes it made from before it destroyed itself, and try to not make them again. Granger then muses that a large factory of mirrors should be built, so that mankind can take a long look at itself. When the meal is over, the band goes back toward the city, to help rebuild society.
Historical context.
Bradbury's lifelong passion with books began at an early age. As a frequent visitor to his local libraries in the 1920s and 1930s, he recalls being disappointed because they did not stock popular science fiction novels, like those of H. G. Wells, because, at the time, they were not deemed literary enough. Between this and learning about the destruction of the Library of Alexandria, a great impression was made on the young man about the vulnerability of books to censure and destruction. Later as a teenager, Bradbury was horrified by the Nazi book burnings and later Joseph Stalin's campaign of political repression, the "Great Purge", in which writers and poets, among many others, were arrested and often executed.
After the 1945 conclusion of World War II shortly after the atomic bombings of Hiroshima and Nagasaki, the United States focused its concern on the Soviet atomic bomb project and the expansion of communism. The House Un-American Activities Committee (HUAC)—formed in 1938 to investigate American citizens and organizations suspected of having communist ties—held hearings in 1947 to investigate alleged communist influence in Hollywood movie-making. These hearings resulted in the blacklisting of the so-called "Hollywood Ten", a group of influential screenwriters and directors. This governmental interference in the affairs of artists and creative types greatly angered Bradbury. Bitter and concerned about the workings of his government, a late 1949 nighttime encounter with an overzealous police officer would inspire Bradbury to write "The Pedestrian", a short story which would go on to become "The Fireman" and then "Fahrenheit 451". The rise of Senator Joseph McCarthy's hearings hostile to accused communists starting in 1950, would only deepen Bradbury's contempt over government overreach.
The same year HUAC began investigating Hollywood is often considered the beginning of the Cold War, as in March 1947, the Truman Doctrine was announced. By about 1950, the Cold War was in full swing and the American public's fear of atomic warfare and communist influence was at a feverish level. The stage was set for Bradbury to write the dramatic nuclear holocaust ending of "Fahrenheit 451", exemplifying the type of scenario feared by many Americans of the time.
Bradbury's early life witnessed the Golden Age of Radio while the transition to the Golden Age of Television began right around the time he started to work on the stories that would eventually lead to "Fahrenheit 451". Bradbury saw these forms of media as a threat to the reading of books, indeed as a threat to society, as he believed they could act as a distraction from important affairs. This contempt for mass media and technology would express itself through Mildred and her friends and is an important theme in the book.
Writing and development.
"Fahrenheit 451" developed out of a series of ideas Bradbury had visited in previously written stories. For many years, he tended to single out "The Pedestrian" in interviews and lectures as sort of a proto-"Fahrenheit 451". In the Preface of his 2006 anthology "Match to Flame: The Fictional Paths to Fahrenheit 451" he states that this is an oversimplification. The full genealogy of "Fahrenheit 451" given in "Match to Flame" is involved. The following covers the most salient aspects.
Between 1947 and 1948, Bradbury wrote the short story "Bright Phoenix" (not published until the May 1963 issue of "The Magazine of Fantasy & Science Fiction") about a librarian who confronts a book-burning "Chief Censor" named Jonathan Barnes. Barnes is a clear foreshadow of the ominous Captain Beatty of "Fahrenheit 451".
In late 1949, Bradbury was stopped and questioned by a police officer while walking late one night. When asked "What are you doing?", Bradbury wisecracked, "Putting one foot in front of another." This incident inspired Bradbury to write the 1951 short story "The Pedestrian". In "The Pedestrian", Leonard Mead is harassed and detained by the city's remotely operated police cruiser (there's only one) for taking nighttime walks, something that has become extremely rare in this future-based setting: everybody else stays inside and watches television ("viewing screens"). Alone and without an alibi, Mead is taken to the "Psychiatric Center for Research on Regressive Tendencies" for his peculiar habit. "Fahrenheit 451" would later echo this theme of an authoritarian society distracted by broadcast media.
Bradbury expanded the book-burning premise of "Bright Phoenix" and the totalitarian future of "The Pedestrian" into "The Fireman", a novella published in the February 1951 issue of "Galaxy Science Fiction". "The Fireman" was written in the basement of UCLA's Powell Library on a typewriter that he rented for a fee of ten cents per half hour. The first draft was 25,000 words long and was completed in nine days.
Urged by a publisher at Ballantine Books to double the length of his story to make a novel, Bradbury returned to the same typing room and expanded his work into "Fahrenheit 451", taking just nine days. The completed book was published by Ballantine in 1953.
Supplementary material.
Bradbury has supplemented the novel with various front and back matter, including a 1979 coda, a 1982 afterword, a 1993 foreword, and several introductions. In these he provides some commentary on the themes of the novel, thoughts on the movie adaptation, and numerous personal anecdotes related to the writing and development.
Publication history.
The first U.S. printing was a paperback version from October 1953 by The Ballantine Publishing Group. Shortly after the paperback, a hardback version was released that included a special edition of 200 signed and numbered copies bound in asbestos. These were technically collections because the novel was published with two short stories: "The Playground" and "And the Rock Cried Out", which have been absent in later printings. A few months later, the novel was serialized in the March, April, and May 1954 issues of nascent "Playboy" magazine.
Expurgation.
Starting in January 1967, "Fahrenheit 451" was subject to expurgation by its publisher, Ballantine Books with the release of the "Bal-Hi Edition" aimed at high school students. Among the changes made by the publisher were the censorship of the words "hell", "damn", and "abortion"; the modification of seventy-five passages; and the changing of two episodes. In the one case, a drunk man became a "sick man" while cleaning fluff out of a human navel became "cleaning ears" in the other. For a while both the censored and uncensored versions were available concurrently but by 1973 Ballantine was publishing only the censored version. This continued until 1979 when it came to Bradbury's attention:
In 1979, one of Bradbury's friends showed him an expurgated copy. Bradbury demanded that Ballantine Books withdraw that version and replace it with the original, and in 1980 the original version once again became available. In this reinstated work, in the Author's Afterword, Bradbury relates to the reader that it is not uncommon for a publisher to expurgate an author's work, but he asserts that he himself will not tolerate the practice of manuscript "mutilation".
The "Bal-Hi" editions are now referred to by the publisher as the "Revised Bal-Hi" editions.
Non-print publications.
An audiobook version read by Bradbury himself was released in 1976 and received a Spoken Word Grammy nomination. Another audiobook was released in 2005 narrated by Christopher Hurt. The e-book version was released in December 2011.
Reception.
In 1954, "Galaxy Science Fiction" reviewer Groff Conklin placed the novel "among the great works of the imagination written in English in the last decade or more." The "Chicago Sunday Tribune"'s August Derleth described the book as "a savage and shockingly savage prophetic view of one possible future way of life," calling it "compelling" and praising Bradbury for his "brilliant imagination". Over half a century later, Sam Weller wrote, "upon its publication, "Fahrenheit 451" was hailed as a visionary work of social commentary." Today, "Fahrenheit 451" is still viewed as an important cautionary tale against conformity and book burning.
When the book was first published there were those who did not find merit in the tale. Anthony Boucher and J. Francis McComas were less enthusiastic, faulting the book for being "simply padded, occasionally with startlingly ingenious gimmickry, ... often with coruscating cascades of verbal brilliance too often merely with words." Reviewing the book for "Astounding Science Fiction", P. Schuyler Miller characterized the title piece as "one of Bradbury's bitter, almost hysterical diatribes," and praised its "emotional drive and compelling, nagging detail." Similarly, "The New York Times" was unimpressed with the novel and further accused Bradbury of developing a "virulent hatred for many aspects of present-day culture, namely, such monstrosities as radio, TV, most movies, amateur and professional sports, automobiles, and other similar aberrations which he feels debase the bright simplicity of the thinking man's existence."
Censorship/banning incidents.
In the years since its publication, "Fahrenheit 451" has occasionally been banned, censored, or redacted in some schools by parents and teaching staff either unaware of or indifferent to the inherent irony of such censorship. The following are some notable incidents:
Themes.
Discussions about "Fahrenheit 451" often center on its story foremost as a warning against state-based censorship. Indeed, when Bradbury wrote the novel during the McCarthy era, he was concerned about censorship in the United States. During a radio interview in 1956, Bradbury said:
I wrote this book at a time when I was worried about the way things were going in this country four years ago. Too many people were afraid of their shadows; there was a threat of book burning. Many of the books were being taken off the shelves at that time. And of course, things have changed a lot in four years. Things are going back in a very healthy direction. But at the time I wanted to do some sort of story where I could comment on what would happen to a country if we let ourselves go too far in this direction, where then all thinking stops, and the dragon swallows his tail, and we sort of vanish into a limbo and we destroy ourselves by this sort of action.
As time went by, Bradbury tended to dismiss censorship as a chief motivating factor for writing the story. Instead he usually claimed that the real messages of "Fahrenheit 451" were about the dangers of an illiterate society infatuated with mass media and the threat of minority and special interest groups to books. In the late 1950s, Bradbury recounted:
In writing the short novel "Fahrenheit 451", I thought I was describing a world that might evolve in four or five decades. But only a few weeks ago, in Beverly Hills one night, a husband and wife passed me, walking their dog. I stood staring after them, absolutely stunned. The woman held in one hand a small cigarette-package-sized radio, its antenna quivering. From this sprang tiny copper wires which ended in a dainty cone plugged into her right ear. There she was, oblivious to man and dog, listening to far winds and whispers and soap-opera cries, sleep-walking, helped up and down curbs by a husband who might just as well not have been there. This was "not" fiction.
This story echoes Mildred's "Seashell ear-thimbles" (i.e., a brand of in-ear headphones) that act as an emotional barrier between her and Montag. In a 2007 interview, Bradbury maintained that people misinterpret his book and that "Fahrenheit 451" is really a statement on how mass media like television marginalizes the reading of literature. Regarding minorities, he wrote in his 1979 Coda:
There is more than one way to burn a book. And the world is full of people running about with lit matches. Every minority, be it Baptist/Unitarian, Irish/Italian/Octogenarian/Zen Buddhist, Zionist/Seventh-day Adventist, Women's Lib/Republican, Mattachine/Four Square Gospel feels it has the will, the right, the duty to douse the kerosene, light the fuse. [...] Fire-Captain Beatty, in my novel "Fahrenheit 451", described how the books were burned first by minorities, each ripping a page or a paragraph from this book, then that, until the day came when the books were empty and the minds shut and the libraries closed forever. [...] Only six weeks ago, I discovered that, over the years, some cubby-hole editors at Ballantine Books, fearful of contaminating the young, had, bit by bit, censored some seventy-five separate sections from the novel. Students, reading the novel, which, after all, deals with censorship and book-burning in the future, wrote to tell me of this exquisite irony. Judy-Lynn del Rey, one of the new Ballantine editors, is having the entire book reset and republished this summer with all the damns and hells back in place.
Book-burning censorship, Bradbury would argue, was a side-effect of the these two primary factors; this is consistent with Captain Beatty's speech to Montag about the history of the firemen. According to Bradbury, it is the people, not the state, who are the culprit in "Fahrenheit 451". Nevertheless, the role on censorship, state-based or otherwise, is still perhaps the most frequent theme explored in the work.
A variety of other themes in the novel besides censorship have been suggested. Two major themes are resistance to conformity and control of individuals via technology and mass media. Bradbury explores how the government is able to use mass media to influence society and suppress individualism through book burning. The characters Beatty and Faber point out the American population is to blame. Due to their constant desire for a simplistic, positive image, books must be suppressed. Beatty blames the minority groups, who would take offense to published works that displayed them in an unfavorable light. Faber went further to state that the American population simply stopped reading on their own. He notes that the book burnings themselves became a form of entertainment to the general public.
Predictions for the future.
Bradbury described himself as "a "preventor" of futures, not a predictor of them." He did not believe that book burning was an inevitable part of our future; he wanted to warn against its development. In a later interview, when asked if he believes that teaching "Fahrenheit 451" in schools will prevent his totalitarian vision of the future, Bradbury replied in the negative. Rather, he states that education must be at the kindergarten and first-grade level. If students are unable to read then, they will be unable to read "Fahrenheit 451".
In terms of technology, Sam Weller notes that Bradbury "predicted everything from flat-panel televisions to iPod earbuds and twenty-four-hour banking machines."
Adaptations.
"Playhouse 90" broadcast "A Sound of Different Drummers" on CBS in 1957, written by Robert Alan Aurthur. The play combined plot ideas from "Fahrenheit 451" and "Nineteen Eighty-Four". Bradbury sued and eventually won on appeal.
A film adaptation written and directed by François Truffaut and starring Oskar Werner and Julie Christie was released in 1966.
BBC Radio produced a one-off dramatization of the novel in 1982 starring Michael Pennington. It was broadcast again on February 12, 2012, and April 7 and 8, 2013, on BBC Radio 4 Extra.
In 1984, the novel was adapted into a computer text adventure game of the same name by the software company Trillium.
In 2006, the Drama Desk Award winning Godlight Theatre Company produced and performed the New York City premiere of Ray Bradbury's "Fahrenheit 451" at 59E59 Theaters. After the completion of the New York run, the production then transferred to the Edinburgh Festival where it was a 2006 Edinburgh Festival "Pick of the Fringe".
The Off-Broadway theatre The American Place Theatre presented a one man show adaptation of "Fahrenheit 451" as a part of their 2008–2009 Literature to Life season.
In June 2009, a graphic novel edition of the book was published. Entitled "Ray Bradbury's Fahrenheit 451: The Authorized Adaptation", the paperback graphic adaptation was illustrated by Tim Hamilton. The introduction in the novel is written by Bradbury.
"Fahrenheit 451" inspired the Birmingham Repertory Theatre production "Time Has Fallen Asleep in the Afternoon Sunshine", which was performed at the Birmingham Central Library in April 2012.
A new film adaptation is in development for HBO and is to be directed by Ramin Bahrani.

</doc>
<doc id="10957" url="https://en.wikipedia.org/wiki?curid=10957" title="Francis Xavier">
Francis Xavier

Saint Francis Xavier, S.J., born Francisco de Jasso y Azpilicueta (7 April 15063 December 1552), was a Navarrese Basque Roman Catholic missionary, born in Xavier, Kingdom of Navarre (now part of Spain), and a co-founder of the Society of Jesus. He was a companion of St. Ignatius of Loyola and one of the first seven Jesuits who took vows of poverty and chastity at Montmartre, Paris in 1534. He led an extensive mission into Asia, mainly in the Portuguese Empire of the time and was influential in evangelization work, most notably in India. He also was the first Christian missionary to venture into Japan, Borneo, the Maluku Islands, and other areas. In those areas, struggling to learn the local languages and in the face of opposition, he had less success than he had enjoyed in India. Xavier was about to extend his missionary preaching to China but died in Shangchuan Island shortly before he could do so.
He was beatified by Pope Paul V on 25 October 1619 and canonized by Pope Gregory XV on 12 March 1622. In 1624 he was made co-patron of Navarre alongside Santiago. Known as the "Apostle of the Indies," and the "Apostle of Japan", he is considered to be one of the greatest missionaries since St. Paul. In 1927, Pope Pius XI published the decree "Apostolicorum in Missionibus" naming St. Francis Xavier, along with St. Thérèse of Lisieux, co-patron of all foreign missions. He is now co-patron saint of Navarre with San Fermin. The Day of Navarre (Día de Navarra) in Spain marks the anniversary of Saint Francis Xavier's death, on 3 December 1552.
Early life.
Francis Xavier was born in the royal castle of Xavier, in the Kingdom of Navarre, on 7 April 1506 according to a family register. He was the youngest son of Juan de Jasso y Atondo, seneschal of Xavier castle, who belonged to a prosperous farming family and had acquired a doctorate in law at the University of Bologna, and later became privy counselor and finance minister to King John III of Navarre (Jean d'Albret). Francis' mother was Doña María de Azpilcueta y Aznárez, sole heiress of two noble Navarrese families. He was thus related to the great theologian and philosopher Martín de Azpilcueta. Notwithstanding different interpretations on his first language, no evidence suggests that Xavier's mother tongue was other than Basque, as stated by himself, and confirmed by the sociolinguistic environment of the time.
In 1512, Ferdinand, King of Aragon and regent of Castile, invaded Navarre, initiating a war that lasted over 18 years. Three years later, Francis' father died when Francis was only nine years old. In 1516, Francis's brothers participated in a failed Navarrese-French attempt to expel the Spanish invaders from the kingdom. The Spanish Governor, Cardinal Cisneros, confiscated the family lands, demolished the outer wall, the gates, and two towers of the family castle, and filled in the moat. In addition, the height of the keep was reduced by half. Only the family residence inside the castle was left. In 1522 one of Francis's brothers participated with 200 Navarrese nobles in dogged but failed resistance against the Castilian Count of Miranda in Amaiur, Baztan, the last Navarrese territorial position south of the Pyrenees.
Until he left for studies in Paris in 1525, Francis' life was surrounded by this war, which ended with Spanish conquest of Navarre in 1530.
In 1525, Francis went to study at the Collège Sainte-Barbe, University of Paris, where he would spend the next eleven years. In the early days he acquired some reputation as an athlete and a fine high-jumper.
In 1529, Francis shared lodgings with his friend Pierre Favre. A new student, Ignatius of Loyola, came to room with them. At 38, Ignatius was much older than Peter and Francis, who were both 23 at the time. Pierre was won over by Ignatius to become a priest, but Francis had aspirations of worldly advancement. At first Francis was not much taken with Ignatius. He regarded the new lodger as a joke and was sarcastic about his efforts to convert students. Only after Pierre left their lodgings to visit his family, when Ignatius was alone with the proud Navarrese, was he was able to slowly break down Francis's stubborn resistance. According to most biographies Ignatius is said to have posed the question: "What will it profit a man to gain the whole world, and lose his own soul?" However, according to James Broderick such method is not characteristic of Ignatius and there is no evidence that he employed it at all.
In 1530 Francis received the degree of Master of Arts, and afterwards taught Aristotelian philosophy at Beauvais College, University of Paris.
On 15 August 1534, seven students met in a crypt beneath the Church of Saint Denis (now Saint Pierre de Montmartre), in Montmartre outside Paris. They were Francis, Ignatius of Loyola, Alfonso Salmeron, Diego Laínez, Nicolás Bobadilla from Spain, Peter Faber from Savoy, and Simão Rodrigues from Portugal. They made private vows of poverty, chastity, and obedience to the Pope, and also vowed to go to the Holy Land to convert infidels. Francis began his study of theology in 1534 and was ordained on June 24, 1537.
In 1539, after long discussions, Ignatius drew up a formula for a new monastic order, the Society of Jesus (the Jesuits). Ignatius's plan for the order was approved by Pope Paul III in 1540.
Missionary work.
In 1540 King John of Portugal had Pedro Mascarenhas, Portuguese ambassador to the Vatican, request Jesuit missionaries to spread the faith in his new Indian possessions, where the king believed that Christian values were eroding among the Portuguese. After successive appeals to the Pope asking for missionaries for the East Indies under the Padroado agreement, John III was encouraged by Diogo de Gouveia, rector of the Collège Sainte-Barbe, to recruit the newly graduated youngsters that would establish the Society of Jesus.
Loyola promptly appointed Nicholas Bobadilla and Simão Rodrigues. At the last moment, however, Bobadilla became seriously ill. With some hesitance and uneasiness, Ignatius asked Francis to go in Bobadilla's place. Thus, Xavier accidentally began his life as the first Jesuit missionary.
Leaving Rome on 15 March 1540, in the Ambassador's train, Francis took with him a breviary, a catechism, and by Croatian humanist Marko Marulić, a Latin book that had become popular in the Counter-Reformation. According to a 1549 letter of F. Balthasar Gago in Goa, it was the only book that Francis read or studied. Francis reached Lisbon in June 1540 and four days after his arrival, he and Rodrigues were summoned to a private audience with the King and the Queen.
Francis Xavier devoted much of his life to missions in Asia, mainly in four centers: Malacca, Amboina and Ternate, Japan, and China. His growing information about new places indicated to him that he had to go to what he understood were centers of influence for the whole region. China loomed large from his days in India. Japan was particularly attractive because of its culture. For him, these areas were interconnected; they could not be evangelized separately.
Goa and India.
He left Lisbon on 7 April 1541, Xavier's thirty-fifth birthday, along with two other Jesuits and the new viceroy Martim Afonso de Sousa, on board the "Santiago". As he departed, Francis was given a brief from the pope appointing him apostolic nuncio to the East. From August until March 1542 he remained in Portuguese Mozambique, and arrived in Goa, then capital of Portuguese India on 6 May 1542, thirteen months after leaving Lisbon.
Following quickly on the great voyages of discovery, the Portuguese had established themselves at Goa thirty years earlier. Francis' primary mission, as ordered by King John III, was to restore Christianity among the Portuguese settlers. According to Teotonio R. DeSouza, recent critical accounts indicate that apart from the posted civil servants, "the great majority of those who were dispatched as 'discoverers' were the riff-raff of Portuguese society, picked up from Portuguese jails." Nor did the soldiers, sailors, or merchants come to do missionary work, and Imperial policy permitted the outflow of disaffected nobility. Many of the arrivals formed liaisons with local women and adopted Indian culture. Missionaries often wrote against the "scandalous and undisciplined" behavior of their fellow Christians.
The Christian population had churches, clergy, and a bishop, but there were few preachers and no priests beyond the walls of Goa. Xavier decided that he must begin by instructing the Portuguese themselves, and gave much of his time to the teaching of children. The first five months he spent in preaching and ministering to the sick in the hospitals. After that, he walked through the streets ringing a bell to summon the children and servants to catechism. He was invited to head Saint Paul's College, a pioneer seminary for the education of secular priests, which became the first Jesuit headquarters in Asia.
Xavier soon learned that along the Pearl Fishery Coast, which extends from Cape Comorin on the southern tip of India to the island of Manaar, off Ceylon (Sri Lanka), there was a Jāti of people called Paravas. Many of them had been baptized ten years before, merely to please the Portuguese, who had helped them against the Moors, but remained uninstructed in the faith. Accompanied by several native clerics from the seminary at Goa, he set sail for Cape Comorin in October 1542. First he set himself to learn the language of the Paravas; he taught those who had already been baptized, and preached to those who weren't. His efforts with the high-caste Brahmins remained unavailing.
He devoted almost three years to the work of preaching to the people of southern India and Ceylon, converting many. Many were the difficulties and hardships which Xavier had to encounter at this time, sometimes because the Portuguese soldiers, far from seconding his work, hampered it by their bad example and vicious habits. He built nearly 40 churches along the coast, including St. Stephen's Church, Kombuthurai, mentioned in his letters dated 1544.
During this time, he was able to visit the tomb of St. Thomas the Apostle in Mylapore (now part of Madras (Chennai) then in Portuguese India). He set his sights eastward in 1545 and planned a missionary journey to Makassar on the island of Celebes (today's Indonesia).
As the first Jesuit in India, Francis had difficulty achieving much success in his missionary trips. His successors, such as de Nobili, Matteo Ricci, and Beschi, attempted to convert the noblemen first as a means to influence more people, while Francis had initially interacted most with the lower classes (later though, in Japan, Francis changed tack by paying tribute to the Emperor and seeking an audience with him).
South East Asia.
In the spring of 1545 Xavier started for Portuguese Malacca. He laboured there for the last months of that year. About January 1546, Xavier left Malacca for the Maluku Islands, where the Portuguese had some settlements. For a year and a half he preached the Gospel there. He went first to Ambon Island, where he stayed until mid-June. He then visited other Maluku Islands, including Ternate, Baranura, and Morotai. Shortly after Easter 1546, he returned to Ambon Island; a few months later he returned to Malacca.
Japan.
In Malacca in December 1547, Francis Xavier met a Japanese man named Anjirō. Anjirō had heard of Francis in 1545 and had traveled from Kagoshima to Malacca to meet him. Having been charged with murder, Anjirō had fled Japan. He told Francis extensively about his former life and the customs and culture of his homeland. Anjirō became the first Japanese Christian and adopted the name of 'Paulo de Santa Fe'. He later helped Xavier as a mediator and translator for the mission to Japan that now seemed much more possible.
In January 1548 Francis returned to Goa to attend to his responsibilities as superior of the mission there. The next 15 months were occupied with various journeys and administrative measures. He left Goa on 15 April 1549, stopped at Malacca, and visited Canton. He was accompanied by Anjiro, two other Japanese men, Father Cosme de Torrès, and Brother João Fernandes. He had taken with him presents for the "King of Japan" since he was intending to introduce himself as the Apostolic Nuncio.
Europeans had already come to Japan: the Portuguese had landed in 1543 on the island of Tanegashima, where they introduced the first firearms to Japan.
From Amboina, he wrote to his companions in Europe: "I asked a Portuguese merchant, … who had been for many days in Anjirō’s country of Japan, to give me … some information on that land and its people from what he had seen and heard …. All the Portuguese merchants coming from Japan tell me that if I go there I shall do great service for God our Lord, more than with the pagans of India, for they are a very reasonable people. (To His Companions Residing in Rome, From Cochin, 20 January 1548, no. 18, p. 178).
Francis Xavier reached Japan on 27 July 1549, with Anjiro and three other Jesuits, but he was not permitted to enter any port his ship arrived at until 15 August, when he went ashore at Kagoshima, the principal port of Satsuma Province on the island of Kyūshū. As a representative of the Portuguese king, he was received in a friendly manner. Shimazu Takahisa (1514–1571), daimyo of Satsuma, gave a friendly reception to Francis on 29 September 1549, but in the following year he forbade the conversion of his subjects to Christianity under penalty of death; Christians in Kagoshima could not be given any catechism in the following years. The Portuguese missionary Pedro de Alcáçova would later write in 1554:
He was hosted by Anjirō's family until October 1550. From October to December 1550, he resided in Yamaguchi. Shortly before Christmas, he left for Kyoto but failed to meet with the Emperor. He returned to Yamaguchi in March 1551, where he was permitted to preach by the daimyo of the province. However, lacking fluency in the Japanese language, he had to limit himself to reading aloud the translation of a catechism.
Francis was the first Jesuit to go to Japan as a missionary. He brought with him paintings of the Madonna and the Madonna and Child. These paintings were used to help teach the Japanese about Christianity. There was a huge language barrier as Japanese was unlike other languages the missionaries had previously encountered. For a long time Francis struggled to learn the language.
Having learned that evangelical poverty had not the appeal in Japan that it had in Europe and in India, he decided to change his method of approach. Hearing after a time that a Portuguese ship had arrived at a port in the province of Bungo in Kyushu and that the prince there would like to see him, Xavier now set out southward. The Jesuit, in a fine cassock, surplice, and stole, was attended by thirty gentlemen and as many servants, all in their best clothes. Five of them bore on cushions valuable articles, including a portrait of Our Lady and a pair of velvet slippers, these not gifts for the prince, but solemn offerings to Xavier, to impress the onlookers with his eminence. Handsomely dressed, with his companions acting as attendants, he presented himself before Oshindono, the ruler of Nagate, and as a representative of the great kingdom of Portugal offered him the letters and presents, a musical instrument, a watch, and other attractive objects which had been given him by the authorities in India for the emperor.
For forty-five years the Jesuits were the only missionaries in Asia, but the Franciscans also began proselytizing in Asia as well. Christian missionaries were later forced into exile, along with their assistants. Some were able to stay behind, however Christianity was then kept underground as to not be persecuted.
The Japanese people were not easily converted; many of the people were already Buddhist or Shinto. Francis tried to combat the disposition of some of the Japanese that a God who had created everything, including evil, could not be good. The concept of Hell was also a struggle; the Japanese were bothered by the idea of their ancestors living in Hell. Despite Francis' different religion, he felt that they were good people, much like Europeans, and could be converted.
Xavier was welcomed by the Shingon monks since he used the word "Dainichi" for the Christian God; attempting to adapt the concept to local traditions. As Xavier learned more about the religious nuances of the word, he changed to "Deusu" from the Latin and Portuguese "Deus". The monks later realized that Xavier was preaching a rival religion and grew more aggressive towards his attempts at conversion.
With the passage of time, his sojourn in Japan could be considered somewhat fruitful as attested by congregations established in Hirado, Yamaguchi, and Bungo. Xavier worked for more than two years in Japan and saw his successor-Jesuits established. He then decided to return to India. Historians debate the exact path he returned by, but from evidence attributed to the captain of his ship, he may have traveled through Tanegeshima and Minato, and avoided Kagoshima because of the hostility of the daimyo. During his trip, a tempest forced him to stop on an island near Guangzhou, China where he met Diogo Pereira, a rich merchant and an old friend from Cochin. Pereira showed him a letter from Portuguese prisoners in Guangzhou, asking for a Portuguese ambassador to speak to the Chinese Emperor on their behalf. Later during the voyage, he stopped at Malacca on 27 December 1551, and was back in Goa by January 1552.
On 17 April he set sail with Diogo Pereira on the "Santa Cruz" for China. He planned to introduce himself as Apostolic Nuncio and Pereira as ambassador of the King of Portugal. But then he realized that he had forgotten his testimonial letters as an Apostolic Nuncio. Back in Malacca, he was confronted by the "capitão" Álvaro de Ataíde da Gama who now had total control over the harbor. The "capitão" refused to recognize his title of Nuncio, asked Pereira to resign from his title of ambassador, named a new crew for the ship, and demanded the gifts for the Chinese Emperor be left in Malacca.
In late August 1552, the "Santa Cruz" reached the Chinese island of Shangchuan, 14 km away from the southern coast of mainland China, near Taishan, Guangdong, 200 km south-west of what later became Hong Kong. At this time, he was accompanied only by a Jesuit student, Álvaro Ferreira, a Chinese man called António, and a Malabar servant called Christopher. Around mid-November he sent a letter saying that a man had agreed to take him to the mainland in exchange for a large sum of money. Having sent back Álvaro Ferreira, he remained alone with António. He died at Shangchuan from a fever on 3 December 1552, while he was waiting for a boat that would agree to take him to mainland China.
Burials and relics.
He was first buried on a beach at Shangchuan Island, Taishan, Guangdong. His incorrupt body was taken from the island in February 1553 and was temporarily buried in St. Paul's church in Portuguese Malacca on 22 March 1553. An open grave in the church now marks the place of Xavier's burial. Pereira came back from Goa, removed the corpse shortly after 15 April 1553, and moved it to his house. On 11 December 1553, Xavier's body was shipped to Goa. The body is now in the Basilica of Bom Jesus in Goa, where it was placed in a glass container encased in a silver casket on 2 December 1637. This casket, constructed by Goan silversmiths between 1636 and 1637, was an exemplary blend of Italian and Indian aesthetic sensibilities. There are 32 silver plates on all the four sides of the casket depicting different episodes from the life of the Saint:
The right forearm, which Xavier used to bless and baptize his converts, was detached by Superior General Claudio Acquaviva in 1614. It has been displayed since in a silver reliquary at the main Jesuit church in Rome, Il Gesù.
Another of Xavier's arm bones was brought to Macau where it was kept in a silver reliquary. The relic was destined for Japan but religious persecution there persuaded the church to keep it in Macau's Cathedral of St. Paul. It was subsequently moved to St. Joseph's and in 1978 to the Chapel of St. Francis Xavier on Coloane Island. More recently the relic was moved to St. Joseph's Church.
In 2006, on the 500th anniversary of his birth, the Xavier Tomb Monument and Chapel on the Shangchuan Island, in ruins after years of neglect under communist rule in China was restored with the support from the alumni of Wah Yan College, a Jesuit high school in Hong Kong.
Veneration.
Beatification and canonization.
Francis Xavier was beatified by Paul V on 25 October 1619, and was canonized by Gregory XV on 12 March (12 April) 1622, at the same time as Ignatius Loyola. Pius XI proclaimed him the "Patron of Catholic Missions". His feast day is 3 December.
Pilgrimage centres.
Goa.
Saint Francis Xavier's relics are kept in a silver casket, elevated inside the Bom Jesus Basilica and are exposed (being brought to ground level) generally every ten years, but this is discretionary. The sacred relics went on display starting on 22 November 2014 at the XVII Solemn Exposition. The display closed on 4 January 2015. The previous exposition, the sixteenth, was held from 21 November 2004 to 2 January 2005.
Relics of Saint Francis Xavier are also found in the Espirito Santo (Holy Spirit) Church, Margão, in Sanv Fransiku Xavierachi Igorz (Church of St. Francis Xavier), Batpal, Canacona, Goa and at St. Francis Xavier Chapel, Portais, Panjim.
Other places.
Other pilgrimage centres include Saint Francis Xavier's birthplace in Navarra, Church of Il Gesu, Rome, Malacca (where he was buried for 2 years, before being brought to Goa), Sancian (Place of death) etc.
In Magdalena de Kino in Sonora, Mexico in the Temple of Santa María Magdalena, there is a statue of San Francisco Xavier, an important historical figure for both Sonora and the neighboring U.S. state of Arizona. The statue is said to be miraculous and is the object of pilgrimage for many of the region.
Novena of grace.
The Novena of Grace is a popular devotion to Francis Xavier, typically prayed either on the nine days before 3 December, or on 4 March through 12 March (the anniversary of Pope Gregory XV's canonization of Xavier in 1622). It began with the Italian Jesuit missionary Marcello Mastrilli. Before he could travel to the Far East, Mastrilli was gravely injured in a freak accident after a festive celebration dedicated to the Immaculate Conception in Naples. Delirious and on the verge of death, Mastrilli saw Xavier, who he later said asked him to choose between traveling or death by holding the respective symbols, to which Mastrilli answered, ""I choose that which God wills."" Upon regaining his health, Mastrilli made his way via Goa and the Philippines to Satsuma, Japan. The Tokugawa Shogunate beheaded the missionary in October 1637, after undergoing three days of tortures involving the volcanic sulfurous fumes from Mt. Unzen, known as the "Hell mouth" or "pit" that had supposedly caused an earlier missionary to renounce his faith.
Legacy.
St. Francis Xavier is noteworthy for his missionary work, both as organizer and as pioneer, reputed to have converted more people than anyone else has done since Saint Paul. Pope Benedict XVI said of both Ignatius of Loyola and Francis Xavier: "not only their history which was interwoven for many years from Paris and Rome, but a unique desire — a unique passion, it could be said — moved and sustained them through different human events: the passion to give to God-Trinity a glory always greater and to work for the proclamation of the Gospel of Christ to the peoples who had been ignored." By consulting with the earlier ancient Christians of St. Thomas in India, Xavier developed Jesuit missionary methods. His success also spurred many Europeans to join the order, as well as become missionaries throughout the world. His personal efforts most affected Christians in India and the East Indies (Indonesia, Malaysia, Timor). India still has numerous Jesuit missions, and many more schools. Xavier also worked to propagate Christianity in China and Japan. However, following the persecutions of Toyotomi Hideyoshi and the subsequent closing of Japan to foreigners, the Christians of Japan were forced to go underground to develop an independent Christian culture. Likewise, while Xavier inspired many missionaries to China, Chinese Christians also were forced underground and developed their own Christian culture.
Francis Xavier is the patron saint of his native Navarre, which celebrates his feast day on 3 December as a government holiday. In addition to Roman Catholic masses remembering Xavier on that day (now known as the Day of Navarra), celebrations in the surrounding weeks honor the region's cultural heritage. Furthermore, in the 1940s, devoted Catholics instituted the Javierada, an annual day-long pilgrimage (often on foot) from the capital at Pamplona to Xavier, where his order has built a basilica and museum and restored his family's castle.
Namesake.
As the foremost saint from Navarre and one of the main Jesuit saints, he is much venerated in Spain and the Hispanic countries where "Francisco Javier" or "Javier" are common male given names.
The alternative spelling "Xavier" is also popular in Portugal, Catalonia, Brazil, France, Belgium, and southern Italy. In India, the spelling "Xavier" is almost always used, and the name is quite common among Christians, especially in Goa and the southern states of Tamil Nadu, Kerala, Karnataka. The names "Francisco Xavier", "António Xavier", "João Xavier", "Caetano Xavier", "Domingos Xavier" et cetera, were very common till quite recently in Goa. In Austria and Bavaria the name is spelled as "Xaver" (pronounced [ˈk͡saːfɐ]) and often used in addition to Francis as "Franz-Xaver" [frant͡sˈk͡saːfɐ]. Many Catalan men are named for him, often using the two-name combination Francesc Xavier. In English speaking countries, "Xavier" until recently was likely to follow "Francis"; in the 2000s, however, "Xavier" by itself has become more popular than "Francis", and since 2001 is now one of the hundred most common male baby names in the U.S.A. Furthermore, the Sevier family name, possibly most famous in the United States for John Sevier originated from the name Xavier.
Many churches all over the world, often founded by Jesuits, have been named in honor of Xavier. Those in the United States include the historic St. Francis Xavier Shrine at Warwick, Maryland, (founded 1720, and at which American founding father, Charles Carroll of Carrollton, (1737–1832), (longest living signer and only Catholic at the Continental Congress to sign the Declaration of Independence, 1776) and cousin to the first American-born Bishop John Carroll, (1735–1815), Bishop and later Archbishop of Baltimore, 1790–1815, (at the Roman Catholic Archdiocese of Baltimore) began their education), also the American educational teaching order Xaverian Brothers, the Basilica of St. Francis Xavier in Dyersville, Iowa, and the Mission San Xavier del Bac in Tucson, Arizona (founded in 1692, and known for its Spanish Colonial architecture).
In art.
Rubens painted "St Francis Xavier Raising the Dead", for a Jesuit church in Antwerp, and in which he depicted one of St Francis' many miracles (in this case a resurrection).
In popular culture.
Francis Xavier's name is mentioned in an unnamed episode of the satirical webcomic Saturday Morning Breakfast Cereal.
Missionary.
Shortly before leaving he had issued a famous instruction to Father Gaspar Barazeuz who was leaving to go to Ormuz (a kingdom on an island in the Persian Gulf, formerly attached to the Empire of Persia, now part of Iran), that he should mix with sinners:
Modern scholars place the number of people converted to Christianity by Francis Xavier at around 30,000. And while some of Xavier's methods have been since criticized (he forced converts to take Portuguese names and dress in Western clothes, approved the persecution of the Eastern Church, and used the Goa government as a missionary tool), he has also earned praise. He insisted that missionaries adapt to many of the customs, and most certainly the language, of the culture they wish to evangelize. And unlike later missionaries, Xavier supported an educated native clergy. Though for a time, it seemed his work in Japan was subsequently destroyed by persecution, Protestant missionaries three centuries later discovered that approx. 100,000 Christians still practiced in the Nagasaki area.
Francis Xavier's work initiated permanent change in eastern Indonesia, and he was known as the 'Apostle of the Indies' where in 1546–1547 he worked in the Maluku Islands among the people of Ambon, Ternate, and Morotai (or Moro), and laid the foundations for a permanent mission. After he left the Maluku Islands, others carried on his work and by the 1560s there were 10,000 Roman Catholics in the area, mostly on Ambon. By the 1590s there were 50,000 to 60,000.
Role in the Goa Inquisition.
It was in Kenya that Francis Xavier had his first contact with another religion, Islam. Although extremely tolerant of human flaws, he, as many Christians of that time, was not tolerant of other religions, which he considered to be "Devil’s instruments". Deeply imbued with the theology of the later Augustine, he was fiercely "jealous" of "God’s greater glory" and deeply suspicious of the "untutored" efforts of man to scale the heights of the spirit. This worldview led him to missionary tactics that even the Jesuit James Patrick Broderick, though writing an admiring biography, condemns Xavier’s “woefully inadequate views about Indian religion and civilization”.
The role of Francis Xavier in the Goa Inquisition is controversial. He had written to King João III of Portugal in 1546, encouraging him to dispatch the Inquisition to Goa, which he did many years later in 1560. Francis Xavier died in 1552 without living to see the horrors of the Goa Inquisition, but some historians believe that he was aware of the Portuguese Inquisition's brutality. In an interview to an Indian newspaper, historian Teotónio de Souza stated that Francis Xavier and Simão Rodrigues, another founder-member of the Society of Jesus, were together in Lisbon before Francis left for India. Both were asked to assist spiritually the prisoners of the Inquisition and were present at the very first Auto-da-fé celebrated in Portugal in September 1540, at which 23 were absolved and two were condemned to be burnt, including a French cleric. Hence, he believes that Xavier was aware of the brutality of the Inquisition.

</doc>
<doc id="10958" url="https://en.wikipedia.org/wiki?curid=10958" title="Fossil">
Fossil

Fossils (from Classical Latin "fossilis"; literally, "obtained by digging") are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in "fossiliferous" (fossil-containing) rock formations and sedimentary layers (strata) is known as the "fossil record".
The study of fossils across geological time, how they were formed, and the evolutionary relationships between taxa (phylogeny) are some of the most important functions of the science of paleontology. Such a preserved specimen is called a "fossil" if it is older than some minimum age, most often the arbitrary date of 10,000 years. Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest, chemical fossils from the Archaean Eon, up to 3.48 billion years old,</ref> or even older, 4.1 billion years old, according to a 2015 study. The observation that certain fossils were associated with certain rock strata led early geologists to recognize a geological timescale in the 19th century. The development of radiometric dating techniques in the early 20th century allowed geologists to determine the numerical or ""absolute" age" of the various strata and thereby the included fossils.
Like extant organisms, fossils vary in size from microscopic, even single bacterial cells one micrometer in diameter, to gigantic, such as dinosaurs and trees many meters long and weighing many tons. A fossil normally preserves only a portion of the deceased organism, usually that portion that was partially mineralized during life, such as the bones and teeth of vertebrates, or the chitinous or calcareous exoskeletons of invertebrates. Fossils may also consist of the marks left behind by the organism while it was alive, such as animal tracks or feces (coprolites). These types of fossil are called trace fossils (or "ichnofossils"), as opposed to "body fossils". Finally, past life leaves some markers that cannot be seen but can be detected in the form of biochemical signals; these are known as "chemofossils" or biosignatures.
Fossilization processes.
The process of fossilization varies according to tissue type and external conditions.
Permineralization.
Permineralization is a process of fossilization that occurs when an organism is buried. The empty spaces within an organism (spaces filled with liquid or gas during life) become filled with mineral-rich groundwater. Minerals precipitate from the groundwater, occupying the empty spaces. This process can occur in very small spaces, such as within the cell wall of a plant cell. Small scale permineralization can produce very detailed fossils. For permineralization to occur, the organism must become covered by sediment soon after death or soon after the initial decay process. The degree to which the remains are decayed when covered determines the later details of the fossil. Some fossils consist only of skeletal remains or teeth; other fossils contain traces of skin, feathers or even soft tissues. This is a form of diagenesis.
Casts and molds.
In some cases the original remains of the organism completely dissolve or are otherwise destroyed. The remaining organism-shaped hole in the rock is called an "external mold". If this hole is later filled with other minerals, it is a "cast". An endocast or "internal mold" is formed when sediments or minerals fill the internal cavity of an organism, such as the inside of a bivalve or snail or the hollow of a skull.
Authigenic mineralization.
This is a special form of cast and mold formation. If the chemistry is right, the organism (or fragment of organism) can act as a nucleus for the precipitation of minerals such as siderite, resulting in a nodule forming around it. If this happens rapidly before significant decay to the organic tissue, very fine three-dimensional morphological detail can be preserved. Nodules from the Carboniferous Mazon Creek fossil beds of Illinois, USA, are among the best documented examples of such mineralization.
Replacement and recrystallization.
Replacement occurs when the shell, bone or other tissue is replaced with another mineral. In some cases mineral replacement of the original shell occurs so gradually and at such fine scales that microstructural features are preserved despite the total loss of original material. A shell is said to be "recrystallized" when the original skeletal compounds are still present but in a different crystal form, as from aragonite to calcite.
Adpression (compression-impression).
Compression fossils, such as those of fossil ferns, are the result of chemical reduction of the complex organic molecules composing the organism's tissues. In this case the fossil consists of original material, albeit in a geochemically altered state. This chemical change is an expression of diagenesis. Often what remains is a carbonaceous film known as a phytoleim, in which case the fossil is known as a compression. Often, however, the phytoleim is lost and all that remains is an impression of the organism in the rock—an impression fossil. In many cases, however, compressions and impressions occur together. For instance, when the rock is broken open, the phytoleim will often be attached to one part (compression), whereas the counterpart will just be an impression. For this reason, one term covers the two modes of preservation: "adpression".
Soft tissue, cell and molecular preservation.
Because of their antiquity, an unexpected exception to the alteration of an organism's tissues by chemical reduction of the complex organic molecules during fossilization has been the discovery of soft tissue in dinosaur fossils, including blood vessels, and the isolation of proteins and evidence for DNA fragments. In 2014, Mary Schweitzer and her colleagues reported the presence of iron particles (goethite-aFeO(OH)) associated with soft tissues recovered from dinosaur fossils. Based on various experiments that studied the interaction of iron in haemoglobin with blood vessel tissue they proposed that solution hypoxia coupled with iron chelation enhances the stability and preservation of soft tissue and provides the basis for an explanation for the unforeseen preservation of fossil soft tissues. However, a slightly older study based on eight taxa ranging in time from the Devonian to the Jurassic found that reasonably well-preserved fibrils that probably represent collagen were preserved in all these fossils, and that the quality of preservation depended mostly on the arrangement of the collagen fibers, with tight packing favoring good preservation. There seemed to be no correlation between geological age and quality of preservation, within that timeframe.
Carbonization.
Carbonaceous films are thin coatings which consist predominantly of the chemical element carbon. The soft tissues of organisms are made largely of organic carbon compounds and during diagenesis under reducing conditions only a thin film of carbon residue is left which forms a silhouette of the original organism.
Bioimmuration.
Bioimmuration occurs when a skeletal organism overgrows or otherwise subsumes another organism, preserving the latter, or an impression of it, within the skeleton. Usually it is a sessile skeletal organism, such as a bryozoan or an oyster, which grows along a substrate, covering other sessile sclerobionts. Sometimes the bioimmured organism is soft-bodied and is then preserved in negative relief as a kind of external mold. There are also cases where an organism settles on top of a living skeletal organism that grows upwards, preserving the settler in its skeleton. Bioimmuration is known in the fossil record from the Ordovician to the Recent.
Fossil record.
Estimating dates.
Paleontology seeks to map out how life evolved across geologic time. A substantial hurdle is the difficulty of working out fossil ages. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50 million years old an absolute age, and can be accurate to within 0.5% or better. Although radiometric dating requires careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to its decay products shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are volcanic ash layers, which may provide termini for the intervening sediments.
Stratigraphy.
Consequently, palaeontologists rely on stratigraphy to date fossils. Stratigraphy is the science of deciphering the "layer-cake" that is the sedimentary record. Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age is claimed to lie between the two known ages. Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly adjacent. However, fossils of species that survived for a relatively short time can be used to match isolated rocks: this technique is called "biostratigraphy". For instance, the conodont "Eoplacognathus pseudoplanus" has a short range in the Middle Ordovician period. If rocks of unknown age have traces of "E. pseudoplanus", they have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and occupy a short time range to be useful. Misleading results are produced if the index fossils are incorrectly dated. Stratigraphy and biostratigraphy can in general provide only relative dating ("A" was before "B"), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching rocks of the same age across continents.
Family-tree relationships also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated "family tree" says A was an ancestor of B and C, then A must have evolved earlier.
It is also possible to estimate how long ago two living clades diverged – i.e. approximately how long ago their last common ancestor must have lived  – by assuming that DNA mutations accumulate at a constant rate. These "molecular clocks", however, are fallible, and provide only approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques may vary by a factor of two.
Limitations.
Organisms are only rarely preserved as fossils in the best of circumstances, and only a fraction of such fossils have been discovered. This is illustrated by the fact that the number of species known through the fossil record is less than 5% of the number of known living species, suggesting that the number of species known through fossils must be far less than 1% of all the species that have ever lived. Because of the specialized and rare circumstances required for a biological structure to fossilize, only a small percentage of life-forms can be expected to be represented in discoveries, and each discovery represents only a snapshot of the process of evolution. The transition itself can only be illustrated and corroborated by transitional fossils, which will never demonstrate an exact half-way point.
The fossil record is heavily slanted toward organisms with hard parts, leaving most groups of soft-bodied organisms with little to no role. It is replete with the mollusks, the vertebrates, the echinoderms, the brachiopods and some groups of arthropods.
Lagerstätten.
Fossil sites with exceptional preservation—sometimes including preserved soft tissues—are known as Lagerstätten. These formations may have resulted from carcass burial in an anoxic environment with minimal bacteria, thus slowing decomposition. Lagerstätten span geological time from the Cambrian period to the present. Worldwide, some of the best examples of near-perfect fossilization are the Cambrian Maotianshan shales and Burgess Shale, the Devonian Hunsrück Slates, the Jurassic Solnhofen limestone, and the Carboniferous Mazon Creek localities.
Stromatolites.
Stromatolites are layered accretionary structures formed in shallow water by the trapping, binding and cementation of sedimentary grains by biofilms of microorganisms, especially cyanobacteria. Stromatolites provide some of the most ancient fossil records of life on Earth, dating back more than 3.5 billion years ago.
Stromatolites were much more abundant in Precambrian times. While older, Archean fossil remains are presumed to be colonies of cyanobacteria, younger (that is, Proterozoic) fossils may be primordial forms of the eukaryote chlorophytes (that is, green algae). One genus of stromatolite very common in the geologic record is "Collenia". The earliest stromatolite of confirmed microbial origin dates to 2.724 billion years ago.
A 2009 discovery provides strong evidence of microbial stromatolites extending as far back as 3.45 billion years ago.</ref>
Stromatolites are a major constituent of the fossil record for life's first 3.5 billion years, peaking about 1.25 billion years ago. They subsequently declined in abundance and diversity, which by the start of the Cambrian had fallen to 20% of their peak. The most widely supported explanation is that stromatolite builders fell victims to grazing creatures (the Cambrian substrate revolution), implying that sufficiently complex organisms were common over 1 billion years ago.
The connection between grazer and stromatolite abundance is well documented in the younger Ordovician evolutionary radiation; stromatolite abundance also increased after the end-Ordovician and end-Permian extinctions decimated marine animals, falling back to earlier levels as marine animals recovered. Fluctuations in metazoan population and diversity may not have been the only factor in the reduction in stromatolite abundance. Factors such as the chemistry of the environment may have been responsible for changes.
While prokaryotic cyanobacteria themselves reproduce asexually through cell division, they were instrumental in priming the environment for the evolutionary development of more complex eukaryotic organisms. Cyanobacteria (as well as extremophile Gammaproteobacteria) are thought to be largely responsible for increasing the amount of oxygen in the primeval earth's atmosphere through their continuing photosynthesis. Cyanobacteria use water, carbon dioxide and sunlight to create their food. A layer of mucus often forms over mats of cyanobacterial cells. In modern microbial mats, debris from the surrounding habitat can become trapped within the mucus, which can be cemented by the calcium carbonate to grow thin laminations of limestone. These laminations can accrete over time, resulting in the banded pattern common to stromatolites. The domal morphology of biological stromatolites is the result of the vertical growth necessary for the continued infiltration of sunlight to the organisms for photosynthesis. Layered spherical growth structures termed oncolites are similar to stromatolites and are also known from the fossil record. Thrombolites are poorly laminated or non-laminated clotted structures formed by cyanobacteria common in the fossil record and in modern sediments.
The Zebra River Canyon area of the Kubis platform in the deeply dissected Zaris Mountains of south western Namibia provides an extremely well exposed example of the thrombolite-stromatolite-metazoan reefs that developed during the Proterozoic period, the stromatolites here being better developed in updip locations under conditions of higher current velocities and greater sediment influx.
Types.
Index.
Index fossils (also known as guide fossils, indicator fossils or zone fossils) are fossils used to define and identify geologic periods (or faunal stages). They work on the premise that, although different sediments may look different depending on the conditions under which they were deposited, they may include the remains of the same species of fossil. The shorter the species' time range, the more precisely different sediments can be correlated, and so rapidly evolving species' fossils are particularly valuable. The best index fossils are common, easy-to-identify at species level and have a broad distribution—otherwise the likelihood of finding and recognizing one in the two sediments is poor
Trace.
Trace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and they reflect animal behaviours. Many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).
Coprolites are classified as trace fossils as opposed to body fossils, as they give evidence for the animal's behaviour (in this case, diet) rather than morphology. They were first described by William Buckland in 1829. Prior to this they were known as "fossil fir cones" and "bezoar stones." They serve a valuable purpose in paleontology because they provide direct evidence of the predation and diet of extinct organisms. Coprolites may range in size from a few millimetres to over 60 centimetres.
Transitional.
A "transitional fossil" is any fossilized remains of a life form that exhibits traits common to both an ancestral group and its derived descendant group. This is especially important where the descendant group is sharply differentiated by gross anatomy and mode of living from the ancestral group. Because of the incompleteness of the fossil record, there is usually no way to know exactly how close a transitional fossil is to the point of divergence. These fossils serve as a reminder that taxonomic divisions are human constructs that have been imposed in hindsight on a continuum of variation.
Microfossils.
Microfossil is a descriptive term applied to fossilized plants and animals whose size is just at or below the level at which the fossil can be analyzed by the naked eye. A commonly applied cutoff point between "micro" and "macro" fossils is 1 mm. Microfossils may either be complete (or near-complete) organisms in themselves (such as the marine plankters foraminifera and coccolithophores) or component parts (such as small teeth or spores) of larger animals or plants. Microfossils are of critical importance as a reservoir of paleoclimate information, and are also commonly used by biostratigraphers to assist in the correlation of rock units.
Resin.
Fossil resin (colloquially called amber) is a natural polymer found in many types of strata throughout the world, even the Arctic. The oldest fossil resin dates to the Triassic, though most dates to the Cenozoic. The excretion of the resin by certain plants is thought to be an evolutionary adaptation for protection from insects and to seal wounds. Fossil resin often contains other fossils called inclusions that were captured by the sticky resin. These include bacteria, fungi, other plants, and animals. Animal inclusions are usually small invertebrates, predominantly arthropods such as insects and spiders, and only extremely rarely a vertebrate such as a small lizard. Preservation of inclusions can be exquisite, including small fragments of DNA.
Derived.
A "derived", "reworked" or "remanié fossil" is a fossil found in rock that accumulated significantly later than when the fossilized animal or plant died:"Reworked fossil" in "Glossary of Geology"</ref> Reworked fossils are created by erosion exhuming (freeing) fossils from the rock formation in which they were originally deposited and their redeposition in an younger sedimentary deposit.
Wood.
Fossil wood is wood that is preserved in the fossil record. Wood is usually the part of a plant that is best preserved (and most easily found). Fossil wood may or may not be petrified. The fossil wood may be the only part of the plant that has been preserved: therefore such wood may get a special kind of botanical name. This will usually include "xylon" and a term indicating its presumed affinity, such as "Araucarioxylon" (wood of "Araucaria" or some related genus), "Palmoxylon" (wood of an indeterminate palm), or "Castanoxylon" (wood of an indeterminate chinkapin).
Subfossil.
The term subfossil can be used to refer to remains, such as bones, nests, or defecations, whose fossilization process is not complete, either because the length of time since the animal involved was living is too short (less than 10,000 years) or because the conditions in which the remains were buried were not optimal for fossilization. Subfossils are often found in caves or other shelters where they can be preserved for thousands of years. The main importance of subfossil vs. fossil remains is that the former contain organic material, which can be used for radiocarbon dating or extraction and sequencing of DNA, protein, or other biomolecules. Additionally, isotope ratios can provide much information about the ecological conditions under which extinct animals lived. Subfossils are useful for studying the evolutionary history of an environment and can be important to studies in paleoclimatology.
Subfossils are often found in depositionary environments, such as lake sediments, oceanic sediments, and soils. Once deposited, physical and chemical weathering can alter the state of preservation.
Chemical fossils.
Chemical fossils are chemicals found in rocks and fossil fuels (petroleum, coal, and natural gas) that provide an organic signature for ancient life. Molecular fossils and isotope ratios represent two types of chemical fossils. The oldest traces of life on Earth are fossils of this type, including carbon isotope anomalies found in zircons that imply the existence of life as early as 4.1 billion years ago.
Astrobiology.
It has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on the planet Mars. Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.
On 24 January 2014, NASA reported that current studies by the "Curiosity" and "Opportunity" rovers on Mars will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.
Pseudofossils.
"Pseudofossils" are visual patterns in rocks that are produced by geologic processes rather than biologic processes. They can easily be mistaken for real fossils. Some pseudofossils, such as dendrites, are formed by naturally occurring fissures in the rock that get filled up by percolating minerals. Other types of pseudofossils are kidney ore (round shapes in iron ore) and moss agates, which look like moss or plant leaves. Concretions, spherical or ovoid-shaped nodules found in some sedimentary strata, were once thought to be dinosaur eggs, and are often mistaken for fossils as well.
History of the study of fossils.
Gathering fossils dates at least to the beginning of recorded history. The fossils themselves are referred to as the fossil record. The fossil record was one of the early sources of data underlying the study of evolution and continues to be relevant to the history of life on Earth. Paleontologists examine the fossil record to understand the process of evolution and the way particular species have evolved.
Before Darwin.
Many early explanations relied on folktales or mythologies. In China the fossil bones of ancient mammals including "Homo erectus" were often mistaken for "dragon bones" and used as medicine and aphrodisiacs. In the West fossilized sea creatures on mountainsides were seen as proof of the biblical deluge.
In 1027, the Persian Avicenna explained fossils' stoniness in "The Book of Healing":
Greek scholar Aristotle realized that fossil seashells from rocks were similar to those found on the beach, indicating the fossils were once living animals. Aristotle previously explained it in terms of vaporous exhalations, which Avicenna modified into the theory of petrifying fluids ("succus lapidificatus"), later elaborated by Albert of Saxony in the 14th century and accepted in some form by most naturalists by the 16th century.
More scientific views of fossils emerged during the Renaissance. Leonardo da Vinci concurred with Aristotle's view that fossils were the remains of ancient life. For example, da Vinci noticed discrepancies with the biblical flood narrative as an explanation for fossil origins:
William Smith (1769–1839), an English canal engineer, observed that rocks of different ages (based on the law of superposition) preserved different assemblages of fossils, and that these assemblages succeeded one another in a regular and determinable order. He observed that rocks from distant locations could be correlated based on the fossils they contained. He termed this the principle of "faunal succession". This principle became one of Darwin's chief pieces of evidence that biological evolution was real.
Georges Cuvier came to believe that most if not all the animal fossils he examined were remains of extinct species. This led Cuvier to become an active proponent of the geological school of thought called catastrophism. Near the end of his 1796 paper on living and fossil elephants he said:
Linnaeus and Darwin.
Early naturalists well understood the similarities and differences of living species leading Linnaeus to develop a hierarchical classification system still in use today. Darwin and his contemporaries first linked the hierarchical structure of the tree of life with the then very sparse fossil record. Darwin eloquently described a process of descent with modification, or evolution, whereby organisms either adapt to natural and changing environmental pressures, or they perish.
When Darwin wrote "On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life", the oldest animal fossils were those from the Cambrian Period, now known to be about 540 million years old. He worried about the absence of older fossils because of the implications on the validity of his theories, but he expressed hope that such fossils would be found, noting that: "only a small portion of the world is known with accuracy." Darwin also pondered the sudden appearance of many groups (i.e. phyla) in the oldest known Cambrian fossiliferous strata.
Further discoveries.
Since Darwin's time, the fossil record has been extended to between 2.3 and 3.5 billion years. Most of these Precambrian fossils are microscopic bacteria or microfossils. However, macroscopic fossils are now known from the late Proterozoic. The Ediacara biota (also called Vendian biota) dating from 575 million years ago collectively constitutes a richly diverse assembly of early multicellular eukaryotes.
The fossil record and faunal succession form the basis of the science of biostratigraphy or determining the age of rocks based on embedded fossils. For the first 150 years of geology, biostratigraphy and superposition were the only means for determining the relative age of rocks. The geologic time scale was developed based on the relative ages of rock strata as determined by the early paleontologists and stratigraphers.
Since the early years of the twentieth century, absolute dating methods, such as radiometric dating (including potassium/argon, argon/argon, uranium series, and, for very recent fossils, radiocarbon dating) have been used to verify the relative ages obtained by fossils and to provide absolute ages for many fossils. Radiometric dating has shown that the earliest known stromatolites are over 3.4 billion years old.
Modern view.
Paleontology has joined with evolutionary biology to share the interdisciplinary task of outlining the tree of life, which inevitably leads backwards in time to Precambrian microscopic life when cell structure and functions evolved. Earth's deep time in the Proterozoic and deeper still in the Archean is only "recounted by microscopic fossils and subtle chemical signals." Molecular biologists, using phylogenetics, can compare protein amino acid or nucleotide sequence homology (i.e., similarity) to evaluate taxonomy and evolutionary distances among organisms, with limited statistical confidence. The study of fossils, on the other hand, can more specifically pinpoint when and in what organism a mutation first appeared. Phylogenetics and paleontology work together in the clarification of science's still dim view of the appearance of life and its evolution.
Niles Eldredge's study of the "Phacops" trilobite genus supported the hypothesis that modifications to the arrangement of the trilobite's eye lenses proceeded by fits and starts over millions of years during the Devonian. Eldredge's interpretation of the "Phacops" fossil record was that the aftermaths of the lens changes, but not the rapidly occurring evolutionary process, were fossilized. This and other data led Stephen Jay Gould and Niles Eldredge to publish their seminal paper on punctuated equilibrium in 1971.
Example of modern development.
Synchrotron X-ray tomographic analysis of early Cambrian bilaterian embryonic microfossils yielded new insights of metazoan evolution at its earliest stages. The tomography technique provides previously unattainable three-dimensional resolution at the limits of fossilization. Fossils of two enigmatic bilaterians, the worm-like "Markuelia" and a putative, primitive protostome, "Pseudooides", provide a peek at germ layer embryonic development. These 543-million-year-old embryos support the emergence of some aspects of arthropod development earlier than previously thought in the late Proterozoic. The preserved embryos from China and Siberia underwent rapid diagenetic phosphatization resulting in exquisite preservation, including cell structures. This research is a notable example of how knowledge encoded by the fossil record continues to contribute otherwise unattainable information on the emergence and development of life on Earth. For example, the research suggests "Markuelia" has closest affinity to priapulid worms, and is adjacent to the evolutionary branching of Priapulida, Nematoda and Arthropoda.
Trading and collecting.
"Fossil trading" is the practice of buying and selling fossils. This is many times done illegally with artifacts stolen from research sites, costing many important scientific specimens each year.</ref> The problem is quite pronounced in China, where many specimens have been stolen.
"Fossil collecting" (some times, in a non-scientific sense, fossil hunting) is the collection of fossils for scientific study, hobby, or profit. Fossil collecting, as practiced by amateurs, is the predecessor of modern paleontology and many still collect fossils and study fossils as amateurs. Professionals and amateurs alike collect fossils for their scientific value.

</doc>
<doc id="10960" url="https://en.wikipedia.org/wiki?curid=10960" title="Family Educational Rights and Privacy Act">
Family Educational Rights and Privacy Act

The Family Educational Rights and Privacy Act of 1974 (FERPA or the Buckley Amendment) is a United States federal law that governs the access of American citizens' educational information and records.
Overview.
FERPA gives parents access to their child's education records, an opportunity to seek to have the records amended, and some control over the disclosure of information from the records. With several exceptions, schools must have a student's consent prior to the disclosure of education records "after that student is 18 years old". The law applies only to educational agencies and institutions that receive funding under a program administered by the U.S. Department of Education. Other regulations under this act, effective starting January 3, 2012, allow for greater disclosures of personal and directory student identifying information and regulate student IDs and e-mail addresses.
Examples of situations affected by FERPA include school employees divulging information to anyone other than the student about the student's grades or behavior, and school work posted on a bulletin board with a grade. Generally, schools must have written permission from the parent or eligible student in order to release any information from a student's education record.
This privacy policy also governs how state agencies transmit testing data to federal agencies. For example see Education Data Network.
This U.S. federal law also gave students 18 years of age or older, or students of any age if enrolled in any post-secondary educational institution, the right of privacy regarding grades, enrollment, and even billing information, unless the school has specific permission from the student to share that specific type of information.
FERPA also permits a school to disclose personally identifiable information from education records of an "eligible student" (a student age 18 or older or enrolled in a postsecondary institution at any age) to his or her parents if the student is a "dependent student" as that term is defined in Section 152 of the Internal Revenue Code. Generally, if either parent has claimed the student as a dependent on the parent's most recent income tax statement, the school may non-consensually disclose the student's education records to both parents.
The law allowed students who apply to an educational institution such as graduate school permission to view recommendations submitted by others as part of the application. However, on standard application forms, students are given the option to waive this right.
FERPA specifically excluded employees of an educational institution if they are not students.
The act is also referred to as the "Buckley Amendment", for one of its proponents, Senator James L. Buckley of New York.
FERPA and access to public records.
The misapplication of FERPA to conceal public records that are not "educational" in nature has been widely criticized, including by the act's primary Senate sponsor.
FERPA and student medical records.
Legal experts have debated the issue of whether student medical records (for example records of therapy sessions with a therapist at an on-campus counseling center) might be released to the school administration under certain triggering events, such as when a student sues their college or university.

</doc>
<doc id="10963" url="https://en.wikipedia.org/wiki?curid=10963" title="Forgetting">
Forgetting

Forgetting is the apparent loss or modification of information already encoded and stored in an individual's long term memory. It is a spontaneous or gradual process in which old memories are unable to be recalled from memory storage. Forgetting also helps to reconcile the storage of new information with old knowledge. Problems with remembering, learning and retaining new information are a few of the most common complaints of older adults. Memory performance is usually related to the active functioning of three stages. These three stages are encoding, storage and retrieval. Many different factors influence the actual process of forgetting. An example of one of these factors could be the amount of time the new information is stored in the memory. Events involved with forgetting can happen either before or after the actual memory process. The amount of time the information is stored in the memory, depending on the minutes hours or even days, can increase or decrease depending on how well the information is encoded.
Studies show that retention improves with increased rehearsal. This improvement occurs because rehearsal helps to transfer information into long term memory - practise makes perfect.
It is subject to delicately balanced optimization that ensures that relevant memories are recalled. Forgetting can be reduced by repetition and/or more elaborate cognitive processing of information. Emotional states are just one of the many factors that have been found to effect this process of forgetting. As a disorder or in more severe cases this may be described as amnesia.
Forgetting functions (amount remembered as a function of time since an event was first experienced) have been extensively analyzed. The most recent evidence suggests that a power function provides the closest mathematical fit to the forgetting function.
It is inability to encode, to store and retrieve the previously learned information from long term memory over varying periods of times.
Summary.
Failing to retrieve an event does not mean that this specific event has been forever forgotten. This could just mean the information was not encoded well. Research has shown that there are a few health behaviors that to some extent can prevent forgetting from happening so often. One of the simplest ways to keep the brain healthy and prevent forgetting is to stay active and exercise. Staying active is important because overall it keeps the body healthy. When the body is healthy the brain is healthy and less inflamed as well. Older adults who were more active were found to have had less episodes of forgetting compared to those older adults who were less active. A healthy diet can also contribute to a healthier brain and aging process which in turn results in less frequent forgetting. Reviewing information in ways that involve active retrieval seems to slow the rate of forgetting. Paul Connerton stated that there are seven types of forgetting, which are repressive erasure, prescriptive forgetting, formation of new identity, structural amnesia, annulment, planned obsolescence, and humiliated silence.
History.
One to study the mechanisms of forgetting was the German psychologist Hermann Ebbinghaus. Using himself as the sole subject in his experiment, he memorized lists of three letter nonsense syllable words—two consonants and one vowel in the middle. He then measured his own capacity to relearn a given list of words after a variety of given time period. He found that forgetting occurs in a systematic manner, beginning rapidly and then leveling off. Although his methods were primitive, his basic premises have held true today and have been reaffirmed by more methodologically sound methods. The Ebbinghaus "forgetting curve" is the name of his results which he plotted out and made 2 conclusions. The first being that much of what we forget is lost soon after it is originally learned. The second being that the amount of forgetting eventually levels off.
Around the same time, psychologist Sigmund Freud had interesting thought of motivating forgetting. He believed that people intentionally forgot things in order to push bad thoughts and feelings deep into their unconscious. He called it repression. This idea can also be known as motivated forgetting or psychogenic amnesia.
Measurements.
The four ways forgetting can be measured are as follows:
Free recall.
Free recall is a basic paradigm used to study human memory. In a free recall task, a subject is presented a list of to-be-remembered items, one at a time. For example, an experimenter might read a list of 20 words aloud, presenting a new word to the subject every 4 seconds. At the end of the presentation of the list, the subject is asked to recall the items (e.g., by writing down as many items from the list as possible). It is called a free recall task because the subject is free to recall the items in any order that he or she desires.
Prompted (cued) recall.
Prompted recall is a slight variation of free recall that consists of presenting hints or prompts to increase the likelihood that the behavior will be produced. Usually these prompts are stimuli that were not there during the training period. Thus in order to measure the degree of forgetting, one can see how many prompts the subject misses or the number of prompts required to produce the behavior.
Relearning method.
This method measures forgetting by the amount of training required to reach the previous level of performance. German psychologist Hermann Ebbinghaus (1885) used this method on himself. He memorized lists of nonsensical syllables until he could repeat the list two times without error. After a certain interval, he relearned the list and saw how long it would take him to do this task. If it took fewer times, then there had been less forgetting. His experiment was one of the first to study forgetting.
Recognition.
For this type of measurement, a participant has to identify material that was previously learned. The participant is asked to remember a list of material. Later on they are shown the same list of material with additional information and they are asked to identify the material that was on the original list. The more they recognize, the less information is forgotten.
Theories of forgetting.
The four main theories of forgetting apparent in the study of psychology are as follows:
Cue-dependent forgetting.
Cue-dependent forgetting (also, context-dependent forgetting) or retrieval failure, is the failure to recall a memory due to missing stimuli or cues that were present at the time the memory was encoded. Encoding is the first step in creating and remembering a memory. How well something has been encoded in the memory can be measured by completing specific tests of retrieval. Examples of these tests would be explicit ones like cued recall or implicit tests like word fragment completion. Cue-dependent forgetting is one of five cognitive psychology theories of forgetting. This theory states that a memory is sometimes temporarily forgotten purely because it cannot be retrieved, but the proper cue can bring it to mind. A good metaphor for this is searching for a book in a library without the reference number, title, author or even subject. The information still exists, but without these cues retrieval is unlikely. Furthermore, a good retrieval cue must be consistent with the original encoding of the information. If the sound of the word is emphasized during the encoding process, the cue that should be used should also put emphasis on the phonetic quality of the word. Information is available however, just not readily available without these cues. Depending on the age of a person, retrieval cues and skills may not work as well. This is usually common in older adults but that is not always the case. When information is encoded into the memory and retrieved with a technique called spaced retrieval, this helps older adults retrieve the events stored in the memory better. There is also evidence from different studies that show age related changes in memory. These specific studies have shown that episodic memory performance does in fact decline with age and have made known that older adults produce vivid rates of forgetting when two items are combined and not encoded.
Trace decay.
Trace decay theory explains memories that are stored in both short term and long term memory system, and assumes that the memories leave a trace in the brain. According to this theory, short term memory (STM) can only retain information for a limited amount of time, around 15 to 30 seconds unless it is rehearsed. If it is not rehearsed, the information will start to gradually fade away and decay. Donald Hebb proposed that incoming information causes a series of neurons to create a neurological memory trace in the brain which would result in change in the morphological and/or chemical changes in the brain and would fade with time. Repeated firing causes a structural change in the synapses. Rehearsal of repeated firing maintains the memory in STM until a structural change is made. Therefore, forgetting happens as a result of automatic decay of the memory trace in brain. This theory states that the events between learning and recall have no effects on recall; the important factor that affects is the duration that the information has been retained. Hence, as longer time passes more of traces are subject to decay and as a result the information is forgotten. 
One major problem about this theory is that in real-life situation, the time between encoding a piece of information and recalling it, is going to be filled with all different kinds of events that might happen to the individual. Therefore, it is difficult to conclude that forgetting is a result of only the time duration. It is also important to consider the effectiveness of this theory. Although it seems very plausible, it is about impossible to test. It is difficult to create a situation where there is a blank period of time between presenting the material and recalling it later.
Organic causes.
Forgetting that occurs through physiological damage or dilapidation to the brain are referred to as organic causes of forgetting. These theories encompass the loss of information already retained in long term memory or the inability to encode new information again. Examples include Alzheimer's, Amnesia, Dementia, consolidation theory and the gradual slowing down of the central nervous system due to aging.
Interference theories.
Interference theory refers to the idea that when the learning of something new causes forgetting of older material on the basis of competition between the two. This essentially states that memory's information may become confused or combined with other information during encoding, resulting in the distortion or disruption of memories. In nature, the interfering items are said to originate from an overstimulating environment. Interference theory exists in three branches: Proactive, Retroactive and Output. Retroactive and Proactive inhibition each referring in contrast to the other. Retroactive interference is when new information (memories) interferes with older information. On the other hand, proactive interference is when old information interferes with the retrieval of new information. This is sometimes thought to occur especially when memories are similar. Output Interference occurs when the initial act of recalling specific information interferes with the retrieval of the original information. This theory shows an astonishing contradiction: an extremely intelligent individual is expected to forget more hastily than one who has a slow mentality. For this reason, an intelligent individual has stored up more memory in his mind which will cause interferences and impair their ability to recall specific information. Based off current research, testing interference has only been carried out by recalling from a list of words rather than using situation from daily lives, thus it's hard to generalize the findings for this theory.
Decay theory.
Decay theory states that when something new is learned, a neurochemical, physical "memory trace" is formed in the brain and over time this trace tends to disintegrate, unless it is occasionally used. Decay theory states the reason we eventually forget something or an event is because the memory of it fades with time. If we do not attempt to look back at an event, the greater the interval time between the time when the event from happening and the time when we try to remember, the memory will start to fade. Time is the greatest impact in remembering an event.
Definitions and controversy.
Forgetting can have very different causes than simply removal of stored content. Forgetting can mean access problems, availability problems, or can have other reasons such as amnesia caused by an accident.
A debatable yet popular concept is "trace decay", which can occur in both short and long-term memory. This theory, applicable mostly to short-term memory, is supposedly contradicted by the fact that one is able to ride a bike even after not having done so for decades. "Flashbulb memories" are another piece of seemingly contradicting evidence. It is believed that certain memories "trace decay" while others don't. Sleep is believed to play a key role in halting trace decay, although the exact mechanism of this is unknown.

</doc>
<doc id="10965" url="https://en.wikipedia.org/wiki?curid=10965" title="Fay Wray">
Fay Wray

Vina Fay Wray (September 15, 1907 – August 8, 2004) was a Canadian-born American actress most noted for playing the female lead in "King Kong". Through an acting career that spanned 57 years, Wray attained international renown as an actress in horror movie roles. She was one of the first "scream queens".
After appearing in minor movie roles, Wray gained media attention being selected as one of the "WAMPAS Baby Stars". This led to her being contracted to Paramount Pictures as a teenager, where she made more than a dozen movies. After leaving Paramount, she signed deals with various film companies, being cast in her first horror film roles among many other types of roles, including in "The Bowery" (1933) and "Viva Villa" (1934), both huge productions starring Wallace Beery. For RKO Radio Pictures, Inc., she starred in the film with which she is most identified, "King Kong" (1933). After the success of "King Kong", Wray made numerous appearances in both film and television before retiring in 1980.
Early life.
Wray was born on a ranch near Cardston in the province of Alberta, Canada, to Mormon parents, Elvina Marguerite Jones, who was from Salt Lake City, Utah, and Joseph Heber Wray, who was from Kingston upon Hull, England. She was one of six children. 
Her family returned to the United States a few years after she was born; they moved to Salt Lake City in 1912 and moved to Lark, Utah in 1914. In 1919, the Wray family returned to Salt Lake City, and then relocated to Hollywood, where Fay attended Hollywood High School.
Early acting career.
In 1923, Wray appeared in her first film at the age of 16, when she landed a role in a short historical film sponsored by a local newspaper. In the 1920s, Wray landed a major role in the silent film "The Coast Patrol" (1925), as well as uncredited bit parts at the Hal Roach Studios.
In 1926, the Western Association of Motion Picture Advertisers selected Wray as one of the "WAMPAS Baby Stars", a group of women whom they believed to be on the threshold of movie stardom. She was at the time under contract to Universal Studios, mostly co-starring in low-budget Westerns opposite Buck Jones.
The following year, Wray was signed to a contract with Paramount Pictures. In 1928, director Erich von Stroheim cast her as the main female lead in his film "The Wedding March", released by Paramount. While the film was noted for its high budget and production values, it was a financial failure, but gave Wray her first lead role. Wray stayed with Paramount to make more than a dozen films and to make the transition from silent films to "talkie" films.
Horror films and "King Kong".
After leaving Paramount, Wray signed to various film companies. Under these deals, Wray was cast in various horror films, including "Doctor X". However, her greatest known films were produced under her deal with RKO Radio Pictures, Inc.. Her first film under RKO was "The Most Dangerous Game" (1932), co-starring Joel McCrea and shot at night on the same jungle sets that were being used for "King Kong" during the day, with the leads from both films, Wray and Robert Armstrong, appearing in both movies.
"The Most Dangerous Game" was followed by Wray's most memorable film, "King Kong". According to Wray, Jean Harlow had been RKO's original choice, but because MGM put Harlow under exclusive contract during the preproduction phase of the film, she became unavailable and Wray was approached by director Merian C. Cooper to play the role of Ann Darrow, the blonde captive of King Kong. Wray was paid $10,000 to play the role. The film was a commercial success. Wray was reportedly proud that the film saved RKO from bankruptcy. Wray's role would become the one with which she would be most associated.
Later career.
She continued to star in various films, including "The Richest Girl in the World", a second film with Joel McCrea, but by the early 1940s, her appearances became less frequent. She retired from acting in 1942, after her second marriage. However, due to financial exigencies, she continued in her acting career, and over the next three decades, Wray appeared in several film roles and also frequently on television.
Wray was cast in the 1953-54 ABC situation comedy, "The Pride of the Family", as Catherine Morrison. Paul Hartman played her husband, Albie Morrison. Natalie Wood and Robert Hyatt played their children, Ann and Junior Morrison, respectively. In 1955, Wray appeared with fellow WAPMAS Baby Star, Joan Crawford in "Queen Bee".
Wray appeared in three episodes of CBS's courtroom drama "Perry Mason", the first of which was "The Case Of The Prodigal Parent" (episode 1-36) aired June 7, 1958. In 1959, she portrayed murder victim Lorna Thomas in "The Case of the Watery Witness". In 1959, Wray was cast as Tula Marsh in the episode "The Second Happiest Day" of the CBS anthology series "Playhouse 90". Other roles around this time were in the episodes "Dip in the Pool" and "The Morning After" of CBS's "Alfred Hitchcock Presents". In 1960, she appeared as Clara in an episode of "77 Sunset Strip", "Who Killed Cock Robin?". Another 1960 role was that of Mrs. Staunton, with Gigi Perreau as her daughter, in the episode "Flight from Terror" of the ABC adventure series, "The Islanders".
Wray appeared in a 1961 episode of "The Real McCoys" titled "Theatre in the Barn". In 1963, she played Mrs. Brubaker in the episode "You're So Smart, Why Can't You Be Good?" of the NBC medical drama about psychiatry, "The Eleventh Hour". In 1965, she played voodoo practitioner Mignon Germaine in "The Case of the Fatal Fetish". She ended her acting career in the 1980 made-for-television film, "Gideon's Trumpet".
In 1988, she published her autobiography, "On the Other Hand". In her later years, Wray continued to make public appearances. In 1991, she was crowned Queen of the Beaux Arts Ball presiding with King Herbert Huncke.
She was approached by James Cameron to play the part of Rose Dawson Calvert for his 1997 blockbuster "Titanic" with Kate Winslet to play her younger self, but she turned down the role and the part of Rose was given to Gloria Stuart. She was a special guest at the 70th Academy Awards, where the show's host, Billy Crystal, introduced her as the "Beauty who charmed the Beast". She was the only 1920s Hollywood actress in attendance that evening. On October 3, 1998, she appeared at the Pine Bluff Film Festival, which showed "The Wedding March" (with live orchestral accompaniment).
In January 2003, the 95-year-old Wray appeared at the 2003 Palm Beach International Film Festival to celebrate the Rick McKay documentary film "", where she was also honored with a "Legend in Film" award. In her later years, she also visited the Empire State Building frequently, once visiting in 1991 as a guest of honor at the building's 60th anniversary, and also in May 2004, which was among her last public appearances. Her final public appearance was at an after-party at the Sardi's restaurant in New York City, following the premiere of the documentary film "Broadway: The Golden Age, by the Legends Who Were There". 
Personal life.
Wray was married three times – to the writers John Monk Saunders and Robert Riskin and to the neurosurgeon Dr. Sanford Rothenberg (January 28, 1919 – January 4, 1991). She had three children: Susan Saunders, Victoria Riskin, and Robert Riskin, Jr. She became a naturalized citizen of the United States in 1933. In her autobiography "On The Other Hand: A Life Story" she declares herself a Republican.
Death.
In 2004, Wray was approached by director Peter Jackson to appear in a small cameo for the 2005 remake of "King Kong". She met with Naomi Watts, who was to play the role of Ann Darrow. She politely declined the cameo, and claimed the original "Kong" to be the true "King". Before filming of the remake commenced, Wray died in her sleep of natural causes on August 8, 2004, in her Manhattan apartment. Her friend Rick McKay said that "she just kind of drifted off quietly as if she was going to sleep... she just kind of gave out." She was 96 years old. Wray is interred at the Hollywood Forever Cemetery in Hollywood, California.
Two days after her death, the lights of the Empire State Building were extinguished for 15 minutes in her memory.
Honors.
In 1989, Wray was awarded the Women in Film Crystal Award. Wray was honored with a "Legend in Film" award at the 2003 Palm Beach International Film Festival. For her contribution to the motion picture industry, Wray was honored with a star on the Hollywood Walk of Fame at 6349 Hollywood Blvd. She received a star posthumously on Canada's Walk of Fame in Toronto on June 5, 2005. A small park near Lee's Creek on Main Street in Cardston, Alberta, her birthplace, was named Fay Wray Park in her honour. The small sign at the edge of the park on Main Street has a silhouette of King Kong on it, remembering her role in the film "King Kong". A large oil portrait of Wray by Alberta artist Neil Boyle is on display in the Empress Theatre in Fort Macleod, Alberta. In May 2006, Wray became one of the first four entertainers to ever be honored by Canada Post by being featured on a postage stamp.

</doc>
<doc id="10967" url="https://en.wikipedia.org/wiki?curid=10967" title="Forgetting curve">
Forgetting curve

The forgetting curve hypothesizes the decline of memory retention in time. This curve shows how information is lost over time when there is no attempt to retain it. A related concept is the strength of memory that refers to the durability that memory traces in the brain. The stronger the memory, the longer period of time that a person is able to recall it. A typical graph of the forgetting curve purports to show that humans tend to halve their memory of newly learned knowledge in a matter of days or weeks unless they consciously review the learned material.
The forgetting curve supports one of the seven kinds of memory failures: transience, which is the process of forgetting that occurs with the passage of time.
History.
In 1885, Hermann Ebbinghaus extrapolated the hypothesis of the exponential nature of forgetting. The following formula can roughly describe it:
where formula_2 is memory retention, formula_3 is the relative strength of memory, and formula_4 is time.
Hermann Ebbinghaus ran a limited, incomplete study on himself and published his hypothesis in 1885 as "Über das Gedächtnis" (later translated into English as "Memory: A Contribution to Experimental Psychology"). Ebbinghaus studied the memorisation of nonsense syllables, such as "WID" and "ZOF" by repeatedly testing himself after various time periods and recording the results. He plotted these results on a graph creating what is now known as the "forgetting curve". From his discovery regarding the "forgetting curve", Ebbinghaus came up with the effects of "overlearning". Essentially, if you practiced something more than what is usually necessary to memorize it, you would have effectively achieved overlearning. Overlearning ensures that information is more impervious to being lost or forgotten, and the forgetting curve for this overlearned material is shallower.
Description.
Ebbinghaus hypothesized that the speed of forgetting depends on a number of factors such as the difficulty of the learned material (e.g. how meaningful it is), its representation and physiological factors such as stress and sleep. He further hypothesized that the basal forgetting rate differs little between individuals. He concluded that the difference in performance (e.g. at school) can be explained by mnemonic representation skills.
He went on to hypothesize that basic training in mnemonic techniques can help overcome those differences in part. He asserted that the best methods for increasing the strength of memory are:
His premise was that each repetition in learning increases the optimum interval before the next repetition is needed (for near-perfect retention, initial repetitions may need to be made within days, but later they can be made after years). Later research suggested that, other than the two factors Ebbinghaus proposed, higher original learning would also produce slower forgetting.
Spending time each day to remember information, such as that for exams, will greatly decrease the effects of the forgetting curve. Reviewing material in the first 24 hours after learning information is the optimum time to re-read notes and reduce the amount of knowledge forgotten.
Some memories remain free from the detrimental effects of interference and don’t necessarily follow the typical forgetting curve as various noise and outside factors influence what information would be remembered.
There is debate among supporters of the hypothesis about the shape of the curve for events and facts that are more significant to the subject. Some supporters, for example, suggest that memories of shocking events such as the Kennedy Assassination or 9/11 are vividly imprinted in memory (flashbulb memory). Others have compared contemporaneous written recollections with recollections recorded years later, and found considerable variations as the subject's memory incorporates after-acquired information. There is considerable research in this area as it relates to eyewitness identification testimony. It should be noted that eye witness accounts are demonstrably unreliable.

</doc>
<doc id="10969" url="https://en.wikipedia.org/wiki?curid=10969" title="Field-programmable gate array">
Field-programmable gate array

A field-programmable gate array (FPGA) is an integrated circuit designed to be configured by a customer or a designer after manufacturing hence "field-programmable". The FPGA configuration is generally specified using a hardware description language (HDL), similar to that used for an application-specific integrated circuit (ASIC). (Circuit diagrams were previously used to specify the configuration, as they were for ASICs, but this is increasingly rare.)
FPGAs contain an array of programmable logic blocks, and a hierarchy of reconfigurable interconnects that allow the blocks to be "wired together", like many logic gates that can be inter-wired in different configurations. Logic blocks can be configured to perform complex combinational functions, or merely simple logic gates like AND and XOR. In most FPGAs, logic blocks also include memory elements, which may be simple flip-flops or more complete blocks of memory.
Technical design.
Contemporary field-programmable gate arrays (FPGAs) have large resources of logic gates and RAM blocks to implement complex digital computations. As FPGA designs employ very fast I/Os and bidirectional data buses, it becomes a challenge to verify correct timing of valid data within setup time and hold time. Floor planning enables resources allocation within FPGAs to meet these time constraints. FPGAs can be used to implement any logical function that an ASIC could perform. The ability to update the functionality after shipping, partial re-configuration of a portion of the design and the low non-recurring engineering costs relative to an ASIC design (notwithstanding the generally higher unit cost), offer advantages for many applications.
Some FPGAs have analog features in addition to digital functions. The most common analog feature is programmable slew rate on each output pin,
allowing the engineer to set low rates on lightly loaded pins that would otherwise ring or couple unacceptably, and to set higher rates on heavily loaded pins on high-speed channels that would otherwise run too slowly.
Quartz-crystal oscillators, on-chip resistance-capacitance oscillators and phase-locked loops with embedded voltage-controlled oscillators used for clock generation and management and for high-speed serializer-deserializer (SERDES) transmit clocks and receiver clock recovery are common analog features. Another relatively common analog feature is differential comparators on input pins designed to be connected to differential signaling channels.
A few "mixed signal FPGAs" have integrated peripheral analog-to-digital converters (ADCs) and digital-to-analog converters (DACs) with analog signal conditioning blocks allowing them to operate as a system-on-a-chip.
Such devices blur the line between an FPGA, which carries digital ones and zeros on its internal programmable interconnect fabric,
and field-programmable analog array (FPAA), which carries analog values on its internal programmable interconnect fabric.
History.
The FPGA industry sprouted from programmable read-only memory (PROM) and programmable logic devices (PLDs). PROMs and PLDs both had the option of being programmed in batches in a factory or in the field (field-programmable). However, programmable logic was hard-wired between logic gates.
In the late 1980s, the Naval Surface Warfare Center funded an experiment proposed by Steve Casselman to develop a computer that would implement 600,000 reprogrammable gates. Casselman was successful and a patent related to the system was issued in 1992.
Some of the industry's foundational concepts and technologies for programmable logic arrays, gates, and logic blocks are founded in patents awarded to David W. Page and LuVerne R. Peterson in 1985.
Altera was founded in 1983 and delivered the industry’s first reprogrammable logic device in 1984 – the EP300 – which featured a quartz window in the package that allowed users to shine an ultra-violet lamp on the die to erase the EPROM cells that held the device configuration.
Xilinx co-founders Ross Freeman and Bernard Vonderschmitt invented the first commercially viable field-programmable gate array in 1985 – the XC2064. The XC2064 had programmable gates and programmable interconnects between gates, the beginnings of a new technology and market. The XC2064 had 64 configurable logic blocks (CLBs), with two three-input lookup tables (LUTs). More than 20 years later, Freeman was entered into the National Inventors Hall of Fame for his invention.
Altera and Xilinx continued unchallenged and quickly grew from 1985 to the mid-1990s, when competitors sprouted up, eroding significant market share. By 1993, Actel (now Microsemi) was serving about 18 percent of the market. By 2010, Altera (31 percent), Actel (10 percent) and Xilinx (36 percent) together represented approximately 77 percent of the FPGA market.
The 1990s were an explosive period of time for FPGAs, both in sophistication and the volume of production. In the early 1990s, FPGAs were primarily used in telecommunications and networking. By the end of the decade, FPGAs found their way into consumer, automotive, and industrial applications.
Modern developments.
A recent trend has been to take the coarse-grained architectural approach a step further by combining the logic blocks and interconnects of traditional FPGAs with embedded microprocessors and related peripherals to form a complete "system on a programmable chip". This work mirrors the architecture by Ron Perlof and Hana Potash of Burroughs Advanced Systems Group which combined a reconfigurable CPU architecture on a single chip called the SB24. That work was done in 1982. Examples of such hybrid technologies can be found in the Xilinx Zynq-7000 All Programmable SoC, which includes a 1.0 GHz dual-core ARM Cortex-A9 MPCore processor embedded within the FPGA's logic fabric or in the Altera Arria V FPGA, which includes an 800 MHz dual-core ARM Cortex-A9 MPCore. The Atmel FPSLIC is another such device, which uses an AVR processor in combination with Atmel's programmable logic architecture. The Microsemi SmartFusion devices incorporate an ARM Cortex-M3 hard processor core (with up to 512 kB of flash and 64 kB of RAM) and analog peripherals such as a multi-channel ADC and DACs to their flash-based FPGA fabric.
In 2010, Xilinx Inc introduced the first All Programmable System on a Chip branded Zynq™-7000 that fused features of an ARM high-end microcontroller (hard-core implementations of a 32-bit processor, memory, and I/O) with an 28 nm FPGA fabric to make it easier for embedded designers to use. The extensible processing platform enables system architects and embedded software developers to apply a combination of serial and parallel processing to their embedded system designs, for which the general trend has been to progressively increasing complexity. The high level of integration helps to reduce power consumption and dissipation, and the reduced parts count versus using an FPGA with a separate CPU chip leads to a lower parts cost, a smaller system, and higher reliability since most failures in modern electronics occur on PCBs in the connections between chips instead of within the chips themselves.
An alternate approach to using hard-macro processors is to make use of soft processor cores that are implemented within the FPGA logic. Nios II, MicroBlaze and Mico32 are examples of popular softcore processors.
As previously mentioned, many modern FPGAs have the ability to be reprogrammed at "run time", and this is leading to the idea of reconfigurable computing or reconfigurable systems – CPUs that reconfigure themselves to suit the task at hand.
Additionally, new, non-FPGA architectures are beginning to emerge. Software-configurable microprocessors such as the Stretch S5000 adopt a hybrid approach by providing an array of processor cores and FPGA-like programmable cores on the same chip.
Companies like Microsoft have started to use FPGA to accelerate high-performance, computationally intensive systems (like the data centers that operate their Bing search engine), due to the performance per Watt advantage FPGAs deliver.
FPGA comparisons.
Historically, FPGAs have been slower, less energy efficient and generally achieved less functionality than their fixed ASIC counterparts. An older study had shown that designs implemented on FPGAs need on average 40 times as much area, draw 12 times as much dynamic power, and run at one third the speed of corresponding ASIC implementations. More recently, FPGAs such as the Xilinx Virtex-7 or the Altera Stratix 5 have come to rival corresponding ASIC and ASSP solutions by providing significantly reduced power, increased speed, lower materials cost, minimal implementation real-estate, and increased possibilities for re-configuration 'on-the-fly'. Where previously a design may have included 6 to 10 ASICs, the same design can now be achieved using only one FPGA.
Advantages of FPGAs include the ability to re-program in the field to fix bugs, and may include a shorter time to market and lower non-recurring engineering costs. Vendors can also take a middle road by developing their hardware on ordinary FPGAs, but manufacture their final version as an ASIC so that it can no longer be modified after the design has been committed.
Xilinx claims that several market and technology dynamics are changing the ASIC/FPGA paradigm:
These trends make FPGAs a better alternative than ASICs for a larger number of higher-volume applications than they have been historically used for, to which the company attributes the growing number of FPGA design starts (see History).
Some FPGAs have the capability of partial re-configuration that lets one portion of the device be re-programmed while other portions continue running.
Complex programmable logic devices (CPLD).
The primary differences between CPLDs (complex programmable logic devices) and FPGAs are architectural. A CPLD has a somewhat restrictive structure consisting of one or more programmable sum-of-products logic arrays feeding a relatively small number of clocked registers. The result of this is less flexibility, with the advantage of more predictable timing delays and a higher logic-to-interconnect ratio. The FPGA architectures, on the other hand, are dominated by interconnect. This makes them far more flexible (in terms of the range of designs that are practical for implementation within them) but also far more complex to design for.
In practice, the distinction between FPGAs and CPLDs is often one of size as FPGAs are usually much larger in terms of resources than CPLDs. Typically only FPGAs contain more complex embedded functions such as adders, multipliers, memory, and serdes. Another common distinction is that CPLDs contain embedded flash to store their configuration while FPGAs usually, but not always, require external nonvolatile memory.
Security considerations.
With respect to security, FPGAs have both advantages and disadvantages as compared to ASICs or secure microprocessors. FPGAs' flexibility makes malicious modifications during fabrication a lower risk. Previously, for many FPGAs, the design bitstream was exposed while the FPGA loads it from external memory (typically on every power-on). All major FPGA vendors now offer a spectrum of security solutions to designers such as bitstream encryption and authentication. For example, Altera and Xilinx offer AES (up to 256 bit) encryption for bitstreams stored in an external flash memory.
FPGAs that store their configuration internally in nonvolatile flash memory, such as Microsemi's ProAsic 3 or Lattice's XP2 programmable devices, do not expose the bitstream and do not need encryption. In addition, flash memory for a lookup table provides single event upset protection for space applications.. Customers wanting a higher guarantee of tamper resistance can use write-once, Antifuse FPGAs from vendors such as Microsemi.
With its Stratix 10 FPGAs and SoCs, Altera introduced a Secure Device Manager and physically uncloneable functions to provide high levels of protection against physical attacks.
Applications.
An FPGA can be used to solve any problem which is computable. This is trivially proven by the fact FPGA can be used to implement a soft microprocessor, such as the Xilinx MicroBlaze or Altera Nios II. Their advantage lies in that they are sometimes significantly faster for some applications because of their parallel nature and optimality in terms of the number of gates used for a certain process.
Specific applications of FPGAs include digital signal processing, software-defined radio, ASIC prototyping, medical imaging, computer vision, speech recognition, cryptography, bioinformatics, computer hardware emulation, radio astronomy, metal detection and a growing range of other areas.
FPGAs originally began as competitors to CPLDs and competed in a similar space, that of glue logic for PCBs. As their size, capabilities, and speed increased, they began to take over larger and larger functions to the point where some are now marketed as full systems on chips (SoC). Particularly with the introduction of dedicated multipliers into FPGA architectures in the late 1990s, applications which had traditionally been the sole reserve of DSPs began to incorporate FPGAs instead.
Another trend on the usage of FPGAs is hardware acceleration, where one can use the FPGA to accelerate certain parts of an algorithm and share part of the computation between the FPGA and a generic processor.
Traditionally, FPGAs have been reserved for specific vertical applications where the volume of production is small. For these low-volume applications, the premium that companies pay in hardware costs per unit for a programmable chip is more affordable than the development resources spent on creating an ASIC for a low-volume application. Today, new cost and performance dynamics have broadened the range of viable applications.
Common FPGA Applications:
Architecture.
Logic blocks.
The most common FPGA architecture consists of an array of logic blocks (called Configurable Logic Block, CLB, or Logic Array Block, LAB, depending on vendor), I/O pads, and routing channels. Generally, all the routing channels have the same width (number of wires). Multiple I/O pads may fit into the height of one row or the width of one column in the array.
An application circuit must be mapped into an FPGA with adequate resources. While the number of CLBs/LABs and I/Os required is easily determined from the design, the number of routing tracks needed may vary considerably even among designs with the same amount of logic. For example, a crossbar switch requires much more routing than a systolic array with the same gate count. Since unused routing tracks increase the cost (and decrease the performance) of the part without providing any benefit, FPGA manufacturers try to provide just enough tracks so that most designs that will fit in terms of Lookup tables (LUTs) and I/Os can be routed. This is determined by estimates such as those derived from Rent's rule or by experiments with existing designs.
In general, a logic block (CLB or LAB) consists of a few logical cells (called ALM, LE, Slice etc.). A typical cell consists of a 4-input LUT, a Full adder (FA) and a D-type flip-flop, as shown below. The LUTs are in this figure split into two 3-input LUTs. In "normal mode" those are combined into a 4-input LUT through the left mux. In "arithmetic" mode, their outputs are fed to the FA. The selection of mode is programmed into the middle multiplexer. The output can be either synchronous or asynchronous, depending on the programming of the mux to the right, in the figure example. In practice, entire or parts of the FA are put as functions into the LUTs in order to save space.
Hard blocks.
Modern FPGA families expand upon the above capabilities to include higher level functionality fixed into the silicon. Having these common functions embedded into the silicon reduces the area required and gives those functions increased speed compared to building them from primitives. Examples of these include multipliers, generic DSP blocks, embedded processors, high speed I/O logic and embedded memories.
Higher-end FPGAs can contain high speed multi-gigabit transceivers and "hard IP cores" such as processor cores, Ethernet MACs, PCI/PCI Express controllers, and external memory controllers. These cores exist alongside the programmable fabric, but they are built out of transistors instead of LUTs so they have ASIC level performance and power consumption while not consuming a significant amount of fabric resources, leaving more of the fabric free for the application-specific logic. The multi-gigabit transceivers also contain high performance analog input and output circuitry along with high-speed serializers and deserializers, components which cannot be built out of LUTs. Higher-level PHY layer functionality such as line coding may or may not be implemented alongside the serializers and deserializers in hard logic, depending on the FPGA.
Clocking.
Most of the circuitry built inside of an FPGA is synchronous circuitry that requires a clock signal. FPGAs contain dedicated global and regional routing networks for clock and reset so they can be delivered with minimal skew. Also, FPGAs generally contain analog PLL and/or DLL components to synthesize new clock frequencies as well as attenuate jitter. Complex designs can use multiple clocks with different frequency and phase relationships, each forming separate clock domains. These clock signals can be generated locally by an oscillator or they can be recovered from a high speed serial data stream. Care must be taken when building clock domain crossing circuitry to avoid metastability. FPGAs generally contain block RAMs that are capable of working as dual port RAMs with different clocks, aiding in the construction of building FIFOs and dual port buffers that connect differing clock domains.
3D architectures.
To shrink the size and power consumption of FPGAs, vendors such as Tabula and Xilinx have introduced new 3D or stacked architectures. Following the introduction of its 28 nm 7-series FPGAs, Xilinx revealed that several of the highest-density parts in those FPGA product lines will be constructed using multiple dies in one package, employing technology developed for 3D construction and stacked-die assemblies.
Xilinx’s approach stacks several (three or four) active FPGA die side-by-side on a silicon interposer – a single piece of silicon that carries passive interconnect. The multi-die construction also allows different parts of the FPGA to be created with different process technologies, as the process requirements are different between the FPGA fabric itself and the very high speed 28 Gbit/s serial transceivers. An FPGA built in this way is called a "heterogeneous FPGA".
Altera’s heterogeneous approach involves using a single monolithic FPGA die and connecting other die/technologies to the FPGA using Intel’s embedded multi-die interconnect bridge (EMIB) technology.
FPGA design and programming.
To define the behavior of the FPGA, the user provides a hardware description language (HDL) or a schematic design. The HDL form is more suited to work with large structures because it's possible to just specify them numerically rather than having to draw every piece by hand. However, schematic entry can allow for easier visualisation of a design.
Then, using an electronic design automation tool, a technology-mapped netlist is generated. The netlist can then be fitted to the actual FPGA architecture using a process called place-and-route, usually performed by the FPGA company's proprietary place-and-route software. The user will validate the map, place and route results via timing analysis, simulation, and other verification methodologies. Once the design and validation process is complete, the binary file generated (also using the FPGA company's proprietary software) is used to (re)configure the FPGA. This file is transferred to the FPGA/CPLD via a serial interface (JTAG) or to an external memory device like an EEPROM.
The most common HDLs are VHDL and Verilog, although in an attempt to reduce the complexity of designing in HDLs, which have been compared to the equivalent of assembly languages, there are moves to raise the abstraction level through the introduction of alternative languages. National Instruments' LabVIEW graphical programming language (sometimes referred to as "G") has an FPGA add-in module available to target and program FPGA hardware.
To simplify the design of complex systems in FPGAs, there exist libraries of predefined complex functions and circuits that have been tested and optimized to speed up the design process. These predefined circuits are commonly called "IP cores", and are available from FPGA vendors and third-party IP suppliers (rarely free, and typically released under proprietary licenses). Other predefined circuits are available from developer communities such as OpenCores (typically released under free and open source licenses such as the GPL, BSD or similar license), and other sources.
In a typical design flow, an FPGA application developer will simulate the design at multiple stages throughout the design process. Initially the RTL description in VHDL or Verilog is simulated by creating test benches to simulate the system and observe results. Then, after the synthesis engine has mapped the design to a netlist, the netlist is translated to a gate level description where simulation is repeated to confirm the synthesis proceeded without errors. Finally the design is laid out in the FPGA at which point propagation delays can be added and the simulation run again with these values back-annotated onto the netlist.
More recently, OpenCL is being used by programmers to take advantage of the performance and power efficiencies that FPGAs provide. OpenCL allows programmers to develop code in the C programming language and target FPGA functions as OpenCL kernels using OpenCL constructs.
Major manufacturers.
Xilinx and Altera are the current FPGA market leaders and long-time industry rivals. Together, they control over 80 percent of the market. Both Xilinx and Altera provide proprietary Windows and Linux design software (ISE/Vivado and Quartus) which enable engineers to design, analyse, simulate and synthesize (compile) their designs.
Other manufacturers include:
In March 2010, Tabula announced their FPGA technology that uses time-multiplexed logic and interconnect that claims potential cost savings for high-density applications. On March 24, 2015, Tabula officially shut down.
On June 1, 2015, Intel announced it would acquire Altera for approximately $16.7 billion and completed the acquisition on December 30, 2015.

</doc>
<doc id="10971" url="https://en.wikipedia.org/wiki?curid=10971" title="Free-running sleep">
Free-running sleep

Free-running sleep is a sleep pattern that is not adjusted, entrained, to the 24-hour cycle in nature nor to any artificial cycle. It occurs as the sleep disorder non-24-hour sleep-wake disorder or artificially as part of experiments used in the study of circadian and other rhythms in biology. Study subjects are shielded from all time cues, often by a constant light protocol, by a constant dark protocol or by the use of light/dark conditions to which the organism cannot entrain such as the ultrashort protocol of one hour dark and two hours light. Also, limited amounts of food can be made available at short intervals so as to avoid entrainment to mealtimes. Subjects are thus forced to live by their internal circadian "clocks".
Background.
The individual's or animal's circadian phase can be known only by the monitoring of some kind of output of the circadian system, the internal "body clock". The researcher can precisely determine, for example, the daily cycles of gene-activity, body temperature, blood pressure, hormone secretion and/or sleep and activity/alertness. Alertness in humans can be determined by many kinds of verbal and non-verbal tests; activity in animals by observation, for example of wheel-running in rodents.
When animals or people "free-run", experiments can be done to see what sort of signals, known as "zeitgebers", are effective in entrainment. Also, much work has been done to see how long or short a circadian cycle the different organisms can be entrained to. For example, some animals can be entrained to a 22-hour day, but they can not be entrained to a 20-hour day. In recent studies funded by the U.S. space industry, it has been shown that most humans can be entrained to a 23.5-hour day and to a 24.65-hour day.
The effect of unintended time cues is called "masking" and can totally confound experimental results. Examples of masking are morning rush traffic audible to the subjects and researchers or maintenance staff visiting subjects with a regular schedule.
In humans.
Non-24-hour sleep–wake disorder, also referred to as "free-running disorder" (FRD) or "Non-24", is one of the circadian rhythm sleep disorders in humans. It affects more than half of people who are totally blind and a smaller number of sighted individuals.
Among blind people, the cause is the inability to register, and therefore to entrain to, light cues. The many blind people who do entrain to the 24-hour light/dark cycle have eyes with functioning retinas including operative non-visual light-sensitive cells, ipRGCs. These ganglion cells, which contain melanopsin, convey their signals to the "circadian clock" via the retinohypothalamic tract (branching off from the optic nerve), linking the retina to the pineal gland.
Among sighted individuals, FRD usually first appears in the teens or early twenties. As with delayed sleep phase disorder (DSPS or DSPD), in the absence of neurological damage due to trauma or stroke, cases almost never appear after the age of 30. FRD affects more sighted males than sighted females. A quarter of sighted individuals with FRD also have an associated psychiatric condition, and a quarter of them have previously shown symptoms of DSPS.
The term "free-running sleep" has occasionally been used by non-scientists to indicate intentional facilitation of the natural sleep/wake cycle. In this context, free-running sleep means that a person chooses to sleep when sleepy and to awaken spontaneously (specifically without an alarm clock or reference to the time of day).

</doc>
<doc id="10972" url="https://en.wikipedia.org/wiki?curid=10972" title="Fenrir">
Fenrir

In Norse mythology, Fenrir (Old Norse: "fen-dweller"), Fenrisúlfr (Old Norse: "Fenris wolf"), Hróðvitnir (Old Norse: "fame-wolf"), or Vánagandr (Old Norse: "the monster of the river Ván") is a monstrous wolf. Fenrir is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda" and "Heimskringla", written in the 13th century by Snorri Sturluson. In both the "Poetic Edda" and "Prose Edda", Fenrir is the father of the wolves Sköll and Hati Hróðvitnisson, is a son of Loki, and is foretold to kill the god Odin during the events of Ragnarök, but will in turn be killed by Odin's son Víðarr.
In the "Prose Edda", additional information is given about Fenrir, including that, due to the gods' knowledge of prophecies foretelling great trouble from Fenrir and his rapid growth, the gods bound him, and as a result Fenrir bit off the right hand of the god Týr. Depictions of Fenrir have been identified on various objects, and scholarly theories have been proposed regarding Fenrir's relation to other canine beings in Norse mythology. Fenrir has been the subject of artistic depictions, and he appears in literature.
Attestations.
"Poetic Edda".
Fenrir is mentioned in three stanzas of the poem "Völuspá", and in two stanzas of the poem "Vafþrúðnismál". In stanza 40 of the poem "Völuspá", a völva divulges to Odin that, in the east, an old woman sat in the forest Járnviðr, "and bred there the broods of Fenrir. There will come from them all one of that number to be a moon-snatcher in troll's skin." Further into the poem, the völva foretells that Odin will be consumed by Fenrir at Ragnarök:
In the stanza that follows, the völva describes that Odin's "tall child of Triumph's Sire" (Odin's son Víðarr) will then come to "strike at the beast of slaughter," and with his hands, he will drive a sword into the heart of "Hveðrungr's son," avenging the death of his father.
In the first of two stanzas mentioning Fenrir in "Vafþrúðnismál", Odin poses a question to the wise jötunn Vafþrúðnir:
In the stanza that follows, Vafþrúðnir responds that Sól (here referred to as "Álfröðull"), will bear a daughter before Fenrir attacks her, and that this daughter shall continue the paths of her deceased mother through the heavens.
"Prose Edda".
In the "Prose Edda", Fenrir is mentioned in three books: "Gylfaginning", "Skáldskaparmál" and "Háttatal".
"Gylfaginning" chapters 13 and 25.
In chapter 13 of the "Prose Edda" book "Gylfaginning", Fenrir is first mentioned in a stanza quoted from "Völuspá". Fenrir is first mentioned in prose in chapter 25, where the enthroned figure of High tells Gangleri (described as King Gylfi in disguise) about the god Týr. High says that one example of Týr's bravery is that when the Æsir were luring Fenrir (referred to here as "Fenrisúlfr") to place the fetter Gleipnir on the wolf, Týr placed his hand within the wolf's mouth as a pledge. This was done at Fenrir's own request because he did not trust that the Æsir would let him go. As a result, when the Æsir refused to release him, he bit off Týr's hand at a location "now called the wolf-joint" (the wrist), causing Týr to be one-handed and "not considered to be a promoter of settlements between people."
"Gylfaginning" chapter 34.
In chapter 34, High describes Loki, and says that Loki had three children with a female named Angrboða located in the land of Jötunheimr; Fenrisúlfr, the serpent Jörmungandr, and the female being Hel. High continues that, once the gods found that these three children were being brought up in the land of Jötunheimr, and when the gods "traced prophecies that from these siblings great mischief and disaster would arise for them" the gods expected a lot of trouble from the three children, partially due to the nature of the mother of the children, yet worse so due to the nature of their father.
High says that Odin sent the gods to gather the children and bring them to him. Upon their arrival, Odin threw Jörmungandr into "that deep sea that lies round all lands", and then threw Hel into Niflheim, and bestowed upon her authority over nine worlds. However, the Æsir brought up the wolf "at home", and only Týr had the courage to approach Fenrir, and give Fenrir food. The gods noticed that Fenrir was growing rapidly every day, and since all prophecies foretold that Fenrir was destined to cause them harm, the gods formed a plan. The gods prepared three fetters: The first, greatly strong, was called Leyding. They brought Leyding to Fenrir and suggested that the wolf try his strength with it. Fenrir judged that it was not beyond his strength, and so let the gods do what they wanted with it. At Fenrir's first kick the bind snapped, and Fenrir loosened himself from Leyding. The gods made a second fetter, twice as strong, and named it Dromi. The gods asked Fenrir to try the new fetter, and that should he break this feat of engineering, Fenrir would achieve great fame for his strength. Fenrir considered that the fetter was very strong, yet also that his strength had grown since he broke Leyding, yet that he would have to take some risks if he were to become famous. Fenrir allowed them to place the fetter.
When the Æsir exclaimed that they were ready, Fenrir shook himself, knocked the fetter to the ground, strained hard, and kicking with his feet, snapped the fetter – breaking it into pieces that flew far into the distance. High says that, as a result, to "loose from Leyding" or to "strike out of Dromi" have become sayings for when something is achieved with great effort. The Æsir started to fear that they would not be able to bind Fenrir, and so Odin sent Freyr's messenger Skírnir down into the land of Svartálfaheimr to "some dwarfs" and had them make a fetter called Gleipnir. The dwarves constructed Gleipnir from six mythical ingredients. After an exchange between Gangleri and High, High continues that the fetter was smooth and soft as a silken ribbon, yet strong and firm. The messenger brought the ribbon to the Æsir, and they thanked him heartily for completing the task.
The Æsir went out on to the lake Amsvartnir sent for Fenrir to accompany them, and continued to the island Lyngvi (Old Norse "a place overgrown with heather"). The gods showed Fenrir the silken fetter Gleipnir, told him to tear it, stated that it was much stronger than it appeared, passed it among themselves, used their hands to pull it, and yet it did not tear. However, they said that Fenrir would be able to tear it, to which Fenrir replied:
"It looks to me that with this ribbon as though I will gain no fame from it if I do tear apart such a slender band, but if it is made with art and trickery, then even if it does look thin, this band is not going on my legs."
The Æsir said Fenrir would quickly tear apart a thin silken strip, noting that Fenrir earlier broke great iron binds, and added that if Fenrir wasn't able to break slender Gleipnir then Fenrir is nothing for the gods to fear, and as a result would be freed. Fenrir responded:
"If you bind me so that I am unable to release myself, then you will be standing by in such a way that I should have to wait a long time before I got any help from you. I am reluctant to have this band put on me. But rather than that you question my courage, let someone put his hand in my mouth as a pledge that this is done in good faith."
With this statement, all of the Æsir look to one another, finding themselves in a dilemma. Everyone refused to place their hand in Fenrir's mouth until Týr put out his right hand and placed it into the wolf's jaws. When Fenrir kicked, Gleipnir caught tightly, and the more Fenrir struggled, the stronger the band grew. At this, everyone laughed, except Týr, who there lost his right hand. When the gods knew that Fenrir was fully bound, they took a cord called Gelgja (Old Norse "fetter") hanging from Gleipnir, inserted the cord through a large stone slab called Gjöll (Old Norse "scream"), and the gods fastened the stone slab deep into the ground. After, the gods took a great rock called Thviti (Old Norse "hitter, batterer"), and thrust it even further into the ground as an anchoring peg. Fenrir reacted violently; he opened his jaws very wide, and tried to bite the gods. 
Then the gods thrust a sword into his mouth. Its hilt touched the lower jaw and its point the upper one; by means of it the jaws of the wolf were spread apart and the wolf gagged.
Fenrir "howled horribly," saliva ran from his mouth, and this saliva formed the river Ván (Old Norse "hope"). There Fenrir will lie until Ragnarök. Gangleri comments that Loki created a "pretty terrible family" though important, and asks why the Æsir did not just kill Fenrir there since they expected great malice from him. High replies that "so greatly did the gods respect their holy places and places of sanctuary that they did not want to defile them with the wolf's blood even though the prophecies say that he will be the death of Odin."
"Gylfaginning" chapters 38 and 51.
In chapter 38, High says that there are many men in Valhalla, and many more who will arrive, yet they will "seem too few when the wolf comes." In chapter 51, High foretells that as part of the events of Ragnarök, after Fenrir's son Sköll has swallowed the sun and his other son Hati Hróðvitnisson has swallowed the moon, the stars will disappear from the sky. The earth will shake violently, trees will be uprooted, mountains will fall, and all binds will snap – Fenrisúlfr will be free. Fenrisúlfr will go forth with his mouth opened wide, his upper jaw touching the sky and his lower jaw the earth, and flames will burn from his eyes and nostrils. Later, Fenrisúlfr will arrive at the field Vígríðr with his brother Jörmungandr. With the forces assembled there, an immense battle will take place. During this, Odin will ride to fight Fenrisúlfr. During the battle, Fenrisúlfr will eventually swallow Odin, killing him, and Odin's son Víðarr will move forward and kick one foot into the lower jaw of the wolf. This foot will bear a legendary shoe "for which the material has been collected throughout all time." With one hand, Víðarr will take hold of the wolf's upper jaw and tear apart his mouth, killing Fenrisúlfr. High follows this prose description by citing various quotes from "Völuspá" in support, some of which mention Fenrir.
"Skáldskaparmál" and "Háttatal".
In the Epilogue section of the "Prose Edda" book "Skáldskaparmál", a euhemerized monologue equates Fenrisúlfr to Pyrrhus, attempting to rationalize that "it killed Odin, and Pyrrhus could be said to be a wolf according to their religion, for he paid no respect to places of sanctuary when he killed the king in the temple in front of Thor's altar." In chapter 2, "wolf's enemy" is cited as a kenning for Odin as used by the 10th century skald Egill Skallagrímsson. In chapter 9, "feeder of the wolf" is given as a kenning for Týr and, in chapter 11, "slayer of Fenrisúlfr" is presented as a kenning for Víðarr. In chapter 50, a section of "Ragnarsdrápa" by the 9th century skald Bragi Boddason is quoted that refers to Hel, the being, as "the monstrous wolf's sister." In chapter 75, names for wargs and wolves are listed, including both "Hróðvitnir" and "Fenrir."
"Fenrir" appears twice in verse as a common noun for a "wolf" or "warg" in chapter 58 of "Skáldskaparmál", and in chapter 56 of the book "Háttatal". Additionally, the name "Fenrir" can be found among a list of jötnar in chapter 75 of "Skáldskaparmál".
"Heimskringla".
At the end of the "Heimskringla" saga "Hákonar saga góða", the poem "Hákonarmál" by the 10th century skald Eyvindr skáldaspillir is presented. The poem is about the fall of King Haakon I of Norway; although he is Christian, he is taken by two valkyries to Valhalla, and is there received as one of the Einherjar. Towards the end of the poem, a stanza relates sooner will the bonds of Fenrir snap than as good a king as Haakon shall stand in his place:
Archaeological record.
Thorwald's Cross.
, a partially surviving runestone erected at Kirk Andreas on the Isle of Man, depicts a bearded human holding a spear downward at a wolf, his right foot in its mouth, while a large bird sits at his shoulder. Rundata dates it to 940, while Pluskowski dates it to the 11th century. This depiction has been interpreted as Odin, with a raven or eagle at his shoulder, being consumed by Fenrir at Ragnarök. On the reverse of the stone is another image parallel to it that has been described as Christ triumphing over Satan. These combined elements have led to the cross as being described as "syncretic art"; a mixture of pagan and Christian beliefs.
Gosforth Cross.
The mid-11th century Gosforth Cross, located in Cumbria, England, has been described as depicting a combination of scenes from the Christian Judgement Day and the pagan Ragnarök. The cross features various figures depicted in Borre style, including a man with a spear facing a monstrous head, one of whose feet is thrust into the beast's forked tongue and on its lower jaw, while a hand is placed against its upper jaw, a scene interpreted as Víðarr fighting Fenrir. This depiction has been theorized as a metaphor for Christ's defeat of Satan.
Ledberg stone.
The 11th century Ledberg stone in Sweden, similarly to Thorwald's Cross, features a figure with his foot at the mouth of a four-legged beast, and this may also be a depiction of Odin being devoured by Fenrir at Ragnarök. Below the beast and the man is a depiction of a legless, helmeted man, with his arms in a prostrate position. The Younger Futhark inscription on the stone bears a commonly seen memorial dedication, but is followed by an encoded runic sequence that has been described as "mysterious," and "an interesting magic formula which is known from all over the ancient Norse world."
Other.
If the images on the Tullstorp Runestone are correctly identified as depicting Ragnarök, then Fenrir is shown above the ship Naglfar.
Meyer Schapiro theorizes a connection between the "Hell Mouth" that appears in medieval Christian iconography and Fenrir. According to Schapiro, "the Anglo-Saxon taste for the Hell Mouth was perhaps influenced by the northern pagan myth of the Crack of Doom and the battle with the wolf, who devoured Odin."
Theories.
In reference to Fenrir's presentation in the "Prose Edda", Andy Orchard theorizes that "the hound (or wolf)" Garmr, Sköll, and Hati Hróðvitnisson were originally simply all Fenrir, stating that "Snorri, characteristically, is careful to make distinctions, naming the wolves who devour the sun and moon as Sköll and Hati Hróðvitnisson respectively, and describing an encounter between Garm and Týr (who, one would have thought, might like to get his hand on Fenrir) at Ragnarök."
John Lindow says that it is unclear why the gods decide to raise Fenrir as opposed to his siblings Hel and Jörmungandr in "Gylfaginning" chapter 35, theorizing that it may be "because Odin had a connection with wolves? Because Loki was Odin's blood brother?" Referring to the same chapter, Lindow comments that neither of the phrases that Fenrir's binding result in have left any other traces. Lindow compares Fenrir's role to his father Loki and Fenrir's brother Jörmungandr, in that they all spend time with the gods, are bound or cast out by them, return "at the end of the current mythic order to destroy them, only to be destroyed himself as a younger generation of gods, one of them his slayer, survives into the new world order."
Indo-European parallels have been proposed between myths of Fenrir and the Persian demon Ahriman. The Yashts refer to a story where Taxma Urupi rode Angra Mainyu as a horse for thirty years. An elaboration of this allusion is found only in a late Parsi commentary. The ruler Taxmoruw (Taxma Urupi) managed to lasso Ahriman (Angra Mainyu) and keep him tied up while taking him for a ride three times a day. After thirty years Ahriman outwitted and swallowed Taxmoruw. In a sexual encounter with Ahriman, Jamshid, Taxmoruw's brother, inserted his hand into Ahriman's anus and pulled out his brother's corpse. His hand withered from contact with the diabolic innards. The suggested parallels with Fenrir myths are the binding of an evil being by a ruler figure and the subsequent swallowing of the ruler figure by the evil being (Odin and Fenrir), trickery involving the thrusting of a hand into a monster's orifice and the affliction of the inserted limb (Týr and Fenrir).
Ethologist Valerius Geist wrote that Fenrir's maiming and ultimate killing of Odin, who had previously nurtured him, was likely based on true experiences of wolf-behaviour, seeing as wolves are genetically encoded to rise up the pack hierarchy and have on occasion been recorded to rebel against and kill their parents. Geist states that "apparently, even the ancients knew that wolves may turn on their parents and siblings and kill them."
Modern influence.
Fenrir has been depicted in the artwork "Odin and Fenris" (1909) and "The Binding of Fenris" (around 1900) by Dorothy Hardy, "Odin und Fenriswolf" and "Fesselung des Fenriswolfe" (1901) by Emil Doepler, and is the subject of the metal sculpture "Fenrir" by A. V. Gunnerud located on the island of Askøy, Norway.
Fenrir appears in modern literature in the poem "Om Fenrisulven og Tyr" (1819) by Adam Gottlob Oehlenschläger (collected in "Nordens Guder"), the novel "Der Fenriswolf" by K. H. Strobl, and "Til kamp mod dødbideriet" (1974) by E. K. Reich and E. Larsen.

</doc>
