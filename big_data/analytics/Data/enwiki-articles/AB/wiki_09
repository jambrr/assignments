<doc id="7200" url="https://en.wikipedia.org/wiki?curid=7200" title="Class action">
Class action

A class action, class suit, or representative action is a type of lawsuit where one of the parties is a group of people who are represented collectively by a member of that group. The class action originated in the United States and is still predominantly a U.S. phenomenon, but several European countries with civil law, have made changes in recent years to allow consumer organizations to bring claims on behalf of consumers.
Description.
In a typical class action, a plaintiff sues a defendant or a number of defendants on behalf of a group, or class, of absent parties. This differs from a traditional lawsuit, where one party sues another party for redress of a wrong, and all of the parties are present in court. Although standards differ between states and countries, class actions are most common where the allegations involve a large number of people who have been injured by the same defendant in the same way. Instead of each injured person bringing their own lawsuit, the class action allows all the claims of all class members—whether they know they have been injured or not—to be resolved in a single proceeding.
History.
Class actions in English courts.
The ancestor of the class action was what modern observers call "group litigation", which appears to have been quite common in medieval England from about 1200 onward. These lawsuits involved groups of people either suing or being sued in actions at common law. These groups were usually based on existing societal structures like villages, towns, parishes, and guilds. Unlike modern courts, the medieval English courts did not question the right of the actual plaintiffs to sue on behalf of a group or a few representatives to defend an entire group.
From 1400 to 1700, group litigation gradually switched from being the norm in England to the exception. The development of the concept of the corporation led to the wealthy supporters of the corporate form becoming suspicious of all unincorporated legal entities, which in turn led to the modern concept of the unincorporated or voluntary association. The tumultuous history of the Wars of the Roses and then the Star Chamber resulted in periods during which the common law courts were frequently paralyzed, and out of the confusion the Court of Chancery emerged with exclusive jurisdiction over group litigation. By 1850, Parliament had enacted several statutes on a case-by-case basis to deal with issues regularly faced by certain types of organizations, like joint-stock companies, and with the impetus for most types of group litigation removed, it went into a steep decline in English jurisprudence from which it never recovered. It was further weakened by the fact that equity pleading in general was falling into disfavor, which culminated in the Judicature Acts of 1874 and 1875. Group litigation was essentially dead in England after 1850.
Early class actions in the United States.
Class actions survived in the United States thanks to the influence of Supreme Court Associate Justice Joseph Story, who imported it into U.S. law through summary discussions in his two equity treatises as well as his opinion in "West v. Randall" (1820). However, Story did not necessarily endorse class actions, because he "could not conceive of a modern function or a coherent theory for representative litigation."
The oldest predecessor to the class action rule in the United States was Equity Rule 48, promulgated in 1833, which allowed for representative suits in situations where there were too many individual parties (which now forms the first requirement for class action litigation, numerosity). However, this rule did not allow such suits to bind similarly situated absent parties, which rendered the rule ineffective. Within ten years, the Supreme Court interpreted Rule 48 in such a way so that it could apply to absent parties under certain circumstances, but only by ignoring the plain meaning of the rule. In the early 20th century, Equity Rule 48 was replaced with Equity Rule 38 as part of a major restructuring of the Equity Rules, and when federal courts merged their legal and equitable procedural systems in 1938, Equity Rule 38 became Rule 23 of the Federal Rules of Civil Procedure.
1966 revisions to Rule 23 and the birth of the modern class action.
A major revision of the FRCP in 1966 radically transformed Rule 23, made the opt-out class action the standard option, and gave birth to the modern class action. Entire treatises have been written since to summarize the huge mass of law that sprang up from the 1966 revision of Rule 23. Just as medieval group litigation bound all members of the group regardless of whether they all actually appeared in court, the modern class action binds "all" members of the class, except for those who choose to opt out (if the rules permit them to do so).
The Advisory Committee that drafted the new Rule 23 in the mid-1960s was influenced by two major developments. First was the suggestion of Harry Kalven, Jr. and Maurice Rosenfeld in 1941 that class action litigation by individual shareholders on behalf of all shareholders of a company could effectively supplement direct government regulation of securities markets and other similar markets. The second development was the rise of the African-American civil rights movement, environmentalism, and consumerism. The groups behind these movements, as well as many others in the 1960s, 1970s, and 1980s, all turned to class actions as a means for achieving their goals. For example, a 1978 environmental law treatise reprinted the "entire" text of Rule 23 and mentioned "class actions" 14 times in its index.
Businesses targeted by class actions for inflicting massive aggregate harm have sought ways to avoid class actions altogether. In the 1990s, the U.S. Supreme Court issued a number of decisions which strengthened the "federal policy favoring arbitration". In response, lawyers have added provisions to consumer contracts of adhesion called "collective action waivers", which prohibit those signing the contracts from bringing class action suits. In the 2011 court case "AT&T Mobility v. Concepcion", the U.S. Supreme Court ruled in a 5–4 decision that the Federal Arbitration Act of 1925 preempts state laws that prohibit contracts from disallowing class action lawsuits, which will make it more difficult for consumers to file class action lawsuits. The dissent pointed to a saving clause in the federal act which allowed states to determine how a contract or its clauses may be revoked.
In two major cases in the 21st century, the United States Supreme Court ruled 5-4 against certification of class actions due to differences in each individual members' circumstances; first in the 2011 case Wal-Mart v. Dukes and later in the 2013 case Comcast Corp. v. Behrend.
Companies may insert the phrase “may elect to resolve any claim by individual arbitration.” into their consumer and employment contracts to use arbitration and prevent class action lawsuits.
Modern class actions in the United States.
Federal courts.
In the United States federal courts, class actions are governed by Federal Rules of Civil Procedure Rule and 28 U.S.C.A. § 1332(d). Cases in federal courts are only allowed to proceed as class actions if the court has jurisdiction to hear the case, and if the case meets the criteria set out in Rule 23. In the vast majority of federal class actions, the class is acting as the plaintiff. However, Rule 23 also provides for defendant class actions.
Typically, federal courts are thought to be more favorable for defendants, and state courts more favorable for plaintiffs. Many class actions are filed initially in state court. The defendant will frequently try to remove the case to federal court. The Class Action Fairness Act of 2005 increases defendants' ability to remove state cases to federal court by giving federal courts original jurisdiction for all class actions with damages exceeding $5,000,000 exclusive of interest and costs. It should be noted, however, that the Class Action Fairness Act contains carve-outs for "inter alia", shareholder class actions covered by the Private Securities Litigation Reform Act of 1995 and those concerning internal corporate governance issues (the latter typically being brought as shareholder derivative actions in the state courts of Delaware, the state of incorporation of most large corporations).
Jurisdiction.
Class actions may be brought in federal court if the claim arises under federal law or if the claim falls under 28 USCA § 1332(d). Under § 1332(d) (2) the federal district courts have original jurisdiction over any civil action where the amount in controversy exceeds $5,000,000 and
Nationwide plaintiff classes are possible, but such suits must have a commonality of issues across state lines. This may be difficult if the civil law in the various states lack significant commonalities. Large class actions brought in federal court frequently are consolidated for pre-trial purposes through the device of multidistrict litigation (MDL). It is also possible to bring class actions under state law, and in some cases the court may extend its jurisdiction to all the members of the class, including out of state (or even internationally) as the key element is the jurisdiction that the court has over the defendant.
Class certification Under Rule 23.
For the case to proceed as a class action and bind absent class members, the court must certify the class under Rule 23 on a motion from the party wishing to proceed on a class basis. For a class to be certified, the moving party must meet all of the criteria listed under Rule 23(a), and at least one of the criteria listed under Rule 23(b).
The 23(a) criteria are referred to as numerosity, commonality, typicality, and adequacy. Numerosity refers to the number of people in the class. To be certified, the class has to have enough members that simply adding each of them as a named party to the lawsuit would be impractical. There is no bright-line rule to determine numerosity, but classes with hundreds of members are generally deemed to be sufficiently numerous. To satisfy commonality, there must be a common question of law and fact such that "determination of its truth or falsity will resolve an issue that is central to the validity of each one of the claims in one stroke." The typicality requirement ensures that the claims or defenses of the named plaintiff are typical of those of everyone else in the class. Finally, adequacy requirement states that the named plaintiff must fairly and adequately represent the interests of the absent class members.
Notice and settlement.
Due process requires in most cases that notice describing the class action be sent, published, or broadcast to class members. As part of this notice procedure, there may have to be several notices, first a notice giving class members the opportunity to opt out of the class, i.e. if individuals wish to proceed with their own litigation they are entitled to do so, only to the extent that they give timely notice to the class counsel or the court that they are opting out. Second, if there is a settlement proposal, the court will usually direct the class counsel to send a settlement notice to all the members of the certified class, informing them of the details of the proposed settlement.
State courts.
Since 1938, many states have adopted rules similar to the FRCP. However, some states, like California, have civil procedure systems, which deviate significantly from the federal rules; the California Codes provide for four separate types of class actions. As a result, there are two separate treatises devoted solely to the complex topic of California class actions. Some states, such as Virginia, do not provide for any class actions, while others, such as New York, limit the types of claims that may be brought as class actions. In 2013, several class action lawsuits have been filed under both the federal Fair Debt Collection Practices Act and similar Michigan state laws against some of Michigan's largest medical providers and collection agencies which may have a dramatic impact on the collection of unpaid medical bills and the collection industry as a whole.
Statistics.
As of 2010, there was no publicly maintained list of nonsecurities class action settlements, although a securities class action database exists in the Stanford Law School Securities Class Action Clearinghouse and several for-profit companies maintain lists of the securities settlements. One study of federal settlements required the researcher to manually search databases of lawsuits for the relevant records, although state class actions were not included due to the difficulty in gathering the information. Another source of data is U.S. Bureau of Justice Statistics "Civil Justice Survey of State Courts", which in 2009 was published with an overview of 2005 statistics.
Advantages.
Class actions may offer a number of advantages because they aggregate a large number of individualized claims into one representational lawsuit.
First, aggregation can increase the efficiency of the legal process, and lower the costs of litigation. In cases with common questions of law and fact, aggregation of claims into a class action may avoid the necessity of repeating "days of the same witnesses, exhibits and issues from trial to trial." "Jenkins v. Raymark Indus. Inc.", 782 F.2d 468, 473 (5th Cir. 1986) (granting certification of a class action involving asbestos).
Second, a class action may overcome "the problem that small recoveries do not provide the incentive for any individual to bring a solo action prosecuting his or her rights." "Amchem Prods., Inc. v. Windsor", 521 U.S. 591, 617 (1997) (quoting "Mace v. Van Ru Credit Corp.", 109 F.3d 388, 344 (7th Cir. 1997)). "A class action solves this problem by aggregating the relatively paltry potential recoveries into something worth someone’s (usually an attorney’s) labor." "Amchem Prods., Inc.", 521 U.S. at 617 (quoting "Mace", 109 F.3d at 344). In other words, a class action ensures that a defendant who engages in widespread harmbut does so minimally against each individual plaintiffmust compensate those individuals for their injuries. For example, thousands of shareholders of a public company may have losses too small to justify separate lawsuits, but a class action can be brought efficiently on behalf of all shareholders. Perhaps even more important than compensation is that class treatment of claims may be the only way to impose the costs of wrongdoing on the wrongdoer, thus deterring future wrongdoing.
Third, class action cases may be brought to purposely change behavior of a class of which the defendant is a member. Landeros v. Flood was a landmark case used to purposefully change the behavior of doctors, and encourage them to report suspected child abuse. Otherwise, they would face the threat of civil action for damages in tort proximately flowing from the failure to report the suspected injuries. Previously, many physicians had remained reluctant to report cases of apparent child abuse, despite existing law that required it.
Fourth, in "limited fund" cases, a class action ensures that all plaintiffs receive relief and that early-filing plaintiffs do not raid the fund ("i.e.", the defendant) of all its assets before other plaintiffs may be compensated. "See" "Ortiz v. Fibreboard Corp.", 527 U.S. 815 (1999). A class action in such a situation centralizes all claims into one venue where a court can equitably divide the assets amongst all the plaintiffs if they win the case.
Finally, a class action avoids the situation where different court rulings could create "incompatible standards" of conduct for the defendant to follow. See Fed. R. Civ. P. 23(b)(1)(A). For example, a court might certify a case for class treatment where a number of individual bond-holders sue to determine whether they may convert their bonds to common stock. Refusing to litigate the case in one trial could result in different outcomes and inconsistent standards of conduct for the defendant corporation. Thus, courts will generally allow a class action in such a situation. "See, e.g., Van Gemert v. Boeing Co.", 259 F. Supp. 125 (S.D.N.Y. 1966).
Whether a class action is superior to individual litigation depends on the case and is determined by the judge's ruling on a motion for class certification. The Advisory Committee Note to Rule 23, for example, states that mass torts are ordinarily "not appropriate" for class treatment. Class treatment may not improve the efficiency of a mass tort because the claims frequently involve individualized issues of law and fact that will have to be re-tried on an individual basis. "See Castano v. Am. Tobacco Co.", 84 F.3d 734 (5th Cir. 1996) (rejecting nationwide class action against tobacco companies). Mass torts also involve high individual damage awards; thus, the absence of class treatment will not impede the ability of individual claimants to seek justice. "See id." Other cases, however, may be more conducive to class treatment.
The preamble to the Class Action Fairness Act of 2005, passed by the United States Congress, found:
Class-action lawsuits are an important and valuable part of the legal system when they permit the fair and efficient resolution of legitimate claims of numerous parties by allowing the claims to be aggregated into a single action against a defendant that has allegedly caused harm. 
Criticisms.
There are several criticisms of class actions. The preamble to the Class Action Fairness Act stated that some abusive class actions harmed class members with legitimate claims and defendants that have acted responsibly, adversely affected interstate commerce, and undermined public respect for the country's judicial system.
Class members often receive little or no benefit from class actions. Examples cited for this include large fees for the attorneys, while leaving class members with coupons or other awards of little or no value; unjustified awards are made to certain plaintiffs at the expense of other class members; and confusing notices are published that prevent class members from being able to fully understand and effectively exercise their rights.
For example, in the United States, class lawsuits sometimes bind all class members with a low settlement. These "coupon settlements" (which usually allow the plaintiffs to receive a small benefit such as a small check or a coupon for future services or products with the defendant company) are a way for a defendant to forestall major liability by precluding a large number of people from litigating their claims separately, to recover reasonable compensation for the damages. However, existing law requires judicial approval of all class action settlements, and in most cases class members are given a chance to opt out of class settlement, though class members, despite opt-out notices, may be unaware of their right to opt out because they did not receive the notice, did not read it, or did not understand it.
The Class Action Fairness Act of 2005 addresses these concerns. Coupon settlements may be scrutinized by an independent expert before judicial approval in order to ensure that the settlement will be of value to the class members (28 U.S.C.A. 1712(d)). Further, if the action provides for settlement in coupons, "the portion of any attorney’s fee award to class counsel that is attributable to the award of the coupons shall be based on the value to class members of the coupons that are redeemed." 28 U.S.C.A. 1712(a).
Ethics.
Class action cases present significant ethical challenges. Defendants can hold reverse auctions and any of several parties can engage in collusive settlement discussions. Subclasses may have interests that diverge greatly from the class, but may be treated the same. Proposed settlements could offer some groups (such as former customers) much greater benefits than others. In one paper presented at an ABA conference on class actions in 2007, authors commented that "competing cases can also provide opportunities for collusive settlement discussions and reverse auctions by defendants anxious to resolve their new exposure at the most economic cost." 
Defendant class action.
Although normally plaintiffs are the class, defendant class actions are also possible. For example, in 2005, the Roman Catholic Archdiocese of Portland in Oregon was sued as part of the Catholic priest sex-abuse scandal. All parishioners of the Archdiocese's churches were cited as a defendant class. This was done to include their assets (local churches) in any settlement. Where both the plaintiffs and the defendants have been organized into court-approved classes, the action is called a bilateral class action.
Mass actions.
In a class action, the plaintiff seeks court approval to litigate on behalf of a group of similarly situated persons. Not every plaintiff looks for, or could obtain, such approval. As a procedural alternative, plaintiff's counsel may attempt to sign up every similarly situated person that counsel can find as a client. Plaintiff's counsel can then join the claims of all of these persons in one complaint, a so-called "mass action", hoping to have the same efficiencies and economic leverage as if a class had been certified.
Because mass actions operate outside the detailed procedures laid out for class actions, they can pose special difficulties for both plaintiffs, defendants, and the court. For example, settlement of class actions follows a predictable path of negotiation with class counsel and representatives, court scrutiny, and notice. There may not be a way to uniformly settle all of the many claims brought via a mass action. Some states permit plaintiff's counsel to settle for all the mass action plaintiffs according to a majority vote, for example. Other states, such as New Jersey, require each plaintiff to approve the settlement of that plaintiff's own individual claims.
Class actions outside the United States.
Argentina.
Class actions were recognized in "Halabi" leading case (Supreme Court, 2009).
Australia and New Zealand.
Class actions became part of the Australian legal landscape only when the Federal Parliament amended the Federal Court of Australia Act1 ("the FCAA") in 1992 to introduce the "representative proceedings", the equivalent of the American "class actions". Likewise, class actions appeared slowly in New Zealand legal system. However, a group can bring litigation through the action of a representative under the High Court Rules which provide that one or a multitude of persons may sue on behalf of, or for the benefit of, all persons "with the same interest in the subject matter of a proceeding". The presence and expansion of litigation funders have been playing a significant role in the emergence of class actions in New Zealand. For example, the "Fair Play on Fees" proceedings in relation to penalty fees charged by banks was funded by Litigation Lending Services (LLS), a company specializing in the funding and management of litigation in Australia and New Zealand. It was the biggest class action suit in New Zealand history.
Austria.
The Austrian Code of Civil Procedure (Zivilprozessordnung – ZPO) does not provide for a special proceeding for complex class action litigation. However, Austrian consumer organizations (Verein für Konsumenteninformation/VKI and the Federal Chamber of Labour/Bundesarbeitskammer) have, in recent years, brought claims on behalf of hundreds or even thousands of consumers. In these cases the individual consumers assigned their claims to one entity, who has then brought an ordinary (two party) lawsuit over the assigned claims. The monetary benefits were redistributed among the class. This technique, soon labelled as “class action Austrian style”, allows for a significant reduction of overall costs. The Austrian Supreme Court, in a recent judgment, has confirmed the legal admissibility of these lawsuits under the condition that all claims are essentially based on the same grounds.
The Austrian Parliament has unanimously requested the Austrian Federal Minister for Justice to examine the possibility of new legislation providing for a cost-effective and appropriate way to deal with mass claims. Together with the Austrian Ministry for Social Security, Generations and Consumer Protection, the Justice Ministry opened the discussion with a conference held in Vienna in June, 2005. With the aid of a group of experts from many fields, the Justice Ministry began drafting the new law in September, 2005. With the individual positions varying greatly, a political consensus could not be reached.
Canada.
Provincial laws in Canada allow class actions. All provinces permit plaintiff classes and some permit defendant classes. Quebec was the first province to enact class proceedings legislation in 1978. Ontario was next with the Class Proceedings Act, 1992. As of 2008, 9 of 10 provinces have enacted comprehensive class actions legislation. In Prince Edward Island, where no comprehensive legislation exists, following the decision of the Supreme Court of Canada in "Western Canadian Shopping Centres Inc. v. Dutton", 2 S.C.R. 534, class actions may be advanced under a local rule of court. The Federal Court of Canada permits class actions under Part V.1. of the Federal Courts Rules.
Legislation in Saskatchewan, Manitoba, Ontario, and Nova Scotia expressly or by judicial opinion have been read to allow for what are informally known as national "opt-out" class actions, whereby residents of other provinces may be included in the class definition and potentially be bound by the court's judgment on common issues unless they opt out in a prescribed manner and time. Court rulings have determined that this permits a court in one province to include residents of other provinces in the class action on an "opt-out" basis.
Recent judicial opinions have indicated that provincial legislative national opt-out powers should not be exercised to interfere with the ability of another province to certify a parallel class action for residents of other provinces. The first court to certify will generally exclude residents of provinces whose courts have certified a parallel class action. However, in the Vioxx litigation, two provincial courts recently certified overlapping class actions whereby Canadian residents are class members in two class actions in two provinces. Both decisions are under appeal.
The largest class action suit to date in Canada was settled in 2005 after Nora Bernard initiated efforts that led to an estimated 79,000 survivors of Canada's residential school system suing the Canadian government. The settlement amounted to upwards of $5 billion.
France.
Under French law, an association can represent the collective interests of consumers; however, each claimant must be individually named in the lawsuit. On January 4, 2005, President Chirac urged changes that would provide greater consumer protection. A draft bill was proposed in April 2006. Under the proposals the court will be able to decide whether to allow an action brought by an association on behalf of consumers (which must comprise at least two individuals) for goods purchased under a standard contract. After such an action is brought, the association would be entitled to identify additional consumers for a one-month period. The court would determine the damages that must be awarded to the consumers who have opted in to the proceedings, with damages limited to 2,000 Euros; contingent fees for attorneys would be barred. The president of the French Supreme Court recently declared that "class actions are inescapable."
Following the change of majority in France in 2012, the new government is once more talking of introducing class actions into French law. The project of "loi Hamon" of May 2013 aims to limit the class action to consumer and competition disputes. The law was passed on the 17th of March 2014.
Germany.
On November 1, 2005, Germany enacted the “Act on Model Case Proceedings in Disputes under Capital Markets Law (Capital Markets Model Case Act)” allowing sample proceedings to be brought before the courts in litigation arising from mass capital markets transactions. It does not apply to any other civil law proceeding. It is not like class actions in the United Statesit only applies to parties who have already filed suit and does not allow a claim to be brought in the name of an unknown group of claimants. The effects of the new law will be monitored over the next five years. It contains a ‘sunset clause’, and it will automatically cease to have effect on November 1, 2010, unless the legislature decides to prolong the law, or extend it to other mass civil case proceedings.
Italy.
Italy has class action legislation. Consumer associations can file claims on behalf of groups of consumers to obtain judicial orders against corporations that cause injury or damage to consumers. These types of claims are increasing and Italian courts have recently allowed them against banks that continue to apply compound interest on retail clients’ current account overdrafts. The introduction of class actions is on the new government’s agenda. On November 19, 2007, the Senato della Repubblica passed a class action law in Finanziara 2008, a financial document for the economy management of the government. Now (from 10 December 2007), in order of Italian legislation system, the law is before the House and has to be passed also by the Camera dei Deputati, the second house of Italian Parliament, to become an effective law. In 2004, the Italian parliament considered the introduction of a type of class action, specifically in the area of consumers’ law. To date, no such law has been enacted, but scholars demonstrated that class actions ("azioni rappresentative") do not contrast with Italian principles of civil procedure. Class action is regulated by art. 140 bis of the Italian consumers' code and will be in force from 1 July 2009.
India.
Decisions of the Indian Supreme Court in the 1980s loosened strict "locus standi" requirements to permit the filing of suits on behalf of rights deprived sections of society by public minded individuals or bodies. Although not strictly "class action litigation" as it is understood in American law, Public Interest Litigation arose out of the wide powers of judicial review granted to the Supreme Court of India and the various High Courts under and Article 226 of the Constitution of India, respectively. The sort of remedies sought from courts in Public Interest Litigation go beyond mere award of damages to all affected groups and have sometimes (controversially) gone on to include Court monitoring of the implementation of legislation and even the framing of guidelines in the absence of Parliamentary legislation.
However, this innovative jurisprudence did not help the victims of the Bhopal Gas Tragedy, who were unable to fully prosecute a class action litigation (as understood in the American sense) against Union Carbide due to procedural rules that would make such litigation impossible to conclude and unwieldy to carry out. Instead, the Government of India exercised its right of parens patriae to appropriate all the claims of the victims and proceeded to litigate on their behalf, first in the New York courts and later, in the Indian courts. Ultimately, the matter was settled between the Union of India and Union Carbide (in a settlement overseen by the Supreme Court of India) for a sum of as a complete settlement of all claims of all victims for all time to come.
Public Interest Litigation has now broadened in scope to cover larger and larger groups of citizens who may be affected by Government inaction. Recent examples of this trend include the conversion of all public transport in the city of Delhi from Diesel engines to CNG engines on the basis of the orders of the Delhi High Court; the monitoring of forest use by the High Courts and the Supreme Court to ensure that there is no unjustified loss of forest cover; and the directions mandating the disclosure of assets of electoral candidates for the Houses of Parliament and State Assembly.
Of late, the Supreme Court has observed that the PIL has tended to become a means to gain publicity or obtain relief contrary to constitutionally valid legislation and policy. Observers point out that many High Courts and certain Supreme Court judges are reluctant to entertain PILs, even those filed by well-known Non-governmental organizations (NGO) and activists, citing concerns of Separation of powers and the parliamentary sovereignty.
Netherlands.
Dutch law allows associations ("verenigingen") and foundations ("stichtingen") to bring a so-called collective action on behalf of other persons, provided they can represent the interests of such persons according to their by-laws ("statuten") (section 3:305a Dutch Civil Code). All types of actions are permitted, excluding a claim for monetary damages. Most class actions over the past decade have been in the field of securities fraud and financial services. The acting association or foundation may come to a collective settlement with the defendant. The settlement may also include – and usually primarily consists of – monetary compensation of damages. Such settlement can be declared binding for all injured parties by the Amsterdam Court of Appeal (section 7:907 Dutch Civil Code). The injured parties have an opt-out right during the opt-out period set by the Court, usually 3 to 6 months. Interestingly, settlements involving injured parties from outside The Netherlands can also be declared binding by the Court. Notably since US courts are reluctant to take up class actions brought on behalf of injured parties not residing in the US who have suffered damages due to acts or omissions committed outside the US, it may be interesting to combine a US class action and a Dutch collective action to be able to come to a settlement that covers plaintiffs worldwide. An example of this is the Royal Dutch Shell Oil Reserves Settlement that was declared binding upon both US and non-US plaintiffs.
Spain.
Spanish law allows nominated consumer associations to take action to protect the interests of consumers. A number of groups already have the power to bring collective or class actions: certain consumer associations, bodies legally constituted to defend the ‘collective interest’ and groups of injured parties.
Recent changes to Spanish civil procedure rules include the introduction of a quasi-class action right for certain consumer associations to claim damages on behalf of unidentified classes of consumers. The rules require consumer associations to represent an adequate number of affected parties who have suffered the same harm. Also any judgment made by the Spanish court will list the individual beneficiaries or, if that is not possible, conditions that need to be fulfilled for a party to benefit from a judgment.
Switzerland.
Swiss law does not allow for any form of class action. When the government proposed a new federal code of civil procedure in 2006, replacing the cantonal codes of civil procedure, it rejected the introduction of class actions, arguing that
United Kingdom.
The Civil Procedure Rules of the courts of England and Wales came into force in 1999 and have provided for representative actions in limited circumstances (under Part 19.6 ). These have not been much used, with only two reported cases at the court of first instance in the first ten years after the Civil Procedure Rules took effect.

</doc>
<doc id="7201" url="https://en.wikipedia.org/wiki?curid=7201" title="Contempt of court">
Contempt of court

Contempt of court, often referred to simply as "contempt", is the offense of being disobedient to or disrespectful towards a court of law and its officers in the form of behavior that opposes or defies authority, justice, and dignity of the court. It manifests itself in willful disregard of or disrespect for the authority of a court of law, which is often behavior that is illegal because it does not obey or respect the rules of a law court.
As explained in the People's Law Dictionary by Gerald and Kathleen Hill, "there are essentially two types of contempt: (1) being rude, disrespectful to the judge or other attorneys or causing a disturbance in the courtroom, particularly after being warned by the judge; (2) willful failure to obey an order of the court." Contempt proceedings are especially used to enforce equitable remedies, such as injunctions.
When a court decides that an action constitutes contempt of court, it can issue a court order that in the context of a court trial or hearing declares a person or organization to have disobeyed or been disrespectful of the court's authority, called "found" or "held in contempt"; this is the judge's strongest power to impose sanctions for acts that disrupt the court's normal process.
A finding of being in contempt of court may result from a failure to obey a lawful order of a court, showing disrespect for the judge, disruption of the proceedings through poor behaviour, or publication of material deemed likely to jeopardize a fair trial. A judge may impose sanctions such as a fine or jail for someone found guilty of contempt of court. Judges in common law systems usually have more extensive power to declare someone in contempt than judges in civil law systems. The client or person must be proven to be guilty before he/she will be punished.
In use today.
Contempt of court is essentially seen as a form of disturbance that may impede the functionality of the court. The judge may impose fines and jail time upon any person committing contempt of court. The person is usually let out upon his agreement to fulfill the wishes of the court. Civil contempt can involve acts of omission. The judge will make use of warnings in most situations that may lead to a person being charged with contempt. It is relatively rare that a person is charged for contempt without first receiving at least one warning from the judge. Constructive contempt, also called consequential contempt is when a person fails to fulfill the will of the court as it applies to outside obligations of the person. In most cases, constructive contempt is considered to be in the realm of civil contempt because of its passive nature.
Indirect contempt is something that is associated with civil and constructive contempt and involves a failure to follow court orders. Criminal contempt includes anything that could be called a disturbance such as repeatedly talking out of turn, bringing forth previously banned evidence, or harassment of any other party in the courtroom. Direct contempt is an unacceptable act in the presence of the judge (in facie curiae), and generally begins with a warning, and may be accompanied by an immediate imposition of punishment. Yawning in some cases can be considered contempt of court.
Contempt of court has a significant impact on journalism in the form of restrictions on court reporting which are set out in statute in the UK.
Australia.
In Australia a judge may impose a fine or jail.
Canada.
Common law offence.
In Canada, contempt of court is an exception to the general principle that all criminal offences are set out in the federal Criminal Code. Contempt of court is the only remaining common law offence in Canada.
Contempt of Court includes the following behaviours:
Canadian Federal courts.
"This section applies only to Federal Court of Appeal and Federal Court."
Under Federal Court Rules, Rules 466, and Rule 467 a person who is accused of Contempt needs to be first served with a contempt order and then appear in court to answer the charges. Convictions can only be made when proof beyond a reasonable doubt is achieved.
If it is a matter of urgency or the contempt was done in front of a judge, that person can be punished immediately. Punishment can range from the person being imprisoned for a period of less than five years or until the person complies with the order or fine.
Tax Court of Canada.
Under Tax Court of Canada Rules of "Tax Court of Canada Act", a person who is found to be in contempt may be imprisoned for a period of less than two years or fined. Similar procedures for serving an order first is also used at the Tax Court.
Provincial courts.
Different procedures exist for different provincial courts. For example, in British Columbia, Justice of Peace can only issue summon to the offender for Contempt, for which will be dealt with by a judge, even if the offence was done at the face of the Justice.
Hong Kong.
Judges from the Court of Final Appeal, High Court, District Courts along with members from the various tribunals and Coroner's Court all have the power to impose immediate punishments for contempt in the face of the court, derived from legislation or through common law:
The use of insulting or threatening language in the magistrates' courts or against a magistrate is in breach of section 99 of the Magistrates Ordinance (Cap 227) which states the magistrate can 'summarily sentence the offender to a fine at level 3 and to imprisonment for 6 months.'
In addition, certain appeal boards are given the statutory authority for contempt by them (i.e., Residential Care Home, Hotel and Guesthouse Accommodation, Air Pollution Control, etc.). For contempt in front of these boards, the chairperson will certify the act of contempt to the Court of First Instance who will then proceed with a hearing and determine the punishment.
India.
In India contempt of court is of two types:
1. Civil Contempt
Under Section 2(b) of the Contempt of Courts Act of 1971, civil contempt has been defined as wilful disobedience to any judgment, decree, direction, order, writ or other process of a court or wilful breach of an undertaking given to a court.
2. Criminal Contempt
Under Section 2(c) of the Contempt of Courts Act of 1971, criminal contempt has been defined as the publication (whether by words, spoken or written, or by signs, or by visible representation, or otherwise) of any matter or the doing of any other act whatsoever which:
(i) Scandalises or tends to scandalise, or lowers or tends to lower the authority of, any court, or
(ii) Prejudices, or interferes or tends to interfere with the due course of any judicial proceeding, or
(iii) Interferes or tends to interfere with, or obstructs or tends to obstruct, the administration of justice in any other manner. 
(a) 'High Court' means the high court for a state or a union territory and includes the court of the judicial commissioner in any union territory.
England and Wales.
In English law (a common law jurisdiction) the law on contempt is partly set out in case law, and partly specified in the Contempt of Court Act 1981. Contempt may be a criminal or civil offence. The maximum sentence for criminal contempt is two years.
Disorderly, contemptuous, or insolent behaviour toward the judge or magistrates while holding the court, tending to interrupt the due course of a trial or other judicial proceeding, may be prosecuted as "direct" contempt. The term "direct" means that the court itself cites the person in contempt by describing the behaviour observed on the record. Direct contempt is distinctly different from indirect contempt, wherein another individual may file papers alleging contempt against a person who has willfully violated a lawful court order.
Criminal contempt of court.
The Crown Court is a superior court of record under the Senior Courts Act 1981 and accordingly has power to punish for contempt of its own motion. The Divisional Court has stated that this power applies in three circumstances:
Where it is necessary to act quickly the judge (even the trial judge) may act to sentence for contempt.
Where it is not necessary to be so urgent, or where indirect contempt has taken place the Attorney General can intervene and the Crown Prosecution Service will institute criminal proceedings on his behalf before a Divisional Court of the Queen's Bench Division of the High Court of Justice of England and Wales.
Magistrates' courts are not superior courts of record, but nonetheless have powers granted under the Contempt of Court Act 1981. They may detain any person who insults the court or otherwise disrupts its proceedings until the end of the sitting. Upon the contempt being either admitted or proved the judge or JP may imprison the offender for a maximum of one month, fine them up to GBP £2,500, or do both.
It is contempt of court to bring an audio recording device or picture-taking device of any sort into an English court without the consent of the court.
It is not contempt of court (under section 10 of the Act) for a journalist to refuse to disclose his sources, unless the court has considered the evidence available and determined that the information is "necessary in the interests of justice or national security or for the prevention of disorder or crime".
Strict liability contempt.
Under the Contempt of Court Act 1981 it is criminal contempt of court to publish anything which creates a real risk that the course of justice in proceedings may be seriously impaired. It only applies where proceedings are active, and the Attorney General has issued guidance as to when he believes this to be the case, and there is also statutory guidance. The clause prevents the newspapers and media from publishing material that is too extreme or sensationalist about a criminal case until the trial or linked trials are over and the juries have given their verdicts.
Section 2 of the Act limits the common law presumption that conduct may be treated as contempt regardless of intention: now only cases where there is a substantial risk of serious prejudice to a trial are affected.
Civil contempt.
In civil proceedings there are two main ways in which contempt is committed:
United States.
Under the United States jurisprudence, acts of contempt are divided into direct or indirect and civil or criminal. Direct contempt occurs in the presence of a judge; civil contempt is "coercive and remedial" as opposed to punitive. In the United States, relevant statutes include and Federal Rule of Criminal Procedure 42.
Contempt of court in a civil suit is generally not considered to be a criminal offense, with the party benefiting from the order also holding responsibility for the enforcement of the order. However, some cases of civil contempt have been perceived as intending to harm the reputation of the plaintiff, or to a lesser degree, the judge or the court.
Sanctions for contempt may be criminal or civil. If a person is to be punished criminally, then the contempt must be proven beyond a reasonable doubt, but once the charge is proven, then punishment (such as a fine or, in more serious cases, imprisonment) is imposed unconditionally. The civil sanction for contempt (which is typically incarceration in the custody of the sheriff or similar court officer) is limited in its imposition for so long as the disobedience to the court's order continues: once the party complies with the court's order, the sanction is lifted. The imposed party is said to "hold the keys" to his or her own cell, thus conventional due process is not required. The burden of proof for civil contempt, however, is a preponderance of the evidence, and theoretically punitive sanctions (punishment) can only be imposed after due process but the due process is unpublished.
In civil contempt cases there is no principle of proportionality. In "Chadwick v. Janecka" (3d Cir. 2002), a U.S. court of appeals held that H. Beatty Chadwick could be held indefinitely under federal law, for his failure to produce US$2.5 million as state court ordered in a civil trial. Chadwick had been imprisoned for nine years at that time and continued to be held in prison until 2009, when a state court set him free after 14 years, making his imprisonment the longest on a contempt charge to date.
Civil contempt is only appropriate when the imposed party has the power to comply with the underlying order. Controversial contempt rulings have periodically arisen from cases involving asset protection trusts, where the court has ordered a settlor of an asset protection trust to repatriate assets so that the assets may be made available to a creditor. A court cannot maintain an order of contempt where the imposed party does not have the ability to comply with the underlying order. This claim when made by the imposed party is known as the "impossibility defense".
Contempt of court is considered a prerogative of the court, and "the requirement of a jury does not apply to 'contempts committed in disobedience of any lawful writ, process, order, rule, decree, or command entered in any suit or action brought or prosecuted in the name of, or on behalf of, the United States'". This stance is not universally agreed with by other areas of the legal world, and there have been many calls to have contempt cases to be tried by jury, rather than by judge, as a potential conflict of interest rising from a judge both accusing and sentencing the defendant. At least one Supreme Court Justice has made calls for jury trials to replace judge trials on contempt cases.
The United States Marshals Service is the agency component that first holds all federal prisoners. It uses the Prisoner Population Management System /Prisoner Tracking System. The only types of records that are disclosed as being in the system are those of "federal prisoners who are in custody pending criminal proceedings." The records of "alleged civil contempors" are not listed in the Federal Register as being in the system leading to a potential claim for damages under The Privacy Act, .
News media in the United States.
In the United States, because of the broad protections granted by the First Amendment, with extremely limited exceptions, unless the media outlet is a party to the case, a media outlet cannot be found in contempt of court for reporting about a case because a court cannot order the media in general not to report on a case or forbid it from reporting facts discovered publicly. Newspapers cannot be closed because of their content.
Criticism.
There have been criticisms over the practice of trying contempt from the bench. In particular, Supreme Court Justice Hugo Black wrote in a dissent, "It is high time, in my judgment, to wipe out root and branch the judge-invented and judge-maintained notion that judges can try criminal contempt cases without a jury."

</doc>
<doc id="7202" url="https://en.wikipedia.org/wiki?curid=7202" title="Corroborating evidence">
Corroborating evidence

Corroborating evidence (or corroboration) is evidence that tends to support a proposition that is already supported by some initial evidence, therefore confirming the proposition. For example, W, a witness, testifies that she saw X drive his automobile into a green car. Meanwhile Y, another witness, testifies that when he examined X's car, later that day, he noticed green paint on its fender. There can also be corroborating evidence related to a certain source, such as what makes an author think a certain way due to the evidence that was supplied by witnesses or objects.
Another type of corroborating evidence comes from using the Baconian method, i.e. the method of agreement, method of difference, and method of concomitant variations.
These methods are followed in experimental design. They were codified by Francis Bacon, and developed further by John Stuart Mill and consist of controlling several variables, in turn, to establish which variables are causally connected. These principles are widely used intuitively in various kinds of proofs, demonstrations and investigations, in addition to being fundamental to experimental design.
In law, corroboration refers to the requirement in some jurisdictions, such as in Scotland, that any evidence adduced be backed up by at least one other source (see Corroboration in Scots law).
Corroboration broken down.
If one person says, “this is what I intended by the action I took,” and his friend agrees that his actions could have looked like what their friend intended. Then it can be generally agreed that is what happened.
If one person says, “this is what I meant by what I said,” and his friend agrees that was their understanding of what was meant by their friend. Then it can be generally agreed that is what was meant.
Think of this like backing up your mate in the playground no matter what they did or said.
An example of corroboration.
First person walks in turns and looks back the way they had come; this allows a person to say I had a clear view of another person that enters later. Second person walks in and on some pretext starts shouting, suddenly raising arms in the air and shaking them, who notices if fists are open or closed when the eye is on the action of the arm; this can look threatening and causes added stress in already tense circumstances. This can provoke an already bad situation into violence and can be used when people need to justify their actions or their presence. We made a mistake but look what happened someone was assaulted. Later it can be said I had a clear view of the other person and my friend raised their arms in supplication or in surrender and can demonstrate this with arms raised slowly, hands open and palms out. This involves two people, is premeditated, threatening, causes fear and alarm and is also used to make corroboration work against anyone. Common line afterward whether you were baited or goaded you still did it. This works well if both people have authority and in a position where they are commonly accepted to be of good character. There is other variations on this and is often used to provoke criminal action where there has been no other disorderly conduct but.
Corroboration is not needed in certain instances. For example, there are certain statutory exceptions. In the Education (Scotland) Act, it is only necessary to produce a register as proof of lack of attendance. No further evidence is needed.
England and Wales.
Perjury
See section 13 of the Perjury Act 1911.
Speeding offences
See section 89(2) of the Road Traffic Regulation Act 1984.
Sexual offences
See section 32 of the Criminal Justice and Public Order Act 1994.
Confessions by mentally handicapped persons
See section 77 of the Police and Criminal Evidence Act 1984.
Evidence of children
See section 34 of the Criminal Justice Act 1988.
Evidence of accomplices
See section 32 of the Criminal Justice and Public Order Act 1994.

</doc>
<doc id="7203" url="https://en.wikipedia.org/wiki?curid=7203" title="Cross-examination">
Cross-examination

In law, cross-examination is the interrogation of a witness called by one's opponent. It is preceded by direct examination (in the United Kingdom, Australia, Canada, South Africa, India and Pakistan known as examination-in-chief) and may be followed by a redirect (re-examination in England, Scotland, Australia, Canada, South Africa, India, Hong Kong, and Pakistan).
Variations by jurisdiction.
In the United States federal Courts, a cross-examining attorney is typically not permitted to ask questions that do not pertain to the testimony offered during direct examination, but most state courts do permit a lawyer to cross-examine a witness on matters not raised during direct examination. Similarly, courts in England, South Africa, Australia, and Canada allow a cross-examiner to exceed the scope of direct examination.
Since a witness called by the opposing party is presumed to be hostile, cross-examination does permit leading questions. A witness called by the direct examiner, on the other hand, may only be treated as hostile by that examiner after being permitted to do so by the judge, at the request of that examiner and as a result of the witness being openly antagonistic and/or prejudiced against the opposing party.
The art of cross-examination.
The main purposes of cross-examination are to elicit favorable facts from the witness, or to impeach the credibility of the testifying witness to lessen the weight of unfavorable testimony. Cross-examination frequently produces critical evidence in trials, especially if a witness contradicts previous testimony. The advocate Edward Marshall-Hall built his career on cross-examination that often involved histrionic outbursts designed to sway jurors. Most experienced and skilled cross-examiners however, refrain from caustic or abrasive cross-examination so as to avoid alienating jurors. John Mortimer, Queen's Counsel, observed that "cross-examination" was not the art of examining crossly. Indeed the good cross-examiner gets a witness to assert to a series of linked propositions culminating in one that undermines that witnesses' evidence rather than pursuing an antagonistic approach.
Cross-examination is considered an essential component of a jury trial because of the impact it has on the opinions of the judge and jury. Few lawyers practice trial law or complex litigation and typically refer such cases to those who have the time, resources and experience to handle a complex trial and the commitment involved to complete a trial successfully. Few attorneys get the practice necessary to develop the techniques needed to do an effective job cross-examining a witness. 
It is sometimes referred to as an art form, because of the need for an attorney to know precisely how to elicit the testimony from the opposing witness that will help, not hinder, their client's case. Typically a cross-examiner must not only be effective at getting the witness to reveal the truth, but in most cases to reveal confusion as to the facts such as time, dates, people, places, wording etc. More often than not a cross-examiner will also attempt to undermine the credibility of a witness if he or she will not be perceived to be a bully (such as discrediting a very elderly person or young child). 
The cross-examiner often needs to discredit a potentially biased or damaging witness in the eyes of the jury without appearing to be doing so in an unfair way. Typically the cross-examiner must appear friendly, talk softly and sincerely to relax the guarded witness. Or on other occasions they may start by being more confrontational, unsettling an already disturbed witness. They typically begin repeating similar basic questions in a variety of different ways to get different responses, which will then be used against the witness as misstatements of fact later when the attorney wants to make their point. If it is too obvious the questions are too clearly repetitive and making the witness nervous, the other attorney may accuse the cross examiner of badgering the witness. There is a fine line between badgering and getting the witness to restate facts differently that is typically pursued. 
The less the witness says, and the slower the witness speaks, the more control they can maintain under the pressure of a crafty opponent. The key for a witness is to understand the facts that they believe to be the case and not add additional thoughts to those facts, lest they be used to undermine the testimony. Sticking to the brief known facts is key for the witness, making it difficult for the cross-examiner to make the witness appear confused, biased or deceitful. The cross examiner will assume the witness has been told that and begin asking supporting questions about where the witness was, what time it was, what the witness saw, what they said, and sooner or later upon asking again the witness may use a different word that will give the cross-examiner a chance to ask the question again doubtfully and pointedly implying contradiction. The witness will try typically to explain and clarify, which sometimes reveals weakness in the witness's statements of fact. Other times the witness is just being truthful but undermined for the purpose of casting doubt to the jury and or judge.
There is a measure of drama that cross-examination adds to any trial because of the challenging of the statements made by a witness. In the 1903 book titled "The Art of Cross-Examination" by Francis L. Wellmann much effort is devoted to highlighting components of cross-examination and the impact on trials of the past century. An example of an inflammatory way a question will be asked by a cross-examiner to a witness he was trying to undermine would be "What is your recollection toDAY?" implying it was stated differently yesterday. Simply the accent of syllables can leave a bewildered jury believing they must put their guard up with a witness–or in some cases the cross-examiner if they are not careful. The book freely uses accenting in its dialogue to give the reader such insight as to how cross-examiners rattle witnesses to obtain their desired effect for the jury.
In most common-law countries, cross-examiners are expected to follow the well-established rule in "Browne v. Dunn".
Affecting the outcome of jury trials.
Cross-examination is a key component in a trial and the topic is given substantial attention during courses on Trial Advocacy. The opinions by a jury or judge are often changed during cross examination if doubt is cast on the witness. In other times a credible witness affirms the belief in their original statements or in some cases enhances the judge's or jury's belief. Though the closing argument is often considered the deciding moment of a trial, effective cross-examination wins trials. 
Attorneys anticipate hostile witness' responses during pretrial planning, and often attempt to shape the witnesses' perception of the questions to draw out information helpful to the attorney's case. Typically during an attorney's closing argument he will repeat any admissions made by witnesses that favor their case. Indeed, in the United States, cross-examination is seen as a core part of the entire adversarial system of justice, in that it "is the principal means by which the believability of a witness and the truth of his testimony are tested." Another key component affecting a trial outcome is the jury selection, in which attorneys will attempt to include jurors from whom they feel they can get a favorable response or at the least unbiased fair decision. So while there are many factors affecting the outcome of a trial, the cross-examination of a witness will often have an impact on an open minded unbiased jury searching for the certainty of facts upon which to base their decision.

</doc>
<doc id="7206" url="https://en.wikipedia.org/wiki?curid=7206" title="Christiania">
Christiania

Christiania may refer to:

</doc>
<doc id="7207" url="https://en.wikipedia.org/wiki?curid=7207" title="Charles d'Abancour">
Charles d'Abancour

Charles Xavier Joseph de Franque Ville d'Abancour (4 July 1758 – 9 September 1792) was a French statesman, minister to Louis XVI.
Biography.
D'Abancourt was born in Douai, and was the nephew of Charles Alexandre de Calonne. He was Louis XVI's last minister of war (July 1792), and organised the defence of the Tuileries Palace during the 10th of August attack. Ordered by the Legislative Assembly to send away the Swiss Guards, he refused, and was arrested for treason to the nation and sent to Orléans to be tried.
At the end of August the Assembly ordered Abancourt and the other prisoners at Orléans to be transferred to Paris with an escort commanded by Claude Fournier "l'Americain". At Versailles, they learned of the September Massacres in Paris. Abancourt and his fellow-prisoners were murdered in cold blood in massacres on 9 September 1792 at Versailles, and Fournier was unjustly charged with complicity in the crime.

</doc>
<doc id="7211" url="https://en.wikipedia.org/wiki?curid=7211" title="Curtiss P-40 Warhawk">
Curtiss P-40 Warhawk

The Curtiss P-40 Warhawk is an American single-engined, single-seat, all-metal fighter and ground-attack aircraft that first flew in 1938. The P-40 design was a modification of the previous Curtiss P-36 Hawk which reduced development time and enabled a rapid entry into production and operational service. The Warhawk was used by most Allied powers during World War II, and remained in frontline service until the end of the war. It was the third most-produced American fighter, after the P-51 and P-47; by November 1944, when production of the P-40 ceased, 13,738 had been built, all at Curtiss-Wright Corporation's main production facilities at Buffalo, New York.
P-40 Warhawk was the name the United States Army Air Corps and after June 1941, USAAF-adopted name for all models, making it the official name in the United States for all P-40s. The British Commonwealth and Soviet air forces used the name Tomahawk for models equivalent to the P-40B and P-40C, and the name Kittyhawk for models equivalent to the P-40D and all later variants.
P-40s first saw combat with the British Commonwealth squadrons of the Desert Air Force in the Middle East and North African campaigns, during June 1941. No. 112 Squadron Royal Air Force, was among the first to operate Tomahawks in North Africa and the unit was the first Allied military aviation unit to feature the "shark mouth" logo, copying similar markings on some Luftwaffe Messerschmitt Bf 110 twin-engine fighters. 
The P-40's lack of a two-speed supercharger made it inferior to Luftwaffe fighters such as the Messerschmitt Bf 109 or the Focke-Wulf Fw 190 in high-altitude combat and it was rarely used in operations in Northwest Europe. However, between 1941 and 1944, the P-40 played a critical role with Allied air forces in three major theaters: North Africa, the Southwest Pacific, and China. It also had a significant role in the Middle East, Southeast Asia, Eastern Europe, Alaska and Italy. The P-40's performance at high altitudes was not as important in those theaters, where it served as an air superiority fighter, bomber escort and fighter-bomber. Although it gained a postwar reputation as a mediocre design, suitable only for close air support, recent research including scrutiny of the records of individual Allied squadrons indicates that this was not the case: the P-40 performed surprisingly well as an air superiority fighter, at times suffering severe losses but also taking a very heavy toll of enemy aircraft. The P-40 offered the additional advantage of low cost, which kept it in production as a ground-attack aircraft long after it was obsolete as a fighter. In 2008, 29 P-40s were airworthy.
Design and development.
Origins.
On 14 October 1938, Curtiss test pilot Edward Elliott flew the prototype XP-40, on its first flight in Buffalo. The XP-40 was the 10th production Curtiss P-36 Hawk, with its Pratt & Whitney R-1830 (Twin Wasp) 14-cylinder air-cooled radial engine replaced at the direction of Chief Engineer Don R. Berlin by a liquid-cooled, supercharged Allison V-1710 V-12 engine. The first prototype placed the glycol coolant radiator in an underbelly position on the fighter, just aft of the wing's trailing edge. USAAC Fighter Projects Officer Lieutenant Benjamin S. Kelsey flew this prototype some 300 miles in 57 minutes, approximately . Hiding his disappointment, he told reporters that future versions would likely go faster. Kelsey was interested in the Allison engine because it was sturdy and dependable, and it had a smooth, predictable power curve. The V-12 engine offered as much power as a radial engine but had a smaller frontal area and allowed a more streamlined cowl than an aircraft with radial engines, promising a theoretical 5% increase in top speed.
Curtiss engineers worked to improve the XP-40's speed by moving the radiator forward in steps. Seeing little gain, Kelsey ordered the aircraft to be evaluated in a NACA wind tunnel to identify solutions for better aerodynamic qualities. From 28 March to 11 April 1939, the prototype was studied by NACA. Based on the data obtained, Curtiss moved the glycol coolant radiator forward to the chin; its new air scoop also accommodated the oil cooler air intake. Other improvements to the landing gear doors and the exhaust manifold combined to give performance that was satisfactory to the USAAC. Without beneficial tail winds, Kelsey flew the XP-40 from Wright Field back to Curtiss's plant in Buffalo at an average speed of . Further tests in December 1939 proved the fighter could reach .
An unusual production feature was a special truck rig to speed delivery at the main Curtiss plant in Buffalo, New York. The rig moved the newly built P-40s in two main components, the main wing and the fuselage, the eight miles from the plant to the airport where the two units were mated for flight and delivery.
Performance characteristics.
The P-40 was originally conceived as a pursuit aircraft and was very agile at low and medium altitudes but suffered due to lack of power at higher altitudes. At medium and high speeds it was one of the tightest turning early monoplane designs of the war, and it could out turn most opponents it faced in North Africa and the Russian Front. In the Pacific Theater, like all Allied Fighters it was out turned at lower speeds by the lightweight fighters A6M Zero and Nakajima Ki-43 "Oscar" which did not possess the structural strength of the P-40 for high speed hard turns. The American Volunteer Group Commander Claire Chennault advised against prolonged dog fighting with the Japanese fighters due to the resulting airspeed reduction which favored the lightweight Japanese designs' low speed turning superiority.
Allison V-1710 engines produced about at sea level and at : not powerful by the standards of the time and the early P-40 variants' top speeds were only average. Also, the single-stage, single-speed supercharger meant that the P-40 could not compete with contemporary designs as a high-altitude fighter. Later versions, with Allisons or more powerful 1,400 hp Packard Merlin engines were more capable. Climb performance was fair to poor, depending on the subtype. Dive acceleration was good and dive speed was excellent. The highest-scoring P-40 ace, Clive Caldwell (RAAF), who claimed 22 of his 28½ kills in the type, said that the P-40 had "almost no vices", although "it was a little difficult to control in terminal velocity". Caldwell added that the P-40 was "faster downhill than almost any other aeroplane with a propeller." The P-40 had one of the fastest maximum dive speeds of any fighter of the early war period and good high speed handling.
The P-40 tolerated harsh conditions in the widest possible variety of climates. It was a semi-modular design and thus easy to maintain in the field. It lacked innovations of the time, such as boosted ailerons or automatic leading edge slats, but it had a strong structure including a five-spar wing, which enabled P-40s to pull high G turns and even survive some midair collisions: both accidental impacts and intentional ramming attacks against enemy aircraft were occasionally recorded as victories by the Desert Air Force and Soviet Air Forces. Caldwell said P-40s "would take a tremendous amount of punishment, violent aerobatics as well as enemy action." Operational range was good by early war standards, and was almost double that of the Supermarine Spitfire or Messerschmitt Bf 109, although it was inferior to the Mitsubishi A6M Zero, Nakajima Ki-43 and Lockheed P-38 Lightning.
Caldwell found the P-40C Tomahawk's armament of two .50 in (12.7 mm) Browning AN/M2 "light-barrel" dorsal nose-mount synchronized machine guns and two .303 Browning machine guns in each wing to be inadequate. This was rectified with the P-40D (Kittyhawk I) which abandoned the synchronized gun mounts and instead had two .50 in (12.7 mm) guns in each wing, although Caldwell still preferred the earlier Tomahawk in other respects. The D had armor around the engine and the cockpit, which enabled it to withstand considerable damage. This was one of the characteristics that allowed Allied pilots in Asia and the Pacific to attack Japanese fighters head on, rather than try to out-turn and out-climb their opponents. Late-model P-40s were regarded as well armored. Visibility was adequate, although hampered by an overly complex windscreen frame, and completely blocked to the rear in early models due to the raised turtledeck. Poor ground visibility and the relatively narrow landing gear track led to many losses due to accidents on the ground.
While early models of the P-40 were being produced, Curtiss began testing a follow-on design, the Curtiss XP-46. As it offered no particular improvements over the latest P-40s, the program was cancelled.
Operational history.
In April 1939, the U.S. Army Air Corps, having witnessed the new, sleek, high-speed, in-line-engined fighters of the European air forces, placed the largest single fighter order it had ever made: 524 P-40s.
French Air Force.
An early order came from the French "Armée de l'Air", which was already operating P-36s. The "Armée de l'Air" initially ordered 100 (later the order was increased to 230) as the Hawk 81A-1 but the French military had been defeated before the aircraft had left the factory, consequently, the aircraft were diverted to British and Commonwealth service (as the Tomahawk I), in some cases complete with metric flight instruments.
In late 1942, as French forces in North Africa split from the Vichy government to side with the Allies, U.S. forces transferred P-40Fs from 33rd FG to the "GC II/5", a squadron that was historically associated with the Lafayette Escadrille. GC II/5 used its P-40Fs and Ls in combat in Tunisia and, later, for patrol duty off the Mediterranean coast until mid-1944 when they were replaced by Republic P-47D Thunderbolts.
British Commonwealth units in Mediterranean and European theatres.
Deployment.
In all, 18 Royal Air Force (RAF) squadrons, as well as four Royal Canadian Air Force (RCAF), three South African Air Force (SAAF), and two Royal Australian Air Force (RAAF) squadrons serving with RAF formations, used P-40s.
The first units to convert were Hawker Hurricane squadrons of the Desert Air Force (DAF), in early 1941. The first Tomahawks delivered came without armor, bulletproof windscreens or self-sealing fuel tanks. These were installed in subsequent shipments. Pilots used to British-designed fighters sometimes found it difficult to adapt to the P-40's rear-folding landing gear, which was more prone to collapse than the lateral-folding landing gear found on the Hawker Hurricane or Supermarine Spitfire. In contrast to the "three-point landing" commonly employed with British types, P-40 pilots were obliged to use a "wheels landing": a longer, low angle approach that touched down on the main wheels first.
Testing showed the aircraft did not have adequate performance for use in Northwest Europe in high-altitude combat due to the effective service ceiling limitation. Spitfires used in the theater operated at heights around , while the P-40's Allison engine, with its single-stage, low altitude rated supercharger, worked best at or lower. When the Tomahawk was used by Allied units based in the UK from February 1941, this limitation relegated the Tomahawk to low-level reconnaissance with RAF Army Cooperation Command
and only one squadron, No. 403 Squadron RCAF was used in the fighter role. Subsequently, the British Air Ministry deemed the P-40 completely unsuitable for the theater. UK P-40 squadrons from mid-1942 re-equipped with aircraft such as Mustangs.
The Tomahawk was superseded in North Africa by the more powerful Kittyhawk ("D"-mark onwards) types from early 1942, though some Tomahawks remained in service until 1943. Kittyhawks included many major improvements, and were the DAF's air superiority fighter for the critical first few months of 1942, until "tropicalised" Spitfires were available. In 2012, the virtually intact remains of a Kittyhawk were found; it had run out of fuel in the Egyptian Sahara in June 1942.
DAF units received nearly 330 Packard V-1650 Merlin-powered P-40Fs, called Kittyhawk IIs, most of which went to the USAAF, and the majority of the 700 "lightweight" L models, also powered by the Packard Merlin, in which the armament was reduced to four .50 in (12.7 mm) Brownings (Kittyhawk IIA). The DAF also received some 21 of the later P-40K and the majority of the 600 P-40Ms built; these were known as Kittyhawk IIIs. The "lightweight" P-40Ns (Kittyhawk IV) arrived from early 1943 and were used mostly in the fighter-bomber role. 
From July 1942 until mid-1943, elements of the U.S. 57th Fighter Group (57th FG) were attached to DAF P-40 units. The British government also donated 23 P-40s to the Soviet Union.
Combat performance.
Tomahawks and Kittyhawks bore the brunt of "Luftwaffe" and "Regia Aeronautica" fighter attacks during the North African campaign. The P-40s were considered superior to the Hurricane, which they replaced as the primary fighter of the Desert Air Force. 
The P-40 initially proved quite effective against Axis aircraft and contributed to a slight shift of momentum in the Allied favor. The gradual replacement of Hurricanes by the Tomahawks and Kittyhawks led to the "Luftwaffe" accelerating retirement of the Bf 109E and introducing the newer Bf 109F; these were to be flown by the veteran pilots of elite "Luftwaffe" units, such as "Jagdgeschwader" 27 (JG27), in North Africa.
The P-40 was generally considered roughly equal or slightly superior to the Bf 109 at low altitude, but inferior at high altitude, particularly against the Bf 109F. Most air combat in North Africa took place well below , thus negating much of the Bf 109's superiority. The P-40 usually had an edge over the Bf 109 in horizontal maneuverability (turning), dive speed and structural strength, was roughly equal in firepower, but was slightly inferior in speed and outclassed in rate of climb and operational ceiling.
The P-40 was generally superior to early Italian fighter types, such as the Fiat G.50 and the Macchi C.200. Its performance against the Macchi C.202 "Folgore" elicited varying opinions. Some observers consider the Macchi C.202 superior. Clive Caldwell, who scored victories against them in his P-40, felt that the "Folgore" was superior to both the P-40 and the Bf 109 except that its armament of only two or four machine guns was inadequate. Other observers considered the two equally matched, or favored the "Folgore" in aerobatic performance, such as turning radius. Aviation historian Walter J. Boyne wrote that over Africa, the P-40 and the "Folgore" were "equivalent".
Against its lack of high-altitude performance, the P-40 was considered to be a stable gun platform, and its rugged construction meant that it was able to operate from rough front line airstrips with a good rate of serviceability.
The earliest victory claims by P-40 pilots include Vichy French aircraft, during the 1941 Syria-Lebanon campaign, against Dewoitine D.520s, a type often considered to be the best French fighter used during World War II. The P-40 was deadly against Axis bombers in the theater, as well as against the Bf 110 twin-engine fighter.
In June 1941, Caldwell, who was serving at the time with No. 250 Squadron RAF in Egypt, and flying as F/O Jack Hamlyn's wingman, recorded in his log book that he was involved in the first air combat victory for the P-40. This was a CANT Z.1007 bomber on 6 June. The claim was not officially recognized, as the crash of the CANT was not witnessed. The first official victory occurred on 8 June, when Hamlyn and Flt Sgt Tom Paxton destroyed a CANT Z.1007 from "211a Squadriglia" of the "Regia Aeronautica", over Alexandria. Several days later, the Tomahawk was in action over Syria with No. 3 Squadron RAAF, which claimed 19 aerial victories over Vichy French aircraft during June and July 1941, for the loss of one P-40 (as well as one lost to ground fire).
Some DAF units initially failed to use the P-40's strengths and/or utilised outdated defensive tactics such as the Lufbery circle. However, the superior climb rate of the Bf 109 enabled fast, swooping attacks, neutralizing the advantages offered by conventional defensive tactics. Various new formations were tried by Tomahawk units in 1941-42, including: "fluid pairs" (similar to the German "rotte"); one or two "weavers" at the back of a squadron in formation, and whole squadrons bobbing and weaving in loose formations. Werner Schröer, who was credited with destroying 114 Allied aircraft in only 197 combat missions, referred to the latter formation as "bunches of grapes", because he found them so easy to pick off. The leading German "expert" in North Africa, Hans-Joachim Marseille, claimed as many as 101 P-40s during his career.
From 26 May 1942, all Kittyhawk units operated primarily as fighter-bomber units, giving rise to the nickname "Kittybomber". As a result of this change in role, and because DAF P-40 squadrons were frequently used in bomber escort and close air support missions, they suffered relatively high attrition rates; many Desert Air Force P-40 pilots were caught flying low and slow by marauding Bf 109s.
Caldwell believed that Operational Training Units did not properly prepare pilots for air combat in the P-40, and as a commander, stressed the importance of training novice pilots properly.
Nevertheless, competent pilots who took advantage of the P-40's strengths were effective against the best of the "Luftwaffe" and "Regia Aeronautica". For example, on one occasion in August 1941, Caldwell was attacked by two Bf 109s, one of them piloted by German ace Werner Schröer. Although Caldwell was wounded three times, and his Tomahawk was hit by more than 100 bullets and five 20 mm cannon shells, Caldwell shot down Schröer's wingman and returned to base. Some sources also claim that in December 1941, Caldwell killed a prominent German "Experte", Erbo von Kageneck (69 kills), while flying a P-40. Caldwell's victories in North Africa included 10 Bf 109s and two Macchi C.202s. Billy Drake of 112 Squadron was the leading British P-40 ace with 13 victories. James "Stocky" Edwards (RCAF), who achieved 12 kills in the P-40 in North Africa, shot down German ace Otto Schulz (51 kills) while flying a Kittyhawk with No. 260 Squadron RAF. Caldwell, Drake, Edwards and Nicky Barr were among at least a dozen pilots who achieved ace status twice over while flying the P-40. A total of 46 British Commonwealth pilots became aces in P-40s, including seven double aces.
Chinese Air Force.
Flying Tigers (American Volunteer Group).
The Flying Tigers, known officially as the 1st American Volunteer Group (AVG), were a unit of the Chinese Air Force, recruited from U.S. aviators. From late 1941, the P-40B was used by the Flying Tigers. They were divided into three pursuit squadrons, the "Adam & Eves", the "Panda Bears" and the "Hell's Angels".
Compared to opposing Japanese fighters, the P-40B's strengths were that it was sturdy, well armed, faster in a dive and possessed an excellent rate of roll. While the P-40s could not match the maneuverability of the Japanese Army air arm's Nakajima Ki-27s and Ki-43s, nor the much more famous Zero naval fighter in a slow speed turning dogfight, at higher speeds the P-40s were more than a match. AVG leader Claire Chennault trained his pilots to use the P-40's particular performance advantages. The P-40 had a higher dive speed than any Japanese fighter aircraft of the early war years, for example, and could be used to exploit so-called "boom-and-zoom" tactics. The AVG was highly successful, and its feats were widely published, to boost sagging public morale at home, by an active cadre of international journalists. According to their official records, in just 6 1/2 months, the Flying Tigers destroyed 115 enemy aircraft for the loss of just four of their own in air-to-air combat.
The Model B's received by Chennault and assembled in Burma at the end of 1941 were not well liked. There were no auxiliary fuel tanks that could be dropped before going into combat, and there were no bomb racks on the wings. Chennault considered the liquid-cooled engine vulnerable in combat because a single bullet through the coolant tank would cause the engine to overheat in minutes. The Tomahawks also had no radios, so the AVG improvised by installing a fragile radio transceiver, RCA-7-H, which had been built for a Piper Cub. Because the plane lacked a turbo-supercharger, its effective ceiling was about 25,000 feet. The most critical problem was the lack of spare parts; the only source of spare parts was damaged aircraft. The planes were thought to be what no one else wanted, dangerous and difficult to fly. But the plane had advantages: its gas tanks were self-sealing and could take hits without catching on fire. There were two heavy sheets of steel behind the pilot's head and back. The plane as a whole was ruggedly constructed.
In the spring of 1942, the AVG received a small number of Model E's, each equipped with a radio, six .50-caliber machine guns, and auxiliary bomb racks that could hold 35-lb fragmentation bombs. Chennault's armorer fitted the new planes with additional bomb racks that held the 570-lb Russian bombs, which the Chinese had in abundance. These planes were used in the battle of the Salween River Gorge in late May, 1942, which kept the Japanese from entering China from Burma and threatening Kunming. Spare parts were still a problem. "Scores of new planes...were now in India, and there they stayed--in case the Japanese decided to invade...the AVG was lucky to get a few tires and spark plugs with which to carry on its daily war."
4th Air Group.
China received 27 P-40E in early 1943. These were assigned to squadrons of the 4th Air Group.
United States Army Air Forces.
A total of 15 entire USAAF pursuit/fighter groups (FG), along with other pursuit/fighter squadrons and a few tactical reconnaissance (TR) units, operated the P-40 during 1941–45.
As was also the case with the Bell P-39 Airacobra, many USAAF officers considered the P-40 inadequate, and it was gradually replaced by the Lockheed P-38 Lightning, the Republic P-47 Thunderbolt and the North American P-51 Mustang. However, the bulk of the fighter operations by the USAAF in 1942–43 were borne by the P-40 and the P-39. In the Pacific, these two fighters, along with the U.S. Navy's Grumman F4F Wildcat, contributed more than any other U.S. types to breaking Japanese air power during this critical period.
Pacific theaters.
The P-40 was the main USAAF fighter aircraft in the South West Pacific and Pacific Ocean theaters during 1941–42.
In the first major battles, at Pearl Harbor and in the Philippines, USAAF P-40 squadrons suffered crippling losses on the ground and in the air to Japanese fighters such as the Ki-43 Oscar and A6M Zero. During the attack on Pearl Harbor, a few P-40s managed to shoot down several Japanese planes, most notably by George Welch and Kenneth Taylor.
However, in the Dutch East Indies campaign, the 17th Pursuit Squadron (Provisional), formed from USAAF pilots evacuated from the Philippines, claimed 49 Japanese aircraft destroyed, for the loss of 17 P-40s The seaplane tender USS "Langley" was sunk by Japanese planes while delivering P-40s to Tjilatjap, Java. In the Solomon Islands and New Guinea Campaigns, as well as the air defence of Australia, improved tactics and training allowed the USAAF to more effectively utilize the strengths of the P-40.
Due to aircraft fatigue, scarcity of spare parts and replacement problems, the US Fifth Air Force and Royal Australian Air Force created a joint P-40 management and replacement pool on 30 July 1942 and many P-40s went back and forth between the air forces.
The 49th Fighter Group was in action in the Pacific from the beginning of the war. Robert DeHaven scored 10 kills (of 14 overall) in the P-40 with the 49th FG. He compared the P-40 favorably with the P-38:
The 8th, 15th, 18th, 24th, 49th, 343rd and 347th PGs/FGs, flew P-40s in the Pacific theaters between 1941 and 1945, with most units converting to P-38s during 1943-44. In 1945, the 71st Reconnaissance Group employed them as armed forward air controllers during ground operations in the Philippines until it received delivery of P-51s. They claimed 655 aerial victories.
Contrary to conventional wisdom, with sufficient altitude the P-40 could actually turn with the A6M and other Japanese fighters, using a combination of a nose-down vertical turn with a bank turn, a technique known as a low yo-yo. Robert DeHaven describes how this tactic was used in the 49th Fighter group:
China Burma India Theater.
USAAF and Chinese P-40 pilots performed well in this theater, scoring high kill ratios against Japanese types such as the Ki-43, Nakajima Ki-44 "Tojo" and the Zero. The P-40 remained in use in the China Burma India Theater (CBI) until 1944, and was reportedly preferred over the P-51 Mustang by some US pilots flying in China.
The American Volunteer Group (Flying Tigers) was integrated into the USAAF as the 23rd Fighter Group in June 1942. The unit continued to fly newer model P-40s until the end of the war, racking up a high kill-to-loss ratio.
In the very important Battle of the Salween River Gorge of May 1942 the AVG used the P40E model equipped with wing racks that could carry six 35-pound fragmentation bombs; in addition, Chennault's armorer developed belly racks for the planes that could carry the Russian 570-pound bombs, which the Chinese had in large quantity...
Units arriving in the CBI after the AVG in the 10th and 14th Air Forces continued to perform well with the P-40, claiming 973 kills in the theater, or 64.8 percent of all enemy aircraft shot down. Aviation historian Carl Molesworth stated that "...the P-40 simply dominated the skies over Burma and China. They were able to establish air superiority over free China, northern Burma and the Assam valley of India in 1942, and they never relinquished it."
In addition to the 23rd FG, the 3rd, 5th, 51st and 80th FGs, along with the 10th TRS, operated the P-40 in the CBI. In addition to its role as a fighter aircraft, CBI P-40 pilots used the aircraft very effectively as a fighter-bomber. The 80th Fighter Group in particular used its so-called "B-40" (P-40s carrying 1,000-pound high explosive bombs) to destroy Japanese-held bridges and kill bridge repair crews, sometimes demolishing their target with a single bomb. At least 40 U.S. pilots reached ace status while flying the P-40 in the CBI.
Europe and Mediterranean theaters.
On 14 August 1942, the first confirmed victory by a USAAF unit over a German aircraft in World War II was achieved by a P-40C pilot. 2nd Lt Joseph D. Shaffer, of the 33rd Fighter Squadron, intercepted a Focke-Wulf Fw 200C-3 maritime patrol plane that overflew his base at Reykjavík, Iceland. Shaffer damaged the Fw 200, which was finished off by a P-38F.
Warhawks were used extensively in the Mediterranean Theater of Operations (MTO) by USAAF units, including the 33rd, 57th, 58th, 79th, 324th and 325th Fighter Groups.
While the P-40 suffered heavy loses in the MTO, many USAAF P-40 units achieved high kill-to-loss ratios against Axis aircraft. For example, the 324th FG scored better than a 2:1 ratio in the MTO. In all, 23 U.S. pilots became aces in the MTO while flying the P-40, most of them during the first half of 1943. As in the Pacific, success in combat depended in part on experience and effective tactics.
Individual pilots from the 57th FG were the first USAAF P-40 pilots to see action in the MTO, while attached to Desert Air Force Kittyhawk squadrons, from July 1942. The 57th was also the main unit involved in the "Palm Sunday Massacre", on 18 April 1943. Decoded Ultra signals had given away a plan for a large formation of German Junkers Ju 52 transports to cross the Mediterranean, escorted by German and Italian fighters. Between 1630 and 1830 hours, all wings of the group were engaged in an intensive effort against the enemy air transports. Of the four Kittyhawk wings, three had left the patrol area before a convoy of a 100+ enemy transports were sighted by 57 Group, which tallied 74 aircraft destroyed. 57 Group was last in the area, and intercepted the Ju 52s escorted by large numbers of Bf 109s, Bf 110s and Macchi C.202s. In all, they claimed 58 Ju 52s, 14 Bf 109s and two Bf 110s destroyed, with a number of others probably destroyed and damaged. Between 20–40 of the E/A landed on the beaches around Cap Bon to avoid being shot down. Six Allied fighters were lost, five of them P-40s.
On 22 April, in Operation Flax, a similar force of P-40s attacked a formation of 14 Messerschmitt Me 323 "Gigant" ("Giant") six-engine transports, covered by seven Bf 109s from II./JG 27. All the transports were shot down, for a loss of three P-40s destroyed. The 57th FG was equipped with the Curtiss fighter until early 1944, during which time they were credited with at least 140 air-to-air kills.
On 23 February 1943, during Operation Torch, the pilots of the 58th FG flew 75 P-40Ls off the aircraft carrier to the newly captured Vichy French airfield, Cazas, near Casablanca, in French Morocco. The aircraft resupplied the 33rd FG and the pilots were reassigned.
The 325th FG (known as the "Checkertail Clan") flew P-40s in the MTO. The 325th was credited with at least 133 air-to-air kills in April–October 1943, of which 95 were Bf 109s and 26 were Macchi C.202s, for the loss of 17 P-40s in combat. An anecdote concerning the 325th FG, indicates what could happen if Bf 109 pilots made the mistake of trying to out-turn the P-40. 325th FG historian Carol Cathcart wrote: "on 30 July, 20 P-40s of the 317th Squadron ... took off on a fighter sweep ... over Sardinia. As they turned to fly south over the west part of the island, they were attacked near Sassari... The attacking force consisted of 25 to 30 Bf 109s and Macchi C.202s... In the brief, intense battle that occurred ... 317th claimed 21 enemy aircraft." Cathcart states that Lt. Robert Sederberg who assisted a comrade being attacked by five Bf 109s, destroyed at least one German aircraft, and may have shot down as many as five. Sederberg was shot down in the dogfight and became a prisoner of war.
A famous African-American unit, the 99th FS, better known as the "Tuskegee Airmen" or "Redtails", flew P-40s in stateside training and for their initial eight months in the MTO. On 9 June 1943, they became the first African-American fighter pilots to engage enemy aircraft, over Pantelleria, Italy. A single Focke Wulf Fw 190 was reported damaged by Lieutenant Willie Ashley Jr. On 2 July the squadron claimed its first verified kill; a Fw 190 destroyed by Captain Charles Hall. The 99th continued to score with P-40s until February 1944, when they were assigned P-39s and P-51 Mustangs.
The much-lightened P-40L was most heavily used in the MTO, primarily by U.S. pilots. Many US pilots stripped down their P-40s even further to improve performance, often removing two or more of the wing guns from the P-40F/L.
Royal Australian Air Force.
The Kittyhawk was the main fighter used by the RAAF in World War II, in greater numbers than the Spitfire. Two RAAF squadrons serving with the Desert Air Force, No. 3 and No. 450 Squadrons, were the first Australian units to be assigned P-40s. Other RAAF pilots served with RAF or SAAF P-40 squadrons in the theater.
Many RAAF pilots achieved high scores in the P-40. At least five reached "double ace" status: Clive Caldwell, Nicky Barr, John Waddy, Bob Whittle (11 kills each) and Bobby Gibbes (10 kills) in the Middle East, North African and/or New Guinea campaigns. In all, 18 RAAF pilots became aces while flying P-40s.
Nicky Barr, like many Australian pilots, considered the P-40 a reliable mount: "The Kittyhawk became, to me, a friend. It was quite capable of getting you out of trouble more often than not. It was a real warhorse."
At the same time as the heaviest fighting in North Africa, the Pacific War was also in its early stages, and RAAF units in Australia were completely lacking in suitable fighter aircraft. Spitfire production was being absorbed by the war in Europe; P-38s were trialled, but were difficult to obtain; Mustangs had not yet reached squadrons anywhere, and Australia's tiny and inexperienced aircraft industry was geared towards larger aircraft. USAAF P-40s and their pilots originally intended for the U.S. Far East Air Force in the Philippines, but diverted to Australia as a result of Japanese naval activity were the first suitable fighter aircraft to arrive in substantial numbers. By mid-1942, the RAAF was able to obtain some USAAF replacement shipments; the P-40 was given the RAAF designation A-29.
RAAF Kittyhawks played a crucial role in the South West Pacific theater. They fought on the front line as fighters during the critical early years of the Pacific War, and the durability and bomb-carrying abilities (1,000 lb/454 kg) of the P-40 also made it ideal for the ground attack role. For example, 75, and 76 Squadrons played a critical role during the Battle of Milne Bay, fending off Japanese aircraft and providing effective close air support for the Australian infantry, negating the initial Japanese advantage in light tanks and sea power.
The RAAF units that most used Kittyhawks in the South West Pacific were 75, 76, 77, 78, 80, 82, 84 and 86 Squadrons. These squadrons saw action mostly in the New Guinea and Borneo campaigns.
Late in 1945, RAAF fighter squadrons in the South West Pacific began converting to P-51Ds. However, Kittyhawks were in use with the RAAF until the end of the war, in Borneo. In all, the RAAF acquired 841 Kittyhawks (not counting the British-ordered examples used in North Africa), including 163 P-40E, 42 P-40K, 90 P-40 M and 553 P-40N models. In addition, the RAAF ordered 67 Kittyhawks for use by No. 120 (Netherlands East Indies) Squadron (a joint Australian-Dutch unit in the South West Pacific). The P-40 was retired by the RAAF in 1947.
Royal Canadian Air Force.
A total of 13 Royal Canadian Air Force units operated the P-40 in the North West European or Alaskan theaters.
In mid-May 1940, Canadian and US officers watched comparative tests of a XP-40 and a Spitfire, at RCAF Uplands, Ottawa. While the Spitfire was considered to have performed better, it was not available for use in Canada and the P-40 was ordered to meet home air defense requirements. In all, eight Home War Establishment Squadrons were equipped with the Kittyhawk: 72 Kittyhawk I, 12 Kittyhawk Ia, 15 Kittyhawk III and 35 Kittyhawk IV aircraft, for a total of 134 aircraft. These aircraft were mostly diverted from RAF Lend-Lease orders for service in Canada. The P-40 Kittyhawks were obtained in lieu of 144 P-39 Airacobras originally allocated to Canada but reassigned to the RAF.
However, before any home units received the P-40, three RCAF Article XV squadrons operated Tomahawk aircraft from bases in the United Kingdom. No. 403 Squadron RCAF, a fighter unit, used the Tomahawk Mk II briefly before converting to Spitfires. Two Army Co-operation (close air support) squadrons: 400 and 414 Sqns trained with Tomahawks, before converting to Mustang Mk. I aircraft and a fighter/reconnaissance role. Of these, only No. 400 Squadron used Tomahawks operationally, conducting a number of armed sweeps over France in the late 1941. RCAF pilots also flew Tomahawks or Kittyhawks with other British Commonwealth units based in North Africa, the Mediterranean, South East Asia and (in at least one case) the South West Pacific.
In 1942, the Imperial Japanese Navy occupied two islands, Attu and Kiska, in the Aleutians, off Alaska. RCAF home defense P-40 squadrons saw combat over the Aleutians, assisting the USAAF. The RCAF initially sent 111 Squadron, flying the Kittyhawk I, to the US base on Adak island. During the drawn-out campaign, 12 Canadian Kittyhawks operated on a rotational basis from a new, more advanced base on Amchitka, southeast of Kiska. 14 and 111 Sqns took "turn-about" at the base. During a major attack on Japanese positions at Kiska on 25 September 1942, Squadron Leader Ken Boomer shot down a Nakajima A6M2-N ("Rufe") seaplane. The RCAF also purchased 12 P-40Ks directly from the USAAF while in the Aleutians. After the Japanese threat diminished, these two RCAF squadrons returned to Canada and eventually transferred to England without their Kittyhawks.
In January 1943, a further Article XV unit, 430 Squadron was formed at RAF Hartford Bridge, England and trained on obsolete Tomahawk IIA. The squadron converted to the Mustang I before commencing operations in mid-1943.
In early 1945 pilots from No. 133 Squadron RCAF, operating the P-40N out of RCAF Patricia Bay, (Victoria, BC), intercepted and destroyed two Japanese balloon-bombs, which were designed to cause wildfires on the North American mainland. On 21 February, Pilot Officer E. E. Maxwell shot down a balloon, which landed on Sumas Mountain in Washington State. On 10 March, Pilot Officer J. 0. Patten destroyed a balloon near Saltspring Island, BC. The last interception took place on 20 April 1945 when Pilot Officer P.V. Brodeur from 135 Squadron out of Abbotsford, British Columbia shot down a balloon over Vedder Mountain.
The RCAF units that operated P-40s were, in order of conversion: 
Royal New Zealand Air Force.
Some Royal New Zealand Air Force (RNZAF) pilots and New Zealanders in other air forces flew British P-40s while serving with DAF squadrons in North Africa and Italy, including the ace Jerry Westenra.
A total of 301 P-40s were allocated to the RNZAF under Lend-Lease, for use in the Pacific Theater, although four of these were lost in transit. The aircraft equipped 14 Squadron, 15 Squadron, 16 Squadron, 17 Squadron, 18 Squadron, 19 Squadron and 20 Squadron.
RNZAF P-40 squadrons were successful in air combat against the Japanese between 1942 and 1944. Their pilots claimed 100 aerial victories in P-40s, whilst losing 20 aircraft in combat Geoff Fisken, the highest scoring British Commonwealth ace in the Pacific, flew P-40s with 15 Squadron, although half of his victories were claimed with the Brewster Buffalo.
The overwhelming majority of RNZAF P-40 victories were scored against Japanese fighters, mostly Zeroes. Other victories included Aichi D3A "Val" dive bombers. The only confirmed twin engine claim, a Ki-21 "Sally" (misidentified as a G4M "Betty") fell to Fisken in July 1943.
From late 1943 and 1944, RNZAF P-40s were increasingly used against ground targets, including the innovative use of naval depth charges as improvised high-capacity bombs. The last front line RNZAF P-40s were replaced by Vought F4U Corsairs in 1944. The P-40s were relegated to use as advanced pilot trainers.
The remaining RNZAF P-40s, excluding the 20 shot down and 154 written off, were mostly scrapped at Rukuhia in 1948.
Soviet Union.
The Soviet "Voyenno-Vozdushnye Sily" (VVS; "Military Air Forces") and "Morskaya Aviatsiya" (MA; "Naval Air Service") also referred to P-40s as "Tomahawks" and "Kittyhawks". In fact, the Curtiss P-40 Tomahawk / Kittyhawk was the first Allied fighter supplied to the USSR under the Lend-Lease agreement.
The Soviet units received 247 P-40B/Cs (equivalent to the Tomahawk IIA/B in RAF service) and 2,178 P-40E, -K, -L, and -N models between 1941 and 1944. The Tomahawks were shipped from Great Britain and directly from the US, many of them arriving incomplete, lacking machine guns and even the lower half of the engine cowling. In late September 1941, the first 48 P-40s were assembled and checked in USSR. Test flights showed some manufacturing defects: generator and oil pump gears and generator shafts failed repeatedly, which led to emergency landings. The test report indicated that the Tomahawk was inferior to Soviet "M-105P-powered production fighters in speed and rate of climb. However, it had good short field performance, horizontal manoeuvrability, range and endurance."
Nevertheless, Tomahawks and Kittyhawks were used against the Germans. The 126th IAP fighting on the Western and Kalinin fronts were the first unit to receive the P-40. The regiment entered action on 12 October 1941. By 15 November 1941, that unit had shot down 17 German aircraft. However, Lt (SG) Smirnov noted that the P-40 armament was sufficient for strafing enemy lines but rather ineffective in aerial combat. Another pilot, S.G. Ridnyy (Hero of Soviet Union), remarked that he had to shoot half the ammunition at 50–100 meters (164–339 ft) to shoot down an enemy aircraft.
In January 1942, some 198 aircraft sorties were flown (334 flying hours) and 11 aerial engagements were conducted, in which five Bf 109s, one Ju 88, and one He 111 were downed. These statistics reveal a surprising fact: it turns out that the Tomahawk was fully capable of successful air combat with a Bf 109. The reports of pilots about the circumstances of the engagements confirm this fact. On 18 January 1942, Lieutenants S. V. Levin and I. P. Levsha (in pair) fought an engagement with seven Bf 109s and shot down two of them without loss. On 22 January, a flight of three aircraft led by Lieutenant E. E. Lozov engaged 13 enemy aircraft and shot down two Bf 109Es, again without loss. Altogether, in January, two Tomahawks were lost; one downed by German antiaircraft artillery and one lost to Messerschmitts.
The Soviets stripped down their P-40s significantly for combat, in many cases removing the wing guns altogether in P-40B/C types, for example. Soviet Air Force reports state that they liked the range and fuel capacity of the P-40, which were superior to most of the Soviet fighters, though they still preferred the P-39. Soviet pilot Nikolai G. Golodnikov recalled: "The cockpit was vast and high. At first it felt unpleasant to sit waist-high in glass, as the edge of the fuselage was almost at waist level. But the bullet-proof glass and armoured seat were strong and visibility was good. The radio was also good. It was powerful, reliable, but only on HF (high frequency). The American radios did not have hand microphones but throat microphones. These were good throat mikes: small, light and comfortable." The biggest complaint of some Soviet airmen was its poor climb rate and problems with maintenance, especially with burning out the engines. VVS pilots usually flew the P-40 at War Emergency Power settings while in combat, bringing the acceleration and speed performance closer to that of their German rivals, but could burn out engines in a matter of weeks. They also had difficulty with the more demanding requirements for fuel quality and oil purity of the Allison engines. A fair number of burnt out P-40s were re-engined with Soviet Klimov engines but these performed relatively poorly and were relegated to rear area use.
The P-40 saw the most front line use in Soviet hands in 1942 and early 1943. Deliveries over the Alaska-Siberia ALSIB ferry route began in October 1942. It was used in the northern sectors and played a significant role in the defense of Leningrad. The most numerically important types were P-40B/C, P-40E and P-40K/M. By the time the better P-40F and N types became available, production of superior Soviet fighters had increased sufficiently so that the P-40 was replaced in most Soviet Air Force units by the Lavochkin La-5 and various later Yakovlev types. In spring 1943, Lt D.I. Koval of the 45th IAP gained ace status on the North-Caucasian front, shooting down six German aircraft flying a P-40. Some Soviet P-40 squadrons had good combat records. While Soviet pilots became aces on the P-40, not as many as on the P-39 Airacobra, which was the most numerous Lend Lease fighter used by the Soviet Union. However Soviet commanders considered the Kittyhawk to significantly outclass the Hurricane, although it was "not in the same league as the Yak-1".
Japan.
The Japanese Army captured some P-40s and later operated a number in Burma. The Japanese appear to have had as many as 10 flyable P-40Es. For a brief period in 1943, a few of them were actually used operationally by 2 "Hiko Chutai", 50 "Hiko Sentai" (2nd Air Squadron, 50th Air Regiment) in the defense of Rangoon. Testimony of this is given by Yasuhiko Kuroe, a member of the 64 "Hiko Sentai". In his memoirs, he says one Japanese-operated P-40 was shot down in error by a friendly Mitsubishi Ki-21 "Sally" over Rangoon.
Other nations.
The P-40 was used by over two dozen countries during and after the war. The P-40 was used by Brazil, Egypt, Finland and Turkey. The last P-40s in military service, used by the Brazilian Air Force (FAB), were retired in 1954.
In the air war over Finland, several Soviet P-40s were shot down or had to crash-land due to other reasons. The Finns, short of good aircraft, collected these and managed to repair one P-40M, P-40M-10-CU 43-5925, "white 23", which received Finnish Air Force serial number KH-51 (KH denoting "Kittyhawk", as the British designation of this type was Kittyhawk III). This aircraft was attached to an operational squadron HLeLv 32 of the Finnish Air Force, but lack of spares kept it on the ground, with the exception of a few evaluation flights.
Several P-40Ns were used by the Royal Netherlands East Indies Army Air Force against the Japanese before being used during the fighting in Indonesia until February 1949.
Variants and development stages.
This new liquid-cooled engine fighter had a radiator mounted under the rear fuselage
but the prototype XP-40 was later modified and the radiator was moved forward under the engine.
Survivors.
On 11 May 2012, a crashed P-40 was found in the Sahara desert. No trace of the pilot has been found to date. Due to the extreme arid conditions, little corrosion of the metal surfaces occurred. The conditions in which it was found are similar to those preferred for aircraft boneyard. Plans are being made to move the aircraft to a British museum.
Of the 13,738 P-40s built, only 28 P-40s remain airworthy, with three of them being converted to dual-controls/dual-seat configuration. Approximately 13 aircraft are on static display and another 36 airframes are under restoration for either display or flight.

</doc>
<doc id="7212" url="https://en.wikipedia.org/wiki?curid=7212" title="Creed">
Creed

A creed (also "confession", "symbol", or "statement of faith") is a statement of the shared beliefs of a religious community in the form of a fixed formula summarizing core tenets.
One of the most widely used creeds in Christianity is the Nicene Creed, first formulated in AD 325 at the First Council of Nicaea. It was based on Christian understanding of the Canonical Gospels, the letters of the New Testament and to a lesser extent the Old Testament. Affirmation of this creed, which describes the Trinity, is generally taken as a fundamental test of orthodoxy for most Christian denominations. The Apostles' Creed is also broadly accepted. Some Christian denominations and other groups have rejected the authority of those creeds.
Muslims declare the "shahada", or testimony: "I bear witness that there is no god but (the One) God "(Allah)", and I bear witness that Muhammad is God's messenger."
Whether Judaism is creedal has been a point of some controversy. Although some say Judaism is noncreedal in nature, others say it recognizes a single creed, the "Shema Yisrael", which begins: "Hear, O Israel: the our God, the is one."
Terminology.
The word "creed" is particularly used for a concise statement which is recited as part of liturgy. The term is anglicized from Latin "credo" "I believe", the incipit of the Latin texts of the Apostles' Creed and the Nicene Creed. A creed is sometimes referred to as a "symbol" in a specialized meaning of that word (which was first introduced to Late Middle English in this sense), after Latin "symbolum" "creed" (as in "Symbolum Apostolorum" = "Apostles' Creed"), after Greek "symbolon" "token, watchword"
Some longer statements of faith in the Protestant tradition are instead called "confessions of faith", or simply "confession" (as in e.g. Helvetic Confession).
Within Evangelicalism, the terms "doctrinal statement" or "doctrinal basis" tend to be preferred. Doctrinal statements may include positions on lectionary and translations of the Bible, particularly in fundamentalist churches of the King James Only movement.
The term "creed" is sometimes extended to comparable concepts in non-Christian theologies; thus the Islamic concept of "ʿaqīdah" (literally "bond, tie") is often rendered as "creed".
Christian creeds.
Several creeds have originated in Christianity.
Christian confessions of faith.
Protestant denominations are usually associated with confessions of faith, which are similar to creeds but usually longer.
Christians without creeds.
Some Christian denominations, and particularly those descending from the Radical Reformation, do not profess a creed. The Quakers, also known as the Religious Society of Friends, believe that they have no need for creedal formulations of faith. The Church of the Brethren also espouses no creed, referring to the New Testament, as their "rule of faith and practice." Jehovah's Witnesses contrast "memorizing or repeating creeds" with acting to "do what Jesus said". Unitarian Universalists, who practice probably the most liberal of all religions, do not share a creed.
Many evangelical Protestants similarly reject creeds as definitive statements of faith, even while agreeing with some creeds' substance. The Baptists have been non-creedal "in that they have not sought to establish binding authoritative confessions of faith on one another". While many Baptists are not opposed to the ancient creeds, they regard them as "not so final that they cannot be revised and re-expressed. At best, creeds have a penultimacy about them and, of themselves, could never be the basis of Christian fellowship". Moreover, Baptist "confessions of faith" have often had a clause such as this from the First London (Particular) Baptist Confession (Revised edition, 1646):
Similar reservations about the use of creeds can be found in the Restoration Movement and its descendants, the Christian Church (Disciples of Christ), the Churches of Christ, and the Christian churches and churches of Christ. Restorationists profess "no creed but Christ".
Bishop John Shelby Spong, retired Episcopal Bishop of Newark, has written that dogmas and creeds were merely "a stage in our development" and "part of our religious childhood." In his book, "Sins of the Scripture", Spong claims that "Jesus seemed to understand that no one can finally fit the holy God into his or her creeds or doctrines. That is idolatry."
Latter Day Saints.
Within the sects of the Latter Day Saint movement, the "Articles of Faith" are a list composed by Joseph Smith as part of an 1842 letter sent to "Long" John Wentworth, editor of the "Chicago Democrat". It is canonized with the "Pearl of Great Price", part of the standard works of The Church of Jesus Christ of Latter-day Saints.
Creedal works include:
Jewish creed.
Whether Judaism is creedal in character has generated some controversy. Rabbi Milton Steinberg wrote that "By its nature Judaism is averse to formal creeds which of necessity limit and restrain thought" and asserted in his book "Basic Judaism" (1947) that "Judaism has never arrived at a creed." The 1976 Centenary Platform of the Central Conference of American Rabbis, an organization of Reform rabbis, agrees that "Judaism emphasizes action rather than creed as the primary expression of a religious life."
Others, however, characterize the Shema Yisrael as a creedal statement in strict monotheism embodied in a single prayer: "Hear O Israel, the Lord is our God, the Lord is One" (; transliterated "Shema Yisrael Adonai Eloheinu Adonai Echad").
A notable statement of Jewish principles of faith was drawn up by Maimonides as his 13 Principles of Faith.
Islamic creed.
The shahada, the two-part statement that "There is no god but Allah; Muhammad is the messenger of Allah" is often popularly called "the Islamic creed" and its utterance is one of the "five pillars" of Sunni Islam.
In Islamic theology, the term most closely corresponding to "creed" is "ʿaqīdah" ()
The first such creed was written as "a short answer to the pressing heresies of the time" is known as "Al-Fiqh Al-Akbar" and ascribed to Abū Ḥanīfa. Two well known creeds were the "Fiqh Akbar II" "representative" of the al-Ash'ari, and "Fiqh Akbar III", "representative" of the Ash-Shafi'i. Al-Ghazali also had a ʿAqīdah.
"Iman" () in Islamic theology denotes a believer's religious faith . Its most simple definition is the belief in the six articles of faith, known as "arkān al-īmān".

</doc>
<doc id="7213" url="https://en.wikipedia.org/wiki?curid=7213" title="Claudius Aelianus">
Claudius Aelianus

Claudius Aelianus (; c. 175 – c. 235 CE), often seen as just Aelian (), born at Praeneste, was a Roman author and teacher of rhetoric who flourished under Septimius Severus and probably outlived Elagabalus, who died in 222. He spoke Greek so perfectly that he was called "honey-tongued" ("meliglossos"); Roman-born, he preferred Greek authors, and wrote in a slightly archaizing Greek himself.
His two chief works are valuable for the numerous quotations from the works of earlier authors, which are otherwise lost, and for the surprising lore, which offers unexpected glimpses into the Greco-Roman world-view.
"De Natura Animalium".
"On the Nature of Animals" ("On the Characteristics of Animals" is an alternative title; Greek: Περὶ Ζῴων Ἰδιότητος; usually cited, though, by its Latin title: "De Natura Animalium") is a curious collection, in seventeen books, of brief stories of natural history, sometimes selected with an eye to conveying allegorical moral lessons, sometimes because they are just so astonishing:
The Loeb Classical Library introduction characterizes the book as
Aelian's anecdotes on animals rarely depend on direct observation: they are almost entirely taken from written sources, often Pliny the Elder, but also other authors and works now lost, to whom he is thus a valuable witness. He is more attentive to marine life than might be expected, though, and this seems to reflect first-hand personal interest; he often quotes "fishermen". At times he strikes the modern reader as thoroughly credulous, but at others he specifically states that he is merely reporting what is told by others, and even that he does not believe them. Aelian's work is one of the sources of medieval natural history and of the bestiaries of the Middle Ages.
The text as it has come down to us is badly mangled and garbled and replete with later interpolations. Conrad Gessner (or Gesner), the Swiss scientist and natural historian of the Renaissance, made a Latin translation of Aelian's work, to give it a wider European audience. An English translation by A. F. Scholfield has been published in the Loeb Classical Library, 3 vols. (19[ ]-59).
"Varia Historia".
"Various History" (Ποικίλη Ἱστορία) — for the most part preserved only in an abridged form — is Aelian's other well-known work, a miscellany of anecdotes and biographical sketches, lists, pithy maxims, and descriptions of natural wonders and strange local customs, in 14 books, with many surprises for the cultural historian and the mythographer, anecdotes about the famous Greek philosophers, poets, historians, and playwrights and myths instructively retold. The emphasis is on "various" moralizing tales about heroes and rulers, athletes and wise men; reports about food and drink, different styles in dress or lovers, local habits in giving gifts or entertainments, or in religious beliefs and death customs; and comments on Greek painting. Aelian gives an account of fly fishing, using lures of red wool and feathers, of lacquerwork, serpent worship — Essentially the "Various History" is a Classical "magazine" in the original senses of that word. He is not perfectly trustworthy in details, and his agenda was heavily influenced by Stoic opinions, perhaps so that his readers will not feel guilty, but Jane Ellen Harrison found survivals of archaic rites mentioned by Aelian very illuminating in her "Prolegomena to the Study of Greek Religion" (1903, 1922).
The first printing was in 1545. The standard modern text is Mervin R. Dilts's, of 1974.
Two English translations of the "Various History," by Fleming (1576) and Stanley (1665) made Aelian's miscellany available to English readers, but after 1665 no English translation appeared, until three English translations appeared almost simultaneously: James G. DeVoto, "Claudius Aelianus: Ποιϰίλης Ἱοτορίας ("Varia Historia")" Chicago, 1995; Diane Ostrom Johnson, "An English Translation of Claudius Aelianus' "Varia Historia"", 1997; and N. G. Wilson, "Aelian: Historical Miscellany" in the Loeb Classical Library.
Other works.
Considerable fragments of two other works, "On Providence" and "Divine Manifestations", are preserved in the early medieval encyclopedia, the "Suda." Twenty "letters from a farmer" after the manner of Alciphron are also attributed to him. The letters are invented compositions to a fictitious correspondent, which are a device for vignettes of agricultural and rural life, set in Attica, though mellifluous Aelian once boasted that he had never been outside Italy, never been aboard a ship (which is at variance, though, with his own statement, "de Natura Animalium" XI.40, that he had seen the bull Serapis with his own eyes). Thus conclusions about actual agriculture in the "Letters" are as likely to evoke Latium as Attica. The fragments have been edited in 1998 by D. Domingo-Foraste, but are not available in English. The "Letters" are available in the Loeb Classical Library, translated by Allen Rogers Benner and Francis H. Fobes (1949).

</doc>
<doc id="7214" url="https://en.wikipedia.org/wiki?curid=7214" title="Callisto (mythology)">
Callisto (mythology)

In Greek mythology, Callisto or Kallisto (; ) was a nymph of Lycaon. Transformed into a bear and set among the stars, she was the bear-mother of the Arcadians, through her son Arcas.
The fourth Galilean moon of Jupiter is named after Callisto.
Origin of the myth.
The name "Kalliste" (Καλλίστη), "most beautiful", may be recognized as an epithet of the goddess herself, though none of the inscriptions at Athens that record priests of "Artemis Kalliste" (Άρτεμις Καλλίστη), dates before the third century BCE. Artemis Kalliste was worshipped in Athens in a shrine which lay outside the Dipylon gate, by the side of the road to the Academy. W. S. Ferguson suggested that Artemis Soteira and Artemis Kalliste were joined in a common cult administered by a single priest. The bearlike character of Artemis herself was a feature of the Brauronia.
The myth in "Catasterismi" may be derived from the fact that a set of constellations appear close together in the sky, in and near the Zodiac sign of Libra, namely Ursa Minor, Ursa Major, Boötes, and Virgo. The constellation Boötes, was explicitly identified in the Hesiodic "Astronomia" (Αστρονομία) as Arcas, the "Bear-warden" ("Arktophylax"; Αρκτοφύλαξ):
"He is Arkas the son of Kallisto and Zeus, and he lived in the country about Lykaion. After Zeus had seduced Kallisto, Lykaon, pretending not to know of the matter, entertained Zeus, as Hesiod says, and set before him on the table the babe which he had cut up."
Aeschylus' tragedy "Callisto" is lost.
According to Julien d'Huy who used phylogenetical and statistical tools, the story could be a recent transformation of a Palaeolithical myth
Myth.
As a follower of Artemis, Callisto, who Hesiod said was the daughter of Lycaon, king of Arcadia, took a vow to remain a virgin, as did all the nymphs of Artemis. But to have sex with her Zeus disguised himself as Artemis (Diana) herself, in order to lure her into his embrace. Callisto was then turned into a bear, as Hesiod had told it:
"...but afterwards, when she was already with child, was seen bathing and so discovered. Upon this, the goddess was enraged and changed her into a beast. Thus she became a bear and gave birth to a son called Arcas."
Either Artemis "slew Kallisto with a shot of her silver bow," perhaps urged by the wrath of Juno (Hera) or later Arcas, the eponym of Arcadia, nearly killed his bear-mother, when she had wandered into the forbidden precinct of Zeus. In every case, Zeus placed them both in the sky as the constellations Ursa Major, called "Arktos" (αρκτος), the "Bear", by Greeks, and Ursa Minor.
According to Ovid, it was Jupiter (Zeus) who took the form of Diana (Artemis) so that he might evade his wife Juno’s detection, forcing himself upon Callisto while she was separated from Diana and the other nymphs. Callisto's subsequent pregnancy was discovered several months later while she was bathing with Diana and her fellow nymphs. Diana became enraged when she saw that Callisto was pregnant and expelled her from the group. Callisto later gave birth to Arcas. Juno then took the opportunity to avenge her wounded pride and transformed the nymph into a bear. Sixteen years later Callisto, still a bear, encountered her son Arcas hunting in the forest. Just as Arcas was about to kill his own mother with his javelin, Jupiter averted the tragedy by placing mother and son amongst the stars as Ursa Major and Minor, respectively. Juno, enraged that her attempt at revenge had been frustrated, appealed to Oceanus that the two might never meet his waters, thus providing a poetic explanation for their circumpolar positions.
The stars of Ursa Major were all circumpolar in Athens of 400 BCE, and all but the stars in the Great Bear's left foot were circumpolar in Ovid's Rome, in the first century CE. Now, however, due to the precession of the equinoxes, the feet of the Great Bear constellation do sink below the horizon from Rome and especially from Athens; however, Ursa Minor (Arcas) does remain completely above the horizon, even from latitudes as far south as Honolulu and Hong Kong.

</doc>
<doc id="7218" url="https://en.wikipedia.org/wiki?curid=7218" title="Cookie">
Cookie

A cookie is a small, flat, sweet, baked good, usually containing flour, eggs, sugar, and either butter, cooking oil or another oil or fat. It may include other ingredients such as raisins, oats, chocolate chips or nuts. 
In most English-speaking countries except for the US and Canada, crisp cookies are called biscuits. Chewier cookies are commonly called "cookies" even in the UK. Some cookies may also be named by their shape, such as date squares or bars.
Cookies or biscuits may be mass-produced in factories, made in small bakeries or home-made. Biscuit or cookie variants include sandwich biscuits such as Custard creams, Jammy Dodgers, Bourbons and Oreos, marshmallow or jam and dipping the cookie in chocolate or another sweet coating. Cookies are often served with beverages such as milk, coffee or tea. Factory-made cookies are sold in grocery stores, convenience stores and vending machines. Fresh-baked cookies are sold at bakeries and coffeehouses, with the latter ranging from small business-sized establishments to multinational corporations such as Starbucks.
Terminology.
In most English-speaking countries outside North America, including the United Kingdom, the most common word for a crisp cookie is biscuit. The term cookie is normally used to describe chewier ones. However, in many regions both terms are used.
In Scotland the term cookie is sometimes used to describe a plain bun.
Cookies that are baked as a solid layer on a sheet pan and then cut, rather than being baked as individual pieces, are called bar cookies or traybakes.
Etymology.
Its American name derives from the Dutch word "koekje" or more precisely its informal, dialect variant "koekie" which means "little cake," and arrived in American English with the Dutch settlement of New Netherland, in the early 1600s.
According to the Scottish National Dictionary, its Scottish name derives from the diminutive form (+ suffix "-ie") of the word "cook", giving the Middle Scots "cookie", "cooky" or "cu(c)kie". It also gives an alternative etymology, from the Dutch word "koekje", the diminutive of "koek", a cake. There was much trade and cultural contact across the North Sea between the Low Countries and Scotland during the Middle Ages, which can also be seen in the history of curling and, perhaps, golf.
Description.
Cookies are most commonly baked until crisp or just long enough that they remain soft, but some kinds of cookies are not baked at all. Cookies are made in a wide variety of styles, using an array of ingredients including sugars, spices, chocolate, butter, peanut butter, nuts, or dried fruits. The softness of the cookie may depend on how long it is baked.
A general theory of cookies may be formulated this way. Despite its descent from cakes and other sweetened breads, the cookie in almost all its forms has abandoned water as a medium for cohesion. Water in cakes serves to make the base (in the case of cakes called "batter") as thin as possible, which allows the bubbles – responsible for a cake's fluffiness – to better form. In the cookie, the agent of cohesion has become some form of oil. Oils, whether they be in the form of butter, vegetable oils, or lard, are much more viscous than water and evaporate freely at a much higher temperature than water. Thus a cake made with butter or eggs instead of water is far denser after removal from the oven.
Oils in baked cakes do not behave as soda tends to in the finished result. Rather than evaporating and thickening the mixture, they remain, saturating the bubbles of escaped gases from what little water there might have been in the eggs, if added, and the carbon dioxide released by heating the baking powder. This saturation produces the most texturally attractive feature of the cookie, and indeed all fried foods: crispness saturated with a moisture (namely oil) that does not sink into it.
History.
Cookie-like hard wafers have existed for as long as baking is documented, in part because they deal with travel very well, but they were usually not sweet enough to be considered cookies by modern standards.
Cookies appear to have their origins in 7th century AD Persia, shortly after the use of sugar became relatively common in the region. They spread to Europe through the Muslim conquest of Spain. By the 14th century, they were common in all levels of society throughout Europe, from royal cuisine to street vendors.
With global travel becoming widespread at that time, cookies made a natural travel companion, a modernized equivalent of the travel cakes used throughout history. One of the most popular early cookies, which traveled especially well and became known on every continent by similar names, was the jumble, a relatively hard cookie made largely from nuts, sweetener, and water.
Cookies came to America through the Dutch in New Amsterdam in the late 1620s. The Dutch word "koekje" was Anglicized to "cookie" or cooky. The earliest reference to cookies in America is in 1703, when "The Dutch in New York provided...'in 1703...at a funeral 800 cookies...'"
The most common modern cookie, given its style by the creaming of butter and sugar, was not common until the 18th century.
Classification.
Cookies are broadly classified according to how they are formed, including at least these categories:
Cookies also may be decorated with an icing, especially chocolate, and closely resemble a type of confectionery.

</doc>
<doc id="7220" url="https://en.wikipedia.org/wiki?curid=7220" title="Common Gateway Interface">
Common Gateway Interface

Common Gateway Interface (CGI) is a standard way for web servers to interface with executable programs installed on a server that generate web pages dynamically.
Such programs are known as "CGI scripts" or simply "CGIs"; they are usually written in a scripting language, but can be written in any programming language.
History.
In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification. The other Web server developers adopted it, and it has been a standard for Web servers ever since. A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined. This work resulted in RFC 3875, which specified CGI Version 1.1. Specifically mentioned in the RFC are the following contributors:
Overview.
Each Web server runs HTTP server software, which responds to requests from Web browsers. Generally, the HTTP server has a directory (folder), which is designated as a document collection — files that can be sent to Web browsers connected to this server. For example, if the Web server has the domain name codice_1, and its document collection is stored at codice_2 in the local file system, then the Web server will respond to a request for codice_3 by sending to the browser the (pre-written) file codice_4.
CGI extends this system by allowing the owner of the Web server to designate a directory within the document collection as containing executable scripts (or binary files) instead of pre-written pages; this is known as a CGI directory. For example, codice_5 could be designated as a CGI directory on the Web server. If a Web browser requests a URL that points to a file within the CGI directory (e.g., codice_6), then, instead of simply sending that file (codice_7) to the Web browser, the HTTP server runs the specified script and passes the output of the script to the Web browser. That is, anything that the script sends to standard output is passed to the Web client instead of being shown on-screen in a terminal window.
The CGI system also allows the Web browser to send information to the script via the URL or an HTTP POST request. If a slash and additional directory name(s) are appended to the URL immediately after the name of the script, then that path is stored in the codice_8 environment variable before the script is called. If parameters are sent to the script via an HTTP GET request (a question mark appended to the URL, followed by param=value pairs), then those parameters are stored in the codice_9 environment variable before the script is called. If parameters are sent to the script via an HTTP POST request, they are passed to the script's standard input. The script can then read these environment variables or data from standard input and adapt to the Web browser's request. 
Syntax.
The following Perl program shows all the environment variables passed by the Web server:
If a Web browser issues a request for the environment variables at codice_10, a 64-bit Microsoft Windows web server running cygwin returns the following information:
From the environment, it can be seen that the Web browser is Firefox running on a Windows 7 PC, the Web server is Apache running on a system that emulates Unix, and the CGI script is named codice_11.
The program could then generate any content, write that to standard output, and the Web server will transmit it to the browser.
The following are environment variables passed to CGI programs:
The program returns the result to the Web server in the form of standard output, beginning with a header and a blank line.
The header is encoded in the same way as an HTTP header and must include the MIME type of the document returned. The headers, supplemented by the Web server, are generally forwarded with the response back to the user.
Here is a simple CGI program in Python along with the HTML that handles a simple addition problem.<syntaxhighlight lang="html">
<!DOCTYPE html>
<html>
</html>
</syntaxhighlight><syntaxhighlight lang="python">
import cgi
import cgitb
cgitb.enable()
input_data=cgi.FieldStorage()
print 'Content-Type:text/html' #HTML is following
print #Leave a blank line
print 'Addition Results'
try:
except:
sum=num1+num2
print '{0} + {1} = {2}'.format(num1,num2,sum)'
</syntaxhighlight>This Python CGI gets the inputs from the HTML and adds the two numbers together.
Deployment.
A Web server that supports CGI can be configured to interpret a URL that it serves as a reference to a CGI script. A common convention is to have a codice_36 directory at the base of the directory tree and treat all executable files within this directory (and no other, for security) as CGI scripts. Another popular convention is to use filename extensions; for instance, if CGI scripts are consistently given the extension codice_37, the web server can be configured to interpret all such files as CGI scripts. While convenient, and required by many prepackaged scripts, it opens the server to attack if a remote user can upload executable code with the proper extension.
In the case of HTTP PUT or POSTs, the user-submitted data are provided to the program via the standard input. The Web server creates a subset of the environment variables passed to it and adds details pertinent to the HTTP environment.
Uses.
CGI is often used to process inputs information from the user and produce the appropriate output. An example of a CGI program is one implementing a Wiki. The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result. The web server receives the input from the CGI and transmits it to the user agent. If the "Edit this page" link is clicked, the CGI populates an HTML codice_38 or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.
Alternatives.
Calling a command generally means the invocation of a newly created process on the server. Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.
If the command is called often, the resulting workload can quickly overwhelm the server.
The overhead involved in interpretation may be reduced by using compiled CGI programs, such as those in C/C++, rather than using Perl or other interpreted languages. The overhead involved in process creation can be reduced by techniques such as FastCGI that "prefork" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_php.
Several approaches can be adopted for remedying this:
The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.

</doc>
<doc id="7222" url="https://en.wikipedia.org/wiki?curid=7222" title="Choctaw">
Choctaw

The Choctaw (In the Choctaw language, Chahta) are Native American people originally from the Southeastern United States (modern-day Mississippi, Florida, Alabama, and Louisiana). The Choctaw language belongs to the Muskogean language family group.
The Choctaw are descendants of the peoples of the Hopewell and Mississippian cultures, who lived throughout the east of the Mississippi River valley and its tributaries. About 1,700 years ago, the Hopewell people built Nanih Waiya, a great earthwork mound, which is still considered sacred by the Choctaw. The early Spanish explorers of the mid-16th century encountered Mississippian-culture villages and chiefs. The anthropologist John Swanton suggested that the Choctaw derived their name from an early leader. Henry Halbert, a historian, suggests that their name is derived from the Choctaw phrase "Hacha hatak" (river people).
The Choctaw coalesced as a people in the 17th century, and developed three distinct political and geographical divisions: eastern, western and southern, which sometimes created differing alliances with nearby European powers. These included the French, based on the Gulf Coast and in Louisiana, the English of the Southeast, and the Spanish of Florida and Louisiana during the colonial era. During the American Revolution, most Choctaw supported the Thirteen Colonies' bid for independence from the British Crown. They never went to war against the United States prior to Indian Removal.
In the 19th century, the Choctaw became known as one of the "Five Civilized Tribes" because they adopted numerous practices of their United States neighbors. The Choctaw and the United States (US) agreed to nine treaties and, by the last three, the US gained vast land cessions and deracinated most Choctaw west of the Mississippi River to Indian Territory. They were the first Native Americans forced under the Indian Removal Act. The Choctaw were exiled because the U.S. wanted to expand territory available for settlement by European Americans, to save the tribe from extinction, and to acquire their natural resources. The Choctaw negotiated the largest area and most desirable lands in Indian Territory. Their early government had three districts, each with its own chief, who together with the town chiefs sat on the National Council. They appointed a Choctaw Delegate to represent them with the US government in Washington, DC.
By the 1831 Treaty of Dancing Rabbit Creek, those Choctaw who chose to stay in the newly formed state of Mississippi were one of the first major non-European ethnic groups to become U.S. citizens. (Article 8 in the 1817 treaty with the Cherokee stated Cherokees may wish to become citizens of the United States.) During the American Civil War, the Choctaw in both Oklahoma and Mississippi mostly sided with the Confederate States of America. The Confederacy suggested it would support a state under Indian control if it won the war. In a new treaty after the war, the US required them to emancipate their slaves and offer them full citizenship; they have become known as Choctaw Freedmen. After the Civil War, the Mississippi Choctaw fell into obscurity for some time.
The Choctaw in Oklahoma struggled to build a nation, transferring the Choctaw Academy there and opening one for girls in the 1840s. In the aftermath of the Dawes Act, the US dissolved tribal governments and appointed chiefs. During World War I, Choctaw soldiers served in the U.S. military as the first Native American codetalkers, using the Choctaw language. After the Indian Reorganization Act of 1934, the Choctaw reconstituted their government, and the Choctaw Nation had kept their culture alive despite years of pressure for assimilation. The third largest federally recognized tribe, since the mid-twentieth century, they have created new institutions, such as a tribal college, housing authority, and justice system. Today the Choctaw Nation of Oklahoma, the Mississippi Band of Choctaw Indians, and the Jena Band of Choctaw Indians are the three federally recognized Choctaw tribes; Mississippi recognizes another band and smaller Choctaw groups are located in Louisiana and Texas.
History.
Paleo-Indian period.
Many thousands of years ago groups classified by anthropologists as Paleo-Indians lived in what today is referred to as the American South. These groups were hunter-gatherers who hunted a wide range of animals, including a variety of megafauna, which became extinct following the end of the Pleistocene age. The 19th-century historian Horatio Cushman noted that Choctaw oral history accounts suggested their ancestors had known of mammoths in the Tombigbee River area; this suggests that the Choctaw ancestors had been in the Mississippi area for at least 4,000–8,000 years. Cushman wrote: "the ancient Choctaw through their tradition (said) 'they saw the mighty beasts of the forests, whose tread shook the earth." Scholars believe that Paleo-Indians were specialized, highly mobile foragers who hunted late Pleistocene fauna such as bison, mastodons, caribou, and mammoths. Direct evidence in the Southeast is meager, but archaeological discoveries in related areas support this hypothesis.
Woodland culture.
Later cultures became more complex. Moundbuilding cultures included the Woodland period people who first built Nanih Waiya. Scholars believe the mound was contemporary with such earthworks as Igomar Mound in Mississippi and Pinson Mounds in Tennessee. Based on dating of surface artifacts, the Nanih Waiya mound was likely constructed and first occupied by indigenous peoples about 0-300 CE, in the Middle Woodland period.
The original site was bounded on three sides by an earthwork circular enclosure, about ten feet high and encompassing a square mile. Occupation of Nanih Waiya and several smaller nearby mounds likely continued through 700 CE, the Late Woodland Period. The smaller mounds may also have been built by later cultures. As they have been lost to cultivation since the late 19th century and the area has not been excavated, theories have been speculation.
Mississippian culture.
The Mississippian culture was a Native American culture that flourished in what is now the Midwestern, Eastern, and Southeastern United States from 800 to 1500 CE. The Mississippian culture developed in the lower Mississippi river valley and its tributaries, including the Ohio River. In present-day Mississippi, Moundville, Plaquemine,
When the Spanish made their first forays inland in the 16th century from the shores of the Gulf of Mexico, they encountered some chiefdoms of the Mississippians, but others were already in decline, or had disappeared. The Mississippian culture are the peoples encountered by other early Spanish explorers, beginning on April 2, 1513, with Juan Ponce de León's Florida landing and the 1526 Lucas Vázquez de Ayllón expedition in South Carolina and Georgia region. A Spanish expedition in the later 16th century, in what is now western North Carolina, encountered people of the Mississippian culture at Joara and settlements further west. The Spanish built a fort at Joara and left a garrison there, as well as five other forts. The following year all the Spanish garrisons were killed and the forts destroyed by the Native Americans, who ended Spanish colonization attempts in the interior.
17th century emergence of Choctaw.
The contemporary historian Patricia Galloway argues from fragmentary archaeological and cartographic evidence that the Choctaw did not exist as a unified people before the 17th century. Only then did various southeastern peoples, remnants of Moundville, Plaquemine, and other Mississippian cultures, coalesce to form a self-consciously Choctaw people. The historical homeland of the Choctaw, or of the peoples from whom the Choctaw nation arose, included the area of "Nanih Waiya", an earthwork mound in present-day Winston County, Mississippi, which they considered sacred ground. Their homeland was bounded by the Tombigbee River to the east, the Pearl River on the north and west, and "the Leaf-Pascagoula system" to the South. This area was mostly uninhabited during the Mississippian -culture period.
While Nanih Waiya mound continued to be a ceremonial center and object of veneration, scholars believe Native Americans traveled to it during the Mississippian culture period. From the 17th century on, the Choctaw occupied this area and revered this site as the center of their origin stories. These included stories of migration to this site from west of the great river (believed to refer to the Mississippi River.)
In "Histoire de La Louisiane" (Paris, 1758), French explorer Antoine-Simon Le Page du Pratz recounted that "...when I asked them from whence the Chat-kas came, to express the suddenness of their appearance they replied that they had come out from under the earth." American scholars took this as intended to explain the Choctaws' immediate appearance, and not a literal creation account. It was perhaps the first European writing that included part of the Choctaw origin story.
Early 19th century and contemporary Choctaw storytellers describe that the Choctaw people emerged from either Nanih Waiya mound or cave. A companion story describes their migration journey from the west, beyond the Mississippi River, when they were directed by their leader's use of a sacred red pole.
Contact era.
After the castaway Cabeza de Vaca of the ill-fated Narváez expedition returned to Spain, he described to the Court that the New World was the "richest country in the world." It commissioned the Spaniard Hernando de Soto to lead the first expedition into the interior of the North American continent. De Soto, convinced of the "riches", wanted Cabeza de Vaca to accompany him on the expedition. Cabeza de Vaca declined because of a payment dispute. From 1540 to 1543, Hernando de Soto traveled through present-day Florida and Georgia, and then into the Alabama and Mississippi areas that would later be inhabited by the Choctaw.
De Soto had the best-equipped militia at the time. As the brutalities of the de Soto expedition through the Southeast became known, ancestors to the Choctaw rose in defense. The Battle of Mabila, an ambush arranged by Chief Tuskaloosa, was a turning point for the de Soto venture. The battle "broke the back" of the campaign, and they never fully recovered.
The archaeological record for the period between 1567 and 1699 is not complete or well-studied. It appears that some Mississippian settlements were abandoned well before the 17th century. Similarities in pottery coloring and burials suggest the following scenario for the emergence of the distinctive Choctaw society.
According to Patricia Galloway, the Choctaw region of Mississippi, generally located between the Yazoo basin to the north and the Natchez bluffs to the south, was slowly occupied by Burial Urn people from the Bottle Creek Indian Mounds area in the Mobile, Alabama delta, along with remnants of people from the Moundville chiefdom (near present-day Tuscaloosa, Alabama), which had collapsed some years before. Facing severe depopulation, they fled westward, where they combined with the Plaquemines and a group of “prairie people” living near the area. When this occurred is not clear. In the space of several generations, they created a new society which became known as Choctaw (albeit with a strong Mississippian background).
Other scholars note the Choctaw oral history recounting their long migration from west of the Mississippi River.
French colonization (1682).
In 1682 La Salle was the first French explorer to venture into the southeast along the Mississippi River. His expedition did not meet with the Choctaw; it established a post along the Arkansas River. The post signaled to the English that the French were serious at colonization in the South. The Choctaw allied with French colonists as a defense against the English, who had been taking Choctaws as captives for the Indian slave trade.
The first direct recorded contact between the Choctaw and the French was with Pierre Le Moyne d'Iberville in 1699; indirect contact had likely occurred between the Choctaw and British settlers through other tribes, including the Creek and Chickasaw. The Choctaw, along with other tribes, had formed a relationship with New France, French Louisiana. Illegal fur trading may have led to further unofficial contact. 
As the historian Greg O'Brien has noted, the Choctaw developed three distinct political and geographic regions, which during the colonial period sometimes had differing alliances with trading partners among the French, Spanish and English. They also expressed differences during and after the American Revolutionary War. Their divisions were roughly eastern, western (near present-day Vicksburg, Mississippi) and southern (Six Towns). Each division was headed by a principal chief, and subordinate chiefs led each of the towns within the area. All the chiefs would meet on a National Council, but the society was highly decentralized for some time.
The French were the main trading partners of the Choctaw before the Seven Years' War, and the British had established some trading. After Great Britain defeated France, it ceded its territory east of the Mississippi River. From 1763 to 1781, Britain was the Choctaw main trading partner. With Spanish forces based in New Orleans in 1766, when they took over French territory west of the Mississippi, the Choctaw sometimes traded with them to the west. Spain declared war against Great Britain during the American Revolution in 1779.
United States relations.
During the American Revolution, the Choctaw divided over whether to support Britain or Spain. Some Choctaw warriors from the western and eastern divisions supported the British in the defense of Mobile and Pensacola. Chief Franchimastabé led a Choctaw war party with British forces against American rebels in Natchez. The Americans had left by the time Franchimastabé arrived, but the Choctaw occupied Natchez for weeks and convinced residents to remain loyal to Britain.
Other Choctaw companies joined Washington's army during the war, and served the entire duration. Bob Ferguson, a Southeastern Indian historian, noted, " 1775 the American Revolution began a period of new alignments for the Choctaws and other southern Indians. Choctaw scouts served under Washington, Morgan, Wayne and Sullivan."
Over a thousand Choctaw fought for Britain, largely against Spain's campaigns along the Gulf Coast. At the same time, a significant number of Choctaw aided Spain.
Ferguson wrote that with the end of the Revolution, "'Franchimastabe', Choctaw head chief, went to Savannah, Georgia to secure American trade." In the next few years, some Choctaw scouts served in Ohio with U.S. General Anthony Wayne in the Northwest Indian War.
George Washington (first U.S. President) and Henry Knox (first U.S. Secretary of War) proposed the cultural transformation of Native Americans. Washington believed that Native American society was inferior to that of the European Americans. He formulated a policy to encourage the "civilizing" process, and Thomas Jefferson continued it. The historian Robert Remini wrote, "hey presumed that once the Indians adopted the practice of private property, built homes, farmed, educated their children, and embraced Christianity, these Native Americans would win acceptance from white Americans."
Washington's six-point plan included impartial justice toward Indians; regulated buying of Indian lands; promotion of commerce; promotion of experiments to civilize or improve Indian society; presidential authority to give presents; and punishing those who violated Indian rights. The government appointed agents, such as Benjamin Hawkins, to live among the Indians and to teach them through example and instruction, how to live like whites. While living among the Choctaw for nearly 30 years, Hawkins married Lavinia Downs, a Choctaw woman. As the people had a matrilineal system of property and hereditary leadership, their children were born into the mother's clan and gained their status from her people. In the late eighteenth and early nineteenth century, a number of Scots-Irish traders lived among the Choctaw and married high-status women. Choctaw chiefs saw these as strategic alliances to build stronger relationships with the Americans in a changing environment that influenced ideas of capital and property. The children of such marriages were Choctaw, first and foremost. Some of the sons were educated in Anglo-American schools and became important interpreters and negotiators for Choctaw-US relations.
Hopewell council and treaty (1786).
Starting in October 1785, "Taboca", a Choctaw prophet/chief, led over 125 Choctaws to the Keowee, near Seneca Old Town, now known as Hopewell, South Carolina. After two months of travel, they met with U.S. representatives Benjamin Hawkins, Andrew Pickens, and Joseph Martin. In high Choctaw ceremonial symbolism, they named, adopted, smoked, and performed dances, revealing the complex and serious nature of Choctaw diplomacy. One such dance was the eagle tail dance. The Choctaw explained that the bald eagle, who has direct contact with the upper world of the sun, is a symbol of peace. Choctaw women painted in white would adopt and name commissioners as kin. Smoking sealed agreements between peoples and the shared pipes sanctified peace between the two nations.
After the rituals, the Choctaw asked John Woods to live with them to improve communication with the U.S. In exchange they allowed Taboca to visit the United States Congress. On January 3, 1786, the Treaty of Hopewell was signed. Article 11 stated, "he hatchet shall be forever buried, and the peace given by the United States of America, and friendship re-established between the said states on the one part, and all the Choctaw nation on the other part, shall be universal; and the contracting parties shall use their utmost endeavors to maintain the peace given as aforesaid, and friendship re-established."
The treaty required Choctaws to return escaped slaves to colonists, to turn over any Choctaw convicted of crimes by the U.S., establish borderlines between the U.S. and Choctaw Nation, and the return any property captured from colonists during the Revolutionary War.
After the Revolutionary War, the Choctaw were reluctant to ally themselves with countries hostile to the United States. John R. Swanton wrote, "the Choctaw were never at war with the Americans. A few were induced by "Tecumseh" (a Shawnee leader who sought support from various Native American tribes) to ally themselves with the hostile Creeks the early 19th century, but the Nation as a whole was kept out of anti-American alliances by the influence of "Apushmataha", greatest of all Choctaw chiefs."
War of 1812.
Early in 1811, the Shawnee leader "Tecumseh" gathered Indian tribes in an alliance to try to expel U.S. settlers from the Northwest area south of the Great Lakes. Tecumseh met the Choctaws to persuade them to join the alliance. "Pushmataha", considered by historians to be the greatest Choctaw leader, countered Tecumseh's influence. As chief for the Six Towns (southern) district, Pushmataha strongly resisted such a plan, arguing that the Choctaw and their neighbors the Chickasaw had always lived in peace with European Americans, had learned valuable skills and technologies, and had received honest treatment and fair trade. The joint Choctaw-Chickasaw council voted against alliance with Tecumseh. On Tecumseh's departure, Pushmataha accused him of tyranny over his own Shawnee and other tribes. Pushmataha warned Tecumeseh that he would fight against those who fought the United States.
On the eve of the War of 1812, Governor William C. C. Claiborne of Louisiana sent interpreter Simon Favre to give a talk to the Choctaws, urging them to stay out of this "white man's war." Ultimately, however, the Choctaw did become involved, and with the outbreak of the war, Pushmataha led the Choctaws in alliance with the U.S., arguing in favor of opposing the Creek Red Sticks' alliance with Britain after the massacre at Fort Mims. Pushmataha arrived at St. Stephens, Alabama in mid-1813 with an offer of alliance and recruitment. He was escorted to Mobile to speak with General Flournoy, then commanding the district. Flournoy initially declined Pushmataha's offer and offended the chief. However, Flournoy's staff quickly convinced him to reverse his decision. A courier with a message accepting the offer of alliance caught up with Pushmataha at St. Stephens.
Returning to Choctaw territory, Pushmataha raised a company of 125 Choctaw warriors with a rousing speech and was commissioned (as either a lieutenant colonel or a brigadier general) in the United States Army at St. Stephens. After observing that the officers and their wives would promenade along the Alabama River, Pushmataha summoned his own wife to St. Stephens to accompany him.
He joined the U.S. Army under General Ferdinand Claiborne in mid-November, and some 125 Choctaw warriors took part in an attack on Creek forces at Kantachi (near present day Econochaca, Alabama) on 23 December 1813. With this victory, Choctaw began to volunteer in greater numbers from the other two districts of the tribe. By February 1814, a larger band of Choctaws under Pushmataha had joined General Andrew Jackson's force for the sweeping of the Creek territories near Pensacola, Florida. Many Choctaw departed from Jackson's main force after the final defeat of the Creek at the Battle of Horseshoe Bend. By the Battle of New Orleans, only a few Choctaw remained with the army; they were the only Native American tribe represented in the battle.
Doak's Stand (1820).
In October 1820, Andrew Jackson and Thomas Hinds were sent as commissioners representing the United States, to conduct a treaty that would require the Choctaw to surrender to the United States a portion of their country located in present day Mississippi. They met with chiefs, mingos (leaders), and headsmen such as Colonel Silas Dinsmore and Chief Pushmataha at Doak's Stand on the Natchez Trace.
The convention began on October 10 with a talk by "Sharp Knife", the nickname of Jackson, to more than 500 Choctaws. Pushmataha accused Jackson of deceiving them about the quality of land west of the Mississippi. Pushmataha responded to Jackson's retort with "I know the country well ... The grass is everywhere very short ... There are but few beavers, and the honey and fruit are rare things." Jackson resorted to threats, which pressured the Choctaws to sign the Doak's Stand treaty. Pushmataha would continue to argue with Jackson about the conditions of the treaty. Pushmataha assertively stated "that no alteration shall be made in the boundaries of the portion of our territory that will remain, until the Choctaw people are sufficiently progressed in the arts of civilization to become citizens of the States, owning land and homes of their own, on an equal footing with the white people." Jackson responded with "That ... is a magnificent rangement and we consent to it, Citizenship, readily." Historian Anna Lewis stated that "Apuckshunubbee", a Choctaw district chief, was blackmailed by Jackson to sign the treaty. On October 18, the Treaty of Doak's Stand was signed.
Article 4 of the Treaty of Doak's Stand prepared Choctaws to become U.S. citizens when he or she became "civilized." This article would later influence Article 14 in the Treaty of Dancing Rabbit Creek.
Negotiations with the US government (1820s).
Apuckshunubbee, Pushmataha, and Mosholatubbee, the principal chiefs of the three divisions of Choctaw, led a delegation to Washington City (the 19th century name for Washington, D.C.) to discuss the problems of European Americans' squatting on Choctaw lands. They sought either expulsion of the settlers or financial compensation for the loss of their lands. The group also included Talking Warrior, Red Fort, Nittahkachee, who was later Principal Chief; Col. Robert Cole and David Folsom, both Choctaw of mixed-race ancestry; Captain Daniel McCurtain, and Major John Pitchlynn, the U.S. interpreter, who had been raised by the Choctaw after having been orphaned when young and married a Choctaw woman. Apuckshunubbee died in Maysville, Kentucky of an accident during the trip before the party reached Washington.
Pushmataha met with President James Monroe and gave a speech to Secretary of War John C. Calhoun, reminding him of the longstanding alliances between the United States and the Choctaws. He said, " can say and tell the truth that no Choctaw ever drew his bow against the United States ... My nation has given of their country until it is very small. We are in trouble." On January 20, 1825, Pushmataha and other chiefs signed the Treaty of Washington City, by which the Choctaw ceded more territory to the United States.
Pushmataha died in Washington of a respiratory disease described as croup, before the delegation returned to the Choctaw Nation. He was given full U.S. military burial honors at the Congressional Cemetery in Washington, D.C.
The deaths of these two strong division leaders was a major loss to the Choctaw Nation, but younger leaders were arising who were educated in European-American schools and led adaptation of the culture. Threatened with European-American encroachment, the Choctaw continued to adapt and take on some technology, housing styles, and accepted missionaries to the Choctaw Nation, in the hopes of being accepted by the Mississippi and national government. In 1825 the National Council approved the founding of the Choctaw Academy for education of its young men, urged by Peter Pitchlynn, a young leader and future chief. The school was established in Blue Spring, Scott County, Kentucky; it was operated there until 1842, when the staff and students were transferred to the Choctaw Nation, Indian Territory. There they founded the Spencer Academy in 1844.
With the election of Andrew Jackson as president in 1828, many of the Choctaw realized that removal was inevitable. They continued to adopt useful European practices but faced Jackson's and settlers' unrelenting pressure.
1830 election and treaty.
In March 1830 the division chiefs resigned, and the National Council elected Greenwood LeFlore, chief of the western division, as Principal Chief of the nation to negotiate with the US government on their behalf, the first time such a position had been authorized. Believing removal was inevitable and hoping to preserve rights for Choctaw in Indian Territory and Mississippi, LeFlore drafted a treaty and sent it to Washington, DC. There was considerable turmoil in the Choctaw Nation among people who thought he would and could resist removal, but the chiefs had agreed they could not undertake armed resistance.
Treaty of Dancing Rabbit Creek (1830).
At Andrew Jackson's request, the United States Congress opened what became a fierce debate on an Indian Removal Bill. In the end, the bill passed, but the vote was very close. The Senate passed the measure 28 to 19, while in the House it narrowly passed, 102 to 97. Jackson signed the legislation into law June 30, 1830, and turned his focus onto the Choctaw in Mississippi Territory.
On August 25, 1830, the Choctaw were supposed to meet with Andrew Jackson in Franklin, Tennessee, but Greenwood Leflore, a district Choctaw chief, informed Secretary of War John H. Eaton that his warriors were fiercely opposed to attending. President Jackson was angered. Journalist Len Green writes "although angered by the Choctaw refusal to meet him in Tennessee, Jackson felt from LeFlore's words that he might have a foot in the door and dispatched Secretary of War Eaton and John Coffee to meet with the Choctaws in their nation." Jackson appointed Eaton and General John Coffee as commissioners to represent him to meet the Choctaws at the Dancing Rabbit Creek near present-day Noxubee County, Mississippi.
The commissioners met with the chiefs and headmen on September 15, 1830, at Dancing Rabbit Creek. In a carnival-like atmosphere, they tried to explain the policy of removal to an audience of 6,000 men, women, and children. The Choctaws faced migration or submitting to U.S. law as citizens. The treaty required them to cede their remaining traditional homeland to the United States; however, a provision in the treaty made removal more acceptable.
On September 27, 1830, the Treaty of Dancing Rabbit Creek was signed. It represented one of the largest transfers of land that was signed between the U.S. Government and Native Americans without being instigated by warfare. By the treaty, the Choctaw signed away their remaining traditional homelands, opening them up for European-American settlement. Article 14 allowed for some Choctaw to stay in Mississippi, and nearly 1,300 Choctaws chose to do so. They were one of the first major non-European ethnic group to become U.S. citizens. Article 22 sought to put a Choctaw representative in the U.S. House of Representatives. The Choctaw at this crucial time split into two distinct groups: the Choctaw Nation of Oklahoma and the Mississippi Band of Choctaw Indians. The nation retained its autonomy, but the tribe in Mississippi submitted to state and federal laws.
Removal era.
After ceding nearly , the Choctaw emigrated in three stages: the first in the fall of 1831, the second in 1832 and the last in 1833. Nearly 15,000 Choctaws made the move to what would be called Indian Territory and then later Oklahoma. About 2,500 died along the Trail of Tears. The Treaty of Dancing Rabbit Creek was ratified by the U.S. Senate on February 25, 1831, and the President was anxious to make it a model of removal. Principal Chief George W. Harkins wrote a farewell letter to the American people before the removals began. It was widely published
Alexis de Tocqueville, noted French political thinker and historian, witnessed the Choctaw removals while in Memphis, Tennessee in 1831:
Approximately 4,000–6,000 Choctaw remained in Mississippi in 1831 after the initial removal efforts. The U.S. agent William Ward, who was responsible for Choctaw registration in Mississippi under article XIV, strongly opposed their treaty rights. Although estimates suggested 5000 Choctaw remained in Mississippi, only 143 family heads (for a total of 276 adult persons) received lands under the provisions of Article 14. For the next ten years, the Choctaws in Mississippi were objects of increasing legal conflict, racism, harassment, and intimidation. The Choctaws described their situation in 1849: "we have had our habitations torn down and burned, our fences destroyed, cattle turned into our fields and we ourselves have been scourged, manacled, fettered and otherwise personally abused, until by such treatment some of our best men have died." Joseph B. Cobb, who moved to Mississippi from Georgia, described the Choctaw as having "no nobility or virtue at all, and in some respect he found blacks, especially native Africans, more interesting and admirable, the red man's superior in every way. The Choctaw and Chickasaw, the tribes he knew best, were beneath contempt, that is, even worse than black slaves." Removal continued throughout the 19th and 20th centuries. In 1846 1,000 Choctaw removed, and in 1903, another 300 Mississippi Choctaw were persuaded to move to the Nation in Oklahoma. By 1930 only 1,665 remained in Mississippi.
Pre-Civil War (1840).
[[Image:Malmaison.jpg|thumb|right|315px|Choctaw chief Greenwood LeFlore's plantation home, Malmaison, was built in 1852 near Greenwood, Mississippi and was described as a "palace in the wilderness."]]
In the 1840s, the Choctaw chief Greenwood LeFlore stayed in Mississippi after the signing of Treaty of Dancing Rabbit Creek and became an American citizen, a successful businessman, and a state politician. He was elected as a Mississippi representative and senator, was a fixture of Mississippi high society, and a personal friend of Jefferson Davis. He represented his county in the state house for two terms and served as a state senator for one term. Some of the elite used Latin language, an indulgence used by some politicians. LeFlore, in defense of his heritage, spoke in the Choctaw language and asked the Senate floor which was better understood, Latin or Choctaw.
Midway through the Great Irish Famine (1845–1849), a group of Choctaw collected $710 (although many articles say the original amount was $170 after a misprint in Angi Debo's "The Rise and Fall of the Choctaw Nation") and sent it to help starving Irish men, women, and children.
"It had been just 16 years since the Choctaw people had experienced the Trail of Tears, and they had faced starvation ... It was an amazing gesture. By today's standards, it might be a million dollars" according to Judy Allen, editor of the Choctaw Nation of Oklahoma's newspaper, "Bishinik", based at the Oklahoma Choctaw tribal headquarters in Durant, Oklahoma. To mark the 150th anniversary, eight Irish people retraced the Trail of Tears. In the late 20th century, Irish President Mary Robinson extolled the donation in a public commemoration.
For the Choctaw who remained in or returned to Mississippi after 1855, the situation deteriorated. Many lost their lands and money to unscrupulous whites. The state of Mississippi refused the Choctaw any participation in government. Their limited understanding of the English language caused them to live in isolated groups. In addition, they were prohibited from attending any of the few institutions of higher learning, as the European Americans considered them free people of color and excluded from the segregated white institutions. The state had no public schools prior to those established during the Reconstruction era.
American Civil War (1861).
At the beginning of the American Civil War, Albert Pike was appointed as Confederate envoy to Native Americans. In this capacity he negotiated several treaties, including the Treaty with Choctaws and Chickasaws in July 1861. The treaty covered sixty-four terms, covering many subjects, such as Choctaw and Chickasaw nation sovereignty, Confederate States of America citizenship possibilities, and an entitled delegate in the House of Representatives of the Confederate States of America.
Some Choctaw identified with the Southern cause and a few owned slaves. In addition, they well remembered and resented the Indian removals from thirty years earlier, and the poor services they received from the federal government. The main reason the Choctaw Nation agreed to sign the treaty was for protection from regional tribes. Soon Confederate battalions were formed in Indian Territory and later in Mississippi in support of the southern cause.
The Confederacy wanted to recruit Indians east of the Mississippi River in 1862, so they opened up a recruiting camp in Mobile, Alabama. The Confederacy advertised in the "Mobile Advertiser and Register" for recruits:
J. W. Pierce and S. G. Spann organized the Choctaw Confederates in Mississippi between 1862 and 1863. Pierce's men have been noted to track down Confederate deserters in Jones County and surrounding areas. After a Confederate troop train wreck, referred to as the Chunky Creek Train Wreck of 1863, near Hickory, Mississippi, the new Choctaw Battalion led rescue and recovery efforts. Led by Jack Amos and Elder Williams, the Indians rushed to the scene, stripped, and plunged into the flooded creek. Many of the passengers were rescued due to their heroic acts." The noted Choctaw historian Clara Kidwell writes, "in an act of heroism in Mississippi, Choctaws rescued twenty-three survivors and retrieved ninety bodies when a Confederate troop train plunged off a bridge and fell into the Chunky River."
Major S. G. Spann, the commander of U. C. V. Dabney H. Maury Camp, of Meridian, Mississippi wrote about the deeds of the Choctaw years after the Civil War had ended. After the Union captured Mississippi Choctaws in Ponchatoula, Louisiana, they were taken to New York, where several died in a Union prison. Spann describes the incident, "J.W. Pearce established two camps—a recruiting camp in Newton County and a drill camp at Tangipahoa—just beyond the State boundary line in Louisiana in the fall of 1862. New Orleans at that time was in the hands of the Federal Gen. B.F. Butler. Without notice a reconnoitering party of the enemy raided the camp, and captured over two dozen Indians and several non-commissioned white officers and carried them to New Orleans. All the officers and several of the Indians escaped and returned to the Newton County camp; but all the balance of the captured Indians were carried to New York, and were daily paraded in the public parks as curiosities for the sport of sight-seers."
Choctaw Under Reconstruction (1865).
Mississippi Choctaw.
From about 1865 to 1918, Mississippi Choctaws were largely ignored by governmental, health, and educational services and fell into obscurity. In the aftermath of the Civil War, their issues were pushed aside in the struggle between defeated Confederates, freedmen and Union sympathizers. Records about the Mississippi Choctaw during this period are non-existent. They had no legal recourse, and were often bullied and intimidated by local whites, who tried to re-establish white supremacy. They chose to live in isolation and practiced their culture as they had for generations.
Following the Reconstruction era and conservative Democrats' regaining political power in the late 1870s, white state legislators passed laws establishing Jim Crow laws and legal segregation by race. In addition, they effectively disfranchised freedmen and Native Americans by the new Mississippi constitution of 1890, which changed rules regarding voter registration and elections to discriminate against both groups. The white legislators effectively divided society into two groups: white and "colored," into which they classified Mississippi Choctaw and other Native Americans. They subjected the Choctaw to racial segregation and exclusion from public facilities along with freedmen and their descendants. The Choctaw were non-white, landless, and had minimal legal protection.
At the turn of the 20th century, only 1,253 Choctaw Indians remained in Mississippi. "The beginning of the 20th century found Mississippi Choctaws struggling to overcome poverty, discrimination, and lack of opportunity." 
After a US Congressional investigation discovered their poor living conditions, in 1918 the US Bureau of Indian Affairs (BIA) established the Choctaw Agency. Under segregation, few schools were open to Choctaw children, whom the white southerners classified as non-whites. The Choctaw agency was based in Philadelphia, Mississippi, a center of several Indian communities. It set up elementary schools and worked to address the poor health conditions of the Choctaw, building a hospital in Philadelphia for tribal members. Dr. Frank McKinley was the first superintendent. Prior to McKinley's arrival, the Choctaw had grouped themselves in six communities.
Because the state remained dependent on agriculture, despite the declining price of cotton, most landless men earned a living by becoming sharecroppers. The women created and sold traditional hand-woven baskets. Choctaw sharecropping declined following World War II as major planters had adopted mechanization, which reduced the need for labor.
Choctaw Nation.
The Confederacy’s loss was also the Choctaw Nation’s loss. Prior to removal, the Choctaws had interacted with Africans in their native homeland of Mississippi, and the wealthiest had bought slaves. The Choctaw who developed larger plantations adopted chattel slavery, as practiced by European Americans, to gain sufficient labor. During the antebellum period, enslaved African Americans had more formal legal protection under United States law than did the Choctaw. Moshulatubbee, the chief of the western region, held slaves, as did many of the Europeans who married into the Choctaw nation. The Choctaw took slaves with them to Indian Territory during removal, and descendants purchased others there. They kept slavery until 1866. After the Civil War, they were required by treaty with the United States to emancipate the slaves within their Nation and, for those who chose to stay, offer them full citizenship and rights. Former slaves of the Choctaw Nation were called the Choctaw Freedmen. After considerable debate, the Choctaw Nation granted Choctaw Freedmen citizenship in 1885. In post-war treaties, the US government also acquired land in the western part of the territory and access rights for railroads to be built across Indian Territory. Choctaw chief, Allen Wright, suggested "Oklahoma" (red man, a "portmanteau" of the Choctaw words "okla" "man" and "humma" "red") as the name of a territory created from Indian Territory in 1890.
The improved transportation afforded by the railroads increased the pressure on the Choctaw Nation. It drew large-scale mining and timber operations, which added to tribal receipts. But, the railroads and industries also attracted European-American settlers, including new immigrants to the United States.
With the goal of assimilating the Native Americans, the Curtis Act of 1898, sponsored by a Native American who believed that was the way for his people to do better, ended tribal governments. In addition, it proposed the end of communal, tribal lands. Continuing the struggle over land and assimilation, the US proposed the end to the tribal lands held in common, and allotment of lands to tribal members in severalty (individually). The US declared land in excess of the registered households needs to be "surplus" to the tribe, and took it for sale to new European-American settlers. In addition, individual ownership meant that Native Americans could sell their individual plots. This would also enable new settlers to buy land from those Native Americans who wished to sell. The US government set up the Dawes Commission to manage the land allotment policy; it registered members of the tribe and made allocations of lands.
Beginning in 1894, the Dawes Commission was established to register Choctaw and other families of the Indian Territory, so that the former tribal lands could be properly distributed among them. The final list included 18,981 citizens of the Choctaw Nation, 1,639 Mississippi Choctaw, and 5,994 former slaves (and descendants of former slaves), most held by Choctaws in the Indian/Oklahoma Territory. (At the same time, the Dawes Commission registered members of the other Five Civilized Tribes for the same purpose. The Dawes Rolls have become important records for proving tribal membership.) Following completion of the land allotments, the US proposed to end tribal governments of the Five Civilized Tribes and admit the two territories jointly as a state.
Territory transition to Oklahoma statehood (1889).
The establishment of Oklahoma Territory following the Civil War was a required land cession by the Five Civilized Tribes, who had supported the Confederacy. The government used its railroad access to the Oklahoma Territory to stimulate development there. The Indian Appropriations Bill of 1889 included an amendment by Illinois Representative William McKendree Springer, that authorized President Benjamin Harrison to open the two million acres (8,000 km²) of Oklahoma Territory for settlement, resulting in the Land Run of 1889. The Choctaw Nation was overwhelmed with new settlers and could not regulate their activities. In the late 19th century, Choctaws suffered almost daily from violent crimes, murders, thefts and assaults from whites and from other Choctaws. Intense factionalism divided the traditionalistic "Nationalists" and pro-assimilation "Progressives," who fought for control.
In 1905, delegates of the Five Civilized Tribes met at the Sequoyah Convention to write a constitution for an Indian-controlled state. They wanted to have Indian Territory admitted as the State of Sequoyah. Although they took a thoroughly developed proposal to Washington, DC, seeking approval, eastern states' representatives opposed it, not wanting to have two western states created in the area, as the Republicans feared that both would be Democrat-dominated, as the territories had a southern tradition of settlement. President Theodore Roosevelt, a Republican, ruled that the Oklahoma and Indian territories had to be jointly admitted as one state, Oklahoma. To achieve this, tribal governments had to end and all residents accept state government. Many of the leading Native American representatives from the Sequoyah Convention participated in the new state convention. Its constitution was based on many elements of the one developed for the State of Sequoyah.
In 1906 the U.S. dissolved the governments of the Five Civilized Tribes. This action was part of continuing negotiations by Native Americans and European Americans over the best proposals for the future. The Choctaw Nation continued to protect resources not stipulated in treaty or law. On November 16, 1907, Oklahoma was admitted to the union as the 46th state.
World War I (1918).
In the closing days of World War I, a group of Choctaws serving in the U.S. Army used their native language as the basis for secret communication among Americans, as Germans could not understand it. They are now called the Choctaw Code Talkers. The Choctaws were the Native American innovators who served as code talkers. Captain Lawrence, a company commander, overheard Solomon Louis and Mitchell Bobb conversing in the Choctaw language. He learned there were eight Choctaw men in the battalion.
Fourteen Choctaw Indian men in the Army's 36th Division trained to use their language for military communications. Their communications, which could not be understood by Germans, helped the American Expeditionary Force win several key battles in the Meuse-Argonne Campaign in France, during the last big German offensive of the war. Within 24 hours after the US Army starting using the Choctaw speakers, they turned the tide of battle by controlling their communications. In less than 72 hours, the Germans were retreating and the Allies were on full attack. The 14 Choctaw Code Talkers were Albert Billy, Mitchell Bobb, Victor Brown, Ben Caterby, James Edwards, Tobias Frazer, Ben Hampton, Solomon Louis, Pete Maytubby, Jeff Nelson, Joseph Oklahombi, Robert Taylor, Calvin Wilson, and Captain Walter Veach.
More than 70 years passed before the contributions of the Choctaw Code talkers were fully recognized. On November 3, 1989, in recognition of the important role the Choctaw Code Talkers played during World War I, the French government presented the "Chevalier de L'Ordre National du Mérite" (the Knight of the National Order of Merit) to the Choctaws Code Talkers. 
The US Army again used Choctaw speakers for coded language during World War II.
Reorganization (1934).
During the Great Depression and the Roosevelt Administration, officials began numerous initiatives to alleviate some of the social and economic conditions in the South. The 1933 "Special Narrative Report" described the dismal state of welfare of Mississippi Choctaws, whose population by 1930 had declined to 1,665 people. John Collier, the US Commissioner for Indian Affairs (now BIA), had worked for a decade on Indian affairs and been developing ideas to change federal policy. He used the report as instrumental support to re-organize the Mississippi Choctaw as the Mississippi Band of Choctaw Indians. This enabled them to establish their own tribal government, and gain a beneficial relationship with the federal government.
In 1934, President Franklin Roosevelt signed into law the Indian Reorganization Act. This law proved critical for survival of the Mississippi Choctaw. Baxter York, Emmett York, and Joe Chitto worked on gaining recognition for the Choctaw. They realized that the only way to gain recognition was to adopt a constitution. A rival organization, the Mississippi Choctaw Indian Federation, opposed tribal recognition because of fears of dominance by the Bureau of Indian Affairs (BIA). They disbanded after leaders of the opposition were moved to another jurisdiction. The first Mississippi Band of Choctaw Indians tribal council members were Baxter and Emmett York with Joe Chitto as the first chairperson.
With the tribe's adoption of government, in 1944 the Secretary of the Interior declared that would be held in trust for the Choctaw of Mississippi. Lands in Neshoba and surrounding counties were set aside as a federal Indian reservation. Eight communities were included in the reservation land: Bogue Chitto, Bogue Homa, Conehatta, Crystal Ridge, Pearl River, Red Water, Tucker, and Standing Pine.
Under the Indian Reorganization Act, the Mississippi Choctaws re-organized on April 20, 1945 as the Mississippi Band of Choctaw Indians. This gave them some independence from the Democrat-dominated state government, which continued with enforcement of racial segregation and discrimination.
World War II (1941).
World War II was a significant turning point for Choctaws and Native Americans in general. Although the Treaty of Dancing Rabbit Creek stated Mississippi Choctaws had U.S. citizenship, they had become associated with "colored people" as non-white in a state that had imposed racial segregation under Jim Crow laws. State services for Native Americans were non-existent. The state was poor and still dependent on agriculture. In its system of segregation, services for minorities were consistently underfunded. The state constitution and voter registration rules dating from the turn of the 20th century kept most Native Americans from voting, making them ineligible to serve on juries or to be candidates for local or state offices. They were without political representation.
A Mississippi Choctaw veteran stated, "Indians were not supposed to go in the military back then ... the military was mainly for whites. My category was white instead of Indian. I don't know why they did that. Even though Indians weren't citizens of this country, couldn't register to vote, didn't have a draft card or anything, they took us anyway."
Van Barfoot, a Choctaw from Mississippi, who was a sergeant and later a second lieutenant in the U.S. Army, 157th Infantry, 45th Infantry Division, received the Medal of Honor. Barfoot was commissioned a second lieutenant after he destroyed two German machine gun nests, took 17 prisoners, and disabled an enemy tank.
Post-Reorganization (1946).
After World War II, pressure in Congress mounted to reduce Washington's authority on Native American lands and liquidate the government's responsibilities to them. In 1953 the House of Representatives passed Resolution 108, proposing an end to federal services for 13 tribes deemed ready to handle their own affairs. The same year, Public Law 280 transferred jurisdiction over tribal lands to state and local governments in five states. Within a decade Congress terminated federal services to more than sixty groups despite intense opposition by Indians. Congress settled on a policy to terminate tribes as quickly as possible. Out of concern for the isolation of many Native Americans in rural areas, the federal government created relocation programs to cities to try to expand their employment opportunities. Indian policy experts hoped to expedite assimilation of Native Americans to the larger American society, which was becoming urban. In 1959, the Choctaw Termination Act was passed. Unless repealed by the federal government, the Choctaw Nation of Oklahoma would effectively be terminated as a sovereign nation as of August 25, 1970.
President John F. Kennedy halted further termination in 1961 and decided against implementing additional terminations. He did enact some of the last terminations in process, such as with the Ponca. Both presidents Lyndon Johnson and Richard Nixon repudiated termination of the federal government's relationship with Native American tribes.
Mississippi Choctaw Self-Determination Era.
The Choctaw people continued to struggle economically due to bigotry, cultural isolation, and lack of jobs. The Choctaw, who for 150 years had been neither white nor black, were "left where they had always been"—in poverty. Will D. Campbell, a Baptist minister and Civil Rights activist, witnessed the destitution of the Choctaw. He would later write, "the thing I remember the most ... was the depressing sight of the Choctaws, their shanties along the country roads, grown men lounging on the dirt streets of their villages in demeaning idleness, sometimes drinking from a common bottle, sharing a roll-your-own cigarette, their half-clad children a picture of hurting that would never end." With reorganization and establishment of tribal government, however, over the next decades they took control of "schools, health care facilities, legal and judicial systems, and social service programs."
The Choctaws witnessed the social forces that brought Freedom Summer and its after effects to their ancient homeland. The Civil Rights Era produced significant social change for the Choctaw in Mississippi, as their civil rights were enhanced. Prior to the Civil Rights Act of 1964, most jobs were given to whites, then blacks. Donna Ladd wrote that a Choctaw, now in her 40s, remembers "as a little girl, she thought that a 'white only' sign in a local store meant she could only order white, or vanilla, ice cream. It was a small story, but one that shows how a third race can easily get left out of the attempts for understanding."
On June 21, 1964 James Chaney, Andrew Goodman, and Michael Schwerner (renowned civil rights workers) disappeared; their remains were later found in a newly constructed dam. A crucial turning point in the FBI investigation came when the charred remains of the murdered Mississippi civil rights workers' station wagon was found on a Mississippi Choctaw reservation. Two Choctaw women, who were in the back seat of a deputy's patrol car, said they witnessed the meeting
of two conspirators who expressed their desire to "beat-up" the boys. The end of legalized racial segregation permitted the Choctaws to participate in public institutions and facilities that had been reserved exclusively for white patrons.
Phillip Martin, who had served in the U. S. Army in Europe during World War II, returned to visit his former Neshoba County, Mississippi home. After seeing the poverty of his people, he decided to stay to help. Martin served as chairperson in various Choctaw committees up until 1977.
Martin was elected as Chief of the Mississippi Band of Choctaw Indians. He served a total of 30 years, being re-elected until 2007. Martin died in Jackson, Mississippi, on February 4, 2010. He was eulogized as a visionary leader, who had lifted his people out of poverty with businesses and casinos built on tribal land.
Changes after the civil rights era were reflected in sports and other venues. In 1981, the African-American athlete, Marcus Dupree, played his final high school football game at Warriors Stadium of the Choctaw Indian Reservation's tribal high school. He finished with 5,284 rushing yards on 8.3 yards per carry; there was considerable media coverage to witness the record-breaking event that was praised by many Mississippians.
Choctaws today.
In the social changes around the Civil Rights Era, between 1965 and 1982 Native Americans renewed their commitments to the value of their ancient heritage. Working to celebrate their own strengths and exercise appropriate rights; they dramatically reversed the trend toward abandonment of Indian culture and tradition. During the 1960s, Community Action programs connected with Native Americans were based on citizen participation. In the 1970s, the Choctaws repudiated the extremes of Indian activism. The Oklahoma Choctaw sought a local grassroots solution to reclaim their cultural identity and sovereignty as a nation. The Mississippi Choctaw would lay the foundations of business ventures. Policy continued toward the ideology of Self-Determination.
Soon after this, Congress passed the landmark Indian Self-Determination and Education Assistance Act of 1975, completing a 15-year period of federal policy reform with regard to American Indian tribes. The legislation included means by which tribes could negotiate contracts with the BIA to manage more of their own education and social service programs. In addition, it provided direct grants to help tribes develop plans for assuming responsibility. It also provided for Indian parents' involvement on school boards.
Beginning in 1979 the tribal council worked on a variety of economic development initiatives, first geared toward attracting industry to the reservation. They had many people available to work, natural resources and no taxes. Industries have included automotive parts, greeting cards, direct mail and printing, and plastic-molding. The Mississippi Band of Choctaw Indians is one of the state's largest employers, running 19 businesses and employing 7,800 people.
Starting with New Hampshire in 1963, numerous state governments began to operate lotteries and other gambling to raise money for government services. In 1987 the Supreme Court of the United States ruled that federally recognized tribes could operate gaming facilities on reservation land free from state regulation. In 1988 the U.S. Congress enacted the Indian Gaming Regulatory Act (IGRA). It set the terms for Native American tribes to operate casinos.
After years of waiting under the Ray Mabus administration, Mississippi Governor Kirk Fordice in 1992 gave permission for the Mississippi Band of Choctaw Indians to develop Class III gaming. The Mississippi Band of Choctaw Indians (MBCI) has one of the largest casino resorts in the nation; it is located in Philadelphia, Mississippi. The Silver Star Casino opened its doors in 1994. The Golden Moon Casino opened in 2002. The casinos are collectively known as the Pearl River Resort. 
The Choctaw Nation of Oklahoma has its own gaming operations: the Choctaw Casino Resort and Choctaw Casino Bingo, popular gaming destinations in Durant. Near the Oklahoma-Texas border, they serve residents of Southern Oklahoma and North Texas. The largest regional population base from which they draw is the Dallas-Fort Worth Metroplex.
After nearly two hundred years, the Choctaw have retaken control of the ancient site of Nanih Waiya. For years protected as a Mississippi state park, "Nanih Waiya" was returned to the Choctaw in 2006, under Mississippi Legislature State Bill 2803.
Jack Abramoff and Indian casino lobbying.
In the second half of the 1990s, Abramoff was employed by Preston Gates Ellis & Rouvelas Meeds LLP, the lobbying arm of Preston Gates & Ellis LLP law firm based in Seattle, Washington. In 1995, Abramoff began representing Native American tribes with gambling interests, starting with the Mississippi Band of Choctaw Indians.
The Choctaw originally had lobbied the federal government directly, but beginning in 1994, they found that many of the congressional members who had responded to their issues had either retired or were defeated in the "Republican Revolution" of the 1994 elections. Nell Rogers, the tribe's specialist on legislative affairs, had a friend who was familiar with the work of Abramoff and his father as Republican activists. The tribe contacted Preston Gates, and soon after hired the firm and Abramoff.
Abramoff succeeded in gaining defeat of a Congressional bill to use the unrelated business income tax (UBIT) to tax Native American casinos, sponsored by Reps. Bill Archer (R-TX) and Ernest Istook (R-OK). Since the matter involved taxation, Abramoff enlisted help from his college Republican acquaintance Grover Norquist, and his Americans for Tax Reform (ATR). The bill was eventually defeated in 1996 in the Senate, due in part to grassroots work by ATR, for which the Choctaw paid $60,000.
According to "Washington Business Forward", a lobbying trade magazine, Senator Tom DeLay was a major factor in those victories. The fight strengthened Abramoff's alliance with him.
Purporting to represent Native Americans before Congress and state governments in this new field, Jack Abramoff and Michael Scanlon used fraudulent means to gain profits of $15 million in total payments from the Mississippi Band of Choctaw Indians. Congressional hearings were held on the issue and charges were brought against Abramoff and Scanlon. In an e-mail sent January 29, 2002, Abramoff wrote to Scanlon, "I have to meet with the monkeys from the Choctaw tribal council."
On January 3, 2006, Abramoff pled guilty to three felony counts — conspiracy, fraud, and tax evasion — involving charges stemming principally from his lobbying activities in Washington on behalf of Native American tribes. In addition, Abramoff and other defendants must make restitution of at least $25 million that was defrauded from clients, most notably the Native American tribes.
MOWA Choctaw.
Other Choctaw tribes located in the United States include:
Alabama
MOWA Band of Choctaw Indians (www.mowachoctaw.homestead.com)*The MOWA Choctaw reside on a 300-acre reservation in southwestern Alabama with a total enrolled population of 3,600. The tribe generationally attended federal and closely related Indian mission boarding schools such as Bacone in Muskogee, Oklahoma and Haskell in Lawrence, Kansas. The last fluent speaker of the tribe passed away in 1984, though tribal language classes have been on-going in the community since the 1960s. Nearly 30 federally recognized tribes (including the Choctaw Nation of Oklahoma), as well as members of the United Houman Nation and Lumbee Nation are intermarried and have children, grandchildren, and great-grandchildren within the community. The tribe has letters of support for federal recognition from numerous federal tribes, anthropologists, federal tribal leaders, Indian luminaries such as Vine Deloria, Jr., the National Congress of American Indians, NAACP, and other organizations. Copies of these letters and resolutions of support can be found on the tribes website. The Senate Select Committee on Indian Affairs voted 11-2 in support of federal recognition for the MOWA Choctaw. To date the tribe has had 12 Congressional Bills, 3 appeals through the office of federal acknowledgement, and a federal lawsuit directed at its efforts for federal recognition.
Mississippi
Live Oak Choctaw
Louisiana
Bayou Lacombe Choctaw
Clifton Choctaw
Jena Band of Choctaw Indians
Choctaw-Apache of Ebarb
Choctaw in the 2010 Census.
The 2010 Census found Choctaw living in every state of the Union. The states with the largest Choctaw populations were:
Culture.
The Choctaw people are believed to have coalesced in the 17th century, perhaps from peoples from Alabama and the Plaquemine culture. Their culture continued to evolve in the Southeast. The Choctaw practiced Head flattening as a ritual adornment for its people, but the practice eventually fell out of favor. Some of their communities had extensive trade and interaction with Europeans, including people from Spain, France, and England greatly shaped it as well. After the United States was formed and its settlers began to move into the Southeast, the Choctaw were among the Five Civilized Tribes, who adopted some of their ways. They transitioned to yeoman farming methods, and accepted European Americans and African Americans into their society. In mid-summer the Mississippi Band of Choctaw Indians celebrate their traditional culture during the Choctaw Indian Fair with ball games, dancing, cooking and entertainment.
Clans.
Within the Choctaws were two distinct moieties: "Imoklashas" (elders) and "Inhulalatas" (youth). Each moiety had several clans or "Iskas"; it is estimated there were about 12 Iskas altogether. The people had a matrilineal kinship system, with children born into the clan or iska of the mother and taking their social status from it. In this system, their maternal uncles had important roles. Identity was established first by moiety and iska; so a Choctaw identified first as Imoklasha or Inhulata, and second as Choctaw. Children belonged to the Iska of their mother. The following were some major districts:
By the early 1930s, the anthropologist John Swanton wrote of the Choctaw: "here are only the faintest traces of groups with truly totemic designations, the animal and plant names which occur seeming not to have had a totemic connotation."
Swanton wrote, "Adam Hodgson ... told ... that there were tribes or families among the Indians, somewhat similar to the Scottish clans; such as, the Panther family, the Bird family, Raccoon Family, the Wolf family." The following are possible totemic clan designations:
Games.
Choctaw stickball, the oldest field sport in North America, was also known as the "little brother of war" because of its roughness and substitution for war. When disputes arose between Choctaw communities, stickball provided a civil way to settle issues. The stickball games would involve as few as twenty or as many as 300 players. The goal posts could be from a few hundred feet apart to a few miles. Goal posts were sometimes located within each opposing team's village. A Jesuit priest referenced stickball in 1729, and George Catlin painted the subject. The Mississippi Band of Choctaw Indians continue to practice the sport.
Chunkey was a game using a stone-shaped disk that was about 1–2 inches in length. Players would throw the disk down a corridor so that it could roll past the players at great speed. As the disk rolled down the corridor, players would throw wooden shafts at it. The object of the game was to strike the disk or prevent your opponents from hitting it.
Other games included using corn, cane, and moccasins. The corn game used five to seven kernels of corn. One side was blackened and the other side white. Players won points based on each color. One point was awarded for the black side and 5-7 points for the white side. There were usually only two players.
Language.
The Choctaw language is a member of the Muskogean family and was well known among the frontiersmen, such as Andrew Jackson and William Henry Harrison, of the early 19th century. The language is closely related to Chickasaw, and some linguists consider the two dialects a single language. The Choctaw language is the essence of tribal culture, tradition, and identity. Many Choctaw adults learned to speak the language before speaking English. The language is a part of daily life on the Mississippi Choctaw reservation. The following table is an example of Choctaw text and its translation:
Religion.
The Choctaw believed in a good spirit and an evil spirit. They may have been sun, or "Hushtahli", worshippers. The historian Swanton wrote,
"Choctaws anciently regarded the sun as a deity ... the sun was ascribed the power of life and death. He was represented as looking down upon the earth, and as long as he kept his flaming eye fixed on any one, the person was safe ... fire, as the most striking representation of the sun, was considered as possessing intelligence, and as acting in concert with the sun ... [having constant intercourse with the sun ..."
The word "nanpisa" (the one who sees) expressed the reverence the Choctaw had for the sun.
Choctaw prophets were known to have addressed the sun. Swanton wrote, "an old Choctaw informed Wright that before the arrival of the missionaries, they had no conception of prayer. He added, "I have indeed heard it asserted by some, that anciently their hopaii, or prophets, on some occasions were accustomed to address the sun ..."
Traditional clothing.
The colorful dresses worn by today's Choctaw are made by hand. They are based on designs of their ancestors, who adapted 19th-century European-American styles to their needs. Today many Choctaw wear such traditional clothing mainly for special events. Choctaw elders, especially the women, dress in their traditional garb every day. Choctaw dresses are trimmed by full diamond, half diamond or circle, and crosses that represent stickball sticks.
Communal economy.
Early Choctaw communities worked communally and shared their harvest. They had trouble understanding why English settlers allowed their poor to suffer from hunger.
Treaties.
Land was the most valuable asset, which the Native Americans held in collective stewardship. The United States systematically obtained Choctaw land for conventional European-American settlement through treaties, legislation, and threats of warfare. Although the Choctaw made treaties with Great Britain, France, Spain, and the Confederate States of America; the nation signed only nine treaties with the United States. Some treaties which the US made with other nations, such as the Treaty of San Lorenzo, indirectly affected the Choctaw.
Reservations.
Reservations can be found in Alabama-(MOWA Band of Choctaw Indians), Louisiana-(Jena Band of Choctaw Indians; United Houma Nation; Choctaw-Apache Tribe of Ebarb; Bayou Lacombe Choctaw; Clifton Choctaw), Texas-(Mount Tabor Indian Community), Mississippi-(Mississippi Band of Choctaw Indians), and Oklahoma-(Choctaw Nation of Oklahoma). Other population centers include California, Oregon, Dallas, Houston and Chicago.
The Oklahoma reservation, is defined by treaty.

</doc>
<doc id="7224" url="https://en.wikipedia.org/wiki?curid=7224" title="Calypso">
Calypso

Calypso may refer to:
Music.
King Short Shirt (McLean Emanuel) Antigua and Barbuda Album Ghetto Vibes 1976

</doc>
<doc id="7225" url="https://en.wikipedia.org/wiki?curid=7225" title="Chemical affinity">
Chemical affinity

In chemical physics and physical chemistry, chemical affinity is the electronic property by which dissimilar chemical species are capable of forming chemical compounds. Chemical affinity can also refer to the tendency of an atom or compound to combine by chemical reaction with atoms or compounds of unlike composition.
History.
Early Theories.
The idea of "affinity" is extremely old. Many attempts have been made at identifying its origins. The majority of such attempts, however, except in a general manner, end in futility since "affinities" lie at the basis of all magic, thereby pre-dating science. Physical chemistry, however, was one of the first branches of science to study and formulate a "theory of affinity". The name "affinitas" was first used in the sense of chemical relation by German philosopher Albertus Magnus near the year 1250. Later, those as Robert Boyle, John Mayow, Johann Glauber, Isaac Newton, and Georg Stahl put forward ideas on elective affinity in attempts to explain how heat is evolved during combustion reactions.
The term "affinity" has been used figuratively since c. 1600 in discussions of structural relationships in chemistry, philology, etc., and reference to "natural attraction" is from 1616. "Chemical affinity", historically, has referred to the "force" that causes chemical reactions. as well as, more generally, and earlier, the ″tendency to combine″ of any pair of substances. The broad definition, used generally throughout history, is that chemical affinity is that whereby substances enter into or resist decomposition.
The modern term chemical affinity is a somewhat modified variation of its eighteenth-century precursor "elective affinity" or elective attractions, a term that was used by the 18th century chemistry lecturer William Cullen. Whether Cullen coined the phrase is not clear, but his usage seems to predate most others, although it rapidly became widespread across Europe, and was used in particular by the Swedish chemist Torbern Olof Bergman throughout his book "De attractionibus electivis" (1775). Affinity theories were used in one way or another by most chemists from around the middle of the 18th century into the 19th century to explain and organise the different combinations into which substances could enter and from which they could be retrieved. Antoine Lavoisier, in his famed 1789 "Traité Élémentaire de Chimie (Elements of Chemistry)", refers to Bergman’s work and discusses the concept of elective affinities or attractions.
According to chemistry historian Henry Leicester, the influential 1923 textbook "Thermodynamics and the Free Energy of Chemical Reactions" by Gilbert N. Lewis and Merle Randall led to the replacement of the term "affinity" by the term "free energy" in much of the English-speaking world.
According to Prigogine, the term was introduced and developed by Théophile de Donder.
Goethe used the concept in his novel Elective Affinities, (1809)
Visual Representations.
The affinity concept was very closely linked to the visual representation of substances on a table. The first-ever "affinity table", which was based on displacement reactions, was published in 1718 by the French chemist Étienne François Geoffroy. Geoffroy's name is best known in connection with these tables of "affinities" ("tables des rapports"), which were first presented to the French Academy of Sciences in 1718 and 1720, as shown below:
During the 18th century many versions of the table were proposed with leading chemists like Torbern Bergman in Sweden and Joseph Black in Scotland adapting it to accommodate new chemical discoveries. All the tables were essentially lists, prepared by collating observations on the actions of substances one upon another, showing the varying degrees of affinity exhibited by analogous bodies for different reagents.
Crucially, the table was the central graphic tool used to teach chemistry to students and its visual arrangement was often combined with other kinds diagrams. Joseph Black, for example, used the table in combination with chiastic and circlet diagrams to visualise the core principles of chemical affinity. Affinity tables were used throughout Europe until the early 19th century when they were displaced by affinity concepts introduced by Claude Berthollet.
Modern conceptions.
In chemical physics and physical chemistry, chemical affinity is the electronic property by which dissimilar chemical species are capable of forming chemical compounds. Chemical affinity can also refer to the tendency of an atom or compound to combine by chemical reaction with atoms or compounds of unlike composition.
In modern terms, we relate affinity to the phenomenon whereby certain atoms or molecules have the tendency to aggregate or bond. For example, in the 1919 book "Chemistry of Human Life" physician George W. Carey states that, "Health depends on a proper amount of iron phosphate Fe3(PO4)2 in the blood, for the molecules of this salt have chemical affinity for oxygen and carry it to all parts of the organism." In this antiquated context, chemical affinity is sometimes found synonymous with the term "magnetic attraction". Many writings, up until about 1925, also refer to a "law of chemical affinity".
Ilya Prigogine summarized the concept of affinity, saying, "All chemical reactions drive the system to a state of equilibrium in which the "affinities" of the reactions vanish."
Thermodynamics.
The present IUPAC definition is that affinity "A" is the negative partial derivative of Gibbs free energy "G" with respect to extent of reaction "ξ" at constant pressure and temperature. That is,
It follows that affinity is positive for spontaneous reactions.
In 1923, the Belgian mathematician and physicist Théophile de Donder derived a relation between affinity and the Gibbs free energy of a chemical reaction. Through a series of derivations, de Donder showed that if we consider a mixture of chemical species with the possibility of chemical reaction, it can be proven that the following relation holds:
With the writings of Théophile de Donder as precedent, Ilya Prigogine and Defay in "Chemical Thermodynamics" (1954) defined chemical affinity as the rate of change of the uncompensated heat of reaction "Q"' as the reaction progress variable or reaction extent "ξ" grows infinitesimally:

</doc>
<doc id="7227" url="https://en.wikipedia.org/wiki?curid=7227" title="Comet Hale–Bopp">
Comet Hale–Bopp

Comet Hale–Bopp (formally designated C/1995 O1) is a comet that was perhaps the most widely observed of the 20th century and one of the brightest seen for many decades. It was visible to the naked eye for a record 18 months, twice as long as the previous record holder, the Great Comet of 1811.
Hale–Bopp was discovered on July 23, 1995, at a great distance from the Sun, raising expectations that the comet would brighten considerably by the time it passed close to Earth. Although predicting the brightness of comets with any degree of accuracy is very difficult, Hale–Bopp met or exceeded most predictions when it passed perihelion on April 1, 1997. The comet was dubbed the Great Comet of 1997.
Discovery.
The comet was discovered independently on July 23, 1995 by two observers, Alan Hale and Thomas Bopp, both in the United States.
Hale had spent many hundreds of hours searching for comets without success, and was tracking known comets from his driveway in New Mexico when he chanced upon Hale–Bopp just after midnight. The comet had an apparent magnitude of 10.5 and lay near the globular cluster M70 in the constellation of Sagittarius. Hale first established that there was no other deep-sky object near M70, and then consulted a directory of known comets, finding that none were known to be in this area of the sky. Once he had established that the object was moving relative to the background stars, he emailed the Central Bureau for Astronomical Telegrams, the clearing house for astronomical discoveries.
Bopp did not own a telescope. He was out with friends near Stanfield, Arizona observing star clusters and galaxies when he chanced across the comet while at the eyepiece of his friend's telescope. He realized he might have spotted something new when, like Hale, he checked his star maps to determine if any other deep-sky objects were known to be near M70, and found that there were none. He alerted the Central Bureau for Astronomical Telegrams through a Western Union telegram. Brian G. Marsden, who had run the bureau since 1968, laughed, "Nobody sends telegrams anymore. I mean, by the time that telegram got here, Alan Hale had already e-mailed us three times with updated coordinates."
The following morning, it was confirmed that this was a new comet, and it was given the designation C/1995 O1. The discovery was announced in International Astronomical Union circular 6187.
The comet may have been observed by ancient Egyptians during the reign of pharaoh Pepi I (2332–2283 BC). In Pepi's pyramid in Saqqara is a text referring to an "nhh-star" as a companion of the pharaoh in the heavens, where "" is the hieroglyph for long hair.
Early observation.
Hale–Bopp's orbital position was calculated as 7.2 astronomical units (AU) from the Sun, placing it between Jupiter and Saturn and by far the greatest distance from Earth at which a comet had been discovered by amateurs. Most comets at this distance are extremely faint, and show no discernible activity, but Hale–Bopp already had an observable coma. An image taken at the Anglo-Australian Telescope in 1993 was found to show the then-unnoticed comet some 13 AU from the Sun, a distance at which most comets are essentially unobservable. (Halley's Comet was more than 100 times fainter at the same distance from the Sun.) Analysis indicated later that its comet nucleus was 60±20 kilometres in diameter, approximately six times the size of Halley.
Its great distance and surprising activity indicated that comet Hale–Bopp might become very bright indeed when it reached perihelion in 1997. However, comet scientists were wary – comets can be extremely unpredictable, and many have large outbursts at great distance only to diminish in brightness later. Comet Kohoutek in 1973 had been touted as a 'comet of the century' and turned out to be unspectacular.
Perihelion.
Hale–Bopp became visible to the naked eye in May 1996, and although its rate of brightening slowed considerably during the latter half of that year, scientists were still cautiously optimistic that it would become very bright. It was too closely aligned with the Sun to be observable during December 1996, but when it reappeared in January 1997 it was already bright enough to be seen by anyone who looked for it, even from large cities with light-polluted skies.
The Internet was a growing phenomenon at the time, and numerous websites that tracked the comet's progress and provided daily images from around the world became extremely popular. The Internet played a large role in encouraging the unprecedented public interest in comet Hale–Bopp.
As the comet approached the Sun, it continued to brighten, shining at 2nd magnitude in February, and showing a growing pair of tails, the blue gas tail pointing straight away from the Sun and the yellowish dust tail curving away along its orbit. On March 9, a solar eclipse in China, Mongolia and eastern Siberia allowed observers there to see the comet in the daytime. Hale–Bopp had its closest approach to Earth on March 22, 1997 at a distance of 1.315 AU.
As it passed perihelion on April 1, 1997 the comet developed into a spectacular sight. It shone brighter than any star in the sky except Sirius, and its dust tail stretched 40–45 degrees across the sky. The comet was visible well before the sky got fully dark each night, and while many great comets are very close to the Sun as they pass perihelion, comet Hale–Bopp was visible all night to northern hemisphere observers.
After perihelion.
After its perihelion passage, the comet moved into the southern celestial hemisphere. The comet was much less impressive to southern hemisphere observers than it had been in the northern hemisphere, but southerners were able to see the comet gradually fade from view during the second half of 1997. The last naked-eye observations were reported in December 1997, which meant that the comet had remained visible without aid for 569 days, or about 18 and a half months. The previous record had been set by the Great Comet of 1811, which was visible to the naked eye for about 9 months.
The comet continued to fade as it receded, but is still being tracked by astronomers. In October 2007, 10 years after the perihelion and at distance of 25.7 AU from Sun, the comet was still active as indicated by the detection of the CO-driven coma. Herschel Space Observatory images taken in 2010 suggest comet Hale–Bopp is covered in a fresh frost layer. Hale–Bopp was again detected in December 2010 when it was 30.7 AU away from the Sun, and on August 7, 2012 at a 33.2 AU distance from the Sun. Astronomers expect that the comet will remain observable with large telescopes until perhaps 2020, by which time it will be nearing 30th magnitude. By this time it will become very difficult to distinguish the comet from the large numbers of distant galaxies of similar brightness.
Orbital changes.
The comet likely made its previous perihelion 4,200 years ago. The comet's orbit is almost perpendicular to the plane of the ecliptic, which ensures that close approaches to planets are rare. However, in April 1996 the comet passed within 0.77 AU of Jupiter, close enough for its orbit to be affected by the planet's gravity. The comet's orbit was shortened considerably to a period of roughly 2,533 years, and it will next return to the inner Solar System around the year 4385. Its greatest distance from the Sun (aphelion) will be about 370 AU, reduced from about 525 AU.
Over many orbits, the cumulative effect of gravitational perturbations on comets with high orbital inclinations and small perihelion distances is generally to reduce the perihelion distance to very small values. Hale–Bopp has about a 15% chance of eventually becoming a sungrazing comet through this process.
It has been calculated that the previous visit by Hale–Bopp occurred in July 2215 BC. The comet may have presented a similar sight to people then, as the estimated closest approach to Earth was 1.4 AU, but no records of it have survived. Hale–Bopp may have had a near collision with Jupiter in early June 2215 BC, which probably caused a dramatic change in its orbit, and 2215 BC may have been its first passage through the inner Solar System.
The estimated probability of Hale-Bopp's striking Earth in future passages through the inner Solar System is remote, about 2.5×10−9 per orbit. However, given that the comet nucleus is around 60 km in diameter, the consequences of such an impact would be apocalyptic. Weissman conservatively estimates the diameter at 35 km; an estimated density of 0.6 g/cm3 then gives a cometary mass of 1.3×1019 g. At a probable impact velocity of 52.5 km/s, impact energy can be calculated as 1.9×1032 ergs, or 4.4×109 megatons, about 44 times the estimated energy of the K-T impact event.
Scientific results.
Comet Hale–Bopp was observed intensively by astronomers during its perihelion passage, and several important advances in cometary science resulted from these observations. The dust production rate of the comet was very high (up to 2.0 kg/s), which may have made the inner coma optically thick. Based on the properties of the dust grains—high temperature, high albedo and strong 10 μm silicate emission feature—the astronomers concluded the dust grains are smaller than observed in any other comet.
Hale–Bopp showed the highest ever linear polarization detected for any comet. Such polarization is the result of solar radiation getting scattered by the dust particles in the coma of the comet and depends on the nature of the grains. It further confirms that the dust grains in the coma of comet Hale–Bopp were smaller than inferred in any other comet.
Sodium tail.
One of the most remarkable discoveries was that the comet had a third type of tail. In addition to the well-known gas and dust tails, Hale–Bopp also exhibited a faint sodium tail, only visible with powerful instruments with dedicated filters. Sodium emission had been previously observed in other comets, but had not been shown to come from a tail. Hale–Bopp's sodium tail consisted of neutral atoms (not ions), and extended to some 50 million kilometres in length.
The source of the sodium appeared to be the inner coma, although not necessarily the nucleus. There are several possible mechanisms for generating a source of sodium atoms, including collisions between dust grains surrounding the nucleus, and 'sputtering' of sodium from dust grains by ultraviolet light. It is not yet established which mechanism is primarily responsible for creating Hale–Bopp's sodium tail, and the narrow and diffuse components of the tail may have different origins.
While the comet's dust tail roughly followed the path of the comet's orbit and the gas tail pointed almost directly away from the Sun, the sodium tail appeared to lie between the two. This implies that the sodium atoms are driven away from the comet's head by radiation pressure.
Deuterium abundance.
The abundance of deuterium in comet Hale–Bopp in the form of heavy water was found to be about twice that of Earth's oceans. If Hale–Bopp's deuterium abundance is typical of all comets, this implies that although cometary impacts are thought to be the source of a significant amount of the water on Earth, they cannot be the only source.
Deuterium was also detected in many other hydrogen compounds in the comet. The ratio of deuterium to normal hydrogen was found to vary from compound to compound, which astronomers believe suggests that cometary ices were formed in interstellar clouds, rather than in the solar nebula. Theoretical modelling of ice formation in interstellar clouds suggests that comet Hale–Bopp formed at temperatures of around 25–45 Kelvin.
Organics.
Spectroscopic observations of Hale–Bopp revealed the presence of many organic chemicals, several of which had never been detected in comets before. These complex molecules may exist within the cometary nucleus, or might be synthesised by reactions in the comet.
Detection of argon.
Hale–Bopp was the first comet where the noble gas argon was detected. Noble gases are chemically inert and highly volatile, and since different noble elements have different sublimation temperatures, they can be used for probing the temperature histories of the cometary ices. Krypton has a sublimation temperature of 16–20 K and was found to be depleted more than 25 times relative to the solar abundance, while argon with its higher sublimation temperature was enriched relative to the solar abundance. Together these observations indicate that the interior of Hale–Bopp has always been colder than 35–40 K, but has at some point been warmer than 20 K. Unless the solar nebula was much colder and richer in argon than generally believed, this suggests that the comet formed beyond Neptune in the Kuiper belt region and then migrated outward to the Oort cloud.
Rotation.
Comet Hale–Bopp's activity and outgassing were not spread uniformly over its nucleus, but instead came from several specific jets. Observations of the material streaming away from these jets allowed astronomers to measure the rotation period of the comet, which was found to be about 11 hours 46 minutes.
Binary nucleus question.
In 1997 a paper was published that hypothesised the existence of a binary nucleus to fully explain the observed pattern of comet Hale–Bopp's dust emission observed in October 1995. The paper was based on theoretical analysis, and did not claim an observational detection of the proposed satellite nucleus, but estimated that it would have a diameter of about 30 km, with the main nucleus being about 70 km across, and would orbit in about three days at a distance of about 180 km. This analysis was confirmed by observations in 1996 using Wide-Field Planetary Camera 2 of the Hubble Space Telescope which had taken images of the comet that revealed the satellite.
Although observations using adaptive optics in late 1997 and early 1998 showed a double peak in the brightness of the nucleus, controversy still exists over whether such observations can only be explained by a binary nucleus. The discovery of the satellite was not confirmed by other observations. Also, while comets have been observed to break up before, no case has previously been found of a stable binary nucleus. Given the very small mass of this comet, the orbit of the binary nucleus would be easily disrupted by the gravity of the Sun and planets.
UFO claims.
In November 1996 amateur astronomer Chuck Shramek of Houston, Texas took a CCD image of the comet, which showed a fuzzy, slightly elongated object nearby. When his computer sky-viewing program did not identify the star, Shramek called the Art Bell radio program Coast to Coast AM to announce that he had discovered a "Saturn-like object" following Hale–Bopp. UFO enthusiasts, such as remote viewing proponent Courtney Brown, soon concluded that there was an alien spacecraft following the comet.
Several astronomers, including Alan Hale, claimed the object was simply an 8.5-magnitude star, SAO141894, which did not appear on Shramek's computer program because the user preferences were set incorrectly. Later, Art Bell even claimed to have obtained an image of the object from an anonymous astrophysicist who was about to confirm its discovery. However, astronomers Olivier Hainaut and David J. Tholen of the University of Hawaii stated that the alleged photo was an altered copy of one of their own comet images.
A few months later, in March 1997, 39 members of the cult Heaven's Gate committed mass suicide with the intention of teleporting to a spaceship they believed was flying behind the comet.
Nancy Lieder, a self-proclaimed contactee who claims to receive messages from aliens through an implant in her brain, stated that Hale–Bopp was a fiction designed to distract the population from the coming arrival of "Nibiru" or "Planet X", a giant planet whose close passage would disrupt the Earth's rotation, causing global cataclysm. Although Lieder's original date for the apocalypse, May 2003, passed without incident, predictions of the imminent arrival of Nibiru continued by various conspiracy websites, most of whom tied it to the 2012 phenomenon.
Legacy.
Its lengthy period of visibility and extensive coverage in the media meant that Hale–Bopp was probably the most-observed comet in history, making a far greater impact on the general public than the return of Halley's Comet in 1986, and certainly seen by a greater number of people than witnessed any of Halley's previous appearances. For instance, 69% of Americans had seen Hale–Bopp by April 9, 1997.
Hale–Bopp was a record-breaking comet—the farthest comet from the Sun discovered by amateurs, with the largest well-measured cometary nucleus known after 95P/Chiron, and it was visible to the naked eye for twice as long as the previous record-holder. It was also brighter than magnitude 0 for eight weeks, longer than any other recorded comet.

</doc>
<doc id="7230" url="https://en.wikipedia.org/wiki?curid=7230" title="Conspiracy">
Conspiracy

Conspiracy or conspirator may refer to: 

</doc>
<doc id="7232" url="https://en.wikipedia.org/wiki?curid=7232" title="Cholistan Desert">
Cholistan Desert

The Cholistan Desert (; Punjabi: ), also locally known as Rohi (), sprawls thirty kilometers from Bahawalpur, Punjab, Pakistan and covers an area of . It adjoins the Thar Desert, extending over to Sindh and into India.
The word Cholistan is derived from the Turkic word "chol", meaning "desert". The people of Cholistan lead a semi-nomadic life, moving from one place to another in search of water and fodder for their animals. The dry bed of the Hakra River runs through the area, along which many settlements of the Indus Valley Civilization have been found.
The desert also hosts an annual Jeep rally, known as Cholistan Desert Jeep Rally. It is the biggest motor sports event in Pakistan.
Culture and traditions.
Local dialect.
The local marwari dialect spoken throughout Cholistan reflect a number of features from its historical and geographical background. In addition to their local dialect, members of most communities are able to converse fluently in Punjabi Dialects (Riasiti and Standard), and Urdu, as both are far more easily and more widely understood.
Arts and crafts.
In a harsh and barren land where rainfall is very sparse and unreliable, Cholistanis rely mainly on their livestock of sheep, goats, and camel. However, in cold nights of winter they huddle indoor and engage themselves in various arts and crafts such as textiles, weaving, leatherwork, and pottery.
Local crafts.
As mentioned above, the Indus Valley has always been occupied by the wandering nomadic tribes, who are fond of isolated areas, as such areas allow them to lead life free of foreign intrusion, enabling them to establish their own individual and unique cultures. Cholistan till the era of Mughal rule had also been isolated from outside influence. During the rule of Mughal Emperor Akbar, it became a proper productive unit. The entire area was ruled by a host of kings who securely guarded their frontiers. The rulers were the great patrons of art, and the various crafts underwent a simultaneous and parallel development, influencing each other. Masons, stone carvers, artisans, artists, and designers started rebuilding the old cities and new sites, and with that flourished new courts, paintings, weaving, and pottery. The fields of architecture, sculpture, terra cotta, and pottery developed greatly in this phase.
Livestock.
The backbone of Cholistan economy is cattle breeding. It has the major importance for satisfying the area's major needs for cottage industry as well as milk meat and fat. Because of the nomadic way of life the main wealth of the people are their cattle that are bred for sale, milked or shorn for their wool. Moreover, isolated as they were, they had to depend upon themselves for all their needs like food, clothing, and all the items of daily use. So all their crafts initially stemmed from necessity but later on they started exporting their goods to the other places as well. The estimated number of livestock in the desert areas is 1.6 million.
Cotton and wool products.
Cholistan produces very superior type of carpet wool as compared to that produced in other parts of Pakistan. From this wool they knit beautiful carpets, rugs and other woolen items. This includes blankets, which is also a local necessity for the desert is not just a land of dust and heat, but winter nights here are very cold, usually below freezing points. Khes and pattu are also manufactured with wool or cotton. Khes is a form of blanket with a field of black white and pattu has a white ground base. Cholistanis now sell the wool for it brings maximum profit.
Textiles.
It may be mentioned that cotton textiles have always been a hallmark of craft of Indus valley civilization. Various kinds of khaddar-cloth are made for local consumption, and fine khaddar bedclothes and coarse lungies are woven here. A beautiful cloth called Sufi is also woven of silk and cotton, or with cotton wrap and silk wool. Gargas are made with numerous patterns and color, having complicated embroidery, mirror, and patchwork. Ajrak is another specialty of Cholistan. It is a special and delicate printing technique on both sides of the cloth in indigo blue and red patterns covering the base cloth. Cotton turbans and shawls are also made here. Chunri is another form of dopattas, having innumerable colors and patterns like dots, squares, and circles on it.
Camel products.
Camels are highly valued by the desert dwellers. Camels are not only useful for transportation and loading purposes, but its skin and wool are also quite worthwhile. Camel wool is spun and woven into beautiful woolen blankets known as falsies and into stylish and durable rugs. The camel's leather is also utilized in making kuppies, goblets, and expensive lampshades.
Leatherwork.
Leatherwork is another important local cottage industry due to the large number of livestock here. Other than the products mentioned above, Khusa (shoes) is a specialty of this area. Cholistani khusas are very famous for the quality of workmanship, variety, and richness of designs especially when stitched and embroidered with golden or brightly colored threads.
Jewelry.
The Cholistanis are fond of jewelry, especially gold jewelry. The chief ornaments made and worn by them are "Nath" (nose gay), "Katmala" (necklace) "Kangan" (bracelet), and "Pazeb" (anklets). Gold and silver bangles are also a product of Cholistan. The locals similarly work in enamel, producing enamel buttons, earrings, bangles, and rings.
Love for colors.
The great desert though considered to be colorless and drab, is not wholly devoid of color. Its green portion plays the role of "color belt" especially after rains when vegetation growth is at its peak. Adding to that the locals always wear brightly colored clothes mostly consisting of brilliant reds, blazing oranges shocking pinks, and startling yellows and greens. Even the cloth trappings of their bullocks and camels are richly colored and highly textured.
Forests in Cholistan.
There is a rain forest in Cholistan named "Dodhla Forest"
Terra cotta.
The Indus Civilization was the earliest center of ceramics, and thus the pottery of Cholistan has a long history. Local soil is very fine, thus most suitable for making pottery. The fineness of the earth can be observed on the Kacha houses which are actually plastered with mud but look like white cemented. The chief Cholistani ceramic articles are their surahies, piyalas, and glasses, remarkable for their lightness and fine finishing.
In the early times only the art of pottery and terracotta developed, but from the seventh century onwards, a large number of temples and images were also built on account of the intensified religious passions and the accumulation of wealth in cities. The building activity reached to such an extent that some cities actually became city temples. In fact the area particularly came to be known for its forts, villas, palaces, havelis, gateways, fortifications, and city walls.

</doc>
<doc id="7233" url="https://en.wikipedia.org/wiki?curid=7233" title="Causantín mac Cináeda">
Causantín mac Cináeda

Causantín or Constantín mac Cináeda (in Modern Gaelic, "Còiseam mac Choinnich"; died 877) was a king of the Picts. He is often known as Constantine I in reference to his place in modern lists of kings of Scots, but contemporary sources described Causantín only as a Pictish king. A son of Cináed mac Ailpín ("Kenneth MacAlpin"), he succeeded his uncle Domnall mac Ailpín as Pictish king following the latter's death on 13 April 862. It is likely that Causantín's (Constantine I) reign witnessed increased activity by Vikings, based in Ireland, Northumbria and northern Britain. He died fighting one such invasion.
Sources.
Very few records of ninth century events in northern Britain survive. The main local source from the period is the "Chronicle of the Kings of Alba", a list of kings from Cináed mac Ailpín (died 858) to Cináed mac Maíl Coluim (died 995). The list survives in the Poppleton Manuscript, a thirteenth-century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton Manuscript version were added from the tenth century onwards. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Cináed mac Ailpín may date from the end of the tenth century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain. The Pictish king-lists originally ended with this Causantín, who was reckoned the seventieth and last king of the Picts.
For narrative history the principal sources are the "Anglo-Saxon Chronicle" and the Irish annals. While Scandinavian sagas describe events in 9th century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. If the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts—the modern regions of north-west England and all of northern and western Scotland—are non-existent, and archaeology and toponymy are of primary importance.
Languages and names.
Writing a century before Causantín was born, Bede recorded five languages in Britain. Latin, the common language of the church; Old English, the language of the Angles and Saxons; Irish, spoken on the western coasts of Britain and in Ireland; Brythonic, ancestor of the Welsh language, spoken in large parts of western Britain; and Pictish, spoken in northern Britain. By the ninth century a sixth language, Old Norse, had arrived with the Vikings.
Amlaíb and Ímar.
Viking activity in northern Britain appears to have reached a peak during Causantín's reign. Viking armies were led by a small group of men who may have been kinsmen. Among those noted by the Irish annals, the "Chronicle of the Kings of Alba" and the "Anglo-Saxon Chronicle" are Ívarr—Ímar in Irish sources—who was active from East Anglia to Ireland, Halfdán—Albdann in Irish, Healfdene in Old English— and Amlaíb or Óláfr. As well as these leaders, various others related to them appear in the surviving record.
Viking activity in Britain increased in 865 when the Great Heathen Army, probably a part of the forces which had been active in Francia, landed in East Anglia. The following year, having obtained tribute from the East Anglian King Edmund, the Great Army moved north, seizing York, chief city of the Northumbrians. The Great Army defeated an attack on York by the two rivals for the Northumbrian throne, Osberht and Ælla, who had put aside their differences in the face of a common enemy. Both would-be kings were killed in the failed assault, probably on 21 March 867. Following this, the leaders of the Great Army are said to have installed one Ecgberht as king of the Northumbrians. Their next target was Mercia where King Burgred, aided by his brother-in-law King Æthelred of Wessex, drove them off.
While the kingdoms of East Anglia, Mercia and Northumbria were under attack, other Viking armies were active in the far north. Amlaíb and Auisle (Ásl or Auðgísl), said to be his brother, brought an army to Fortriu and obtained tribute and hostages in 866. Historians disagree as to whether the army returned to Ireland in 866, 867 or even in 869. Late sources of uncertain reliability state that Auisle was killed by Amlaíb in 867 in a dispute over Amlaíb's wife, the daughter of Cináed. It is unclear whether, if accurate, this woman should be identified as a daughter of Cináed mac Ailpín, and thus Causantín's sister, or as a daughter of Cináed mac Conaing, king of Brega. While Amlaíb and Auisle were in north Britain, the "Annals of Ulster" record that Áed Findliath, High King of Ireland, took advantage of their absence to destroy the longphorts along the northern coasts of Ireland. Áed Findliath was married to Causantín's sister Máel Muire. She later married Áed's successor Flann Sinna. Her death is recorded in 913.
In 870, Amlaíb and Ívarr attacked Dumbarton Rock, where the River Leven meets the River Clyde, the chief place of the kingdom of Alt Clut, south-western neighbour of Pictland. The siege lasted four months before the fortress fell to the Vikings who returned to Ireland with many prisoners, "Angles, Britons and Picts", in 871. Archaeological evidence suggests that Dumbarton Rock was largely abandoned and that Govan replaced it as the chief place of the kingdom of Strathclyde, as Alt Clut was later known. King Artgal of Alt Clut did not long survive these events, being killed "at the instigation" of Causantín son of Cináed two years later. Artgal's son and successor Run was married to a sister of Causantín.
Amlaíb disappears from Irish annals after his return to Ireland in 871. According to the "Chronicle of the Kings of Alba" he was killed by Causantín either in 871 or 872 when he returned to Pictland to collect further tribute. His ally Ívarr died in 873.
Last days of the Pictish kingdom.
In 875, the "Chronicle" and the "Annals of Ulster" again report a Viking army in Pictland. A battle, fought near Dollar, was a heavy defeat for the Picts; the "Annals of Ulster" say that "a great slaughter of the Picts resulted". In 877, shortly after building a new church for the Culdees at St Andrews, Causantín was captured and executed (or perhaps killed in battle) after defending against Viking raiders. Although there is agreement on the time and general manner of his death, it is not clear where this happened. Some believe he was beheaded on a Fife beach, following a battle at Fife Ness, near Crail. William Forbes Skene reads the "Chronicle" as placing Causantín's death at Inverdovat (by Newport-on-Tay), which appears to match the Prophecy of Berchán. The account in the "Chronicle of Melrose" names the place as the "Black Cave," and John of Fordun calls it the "Black Den". Causantín was buried on Iona.
Aftermath.
Causantín's son Domnall and his descendants represented the main line of the kings of Alba and later Scotland.

</doc>
<doc id="7234" url="https://en.wikipedia.org/wiki?curid=7234" title="Constantine II (emperor)">
Constantine II (emperor)

Constantine II (; January/February 316 – 340) was Roman Emperor from 337 to 340. Son of Constantine the Great and co-emperor alongside his brothers, his attempt to exert his perceived rights of primogeniture led to his death in a failed invasion of Italy in 340.
Career.
The eldest son of Constantine the Great and Fausta, after the death of his half-brother Crispus, Constantine II was born in Arles in February 316 and raised as a Christian. On 1 March 317, he was made Caesar. In 323, at the age of seven, he took part in his father's campaign against the Sarmatians. At age ten, he became commander of Gaul, following the death of Crispus. An inscription dating to 330 records the title of "Alamannicus", so it is probable that his generals won a victory over the Alamanni. His military career continued when Constantine I made him field commander during the 332 campaign against the Goths.
Following the death of his father in 337, Constantine II initially became emperor jointly with his brothers Constantius II and Constans, with the Empire divided between them and their cousins, the "Caesars" Dalmatius and Hannibalianus. This arrangement barely survived Constantine I’s death, as his sons arranged the slaughter of most of the rest of the family by the army. As a result, the three brothers gathered together in Pannonia and there, on 9 September 337, divided the Roman world between themselves. Constantine, proclaimed "Augustus" by the troops received Gaul, Britannia and Hispania.
He was soon involved in the struggle between factions rupturing the unity of the Christian Church. The Western portion of the Empire, under the influence of the Popes in Rome, favored Catholicism over Arianism, and through their intercession they convinced Constantine to free Athanasius, allowing him to return to Alexandria. This action aggravated Constantius II, who was a committed supporter of Arianism.
Constantine was initially the guardian of his younger brother Constans, whose portion of the empire was Italia, Africa and Illyricum. Constantine soon complained that he had not received the amount of territory that was his due as the eldest son. Annoyed that Constans had received Thrace and Macedonia after the death of Dalmatius, Constantine demanded that Constans hand over the African provinces, to which he agreed in order to maintain a fragile peace. Soon, however, they began quarreling over which parts of the African provinces belonged to Carthage, and thus Constantine, and which belonged to Italy, and therefore Constans.
Further complications arose when Constans came of age and Constantine, who had grown accustomed to dominating his younger brother, would not relinquish the guardianship. In 340 Constantine marched into Italy at the head of his troops. Constans, at that time in Dacia, detached and sent a select and disciplined body of his Illyrian troops, stating that he would follow them in person with the remainder of his forces. Constantine was engaged in military operations and was killed in an ambush outside Aquileia. Constans then took control of his deceased brother's realm.

</doc>
<doc id="7235" url="https://en.wikipedia.org/wiki?curid=7235" title="Constantine II of Scotland">
Constantine II of Scotland

Constantine, son of Áed (Medieval Gaelic: "Constantín mac Áeda"; Modern Gaelic: "Còiseam mac Aoidh", known in most modern regnal lists as Constantine II; before 879 – 952) was an early King of Scotland, known then by the Gaelic name "Alba". The Kingdom of Alba, a name which first appears in Constantine's lifetime, was in northern Great Britain. The core of the kingdom was formed by the lands around the River Tay. Its southern limit was the River Forth, northwards it extended towards the Moray Firth and perhaps to Caithness, while its western limits are uncertain. Constantine's grandfather Kenneth I of Scotland (Cináed mac Ailpín, died 858) was the first of the family recorded as a king, but as king of the Picts. This change of title, from king of the Picts to king of Alba, is part of a broader transformation of Pictland and the origins of the Kingdom of Alba are traced to Constantine's lifetime.
His reign, like those of his predecessors, was dominated by the actions of Viking rulers in the British Isles, particularly the Uí Ímair ("the grandsons of Ímar", or Ivar the Boneless). During Constantine's reign the rulers of the southern kingdoms of Wessex and Mercia, later the Kingdom of England, extended their authority northwards into the disputed kingdoms of Northumbria. At first allied with the southern rulers against the Vikings, Constantine in time came into conflict with them. King Æthelstan was successful in securing Constantine's submission in 927 and 934, but the two again fought when Constantine, allied with the Strathclyde Britons and the Viking king of Dublin, invaded Æthelstan's kingdom in 937, only to be defeated at the great battle of Brunanburh. In 943 Constantine abdicated the throne and retired to the Céli Dé (Culdee) monastery of St Andrews where he died in 952. He was succeeded by his predecessor's son Malcolm I (Máel Coluim mac Domnaill).
Constantine's reign of 43 years, exceeded in Scotland only by that of King William the Lion before the Union of the Crowns in 1603, is believed to have played a defining part in the gaelicisation of Pictland, in which his patronage of the Irish Céli Dé monastic reformers was a significant factor. During his reign the words "Scots" and "Scotland" () are first used to mean part of what is now Scotland. The earliest evidence for the ecclesiastical and administrative institutions which would last until the Davidian Revolution also appears at this time.
Sources.
Compared to neighbouring Ireland and Anglo-Saxon England, few records of 9th- and 10th-century events in Scotland survive. The main local source from the period is the "Chronicle of the Kings of Alba", a list of kings from Kenneth MacAlpin (died 858) to Kenneth II (Cináed mac Maíl Coluim, died 995). The list survives in the Poppleton Manuscript, a 13th-century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton Manuscript version were added in the 10th and 12th centuries. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Kenneth MacAlpin may date from the end of the 10th century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain.
For narrative history the principal sources are the "Anglo-Saxon Chronicle" and the Irish annals. The evidence from charters created in the Kingdom of England provides occasional insight into events in northern Britain. While Scandinavian sagas describe events in 10th-century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. Mainland European sources rarely concern themselves with affairs in Britain, and even less commonly with events in northern Britain, but the life of Saint Cathróe of Metz, a work of hagiography written in Germany at the end of the 10th century, provides plausible details of the saint's early life in north Britain.
While the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts—the modern regions of north-west England and all of northern and western Scotland—are non-existent, and archaeology and toponymy are of primary importance.
Pictland from Constantín mac Fergusa to Constantine I.
The dominant kingdom in eastern Scotland before the Viking Age was the northern Pictish kingdom of Fortriu on the shores of the Moray Firth. By the 9th century, the Gaels of Dál Riata (Dalriada) were subject to the kings of Fortriu of the family of Constantín mac Fergusa (Constantine son of Fergus). Constantín's family dominated Fortriu after 789 and perhaps, if Constantín was a kinsman of Óengus I of the Picts (Óengus son of Fergus), from around 730. The dominance of Fortriu came to an end in 839 with a defeat by Viking armies reported by the "Annals of Ulster" in which King Uen of Fortriu and his brother Bran, Constantín's nephews, together with the king of Dál Riata, Áed mac Boanta, "and others almost innumerable" were killed. These deaths led to a period of instability lasting a decade as several families attempted to establish their dominance in Pictland. By around 848 Kenneth MacAlpin had emerged as the winner.
Later national myth made Kenneth MacAlpin the creator of the kingdom of Scotland, the founding of which was dated from 843, the year in which he was said to have destroyed the Picts and inaugurated a new era. The historical record for 9th century Scotland is meagre, but the Irish annals and the 10th-century "Chronicle of the Kings of Alba" agree that Kenneth was a Pictish king, and call him "king of the Picts" at his death. The same style is used of Kenneth's brother Donald I (Domnall mac Ailpín) and sons Constantine I (Constantín mac Cináeda) and Áed (Áed mac Cináeda).
The kingdom ruled by Kenneth's descendants—older works used the name House of Alpin to describe them but descent from Kenneth was the defining factor, Irish sources referring to "Clann Cináeda meic Ailpín" ("the Clan of Kenneth MacAlpin")—lay to the south of the previously dominant kingdom of Fortriu, centred in the lands around the River Tay. The extent of Kenneth's nameless kingdom is uncertain, but it certainly extended from the Firth of Forth in the south to the Mounth in the north. Whether it extended beyond the mountainous spine of north Britain—Druim Alban—is unclear. The core of the kingdom was similar to the old counties of Mearns, Forfar, Perth, Fife, and Kinross. Among the chief ecclesiastical centres named in the records are Dunkeld, probably seat of the bishop of the kingdom, and "Cell Rígmonaid" (modern St Andrews).
Kenneth's son Constantine died in 876, probably killed fighting against a Viking army which had come north from Northumbria in 874. According to the king lists, he was counted the 70th and last king of the Picts in later times.
Britain and Ireland at the end of the 9th century.
In 899 Alfred the Great, king of Wessex, died leaving his son Edward the Elder as ruler of Britain south of the River Thames and his daughter Æthelflæd and son-in-law Æthelred ruling the western, English part of Mercia. The situation in the Danish kingdoms of eastern Britain is less clear. King Eohric was probably ruling in East Anglia, but no dates can reliably be assigned to the successors of Guthfrith of York in Northumbria. It is known that Guthfrith was succeeded by Sigurd and Cnut, although whether these men ruled jointly or one after the other is uncertain. Northumbria may have been divided by this time between the Viking kings in York and the local rulers, perhaps represented by Eadulf, based at Bamburgh who controlled the lands from the River Tyne or River Tees to the Forth in the north.
In Ireland, Flann Sinna, married to Constantine's aunt Máel Muire, was dominant. The years around 900 represented a period of weakness among the Vikings and Norse-Gaels of Dublin. They are reported to have been divided between two rival leaders. In 894 one group left Dublin, perhaps settling on the Irish Sea coast of Britain between the River Mersey and the Firth of Clyde. The remaining Dubliners were expelled in 902 by Flann Sinna's son-in-law Cerball mac Muirecáin, and soon afterwards appeared in western and northern Britain.
To the south-west of Constantine's lands lay the kingdom of Strathclyde. This extended north into the Lennox, east to the River Forth, and south into the Southern Uplands. In 900 it was probably ruled by King Dyfnwal.
The situation of the Gaelic kingdoms of Dál Riata in western Scotland is uncertain. No kings are known by name after Áed mac Boanta. The Frankish "Annales Bertiniani" may record the conquest of the Inner Hebrides, the seaward part of Dál Riata, by Northmen in 849. In addition to these, the arrival of new groups of Vikings from northern and western Europe was still commonplace. Whether there were Viking or Norse-Gael kingdoms in the Western Isles or the Northern Isles at this time is debated.
Early life.
Áed, Constantine's father, succeeded Constantine's uncle and namesake Constantine I in 876 but was killed in 878. Áed's short reign is glossed as being of no importance by most king lists. Although the date of his birth is nowhere recorded, Constantine II cannot have been born any later than the year after his father's death, that is 879. His name may suggest that he was born rather earlier, during the reign of his uncle Constantine I.
After Áed's death there is a two decade gap until the death of Donald II (Domnall mac Constantín) in 900 during which nothing is reported in the Irish annals. The entry for the reign between Áed and Donald II is corrupt in the "Chronicle of the Kings of Alba", and in this case the "Chronicle" is at variance with every other king list. According to the "Chronicle", Áed was followed by Eochaid, a grandson of Kenneth MacAlpin, who is somehow connected with Giric, but all other lists say that Giric ruled after Áed and make great claims for him. Giric is not known to have been a kinsman of Kenneth's, although it has been suggested that he was related to him by marriage. The major changes in Pictland which began at about this time have been associated by Alex Woolf and Archie Duncan with Giric's reign.
Woolf suggests that Constantine and his cousin Donald may have passed Giric's reign in exile in Ireland where their aunt Máel Muire was wife of two successive High Kings of Ireland, Áed Findliath and Flann Sinna. Giric died in 889. If he had been in exile, Constantine may have returned to Pictland where his cousin Donald II became king. Donald's reputation is suggested by the epithet "dasachtach", a word used of violent madmen and mad bulls, attached to him in the 11th-century writings of Flann Mainistrech, echoed by the his description in the "Prophecy of Berchan" as "the rough one who will think relics and psalms of little worth". Wars with the Viking kings in Britain and Ireland continued during Donald's reign and he was probably killed fighting yet more Vikings at Dunnottar in the Mearns in 900. Constantine succeeded him as king.
Vikings and bishops.
The earliest event recorded in the "Chronicle of the Kings of Alba" in Constantine's reign is an attack by Vikings and the plundering of Dunkeld "and all Albania" in his third year. This is the first use of the word Albania, the Latin form of the Old Irish "Alba", in the "Chronicle" which until then describes the lands ruled by the descendants of Cináed as Pictavia.
These Northmen may have been some of those who were driven out of Dublin in 902, but could also have been the same group who had defeated Domnall in 900. The "Chronicle" states that the Northmen were killed in "Srath Erenn", which is confirmed by the "Annals of Ulster" which records the death of Ímar grandson of Ímar and many others at the hands of the men of Fortriu in 904. This Ímar was the first of the Uí Ímair, that is the grandsons of Ímar, to be reported; three more grandsons of Ímar appear later in Constantín's reign. The "Fragmentary Annals of Ireland" contain an account of the battle, and this attributes the defeat of the Norsemen to the intercession of Saint Columba following fasting and prayer. An entry in the "Chronicon Scotorum" under the year 904 may possibly contain a corrupted reference to this battle.
The next event reported by the "Chronicle of the Kings of Alba" is dated to 906. This records that:King Constantine and Bishop Cellach met at the "Hill of Belief" near the royal city of Scone and pledged themselves that the laws and disciplines of the faith, and the laws of churches and gospels, should be kept "pariter cum Scottis". The meaning of this entry, and its significance, have been the subject of debate.
The phrase "pariter cum Scottis" in the Latin text of the "Chronicle" has been translated in several ways. William Forbes Skene and Alan Orr Anderson proposed that it should be read as "in conformity with the customs of the Gaels", relating it to the claims in the king lists that Giric liberated the church from secular oppression and adopted Irish customs. It has been read as "together with the Gaels", suggesting either public participation or the presence of Gaels from the western coasts as well as the people of the east coast. Finally, it is suggested that it was the ceremony which followed "the custom of the Gaels" and not the agreements.
The idea that this gathering agreed to uphold Irish laws governing the church has suggested that it was an important step in the gaelicisation of the lands east of Druim Alban. Others have proposed that the ceremony in some way endorsed Constantine's kingship, prefiguring later royal inaugurations at Scone. Alternatively, if Bishop Cellach was appointed by Giric, it may be that the gathering was intended to heal a rift between king and church.
Return of the Uí Ímair.
Following the events at Scone, there is little of substance reported for a decade. A story in the "Fragmentary Annals of Ireland", perhaps referring to events some time after 911, claims that Queen Æthelflæd, who ruled in Mercia, allied with the Irish and northern rulers against the Norsemen on the Irish sea coasts of Northumbria. The "Annals of Ulster" record the defeat of an Irish fleet from the kingdom of Ulaid by Vikings "on the coast of England" at about this time.
In this period the "Chronicle of the Kings of Alba" reports the death of Cormac mac Cuilennáin, king of Munster, in the eighth year of Constantine's reign. This is followed by an undated entry which was formerly read as "In his time Domnall Dyfnwal, king of the Britons died, and Domnall son of Áed was elected". This was thought to record the election of a brother of Constantine named Domnall to the kingship of the Britons of Strathclyde and was seen as early evidence of the domination of Strathclyde by the kings of Alba. The entry in question is now read as "...Dynfwal... and Domnall son Áed king of Ailech died", this Domnall being a son of Áed Findliath who died on 915. Finally, the deaths of Flann Sinna and Niall Glúndub are recorded.
There are more reports of Viking fleets in the Irish Sea from 914 onwards. By 916 fleets under Sihtric Cáech and Ragnall, said to be grandsons of Ímar (that is, they belonged to the same Uí Ímair kindred as the Ímar who was killed in 904), were very active in Ireland. Sihtric inflicted a heavy defeat on the armies of Leinster and retook Dublin in 917. The following year Ragnall appears to have returned across the Irish sea intent on establishing himself as king at York. The only precisely dated event in the summer of 918 is the death of Queen Æthelflæd on 918 at Tamworth, Staffordshire. Æthelflæd had been negotiating with the Northumbrians to obtain their submission, but her death put an end to this and her successor, her brother Edward the Elder, was occupied with securing control of Mercia.
The northern part of Northumbria, and perhaps the whole kingdom, had probably been ruled by Ealdred son of Eadulf since 913. Faced with Ragnall's invasion, Ealdred came north seeking assistance from Constantine. The two advanced south to face Ragnall, and this led to a battle somewhere on the banks of the River Tyne, probably at Corbridge where Dere Street crosses the river. The Battle of Corbridge appears to have been indecisive; the "Chronicle of the Kings of Alba" is alone in giving Constantine the victory.
The report of the battle in the "Annals of Ulster" says that none of the kings or mormaers among the men of Alba were killed. This is the first surviving use of the word mormaer; other than the knowledge that Constantine's kingdom had its own bishop or bishops and royal villas, this is the only hint to the institutions of the kingdom.
After Corbridge, Ragnall enjoyed only a short respite. In the south, Alfred's son Edward had rapidly secured control of Mercia and had a burh constructed at Bakewell in the Peak District from which his armies could easily strike north. An army from Dublin led by Ragnall's kinsman Sihtric struck at north-western Mercia in 919, but in 920 or 921 Edward met with Ragnall and other kings. The "Anglo-Saxon Chronicle" states that these king "chose Edward as father and lord". Among the other kings present were Constantine, Ealdred son of Eadwulf, and the king of Strathclyde, either Dyfnwal II or, more probably, Owen I. Here, again, a new term appears in the record, the "Anglo-Saxon Chronicle" for the first time using the word "scottas", from which Scots derives, to describe the inhabitants of Constantine's kingdom in its report of these events.
Edward died in 924. His realms appear to have been divided with the West Saxons recognising Ælfweard while the Mercians chose Æthelstan who had been raised at Æthelflæd's court. Ælfweard died within weeks of his father and Æthelstan was inaugurated as king of all of Edward's lands in 925.
Æthelstan.
By 926 Sihtric had evidently acknowledged Æthelstan as over-king, adopting Christianity and marrying a sister of Æthelstan at Tamworth. Within the year he may have abandoned his new faith and repudiated his wife, but before Æthelstan and he could fight, Sihtric died suddenly in 927. His kinsman, perhaps brother, Gofraid, who had remained as his deputy in Dublin, came from Ireland to take power in York, but failed. Æthelstan moved quickly, seizing much of Northumbria. In less than a decade, the kingdom of the English had become by far the greatest power in Britain and Ireland, perhaps stretching as far north as the Firth of Forth.
John of Worcester's chronicle suggests that Æthelstan faced opposition from Constantine, from Owain of Strathclyde, and from the Welsh kings. William of Malmesbury writes that Gofraid, together with Sihtric's young son Olaf Cuaran fled north and received refuge from Constantine, which led to war with Æthelstan. A meeting at Eamont Bridge on 927 was sealed by an agreement that Constantine, Owen of Strathclyde, Hywel Dda, and Ealdred would "renounce all idolatry": that is, they would not ally with the Viking kings. William states that Æthelstan stood godfather to a son of Constantine, probably Indulf (Ildulb mac Constantín), during the conference.
Æthelstan followed up his advances in the north by securing the recognition of the Welsh kings. For the next seven years, the record of events in the north is blank. Æthelstan's court was attended by the Welsh kings, but not by Constantine or Owen of Strathclyde. This absence of record means that Æthelstan's reasons for marching north against Constantine in 934 are unclear.
Æthelstan's campaign is reported by in brief by the "Anglo-Saxon Chronicle", and later chroniclers such as John of Worcester, William of Malmesbury, Henry of Huntingdon, and Symeon of Durham add detail to that bald account. Æthelstan's army began gathering at Winchester by 934, and reached Nottingham by . He was accompanied by many leaders, including the Welsh kings Hywel Dda, Idwal Foel, and Morgan ab Owain. From Mercia the army went north, stopping at Chester-le-Street, before resuming the march accompanied by a fleet of ships. Owen of Strathclyde was defeated and Symeon states that the army went as far north as Dunnottar and Fortriu, while the fleet is said to have raided Caithness, by which a much larger area, including Sutherland, is probably intended. It is unlikely that Constantine's personal authority extended so far north, and while the attacks may have been directed at his allies, they may also have been simple looting expeditions.
The "Annals of Clonmacnoise" state that "the Scottish men compelled [Æthelstan] to return without any great victory", while Henry of Huntingdon claims that the English faced no opposition. A negotiated settlement may have ended matters: according to John of Worcester, a son of Constantine was given as a hostage to Æthelstan and Constantine himself accompanied the English king on his return south. He witnessed a charter with Æthelstan at Buckingham on 934 in which he is described as "subregulus", that is a king acknowledging Æthelstan's overlordship. The following year, Constantine was again in England at Æthelstan's court, this time at Cirencester where he appears as a witness, appearing as the first of several subject kings, followed by Owen of Strathclyde and Hywel Dda, who subscribed to the diploma. At Christmas of 935, Owen of Strathclyde was once more at Æthelstan's court along with the Welsh kings, but Constantine was not. His return to England less than two years later would be in very different circumstances.
Brunanburh and after.
Following his disappearance from Æthelstan's court after 935, there is no further report of Constantine until 937. In that year, together with Owen of Strathclyde and Olaf Guthfrithson of Dublin, Constantine invaded England. The resulting battle of Brunanburh—"Dún Brunde"—is reported in the "Annals of Ulster" as follows:a great battle, lamentable and terrible was cruelly fought... in which fell uncounted thousands of the Northmen.  ...And on the other side, a multitude of Saxons fell; but Æthelstan, the king of the Saxons, obtained a great victory. The battle was remembered in England a generation later as "the Great Battle". When reporting the battle, the "Anglo-Saxon Chronicle" abandons its usual terse style in favour of a heroic poem vaunting the great victory. In this the "hoary" Constantine, by now around 60 years of age, is said to have lost a son in the battle, a claim which the "Chronicle of the Kings of Alba" confirms. The "Annals of Clonmacnoise" give his name as Cellach. For all its fame, the site of the battle is uncertain and several sites have been advanced, with Bromborough on the Wirral the most favoured location.
Brunanburh, for all that it had been a famous and bloody battle, settled nothing. On 939 Æthelstan, the "pillar of the dignity of the western world" in the words of the "Annals of Ulster", died at Malmesbury. He was succeeded by his brother Edmund, then aged 18. Æthelstan's empire, seemingly made safe by the victory of Brunanburh, collapsed in little more than a year from his death when Amlaíb returned from Ireland and seized Northumbria and the Mercian Danelaw. Edmund spent the remainder of Constantín's reign rebuilding the empire.
For Constantine's last years as king there is only the meagre record of the "Chronicle of the Kings of Alba". The death of Æthelstan is reported, as are two others. The first of these, in 938, is that of Dubacan, mormaer of Angus or son of the mormaer. Unlike the report of 918, on this occasion the title mormaer is attached to a geographical area, but it is unknown whether the Angus of 938 was in any way similar to the later mormaerdom or earldom. The second death, entered with that of Æthelstan, is that of Eochaid mac Ailpín, who may, from his name, have been a kinsman of Constantín.
Abdication and posterity.
By the early 940s Constantine was an old man, perhaps more than 70 years of age. The kingdom of Alba was too new to be said to have a customary rule of succession, but Pictish and Irish precedents favoured an adult successor descended from Kenneth MacAlpin. Constantine's surviving son Indulf, probably baptised in 927, would have been too young to be a serious candidate for the kingship in the early 940s, and the obvious heir was Constantine's nephew, Malcolm I. As Malcolm was born no later than 901, by the 940s he was no longer a young man, and may have been impatient. Willingly or not—the 11th-century "Prophecy of Berchán", a verse history in the form of a supposed prophecy, states that it was not a voluntary decision—Constantine abdicated in 943 and entered a monastery, leaving the kingdom to Malcolm.
Although his retirement may have been involuntary, the "Life" of Cathróe of Metz and the "Prophecy of Berchán" portray Constantine as a devout king. The monastery which Constantine retired to, and where he is said to have been abbot, was probably that of St Andrews. This had been refounded in his reign and given to the reforming Céli Dé (Culdee) movement. The Céli Dé were subsequently to be entrusted with many monasteries throughout the kingdom of Alba until replaced in the 12th century by new orders imported from France.
Seven years later the "Chronicle of the Kings of Alba" says:[Malcolm I] plundered the English as far as the river Tees, and he seized a multitude of people and many herds of cattle: and the Scots called this the raid of Albidosorum, that is, Nainndisi. But others say that Constantine made this raid, asking of the king, Malcolm, that the kingship should be given to him for a week's time, so that he could visit the English. In fact, it was Malcolm who made the raid, but Constantine incited him, as I have said. Woolf suggests that the association of Constantine with the raid is a late addition, one derived from a now-lost saga or poem.
Constantine's death in 952 is recorded by the Irish annals, who enter it among ecclesiastics. His son Indulf would become king on Malcolm's death. The last of Constantine's certain descendants to be king in Alba was a great-grandson, Constantine III (Constantín mac Cuiléin). Another son had died at Brunanburh, and, according to John of Worcester, Amlaíb mac Gofraid was married to a daughter of Constantine. It is possible that Constantine had other children, but like the name of his wife, or wives, this has not been recorded.
The form of kingdom which appeared in Constantine's reign continued in much the same way until the Davidian Revolution in the 12th century. As with his ecclesiastical reforms, his political legacy was the creation of a new form of Scottish kingship that lasted for two centuries after his death.

</doc>
<doc id="7236" url="https://en.wikipedia.org/wiki?curid=7236" title="Constantine the Great">
Constantine the Great

Constantine the Great (; Greek: Κωνσταντῖνος ὁ Μέγας; 27 February 272 AD – 22 May 337 AD), also known as Constantine I or Saint Constantine (in the Orthodox Church as Saint Constantine the Great, Equal-to-the-Apostles), was a Roman Emperor from 306 to 337 AD. Constantine was the son of Flavius Valerius Constantius, a Roman army officer, and his consort Helena. His father became "Caesar", the deputy emperor in the west in 293 AD. Constantine was sent east, where he rose through the ranks to become a military tribune under the emperors Diocletian and Galerius. In 305, Constantius was raised to the rank of "Augustus", senior western emperor, and Constantine was recalled west to campaign under his father in Britannia (Britain). Acclaimed as emperor by the army at Eboracum (Modern-day York) after his father's death in 306 AD, Constantine emerged victorious in a series of civil wars against the emperors Maxentius and Licinius to become sole ruler of both west and east by 324 AD.
As emperor, Constantine enacted many administrative, financial, social, and military reforms to strengthen the empire. The government was restructured and civil and military authority separated. A new gold coin, the solidus, was introduced to combat inflation. It would become the standard for Byzantine and European currencies for more than a thousand years. The first Roman emperor to claim conversion to Christianity, Constantine played an influential role in the proclamation of the Edict of Milan in 313, which decreed tolerance for Christianity in the empire. He called the First Council of Nicaea in 325, at which the Nicene Creed was professed by Christians. In military matters, the Roman army was reorganised to consist of mobile field units and garrison soldiers capable of countering internal threats and barbarian invasions. Constantine pursued successful campaigns against the tribes on the Roman frontiers—the Franks, the Alamanni, the Goths, and the Sarmatians—even resettling territories abandoned by his predecessors during the turmoil of the previous century.
The age of Constantine marked a distinct epoch in the history of the Roman Empire. He built a new imperial residence at Byzantium and renamed the city Constantinople after himself (the laudatory epithet of "New Rome" came later, and was never an official title). It would later become the capital of the Empire for over one thousand years; for which reason the later Eastern Empire would come to be known as the Byzantine Empire. His more immediate political legacy was that, in leaving the empire to his sons, he replaced Diocletian's tetrarchy with the principle of dynastic succession. His reputation flourished during the lifetime of his children and centuries after his reign. The medieval church upheld him as a paragon of virtue while secular rulers invoked him as a prototype, a point of reference, and the symbol of imperial legitimacy and identity. Beginning with the Renaissance, there were more critical appraisals of his reign due to the rediscovery of anti-Constantinian sources. Critics portrayed him as a tyrant. Trends in modern and recent scholarship attempted to balance the extremes of previous scholarship.
Constantine—as possibly the first Christian emperor (although that title could possibly go to Philip the Arab)—is a significant figure in the history of Christianity. The Church of the Holy Sepulchre, built on his orders at the purported site of Jesus' tomb in Jerusalem, became the holiest place in Christendom. The Papal claim to temporal power was based on the supposed Donation of Constantine. He is venerated as a saint by Eastern Orthodox Christians, Byzantine Catholics, and Anglicans.
Sources.
Constantine was a ruler of major historical importance, and he has always been a controversial figure. The fluctuations in Constantine's reputation reflect the nature of the ancient sources for his reign. These are abundant and detailed, but have been strongly influenced by the official propaganda of the period, and are often one-sided. There are no surviving histories or biographies dealing with Constantine's life and rule. The nearest replacement is Eusebius of Caesarea's "Vita Constantini", a work that is a mixture of eulogy and hagiography. Written between 335 AD and circa 339 AD, the "Vita" extols Constantine's moral and religious virtues. The "Vita" creates a contentiously positive image of Constantine, and modern historians have frequently challenged its reliability. The fullest secular life of Constantine is the anonymous "Origo Constantini". A work of uncertain date, the "Origo" focuses on military and political events, to the neglect of cultural and religious matters.
Lactantius' "De Mortibus Persecutorum", a political Christian pamphlet on the reigns of Diocletian and the Tetrarchy, provides valuable but tendentious detail on Constantine's predecessors and early life. The ecclesiastical histories of Socrates, Sozomen, and Theodoret describe the ecclesiastic disputes of Constantine's later reign. Written during the reign of Theodosius II (408–50 AD), a century after Constantine's reign, these ecclesiastic historians obscure the events and theologies of the Constantinian period through misdirection, misrepresentation and deliberate obscurity. The contemporary writings of the orthodox Christian Athanasius and the ecclesiastical history of the Arian Philostorgius also survive, though their biases are no less firm.
The epitomes of Aurelius Victor ("De Caesaribus"), Eutropius ("Breviarium"), Festus ("Breviarium"), and the anonymous author of the "Epitome de Caesaribus" offer compressed secular political and military histories of the period. Although not Christian, the epitomes paint a favorable image of Constantine, but omit reference to Constantine's religious policies. The "Panegyrici Latini", a collection of panegyrics from the late third and early fourth centuries, provide valuable information on the politics and ideology of the tetrarchic period and the early life of Constantine. Contemporary architecture, such as the Arch of Constantine in Rome and palaces in Gamzigrad and Córdoba, epigraphic remains, and the coinage of the era complement the literary sources.
Early life.
Flavius Valerius Constantinus, as he was originally named, was born in the city of Naissus, (today Niš, Serbia) part of the Dardania province of Moesia on 27 February, probably 272 AD. His father was Flavius Constantius, a native of Dardania province of Moesia (later Dacia Ripensis). In a personal account written in 362 A.D. Emperor Julian, Constantius' grandson and Constantine's nephew, was considering himself the offspring of a Thracian family. Constantine probably spent little time with his father who was an officer in the Roman army, part of the Emperor Aurelian's imperial bodyguard. Being described as a tolerant and politically skilled man, Constantius advanced through the ranks, earning the governorship of Dalmatia from Emperor Diocletian, another of Aurelian's companions from Illyricum, in 284 or 285. Constantine's mother was Helena, possibly a Bithynian woman of low social standing. It is uncertain whether she was legally married to Constantius or merely his concubine. It is unclear if Constantine could speak Thracian, his main language being Latin, during his public speeches he needed Greek translators.
In July 285 AD, Diocletian declared Maximian, another colleague from Illyricum, his co-emperor. Each emperor would have his own court, his own military and administrative faculties, and each would rule with a separate praetorian prefect as chief lieutenant. Maximian ruled in the West, from his capitals at Mediolanum (Milan, Italy) or Augusta Treverorum (Trier, Germany), while Diocletian ruled in the East, from Nicomedia (İzmit, Turkey). The division was merely pragmatic: the Empire was called "indivisible" in official panegyric, and both emperors could move freely throughout the Empire. In 288, Maximian appointed Constantius to serve as his praetorian prefect in Gaul. Constantius left Helena to marry Maximian's stepdaughter Theodora in 288 or 289.
Diocletian divided the Empire again in 293 AD, appointing two Caesars (junior emperors) to rule over further subdivisions of East and West. Each would be subordinate to their respective Augustus (senior emperor) but would act with supreme authority in his assigned lands. This system would later be called the Tetrarchy. Diocletian's first appointee for the office of Caesar was Constantius; his second was Galerius, a native of Felix Romuliana. According to Lactantius, Galerius was a brutal, animalistic man. Although he shared the paganism of Rome's aristocracy, he seemed to them an alien figure, a semi-barbarian. On 1 March, Constantius was promoted to the office of Caesar, and dispatched to Gaul to fight the rebels Carausius and Allectus. In spite of meritocratic overtones, the Tetrarchy retained vestiges of hereditary privilege, and Constantine became the prime candidate for future appointment as Caesar as soon as his father took the position. Constantine went to the court of Diocletian, where he lived as his father's heir presumptive.
In the East.
Constantine received a formal education at Diocletian's court, where he learned Latin literature, Greek, and philosophy. The cultural environment in Nicomedia was open, fluid and socially mobile, and Constantine could mix with intellectuals both pagan and Christian. He may have attended the lectures of Lactantius, a Christian scholar of Latin in the city. Because Diocletian did not completely trust Constantius—none of the Tetrarchs fully trusted their colleagues—Constantine was held as something of a hostage, a tool to ensure Constantius's best behavior. Constantine was nonetheless a prominent member of the court: he fought for Diocletian and Galerius in Asia, and served in a variety of tribunates; he campaigned against barbarians on the Danube in 296 AD, and fought the Persians under Diocletian in Syria (297 AD) and under Galerius in Mesopotamia (298–299 AD). By late 305 AD, he had become a tribune of the first order, a "tribunus ordinis primi".
Constantine had returned to Nicomedia from the eastern front by the spring of 303 AD, in time to witness the beginnings of Diocletian's "Great Persecution", the most severe persecution of Christians in Roman history. In late 302, Diocletian and Galerius sent a messenger to the oracle of Apollo at Didyma with an inquiry about Christians. Constantine could recall his presence at the palace when the messenger returned, when Diocletian accepted his court's demands for universal persecution. On 23 February 303 AD, Diocletian ordered the destruction of Nicomedia's new church, condemned its scriptures to the flames, and had its treasures seized. In the months that followed, churches and scriptures were destroyed, Christians were deprived of official ranks, and priests were imprisoned.
It is unlikely that Constantine played any role in the persecution. In his later writings he would attempt to present himself as an opponent of Diocletian's "sanguinary edicts" against the "worshippers of God", but nothing indicates that he opposed it effectively at the time. Although no contemporary Christian challenged Constantine for his inaction during the persecutions, it remained a political liability throughout his life.
On 1 May 305 AD, Diocletian, as a result of a debilitating sickness taken in the winter of 304–305 AD, announced his resignation. In a parallel ceremony in Milan, Maximian did the same. Lactantius states that Galerius manipulated the weakened Diocletian into resigning, and forced him to accept Galerius' allies in the imperial succession. According to Lactantius, the crowd listening to Diocletian's resignation speech believed, until the very last moment, that Diocletian would choose Constantine and Maxentius (Maximian's son) as his successors. It was not to be: Constantius and Galerius were promoted to Augusti, while Severus and Maximinus Daia, Galerius' nephew, were appointed their Caesars respectively. Constantine and Maxentius were ignored.
Some of the ancient sources detail plots that Galerius made on Constantine's life in the months following Diocletian's abdication. They assert that Galerius assigned Constantine to lead an advance unit in a cavalry charge through a swamp on the middle Danube, made him enter into single combat with a lion, and attempted to kill him in hunts and wars. Constantine always emerged victorious: the lion emerged from the contest in a poorer condition than Constantine; Constantine returned to Nicomedia from the Danube with a Sarmatian captive to drop at Galerius' feet. It is uncertain how much these tales can be trusted.
In the West.
Constantine recognized the implicit danger in remaining at Galerius's court, where he was held as a virtual hostage. His career depended on being rescued by his father in the west. Constantius was quick to intervene. In the late spring or early summer of 305 AD, Constantius requested leave for his son to help him campaign in Britain. After a long evening of drinking, Galerius granted the request. Constantine's later propaganda describes how he fled the court in the night, before Galerius could change his mind. He rode from post-house to post-house at high speed, hamstringing every horse in his wake. By the time Galerius awoke the following morning, Constantine had fled too far to be caught. Constantine joined his father in Gaul, at Bononia (Boulogne) before the summer of 305 AD.
From Bononia they crossed the Channel to Britain and made their way to Eboracum (York), capital of the province of Britannia Secunda and home to a large military base. Constantine was able to spend a year in northern Britain at his father's side, campaigning against the Picts beyond Hadrian's Wall in the summer and autumn. Constantius's campaign, like that of Septimius Severus before it, probably advanced far into the north without achieving great success. Constantius had become severely sick over the course of his reign, and died on 25 July 306 in Eboracum (York). Before dying, he declared his support for raising Constantine to the rank of full Augustus. The Alamannic king Chrocus, a barbarian taken into service under Constantius, then proclaimed Constantine as Augustus. The troops loyal to Constantius' memory followed him in acclamation. Gaul and Britain quickly accepted his rule; Iberia, which had been in his father's domain for less than a year, rejected it.
Constantine sent Galerius an official notice of Constantius's death and his own acclamation. Along with the notice, he included a portrait of himself in the robes of an Augustus. The portrait was wreathed in bay. He requested recognition as heir to his father's throne, and passed off responsibility for his unlawful ascension on his army, claiming they had "forced it upon him". Galerius was put into a fury by the message; he almost set the portrait on fire. His advisers calmed him, and argued that outright denial of Constantine's claims would mean certain war. Galerius was compelled to compromise: he granted Constantine the title "Caesar" rather than "Augustus" (the latter office went to Severus instead). Wishing to make it clear that he alone gave Constantine legitimacy, Galerius personally sent Constantine the emperor's traditional purple robes. Constantine accepted the decision, knowing that it would remove doubts as to his legitimacy.
Early rule.
Constantine's share of the Empire consisted of Britain, Gaul, and Spain. He therefore commanded one of the largest Roman armies, stationed along the important Rhine frontier. After his promotion to emperor, Constantine remained in Britain, driving back the tribes of the Picts and secured his control in the northwestern dioceses. He completed the reconstruction of military bases begun under his father's rule, and ordered the repair of the region's roadways. He soon left for Augusta Treverorum (Trier) in Gaul, the Tetrarchic capital of the northwestern Roman Empire. The Franks, after learning of Constantine's acclamation, invaded Gaul across the lower Rhine over the winter of 306–307 AD. Constantine drove them back beyond the Rhine and captured two of their kings, Ascaric and Merogaisus. The kings and their soldiers were fed to the beasts of Trier's amphitheater in the "adventus" (arrival) celebrations that followed.
Constantine began a major expansion of Trier. He strengthened the circuit wall around the city with military towers and fortified gates, and began building a palace complex in the northeastern part of the city. To the south of his palace, he ordered the construction of a large formal audience hall, and a massive imperial bathhouse. Constantine sponsored many building projects across Gaul during his tenure as emperor of the West, especially in Augustodunum (Autun) and Arelate (Arles). According to Lactantius, Constantine followed his father in following a tolerant policy towards Christianity. Although not yet a Christian, he probably judged it a more sensible policy than open persecution, and a way to distinguish himself from the "great persecutor", Galerius. Constantine decreed a formal end to persecution, and returned to Christians all they had lost during the persecutions.
Because Constantine was still largely untried and had a hint of illegitimacy about him, he relied on his father's reputation in his early propaganda: the earliest panegyrics to Constantine give as much coverage to his father's deeds as to those of Constantine himself. Constantine's military skill and building projects soon gave the panegyrist the opportunity to comment favorably on the similarities between father and son, and Eusebius remarked that Constantine was a "renewal, as it were, in his own person, of his father's life and reign". Constantinian coinage, sculpture and oratory also shows a new tendency for disdain towards the "barbarians" beyond the frontiers. After Constantine's victory over the Alemanni, he minted a coin issue depicting weeping and begging Alemannic tribesmen—"The Alemanni conquered"—beneath the phrase "Romans' rejoicing". There was little sympathy for these enemies. As his panegyrist declared: "It is a stupid clemency that spares the conquered foe."
Maxentius' rebellion.
Following Galerius' recognition of Constantine as caesar, Constantine's portrait was brought to Rome, as was customary. Maxentius mocked the portrait's subject as the son of a harlot, and lamented his own powerlessness. Maxentius, envious of Constantine's authority, seized the title of emperor on 28 October 306 AD. Galerius refused to recognize him, but failed to unseat him. Galerius sent Severus against Maxentius, but during the campaign, Severus' armies, previously under command of Maxentius' father Maximian, defected, and Severus was seized and imprisoned. Maximian, brought out of retirement by his son's rebellion, left for Gaul to confer with Constantine in late 307 AD. He offered to marry his daughter Fausta to Constantine, and elevate him to Augustan rank. In return, Constantine would reaffirm the old family alliance between Maximian and Constantius, and offer support to Maxentius' cause in Italy. Constantine accepted, and married Fausta in Trier in late summer 307 AD. Constantine now gave Maxentius his meagre support, offering Maxentius political recognition.
Constantine remained aloof from the Italian conflict, however. Over the spring and summer of 307 AD, he had left Gaul for Britain to avoid any involvement in the Italian turmoil; now, instead of giving Maxentius military aid, he sent his troops against Germanic tribes along the Rhine. In 308 AD, he raided the territory of the Bructeri, and made a bridge across the Rhine at Colonia Agrippinensium (Cologne). In 310 AD, he marched to the northern Rhine and fought the Franks. When not campaigning, he toured his lands advertising his benevolence, and supporting the economy and the arts. His refusal to participate in the war increased his popularity among his people, and strengthened his power base in the West. Maximian returned to Rome in the winter of 307–308 AD, but soon fell out with his son. In early 308 AD, after a failed attempt to usurp Maxentius' title, Maximian returned to Constantine's court.
On 11 November 308 AD, Galerius called a general council at the military city of Carnuntum (Petronell-Carnuntum, Austria) to resolve the instability in the western provinces. In attendance were Diocletian, briefly returned from retirement, Galerius, and Maximian. Maximian was forced to abdicate again and Constantine was again demoted to Caesar. Licinius, one of Galerius' old military companions, was appointed Augustus in the western regions. The new system did not last long: Constantine refused to accept the demotion, and continued to style himself as Augustus on his coinage, even as other members of the Tetrarchy referred to him as a Caesar on theirs. Maximinus Daia was frustrated that he had been passed over for promotion while the newcomer Licinius had been raised to the office of Augustus, and demanded that Galerius promote him. Galerius offered to call both Maximinus and Constantine "sons of the Augusti", but neither accepted the new title. By the spring of 310 AD, Galerius was referring to both men as Augusti.
Maximian's rebellion.
In 310 AD, a dispossessed Maximian rebelled against Constantine while Constantine was away campaigning against the Franks. Maximian had been sent south to Arles with a contingent of Constantine's army, in preparation for any attacks by Maxentius in southern Gaul. He announced that Constantine was dead, and took up the imperial purple. In spite of a large donative pledge to any who would support him as emperor, most of Constantine's army remained loyal to their emperor, and Maximian was soon compelled to leave. Constantine soon heard of the rebellion, abandoned his campaign against the Franks, and marched his army up the Rhine. At Cabillunum (Chalon-sur-Saône), he moved his troops onto waiting boats to row down the slow waters of the Saône to the quicker waters of the Rhone. He disembarked at Lugdunum (Lyon). Maximian fled to Massilia (Marseille), a town better able to withstand a long siege than Arles. It made little difference, however, as loyal citizens opened the rear gates to Constantine. Maximian was captured and reproved for his crimes. Constantine granted some clemency, but strongly encouraged his suicide. In July 310 AD, Maximian hanged himself.
In spite of the earlier rupture in their relations, Maxentius was eager to present himself as his father's devoted son after his death. He began minting coins with his father's deified image, proclaiming his desire to avenge Maximian's death. Constantine initially presented the suicide as an unfortunate family tragedy. By 311 AD, however, he was spreading another version. According to this, after Constantine had pardoned him, Maximian planned to murder Constantine in his sleep. Fausta learned of the plot and warned Constantine, who put a eunuch in his own place in bed. Maximian was apprehended when he killed the eunuch and was offered suicide, which he accepted. Along with using propaganda, Constantine instituted a "damnatio memoriae" on Maximian, destroying all inscriptions referring to him and eliminating any public work bearing his image.
The death of Maximian required a shift in Constantine's public image. He could no longer rely on his connection to the elder emperor Maximian, and needed a new source of legitimacy. In a speech delivered in Gaul on 25 July 310 AD, the anonymous orator reveals a previously unknown dynastic connection to Claudius II, a 3rd Century emperor famed for defeating the Goths and restoring order to the empire. Breaking away from tetrarchic models, the speech emphasizes Constantine's ancestral prerogative to rule, rather than principles of imperial equality. The new ideology expressed in the speech made Galerius and Maximian irrelevant to Constantine's right to rule. Indeed, the orator emphasizes ancestry to the exclusion of all other factors: "No chance agreement of men, nor some unexpected consequence of favor, made you emperor," the orator declares to Constantine.
The oration also moves away from the religious ideology of the Tetrarchy, with its focus on twin dynasties of Jupiter and Hercules. Instead, the orator proclaims that Constantine experienced a divine vision of Apollo and Victory granting him laurel wreaths of health and a long reign. In the likeness of Apollo Constantine recognized himself as the saving figure to whom would be granted "rule of the whole world", as the poet Virgil had once foretold. The oration's religious shift is paralleled by a similar shift in Constantine's coinage. In his early reign, the coinage of Constantine advertised Mars as his patron. From 310 AD on, Mars was replaced by Sol Invictus, a god conventionally identified with Apollo. There is little reason to believe that either the dynastic connection or the divine vision are anything other than fiction, but their proclamation strengthened Constantine's claims to legitimacy and increased his popularity among the citizens of Gaul.
Civil wars.
War against Maxentius.
By the middle of 310 AD, Galerius had become too ill to involve himself in imperial politics. His final act survives: a letter to the provincials posted in Nicomedia on 30 April 311 AD, proclaiming an end to the persecutions, and the resumption of religious toleration. He died soon after the edict's proclamation, destroying what little remained of the tetrarchy. Maximinus mobilized against Licinius, and seized Asia Minor. A hasty peace was signed on a boat in the middle of the Bosphorus. While Constantine toured Britain and Gaul, Maxentius prepared for war. He fortified northern Italy, and strengthened his support in the Christian community by allowing it to elect a new Bishop of Rome, Eusebius.
Maxentius' rule was nevertheless insecure. His early support dissolved in the wake of heightened tax rates and depressed trade; riots broke out in Rome and Carthage; and Domitius Alexander was able to briefly usurp his authority in Africa. By 312 AD, he was a man barely tolerated, not one actively supported, even among Christian Italians. In the summer of 311 AD, Maxentius mobilized against Constantine while Licinius was occupied with affairs in the East. He declared war on Constantine, vowing to avenge his father's "murder". To prevent Maxentius from forming an alliance against him with Licinius, Constantine forged his own alliance with Licinius over the winter of 311–312 AD, and offered him his sister Constantia in marriage. Maximin considered Constantine's arrangement with Licinius an affront to his authority. In response, he sent ambassadors to Rome, offering political recognition to Maxentius in exchange for a military support. Maxentius accepted. According to Eusebius, inter-regional travel became impossible, and there was military buildup everywhere. There was "not a place where people were not expecting the onset of hostilities every day".
Constantine's advisers and generals cautioned against preemptive attack on Maxentius; even his soothsayers recommended against it, stating that the sacrifices had produced unfavorable omens. Constantine, with a spirit that left a deep impression on his followers, inspiring some to believe that he had some form of supernatural guidance, ignored all these cautions. Early in the spring of 312 AD, Constantine crossed the Cottian Alps with a quarter of his army, a force numbering about 40,000. The first town his army encountered was Segusium (Susa, Italy), a heavily fortified town that shut its gates to him. Constantine ordered his men to set fire to its gates and scale its walls. He took the town quickly. Constantine ordered his troops not to loot the town, and advanced with them into northern Italy.
At the approach to the west of the important city of Augusta Taurinorum (Turin, Italy), Constantine met a large force of heavily armed Maxentian cavalry. In the ensuing battle Constantine's army encircled Maxentius' cavalry, flanked them with his own cavalry, and dismounted them with blows from his soldiers' iron-tipped clubs. Constantine's armies emerged victorious. Turin refused to give refuge to Maxentius' retreating forces, opening its gates to Constantine instead. Other cities of the north Italian plain sent Constantine embassies of congratulation for his victory. He moved on to Milan, where he was met with open gates and jubilant rejoicing. Constantine rested his army in Milan until mid-summer 312 AD, when he moved on to Brixia (Brescia).
Brescia's army was easily dispersed, and Constantine quickly advanced to Verona, where a large Maxentian force was camped. Ruricius Pompeianus, general of the Veronese forces and Maxentius' praetorian prefect, was in a strong defensive position, since the town was surrounded on three sides by the Adige. Constantine sent a small force north of the town in an attempt to cross the river unnoticed. Ruricius sent a large detachment to counter Constantine's expeditionary force, but was defeated. Constantine's forces successfully surrounded the town and laid siege. Ruricius gave Constantine the slip and returned with a larger force to oppose Constantine. Constantine refused to let up on the siege, and sent only a small force to oppose him. In the desperately fought encounter that followed, Ruricius was killed and his army destroyed. Verona surrendered soon afterwards, followed by Aquileia, Mutina (Modena), and Ravenna. The road to Rome was now wide open to Constantine.
Maxentius prepared for the same type of war he had waged against Severus and Galerius: he sat in Rome and prepared for a siege. He still controlled Rome's praetorian guards, was well-stocked with African grain, and was surrounded on all sides by the seemingly impregnable Aurelian Walls. He ordered all bridges across the Tiber cut, reportedly on the counsel of the gods, and left the rest of central Italy undefended; Constantine secured that region's support without challenge. Constantine progressed slowly along the "Via Flaminia", allowing the weakness of Maxentius to draw his regime further into turmoil. Maxentius' support continued to weaken: at chariot races on 27 October, the crowd openly taunted Maxentius, shouting that Constantine was invincible. Maxentius, no longer certain that he would emerge from a siege victorious, built a temporary boat bridge across the Tiber in preparation for a field battle against Constantine. On 28 October 312 AD, the sixth anniversary of his reign, he approached the keepers of the Sibylline Books for guidance. The keepers prophesied that, on that very day, "the enemy of the Romans" would die. Maxentius advanced north to meet Constantine in battle.
Constantine and his army adopt the Greek letters for Christ's initials: Chi Rho.
Maxentius organized his forces—still twice the size of Constantine's—in long lines facing the battle plain, with their backs to the river. Constantine's army arrived at the field bearing unfamiliar symbols on either its standards or its soldiers' shields. According to Lactantius, Constantine was visited by a dream the night before the battle, wherein he was advised "to mark the heavenly sign of God on the shields of his soldiers ... by means of a slanted letter X with the top of its head bent round, he marked Christ on their shields." Eusebius describes another version, where, while marching at midday, "he saw with his own eyes in the heavens a trophy of the cross arising from the light of the sun, carrying the message, "In Hoc Signo Vinces" or "with this sign, you will conquer"; in Eusebius's account, Constantine had a dream the following night, in which Christ appeared with the same heavenly sign, and told him to make a standard, the "labarum", for his army in that form. Eusebius is vague about when and where these events took place, but it enters his narrative before the war against Maxentius begins. Eusebius describes the sign as Chi (Χ) traversed by Rho (Ρ): ☧, a symbol representing the first two letters of the Greek spelling of the word "Christos" or Christ. In 315 AD a medallion was issued at Ticinum showing Constantine wearing a helmet emblazoned with the "Chi Rho", and coins issued at Siscia in 317/318 AD repeat the image. The figure was otherwise rare, however, and is uncommon in imperial iconography and propaganda before the 320s.
Constantine deployed his own forces along the whole length of Maxentius' line. He ordered his cavalry to charge, and they broke Maxentius' cavalry. He then sent his infantry against Maxentius' infantry, pushing many into the Tiber where they were slaughtered and drowned. The battle was brief: Maxentius' troops were broken before the first charge. Maxentius' horse guards and praetorians initially held their position, but broke under the force of a Constantinian cavalry charge; they also broke ranks and fled to the river. Maxentius rode with them, and attempted to cross the bridge of boats, but he was pushed by the mass of his fleeing soldiers into the Tiber, and drowned.
In Rome.
Constantine entered Rome on 29 October 312. He staged a grand "adventus" in the city, and was met with popular jubilation. Maxentius' body was fished out of the Tiber and decapitated. His head was paraded through the streets for all to see. After the ceremonies, Maxentius' disembodied head was sent to Carthage; at this, Carthage would offer no further resistance. Unlike his predecessors, Constantine neglected to make the trip to the Capitoline Hill and perform customary sacrifices at the Temple of Jupiter. He did, however, choose to honor the Senatorial Curia with a visit, where he promised to restore its ancestral privileges and give it a secure role in his reformed government: there would be no revenge against Maxentius' supporters. In response, the Senate decreed him "title of the first name", which meant his name would be listed first in all official documents, and acclaimed him as "the greatest Augustus". He issued decrees returning property lost under Maxentius, recalling political exiles, and releasing Maxentius' imprisoned opponents.
An extensive propaganda campaign followed, during which Maxentius' image was systematically purged from all public places. Maxentius was written up as a "tyrant", and set against an idealized image of the "liberator", Constantine. Eusebius, in his later works, is the best representative of this strand of Constantinian propaganda. Maxentius' rescripts were declared invalid, and the honors Maxentius had granted to leaders of the Senate were invalidated. Constantine also attempted to remove Maxentius' influence on Rome's urban landscape. All structures built by Maxentius were re-dedicated to Constantine, including the Temple of Romulus and the Basilica of Maxentius. At the focal point of the basilica, a stone statue of Constantine holding the Christian "labarum" in its hand was erected. Its inscription bore the message the statue had already made clear: By this sign Constantine had freed Rome from the yoke of the tyrant.
Where he did not overwrite Maxentius' achievements, Constantine upstaged them: the Circus Maximus was redeveloped so that its total seating capacity was twenty-five times larger than that of Maxentius' racing complex on the Via Appia. Maxentius' strongest supporters in the military were neutralized when the Praetorian Guard and Imperial Horse Guard ("equites singulares") were disbanded. Their tombstones were ground up and put to use in a basilica on the Via Labicana. On November 9, 312 AD, barely two weeks after Constantine captured the city, the former base of the Imperial Horse Guard was chosen for redevelopment into the Lateran Basilica. The Legio II Parthica was removed from Alba (Albano Laziale), and the remainder of Maxentius' armies were sent to do frontier duty on the Rhine.
Wars against Licinius.
In the following years, Constantine gradually consolidated his military superiority over his rivals in the crumbling Tetrarchy. In 313, he met Licinius in Milan to secure their alliance by the marriage of Licinius and Constantine's half-sister Constantia. During this meeting, the emperors agreed on the so-called Edict of Milan,
officially granting full tolerance to Christianity and all religions in the Empire.
The document had special benefits for Christians, legalizing their religion and granting them restoration for all property seized during Diocletian's persecution. It repudiates past methods of religious coercion and used only general terms to refer to the divine sphere—"Divinity" and "Supreme Divinity", "summa divinitas".
The conference was cut short, however, when news reached Licinius that his rival Maximin had crossed the Bosporus and invaded European territory. Licinius departed and eventually defeated Maximin, gaining control over the entire eastern half of the Roman Empire. Relations between the two remaining emperors deteriorated, as Constantine suffered an assassination attempt at the hands of a character that Licinius wanted elevated to the rank of Caesar; Licinius, for his part had Constantine's statues in Emona destroyed.
In either 314 or 316 the two Augusti fought against one another at the Battle of Cibalae, with Constantine being victorious. They clashed again at the Battle of Mardia in 317, and agreed to a settlement in which Constantine's sons Crispus and Constantine II, and Licinius' son Licinianus were made "caesars".
After this arrangement, Constantine ruled the dioceses of Pannonia and Macedonia and took residence at Sirmium, from whence he could wage war on the Goths and Sarmatians in 322, and on the Goths in 323.
In the year 320, Licinius allegedly reneged on the religious freedom promised by the Edict of Milan in 313 and began to oppress Christians anew,
generally without bloodshed, but resorting to confiscations and sacking of Christian office-holders. Although this characterization of Licinius as anti-Christian is somewhat doubtful, the fact is that he seems to have been far less open in his support of Christianity than Constantine. Therefore, Licinius was prone to see the Church as a force more loyal to Constantine than to the Imperial system in general – the explanation offered by the Church historian Sozomen.
This dubious arrangement eventually became a challenge to Constantine in the West, climaxing in the great civil war of 324. Licinius, aided by Goth mercenaries, represented the past and the ancient Pagan faiths. Constantine and his Franks marched under the standard of the "labarum", and both sides saw the battle in religious terms. Outnumbered, but fired by their zeal, Constantine's army emerged victorious in the Battle of Adrianople. Licinius fled across the Bosphorus and appointed Martius Martinianus, the commander of his bodyguard, as Caesar, but Constantine next won the Battle of the Hellespont, and finally the Battle of Chrysopolis on 18 September 324.
Licinius and Martinianus surrendered to Constantine at Nicomedia on the promise their lives would be spared: they were sent to live as private citizens in Thessalonica and Cappadocia respectively, but in 325 Constantine accused Licinius of plotting against him and had them both arrested and hanged; Licinius's son (the son of Constantine's half-sister) was also killed.
Thus Constantine became the sole emperor of the Roman Empire.
Later rule.
Foundation of Constantinople.
Licinius' defeat came to represent the defeat of a rival center of Pagan and Greek-speaking political activity in the East, as opposed to the Christian and Latin-speaking Rome, and it was proposed that a new Eastern capital should represent the integration of the East into the Roman Empire as a whole, as a center of learning, prosperity, and cultural preservation for the whole of the Eastern Roman Empire. Among the various locations proposed for this alternative capital, Constantine appears to have toyed earlier with Serdica (present-day Sofia), as he was reported saying that ""Serdica is my Rome"". Sirmium and Thessalonica were also considered. Eventually, however, Constantine decided to work on the Greek city of Byzantium, which offered the advantage of having already been extensively rebuilt on Roman patterns of urbanism, during the preceding century, by Septimius Severus and Caracalla, who had already acknowledged its strategic importance. The city was thus founded in 324, dedicated on 11 May 330 and renamed "Constantinopolis" ("Constantine's City" or Constantinople in English). Special commemorative coins were issued in 330 to honor the event. The new city was protected by the relics of the True Cross, the Rod of Moses and other holy relics, though a cameo now at the Hermitage Museum also represented Constantine crowned by the tyche of the new city. The figures of old gods were either replaced or assimilated into a framework of Christian symbolism. Constantine built the new Church of the Holy Apostles on the site of a temple to Aphrodite. Generations later there was the story that a divine vision led Constantine to this spot, and an angel no one else could see, led him on a circuit of the new walls. The capital would often be compared to the 'old' Rome as "Nova Roma Constantinopolitana", the "New Rome of Constantinople".
Religious policy.
Constantine was the first emperor to stop Christian persecutions and to legalise Christianity along with all other religions and cults in the Roman Empire.
In February 313, Constantine met with Licinius in Milan, where they developed the Edict of Milan. The edict stated that Christians should be allowed to follow the faith without oppression. This removed penalties for professing Christianity, under which many had been martyred previously, and returned confiscated Church property. The edict protected from religious persecution not only Christians but all religions, allowing anyone to worship whichever deity they chose. A similar edict had been issued in 311 by Galerius, then senior emperor of the Tetrarchy; Galerius' edict granted Christians the right to practise their religion but did not restore any property to them. The Edict of Milan included several clauses which stated that all confiscated churches would be returned as well as other provisions for previously persecuted Christians.
Scholars debate whether Constantine adopted his mother St. Helena's Christianity in his youth, or whether he adopted it gradually over the course of his life. Constantine would retain the title of "pontifex maximus" until his death, a title emperors bore as heads of the pagan priesthood, as would his Christian successors on to Gratian ("r". 375–383). According to Christian writers, Constantine was over 40 when he finally declared himself a Christian, writing to Christians to make clear that he believed he owed his successes to the protection of the Christian High God alone. Throughout his rule, Constantine supported the Church financially, built basilicas, granted privileges to clergy (e.g. exemption from certain taxes), promoted Christians to high office, and returned property confiscated during the Diocletianic persecution. His most famous building projects include the Church of the Holy Sepulchre, and Old Saint Peter's Basilica.
However, Constantine certainly did not patronize Christianity alone. After gaining victory in the Battle of the Milvian Bridge (312), a triumphal arch—the Arch of Constantine—was built (315) to celebrate his triumph. The arch is decorated with images of the goddess Victoria. At the time of its dedication, sacrifices to gods like Apollo, Diana, and Hercules were made. Absent from the Arch are any depictions of Christian symbolism. However, as the Arch was commissioned by the Senate, the absence of Christian symbols may reflect the role of the Curia at the time as a pagan redoubt.
Later in 321, Constantine instructed that Christians and non-Christians should be united in observing the venerable day of the sun, or Sunday referring to the sun-worship that Aurelian had established as an official cult. Furthermore, and long after his oft alleged conversion to Christianity, Constantine's coinage continued to carry the symbols of the sun. Even after the pagan gods had disappeared from the coinage, Christian symbols appeared only as Constantine's "personal" attributes: the chi rho between his hands or on his labarum, but never on the coin itself. Even when Constantine dedicated the new capital of Constantinople, which became the seat of Byzantine Christianity for a millennium, he did so wearing the Apollonian sun-rayed Diadem; no Christian symbols were present at this dedication.
The reign of Constantine established a precedent for the position of the emperor as having great influence and ultimate regulatory authority within the religious discussions involving the early Christian councils of that time, e.g., most notably the dispute over Arianism, and the nature of God. Constantine himself disliked the risks to societal stability that religious disputes and controversies brought with them, preferring where possible to establish an orthodoxy. One way in which Constantine used his influence over the early Church councils was to seek to establish a consensus over the oft debated and argued issue over the nature of God.
Most notably, from 313 to 316 bishops in North Africa struggled with other Christian bishops who had been ordained by Donatus in opposition to Caecilian. The African bishops could not come to terms and the Donatists asked Constantine to act as a judge in the dispute. Three regional Church councils and another trial before Constantine all ruled against Donatus and the Donatism movement in North Africa. In 317 Constantine issued an edict to confiscate Donatist church property and to send Donatist clergy into exile. More significantly, in 325 he summoned the Council of Nicaea, effectively the first Ecumenical Council (unless the Council of Jerusalem is so classified). The Council of Nicaea is most known for its dealing with Arianism and for instituting the Nicene Creed.
Constantine enforced the prohibition of the First Council of Nicaea against celebrating the Lord's Supper on the day before the Jewish Passover (14 "Nisan") (see Quartodecimanism and Easter controversy). This marked a definite break of Christianity from the Judaic tradition. From then on the Roman Julian Calendar, a solar calendar, was given precedence over the lunisolar Hebrew Calendar among the Christian churches of the Roman Empire.
Constantine made some new laws regarding the Jews, but while some of his edicts were unfavorable towards Jews, they were not harsher than those of his predecessors. It was made illegal for Jews to seek converts or to attack other Jews who had converted to Christianity. They were forbidden to own Christian slaves or to circumcise their slaves. On the other hand, Jewish clergy were given the same exemptions as Christian clergy.
Administrative reforms.
[[File:0 Constantinus I - Palazzo dei Conservatori (2).JPG|thumb|upright
Beginning in the mid-3rd century the emperors began to favor members of the equestrian order over senators, who had had a monopoly on the most important offices of state. Senators were stripped of the command of legions and most provincial governorships (as it was felt that they lacked the specialized military upbringing needed in an age of acute defense needs), such posts being given to equestrians by Diocletian and his colleagues—following a practice enforced piecemeal by their predecessors. The emperors, however, still needed the talents and the help of the very rich, who were relied on to maintain social order and cohesion by means of a web of powerful influence and contacts at all levels. Exclusion of the old senatorial aristocracy threatened this arrangement.
In 326, Constantine reversed this pro-equestrian trend, raising many administrative positions to senatorial rank and thus opening these offices to the old aristocracy, and at the same time elevating the rank of already existing equestrians office-holders to senator, degrading the equestrian order —at least as a bureaucratic rank —in the process, so that by the end of the 4th century the title of "perfectissimus" was granted only to mid-low officials.
By the new Constantinian arrangement, one could become a senator, either by being elected praetor or (in most cases) by fulfilling a function of senatorial rank: from then on, holding of actual power and social status were melded together into a joint imperial hierarchy. At the same time, Constantine gained with this the support of the old nobility, as the Senate was allowed itself to elect praetors and quaestors, in place of the usual practice of the emperors directly creating new magistrates ("adlectio"). In one inscription in honor of city prefect (336–337) Ceionius Rufus Albinus, it was written that Constantine had restored the Senate "the "auctoritas" it had lost at Caesar's time".
The Senate as a body remained devoid of any significant power; nevertheless, the senators, who had been marginalized as potential holders of imperial functions during the 3rd century, could now dispute such positions alongside more upstart bureaucrats. Some modern historians see in those administrative reforms an attempt by Constantine at reintegrating the senatorial order into the imperial administrative elite to counter the possibility of alienating pagan senators from a Christianized imperial rule; however, such an interpretation remains conjectural, given the fact that we do not have the precise numbers about pre-Constantine conversions to Christianity in the old senatorial milieu—some historians suggesting that early conversions among the old aristocracy were more numerous than previously supposed.
Constantine's reforms had to do only with the civilian administration: the military chiefs, who since the Crisis of the Third Century had risen from the ranks, remained outside the senate, in which they were included only by Constantine's children.
Monetary reforms.
After the runaway inflation of the third century, associated with the production of fiat money to pay for public expenses, Diocletian had tried unsuccessfully to re establish trustworthy minting of silver and billon coins. The failure of the various Diocletianic attempts at the restoration of a functioning silver coin resided in the fact that the silver currency was overvalued in terms of its actual metal content, and therefore could only circulate at much discounted rates. Minting of the Diocletianic "pure" silver "argenteus" ceased, therefore, soon after 305, while the billon currency continued to be used until the 360s. From the early 300s on, Constantine forsook any attempts at restoring the silver currency, preferring instead to concentrate on minting large quantities of good standard gold pieces—the solidus, 72 of which made a pound of gold. New (and highly debased) silver pieces would continue to be issued during Constantine's later reign and after his death, in a continuous process of retariffing, until this bullion minting eventually ceased, "de jure", in 367, with the silver piece being "de facto" continued by various denominations of bronze coins, the most important being the "centenionalis". These bronze pieces continued to be devalued, assuring the possibility of keeping fiduciary minting alongside a gold standard. The anonymous author of the possibly contemporary treatise on military affairs "De Rebus Bellicis" held that, as a consequence of this monetary policy, the rift between classes widened: the rich benefited from the stability in purchasing power of the gold piece, while the poor had to cope with ever-degrading bronze pieces. Later emperors like Julian the Apostate tried to present themselves as advocates of the "humiles" by insisting on trustworthy mintings of the bronze currency.
Constantine's monetary policy were closely associated with his religious ones, in that increased minting was associated with measures of confiscation—taken since 331 and closed in 336—of all gold, silver and bronze statues from pagan temples, who were declared as imperial property and, as such, as monetary assets. Two imperial commissioners for each province had the task of getting hold of the statues and having them melded for immediate minting—with the exception of a number of bronze statues who were used as public monuments for the beautification of the new capital in Constantinople.
Executions of Crispus and Fausta.
On some date between 15 May and 17 June 326, Constantine had his eldest son Crispus, by Minervina, seized and put to death by "cold poison" at Pola (Pula, Croatia). In July, Constantine had his wife, the Empress Fausta, killed at the behest of his mother, Helena. Fausta was left to die in an over-heated bath. Their names were wiped from the face of many inscriptions, references to their lives in the literary record were erased, and the memory of both was condemned. Eusebius, for example, edited praise of Crispus out of later copies of his "Historia Ecclesiastica", and his "Vita Constantini" contains no mention of Fausta or Crispus at all. Few ancient sources are willing to discuss possible motives for the events; those few that do, offer unconvincing rationales, are of later provenance, and are generally unreliable. At the time of the executions, it was commonly believed that the Empress Fausta was either in an illicit relationship with Crispus, or was spreading rumors to that effect. A popular myth arose, modified to allude to Hippolytus–Phaedra legend, with the suggestion that Constantine killed Crispus and Fausta for their immoralities. One source, the largely fictional "Passion of Artemius", probably penned in the eighth century by John of Damascus, makes the legendary connection explicit. As an interpretation of the executions, the myth rests on only "the slimmest of evidence": sources that allude to the relationship between Crispus and Fausta are late and unreliable, and the modern suggestion that Constantine's "godly" edicts of 326 and the irregularities of Crispus are somehow connected rests on no evidence at all.
Although Constantine created his apparent heirs "Caesars", following a pattern established by Diocletian, he gave his creations a hereditary character, alien to the tetrarchic system: Constantine's Caesars were to be kept in the hope of ascending to Empire, and entirely subordinated to their Augustus, as long as he was alive. Therefore, an alternative explanation for the execution of Crispus was, perhaps, Constantine's desire to keep a firm grip on his prospective heirs, this—and Fausta's desire for having her sons inheriting instead of their half-brother—being reason enough for killing Crispus; the subsequent execution of Fausta, however, was probably meant as a reminder to her children that Constantine would not hesitate in "killing his own relatives when he felt this was necessary".
Later campaigns.
Constantine considered Constantinople as his capital and permanent residence. He lived there for a good portion of his later life. He rebuilt Trajan's bridge across the Danube, in hopes of reconquering Dacia, a province that had been abandoned under Aurelian. In the late winter of 332, Constantine campaigned with the Sarmatians against the Goths. The weather and lack of food cost the Goths dearly: reportedly, nearly one hundred thousand died before they submitted to Rome. In 334, after Sarmatian commoners had overthrown their leaders, Constantine led a campaign against the tribe. He won a victory in the war and extended his control over the region, as remains of camps and fortifications in the region indicate. Constantine resettled some Sarmatian exiles as farmers in Illyrian and Roman districts, and conscripted the rest into the army. Constantine took the title "Dacicus maximus" in 336.
In the last years of his life Constantine made plans for a campaign against Persia. In a letter written to the king of Persia, Shapur, Constantine had asserted his patronage over Persia's Christian subjects and urged Shapur to treat them well. The letter is undatable. In response to border raids, Constantine sent Constantius to guard the eastern frontier in 335. In 336, prince Narseh invaded Armenia (a Christian kingdom since 301) and installed a Persian client on the throne. Constantine then resolved to campaign against Persia himself. He treated the war as a Christian crusade, calling for bishops to accompany the army and commissioning a tent in the shape of a church to follow him everywhere. Constantine planned to be baptized in the Jordan River before crossing into Persia. Persian diplomats came to Constantinople over the winter of 336–337, seeking peace, but Constantine turned them away. The campaign was called off, however, when Constantine became sick in the spring of 337.
Sickness and death.
Constantine had known death would soon come. Within the Church of the Holy Apostles, Constantine had secretly prepared a final resting-place for himself. It came sooner than he had expected. Soon after the Feast of Easter 337, Constantine fell seriously ill. He left Constantinople for the hot baths near his mother's city of Helenopolis (Altinova), on the southern shores of the Gulf of İzmit. There, in a church his mother built in honor of Lucian the Apostle, he prayed, and there he realized that he was dying. Seeking purification, he became a catechumen, and attempted a return to Constantinople, making it only as far as a suburb of Nicomedia. He summoned the bishops, and told them of his hope to be baptized in the River Jordan, where Christ was written to have been baptized. He requested the baptism right away, promising to live a more Christian life should he live through his illness. The bishops, Eusebius records, "performed the sacred ceremonies according to custom". He chose the Arianizing bishop Eusebius of Nicomedia, bishop of the city where he lay dying, as his baptizer. In postponing his baptism, he followed one custom at the time which postponed baptism until after infancy. It has been thought that Constantine put off baptism as long as he did so as to be absolved from as much of his sin as possible. Constantine died soon after at a suburban villa called Achyron, on the last day of the fifty-day festival of Pentecost directly following Pascha (or Easter), on 22 May 337.
Although Constantine's death follows the conclusion of the Persian campaign in Eusebius's account, most other sources report his death as occurring in its middle. Emperor Julian (a nephew of Constantine), writing in the mid-350s, observes that the Sassanians escaped punishment for their ill-deeds, because Constantine died "in the middle of his preparations for war". Similar accounts are given in the "Origo Constantini", an anonymous document composed while Constantine was still living, and which has Constantine dying in Nicomedia; the "Historiae abbreviatae" of Sextus Aurelius Victor, written in 361, which has Constantine dying at an estate near Nicomedia called Achyrona while marching against the Persians; and the "Breviarium" of Eutropius, a handbook compiled in 369 for the Emperor Valens, which has Constantine dying in a nameless state villa in Nicomedia. From these and other accounts, some have concluded that Eusebius's "Vita" was edited to defend Constantine's reputation against what Eusebius saw as a less congenial version of the campaign.
Following his death, his body was transferred to Constantinople and buried in the Church of the Holy Apostles there. He was succeeded by his three sons born of Fausta, Constantine II, Constantius II and Constans. A number of relatives were killed by followers of Constantius, notably Constantine's nephews Dalmatius (who held the rank of Caesar) and Hannibalianus, presumably to eliminate possible contenders to an already complicated succession. He also had two daughters, Constantina and Helena, wife of Emperor Julian.
Legacy.
Although he earned his honorific of "The Great" ("Μέγας") from Christian historians long after he had died, he could have claimed the title on his military achievements and victories alone. Besides reuniting the Empire under one emperor, Constantine won major victories over the Franks and Alamanni in 306–308, the Franks again in 313–314, the Goths in 332 and the Sarmatians in 334. By 336, Constantine had reoccupied most of the long-lost province of Dacia, which Aurelian had been forced to abandon in 271. At the time of his death, he was planning a great expedition to end raids on the eastern provinces from the Persian Empire. Serving for a total of almost 31 years (combining his years as co-ruler and sole ruler), he was also the longest serving emperor since Augustus and the second longest serving emperor in Roman history.
In the cultural sphere Constantine contributed to the revival of the clean shaven face fashion of the Roman emperors from Augustus to Trajan, which was originally introduced among the Romans by Scipio Africanus. This new Roman imperial fashion lasted until the reign of Phocas.
The Byzantine Empire considered Constantine its founder and the Holy Roman Empire reckoned him among the venerable figures of its tradition. In the later Byzantine state, it had become a great honor for an emperor to be hailed as a "new Constantine". Ten emperors, including the last emperor of the Eastern Roman Empire, carried the name. Monumental Constantinian forms were used at the court of Charlemagne to suggest that he was Constantine's successor and equal. Constantine acquired a mythic role as a warrior against "heathens". The motif of the Romanesque equestrian, the mounted figure in the posture of a triumphant Roman emperor, became a visual metaphor in statuary in praise of local benefactors. The name "Constantine" itself enjoyed renewed popularity in western France in the eleventh and twelfth centuries. Most Nicene churches consider Constantine a saint (Άγιος Κωνσταντίνος, Saint Constantine). In the Orthodox Church he is called "isapostolos" (Ισαπόστολος Κωνσταντίνος)—an equal of the Apostles.
The Niš Airport is named "Constantine the Great" in honor of him. A large Cross was planned to be built on a hill overlooking Niš, but the project was cancelled. In 2012, a memorial was erected in Niš in his honor. The "Commemoration of the Edict of Milan" was held in Niš in 2013.
Historiography.
During his life and those of his sons, Constantine was presented as a paragon of virtue. Pagans such as Praxagoras of Athens and Libanius showered him with praise. When the last of his sons died in 361, however, his nephew (and son-in-law) Julian the Apostate wrote the satire "Symposium, or the Saturnalia", which denigrated Constantine, calling him inferior to the great pagan emperors, and given over to luxury and greed. Following Julian, Eunapius began—and Zosimus continued—a historiographic tradition that blamed Constantine for weakening the Empire through his indulgence to the Christians.
In medieval times, when the Roman Catholic Church was dominant, Catholic historians presented Constantine as an ideal ruler, the standard against which any king or emperor could be measured. The Renaissance rediscovery of anti-Constantinian sources prompted a re-evaluation of Constantine's career. The German humanist Johann Löwenklau, discoverer of Zosimus' writings, published a Latin translation thereof in 1576. In its preface, he argued that Zosimus' picture of Constantine was superior to that offered by Eusebius and the Church historians, and damned Constantine as a tyrant. Cardinal Caesar Baronius, a man of the Counter-Reformation, criticized Zosimus, favoring Eusebius' account of the Constantinian era. Baronius' "Life of Constantine" (1588) presents Constantine as the model of a Christian prince. For his "History of the Decline and Fall of the Roman Empire" (1776–89), Edward Gibbon, aiming to unite the two extremes of Constantinian scholarship, offered a portrait of Constantine built on the contrasted narratives of Eusebius and Zosimus. In a form that parallels his account of the empire's decline, Gibbon presents a noble war hero corrupted by Christian influences, who transforms into an Oriental despot in his old age: "a hero ... degenerating into a cruel and dissolute monarch".
Modern interpretations of Constantine's rule begin with Jacob Burckhardt's "The Age of Constantine the Great" (1853, rev. 1880). Burckhardt's Constantine is a scheming secularist, a politician who manipulates all parties in a quest to secure his own power. Henri Grégoire, writing in the 1930s, followed Burckhardt's evaluation of Constantine. For Grégoire, Constantine developed an interest in Christianity only after witnessing its political usefulness. Grégoire was skeptical of the authenticity of Eusebius' "Vita", and postulated a pseudo-Eusebius to assume responsibility for the vision and conversion narratives of that work. Otto Seeck, in "Geschichte des Untergangs der antiken Welt" (1920–23), and André Piganiol, in "L'empereur Constantin" (1932), wrote against this historiographic tradition. Seeck presented Constantine as a sincere war hero, whose ambiguities were the product of his own naïve inconsistency. Piganiol's Constantine is a philosophical monotheist, a child of his era's religious syncretism. Related histories by A.H.M. Jones ("Constantine and the Conversion of Europe", 1949) and Ramsay MacMullen ("Constantine", 1969) gave portraits of a less visionary, and more impulsive, Constantine.
These later accounts were more willing to present Constantine as a genuine convert to Christianity. Beginning with Norman H. Baynes' "Constantine the Great and the Christian Church" (1929) and reinforced by Andreas Alföldi's "The Conversion of Constantine and Pagan Rome" (1948), a historiographic tradition developed which presented Constantine as a committed Christian. T. D. Barnes's seminal "Constantine and Eusebius" (1981) represents the culmination of this trend. Barnes' Constantine experienced a radical conversion, which drove him on a personal crusade to convert his empire. Charles Matson Odahl's recent "Constantine and the Christian Empire" (2004) takes much the same tack. In spite of Barnes' work, arguments over the strength and depth of Constantine's religious conversion continue. Certain themes in this school reached new extremes in T.G. Elliott's "The Christianity of Constantine the Great" (1996), which presented Constantine as a committed Christian from early childhood. A similar view of Constantine is held in Paul Veyne's recent (2007) work, "Quand notre monde est devenu chrétien", which does not speculate on the origins of Constantine's Christian motivation, but presents him, in his role as Emperor, as a religious revolutionary who fervently believed himself meant "to play a providential role in the millenary economy of the salvation of humanity".
Donation of Constantine.
Latin Rite Catholics considered it inappropriate that Constantine was baptized only on his death-bed and by an unorthodox bishop, as it undermined the authority of the Papacy. Hence, by the early fourth century, a legend had emerged that Pope Sylvester I (314–335) had cured the pagan emperor from leprosy. According to this legend, Constantine was soon baptized, and began the construction of a church in the Lateran Palace. In the eighth century, most likely during the pontificate of Stephen II (752–757), a document called the Donation of Constantine first appeared, in which the freshly converted Constantine hands the temporal rule over "the city of Rome and all the provinces, districts, and cities of Italy and the Western regions" to Sylvester and his successors. In the High Middle Ages, this document was used and accepted as the basis for the Pope's temporal power, though it was denounced as a forgery by Emperor Otto III and lamented as the root of papal worldliness by the poet Dante Alighieri. The 15th century philologist Lorenzo Valla proved the document was indeed a forgery.
Geoffrey of Monmouth's "Historia".
During the medieval period, Britons regarded Constantine as a king of their own people, particularly associating him with Caernarfon in Gwynedd. While some of this is owed to his fame and his proclamation as Emperor in Britain, there was also confusion of his family with Magnus Maximus's supposed wife Saint Elen and her son, another Constantine . In the 12th century Henry of Huntingdon included a passage in his "Historia Anglorum" that the emperor Constantine's mother was a Briton, making her the daughter of King Cole of Colchester. Geoffrey of Monmouth expanded this story in his highly fictionalized "Historia Regum Britanniae", an account of the supposed Kings of Britain from their Trojan origins to the Anglo-Saxon invasion. According to Geoffrey, Cole was King of the Britons when Constantius, here a senator, came to Britain. Afraid of the Romans, Cole submitted to Roman law so long as he retained his kingship. However, he died only a month later, and Constantius took the throne himself, marrying Cole's daughter Helena. They had their son Constantine, who succeeded his father as King of Britain before becoming Roman Emperor.
Historically, this series of events is extremely improbable. Constantius had already left Helena by the time he left for Britain. Additionally, no earlier source mentions that Helena was born in Britain, let alone that she was a princess. Henry's source for the story is unknown, though it may have been a lost hagiography of Helena.
Documentaries.
Documentaries of Constantine include:PBS-From Jesus To Christ. The First Christians Chapter 12 and PBS Christ to Constantine Episode 6 Constantine
Citations.
Essays from "The Cambridge Companion to the Age of Constantine" are marked with a "(CC)".

</doc>
<doc id="7237" url="https://en.wikipedia.org/wiki?curid=7237" title="Common Language Infrastructure">
Common Language Infrastructure

The Common Language Infrastructure (CLI) is an open specification developed by Microsoft and standardized by ISO and ECMA that describes executable code and a runtime environment that allow multiple high-level languages to be used on different computer platforms without being rewritten for specific architectures. The .NET Framework and the free and open source Mono and Portable.NET are implementations of the CLI.
Overview.
Among other things, the CLI specification describes the following four aspects:
All compatible languages compile to Common Intermediate Language (CIL), which is an intermediate language that is abstracted from the platform hardware. When the code is executed, the platform-specific VES will compile the CIL to the machine language according to the specific hardware and operating system.
Standardization and licensing.
In August 2000, Microsoft, Hewlett-Packard, Intel, and others worked to standardize CLI. By December 2001, it was ratified by the ECMA, with ISO standardization following in April 2003.
Microsoft and its partners hold patents for CLI. ECMA and ISO require that all patents essential to implementation be made available under "reasonable and non-discriminatory (RAND) terms." It is common for RAND licensing to require some royalty payment, which could be a cause for concern with Mono. As of January 2013, neither Microsoft nor its partners have identified any patents essential to CLI implementations subject to RAND terms.
As of July 2009, Microsoft added C# and CLI to the list of specifications that the Microsoft Community Promise applies to, so anyone can safely implement specified editions of the standards without fearing a patent lawsuit from Microsoft. To implement the CLI standard requires conformance to one of the supported and defined profiles of the standard, the minimum of which is the kernel profile. The kernel profile is actually a very small set of types to support in comparison to the well known core library of default .NET installations. However, the conformance clause of the CLI allows for extending the supported profile by adding new methods and types to classes, as well as deriving from new namespaces. But it does not allow for adding new members to interfaces. This means that the features of the CLI can be used and extended, as long as the conforming profile implementation does not change the behavior of a program intended to run on that profile, while allowing for unspecified behavior from programs written specifically for that implementation.
In 2012, ECMA and ISO published the new edition of the CLI standard, which is not covered by the Community Promise.

</doc>
<doc id="7239" url="https://en.wikipedia.org/wiki?curid=7239" title="Cricket World Cup">
Cricket World Cup

The ICC Cricket World Cup is the international championship of One Day International (ODI) cricket. The event is organised by the sport's governing body, the International Cricket Council (ICC), every four years, with preliminary qualification rounds leading up to a finals tournament. The tournament is one of the world's most viewed sporting events and is considered the "flagship event of the international cricket calendar" by the ICC.
The first World Cup was organised in England in June 1975, with the first ODI cricket match having been played only four years earlier. However, a separate Women's Cricket World Cup had been held two years before the first men's tournament, and a tournament involving multiple international teams had been held as early as 1912, when a triangular tournament of Test matches was played between Australia, England and South Africa. Each of the first three World Cups was held in England. From the 1987 tournament onwards, hosting has been shared between countries under an unofficial rotation system, with fourteen ICC members having hosted at least one match in the tournament.
The finals of the World Cup are contested by the ten full members of the ICC (all of which are Test-playing teams) and a number of teams made up from associate and affiliate members of the ICC, selected via the World Cricket League and a later qualifying tournament. A total of twenty teams have competed in the eleven editions of the tournament, with fourteen competing in the 2015 tournament. Australia has won the tournament five times, with the West Indies, India (twice each), Pakistan and Sri Lanka (once each) also having won the tournament. The best performance by a non-full-member team came when Kenya made the semi-finals of the 2003 tournament.
History.
Before the first Cricket World Cup.
The first international cricket match was played between Canada and the United States, on 24 and 25 September 1844. However, the first credited Test match was played in 1877 between Australia and England, and the two teams competed regularly for The Ashes in subsequent years. South Africa was admitted to Test status in 1889. Representative cricket teams were selected to tour each other, resulting in bilateral competition. Cricket was also included as an Olympic sport at the 1900 Paris Games, where Great Britain defeated France to win the gold medal. This was the only appearance of cricket at the Summer Olympics.
The first multilateral competition at international level was the 1912 Triangular Tournament, a Test cricket tournament played in England between all three Test-playing nations at the time: England, Australia and South Africa. The event was not a success: the summer was exceptionally wet, making play difficult on damp uncovered pitches, and attendances were poor, attributed to a "surfeit of cricket". Since then, international Test cricket has generally been organised as bilateral series: a multilateral Test tournament was not organised again until the triangular Asian Test Championship in 1999.
The number of nations playing Test cricket increased gradually over time, with the addition of West Indies in 1928, New Zealand in 1930, India in 1932, and Pakistan in 1952. However, international cricket continued to be played as bilateral Test matches over three, four or five days.
In the early 1960s, English county cricket teams began playing a shortened version of cricket which only lasted for one day. Starting in 1962 with a four-team knockout competition known as the Midlands Knock-Out Cup, and continuing with the inaugural Gillette Cup in 1963, one-day cricket grew in popularity in England. A national Sunday League was formed in 1969. The first One-Day International match was played on the fifth day of a rain-aborted Test match between England and Australia at Melbourne in 1971, to fill the time available and as compensation for the frustrated crowd. It was a forty over game with eight balls per over.
In the late 1970s, Kerry Packer established the rival World Series Cricket (WSC) competition. It introduced many of the now commonplace features of One Day International cricket, including coloured uniforms, matches played at night under floodlights with a white ball and dark sight screens, and, for television broadcasts, multiple camera angles, effects microphones to capture sounds from the players on the pitch, and on-screen graphics. The first of the matches with coloured uniforms was the WSC Australians in wattle gold versus WSC West Indians in coral pink, played at VFL Park in Melbourne on 17 January 1979. The success and popularity of the domestic one-day competitions in England and other parts of the world, as well as the early One-Day Internationals, prompted the ICC to consider organising a Cricket World Cup.
Prudential World Cups (1975–1983).
The inaugural Cricket World Cup was hosted in 1975 by England, the only nation able to put forward the resources to stage an event of such magnitude at the time. The 1975 tournament started on 7 June. The first three events were held in England and officially known as the Prudential Cup after the sponsors Prudential plc. The matches consisted of 60 six-ball overs per team, played during the daytime in traditional form, with the players wearing cricket whites and using red cricket balls.
Eight teams participated in the first tournament: Australia, England, India, New Zealand, Pakistan, and the West Indies (the six Test nations at the time), together with Sri Lanka and a composite team from East Africa. One notable omission was South Africa, who were banned from international cricket due to apartheid. The tournament was won by the West Indies, who defeated Australia by 17 runs in the final at Lord's.
The 1979 World Cup saw the introduction of the ICC Trophy competition to select non-Test playing teams for the World Cup, with Sri Lanka and Canada qualifying. The West Indies won a second consecutive World Cup tournament, defeating the hosts England by 92 runs in the final. At a meeting which followed the World Cup, the International Cricket Conference agreed to make the competition a quadrennial event.
The 1983 event was hosted by England for a third consecutive time. By this stage, Sri Lanka had become a Test-playing nation, and Zimbabwe qualified through the ICC Trophy. A fielding circle was introduced, away from the stumps. Four fieldsmen needed to be inside it at all times. The teams faced each other twice, before moving into the knock-outs. India, an outsider, quoted at 66–1 to win by bookmakers before the competition began, were crowned champions after upsetting the West Indies by 43 runs in the final.
Different champions (1987–1996).
India and Pakistan jointly hosted the 1987 tournament, the first time that the competition was held outside England. The games were reduced from 60 to 50 overs per innings, the current standard, because of the shorter daylight hours in the Indian subcontinent compared with England's summer. Australia won the championship by defeating England by 7 runs in the final, the closest margin in World Cup final history.
The 1992 World Cup, held in Australia and New Zealand, introduced many changes to the game, such as coloured clothing, white balls, day/night matches, and a change to the fielding restriction rules. The South African cricket team participated in the event for the first time, following the fall of the apartheid regime and the end of the international sports boycott. Pakistan overcame a dismal start in the tournament to eventually defeat England by 22 runs in the final and emerge as winners.
The 1996 championship was held in the Indian subcontinent for a second time, with the inclusion of Sri Lanka as host for some of its group stage matches. In the semi-final, Sri Lanka, heading towards a crushing victory over India at Eden Gardens after the hosts lost eight wickets while scoring 120 runs in pursuit of 252, were awarded victory by default after crowd unrest broke out in protest against the Indian performance. Sri Lanka went on to win their maiden championship by defeating Australia by seven wickets in the final at Lahore.
Australian treble (1999–2007).
In 1999 the event was hosted by England, with some matches also being held in Scotland, Ireland, Wales and the Netherlands. Twelve teams contested the World Cup. Australia qualified for the semi-finals after reaching their target in their Super 6 match against South Africa off the final over of the match. They then proceeded to the final with a tied match in the semi-final also against South Africa where a mix-up between South African batsmen Lance Klusener and Allan Donald saw Donald drop his bat and stranded mid-pitch to be run out. In the final, Australia dismissed Pakistan for 132 and then reached the target in less than 20 overs and with eight wickets in hand.
South Africa, Zimbabwe and Kenya hosted the 2003 World Cup. The number of teams participating in the event increased from twelve to fourteen. Kenya's victories over Sri Lanka and Zimbabwe, among others – and a forfeit by the New Zealand team, which refused to play in Kenya because of security concerns – enabled Kenya to reach the semi-finals, the best result by an associate. In the final, Australia made 359 runs for the loss of two wickets, the largest ever total in a final, defeating India by 125 runs.
In 2007 the tournament was hosted by the West Indies and expanded to sixteen teams. Following Pakistan's upset loss to World Cup debutants Ireland in the group stage, Pakistani coach Bob Woolmer was found dead in his hotel room. Jamaican police had initially launched a murder investigation into Woolmer's death but later confirmed that he died of heart failure. Australia defeated Sri Lanka in the final by 53 runs (D/L) in farcical light conditions, and extended their undefeated run in the World Cup to 29 matches and winning three straight championships.
Hosts triumph (2011-2015).
India, Sri Lanka and Bangladesh together hosted the 2011 Cricket World Cup. Pakistan were stripped of their hosting rights following the terrorist attack on the Sri Lankan cricket team, with the games originally scheduled for Pakistan redistributed to the other host countries. The number of teams participating in the World Cup dropped down to fourteen. Australia lost their final group stage match against Pakistan on 19 March 2011, ending an unbeaten streak of 35 World Cup matches, which had begun on 23 May 1999. India won their second World Cup title by beating Sri Lanka by 6 wickets in the final in Mumbai, and became the first country to win the final on home soil.
Australia and New Zealand jointly hosted the 2015 Cricket World Cup. The number of participants remained at fourteen. Ireland was the most successful Associate nation with a total of three wins in the tournament. New Zealand beat South Africa in a thrilling first semi-final to qualify for their maiden World Cup final. Australia defeated New Zealand by seven wickets in the final at Melbourne to lift the World Cup for the fifth time.With this Australia became the first country to win the world cup on every continent held in the world cup
Format.
Qualification.
The Test-playing nations qualify automatically for the World Cup main event while the other teams have to qualify through a series of preliminary qualifying tournaments. A new qualifying format was introduced for the 2015 Cricket World Cup. The top two teams of the 2011–13 ICC World Cricket League Championship qualify directly. The remaining six teams join the third and fourth-placed teams of 2011 ICC World Cricket League Division Two and the top two teams of the 2013 ICC World Cricket League Division Three in the World Cup Qualifier to decide the remaining two places.
Qualifying tournaments were introduced for the second World Cup, where two of the eight places in the finals were awarded to the leading teams in the ICC Trophy. The number of teams selected through the ICC Trophy had varied throughout the years. The World Cricket League (administered by the International Cricket Council) is the qualification system provided to allow the Associate and Affiliate members of the ICC more opportunities to qualify. The name "ICC Trophy" has been changed to "ICC World Cup Qualifier".
Under the current qualifying process, the World Cricket League, all Associate and Affiliate members of the ICC are able to qualify for the World Cup. Associate and Affiliate members must play between two and five stages in the ICC World Cricket League to qualify for the World Cup finals, depending on the Division in which they start the qualifying process.
Process summary in chronological order (2011-2014):
Tournament.
The format of the Cricket World Cup has changed greatly over the course of its history. Each of the first four tournaments was played by eight teams, divided into two groups of four. The competition consisted of two stages, a group stage and a knock-out stage. The four teams in each group played each other in the round-robin group stage, with the top two teams in each group progressing to the semi-finals. The winners of the semi-finals played against each other in the final. With South Africa returning in the fifth tournament in 1992 as a result of the end of the apartheid boycott, nine teams played each other once in the group phase, and the top four teams progressed to the semi-finals. The tournament was further expanded in 1996, with two groups of six teams. The top four teams from each group progressed to quarter-finals and semi-finals.
A distinct format was used for the 1999 and 2003 World Cups. The teams were split into two pools, with the top three teams in each pool advancing to the "Super 6". The "Super 6" teams played the three other teams that advanced from the other group. As they advanced, the teams carried their points forward from previous matches against other teams advancing alongside them, giving them an incentive to perform well in the group stages. The top four teams from the "Super 6" stage progressed to the semi-finals, with the winners playing in the final.
The format used in the 2007 World Cup involved 16 teams allocated into four groups of four. Within each group, the teams played each other in a round-robin format. Teams earned points for wins and half-points for ties. The top two teams from each group moved forward to the "Super 8" round. The "Super 8" teams played the other six teams that progressed from the different groups. Teams earned points in the same way as the group stage, but carried their points forward from previous matches against the other teams who qualified from the same group to the "Super 8" stage. The top four teams from the "Super 8" round advanced to the semi-finals, and the winners of the semi-finals played in the final.
The format used in the 2011 and 2015 World Cups featured two groups of seven teams, each playing in a round-robin format. The top four teams from each group proceeded to the knock out stage consisting of quarter-finals, semi-finals and ultimately the final.
Trophy.
The ICC Cricket World Cup Trophy is presented to the winners of the World Cup. The current trophy was created for the 1999 championships, and was the first permanent prize in the tournament's history. Prior to this, different trophies were made for each World Cup. The trophy was designed and produced in London by a team of craftsmen from Garrard & Co over a period of two months.
The current trophy is made from silver and gild, and features a golden globe held up by three silver columns. The columns, shaped as stumps and bails, represent the three fundamental aspects of cricket: batting, bowling and fielding, while the globe characterises a cricket ball. The seam is tilted to symbolize the axial tilt of the Earth. It stands 60 centimetres high and weighs approximately 11 kilograms. The names of the previous winners are engraved on the base of the trophy, with space for a total of twenty inscriptions. The ICC keeps the original trophy. A replica differing only in the inscriptions is permanently awarded to the winning team.
Media coverage.
The tournament is the world's third largest with only the FIFA World Cup and the Summer Olympics exceeding it. The 2011 Cricket World Cup final was televised in over 200 countries to over 2.2 billion television viewers. Television rights, mainly for the 2011 and 2015 World Cup, were sold for over US$1.1 billion, and sponsorship rights were sold for a further US$500 million. The 2003 Cricket World Cup matches were attended by 626,845 people, while the 2007 Cricket World Cup sold more than 672,000 tickets.
Successive World Cup tournaments have generated increasing media attention as One-Day International cricket has become more established. The 2003 World Cup in South Africa was the first to sport a mascot, Dazzler the zebra. An orange mongoose known as "Mello" was the mascot for the 2007 Cricket World Cup. Stumpy, a blue elephant was the mascot for the 2011 World Cup.
On 13 February, the opening of the 2015 tournament was celebrated with a Google Doodle.
Selection of hosts.
The International Cricket Council's executive committee votes for the hosts of the tournament after examining the bids made by the nations keen to hold a Cricket World Cup.
England hosted the first three competitions. The ICC decided that England should host the first tournament because it was ready to devote the resources required to organising the inaugural event. India volunteered to host the third Cricket World Cup, but most ICC members preferred England as the longer period of daylight in England in June meant that a match could be completed in one day. The 1987 Cricket World Cup was held in India and Pakistan, the first hosted outside England.
Many of the tournaments have been jointly hosted by nations from the same geographical region, such as South Asia in 1987, 1996 and 2011, Australasia in 1992 and 2015, Southern Africa in 2003 and West Indies in 2007.
Results.
Twenty nations have qualified for the Cricket World Cup at least once (excluding qualification tournaments). Seven teams have competed in every finals tournament, five of which have won the title. The West Indies won the first two tournaments, Australia has won five, India has won two, while Pakistan and Sri Lanka have each won once. The West Indies (1975 and 1979) and Australia (1999, 2003 and 2007) are the only nations to have won consecutive titles. Australia has played in seven of the eleven final matches (1975, 1987, 1996, 1999, 2003, 2007, 2015). England has yet to win the World Cup, but has been runners-up three times (1979, 1987, 1992). The best result by a non-Test playing nation is the semi-final appearance by Kenya in the 2003 tournament; while the best result by a non-Test playing team on their debut is the Super 8 (second round) by Ireland in 2007.
Sri Lanka as a co-host of the 1996 Cricket World Cup was the first host to win the tournament though the final was held in Pakistan. India won in 2011 as host and was the first team to win in a final played in their own country. Australia repeated the feat in 2015. England is the only other host to have made the final, in 1979. Other countries which have achieved or equalled their best World Cup results while co-hosting the tournament are New Zealand as finalists in 2015; Zimbabwe who reached the Super Six in 2003; and Kenya as semi-finalists in 2003. In 1987, co-hosts India and Pakistan both reached the semi-finals, but were eliminated by Australia and England respectively. Australia in 1992, England in 1999, South Africa in 2003, and Bangladesh in 2011 have been the host teams that were eliminated in the first round.
Teams' performances.
An overview of the teams' performances in every World Cup:
†No longer exists.
Before the 1992 World Cup, South Africa was banned due to apartheid.
The number of wins followed by Run-rate is the criteria for determining the rankings till the 1987 World Cup.
The number of points followed by, head to head performance and then net run-rate is the criteria for determining the rankings for the World Cups from 1992 onwards.
Legend
Debutant teams.
†No longer exists.
Overview.
The table below provides an overview of the performances of teams over past World Cups, as of the end of group stage of the 2015 tournament. Teams are sorted by best performance, then by appearances, total number of wins, total number of games, and alphabetical order respectively.
†No longer exists.
Awards.
Man of the tournament.
Since 1992, one player has been declared as "Man of the Tournament" at the end of the World Cup finals:
Man of the Match in the Final.
There were no Man of the Tournament awards before 1992 but Man of the Match awards have always been given for individual matches. Winning the Man of the Match in the final is logically noteworthy, as this indicates the player deemed to have played the biggest part in the World Cup final. To date the award has always gone to a member of the winning side. The Man of the Match award in the final of the competition has been awarded to:

</doc>
<doc id="7241" url="https://en.wikipedia.org/wiki?curid=7241" title="Commonwealth Heads of Government Meeting">
Commonwealth Heads of Government Meeting

The Commonwealth Heads of Government Meeting (CHOGM; or) is a biennial summit meeting of the heads of government from all Commonwealth nations. Every two years the meeting is held in a different member state and is chaired by that nation's respective Prime Minister or President who becomes the Commonwealth Chair-in-Office until the next meeting. Queen Elizabeth II, who is the Head of the Commonwealth, attended every CHOGM beginning with Ottawa in 1973 until Perth in 2011, although her formal participation only began in 1997. However, she was represented by the Prince of Wales at the 2013 meeting as the 87-year-old monarch was curtailing her overseas travel. The Queen continues to attend CHOGMs held in Europe and was present at the 2015 summit in Malta and is expected to attend the 2018 CHOGM which is to be held in London.
The first CHOGM was held in 1971, and there have been 24 held in total: the most recent was held in Valletta, Malta. They are held once every two years, although this pattern has twice been interrupted. They are held around the Commonwealth, rotating by invitation amongst its members.
In the past, CHOGMs have attempted to orchestrate common policies on certain contentious issues and current events, with a special focus on issues affecting member nations. CHOGMs have discussed the continuation of apartheid rule in South Africa and how to end it, military coups in Pakistan and Fiji, and allegations of electoral fraud in Zimbabwe. Sometimes the member states agree on a common idea or solution, and release a joint statement declaring their opinion. More recently, beginning at the 1997 CHOGM, the meeting has had an official 'theme', set by the host nation, on which the primary discussions have been focused.
History.
The meetings originated with the leaders of the self-governing colonies of the British Empire. The First Colonial Conference in 1887 was followed by periodic meetings, known as Imperial Conferences from 1907, of government leaders of the Empire. The development of the independence of the dominions, and the creation of a number of new dominions, as well as the invitation of Southern Rhodesia (which also attended as a "sui generis" colony), changed the nature of the meetings. As the dominion leaders asserted themselves more and more at the meetings, it became clear that the time for 'imperial' conferences was over.
From the ashes of the Second World War, seventeen Commonwealth Prime Ministers' Conferences were held between 1944 and 1969. Of these, sixteen were held in London, reflecting then-prevailing views of the Commonwealth as the continuation of the Empire and the centralisation of power in the British Commonwealth Office (the one meeting outside London, in Lagos, was an extraordinary meeting held in January 1966 to co-ordinate policies towards Rhodesia). Two supplementary meetings were also held during this period: a Commonwealth Statesmen's meeting to discuss peace terms in April 1945, and a Commonwealth Economic Conference in 1952.
The 1960s saw an overhaul of the Commonwealth. The swift expansion of the Commonwealth after decolonisation saw the newly independent countries demand the creation of the Commonwealth Secretariat, and the United Kingdom, in response, successfully founding the Commonwealth Foundation. This decentralisation of power demanded a reformulation of the meetings. Instead of the meetings always being held in London, they would rotate across the membership, subject to countries' ability to host the meetings: beginning with Singapore in 1971. They were also renamed the 'Commonwealth Heads of Government Meetings' to reflect the growing diversity of the constitutional structures in the Commonwealth.
Structure.
The core of the CHOGM are the executive sessions, which are the formal gatherings of the heads of government to do business. However, the majority of the important decisions are held not in the main meetings themselves, but at the informal 'retreats': introduced at the second CHOGM, in Ottawa, by Prime Minister of Canada Pierre Trudeau, but reminiscent of the excursions to Chequers or Dorneywood in the days of the Prime Ministers' Conferences. The rules are very strict: allowing the head of the delegation, his or her spouse, and one other person. The additional member can be of any capacity (personal, political, security, etc.), but he or she has only occasional and intermittent access to the head. It is usually at the retreat where, isolated from their advisers, the heads resolve the most intransigent issues: leading to the Gleneagles Agreement in 1977, the Lusaka Declaration in 1979, the Langkawi Declaration in 1989, the Millbrook Programme in 1995, the Aso Rock Declaration in 2003, and the Colombo Declaration on Sustainable, Inclusive and Equitable Development in 2013.
The 'fringe' of civil society organisations, including the Commonwealth Family and local groups, adds a cultural dimension to the event, and brings the CHOGM a higher media profile and greater acceptance by the local population. First officially recognised at Limassol in 1993, these events, spanning a longer period than the meeting itself, have, to an extent, preserved the length of the CHOGM: but only in the cultural sphere. Other meetings, such as those of the Commonwealth Ministerial Action Group, Commonwealth Business Council, and respective foreign ministers, have also dealt with business away from the heads of government themselves.
As the scope of the CHOGM has expanded beyond the meetings of the heads of governments themselves, the CHOGMs have become progressively shorter, and their business compacted into less time. The 1971 CHOGM lasted for nine days, and the 1977 and 1991 CHOGMs for seven days each. However, Harare's epochal CHOGM was the last to last a week; the 1993 CHOGM lasted for five days, and the contentious 1995 CHOGM for only three-and-a-half. The 2005 and subsequent conferences were held over two-and-a-half days.
Issues.
During the 1980s, CHOGMs were dominated by calls for the Commonwealth to impose sanctions on South Africa to pressure the country to end apartheid. The division between Britain, during the government of Margaret Thatcher which resisted the call for sanctions and African Commonwealth countries, and the rest of the Commonwealth was intense at times and led to speculation that the organisation might collapse. According to one of Margaret Thatcher's former aides, Mrs. Thatcher, very privately, used to say that CHOGM stood for "Compulsory Handouts to Greedy Mendicants."
In 2011, British Prime Minister David Cameron informed the British House of Commons that his proposals to reform the rules governing royal succession, a change which would require the approval of all sixteen Commonwealth realms, was approved at the 28–30 October CHOGM in Perth, subsequently referred to as the Perth Agreement.
Agenda.
Under the Millbrook Commonwealth Action Programme, each CHOGM is responsible for renewing the remit of the Commonwealth Ministerial Action Group, whose responsibility it is to uphold the Harare Declaration on the core political principles of the Commonwealth.
Incidents.
A bomb exploded at the Sydney Hilton Hotel, the venue for the February 1978 Commonwealth Heads of Government Regional Meeting. Twelve foreign heads of government were staying in the hotel at the time. Most delegates were evacuated by Royal Australian Air Force helicopters and the meeting was moved to Bowral, protected by 800 soldiers of the Australian Army.
As the convocation of heads of governments and permanent Commonwealth staff and experts, CHOGMs are the highest institution of action in the Commonwealth, and rare occasions on which Commonwealth leaders all come together. CHOGMs have been the venues of many of the Commonwealth's most dramatic events. Robert Mugabe announced Zimbabwe's immediate withdrawal from the Commonwealth at the 2003 CHOGM, and Nigeria's execution of Ken Saro-Wiwa and eight others on the first day of the 1995 CHOGM led to that country's suspension.
It has also been the trigger of a number of events that have shaken participating countries domestically. The departure of Uganda's President Milton Obote to the 1971 CHOGM allowed Idi Amin to overthrow Obote's government. Similarly, President James Mancham's attendance of the 1977 CHOGM gave Prime Minister France-Albert René the opportunity to seize power in the Seychelles.
List of meetings.
The 25th CHOGM was originally scheduled for Vanuatu in 2017 but the country rescinded its offer to host after Cyclone Pam devastated the country's infrastructure in March 2015. The meeting was rescheduled for the United Kingdom in the spring of 2018 which also resulted in the 26th CHOGM, originally scheduled for Malaysia in 2019, to be reschuduled for 2020.

</doc>
<doc id="7242" url="https://en.wikipedia.org/wiki?curid=7242" title="Chinese classics">
Chinese classics

Chinese classic texts or canonical texts () refers to the Chinese texts which originated before the imperial unification by the Qin dynasty in 221 BC, particularly the "Four Books and Five Classics" of the Neo-Confucian tradition, themselves a customary abridgment of the "Thirteen Classics". All of these pre-Qin texts were written in classical Chinese. All three canons are collectively known as the classics (t , s , "jīng", lit. "warp").
Chinese classic texts may more broadly refer to texts written either in vernacular Chinese or in the classical Chinese that was current until the fall of the last imperial dynasty, the Qing, in 1912. These can include "shi" (, historical works), "zi" (, philosophical works belonging to schools of thought other than the Confucian but also including works on agriculture, medicine, mathematics, astronomy, divination, art criticism, and other miscellaneous writings) and "ji" (, literary works) as well as "jing (Chinese medicine)".
In the Ming and Qing dynasties, the Four Books and Five Classics were the subject of mandatory study by those Confucian scholars who wished to take the imperial exams to become government officials. Any political discussion was full of references to this background, and one could not be one of the literati (or, in some periods, even a military officer) without having memorized them. Generally, children first memorized the Chinese characters of the "Three Character Classic" and the "Hundred Family Surnames" and then went on to memorize the other classics. The literate elite therefore shared a common culture and set of values.
Scholarship on these texts naturally divides itself into two periods, before and after the burning of the books during the fall of the Qin dynasty, when many of the original pre-Qin texts were lost.

</doc>
<doc id="7243" url="https://en.wikipedia.org/wiki?curid=7243" title="Call centre">
Call centre

A call centre or call center is a centralised office used for receiving or transmitting a large volume of requests by telephone. An inbound call centre is operated by a company to administer incoming product support or information inquiries from consumers. Outbound call centers are operated for telemarketing, solicitation of charitable or political donations, debt collection and market research. A contact centre is a location for centralised handling of individual communications, including letters, faxes, live support software, social media, instant message, and e-mail.
A call centre has an open workspace for call centre agents, with work stations that include a computer for each agent, a telephone set/headset connected to a telecom switch, and one or more supervisor stations. It can be independently operated or networked with additional centres, often linked to a corporate computer network, including mainframes, microcomputers and LANs. Increasingly, the voice and data pathways into the centre are linked through a set of new technologies called computer telephony integration.
The contact centre is a central point from which all customer contacts are managed. Through contact centres, valuable information about company are routed to appropriate people, contacts to be tracked and data to be gathered. It is generally a part of company’s customer relationship management.
A contact centre can be defined as a coordinated system of people, processes, technologies and strategies that provides access to information, resources, and expertise, through appropriate channels of communication, enabling interactions that create value for the customer and organization. Contact centres, along with call centres and communication centres all fall under a larger umbrella labelled as the contact centre management industry. This is becoming a rapidly growing recruitment sector in itself, as the capabilities of contact centres expand and thus require ever more complex systems and highly skilled operational and management staff.
The majority of large companies use contact centres as a means of managing their customer interaction. These centres can be operated by either an in house department responsible or outsourcing customer interaction to a third party agency (known as Outsourcing Call Centres).
History.
The earliest call centers were created during the 1960s, and were known as "Private Automated Business Exchanges" (PABX). The earliest example of a call centre was in the UK is at the Birmingham Press and Mail. They had a GEC PABX 4 ACD, installed in 1965.
The coining of the term "call center" is more recent, with the first published use of the term in 1983.
Technology.
Call centre technologies include speech recognition software to allow computers to handle first level of customer support, text mining and natural language processing to allow better customer handling, agent training by automatic mining of best practices from past interactions, support automation and many other technologies to improve agent productivity and customer satisfaction. Automatic lead selection or lead steering is also intended to improve efficiencies, both for inbound and outbound campaigns. This allows inbound calls to be directly routed to the appropriate agent for the task, whilst minimizing wait times and long lists of irrelevant options for people calling in. For outbound calls, lead selection allows management to designate what type of leads go to which agent based on factors including skill, socioeconomic factors and past performance and percentage likelihood of closing a sale per lead.
The universal queue standardizes the processing of communications across multiple technologies such as fax, phone, and email. The virtual queue provides callers with an alternative to waiting on hold when no agents are available to handle inbound call demand.
Premises-based technology.
Historically, call centres have been built on Private branch exchange (PBX) equipment that is owned, hosted, and maintained by the call centre operator themselves. The PBX can provide functions such as automatic call distribution, interactive voice response, and skills-based routing.
Virtual call centre.
In virtual call centre model, the call centre's operator pays a monthly or annual fee to a vendor that hosts the call centre telephony equipment in their own data centre. In this model, the operator does not own, operate or host the equipment that the call centre runs on. Agents connect to the vendor's equipment through traditional PSTN telephone lines, or over voice over IP. Calls to and from prospects or contacts originate from or terminate at the vendor's data centre, rather than at the call centre operator's premises. The vendor's telephony equipment then connects the calls to the call centre operator's agents.
Virtual call centre technology allows people to work from home, instead of in a traditional, centralised, call centre location, which increasingly allows people with physical or other disabilities that prevent them from leaving the house, to work. The only required equipment is Internet access and a workstation. The companies are preferring Virtual Call Centre services due to cost advantage. Companies can start their call centre business immediately without installing the basic infrastructure like Dialer, ACD and IVRS.
Cloud computing.
Cloud computing for call centres extends cloud computing to software as a service, or hosted, on-demand call centres by providing application programming interfaces (APIs) on the call centre cloud computing platform that allow call centre functionality to be integrated with cloud-based customer relationship management, leads management, and other applications. Computer telephony integration APIs provide developers with access to basic telephony controls and sophisticated call handling on the call centre platform from a separate application. Configuration APIs provide programmatic control of administrative functions of the call centre platform which are typically accessed by a human administrator through a graphical user interface.
Description.
Services.
Contact centers run support or help desks, which regularly answers technical questions from customers and assists them using their equipment or software. Support desks are used by companies in the computing, telecommunications and consumer electronics industries.
Customer service contact centres answer specific queries relating to customer issues, in the banking and utility sectors these are frequently used to answer customer questions relating to their account or payments, this type of service may even be used to respond to customer complaints and undertake retention strategies for unsatisfied customers.
Contact centres also carry out sales and marketing activities; these can be performed through cold calling strategies and increasingly through live chat applications on company websites.
Dynamics.
A contact centre supports interaction with customers over a variety of media, including telephony, e-mail, and internet chat. A telephone answering service is a more personalised version of the call centre, where agents get to know more about their customers and their callers; and therefore look after calls just as if based in their customers' office.
Calls may be "inbound" or "outbound". Inbound calls are made by consumers, for example to obtain information, report a malfunction, or ask for help. In contrast, outbound calls are made by agents to consumers, usually for sales purposes (telemarketing). A "blended" center combines both inbound and outbound campaigns where each type of agent (inbound or outbound) can handle the overflow of the other.
Call centre staff are often organised into a multi-tier support system for more efficient handling of calls. The first tier consists of operators, who initially answer calls and provide general information. If a caller requires more assistance, the call is forwarded to the second tier (in the appropriate department depending on the nature of the call). In some cases, there are three or more tiers of support staff. Typically the third tier of support is formed of product engineers/developers or highly skilled technical support staff for the product.
Outsourcing.
In contrast to in house management, outsourced bureau contact centres are a model of contact centre that provide services on a "pay per use" model. The overheads of the contact centre are shared by many clients, thereby supporting a very cost effective model, especially for low volumes of calls. Outsourced centers have grown in popularity. There is criticism of the outsourcing model.
Companies that regularly utilise outsourced contact centre services include British Sky Broadcasting and Orange in the telecommunications industry, Adidas in the sports and leisure sector, Audi in car manufacturing and charities such as the RSPCA.
Outsourced call centers are often located in the developing countries, where wages are significantly lower. The call center industry in the Philippines and call center industry in the Bangladesh serve as good examples.
Call Centers in Healthcare.
The healthcare industry has used outbound call center programs for years to help manage billing, collections, and patient communication. The inbound call center is a new and increasingly popular service for many types of healthcare facilities, including large hospitals. Inbound call centers can be outsourced or managed in-house. MountainStar Healthcare is one of the first companies to build an inbound call center in-house. American Health Connection is an American healthcare call center corporation with a focus on inbound call centers that utilize outsourcing.
These healthcare call centers are designed to help streamline communications, enhance patient retention and satisfaction, reduce expenses and improve operational efficiencies.
Evaluation.
Mathematical theory.
Queueing theory is a branch of mathematics in which models of service systems have been developed. A call centre can be seen as a queueing network and results from queueing theory such as the probability an arriving customer needs to wait before starting service useful for provisioning capacity. (Erlang's C formula is such a result for an M/M/c queue and approximations exist for an M/G/k queue.) Statistical analysis of call centre data has suggested arrivals are governed by an inhomogeneous Poisson process and jobs have a log-normal service time distribution.
Call centre operations have been supported by mathematical models beyond queueing, with operations research, which considers a wide range of optimisation problems seeking to reduce waiting times while keeping server utilisation and therefore efficiency high.
Metrics.
Some vital call centre performance metrics are listed below:
Criticism and performance.
Some critics of call centres argue that the work atmosphere in such an environment is dehumanising. Others point to the low rates of pay and restrictive working practices of some employers. There has been much controversy over such things as restricting the amount of time that an employee can spend in the toilet. Call centres have also been the subject of complaints by callers who find the staff often do not have enough skill or authority to resolve problems, while the staff sometimes appear apathetic. Other research illustrates how call center workers develop ways to counter or resist this environment by integrating local cultural sensibilities or embracing a vision of a new life.
Telephone calls are easily monitored, and the close monitoring of call centre staff is widespread. This has the benefit of helping the company to plan the workload and time of its employees. However, it has also been argued that such close monitoring breaches the human right to privacy. Most call centres provide electronic reports that outline performance metrics, quarterly highlights and other information about the calls made and received.
Criticisms of call centres generally follow a number of common themes, from both callers and call centre staff. From callers, common criticisms include:
Common criticisms from staff include:
The net-net of these concerns is that call centres as a business process exhibit levels of variability. The experience a customer gets and the results a company achieves on a given call are almost totally dependent on the quality of the agent answering that call. Call centres are beginning to address this by using agent-assisted automation to standardise the process all agents use. Anton and Phelps have provided a detailed manual on how to conduct the performance evaluation of the business, whereas others are using various scientific technologies to do the jobs. However, more popular alternatives are using personality and skill based approaches. The various challenges encountered by call operators are discussed by several authors.
Unionization.
Unions in North America have made some effort to gain members from this sector, including the Communications Workers of America and the United Steelworkers. In Australia, the National Union of Workers represents unionised workers; their activities form part of the Australian labour movement. In Europe, Uni Global Union of Switzerland is involved in assisting unionisation in this realm and in Germany Vereinte Dienstleistungsgewerkschaft represents call centre workers.
Media portrayals.
Indian call centres have been the focus of several documentary films, the 2004 film "Thomas L. Friedman Reporting: The Other Side of Outsourcing", the 2005 films "John and Jane", "Nalini by Day, Nancy by Night", and "1-800-India: Importing a White-Collar Economy", and the 2006 film "Bombay Calling", among others. An Indian call centre is also the subject of the 2006 film "Outsourced (film)" and a key location in the 2008 film, "Slumdog Millionaire". The 2014 BBC fly on the wall documentary series "The Call Centre" gave an often distorted although humorous view of life in a Welsh call centre.

</doc>
<doc id="7246" url="https://en.wikipedia.org/wiki?curid=7246" title="Charles Messier">
Charles Messier

Charles Messier (; 26 June 1730 – 12 April 1817) was a French astronomer most notable for publishing an astronomical catalogue consisting of nebulae and star clusters that came to be known as the 110 "Messier objects". The purpose of the catalogue was to help astronomical observers, in particular comet hunters such as himself, distinguish between permanent and transient visually diffuse objects in the sky.
Biography.
Messier was born in Badonviller in the Lorraine region of France, being the tenth of twelve children of Françoise B. Grandblaise and Nicolas Messier, a Court usher. Six of his brothers and sisters died while young and in 1741, his father died. Charles' interest in astronomy was stimulated by the appearance of the spectacular, great six-tailed comet in 1744 and by an annular solar eclipse visible from his hometown on 25 July 1748.
In 1751 he entered the employ of Joseph Nicolas Delisle, the astronomer of the French Navy, who instructed him to keep careful records of his observations. Messier's first documented observation was that of the Mercury transit of 6 May 1753.
In 1764, he was made a fellow of the Royal Society, in 1769, he was elected a foreign member of the Royal Swedish Academy of Sciences, and on 30 June 1770, he was elected to the French Academy of Sciences.
Messier discovered 13 comets :
Messier is buried in Père Lachaise Cemetery, Paris, in Section 11. The grave is fairly plain and faintly inscribed, and while it is not on most maps of the cemetery, it can be found near the grave of Frédéric Chopin, slightly to the west and directly north, and behind the small mausoleum of the jeweller Abraham-Louis Breguet.
The Messier catalogue.
Messier's occupation as a comet hunter led him to continually come across fixed diffuse objects in the night sky which could be mistaken for comets (they are known today to be galaxies (39), nebulae (7), Planetary nebulae (5), and star clusters (55) ). He compiled a list of them, in collaboration with his friend and assistant Pierre Méchain (who may have found at least 20 of the objects), to avoid wasting time sorting them out from the comets they were looking for.
Messier did his observing with a 100 mm (four inch) refracting telescope from Hôtel de Cluny (now the Musée national du Moyen Âge), in downtown Paris, France. The list he compiled contains only objects found in the sky area he could observe, from the north celestial pole to a celestial latitude of about −35.7°. They are not organized scientifically by object type, or even by location. The first version of Messier's catalogue contained 45 objects and was published in 1774 in the journal of the French Academy of Sciences in Paris. In addition to his own discoveries, this version included objects previously observed by other astronomers, with only 17 of the 45 objects being Messier’s. By 1780 the catalog had increased to 80 objects.
The final version of the catalogue was published in 1781, in the 1784 issue of "Connaissance des Temps".</ref> The final list of Messier objects had grown to 103. On several occasions between 1921 and 1966, astronomers and historians discovered evidence of another seven objects that were observed either by Messier or by Méchain, shortly after the final version was published. These seven objects, M104 through M110, are accepted by astronomers as "official" Messier objects.
The objects' Messier designations, from M1 to M110, still are in use by professional and amateur astronomers today and their relative brightness makes them popular objects in the amateur astronomical community.
Legacy.
The crater Messier on the Moon and the asteroid 7359 Messier were named in his honor.

</doc>
<doc id="7247" url="https://en.wikipedia.org/wiki?curid=7247" title="Cemetery H culture">
Cemetery H culture

The Cemetery H culture was a Bronze Age culture in the Punjab, north-western India, from about 1900 BCE until about 1300 BCE. It has been related to both the late phase of the Harappan (Indus Valley) civilisation, and the Indo-Aryan migrations.
Origins.
The Cemetery H culture was located in and around the Punjab region in present-day India and Pakistan. It was named after a cemetery found in "area H" at Harappa. Remains of the culture have been dated from about 1900 BCE until about 1300 BCE.
According to Rafique Mughal, the Cemetery H culture developed out of the northern part of the Indus Valley Civilization around 1700 BCE, being part of the Punjab Phase, one of three cultural phases that developed in the Localization Era or "Late Harappan phase" of the Indus Valley Tradition. According to Kenoyer, the Cemetery H culture "may only reflect a change in the focus of settlement organization from that which was the pattern of the earlier Harappan phase and not cultural discontinuity, urban decay, invading aliens, or site abandonment, all of which have been suggested in the past." According to Kennedy and Mallory & Adams, the Cemetery H culture also "shows clear biological affinities" with the earlier population of Harappa. 
Some traits of the Cemetery H culture have been associated with the Swat culture, which has been regarded as evidence of the Indo-Aryan movement toward the Indian subcontinent. According to Parpola, the Cemetery H culture represents a first wave of Indo-Aryan migration from as early as 1900 BCE, which was followed by a migration to the Punjab c. 1700-1400 BCE. According to Kochhar, the Swat IV co-founded the Harappan Cemetery H phase in Punjab (2000-1800 BCE), while the Rigvedic Indo-Aryans of Swat V later absorbed the Cemetery H people and gave rise to the Painted Grey Ware culture (to 1400 BCE).
Together with the Gandhara grave culture and the Ochre Coloured Pottery culture, the Cemetery H culture is considered by some scholars as a factor in the formation of the Vedic civilization.
Features.
The distinguishing features of this culture include:
Archeology.
Cremation in India is first attested in the Cemetery H culture, which is coincidentally referred to in the Vedas. The Rigveda contains a reference to the emerging practice, in RV 10.15.14, where the forefathers "both cremated ("agnidagdhá-") and uncremated ("ánagnidagdha-")" are invoked.

</doc>
<doc id="7248" url="https://en.wikipedia.org/wiki?curid=7248" title="Corrado Gini">
Corrado Gini

Corrado Gini (May 23, 1884 – March 13, 1965) was an Italian statistician, demographer and sociologist who developed the Gini coefficient, a measure of the income inequality in a society. Gini was a proponent of organicism and applied it to nations.
Career.
Gini was born on May 23, 1884, in Motta di Livenza, near Treviso, into an old landed family. He entered the Faculty of Law at the University of Bologna, where in addition to law he studied mathematics, economics, and biology.
Gini's scientific work ran in two directions: towards the social sciences and towards statistics. His interests ranged well beyond the formal aspects of statistics—to the laws that govern biological and social phenomena.
His first published work was "Il sesso dal punto di vista statistico" (1908). This work is a thorough review of the natal sex ratio, looking at past theories and at how new hypothesis fit the statistical data. In particular, it presents evidence that the tendency to produce one or the other sex of child is, to some extent, heritable.
In 1910, he acceded to the Chair of Statistics in the University of Cagliari and then at Padua in 1913.
He founded the statistical journal "Metron" in 1920, directing it until his death; it only accepted articles with practical applications.
He became a professor at the Sapienza University of Rome in 1925. At the University, he founded a lecture course on sociology, maintaining it until his retirement. He also set up the School of Statistics in 1928, and, in 1936, the Faculty of Statistical, Demographic and Actuarial Sciences.
In 1929, Gini founded the Italian Committee for the Study of Population Problems ("Comitato italiano per lo studio dei problemi della popolazione) " which, two years later, organised the first Population Congress in Rome.
In 1926, he was appointed President of the Central Institute of Statistics in Rome. This he organised as a single centre for Italian statistical services. He resigned in 1932 in protest at interference in his work by the fascist state.
Milestones during the rest of his career include:
Italian Unionist Movement.
On October 12, 1944, Gini joined with the Calabrian activist Santi Paladino, and fellow-statistician Ugo Damiani to found the Italian Unionist Movement, for which the emblem was the Stars and Stripes, the Italian flag and a world map. According to the three men, the Government of the United States should annex all free and democratic nations worldwide, thereby transforming itself into a world government, and allowing Washington DC to maintain Earth in a perpetual condition of peace. The party existed up to 1948 but had little success and its aims were not supported by the United States.
Organicism and nations.
Gini was a proponent of organicism and saw nations as organic in nature. Gini shared the view held by Oswald Spengler that populations go through a cycle of birth, growth, and decay. Gini claimed that nations at a primitive level have a high birth rate, but, as they evolve, the upper classes birth rate drops while the lower class birth rate, while higher, will inevitably deplete as their stronger members emigrate, die in war, or enter into the upper classes. If a nation continues on this path without resistance, Gini claimed the nation would enter a final decadent stage where the nation would degenerate as noted by decreasing birth rate, decreasing cultural output, and the lack of imperial conquest. At this point, the decadent nation with its aging population can be overrun by a more youthful and vigorous nation. Gini's organicist theories of nations and natality are believed to have influenced policies of Italian Fascism.
Honours.
The following honorary degrees were conferred upon him:

</doc>
<doc id="7249" url="https://en.wikipedia.org/wiki?curid=7249" title="Crankshaft">
Crankshaft

A crankshaft—related to "crank"—is a mechanical part able to perform a conversion between reciprocating motion and rotational motion. In a reciprocating engine, it translates reciprocating motion of the piston into rotational motion; whereas in a reciprocating compressor, it converts the rotational motion into reciprocating motion. In order to do the conversion between two motions, the crankshaft has "crank throws" or "crankpins", additional bearing surfaces whose axis is offset from that of the crank, to which the "big ends" of the connecting rods from each cylinder attach.
It is typically connected to a flywheel to reduce the pulsation characteristic of the four-stroke cycle, and sometimes a torsional or vibrational damper at the opposite end, to reduce the torsional vibrations often caused along the length of the crankshaft by the cylinders farthest from the output end acting on the torsional elasticity of the metal.
History.
Roman Empire.
A Roman iron crank of yet unknown purpose dating to the 2nd century AD was excavated in Augusta Raurica, Switzerland. The 82.5 cm long piece has fitted to one end a 15 cm long bronze handle, the other handle being lost.
The earliest evidence, anywhere in the world, for a crank and connecting rod in a machine appears in the late Roman Hierapolis sawmill from the 3rd century AD and two Roman stone sawmills at Gerasa, Roman Syria, and Ephesus, Asia Minor (both 6th century AD). On the pediment of the Hierapolis mill, a waterwheel fed by a mill race is shown transmitting power through a gear train to two frame saws, which cut rectangular blocks by way of some kind of connecting rods and, through mechanical necessity, cranks. The accompanying inscription is in Greek.
The crank and connecting rod mechanisms of the other two archaeologically attested sawmills worked without a gear train. In ancient literature, we find a reference to the workings of water-powered marble saws close to Trier, now Germany, by the late 4th century poet Ausonius; about the same time, these mill types seem also to be indicated by the Christian saint Gregory of Nyssa from Anatolia, demonstrating a diversified use of water-power in many parts of the Roman Empire. The three finds push back the date of the invention of the crank and connecting rod back by a full millennium; for the first time, all essential components of the much later steam engine were assembled by one technological culture:
Medieval East.
Al-Jazari (1136–1206) described a crank and connecting rod system in a rotating machine in two of his water-raising machines. His twin-cylinder pump incorporated a crankshaft, though the device was unnecessarily complex.
In China, the potential of the crank of converting circular motion into reciprocal one never seems to have been fully realized, and the crank was typically absent from such machines until the turn of the 20th century.
Medieval Europe.
The Italian physician Guido da Vigevano (c. 1280−1349), planning for a new crusade, made illustrations for a paddle boat and war carriages that were propelled by manually turned compound cranks and gear wheels (center of image). The Luttrell Psalter, dating to around 1340, describes a grindstone rotated by two cranks, one at each end of its axle; the geared hand-mill, operated either with one or two cranks, appeared later in the 15th century;
Renaissance Europe.
The first depictions of the compound crank in the carpenter's brace appear between 1420 and 1430 in various northern European artwork. The rapid adoption of the compound crank can be traced in the works of the Anonymous of the Hussite Wars, an unknown German engineer writing on the state of the military technology of his day: first, the connecting-rod, applied to cranks, reappeared, second, double compound cranks also began to be equipped with connecting-rods and third, the flywheel was employed for these cranks to get them over the 'dead-spot'.
In Renaissance Italy, the earliest evidence of a compound crank and connecting-rod is found in the sketch books of Taccola, but the device is still mechanically misunderstood. A sound grasp of the crank motion involved is demonstrated a little later by Pisanello, who painted a piston-pump driven by a water-wheel and operated by two simple cranks and two connecting-rods.
One of the drawings of the Anonymous of the Hussite Wars shows a boat with a pair of paddle-wheels at each end turned by men operating compound cranks (see above). The concept was much improved by the Italian Roberto Valturio in 1463, who devised a boat with five sets, where the parallel cranks are all joined to a single power source by one connecting-rod, an idea also taken up by his compatriot Francesco di Giorgio.
Crankshafts were also described by Konrad Kyeser (d. 1405), Leonardo da Vinci (1452–1519) and a Dutch "farmer" by the name Cornelis Corneliszoon van Uitgeest in 1592. His wind-powered sawmill used a crankshaft to convert a windmill's circular motion into a back-and-forward motion powering the saw. Corneliszoon was granted a patent for his crankshaft in 1597.
From the 16th century onwards, evidence of cranks and connecting rods integrated into machine design becomes abundant in the technological treatises of the period: Agostino Ramelli's "The Diverse and Artifactitious Machines" of 1588 alone depicts eighteen examples, a number that rises in the "Theatrum Machinarum Novum" by Georg Andreas Böckler to 45 different machines, one third of the total.
Internal combustion engines.
Large engines are usually multicylinder to reduce pulsations from individual firing strokes, with more than one piston attached to a complex crankshaft. Many small engines, such as those found in mopeds or garden machinery, are single cylinder and use only a single piston, simplifying crankshaft design.
A crankshaft is subjected to enormous stresses, potentially equivalent of several tonnes of force. The crankshaft is connected to the fly-wheel (used to smooth out shock and convert energy to torque), the engine block, using bearings on the main journals, and to the pistons via their respective con-rods. An engine loses up to 75% of its generated energy in the form of friction, noise and vibration in the crankcase and piston area. The remaining losses occur in the valvetrain (timing chains, belts, pulleys, camshafts, lobes, valves, seals etc.) heat and blow by.
Bearings.
The crankshaft has a linear axis about which it rotates, typically with several bearing journals riding on replaceable bearings (the main bearings) held in the engine block. As the crankshaft undergoes a great deal of sideways load from each cylinder in a multicylinder engine, it must be supported by several such bearings, not just one at each end. This was a factor in the rise of V8 engines, with their shorter crankshafts, in preference to straight-8 engines. The long crankshafts of the latter suffered from an unacceptable amount of flex when engine designers began using higher compression ratios and higher rotational speeds. High performance engines often have more main bearings than their lower performance cousins for this reason.
Piston stroke.
The distance the axis of the crank throws from the axis of the crankshaft determines the piston stroke measurement, and thus engine displacement. A common way to increase the low-speed torque of an engine is to increase the stroke, sometimes known as "shaft-stroking." This also increases the reciprocating vibration, however, limiting the high speed capability of the engine. In compensation, it improves the low speed operation of the engine, as the longer intake stroke through smaller valve(s) results in greater turbulence and mixing of the intake charge. Most modern high speed production engines are classified as "over square" or short-stroke, wherein the stroke is less than the diameter of the cylinder bore. As such, finding the proper balance between shaft-stroking speed and length leads to better results.
Engine configuration.
The configuration, meaning the number of pistons and their placement in relation to each other leads to straight, V or flat engines. The same basic engine block can sometimes be used with different crankshafts, however, to alter the firing order. For instance, the 90° V6 engine configuration, in older days sometimes derived by using six cylinders of a V8 engine with a 3 throw crankshaft, produces an engine with an inherent pulsation in the power flow due to the "gap" between the firing pulses alternates between short and long pauses because the 90 degree engine block does not correspond to the 120 degree spacing of the crankshaft. The same engine, however, can be made to provide evenly spaced power pulses by using a crankshaft with an individual crank throw for each cylinder, spaced so that the pistons are actually phased 120° apart, as in the GM 3800 engine. While most production V8 engines use four crank throws spaced 90° apart, high-performance V8 engines often use a "flat" crankshaft with throws spaced 180° apart, essentially resulting in two straight four engines running on a common crankcase. The difference can be heard as the flat-plane crankshafts result in the engine having a smoother, higher-pitched sound than cross-plane (for example, IRL IndyCar Series compared to NASCAR Sprint Cup Series, or a Ferrari 355 compared to a Chevrolet Corvette). This type of crankshaft was also used on early types of V8 engines. See the main article on crossplane crankshafts.
Engine balance.
For some engines it is necessary to provide counterweights for the reciprocating mass of each piston and connecting rod to improve engine balance. These are typically cast as part of the crankshaft but, occasionally, are bolt-on pieces. While counter weights add a considerable amount of weight to the crankshaft, it provides a smoother running engine and allows higher RPM levels to be reached.
Flying arms.
In some engine configurations, the crankshaft contains direct links between adjacent crankpins (without an intermediate main bearing, as is usually the case), thus half as many crankthrows as pistons are used. These links are called "flying arms". This arrangement is sometimes used in V6 and V8 engines as it enables the engine to be designed with different V angles than what would otherwise be required to create an even firing interval, while still using fewer main bearings than would normally be required with a single piston per crankthrow. This arrangement reduces weight and engine length at the expense of less crankshaft rigidity.
Rotary aircraft engines.
Some early aircraft engines were a rotary engine design, where the crankshaft was fixed to the airframe and instead the cylinders rotated with the propeller.
Radial engines.
The radial engine is a reciprocating type internal combustion engine configuration in which the cylinders point outward from a central crankshaft like the spokes of a wheel. It resembles a stylized star when viewed from the front, and is called a "star engine" (German Sternmotor, French Moteur en étoile) in some languages. The radial configuration was very commonly used in aircraft engines before turbine engines became predominant.
Construction.
Crankshafts can be monolithic (made in a single piece) or assembled from several pieces. Monolithic crankshafts are most common, but some smaller and larger engines use assembled crankshafts.
Forging and casting.
Crankshafts can be forged from a steel bar usually through roll forging or cast in ductile steel. Today more and more manufacturers tend to favor the use of forged crankshafts due to their lighter weight, more compact dimensions and better inherent damping. With forged crankshafts, vanadium microalloyed steels are mostly used as these steels can be air cooled after reaching high strengths without additional heat treatment, with exception to the surface hardening of the bearing surfaces. The low alloy content also makes the material cheaper than high alloy steels. Carbon steels are also used, but these require additional heat treatment to reach the desired properties. Iron crankshafts are today mostly found in cheaper production engines (such as those found in the Ford Focus diesel engines) where the loads are lower. Some engines also use cast iron crankshafts for low output versions while the more expensive high output version use forged steel.
Machining.
Crankshafts can also be machined out of a billet, often a bar of high quality vacuum remelted steel. Though the fiber flow (local inhomogeneities of the material's chemical composition generated during casting) doesn’t follow the shape of the crankshaft (which is undesirable), this is usually not a problem since higher quality steels, which normally are difficult to forge, can be used. These crankshafts tend to be very expensive due to the large amount of material that must be removed with lathes and milling machines, the high material cost, and the additional heat treatment required. However, since no expensive tooling is needed, this production method allows small production runs without high costs.
In an effort to reduce costs, used crankshafts may also be machined. A good core may often be easily reconditioned by a crankshaft grinding process. Severely damaged crankshafts may also be repaired with a welding operation, prior to grinding, that utilizes a submerged arc welding machine. To accommodate the smaller journal diameters a ground crankshaft has, and possibly an over-sized thrust dimension, undersize engine bearings are used to allow for precise clearances during operation.
Machining or remanufacturing crankshafts are precision machined to exact tolerances with no odd size crankshaft bearings or journals. Thrust surfaces are micro-polished to provide precise surface finishes for smooth engine operation and reduced thrust bearing wear. Every journal is inspected and measured with critical accuracy. After machining, oil holes are chamfered to improve lubrication and every journal polished to a smooth finish for long bearing life. Remanufactured crankshafts are thoroughly cleaned with special emphasis to flushing and brushing out oil passages to remove any contaminants. Typically there are 23 steps to remanufacturing a crankshaft which are as follows:
Step 1: Industrial Cleaning.
The first step in the industrial crankshaft remanufacturing process is cleaning the entire crankshaft. Machine shops soak the rebuilt crankshafts in a hot tank and use a power washing station on the overall shaft as needed. Next machinists then wire brush all oil holes to remove caked on residue and other substances.
Step 2: Inspection and Magnaflux.
The second step in the crankshaft remanufacturing process is using a magnaflux method known as magnetic particle inspection to check for cracks. The crankshaft is maganitized and sprayed with a magnaflux powder which, under blacklight conditions, makes any cracks or imperfections visible. All remanufactured crankshafts are magnafluxed for imperfections before proceeding forward in the manufacturing process.
Step 3: Check Counterweights.
The machine shop then removes and cleans the counterweights. The production facility then checks the counterweights to make sure they are tight. If the counterweights are loose a technician then replaces all of the counterweight bolts. Counterweights are inspected for cracks before being replaced or retightened. In step sixteen the machinist re-installs the counterweights back into the rebuilt crankshafts.
Step 4: Check Crankshaft Bearings and Straightness.
The machinist then inspects the entire incoming remanufactured crankshaft for damage and determines the size of the journals and mains. Next the machinist checks the hardness of the mains and journals. It is crucial to also inspect the crankshaft bearings and check the straightness of the overall crankshaft. Re-straightening the industrial crankshaft if not up to OEM standards occurs in step seven. Veteran machine shops typically do not re-straighten the rebuilt crankshafts until a quality control technician checks the bolt holes and seals the surface for divots.
Step 5: Check Bolt Holes.
The technician checks the keyway, nose, bolt holes and seals the surface for non-conformities. Usually machine shops will tap bolt holes up to but not more than ½” on all remanufactured crankshafts.
Step 6: Stamp Counterweight Webbing.
The rebuild team next stamps the counterweights & webbing in proper firing order (alpha if numeric & vice versa). Technicians then stamp the employee ID#, Work Order # and date on #1 rod webbing. Stamping this information on the rod webbing helps keep the quality control process order in case of future issues during the manufacturing process.
Step 7: Re-straightening for Rebuilt Crankshafts.
The seventh step is industrial crankshaft re-straightening. If the remanufactured crankshaft is deemed un-straight than technicians use an industrial straightening machine on the crankshaft. The straightening machine determines how many dials are out of line. To re-straighten the shaft technicians heat up the crankshaft to 500-600 degrees. Any more than 700 degrees takes the hardness out of the shaft. The strightener process corrects the bent crankshaft to the proper OEM specifications for rebuilt crankshafts.
Step 8: Repeat Magnaflux Process.
The eight step in the process is repeating the magnaflux process if straightening was performed. Anytime metal is being stressed it is imperative to re-inspect for cracks and structural imperfections on the reman crankshaft.
Step 9: Undercutting.
The ninth step in the industrial crankshaft remanufacturing process is undercutting. Technicians undercut the rod or journals to eliminate wear before buildup.
Step 10: Thermal Spraying.
The tenth step is the prevention of further buildup via metalizing often called thermal spraying. Thermal spraying has been around for well over 100 years but is still widely known as the best preventative corrosion fighting technique in the world. Thermal spraying is also known for changing the surface of the metallic component and is common with rebuilt crankshafts. Thermal spraying involves protrusion of molten particles onto the heated metallic surface where is bonds and forms a smooth coating interwoven into the structure. There are many different types of thermal spray alloys that can be employed for remanufactured crankshafts. Typically, boron alloys are used as they very dense, hard and are oxide free. They also prevent against abrasive materials that cause divots, scratches and cracks in addition to preventing surface erosion and corrosion. Thermal spray is an important step some machine shops employ, but not always performed in the industry.
Step 11: Industrial Crankshaft Welding.
The welding process for remanufactured crankshafts is called submerged dark welding. It is a powdered flux plus a weld which combines to produce a more precise weld. The most common flux powder used is called #1 Flux 2245 HD. This powder eliminates the need for technicians to wear weld masking and reduces the amount of dust by-product.
Step 12: Relieve Structural Stress.
The twelfth step is to relieve stress upon the entire rebuilt crankshaft structure by heating it up again to 500-600 degrees.
Step 13: Recheck for Straightness.
Next step is to check for overall straightness of the remanufactured crankshaft once again. If the remanufactured crankshaft is out of alignment then the technician repeats step 7 and re-straighten the structure. Each of the remanufactured crankshafts is checked multiple times throughout the remanufacturing process to ensure quality control. If the straightness is not compromised the rebuilt crankshafts can proceed to step thirteen which is crankshaft grinding.
Step 14: Rough Crankshaft Grinding.
This is one of the most important steps in the remanufacturing process of industrial crankshafts. This step involves rough grinding the excess material from the rod or journals and is known as crankshaft grinding. On the rod there are various mains that need to be reground to proper OEM specifications. These rods are spun grind to the next undersize using the pultrusion crankshaft grinding machine. Rod mains are ground inside and outside. Machine shops have the ability to “crankshaft grind” to any size to bring back the crankshaft to standard OEM specifications.
Step 15: Finished Crankshaft Grinding.
Next the technician performs a finished crankshaft grinding procedure. The finished crankshaft grinding is a more precise grind which reaches the correct OEM specifications. Before the technician starts the crankshaft grinding they should see what crankshaft bearings are available and start from there. For example the OEM specification for a Caterpillar 3306 Rod is 2.9987” – 3.0003”. Top industrial crankshaft grinding technicians always stop at the high end of the tolerance level. Lastly the technician further refines the crankshaft grinding process in during the micro-polishing process at step eighteen.
Step 16: Shot Peening.
The next step is to process the industrial crankshaft in using shot peen machinery. Shot peening adds an additional layer of hardness to the reman crankshaft.
Step 17: Replace or Re-tighten Counterweights.
Step 17 involves replacing the counterweights in proper firing order. Either the new counterweights are installed or the old counterweight bolts are retightened and tested.
Step 18: Determine Proper Balance.
The machine shop then determines if the proper rotational balance of the remanufactured crankshafts is achieved. In the engine the crankshaft, pistons and rods all in a constant rotation. The counterweights are designed to offset the weight of the rod and the pistons in the engine. When in motion the kinetic energy and the sum of all forces should be equal to zero on all moving parts. If the reman crankshaft counterweights are imbalanced it adds additional stress on other components of the engine. The technician should then make sure the internal balance and the external balance of the crankshaft counterweights are properly aligned.
Step 19: Micro-polishing.
Then the technician micro-polishes each of the rebuilt crankshafts by hand. To further refine the crankshaft grinding process the machinist makes the most precise fit by micro-polishing the component with a 600 grit emery cloth. Through micro-polishing and industrial crankshaft grinding, the machine shop achieves the recommended Rockwell hardness and Ra finish (Roughness Parameter).
Step 20: Test Reman Crankshaft Rockwell Hardness.
Next the technician checks the industry standard hardness. Industry standards crankshaft hardness is 40 on the Rockwell hardness scale. A 45-50 rating is what most reputable machine shops try to employ for all remanufactured crankshafts. When possible it is wise to go beyond industry standards to prevent any future weaknesses within the unit. Typically, hardness can be reduced if the engine is out of oil or the journal is spun incorrectly.
Step 21: Final Quality Control Inspection.
Quality control inspects all of the finished reman crankshafts for internal and external mistakes. A typical quality control department uses separate testing and analytical measurement tools from the technicians to ensure accuracy. If the rebuilt crankshaft passes the quality control inspection it goes onto the rust proofing stage.
Step 22: Rustproof Remanufactured Crankshaft.
The vast majority of machine shops apply rust proofing to all remanufactured crankshafts using Cosmoline, which is standard rust-proofing for engine parts.
Step 23: Packaging.
Lastly the machine shop packs the finished rebuilt crankshaft correctly making sure to using proper boxing and damage proof coverings. It is important to cover the rod journals (varies per crankshaft) with paper & tape in place.
Microfinishing.
To achieve the required specifications, automotive manufacturers which design and produce high-volume, low-cost powertrain components, strive toward surpassing stringent emissions and efficiency regulations (see Euro 6c standards for reference) to reduce losses. In motorsport, powertrain developers strive to increase power output by reducing weight, using strong metal alloys, hardening crankshafts, improving balance, reducing friction and vibration as previously described.
To achieve the required specifications, automotive and motorsport powertrain designers and manufacturers adopt a process called microfinishing. Microfinishing (or superfinishing) is an engineering function concerned with metrology and tribology. Microfinishing takes place after the crankshaft grinding process, and is used to improve the geometry of the crankshaft journals from waviness, peaks and lapping caused by the grinding process and establish surface roughness as low as "R"a = 0.01 µm if required.
Microfinished crankshafts show improved roundness and cylindricity for each main and pin and thrust journal, and where applicable the oil seal journal. Another important function to a geometrically correct shape is to provide it with a specific surface roughness as per design requirements for optimum lubrication hydrodynamics (essential for crankshafts in engines with stop/start fuel saving technology).
Today, crankshafts used in outboard engines, motorbikes, cars, trucks, busses, marine engines and electric generators and racing engines, are all microfinished for optimum performance. They are designed and manufactured to transfer as much energy to the fly-wheel and drivetrain and absorb as much power from the con-rods, as efficiently as possible for as long as possible.
With this new technology, a light weight, turbocharged 2.0 liter, 4 cylinder diesel engine, (with a low-cost 4 pin, 5 main induction hardened, cast steel, microfinished crankshaft), in a small family car, potentially delivers 180 hp and provides an average fuel consumption of 60 miles per gallon and beyond.
Fatigue strength.
The fatigue strength of crankshafts is usually increased by using a radius at the ends of each main and crankpin bearing. The radius itself reduces the stress in these critical areas, but since the radius in most cases is rolled, this also leaves some compressive residual stress in the surface, which prevents cracks from forming.
Hardening.
Most production crankshafts use induction hardened bearing surfaces, since that method gives good results with low costs. It also allows the crankshaft to be reground without re-hardening. But high performance crankshafts, billet crankshafts in particular, tend to use nitridization instead. Nitridization is slower and thereby more costly, and in addition it puts certain demands on the alloying metals in the steel to be able to create stable nitrides. The advantage of nitridization is that it can be done at low temperatures, it produces a very hard surface, and the process leaves some compressive residual stress in the surface, which is good for fatigue properties. The low temperature during treatment is advantageous in that it doesn’t have any negative effects on the steel, such as annealing. With crankshafts that operate on roller bearings, the use of carburization tends to be favored due to the high Hertzian contact stresses in such an application. Like nitriding, carburization also leaves some compressive residual stresses in the surface.
Counterweights.
Some expensive, high performance crankshafts also use heavy-metal counterweights to make the crankshaft more compact. The heavy-metal used is most often a tungsten alloy but depleted uranium has also been used. A cheaper option is to use lead, but compared with tungsten its density is much lower.
Stress on crankshafts.
The shaft is subjected to various forces but generally needs to be analysed in two positions.
Firstly, failure may occur at the position of maximum bending; this may be at the centre of the crank or at either end. In such a condition the failure is due to bending and the pressure in the cylinder is maximal. Second, the crank may fail due to twisting, so the conrod needs to be checked for shear at the position of maximal twisting. The pressure at this position is the maximal pressure, but only a fraction of maximal pressure."

</doc>
<doc id="7250" url="https://en.wikipedia.org/wiki?curid=7250" title="CNS">
CNS

CNS may refer to:

</doc>
<doc id="7251" url="https://en.wikipedia.org/wiki?curid=7251" title="Central nervous system">
Central nervous system

The central nervous system (CNS) is the part of the nervous system consisting of the brain and spinal cord. The central nervous system is so named because it integrates information it receives from, and coordinates and influences the activity of, all parts of the bodies of bilaterally symmetric animals—that is, all multicellular animals except sponges and radially symmetric animals such as jellyfish—and it contains the majority of the nervous system. Arguably, many consider the retina and the optic nerve (2nd cranial nerve), as well as the olfactory nerves (1st) and olfactory epithelium as parts of the CNS, synapsing directly on brain tissue without intermediate ganglia. Following this classification the olfactory epithelium is the only central nervous tissue in direct contact with the environment, which opens up for therapeutic treatments.
Structure.
The central nervous system consists of the two major structures: the brain and spinal cord.
The brain is encased in the skull, and protected by the cranium. The spinal cord is continuous with the brain and lies caudally to the brain, and is protected by the vertebra. The spinal cord reaches from the base of the skull, continues through or starting below the foramen magnum, and terminates roughly level with the first or second lumbar vertebra, occupying the upper sections of the vertebral canal.
White and gray matter.
Microscopically, there are differences between the neurons and tissue of the central nervous system and the peripheral nervous system. The central nervous system is divided in white and gray matter. This can also be seen macroscopically on brain tissue. The white matter consists of axons and oligodendrocytes, while the gray matter consists of neurons and unmyelinated fibers. Both tissues include a number of glial cells (although the white matter contains more), which are often referred to as supporting cells of the central nervous system. Different forms of glial cells have different functions, some acting almost as scaffolding for neuroblasts to climb during neurogenesis such as bergmann glia, while others such as microglia are a specialized form of macrophage, involved in the immune system of the brain as well as the clearance of various metabolites from the brain tissue. Astrocytes may be involved with both clearance of metabolites as well as transport of fuel and various beneficial substances to neurons from the capillaries of the brain. Upon CNS injury astrocytes will proliferate, causing gliosis, a form of neuronal scar tissue, lacking in functional neurons.
The brain (cerebrum as well as midbrain and hindbrain) consists of a cortex, composed of neuron-bodies constituting gray matter, while internally there is more white matter that form tracts and commissures. Apart from cortical gray matter there is also subcortical gray making up a large number of different nuclei.
Spinal cord.
From and to the spinal cord are projections of the peripheral nervous system in the form of spinal nerves (sometimes segmental nerves). The nerves connect the spinal cord with skin, joints, muscles etc. and allow for the transmission of efferent motor as well as afferent sensory signals and stimuli. This allows for voluntary and involuntary motions of muscles, as well as the perception of senses.
All in all 31 spinal nerves project from the brain stem, some forming plexa as they branch out, such as the brachial plexa, sacral plexa etc. Each spinal nerve will carry both sensory and motor signals, but the nerves synapse at different regions of the spinal cord, either from the periphery to sensory relay neurons that relay the information to the CNS or from the CNS to motor neurons, which relay the information out.
The spinal cord relays information up to the brain through spinal tracts through the "final common pathway" to the thalamus and ultimately to the cortex. Not all information is relayed to the cortex, and does not reach our immediate consciousness, but is instead transmitted only to the thalamus which sorts and adapts accordingly. This in turn may explain why we are not constantly aware of all aspects of our surroundings.
Cranial nerves.
Apart from the spinal cord, there are also peripheral nerves of the PNS that synapse through intermediaries or ganglia directly on the CNS. These 12 nerves exist in the head and neck region and are called cranial nerves. Cranial nerves bring information to the CNS to and from the face, as well as to certain muscles (such as the trapezius muscle, which is innervated by accessory nerves as well as certain cervical spinal nerves).
Two pairs of cranial nerves; the olfactory nerves and the optic nerves are often considered structures of the central nervous system. This is because they do not synapse first on peripheral ganglia, but directly on central nervous neurons. The olfactory epithelium is significant in that it consists of central nervous tissue expressed in direct contact to the environment, allowing for administration of certain pharmaceuticals and drugs.
Brain.
Rostrally to the spinal cord lies the brain.
The brain makes up the largest portion of the central nervous system, and is often the main structure referred to when speaking of the nervous system. The brain is the major functional unit of the central nervous system. While the spinal cord has certain processing ability such as that of spinal locomotion and can process reflexes, the brain is the major processing unit of the nervous system.
Brainstem.
The brainstem consists of the medulla, the pons and the midbrain. The medulla can be referred to as an extension of the spinal cord, and its organization and functional properties are similar to those of the spinal cord. The tracts passing from the spinal cord to the brain pass through here.
Regulatory functions of the medulla nuclei include control of the blood pressure and breathing. Other nuclei are involved in balance, taste, hearing and control of muscles of the face and neck.
The next structure rostral to the medulla is the pons, which lies on the ventral anterior side of the brainstem. Nuclei in the pons include pontine nuclei which work with the cerebellum and transmit information between the cerebellum and the cerebral cortex.
In the dorsal posterior pons lie nuclei that have to do with breathing, sleep and taste.
The midbrain (or mesencephalon) is situated above and rostral to the pons, and includes nuclei linking distinct parts of the motor system, among others the cerebellum, the basal ganglia and both cerebral hemispheres. Additionally parts of the visual and auditory systems are located in the mid brain, including control of automatic eye movements.
The brainstem at large provides entry and exit to the brain for a number of pathways for motor and autonomic control of the face and neck through cranial nerves, and autonomic control of the organs is mediated by the tenth cranial (vagus) nerve. A large portion of the brainstem is involved in such autonomic control of the body. Such functions may engage the heart, blood vessels, pupillae, among others.
The brainstem also hold the reticular formation, a group of nuclei involved in both arousal and alertness.
Cerebellum.
The cerebellum lies behind the pons. The cerebellum is composed of several dividing fissures and lobes. Its function includes the control of posture, and the coordination of movements of parts of the body, including the eyes and head as well as the limbs. Further it is involved in motion that has been learned and perfected though practice, and will adapt to new learned movements.
Despite its previous classification as a motor structure, the cerebellum also displays connections to areas of the cerebral cortex involved in language as well as cognitive functions. These connections have been shown by the use of medical imaging techniques such as fMRI and PET.
The body of the cerebellum holds more neurons than any other structure of the brain including that of the larger cerebrum (or cerebral hemispheres), but is also more extensively understood than other structures of the brain, and includes fewer types of different neurons. It handles and processes sensory stimuli, motor information as well as balance information from the vestibular organ.
Diencephalon.
The two structures of the diencephalon worth noting are the thalamus and the hypothalamus. The thalamus acts as a linkage between incoming pathways from the peripheral nervous system as well as the optical nerve (though it does not receive input from the olfactory nerve) to the cerebral hemispheres. Previously it was considered only a "relay station", but it is engaged in the sorting of information that will reach cerebral hemispheres (neocortex).
Apart from its function of sorting information from the periphery, the thalamus also connects the cerebellum and basal ganglia with the cerebrum. In common with the aforementioned reticular system the thalamus is involved in wakefullness and consciousness, such as though the SCN.
The hypothalamus engages in functions of a number of primitive emotions or feelings such as hunger, thirst and maternal bonding. This is regulated partly through control of secretion of hormones from the pituitary gland. Additionally the hypothalamus plays a role in motivation and many other behaviors of the individual.
Cerebrum.
The cerebrum of cerebral hemispheres make up the largest portion of the human brain. Various structures combine forming the cerebral hemispheres, among others, the cortex, basal ganglia, amygdala and hippocampus. The hemispheres together control a large portion of the functions of the human brain such as emotion, memory, perception and motor functions. Apart from this the cerebral hemispheres stand for the cognitive capabilities of the brain.
Connecting each of the hemispheres is the corpus callosum as well as several additional commissures.
One of the most important parts of the cerebral hemispheres is the cortex, made up of gray matter covering the surface of the brain. Functionally, the cerebral cortex is involved in planning and carrying out of everyday tasks.
The hippocampus is involved in storage of memories, the amygdala plays a role in perception and communication of emotion, while the basal ganglia play a major role in the coordination of voluntary movement.
Difference from the peripheral nervous system.
This differentiates the central nervous system from the peripheral nervous system, which consists of neurons, axons and Schwann cells. Oligodendrocytes and Schwann cells have similar functions in the central and peripheral nervous system respectively. Both act to add myelin sheaths to the axons, which acts as a form of insulation allowing for better and faster proliferation of electrical signals along the nerves. Axons in the central nervous system are often very short (barely a few millimeters) and do not need the same degree of isolation as peripheral nerves do. Some peripheral nerves can be over 1m in length, such as the nerves to the big toe. To ensure signals move at sufficient speed, myelination is needed.
The way in which the Schwann cells and oligodendrocytes myelinate nerves differ. A Schwann cell usually myelinates a single axon, completely surrounding it. Sometimes they may myelinate many axons, especially when in areas of short axons. Oligodendrocytes usually myelinate several axons. They do this by sending out thin projections of their cell membrane which envelop and enclose the axon.
Development.
During early development of the vertebrate embryo, a longitudinal groove on the neural plate gradually deepens and the ridges on either side of the groove (the neural folds) become elevated, and ultimately meet, transforming the groove into a closed tube, the ectodermal wall of which forms the rudiment of the nervous system. This tube initially differentiates into three vesicles (pockets): the prosencephalon at the front, the mesencephalon, and, between the mesencephalon and the spinal cord, the rhombencephalon. (By six weeks in the human embryo) the prosencephalon then divides further into the telencephalon and diencephalon; and the rhombencephalon divides into the metencephalon and myelencephalon.
As a vertebrate grows, these vesicles differentiate further still. The telencephalon differentiates into, among other things, the striatum, the hippocampus and the neocortex, and its cavity becomes the first and second ventricles. Diencephalon elaborations include the subthalamus, hypothalamus, thalamus and epithalamus, and its cavity forms the third ventricle. The tectum, pretectum, cerebral peduncle and other structures develop out of the mesencephalon, and its cavity grows into the mesencephalic duct (cerebral aqueduct). The metencephalon becomes, among other things, the pons and the cerebellum, the myelencephalon forms the medulla oblongata, and their cavities develop into the fourth ventricle.
Evolution.
Planarians, members of the phylum Platyhelminthes (flatworms), have the simplest, clearly defined delineation of a nervous system into a central nervous system (CNS) and a peripheral nervous system (PNS).
Their primitive brains, consisting of two fused anterior ganglia, and longitudinal nerve cords form the CNS; the laterally projecting nerves form the PNS. A molecular study found that more than 95% of the 116 genes involved in the nervous system of planarians, which includes genes related to the CNS, also exist in humans. Like planarians, vertebrates have a distinct CNS and PNS, though more complex than those of planarians.
The CNS of chordates differs from that of other animals in being placed dorsally in the body, above the gut and notochord/spine. The basic pattern of the CNS is highly conserved throughout the different species of vertebrates and during evolution. The major trend that can be observed is towards a progressive telencephalisation: the telencephalon of reptiles is only an appendix to the large olfactory bulb, while in mammals it makes up most of the volume of the CNS. In the human brain, the telencephalon covers most of the diencephalon and the mesencephalon. Indeed, the allometric study of brain size among different species shows a striking continuity from rats to whales, and allows us to complete the knowledge about the evolution of the CNS obtained through cranial endocasts.
Mammals – which appear in the fossil record after the first fishes, amphibians, and reptiles – are the only vertebrates to possess the evolutionarily recent, outermost part of the cerebral cortex known as the neocortex.
The neocortex of monotremes (the duck-billed platypus and several species of spiny anteaters) and of marsupials (such as kangaroos, koalas, opossums, wombats, and Tasmanian devils) lack the convolutions – gyri and sulci – found in the neocortex of most placental mammals (eutherians).
Within placental mammals, the size and complexity of the neocortex increased over time. The area of the neocortex of mice is only about 1/100 that of monkeys, and that of monkeys is only about 1/10 that of humans. In addition, rats lack convolutions in their neocortex (possibly also because rats are small mammals), whereas cats have a moderate degree of convolutions, and humans have quite extensive convolutions. Extreme convolution of the neocortex is found in dolphins, possibly related to their complex echolocation.
Clinical significance.
Diseases.
There are many central nervous system diseases and conditions, including infections of the central nervous system such as encephalitis and poliomyelitis, early-onset neurological disorders including ADHD and autism, late-onset neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and essential tremor, autoimmune and inflammatory diseases such as multiple sclerosis and acute disseminated encephalomyelitis, genetic disorders such as Krabbe's disease and Huntington's disease, as well as amyotrophic lateral sclerosis and adrenoleukodystrophy. Lastly, cancers of the central nervous system can cause severe illness and, when malignant, can have very high mortality rates.
Specialty professional organizations recommend that neurological imaging of the brain be done only to answer a specific clinical question and not as routine screening.

</doc>
<doc id="7252" url="https://en.wikipedia.org/wiki?curid=7252" title="Cell cycle">
Cell cycle

The cell cycle or cell-division cycle is the series of events that take place in a cell leading to its division and duplication of its DNA (DNA replication) to produce two daughter cells. In bacteria, which lack a cell nucleus, the cell cycle is divided into the B, C, and D periods. The B period extends from the end of cell division to the beginning of DNA replication. DNA replication occurs during the C period. The D period refers to the stage between the end of DNA replication and the splitting of the bacterial cell into two daughter cells. In cells with a nucleus, as in eukaryotes, the cell cycle is also divided into three periods: interphase, the mitotic (M) phase, and cytokinesis. During interphase, the cell grows, accumulating nutrients needed for mitosis, preparing it for cell division and duplicating its DNA. During the mitotic phase, the cell splits itself into two distinct daughter cells. During the final stage, cytokinesis, the new cell is completely divided. To ensure the proper division of the cell, there are control mechanisms known as cell cycle checkpoints.
The cell-division cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of cell division.
Cell cycle phases.
G0 phase.
The word "post-mitotic" is sometimes used to refer to both quiescent and senescent cells. Nonproliferative cells in multicellular eukaryotes generally enter the quiescent G0 state from G1 and may remain quiescent for long periods of time, possibly indefinitely (as is often the case for neurons). This is very common for cells that are fully differentiated. Cellular senescence occurs in response to DNA damage or degradation that would make a cell's progeny nonviable; for example, become cancerous. Some cells enter the G0 phase semi-permanently e.g., some liver, kidney, stomach cells. Many cells do not enter G0 and continue to divide throughout an organism's life, e.g. epithelial cells.
Interphase.
Before a cell can enter cell division, it needs to take in nutrients. All of the preparations are done during interphase. Interphase is a series of changes that takes place in a newly formed cell and its nucleus, before it becomes capable of division again. It is also called preparatory phase or intermitosis. Previously it was called resting stage because there is no apparent activity related to cell division.Typically interphase lasts for at least 90% of the total time required for the cell cycle.
Interphase proceeds in three stages, G1, S, and G2, preceded by the previous cycle of mitosis and cytokinesis. The cell's nuclear chromosomes are duplicated during S phase.
G1 Phase.
The first phase within interphase, from the end of the previous M phase until the beginning of DNA synthesis, is called G1 (G indicating "gap"). It is also called the growth phase. During this phase, the biosynthetic activities of the cell, which are considerably slowed down during M phase, resume at a high rate. The duration of G1 is highly variable, even among different cells of the same species. In this phase, the cell increases its supply of proteins, increases the number of organelles (such as mitochondria, ribosomes), and grows in size.
S Phase.
The ensuing S phase starts when DNA replication commences; when it is completed, all of the chromosomes have been replicated, i.e., each chromosome has two (sister) chromatids. Thus, during this phase, the amount of DNA in the cell has effectively doubled, though the ploidy of the cell remains the same. During this phase, synthesis is completed as quickly as possible due to the exposed base pairs being sensitive to harmful external factors such as mutagens
Mitotic phase.
The relatively brief "M phase" consists of nuclear division (karyokinesis). It is a relatively short period of the cell cycle. M phase is complex and highly regulated. The sequence of events is divided into phases, corresponding to the completion of one set of activities and the start of the next. These phases are sequentially known as:
Mitosis is the process by which a eukaryotic cell separates the chromosomes in its cell nucleus into two identical sets in two nuclei. During the process of mitosis the pairs of chromosomes condense and attach to fibers that pull the sister chromatids to opposite sides of the cell. It is generally followed immediately by cytokinesis, which divides the nuclei, cytoplasm, organelles and cell membrane into two cells containing roughly equal shares of these cellular components. Mitosis and cytokinesis together define the mitotic (M) phase of the cell cycle – the division of the mother cell into two daughter cells, genetically identical to each other and to their parent cell. This accounts for approximately 10% of the cell cycle.
Mitosis occurs exclusively in eukaryotic cells, but occurs in different ways in different species. For example, animals undergo an "open" mitosis, where the nuclear envelope breaks down before the chromosomes separate, while fungi such as "Aspergillus nidulans" and "Saccharomyces cerevisiae" (yeast) undergo a "closed" mitosis, where chromosomes divide within an intact cell nucleus. Prokaryotic cells, which lack a nucleus, divide by a process called binary fission.
Because cytokinesis usually occurs in conjunction with mitosis, "mitosis" is often used interchangeably with "M phase". However, there are many cells where mitosis and cytokinesis occur separately, forming single cells with multiple nuclei in a process called endoreplication. This occurs most notably among the fungi and slime moulds, but is found in various groups. Even in animals, cytokinesis and mitosis may occur independently, for instance during certain stages of fruit fly embryonic development. Errors in mitosis can either kill a cell through apoptosis or cause mutations that may lead to cancer.
Regulation of eukaryotic cell cycle.
Regulation of the cell cycle involves processes crucial to the survival of a cell, including the detection and repair of genetic damage as well as the prevention of uncontrolled cell division. The molecular events that control the cell cycle are ordered and directional; that is, each process occurs in a sequential fashion and it is impossible to "reverse" the cycle.
Role of cyclins and CDKs.
Two key classes of regulatory molecules, cyclins and cyclin-dependent kinases (CDKs), determine a cell's progress through the cell cycle. Leland H. Hartwell, R. Timothy Hunt, and Paul M. Nurse won the 2001 Nobel Prize in Physiology or Medicine for their discovery of these central molecules. Many of the genes encoding cyclins and CDKs are conserved among all eukaryotes, but in general more complex organisms have more elaborate cell cycle control systems that incorporate more individual components. Many of the relevant genes were first identified by studying yeast, especially "Saccharomyces cerevisiae"; genetic nomenclature in yeast dubs many of these genes "cdc" (for "cell division cycle") followed by an identifying number, e.g. "cdc25" or "cdc20".
Cyclins form the regulatory subunits and CDKs the catalytic subunits of an activated heterodimer; cyclins have no catalytic activity and CDKs are inactive in the absence of a partner cyclin. When activated by a bound cyclin, CDKs perform a common biochemical reaction called phosphorylation that activates or inactivates target proteins to orchestrate coordinated entry into the next phase of the cell cycle. Different cyclin-CDK combinations determine the downstream proteins targeted. CDKs are constitutively expressed in cells whereas cyclins are synthesised at specific stages of the cell cycle, in response to various molecular signals.
General mechanism of cyclin-CDK interaction.
Upon receiving a pro-mitotic extracellular signal, G1 cyclin-CDK complexes become active to prepare the cell for S phase, promoting the expression of transcription factors that in turn promote the expression of S cyclins and of enzymes required for DNA replication. The G1 cyclin-CDK complexes also promote the degradation of molecules that function as S phase inhibitors by targeting them for ubiquitination. Once a protein has been ubiquitinated, it is targeted for proteolytic degradation by the proteasome. However, results from a recent study of E2F transcriptional dynamics at the single-cell level argue that the role of G1 cyclin-CDK activities, in particular cyclin D-CDK4/6, is to tune the timing rather than the commitment of cell cycle entry.
Active S cyclin-CDK complexes phosphorylate proteins that make up the pre-replication complexes assembled during G1 phase on DNA replication origins. The phosphorylation serves two purposes: to activate each already-assembled pre-replication complex, and to prevent new complexes from forming. This ensures that every portion of the cell's genome will be replicated once and only once. The reason for prevention of gaps in replication is fairly clear, because daughter cells that are missing all or part of crucial genes will die. However, for reasons related to gene copy number effects, possession of extra copies of certain genes is also deleterious to the daughter cells.
Mitotic cyclin-CDK complexes, which are synthesized but inactivated during S and G2 phases, promote the initiation of mitosis by stimulating downstream proteins involved in chromosome condensation and mitotic spindle assembly. A critical complex activated during this process is a ubiquitin ligase known as the anaphase-promoting complex (APC), which promotes degradation of structural proteins associated with the chromosomal kinetochore. APC also targets the mitotic cyclins for degradation, ensuring that telophase and cytokinesis can proceed.
Specific action of cyclin-CDK complexes.
Cyclin D is the first cyclin produced in the cell cycle, in response to extracellular signals (e.g. growth factors). Cyclin D binds to existing CDK4, forming the active cyclin D-CDK4 complex. Cyclin D-CDK4 complex in turn phosphorylates the retinoblastoma susceptibility protein (Rb). The hyperphosphorylated Rb dissociates from the E2F/DP1/Rb complex (which was bound to the E2F responsive genes, effectively "blocking" them from transcription), activating E2F. Activation of E2F results in transcription of various genes like cyclin E, cyclin A, DNA polymerase, thymidine kinase, etc. Cyclin E thus produced binds to CDK2, forming the cyclin E-CDK2 complex, which pushes the cell from G1 to S phase (G1/S, which initiates the G2/M transition). Cyclin B-cdk1 complex activation causes breakdown of nuclear envelope and initiation of prophase, and subsequently, its deactivation causes the cell to exit mitosis.
A recent quantitative study of E2F transcriptional dynamics at the single-cell level by using engineered fluorescent reporter cells proposed an alternative model for interpreting the control of cell cycle entry. Genes that regulate the amplitude of E2F accumulation, such as Myc, determine the commitment into cell cycle and S phase entry. G1 cyclin-CDK activities are not the driver of cell cycle entry. Instead, they primarily tune the timing of E2F increase, thereby modulating the pace of cell cycle progression.
Inhibitors.
Two families of genes, the "cip/kip" ("CDK interacting protein/Kinase inhibitory protein") family and the INK4a/ARF ("In"hibitor of "K"inase 4/"A"lternative "R"eading "F"rame) family, prevent the progression of the cell cycle. Because these genes are instrumental in prevention of tumor formation, they are known as tumor suppressors.
The "cip/kip" family includes the genes p21, p27 and p57. They halt cell cycle in G1 phase, by binding to, and inactivating, cyclin-CDK complexes. p21 is activated by p53 (which, in turn, is triggered by DNA damage e.g. due to radiation). p27 is activated by Transforming Growth Factor of β (TGF β), a growth inhibitor.
The INK4a/ARF family includes p16INK4a, which binds to CDK4 and arrests the cell cycle in G1 phase, and p14ARF which prevents p53 degradation.
Synthetic inhibitors of Cdc25 could also be useful for the arrest of cell cycle and therefore be useful as antineoplastic and anticancer agents.
Transcriptional regulatory network.
Current evidence suggests that a semi-autonomous transcriptional network acts in concert with the CDK-cyclin machinery to regulate the cell cycle. Several gene expression studies in "Saccharomyces cerevisiae" have identified 800-1200 genes that change expression over the course of the cell cycle. They are transcribed at high levels at specific points in the cell cycle, and remain at lower levels throughout the rest of the cycle. While the set of identified genes differs between studies due to the computational methods and criteria used to identify them, each study indicates that a large portion of yeast genes are temporally regulated.
Many periodically expressed genes are driven by transcription factors that are also periodically expressed. One screen of single-gene knockouts identified 48 transcription factors (about 20% of all non-essential transcription factors) that show cell cycle progression defects. Genome-wide studies using high throughput technologies have identified the transcription factors that bind to the promoters of yeast genes, and correlating these findings with temporal expression patterns have allowed the identification of transcription factors that drive phase-specific gene expression. The expression profiles of these transcription factors are driven by the transcription factors that peak in the prior phase, and computational models have shown that a CDK-autonomous network of these transcription factors is sufficient to produce steady-state oscillations in gene expression).
Experimental evidence also suggests that gene expression can oscillate with the period seen in dividing wild-type cells independently of the CDK machinery. Orlando "et al." used microarrays to measure the expression of a set of 1,271 genes that they identified as periodic in both wild type cells and cells lacking all S-phase and mitotic cyclins ("clb1,2,3,4,5,6"). Of the 1,271 genes assayed, 882 continued to be expressed in the cyclin-deficient cells at the same time as in the wild type cells, despite the fact that the cyclin-deficient cells arrest at the border between G1 and S phase. However, 833 of the genes assayed changed behavior between the wild type and mutant cells, indicating that these genes are likely directly or indirectly regulated by the CDK-cyclin machinery. Some genes that continued to be expressed on time in the mutant cells were also expressed at different levels in the mutant and wild type cells. These findings suggest that while the transcriptional network may oscillate independently of the CDK-cyclin oscillator, they are coupled in a manner that requires both to ensure the proper timing of cell cycle events. Other work indicates that phosphorylation, a post-translational modification, of cell cycle transcription factors by Cdk1 may alter the localization or activity of the transcription factors in order to tightly control timing of target genes.
While oscillatory transcription plays a key role in the progression of the yeast cell cycle, the CDK-cyclin machinery operates independently in the early embryonic cell cycle. Before the midblastula transition, zygotic transcription does not occur and all needed proteins, such as the B-type cyclins, are translated from maternally loaded mRNA.
DNA replication and DNA replication origin activity.
Analyses of synchronized cultures of "Saccharomyces cerevisiae" under conditions that prevent DNA replication initiation without delaying cell cycle progression showed that origin licensing decreases the expression of genes with origins near their 3' ends, revealing that downstream origins can regulate the expression of upstream genes. This confirms previous predictions from mathematical modeling of a global causal coordination between DNA replication origin activity and mRNA expression, and shows that mathematical modeling of DNA microarray data can be used to correctly predict previously unknown biological modes of regulation.
Checkpoints.
Cell cycle checkpoints are used by the cell to monitor and regulate the progress of the cell cycle. Checkpoints prevent cell cycle progression at specific points, allowing verification of necessary phase processes and repair of DNA damage. The cell cannot proceed to the next phase until checkpoint requirements have been met. Checkpoints typically consist of a network of regulatory proteins that monitor and dictate the progression of the cell through the different stages of the cell cycle.
There are several checkpoints to ensure that damaged or incomplete DNA is not passed on to daughter cells. Three main checkpoints exist: the G1/S checkpoint, the G2/M checkpoint and the metaphase (mitotic) checkpoint.
G1/S transition is a rate-limiting step in the cell cycle and is also known as restriction point. This is where the cell checks whether it has enough raw materials to fully replicate its DNA (nucleotide bases, DNA synthase, chromatin, etc.). An unhealthy or malnourished cell will get stuck at this checkpoint.
The G2/M checkpoint is where the cell ensures that it has enough cytoplasm and phospholipids for two daughter cells. But sometimes more importantly, it checks to see if it is the right time to replicate. There are some situations where many cells need to all replicate simultaneously (for example, a growing embryo should have a symmetric cell distribution until it reaches the mid-blastula transition). This is done by controlling the G2/M checkpoint.
The metaphase checkpoint is a fairly minor checkpoint, in that once a cell is in metaphase, it has committed to undergoing mitosis. However that's not to say it isn't important. In this checkpoint, the cell checks to ensure that the spindle has formed and that all of the chromosomes are aligned at the spindle equator before anaphase begins.
While these are the three "main" checkpoints, not all cells are have to pass through each of these checkpoints in this order to replicate. Many types of cancer are caused by mutations that allow the cells to speed through the various checkpoints or even skip them altogether. Going from S to M to S phase almost consecutively. Because these cells have lost their checkpoints, any DNA mutations that may have occurred are disregarded and passed on to the daughter cells. This is one reason why cancer cells have a tendency to exponentially accrue mutations. Aside from cancer cells, many fully differentiated cell types no longer replicate so they leave the cell cycle and stay in G0 until their death. Thus removing the need for cellular checkpoints. An alternative model of the cell cycle response to DNA damage has also been proposed, known as the postreplication checkpoint.
Checkpoint regulation plays an important role in an organism's development. In sexual reproduction, when egg fertilization occurs, when the sperm binds to the egg, it releases signalling factors that notify the egg that it has been fertilized. Among other things, this induces the now fertilized oocyte to return from its previously dormant, G0, state back into the cell cycle and on to mitotic replication and division.
p53 plays an important role in triggering the control mechanisms at both G1/S and G2/M checkpoints. In addition to p53, checkpoint regulators are being heavily researched for their roles in cancer growth and proliferation.
Role in tumor formation.
A disregulation of the cell cycle components may lead to tumor formation. As mentioned above, when some genes like the cell cycle inhibitors, RB, p53 etc. mutate, they may cause the cell to multiply uncontrollably, forming a tumor. Although the duration of cell cycle in tumor cells is equal to or longer than that of normal cell cycle, the proportion of cells that are in active cell division (versus quiescent cells in G0 phase) in tumors is much higher than that in normal tissue. Thus there is a net increase in cell number as the number of cells that die by apoptosis or senescence remains the same.
The cells which are actively undergoing cell cycle are targeted in cancer therapy as the DNA is relatively exposed during cell division and hence susceptible to damage by drugs or radiation. This fact is made use of in cancer treatment; by a process known as debulking, a significant mass of the tumor is removed which pushes a significant number of the remaining tumor cells from G0 to G1 phase (due to increased availability of nutrients, oxygen, growth factors etc.). Radiation or chemotherapy following the debulking procedure kills these cells which have newly entered the cell cycle.
The fastest cycling mammalian cells in culture, crypt cells in the intestinal epithelium, have a cycle time as short as 9 to 10 hours. Stem cells in resting mouse skin may have a cycle time of more than 200 hours. Most of this difference is due to the varying length of G1, the most variable phase of the cycle. M and S do not vary much.
In general, cells are most radiosensitive in late M and G2 phases and most resistant in late S.
For cells with a longer cell cycle time and a significantly long G1 phase, there is a second peak of resistance late in G1.
The pattern of resistance and sensitivity correlates with the level of sulfhydryl compounds in the cell. Sulfhydryls are natural substances that protect cells from radiation damage and tend to be at their highest levels in S and at their lowest near mitosis.

</doc>
<doc id="7253" url="https://en.wikipedia.org/wiki?curid=7253" title="Cartesian">
Cartesian

Cartesian means of or relating to the French philosopher René Descartes—from his Latinized name "Cartesius". It may refer to:

</doc>
<doc id="7255" url="https://en.wikipedia.org/wiki?curid=7255" title="Connection (dance)">
Connection (dance)

In partner dancing, connection is a physical communication method used by a pair of dancers to facilitate synchronized dance movement, in which one dancer (the "lead") directs the movements of the other dancer (the "follower") by means of non-verbal directions conveyed through a physical connection between the dancers. It is an essential technique in many types of partner dancing and is used extensively in partner dances that feature significant physical contact between the dancers, including the Argentine Tango, Lindy Hop, Balboa, East Coast Swing, West Coast Swing, Salsa, and Modern Jive.
Other forms of communication, such as visual cues or spoken cues, sometimes aid in connecting with one's partner, but are often used in specific circumstances (e.g., practicing figures, or figures which are purposely danced without physical connection). Connection can be used to transmit power and energy as well as information and signals; some dance forms (and some dancers) primarily emphasize power or signaling, but most are probably a mixture of both.
Following and leading in a partner dance is accomplished by maintaining a physical connection called the frame that allows the leader to transmit body movement to the follower, and for the follower to suggest ideas to the leader. A frame is a stable structural combination of both bodies maintained through the dancers' arms and/or legs.
Connection occurs in both open and closed dance positions (also called "open frame" and "closed frame").
In closed position with body contact, connection is achieved by maintaining the frame. The follower moves to match the leader, maintaining the pressure between the two bodies as well as the position.
When creating frame, tension is the primary means of establishing communication. Changes in tension are made to create rhythmic variations in moves and movements, and are communicated through points of contact. In an open position or a closed position without body contact, the hands and arms alone provide the connection, which may be one of three forms: tension, compression or neutral.
In swing dances, tension and compression may be maintained for a significant period of time. In other dances, such as Latin, tension and compression may be used as indications of upcoming movement. However, in both styles, tension and compression do not signal immediate movement: the follow must be careful not to move prior to actual movement by the lead. Until then, the dancers must match pressures without moving their hands. In some styles of Lindy Hop, the tension may become quite high without initiating movement.
The general rule for open connections is that moves of the leader's hands back, forth, left or right are originated through moves of the entire body. Accordingly, for the follower, a move of the connected hand is immediately transformed into the corresponding move of the body. Tensing the muscles and locking the arm achieves this effect but is neither comfortable nor correct. Such tension eliminates the subtler communication in the connection, and eliminates free movement up and down, such as is required to initiate many turns.
Instead of just tensing the arms, connection is achieved by engaging the shoulder, upper body and torso muscles. Movement originates in the body's core. A leader leads by moving himself and maintaining frame and connection. Different forms of dance and different movements within each dance may call for differences in the connection. In some dances the separation distance between the partners remains pretty constant. In others e.g. Modern Jive moving closer together and further apart are fundamental to the dance, requiring flexion and extension of the arms, alternating compression and tension.
The connection between two partners has a different feel in every dance and with every partner. Good social dancers adapt to the conventions of the dance and the responses of their partners.

</doc>
<doc id="7257" url="https://en.wikipedia.org/wiki?curid=7257" title="Caste">
Caste

Caste is a form of social stratification characterized by endogamy, hereditary transmission of a lifestyle which often includes an occupation, ritual status in a hierarchy and customary social interaction and exclusion based on cultural notions of purity and pollution. Its paradigmatic ethnographic example is the division of Indian society into rigid social groups, with roots in India's ancient history and persisting until today. However, the economic significance of the caste system in India has been declining as a result of urbanization and affirmative action programs. A subject of much scholarship by sociologists and anthropologists, the Indian caste system is sometimes used as an analogical basis for the study of caste-like social divisions existing outside India. The term is also applied to non-human populations.
Etymology.
The English word "caste" derives from the Spanish and Portuguese "casta", which the "Oxford English Dictionary" quotes John Minsheu's Spanish dictionary (1599) to mean, "race, lineage, or breed". When the Spanish colonized the New World, they used the word to mean a "clan or lineage." However, it was the Portuguese who employed "casta" in the primary modern sense when they applied it to the thousands of in-marrying hereditary Indian social groups they encountered upon their arrival in India in 1498. The use of the spelling "caste," with this latter meaning, is first attested to in English in 1613.
In South Asia.
India.
Historically, India has never had a caste system. What existed were, 'Varna' and Jati. Varna was a system of classification by profession and was fluid. There was no rigidity. Jaati referred to the ethnic origin of the different Indian people. When the Europeans came to India they applied the Portuguese term 'caste' to describe the Varna system of Idia and further linked it to jaati which made the system rigid. Since Varna could change, but jaati could not, by linking the two, the system acquired rigidity and gradually the belief arose that caste system originated in India. The truth is that social stratification has existed in all societies. 
As per western thought, the caste system in India has consisted of thousands of endogamous groups called "Jatis" or "Quoms" (among Muslims). Starting with the British colonial Census of 1901 led by Herbert Hope Risley, all the Jatis were grouped under the theoretical "varnas" categories. According to political scientist Lloyd Rudolph, Risley believed that "varna", however ancient, could be applied to all the modern castes found in India, and " meant to identify and place several hundred million Indians within it." The terms "varna" (conceptual classification based on occupation) and "jāti" (caste) are two distinct concepts: while varna is the idealised four-part division envisaged by the Twice-Borns, jāti (community) refers to the thousands of actual endogamous groups prevalent across the subcontinent. The classical authors scarcely speak of anything other than the varnas, as it provided a convenient shorthand; but a problem arises when even Indologists, most of whom are non-Indians, sometimes confuse the two.
Independent India has witnessed caste-related violence. In 2005 government statistics recorded approximately 110,000 cases of reported violent acts, including rape and murder, committed against Dalits ( A more recent data of year 2012 may be used to replace outdated 2007 data: The National Crime Records Bureau (NCRB) records crimes against scheduled castes and scheduled tribes - the most disadvantaged groups - in a separate category. These crimes are grievously under-reported, but even so the figures for 2012 are revealing: 651 cases of murder, 3,855 cases where people were hurt, 1,576 cases of rape, 490 cases of kidnapping and abduction, and 214 cases of arson. Ref http://www.bbc.com/news/world-asia-india-27774908 )
The economic significance of the caste system in India has been declining as a result of urbanization and affirmative action programs. Upon independence from the British rule, the Indian Constitution listed 1,108 castes across the country as Scheduled Castes in 1950, for positive discrimination. The Untouchable communities are sometimes called "Dalit" or "Harijan" in contemporary literature. In 2001, the proportion of Dalit population was 16.2 percent of India's total population. The majority of the 15 million bonded child workers in India are from the lowest castes.
Nepal.
The Nepalese caste system resembles that of the Indian Jāti system with numerous Jāti divisions with a Varna system superimposed for a rough equivalence. But since the culture and the society is different some of the things are different. Inscriptions attest the beginnings of a caste system during the Lichchhavi period. Jayasthiti Malla (1382–95) categorized Newars into 64 castes (Gellner 2001). A similar exercise was made during the reign of Mahindra Malla (1506–75). The Hindu social code was later set up in Gorkha by Ram Shah (1603–36).
Pakistan.
Religious, historical and sociocultural factors have helped define the bounds of endogamy for Muslims in some parts of Pakistan. There is a preference for endogamous marriages based on the clan-oriented nature of the society, which values and actively seeks similarities in social group identity based on several factors, including religious, sectarian, ethnic, and tribal/clan affiliation. Religious affiliation is itself multilayered and includes religious considerations other than being Muslim, such as sectarian identity (e.g. Shia or Sunni, etc.) and religious orientation within the sect (Isnashari, Ismaili, Ahmedi, etc.).
Both ethnic affiliation (e.g. Pathan, Sindhi, Baloch, Punjabi, etc.) and membership of specific biraderis or zaat/quoms are additional integral components of social identity. Within the bounds of endogamy defined by the above parameters, close consanguineous unions are preferred due to a congruence of key features of group- and individual-level background factors as well as affinities. McKim Marriott claims a social stratification that is hierarchical, closed, endogamous and hereditary is widely prevalent, particularly in western parts of Pakistan. Frederik Barth in his review of this system of social stratification in Pakistan suggested that these are castes.
Sri Lanka.
The caste system in Sri Lanka is a division of society into strata, influenced by the textbook Varnas and Jāti system found in India. Ancient Sri Lankan texts such as the Pujavaliya, Sadharmaratnavaliya and Yogaratnakaraya and inscriptional evidence show that the above hierarchy prevailed throughout the feudal period. The repetition of the same caste hierarchy even as recently as the 18th century, in the British/Kandyan period Kadayimpoth - Boundary books as well, indicates the continuation of the tradition right up to the end of Sri Lanka's monarchy.
Caste-like stratification outside South Asia.
Southeast Asia.
Indonesia.
Balinese caste structure has been described in early 20th-century European literature to be based on three categories – triwangsa (thrice born) or the nobility, dwijati (twice born) in contrast to ekajati (once born) the low folks. Four statuses were identified in these sociological studies, spelled a bit differently from the caste categories for India:
The Brahmana caste was further subdivided by these Dutch ethnographers into two: Siwa and Buda. The Siwa caste was subdivided into five – Kemenuh, Keniten, Mas, Manuba and Petapan. This classification was to accommodate the observed marriage between higher caste Brahmana men with lower caste women. The other castes were similarly further sub-classified by these 19th-century and early-20th-century ethnographers based on numerous criteria ranging from profession, endogamy or exogamy or polygamy, and a host of other factors in a manner similar to "castas" in Spanish colonies such as Mexico, and caste system studies in British colonies such as India.
East Asia.
China and Mongolia.
During the period of Yuan Dynasty, ruler Kublai Khan enforced a "Four Class System", which was a legal caste system. The order of four classes of people was maintained by the information of the descending order were:-
Some scholars notes that it was a kind of psychological indication that the earlier they submitted to Mongolian people, the higher social status they would have. The 'Four Class System' and its people received different treatment in political, legal, and military affairs.
Today, the Hukou system is considered by various sources as the current caste system of China.
There is also significant controversy over the social classes of Tibet, especially with regards to the serfdom in Tibet controversy.
Japan.
In Japan's history, social strata based on inherited position rather than personal merits, was rigid and highly formalized. At the top were the Emperor and Court nobles (kuge), together with the Shogun and daimyo. Below them the population was divided into four classes in a system known as "mibunsei" (身分制). These were: samurai, peasants, craftsmen and merchants. Only the samurai class was allowed to bear arms. A samurai had a right to kill any peasants and other craftsmen and merchants whom he felt were disrespectful. Craftsmen produced products, being the third, and the last merchants were thought to be as the meanest class because they did not produce any products. The castes were further sub-divided; for example, the peasant caste were labelled as "furiuri", "tanagari", "mizunomi-byakusho" amongst others. The castes and sub-classes, as in Europe, were from the same race, religion and culture.
Howell, in his review of Japanese society notes that if a Western power had colonized Japan in the 19th century, they would have discovered and imposed a rigid four-caste hierarchy in Japan.
De Vos and Wagatsuma observe that a systematic and extensive caste system was part of the Japanese society. They also discuss how alleged caste impurity and alleged racial inferiority, concepts often quickly assumed to be slightly different, are superficial terms, two faces of identical inner psychological processes, which expressed themselves in Japan and other countries of the world.
Endogamy was common because marriage across caste lines was socially unacceptable.
Japan had its own untouchable caste, shunned and ostracized, historically referred to by the insulting term "Eta", now called "Burakumin". While modern law has officially abolished the class hierarchy, there are reports of discrimination against the Buraku or Burakumin underclasses. The Burakumin are regarded as "ostracised." The burakumin are one of the main minority groups in Japan, along with the Ainu of Hokkaidō and those of residents of Korean and Chinese descent.
Korea.
The baekjeong (백정) were an “untouchable” outcaste group of Korea. The meaning today is that of butcher. They originate from the Khitan invasion of Korea in the 11th century. The defeated Khitans who had surrendered were settled in isolated communities throughout Goryeo to forestall rebellion. They were valued for their skills in hunting, herding, butchering, and making of leather, common skill sets among nomads. Over time their ethnic origin was forgotten, and they formed the bottom layer of Korean society. It was legally abolished in Korea in 1894 but remained extant in reality until 1930.
In 1392, with the foundation of the Confucian Joseon dynasty, Korea systemised its own native class system. At the top were the two official classes, the Yangban, which literally means "two classes." It was composed of scholars (Munban) and warriors (Muban). Within the Yangban class, the Scholars (Munban) enjoyed a significant social advantage over the warrior (Muban) class.
Beneath the Yangban class were the "Jung-in" (중인-中人: literally "middle people"). They were the technicians. This class was small and specialized in fields such as medicine, accounting, translators, regional bureaucrats, etc.
Beneath the Jung-in were the "Sangmin" (상민-常民: literally 'commoner'). These were independent farmers working their own fields.
Korea had a very large serf population, "nobi", ranging from a third to half of the entire population for most of the millennium between the Silla period and the Joseon Dynasty.
The opening of Korea to foreign Christian missionary activity in the late 19th century saw some improvement in the status of the "baekjeong"; however, everyone was not equal under the Christian congregation, and protests erupted when missionaries attempted to integrate them into worship services, with non-"baekjeong" finding such an attempt insensitive to traditional notions of hierarchical advantage. Also around the same time, the baekjeong began to resist the open social discrimination that existed against them. They focused on social and economic injustices affecting the baekjeong, hoping to create an egalitarian Korean society. Their efforts included attacking social discrimination by the upper class, authorities, and "commoners," and the use of degrading language against children in public schools.
With the Gabo reform of 1896, the class system of Korea was officially abolished. Following the collapse of the Gabo government, the new cabinet, which became the Gwangmu government after the establishment of the Korean Empire, introduced systematic measures for abolishing the traditional class system. One measure was the new household registration system, reflecting the goals of formal social equality, which was implemented by the loyalists’ cabinet. Whereas the old registration system signified household members according to their hierarchical social status, the new system called for an occupation.
While most Koreans by that time had surnames and even bongwan, although still substantial number of cheonmin, mostly consisted of serfs and slaves, and untouchables did not. According to the new system, they were then required to fill in the blanks for surname in order to be registered as constituting separate households. Instead of creating their own family name, some cheonmins appropriated their masters’ surname, while others simply took the most common surname and its bongwan in the local area. Along with this example, activists within and outside the Korean government had based their visions of a new relationship between the government and people through the concept of citizenship, employing the term "Inmin"(people) and later, "Kungmin"(citizen).
North Korea.
Committee for Human Rights in North Korea reported that "Every North Korean citizen is assigned a heredity-based class and socio-political rank over which the individual exercises no control but which determines all aspects of his or her life." Regarded as Songbun, Barbara Demick describes this "class structure" as an updating of the hereditary "caste system", combining Confucianism and Stalinism. She claims that a bad family background is called "tainted blood", and that by law this "tainted blood" lasts for three generations.
Tibet.
See: Social classes of Tibet
Heidi Fjeld has put forth the argument that pre-1950s Tibetan society was functionally a caste system, in contrast to previous scholars who defined the Tibetan social class system as similar to European feudal serfdom, as well as non-scholarly western accounts which seek to romanticize a supposedly 'egalitarian' ancient Tibetan society.
West Asia.
Yezidi society is hierarchical. The secular leader is a hereditary emir or prince, whereas a chief sheikh heads the religious hierarchy. The Yazidi are strictly endogamous; members of the three Yazidi castes, the murids, sheikhs and pirs, marry only within their group.
Iran.
Sassanid society was immensely complex, with separate systems of social organization governing numerous different groups within the empire. Historians believe society comprised four
social classes:
Yemen.
In Yemen there exists a hereditary caste, the African-descended Al-Akhdam who are kept as perennial manual workers. Estimates put their number at over 3.5 million residents who are discriminated, out of a total Yemeni population of around 22 million.
Africa.
Various sociologists have reported caste systems in Africa. The specifics of the caste systems have varied in ethnically and culturally diverse Africa, however the following features are common - it has been a closed system of social stratification, the social status is inherited, the castes are hierarchical, certain castes are shunned while others are merely endogamous and exclusionary. In some cases, concepts of purity and impurity by birth have been prevalent in Africa. In other cases, such as the "Nupe" of Nigeria, the "Beni Amer" of East Africa, and the "Tira" of Sudan, the exclusionary principle has been driven by evolving social factors.
West Africa.
Among the Igbo of Nigeria - especially Enugu, Anambra, Imo, Abia, Ebonyi, Edo and Delta states of the country - Obinna finds Osu caste system has been and continues to be a major social issue. The Osu caste is determined by one's birth into a particular family irrespective of the religion practised by the individual. Once born into Osu caste, this Nigerian person is an outcast, shunned and ostracized, with limited opportunities or acceptance, regardless of his or her ability or merit. Obinna discusses how this caste system-related identity and power is deployed within government, Church and indigenous communities.
The "osu" class systems of eastern Nigeria and southern Cameroon are derived from indigenous religious beliefs and discriminate against the "Osus" people as "owned by deities" and outcasts.
The Songhai economy was based on a caste system. The most common were metalworkers, fishermen, and carpenters. Lower caste participants consisted of mostly non-farm working immigrants, who at times were provided special privileges and held high positions in society. At the top were noblemen and direct descendants of the original Songhai people, followed by freemen and traders.
In a review of social stratification systems in Africa, Richter reports that the term caste has been used by French and American scholars to many groups of West African artisans. These groups have been described as inferior, deprived of all political power, have a specific occupation, are hereditary and sometimes despised by others. Richter illustrates caste system in Ivory Coast, with six sub-caste categories. Unlike other parts of the world, mobility is sometimes possible within sub-castes, but not across caste lines. Farmers and artisans have been, claims Richter, distinct castes. Certain sub-castes are shunned more than others. For example, exogamy is rare for women born into families of woodcarvers.
Similarly, the Mandé societies in Gambia, Ghana, Guinea, Ivory Coast, Liberia, Senegal and Sierra Leone have social stratification systems that divide society by ethnic ties. The Mande class system regards the "jonow" slaves as inferior. Similarly, the Wolof in Senegal is divided into three main groups, the "geer" (freeborn/nobles), "jaam" (slaves and slave descendants) and the underclass "neeno". In various parts of West Africa, Fulani societies also have class divisions. Other castes include "Griots", "Forgerons", and "Cordonniers".
Tamari has described endogamous castes of over fifteen West African peoples, including the Tukulor, Songhay, Dogon, Senufo, Minianka, Moors, Manding, Soninke, Wolof, Serer, Fulani, and Tuareg. Castes appeared among the "Malinke" people no later than 14th century, and was present among the "Wolof" and "Soninke", as well as some "Songhay" and "Fulani" populations, no later than 16th century. Tamari claims that wars, such as the "Sosso-Malinke" war described in the "Sunjata" epic, led to the formation of blacksmith and bard castes among the people that ultimately became the Mali empire.
As West Africa evolved over time, sub-castes emerged that acquired secondary specializations or changed occupations. Endogamy was prevalent within a caste or among a limited number of castes, yet castes did not form demographic isolates according to Tamari. Social status according to caste was inherited by off-springs automatically; but this inheritance was paternal. That is, children of higher caste men and lower caste or slave concubines would have the caste status of the father.
Central Africa.
Ethel M. Albert in 1960 claimed that the societies in Central Africa were caste-like social stratification systems. Similarly, in 1961, Maquet notes that the society in Rwanda and Burundi can be best described as castes. The Tutsi, noted Maquet, considered themselves as superior, with the more numerous Hutu and the least numerous Twa regarded, by birth, as respectively, second and third in the hierarchy of Rwandese society. These groups were largely endogamous, exclusionary and with limited mobility. Maquet's theories have been controversial.
Horn of Africa.
In a review published in 1977, Todd reports that numerous scholars report a system of social stratification in different parts of Africa that resembles some or all aspects of caste system. Examples of such caste systems, he claims, are to be found in Ethiopia in communities such as the Gurage and Konso. He then presents the Dime of Southwestern Ethiopia, amongst whom there operates a system which Todd claims can be unequivocally labelled as caste system. The Dime have seven castes whose size varies considerably. Each broad caste level is a hierarchical order that is based on notions of purity, non-purity and impurity. It uses the concepts of defilement to limit contacts between caste categories and to preserve the purity of the upper castes. These caste categories have been exclusionary, endogamous and the social identity inherited. Alula Pankhurst has published a study of caste groups in SW Ethiopia.
Among the Kafa, there were also traditionally groups labeled as castes. "Based on research done before the Derg regime, these studies generally presume the existence of a social hierarchy similar to the caste system. At the top of this hierarchy were the Kafa, followed by occupational groups including blacksmiths (Qemmo), weavers (Shammano), bards (Shatto), potters, and tanners (Manno). In this hierarchy, the Manjo were commonly referred to as hunters, given the lowest status equal only to slaves."
The Borana Oromo of southern Ethiopia in the Horn of Africa also have a class system, wherein the Wata, an acculturated hunter-gatherer group, represent the lowest class. Though the Wata today speak the Oromo language, they have traditions of having previously spoken another language before adopting Oromo.
The traditionally nomadic Somali people are divided into clans, wherein the Rahanweyn agro-pastoral clans and the occupational clans such as the Madhiban were traditionally sometimes treated as outcasts. As Gabboye, the Madhiban along with the Yibir and Tumaal (collectively referred to as "sab") have since obtained political representation within Somalia, and their general social status has improved with the expansion of urban centers.
Europe.
France and Spain.
For centuries, through the modern times, the majority regarded Cagots of western France and northern Spain as an inferior caste, the untouchables. While they had the same skin color and religion as the majority, in the churches they had to use segregated doors, drink from segregated fonts, and receive communion on the end of long wooden spoons. It was a closed social system. The socially isolated Cagots were endogamous, and chances of social mobility non-existent.

</doc>
<doc id="7258" url="https://en.wikipedia.org/wiki?curid=7258" title="Creation">
Creation

Creation may refer to:

</doc>
<doc id="7262" url="https://en.wikipedia.org/wiki?curid=7262" title="Coral 66">
Coral 66

CORAL (Computer On-line Real-time Applications Language) is a programming language originally developed in 1964 at the Royal Radar Establishment (RRE), Malvern, UK, as a subset of JOVIAL. Coral 66 was subsequently developed by I. F. Currie and M. Griffiths under the auspices of IECCA (Inter-Establishment Committee for
Computer Applications). Its official definition, edited by Woodward, Wetherall and Gorman, was first published in 1970.
Overview.
Coral 66 is a general-purpose programming language based on ALGOL 60, with some features from Coral 64, JOVIAL, and FORTRAN. It includes structured record types (as in Pascal) and supports the packing of data into limited storage (also as in Pascal). Like Edinburgh IMP it allows embedded assembler, and also offers good run-time checking and diagnostics. It is specifically intended for real-time and embedded applications and for use on computers with limited processing power, including those limited to fixed point arithmetic and those without support for dynamic storage allocation.
The language was an inter-service standard for British military programming, and was also widely adopted for civil purposes in the British control and automation industry. It was used to write software for both the Ferranti and GEC computers from 1971 onwards. Implementations also exist for the Interdata 8/32, PDP-11, VAX, Alpha platforms and HP Integrity servers; for the Honeywell, and for the Computer Technology Limited (CTL, later ITL) Modular-1; as well as for SPARC running Solaris and Intel running Linux.
A variant of Coral 66 was developed during the late 1970s/early 1980s by the British GPO, in conjunction with GEC, STC and Plessey, for use on the System X digital telephone exchange control computers, known as PO-CORAL. This was later renamed BT-CORAL when British Telecom was spun off from the Post Office. Unique features of this language were the focus on real-time execution, message processing, limits on statement execution between waiting for input, and a prohibition on recursion to remove the need for a stack.
As Coral was aimed at a variety of real-time work, rather than general office DP, there was no standardised equivalent to a stdio library. IECCA recommended a primitive I/O package to accompany any compiler (in a document titled "Input/Output of Character data in Coral 66 Utility Programs"). Most implementers avoided this by producing Coral interfaces to existing Fortran and, later, C libraries.
Perhaps CORAL's most significant contribution to computing was the enforcement of quality control in commercial compilers. To have a CORAL compiler approved by IECCA, and thus allowing a compiler to be marketed as a CORAL 66 compiler, the candidate compiler had to compile and execute an official suite of 25 test programs and 6 benchmark programs. The process was part of the BS5905 approval process. This methodology was observed and adapted later by DoD for the official certification of Ada compilers.
Source code for a Coral 66 compiler (written in BCPL) has been recovered and the "Official Definition of Coral 66" document by HMSO has been scanned; the Ministry of Defence patent office has issued a licence to the Edinburgh Computer History project to allow them to put both the code and the language reference online for non-commercial use.

</doc>
