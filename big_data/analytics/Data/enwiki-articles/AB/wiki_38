<doc id="9051" url="https://en.wikipedia.org/wiki?curid=9051" title="Drop kick">
Drop kick

A drop kick is a type of kick in various codes of football. It involves a player dropping the ball and then kicking it when it bounces off the ground.
Drop kicks are most importantly used as a method of restarting play and scoring points in rugby union and rugby league. Association football goalkeepers also often return the ball to play with drop kicks. The kick was once in wide use in both Australian rules football and gridiron football, but is today rarely seen in either sport.
Rugby.
Drop kick technique.
The drop kick technique in rugby codes is usually to hold the ball with one end pointing downwards in two hands above the kicking leg. The ball is dropped onto the ground in front of the kicking foot, which makes contact at the moment or fractionally after the ball touches the ground, called the "half-volley". The kicking foot usually makes contact with the ball slightly on the instep.
In a rugby union kick-off, or drop out, the kicker usually aims to kick the ball high but not a great distance, and so usually strikes the ball after it has started to bounce off the ground, so the contact is made close to the bottom of the ball.
Rugby league.
In rugby league, drop kicks are mandatory to restart play from the goal line (called a goal line drop-out) after the defending team is tackled or knocks on in the in-goal area or the defending team causes the ball to go dead or into touch-in-goal. Drop kicks are also mandatory to restart play from the 20 metre line after an unsuccessful penalty goal attempt goes dead or into touch-in-goal and to score a drop goal (sometimes known as a field goal) in open play, which is worth one point.
Drop kicks are optional for a penalty kick to score a penalty goal (this being done rarely, as place kicks are generally used) and when kicking for touch (the sideline) from a penalty, although the option of a punt kick is usually taken instead.
Rugby union.
In rugby union, a drop kick is used for the kick-off and restarts and to score a drop goal (sometimes called a field goal). Originally, it was one of only two ways to score points, along with the place kick.
Drop kicks are mandatory from the centre spot to start a half (a kick-off), from the centre spot to restart the game after points have been scored, to restart play from the 22-metre line (called a drop-out) after the ball is touched down or made dead in the in-goal area by the defending team when the attacking team kicked or took the ball into the in-goal area, and to score a drop goal (sometimes called a field goal) in open play, which is worth three points.
Drop kicks are optional for a conversion kick after a try has been scored. This is rare, as place kicks are almost always used for the conversion; a drop kick is sometimes used late in a game if the scoring team needs to score again quickly, and taking a place kick would be slower. Also, if a gust of wind blows the ball over on a place kick attempt after the kicker has begun his run-up, thus allowing the opposing team to begin a charge down, then there is no time to reset the ball, and the kicker may attempt a quick drop kick. Drop kicks are also optional for a penalty kick to score a penalty goal. This is rare, as place kicks are almost always used. When kicking for touch (the sideline) from a penalty, a drop kick may be used. This is rare, as the option of a punt kick is almost always taken instead.
Rugby sevens.
The usage of drop kicks in rugby sevens is the same as in rugby union, except that drop kicks are used for all conversion attempts and for penalty kicks, both of which must be taken within 40 seconds of the try being scored or the award of the penalty.
American and Canadian (gridiron) football.
In both American and Canadian football, one method of scoring a field goal or extra point is by drop-kicking the football through the goal.
It contrasts to a punt, wherein the player kicks the ball without letting it hit the ground first; and a placekick, wherein the player kicks a stationary ball off the ground: "from placement". A drop kick is significantly more difficult; as Jim Thorpe once explained, "I regard the place kick as almost two to one safer than the drop kick in attempting a goal from the field."
The drop kick was often used in early football as a surprise tactic. The ball would be snapped or lateraled to a back, who would fake a run or pass, but then would kick the field goal instead. This method of scoring worked well in the 1920s and early 1930s, when the football was rounder at the ends (similar to a modern rugby ball). Early football stars such as Charles Brickley, Frank Hudson, Jim Thorpe, Paddy Driscoll, and Al Bloodgood were skilled drop-kickers; Driscoll in 1925 and Bloodgood in 1926 hold a tied NFL record of four drop kicked field goals in a single game. Driscoll's 55 yard drop kick in 1924 stood as the unofficial record for field goal range until Bert Rechichar kicked a 56-yard field goal (by placekick) in 1953.
In 1934, the ball was made more pointed at the ends. This made passing the ball easier, as was its intent, but made the drop kick obsolete, as the more pointed ball did not bounce up from the ground reliably. The drop kick was supplanted by the place kick, which cannot be attempted out of a formation generally used as a running or passing set. The drop kick remains in the rules, but is seldom seen, and rarely effective when attempted.
In Canadian football the drop kick can be taken from any point on the field, unlike placekicks which must be attempted behind the line of scrimmage.
NFL.
Before the NFL–AFL merger, the last successful drop kick in the NFL was executed by Scooter McLean of the Chicago Bears in their 37–9 victory over the New York Giants on December 21, 1941, in the NFL Championship game at Chicago's Wrigley Field. Though it was not part of the NFL at the time, the All-America Football Conference saw its last drop kick November 28, 1948, when Joe Vetrano of the San Francisco 49ers drop kicked an extra point after a muffed snap against the Cleveland Browns.
The only successful drop kick in the NFL since the 1940s was by Doug Flutie, the backup quarterback of the New England Patriots, against the Miami Dolphins on January 1, 2006, for an extra point after a touchdown. Flutie had estimated "an 80 percent chance" of making the drop kick, which was called to give Flutie, 43 at the time, the opportunity to make a historic kick in his final NFL game; the drop kick was his last play in the NFL. After the game, New England coach Bill Belichick said, "I think Doug deserves it," and Flutie said, "I just thanked him for the opportunity."
Dallas Cowboys punter Mat McBriar attempted a maneuver similar to a drop kick during the 2010 Thanksgiving Day game after a botched punt attempt, but the ball bounced several times before the kick and the sequence of events is officially recorded as a fumble, followed by an illegal kick, with the fumble being recovered by the New Orleans Saints 29 yards downfield from the spot of the kick. The Saints declined the illegal kick penalty.
New England Patriots kicker Stephen Gostkowski attempted an onside drop kick on a free kick after a safety against the Pittsburgh Steelers on October 30, 2011. The kick went out of bounds.
New Orleans Saints quarterback Drew Brees, a former teammate of Flutie's, executed a drop kick late on an extra point attempt in the fourth quarter of the 2012 Pro Bowl; the kick fell short.
New England Patriots special teams player Nate Ebner attempted an onside drop kick on a kickoff after a Patriots touchdown against the Philadelphia Eagles on December 6, 2015. The kick was recovered by the Eagles at their own 41 yard line. Two weeks later, on December 20, Buffalo Bills punter Colton Schmidt executed what is believed to be an unintentional drop kick after a botched punt against the Washington Redskins; because the Redskins recovered the kick, it was treated as a punt (and not as a field goal attempt, which would have pushed the ball back to the spot of the kick).
NCAA.
The last successful drop kick extra point in the NCAA was by Jason Millgan of Hartwick College on December 11, 1998, St. Lawrence University.
Canadian football.
In the Canadian game, the drop kick can be attempted at any time by either team. Any player on the kicking team behind the kicker, and including the kicker, can recover the kick. When a drop kick goes out of bounds, possession on the next scrimmage goes to the non-kicking team.
On September 8, 1974, Tom Wilkinson, quarterback for the Edmonton Eskimos, unsuccessfully attempted a drop kick field goal in the final seconds of a 24–2 romp over the Winnipeg Blue Bombers. This may have been the last time the play was deliberately attempted in the CFL.
During one game in the 1980s, Hamilton Tiger-Cats wide receiver Earl Winfield was unable to field a punt properly; in frustration, he kicked the ball out of bounds. The kick was considered a drop kick and led to a change of possession, with the punting team regaining possession of the ball.
Arena football.
In Arena football, a drop-kicked extra point counts for two points rather than one and a drop-kicked field goal counts for four points rather than three. The most recent conversion of a drop kick was by Geoff Boyer of the Pittsburgh Power on June 16, 2012; it was the first successful conversion in the Arena Football League since 1997.
Australian rules football.
Once the preferred method of conveying the ball over long distances, the drop-kick has been superseded by the drop punt as a more accurate means of delivering the ball to a fellow player. It is, therefore, no longer used in the game as it is considered too unpredictable.

</doc>
<doc id="9053" url="https://en.wikipedia.org/wiki?curid=9053" title="Diaeresis">
Diaeresis

Diaeresis (dieresis, diëresis) may refer to:

</doc>
<doc id="9055" url="https://en.wikipedia.org/wiki?curid=9055" title="Derry">
Derry

Derry (), officially Londonderry (), is the second-largest city in Northern Ireland and the fourth-largest city on the island of Ireland. The name Derry is an anglicisation of the Irish name "Daire" or "Doire" meaning "oak grove". In 1613, the city was granted a Royal Charter by King James I and gained the "London" prefix to reflect the funding of its construction by the London guilds. While the city is more usually known colloquially as Derry, Londonderry is also commonly used and remains the legal name.
The old walled city lies on the west bank of the River Foyle, which is spanned by two road bridges and one footbridge. The city now covers both banks ("Cityside" on the west and "Waterside" on the east). The city district also extends to rural areas to the southeast. The population of the city proper (the area defined by its 17th century charter) was 83,652 in the 2001 Census, while the Derry Urban Area had a population of 90,736. The district is administered by Derry City & Strabane District Council and contains both Londonderry Port and City of Derry Airport.
Derry is close to the border with County Donegal, with which it has had a close link for many centuries. The person traditionally seen as the 'founder' of the original Derry is Saint Colmcille, a holy man from Tír Chonaill, the old name for almost all of modern County Donegal (of which the west bank of the Foyle was a part before 1610).
In 2013, Derry became the inaugural UK City of Culture, having been awarded the title in July 2010.
Name.
According to the city's Royal Charter of 10 April 1662, the official name is "Londonderry". This was reaffirmed in a High Court decision in January 2007 when Derry City Council sought guidance on the procedure for effecting a name change. The council had changed its name from "Londonderry City Council" to "Derry City Council" in 1984; the court case was seeking clarification as to whether this had also changed the name of the city. The decision of the court was that it had not but it was clarified that the correct procedure to do so was via a petition to the Privy Council. Derry City Council since started this process and were involved in conducting an equality impact assessment report (EQIA). Firstly it held an opinion poll of district residents in 2009, which reported that 75% of Catholics and 77% of Nationalists found the proposed change acceptable, compared to 6% of Protestants and 8% of Unionists. Then the EQIA held two consultative forums, and solicited comments from the general public on whether or not the city should have its name changed to Derry. A total of 12,136 comments were received, of which 3,108 were broadly in favour of the proposal, and 9,028 opposed to it. On 23 July 2015, the council voted in favour of a motion to change the official name of the city to Derry and to write to Mark H. Durkan, Northern Ireland Minister of the Environment, to ask how the change could be effected.
Despite the official name, the city is more usually known as simply "Derry", which is an anglicisation of the Irish "Daire" or "Doire", and translates as "oak-grove/oak-wood". The name derives from the settlement's earliest references, "Daire Calgaich" ("oak-grove of Calgach"). The name was changed from Derry in 1613 during the Plantation of Ulster to reflect the establishment of the city by the London guilds.
The name "Derry" is preferred by nationalists and it is broadly used throughout Northern Ireland's Catholic community, as well as that of the Republic of Ireland, whereas many unionists prefer "Londonderry"; however in everyday conversation Derry is used by most Protestant residents of the city. Linguist Kevin McCafferty argues that "It is not, strictly speaking, correct that Northern Ireland Catholics call it Derry, while Protestants use the Londonderry form, although this pattern has become more common locally since the mid-1980s, when the city council changed its name by dropping the prefix". In McCafferty's survey of language use in the city, "only very few interviewees—all Protestants—use the official form".
Apart from the name of Derry City Council, the city is usually known as Londonderry in official use within the UK. In the Republic of Ireland, the city and county are almost always referred to as Derry, on maps, in the media and in conversation. In April 2009, however, the Republic of Ireland's Minister for Foreign Affairs, Micheál Martin, announced that Irish passport holders who were born there could record either Derry or Londonderry as their place of birth. Whereas official road signs in the Republic use the name "Derry", those in Northern Ireland bear "Londonderry" (sometimes abbreviated to "L'Derry"), although some of these have been defaced with the reference to "London" obscured. Usage varies among local organisations, with both names being used. Examples are City of Derry Airport, City of Derry Rugby Club, Derry City FC and the Protestant Apprentice Boys of Derry, as opposed to Londonderry Port, Londonderry YMCA Rugby Club and Londonderry Chamber of Commerce. Most companies within the city choose local area names such as Pennyburn, Rosemount or "Foyle" from the River Foyle to avoid alienating the other community. Londonderry railway station is often referred to as Waterside railway station within the city but is called Derry/Londonderry at other stations. The council changed the name of the local government district covering the city to Derry on 7 May 1984, consequently renaming itself Derry City Council. This did not change the name of the city, although the city is coterminous with the district, and in law the city council is also the "Corporation of Londonderry" or, more formally, the "Mayor, Aldermen and Citizens of the City of Londonderry". The form "Londonderry" is used for the post town by the Royal Mail, however use of Derry will still ensure delivery.
The city is also nicknamed "the Maiden City" by virtue of the fact that its walls were never breached despite being besieged on three separate occasions in the 17th century, the most notable being the Siege of Derry of 1688/89. It is also nicknamed "Stroke City" by local broadcaster, Gerry Anderson, due to the 'politically correct' use of the oblique notation Derry/Londonderry (which appellation has itself been used by BBC Television). A recent addition to the landscape has been the erection of several large stone columns on main roads into the city welcoming drivers, euphemistically, to "the walled city".
The name Derry is very much in popular use throughout Ireland for the naming of places, and there are at least six towns bearing that name and at least a further 79 places. The word Derry often forms part of the place name, for example Derrybeg, Derryboy, Derrylea and Derrymore.
The names Derry and Londonderry are not limited to Ireland. There is a town called Derry situated right beside another town called Londonderry in New Hampshire in the US. There are also Londonderrys in Yorkshire, England, in Vermont, USA, in Nova Scotia, Canada, and in northern and eastern Australia. Londonderry Island is situated off Tierra del Fuego in Chile.
Derry is also a fictional town in Maine, USA, used in some Stephen King novels.
City walls.
Derry is the only remaining completely intact walled city in Ireland and one of the finest examples of a walled city in Europe. The walls constitute the largest monument in State care in Northern Ireland and, as the last walled city to be built in Europe, stands as the most complete and spectacular.
The Walls were built during the period 1613–1619 by The Honourable The Irish Society as defences for early 17th century settlers from England and Scotland. The Walls, which are approximately in circumference and which vary in height and width between , are completely intact and form a walkway around the inner city. They provide a unique promenade to view the layout of the original town which still preserves its Renaissance style street plan. The four original gates to the Walled City are Bishop's Gate, Ferryquay Gate, Butcher Gate and Shipquay Gate. Three further gates were added later, Magazine Gate, Castle Gate and New Gate, making seven gates in total. Historic buildings within the walls include the 1633 Gothic cathedral of St Columb, the Apprentice Boys Memorial Hall and the courthouse.
It is one of the few cities in Europe that never saw its fortifications breached, withstanding several sieges including one in 1689 which lasted 105 days, hence the city's nickname, The Maiden City.
History.
Early history.
Derry is one of the oldest continuously inhabited places in Ireland. The earliest historical references date to the 6th century when a monastery was founded there by St Columba or Colmcille, a famous saint from what is now County Donegal, but for thousands of years before that people had been living in the vicinity.
Before leaving Ireland to spread Christianity elsewhere, Colmcille founded a monastery at Derry (which was then called "Doire Calgach"), on the west bank of the Foyle. According to oral and documented history, the site was granted to Colmcille by a local king. The monastery then remained in the hands of the federation of Columban churches who regarded Colmcille as their spiritual mentor. The year 546 is often referred to as the date that the original settlement was founded. However, it is now accepted by historians that this was an erroneous date assigned by medieval chroniclers. It is accepted that between the 6th century and the 11th century, Derry was known primarily as a monastic settlement.
The town became strategically more significant during the Tudor conquest of Ireland and came under frequent attack. During O'Doherty's Rebellion in 1608 it was attacked by Sir Cahir O'Doherty, Irish chieftain of Inishowen, who burnt much of the town and killed the governor George Paulet. The soldier and statesman Sir Henry Docwra made vigorous efforts to develop the town, earning the reputation of being " the founder of Derry"; but he was accused of failing to prevent the O'Doherty attack, and returned to England.
Plantation.
What became the City of Derry was part of the relatively new County Donegal up until 1610. In that year, the west bank of the future city was transferred by the English Crown to The Honourable The Irish Society and was combined with County Coleraine, part of County Antrim and a large portion of County Tyrone to form County Londonderry. Planters organised by London livery companies through The Honourable The Irish Society arrived in the 17th century as part of the Plantation of Ulster, and rebuilt the town with high walls to defend it from Irish insurgents who opposed the plantation. The aim was to settle Ulster with a population supportive of the Crown. It was then renamed "Londonderry".
This city was the first planned city in Ireland: it was begun in 1613, with the walls being completed in 1619, at a cost of £10,757. The central diamond within a walled city with four gates was thought to be a good design for defence. The grid pattern chosen was subsequently much copied in the colonies of British North America. The charter initially defined the city as extending three Irish miles (about 6.1 km) from the centre.
The modern city preserves the 17th century layout of four main streets radiating from a central Diamond to four gateways  – Bishop's Gate, Ferryquay Gate, Shipquay Gate and Butcher's Gate. The city's oldest surviving building was also constructed at this time: the 1633 Plantation Gothic cathedral of St Columb. In the porch of the cathedral is a stone that records completion with the inscription: "If stones could speake, then London's prayse should sound, Who built this church and cittie from the grounde."
17th-century upheavals.
During the 1640s, the city suffered in the Wars of the Three Kingdoms, which began with the Irish Rebellion of 1641, when the Gaelic Irish insurgents made a failed attack on the city. In 1649 the city and its garrison, which supported the republican Parliament in London, were besieged by Scottish Presbyterian forces loyal to King Charles I. The Parliamentarians besieged in Derry were relieved by a strange alliance of Roundhead troops under George Monck and the Irish Catholic general Owen Roe O'Neill. These temporary allies were soon fighting each other again however, after the landing in Ireland of the New Model Army in 1649. The war in Ulster was finally brought to an end when the Parliamentarians crushed the Irish Catholic Ulster army at the Battle of Scarrifholis, near Letterkenny in nearby County Donegal, in 1650.
During the Glorious Revolution, only Derry and nearby Enniskillen had a Protestant garrison by November 1688. An army of around 1,200 men, mostly ""Redshanks"" (Highlanders), under Alexander Macdonnell, 3rd Earl of Antrim, was slowly organised (they set out on the week William of Orange landed in England). When they arrived on 7 December 1688 the gates were closed against them and the Siege of Derry began. In April 1689, King James came to the city and summoned it to surrender. The King was rebuffed and the siege lasted until the end of July with the arrival of a relief ship.
18th and 19th centuries.
The city was rebuilt in the 18th century with many of its fine Georgian style houses still surviving. The city's first bridge across the River Foyle was built in 1790. During the 18th and 19th centuries the port became an important embarkation point for Irish emigrants setting out for North America. Some of these founded the colonies of Derry and Londonderry in the state of New Hampshire.
Also during the 19th century, it became a destination for migrants fleeing areas more severely affected by the Irish Potato Famine. One of the most notable shipping lines was the McCorkell Line operated by Wm. McCorkell & Co. Ltd. from 1778. The McCorkell's most famous ship was the Minnehaha, which was known as the "Green Yacht from Derry".
Early 20th century.
World War I.
The city contributed over 5,000 men to the British Army from Catholic and Protestant families.
Partition.
During the Irish War of Independence, the area was rocked by sectarian violence, partly prompted by the guerilla war raging between the Irish Republican Army and British forces, but also influenced by economic and social pressures. By mid-1920 there was severe sectarian rioting in the city. Many lives were lost and in addition many Catholics and Protestants were expelled from their homes during this communal unrest. After a week's violence, a truce was negotiated by local politicians on both unionist and republican sides.
In 1921, following the Anglo-Irish Treaty and the Partition of Ireland, it unexpectedly became a 'border city', separated from much of its traditional economic hinterland in County Donegal.
World War II.
During World War II, the city played an important part in the Battle of the Atlantic.
Ships from the Royal Navy, the Royal Canadian Navy, and other Allied navies were stationed in the city and the United States military established a base. Over 20,000 Royal Navy, 10,000 Royal Canadian Navy, and 6,000 American Navy personnel were stationed in the city during the war.
The establishment of the American presence in the city was the result of a secret agreement between the Americans and the British before the Americans entered the war. It was the first American naval base in Europe and the terminal for American convoys en route to Europe.
The reason for such a high degree of military and naval activity was self-evident: Derry was the United Kingdom's westernmost port; indeed, the city was the westernmost Allied port in Europe: thus, Derry was a crucial jumping-off point, together with Glasgow and Liverpool, for the shipping convoys that ran between Europe and North America. The large numbers of military personnel in Derry substantially altered the character of the city, bringing in some outside colour to the local area, as well as some cosmopolitan and economic buoyancy during these years. Several airfields were built in the outlying regions of the city at this time, Maydown, Eglinton and Ballykelly. RAF Eglinton went on to become City of Derry Airport.
The city contributed significant number of men to the war effort throughout the services, most notably the 500 men in the 9th (Londonderry) Heavy Anti-Aircraft Regiment, known as the 'Derry Boys'. This regiment served in North Africa, the Sudan, Italy and mainland UK. Many others served in the Merchant Navy taking part in the convoys that supplied the UK and Russia during the war.
The border location of the city, and influx of trade from the military convoys allowed for significant smuggling operations to develop in the city.
At the conclusion of the Second World War, eventually some 60 U-boats of the German Kriegsmarine ended in the city's harbour at Lisahally after their surrender. The initial surrender was attended by Admiral Sir Max Horton, Commander-in-Chief of the Western Approaches, and Sir Basil Brooke, third Prime Minister of Northern Ireland.
Late 20th century.
1950s and 1960s.
The city languished after the second world war, with unemployment and development stagnating. A large campaign, led by the University for Derry Committee, to have Northern Ireland's second university located in the city, ended in failure.
The Civil Rights Movement.
Derry was a focal point for the nascent civil rights movement in Northern Ireland.
Catholics were discriminated against under Unionist government in Northern Ireland, both politically and economically. In the late 1960s the city became the flashpoint of disputes about institutional gerrymandering. Political scientist John Whyte explains that:
All the accusations of gerrymandering, practically all the complaints about housing and regional policy, and a disproportionate amount of the charges about public and private employment come from this area. The area – which consisted of Counties Tyrone and Fermanagh, Londonderry County Borough, and portions of Counties Londonderry and Armagh – had less than a quarter of the total population of Northern Ireland yet generated not far short of three-quarters of the complaints of discrimination...The unionist government must bear its share of responsibility. It put through the original gerrymander which underpinned so many of the subsequent malpractices, and then, despite repeated protests, did nothing to stop those malpractices continuing. The most serious charge against the Northern Ireland government is not that it was directly responsible for widespread discrimination, but that it allowed discrimination on such a scale over a substantial segment of Northern Ireland.
A civil rights demonstration in 1968 led by the Northern Ireland Civil Rights Association was banned by the Government and blocked using force by the Royal Ulster Constabulary. The events that followed the August 1969 Apprentice Boys parade resulted in the Battle of the Bogside, when Catholic rioters fought the police, leading to widespread civil disorder in Northern Ireland and is often dated as the starting point of the Troubles.
On Sunday 30 January 1972, 13 unarmed civilians were shot dead by British paratroopers during a civil rights march in the Bogside area. Another 13 were wounded and one further man later died of his wounds. This event came to be known as Bloody Sunday.
The Troubles.
The conflict which became known as the Troubles is widely regarded as having started in Derry with the Battle of the Bogside. The Civil Rights movement had also been very active in the city. In the early 1970s the city was heavily militarised and there was widespread civil unrest. Several districts in the city constructed barricades to control access and prevent the forces of the state from entering.
Violence eased towards the end of the Troubles in the late 1980s and early 1990s. Irish journalist Ed Maloney claims in "The Secret History of the IRA" that republican leaders there negotiated a "de facto" ceasefire in the city as early as 1991. Whether this is true or not, the city did see less bloodshed by this time than Belfast or other localities.
The city was visited by a killer whale in November 1977 at the height of the Troubles; it was dubbed Dopey Dick by the thousands who came from miles around to see him.
Governance.
The local district council is Derry City Council, which consists of five electoral areas: Cityside, Northland, Rural, Shantallow and Waterside. The council of 30 members is re-elected every four years. As of the 2011 election, 14 Social Democratic and Labour Party (SDLP) members, ten Sinn Féin, five Democratic Unionist Party (DUP), and one Ulster Unionist Party (UUP) make up the council. The mayor and deputy mayor are elected annually by councillors.
The local authority boundaries correspond to the Foyle constituency of the Parliament of the United Kingdom and the Foyle constituency of the Northern Ireland Assembly. In European Parliament elections, it is part of the Northern Ireland constituency.
Coat of arms and motto.
The devices on the city's arms are a skeleton and a three-towered castle on a black field, with the "chief" or top third of the shield depicting the arms of the City of London: a red cross and sword on white. In the centre of the cross is a gold harp.
The blazon of the arms is as follows:
"Sable, a human skeleton Or seated upon a mossy stone proper and in dexter chief a castle triple towered argent on a chief also argent a cross gules thereon a harp or and in the first quarter a sword erect gules"
According to documents in the College of Arms in London and the Office of the Chief Herald of Ireland in Dublin, the arms of the city were confirmed in 1613 by Daniel Molyneux, Ulster King of Arms. The College of Arms document states that the original arms of the City of Derry were "ye picture of death (or a skeleton) on a moissy stone & in ye dexter point a castle" and that upon grant of a charter of incorporation and the renaming of the city as Londonderry in that year the first mayor had requested the addition of a "chief of London".
Theories have been advanced as to the meaning of the "old" arms of Derry, before the addition of the chief bearing the arms of the City of London:
In 1979, Londonderry City Council, as it was then known, commissioned a report into the city's arms and insignia, as part of the design process for an heraldic badge. The published report found that there was no basis for any of the popular explanations for the skeleton and that it was "purely symbolic and does not refer to any identifiable person".
The 1613 records of the arms depicted a harp in the centre of the cross, but this was omitted from later depictions of the city arms, and in the Letters Patent confirming the arms to Londonderry Corporation in 1952. In 2002 Derry City Council applied to the College of Arms to have the harp restored to the city arms, and Garter and Norroy & Ulster Kings of Arms accepted the 17th century evidence, issuing letters patent to that effect in 2003.
The motto attached to the coat of arms reads in Latin, "Vita, Veritas, Victoria". This translates into English as, "Life, Truth, Victory".
The councillors elected in 2014 for the city are:
Geography.
Derry is characterised by its distinctively hilly topography. The River Foyle forms a deep valley as it flows through the city, making Derry a place of very steep streets and sudden, startling views. The original walled city of Londonderry lies on a hill on the west bank of the River Foyle. In the past, the river branched and enclosed this wooded hill as an island; over the centuries, however, the western branch of the river dried up and became a low-lying and boggy district that is now called the Bogside.
Today, modern Derry extends considerably north and west of the city walls and east of the river. The half of the city the west of the Foyle is known as the Cityside and the area east is called the Waterside. The Cityside and Waterside are connected by the Craigavon Bridge and Foyle Bridge, and by a foot bridge in the centre of the city called Peace Bridge. The district also extends into rural areas to the southeast of the city.
This much larger city, however, remains characterised by the often extremely steep hills that form much of its terrain on both sides of the river. A notable exception to this lies on the north-eastern edge of the city, on the shores of Lough Foyle, where large expanses of sea and mudflats were reclaimed in the middle of the 19th century. Today, these slob lands are protected from the sea by miles of sea walls and dikes. The area is an internationally important bird sanctuary, ranked among the top 30 wetland sites in the UK.
Other important nature reserves lie at Ness Country Park, east of Derry; and at Prehen Wood, within the city's south-eastern suburbs.
Climate.
Derry has, like most of Ireland, a temperate maritime climate according to the Köppen climate classification system. The nearest official Met Office Weather Station for which climate data is available is Carmoney, just west of City of Derry Airport and about north east of the city centre. However, observations ceased in 2004 and the nearest Weather Station is currently Ballykelly, due east north east. Typically, 27 nights of the year will report an air frost at Ballykelly, and at least 1 mm of precipitation will be reported on 170 days (1981–2010 averages).
The lowest temperature recorded at Carmoney was on 27 December 1995.
Demography.
Derry Urban Area (DUA), including the city and the neighbouring settlements of Culmore, Newbuildings and Strathfoyle, is classified as a city by the Northern Ireland Statistics and Research Agency (NISRA) since its population exceeds 75,000. On census day (27 March 2011) there were 105,066 people living in Derry Urban Area. Of these, 27% were aged under 16 years and 14% were aged 60 and over; 49% of the population were male and 51% were female; 75% were from a Roman Catholic background and 23% (up three percent from 2001) were from a Protestant background.
The mid-2006 population estimate for the wider Derry City Council area was 107,300. Population growth in 2005/06 was driven by natural change, with net out-migration of approximately 100 people.
The city was one of the few in Ireland to experience an increase in population during the Irish Potato Famine as migrants came to it from other, more heavily affected areas.
Protestant minority.
Concerns have been raised by both communities over the increasingly divided nature of the city. There were about 17,000 Protestants on the west bank of the River Foyle in 1971. The proportion then rapidly declined; there are now only about 2,000 on the west bank, and it is feared that the city could become permanently divided.
However, concerted efforts have been made by local community, church and political leaders from both traditions to redress the problem. A conference to bring together key actors and promote tolerance was held in October 2006. The Rt Rev. Dr Ken Good, the Church of Ireland Bishop of Derry and Raphoe, said he was happy living on the cityside. "I feel part of it. It is my city and I want to encourage other Protestants to feel exactly the same", he said.
Support for Protestants in the district has been strong from the former SDLP city Mayor Helen Quigley. Cllr Quigley has made inclusion and tolerance key themes of her mayoralty. The Mayor Helen Quigley said it is time for "everyone to take a stand to stop the scourge of sectarian and other assaults in the city."
Economy.
History.
The economy of the district was based significantly on the textile industry until relatively recently. For many years women were often the sole wage earners working in the shirt factories while the men predominantly in comparison had high levels of unemployment. This led to significant male emigration. The history of shirt making in the city dates back as far as 1831 and is said to have been started by William Scott and his family who first exported shirts to Glasgow. Within 50 years, shirt making in the city was the most prolific in the UK with garments being exported all over the world. It was known so well that the industry received a mention in "Das Kapital" by Karl Marx, when discussing the factory system:
The industry reached its peak in the 1920s employing around 18,000 people. In modern times however the textile industry declined due to in most part cheaper Asian wages.
A long-term foreign employer in the area is Du Pont, which has been based at Maydown since 1958, its first European production facility. Originally Neoprene was manufactured at Maydown and subsequently followed by Hypalon. More recently Lycra and Kevlar production units were active. Thanks to a healthy worldwide demand for Kevlar which is made at the plant, the facility recently undertook a £40 million upgrade to expand its global Kevlar production. Du Pont has stated that contributing factors to its continued commitment to Maydown are "low labor costs, excellent communications, and tariff-free, easy access to the Britain and European continent."
Inward investment.
In the last 15 years there has been a drive to increase inward investment in the city, more recently concentrating on digital industries. Currently the three largest private-sector employers are American firms. Economic successes have included call centres and a large investment by Seagate, which has operated a factory in the Springtown Industrial Estate since 1993. Seagate currently employs over 1,000 people, producing more than half of the company's total requirement for hard drive read-write heads.
A controversial new employer in the area was Raytheon Systems Limited, a software division of the American defence contractor, which was set up in Derry in 1999. Although some of the local people welcomed the jobs boost, others in the area objected to the jobs being provided by a firm involved heavily in the arms trade. Following four years of protest by the Foyle Ethical Investment Campaign, in 2004 Derry City Council passed a motion declaring the district a "A 'No – Go' Area for the Arms Trade", and in 2006 its offices were briefly occupied by anti-war protestors who became known as the Raytheon 9. In 2009, the company announced that it was not renewing its lease when it expired in 2010 and was looking for a new location for its operations.
Significant multinational employers in the region include Firstsource of India, DuPont, INVISTA, Stream International, Seagate Technology, Perfecseal, NTL, Raytheon and Northbrook Technology of the United States, Arntz Belting and Invision Software of Germany, and Homeloan Management of the UK. Major local business employers include Desmonds, Northern Ireland's largest privately owned company, manufacturing and sourcing garments, E&I Engineering, St. Brendan's Irish Cream Liqueur and McCambridge Duffy, one of the largest insolvency practices in the UK.
Even though the city provides cheap labour by standards in Western Europe, critics have noted that the grants offered by the Northern Ireland Industrial Development Board have helped land jobs for the area that only last as long as the funding lasts. This was reflected in questions to the Parliamentary Under-Secretary of State for Northern Ireland, Richard Needham, in 1990. It was noted that it cost £30,000 to create one job in an American firm in Northern Ireland.
Critics of investment decisions affecting the district often point to the decision to build a new university building in nearby (predominantly Protestant) Coleraine rather than developing the Ulster UniversityMagee Campus. Another major government decision affecting the city was the decision to create the new town of Craigavon outside Belfast, which again was detrimental to the development of the city. Even in October 2005, there was perceived bias against the comparatively impoverished North West of the province, with a major civil service job contract going to Belfast. Mark Durkan, the Social Democratic and Labour Party (SDLP) leader and Member of Parliament (MP) for Foyle was quoted in the "Belfast Telegraph" as saying:
In July 2005, the Irish Minister for Finance, Brian Cowen, called for a joint task force to drive economic growth in the cross border region. This would have implications for Counties Londonderry, Tyrone, and Donegal across the border.
Shopping.
The city is the north west's foremost shopping district, housing two large shopping centres along with numerous shop packed streets serving much of the greater county, as well as Tyrone and Donegal. While retail developments in Letterkenny have lessened cross-border traffic from north County Donegal, the weakness of the pound sterling over the course of 2009 made border towns such as Derry attractive to shoppers from south of the border.
The city centre has two main shopping centres; the Foyleside Shopping Centre which has 45 stores and 1430 parking spaces, and the Richmond Centre, which has 39 retail units. The Quayside Shopping Centre also serves the city-side and there is also Lisnagelvin Shopping Centre in the Waterside. These centres, as well as local-run businesses, feature numerous national and international stores. A recent addition was the Crescent Link Retail Park located in the Waterside with many international chain stores, including Homebase, Currys & PC World (stores combined), Carpet Right, Maplin, Argos Extra, Toys R Us, Halfords, DW Sports (formerly JJB Sports), Pets at Home, Next Home, Starbucks, McDonalds, Tesco Express and M&S Simply Food. In the short period of time that this site has been operational, it has quickly grown to become the second largest retail park in Northern Ireland (second only to Sprucefield in Lisburn). Plans have also been approved for Derry's first Asda store, which will be located at the retail park sharing a unit with Homebase. Sainsbury's also applied for planning permission for a store at Crescent Link, but Environment Minister Alex Attwood turned it down.
The city is also home to the world's oldest independent department store; Austins. Established in 1830, Austins predates Jenners of Edinburgh by 5 years, Harrods of London by 15 years and Macy's of New York by 25 years. The store's five-story Edwardian building is located within the walled city in the area known as The Diamond.
Landmarks.
Derry is renowned for its architecture. This can be primarily ascribed to the formal planning of the historic walled city of Derry at the core of the modern city. This is centred on the Diamond with a collection of late Georgian, Victorian and Edwardian buildings maintaining the gridlines of the main thoroughfares (Shipquay Street, Ferryquay Street, Butcher Street and Bishop Street) to the City Gates. St Columb's Cathedral does not follow the grid pattern reinforcing its civic status. This Church of Ireland Cathedral was the first post-Reformation Cathedral built for an Anglican church. The construction of the Roman Catholic St Eugene's Cathedral in the Bogside in the 19th-century was another major architectural addition to the city. The more recent infill buildings within the walls are of varying quality and in many cases these were low quality hurriedly constructed replacements for 1970s bomb damaged buildings. The Townscape Heritage Initiative has funded restoration works to key listed buildings and other older structures.
In the three centuries since their construction, the city walls have been adapted to meet the needs of a changing city. The best example of this adaptation is the insertion of three additional gates – Castle Gate, New Gate and Magazine Gate – into the walls in the course of the 19th century. Today, the fortifications form a continuous promenade around the city centre, complete with cannon, avenues of mature trees and views across Derry. Historic buildings within the city walls include St Augustine's Church, which sits on the city walls close to the site of the original monastic settlement; the copper-domed Austin's department store, which claims to the oldest such store in the world; and the imposing Greek Revival Courthouse on Bishop Street. The red-brick late-Victorian Guildhall, also crowned by a copper dome, stands just beyond Shipquay Gate and close to the river front.
There are many museums and sites of interest in and around the city, including the Foyle Valley Railway Centre, the Amelia Earhart Centre And Wildlife Sanctuary, the Apprentice Boys Memorial Hall, Ballyoan Cemetery, The Bogside, numerous murals by the Bogside Artists, Derry Craft Village, Free Derry Corner, O'Doherty Tower (now home to part of the Tower Museum), the Guildhall, the Harbour Museum, the Museum of Free Derry, Chapter House Museum, the Workhouse Museum, the Nerve Centre, St. Columb's Park and Leisure Centre, St Eugene's Cathedral, Creggan Country Park, The Millennium Forum and the Foyle and Craigavon bridges.
The city has seen a large boost to its economy in the form of tourism over the last few years. Cheap flights offered by budget airlines have enticed many people to visit the city. Tourism mainly focuses around the pubs, mainly those of Waterloo Street. Other attractions include museums, a vibrant shopping centre and trips to the Giant's Causeway, which is approximately away, though poorly connected by public transport. Lonely Planet called Londonderry the fourth best city in the world to see in 2013.
Future projects include the Walled City Signature Project, which intends to ensure that the city's walls become a world class tourist experience. The Ilex Urban Regeneration Company is charged with delivering several landmark redevelopments. It has taken control of two former British Army barracks in the centre of the city. The Ebrington site is nearing completion and is linked to the city centre by the new Peace Bridge.
Transport.
The transport network is built out of a complex array of old and modern roads and railways throughout the city and county. The city's road network also makes use of two bridges to cross the River Foyle, the Craigavon Bridge and the Foyle Bridge, the longest bridge in Ireland. Derry also serves as a major transport hub for travel throughout nearby County Donegal.
In spite of it being the second city of Northern Ireland (and it being the second-largest city in all of Ulster), road and rail links to other cities are below par for its standing. Many business leaders claim that government investment in the city and infrastructure has been badly lacking. Some have stated that this is due to its outlying border location whilst others have cited a sectarian bias against the region west of the River Bann due to its high proportion of Catholics. There is no direct motorway link with Dublin or Belfast. The rail link to Belfast has been downgraded over the years so that presently it is not a viable alternative to the roads for industry to rely on. There are currently plans for £1 billion worth of transport infrastructure investment in and around the district. Planned upgrades to the A5 Dublin road agreed as part of the Good Friday Agreement and St. Andrews Talks fell through when the government of the Republic of Ireland reneged on its funding citing the recent economic crisis.
Buses.
Most public transport in Northern Ireland is operated by the subsidiaries of Translink. Originally the city's internal bus network was run by Ulsterbus, which still provides the city's connections with other towns in Northern Ireland. The city's buses are now run by Ulsterbus Foyle, just as Translink Metro now provides the bus service in Belfast. The Ulsterbus Foyle network offers 13 routes across the city into the suburban areas, excluding an Easibus link which connects to the Waterside and Drumahoe, and a free Rail Link Bus runs from the Waterside Railway Station to the city centre. All buses leave from the Foyle Street Bus Station in the city centre.
Long distance buses depart from Foyle Street Bus Station to destinations throughout Ireland. Buses are operated by both Ulsterbus and Bus Éireann on cross-border routes. Lough Swilly formerly operated buses to Co. Donegal, but the company entered liquidation and is no longer in operation. There is a half-hourly service to Belfast every day, called the Maiden City Flyer, which is the Goldline Express flagship route. There are hourly services to Strabane, Omagh, Coleraine, Letterkenny and Buncrana, and up to twelve services a day to bring people to Dublin. There is a daily service to Sligo, Galway, Shannon Airport and Limerick.
Air.
City of Derry Airport, the council-owned airport near Eglinton, has been growing in recent years with new investment in extending the runway and plans to redevelop the terminal. It is hoped that the new investment will add to the airport's currently limited array of domestic and international flights and reduce the annual subsidy of £3.5 million from the local council.
The A2 from Maydown to Eglinton, serving airport, has recently been turned into a dual carriageway. City of Derry airport is the main regional airport for County Donegal, County Londonderry and west County Tyrone as well as Derry City itself.
The airport is served by Ryanair with scheduled flights to Birmingham International Airport, Glasgow Airport, Liverpool, and London Stansted all year round with a summer schedule to Alicante and Faro.
Railways.
Northern Ireland Railways (N.I.R.) has a single route from Londonderry railway station (also known as Waterside Station) on the Waterside to Belfast Great Victoria Street via , , , , Mossley West and Belfast Central. The service, which had been allowed to deteriorate in the 1990s, has since been improved by increased investment.
In 2008 the Department for Regional Development announced plans to have the track re-laid between Derry and Coleraine by 2013, add a passing loop to increase traffic capacity and increase the number of trains by introducing two additional diesel multiple units. The £86 million plan will reduce the journey time to Belfast by 30 minutes and allow commuter trains to arrive before 9 a.m. for the first time. Many still do not use the train, because, at over two hours, it is slower centre-to-centre than the 100-minute Ulsterbus Goldline Express service.
Railway history.
Throughout the first half of the 20th century the city was served by four different railways that between them linked the city with much of the province of Ulster, plus a harbour railway network that linked the other four lines. There was also a tramway on the City side of the Foyle.
19th and 20th century growth.
Derry's first railway was the Irish gauge () Londonderry and Enniskillen Railway (L&ER). Construction began in 1845 from a temporary station at Cow Market on the City side of the Foyle, reached Strabane in 1847 and was extended from Cow Market to its permanent terminus at Foyle Road in 1850. The L&ER reached Omagh in 1852 and Enniskillen in 1854, and was absorbed into the Great Northern Railway (Ireland) in 1883.
The Londonderry and Coleraine Railway (L&CR), also Irish gauge, reached the city in 1852 and opened its terminus at Waterside. The Belfast and Northern Counties Railway leased the line from 1861 and took it over in 1871.
The Londonderry and Lough Swilly Railway opened between Farland Point on Lough Swilly and a temporary terminus at Pennyburn in 1863. In 1866 it extended from Pennyburn to its permanent terminus at Graving Dock. The L&LSR was Irish gauge until 1885, when it was converted to narrow gauge for through running with the Letterkenny Railway.
The Londonderry Port and Harbour Commissioners (LPHC) linked Graving Dock and Foyle Road stations with a railway through Middle Quay in 1867, and linked this line with Waterside station by a railway over the new Carlisle Bridge in 1868. The bridge was replaced in 1933 with the double-deck Craigavon Bridge, with the LPHC railway on its lower deck.
In 1900 the gauge Donegal Railway extended from Strabane to Derry, establishing a terminus at Victoria Road. This was next to Carlisle Bridge and had a junction with the LPHC railway. The LPHC line was altered to dual gauge which allowed gauge traffic between the Donegal Railway and L&LSR as well as Irish gauge traffic between the GNR and B&NCR. In 1906 the Northern Counties Committee (NCC, successor to the B&NCR) and the GNR jointly took over the Donegal Railway, making it the County Donegal Railways Joint Committee (CDRJC).
The United Kingdom Government subsidised both the L&LSR and the Donegal Railway to build long extensions into remote parts of County Donegal. By 1905 these served much of the county, making Derry (and also Strabane) a key rail hub for the county.
The City of Derry Tramways was opened in 1897. This was a standard gauge () line served by horse trams and was never electrified. The tramway had only one line, was long, and ran along the City side of the Foyle parallel to the LPHC's line on that side of the river. It was closed in 1919.
20th century decline.
The partition of Ireland in 1922 turned the boundary with County Donegal into an international frontier. This changed trade patterns to the railways' detriment and placed border posts on every line to and from Derry except the NCC route to . The L&LSR crossed the border between Pennyburn and Bridge End, the CDRJC crossed just beyond Strabane, and the GNR line crossed twice between Derry and Strabane. Stops for customs inspections greatly delayed trains and disrupted timekeeping.
Over the next few years customs agreements between the two states enabled GNR trains to and from Derry to pass through the Free State without inspection unless they were scheduled to serve local stations on the west bank of the Foyle, and for goods on all railways to be carried between different parts of the Free State to pass through Northern Ireland under customs bond. However, local passenger and goods traffic continued to be delayed by customs examinations.
In the 1920s and 30s and again after the Second World War the railways also faced increasing road competition. The L&LSR closed its line in 1953, followed by the CDRJC in 1954. The Ulster Transport Authority took over the NCC in 1949 and the GNR's lines in Northern Ireland in 1958. The UTA also took over the LPHC railway, which it closed in 1962. In accordance with The Benson Report submitted to the Northern Ireland Government in 1963, the UTA closed the former GNR line to Derry in 1965.
Since 1965 the former L&CR line has been Derry's sole railway link. As such it has carried not only passenger services between Derry and Belfast but also CIÉ freight services using Derry as a railhead for Donegal.
Road network.
The road network has historically seen under-investment and has lacked good road connections to both Belfast and Dublin for many years. Long overdue, the largest road investment in the north west's history is now (2010) taking place with building of the 'A2 Broadbridge Maydown to City of Derry Airport dualling' project and announcement of the 'A6 Londonderry to Dungiven Dualling Scheme' which will help to reduce the travel time to Belfast. The latter project brings a dual-carriageway link between Northern Ireland's two largest cities one step closer. The project is costing £320 million and is expected to be completed in 2016.
In October 2006 the Government of Ireland announced that it was to invest €1 billion in Northern Ireland; and one of the planned projects will be 'The A5 Western Transport Corridor', the complete upgrade of the A5 Derry – Omagh – Aughnacloy (– Dublin) road, around long, to dual carriageway standard.
It is not yet known if these two separate projects will connect at any point, although there have been calls for some form of connection between the two routes. In June 2008 Conor Murphy, Minister for Regional Development, announced that there will be a study into the feasibility of connecting the A5 and A6. Should it proceed, the scheme would most likely run from Drumahoe to south of Prehen along the south east of the City.
Sea.
Londonderry Port at Lisahally is the United Kingdom's most westerly port and has capacity for 30,000-ton vessels. The Londonderry Port and Harbour Commissioners (LPHC) announced record turnover, record profits and record tonnage figures for the year ended March 2008. The figures are the result of a significant capital expenditure programme for the period 2000 to 2007 of about £22 million. Tonnage handled by LPHC increased almost 65% between 2000 and 2007, according to the latest annual results.
The port gave vital Allied service in the longest running campaign of the Second World War, the Battle of the Atlantic, and saw the surrender of the German U-Boat fleet at Lisahally on 8 May 1945.
Inland waterways.
The tidal River Foyle is navigable from the coast at Derry to approximately inland. In 1796, the Strabane Canal was opened, continuing the navigation a further southwards to Strabane. The canal was closed in 1962.
Education.
Derry is home to the Magee Campus of Ulster University, formerly Magee College. However, Lockwood's 1960s decision to locate Northern Ireland's second university in Coleraine rather than Derry helped contribute to the formation of the civil rights movement that ultimately led to The Troubles. Derry was the town more closely associated with higher learning, with Magee College already more than a century old by that time. In the mid-1980s a half-hearted attempt was made at rectifying this mistake by forming Magee College as a campus of the Ulster University but this has failed to stifle calls for the establishment of an independent University in Derry that can grow to it full potential. The campus has never thrived and currently only has 3,500 students out of a total Ulster University student population of 27,000. Ironically, although Coleraine is blamed by many in the city for 'stealing the University', it has only 5,000 students, the remaining 19,000 being based in Belfast.
The North West Regional College is also based in the city. In recent years it has grown to almost 30,000 students.
One of the two oldest secondary schools in Northern Ireland is located in Derry, Foyle and Londonderry College. It was founded in 1616 by the merchant taylors and remains a popular choice. Other secondary schools include St. Columb's College, Oakgrove Integrated College, St Cecilia's College, St Mary's College, St. Joseph's Boys' School, Lisneal College, Thornhill College, Lumen Christi College and St. Brigid's College. There are also numerous primary schools.
Sports.
The city is home to sports clubs and teams. Both association football and Gaelic football are popular in the area.
Association football.
In association football, the city's most prominent clubs include Derry City who play in the national league of the Republic of Ireland; Institute of the NIFL Championship and Oxford United Stars and Trojans, both of the Northern Ireland Intermediate League.
In addition to these clubs, who all play in national leagues, other clubs are based in the city. The local football league governed by the IFA is the North-West Junior League, which contains many clubs from the city, such as BBOB (Boys Brigade Old Boys) and Lincoln Courts. The city's other junior league is the Derry and District League and teams from the city and surrounding areas participate, including Don Boscos and Creggan Swifts. The Foyle Cup youth soccer tournament is held annually in the city. It has attracted many notable teams in the past, including Werder Bremen, IFK Göteborg and Ferencváros.
Gaelic football.
In Gaelic football Derry GAA are the county team and play in the Gaelic Athletic Association's National Football League, Ulster Senior Football Championship and All-Ireland Senior Football Championship. They also field hurling teams in the equivalent tournaments. There are many Gaelic games clubs in and around the city, for example Na Magha CLG, Steelstown GAC, Doire Colmcille CLG, Seán Dolans GAC, Na Piarsaigh CLG Doire Trasna and Slaughtmanus GAC.
Boxing.
There are many boxing clubs, the most well-known being "The Ring Boxing Club", which is associated with Charlie Nash and John Duddy, amongst others.
Rugby Union.
Rugby Union is also quite popular in the city, with the City of Derry Rugby Club situated not far from the city centre. City of Derry won both the Ulster Towns Cup and the Ulster Junior Cup in 2009.
Londonderry YMCA RFC is another rugby club and is based in Drumahoe which is just outside the city.
Basketball.
The city's only basketball club is North Star Basketball Club which has teams in the Basketball Northern Ireland senior and junior Leagues.
Cricket.
Cricket is also a popular sport in the city, particularly in the Waterside. The city is home to two cricket clubs, Brigade Cricket Club and Glendermott Cricket Club, both of whom play in the North West Senior League.
Golf.
Golf is also a sport which is popular with many in the City. There are two golf clubs situated in the city, City of Derry Golf Club and Foyle International Golf Centre.
Culture.
In recent years the city and surrounding countryside have become well known for their artistic legacy, producing Nobel Prize-winning poet Seamus Heaney, poet Seamus Deane, playwright Brian Friel, writer and music critic Nik Cohn, artist Willie Doherty, socio-political commentator and activist Eamonn McCann and bands such as The Undertones. The large political gable-wall murals of Bogside Artists, Free Derry Corner, the Foyle Film Festival, the Derry Walls, St Eugene's and St Columb's Cathedrals and the annual Halloween street carnival are popular tourist attractions. In 2010, Derry was named the UK's tenth 'most musical' city by PRS for Music.
In May 2013 a perpetual Peace Flame Monument was unveiled by Martin Luther King III and Presbyterian minister Rev. David Latimer. The flame was lit by children from both traditions in the city and is one of only 15 such flames across the world.
Media.
The local papers the "Derry Journal" (known as the "Londonderry Journal" until 1880) and the "Londonderry Sentinel" reflect the divided history of the city: the "Journal" was founded in 1772 and is Ireland's second oldest newspaper; the "Sentinel" newspaper was formed in 1829 when new owners of the "Journal" embraced Catholic Emancipation, and the editor left the paper to set up the "Sentinel". There are numerous radio stations receivable: the largest stations based in the city are BBC Radio Foyle and the commercial station Q102.9. There was a locally based television station, C9TV, one of only two local or 'restricted' television services in Northern Ireland, which ceased broadcasts in 2007.
Night-life.
The city's night-life is mainly focused on the weekends, with several bars and clubs providing "student nights" during the weekdays. Waterloo Street and Strand Road provide the main venues. Waterloo Street, a steep street lined with both Irish traditional and modern pubs, frequently has live rock and traditional music at night. The city is renowned for producing talented musicians and many bands perform in venues around the city, for example the Smalltown America duo, Fighting with Wire and Jetplane Landing. Numerous other young local and indeed international bands perform at the Nerve Centre.
Notable people.
Notable people who were born or have lived in Derry include:

</doc>
<doc id="9058" url="https://en.wikipedia.org/wiki?curid=9058" title="European influence in Afghanistan">
European influence in Afghanistan

The European influence in Afghanistan refers to political, social, and mostly imperialistic influence several European nations and colonial powers have had on the historical development of Afghanistan.
Rise of Dost Mohammad Khan.
After the decline of the Durrani dynasty in 1823, Dost Mohammad Khan established the Barakzai dynasty after becoming the next Emir of Afghanistan. It was not until 1826 that the energetic Dost Mohammad Khan was able to exert sufficient control over his brothers to take over the throne in Kabul, where he proclaimed himself the Shah.
Dost Mohammad achieved prominence among his brothers through clever use of the support of his mother's Qizilbash tribesmen and his own youthful apprenticeship under his brother, Fateh Khan. Among the many problems he faced was repelling Sikh encroachment on the Pashtun areas east of the Khyber Pass. After working assiduously to establish control and stability in his domains around Kabul, the Shah next chose to confront the warring Sikhs.
In 1834 Dost Mohammad defeated an invasion by the former ruler, Shuja Shah Durrani, but his absence from Kabul gave the Sikhs the opportunity to expand westward. Ranjit Singh's forces occupied Peshawar, moving from there into territory ruled directly by Kabul. In 1836 Dost Mohammad's forces, under the command of his son Akbar Khan, defeated the Sikhs at the Battle of Jamrud, a post fifteen kilometres west of Peshawar. This was a pyrrhic victory and they failed to fully dislodge the Sikhs from Jamrud. The Afghan leader did not follow up this triumph by retaking Peshawar, however, but instead contacted Lord Auckland, the new British governor general in British India, for help in dealing with the Sikhs. With this letter, Dost Mohammad formally set the stage for British intervention in Afghanistan. At the heart of the Great Game lay the willingness of Britain and Russia to subdue, subvert, or subjugate the small independent states that lay between Russia and British India.
The Great Game.
The British became the major power in the Indian sub-continent after the Treaty of Paris (1763) and began to show interest in Afghanistan as early as their 1809 treaty with Shuja Shah Durrani. It was the threat of the expanding Russian Empire beginning to push for an advantage in the Afghanistan region that placed pressure on British India, in what became known as the "Great Game". The Great Game set in motion the confrontation of the British and Russian empires, whose spheres of influence moved steadily closer to one another until they met in Afghanistan. It also involved Britain's repeated attempts to impose a puppet government in Kabul. The remainder of the 19th century saw greater European involvement in Afghanistan and her surrounding territories and heightened conflict among the ambitious local rulers as Afghanistan's fate played out globally.
The débâcle of the Afghan civil war left a vacuum in the Hindu Kush area that concerned the British, who were well aware of the many times in history it had been employed as the invasion route to South Asia. In the early decades of the 19th century, it became clear to the British that the major threat to their interests in India would not come from the fragmented Afghan empire, the Iranians, or the French, but from the Russians, who had already begun a steady advance southward from the Caucasus winning decisive wars against the Ottoman Turks and Qajar Persians.
At the same time, the Russians feared permanent British occupation in Central Asia as the British encroached northward, taking the Punjab, Sindh, and Kashmir; later to become Pakistan. The British viewed Russia's absorption of the Caucasus, the Kyrgyz and Turkmen lands, the Khanate of Khiva, and the Emirate of Bukhara with equal suspicion as a threat to their interests in the Asian subcontinent.
In addition to this rivalry between Britain and Russia, there were two specific reasons for British concern over Russia's intentions. First was the Russian influence at the Iranian court, which prompted the Russians to support Iran in its attempt to take Herat, historically the western gateway to Afghanistan and northern India. In 1837 Iran advanced on Herat with the support and advice of Russian officers. The second immediate reason was the presence in Kabul in 1837 of a Russian agent, Yan Vitkevich, who was ostensibly there, as was the British agent Alexander Burnes, for commercial discussions.
The British demanded that Dost Mohammad sever all contact with the Iranians and Russians, remove Vitkevich from Kabul, surrender all claims to Peshawar, and respect Peshawar's independence as well as that of Kandahar, which was under the control of his brothers at the time. In return, the British government intimated that it would ask Ranjit Singh to reconcile with the Afghans. When Auckland refused to put the agreement in writing, Dost Mohammad turned his back on the British and began negotiations with Vitkevich.
In 1838 Auckland, Ranjit Singh, and Shuja signed an agreement stating that Shuja would regain control of Kabul and Kandahar with the help of the British and Sikhs; he would accept Sikh rule of the former Afghan provinces already controlled by Ranjit Singh, and that Herat would remain independent. In practice, the plan replaced Dost Mohammad with a British figurehead whose autonomy would be as limited as that of other Indian princes.
It soon became apparent to the British that Sikh participation, advancing toward Kabul through the Khyber Pass while Shuja and the British advanced through Kandahar, would not be forthcoming. Auckland's plan in the spring of 1838 was for the Sikhs to place Shuja on the Afghan throne, with British support. By the end of the summer however, the plan had changed; now the British alone would impose the pliant Shuja Shah.
First Anglo-Afghan War, 1838–1842.
To justify his plan, the Governor-General of India Lord Auckland issued the Simla Manifesto in October 1838, setting forth the necessary reasons for British intervention in Afghanistan. The manifesto stated that in order to ensure the welfare of India, the British must have a trustworthy ally on India's western frontier. The British pretense that their troops were merely supporting Shah Shujah's small army in retaking what was once his throne fooled no one. Although the Simla Manifesto stated that British troops would be withdrawn as soon as Shuja was installed in Kabul, Shuja's rule depended entirely on British arms to suppress rebellion and on British funds to buy the support of tribal chiefs. The British denied that they were invading Afghanistan, instead claiming they were merely supporting its legitimate Shuja government "against foreign interference and factious opposition".
In November 1841 insurrection and massacre flared up in Kabul. The British vacillated and disagreed and were beleaguered in their inadequate cantonments. The British negotiated with the most influential sirdars, cut off as they were by winter and insurgent tribes from any hope of relief. Mohammad Akbar Khan, son of the captive Dost Mohammad, arrived in Kabul and became effective leader of the sirdars. At a conference with them Sir William MacNaghten was killed, but in spite of this, the sirdars' demands were agreed to by the British and they withdrew. During the withdrawal they were attacked by Ghilzai tribesmen and in running battles through the snowbound passes nearly the entire column of 4,500 troops and 12,000 camp followers were killed. Of the British only one, Dr. William Brydon, reached Jalalabad, while a few others were captured.
Afghan forces loyal to Akbar Khan besieged the remaining British contingents at Kandahar, Ghazni and Jalalabad. Ghazni fell, but the other garrisons held out, and with the help of reinforcements from India their besiegers were defeated. While preparations were under way for a renewed advance on Kabul, the new Governor-General Lord Ellenborough ordered British forces to leave Afghanistan after securing the release of the prisoners from Kabul and taking reprisals. The forces from Kandahar and Jalalabad again defeated Akbar Khan, retook Ghazni and Kabul, inflicted widespread devastation and rescued the prisoners before withdrawing through the Khyber Pass.
Mid-nineteenth century.
After months of chaos in Kabul, Mohammad Akbar Khan secured local control and in April 1843 his father Dost Mohammad, who had been released by the British, returned to the throne in Afghanistan. In the following decade, Dost Mohammad concentrated his efforts on reconquering Mazari Sharif, Konduz, Badakhshan, and Kandahar. Mohammad Akbar Khan died in 1845. During the Second Anglo-Sikh War (1848–49), Dost Mohammad's last effort to take Peshawar failed.
By 1854 the British wanted to resume relations with Dost Mohammad, whom they had essentially ignored in the intervening twelve years. The 1855 Treaty of Peshawar reopened diplomatic relations, proclaimed respect for each side's territorial integrity, and pledged both sides as friends of each other's friends and enemies of each other's enemies.
In 1857 an addendum to the 1855 treaty permitted a British military mission to become a presence in Kandahar (but not Kabul) during a conflict with the Persians, who had attacked Herat in 1856. During the Indian Rebellion of 1857, some British officials suggested restoring Peshawar to Dost Mohammad, in return for his support against the rebellious sepoys of the Bengal Army, but this view was rejected by British political officers on the North West frontier, who believed that Dost Mohammad would see this as a sign of weakness and turn against the British.
In 1863 Dost Mohammad retook Herat with British acquiescence. A few months later, he died. Sher Ali Khan, his third son, and proclaimed successor, failed to recapture Kabul from his older brother, Mohammad Afzal (whose troops were led by his son, Abdur Rahman) until 1868, after which Abdur Rahman retreated across the Amu Darya and bided his time.
In the years immediately following the First Anglo-Afghan War, and especially after the Indian rebellion of 1857 against the British in India, Liberal Party governments in London took a political view of Afghanistan as a buffer state. By the time Sher Ali had established control in Kabul in 1868, he found the British ready to support his regime with arms and funds, but nothing more. Over the next ten years, relations between the Afghan ruler and Britain deteriorated steadily. The Afghan ruler was worried about the southward encroachment of Russia, which by 1873 had taken over the lands of the khan, or ruler, of Khiva. Sher Ali sent an envoy seeking British advice and support. The previous year the British had signed an agreement with the Russians in which the latter agreed to respect the northern boundaries of Afghanistan and to view the territories of the Afghan Emir as outside their sphere of influence. The British, however, refused to give any assurances to the disappointed Sher Ali.
Second Anglo-Afghan War, 1878–1880.
After tension between Russia and Britain in Europe ended with the June 1878 Congress of Berlin, Russia turned its attention to Central Asia. That same summer, Russia sent an uninvited diplomatic mission to Kabul. Sher Ali tried, but failed, to keep them out. Russian envoys arrived in Kabul on 22 July 1878 and on 14 August, the British demanded that Sher Ali accept a British mission too.
The amir not only refused to receive a British mission but threatened to stop it if it were dispatched. Lord Lytton, the viceroy, ordered a diplomatic mission to set out for Kabul in September 1878 but the mission was turned back as it approached the eastern entrance of the Khyber Pass, triggering the Second Anglo-Afghan War. A British force of about 40,000 fighting men was distributed into military columns which penetrated Afghanistan at three different points. An alarmed Sher Ali attempted to appeal in person to the tsar for assistance, but unable to do so, he returned to Mazari Sharif, where he died on 21 February 1879.
With British forces occupying much of the country, Sher Ali's son and successor, Mohammad Yaqub Khan, signed the Treaty of Gandamak in May 1879 to prevent a British invasion of the rest of the country. According to this agreement and in return for an annual subsidy and vague assurances of assistance in case of foreign aggression, Yaqub relinquished control of Afghan foreign affairs to the British. British representatives were installed in Kabul and other locations, British control was extended to the Khyber and Michni passes, and Afghanistan ceded various frontier areas and Quetta to Britain. The British army then withdrew. Soon afterwards, an uprising in Kabul led to the slaughter of Britain's Resident in Kabul, Sir Pierre Cavagnari and his guards and staff on 3 September 1879, provoking the second phase of the Second Afghan War. Major General Sir Frederick Roberts led the Kabul Field Force over the Shutargardan Pass into central Afghanistan, defeated the Afghan Army at Char Asiab on 6 October 1879 and occupied Kabul. Ghazi Mohammad Jan Khan Wardak staged an uprising and attacked British forces near Kabul in the Siege of the Sherpur Cantonment in December 1879, but his defeat there resulted in the collapse of this rebellion.
Yaqub Khan, suspected of complicity in the massacre of Cavagnari and his staff, was obliged to abdicate. The British considered a number of possible political settlements, including partitioning Afghanistan between multiple rulers or placing Yaqub's brother Ayub Khan on the throne, but ultimately decided to install his cousin Abdur Rahman Khan as emir instead. Ayub Khan, who had been serving as governor of Herat, rose in revolt, defeated a British detachment at the Battle of Maiwand in July 1880 and besieged Kandahar. Roberts then led the main British force from Kabul and decisively defeated Ayub Khan in September at the Battle of Kandahar, bringing his rebellion to an end. Abdur Rahman had confirmed the Treaty of Gandamak, leaving the British in control of the territories ceded by Yaqub Khan and ensuring British control of Afghanistan's foreign policy in exchange for protection and a subsidy. Abandoning the provocative policy of maintaining a British resident in Kabul, but having achieved all their other objectives, the British withdrew.
The Iron Amir, 1880–1901.
As far as British interests were concerned, Abdur Rahman answered their prayers: a forceful, intelligent leader capable of welding his divided people into a state; and he was willing to accept limitations to his power imposed by British control of his country's foreign affairs and the British buffer state policy. His twenty-one-year reign was marked by efforts to modernize and establish control of the kingdom, whose boundaries were delineated by the two empires bordering it. Abdur Rahman turned his considerable energies to what evolved into the creation of the modern state of Afghanistan.
He achieved this consolidation of Afghanistan in three ways. He suppressed various rebellions and followed up his victories with harsh punishment, execution, and deportation. He broke the stronghold of Pashtun tribes by forcibly transplanting them. He transplanted his most powerful Pashtun enemies, the Ghilzai, and other tribes from southern and south-central Afghanistan to areas north of the Hindu Kush with predominantly non-Pashtun populations. The last non-Muslim Afghans of Kafiristan north of Kabul were forcefully converted to Islam. Finally, he created a system of provincial governorates different from old tribal boundaries. Provincial governors had a great deal of power in local matters, and an army was placed at their disposal to enforce tax collection and suppress dissent. Abdur Rahman kept a close eye on these governors, however, by creating an effective intelligence system. During his reign, tribal organization began to be eroded as provincial government officials allowed land to change hands outside the traditional clan and tribal limits.
The Pashtuns battled and conquered the Uzbeks and forced them into the status of ruled people who were discriminated against. Out of anti-Russian strategic interests, the British assisted the Afghan conquest of the Uzbek Khanates, giving weapons to the Afghans and backing the Pashtun colonization of northern Afghanistan, which involved sending massive amounts of Pashtun colonists onto Uzbek land; British literature from the period demonized the Uzbeks.
In addition to forging a nation from the splintered regions making up Afghanistan, Abdur Rahman tried to modernize his kingdom by forging a regular army and the first institutionalized bureaucracy. Despite his distinctly authoritarian personality, Abdur Rahman called for a loya jirga, an assemblage of royal princes, important notables, and religious leaders. According to his autobiography, Abdur Rahman had three goals: subjugating the tribes, extending government control through a strong, visible army, and reinforcing the power of the ruler and the royal family.
During his visit to Rawalpindi in 1885, the Amir requested the Viceroy of India to depute a Muslim Envoy to Kabul who was noble birth and of ruling family background. Mirza Atta Ullah Khan, Sardar Bahadur s/o Khan Bahadur Mirza Fakir Ullah Khan (Saman Burj Wazirabad), a direct descendent of Jarral Rajput Rajas of Rajauri, was selected and approved by the Amir to be the British Envoy to Kabul.
Abdur Rahman also paid attention to technological advance. He brought foreign physicians, engineers (especially for mining), geologists, and printers to Afghanistan. He imported European machinery and encouraged the establishment of small factories to manufacture soap, candles, and leather goods. He sought European technical advice on communications, transport, and irrigation. Local Afghan tribes strongly resisted this modernization. Workmen making roads had to be protected by the army against local warriors. Nonetheless, despite these sweeping internal policies, Abdur Rahman's foreign policy was completely in foreign hands.
The first important frontier dispute was the Panjdeh crisis of 1885, precipitated by Russian encroachment into Central Asia. Having seized the Merv (now Mary) Oasis by 1884, Russian forces were directly adjacent to Afghanistan. Claims to the Panjdeh Oasis were in debate, with the Russians keen to take over all the region's Turkoman domains. After battling Afghan forces in the spring of 1885, the Russians seized the oasis. Russian and British troops were quickly alerted, but the two powers reached a compromise; Russia was in possession of the oasis, and Britain believed it could keep the Russians from advancing any farther. Without an Afghan say in the matter, the Joint Anglo-Russian Boundary Commission agreed that the Russians would relinquish the farthest territory captured in their advance but retain Panjdeh. This agreement on these border sections delineated for Afghanistan a permanent northern frontier at the Amu Darya, but also involved the loss of much territory, especially around Panjdeh.
The second section of Afghan border demarcated during Abdur Rahman's reign was in the Wakhan. The British insisted that Abdur Rahman accept sovereignty over this remote region, where unruly Kyrgyz held sway; he had no choice but to accept Britain's compromise. In 1895 and 1896, another Joint Anglo-Russian Boundary Commission agreed on the frontier boundary to the far northeast of Afghanistan, which bordered Chinese territory (although the Chinese did not formally accept this as a boundary between the two countries until 1964.)
For Abdur Rahman, delineating the boundary with India (through the Pashtun area) was far more significant, and it was during his reign that the Durand Line was drawn. Under pressure, Abdur Rahman agreed in 1893 to accept a mission headed by the British Indian foreign secretary, Sir Mortimer Durand, to define the limits of British and Afghan control in the Pashtun territories. Boundary limits were agreed on by Durand and Abdur Rahman before the end of 1893, but there is some question about the degree to which Abdur Rahman willingly ceded certain regions. There were indications that he regarded the Durand Line as a delimitation of separate areas of political responsibility, not a permanent international frontier, and that he did not explicitly cede control over certain parts (such as Kurram and Chitral) that were already in British control under the Treaty of Gandamak.
The Durand Line cut through tribes and bore little relation to the realities of demography or military strategy. The line laid the foundation not for peace between the border regions, but for heated disagreement between the governments of Afghanistan and British India, and later, Afghanistan and Pakistan over what came to be known as the issue of Pashtunistan or 'Land of the Pashtuns'. (See Siege of Malakand).
The clearest manifestation that Abdur Rahman had established control in Afghanistan was the peaceful succession of his eldest son, Habibullah Khan, to the throne on his father's death in October 1901. Although Abdur Rahman had fathered many children, he groomed Habibullah to succeed him, and he made it difficult for his other sons to contest the succession by keeping power from them and sequestering them in Kabul under his control.
Habibullah Khan, 1901–1919.
Habibullah Khan, Abdur Rahman Khan's eldest son and child of a slave mother, kept a close watch on the palace intrigues revolving around his father's more distinguished wife (a granddaughter of Dost Mohammad), who sought the throne for her own son. Although made secure in his position as ruler by virtue of support from the army which was created by his father, Habibullah was not as domineering as Abdur Rahman. Consequently, the influence of religious leaders as well as that of Mahmud Tarzi, a cousin of the king, increased during his reign.
Mahmud Tarzi, a highly educated, well-traveled poet and journalist, founded an Afghan nationalist newspaper with Habibullah's agreement, and until 1919 he used the newspaper as a platform for rebutting clerical criticism of Western-influenced changes in government and society, for espousing full Afghan independence, and for other reforms. Tarzi's passionate Afghan nationalism influenced a future generation of Asian reformers.
The boundary with Iran was firmly delineated in 1904, replacing the ambiguous line made by a British commission in 1872. Agreement could not be reached, however, on sharing the waters of the Helmand River.
Like all foreign policy developments of this period affecting Afghanistan, the conclusion of the "Great Game" between Russia and Britain occurred without the Afghan ruler's participation. The 1907 Anglo-Russian Entente (the Convention of St. Petersburg) not only divided the region into separate areas of Russian and British influence but also established foundations for Afghan neutrality. The convention provided for Russian acquiescence that Afghanistan was now outside this sphere of influence, and for Russia to consult directly with Britain on matters relating to Russian-Afghan relations. Britain, for its part, would not occupy or annex Afghan territory, or interfere in Afghanistan's internal affairs.
During World War I, Afghanistan remained neutral despite pressure to support Turkey when its sultan proclaimed his nation's participation in what it considered a holy war. Habibullah did, however, entertain an Indo-German-Turkish mission in Kabul in 1915 that had as its titular head the Indian nationalist Mahendra Pratap and was led by Oskar Niedermayer and the German legate Werner Otto von Hentig. After much procrastination, he won an agreement from the Central Powers for a huge payment and arms provision in exchange for attacking British India. But the crafty Afghan ruler clearly viewed the war as an opportunity to play one side off against the other, for he also offered the British to resist a Central Powers attack on India in exchange for an end to British control of Afghan foreign policy.
Third Anglo-Afghan War and Independence.
Amanullah's ten years of reign initiated a period of dramatic change in Afghanistan in both foreign and domestic politics. Amanullah declared full independence and sparked the Third Anglo-Afghan War. Amanullah altered foreign policy in his new relations with external powers and transformed domestic politics with his social, political, and economic reforms. Although his reign ended abruptly, he achieved some notable successes, and his efforts failed as much due to the centrifugal forces of tribal Afghanistan and the machinations of Russia and Britain as to any political folly on his part.
Amanullah came to power just as the entente between Russia and Britain broke down following the Russian Revolution of 1917. Once again Afghanistan provided a stage on which the great powers played out their schemes against one another. Keen to modernise his country and free it from foreign domination, Amanullah, sought to shore up his powerbase. Amidst intrigue in the Afghan court, and political and civil unrest in India, he sought to divert attention from the internal divisions of Afghanistan and unite all faction behind him by attacking the British.
Using the civil unrest in India as an excuse to move troops to the Durand Line, Afghan troops crossed the border at the western end of the Khyber Pass on 3 May 1919 and occupied the village of Bagh, the scene of an earlier uprising in April. In response, the Indian government ordered a full mobilisation and on 6 May 1919 declared war. For the British it had come at a time when they were still recovering from the First World War. The troops that were stationed in India were mainly reserves and Territorials, who were awaiting demobilisation and keen to return to Britain, whilst the few regular regiments that were available were tired and depleted from five years of fighting.
Afghan forces achieved success in the initial days of the war, taking the British and Indians by surprise in two main thrusts as the Afghan regular army was joined by large numbers of Pashtun tribesmen from both sides of the border. A series of skirmishes then followed as the British and Indians recovered from their initial surprise. As a counterbalance to deficiencies in manpower and morale, the British had a considerable advantage in terms of equipment, possessing machine guns, armoured cars, motor transport, wireless communications and aircraft and it was the latter that would prove decisive.
British forces used airpower to shock the Afghans, and the King's home was directly attacked in what is the first case of aerial bombardment in Afghanistan's history. The attacks played a key role in forcing an armistice but brought an angry rebuke from King Amanullah. He wrote: "It is a matter of great regret that the throwing of bombs by zeppelins on London was denounced as a most savage act and the bombardment of places of worship and sacred spots was considered a most abominable operation. While we now see with our own eyes that such operations were a habit which is prevalent among all civilized people of the west"
The fighting concluded in August 1919 and Britain virtually dictated the terms of the Anglo-Afghan Treaty of 1919, a temporary armistice that provided, on one somewhat ambiguous interpretation, for Afghan self-determination in foreign affairs. Before final negotiations were concluded in 1921, however, Afghanistan had already begun to establish its own foreign policy without repercussions anyway, including diplomatic relations with the new government in the Soviet Union in 1919. During the 1920s, Afghanistan established diplomatic relations with most major countries.
Amanullah Khan, 1919–1929.
On 20 February 1919, Habibullah Khan was assassinated on a hunting trip. He had not declared a succession, but left his third son, Amanullah Khan, in charge in Kabul. Amanullah did have an older brother, Nasrullah Khan. But, because Amanullah controlled both the national treasury and the army, Amanullah was well situated to seize power. The army's support allowed Amanullah to suppress other claims and imprison those relatives who would not swear loyalty to him. Within a few months, the new amir had gained the allegiance of most tribal leaders and established control over the cities.
Amanullah Khan's reforms were heavily influenced by Europe. This came through the influence of Mahmud Tarzi, who was both Amanullah Khan's father-in-law and Foreign Minister. Mahmud Tarzi, a highly educated, well-traveled poet, journalist, and diplomat, was a key figure that brought Western dress and etiquette to Afghanistan. He also fought for progressive reforms such as woman's rights, educational rights, and freedom of press. All of these influences, brought by Tarzi and others, were welcomed by Amanullah Khan.
In 1926, Amanullah ended the Emirate of Afghanistan and proclaimed the Kingdom of Afghanistan with himself as king. In 1927 and 1928, King Amanullah Khan and his wife Soraya Tarzi visited Europe. On this trip they were honored and feted. In fact, in 1928 the King and Queen of Afghanistan received honorary degrees from Oxford University. This was an era when other Muslim nations, like Turkey and Egypt were also on the path to modernization. King Amanullah was so impressed with the social progress of Europe that he tried to implement them right away, this met with heavy resistance from the conservative sect and eventually led to his demise.
Amanullah enjoyed early popularity within Afghanistan and he used his power to modernize the country. Amanullah created new cosmopolitan schools for both boys and girls in the region and overturned centuries-old traditions such as strict dress codes for women. He created a new capital city and increased trade with Europe and Asia. He also advanced a modernist constitution that incorporated equal rights and individual freedoms. This rapid modernization though, created a backlash, and a reactionary uprising known as the "Khost rebellion" which was suppressed in 1924.
After Amanullah travelled to Europe in late 1927, opposition to his rule increased. An uprising in Jalalabad culminated in a march to the capital, and much of the army deserted rather than resist. On 14 January 1929, Amanullah abdicated in favor of his brother, King Inayatullah Khan. On 17 January, Inayatullah abdicated and Habibullah Kalakani became the next ruler of Afghanistan and restored the emirate. However, his rule was short lived and, on 17 October 1929, Habibullah Kalakani was overthrown and replaced by King Nadir Khan.
After his abdication in 1929, Amanullah went into temporary exile in India. When he attempted to return to Afghanistan, he had little support from the people. From India, the ex-king traveled to Europe and settled in Italy, and later in Switzerland. Meanwhile, Nadir Khan made sure his return to Afghanistan was impossible by engaging in a propaganda war. Nadir Khan accused Amanullah Khan of kufr with his pro western policies.
Mohammed Zahir Shah, 1933–1973.
In 1933, after the assassination of Nadir Khan, Mohammed Zahir Shah became king.

</doc>
<doc id="9059" url="https://en.wikipedia.org/wiki?curid=9059" title="Dementia praecox">
Dementia praecox

Dementia praecox (a "premature dementia" or "precocious madness") is a chronic, deteriorating psychotic disorder characterized by rapid cognitive disintegration, usually beginning in the late teens or early adulthood. The term was first used in 1891 by Arnold Pick (1851–1924), a professor of psychiatry at Charles University in Prague. His brief clinical report described the case of a person with a psychotic disorder resembling hebephrenia. German psychiatrist Emil Kraepelin (1856–1926) popularised it in his first detailed textbook descriptions of a condition that eventually became a different disease concept and relabeled as schizophrenia. Kraepelin reduced the complex psychiatric taxonomies of the nineteenth century by dividing them into two classes: manic-depressive psychosis and dementia praecox. This division, commonly referred to as the Kraepelinian dichotomy, had a fundamental impact on twentieth-century psychiatry, though it has also been questioned.
The primary disturbance in dementia praecox is a disruption in cognitive or mental functioning in attention, memory, and goal-directed behaviour. Kraepelin contrasted this with manic-depressive psychosis, now termed bipolar disorder, and also with other forms of mood disorder, including major depressive disorder. He eventually concluded that it was not possible to distinguish his categories on the basis of cross-sectional symptoms.
Kraepelin viewed dementia praecox as a progressively deteriorating disease from which no one recovered. However, by 1913, and more explicitly by 1920, Kraepelin admitted that while there may be a residual cognitive defect in most cases, the prognosis was not as uniformly dire as he had stated in the 1890s. Still, he regarded it as a specific disease concept that implied incurable, inexplicable madness.
History.
First use of the term.
"Dementia" is an ancient term which has been in use since at least the time of Lucretius in 50 B.C.E. where it meant "being out of one's mind". Until the seventeenth century dementia referred to states of cognitive and behavioural deterioration leading to psychosocial incompetence. This condition could be innate or acquired and the concept had no reference to a necessarily irreversible condition. It is the concept in this popular notion of psychosocial incapacity that forms the basis for the idea of legal incapacity. By the eighteenth century, at the period when the term entered into European medical discourse, clinical concepts were added to the vernacular understanding such that dementia was now associated with intellectual deficits arising from any cause and at any age. By the end of the nineteenth century the modern 'cognitive paradigm' of dementia was taking root. This holds that dementia is understood in terms of criteria relating to aetiology, age and course which excludes former members of the family of the demented such as adults with acquired head trauma or children with cognitive deficits. Moreover, it was now understood as an irreversible condition and a particular emphasis was placed on memory loss in regard to the deterioration of intellectual functions.
The term "démence précoce" was used in passing to describe the characteristics of a subset of young mental patients by the French physician Bénédict Augustin Morel in 1852 in the first volume of his "Études cliniques." and the term is used more frequently in his textbook "Traité des maladies mentales" which was published in 1860. Morel, whose name will be forever associated with religiously inspired concept of degeneration theory in psychiatry, used the term in a descriptive sense and not to define a specific and novel diagnostic category. It was applied as a means of setting apart a group of young men and women who were suffering from "stupor." As such their condition was characterised by a certain torpor, enervation, and disorder of the will and was related to the diagnostic category of melancholia. He did not conceptualise their state as irreversible and thus his use of the term dementia was equivalent to that formed in the eighteenth century as outlined above.
While some have sought to interpret, if in a qualified fashion, the use by Morel of the term "démence précoce" as amounting to the "discovery" of schizophrenia, others have argued convincingly that Morel's descriptive use of the term should not be considered in any sense as a precursor to Kraepelin's dementia praecox disease concept. This is due to the fact that their concepts of dementia differed significantly from each other, with Kraepelin employing the more modern sense of the word and that Morel was not describing a diagnostic category. Indeed, until the advent of Pick and Kraepelin, Morel's term had vanished without a trace and there is little evidence to suggest that either Pick or indeed Kraepelin were even aware of Morel's use of the term until long after they had published their own disease concepts bearing the same name. As Eugène Minkowski succinctly stated, 'An abyss separates Morel's démence précoce from that of Kraepelin.'
Morel described several psychotic disorders that ended in dementia, and as a result he may be regarded as the first alienist or psychiatrist to develop a diagnostic system based on presumed outcome rather than on the current presentation of signs and symptoms. Morel, however, did not conduct any long-term or quantitative research on the course and outcome of dementia praecox (Kraepelin would be the first in history to do that) so this prognosis was based on speculation. It is impossible to discern whether the condition briefly described by Morel was equivalent to the disorder later called dementia praecox by Pick and Kraepelin.
The time component.
Psychiatric nosology in the nineteenth-century was chaotic and characterised by a conflicting mosaic of contradictory systems. Psychiatric disease categories were based upon short-term and cross-sectional observations of patients from which were derived the putative characteristic signs and symptoms of a given disease concept.
The dominant psychiatric paradigms which gave a semblance of order to this fragmentary picture were Morelian degeneration theory and the concept of "unitary psychosis" ("Einheitspsychose"). This latter notion, derived from the Belgian psychiatrist Joseph Guislain (1797–1860), held that the variety of symptoms attributed to mental illness were manifestations of a single underlying disease process. While these approaches had a diachronic aspect they lacked a conception of mental illness that encompassed a coherent notion of change over time in terms of the natural course of the illness and based upon an empirical observation of changing symptomatology.
In 1863, the Danzig based psychiatrist, Karl Ludwig Kahlbaum (1828–1899), published his text on psychiatric nosology "Die Gruppierung der psychischen Krankheiten (The Classification of Psychiatric Diseases)". Although with the passage of time this work would prove profoundly influential, when it was published it was almost completely ignored by German academia despite the sophisticated and intelligent disease classification system which it proposed. In this book Kahlbaum categorized certain typical forms of psychosis ("vesania typica") as a single coherent type based upon their shared progressive nature which betrayed, he argued, an ongoing degenerative disease process. For Kahlbaum the disease process of "vesania typica" was distinguished by the passage of the sufferer through clearly defined disease phases: a melancholic stage; a manic stage; a confusional stage; and finally a demented stage.
In 1866 Kahlbaum became the director of a private psychiatric clinic in Görlitz (Prussia, today Saxony, a small town near Dresden). He was accompanied by his younger assistant, Ewald Hecker (1843–1909), and during a ten-year collaboration they conducted a series of research studies on young psychotic patients that would become a major influence on the development of modern psychiatry.
Together Kahlbaum and Hecker were the first to describe and name such syndromes as dysthymia, cyclothymia, paranoia, catatonia, and hebephrenia. Perhaps their most lasting contribution to psychiatry was the introduction of the "clinical method" from medicine to the study of mental diseases, a method which is now known as psychopathology.
When the element of time was added to the concept of diagnosis, a diagnosis became more than just a description of a collection of symptoms: diagnosis now also defined by prognosis (course and outcome). An additional feature of the clinical method was that the characteristic symptoms that define syndromes should be described without any prior assumption of brain pathology (although such links would be made later as scientific knowledge progressed). Karl Kahlbaum made an appeal for the adoption of the clinical method in psychiatry in his 1874 book on catatonia. Without Kahlbaum and Hecker there would be no dementia praecox.
Upon his appointment to a full professorship in psychiatry at the University of Dorpat (now Tartu, Estonia) in 1886, Kraepelin gave an inaugural address to the faculty outlining his research programme for the years ahead. Attacking the "brain mythology" of Meynert and the positions of Griesinger and Gudden, Kraepelin advocated that the ideas of Kahlbaum, who was then a marginal and little known figure in psychiatry, should be followed. Therefore, he argued, a research programme into the nature of psychiatric illness should look at a large number of patients over time to discover the course which mental disease could take.
It has also been suggested that Kraepelin's decision to accept the Dorpat post was informed by the fact that there he could hope to gain experience with chronic patients and this, it was presumed, would facilitate the longitudinal study of mental illness.
The quantitative component.
Understanding that objective diagnostic methods must be based on scientific practice, Kraepelin had been conducting psychological and drug experiments on patients and normal subjects for some time when, in 1891, he left Dorpat and took up a position as professor and director of the psychiatric clinic at Heidelberg University. There he established a research program based on Kahlbaum's proposal for a more exact qualitative clinical approach, and his own innovation: a quantitative approach involving meticulous collection of data over time on each new patient admitted to the clinic (rather than only the interesting cases, as had been the habit until then).
Kraepelin believed that by thoroughly describing all of the clinic's new patients on index cards, which he had been using since 1887, researcher bias could be eliminated from the investigation process. He described the method in his posthumously published memoir:
The fourth edition of his textbook, "Psychiatrie", published in 1893, two years after his arrival at Heidelberg, contained some impressions of the patterns Kraepelin had begun to find in his index cards. Prognosis (course and outcome) began to feature alongside signs and symptoms in the description of syndromes, and he added a class of psychotic disorders designated "psychic degenerative processes", three of which were borrowed from Kahlbaum and Hecker: "dementia paranoides" (a degenerative type of Kahlbaum's paranoia, with sudden onset), "catatonia" (per Kahlbaum, 1874) and "dementia praecox", (Hecker's hebephrenia of 1871). Kraepelin continued to equate dementia praecox with hebephrenia for the next six years.
In the March 1896 fifth edition of "Psychiatrie", Kraepelin expressed confidence that his clinical method, involving analysis of both qualitative and quantitative data derived from long term observation of patients, would produce reliable diagnoses including prognosis:
In this edition dementia praecox is still essentially hebephrenia, and it, dementia paranoides and catatonia are described as distinct psychotic disorders among the "metabolic disorders leading to dementia".
Kraepelin's influence on the next century.
In the 1899 (6th) edition of "Psychiatrie", Kraepelin established a paradigm for psychiatry that would dominate the following century, sorting most of the recognized forms of insanity into two major categories: dementia praecox and manic-depressive illness. Dementia praecox was characterized by disordered intellectual functioning, whereas manic-depressive illness was principally a disorder of affect or mood; and the former featured constant deterioration, virtually no recoveries and a poor outcome, while the latter featured periods of exacerbation followed by periods of remission, and many complete recoveries. The class, dementia praecox, comprised the paranoid, catatonic and hebephrenic psychotic disorders, and these forms were found in the Diagnostic and Statistical Manual of Mental Disorders until the fifth edition was released, in May 2013. These terms, however, are still found in general psychiatric nomenclature. The ICD-10 still uses "hebephrenic" to designate the third type.
Change in prognosis.
In the seventh, 1904, edition of "Psychiatrie", Kraepelin accepted the possibility that a small number of patients may recover from dementia praecox. Eugen Bleuler reported in 1908 that in many cases there was no inevitable progressive decline, there was temporary remission in some cases, and there were even cases of near recovery with the retention of some residual defect. In the eighth edition of Kraepelin's textbook, published in four volumes between 1909 and 1915, he described eleven forms of dementia, and dementia praecox was classed as one of the "endogenous dementias". Modifying his previous more gloomy prognosis in line with Bleuler's observations, Kraepelin reported that about 26% of his patients experienced partial remission of symptoms. Kraepelin died while working on the ninth edition of "Psychiatrie" with Johannes Lange (1891–1938), who finished it and brought it to publication in 1927.
Etiology.
Though his work and that of his research associates had revealed a role for heredity, Kraepelin realized nothing could be said with certainty about the aetiology of dementia praecox, and he left out speculation regarding brain disease or neuropathology in his diagnostic descriptions. Nevertheless, from the 1896 edition onwards Kraepelin made clear his belief that poisoning of the brain, "auto-intoxication," probably by sex hormones, may underlie dementia praecox – a theory also entertained by Eugen Bleuler. Both theorists insisted dementia praecox is a biological disorder, not the product of psychological trauma. Thus, rather than a disease of hereditary degeneration or of structural brain pathology, Kraepelin believed dementia praecox was due to a systemic or "whole body" disease process, probably metabolic, which gradually affected many of the tissues and organs of the body before affecting the brain in a final, decisive cascade. Kraepelin, recognizing dementia praecox in Chinese, Japanese, Tamil and Malay patients, suggested in the eighth edition of "Psychiatrie" that, "we must therefore seek the real cause of dementia praecox in conditions which are spread all over the world, which thus do not lie in race or in climate, in food or in any other general circumstance of life..."
Treatment.
Kraepelin had experimented with hypnosis but found it wanting, and disapproved of Freud's and Jung's introduction, based on no evidence, of psychogenic assumptions to the interpretation and treatment of mental illness. He argued that, without knowing the underlying cause of dementia praecox or manic-depressive illness, there could be no disease-specific treatment, and recommended the use of long baths and the occasional use of drugs such as opiates and barbiturates for the amelioration of distress, as well as occupational activities, where suitable, for all institutionalized patients. Based on his theory that dementia praecox is the product of autointoxication emanating from the sex glands, Kraepelin experimented, without success, with injections of thyroid, gonad and other glandular extracts.
Use of term spreads.
Kraepelin noted the dissemination of his new disease concept when in 1899 he enumerated the term's appearance in almost twenty articles in the German-language medical press. In the early years of the twentieth century the twin pillars of the Kraepelinian dichotomy, dementia praecox and manic depressive psychosis, were assiduously adopted in clinical and research contexts among the Germanic psychiatric community. German-language psychiatric concepts were always introduced much faster in America (than, say, Britain) where émigré German, Swiss and Austrian physicians essentially created American psychiatry. Swiss-émigré Adolf Meyer (1866–1950), arguably the most influential psychiatrist in America for the first half of the 20th century, published the first critique of dementia praecox in an 1896 book review of the 5th edition of Kraepelin's textbook. But it was not until 1900 and 1901 that the first three American publications regarding dementia praecox appeared, one of which was a translation of a few sections of Kraepelin's 6th edition of 1899 on dementia praecox.
Adolf Meyer was the first to apply the new diagnostic term in America. He used it at the Worcester Lunatic Hospital in Massachusetts in the fall of 1896. He was also the first to apply Eugen Bleuler's term "schizophrenia" (in the form of "schizophrenic reaction") in 1913 at the Henry Phipps Psychiatric Clinic of the Johns Hopkins Hospital.
The dissemination of Kraepelin's disease concept to the Anglophone world was facilitated in 1902 when Ross Diefendorf, a lecturer in psychiatry at Yale, published an adapted version of the sixth edition of the "Lehrbuch der Psychiatrie". This was republished in 1904 and with a new version, based on the seventh edition of Kraepelin's "Lehrbuch" appearing in 1907 and reissued in 1912. Both dementia praecox (in its three classic forms) and "manic-depressive psychosis" gained wider popularity in the larger institutions in the eastern United States after being included in the official nomenclature of diseases and conditions for record-keeping at Bellevue Hospital in New York City in 1903. The term lived on due to its promotion in the publications of the National Committee on Mental Hygiene (founded in 1909) and the Eugenics Records Office (1910). But perhaps the most important reason for the longevity of Kraepelin's term was its inclusion in 1918 as an official diagnostic category in the uniform system adopted for comparative statistical record-keeping in all American mental institutions, "The Statistical Manual for the Use of Institutions for the Insane". Its many revisions served as the official diagnostic classification scheme in America until 1952 when the first edition of the "Diagnostic and Statistical Manual: Mental Disorders", or DSM-I, appeared. Dementia praecox disappeared from official psychiatry with the publication of DSM-I, replaced by the Bleuler/Meyer hybridization, "schizophrenic reaction".
Schizophrenia was mentioned as an alternate term for dementia praecox in the 1918 "Statistical Manual". In both clinical work as well as research, between 1918 and 1952 five different terms were used interchangeably: dementia praecox, schizophrenia, dementia praecox (schizophrenia), schizophrenia (dementia praecox) and schizophrenic reaction. This made the psychiatric literature of the time confusing since, in a strict sense, Kraepelin's disease was not Bleuler's disease. They were defined differently, had different population parameters, and different concepts of prognosis.
The reception of dementia praecox as an accepted diagnosis in British psychiatry came more slowly, perhaps only taking hold around the time of World War I. There was substantial opposition to the use of the term "dementia" as misleading, partly due to findings of remission and recovery. Some argued that existing diagnoses such as "delusional insanity" or "adolescent insanity" were better or more clearly defined. In France a psychiatric tradition regarding the psychotic disorders predated Kraepelin, and the French never fully adopted Kraepelin's classification system. Instead the French maintained an independent classification system throughout the 20th century. From 1980, when DSM-III totally reshaped psychiatric diagnosis, French psychiatry began to finally alter its views of diagnosis to converge with the North American system. Kraepelin thus finally conquered France via America.
From dementia praecox to schizophrenia.
Due to the influence of alienists such as Adolf Meyer, August Hoch, George Kirby, Charles Macphie Campbell, Smith Ely Jelliffe and William Alanson White, psychogenic theories of dementia praecox dominated the American scene by 1911. In 1925 Bleuler's schizophrenia rose in prominence as an alternative to Kraepelin's dementia praecox. When Freudian perspectives became influential in American psychiatry in the 1920s schizophrenia became an attractive alternative concept. Bleuler corresponded with Freud and was connected to Freud's psychoanalytic movement, and the inclusion of Freudian interpretations of the symptoms of schizophrenia in his publications on the subject, as well as those of C.G. Jung, eased the adoption of his broader version of dementia praecox (schizophrenia) in America over Kraepelin's narrower and prognostically more negative one.
The term "schizophrenia" was first applied by American alienists and neurologists in private practice by 1909 and officially in institutional settings in 1913, but it took many years to catch on. It is first mentioned in "The New York Times" in 1925. Until 1952 the terms dementia praecox and schizophrenia were used interchangeably in American psychiatry, with occasional use of the hybrid terms "dementia praecox (schizophrenia)" or "schizophrenia (dementia praecox)".
Diagnostic manuals.
Editions of the Diagnostic and Statistic Manual of Mental Disorders since the first in 1952 had reflected views of schizophrenia as "reactions" or "psychogenic" (DSM-I), or as manifesting Freudian notions of "defense mechanisms" (as in DSM-II of 1969 in which the symptoms of schizophrenia were interpreted as "psychologically self-protected"). The diagnostic criteria were vague, minimal and wide, including either concepts that no longer exist or that are now labeled as personality disorders (for example, schizotypal personality disorder). There was also no mention of the dire prognosis Kraepelin had made. Schizophrenia seemed to be more prevalent and more psychogenic and more treatable than either Kraepelin or Bleuler would have allowed.
Conclusions.
As a direct result of the effort to construct Research Diagnostic Criteria (RDC) in the 1970s that were independent of any clinical diagnostic manual, Kraepelin's idea that categories of mental disorder should reflect discrete and specific disease entities with a biological basis began to return to prominence. Vague dimensional approaches based on symptoms—so highly favored by the Meyerians and psychoanalysts—were overthrown. For research purposes, the definition of schizophrenia returned to the narrow range allowed by Kraepelin's dementia praecox concept. Furthermore, after 1980 the disorder was a progressively deteriorating one once again, with the notion that recovery, if it happened at all, was rare. This revision of schizophrenia became the basis of the diagnostic criteria in DSM-III (1980). Some of the psychiatrists who worked to bring about this revision referred to themselves as the "neo-Kraepelinians".

</doc>
<doc id="9061" url="https://en.wikipedia.org/wiki?curid=9061" title="Dolphin">
Dolphin

Dolphins are a widely distributed and diverse group of fully aquatic marine mammals. They are an informal grouping within the order Cetacea, excluding whales and porpoises, so to zoologists the grouping is paraphyletic. The dolphins comprise the extant families Delphinidae (the oceanic dolphins), Platanistidae (the Indian river dolphins), Iniidae (the new world river dolphins), and Pontoporiidae (the brackish dolphins). There are 40 extant species of dolphins. Dolphins, alongside other cetaceans, belong to the clade Cetartiodactyla with even-toed ungulates, and their closest living relatives are the hippopotamuses, having diverged about 40 million years ago.
Dolphins range in size from the long and Maui's dolphin to the and killer whale. Several species exhibit sexual dimorphism, in that the males are larger than females. They have streamlined bodies and two limbs that are modified into flippers. Though not quite as flexible as seals, some dolphins can travel at . Dolphins use their conical shaped teeth to capture fast moving prey. They have well-developed hearing − their hearing, which is adapted for both air and water, is so well developed that some can survive even if they are blind. Some species are well adapted for diving to great depths. They have a layer of fat, or blubber, under the skin to keep warm in the cold water.
Although dolphins are widespread, most species prefer the warmer waters of the tropic zones, but some, like the right whale dolphin, prefer colder climates. Dolphins feed largely on fish and squid, but a few, like the killer whale, feed on large mammals, like seals. Male dolphins typically mate with multiple females every year, but females only mate every two to three years. Calves are typically born in the spring and summer months and females bear all the responsibility for raising them. Mothers of some species fast and nurse their young for a relatively long period of time. Dolphins produce a variety of vocalizations, usually in the form of clicks and whistles.
Dolphins are sometimes hunted in places like Japan, in an activity known as dolphin drive hunting. Besides drive hunting, they also face threats from bycatch, habitat loss, and marine pollution. Dolphins have been depicted in various cultures worldwide. Dolphins occasionally feature in literature and film, as in the Warner Bros film Free Willy. Dolphins are sometimes kept in captivity and trained to perform tricks, but breeding success has been poor and the animals often die within a few months of capture. The most common dolphins kept are the killer whales and bottlenose dolphins.
Etymology.
The name is originally from Greek ("delphís"), "dolphin", which was related to the Greek ("delphus"), "womb". The animal's name can therefore be interpreted as meaning "a 'fish' with a womb". The name was transmitted via the Latin "delphinus" (the romanization of the later Greek δελφῖνος – "delphinos"), which in Medieval Latin became "dolfinus" and in Old French "daulphin", which reintroduced the "ph" into the word. The term mereswine (that is, "sea pig") has also historically been used.
The term 'dolphin' can be used to refer to, under the parvorder Odontoceti, all the species in the family Delphinidae (oceanic dolphins) and the river dolphin families Iniidae (South American river dolphins), Pontoporiidae (La Plata dolphin), Lipotidae (Yangtze river dolphin) and Platanistidae (Ganges river dolphin and Indus river dolphin).
This term has often been misused in the US, mainly in the fishing industry, where all small cetaceans (dolphins and porpoises) are considered porpoises, while the fish "dorado" is called dolphin fish. In common usage the term 'whale' is used only for the larger cetacean species, while the smaller ones with a beaked or longer nose are considered 'dolphins'. The name 'dolphin' is used casually as a synonym for bottlenose dolphin, the most common and familiar species of dolphin. There are six species of dolphins commonly thought of as whales, collectively known as blackfish: the killer whale, the melon-headed whale, the pygmy killer whale, the false killer whale, and the two species of pilot whales, all of which are classified under the family Delphinidae and qualify as dolphins. Though the terms 'dolphin' and 'porpoise' are sometimes used interchangeably, porpoises are not considered dolphins and have different physical features such as a shorter beak and spade-shaped teeth; they also differ in their behavior. Porpoises belong to the family Phocoenidae and share a common ancestry with the Delphinidae.
A group of dolphins is called a "school" or a "pod". Male dolphins are called "bulls", females "cows" and young dolphins are called "calves".
Taxonomy.
Hybridization.
In 1933, three strange dolphins beached off the Irish coast; they appeared to be hybrids between Risso's and bottlenose dolphins. This mating was later repeated in captivity, producing a hybrid calf. In captivity, a bottlenose and a rough-toothed dolphin produced hybrid offspring. A common-bottlenose hybrid lives at SeaWorld California. Other dolphin hybrids live in captivity around the world or have been reported in the wild, such as a bottlenose-Atlantic spotted hybrid. The best known hybrid is the wolphin, a false killer whale-bottlenose dolphin hybrid. The wolphin is a fertile hybrid. Two wolphins currently live at the Sea Life Park in Hawaii; the first was born in 1985 from a male false killer whale and a female bottlenose. Wolphins have also been observed in the wild.
Evolution.
Dolphins are descendants of land-dwelling mammals of the artiodactyl order (even-toed ungulates). They are related to the "Indohyus", an extinct chevrotain-like ungulate, from which they split approximately 48 million years ago.
The primitive cetaceans, or archaeocetes, first took to the sea approximately 49 million years ago and became fully aquatic by 5–10 million years later.
Archaeoceti is a parvorder comprising ancient whales. These ancient whales are the predecessors of modern whales, stretching back to their first ancestor that spent their lives near (rarely in) the water. Likewise, the archaeocetes can be anywhere from near fully terrestrial, to semi-aquatic to fully aquatic, but what defines an archaeocete is the presence of visible legs or asymmetrical teeth. Their features became adapted for living in the marine environment. Major anatomical changes include the hearing set-up that channeled vibrations from the jaw to the earbone which occurred with "Ambulocetus" 49 million years ago, a streamlining of the body and the growth of flukes on the tail which occurred around 43 million years ago with "Protocetus", the migration of the nasal openings toward the top of the cranium and the modification of the forelimbs into flippers which occurred with "Basilosaurus" 35 million years ago, and the shrinking and eventual disappearance of the hind limbs which took place with the first odontocetes and mysticetes 34 million years ago. The modern dolphin skeleton has two small, rod-shaped pelvic bones thought to be vestigial hind limbs. In October 2006, an unusual bottlenose dolphin was captured in Japan; it had small fins on each side of its genital slit, which scientists believe to be an unusually pronounced development of these vestigial hind limbs.
Today, the closest living relatives of cetaceans are the hippopotamuses; these share a semi-aquatic ancestor that branched off from other artiodactyls some 60 million years ago. Around 40 million years ago, a common ancestor between the two branched off into cetacea and anthracotheres; anthracotheres became extinct at the end of the Pleistocene two-and-a-half million years ago, eventually leaving only one surviving lineage: the hippo.
Biology.
Anatomy.
Dolphins have torpedo shaped bodies with non-flexible necks, limbs modified into flippers, non-existent external ear flaps, a tail fin, and bulbous heads. Dolphin skulls have small eye orbits, long snouts, and eyes placed on the sides of its head. Dolphins range in size from the long and Maui's dolphin to the and killer whale. Overall, however, they tend to be dwarfed by other Cetartiodactyls. Several species have female-biased sexual dimorphism, with the females being larger than the males.
Dolphins have conical shape teeth, as apposed to their counterparts, porpoise's, spade-shaped teeth. These conical teeth are used to catch swift prey such as fish, squid or large mammals, such as seal.
Breathing involves expelling stale air from their blowhole, forming an upward, steamy spout, followed by inhaling fresh air into the lungs, however this only occurs in the polar regions of the oceans. Dolphins have rather small, unidentifiable spouts.
All dolphins have a thick layer of blubber, thickness varying on climate. This blubber can help with buoyancy, protection to some extent as predators would have a hard time getting through a thick layer of fat, and energy for leaner times; the primary usage for blubber is insulation from the harsh climate. Calves, generally, are born with a thin layer of blubber, which develops at different paces depending on the habitat.
Dolphins have a two-chambered stomach that is similar in structure to terrestrial carnivores. They have fundic and pyloric chambers.
Locomotion.
Dolphins have two flippers on the underside toward the head, a dorsal fin and a tail fin. These flippers contain four digits. Although dolphins do not possess fully developed hind limbs, some possess discrete rudimentary appendages, which may contain feet and digits. Dolphins are fast swimmers in comparison to seals who typically cruise at ; the killer whale, in comparison, can travel at speeds up to . The fusing of the neck vertebrae, while increasing stability when swimming at high speeds, decreases flexibility, which means they are unable to turn their heads. River dolphins, however, have non-fused neck vertebrae and are able to turn their head up to 90°. Dolphins swim by moving their tail fin and rear body vertically, while their flippers are mainly used for steering. Some species log out of the water, which may allow them to travel faster. Their skeletal anatomy allows them to be fast swimmers. All species have a dorsal fin to prevent themselves from involuntarily spinning in the water.
Some dolphins are adapted for diving to great depths. In addition to their streamlined bodies, some can slow their heart rate to conserve oxygen. Some can also re-route blood from tissue tolerant of water pressure to the heart, brain and other organs. Their hemoglobin and myoglobin store oxygen in body tissues and they have twice the concentration of myoglobin than hemoglobin.
Senses.
The dolphin ear has specific adaptations to the marine environment. In humans, the middle ear works as an impedance equalizer between the outside air's low impedance and the cochlear fluid's high impedance. In dolphins, and other marine mammals, there is no great difference between the outer and inner environments. Instead of sound passing through the outer ear to the middle ear, dolphins receive sound through the throat, from which it passes through a low-impedance fat-filled cavity to the inner ear. The dolphin ear is acoustically isolated from the skull by air-filled sinus pockets, which allow for greater directional hearing underwater. Dolphins send out high frequency clicks from an organ known as a melon. This melon consists of fat, and the skull of any such creature containing a melon will have a large depression. This allows dolphins to produce biosonar for orientation. Though most dolphins do not have hair, they do have hair follicles that may perform some sensory function. Beyond locating an object, echolocation also provides the animal with an idea on an object's shape and size, though how exactly this works is not yet understood. The small hairs on the rostrum of the Boto are believed to function as a tactile sense, possibly to compensate for the Boto's poor eyesight.
The dolphin eye is relatively small for its size, yet they do retain a good degree of eyesight. As well as this, the eyes of a dolphin are placed on the sides of its head, so their vision consists of two fields, rather than a binocular view like humans have. When dolphins surface, their lens and cornea correct the nearsightedness that results from the refraction of light; they contain both rod and cone cells, meaning they can see in both dim and bright light, but they have far more rod cells than they do cone cells. Dolphins do, however, lack short wavelength sensitive visual pigments in their cone cells indicating a more limited capacity for color vision than most mammals. Most dolphins have slightly flattened eyeballs, enlarged pupils (which shrink as they surface to prevent damage), slightly flattened corneas and a tapetum lucidum; these adaptations allow for large amounts of light to pass through the eye and, therefore, a very clear image of the surrounding area. They also have glands on the eyelids and outer corneal layer that act as protection for the cornea.
The olfactory lobes are absent in dolphins, suggesting that they have no sense of smell.
Dolphins are not thought to have a good sense of taste, as their taste buds are atrophied or missing altogether. However, some have preferences between different kinds of fish, indicating some sort of attachment to taste.
Behavior.
Dolphins are often regarded as one of Earth's most intelligent animals, though it is hard to say just how intelligent. Comparing species' relative intelligence is complicated by differences in sensory apparatus, response modes, and nature of cognition. Furthermore, the difficulty and expense of experimental work with large aquatic animals has so far prevented some tests and limited sample size and rigor in others. Compared to many other species, however, dolphin behavior has been studied extensively, both in captivity and in the wild. See cetacean intelligence for more details.
Social behavior.
Dolphins are highly social animals, often living in pods of up to a dozen individuals, though pod sizes and structures vary greatly between species and locations. In places with a high abundance of food, pods can merge temporarily, forming a "superpod"; such groupings may exceed 1,000 dolphins. Membership in pods is not rigid; interchange is common. Dolphins can, however, establish strong social bonds; they will stay with injured or ill individuals, even helping them to breathe by bringing them to the surface if needed. This altruism does not appear to be limited to their own species. The dolphin "Moko" in New Zealand has been observed guiding a female Pygmy Sperm Whale together with her calf out of shallow water where they had stranded several times. They have also been seen protecting swimmers from sharks by swimming circles around the swimmers or charging the sharks to make them go away.
Dolphins communicate using a variety of clicks, whistle-like sounds and other vocalizations. Dolphins also use nonverbal communication by means of touch and posturing.
Dolphins also display culture, something long believed to be unique to humans (and possibly other primate species). In May 2005, a discovery in Australia found Indo-Pacific bottlenose dolphins ("Tursiops aduncus") teaching their young to use tools. They cover their snouts with sponges to protect them while foraging. This knowledge is mostly transferred by mothers to daughters, unlike simian primates, where knowledge is generally passed on to both sexes. Using sponges as mouth protection is a learned behavior. Another learned behavior was discovered among river dolphins in Brazil, where some male dolphins use weeds and sticks as part of a sexual display.
Forms of care-giving between fellows and even for members of different species (see Moko (dolphin)) are recorded in various species - such as trying to save weakened fellows or female pilot whales holding up dead calves for long periods.
Dolphins engage in acts of aggression towards each other. The older a male dolphin is, the more likely his body is to be covered with bite scars. Male dolphins engage in acts of aggression apparently for the same reasons as humans: disputes between companions and competition for females. Acts of aggression can become so intense that targeted dolphins sometimes go into exile after losing a fight.
Male bottlenose dolphins have been known to engage in infanticide. Dolphins have also been known to kill porpoises for reasons which are not fully understood, as porpoises generally do not share the same diet as dolphins and are therefore not competitors for food supplies.
Reproduction and sexuality.
Dolphins' reproductive organs are located on the underside of the body. Males have two slits, one concealing the penis and one further behind for the anus. The female has one genital slit, housing the vagina and the anus. Two mammary slits are positioned on either side of the female's genital slit.
Dolphin copulation happens belly to belly; though many species engage in lengthy foreplay, the actual act is usually brief, but may be repeated several times within a short timespan. The gestation period varies with species; for the small Tucuxi dolphin, this period is around 11 to 12 months, while for the orca, the gestation period is around 17 months. Typically dolphins give birth to a single calf, which is, unlike most other mammals, born tail first in most cases. They usually become sexually active at a young age, even before reaching sexual maturity. The age of sexual maturity varies by species and gender.
Dolphins are known to display non-reproductive sexual behavior, engaging in masturbation, stimulation of the genital area of other individuals using the rostrum or flippers, and homosexual contact.
Male dolphins have been known to masturbate by wrapping a live eel around their penis.
Various species of dolphin have been known to engage in sexual behavior up to and including copulation with dolphins of other species. Sexual encounters may be violent, with male dolphins sometimes showing aggressive behavior towards both females and other males. Male dolphins may also work together and attempt to herd females in estrus, keeping the females by their side by means of both physical aggression and intimidation, to increase their chances of reproductive success. Occasionally, dolphins behave sexually towards other animals, including humans.
Feeding.
Various methods of feeding exist among and within species, some apparently exclusive to a single population. Fish and squid are the main food, but the false killer whale and the orca also feed on other marine mammals. Orcas on occasion also hunt whale species larger than themselves.
One common feeding method is herding, where a pod squeezes a school of fish into a small volume, known as a bait ball. Individual members then take turns plowing through the ball, feeding on the stunned fish. Coralling is a method where dolphins chase fish into shallow water to catch them more easily. Orcas and bottlenose dolphins have also been known to drive their prey onto a beach to feed on it, a behaviour known as beach or strand feeding. Some species also whack fish with their flukes, stunning them and sometimes knocking them out of the water.
Reports of cooperative human-dolphin fishing date back to the ancient Roman author and natural philosopher Pliny the Elder. A modern human-dolphin partnership currently operates in Laguna, Santa Catarina, Brazil. Here, dolphins drive fish towards fishermen waiting along the shore and signal the men to cast their nets. The dolphins' reward is the fish that escape the nets.
Vocalizations.
Dolphins are capable of making a broad range of sounds using nasal airsacs located just below the blowhole. Roughly three categories of sounds can be identified: frequency modulated whistles, burst-pulsed sounds and clicks. Dolphins communicate with whistle-like sounds produced by vibrating connective tissue, similar to the way human vocal cords function, and through burst-pulsed sounds, though the nature and extent of that ability is not known. The clicks are directional and are for echolocation, often occurring in a short series called a click train. The click rate increases when approaching an object of interest. Dolphin echolocation clicks are amongst the loudest sounds made by marine animals.
Bottlenose dolphins have been found to have signature whistles, a whistle that is unique to a specific individual. These whistles are used in order for dolphins to communicate with one another by identifying an individual. It can be seen as the dolphin equivalent of a name for humans. These signature whistles are developed during a dolphin's first year; it continues to maintain the same sound throughout its lifetime. In order to obtain each individual whistle sound, dolphins undergo vocal production learning. This consists of an experience with other dolphins that modifies the signal structure of an existing whistle sound. An auditory experience influences the whistle development of each dolphin. Dolphins are able to communicate to one another by addressing another dolphin through mimicking their whistle. The signature whistle of a male bottlenose dolphin tends to be similar to that of their mother, while the signature whistle of a female bottlenose dolphin tends to be more unique. Bottlenose dolphins have a strong memory when it comes to these signature whistles, as they are able to relate to a signature whistle of an individual they have not encountered for over twenty years. Research done on signature whistle usage by other dolphin species is relatively limited. The research on other species done so far has yielded varied outcomes and inconclusive results.
Because dolphins are generally associated in groups, communication is necessary. Signal masking is when other similar sounds (conspecific sounds) interfere with the original acoustic sound. In larger groups, individual whistle sounds are less prominent. Dolphins tend to travel in pods, upon which there are groups of dolphins that range from a few to many. Although they are traveling in these pods, the dolphins do not necessarily swim right next to each other. Rather, they swim within the same general vicinity. In order to prevent losing one of their pod members, there are higher whistle rates. Because their group members were spread out, this was done in order to continue traveling together.
Jumping and playing.
Dolphins frequently leap above the water surface, this being done for various reasons. When travelling, jumping can save the dolphin energy as there is less friction while in the air. This type of travel is known as porpoising. Other reasons include orientation, social displays, fighting, non-verbal communication, entertainment and attempting to dislodge parasites.
Dolphins show various types of playful behavior, often including objects, self-made bubble rings, other dolphins or other animals. When playing with objects or small animals, common behavior includes carrying the object or animal along using various parts of the body, passing it along to other members of the group or taking it from another member, or throwing it out of the water. Dolphins have also been observed harassing animals in other ways, for example by dragging birds underwater without showing any intent to eat them. Playful behaviour that involves an other animal species with active participation of the other animal can also be observed however. Playful human interaction with dolphins being the most obvious example, however playful interactions have been observed in the wild with a number of other species as well, such as Humpback Whales and dogs.
Intelligence.
Dolphins are known to teach, learn, cooperate, scheme, and grieve. The neocortex of many species is home to elongated spindle neurons that, prior to 2007, were known only in hominids. In humans, these cells are involved in social conduct, emotions, judgment, and theory of mind. Cetacean spindle neurons are found in areas of the brain that are homologous to where they are found in humans, suggesting that they perform a similar function.
Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the ⅔ or ¾ exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalization quotient that can be used as another indication of animal intelligence. Killer whales have the second largest brain mass of any animal on earth, next to the sperm whale. The brain to body mass ratio in some is second only to humans.
Self-awareness is seen, by some, to be a sign of highly developed, abstract thinking. Self-awareness, though not well-defined scientifically, is believed to be the precursor to more advanced processes like meta-cognitive reasoning (thinking about thinking) that are typical of humans. Research in this field has suggested that cetaceans, among others, possess self-awareness.
The most widely used test for self-awareness in animals is the mirror test in which a temporary dye is placed on an animal's body, and the animal is then presented with a mirror; they then see if the animal shows signs of self-recognition.
Some disagree with these findings, arguing that the results of these tests are open to human interpretation and susceptible to the Clever Hans effect. This test is much less definitive than when used for primates, because primates can touch the mark or the mirror, while cetaceans cannot, making their alleged self-recognition behavior less certain. Skeptics argue that behaviors that are said to identify self-awareness resemble existing social behaviors, and so researchers could be misinterpreting self-awareness for social responses to another individual. The researchers counter-argue that the behaviors shown are evidence of self-awareness, as they are very different from normal responses to another individual. Whereas apes can merely touch the mark on themselves with their fingers, cetaceans show less definitive behavior of self-awareness; they can only twist and turn themselves to observe the mark.
In 1995, Marten and Psarakos used television to test dolphin self-awareness. They showed dolphins real-time footage of themselves, recorded footage, and another dolphin. They concluded that their evidence suggested self-awareness rather than social behavior. While this particular study has not been repeated since then, dolphins have since passed the mirror test.
Sleeping.
Generally, dolphins sleep with only one brain hemisphere in slow-wave sleep at a time, thus maintaining enough consciousness to breathe and to watch for possible predators and other threats. Earlier sleep stages can occur simultaneously in both hemispheres.
In captivity, dolphins seemingly enter a fully asleep state where both eyes are closed and there is no response to mild external stimuli. In this case, respiration is automatic; a tail kick reflex keeps the blowhole above the water if necessary. Anesthetized dolphins initially show a tail kick reflex. Though a similar state has been observed with wild sperm whales, it is not known if dolphins in the wild reach this state. The Indus river dolphin has a sleep method that is different from that of other dolphin species. Living in water with strong currents and potentially dangerous floating debris, it must swim continuously to avoid injury. As a result, this species sleeps in very short bursts which last between 4 and 60 seconds.
Threats.
Natural threats.
Dolphins have few natural enemies. Some species or specific populations have none, making them apex predators. For most of the smaller species of dolphins, only a few of the larger sharks, such as the bull shark, dusky shark, tiger shark and great white shark, are a potential risk, especially for calves. Some of the larger dolphin species, especially orcas (killer whales), may also prey on smaller dolphins, but this seems rare. Dolphins also suffer from a wide variety of diseases and parasites. The Cetacean morbillivirus in particular has been known to cause regional epizootics often leaving hundreds of animals of various species dead. Symptoms of infection are often a severe combination of pneumonia, encephalitis and damage to the immune system, which greatly impair the cetacean's ability to swim and stay afloat unassisted. A study at the U.S. National Marine Mammal Foundation revealed that dolphins, like humans, develop a natural form of type 2 diabetes which may lead to a better understanding of the disease and new treatments for both humans and dolphins.
Dolphins can tolerate and recover from extreme injuries such as shark bites although the exact methods used to achieve this are not known. The healing process is rapid and even very deep wounds do not cause dolphins to hemorrhage to death. Furthermore, even gaping wounds restore in such a way that the animal's body shape is restored, and infection of such large wounds seems rare.
Human threats.
Some dolphin species face an uncertain future, especially some river dolphin species such as the Amazon river dolphin, and the Ganges and Yangtze river dolphin, which are critically or seriously endangered. A 2006 survey found no individuals of the Yangtze river dolphin, which now appears to be functionally extinct.
Pesticides, heavy metals, plastics, and other industrial and agricultural pollutants that do not disintegrate rapidly in the environment concentrate in predators such as dolphins. Injuries or deaths due to collisions with boats, especially their propellers, are also common.
Various fishing methods, most notably purse seine fishing for tuna and the use of drift and gill nets, unintentionally kill many dolphins. Accidental by-catch in gill nets and incidental captures in antipredator nets that protect marine fish farms are common and pose a risk for mainly local dolphin populations. In some parts of the world, such as Taiji in Japan and the Faroe Islands, dolphins are traditionally considered food and are killed in harpoon or drive hunts. Dolphin meat is high in mercury and may thus pose a health danger to humans when consumed.
Dolphin safe labels attempt to reassure consumers that fish and other marine products have been caught in a dolphin-friendly way. The earliest campaigns with "Dolphin safe" labels were initiated in the 1980s as a result of cooperation between marine activists and the major tuna companies, and involved decreasing incidental dolphin kills by up to 50% by changing the type of nets used to catch tuna. The dolphins are netted only while fishermen are in pursuit of smaller tuna. Albacore are not netted this way, making albacore the only truly dolphin-safe tuna.
Loud underwater noises, such as those resulting from naval sonar use, live firing exercises, and certain offshore construction projects such as wind farms, may be harmful to dolphins, increasing stress, damaging hearing, and causing decompression sickness by forcing them to surface too quickly to escape the noise.
Dolphins and other smaller cetaceans are also hunted in an activity known as dolphin drive hunting. This is accomplished by driving a pod together with boats and usually into a bay or onto a beach. Their escape is prevented by closing off the route to the ocean with other boats or nets. Dolphins are hunted this way in several places around the world, including the Solomon Islands, the Faroe Islands, Peru, and Japan, the most well-known practitioner of this method. By numbers, dolphins are mostly hunted for their meat, though some end up in dolphinariums. Despite the controversial nature of the hunt resulting in international criticism, and the possible health risk that the often polluted meat causes, thousands of dolphins are caught in drive hunts each year.
Relationships with humans.
In history and religion.
Dolphins have long played a role in human culture. Dolphins are sometimes used as symbols, for instance in heraldry. When heraldry developed in the Middle Ages, not much was known about the biology of the dolphin and it was often depicted as a sort of fish. Traditionally, the stylised dolphins in heraldry still may take after this notion, sometimes showing the dolphin skin covered with fish scales.
Dolphins are present in the coat of arms of Anguilla and the coat of arms of Romania, and the coat of arms of Barbados has a dolphin supporter. A well-known historical example of a dolphin in heraldry, was the arms for the former province of the Dauphiné in southern France, from which were derived the arms and the title of the Dauphin of France, the heir to the former throne of France (the title literally means "The Dolphin of France").
"Dolfin" was the name of an aristocratic family in the maritime Republic of Venice, whose most prominent member was the 13th Century Doge Giovanni Dolfin. 
In Greek myths, they were seen invariably as helpers of humankind. Dolphins also seem to have been important to the Minoans, judging by artistic evidence from the ruined palace at Knossos. Dolphins are common in Greek mythology, and many coins from ancient Greece have been found which feature a man, a boy or a deity riding on the back of a dolphin. The Ancient Greeks welcomed dolphins; spotting dolphins riding in a ship's wake was considered a good omen. In both ancient and later art, Cupid is often shown riding a dolphin. A dolphin rescued the poet Arion from drowning and carried him safe to land, at Cape Matapan, a promontory forming the southernmost point of the Peloponnesus. There was a temple to Poseidon and a statue of Arion riding the dolphin.
The Greeks reimagined the Phoenician god Melqart as Melikertês (Melicertes) and made him the son of Athamas and Ino. He drowned but was transfigured as the marine deity Palaemon, while his mother became Leucothea. ("cf" Ino.) At Corinth, he was so closely connected with the cult of Poseidon that the Isthmian Games, originally instituted in Poseidon's honor, came to be looked upon as the funeral games of Melicertes. Phalanthus was another legendary character brought safely to shore (in Italy) on the back of a dolphin, according to Pausanias.
Dionysus was once captured by Etruscan pirates who mistook him for a wealthy prince they could ransom. After the ship set sail Dionysus invoked his divine powers, causing vines to overgrow the ship where the mast and sails had been. He turned the oars into serpents, so terrifying the sailors that they jumped overboard, but Dionysus took pity on them and transformed them into dolphins so that they would spend their lives providing help for those in need. Dolphins were also the messengers of Poseidon and sometimes did errands for him as well. Dolphins were sacred to both Aphrodite and Apollo.
In Hindu mythology the Ganges River Dolphin is associated with Ganga, the deity of the Ganges river. The dolphin is said to be among the creatures which heralded the goddess' descent from the heavens and her mount, the Makara, is sometimes depicted as a dolphin.
The Boto, a species of river dolphin that resides in the Amazon River, are believed to be shapeshifters, or "encantados", who are capable of having children with human women.
In captivity.
Species.
The renewed popularity of dolphins in the 1960s resulted in the appearance of many dolphinaria around the world, making dolphins accessible to the public. Criticism and animal welfare laws forced many to close, although hundreds still exist around the world. In the United States, the best known are the SeaWorld marine mammal parks.
In the Middle East the best known are Dolphin Bay at Atlantis, The Palm and the Dubai Dolphinarium.
Various species of dolphins are kept in captivity. These small cetaceans are more often than not kept in theme parks, such as SeaWorld, commonly known as a dolphinarium. Bottlenose Dolphins are the most common species of dolphin kept in dolphinariums as they are relatively easy to train, have a long lifespan in captivity and have a friendly appearance. Hundreds if not thousands of Bottlenose Dolphins live in captivity across the world, though exact numbers are hard to determine. Other species kept in captivity are Spotted Dolphins, False Killer Whales and Common Dolphins, Commerson's Dolphins, as well as Rough-toothed Dolphins, but all in much lower numbers than the Bottlenose Dolphin. There are also fewer than ten Pilot Whales, Amazon River Dolphins, Risso's Dolphins, Spinner Dolphins, or Tucuxi in captivity. An unusual and very rare hybrid dolphin, known as a Wolphin, is kept at the Sea Life Park in Hawaii, which is a cross between a Bottlenose Dolphin and a False Killer Whale.
Killer whales are well known for their performances in shows, but the number of Orcas kept in captivity is very small, especially when compared to the number of bottlenose dolphins, with only 44 captive killer whales being held in aquaria . The killer whale's intelligence, trainability, striking appearance, playfulness in captivity and sheer size have made it a popular exhibit at aquaria and aquatic theme parks. From 1976 to 1997, 55 whales were taken from the wild in Iceland, 19 from Japan, and three from Argentina. These figures exclude animals that died during capture. Live captures fell dramatically in the 1990s, and by 1999, about 40% of the 48 animals on display in the world were captive-born.
Organizations such as the Mote Marine Laboratory rescue and rehabilitate sick, wounded, stranded or orphaned dolphins while others, such as the Whale and Dolphin Conservation Society and Hong Kong Dolphin Conservation Society, work on dolphin conservation and welfare. India has declared the dolphin as its national aquatic animal in an attempt to protect the endangered Ganges River Dolphin. The Vikramshila Gangetic Dolphin Sanctuary has been created in the Ganges river for the protection of the animals.
Controversy.
Organizations such as World Animal Protection and the Whale and Dolphin Conservation Society campaign against the practice of keeping them in captivity. In captivity, they often develop pathologies, such as the dorsal fin collapse seen in 60–90% of male killer whales. Captives have vastly reduced life expectancies, on average only living into their 20s, although there are examples of killer whales living longer, including several over 30 years old, and two captive orcas, Corky II and Lolita, are in their mid-40s. In the wild, females who survive infancy live 46 years on average, and up to 70–80 years in rare cases. Wild males who survive infancy live 31 years on average, and up to 50–60 years. Captivity usually bears little resemblance to wild habitat, and captive whales' social groups are foreign to those found in the wild. Critics claim captive life is stressful due to these factors and the requirement to perform circus tricks that are not part of wild killer whale behavior. Wild killer whales may travel up to in a day, and critics say the animals are too big and intelligent to be suitable for captivity. Captives occasionally act aggressively towards themselves, their tankmates, or humans, which critics say is a result of stress.
Although dolphins generally interact well with humans, some attacks have occurred, most of them resulting in small injuries. Orcas, the largest species of dolphin, have been involved in fatal attacks on humans in captivity. The record-holder of documented orca fatal attacks is a male named Tilikum, who has lived at SeaWorld since 1992. Tilikum has played a role in the death of three people in three different incidents (1991, 1999 and 2010). Tilikum's behaviour sparked the production of the documentary "Blackfish", which focuses on the consequences of keeping orcas in captivity. There are documented incidents in the wild, too, but none of them fatal.
Fatal attacks from other species are less common, but there is a registered occurrence off the coast of Brazil in 1994, when a man died after being attacked by a bottlenose dolphin named Tião. Tião had suffered harassment by human visitors, including attempts to stick ice cream sticks down her blowhole. Non-fatal incidents occur more frequently, both in the wild and in captivity.
While dolphin attacks occur far less frequently than attacks by other sea animals, such as sharks, some scientists are worried about the careless programs of human-dolphin interaction. Dr. Andrew J. Read, a biologist at the Duke University Marine Laboratory who studies dolphin attacks, points out that dolphins are large and wild predators, so people should be more careful when they interact with them.
Several scientists who have researched dolphin behaviour have proposed that dolphins' unusually high intelligence in comparison to other animals means that dolphins should be seen as non-human persons who should have their own specific rights and that it is morally unacceptable to keep them captive for entertainment purposes or to kill them either intentionally for consumption or unintentionally as by-catch.
Military.
A military dolphin is a dolphin trained for military uses. A number of militaries have employed dolphins for various purposes from finding mines to rescuing lost or trapped humans. The military use of dolphins, however, drew scrutiny during the Vietnam War when rumors circulated that the United States Navy was training dolphins to kill Vietnamese divers. The United States Navy denies that at any point dolphins were trained for combat. Dolphins are still being trained by the United States Navy for other tasks as part of the U.S. Navy Marine Mammal Program. The Russian military is believed to have closed its marine mammal program in the early 1990s. In 2000 the press reported that dolphins trained to kill by the Soviet Navy had been sold to Iran.
Therapy.
Dolphins are an increasingly popular choice of animal-assisted therapy for psychological problems and developmental disabilities. For example, a 2005 study found dolphins an effective treatment for mild to moderate depression. However, this study was criticized on several grounds. For example, it is not known whether dolphins are more effective than common pets. Reviews of this and other published dolphin-assisted therapy (DAT) studies have found important methodological flaws and have concluded that there is no compelling scientific evidence that DAT is a legitimate therapy or that it affords more than fleeting mood improvement.
Consumption.
Cuisine.
In some parts of the world, such as Taiji, Japan and the Faroe Islands, dolphins are traditionally considered as food, and are killed in harpoon or drive hunts.
Dolphin meat is consumed in a small number of countries worldwide, which include Japan and Peru (where it is referred to as "chancho marino", or "sea pork"). While Japan may be the best-known and most controversial example, only a very small minority of the population has ever sampled it.
Dolphin meat is dense and such a dark shade of red as to appear black. Fat is located in a layer of blubber between the meat and the skin. When dolphin meat is eaten in Japan, it is often cut into thin strips and eaten raw as "sashimi", garnished with onion and either horseradish or grated garlic, much as with "sashimi" of whale or horse meat ("basashi"). When cooked, dolphin meat is cut into bite-size cubes and then batter-fried or simmered in a "miso" sauce with vegetables. Cooked dolphin meat has a flavor very similar to beef liver.
Health concerns.
There have been human health concerns associated with the consumption of dolphin meat in Japan after tests showed that dolphin meat contained high levels of mercury. There are no known cases of mercury poisoning as a result of consuming dolphin meat, though the government continues to monitor people in areas where dolphin meat consumption is high. The Japanese government recommends that children and pregnant women avoid eating dolphin meat on a regular basis.
Similar concerns exist with the consumption of dolphin meat in the Faroe Islands, where prenatal exposure to methylmercury and PCBs primarily from the consumption of pilot whale meat has resulted in neuropsychological deficits amongst children.
External links.
Conservation, research and news:
Photos:

</doc>
<doc id="9067" url="https://en.wikipedia.org/wiki?curid=9067" title="Division ring">
Division ring

In abstract algebra, a division ring, also called a skew field, is a ring in which division is possible. Specifically, it is a nonzero ring in which every nonzero element "a" has a multiplicative inverse, i.e., an element "x" with . Stated differently, a ring is a division ring if and only if the group of units equals the set of all nonzero elements. A division ring is a type of noncommutative ring.
Division rings differ from fields only in that their multiplication is not required to be commutative. However, by Wedderburn's little theorem all finite division rings are commutative and therefore finite fields. Historically, division rings were sometimes referred to as fields, while fields were called “commutative fields”.
Relation to fields and linear algebra.
All fields are division rings; more interesting examples are the non-commutative division rings. The best known example is the ring of quaternions H. If we allow only rational instead of real coefficients in the constructions of the quaternions, we obtain another division ring. In general, if "R" is a ring and "S" is a simple module over "R", then, by Schur's lemma, the endomorphism ring of "S" is a division ring; every division ring arises in this fashion from some simple module.
Much of linear algebra may be formulated, and remains correct, for modules over a division ring "D" instead of vector spaces over a field. Doing so it must be specified whether one is considering right or left modules, and some care is needed in properly distinguishing left and right in formulas. Working in coordinates, elements of a finite dimensional right module can be represented by column vectors, which can be multiplied on the right by scalars, and on the left by matrices (representing linear maps); for elements of a finite dimensional left module, row vectors must be used, which can be multiplied on the left by scalars, and on the right by matrices. The dual of a right module is a left module, and vice versa. The transpose of a matrix must be viewed as a matrix over the opposite division ring "D"op in order for the rule to remain valid.
Every module over a division ring is free; i.e., has a basis, and all bases of a module have the same number of elements. Linear maps between finite-dimensional modules over a division ring can be described by matrices; the fact that linear maps by definition commute with scalar multiplication is most conveniently represented in notation by writing them on the "opposite" side of vectors as scalars are. The Gaussian elimination algorithm remains applicable. The column rank of a matrix is the dimension of the right module generated by the columns, and the row rank is dimension of the left module generated by the rows; the same proof as for the vector space case can be used to show that these ranks are the same, and define the rank of a matrix. 
In fact the converse is also true and this gives a "characterization of division rings" via their module category: A unital ring "R" is a division ring if and only if every R-module is free 
The center of a division ring is commutative and therefore a field. Every division ring is therefore a division algebra over its center. Division rings can be roughly classified according to whether or not they are finite-dimensional or infinite-dimensional over their centers. The former are called "centrally finite" and the latter "centrally infinite". Every field is, of course, one-dimensional over its center. The ring of Hamiltonian quaternions forms a 4-dimensional algebra over its center, which is isomorphic to the real numbers.
Ring theorems.
Wedderburn's little theorem: All finite division rings are commutative and therefore finite fields. (Ernst Witt gave a simple proof.)
Frobenius theorem: The only finite-dimensional associative division algebras over the reals are the reals themselves, the complex numbers, and the quaternions.
Related notions.
Division rings "used to be" called "fields" in an older usage. In many languages, a word meaning "body" is used for division rings, in some languages designating either commutative or non-commutative division rings, while in others specifically designating commutative division rings (what we now call fields in English). A more complete comparison is found in the article Field (mathematics).
Skew fields have an interesting semantic feature: a modifier (here "skew") "widens" the scope of the base term (here "field"). Thus a field is a particular type of skew field, and not all skew fields are fields.
While division rings and algebras as discussed here are assumed to have associative multiplication, nonassociative division algebras such as the octonions are also of interest.
A near-field is an algebraic structure similar to a division ring, except that it has only one of the two distributive laws.
External links.
Grillet's Abstract Algebra, section VIII.5's characterization of division rings via their free modules. 

</doc>
<doc id="9069" url="https://en.wikipedia.org/wiki?curid=9069" title="Dia (software)">
Dia (software)

Dia 
is free and open source general-purpose diagramming software, developed originally by Alexander Larsson. Dia uses a controlled single document interface (SDI) similar to GIMP and Inkscape.
Features.
Dia has a modular design with several shape packages available for different needs: flowchart, network diagrams, circuit diagrams, and more. It does not restrict symbols and connectors from various categories from being placed together.
Dia has special objects to help draw entity-relationship models (obsoleted tedia2sql or newer parsediasql can be used to create the SQL DDL), Unified Modeling Language (UML) diagrams, flowcharts, network diagrams, and simple electrical circuits. It is also possible to add support for new shapes by writing simple XML files, using a subset of Scalable Vector Graphics (SVG) to draw the shape.
Dia loads and saves diagrams in a custom XML format which is, by default, gzipped to save space. It can print large diagrams spanning multiple pages and can also be scripted using the Python programming language.
Exports.
Dia can export diagrams to various formats including the following:
Development.
Dia was originally created by Alexander Larsson but he moved on to work on GNOME and other projects. James Henstridge then took over as the lead developer, but he also moved on to other projects. He was followed by Cyrille Chepelov and Lars Ræder Clausen in turn.
Dia is maintained by a group of developers: Hans Breuer, Steffen Macke, and Sameer Sahasrabuddhe.
Dia is written in C, and has an extension system, which also supports writing extensions in Python.

</doc>
<doc id="9070" url="https://en.wikipedia.org/wiki?curid=9070" title="Deep Space 1">
Deep Space 1

Deep Space 1 (DS1) is a spacecraft of the NASA New Millennium Program dedicated to testing a payload of advanced technologies.
Launched on 24 October 1998, the Deep Space mission carried out a flyby of asteroid 9969 Braille, which was selected as the mission's science target. Its mission was extended twice to include an encounter with Comet Borrelly and further engineering testing. Problems during its initial stages and with its star tracker led to repeated changes in mission configuration. While the flyby of the asteroid was a partial success, the encounter with the comet retrieved valuable information. Three of twelve technologies on board had to work within a few minutes of separation from the carrier rocket for the mission to continue.
The Deep Space series was continued by the Deep Space 2 probes, which were launched in January 1999 on Mars Polar Lander and were intended to strike the surface of Mars. Deep Space 1 was the first NASA spacecraft to use ion-powered rocketry, in contrast to the traditional chemical-powered rockets.
Technologies.
Autonav.
The Autonav system, developed by NASA's Jet Propulsion Laboratory, takes images of known bright asteroids. The asteroids in the inner Solar System move in relation to other bodies at a noticeable, predictable speed. Thus a spacecraft can determine its relative position by tracking such asteroids across the star background, which appears fixed over such timescales. Two or more asteroids let the spacecraft triangulate its position; two or more positions in time let the spacecraft determine its trajectory. Existing spacecraft are tracked by their interactions with the transmitters of the Deep Space Network (DSN), in effect an inverse GPS. However, DSN tracking requires many skilled operators, and the DSN is overburdened by its use as a communications network. The use of Autonav reduces mission cost and DSN demands.
The Autonav system can also be used in reverse, tracking the position of bodies relative to the spacecraft. This is used to acquire targets for the scientific instruments. The spacecraft is programmed with the target's coarse location. After initial acquisition, Autonav keeps the subject in frame, even commandeering the spacecraft's attitude control. The next spacecraft to use Autonav was Deep Impact.
SCARLET concentrating solar array.
Primary power for the mission was produced by a new solar array technology, the Solar Concentrator Array with Refractive Linear Element Technology (SCARLET), which uses linear Fresnel lenses made of silicone to concentrate sunlight onto solar cells. ABLE Engineering developed the concentrator technology and built the solar array for DS1, with Entech Inc, who supplied the Fresnel optics, and the NASA Glenn Research Center. The activity was sponsored by the Ballistic Missile Defense Organization. The concentrating lens technology was combined with dual-junction solar cells, which had considerably better performance than the GaAs solar cells that were the state of the art at the time of the mission launch.
The SCARLET arrays generated 2.5 kilowatts at 1 AU, with less size and weight than conventional arrays.
NSTAR ion engine.
Although ion engines had been developed at NASA since the late 1950s, with the exception of the SERT missions in the 1960s, the technology had not been demonstrated in flight on United States spacecraft, though hundreds of Hall-effect engines had been used on Soviet and Russian spacecraft. This lack of a performance history in space meant that despite the potential savings in propellant mass, the technology was considered too experimental to be used for high-cost missions. Furthermore, unforeseen side effects of ion propulsion might in some way interfere with typical scientific experiments, such as fields and particle measurements. Therefore it was a primary mission of the Deep Space 1 demonstration to show long-duration use of an ion thruster on a scientific mission.
The NASA Solar Technology Application Readiness (NSTAR) electrostatic ion thruster, developed at NASA Glenn, achieves a specific impulse of 1000–3000 seconds. This is an order of magnitude higher than traditional space propulsion methods, resulting in a mass savings of approximately half. This leads to much cheaper launch vehicles. Although the engine produces just 92 millinewtons (0.331 ounce-force) thrust at maximal power (2100 W on DS1), the craft achieved high speeds because ion engines thrust continuously for long periods.
The next spacecraft to use NSTAR engines was the Dawn spacecraft, with three redundant units.
Remote Agent.
Remote Agent (remote intelligent self-repair software) (RAX), developed at NASA Ames Research Center and JPL, was the first artificial-intelligence control system to control a spacecraft without human supervision. Remote Agent successfully demonstrated the ability to plan onboard activities and correctly diagnose and respond to simulated faults in spacecraft components through its built-in REPL environment. Autonomous control will enable future spacecraft to operate at greater distances from Earth and to carry out more sophisticated science-gathering activities in deep space. Components of the Remote Agent software have been used to support other NASA missions. Major components of Remote Agent were a robust planner (EUROPA), a plan-execution system (EXEC) and a model-based diagnostic system (Livingstone). EUROPA was used as a ground-based planner for the Mars Exploration Rovers. EUROPA II was used to support the Phoenix Mars Lander and the Mars Science Laboratory. Livingstone2 was flown as an experiment onboard Earth Observing 1, and an F-18 at NASA Dryden Flight Research Center.
Beacon Monitor.
Another method for reducing DSN burdens is the Beacon Monitor experiment. During the long cruise periods of the mission, spacecraft operations are essentially suspended. Instead of data, the craft emits a carrier signal on a predetermined frequency. Without data decoding, the carrier can be detected by much simpler ground antennas and receivers. If the spacecraft detects an anomaly, it changes the carrier between four tones, based on urgency. Ground receivers then signal operators to divert DSN resources. This prevents skilled operators and expensive hardware from babysitting an unburdened mission operating nominally. A similar system is used on the New Horizons Pluto probe to keep costs down during its ten-year cruise from Jupiter to Pluto.
SDST.
The Small Deep Space Transponder (SDST) is a compact and lightweight radio-communications system. Aside from using miniaturized components, the SDST is capable of communicating over the Ka band. Because this band is higher in frequency than bands currently in use by deep-space missions, the same amount of data can be sent by smaller equipment in space and on the ground. Conversely, existing DSN antennas can split time among more missions. At the time of launch, the DSN had a small number of Ka receivers installed on an experimental basis; Ka operations and missions are increasing.
PEPE.
Once at a target, DS1 senses the particle environment with the PEPE (Plasma Experiment for Planetary Exploration) instrument. It maps the objects with the MICAS (Miniature Integrated Camera And Spectrometer) imaging channel and discerns chemical composition with infrared and ultraviolet channels. All channels share a 10 cm telescope, which uses a silicon-carbide mirror.
Achievements.
The ion propulsion engine initially failed after 4.5 minutes of operation. However, it was later restored to action and performed excellently. Early in the mission, material ejected during launch vehicle separation caused the closely spaced ion extraction grids to short-circuit. The contamination was eventually cleared, as the material was eroded by electrical arcing, sublimed by outgassing, or simply allowed to drift out. This was achieved by repeatedly restarting the engine in an engine repair mode, arcing across trapped material.
It was thought that the ion exhaust might interfere with other spacecraft systems, such as radio communications or the science instruments. The PEPE detectors had a secondary function to monitor such effects from the engine. No interference was found.
Another failure was the loss of the star tracker. The star tracker determines spacecraft orientation by comparing the star field to its internal charts. The mission was saved when the MICAS camera was reprogrammed to substitute for the star tracker. Although MICAS is more sensitive, its field-of-view is an order of magnitude smaller, creating a greater information processing burden. Ironically, the star tracker was an off-the-shelf component, expected to be highly reliable.
Without a working star tracker, ion thrusting was temporarily suspended. The loss of thrust time forced the cancellation of a flyby past Comet Wilson-Harrington.
The Autonav system required occasional manual corrections. Most problems were in identifying objects that were too dim, or were difficult to identify because of brighter objects causing diffraction spikes and reflections in the camera, causing Autonav to misidentify targets.
The Remote Agent system was presented with three simulated failures on the spacecraft and correctly handled each event.
Overall this constituted a successful demonstration of fully autonomous planning, diagnosis, and recovery.
The MICAS instrument was a design success, but the ultraviolet channel failed due to an electrical fault. Later in the mission, after the star tracker failure, MICAS assumed this duty as well. This caused continual interruptions in its scientific use during the remaining mission, including the Comet Borrelly encounter.
The flyby of the asteroid 9969 Braille was only a partial success. Deep Space 1 was intended to perform the flyby at at only from the asteroid. Due to technical difficulties, including a software crash shortly before approach, the craft instead passed Braille at a distance of . This, plus Braille's lower albedo, meant that the asteroid was not bright enough for the autonav to focus the camera in the right direction, and the picture shoot was delayed by almost an hour. The resulting pictures were disappointingly indistinct.
However, the flyby of Comet Borrelly was a great success and returned extremely detailed images of the comet's surface. Such images were of higher resolution than the only previous pictures, of Halley's Comet taken by the Giotto spacecraft. The PEPE instrument reported that the comet's fields were offset from the nucleus. This is believed to be due to emission of jets, which were not distributed evenly across the comet's surface.
Despite having no debris shields, the spacecraft survived the comet passage intact. Once again, the sparse comet jets did not appear to point towards the spacecraft. Deep Space 1 then entered its second extended mission phase, focused on retesting the spacecraft's hardware technologies. The focus of this mission phase was on the ion engine systems. The spacecraft eventually ran out of hydrazine fuel for its attitude control thrusters. The highly efficient ion thruster had a sufficient amount of propellant left to perform attitude control in addition to main propulsion, thus allowing the mission to continue.
Current status.
Deep Space 1 succeeded in its primary and secondary objectives including flybys of the asteroid Braille and of Comet Borrelly, returning valuable science data and images. DS1's ion engines were shut down on 18 December 2001 at approximately 20:00:00 UTC, signaling the end of the mission. However, on-board communications remain active in case the craft is needed in the future. It remains within the Solar System, orbiting the Sun.
A proposed alternative end-of-mission plan involved an encounter with the asteroid 1999 KK1 in August 2002. However, cost reasons meant this was not selected.
Target summary.
Before launch it was going to visit 76P/West-Kohoutek-Ikemura and 3352 McAuliffe. Because of the delayed launch, this was changed to 1992 KD (named 9969 Braille) and 107P/Wilson-Harrington, (4015 Wilson–Harrington). It achieved an impaired flyby of Braille and then aimed for 19P/Borrelly. 19P/Borrelly flyby was a success and then 1999 K1 was proposed a target, but not approved. During the mission high quality infrared spectra of Mars were also taken.

</doc>
<doc id="9071" url="https://en.wikipedia.org/wiki?curid=9071" title="King David (disambiguation)">
King David (disambiguation)

David was the second king of the united Kingdom of Israel.
King David may also refer to:

</doc>
<doc id="9072" url="https://en.wikipedia.org/wiki?curid=9072" title="Jacques-Louis David">
Jacques-Louis David

Jacques-Louis David (; ; 30 August 1748 – 29 December 1825) was an influential French painter in the Neoclassical style, considered to be the preeminent painter of the era. In the 1780s his cerebral brand of history painting marked a change in taste away from Rococo frivolity toward a classical austerity and severity, heightened feeling harmonizing with the moral climate of the final years of the "Ancien Régime".
David later became an active supporter of the French Revolution and friend of Maximilien Robespierre (1758–1794), and was effectively a dictator of the arts under the French Republic. Imprisoned after Robespierre's fall from power, he aligned himself with yet another political regime upon his release: that of Napoleon, The First Consul of France. At this time he developed his Empire style, notable for its use of warm Venetian colours. After Napoleon's fall from Imperial power and the Bourbon revival, David exiled himself to Brussels, then in the United Kingdom of the Netherlands, where he remained until his death. David had a large number of pupils, making him the strongest influence in French art of the early 19th century, especially academic Salon painting.
Early life.
Jacques-Louis David was born into a prosperous family in Paris on 30 August 1748. When he was about nine his father was killed in a duel and his mother left him with his well-off architect uncles. They saw to it that he received an excellent education at the Collège des Quatre-Nations, University of Paris, but he was never a good student: he had a facial tumor that impeded his speech, and he was always preoccupied with drawing. He covered his notebooks with drawings, and he once said, "I was always hiding behind the instructor's chair, drawing for the duration of the class". Soon, he desired to be a painter, but his uncles and mother wanted him to be an architect. He overcame the opposition, and went to learn from François Boucher (1703–1770), the leading painter of the time, who was also a distant relative. Boucher was a Rococo painter, but tastes were changing, and the fashion for Rococo was giving way to a more classical style. Boucher decided that instead of taking over David's tutelage, he would send David to his friend, Joseph-Marie Vien (1716–1809), a painter who embraced the classical reaction to Rococo. There David attended the Royal Academy, based in what is now the Louvre.
Each year the Academy awarded an outstanding student the prestigious Prix de Rome, which funded a three- to five-year stay in the Eternal City. The culmination of the Academy's educational program, the Rome trip provided its winners the opportunity to study the remains of classical antiquity and the works of the Italian Renaissance masters at first hand. Each "pensionnaire" was lodged in the French Academy's Roman outpost, which from the years 1737 to 1793 was the Palazzo Mancini in the Via del Corso. David competed for, and failed to win, the prize for three consecutive years (with "Minerva Fighting Mars", "Diana and Apollo Killing Niobe's Children" and "The Death of Seneca"), each failure contributing to his lifelong grudge against the institution. After his second loss in 1772, David went on a hunger strike, which lasted two and a half days before the faculty encouraged him to continue painting. Confident he now had the support and backing needed to win the prize, he resumed his studies with great zeal—only to fail to win the Prix de Rome again the following year. Finally, in 1774, David was awarded the Prix de Rome on the strength of his painting of "Erasistratus Discovering the Cause of Antiochus' Disease", a subject set by the judges. In October 1775 he made the journey to Italy with his mentor, Joseph-Marie Vien, who had just been appointed director of the French Academy at Rome.
While in Italy, David especially studied the works of 17th-century masters such as Poussin, Caravaggio, and the Carracci. Although he declared, "the Antique will not seduce me, it lacks animation, it does not move", David filled twelve sketchbooks with drawings that he and his studio used as model books for the rest of his life. He was introduced to the painter Raphael Mengs (1728–1779), who opposed the tendency in Rococo painting to sweeten and trivialize ancient subjects, advocating instead the rigorous study of classical sources and close adherence to ancient models. Mengs' principled, historicizing approach to the representation of classical subjects profoundly influenced David's pre-revolutionary painting, such as "The Vestal Virgin", probably from the 1780s. Mengs also introduced David to the theoretical writings on ancient sculpture by Johann Joachim Winckelmann (1717–1768), the German scholar held to be the founder of modern art history. In 1779, David toured the newly excavated ruins of Pompeii, which deepened his belief that the persistence of classical culture was an index of its eternal conceptual and formal power. While in Rome, David also assiduously studied the High Renaissance painters, Raphael making a profound and lasting impression on the young French artist.
Early work.
David's fellow students at the academy found him difficult to get along with, but they recognized his genius. David's stay at the French Academy in Rome was extended by a year, but in July 1780 he returned to Paris. There, he found people ready to use their influence for him, and he was made a member of the Royal Academy. He sent the Academy two paintings, and both were included in the Salon of 1781, a high honor. He was praised by his famous contemporary painters, but the administration of the Royal Academy was very hostile to this young upstart. After the Salon, the King granted David lodging in the Louvre, an ancient and much desired privilege of great artists. When the contractor of the King's buildings, M. Pécoul, was arranging with David, he asked the artist to marry his daughter, Marguerite Charlotte. This marriage brought him money and eventually four children. David had his own pupils, about 40 to 50, and was commissioned by the government to paint "Horace defended by his Father", but he soon decided, "Only in Rome can I paint Romans." His father-in-law provided the money he needed for the trip, and David headed for Rome with his wife and three of his students, one of whom, Jean-Germain Drouais (1763–1788), was the Prix de Rome winner of that year.
In Rome, David painted his famous "Oath of the Horatii", 1784. In this piece, the artist references Enlightenment values while alluding to Rousseau's social contract. The republican ideal of the general will becomes the focus of the painting with all three sons positioned in compliance with the father. The Oath between the characters can be read as an act of unification of men to the binding of the state. The issue of gender roles also becomes apparent in this piece, as the women in Horatii greatly contrast the group of brothers. David depicts the father with his back to the women, shutting them out of the oath making ritual; they also appear to be smaller in scale than the male figures. The masculine virility and discipline displayed by the men's rigid and confident stances is also severely contrasted to the slouching, swooning female softness created in the other half of the composition. Here we see the clear division of male-female attributes that confined the sexes to specific roles under Rousseau's popularized doctrine of "separate spheres".
These revolutionary ideals are also apparent in the "Distribution of Eagles". While "Oath of the Horatii" and "The Tennis Court Oath" stress the importance of masculine self-sacrifice for one's country and patriotism, the "Distribution of Eagles" would ask for self-sacrifice for one's Emperor (Napoleon) and the importance of battlefield glory.
In 1787, David did not become the Director of the French Academy in Rome, which was a position he wanted dearly. The Count in charge of the appointments said David was too young, but said he would support him in 6 to 12 years. This situation would be one of many that would cause him to lash out at the Academy in years to come.
For the Salon of 1787, David exhibited his famous "Death of Socrates". "Condemned to death, Socrates, strong, calm and at peace, discusses the immortality of the soul. Surrounded by Crito, his grieving friends and students, he is teaching, philosophizing, and in fact, thanking the God of Health, Asclepius, for the hemlock brew which will ensure a peaceful death... The wife of Socrates can be seen grieving alone outside the chamber, dismissed for her weakness. Plato is depicted as an old man seated at the end of the bed." Critics compared the Socrates with Michelangelo's Sistine Ceiling and Raphael's Stanze, and one, after ten visits to the Salon, described it as "in every sense perfect". Denis Diderot said it looked like he copied it from some ancient bas-relief. The painting was very much in tune with the political climate at the time. For this painting, David was not honored by a royal "works of encouragement".
For his next painting, David created "The Lictors Bring to Brutus the Bodies of His Sons". The work had tremendous appeal for the time. Before the opening of the Salon, the French Revolution had begun. The National Assembly had been established, and the Bastille had fallen. The royal court did not want propaganda agitating the people, so all paintings had to be checked before being hung. David's portrait of Lavoisier, who was a chemist and physicist as well as an active member of the Jacobin party, was banned by the authorities for such reasons. When the newspapers reported that the government had not allowed the showing of "The Lictors Bring to Brutus the Bodies of His Sons", the people were outraged, and the royals were forced to give in. The painting was hung in the exhibition, protected by art students. The painting depicts Lucius Junius Brutus, the Roman leader, grieving for his sons. Brutus's sons had attempted to overthrow the government and restore the monarchy, so the father ordered their death to maintain the republic. Thus, Brutus was the heroic defender of the republic, at the cost of his own family. On the right, the Mother holds her two daughters, and the nurse is seen on the far right, in anguish. Brutus sits on the left, alone, brooding, seemingly dismissing the dead bodies of his sons. Knowing what he did was best for his country, but the tense posture of his feet and toes reveals his inner turmoil. The whole painting was a Republican symbol, and obviously had immense meaning during these times in France.
The French Revolution.
In the beginning, David was a supporter of the Revolution, a friend of Robespierre and a member of the Jacobin Club. While others were leaving the country for new and greater opportunities, David stayed to help destroy the old order; he was a regicide who voted in the National Convention for the Execution of Louis XVI. It is uncertain why he did this, as there were many more opportunities for him under the King than the new order; some people suggest David's love for the classical made him embrace everything about that period, including a republican government.
Others believed that they found the key to the artist's revolutionary career in his personality. Undoubtedly, David's artistic sensibility, mercurial temperament, volatile emotions, ardent enthusiasm, and fierce independence might have been expected to help turn him against the established order but they did not fully explain his devotion to the republican regime. Nor did the vague statements of those who insisted upon his "powerful ambition...and unusual energy of will" actually account for his revolutionary connections. Those who knew him maintained that "generous ardor", high-minded idealism and well-meaning though sometimes fanatical enthusiasm, rather than opportunism and jealousy, motivated his activities during this period.
Soon, David turned his critical sights on the Royal Academy of Painting and Sculpture. This attack was probably caused primarily by the hypocrisy of the organization and their personal opposition against his work, as seen in previous episodes in David's life. The Royal Academy was chock full of royalists, and David's attempt to reform it did not go over well with the members. However, the deck was stacked against this symbol of the old regime, and the National Assembly ordered it to make changes to conform to the new constitution.
David then began work on something that would later hound him: propaganda for the new republic. David's painting of Brutus was shown during the play "Brutus", by the famous Frenchman, Voltaire. The people responded in an uproar of approval.
In 1789, Jacques-Louis David attempted to leave his artistic mark on the historical beginnings of the French Revolution with his painting of "The Oath of the Tennis Court". David undertook this task not out of personal political conviction but rather because he was commissioned to do so. The painting was meant to commemorate the event of the same name but was never completed. A meeting of the Estates General was convened in May to address reforms of the monarchy. Dissent arose over whether the three estates would meet separately, as had been tradition, or as one body. The King's acquiescence with the demands of the upper orders led to the deputies of the Third Estate renaming themselves as the National Assembly on 17 June. They were locked out of the meeting hall three days later when they attempted to meet, and forced to reconvene to the royal indoor tennis court. Presided over by Jean-Sylvain Bailly, they made a 'solemn oath never to separate' until a national constitution had been created. In 1789 this event was seen as a symbol of the national unity against the "ancien regime". David was enlisted by the Society of Friends of the Constitution, the body that would eventually form the Jacobins, to enshrine this symbolic event.
This instance is notable in more ways than one because it eventually led David to finally become involved in politics as he joined the Jacobins. The picture was meant to be massive in scale; the figures in the foreground were to be life-sized portraits of the counterparts, including Jean-Sylvain Bailly, the President of the Constituent Assembly. Seeking additional funding, David turned to the Society of Friends of the Constitution. The funding for the project was to come from over three thousand subscribers hoping to receive a print of the image. However, when the funding was insufficient, the state ended up financing the project.
David set out in 1790 to transform the contemporary event into a major historical picture which would appear at the Salon of 1791 as a large pen and ink drawing. As in the "Oath of the Horatii", David represents the unity of men in the service of a patriotic ideal. The outstretched arms which are prominent in both works betray David's deeply held belief that acts of republican virtue akin to those of the Romans were being played out in France. In what was essentially an act of intellect and reason, David creates an air of drama in this work. The very power of the people appears to be "blowing" through the scene with the stormy weather, in a sense alluding to the storm that would be the revolution.
Symbolism in this work of art closely represents the revolutionary events taking place at the time. The figure in the middle is raising his right arm making the oath that they will never disband until they have reached their goal of creating a "constitution of the realm fixed upon solid foundations." The importance of this symbol is highlighted by the fact that the crowd's arms are angled to his hand forming a triangular shape. Additionally, the open space in the top half contrasted to the commotion in the lower half serves to emphasize the magnitude of the Tennis Court Oath.
In his attempt to depict political events of the Revolution in "real time", David was venturing down a new and untrodden path in the art world. However, Thomas Crow argues that this path "proved to be less a way forward than a cul-de-sac for history painting." Essentially, the history of the demise of David's "The Tennis Court Oath" illustrates the difficulty of creating works of art that portray current and controversial political occurrences. Political circumstances in France proved too volatile to allow the completion of the painting. The unity that was to be symbolized in "The Tennis Court Oath" no longer existed in radicalized 1792. The National Assembly had split between conservatives and radical Jacobins, both vying for political power. By 1792 there was no longer consensus that all the revolutionaries at the tennis court were "heroes". A sizeable number of the heroes of 1789 had become the villains of 1792. In this unstable political climate David's work remained unfinished. With only a few nude figures sketched onto the massive canvas, David abandoned "The Oath of the Tennis Court". To have completed it would have been politically unsound. After this incident, when David attempted to make a political statement in his paintings, he returned to the less politically charged use of metaphor to convey his message.
When Voltaire died in 1778, the church denied him a church burial, and his body was interred near a monastery. A year later, Voltaire's old friends began a campaign to have his body buried in the Panthéon, as church property had been confiscated by the French Government. In 1791 David was appointed to head the organizing committee for the ceremony, a parade through the streets of Paris to the Panthéon. Despite rain, and opposition from conservatives based on the amount of money that was being spent, the procession went ahead. Up to 100,000 people watched the "Father of the Revolution" be carried to his resting place. This was the first of many large festivals organized by David for the republic. He went on to organize festivals for martyrs that died fighting royalists. These funerals echoed the religious festivals of the pagan Greeks and Romans and are seen by many as Saturnalian.
David incorporated many revolutionary symbols into these theatrical performances and orchestrated ceremonial rituals; in effect radicalizing the applied arts, themselves. The most popular symbol for which David was responsible as propaganda minister was drawn from classical Greek images; changing and transforming them with contemporary politics. In an elaborate festival held on the anniversary of the revolt that brought the monarchy to its knees, David's Hercules figure was revealed in a procession following the lady Liberty (Marianne). Liberty, the symbol of Enlightenment ideals was here being overturned by the Hercules symbol; that of strength and passion for the protection of the Republic against disunity and factionalism. In his speech during the procession, David "explicitly emphasized the opposition between people and monarchy; Hercules was chosen, after all, to make this opposition more evident". The ideals that David linked to his Hercules single-handedly transformed the figure from a sign of the old regime into a powerful new symbol of revolution. "David turned him into the representation of a collective, popular power. He took one of the favorite signs of monarchy and reproduced, elevated, and monumentalized it into the sign of its opposite." Hercules, the image, became to the revolutionaries, something to rally around.
In June 1791, the King made an ill-fated attempt to flee the country, but was apprehended short of his goal on the Austrian Netherlands border and was forced to return under guard to Paris. Louis XVI had made secret requests to Emperor Leopold II of Austria, Marie-Antoinette's brother, to restore him to his throne. This was granted and Austria threatened France if the royal couple were hurt. In reaction, the people arrested the King. This led to an Invasion after the trials and execution of Louis and Marie-Antoinette. The Bourbon monarchy was destroyed by the French people in 1792—it would be restored after Napoleon, then destroyed again with the Restoration of the House of Bonaparte. When the new National Convention held its first meeting, David was sitting with his friends Jean-Paul Marat and Robespierre. In the Convention, David soon earned a nickname "ferocious terrorist". Soon, Robespierre's agents discovered a secret vault of the king's proving he was trying to overthrow the government, and demanded his execution. The National Convention held the trial of Louis XVI and David voted for the death of the King, causing his wife, a royalist, to divorce him.
When Louis XVI was executed on 21 January 1793, another man had already died as well—Louis Michel le Peletier de Saint-Fargeau. Le Peletier was killed on the preceding day by a royal bodyguard in revenge for having voted for the death of the King. David was called upon to organize a funeral, and he painted "Le Peletier Assassinated". In it, the assassin's sword was seen hanging by a single strand of horsehair above Le Peletier's body, a concept inspired by the proverbial ancient tale of the sword of Damocles, which illustrated the insecurity of power and position. This underscored the courage displayed by Le Peletier and his companions in routing an oppressive king. The sword pierces a piece of paper on which is written "I vote the death of the tyrant", and as a tribute at the bottom right of the picture David placed the inscription "David to Le Peletier. 20 January 1793". The painting was later destroyed by Le Peletier's royalist daughter, and is known by only a drawing, an engraving, and contemporary accounts. Nevertheless, this work was important in David's career because it was the first completed painting of the French Revolution, made in less than three months, and a work through which he initiated the regeneration process that would continue with The Death of Marat, David's masterpiece.
On 13 July 1793, David's friend Marat was assassinated by Charlotte Corday with a knife she had hidden in her clothing. She gained entrance to Marat's house on the pretense of presenting him a list of people who should be executed as enemies of France. Marat thanked her and said that they would be guillotined next week upon which Corday immediately fatally stabbed him. She was guillotined shortly thereafter. Corday was of an opposing political party, whose name can be seen in the note Marat holds in David's subsequent painting, "The Death of Marat". Marat, a member of the National Convention and a journalist, had a skin disease that caused him to itch horribly. The only relief he could get was in his bath over which he improvised a desk to write his list of suspect counter-revolutionaries who were to be quickly tried and, if convicted, guillotined. David once again organized a spectacular funeral, and Marat was buried in the Panthéon. Marat's body was to be placed upon a Roman bed, his wound displayed and his right arm extended holding the pen which he had used to defend the Republic and its people. This concept was to be complicated by the fact that the corpse had begun to putrefy. Marat's body had to be periodically sprinkled with water and vinegar as the public crowded to see his corpse prior to the funeral on 15 and 16 July. The stench became so bad however that the funeral had to be brought forward to the evening of 16 July. "The Death of Marat", perhaps David's most famous painting, has been called the Pietà of the revolution. Upon presenting the painting to the convention, he said "Citizens, the people were again calling for their friend; their desolate voice was heard: David, take up your brushes.., avenge Marat... I heard the voice of the people. I obeyed." David had to work quickly, but the result was a simple and powerful image.
"The Death of Marat", 1793, became the leading image of the Terror and immortalized both Marat and David in the world of the revolution. This piece stands today as "a moving testimony to what can be achieved when an artist's political convictions are directly manifested in his work". A political martyr was instantly created as David portrayed Marat with all the marks of the real murder, in a fashion which greatly resembles that of Christ or his disciples. The subject although realistically depicted remains lifeless in a rather supernatural composition. With the surrogate tombstone placed in front of him and the almost holy light cast upon the whole scene; alluding to an out of this world existence. "Atheists though they were, David and Marat, like so many other fervent social reformers of the modern world, seem to have created a new kind of religion." At the very center of these beliefs, there stood the republic.
After the King's execution, war broke out between the new Republic and virtually every major power in Europe. David, as a member of the Committee of General Security, contributed directly to the Reign of Terror. The committee was severe. Marie Antoinette went to the guillotine; an event recorded in a famous sketch by David. Portable guillotines killed failed generals, aristocrats, priests and perceived enemies. David organized his last festival: the festival of the Supreme Being. Robespierre had realized what a tremendous propaganda tool these festivals were, and he decided to create a new religion, mixing moral ideas with the Republic, based on the ideas of Rousseau, with Robespierre as the new high priest. This process had already begun by confiscating church lands and requiring priests to take an oath to the state. The festivals, called fêtes, would be the method of indoctrination. On the appointed day, 20 Prairial by the revolutionary calendar, Robespierre spoke, descended steps, and with a torch presented to him by David, incinerated a cardboard image symbolizing atheism, revealing an image of wisdom underneath.
Soon, the war began to go well; French troops marched across the southern half of the Netherlands (which would later become Belgium), and the emergency that had placed the Committee of Public Safety in control was no more. Then plotters seized Robespierre at the National Convention and he was later guillotined, in effect ending the Reign of Terror. As Robespierre was arrested, David yelled to his friend "if you drink hemlock, I shall drink it with you." After this, he supposedly fell ill, and did not attend the evening session because of "stomach pain", which saved him from being guillotined along with Robespierre. David was arrested and placed in prison, first from 2 August to 28 December 1794 and then from 29 May to 3 August 1795. There he painted his own portrait, showing him much younger than he actually was, as well as that of his jailer.
Post-revolution.
After David's wife visited him in jail, he conceived the idea of telling the story of the Sabine Women. "The Sabine Women Enforcing Peace by Running between the Combatants", also called "The Intervention of the Sabine Women" is said to have been painted to honor his wife, with the theme being love prevailing over conflict. The painting was also seen as a plea for the people to reunite after the bloodshed of the revolution.
David conceived a new style for this painting, one which he called the "Grecian style", as opposed to the "Roman style" of his earlier historical paintings. The new style was influenced heavily by the work of art historian Johann Joachim Winkelmann. In David's words, "the most prominent general characteristics of the Greek masterpieces are a noble simplicity and silent greatness in pose as well as in expression."
This work also brought him to the attention of Napoleon. The story for the painting is as follows: "The Romans have abducted the daughters of their neighbors, the Sabines. To avenge this abduction, the Sabines attacked Rome, although not immediately—since Hersilia, the daughter of Tatius, the leader of the Sabines, had been married to Romulus, the Roman leader, and then had two children by him in the interim. Here we see Hersilia between her father and husband as she adjures the warriors on both sides not to take wives away from their husbands or mothers away from their children. The other Sabine Women join in her exhortations." During this time, the martyrs of the Revolution were taken from the Pantheon and buried in common ground, and revolutionary statues were destroyed. When David was finally released to the country, France had changed. His wife managed to get him released from prison, and he wrote letters to his former wife, and told her he never ceased loving her. He remarried her in 1796. Finally, wholly restored to his position, he retreated to his studio, took pupils and for the most part, retired from politics.
In August 1796, David and many other artists signed a petition orchestrated by Quatremère de Quincy which questioned the wisdom of the planned seizure of works of art from Rome. The Director Barras believed that David was "tricked" into signing, although one of David's students recalled that in 1798 his master lamented the fact that masterpieces had been imported from Italy.
Napoleon.
David's close association with the Committee of Public Safety during the Terror resulted in his signing of the death warrant for Alexandre de Beauharnais, a minor noble. Beauharnais's widow, Joséphine, went on to marry Napoleon Bonaparte and became his empress; David himself depicted their coronation in the "Coronation of Napoleon and Josephine, 2 December 1804".
David had been an admirer of Napoleon from their first meeting, struck by Bonaparte's classical features. Requesting a sitting from the busy and impatient general, David was able to sketch Napoleon in 1797. David recorded the face of the conqueror of Italy, but the full composition of Napoleon holding the peace treaty with Austria remains unfinished. Bonaparte had high esteem for David, and asked him to accompany him to Egypt in 1798, but David refused, claiming he was too old for adventuring and sending instead his student, Antoine-Jean Gros.
After Napoleon's successful coup d'état in 1799, as First Consul he commissioned David to commemorate his daring crossing of the Alps. The crossing of the St. Bernard Pass had allowed the French to surprise the Austrian army and win victory at the Battle of Marengo on 14 June 1800. Although Napoleon had crossed the Alps on a mule, he requested that he be portrayed "calm upon a fiery steed". David complied with "Napoleon Crossing the Saint-Bernard." After the proclamation of the Empire in 1804, David became the official court painter of the regime. During this period he took students, one of whom was the Belgian painter Pieter van Hanselaere.
One of the works David was commissioned for was "The Coronation of Napoleon in Notre Dame". David was permitted to watch the event. He had plans of Notre Dame delivered and participants in the coronation came to his studio to pose individually, though never the Emperor (the only time David obtained a sitting from Napoleon had been in 1797). David did manage to get a private sitting with the Empress Joséphine and Napoleon's sister, Caroline Murat, through the intervention of erstwhile art patron Marshal Joachim Murat, the Emperor's brother-in-law. For his background, David had the choir of Notre Dame act as his fill-in characters. Pope Pius VII came to sit for the painting, and actually blessed David. Napoleon came to see the painter, stared at the canvas for an hour and said "David, I salute you." David had to redo several parts of the painting because of Napoleon's various whims, and for this painting, he received twenty-four thousand Francs.
Exile and death.
On the Bourbons returning to power, David figured in the list of proscribed former revolutionaries and Bonapartists—for having voted execution for the deposed King Louis XVI; and for participating in the death of Louis XVII. Mistreated and starved, the imprisoned Louis XVII was forced into a false confession of incest with his mother, Queen Marie-Antoinette. This was untrue, as the son was separated from his mother early and was not allowed communication with her, nevertheless, the allegation helped earn her the guillotine. The newly restored Bourbon King, Louis XVIII, however, granted amnesty to David and even offered him the position of court painter. David refused, preferring self-exile in Brussels. There, he trained and influenced Brussels artists like François-Joseph Navez and Ignace Brice, painted "Cupid and Psyche" and quietly lived the remainder of his life with his wife (whom he had remarried). In that time, he painted smaller-scale mythological scenes, and portraits of citizens of Brussels and Napoleonic émigrés, such as the Baron Gerard.
David created his last great work, "Mars Being Disarmed by Venus and the Three Graces", from 1822 to 1824. In December 1823, he wrote: "This is the last picture I want to paint, but I want to surpass myself in it. I will put the date of my seventy-five years on it and afterwards I will never again pick up my brush." The finished painting—evoking painted porcelain because of its limpid coloration—was exhibited first in Brussels, then in Paris, where his former students flocked to view it. 
The exhibition was profitable — 13,000 francs, after deducting operating costs, thus, more than 10,000 people visited and viewed the painting. In his later years, David remained in full command of his artistic faculties, even after a stroke in the spring of 1825 disfigured his face and slurred his speech. In June 1825, he resolved to embark on an improved version of his "Anger of Achilles" (also known as the "Sacrifice of Iphigenie"); the earlier version was completed in 1819 and is now in the collection of the Kimbell Art Museum, Fort Worth, Texas. David remarked to his friends who visited his studio "this is what is killing me" such was his determination to complete the work, but by October it must have already been well advanced, as his former pupil Gros wrote to congratulate him, having heard reports of the painting's merits. By the time David died, the painting had been completed and the commissioner Ambroise Firmin-Didot brought it back to Paris to include it in the exhibition "Pour les grecs" that he had organised and which opened in Paris in April 1826.
When David was leaving a theater, a carriage struck him, and he later died, on 29 December 1825. At his death, some portraits were auctioned in Paris, they sold for little; the famous "Death of Marat" was exhibited in a secluded room, to avoid outraging public sensibilities. Disallowed return to France for burial, for having been a regicide of King Louis XVI, the body of the painter Jacques-Louis David was buried in Brussels and moved in 1882 to Brussels Cemetery, while some say his heart was buried with his wife at Père Lachaise Cemetery, Paris.
David was made a Chevalier de la Légion d'honneur in 1803. He was promoted to an Officier in 1808. And, in 1815, he was promoted to a Commandant (now Commandeur) de la Légion d'honneur.
Freemasonry.
The theme of the oath that we find in several works like "Tennis court Oath", "The Distribution of the Eagle Standards", "Leonidas at Thermopylae," was perhaps inspired by the rituals of Freemasonry. In 1989 during "David against David" conference Albert Boime could prove on the basis of a document dated in 1787, the belonging of the painter to "La Moderation" Masonic Lodge as affiliate.
Medical analysis of his face.
Jacques-Louis David's facial abnormalities were traditionally reported to be a consequence of a deep facial sword wound after a fencing incident. These left him with a noticeable asymmetry during facial expression and resulted in his difficulty in eating or speaking (he could not pronounce some consonants such as the letter 'r'). A sword scar wound on the left side of his face is present in his self-portrait and sculptures and corresponds to some of the buccal branches of the facial nerve. An injury to this nerve and its branches are likely to have resulted in the difficulties with his left facial movement.
Furthermore, as a result of this injury, he suffered from a growth on his face that biographers and art historians have defined as a benign tumor. These however may have been a granuloma, or even a post-traumatic neuroma. As Simon Schama has pointed out, witty banter and public speaking ability were key aspects of the social culture of 18th-century France. In light of these cultural keystones, David's tumor would have been a heavy obstacle in his social life. David was sometimes referred to as "David of the Tumor".
Portraiture.
In addition to his history paintings, David completed a number of privately commissioned portraits. Warren Roberts, among others, has pointed out the contrast between David's "public style" of painting, as shown in his history paintings, and his "private style", as shown in his portraits.
In the painting of Brutus (1789), the man and his wife are separated, both morally and physically. Paintings like these, depicting the great strength of patriotic sacrifice, made David a popular hero of the revolution.
In the "Portrait of Antoine-Laurent Lavoisier and his wife" (1788), the man and his wife are tied together in an intimate pose. She leans on his shoulder while he pauses from his work to look up at her. David casts them in a soft light, not in the sharp contrast of Brutus or of the Horatii. Also of interest—Lavoisier was a tax collector, as well as a famous chemist. Though he spent some of his money trying to clean up swamps and eradicate malaria, he was nonetheless sent to the guillotine during the Reign of Terror as an enemy of the people. David, then a powerful member of the National Assembly, stood idly by and watched.
Other portraits include paintings of his sister-in-law and her husband, Madame and Monsieur Seriziat. The picture of Monsieur Seriziat depicts a man of wealth, sitting comfortably with his horse-riding equipment. The picture of the Madame shows her wearing an unadorned white dress, holding her young child's hand as they lean against a bed.
Towards the end of David's life, he painted a portrait of his old friend "Abbé Sieyès". Both had been involved in the Revolution, both had survived the purging of political radicals that followed the reign of terror. 
Shift in attitude.
The shift in David's perspective played an important role in the paintings of David's later life, including this one of Sieyès. During the height of the reign of terror, David was an ardent supporter of radicals such as Robespierre and Marat, and twice offered up his life in their defense. He organized revolutionary festivals and painted portraits of martyrs of the revolution, such as Lepeletier, who was assassinated for voting for the death of the king. David was an impassioned speaker at times in the National Assembly. In speaking to the Assembly about the young boy named Bara, another martyr of the revolution, David said, "O Bara! O Viala! The blood that you have spread still smokes; it rises toward Heaven and cries for vengeance."
After Robespierre was sent to the guillotine, however, David was imprisoned and changed the attitude of his rhetoric. During his imprisonment he wrote many letters, pleading his innocence. In one he wrote, "I am prevented from returning to my atelier, which, alas, I should never have left. I believed that in accepting the most honorable position, but very difficult to fill, that of legislator, that a righteous heart would suffice, but I lacked the second quality, understanding."
Later, while explaining his developing "Grecian style" for paintings such as "The Intervention of the Sabine Women", David further commented on a shift in attitude: "In all human activity the violent and transitory develops first; repose and profundity appear last. The recognition of these latter qualities requires time; only great masters have them, while their pupils have access only to violent passions."
Legacy.
Jacques-Louis David was, in his time, regarded as the leading painter in France, and arguably all of Western Europe; many of the painters honored by the restored Bourbons following the French Revolution had been David's pupils. David's student Antoine-Jean Gros for example, was made a Baron and honored by Napoleon Bonaparte's court. Another pupil of David's, Jean Auguste Dominique Ingres became the most important artist of the restored Royal Academy and the figurehead of the Neoclassical school of art, engaging the increasingly popular Romantic school of art that was beginning to challenge Neoclassicism. David invested in the formation of young artists for the Rome Prize, which was also a way to pursue his old rivalry with other contemporary painters such as Joseph Suvee, who also had opened teaching studios. To be one of David's students was considered prestigious and earned his students a lifetime of reputation. He also called on the more advanced students, such as Jérôme-Martin Langlois, to help him paint his large canvases.
Despite David's reputation, he was more fiercely criticized right after his death than at any point during his life. His style came under the most serious criticism for being static, rigid, and uniform throughout all his work. David's art was also attacked for being cold and lacking warmth. David, however, made his career precisely by challenging what he saw as the earlier rigidity and conformity of the French Royal Academy's approach to art. David's later works also reflect his growth in the development of the Empire style, notable for its dynamism and warm colors. It is likely that much of the criticism of David following his death came from David's opponents; during his lifetime David made a great many enemies with his competitive and arrogant personality as well as his role in the Terror. David sent many people to the guillotine and personally signed the death warrants for King Louis XVI and Marie Antoinette. One significant episode in David's political career that earned him a great deal of contempt was the execution of Emilie Chalgrin. A fellow painter Carle Vernet had approached David, who was on the Committee of Public Safety, requesting him to intervene on behalf of his sister, Chalgrin. She had been accused of crimes against the Republic, most notably possessing stolen items. David refused to intervene in her favor, and she was executed. Vernet blamed David for her death, and the episode followed him for the rest of his life and after.
In the last 50 years David has enjoyed a revival in popular favor and in 1948 his two-hundredth birthday was celebrated with an exhibition at the Musée de l'Orangerie in Paris and at Versailles showing his life's works. Following World War II, Jacques-Louis David was increasingly regarded as a symbol of French national pride and identity, as well as a vital force in the development of European and French art in the modern era.
Filmography.
"Danton" (Andrzej Wajda, France, 1982) – Historical drama. Many scenes include David as a silent character watching and drawing. The film focuses on the period of the Terror.

</doc>
<doc id="9074" url="https://en.wikipedia.org/wiki?curid=9074" title="Design science license">
Design science license

Design Science License (DSL) is a copyleft license for any type of free content such as text, images, music. Unlike other open source licenses, the DSL was intended to be used on any type of copyrightable work, including documentation and source code. It was the first "generalized copyleft" license. The DSL was written by Michael Stutz.
The DSL came out in the 1990s, before the formation of the Creative Commons. Once the Creative Commons arrived, Stutz considered the DSL experiment "over" and no longer recommended its use.

</doc>
<doc id="9079" url="https://en.wikipedia.org/wiki?curid=9079" title="Drum kit">
Drum kit

A drum kit (primarily American), drum set (primarily British/Australian), trap set, or just drums is a collection of drums and other percussion instruments, typically cymbals, which are set up on stands to be played by a single player with drumsticks held in both hands and the feet operating pedals that control the hi-hat cymbal and the beater for the bass drum. A drum kit consists of a mix of drums (categorized classically as membranophones, Hornbostel-Sachs high-level classification 2) and idiophones most significantly cymbals but also including the woodblock and cowbell (classified as Hornbostel-Sachs high-level classification 1). In the 2000s, some kits also include electronic instruments (Hornbostel-Sachs classification 53) and both hybrid and entirely electronic kits are used.
A standard modern kit (for a right-handed player), as used in popular music and taught in music schools, contains:
All of these are classed as non-pitched percussion, allowing for the music to be scored using percussion notation, for which a loose semi-standardized form exists for the drum kit. If some or all of them are replaced by electronic drums, the scoring and most often positioning remains the same, allowing a standard teaching approach. The drum kit is usually played while seated on a "drum stool" or "throne". The drum kit differs from instruments that can be used to produce pitched melodies or chords, even though drums are often placed musically alongside others that do, such as the piano or guitar. The drum kit is part of the standard rhythm section used in many types of popular and traditional music styles ranging from rock and pop to blues and jazz. Other standard instruments used in the rhythm section include the electric bass, electric guitar and keyboards.
Many drummers extend their kits from this basic pattern, adding more drums, more cymbals, and many other instruments including pitched percussion. In some styles of music particular extensions are normal, for example double bass drums in heavy metal music and the enlarged kits used by some progressive rock drummers, which may include unusual instruments such as gongs. Some performers use small kits that omit elements from the basic setup, such as some rockabilly drummers. Some drum kit players may have other roles in the band, such as providing backup vocals, or less commonly, lead vocals.
__TOC__
History.
Early development.
Prior to the development of the drum set, the standard way that drums and cymbals were used in military and orchestral music settings was to have the different drums and cymbals played separately by different percussionists. Thus, in an early 1800s orchestra piece, if the score called for bass drum, triangle and cymbals, three percussionists would be hired to play these three instruments. In the 1840s, percussionists began to experiment with foot pedals as a way to enable them to play more than one instrument. In the 1860s, percussionists began to experiment with combining multiple drums into a set. The bass drum, snare drum, cymbals, and other percussion instruments were all played using hand-held drum sticks. Drummers in musical theater shows and stage shows, where the budget for pit orchestras were often limited, contributed to the creation of the drum set because they tried to develop ways so that one drummer could do the job of multiple percussionists.
Double-drumming was developed to enable one person to play the bass and snare with sticks, while the cymbals could be played by tapping the foot on a "low-boy". With this approach, the bass drum was usually played on beats one and three (in 4/4 time). While the music was first designed to accompany marching soldiers, this simple and straightforward drumming approach eventually led to the birth of ragtime music when the simplistic marching beats became more syncopated. This resulted in a greater 'swing' and dance feel. The drum set was initially referred to as a "trap set," and from the late 1800s to the 1930s, drummers were referred to as "trap drummers." By the 1870s, drummers were using an "overhang pedal." Most drummers in the 1870s preferred to do double drumming without any pedal to play multiple drums, rather than use an overhang pedal. Companies patented their pedal systems such as Dee Dee Chandler of New Orleans 1904–05. Liberating the hands for the first time, this evolution saw the bass drum played with the foot of a standing percussionist (thus the term "kick drum"). The bass drum became the central piece around which every other percussion instrument would later revolve.
Ludwig-Musser, William F. Ludwig, Sr., and his brother, Theobald Ludwig, founded the Ludwig & Ludwig Co. in 1909 and patented the first workable bass drum pedal system, paving the way for the modern drum kit. It was the golden age of drum building for many famous drum companies, with Ludwig introducing... "The ornately engraved Black Beauty Brass Snare drum; Slingerland premiered its Radio King solid-maple shell; Leedy invented the floating drum head & self-aligning lug;& Gretsch originated the three-way tension system of the Gladstone snare drum". Wire brushes for use with drums and cymbals were introduced in 1912. The need for brushes arose due to the problem of the drum sound overshadowing the other instruments on stage. Drummers began using metal fly swatters to reduce the volume on stage next to the other acoustic instruments. Drummers could still play the rudimentary snare figures and grooves with brushes they would normally play with drumsticks. As brushes gained popularity, the drum companies started manufacturing brushes.
20th century.
[[File:Juke joint drummer.jpg|right|thumb|250px|
Drummer in a Memphis "juke joint" orchestra playing a kit with four non-tunable toms. Marion Post Wolcott, October 1939]]
By World War I, drum kits were often marching band-style military bass drums with many percussion items suspended on and around them. Drum kits became a central part of jazz music, specifically (but not limited to) Dixieland. The modern drum kit was developed in the Vaudeville era during the 1920s in New Orleans. In 1917, a New Orleans band called "The Original Dixieland Jazz Band " recorded jazz tunes that became hits all over the country. These were the first official jazz recordings. Drummers such as Baby Dodds, "Zutty" Singleton and Ray Baduc had taken the idea of marching rhythms, combining the bass drum and snare drum and "traps", a term used to refer to the percussion instruments associated with immigrant groups, which included miniature cymbals, tom toms, cowbells and woodblocks. They started incorporating these elements with ragtime, which had been popular for a couple of decades, creating an approach which evolved into a jazz drumming style.
Budget constraints and space considerations in musical theatre pit orchestras led bandleaders to pressure fewer percussionists to cover more percussion parts. Metal consoles were developed to hold Chinese tom-toms, with swing-out stands for snare drums and cymbals. On top of the console was a "contraption" tray (shortened to "trap"), used to hold items like whistles, klaxons, and cowbells, so these drums/kits were dubbed "trap kits". Hi-hat stands became available around 1926.
In 1918 Baby Dodds ( Warren "Baby" Dodds, circa 1898–1959), playing on riverboats with Louis Armstrong on the Mississippi, was modifying the military marching set-up and experimenting with playing the drum rims instead of woodblocks, hitting cymbals with sticks (1919), which was not yet common, and adding a side cymbal above the bass drum, what became known as the ride cymbal. Drum maker William Ludwig developed the "sock" or early low-mounted high-hat after observing Dodd's drumming. Ludwig noticed that Dodd tapped his left foot all the time. Dodds had Ludwig raise the newly produced low hats 9 inches higher to make it easier to play, thus creating the modern hi-hat cymbal. Dodds was one of the first drummers to also play the broken-triplet beat that became the standard pulse and roll of modern ride cymbal playing. Dodds also popularized the use of Chinese cymbals.
The 1920s are known as the jazz age or the "Roaring 20's". In 1919, US Congress passed a prohibition law outlawing the manufacturing and transporting of drinking alcohol. When drinking became illegal, it became popular in underground nightclubs. The type of music that was played at these underground establishments that were selling alcohol was jazz. It was not seen as upstanding to listen to or perform jazz music, because it was an African American style and at that time the United States was segregated and racism was a prevalent issue. Because jazz music was seen as great dance music, big band jazz became popular in nightclubs. In the 1920s, freelance drummers emerged. They were hired to play shows, concerts, theaters, clubs and back dancers and artists of various genres. Just as modern drummers have many different roles, so did the drummers of the 1920s. One important role for drummers in the 1920s is what is referred to in modern times as a foley artist. A foley artist is a sound effects person. During silent films, an orchestra was hired to accompany the silent film and the drummer was responsible for providing all the sound effects. Drummers played instruments to imitate gun shots, planes flying overhead, a train coming into a train station, and galloping horses etc.
Sheet music from the 1920s provides evidence that the drummer's sets were starting to evolve in size and sound to support the various acts mentioned above. However, the first "talkies" or films with audio, were released circa 1927 and by 1930 most films were released with a soundtrack and the silent film era was over. The downside of the technological breakthrough was that thousands of drummers who served as sound effect specialists were put out of work overnight. A similar panic was felt by drummers in the 1980s, when electronic drum machines were first released.
Big Band drumming.
In 1929, when the stock market crash resulted in a global depression, one of the things that helped people cope with the trying years was swing jazz music. By the early to mid 1930's, big band swing was being embraced throughout the US it became the country's most popular form of music. The other contributing factor to the big band's success during the 1930s was the popularity of radio. The drum kit played a key role in the big band swing sound. Throughout the 1930s Chick Webb and Gene Krupa at the Savoy Ballroom in Harlem, increased the visual and musical driving force of the drummer and their equipment by simply being so popular and in demand- and they ensured that their drum kits became not only functionally developed but dazzling and well designed. Jazz drummers were influential in developing the concept of the modern drum kit and extending playing techniques. Gene Krupa was the first drummer to head his own orchestra and thrust the drums into the spotlight with his drum solos. Others would soon follow his lead.
As the music of the world was evolving, so was the drum set. Tom-tom drums, small crash cymbals, Chinese cymbals and hi-hat cymbals were added to the drum set. The hi-hats were the primary way for the drummers of the big band era to keep time. Before 1930, while playing the New Orleans jaz and Chicago styles, drummers would choke the cymbals on the "ands" of eighth note figures as an alternative to playing a buzz roll, the rim of the drums, or on the woodblocks to keep time. This muting method of keeping time by choking the crash and china cymbals proved to be awkward, so the drummers of that time came up with the idea of having a foot-operated cymbal. This resulted in the creation of the snowshoe cymbal, a foot-operated cymbal. It enabled drummers to play the eighth note figures ("Boom, Chick, Boom, Chick, Boom, Chick") between the right and left foot, improving the ergonomics and facility of drumset playing and helping drummers to keep a more steady rhythm.
Toward the end of the 1920s, variations of the hi-hats were introduced. One of the most popular hand held hi-hat cymbal variations used was called the "hand sock cymbals". The reason for the name "hi-hat" was because earlier versions of the hi-hat were referred to as a "low boy." The evolution that became the "hi-hats" allowed drummers to play the two cymbals with drum sticks while simultaneously controlling how open or closed the two cymbals were with their foot. The pedal could also be used to play the cymbals with the foot alone, while the right hand played other drums. By the 1930s, Ben Duncan and others popularized streamlined trap kits leading to a basic four piece drum set standard: bass, snare, tom-tom, and a larger floor tom. In time, legs were fitted to larger floor toms, and "consolettes" were devised to hold smaller tom-toms (ride toms) on the bass drum.
Bebop drumming.
In the early 1940s, many jazz musicians, especially African American jazz musicians, started to stray from the popular big band dance music of the 1930s. Their experimentation and quest for deeper expression and freedom on the instrument led to the birth of a new style of music based from Harlem called bebop music. Whereas swing was a popular music designed for dancing, bebop was a "musician's music" designed for listening. During the bebop era, given that bands no longer had to accompany dancers, bandleaders could speed up the tempo. Bebop was also much more based on improvisation, in comparison to the heavily arranged big band scores. Bebop musicians would take an old standard and re-write the melody, add more complex chord changes, resulting in a new composition.
Swing drummers such as Max Roach and Kenny Clarke had already deviated from the large marching band-style bass drums, finding that they were too loud and boomy. Bebop drummers continued this trend, and they started trying out smaller bass drum sizes in the drum set. Bebop drummers' experimentations with new drum sizes and new sounds led to the innovative concept of applying the busy "four on the floor" bass drum rhythms to a new larger cymbal called the ride cymbal. By focusing on keeping time on the new ride cymbal instead of the bass drum, the "feel" went from bass drum and hi-hat heavy, to a lighter melodic feel that has been explained as "floating on top of the time." This allowed drummers to express themselves in a more melodic fashion by playing the rhythms used by the guitar, piano and sax players using the new smaller, more focused bass drums and snare. Louie Bellson also assisted in the innovative sizes and sounds of the 1940s drum set by pioneering the use of two bass drums, or the double bass drum kit.
Rock.
With rock and roll coming into place, a watershed moment occurred between 1962 and 1964 when the Surfaris released "Wipe Out", as well as when Ringo Starr of The Beatles played his Ludwig kit on American television. As rock moved from the nightclubs and bars and into stadiums in the 1960s, there was a trend towards bigger drum kits. The trend towards larger drum kits took momentum in the 1970s with the emergence of progressive rock. By the 1980s, widely popular drummers like Billy Cobham, Carl Palmer, Nicko McBrain, Phil Collins, Stewart Copeland, Simon Phillips and Neil Peart were using large numbers of drums and cymbals. In the 1980s, some drummers began to use electronic drums.
In the 2010s, some drummers use a variety of auxiliary percussion instruments, found objects, and electronics as part of their "drum" kits. Popular electronics include: electronic sound modules; laptop computers used to activate loops, sequences and samples; metronomes and tempo meters; recording devices; and personal sound reinforcement equipment (e.g., a small PA system to amplify electronic drums and provide a monitor).
Recording.
On early recording media (until 1925) such as wax cylinders and discs carved with an engraving needle, sound balancing meant that musicians had to be literally moved in the room. Drums were often put far from the horn (part of the mechanical transducer) to reduce sound distortion. Since this affected the rendition of cymbals at playback, sound engineers of the time remedied the situation by asking drummers to play the content of the cymbals onto woodblocks, temple blocks, and cowbells for their loudness and short decay.
Components.
Terminology.
Breakables, shells, extensions, hardware.
The drum kit may be loosely divided into four parts:
There are several reasons for this division. When more than one band plays in a single performance, the drum kit is often considered part of the backline (the key rhythm section equipment that stays on stage all night, which often also includes a bass amp and a stage piano), and which is shared between/among the drummers. Often the main "drawcard" act will provide the drums, as they are being paid more, possibly have the better gear, and in any case have the prerogative of using their own. However sticks, snare drum and cymbals are commonly swapped, each drummer bringing their own, and sometimes other components. The term "breakables" in this context refers to whatever basic components the "guest" drummer is expected to bring. Similar considerations apply if using a "house kit" (a drum kit owned by the venue, which is rare), even if there is only one band at the performance.
The snare drum and cymbals are the core of the "breakables", as they are particularly critical and individual components of the standard kit, in several related ways.
Much the same considerations apply to bass drum pedals and the stool, but these are not always considered "breakables", particularly if changeover time between bands is very limited. Swapping the snare drum in a standard kit can be done very quickly. Replacing cymbals on stands takes longer, particularly if there are many of them, and cymbals are easily damaged by incorrect mounting, so many drummers prefer to bring their own cymbal stands.
Drum sizes.
See Common configurations below for typical drum sizes.
Traditionally, in America and the United Kingdom, drum sizes were expressed as "depth x diameter", both in inches, but in The United Kingdom it was stated the other way around. More recently, many drum kit manufacturers have begun to express their sizes in terms of "diameter x depth"; still in the measure of inches.
Manufacturers still using the American traditional format in their catalogues include these:
Those using the European measures of diameter x depth include these:
For example, a hanging tom 12 inches in diameter and 8 inches deep would be described by Tama as 8 inches × 12 inches, but by Pearl as 12 inches × 8 inches, and a standard diameter Ludwig snare drum 5 inches deep is a 5-inch × 14-inch, while the UK's Premier Manufacturer offers the same dimensions as: a 14-inch × 5-inch snare.
Drums.
Snare drum.
The snare drum is essential as it is the musical center of the kit. It provides the strongest regular accents, played by the left hand (if right handed), and the backbone for many fills. It produces its distinctive sound, due to the bed of snare wires fitted to the underside of the drum which, when engaged, vibrate with the bottom (snare-side) drum skin (head), creating a snappy, buzzing sound.
Toms.
Tom-tom drums, or "toms" for short, are drums without snares and played with sticks (or whatever tools the music style requires), and are the most numerous drums in most kits. They provide the bulk of most drum fills and solos.
They include:
The smallest and largest drums without snares, such as octobans and gong drums, are sometimes considered toms.
The naming of common configurations is largely a reflection of the number of toms, as only the drums are conventionally counted, and these configurations all contain one snare and one or more bass drums, (though not regularly any standardized use of 2 bass/kick drums) the balance usually being in toms.
Bass drum.
The bass drum (also known as the "kick drum") provides a regular but often-varied foundation to the rhythm. The bass drum is the lowest pitched drum and usually provides the basic beat or timing element with basic pulse patterns. Some drummers may use two or more bass drums or use a double bass drum pedal with a single bass drum. Double bass drumming is an important technique in some heavy metal genres. Using a double bass drum pedal enables a drummer to play a double bass drum style with only one bass drum, saving space in recording/performance areas and reducing time and effort during set-up, tear-down and transportation.
Other drums.
Octobans/Rocket toms (Pearl)/Deccabons were designed for use within a drum kit, extending the tom range upwards in pitch, primarily by their depth; as well as diameter (typically 6"). 
Timbales are tuned much higher than a tom of the same diameter, and normally played with very light, thin, non-tapered sticks. They have relatively thin heads and a very different tone than a tom, but are used by some drummers/percussionists to extend the tom range upwards. Alternatively, they can be fitted with tom heads and tuned as shallow concert toms. Attack Timbales and mini timbales are reduced-diameter timbales designed specifically for drum kit usage, the smaller diameter allowing for thicker heads for the same pitch and head tension and are clearly recognizable in modern genres and in more traditional forms of Latin, Reggae & numerous world music styles too .
Similarly, most hand drum percussion cannot be played easily or suitably with drum sticks without risking damage to the head and to the bearing edge, which is not protected by a drum rim. For use in a drum kit, they may be fitted with a suitable drum head and played with care, or require playing by hand.
Cymbals.
In most drum kits and drum/percussion kits cymbals are as important as the drums themselves. The oldest idiophones in music are cymbals, and were used throughout the ancient Near East, very early in the Bronze Age period. Cymbals are most associated with Turkey and Turkish craftsmanship, where Zildjian (the name means cymbal smith) has predominantly made them since 1623.
Beginners cymbal packs normally contain four cymbals: one ride, one crash, and a pair of hi-hats. A few contain only three cymbals, using a crash/ride instead of the separate ride and crash. The sizes closely follow those given in Common configurations below.
Most drummers soon extend this by adding another crash, a splash, a china/effects cymbal; or even all of those last mentioned.
Ride cymbal.
The ride cymbal is most often used for keeping a constant-rhythm pattern, every beat or more often, as the music requires. Development of this ride technique is generally credited to Baby Dodds.
Most drummers have a single main ride, located near their right hand,(& within easy playing reach, as it is used very regularly) most often a 20" sizing but, 16"-24" diameters are not uncommon. It is most often a heavy, or medium-weighted cymbal that cuts through other instrumental sounds, but some drummers use a swish cymbal, sizzle cymbal or other exotic or lighter metal ride, as the main or only ride in their kit, particularly for jazz, gospel or ballad/folk sounds. In the 1960s Ringo Starr used a sizzle cymbal as a second ride particularly for use during guitar solos.
Hi-hats.
The hi-hat cymbals consist of two cymbals mounted facing each other on a metal pole, with a foot pedal that can be depressed to move the cymbals together. The hi-hats can be sounded by striking the cymbals with one or two sticks or just by opening and closing the cymbals with the footpedal, without striking the cymbals. Different sounds can be created by striking "open hi-hats" (without the pedal depressed) or "closed hi-hats" (with the pedal pressed down). A unique effect can be created by striking an open hi-hat and then closing the cymbals with the footpedal. The hi-hat has a similar function to the ride cymbal; The two are rarely played consistently for long periods at the same time, but one or the other, usually is employed to keep the finer rhythm much of the time within a piece of music. It is played by the right stick of a right-handed drummer. Changing between ride and hi-hat, or between either and a leaner sound with neither, is often used to mark a change from one passage to another, for example; to distinguish verse and chorus.
Crashes.
The crash cymbals are usually the strongest accent markers within the kit, marking crescendos and climaxes, vocal entries, and major changes of mood/swells and effects. A crash cymbal is often accompanied by a strong kick on the bass drum pedal, both for musical effect and to support the stroke. It provides a fuller sound and is a commonly taught technique.
In the very smallest kits, in jazz, and at very high volumes, ride cymbals may be played in with the technique and sound of a crash cymbal. Some hi-hats will also give a useful crash, particularly thinner hats or those with an unusually severe taper. At low volumes, producing a good crash from a cymbal not particularly suited to it is a highly skilled art. Alternatively, specialised crash/ride and ride/crash cymbals are specifically designed to combine both functions.
Other cymbals.
Effects cymbals.
All cymbals other than rides, hi-hats and crashes/splashes are usually called effects cymbals when used in a drum kit, though this is a non-classical or colloquial designation that has become a standardized label.
Most extended kits include one or more splash cymbals and at least one china cymbal. Major cymbal makers produce cymbal extension packs consisting of one splash and one china, or more rarely a second crash, a splash and a china, to match some of their starter packs of ride, crash and hi-hats. However any combination of options can be found in the marketplace.
Some cymbals may be considered effects in some kits but "basic in another set of components . A swish cymbal may, for example serve, as the main ride in some styles of music, but in a larger kit, which includes a conventional ride cymbal as well, it may well be considered an effects cymbal per se.
Accent cymbals.
Cymbals of any type used to provide an accent rather than a regular pattern or groove are known as accent cymbals. While any cymbal can be used to provide an accent, the term is applied more correctly to cymbals for which the main purpose is to provide an accent. Accent cymbals include chime cymbals, small-bell domed cymbals or those with a clear sonorous/oriental chime to them like specialized crash and splash cymbals and many china types too, particularly the smaller and/or thinner ones.
Other acoustic instruments.
Other instruments that have regularly been incorporated into drum kits include:
See also Extended kits below.
Electronic drums.
Electronic drums are used for many purposes. Some drummers use electronic drums for playing in small venues where a very low volume for the band is desired. Since electronic drums do not create any acoustic sound, with all of the drum sounds coming from a keyboard amplifier or PA system, the volume of electronic drums can be much lower than an acoustic kit. Some drummers use electronic drums as practice instruments, because they can be listened to with headphones, enabling a drummer to practice in an apartment or in the middle of the night without disturbing others. Some drummers use electronic drums to take advantage of the huge range of sounds that modern drum modules can produce, which range from sampled sounds of real drums, cymbals and percussion instruments, to synthesized sounds. Drummers' usage of electronic drum equipment can range from adding a single electronic pad to an acoustic kit (e.g., to have access to an instrument that might otherwise be impractical, such as a gong), to using a mix of acoustic drums/cymbals and electronic pads, to using an acoustic kit in which the drums and cymbals have triggers, which can be used to sound electronic drums and other sounds, to having an exclusively electronic kit, which is often set up with the rubber or mesh drum pads and rubber "cymbals" in the usual drumkit locations. A fully electronic kit weighs much less and takes up less space to transport than an acoustic kit.
Electronic drum pads are the second most widely used type of MIDI performance controllers, after music keyboards. Drum controllers may be built into drum machines, they may be standalone control surfaces (e.g., rubber drum pads), or they may emulate the look and feel of acoustic percussion instruments. The pads built into drum machines are typically too small and fragile to be played with sticks, and they are played with fingers. Dedicated drum pads such as the Roland Octapad or the DrumKAT are playable with the hands or with sticks, and are often built in the form of a drum kit. There are also percussion controllers such as the vibraphone-style MalletKAT, and Don Buchla's Marimba Lumina.
As well as providing an alternative to a conventional acoustic drum kit, electronic drums can be incorporated into an acoustic drum kit to supplement it. MIDI triggers can also be installed into acoustic drum and percussion instruments. Pads that can trigger a MIDI device can be homemade from a piezoelectric sensor and a practice pad or other piece of foam rubber.
This is possible in two ways:
In either case, an electronic control unit (sound module/"brain") with suitable sampled/modeled drum sounds, amplification equipment and monitor speakers are required. See Triggered drum kit.
A trigger pad could contain up to four independent sensors, each of them capable of sending information describing the timing and dynamic intensity of a stroke to the drum module/brain. A circular drum pad may have only one sensor for triggering, but a 2015-era cymbal-shaped rubber pad/cymbal will often contain two; one for the body and one for the bell of the cymbal, and perhaps a cymbal choke trigger, to allow drummers to produce this sound.
Trigger sensors are most commonly used to replace the acoustic drum sounds, but they can often also be used effectively with an acoustic kit to augment or supplement an instrument's sound for the needs of the session. For example, in a live performance in a difficult acoustical space, a trigger may be placed on each drum or cymbal, and used to trigger a similar sound. These sounds are then amplified through the PA system so the audience can hear them, and they can be amplified to any level without the feedback or bleed problems associated with microphones in certain settings.
The sound of the drums and cymbals themselves is heard by the drummer and possibly other musicians in close proximity, but even so, the foldback (audio monitor) system will be fed from the electronic sounds rather than the live acoustic sounds. The drums can be heavily dampened (made to resonate less or subdue the sound), and their tuning and even quality is less critical in the latter scenario. In this way, much of the atmosphere of the live performance is retained in a large venue but without some of the problems associated with purely microphone-amplified drums.
Triggers/sensors, can also be used in conjunction with conventional or built-in microphones. If some components of a kit prove more difficult to "mike" than others, triggers may be used on only the more difficult instruments, balancing out a drummer's/band's sound via the mix.
Trigger pads/drums, on the other hand, when deployed in a conventional set-up, are most commonly used to produce sounds not possible with an acoustic kit, or at least not with what is available. Any sound that can be sampled/recorded can be played when the pad is struck, by assigning the recorded sounds to specific triggers . Recordings or samples of barking dogs, sirens, breaking glass and stereo recordings of aircraft taking off and landing have all been used to great effect. Along with the more obvious electronically generated sounds there are synthesized human voices or song parts or even movie audio or digital video/pictures that (depending on device used) can also be played/triggered by electronic drums.
Virtual drums.
Virtual drums are a type of audio software that simulates the sound of a drum kit. Different drum software products offer a variety of features. Those include a recording function, the ability to select from several acoustically distinctive drum kits, as well as the option to incorporate different songs into the session. Some software for the PC can turn any hard surface into a virtual drum kit using only one microphone. Virtual drumming software is often provided for mobile or tablet formats.
Hardware.
"Hardware" is the name given to the metal stands that support the drums, cymbals and other percussion instruments. Generally the term also includes the hi-hat pedal and bass drum pedal or pedals, and the drum stool, but not the drum sticks.
Hardware is carried along with sticks and other accessories in the traps case, and includes:
Particularly for large kits, many or even all of the stands may be replaced by a drum rack.
In some genres, such as jazz, drummers often set up their own drum hardware onstage. Major rock and pop band drummers on tour will often have a drum tech who knows how to set up their hardware and instruments in the drummers' desired location and layout.
Common configurations.
Drum kits are traditionally categorised by the number of drums, ignoring cymbals and other instruments. Snare, tom-tom and bass drums are always counted; Other drums such as octobans may or may not be counted.
The sizes of drums and cymbals given below are typical. Many drummers differ slightly or radically from them. Where no size is given, it is because there is too much variety to call a typical size.
Three-piece.
A three piece drum set is the most basic set. A conventional three-piece kit consists of bass drum, 14" diameter snare drum, 12"-14" hi-hats, and a single 12" diameter hanging tom, 8"–9" in depth, and a suspended cymbal, in the range of 14"–18", both mounted on the bass drum.
Such kits were common in the 1950s and 1960s and may still be found in small acoustic dance bands. It is a common configuration for kits sold through mail order, and, with smaller size drums and cymbals, for very young drummers.
Four-piece.
A four-piece kit extends the three-piece by one tom, either a second hanging tom mounted on the bass drum and often displacing the cymbal, or a floor tom. Normally another cymbal is added as well, so there are separate ride and crash cymbals, either on two stands, or the ride on the bass drum to the player's right and the crash on a stand.
The standard cymbal sizes are 16" crash and 18"–20" ride, with the 20" ride most common.
Four piece with floor tom.
The floor tom is most often 14" for jazz, and 16" otherwise.
Many historic bands and early rock music recordings used this configuration, notable users including Ringo Starr in the Beatles, Mitch Mitchell in the Jimi Hendrix Experience, John Barbata in the Turtles and many others.
The four-piece kit with floor tom remains popular, particularly for jazz.
Four piece with two hanging toms.
If a second hanging tom is used, it is 10" diameter and 8" deep for fusion, or 13" diameter and one inch deeper than the 12" diameter tom. Otherwise, a 14" diameter hanging tom is added to the 12", both being 8" deep. In any case, both toms are most often mounted on the bass drum with the smaller of the two next to the hi-hats (on the left for a right-handed drummer).
These kits are particularly useful for smaller venues where space is limited.
Five-piece.
The five-piece kit is the full entry-level kit and the most common configuration. It adds a third tom, making three in all.
A fusion kit will normally add a 14" tom, either a floor tom or a hanging tom on a stand to the right of the bass drum; in either case, making the tom lineup 10", 12" and 14".
Other kits will normally have 12" and 13" hanging toms plus either a 14" hanging tom on a stand, a 14" floor tom, or a 16" floor tom. For depths, see Tom-tom drum#Modern tom-toms. In recent years, it is very popular to have 10" and 12" hanging toms, with a 16" floor tom. This configuration is often called a hybrid setup.
The bass drum is most commonly 20" in diameter, but rock kits may use 22" or 24", jazz 18", and big bands up to 26".
A second crash cymbal is common, typically an inch or two larger or smaller than the 16", with the larger of the two to the right for a right-handed drummer, but a big band may use crashes up to 17" and ride up to 24" or, very rarely, 26". A rock kit may also substitute a larger ride cymbal or larger hi-hats, typically 22" for the ride and 15" for the hats.
Most five-piece kits, at more than entry level, also have one or more effects cymbals. Adding cymbals beyond the basic ride, hi-hats and one crash configuration requires stands in addition to that of standard drum hardware packs. Because of this, many higher level kits are sold with little or even no hardware, to allow the drummer to choose the stands and also the bass drum pedal they prefer. At the other extreme, many entry level kits are sold as a five-piece kit complete with two cymbal stands, most often one straight and one boom, and some even with a standard cymbal pack, a stool and a pair of 5A drum sticks.
Modern digital kits are often offered in a five-piece kit, usually with one crash and one ride.
Small kits.
If the toms are omitted completely, or the bass drum is replaced by a pedal-operated beater on the bottom skin of a floor tom and the hanging toms omitted, the result is a two-piece "cocktail" (lounge) kit. Such kits are particularly favoured in musical genres such as trad jazz, rockabilly and jump blues.
Some rockabilly kits and beginners kits for very young players omit the hi-hat stand. In rockabilly, this allows the drummer to play standing rather than seated.
Although these kits may be small with respect to the number of drums used, the drums themselves are most often normal sizes, or even larger in the case of the bass drum. Kits using smaller drums in both smaller and larger configurations are also produced for particular uses, such as "boutique" kits designed to reduce visual impact or space requirements, "travelling" kits to reduce luggage volume, and junior kits for very young players. Smaller drums also tend to be quieter, again suiting smaller venues, and many of these kits extend this with easily fitted extra muffling to the point of allowing quiet or even silent practice in a hotel room or bedroom. 
Extended kits.
Common extensions beyond these standard configurations include:
See also other acoustic instruments above. Another versatile extension becoming increasingly common is the use of some electronic drums in a mainly conventional kit.
Less common extensions found particularly, but not exclusive to very large kits, include:
Accessories.
Sticks.
The most common kit-drumming sticks are wooden sticks modeled on, or in some cases identical to, those originally designed for use with the snare drum. These come in a variety of weights, conventionally expressed as a number, and tip designs, expressed as a letter following the number, with the higher numbers indicating lighter sticks. Thus, a 7A is a common jazz stick with a wooden tip, while a 7N is the same weight, with a nylon tip, and a 7B is a wooden tip but with a different tip profile (shorter and rounder than a 7A). A 5A is a common wood tipped rock stick, heavier than a 7A but with a similar profile. The numbers are most commonly odd but even numbers are used occasionally, in the range 2 (heaviest) to 9 (lightest).
The exact meanings of both numbers and letters differ from manufacturer to manufacturer, and some sticks are not described using this system at all, just being known as "Jazz" (typically a 7N or 8N) or "Heavy Rock" (typically a 4B or 5B) for example. The most common general-purpose stick is a 5A (wood tip, for snare tone) or 5N (nylon tip, for cymbal tone).
Materials, other than wood (hickory, maple, oak, persimmon), used for producing sticks include aluminum (used primarily for marching band applications), acrylic (primarily for visual appeal) and graphite (most often used by "heavy hitters", playing Metal, etc.).
Other sticks commonly used are rutes, consisting of a bundle of canes, and wire or nylon drum brushes. More rarely, other beaters such as cartwheel mallets (known to kit drummers as "soft sticks") may be used. It is not uncommon for rock drummers to use the "wrong" (butt) end of a stick, and in view of this, some makers now produce tipless sticks with two "wrong" ends.
Muffles.
Drum muffles can reduce the ring, overtone frequencies, or volume on a snare, bass, or tom. Controlling the ring is useful in studio or live settings when unwanted frequencies can clash with other instruments in the mix. There are internal and external muffling devices which rest on the inside or outside of the drumhead, respectively. Common types of mufflers include muffling rings, gels and tape, and improvised methods, such as placing a wallet near the edge of the head.
Snare drum and tom-tom
Typical ways to muffle a snare or tom include placing an object on the outer edge of the drumhead. A piece of cloth, a wallet, gel, or fitted rings made of mylar are common objects. Also used are external clip-on muffles that work using the same principle. Internal mufflers that lie on the inside of the drumhead are often built into a drum, but are generally considered less effective than external muffles, as they stifle the initial tone, rather than simply reducing the sustain of it.
Bass drum
Muffling the bass can be achieved with the same muffling techniques as the snare, but bass drums in a drum kit are more commonly muffled by adding pillows or another soft filling inside the drum, between the heads. Cutting a small hole in the resonant head can also produce a more muffled tone, and allows manipulation in internally placed muffling. The Evans EQ pad places a pad against the batterhead and, when struck, the pad moves off the head momentarily, then returns to rest against the head, thus reducing the sustain without choking the tone.
Silencers/mutes
Another type of drum muffler is a piece of rubber that fits over the entire drumhead or cymbal. It interrupts contact between the stick and the head which dampens the sound even more. They are typically used in practice settings.
Companies with muffle products:
Historical uses
Muffled drums are often associated with funeral ceremonies as well, such as the funerals of John F. Kennedy and Queen Victoria. The use of muffled drums has been written about by such poets as Henry Wadsworth Longfellow, John Mayne, and Theodore O'Hara. Drums have also been used for therapy and learning purposes, such as when an experienced player will sit with a number of students and by the end of the session have all of them relaxed and playing complex rhythms.
Cases.
Three types of protective covers are common for kit drums:
As with all musical instruments, the best protection is afforded by a combination of a hard case with padding next to the drums or cymbals.
Microphones.
Microphones are used with drums to pick up the sound of the drums and cymbals for a sound recording and/or to pick up the sound of the drum kit so that it can be amplified through a sound reinforcement system. While most drummers use microphones and amplification in live shows in the 2010s, so that the sound engineer can adjust and balance the levels of the drums and cymbals, some bands that play in quieter genres of music and that play in small venues such as coffeehouses play acoustically, without mics or PA amplification. Small jazz groups such as jazz quartets or organ trios that are playing in a small bar will often just use acoustic drums. Of course if the same small jazz groups play on the mainstage of a big jazz festival, the drums will be mic'ed so that they can be adjusted in the sound system mix. A middle ground approach is used by some bands that play in small venues; they do not mic every drum and cymbal, but rather mic only the instruments that the sound engineer wants to be able to control in the mix, such as the bass drum and the snare.
In "micing" a drum kit, dynamic microphones, which can handle high sound-pressure levels, are usually used to close-mic drums, which is the predominant way to mic drums for live shows. Condenser microphones are used for overheads and room mics, an approach which is more common with sound recording applications. Close micing of drums may be done using stands or by mounting the microphones on the rims of the drums, or even using microphones built into the drum itself, which eliminates the need for stands for these microphones, reducing both clutter and set-up time, as well as isolating them. In some styles of music, drummers use electronic effects on drums, such as individual noise gates that mute the attached microphone when the signal is below a threshold volume. This allows the sound engineer to use a higher overall volume for the drum kit by reducing the number of "active" mics which could feed back at any one time.
Drum screen.
In some styles or settings, such as country music clubs or churches, or when a live recording is being made, the drummer may use a perspex or plexiglass "drum screen" (also known as a "drum shield") to dampen the onstage volume of the drums. A screen that completely surrounds the drum kit is known as a "drum booth". In live sound applications, drum shields are used so that the audio engineer can have more control over the volume of drums that the audience hears through the PA system mix and/or to reduce the overall volume of the band in the venue.
Carpets.
Drummers often bring a carpet, mats or rugs to venues to prevent the bass drum and hi-hat stand from "crawling" (moving away) on a slippery surface from the drum head striking the bass drum. The carpet also reduces short reverberation (which is generally but not always an advantage), and helps to prevent damage to the flooring or floor coverings. In shows where multiple drummers will bring their kits onstage over the night, it is common for drummers to mark the location of their stands and pedals with tape, to allow for quicker positioning of a kits in a drummer's accustomed position. Bass drums and hi-hat stands commonly have retractable spikes to help them to grip surfaces such as carpet, or stay stationary (on hard surfaces) with rubber feet.
Practice equipment.
Drummers use a variety of accessories when practicing. Metronomes and beat counters are used to develop a sense of a steady pulse. Drum muffling pads may be used to lessen the volume of drums during practicing. A practice pad, held on the lap, on a leg, or mounted on a stand, is used for near-silent practice with drumsticks. A set of practice pads mounted to simulate an entire drum kit is known as a practice kit. In the 2010s, these have largely been superseded by electronic drums, which can be listened to with headphones for quiet practice and kits with non-sounding mesh heads.
Tuning equipment.
Drummers use a drum key for tuning their drums and adjusting some drum hardware. Besides the basic type of drum key (a T-handled wrench) there are various tuning wrenches and tools. Basic drum keys are divided in three types which allows tuning of three types of tuning screws on drums: square (most used), slotted and hexagonal. Ratchet-type wrenches allow high-tension drums to be tuned easily. Spin keys (utilizing a ball joint) allow rapid head changing. Torque-wrench type keys are available, graphically revealing the torque at each lug. Also, tension gauges, which are set on the head, aid in consistent tuning.
Playing.
Grooves.
Kit drumming, whether playing accompaniment of voices and other instruments or doing a drum solo, consists of two elements:
Fills.
A "fill" is a departure from the repetitive rhythm. Fills vary from a simple, single stroke on a tom, to a distinctive rhythm played on the hi-hat, to sequences several bars long that are short drum solos. As well as adding interest and variation to the music, fills serve an important function in preparing and supporting significant events in songs. A "vocal cue" is a short drum fill that introduces a vocal entry. A fill ending with a cymbal crash on beat one is often used to lead into a chorus or verse.
Drum solos.
A drum solo is an instrumental section that highlights the virtuosity and skill of the drummer. While other instrument solos such as guitar solos are typically accompanied by the other rhythm section instruments, for most drum solos, all the other band members stop playing so that all of the audience's focus will be on the drummer. Drum solos are common in jazz, but they are also used in a number of rock genres. During drum solos, drummers have a great deal of creative freedom, and drummers often use the entire drum kit. In live concerts, drummers may be given long drum solos.
Grips.
Most drummers hold the drumsticks in one of two types of grip:
Within these two types, there is still considerable variation, and even disagreements as to exactly how the stick is held in a particular method. For example, Jim Chapin, an early and influential exponent of the Moeller method, asserts that the technique does not rely on rebound, while Dave Weckl asserts that it does rely on rebound.
Notation.
Drum kit music is most commonly written on a standard five-line staff. In present-day notation, a special "percussion clef" is used, while previously the bass clef was used. However, even if the bass or no clef is used, each line and space is assigned an instrument of the kit, rather than to a pitch.

</doc>
<doc id="9080" url="https://en.wikipedia.org/wiki?curid=9080" title="Dying Earth">
Dying Earth

Dying Earth is a fantasy series by the American author Jack Vance, comprising four books originally published from 1950 to 1984.
Some have been called picaresque. They vary from short story collection to fix-up (novel created from older short stories) perhaps all the way to novel.
The first book in the series, "The Dying Earth", was ranked number 16 of 33 "All Time Best Fantasy Novels" by "Locus" in 1987, based on a poll of subscribers, although it was marketed as a collection and the ISFDB calls it a "loosely connected series of stories".
Setting.
The stories of the "Dying Earth" series are set in the distant future, at a point when the sun is almost exhausted and magic has reasserted itself as a dominant force. The Moon has disappeared and the Sun is in danger of burning out at any time, often flickering as if about to go out, before shining again. The various civilizations of Earth have collapsed for the most part into decadence and its inhabitants overcome with a fatalistic outlook. The Earth is mostly barren and cold, and has become infested with various predatory monsters (possibly created by a magician in a former age).
Origins.
Vance wrote the stories of the first book while he served in the United States Merchant Marine during World War II. In the late 1940s several of his other stories were published in magazines.
According to pulp editor Sam Merwin, Vance's earliest magazine submissions in the 1940s were heavily influenced by the style of James Branch Cabell. Fantasy historian Lin Carter has noted several probable lasting influences of Cabell on Vance's work, and suggests that the early "pseudo-Cabell" experiments bore fruit in "The Dying Earth" (1950).
Series.
The series comprises four books by Vance and some sequels by other authors that may be or may have been canonical.
One 741-page omnibus has been issued as "The Complete Dying Earth" (SF Book Club, 1999) and in both the US and UK as "Tales of the Dying Earth" (2000).
Stories by Vance.
All four books were published with Tables of Contents, the first and fourth as collections. The second and third contained mostly material previously published in short story form but were marketed as novels, the second as a fix-up and the third without acknowledging any previous publication.
1. "The Dying Earth" (the author's preferred title is "Mazirian the Magician") was openly a collection of six stories, all original, although written during Vance's WWII service. ISFDB calls them "slightly connected" and catalogs the last as a novella (17,500 to 40,000 word count).
2. "Eyes of the Overworld" (the author's preferred title is "Cugel the Clever") was a fix-up of six stories, presented as seven. All were novelettes by word count (7500 to 17,500). Five were previously published as noted here.
3. "Cugel's Saga" (the author's preferred title is "Cugel: The Skybreak Spatterlight") was marketed as a novel. ISFDB calls it "wice as large and less episodic than "Eyes of the Overworld"" but qualifies that label. "This is marketed as a novel, but there is a table of contents, and some of the parts were previously published (although none are acknowledged thus)." It catalogs previous publication of three chapters without remark on the degree of revision.
4. "Rhialto the Marvellous" was marketed as a collection, a Foreword and three stories, one previously published. The Foreword is non-narrative canonical fiction presenting the general state of the world in the 21st Aeon (a "short story" loosely).
Sequels.
Some sequels have been written by other authors, either with Vance's authorization or as tributes to his work.
Michael Shea's first publication, the novel "A Quest for Simbilis" (DAW Books, 1974, OCLC 2128177), was an authorized sequel to "Eyes". However, "When Vance returned to the milieu, his "Cugel's Saga" continued the events of "The Eyes of the Overworld" in a different direction."
The tribute anthology "Songs of the Dying Earth" (2009) contains short fiction set in the world of the Dying Earth by numerous writers alongside tributes to Vance's work and influence.
In 2010 Shea wrote another authorized story belonging to the "Dying Earth" series and featuring Cugel as one of characters: "Hew the Tintmaster", published in the anthology "Swords & Dark Magic: The New Sword and Sorcery", ed. Jonathan Strahan and Lou Anders (Eos, 2010, pp. 323–362).
Translations.
WorldCat contributing libraries report holding all four books in French, Spanish, and (in omnibus edition) Hebrew translations; and report holding "The Dying Earth" in five other languages: Finnish, German, Japanese, Polish, and Russian.
The whole first volume (six stories) has been translated also into Esperanto together with two Cugel stories and made available on-line as e-books by a long-time fan and Vance Integral Edition co-worker. Permission to translate and distribute (only into Esperanto) was obtained informally direct from the author and, since his death in 2013, continues with ongoing permission from the author's estate. To date these are three: "Mazirian the Magician", "The Sorcerer Pharesm", and "The Bagful of Dreams" available for free download as EPub, Mobi and PDF.
Legacy.
The Dying Earth subgenre of science fiction is named in recognition of Vance's role in standardizing a setting, the entropically dying earth and sun. Its importance was recognized with the publication of "Songs of the Dying Earth", a tribute anthology edited by George R. R. Martin and Gardner Dozois (Subterranean, 2009). Each short story in the anthology is set on the Dying Earth, and concludes with a short acknowledgement by the author of Vance's influence on them.
Some particular works since Vance should be singled out.
Print.
Gene Wolfe's "The Book of the New Sun" (1980–83) is set in a slightly similar world, and was written under Vance's influence. Wolfe suggested in "The Castle of the Otter", a collection of essays, that he inserted the book "The Dying Earth" into his fictional world under the title "The Book of Gold" (specifically, Wolfe wrote that the "Book of Gold" mentioned in "The Book of the New Sun" is different for each reader, but for him it was "The Dying Earth.")'. Wolfe has extended the series.
Michael Shea's novel "Nifft the Lean" (1982), his second book eight years after "A Quest for Simbilis", also owes much debt to Vance's creation, since the protagonist of the story is a petty thief (not unlike Cugel the Clever), who travels and struggles in an exotic world. Shea returned to Nifft with 1997 and 2000 sequels.
The Archonate stories by Matthew Hughes — the 1994 novel "Fools Errant" and numerous works in this millennium —
take place in "the penultimate age of Old Earth," a period of science and technology that is on the verge of transforming into the magical era of the time of the Dying Earth.
Booklist has called him Vance's "heir apparent." (Review by Carl Hays of The Gist Hunter and Other Stories, Booklist, August 2005)
Role-playing.
The original creators of the "Dungeons & Dragons" games were fans of Jack Vance and incorporated many aspects of the "Dying Earth" series into the game. The magic system, in which a wizard is limited in the number of spells that can be simultaneously remembered and forgets them once they are cast, was based on the magic of Dying Earth. In role-playing game circles, this sort of magic system is called 'Vancian'. Some of the spells from "Dungeons & Dragons" are based on spells mentioned in the "Dying Earth" series, such as the "prismatic spray". Magic items from the "Dying Earth" stories such as ioun stones also made their way into Dungeons & Dragons. One of the deities of magic in "Dungeons & Dragons" is named Vecna (an anagram of Vance).
The Talislanta role-playing game designed by Stephan Michael Sechi and originally published in 1987 by Bard Games was inspired by the works of Jack Vance so much so that the first release, The Chronicles of Talislanta is dedicated to the author.
There is an official "Dying Earth" role-playing game published by Pelgrane Press with an occasional magazine "The Excellent Prismatic Spray" (named for a magic spell). The game situates players in Vance's world populated by desperately extravagant people. Many other role-playing settings pay homage to the series by including fantasy elements he invented such as the darkness-dwelling Grues.

</doc>
<doc id="9082" url="https://en.wikipedia.org/wiki?curid=9082" title="Dispute resolution">
Dispute resolution

Dispute resolution is the process of resolving disputes between parties.
Methods.
Methods of dispute resolution include:
One could theoretically include violence or even war as part of this spectrum, but dispute resolution practitioners do not usually do so; violence rarely ends disputes effectively, and indeed, often only escalates them. 
Dispute resolution processes fall into two major types:
Not all disputes, even those in which skilled intervention occurs, end in resolution. Such intractable disputes form a special area in dispute resolution studies.
Dispute Resolution is an important requirement in International Trade:Negotiation, Mediation, Arbitration and Legal Action.
Judicial dispute resolution.
The legal system provides resolutions for many different types of disputes. However, some disputants will not reach agreement through a collaborative process. Some disputes need the coercive power of the state to enforce a resolution. Perhaps more importantly, many people want a professional advocate when they become involved in a dispute, particularly if the dispute involves perceived legal rights, legal wrongdoing, or threat of legal action against them.
The most common form of judicial dispute resolution is litigation. Litigation is initiated when one party files suit against another. In the United States, litigation is facilitated by the government within federal, state, and municipal courts. The proceedings are very formal and are governed by rules, such as rules of evidence and procedure, which are established by the legislature. Outcomes are decided by an impartial judge and/or jury, based on the factual questions of the case and the application law. The verdict of the court is binding, not advisory; however, both parties have the right to appeal the judgment to a higher court. Judicial dispute resolution is typically adversarial in nature, for example, involving antagonistic parties or opposing interests seeking an outcome most favorable to their position. 
Retired judges or private lawyers often become arbitrators or mediators; however, trained and qualified non-legal dispute resolution specialists form a growing body within the field of ADR. In the United States, many states now have mediation or other ADR programs annexed to the courts, to facilitate settlement of lawsuits.
Extrajudicial dispute resolution.
Some use the term "dispute resolution" to refer only to alternative dispute resolution (ADR), that is, extrajudicial processes such as arbitration, collaborative law, and mediation used to resolve conflict and potential conflict between and among individuals, business entities, governmental agencies, and (in the public international law context) states. ADR generally depends on agreement by the parties to use ADR processes, either before or after a dispute has arisen. ADR has experienced steadily increasing acceptance and utilization because of a perception of greater flexibility, costs below those of traditional litigation, and speedy resolution of disputes, among other perceived advantages. However, some have criticized these methods as taking away the right to seek redress of grievances in the courts, suggesting that extrajudicial dispute resolution may not offer the fairest way for parties not in an equal bargaining relationship, for example in a dispute between a consumer and a large corporation. In addition, in some circumstances, arbitration and other ADR processes may become as expensive as litigation or more so.
Online dispute resolution.
Dispute resolution can also take place on-line or by using technology in certain cases. Online dispute resolution, a growing field of dispute resolution, uses new technologies to solve disputes. Online Dispute Resolution is also called "ODR". Online Dispute Resolution or ODR also involves the application of traditional dispute resolution methods to disputes which arise online.

</doc>
<doc id="9087" url="https://en.wikipedia.org/wiki?curid=9087" title="Dynamical system">
Dynamical system

In mathematics, a dynamical system is a system in which a function describes the time dependence of a point in a geometrical space. Examples include the mathematical models that describe the swinging of a clock pendulum, the flow of water in a pipe, and the number of fish each springtime in a lake.
At any given time a dynamical system has a state given by a set of real numbers (a vector) that can be represented by a point in an appropriate state space (a geometrical manifold). The "evolution rule" of the dynamical system is a function that describes what future states follow from the current state. Often the function is deterministic; in other words, for a given time interval only one future state follows from the current state; however, some systems are stochastic, in that random events also affect the evolution of the state variables.
Overview.
The concept of a dynamical system has its origins in Newtonian mechanics. There, as in other natural sciences and engineering disciplines, the evolution rule of dynamical systems is an implicit relation that gives the state of the system for only a short time into the future. (The relation is either a differential equation, difference equation or other time scale.) To determine the state for all future times requires iterating the relation many times—each advancing time a small step. The iteration procedure is referred to as "solving the system" or "integrating the system". If the system can be solved, given an initial point it is possible to determine all its future positions, a collection of points known as a "trajectory" or "orbit".
Before the advent of computers, finding an orbit required sophisticated mathematical techniques and could be accomplished only for a small class of dynamical systems. Numerical methods implemented on electronic computing machines have simplified the task of determining the orbits of a dynamical system.
For simple dynamical systems, knowing the trajectory is often sufficient, but most dynamical systems are too complicated to be understood in terms of individual trajectories. The difficulties arise because:
History.
Many people regard Henri Poincaré as the founder of dynamical systems. Poincaré published two now classical monographs, "New Methods of Celestial Mechanics" (1892–1899) and "Lectures on Celestial Mechanics" (1905–1910). In them, he successfully applied the results of their research to the problem of the motion of three bodies and studied in detail the behavior of solutions (frequency, stability, asymptotic, and so on). These papers included the Poincaré recurrence theorem, which states that certain systems will, after a sufficiently long but finite time, return to a state very close to the initial state.
Aleksandr Lyapunov developed many important approximation methods. His methods, which he developed in 1899, make it possible to define the stability of sets of ordinary differential equations. He created the modern theory of the stability of a dynamic system.
In 1913, George David Birkhoff proved Poincaré's "Last Geometric Theorem," a special case of the three-body problem, a result that made him world famous. In 1927, he published his "Dynamical Systems"Birkhoff's most durable result has been his 1931 discovery of what is now called the ergodic theorem. Combining insights from physics on the ergodic hypothesis with measure theory, this theorem solved, at least in principle, a fundamental problem of statistical mechanics. The ergodic theorem has also had repercussions for dynamics.
Stephen Smale made significant advances as well. His first contribution is the Smale horseshoe that jumpstarted significant research in dynamical systems. He also outlined a research program carried out by many others.
Oleksandr Mykolaiovych Sharkovsky developed Sharkovsky's Theorem on the periods of discrete dynamical systems in 1964. One of the implications of the theorem is that if a discrete dynamical system on the real line has a periodic point of period 3, then it must have periodic points of every other period.
Basic definitions.
A dynamical system is a manifold "M" called the phase (or state) space endowed with a family of smooth evolution functions Φ"t" that for any element of "t" ∈ "T", the time, map a point of the phase space back into the phase space. The notion of smoothness changes with applications and the type of manifold. There are several choices for the set "T". When "T" is taken to be the reals, the dynamical system is called a "flow"; and if "T" is restricted to the non-negative reals, then the dynamical system is a "semi-flow". When "T" is taken to be the integers, it is a "cascade" or a "map"; and the restriction to the non-negative integers is a "semi-cascade".
Examples.
The evolution function Φ "t" is often the solution of a "differential equation of motion"
The equation gives the time derivative, represented by the dot, of a trajectory "x"("t") on the phase space starting at some point "x"0. The vector field "v"("x") is a smooth function that at every point of the phase space "M" provides the velocity vector of the dynamical system at that point. (These vectors are not vectors in the phase space "M", but in the tangent space "TxM" of the point "x".) Given a smooth Φ "t", an autonomous vector field can be derived from it.
There is no need for higher order derivatives in the equation, nor for time dependence in "v"("x") because these can be eliminated by considering systems of higher dimensions. Other types of differential equations can be used to define the evolution rule:
is an example of an equation that arises from the modeling of mechanical systems with complicated constraints.
The differential equations determining the evolution function Φ "t" are often ordinary differential equations: in this case the phase space "M" is a finite dimensional manifold. Many of the concepts in dynamical systems can be extended to infinite-dimensional manifolds—those that are locally Banach spaces—in which case the differential equations are partial differential equations. In the late 20th century the dynamical system perspective to partial differential equations started gaining popularity.
Linear dynamical systems.
Linear dynamical systems can be solved in terms of simple functions and the behavior of all orbits classified. In a linear system the phase space is the "N"-dimensional Euclidean space, so any point in phase space can be represented by a vector with "N" numbers. The analysis of linear systems is possible because they satisfy a superposition principle: if "u"("t") and "w"("t") satisfy the differential equation for the vector field (but not necessarily the initial condition), then so will "u"("t") + "w"("t").
Flows.
For a flow, the vector field Φ("x") is an affine function of the position in the phase space, that is,
with "A" a matrix, "b" a vector of numbers and "x" the position vector. The solution to this system can be found by using the superposition principle (linearity).
The case "b" ≠ 0 with "A" = 0 is just a straight line in the direction of "b":
When "b" is zero and "A" ≠ 0 the origin is an equilibrium (or singular) point of the flow, that is, if "x"0 = 0, then the orbit remains there.
For other initial conditions, the equation of motion is given by the exponential of a matrix: for an initial point "x"0,
When "b" = 0, the eigenvalues of "A" determine the structure of the phase space. From the eigenvalues and the eigenvectors of "A" it is possible to determine if an initial point will converge or diverge to the equilibrium point at the origin.
The distance between two different initial conditions in the case "A" ≠ 0 will change exponentially in most cases, either converging exponentially fast towards a point, or diverging exponentially fast. Linear systems display sensitive dependence on initial conditions in the case of divergence. For nonlinear systems this is one of the (necessary but not sufficient) conditions for chaotic behavior.
Maps.
A discrete-time, affine dynamical system has the form of a matrix difference equation:
with "A" a matrix and "b" a vector. As in the continuous case, the change of coordinates "x" → "x" + (1 − "A") –1"b" removes the term "b" from the equation. In the new coordinate system, the origin is a fixed point of the map and the solutions are of the linear system "A" "n""x"0.
The solutions for the map are no longer curves, but points that hop in the phase space. The orbits are organized in curves, or fibers, which are collections of points that map into themselves under the action of the map.
As in the continuous case, the eigenvalues and eigenvectors of "A" determine the structure of phase space. For example, if "u"1 is an eigenvector of "A", with a real eigenvalue smaller than one, then the straight lines given by the points along "α" "u"1, with "α" ∈ R, is an invariant curve of the map. Points in this straight line run into the fixed point.
There are also many other discrete dynamical systems.
Local dynamics.
The qualitative properties of dynamical systems do not change under a smooth change of coordinates (this is sometimes taken as a definition of qualitative): a "singular point" of the vector field (a point where "v"("x") = 0) will remain a singular point under smooth transformations; a "periodic orbit" is a loop in phase space and smooth deformations of the phase space cannot alter it being a loop. It is in the neighborhood of singular points and periodic orbits that the structure of a phase space of a dynamical system can be well understood. In the qualitative study of dynamical systems, the approach is to show that there is a change of coordinates (usually unspecified, but computable) that makes the dynamical system as simple as possible.
Rectification.
A flow in most small patches of the phase space can be made very simple. If "y" is a point where the vector field "v"("y") ≠ 0, then there is a change of coordinates for a region around "y" where the vector field becomes a series of parallel vectors of the same magnitude. This is known as the rectification theorem.
The "rectification theorem" says that away from singular points the dynamics of a point in a small patch is a straight line. The patch can sometimes be enlarged by stitching several patches together, and when this works out in the whole phase space "M" the dynamical system is "integrable". In most cases the patch cannot be extended to the entire phase space. There may be singular points in the vector field (where "v"("x") = 0); or the patches may become smaller and smaller as some point is approached. The more subtle reason is a global constraint, where the trajectory starts out in a patch, and after visiting a series of other patches comes back to the original one. If the next time the orbit loops around phase space in a different way, then it is impossible to rectify the vector field in the whole series of patches.
Near periodic orbits.
In general, in the neighborhood of a periodic orbit the rectification theorem cannot be used. Poincaré developed an approach that transforms the analysis near a periodic orbit to the analysis of a map. Pick a point "x"0 in the orbit γ and consider the points in phase space in that neighborhood that are perpendicular to "v"("x"0). These points are a Poincaré section "S"("γ", "x"0), of the orbit. The flow now defines a map, the Poincaré map "F" : "S" → "S", for points starting in "S" and returning to "S". Not all these points will take the same amount of time to come back, but the times will be close to the time it takes "x"0.
The intersection of the periodic orbit with the Poincaré section is a fixed point of the Poincaré map "F". By a translation, the point can be assumed to be at "x" = 0. The Taylor series of the map is "F"("x") = "J" · "x" + O("x"2), so a change of coordinates "h" can only be expected to simplify "F" to its linear part
This is known as the conjugation equation. Finding conditions for this equation to hold has been one of the major tasks of research in dynamical systems. Poincaré first approached it assuming all functions to be analytic and in the process discovered the non-resonant condition. If "λ"1, ..., "λ""ν" are the eigenvalues of "J" they will be resonant if one eigenvalue is an integer linear combination of two or more of the others. As terms of the form "λ""i" – ∑ (multiples of other eigenvalues) occurs in the denominator of the terms for the function "h", the non-resonant condition is also known as the small divisor problem.
Conjugation results.
The results on the existence of a solution to the conjugation equation depend on the eigenvalues of "J" and the degree of smoothness required from "h". As "J" does not need to have any special symmetries, its eigenvalues will typically be complex numbers. When the eigenvalues of "J" are not in the unit circle, the dynamics near the fixed point "x"0 of "F" is called "hyperbolic" and when the eigenvalues are on the unit circle and complex, the dynamics is called "elliptic".
In the hyperbolic case, the Hartman–Grobman theorem gives the conditions for the existence of a continuous function that maps the neighborhood of the fixed point of the map to the linear map "J" · "x". The hyperbolic case is also "structurally stable". Small changes in the vector field will only produce small changes in the Poincaré map and these small changes will reflect in small changes in the position of the eigenvalues of "J" in the complex plane, implying that the map is still hyperbolic.
The Kolmogorov–Arnold–Moser (KAM) theorem gives the behavior near an elliptic point.
Bifurcation theory.
When the evolution map Φ"t" (or the vector field it is derived from) depends on a parameter μ, the structure of the phase space will also depend on this parameter. Small changes may produce no qualitative changes in the phase space until a special value "μ"0 is reached. At this point the phase space changes qualitatively and the dynamical system is said to have gone through a bifurcation.
Bifurcation theory considers a structure in phase space (typically a fixed point, a periodic orbit, or an invariant torus) and studies its behavior as a function of the parameter "μ". At the bifurcation point the structure may change its stability, split into new structures, or merge with other structures. By using Taylor series approximations of the maps and an understanding of the differences that may be eliminated by a change of coordinates, it is possible to catalog the bifurcations of dynamical systems.
The bifurcations of a hyperbolic fixed point "x"0 of a system family "Fμ" can be characterized by the eigenvalues of the first derivative of the system "DF""μ"("x"0) computed at the bifurcation point. For a map, the bifurcation will occur when there are eigenvalues of "DFμ" on the unit circle. For a flow, it will occur when there are eigenvalues on the imaginary axis. For more information, see the main article on Bifurcation theory.
Some bifurcations can lead to very complicated structures in phase space. For example, the Ruelle–Takens scenario describes how a periodic orbit bifurcates into a torus and the torus into a strange attractor. In another example, Feigenbaum period-doubling describes how a stable periodic orbit goes through a series of period-doubling bifurcations.
Ergodic systems.
In many dynamical systems, it is possible to choose the coordinates of the system so that the volume (really a ν-dimensional volume) in phase space is invariant. This happens for mechanical systems derived from Newton's laws as long as the coordinates are the position and the momentum and the volume is measured in units of (position) × (momentum). The flow takes points of a subset "A" into the points Φ "t"("A") and invariance of the phase space means that
In the Hamiltonian formalism, given a coordinate it is possible to derive the appropriate (generalized) momentum such that the associated volume is preserved by the flow. The volume is said to be computed by the Liouville measure.
In a Hamiltonian system, not all possible configurations of position and momentum can be reached from an initial condition. Because of energy conservation, only the states with the same energy as the initial condition are accessible. The states with the same energy form an energy shell Ω, a sub-manifold of the phase space. The volume of the energy shell, computed using the Liouville measure, is preserved under evolution.
For systems where the volume is preserved by the flow, Poincaré discovered the recurrence theorem: Assume the phase space has a finite Liouville volume and let "F" be a phase space volume-preserving map and "A" a subset of the phase space. Then almost every point of "A" returns to "A" infinitely often. The Poincaré recurrence theorem was used by Zermelo to object to Boltzmann's derivation of the increase in entropy in a dynamical system of colliding atoms.
One of the questions raised by Boltzmann's work was the possible equality between time averages and space averages, what he called the ergodic hypothesis. The hypothesis states that the length of time a typical trajectory spends in a region "A" is vol("A")/vol(Ω).
The ergodic hypothesis turned out not to be the essential property needed for the development of statistical mechanics and a series of other ergodic-like properties were introduced to capture the relevant aspects of physical systems. Koopman approached the study of ergodic systems by the use of functional analysis. An observable "a" is a function that to each point of the phase space associates a number (say instantaneous pressure, or average height). The value of an observable can be computed at another time by using the evolution function φ t. This introduces an operator "U" "t", the transfer operator,
By studying the spectral properties of the linear operator "U" it becomes possible to classify the ergodic properties of Φ "t". In using the Koopman approach of considering the action of the flow on an observable function, the finite-dimensional nonlinear problem involving Φ "t" gets mapped into an infinite-dimensional linear problem involving "U".
The Liouville measure restricted to the energy surface Ω is the basis for the averages computed in equilibrium statistical mechanics. An average in time along a trajectory is equivalent to an average in space computed with the Boltzmann factor exp(−β"H"). This idea has been generalized by Sinai, Bowen, and Ruelle (SRB) to a larger class of dynamical systems that includes dissipative systems. SRB measures replace the Boltzmann factor and they are defined on attractors of chaotic systems.
Nonlinear dynamical systems and chaos.
Simple nonlinear dynamical systems and even piecewise linear systems can exhibit a completely unpredictable behavior, which might seem to be random, despite the fact that they are fundamentally deterministic. This seemingly unpredictable behavior has been called "chaos". Hyperbolic systems are precisely defined dynamical systems that exhibit the properties ascribed to chaotic systems. In hyperbolic systems the tangent space perpendicular to a trajectory can be well separated into two parts: one with the points that converge towards the orbit (the "stable manifold") and another of the points that diverge from the orbit (the "unstable manifold").
This branch of mathematics deals with the long-term qualitative behavior of dynamical systems. Here, the focus is not on finding precise solutions to the equations defining the dynamical system (which is often hopeless), but rather to answer questions like "Will the system settle down to a steady state in the long term, and if so, what are the possible attractors?" or "Does the long-term behavior of the system depend on its initial condition?"
Note that the chaotic behavior of complex systems is not the issue. Meteorology has been known for years to involve complex—even chaotic—behavior. Chaos theory has been so surprising because chaos can be found within almost trivial systems. The logistic map is only a second-degree polynomial; the horseshoe map is piecewise linear.
Geometrical definition.
A dynamical system is the tuple formula_10, with formula_11 a manifold (locally a Banach space or Euclidean space), formula_12 the domain for time (non-negative reals, the integers, ...) and "f" an evolution rule "t" → "f" "t" (with formula_13) such that "f t" is a diffeomorphism of the manifold to itself. So, f is a mapping of the time-domain formula_14 into the space of diffeomorphisms of the manifold to itself. In other terms, "f"("t") is a diffeomorphism, for every time "t" in the domain formula_14 .
Measure theoretical definition.
A dynamical system may be defined formally, as a measure-preserving transformation of a sigma-algebra, the quadruplet ("X", Σ, μ, τ). Here, "X" is a set, and Σ is a sigma-algebra on "X", so that the pair ("X", Σ) is a measurable space. μ is a finite measure on the sigma-algebra, so that the triplet ("X", Σ, μ) is a probability space. A map τ: "X" → "X" is said to be Σ-measurable if and only if, for every σ ∈ Σ, one has formula_16. A map τ is said to preserve the measure if and only if, for every σ ∈ Σ, one has formula_17. Combining the above, a map τ is said to be a measure-preserving transformation of "X" , if it is a map from "X" to itself, it is Σ-measurable, and is measure-preserving. The quadruple ("X", Σ, μ, τ), for such a τ, is then defined to be a dynamical system.
The map τ embodies the time evolution of the dynamical system. Thus, for discrete dynamical systems the iterates formula_18 for integer "n" are studied. For continuous dynamical systems, the map τ is understood to be a finite time evolution map and the construction is more complicated.
Multidimensional generalization.
Dynamical systems are defined over a single independent variable, usually thought of as time. A more general class of systems are defined over multiple independent variables and are therefore called multidimensional systems. Such systems are useful for modeling, for example, image processing.
Further reading.
Works providing a broad coverage:
Introductory texts with a unique perspective:
Textbooks
Popularizations:
External links.
Online books or lecture notes:
Research groups:
Simulation software based on Dynamical Systems approach:

</doc>
<doc id="9090" url="https://en.wikipedia.org/wiki?curid=9090" title="Dhimmi">
Dhimmi

A ( "", , collectively "/" "the people of the "dhimma"") is a historical term referring to non-Muslim citizens of an Islamic state. The word literally means "protected person." According to scholars, dhimmis had their rights fully protected in their communities, but as citizens in the Islamic state, had certain restrictions, and it was obligatory for them to pay the jizya tax, which complemented the zakat, or Islamic tax, paid by the Muslim subjects. Dhimmis were excluded from specific duties assigned to Muslims, and did not enjoy certain political rights reserved for Muslims, but were otherwise equal under the laws of property, contract, and obligation.
Under sharia, the dhimmi communities were usually subjected to their own special laws, rather than some of the laws which were applicable only to the Muslim community. For example, the Jewish community in Medina was allowed to have its own Halakhic courts, and the Ottoman millet system allowed its various dhimmi communities to rule themselves under separate legal courts. These courts did not cover cases that involved religious groups outside of their own community, or capital offences. Dhimmi communities were also allowed to engage in certain practices that were usually forbidden for the Muslim community, such as the consumption of alcohol and pork.
Historically, dhimmi status was originally applied to Jews, Christians, and Sabians. This status later also came to be applied to Zoroastrians, Mandaeans, Hindus, and Buddhists. Eventually, the Hanafi school, the largest school of Islamic jurisprudence, and the Maliki school, the second largest school of Islamic jurisprudence, applied this term to all non-Muslims living in Islamic lands outside the sacred area surrounding Mecca, in present-day Saudi Arabia. Some modern Hanafi scholars, however, do not make any legal distinction between a non-Muslim dhimmi and a Muslim citizen.
Moderate Muslims generally reject the dhimma system as inappropriate for the age of nation-states and democracies. There is a range of opinions among 20th century and contemporary theologians about whether the notion of dhimma is appropriate for modern times, and, if so, what form it should take in an Islamic state.
The "dhimma contract".
Based on Quranic verses and Islamic traditions, classical sharia distinguishes between Muslims, followers of other Abrahamic religions, and pagans or people belonging to other polytheistic religions. As monotheists, Jews and Christians have traditionally been considered "People of the Book," and afforded a special status known as "dhimmi" derived from a theoretical contract—"dhimma" or "residence in return for taxes". There are parallels for this in Roman and Jewish law. Muslim governments in the Indus basin readily extended the dhimmi status to the Hindus and Buddhists of India. Eventually, the largest school of Islamic scholarship applied this term to all non-Muslims living in Islamic lands outside the sacred area surrounding Mecca, Saudi Arabia.
Classical sharia incorporated the religious laws and courts of Christians, Jews and Hindus, as seen in the early caliphate, Al-Andalus, Indian subcontinent, and the Ottoman Millet system. Quoting the Qur'anic statement, "Let Christians judge according to what We have revealed in the Gospel", Muhammad Hamidullah writes that Islam has decentralized and "communalized" law and justice. In medieval Islamic societies, the "qadi" (Islamic judge) usually could not interfere in the matters of non-Muslims unless the parties voluntarily chose to be judged according to Islamic law, thus the dhimmi communities living in Islamic states usually had their own laws independent from the sharia law, as with the Jews who would have their own rabbinical courts. These courts did not cover cases that involved other religious groups, or capital offences or threats to public order. By the 18th century, however, dhimmis frequently attended the Ottoman Muslim courts, where cases were taken against them by Muslims, or they took cases against Muslims or other dhimmis. Oaths sworn by dhimmis in these courts were tailored to their beliefs.
Non-Muslims were allowed to engage in certain practices (such as the consumption of alcohol and pork) that were usually forbidden by Islamic law, in point of fact, any Muslim who pours away their wine or forcibly appropriates it is liable to pay compensation. Zoroastrian "self-marriages", that were considered incestuous under sharia, were also tolerated. Ibn Qayyim Al-Jawziyya (1292–1350) opined that non-Muslims were entitled to such practices since they could not be presented to sharia courts and the religious minorities in question held it permissible. This ruling was based on the precedent that the prophet Muhammad did not forbid such self-marriages among Zoroastrians despite coming into contact with Zoroastrians and knowing about this practice. Religious minorities were also free to do as they wished in their own homes, provided they did not publicly engage in illicit sexual activity in ways that could threaten public morals.
However, the classical dhimma contract is no longer enforced. Western influence has been instrumental in eliminating the restrictions and protections of the dhimma contract.
According to law professor H. Patrick Glenn of McGill University, "it is said that the dhimmi are 'excluded from the specifically Muslim privileges, but on the other hand they are excluded from the specifically Muslim duties' while (and here there are clear parallels with western public and private law treatment of aliens—Fremdenrecht, la condition de estrangers), '[for the rest, the Muslim and the dhimmi are equal in practically the whole of the law of property and of contracts and obligations'."
The dhimma contract and sharia law.
The dhimma contract is an integral part of traditional Islamic sharia. From the 9th century AD, the power to interpret and refine law in traditional Islamic societies was in the hands of the scholars ("ulama"). This separation of powers served to limit the range of actions available to the ruler, who could not easily decree or reinterpret law independently and expect the continued support of the community. Through succeeding centuries and empires, the balance between the ulema and the rulers shifted and reformed, but the balance of power was never decisively changed. At the beginning of the 19th century, the Industrial Revolution and the French Revolution introduced an era of European world hegemony that included the domination of most of the lands of Islam. At the end of the Second World War, the European powers found themselves too weakened to maintain their empires. The wide variety in forms of government, systems of law, attitudes toward modernity and interpretations of sharia are a result of the ensuing drives for independence and modernity in the Muslim world.
Muslim states, sects, schools of thought and individuals differ as to exactly what sharia law entails. In addition, Muslim states today utilize a spectrum of legal systems. Most states have a mixed system that implements certain aspects of sharia while acknowledging the supremacy of a constitution. A few, such as Turkey, have declared themselves secular. Local and customary laws may take precedence in certain matters, as well. Islamic law is therefore polynormative, and despite several cases of regression in recent years, the trend is towards modernization and liberalization. Questions of human rights and the status of minorities cannot be generalized with regards to the Muslim world. They must instead be examined on a case-by-case basis, within specific political and cultural contexts, using perspectives drawn from the historical framework.
The end of the dhimma contract.
The status of the dhimmi "was for long accepted with resignation by the Christians and with gratitude by the Jews" but the rising power of Christendom and the radical ideas of the French Revolution caused a wave of discontent among Christian dhimmis. The continuing and growing pressure from the European powers combined with pressure from Muslim reformers gradually relaxed the inequalities between Muslims and non-Muslims.
The collection of the jizya tax from non-Muslims was widespread throughout the history of Islam. In the mid-19th century, the Ottoman Empire significantly relaxed the restrictions and taxes placed on its non-Muslim residents under Ottomanism. These relaxations occurred gradually as part of the Tanzimat reform movement, which began in 1839 with the accession of the Ottoman Sultan, Abdülmecid I.
On November 3, 1839, the Edict of Gülhane (Hatt-i Sharif of Gulhane) edict was put forth by the Sultan, in part proclaiming the principle of equality among all subjects regardless of religion. Part of the motivation for this was a desire to gain support from the British Empire, whose help was needed in a conflict with Egypt.
On February 18, 1856, the Ottoman Reform Edict of 1856 (Hatt-i Humayan) was issued, building upon the 1839 edict. It came about partly as a result of pressure from and the efforts of the ambassadors of Great Britain, France, and Austria, whose respective countries were needed as allies in the Crimean War. It again proclaimed the principle of equality between Muslims and non-Muslims, and produced many specific reforms to this end. For example, the jizya tax was abolished and non-Muslims were allowed to join the army.
Dhimmi communities.
Jews and Christians living under early Muslim rule were considered dhimmis, a status that was later also extended to other non-Muslims like Hindus. They were allowed to "practise their religion, subject to certain conditions, and to enjoy a measure of communal autonomy" and guaranteed their personal safety and security of property, in return for paying tribute and acknowledging Muslim rule. Islamic law and custom prohibited the enslavement of free dhimmis within lands under Islamic rule. Taxation from the perspective of dhimmis who came under the Muslim rule, was "a concrete continuation of the taxes paid to earlier regimes" (but lower under the Muslim rule). They were also exempted from the zakat tax paid by Muslims. The dhimmi communities living in Islamic states had their own laws independent from the Sharia law, such as the Jews who had their own Halakhic courts. The dhimmi communities had their own leaders, courts, personal and religious laws, and "generally speaking, Muslim tolerance of unbelievers was far better than anything available in Christendom, until the rise of secularism in the 17th century". "Muslims guaranteed freedom of worship and livelihood, provided that they remained loyal to the Muslim state and paid a poll tax". "Muslim governments appointed Christian and Jewish professionals to their bureaucracies", and thus, Christians and Jews "contributed to the making of the Islamic civilization".
However, dhimmis faced social and symbolic restrictions, and a pattern of stricter, then more lax, enforcement developed over time. Marshall Hodgson, a historian of Islam, writes that during the era of the High Caliphate (7th–13th Centuries), zealous Shariah-minded Muslims gladly elaborated their code of symbolic restrictions on the dhimmis.
From an Islamic legal perspective, the pledge of protection granted dhimmis the freedom to practice their religion and spared them forced conversions. The dhimmis also served a variety of useful purposes, mostly economic, which was another point of concern to jurists. Religious minorities were free to do whatever they wished in their own homes, but could not "publicly engage in illicit sex in ways that threaten public morals". In some cases, religious practices that Muslims found repugnant were allowed. One example was the Zoroastrian practice of incestuous "self-marriage" where a man could marry his mother, sister or daughter. According to the famous Islamic legal scholar Ibn Qayyim Al-Jawziyya (1292–1350), non-Muslims had the right to engage in such religious practices even if it offended Muslims, under the conditions that such cases not be presented to Islamic Sharia courts and that these religious minorities believed that the practice in question is permissible according to their religion. This ruling was based on the precedent that Muhammad did not forbid such self-marriages among Zoroastrians despite coming in contact with them and having knowledge of their practices.
The Arabs generally established garrisons outside towns in the conquered territories, and had little interaction with the local dhimmi populations for purposes other than the collection of taxes. The conquered Christian, Jewish, Mazdean and Buddhist communities were otherwise left to lead their lives as before.
Christians.
Local Christians in Syria, Iraq, and Egypt were non-Chalcedonians and many may have felt better off under early Muslim rule than under that of the orthodox Greeks of Constantinople.
In 1095, Pope Urban II urged western European Christians to come to the aid of the Christians of Palestine. The subsequent Crusades brought Roman Catholic Christians into contact with Orthodox Christians whose beliefs they discovered to differ from their own perhaps more than they had realized, and whose position under the rule of the Muslim Fatimid Caliphate was less uncomfortable than had been supposed. Consequently, the Eastern Christians provided perhaps less support to the Crusaders than had been expected. When the Arab East came under Ottoman rule in the 16th century, Christian populations and fortunes rebounded significantly. The Ottomans had long experience dealing with Christian and Jewish minorities, and were more tolerant towards religious minorities than the former Muslim rulers, the Mamluks of Egypt. By the 19th century, European pressure on the Ottomans had forced them to remove all dhimma restrictions on their religious minorities.
Jews.
Accustomed to survival in adverse circumstances after many centuries of discrimination and persecution within the Roman Empire, both pre-Christian and Christian, Jews saw the Islamic conquests as just another change of rulers.
María Rosa Menocal, argues that the Jewish dhimmis living under the caliphate, while allowed fewer rights than Muslims, were still better off than in the Christian parts of Europe. Jews from other parts of Europe made their way to al-Andalus, where in parallel to Christian sects regarded as heretical by Catholic Europe, they were not just tolerated, but where opportunities to practice faith and trade were open without restriction save for the prohibitions on proselytization.
Bernard Lewis states:
Professor of Jewish medieval history at Hebrew University of Jerusalem, Hayim Hillel Ben-Sasson, notes:
According to the French historian Claude Cahen, Islam has "shown more toleration than Europe towards the Jews who remained in Muslim lands."
Comparing the treatment of Jews in the medieval Islamic world and medieval Christian Europe, Mark R. Cohen notes that, in contrast to Jews in Christian Europe, the "Jews in Islam were well integrated into the economic life of the larger society", and that they were allowed to practice their religion more freely than they could do in Christian Europe.
According to the scholar Mordechai Zaken, tribal chieftains (also known as aghas) in tribal Muslim societies such as the Kurdish society in Kurdistan would tax their Jewish subjects. The Jews were in fact civilians protected by their chieftains in and around their communities; in return they paid part of their harvest as dues, and contributed their skills and services to their patron chieftain.
Hindus and Buddhists.
By the 10th century, the Turks of Central Asia had brought Islam to the mountains north of the Indic plains. It was not long before they swept south across the Punjab. The Indus basin held a substantial Buddhist population in addition to the ruling Hindu castes, and most converted to Islam over the next two centuries. At the end of the 12th century, the Muslims advanced quickly into the Ganges Plain. In one decade, a Muslim army led by Turkic slaves consolidated resistance around Lahore and brought northern India, as far as Bengal, under Muslim rule. From these Turkic slaves would come sultans, including the founder of the sultanate of Delhi. Muslims and dhimmis alike participated in urbanization and urban prosperity.
By the 15th century, Islamic and Hindu civilization had evolved in a complementary manner, with the Muslims taking the role of a ruling caste in Hindu society. Nevertheless, the Muslims retained their Islamic identities, and were in some ways regarded by Hindus in much the same light as their own lowest castes.
In the 16th century, India came under the influence of the Mughals (Mongols). Babur, a ruler of the Mongol Timuri empire, established a foothold in the north which paved the way for further expansion by his successors. Until it was eclipsed by European hegemony in the 18th century, the Timuri Moghul emperors oversaw a period of coexistence and tolerance between Hindus and Muslims. The emperor Akbar has been described as a universalist. He sought to establish tolerance and equality between all communities and religions, and instituted far reaching social and religious reforms. Not all the Mughal emperors endorsed the ideals espoused by Akbar, indeed Aurangzeb was inclined towards a more fundamentalist approach.
Restrictions.
There were a number of restrictions on dhimmis. In a modern sense the dhimmis would be described as second-class citizens.
Although dhimmis were allowed to perform their religious rituals, they were obliged to do so in a manner not conspicuous to Muslims. Display of non-Muslim religious symbols, such as crosses or icons, was prohibited on buildings and on clothing (unless mandated as part of distinctive clothing). Loud prayers were forbidden, as were the ringing of church bells and the blowing of the shofar. They were also not allowed to build or repair churches without Muslim consent. Moreover, dhimmis were not allowed to seek converts among Muslims. In the Mamluk Egypt, where non-Mamluk Muslims were not allowed to ride horses and camels, dhimmis were prohibited even from riding donkeys inside cities. Sometimes, Muslim rulers issued regulations requiring dhimmis to attach distinctive signs to their houses.
Most of the restrictions were social and symbolic in nature, and a pattern of stricter, then more lax, enforcement developed over time. The major financial disabilities of the dhimmi were the jizya poll tax and the fact dhimmis and Muslims could not inherit from each other. That would create an incentive to convert if someone from the family had already converted. Ira M. Lapidus states that the "payment of the poll tax seems to have been regular, but other obligations were inconsistently enforced and did not prevent many non-Muslims from being important political, business, and scholarly figures. In the late ninth and early tenth centuries, Jewish bankers and financiers were important tat the 'Abbasid court." The jurists and scholars of Islamic sharia law called for humane treatment of the dhimmis.
Jizya tax.
Payment of the "jizya" obligated Muslim authorities to protect dhimmis in civil and military matters. Sura 9 (At-Tawba), verse 29 stipulates that "jizya" be exacted from non-Muslims as a condition required for jihad to cease. Failure to pay the "jizya" could result in the pledge of protection of a dhimmi's life and property becoming void, with the dhimmi facing the alternatives of conversion, enslavement, death or imprisonment, as advocated by Abu Yusuf, the chief qadi (Islamic judge) of Abbasid caliph Harun al-Rashid who ruled over much of modern-day Iraq.
Lewis notes there are varying opinions among scholars as to how much of a burden jizya was. According to Norman Stillman: ""jizya" and "kharaj" were a crushing burden for the non-Muslim peasantry who eked out a bare living in a subsistence economy." Both agree that ultimately, the additional taxation on non-Muslims was a critical factor that drove many dhimmis to leave their religion and accept Islam. However, in some regions the jizya on populations was significantly lower than the zakat, meaning dhimmi populations maintained an economic advantage. According to Cohen, taxation, from the perspective of dhimmis who came under Muslim rule, was "a concrete continuation of the taxes paid to earlier regimes". Lewis observes that the change from Byzantine to Arab rule was welcomed by many among the dhimmis who found the new yoke far lighter than the old, both in taxation and in other matters, and that some, even among the Christians of Syria and Egypt, preferred the rule of Islam to that of Byzantines. Montgomery Watt states, "the Christians were probably better off as dhimmis under Muslim-Arab rulers than they had been under the Byzantine Greeks." In some places, for example Egypt, the jizya was a tax incentive for Christians to convert to Islam.
The importance of dhimmis as a source of revenue for the Muslim community is illuminated in a letter ascribed to Umar I and cited by Abu Yusuf: "if we take dhimmis and share them out, what will be left for the Muslims who come after us? By God, Muslims would not find a man to talk to and profit from his labors."
Most Islamic scholars agree that jizya must be levied only upon adult males. In an important early account, Malik ibn Anas' "Muwatta Imam Malik" reports that the jizya was collected from non-Muslim men only, and additional taxes were to be levied against dhimmis who travelled on business:
The early Islamic scholars took a relatively humane and practical attitude towards the collection of "jizya", compared to the 11th century commentators writing when Islam was under threat both at home and abroad.
The jurist Abu Yusuf, the chief judge of the caliph Harun al-Rashid, rules as follows regarding the manner of collecting the jizya 
In the border provinces, dhimmis were sometimes recruited for military operations. In such cases, they were exempted from jizya for the year of service.
Administration of law.
Religious pluralism existed in medieval Islamic law and ethics. The religious laws and courts of other religions, including Christianity, Judaism and Hinduism, were usually accommodated within the Islamic legal framework, as exemplified in the Caliphate, Al-Andalus, Ottoman Empire and Indian subcontinent. In medieval Islamic societies, the qadi (Islamic judge) usually could not interfere in the matters of non-Muslims unless the parties voluntarily chose to be judged according to Islamic law. The dhimmi communities living in Islamic states usually had their own laws independent from the Sharia law, such as the Jews who had their own Halakha courts.
Dhimmis were allowed to operate their own courts following their own legal systems. However, dhimmis frequently attended the Muslim courts in order to record property and business transactions within their own communities. Cases were taken out against Muslims, against other dhimmis and even against members of the dhimmi's own family. Dhimmis often took cases relating to marriage, divorce or inheritance to the Muslim courts so these cases would be decided under sharia law. Oaths sworn by dhimmis in the Muslim courts were sometimes the same as the oaths taken by Muslims, sometimes tailored to the dhimmis' beliefs.
Muslim men could generally marry dhimmi women who are considered People of the Book, however Islamic jurists rejected the possibility any non-Muslim man might marry a Muslim woman. Bernard Lewis notes that "similar position existed under the laws of Byzantine Empire, according to which a Christian could marry a Jewish woman, but a Jew could not marry a Christian woman under pain of death".
Relevant texts.
Quranic verses as a basis for Islamic policies toward dhimmis.
Lewis states
Hadith.
A hadith by Muhammad, "Whoever killed a "Mu'ahid" (a person who is granted the pledge of protection by the Muslims) shall not smell the fragrance of Paradise though its fragrance can be smelt at a distance of forty years (of traveling).", is considered to be a foundation for the protection of the People of the Book in Muslim ruled countries. Anwar Shah Kashmiri writes in his commentary on Sahih al-Bukhari "Fayd al-Bari" on this hadith: "You know the gravity of sin for killing a Muslim, for its odiousness has reached the point of disbelief, and it necessitates that killer abides in Hell forever. As for killing a non-Muslim citizen [mu'ahid], it is similarly no small matter, for the one who does it will not smell the fragrance of Paradise.
Majid Khadduri cites a similar hadith in regard to the status of the dhimmis: "Whoever wrongs one with whom a compact (treaty) has been made "a dhimmi" and lays on him a burden beyond his strength, I will be his accuser."
Constitution of Medina.
A precedent for the dhimma contract was established with the agreement between Muhammad and the Jews after the Battle of Khaybar, an oasis near Medina. Khaybar was the first territory attacked and conquered by Muslims. When the Jews of Khaybar surrendered to Muhammad after a siege, Muhammad allowed them to remain in Khaybar in return for handing over to the Muslims one half their annual produce.
After Mecca was brought under Islamic rule, deputations from tribes across Arabia came to make terms with Muhammad and the Muslims. The Constitution of Medina, a formal agreement between Muhammad and all the significant tribes and families of Medina (including Muslims, Jews and pagans), declared that non-Muslims in the Ummah had the following rights:
Pact of Umar.
The Pact of Umar, traditionally believed to be between caliph Umar and the conquered Jerusalem Christians in the seventh century, was another source of regulations pertaining to dhimmis. However, Western orientalists doubt the authenticity of the pact, arguing it is usually the victors and not the vanquished who impose rather than propose, the terms of peace, and that it is highly unlikely that the people who spoke no Arabic and knew nothing of Islam could draft such a document. Academic historians believe the Pact of Umar in the form it is known today was a product of later jurists who attributed it to Umar in order to lend greater authority to their own opinions. The similarities between the Pact of Umar and the Theodosian and Justinian Codes of the Eastern Roman Empire suggest that perhaps much of the Pact of Umar was borrowed from these earlier codes by later Islamic jurists. At least some of the clauses of the pact mirror the measures first introduced by the Umayyad caliph Umar II or by the early Abbasid caliphs.
Cultural interactions and cultural differences.
During the Middle Ages, local associations known as "futuwwa" clubs developed across the Islamic lands. There were usually several futuwwah in each town. These clubs catered to varying interests, primarily sports, and might involve distinctive manners of dress and custom. They were known for their hospitality, idealism and loyalty to the group. They often had a militaristic aspect, purportedly for the mutual protection of the membership. These clubs commonly crossed social strata, including among their membership local notables, dhimmi and slaves – to the exclusion of those associated with the local ruler, or amir.
Muslims and Jews were sometimes partners in trade, with the Muslim taking days off on Fridays and Jews taking off on Saturdays.
Andrew Wheatcroft describes how some social customs such as different conceptions of dirt and cleanliness made it difficult for the religious communities to live close to each other, either under Muslim or under Christian rule.
In modern times.
The dhimma and the jizya poll tax are no longer imposed in Muslim majority countries. In the 21st century, jizya is widely regarded as being at odds with contemporary secular conceptions of citizen's civil rights and equality before the law, although there have been occasional reports of religious minorities in conflict zones and areas subject to political instability being forced to pay jizya.
In 2009 it was claimed that a group of militants that referred to themselves as the Taliban imposed the "jizya" on Pakistan's minority Sikh community after occupying some of their homes and kidnapping a Sikh leader.
As late as 2013, in Egypt "jizya" was reportedly being imposed by the Muslim Brotherhood on 15,000 Christian Copts of Dalga village.
In February 2014, the Islamic State of Iraq and the Levant (ISIL) announced that it intended to extract jizya from Christians in the city of Raqqa, Syria, which it controls. Christians who refused to accept the dhimma contract and pay the tax would have to either convert to Islam or die. Wealthy Christians would have to pay half an ounce of gold, the equivalent of USD 664 twice a year; middle-class Christians would have to pay half that amount and poorer ones would be charged one-fourth that amount. In June, the Institute for the Study of War reported that ISIL claims to have collected jizya and fay. On July 18, 2014 the ISIL ordered the Christians in Mosul to accept the dhimma contract and pay the Jizya or convert to Islam. If they refused to accept either of the options they would be killed.

</doc>
<doc id="9091" url="https://en.wikipedia.org/wiki?curid=9091" title="Doctor V64">
Doctor V64

The Doctor V64 (also referred to simply as the V64) is a development and backup device made by Bung Enterprises Ltd that is used in conjunction with the Nintendo 64. The Doctor V64 also had the ability to play Video CDs, audio CDs and had an option for applying stereo 3D effects to the audio.
History.
The Doctor V64 came out in 1996 and was priced around $450 USD. Many third party developers used the V64 instead of the PC64 Development Kit sold by Nintendo; the V64 was considered an attractive, low cost alternative to the expensive N64 development machine, which was manufactured by Silicon Graphics at the time. The CPU of the V64 is a 6502 chip (the CPU from the Nintendo Entertainment System); the operating system is stored in the BIOS chip. It is likely that Bung reused most of the design of their earlier NES clones in the Doctor V64.
The Doctor V64 unit contains a CD-ROM drive which sits underneath the Nintendo 64 and plugs into the expansion slot on the underside of the Nintendo 64. The expansion slot is essentially a mirror image of the cartridge slot on the top of the unit, with the same electrical connections, thus the Nintendo 64 reads data from the Doctor V64 in the same manner as it would from a cartridge plugged into the normal slot.
Using the Doctor V64 involved the solution of two problems: how to boot a game and how to save. In order to get around Nintendo's lockout chip, when using the Doctor V64 a game cartridge is plugged into the Nintendo 64 through an adaptor which connects only the lockout chip. The game cart used for operation had to contain the same lockout chip used by the game backup. The second problem concerned saving progress. Most N64 games saved to the cart itself instead of external memory cards. If the user wanted to keep his progress then the cartridge used had to have the same type of non-volatile memory hardware.
Following the Doctor V64's success, Bung released the Doctor V64 Jr. in December 1998. This was a cost-efficient condensed version of the original V64. The V64jr had no CD drive and plugged into the normal cartridge slot on the top of the Nintendo 64. Data was loaded into the V64jr's battery-backed RAM from a PC via a parallel port connection. The V64Jr had up to 512 megabits (64 MB) of memory storage. At the time this was done to provide for future Nintendo 64 carts that employed larger memory storage. The prohibitive high costs associated with ordering large storage carts kept this occurrence at a minimum. Only a handful of 512 megabit games were released for the Nintendo 64 system.
During the N64's lifetime, Nintendo made one model revision which made the serial port area smaller. This slight change in the N64's plastic casing made the connection to the Doctor V64 difficult to achieve without user modification. This revision may have been a direct reaction of Nintendo to discourage the use of V64 devices. It also explains why Bung decided to drop the use of this port in the later V64Jr models.
The Doctor V64 could be used to read the data from a game cartridge and transfer the data to a PC via the parallel port. This allowed developers and homebrew programmers to upload their game images to the Doctor V64 without having to create a CD backup each time. It also allowed users to upload game images taken from the Internet.
Promotions.
In 1998 and 1999, there was a homebrew competition known as "Presence of Mind" (POM), an N64 demo competition led by dextrose.com. The contest consisted in submitting a user developed N64 program, game or utility. Bung Enterprises promoted the event and supplied prizes (usually Doctor V64 related accessories). Though a contest was planned for 2000 the interest in the N64 was already fading and so did the event. POM contest demo entries can still be found on the Internet.
Legal issues.
The Doctor V64 unit was the first commercially available backup device for the Nintendo 64 unit. Though the unit was sold as a development machine it could be modified to enable the creation and use of commercial game backups. Unlike official development units, the purchase of V64s was not restricted to software companies only. For this reason the unit became a popular choice among those looking to proliferate unlicensed copies of games.
Original Doctor V64 units sold by Bung did not allow the playing of backups. A person would have to modify the unit by themselves in order to make it backup friendly. This usually required a user to download and install a modified Doctor V64 BIOS. Additionally the cartridge adapter had to be opened and soldered in order to allow the operational procedure described early on this article. Though Bung never sold backup enabled V64s many re-sellers would modify the units themselves.
Nintendo made many legal efforts world wide in order to stop the sale of Doctor V64 units. They sued Bung directly as well as specific store retailers in Europe and North America for copyright infringement. Eventually Nintendo managed to have the courts prohibit the sale of Doctor V64 units in the United States.
Main menu.
As with many backup devices of its time, the Doctor V64 implemented text based menu driven screens. The menus were spartan and purely functional in nature. Utilizing the buttons on the V64 unit a user would navigate the menus and issue commands. It was mainly designed for game developers even though it is possible to back up cartridges with it (through the use of an unofficial V64 bios). Some of the menu items related to game backups were removed from the V64's BIOS near the end of its life due to pressure from Nintendo. These items are only available by obtaining a patched V64 BIOS.

</doc>
<doc id="9093" url="https://en.wikipedia.org/wiki?curid=9093" title="De Havilland Mosquito">
De Havilland Mosquito

The de Havilland DH.98 Mosquito is a British multi-role combat aircraft with a two-man crew which served during and after the Second World War. It was one of few operational front-line aircraft of the era constructed almost entirely of wood and was nicknamed "The Wooden Wonder". The Mosquito was also known affectionately as the "Mossie" to its crews. Originally conceived as an unarmed fast bomber, the Mosquito was adapted to roles including low to medium-altitude daytime tactical bomber, high-altitude night bomber, pathfinder, day or night fighter, fighter-bomber, intruder, maritime strike aircraft, and fast photo-reconnaissance aircraft. It was also used by the British Overseas Airways Corporation (BOAC) as a fast transport to carry small high-value cargoes to, and from, neutral countries, through enemy-controlled airspace. A single passenger could be carried in the aircraft's bomb bay, which was adapted for the purpose.
When production of the Mosquito began in 1941, it was one of the fastest operational aircraft in the world. Entering widespread service in 1942, the Mosquito was a high-speed, high-altitude photo-reconnaissance aircraft, continuing in this role throughout the war. From mid-1942 to mid-1943, Mosquito bombers flew high-speed, medium or low-altitude missions against factories, railways and other pinpoint targets in Germany and German-occupied Europe. From late 1943, Mosquito bombers were formed into the Light Night Strike Force and used as pathfinders for RAF Bomber Command's heavy-bomber raids. They were also used as "nuisance" bombers, often dropping Blockbuster bombs – 4,000 lb (1,812 kg) "cookies" – in high-altitude, high-speed raids that German night fighters were almost powerless to intercept.
As a night fighter from mid-1942, the Mosquito intercepted "Luftwaffe" raids on the United Kingdom, notably defeating "Operation Steinbock" in 1944. Starting in July 1942, Mosquito night-fighter units raided "Luftwaffe" airfields. As part of 100 Group, it was a night fighter and intruder supporting RAF Bomber Command's heavy bombers that reduced bomber losses during 1944 and 1945. As a fighter-bomber in the Second Tactical Air Force, the Mosquito took part in "special raids", such as the attack on Amiens Prison in early 1944, and in precision attacks against Gestapo or German intelligence and security forces. Second Tactical Air Force Mosquito supported the British Army during the 1944 Normandy Campaign. From 1943, Mosquitos with RAF Coastal Command strike squadrons attacked "Kriegsmarine" U-boats (particularly in 1943 in the Bay of Biscay, where significant numbers were sunk or damaged) and intercepting transport ship concentrations.
The Mosquito flew with the Royal Air Force (RAF) and other air forces in the European, Mediterranean and Italian theatres. The Mosquito was also operated by the RAF in the South East Asian theatre, and by the Royal Australian Air Force (RAAF) based in the Halmaheras and Borneo during the Pacific War. During the 1950s, the RAF ultimately replaced the Mosquito with the jet-powered English Electric Canberra.
Development.
By the early-mid-1930s, de Havilland had a reputation for innovative high-speed aircraft with the DH.88 Comet racer. The later DH.91 Albatross airliner pioneered the composite wood construction that the Mosquito used. The 22-passenger Albatross could cruise at at , better than the Handley Page H.P.42 and other biplanes it was replacing. The wooden monocoque construction not only saved weight and compensated for the low power of the de Havilland Gipsy Twelve engines used by this aircraft, but simplified production and reduced construction time.
Air Ministry bomber requirements and concepts.
On 8 September 1936, the British Air Ministry issued Specification P.13/36 which called for a twin-engine medium bomber capable of carrying a bomb load of for with a maximum speed of at ; a maximum bomb load of which could be carried over shorter ranges was also specified. Aviation firms entered heavy designs with new high-powered engines and multiple defensive turrets, leading to the production of the Avro Manchester and Handley Page Halifax.
In May 1937, as a comparison to P.13/36, George Volkert, the chief designer of Handley Page, put forward the concept of a fast unarmed bomber. In 20 pages, Volkert planned an aerodynamically clean medium bomber to carry of bombs at a cruising speed of . There was support in the RAF and Air Ministry; Captain R N Liptrot, Research Director Aircraft 3 (RDA3), appraised Volkert's design, calculating that its top speed would exceed the new Supermarine Spitfire. There were, however, counter-arguments that, although such a design had merit, it would not necessarily be faster than enemy fighters for long. The ministry was also considering using non-strategic materials for aircraft production, which, in 1938, had led to specification B.9/38 and the Armstrong Whitworth Albemarle medium bomber, largely constructed from spruce and plywood attached to a steel-tube frame. The idea of a small, fast bomber gained support at a much earlier stage than sometimes acknowledged though it was likely that the Air Ministry envisaged it using light alloy components.
Inception of the de Havilland fast bomber.
Geoffrey de Havilland also believed a bomber with an aerodynamic design, with minimal skin area, was better than the P.13/36 specification. He thought that adapting the Albatross to meet the RAF's requirements could save time. In April 1938, performance estimates were produced for a twin Rolls-Royce Merlin-powered DH.91, with the Bristol Hercules (radial engine) and Napier Sabre (H-engine) as alternatives. On 7 July 1938, Geoffrey de Havilland wrote to Air Marshal Wilfrid Freeman, the Air Council's member for Research and Development, discussing the specification and arguing that in war there would be shortages of duralumin and steel but should be plenty of wood. Although inferior torsionally, the strength to weight ratio of wood was as good as duralumin or steel, and a different approach to a high-speed bomber was possible.
A follow-up letter to Freeman on 27 July said that the P.13/36 specification could not be met by a twin Merlin-powered aircraft and either the top speed or load capacity would be compromised, depending on which was paramount. For example, a larger, slower, turret armed aircraft would have a range of carrying a bomb load, with a maximum of at , and a cruising speed of at . De Havilland believed that much of a compromise, and that getting rid of surplus equipment would lead to a better design. On 4 October 1938, de Havilland projected the performance of another design based on the Albatross, powered by two Merlin Xs, with a three-man crew and six or eight forward-firing guns, plus one or two manually operated guns and a tail turret. Based on a total loaded weight of it would have a top speed of and cruising speed of at .
Still believing this could be improved, and after examining more concepts based on the Albatross and the new all-metal DH.95 Flamingo, de Havilland settled on designing a new aircraft that would be aerodynamically clean, wooden and powered by the Merlin, which offered substantial future development. The new design would be faster than foreseeable enemy fighter aircraft, and could dispense with a defensive armament, which would slow it and make interception or losses to anti-aircraft guns more likely. Instead, high speed and good manoeuvrability would make it easier to evade fighters and ground fire. The lack of turrets simplified production and reduced production time, with a delivery rate far in advance of competing designs. Without armament, the crew could be reduced to a pilot and navigator. Contemporary RAF design philosophy required well-armed heavy bombers more akin to the German "schnellbomber". At a meeting in early October 1938 with Geoffrey de Havilland and Charles Walker (de Havilland's chief engineer), the Air Ministry showed little interest, and instead asked de Havilland to build wings for other bombers as a sub-contractor.
By September 1939 de Havilland had produced preliminary estimates for single- and twin-engined variations of light-bomber designs using different engines, speculating on the effects of defensive armament on their designs. One design, completed on 6 September, was for an aircraft powered by a single Napier Sabre, with a wingspan of and capable of carrying a bomb load . On 20 September, in another letter to Wilfred Freeman, de Havilland wrote "...we believe that we could produce a twin-engine bomber which would have a performance so outstanding that little defensive equipment would be needed." By 4 October work had progressed to a twin-engine light bomber with a wingspan of , and powered by Merlin or Griffon engines, the Merlin favoured due to availability. On 7 October 1939, a month into the war, the nucleus of a design team under Eric Bishop moved to the security and secrecy of Salisbury Hall to work on what was later known as the DH.98. For more versatility, Bishop made provision for four 20 mm cannon in the forward half of the bomb bay, under the cockpit, firing via blast tubes and troughs under the fuselage.
The DH.98 was too radical for the Ministry, which wanted a heavily armed, multi-role aircraft, combining medium bomber, reconnaissance and general purpose roles, as well as capable of carrying torpedoes. With outbreak of war, the Ministry became more receptive, but still sceptical about an unarmed bomber. It thought the Germans would produce fighters faster than expected. It suggested two forward and two rear-firing machine guns for defence. The Ministry also opposed a two-man bomber, wanting at least a third crewman to reduce the work of the others on long flights. The Air Council added further requirements, such as remotely controlled guns, a top speed of at on two-thirds engine power, and a range of with a bomb load. To appease the Ministry, de Havilland built mock-ups with a gun turret just aft of the cockpit but, apart from this compromise, de Havilland made no changes.
On 12 November, at a meeting considering fast bomber ideas put forward by de Havilland, Blackburn, and Bristol, Air Marshal Freeman directed de Havilland to produce a fast aircraft, powered initially by Merlin engines, with options of using progressively more powerful engines, including the Rolls-Royce Griffon and the Napier Sabre. Although estimates were presented for a slightly larger Griffon-powered aircraft, armed with a four-gun tail turret, Freeman got the requirement for defensive weapons dropped, and a draft requirement was raised calling for a high-speed light reconnaissance bomber capable of at .
On 12 December, the Vice-Chief of the Air Staff, Director General of Research and Development, Air Officer Commanding-in-Chief (AOC-in-C) of RAF Bomber Command met to finalize the design and decide how to fit it in the RAF's aims. The AOC-in-C would not accept an unarmed bomber, but insisted on its suitability for reconnaissance missions with F8 or F24 cameras. After company representatives, the Ministry and the RAF's operational commands examined a full-scale mock-up at Hatfield on 29 December 1939, the project received backing. This was confirmed on 1 January 1940, when Air Marshal Freeman chaired a meeting with Geoffrey de Havilland, John Buchanan, (Deputy of Aircraft Production) and John Connolly (Buchanan's chief of staff). de Havilland claimed the DH.98 was the "fastest bomber in the world...it must be useful". Freeman supported it for RAF service, ordering a single prototype for an unarmed bomber to specification B.1/40/dh, which called for a light bomber/reconnaissance aircraft powered by two Rolls-Royce RM3SM (an early designation for the Merlin 21) with ducted radiators, capable of carrying a bomb load. The aircraft was to have a speed of at and a cruising speed of at with a range of at on full tanks. Maximum service ceiling was to be .
On 1 March 1940, Air Marshal Roderic Hill issued a contract under Specification B.1/40, for 50 bomber-reconnaissance variants of the DH.98: this contract included the prototype, which was given the factory serial "E0234". In May 1940, specification F.21/40 was issued, calling for a long-range fighter armed with four 20 mm cannon and four .303 machine guns in the nose, after which de Havilland were authorized to build a prototype of a fighter version of the DH.98. It was decided after debate that this prototype, given the military serial number "W4052", would carry Airborne Interception (AI) Mk.IV equipment as a day and night fighter. By June 1940, the DH.98 had been named "Mosquito". Having the fighter variant kept the Mosquito project alive as there was still criticism within the government and Air Ministry of the usefulness of an unarmed bomber, even after the prototype had shown its capabilities.
Project Mosquito.
Once design of the DH.98 had started, de Havilland built mock-ups, the most detailed at Salisbury Hall, in the hangar where "E0234" was being built. Initially, this was designed with the crew enclosed in the fuselage behind a transparent nose (similar to the Bristol Blenheim or Heinkel He 111H), but this was quickly altered to a more solid nose with a more conventional canopy.
The construction of the prototype began in March 1940, but work was cancelled again after the Battle of Dunkirk, when Lord Beaverbrook, as Minister of Aircraft Production, decided there was no production capacity for aircraft like the DH.98, which was not expected to be in service until early 1942.
Although Lord Beaverbrook told Air Vice-Marshal Freeman that work on the project should stop, he did not issue a specific instruction, and Freeman ignored the request. In June 1940, however, Lord Beaverbrook and the Air Staff ordered that production focus on five existing types, namely the Supermarine Spitfire and Hawker Hurricane fighter and the Vickers Wellington, Armstrong-Whitworth Whitley and Bristol Blenheim bombers. Work on the DH.98 prototype stopped; it seemed that the project would be shut down when the design team were denied the materials with which to build their prototype.
The Mosquito was only reinstated as a priority in July 1940, after de Havilland's General Manager L.C.L Murray, promised Lord Beaverbrook 50 Mosquitoes by December 1941, and this, only after Beaverbrook was satisfied that Mosquito production would not hinder de Havilland's primary work of producing Tiger Moth and Airspeed Oxford trainers and repairing Hurricanes as well as the licence manufacture of Merlin engines. In promising Beaverbrook 50 Mosquitoes by the end of 1941, de Havilland was taking a gamble, because it was unlikely that 50 Mosquitoes could be built in such a limited time; as it transpired only 20 Mosquitoes were built in 1941, but the other 30 were delivered by mid-March 1942. During the Battle of Britain, interruptions to production due to air raid warnings caused nearly a third of de Havilland's factory time to be lost. Nevertheless, work on the prototype went quickly, such that "E0234" was rolled out on 19 November 1940.
In the aftermath of the Battle of Britain, the original order was changed to 20 bomber variants and 30 fighters. It was still uncertain whether the fighter version should have dual or single controls, or should carry a turret, so three prototypes were eventually built: "W4052", "W4053" and "W4073". The latter, both turret armed, were later disarmed, to become the prototypes for the T.III trainer. This caused some delays as half-built wing components had to be strengthened for the expected higher combat load requirements. The nose sections also had to be altered, omitting the clear perspex bomb-aimer's position, to solid noses designed to house four .303 machine guns and their ammunition.
Prototypes and test flights.
On 3 November 1940, the aircraft, still coded "E0234", was transported by road to Hatfield, and placed in a small blast-proof assembly building, where successful engine runs were made on 19 November. Two Merlin 21 two-speed single-stage supercharged engines were installed driving three-bladed de Havilland Hydromatic constant-speed, controllable-pitch propellers.
"E0234" had a wingspan of and was the only Mosquito to be built with Handley Page slats on the outer leading edges of the wings. Test flights showed that these were not needed and they were disconnected and faired over with doped fabric (these slats can still be seen on "W4050"). On 24 November 1940, taxiing trials were carried out by Geoffrey de Havilland, Jr., who was the company's chief test pilot and responsible for maiden flights. The tests were successful and the prototype was subsequently readied for flight testing, making its first flight piloted by Geoffrey de Havilland, on 25 November. The flight was made 11 months after the start of detailed design work, a remarkable achievement considering the conditions of the time.
For this maiden flight, "E0234", weighing , took off from a field beside the shed it was built in. John E. Walker, Chief Engine Installation designer, accompanied de Havilland. The takeoff was "straight forward and easy" and the undercarriage was not retracted until a considerable height was attained. The aircraft reached , with the only problem being the undercarriage doors – which were operated by bungee cords attached to the main undercarriage legs – that remained open by some at that speed. This problem persisted for some time. The left wing of "E0234" also had a tendency to drag to port slightly, so a rigging adjustment, i.e., a slight change in the angle of the wing, was carried out before further flights.
On 5 December 1940, the prototype experienced tail buffeting at speeds between and . The pilot noticed this most in the control column, with handling becoming more difficult. During testing on 10 December, wool tufts were attached to suspect areas to investigate the direction of airflow. The conclusion was that the airflow separating from rear section of the inner engine nacelles was disturbed, leading to a localised stall and the disturbed airflow was striking the tailplane, causing buffeting. In an attempt to smooth the air flow and deflect it from forcefully striking the tailplane, non-retractable slots fitted to the inner engine nacelles and to the leading edge of the tailplane were experimented with. These slots, and wing root fairings fitted to the forward fuselage and leading edge of the radiator intakes, stopped some of the vibration experienced but did not cure the tailplane buffeting.
In February 1941, buffeting was eliminated by incorporating triangular fillets on the trailing edge of the wings and lengthening the nacelles, the trailing edge of which curved up to fair into the fillet some behind the wing's trailing edge: this meant the flaps had to be divided into inboard and outboard sections. January 1941, the prototype was carrying the military serial number "W4050". With the buffeting problems largely resolved, John Cunningham flew "W4050" on 9 February 1941. He was greatly impressed by the "lightness of the controls and generally pleasant handling characteristics". Cunningham concluded that when the type was fitted with AI equipment, it would be a perfect replacement for the Bristol Beaufighter.
During its trials on 16 January 1941, "W4050" outpaced a Spitfire at . The original estimates were that as the Mosquito prototype had twice the surface area and over twice the weight of the Spitfire Mk II, but also with twice its power, the Mosquito would end up being faster. Over the next few months, "W4050" surpassed this estimate, easily beating the Spitfire Mk II in testing at RAF Boscombe Down in February 1941, reaching a top speed of at altitude, compared to a top speed of at for the Spitfire.
On 19 February, official trials began at the Aeroplane and Armament Experimental Establishment (A&AEE) based at Boscombe Down, although de Havilland's representative was surprised by a delay in starting the tests. On 24 February, as "W4050" taxied across the rough airfield, the tailwheel jammed leading to the fuselage fracturing: this was replaced by the fuselage of the Photo-Reconnaissance prototype "W4051" in early March. In spite of this setback, the "Initial Handling Report 767" issued by the A&AEE stated "The aeroplane is pleasant to fly ... Aileron control light and effective ..." The maximum speed reached was at , with an estimated maximum ceiling of and a maximum rate of climb of at .
"W4050" continued to be used for various test programs, essentially as the experimental "workhorse" for the Mosquito family. In late October 1941, it returned to the factory to be fitted with Merlin 61s, the first production Merlins fitted with a two-speed, two-stage supercharger. The first flight with the new engines was on 20 June 1942. "W4050" recorded a maximum speed of at (fitted with straight-through air intakes with snowguards, engines in F.S. gear) and at without snowguards. In October 1942, in connection with development work on the NF Mk XV, "W4050" was fitted with extended wingtips increasing the span to , first flying in this configuration on 8 December. Finally, fitted with high-altitude rated two-stage, two-speed Merlin 77s, it reached in December 1943.
Soon after these flights,"W4050" was grounded and was scheduled to be scrapped, but instead served as an instructional airframe at Hatfield. In September 1958, "W4050" was returned to the Salisbury Hall hangar in which it had been built, restored to its original configuration, and became one of the primary exhibits of the de Havilland Aircraft Heritage Centre.
"W4051", which was designed from the outset to be the prototype for the photo-reconnaissance versions of the Mosquito, was slated to make its first flight in early 1941. However, the fuselage fracture in "W4050" meant that "W4051's" fuselage was used as a replacement; "W4051" was then rebuilt using a production standard fuselage and first flew on 10 June 1941. This prototype continued to use the short engine nacelles, single-piece trailing edge flaps and the "No. 1" tailplane used by "W4050", but had production standard wings and, thus configured, became the only Mosquito prototype to fly operationally.
Construction of the fighter prototype, "W4052" was also carried out at the secret Salisbury Hall facility. This differed from the bomber brethren in a number of ways: it was powered by Merlin 21s; had an altered canopy structure with a flat bullet-proof windscreen; a solid nose mounted four .303 in (7.7 mm) Browning machine guns and their ammunition boxes, accessed by a single large, sideways hinged panel.; four 20 mm (.79 in) Hispano Mk II cannon were housed in a compartment under the cockpit floor, with the breeches projecting into the bomb bay; and the automatic bomb bay doors were replaced by manually operated bay doors which incorporated cartridge ejector chutes.
In accordance with its role as a day/night fighter prototype "W4052" was equipped with AI Mk.IV equipment, complete with an "arrowhead"-shaped transmission aerial mounted between the central Brownings, and receiving aerials through each outer wingtip, and it was painted in overall black RDM2a "Special Night" finish. It was also the first prototype constructed with the extended engine nacelles. "W4052" was later tested with other modifications including bomb racks, drop tanks, barrage balloon cable cutters in the leading edge of the wings, Hamilton airscrews and braking propellers, as well as drooping aileron systems which enabled steep approaches, and a larger rudder tab.
The prototype continued to serve as a test machine until it was scrapped on 28 January 1946.
"4055" flew the first operational Mosquito flight on 17 September 1941.
During flight testing, the Mosquito prototypes were modified to test a number of experimental configurations. "W4050" was fitted with a turret behind the cockpit for drag tests, after which the idea was abandoned in July 1941. "W4052" had the first version of the Youngman Frill airbrake fitted to the fighter prototype. The frill was opened by bellows and venturi effect to provide rapid deceleration during interceptions and was tested between January – August 1942, but was also abandoned when it was discovered that lowering the undercarriage had the same effect with less buffeting.
Production plans and American interest.
The Air Ministry had authorised mass production plans to be drawn up on 21 June 1941, by which time the Mosquito had become one of the world's fastest operational aircraft. The Air Ministry ordered 19 photo-reconnaissance (PR) models and 176 fighters. A further 50 were unspecified; in July 1941, the Air Ministry confirmed these would be unarmed fast bombers. By the end of January 1942, contracts had been awarded for 1,378 Mosquitos of all variants, including 20 T.III trainers and 334 FB.VI bombers. Another 400 were to be built by de Havilland Canada.
On 20 April 1941, "W4050" was demonstrated to Lord Beaverbrook, the Minister of Aircraft Production. The Mosquito made a series of flights, including one rolling climb on one engine. Also present were US General Henry H. Arnold and his aide Major Elwood Quesada, who wrote "I ... recall the first time I saw the Mosquito as being impressed by its performance, which we were aware of. We were impressed by the appearance of the airplane that looks fast usually is fast, and the Mosquito was, by the standards of the time, an extremely well streamlined airplane, and it was highly regarded, highly respected."
The trials set up future production plans between Britain, Australia and Canada. Six days later Arnold returned to America with a full set of manufacturer's drawings. As a result of his report five companies (Beech; Curtiss-Wright; Fairchild; Fleetwings; and Hughes) were asked to evaluate the de Havilland data. The report by Beech Aircraft summed up the general view: "It appears as though this airplane has sacrificed serviceability, structural strength ease of construction and flying characteristics in an attempt to use construction material which is not suitable for the manufacture of efficient airplanes".
The Americans did not pursue their interest, the consensus view view being the Lockheed P-38 Lightning could handle the same duties. However Arnold felt the design was being overlooked, and urged the strategic personalities in the United States Army Air Forces to learn from the design even if they chose not to adopt it.
Several days after the attack on Pearl Harbor, the USAAF then requested one airframe to evaluate on 12 December 1941, signifying that the USAAF recognised they had entered the war without a fast dual-purpose reconnaissance aircraft.
Design.
Overview.
The Mosquito was a fast, twin-engined aircraft with shoulder-mounted wings. The most-produced variant, designated the FB Mk VI (Fighter-bomber Mark 6), was powered by two Merlin Mk 23 or Mk 25 engines driving three-bladed de Havilland hydromatic propellers. The typical fixed armament for an FB Mk VI was four Browning .303 machine guns and four 20 mm Hispano cannon while the offensive load consisted of up to of bombs, or eight RP-3 unguided rockets.
The design was noted for having light and effective control surfaces which allowed for good manoeuvrability, but that the rudder should not be used aggressively at high speeds; poor aileron control at low speeds when landing and taking off was also a problem for inexperienced crews. For flying at low speeds, the flaps had to be set at 15°, speed reduced to and rpm set to 2,650. The speed could be reduced to an acceptable for low speed flying. For cruising the maximum speed for obtaining maximum range was at weight.
The Mosquito had a low stalling speed of with undercarriage and flaps raised. When both were lowered, the stalling speed decreased to . Stall speed at normal approach angle and conditions was . Warning of the stall was given by buffeting and would occur before stall was reached. The conditions and impact of the stall were not severe. The wing did not drop unless the control column was pulled back. The nose drooped gently and recovery was easy.
Early on in the Mosquito's operational life, the cooling intake shrouds that were to cool the exhausts on production aircraft overheated after a while. Flame dampers prevented exhaust glow on night operations, but they had an effect on performance. Multiple ejector and open-ended exhaust stubs helped solve the problem and were used in the PR.VIII, B.IX and B.XVI variants. This increased speed performance in the B.IX alone by per hour.
Fuselage.
The oval-section fuselage was a frameless monocoque shell built in two halves being formed to shape by band clamps over a mahogany or concrete mould, each holding one half of the fuselage, split vertically. The shell halves were made of sheets of Ecuadorean balsawood sandwiched between sheets of Canadian birch, but in areas needing extra strength— such as along cut-outs— stronger woods replaced the balsa filler; the overall thickness of the birch and balsa sandwich skin was only . This sandwich skin was so stiff that no internal reinforcement was necessary from the wing's rear spar to the tail bearing bulkhead.
The join along the vertical centre line greatly aided construction as it allowed the technicians easy access to the fuselage interior. While the glue in the plywood skin dried, carpenters cut a sawtooth joint into the edges of the fuselage shells, other workers installed the controls and cabling on the inside wall.
When the glue completely dried, the two halves were glued and screwed together. The fuselage was strengthened internally by seven bulkheads made up of two plywood skins separated by spruce blocks, which formed the basis on each half for the outer shell. Each bulkhead was a repeat of the spruce design for the fuselage halves; a balsa sheet sandwich between two plywood sheets/skins. Bulkhead number seven carried the fittings and loads for the tailplane and rudder.
The original glue was Casein-based, later replaced by "Aerolite", a synthetic urea-formaldehyde, which was more durable. Many other types of screws and flanges (made of various woods) also held the structure together. The fuselage construction joints were made from balsa wood and plywood strips with the spruce multi-ply being connected by a balsa V joint, along with the interior frame. The spruce would be reinforced by plywood strips at the point where the two halves joined to form the V-joint. Located on top of the joint the plywood formed the outer skin.
During the joining of the two halves ("boxing up"), two laminated wooden clamps would be used in the after portion of the fuselage to act as support. A covering of doped Madapolam (a fine plain woven cotton) fabric was stretched tightly over the shell and a coat of silver dope was applied, after which the exterior camouflage was applied. The fuselage had a large ventral section cut-out (braced during construction) that allowed the fuselage to be lowered onto the wing centre-section. After the wing was secured lower panels were replaced and the bomb bay or armament doors fitted.
Wing.
The all-wood wing was built as a one-piece structure and was not divided into separate construction sections. It was made up of two main spars, spruce and plywood compression ribs, stringers, and a plywood covering. The outer plywood skin was covered and doped like the fuselage. The wing was installed into the roots by means of four large attachment points.
The engine radiators were fitted in the inner wing, just outboard of the fuselage on either side. These gave less drag. The radiators themselves were split into three sections: an oil cooler section outboard, the middle section forming the coolant radiator and the inboard section serving the cabin heater.
The wing contained metal framed and skinned ailerons, but the flaps were made of wood and were hydraulically controlled. The nacelles were mostly wood, although, for strength, the engine mounts were all metal as were the undercarriage parts.
Engine mounts of welded steel tube were added, along with simple landing gear oleos filled with rubber blocks. Wood was used to carry only in-plane loads, with metal fittings used for all triaxially loaded components such as landing gear, engine mounts, control surface mounting brackets, and the wing-to-fuselage junction. The outer leading wing edge had to be brought further forward to accommodate this design. The main tail unit was all wood built. The control surfaces, the rudder and elevator, were aluminium framed and fabric covered.
The total weight of metal castings and forgings used in the aircraft was only .
In November 1944, several crashes occurred in the Far East. At first, it was thought these were as a result of wing structure failures. The casein glue, it was said, cracked when exposed to extreme heat and/or monsoon conditions. This caused the upper surfaces to "lift" from the main spar.
An investigating team led by Major Hereward de Havilland travelled to India and produced a report in early December 1944 stating that "the accidents were not caused by the deterioration of the glue but by shrinkage of the airframe during the wet monsoon season". However a later inquiry by Cabot & Myers definitely attributed the accidents to faulty manufacture and this was confirmed by a further investigation team by the Ministry of Aircraft Production at Defford which found faults in six different Marks of Mosquito (all built at de Havilland's Hatfield and Leavesden plants) which showed similar defects, and none of the aircraft had been exposed to monsoon conditions or termite attack; thus it was concluded that there were construction defects found at the two plants.
It was found that the "Standard of glueing...left much to be desired”. Records at the time showed that accidents caused by "loss of control" were three times more frequent on Mosquitoes than on any other type of aircraft. The Air Ministry forestalled any loss of confidence in the Mosquito by holding to Major de Havilland's initial investigation in India that the accidents were caused "largely by climate" To solve the problem, a sheet of plywood was set along the span of the wing to seal the entire length of the skin joint along the main spar.
As observed by aviation author Bill Sweetman, writing in "Stealth Bomber" an (unanticipated) benefit of the predominantly wooden construction was that the non-metal structure, made of spruce, birch plywood and balsa, presented a weak flicker on contemporary radar screen compared with a heavy bomber. 
Systems.
The fuel systems gave the Mosquito good range and endurance, using up to nine fuel tanks. Two outer wing tanks each contained of fuel. These were complemented by two inner wing fuel tanks, each containing , located between the wing root and engine nacelle.
In the central fuselage were twin fuel tanks mounted between bulkhead number two and three aft of the cockpit. In the FB.VI, these tanks contained each, while in the B.IV and other unarmed Mosquitos each of the two centre tanks contained .
Both the inner wing, and fuselage tanks are listed as the "main tanks" and the total internal fuel load of was initially deemed appropriate for the type. In addition, the FB Mk VI could have larger fuselage tanks, increasing the capacity to . Drop tanks of or could be mounted under each wing, increasing the total fuel load to .
The design of the Mark VI allowed for a provisional long-range fuel tank to increase range for action over enemy territory, for the installation of bomb release equipment specific for depth charges for strikes against enemy shipping, or for the simultaneous use of rocket projectiles along with a drop tank under each wing supplementing the main fuel cells. The FB.VI had a wingspan of , a length (over guns) of . It had a maximum speed of at . Maximum take-off weight was and the range of the aircraft was with a service ceiling of .
In order to reduce fuel vaporisation at the high altitudes at which photographic reconnaissance variants flew, the central and inner wing tanks were pressurised. The pressure venting cock located behind the pilot's seat controlled the pressure valve; as the altitude increased, the valve increased the volume applied by a pump. This system was extended to include field modifications of the fuel tank system.
The engine oil tanks were in the engine nacelles. Each nacelle contained a oil tank, including a air space. The oil tanks themselves had no separate coolant controlling systems. The coolant header tank was in the forward nacelle, behind the propeller. The remaining coolant systems were controlled by the coolant radiators shutters in the forward inner wing compartment, between the nacelle and the fuselage and behind the main engine cooling radiators which were fitted in the leading edge. Electric-pneumatic operated radiator shutters directed and controlled airflow through the ducts and into the coolant valves, to predetermined temperatures.
Electrical power came from a 24 volt DC generator on the starboard (No. 2) engine and an alternator on the port engine which supplied AC power for radios. The radiator shutters, supercharger gear change, gun camera, bomb bay, bomb/rocket release and all the other crew controlled instruments were powered by a 24 volt battery. The radio communication devices included VHF and HF communications, GEE navigation, and IFF and G.P. devices. The electric generators also powered the fire extinguishers. Located on the starboard side of the cockpit, the switches would operate automatically in the event of a crash. In flight, a warning light would flash to indicate a fire, should the pilot not already be aware of it. In later models, to save liquids and engine clean up time in case of belly landing, the fire extinguisher was changed to semi-automatic triggers.
The main landing gear, housed in the nacelles behind the engines, were raised and lowered hydraulically. The main landing gear shock absorbers were de Havilland manufactured and used a system of rubber in compression, rather than hydraulic oleos, with twin pneumatic brakes for each wheel. The Dunlop-Marstrand anti-shimmy tailwheel was also retractable.
Operational history.
The de Havilland Mosquito operated in many roles during the Second World War, being tasked to perform medium bomber, reconnaissance, tactical strike, anti-submarine warfare and shipping attack and night fighter duties, both defensive and offensive, until the end of the war.
In July 1941, the first production Mosquito "W 4051" (a production fuselage combined with some prototype flying surfaces – see section of Article "Prototypes and test flights") was sent to No. 1 Photographic Reconnaissance Unit (PRU), operating at the time at RAF Benson. Consequently, the secret reconnaissance flights of this aircraft were the first active service missions of the Mosquito. In 1944, the journal "Flight" gave 19 September 1941 as date of the first PR mission, at an altitude "of some 20 000 ft."
On 15 November 1941, 105 Squadron, RAF, took delivery of the first operational Mosquito Mk. B.IV bomber, serial no. "W4064". Throughout 1942, 105 Sdn., based at RAF Horsham St. Faith, then from 29 September, RAF Marham, undertook daylight low-level and shallow dive attacks. Apart from the famous Oslo raid, these were mainly on industrial targets in occupied Netherlands, plus northern and western Germany. The crews faced deadly flak and fighters, particularly FW 190’s, which they called “snappers.” Germany still controlled Continental airspace, and the FW 190’s were often already airborne and at an advantageous altitude. It was the Mosquito’s excellent handling capabilities, rather than pure speed, that facilitated those evasions that were successful. During this daylight-raiding phase, aircrew losses were high – even the losses incurred in the squadron’s dangerous Blenheim era were exceeded in percentage terms. The Roll of Honour shows 51 aircrew deaths from the end of May 1942 to April 1943. In the corresponding period, crews gained three Mentions in Despatches, two DFMs and three DFCs.
The Mosquito was first announced publicly on 26 September 1942 after the Oslo Mosquito raid of 25 September. It was featured in "The Times" on the 28 September, and the next day the newspaper published two captioned photographs illustrating the bomb strikes and damage.
Mosquitos were widely used by the RAF Pathfinder Force, marking targets for the main night-time strategic bombing force, as well as flying "nuisance raids" in which Mosquitos often dropped 4,000 lb "Cookies". Despite an initially high loss rate, the Mosquito ended the war with the lowest losses of any aircraft in RAF Bomber Command service. Post war, the RAF found that when finally applied to bombing, in terms of useful damage done, the Mosquito had proved 4.95 times cheaper than the Lancaster. In April 1943, in response to "political humiliation" caused by the Mosquito, Hermann Göring ordered the formation of special "Luftwaffe" units ("Jagdgeschwader 25", commanded by "Oberstleutnant" Herbert Ihlefeld and "Jagdgeschwader 50", under "Major" Hermann Graf) to combat the Mosquito attacks, though these units, which were "little more than glorified squadrons", were not very successful against the elusive RAF aircraft.
In one example of the daylight precision raids carried out by the Mosquito, on 20 January 1943, the 10th anniversary of the Nazis' seizure of power, a Mosquito attack knocked out the main Berlin broadcasting station while Commander in Chief Reichsmarschall Hermann Göring was speaking, putting his speech off the air. Göring himself had strong views about the Mosquito, lecturing a group of German aircraft manufacturers in 1943 that:
The Mosquito also proved a very capable night fighter. Some of the most successful RAF pilots flew the Mosquito. Bob Braham claimed around a third of his 29 kills in a Mosquito, flying mostly daytime operations, while on night fighters Wing Commander Branse Burbridge claimed 21 kills, and Wing Commander John Cunningham claimed 19 of his 20 victories at night on Mosquitos. Mosquitos of No. 100 Group RAF were responsible for the destruction of 257 German aircraft from December 1943 to April 1945. Mosquito fighters from all units accounted for 487 German aircraft during the war, the vast majority of which were night fighters.
One Mosquito is listed as belonging to German secret operations unit "Kampfgeschwader 200", which tested, evaluated and sometimes clandestinely operated captured enemy aircraft during the war. The aircraft was listed on the order of battle of "Versuchsverband OKL"s, "2 Staffel", "Stab Gruppe" on 10 November and 31 December 1944. However, on both lists, the Mosquito is listed as unserviceable.
The Mosquito flew its last official European war mission on 21 May 1945, when Mosquitos of 143 Squadron and 248 Squadron RAF were ordered to continue to hunt German submarines that might be tempted to continue the fight; instead of submarines all the Mosquitos encountered were passive E-boats.
Variants.
Until the end of 1942 the RAF always used Roman numerals (I, II, ...) for mark numbers; 1943–1948 was a transition period during which new aircraft entering service were given Arabic numerals (1, 2, ...) for mark numbers, but older aircraft retained their Roman numerals. From 1948 onwards, Arabic numerals were used exclusively.
Prototypes.
Three prototypes were built, each with a different configuration. The first to fly was "W4050" on 25 November 1940, followed by the fighter "W4052" on 15 May 1941 and the photo-reconnaissance prototype "W4051" on 10 June 1941. "W4051" later flew operationally with 1 Photographic Reconnaissance Unit (1 PRU).
Photo-reconnaissance.
Over 30 Mosquito B Mk IV bombers were converted into the PR Mk IV photo-reconnaissance aircraft. The first operational flight by a PR Mk IV was made by "DK284" in April 1942.
The Mosquito PR Mk VIII, built as a stopgap pending the introduction of the refined PR Mk IX, was the next photo-reconnaissance version. The five VIIIs were converted from B Mk IVs and became the first operational Mosquito version to be powered by two-stage, two-speed supercharged engines, using Rolls-Royce Merlin 61 engines in place of Merlin 21/22s. The first PR Mk VIII, "DK324" first flew on 20 October 1942. The PR Mk VIII had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, a ceiling of , a range of , and a climb rate of 2,500 ft per minute (760 m).
The Mosquito PR Mk IX, 90 of which were built, was the first Mosquito variant with two-stage, two-speed engines to be produced in quantity; the first of these, "LR405", first flew in April 1943. The PR Mk IX was based on the Mosquito B Mk IX bomber and was powered by two Merlin 72/73 or 76/77 engines. It could carry either two , two or two droppable fuel tanks.
The Mosquito PR Mk XVI had a pressurised cockpit and, like the Mk IX, was powered by two Rolls-Royce Merlin 72/73 or 76/77 piston engines. This version was equipped with three overload fuel tanks, totalling in the bomb bay, and could also carry two or drop tanks. A total of 435 of the PR Mk XVI were built. The PR Mk XVI had a maximum speed of , a cruise speed of , ceiling of , a range of , and a climb rate of 2,900 feet per minute (884 m).
The Mosquito PR Mk 32 was a long-range, high-altitude, pressurised photo-reconnaissance version. It was powered by a pair of two-stage supercharged Rolls-Royce Merlin 113 and Merlin 114 piston engines, the Merlin 113 on the starboard side and the Merlin 114 on the port. First flown in August 1944, only five were built and all were conversions from PR.XVIs.
The Mosquito PR Mk 34 and PR Mk 34A was a very long-range unarmed high altitude photo-reconnaissance version. The fuel tank and cockpit protection armour were removed. Additional fuel was carried in a bulged bomb bay: 1,192 gallons which was the equivalent of . A further two 200-gallon (910-litre) drop tanks under the outer wings gave a range of cruising at . Powered by two Merlin 114s first used in the PR.32. The port Merlin 114 drove a Marshal cabin supercharger. A total of 181 were built, including 50 built by Percival Aircraft Company at Luton. The PR.34's maximum speed (TAS) was at sea level, at and at .
All PR.34s were installed with four split F52 vertical cameras, two forward, two aft of the fuselage tank and one F24 oblique camera. Sometimes a K-17 camera was used for air surveys. In August 1945, the PR.34A was the final photo-reconnaissance variant with one Merlin 113A and 114A each delivering .
Colonel Roy M. Stanley II, USAF (RET) wrote: "I consider the Mosquito the best photo-reconnaissance aircraft of the war".
After the end of World War II Spartan Air Services used ten ex-RAF Mosquitoes, mostly B.35's plus one of only six PR.35's built, for high-altitude photographic survey work in Canada.
Bombers.
<br>
On 21 June 1941 the Air Ministry ordered that the last 10 Mosquitoes, ordered as photo-reconnaissance aircraft, should be converted to bombers. These 10 aircraft were part of the original 1 March 1940 production order and became the B Mk IV Series 1. "W4052" was to be the prototype and flew for the first time on 8 September 1941.
The bomber prototype led to the B Mk IV, of which 273 were built: apart from the 10 Series 1s, all of the rest were built as Series 2s with extended nacelles, revised exhaust manifolds, with integrated flame dampers, and larger tailplanes. Series 2 bombers also differed from the Series 1 in having a larger bomb bay to increase the payload to four bombs, instead of the four bombs of Series 1. This was made possible by shortening the tail of the bomb so that these four larger weapons could be carried (or a 2,000 lb (920;kg) total load). The B Mk IV entered service in May 1942 with 105 Squadron.
In April 1943 it was decided to convert a B Mk IV to carry a 4,000 lb (1,812 kg), thin-cased high explosive bomb (nicknamed "Cookie"). The conversion, including modified bomb bay suspension arrangements, bulged bomb bay doors and fairings, was relatively straightforward, and 54 B.IVs were subsequently modified and distributed to squadrons of RAF Bomber Command's Light Night Striking Force. 27 B Mk IVs were later converted for special operations with the Highball anti-shipping weapon, and were used by 618 Squadron, formed in April 1943 specifically to use this weapon. A B Mk IV, "DK290" was initially used as a trials aircraft for the bomb, followed by "DZ471,530 and 533". The B Mk IV had a maximum speed of , a cruising speed of , ceiling of , a range of , and a climb rate of 2,500 ft per minute (762 m).
Other bomber variants of the Mosquito included the Merlin 21 powered B Mk V high-altitude version. Trials with this configuration were made with "W4057" which had the wings strengthened and two additional fuel tanks, or alternatively two bombs. This design was not produced in Britain, but formed the basic design of the Canadian-built B.VII. Only "W4057" was built in prototype form. The Merlin 31 powered B Mk VII was built by de Havilland Canada and first flown on 24 September 1942. It only saw service in Canada, 25 were built. Six were handed over to the United States Army Air Forces.
B Mk IX (54 built) was powered by the Merlin 72,73, 76 or 77. The two-stage Merlin variant was based on the PR.IX. The prototype "DK 324" was converted from a PR.VIII and first flew on 24 March 1943. In October 1943 it was decided that all B Mk IVs and all B Mk IXs then in service would be converted to carry the "Cookie", and all B Mk IXs built after that date were designed to allow them to be converted to carry the weapon. The B Mk IX had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, ceiling of , a range of , and a climb rate of 2,850 feet per minute (869 m). The IX could carry a maximum load of of bombs. A Mosquito B Mk IX holds the record for the most combat operations flown by an Allied bomber in the Second World War. "LR503", known as "F for Freddie" (from its squadron code letters, GB*F), first served with No. 109 and subsequently, No. 105 RAF squadrons. It flew 213 sorties during the war, only to crash at Calgary airport during the Eighth Victory Loan Bond Drive on 10 May 1945, two days after Victory in Europe Day, killing both the pilot, Flt. Lt. Maurice Briggs, DSO, DFC, DFM and navigator Fl. Off. John Baker, DFC and Bar.
The B Mk XVI was powered by the same variations as the B.IX. All B Mk XVIs were capable of being converted to carry the "Cookie". The two-stage powerplants were added along with a pressurised cabin. "DZ540" first flew on 1 January 1944. The prototype was converted from a IV (402 built). The next variant, the B Mk XX, was powered by Packard Merlins 31 and 33s. It was the Canadian version of the IV. Altogether, 245 were built. The B Mk XVI had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, ceiling of , a range of , and a climb rate of 2,800 ft per minute (853 m). The type could carry of bombs.
The B.35 was powered by Merlin 113 and 114As. Some were converted to TT.35s (Target Tugs) and others were used as PR.35s (photo-reconnaissance). The B.35 had a maximum speed of , a cruising speed of , ceiling of , a range of , and a climb rate of 2,700 ft per minute (823 m). A total of 174 B.35s were delivered up to the end of 1945. A further 100 were delivered from 1946 for a grand total of 274, 65 of which were built by Airspeed Ltd.
Fighters.
In August 1942 Britain experienced several incursions of high-altitude Luftwaffe bombers, including the Junkers Ju-86P. As a result of these raids, starting in September 1942, five Mosquito B Mk IV bombers were quickly converted into F Mk XV high-altitude, pressurised fighters, powered by two-stage Merlin 73s and 77s fitted with four-bladed propellers.
The first of these conversions was "MP469", which was wheeled into the experimental shop on 7 September and first flew on 14 September. The bomber nose forward of the cockpit was cut off and a standard fighter nose, complete with four .303 Brownings, grafted on in its place. The pressurised crew cabin retained the bomber's canopy structure, also retaining its vee-shaped, two-piece windscreen, while the bomber's control wheel was replaced by the fighter's control stick. Extended, pointed, wingtips increased the wingspan to . The airframe was lightened by removing all armour plating, bullet-proofing from the fuel and oil tanks, and the outer wing and fuselage tanks (leaving the inner wing tanks with a total capacity of ). Other ancillary equipment was also removed, and smaller-diameter main wheels were fitted after the first few flights. At a loaded weight of the Mk XV was lighter than a standard Mk II, and reached an altitude of . Four more B Mk IVs were converted into F Mk XVs, and, in late 1942, were further modified to become NF MK XVs.
All Mosquito fighters and fighter bombers, apart from the F Mk XV, featured a modified canopy structure incorporating a flat, single piece armoured windscreen, and the crew entry/exit door was moved from the bottom of the forward fuselage to the right side of the nose, just forward of the wing leading edge.
Night fighters.
<br>
At the end of 1940, the Air Staff's preferred night fighter design was the Gloster F.18/40 (derived from their F.9/37). However, although in agreement as to the quality of the Gloster company's design, the Ministry of Aircraft Production was concerned that Gloster would not be able to work on the F.18/40 and also the jet fighter design, considered the greater priority. Consequently, in mid-1941 the Air Staff and MAP agreed that the Gloster aircraft would be dropped and the Mosquito considered for the night fighter requirement instead.
The first production night fighter Mosquitos were designated NF Mk II. A total of 466 were built with the first entering service with No. 157 Squadron in January 1942, replacing the Douglas Havoc. These aircraft were similar to the F Mk II, but were fitted with the AI Mk IV metric wavelength radar. The herring-bone transmitting antenna was mounted on the nose and the dipole receiving antennae were carried under the outer wings. A number of NF IIs had their radar equipment removed and additional fuel tanks installed in the bay behind the cannon for use as night intruders. These aircraft, designated NF II (Special) were first used by 23 Squadron in operations over Europe in 1942. 23 Squadron was then deployed to Malta on 20 December 1942, and operated against targets in Italy.
Ninety-seven NF Mk IIs were upgraded with 3.3 GHz frequency, low-SHF-band AI Mk VIII radar and these were designated NF Mk XII. The NF Mk XIII, of which 270 were built, was the production equivalent of the Mk XII conversions. These "centimetric" radar sets were mounted in a solid "thimble" (Mk XII / XIII) or universal "bull nose" (Mk XVII / XIX) radome, which required the machine guns to be dispensed with.
Four F Mk XVs were converted to the NF Mk XV. These were fitted with AI Mk VIII in a "thimble" radome, and the .303 Brownings were moved into a gun pack fitted under the forward fuselage.
The NF Mk XIX was an improved version of the NF XIII. It could be fitted with American or British AI radars; 220 were built.
The NF Mk 30 was the final wartime variant and was a high-altitude version, powered by two Rolls-Royce Merlin 76s. The NF Mk 30 had a maximum speed of at . It also carried early electronic countermeasures equipment. 526 were built.
Other Mosquito night fighter variants which were planned for, but never built, included the NF Mk X and NF Mk XIV (the latter based on the NF Mk XIII), both of which were to have two-stage Merlins. The NF Mk 31 was a variant of the NF Mk 30, but powered by Packard Merlins.
Mosquito night intruders of 100 Group, Bomber Command, were also fitted with a device called "Serrate" to allow them to track down German night fighters from their "Lichtenstein B/C" (low-UHF-band) and "Lichtenstein SN-2" (lower end of the VHF FM broadcast band) radar emissions, as well as a device named "Perfectos" that tracked German IFF signals.
After the war, two more night fighter versions were developed:
The NF Mk 36 was similar to the Mosquito NF Mk 30, but fitted with the American-built AI.Mk X radar. Powered by two Rolls-Royce Merlin 113/114 piston engines; 266 built. Max level speeds (TAS) with flame dampers fitted were at sea level, at , and at .
The NF Mk 38, 101 of which were built, was also similar to the Mosquito NF Mk 30, but fitted with the British-built AI Mk IX radar. This variant suffered from stability problems and did not enter RAF service: 60 were eventually sold to Yugoslavia. According to the Pilot's Notes and Air Ministry 'Special Flying Instruction TF/487', which posted limits on the Mosquito's maximum speeds, the NF Mk 38 had a VNE of 370 knots (425 mph), without under-wing stores, and within the altitude range of sea level to . However, from 10,000 to the maximum speed was 348 knots (400 mph). As the height increased other recorded speeds were; 15,000 to 320 knots (368 mph); 20,000 to , 295 knots (339 mph); 25,000 to , 260 knots (299 mph); 30,000 to , 235 knots (270 mph). With two added 100-gallon fuel tanks this performance fell; between sea level and 15,000 feet 330 knots (379 mph); between 15,000 to 320 knots (368 mph); 20,000 to , 295 knots (339 mph); 25,000 to , 260 knots (299 mph); 30,000 to , 235 knots (270 mph). Little difference was noted above .
Fighter-bombers.
The FB Mk VI, which first flew on 1 June 1942, was powered by two Merlin 21s or Merlin 25s, and introduced a re-stressed and reinforced "basic" wing structure capable of carrying single or bombs on racks housed in streamlined fairings under each wing, or up to eight RP-3 25lb or 60 lb rockets. In addition fuel lines were added to the wings to enable single or drop tanks to be carried under each wing. The usual fixed armament was four 20 mm Hispano Mk.II cannon and four .303 (7.7 mm) Browning machine guns, while two or bombs could be carried in the bomb bay.
Unlike the F Mk II, the ventral bay doors were split into two pairs, with the forward pair being used to access the cannon, while the rear pair acted as bomb bay doors. The maximum fuel load which could be carried was distributed between internal fuel tanks, plus two overload tanks, each of capacity, which could be fitted in the bomb bay, and two drop tanks. All-out level speed is often given as , although this speed applies to aircraft fitted with saxophone exhausts. The test aircraft ("HJ679") fitted with stub exhausts was found to be performing below expectations. It was returned to de Havilland at Hatfield where it was serviced. Its top speed was then tested and found to be , in line with expectations. 2,298 FB Mk VIs were built, nearly one-third of Mosquito production. Two were converted to TR.33 carrier-borne, maritime strike prototypes.
The FB Mk VI proved capable of holding its own with single-engine fighter aircraft in addition to bombing. For example, on 15 January 1945 Mosquito FB Mk VIs of 143 Squadron were engaged by 30 Focke-Wulf Fw 190s from "Jagdgeschwader 5": nonetheless, the Mosquitos sank an armed trawler and two merchant ships, losing five Mosquitos (two to flak) but shooting down five Fw 190s.
Another fighter-bomber variant was the Mosquito FB Mk XVIII (sometimes known as the "Tsetse") of which one was converted from a FB Mk VI to serve as prototype and 17 were purpose-built. The Mk XVIII was armed with a Molins "6-pounder Class M" cannon: this was a modified QF 6-pounder (57 mm) anti-tank gun fitted with an auto-loader to allow both semi- or fully automatic fire. 25 rounds were carried, with the entire installation weighing . In addition, of armour was added within the engine cowlings, around the nose and under the cockpit floor to protect the engines and crew from heavily armed U-boats, the intended primary target of the Mk XVIII. Two or four .303 (7.7 mm) Browning machine guns were retained in the nose and were used to "sight" the main weapon onto the target.
The Air Ministry initially suspected that this variant would not work, but tests proved otherwise. Although the gun provided the Mosquito with yet more anti-shipping firepower for use against U-boats, it required a steady approach run to aim and fire the gun, making it vulnerable to anti-aircraft fire. The gun had a muzzle velocity of and an excellent range of some 1,800 – 1,500 yards (1,650 – 1,370 m). It was sensitive to sidewards movement; an attack required a dive from at a 30° angle with the turn and bank indicator on centre. A move during the dive could jam the gun. The prototype "HJ732" was converted from a FB.VI and was first flown on 8 June 1943.
The effect of the new weapon was demonstrated on 10 March 1944 when Mk XVIIIs from 248 Squadron (escorted by four Mk VIs) engaged a German convoy of one U-boat and four destroyers, protected by 10 Ju 88s. Three of the Ju 88s were shot down. Pilot Tony Phillips destroyed one Ju 88 with four shells, one of which tore an engine off the Ju 88. The U-boat was damaged. On 25 March, "U-976" was sunk by Molins-equipped Mosquitos. On 10 June, "U-821" was abandoned in the face of intense air attack from No. 248 Squadron, and was later sunk by a Liberator of No. 206 Squadron. On 5 April 1945 Mosquitos with Molins attacked five German surface ships in the Kattegat and again demonstrated their value by setting them all on fire and sinking them. A German "Sperrbrecher" ("minefield breaker") was lost with all hands, with some 200 bodies being recovered by Swedish vessels. Some 900 German soldiers died in total. On 9 April, German U-boats "U-804", "U-843" and "U-1065" were spotted in formation heading for Norway. All were sunk. U-251 and U-2359 followed on 19 April and 2 May 1945.
Despite the preference for rockets, a further development of the large gun idea was carried out using the even larger, 96 mm calibre QF 32-pounder, a gun based on the QF 3.7 inch AA gun designed for tank use, the airborne version using a novel form of muzzle brake. Developed to prove the feasibility of using such a large weapon in the Mosquito, this installation was not completed until after the war, when it was flown and fired in a single aircraft without problems, then scrapped.
Designs based on the Mk VI were the FB Mk 26, built in Canada, and the FB Mk 40, built in Australia, powered by Packard Merlins. The FB.26 improved from the FB.21 using single stage Packard Merlin 225s. Some 300 were built and another 37 converted to T.29 standard. 212 FB.40s were built by de Havilland Australia. Six were converted to PR.40; 28 to PR.41s, one to FB.42 and 22 to T.43 trainers. Most were powered by Packard-built Merlin 31 or 33s.
Trainers.
The Mosquito was also built as the Mosquito T Mk III two-seat trainer. This version, powered by two Rolls-Royce Merlin 21s, was unarmed and had a modified cockpit fitted with dual control arrangements. A total of 348 of the T Mk III were built for the RAF and Fleet Air Arm. de Havilland Australia built 11 T Mk 43 trainers, similar to the Mk III.
Torpedo-bombers.
To meet specification N.15/44 for a navalised Mosquito for Royal Navy use as a torpedo bomber, de Havilland produced a carrier-borne variant. A Mosquito FB.VI was modified as a prototype designated Sea Mosquito TR Mk 33 with folding wings, arrester hook, thimble nose radome, Merlin 25 engines with four-bladed propellers and a new oleo-pneumatic landing gear rather than the standard rubber-in-compression gear. Initial carrier tests of the Sea Mosquito were carried out by Eric "Winkle" Brown aboard HMS "Indefatigable", the first landing-on taking place on 25 March 1944. An order for 100 TR.33s was placed although only 50 were built at Leavesden. Armament was four 20 mm cannon, two 500 lb bombs in the bomb bay (another two could be fitted under the wings), eight 60 lb rockets (four under each wing) and a standard torpedo under the fuselage. The first production TR.33 flew on 10 November 1945. This series was followed by six Sea Mosquito TR Mk 37s, which differed in having ASV Mk XIII radar instead of the TR.33's AN/APS-6.
Target tugs.
The RAF's target tug version was the Mosquito TT Mk 35, which were the last aircraft to remain in operational service with No 3 CAACU at Exeter, being finally retired in 1963. These aircraft were then featured in the film 633 Squadron. 
A number of B Mk XVIs bombers were converted into TT Mk 39 target tug aircraft. The Royal Navy also operated the Mosquito TT Mk 39 for target towing. 
Two ex-RAF FB.6s were converted to TT.6 standard at Manchester (Ringway) Airport by Fairey Aviation in 1953–1954, and delivered to the Belgian Air Force for use as towing aircraft from the Sylt firing ranges.
Canadian-built.
A total of 1,133 (to 1945) Mosquitos were built by De Havilland Canada at Downsview Airfield in Downsview Ontario (now Downsview Park in Toronto Ontario).
Highball.
A number of Mosquito IVs were modified by Vickers-Armstrongs to carry Highball "bouncing bombs" and were allocated Vickers Type numbers:
Production.
Details.
In England, fuselage shells were mainly made by furniture companies, including Ronson, E. Gomme, Parker Knoll and Styles & Mealing. Some of the specialised wood veneer used in the construction of the Mosquito was made by Roddis Manufacturing in Marshfield, Wisconsin in the United States. Hamilton Roddis had teams of young women ironing the (unusually thin) strong wood veneer before shipping to the UK. Wing spars were made by J. B. Heath and Dancer & Hearne. Many of the other parts, including flaps, flap shrouds, fins, leading edge assemblies and bomb doors were also produced in High Wycombe, which was well suited to these tasks because it had a well-established furniture manufacturing industry. Dancer & Hearne processed much of the wood from start to finish, receiving timber and transforming it into finished wing spars at their High Wycombe factory.
About 5,000 of the 7,781 Mosquitos made contained parts made in High Wycombe. In Canada, fuselages were built in the Oshawa, Ontario plant of General Motors of Canada Limited. These were shipped to De Havilland Canada in Toronto for mating to the wings and completion. As a secondary manufacturer, de Havilland Australia started construction in Sydney. These production lines added 1,133 from Canada and 212 from Australia.
Total Mosquito production was 7,781, of which 6,710 were built during the war. The ferrying of Mosquitos from Canada to the war front was problematic, as a small fraction of the aircraft mysteriously disappeared over mid-Atlantic. The theory of "auto-explosion" was offered, and, although a considerable effort by de Havilland Canada to resolve production problems with engine and oil systems reduced the number of aircraft lost, the actual cause of the losses was never established. The company introduced an additional five hours flight testing to clear production aircraft before the ferry flight. By the end of the war nearly 500 Mosquito bombers and fighter-bombers had been delivered successfully by the Canadian operation.
In total, both during the war and after, de Havilland exported 46 FB.VIs and 29 PR. XVIs to Australia, two FB.VI and 18 NF.30s to Belgium, approximately 205 FB.26, T.29 and T.27s to Nationalist China (5 of these were captured by the People's Liberation Army during the Chinese Civil War), 19 FB.VIs to Czechoslovakia in 1948, 6 FB.VIs to Dominica, a few B.IVs, 57 FB.VIs, 29 PR.XVIs and 23 NF.XXXs to France. Some T.IIIs were exported to Israel along with 60 FB.VIs, and at least five PR.XVIs and 14 naval versions. Four T.IIIs, 76 FB.VIs, one FB.40 and four T.43s were exported to New Zealand. Three T.IIIs were exported to Norway, and 18 FB.VIs which were later converted to night fighter standard. South Africa received two F.II and 14 PR.XVI/XIs and Sweden received 60 NF.XIXs. Turkey received 96 FB.VIs and several T.IIIs, and Yugoslavia had 60 NF.38s, 80 FB.VIs and three T.IIIs delivered.
Civilian accidents and incidents.
A number of Mosquitoes were lost in civilian airline service, mostly with British Overseas Airways Corporation during World War II.
Survivors.
There are approximately 30 non-flying Mosquitos around the world with two airworthy examples. The largest collection of Mosquitos is at the de Havilland Aircraft Heritage Centre in the United Kingdom, which owns three aircraft, including the first prototype, "W4050", possibly the only initial prototype of a Second World War aircraft design still in existence in the 21st century.
Specifications.
DH.98 Mosquito F Mk II.
Fighter version.
DH.98 Mosquito B Mk XVI.
The definitive bomber version.

</doc>
<doc id="9099" url="https://en.wikipedia.org/wiki?curid=9099" title="Dave Thomas (businessman)">
Dave Thomas (businessman)

Rex David "Dave" Thomas (July 2, 1932January 8, 2002) was an American businessperson and philanthropist. Thomas was the founder and chief executive officer of Wendy's, a fast-food restaurant chain specializing in hamburgers. He is also known for appearing in more than 800 commercial advertisements for the chain from 1989 to 2002, more than any other company founder in television history.
Early life.
Dave Thomas was born on July 2, 1932 in Atlantic City, New Jersey to a young unmarried woman he never knew. He was adopted at six weeks by Rex and Auleva Thomas, and as an adult became a well-known advocate for adoption, founding the Dave Thomas Foundation for Adoption. After his adoptive mother's death when he was 5, his father moved around the country seeking work. Thomas spent some of his early childhood near Kalamazoo, Michigan with his grandmother, Minnie Sinclair, who he credited with teaching him the importance of service and treating others well and with respect, lessons that helped him in his future business life.
At 12, Thomas had his first job at Regas Restaurant, a fine dining restaurant in downtown Knoxville, Tennessee, then lost it in a dispute with his boss; decades later, Regas Restaurant installed a large autographed poster-photo of Thomas just inside their entrance until the business closed down December 31, 2010. He vowed never to lose another job. Moving with his father, by 15 he was working in Fort Wayne, Indiana at the Hobby House Restaurant owned by the Clauss family. When his father prepared to move again, Dave decided to stay in Fort Wayne, dropping out of high school to work full-time at the restaurant. Thomas, who considered ending his schooling the greatest mistake of his life, did not graduate from high school until 1993, when he obtained a GED.
He subsequently became an education advocate and founded the Dave Thomas Education Center in Coconut Creek, Florida, which offers GED classes to young adults.
U.S. Army.
At the outbreak of the Korean War in 1950, rather than waiting for the draft, he volunteered for the U.S. Army to have some choice in assignments. Having food production and service experience, Thomas requested the Cook's and Baker's School at Fort Benning, Georgia. He was sent to Germany as a mess sergeant and was responsible for the daily meals of 2000 soldiers, rising to the rank of staff sergeant. After his discharge in 1953, Thomas returned to Fort Wayne and the Hobby House.
Fast food career.
Kentucky Fried Chicken.
In the mid-1950s, Kentucky Fried Chicken founder Col. Harland Sanders came to Fort Wayne to find restaurateurs with established businesses in order to try to sell KFC franchises to them. At first, Thomas, who was the head cook at a restaurant, and the Clausses declined Sanders' offer, but Sanders persisted and the Clauss family franchised their restaurant with KFC and later also owned many other KFC franchises in the Midwest. During this time, Thomas worked with Sanders on many projects to make KFC more profitable and to give it brand recognition. Among other things Thomas suggested to Sanders, that were implemented, was that KFC reduce the number of items on the menu and focus on a signature dish. Thomas also suggested Sanders make commercials that he appear in himself. Thomas was sent by the Clauss family in the mid-1960s to help turn around four failing KFC stores they owned in Columbus, Ohio.
By 1968 Thomas had increased sales in the four fried chicken restaurants so much that he sold his share in them back to Sanders for more than $1.5 million. This experience would prove invaluable to Thomas when he began Wendy's about a year later.
Arthur Treacher's.
After serving as a regional director for Kentucky Fried Chicken, Thomas became part of the investor group which founded Arthur Treacher's. His involvement with the new restaurant lasted less than a year before he went on to found Wendy's.
Wendy's.
Thomas opened his first Wendy's in Columbus, Ohio, November 15, 1969. (This original restaurant remained operational until March 2, 2007, when it was closed due to lagging sales.) Thomas named the restaurant after his eight-year-old daughter Melinda Lou, whose nickname was "Wendy", stemming from the child's inability to say her own name at a young age. According to "Bio TV", Dave claims himself that people nicknamed his daughter "Wenda. Not Wendy, but Wenda. 'I'm going to call it Wendy's Old Fashioned Hamburgers'."
In 1982, Thomas resigned from his day-to-day operations at Wendy's. However, by 1985, several company business decisions, including an awkward new breakfast menu and loss in brand awareness due to fizzled marketing efforts, caused the company's new president to urge Thomas back into a more active role with Wendy's. Thomas began to visit franchises and espouse his hardworking, so-called "mop-bucket attitude." In 1989, he took on a significant role as the TV spokesperson in a series of commercials for the brand. Thomas was not a natural actor, and initially, his performances were criticized as stiff and ineffective by advertising critics.
By 1990, after efforts by Wendy's agency, Backer Spielvolgel Bates, to get humor into the campaign, a decision was made to portray Thomas in a more self-deprecating and folksy manner, which proved much more popular with test audiences. Consumer brand awareness of Wendy's eventually regained levels it had not achieved since octogenarian Clara Peller's wildly popular "Where's the beef?" campaign of 1984.
With his natural self-effacing style and his relaxed manner, Thomas quickly became a household name. A company survey during the 1990s, a decade during which Thomas starred in every Wendy's commercial that aired, found that 90% of Americans knew who Thomas was. After more than 800 commercials, it was clear that Thomas played a major role in Wendy's status as the country's third most popular burger restaurant.
In 1994, Thomas made a cameo appearance as himself in "Bionic Ever After?", a reunion TV movie based upon "The Six Million Dollar Man" and "The Bionic Woman".
Personal life.
Dave Thomas had four children besides Melinda: three daughters, Pam, Lori and Molly, and a son, Kenny. Though Kenny died in 2013, Dave's daughters still continue to own and run multiple Wendy's locations. Thomas founded the chain Sisters Chicken and Biscuits in 1978, named in reference to his other 3 daughters.
Honors and memberships.
Thomas, realizing that his success as a high school dropout might convince other teenagers to quit school (something he later claimed was a mistake), became a student at Coconut Creek High School. He earned a GED in 1993. Thomas was inducted into the Junior Achievement U.S. Business Hall of Fame in 1999.
Thomas was an honorary Kentucky colonel, as was former boss Colonel Sanders.
Thomas was posthumously awarded the Presidential Medal of Freedom in 2003.
Death.
Thomas had been battling Carcinoid Cancer (AKA Neuroendocrine Tumor) since the 1990s, the disease had metastasized to his liver. He died on January 8, 2002 at his home in Fort Lauderdale, Florida at the age of 69. Thomas was buried in Union Cemetery in Columbus, Ohio. At the time of his death, there were more than 6,000 Wendy's restaurants operating in North America.

</doc>
<doc id="9101" url="https://en.wikipedia.org/wiki?curid=9101" title="Device driver">
Device driver

In computing, a device driver (commonly referred to as a "driver") is a computer program that operates or controls a particular type of device that is attached to a computer. A driver provides a software interface to hardware devices, enabling operating systems and other computer programs to access hardware functions without needing to know precise details of the hardware being used.
A driver communicates with the device through the computer bus or communications subsystem to which the hardware connects. When a calling program invokes a routine in the driver, the driver issues commands to the device. Once the device sends data back to the driver, the driver may invoke routines in the original calling program. Drivers are hardware dependent and operating-system-specific. They usually provide the interrupt handling required for any necessary asynchronous time-dependent hardware interface.
Purpose.
Device drivers simplify programming by acting as translator between a hardware device and the applications or operating systems that use it. Programmers can write the higher-level application code independently of whatever specific hardware the end-user is using.
For example, a high-level application for interacting with a serial port may simply have two functions for "send data" and "receive data". At a lower level, a device driver implementing these functions would communicate to the particular serial port controller installed on a user's computer. The commands needed to control a 16550 UART are much different from the commands needed to control an FTDI serial port converter, but each hardware-specific device driver abstracts these details into the same (or similar) software interface.
Development.
Writing a device driver requires an in-depth understanding of how the hardware and the software works for a given platform function. Because drivers require low-level access to hardware functions in order to operate, drivers typically operate in a highly privileged environment and can cause system operational issues if something goes wrong. In contrast, most user-level software on modern operating systems can be stopped without greatly affecting the rest of the system. Even drivers executing in user mode can crash a system if the device is erroneously programmed. These factors make it more difficult and dangerous to diagnose problems.
The task of writing drivers thus usually falls to software engineers or computer engineers who work for hardware-development companies. This is because they have better information than most outsiders about the design of their hardware. Moreover, it was traditionally considered in the hardware manufacturer's interest to guarantee that their clients can use their hardware in an optimum way. Typically, the logical device driver (LDD) is written by the operating system vendor, while the physical device driver (PDD) is implemented by the device vendor. But in recent years non-vendors have written numerous device drivers, mainly for use with free and open source operating systems. In such cases, it is important that the hardware manufacturer provides information on how the device communicates. Although this information can instead be learned by reverse engineering, this is much more difficult with hardware than it is with software.
Microsoft has attempted to reduce system instability due to poorly written device drivers by creating a new framework for driver development, called Windows Driver Foundation (WDF). This includes User-Mode Driver Framework (UMDF) that encourages development of certain types of drivers—primarily those that implement a message-based protocol for communicating with their devices—as user-mode drivers. If such drivers malfunction, they do not cause system instability. The Kernel-Mode Driver Framework (KMDF) model continues to allow development of kernel-mode device drivers, but attempts to provide standard implementations of functions that are known to cause problems, including cancellation of I/O operations, power management, and plug and play device support.
Apple has an open-source framework for developing drivers on Mac OS X called the I/O Kit.
In Linux environments, programmers can build device drivers as parts of the kernel, separately as loadable modules, or as user-mode drivers (for certain types of devices where kernel interfaces exist, such as for USB devices). Makedev includes a list of the devices in Linux: ttyS (terminal), lp (parallel port), hd (disk), loop, sound (these include mixer, sequencer, dsp, and audio)...
The Microsoft Windows .sys files and Linux .ko modules contain loadable device drivers. The advantage of loadable device drivers is that they can be loaded only when necessary and then unloaded, thus saving kernel memory.
Kernel mode vs. user mode.
Device drivers, particularly on Microsoft Windows platforms, can run in kernel-mode (Ring 0 on x86 CPUs) or in user-mode (Ring 3 on x86 CPUs). The primary benefit of running a driver in user mode is improved stability, since a poorly written user mode device driver cannot crash the system by overwriting kernel memory. On the other hand, user/kernel-mode transitions usually impose a considerable performance overhead, thereby prohibiting user-mode drivers for low latency and high throughput requirements.
Kernel space can be accessed by user module only through the use of system calls. End user programs like the UNIX shell or other GUI-based applications are part of the user space. These applications interact with hardware through kernel supported functions.
Applications.
Because of the diversity of hardware and operating systems, drivers operate in many different environments. Drivers may interface with:
Common levels of abstraction for device drivers include:
So choosing and installing the correct device drivers for given hardware is often a key component of computer system configuration.
Virtual device drivers.
Virtual device drivers represent a particular variant of device drivers. They are used to emulate a hardware device, particularly in virtualization environments, for example when a DOS program is run on a Microsoft Windows computer or when a guest operating system is run on, for example, a Xen host. Instead of enabling the guest operating system to dialog with hardware, virtual device drivers take the opposite role and emulate a piece of hardware, so that the guest operating system and its drivers running inside a virtual machine can have the illusion of accessing real hardware. Attempts by the guest operating system to access the hardware are routed to the virtual device driver in the host operating system as e.g., function calls. The virtual device driver can also send simulated processor-level events like interrupts into the virtual machine.
Virtual devices may also operate in a non-virtualized environment. For example, a virtual network adapter is used with a virtual private network, while a virtual disk device is used with iSCSI. A good example for virtual device drivers can be Daemon Tools.
There are several variants of virtual device drivers, such as VxDs, VLMs, VDDs.
Open drivers.
Solaris descriptions of commonly used device drivers
Identifiers.
A device on the PCI bus or USB is identified by two IDs which consist of 4 hexadecimal numbers each. The vendor ID identifies the vendor of the device. The device ID identifies a specific device from that manufacturer/vendor.
A PCI device has often an ID pair for the main chip of the device, and also a subsystem ID pair which identifies the vendor, which may be different from the chip manufacturer.

</doc>
<doc id="9103" url="https://en.wikipedia.org/wiki?curid=9103" title="Dimona">
Dimona

Dimona () is an Israeli city in the Negev desert, to the south of Beersheba and west of the Dead Sea above the Arava valley in the Southern District of Israel. Its population at the end of 2007 was 33,600.
Etymology.
The city's name is derived from a biblical town, mentioned in Joshua 15:21-22.
History.
Dimona was one of the development towns created in the 1950s under the leadership of Israel's first Prime Minister, David Ben-Gurion. Dimona itself was conceived in 1953, and settled in 1955, mostly by new immigrants from Northern Africa, who also constructed the city's houses. The emblem of Dimona (as a local council), adopted 2 March 1961, appeared on a stamp issued on 24 March 1965.
When the Israeli nuclear program started later that decade, a location not far from the city was chosen for the Negev Nuclear Research Center due to its relative isolation in the desert and availability of housing.
In spite of a gradual decrease during the 1980s, the city's population began to grow once again with the beginning of the Russian immigration in the 1990s. Currently, Dimona is the third largest city in the Negev, with the population of 33,900. Due to projected rapid population growth in the Negev, the city is expected to triple in size by 2025.
Population.
Dimona is described as "mini-India" by many for its 7,500-strong Indian Jewish community. It is also home to Israel's Black Hebrew community, formerly governed by its founder and spiritual leader, Ben Ammi Ben-Israel, now deceased. The Black Hebrews number about 3,000 in Dimona, with additional families in Arad, Mitzpe Ramon and the Tiberias area. Their official status in Israel was an ongoing issue for many years, but in May 1990, the issue was resolved with the issuing of first B/1 visas, and a year later, issuing of temporary residency. Status was extended to August 2003, when the Israeli Ministry of Interior granted permanent residency.
Economy.
In the early 1980s, textile plants, such as Dimona Textiles Ltd., dominated the industrial landscape. Many plants have since closed. Dimona Silica Industries Ltd. manufactures precipitated silica and calcium carbonate fillers. About a third of the city's population works in industrial workplaces (chemical plants near the Dead Sea like the Dead Sea Works, high-tech companies and textile shops), and another third in the area of services. Due to the introduction of new technologies, many workers have been made redundant in the recent years, creating a total unemployment rate of about 10%. Dimona has taken part of Israel's solar transformation. The Rotem Industrial Complex outside of the city has dozens of solar mirrors that focus the sun's rays on a tower that in turn heats a water boiler to create steam, turning a turbine to create electricity. Luz II, Ltd. plans to use the solar array to test new technology for the three new solar plants to be built in California for Pacific Gas and Electric Company.
Geography and climate.
Dimona is at an average height of about 550–600 meters above sea level. It is in the Negev Desert, therefore it has a desert climate with low humidity for most of the year and little precipitation. Summers are hot with an average max temperature of about 33C in August, the hottest month of the year. Average annual precipitation is about , mostly during the winter.
Transportation.
In the early 1950s, an extension to Dimona and south was constructed from the Railway to Beersheba, designed for freight traffic. A passenger service began in 2005, after pressure from Dimona's municipality. Dimona Railway Station is located in the southwestern part of the city. The main bus terminal is the Dimona Central Bus Station, with lines to Beersheba, Tel Aviv, Eilat, and nearby towns.
Twin towns.
Dimona is twinned with:

</doc>
<doc id="9105" url="https://en.wikipedia.org/wiki?curid=9105" title="DC Comics">
DC Comics

DC Comics, Inc. is an American comic book publisher. It is the publishing unit of DC Entertainment, a company of Warner Bros. Entertainment, which itself is owned by Time Warner. DC Comics is one of the largest and most successful companies operating in American comic books, and produces material featuring numerous well-known heroic characters, including Superman, Batman, Wonder Woman, Green Lantern, Flash, Aquaman, Shazam, Hawkman and Green Arrow. The fictional DC universe also features teams such as the Justice League, the Justice Society of America, and the Teen Titans, and well-known villains such as Joker, Lex Luthor, Catwoman, Darkseid, Ra's al Ghul, Deathstroke, Sinestro, Black Adam and Brainiac. The company has also published non-DC Universe-related material, including "Watchmen", "V for Vendetta" and many titles under their alternative imprint Vertigo.
The initials "DC" came from the company's popular series "Detective Comics", which featured Batman's debut and subsequently became part of the company's name. Originally in Manhattan at 432 Fourth Avenue, the DC Comics offices have been located at 480 and later 575 Lexington Avenue; 909 Third Avenue; 75 Rockefeller Plaza; 666 Fifth Avenue; and 1325 Avenue of the Americas. DC has its headquarters at 1700 Broadway, Midtown Manhattan, New York City, but it was announced in October 2013 that DC Entertainment would relocate its headquarters from New York to Burbank, California in 2015.
Random House distributes DC Comics' books to the bookstore market, while Diamond Comic Distributors supplies the comics shop specialty market. DC Comics and its major, longtime competitor Marvel Comics (owned since 2009 by The Walt Disney Company, Time Warner's main rival) together shared over 80% of the American comic book market in 2008.
History.
Origins.
Entrepreneur Major Malcolm Wheeler-Nicholson's National Allied Publications debuted with the tabloid-sized "New Fun: The Big Comic Magazine" #1 with a cover date of February 1935. The company's second title, "New Comics" #1 (Dec. 1935), appeared in a size close to what would become comic books' standard during the period fans and historians call the Golden Age of Comic Books, with slightly larger dimensions than today's. That title evolved into "Adventure Comics", which continued through issue #503 in 1983, becoming one of the longest-running comic-book series. In 2009 DC revived "Adventure Comics" with its original numbering.
Wheeler-Nicholson's third and final title, "Detective Comics", advertised with a cover illustration dated December 1936, eventually premiered three months late with a March 1937 cover date. The themed anthology series would become a sensation with the introduction of Batman in issue #27 (May 1939). By then, however, Wheeler-Nicholson had gone. In 1937, in debt to printing-plant owner and magazine distributor Harry Donenfeld—who also published pulp magazines and operated as a principal in the magazine distributorship Independent News—Wheeler-Nicholson had to take Donenfeld on as a partner in order to publish "Detective Comics" #1. Detective Comics, Inc. was formed, with Wheeler-Nicholson and Jack S. Liebowitz, Donenfeld's accountant, listed as owners. Major Wheeler-Nicholson remained for a year, but cash-flow problems continued, and he was forced out. Shortly afterward, Detective Comics Inc. purchased the remains of National Allied, also known as Nicholson Publishing, at a bankruptcy auction.
Detective Comics Inc. soon launched a fourth title, "Action Comics", the premiere of which introduced Superman. "Action Comics" #1 (June 1938), the first comic book to feature the new character archetype—soon known as "superheroes"—proved a sales hit. The company quickly introduced such other popular characters as the Sandman and Batman.
On February 22, 2010, a copy of "Action Comics" #1 (June 1938) sold at an auction from an anonymous seller to an anonymous buyer for $1 million, besting the $317,000 record for a comic book set by a different copy, in lesser condition, the previous year.
The Golden Age.
National Allied Publications soon merged with Detective Comics Inc. to form National Comics Publications on September 30, 1946,"DETECTIVE COMICS, INC. was a corporation duly organized and existing under the laws of the State of New York, and was one of the constituent corporations consolidated on September 30, 1946 into defendant NATIONAL COMICS PUBLICATIONS, INC."</ref> which absorbed an affiliated concern, Max Gaines' and Liebowitz' All-American Publications. That year, Gaines let Liebowitz buy him out, and kept only "Picture Stories from the Bible" as the foundation of his own new company, EC Comics. At that point, "Liebowitz promptly orchestrated the merger of All-American and Detective Comics into National Comics... Next he took charge of organizing National Comics, self-distributorship Independent News, and their affiliated firms into a single corporate entity, National Periodical Publications". National Periodical Publications became publicly traded on the stock market in 1961.
Despite the official names "National Comics" and "National Periodical Publications", the company began branding itself as "Superman-DC" as early as 1940, and the company became known colloquially as DC Comics for years before the official adoption of that name in 1977.
The company began to move aggressively against what it saw as copyright-violating imitations from other companies, such as Fox Comics' Wonder Man, which (according to court testimony) Fox started as a copy of Superman. This extended to DC suing Fawcett Comics over Captain Marvel, at the time comics' top-selling character. Despite the fact that parallels between Captain Marvel and Superman seemed more tenuous (Captain Marvel's powers came from magic, unlike Superman's), the courts ruled that substantial and deliberate copying of copyrighted material had occurred. Faced with declining sales and the prospect of bankruptcy if it lost, Fawcett capitulated in 1955 and ceased comics publication. Years later, Fawcett sold the rights for Captain Marvel to DC—which in 1974 revived Captain Marvel in the new title "Shazam!" featuring artwork by his creator, C. C. Beck. In the meantime, the abandoned trademark had been seized by Marvel Comics in 1967, with the creation of their Captain Marvel, disallowing the DC comic itself to be called that. While Captain Marvel did not recapture his old popularity, he later appeared in a Saturday morning live action TV adaptation and gained a prominent place in the mainstream continuity DC calls the DC Universe.
When the popularity of superheroes faded in the late 1940s, the company focused on such genres as science fiction, Westerns, humor, and romance. DC also published crime and horror titles, but relatively tame ones, and thus avoided the mid-1950s backlash against such comics. A handful of the most popular superhero-titles, including "Action Comics" and "Detective Comics", the medium's two longest-running titles as of 2013, continued publication.
The Silver Age.
In the mid-1950s, editorial director Irwin Donenfeld and publisher Liebowitz directed editor Julius Schwartz (whose roots lay in the science-fiction book market) to produce a one-shot Flash story in the try-out title "Showcase". Instead of reviving the old character, Schwartz had writers Robert Kanigher and John Broome, penciler Carmine Infantino, and inker Joe Kubert create an entirely new super-speedster, updating and modernizing the Flash's civilian identity, costume, and origin with a science-fiction bent. The Flash's reimagining in "Showcase" #4 (October 1956) proved sufficiently popular that it soon led to a similar revamping of the Green Lantern character, the introduction of the modern all-star team Justice League of America (JLA), and many more superheroes, heralding what historians and fans call the Silver Age of comic books.
National did not reimagine its continuing characters (primarily Superman, Batman, and Wonder Woman), but radically overhauled them. The Superman family of titles, under editor Mort Weisinger, introduced such enduring characters as Supergirl, Bizarro, and Brainiac. The Batman titles, under editor Jack Schiff, introduced the successful Batwoman, Bat-Girl, Ace the Bat-Hound, and Bat-Mite in an attempt to modernize the strip with non-science-fiction elements. Schwartz, together with artist Infantino, then revitalized Batman in what the company promoted as the "New Look", re-emphasizing Batman as a detective. Meanwhile, editor Kanigher successfully introduced a whole family of Wonder Woman characters having fantastic adventures in a mythological context.
DC's introduction of the reimagined superheroes did not go unnoticed by other comics companies. In 1961, with DC's JLA as the specific spur, Marvel Comics writer-editor Stan Lee and legendary creator Jack Kirby ushered in the sub-Silver Age "Marvel Age" of comics with the debut issue of "The Fantastic Four".
Since the 1940s, when Superman, Batman, and many of the company's other heroes began appearing in stories together, DC's characters inhabited a shared continuity that, decades later, was dubbed the "DC Universe" by fans. With the story "Flash of Two Worlds", in "Flash" #123 (September 1961), editor Schwartz (with writer Gardner Fox and artists Infantino and Joe Giella) introduced a concept that allowed slotting the 1930s and 1940s Golden Age heroes into this continuity via the explanation that they lived on an other-dimensional "Earth 2", as opposed to the modern heroes' "Earth 1"—in the process creating the foundation for what would later be called the DC Multiverse.
A 1966 Batman TV show on the ABC network sparked a temporary spike in comic book sales, and a brief fad for superheroes in Saturday morning animation (Filmation created most of DC's initial cartoons) and other media. DC significantly lightened the tone of many DC comics—particularly "Batman" and "Detective Comics"—to better complement the "camp" tone of the TV series. This tone coincided with the famous "Go-Go Checks" checkerboard cover-dress which featured a black-and-white checkerboard strip at the top of each comic, a misguided attempt by then-managing editor Irwin Donenfeld to make DC's output "stand out on the newsracks".
In 1967, Batman artist Infantino (who had designed popular Silver Age characters Batgirl and the Phantom Stranger) rose from art director to become DC's editorial director. With the growing popularity of upstart rival Marvel Comics threatening to topple DC from its longtime number-one position in the comics industry, he attempted to infuse the company with more focus towards marketing new and existing titles and characters with more adult sensibilities towards an emerging older age group of superhero comic book fans that grew out of Marvel's efforts to market their superhero line to college-aged adults. This push for more mature content reached its peak with the 1986 DC Universe relaunch. He also recruited major talents such as ex-Marvel artist and Spider-Man co-creator Steve Ditko and promising newcomers Neal Adams and Denny O'Neil and replaced some existing DC editors with artist-editors, including Joe Kubert and Dick Giordano, to give DC's output a more artistic critical eye.
Kinney National subsidiary.
In 1967, National Periodical Publications was purchased by Kinney National Company, which later purchased Warner Bros.-Seven Arts and became Warner Communications.
In 1970, Jack Kirby moved from Marvel Comics to DC, at the end of the Silver Age of Comics, in which Kirby's contributions to Marvel played a large, integral role. Given carte blanche to write and illustrate his own stories, he created a handful of thematically linked series he called collectively The Fourth World. In the existing series "Superman's Pal Jimmy Olsen" and in his own, newly launched series "New Gods", "Mister Miracle", and "The Forever People", Kirby introduced such enduring characters and concepts as archvillain Darkseid and the otherdimensional realm Apokolips. While sales were respectable, they did not meet DC management's initially high expectations, and also suffered from a lack of comprehension and internal support from Infantino. By 1973 the "Fourth World" was all cancelled, although Kirby's conceptions would soon become integral to the broadening of the DC Universe. Kirby created other series for DC, including "Kamandi", about a teenaged boy in a post-apocalyptic world of anthropomorphic talking animals.
The Bronze Age.
Following the science-fiction innovations of the Silver Age, the comics of the 1970s and 1980s would become known as the Bronze Age, as fantasy gave way to more naturalistic and sometimes darker themes. Illegal drug use, banned by the Comics Code Authority, explicitly appeared in comics for the first time in Marvel Comics' "The Amazing Spider-Man" in early 1971, and after the Code's updating in response, DC offered a drug-fueled storyline in writer Dennis O'Neil and artist Neal Adams' "Green Lantern", beginning with the story "Snowbirds Don't Fly" in the retitled "Green Lantern / Green Arrow" #85 (Sept. 1971), which depicted Speedy, the teen sidekick of superhero archer Green Arrow, as having become a heroin addict.
Jenette Kahn, a former children's magazine publisher, replaced Infantino as editorial director in January 1976. DC had attempted to compete with the now-surging Marvel by dramatically increasing its output and attempting to win the market by flooding it. This included launching series featuring such new characters as "Firestorm" and "Shade, the Changing Man", as well as an increasing array of non-superhero titles, in an attempt to recapture the pre-Wertham days of post-War comicdom. In June 1978, five months before the release of the first Superman movie, Kahn expanded the line further, increasing the number of titles and story pages, and raising the price from 35 cents to 50 cents. Most series received eight-page back-up features while some had full-length twenty-five page stories. This was a move the company called the "DC Explosion". The move was not successful, however, and corporate parent Warner dramatically cut back on these largely unsuccessful titles, firing many staffers in what industry watchers dubbed "the DC Implosion". In September 1978, the line was dramatically reduced and standard-size books returned to 17 story pages but for a still-increased 40 cents. By 1980, the books returned to 50 cents with a 25-page story count but the story pages replaced house ads in the books.
Seeking new ways to boost market share, the new team of publisher Kahn, vice president Paul Levitz, and managing editor Giordano addressed the issue of talent instability. To that end—and following the example of Atlas/Seaboard Comics and such independent companies as Eclipse Comics—DC began to offer royalties in place of the industry-standard work-for-hire agreement in which creators worked for a flat fee and signed away all rights, giving talent a financial incentive tied to the success of their work. In addition, emulating the era's new television form, the miniseries while addressing the matter of an excessive number of ongoing titles fizzling out within a few issues of their start, DC created the industry concept of the comic book limited series. This publishing format allowed for the deliberate creation of finite storylines within a more flexible publishing format that could showcase creations without forcing the talent into unsustainable openended commitments.
These changes in policy shaped the future of the medium as a whole, and in the short term allowed DC to entice creators away from rival Marvel, and encourage stability on individual titles. In November 1980 DC launched the ongoing series "The New Teen Titans", by writer Marv Wolfman and artist George Pérez, two popular talents with a history of success. Their superhero-team comic, superficially similar to Marvel's ensemble series "X-Men", but rooted in DC history, earned significant sales in part due to the stability of the creative team, who both continued with the title for six full years. In addition, Wolfman and Pérez took advantage of the limited-series option to create a spin-off title, "Tales of the New Teen Titans", to present origin stories of their original characters without having to break the narrative flow of the main series or oblige them to double their work load with another ongoing title.
Modern Age.
This successful revitalization of the Silver Age Teen Titans led DC's editors to seek the same for the wider DC Universe. The result, the Wolfman/Pérez 12-issue limited series "Crisis on Infinite Earths", gave the company an opportunity to realign and jettison some of the characters' complicated backstory and continuity discrepancies. A companion publication, two volumes entitled "The History of the DC Universe", set out the revised history of the major DC characters. "Crisis" featured many key deaths that would shape the DC Universe for the following decades, and separate the timeline of DC publications into pre- and post-"Crisis".
Meanwhile, a parallel update had started in the non-superhero and horror titles. Since early 1984, the work of British writer Alan Moore had revitalized the horror series "The Saga of the Swamp Thing", and soon numerous British writers, including Neil Gaiman and Grant Morrison, began freelancing for the company. The resulting influx of sophisticated horror-fantasy material led to DC in 1993 establishing the Vertigo mature-readers imprint, which did not subscribe to the Comics Code Authority.
Two DC limited series, "" by Frank Miller and "Watchmen" by Moore and artist Dave Gibbons, drew attention in the mainstream press for their dark psychological complexity and promotion of the antihero. These titles helped pave the way for comics to be more widely accepted in literary-criticism circles and to make inroads into the book industry, with collected editions of these series as commercially successful trade paperbacks.
The mid-1980s also saw the end of many long-running DC war comics, including series that had been in print since the 1960s. These titles, all with over 100 issues, included "Sgt. Rock", "G.I. Combat", "The Unknown Soldier", and "Weird War Tales".
Time Warner unit (1990–present).
In March 1989, Warner Communications merged with Time Inc., making DC Comics a subsidiary of Time Warner. In June, the first Tim Burton directed Batman movie was released, and DC began publishing its hardcover series of DC Archive Editions, collections of many of their early, key comics series, featuring rare and expensive stories unseen by many modern fans. Restoration for many of the Archive Editions was handled by Rick Keene with colour restoration by DC's long-time resident colourist, Bob LeRose. These collections attempted to retroactively credit many of the writers and artists who had worked without much recognition for DC during the early period of comics, when individual credits were few and far between.
The comics industry experienced a brief boom in the early 1990s, thanks to a combination of speculative purchasing (mass purchase of the books as collectible items, with intent to resell at a higher value as the rising value of older issues was thought to imply that "all" comics would rise dramatically in price) and several storylines which gained attention from the mainstream media. DC's extended storylines in which Superman was killed, and superhero "Green Lantern" turned into the supervillain Parallax resulted in dramatically increased sales, but the increases were as temporary as the hero's replacements. Sales dropped off as the industry went into a major slump, while manufactured "collectibles" numbering in the millions replaced quality with quantity until fans and speculators alike deserted the medium in droves.
DC's Piranha Press and other imprints (including the mature readers line Vertigo, and Helix, a short-lived science fiction imprint) were introduced to facilitate compartmentalized diversification and allow for specialized marketing of individual product lines. They increased the use of non-traditional contractual arrangements, including the dramatic rise of creator-owned projects, leading to a significant increase in critically lauded work (much of it for Vertigo) and the licensing of material from other companies. DC also increased publication of book-store friendly formats, including trade paperback collections of individual serial comics, as well as original graphic novels.
One of the other imprints was Impact Comics from 1991 to 1992 in which the Archie Comics superheroes were licensed and revamped. The stories in the line were part of its own shared universe.
DC entered into a publishing agreement with Milestone Media that gave DC a line of comics featuring a culturally and racially diverse range of superhero characters. Although the Milestone line ceased publication after a few years, it yielded the popular animated series "Static Shock". DC established Paradox Press to publish material such as the large-format "Big Book of..." series of multi-artist interpretations on individual themes, and such crime fiction as the graphic novel "Road to Perdition". In 1998, DC purchased Wildstorm Comics, Jim Lee's imprint under the Image Comics banner, continuing it for many years as a wholly separate imprint - and fictional universe - with its own style and audience. As part of this purchase, DC also began to publish titles under the fledgling WildStorm sub-imprint America's Best Comics (ABC), a series of titles created by Alan Moore, including "The League of Extraordinary Gentlemen", "Tom Strong", and "Promethea". Moore strongly contested this situation, and DC eventually stopped publishing ABC.
2000s.
In March 2003 DC acquired publishing and merchandising rights to the long-running fantasy series "Elfquest", previously self-published by creators Wendy and Richard Pini under their WaRP Graphics publication banner. This series then followed another non-DC title, Tower Comics' series T.H.U.N.D.E.R. Agents, in collection into DC Archive Editions. In 2004 DC temporarily acquired the North American publishing rights to graphic novels from European publishers 2000 AD and Humanoids. It also rebranded its younger-audience titles with the mascot Johnny DC, and established the CMX imprint to reprint translated manga. In 2006, CMX took over from Dark Horse Comics publication of the webcomic "Megatokyo" in print form. DC also took advantage of the demise of Kitchen Sink Press and acquired the rights to much of the work of Will Eisner, such as his "The Spirit" series and his graphic novels.
In 2004, DC began laying the groundwork for a full continuity-reshuffling sequel to "Crisis on Infinite Earths", promising substantial changes to the DC Universe (and side-stepping the 1994 "Zero Hour" event which similarly tried to ret-con the history of the DCU). In 2005, the critically lauded "Batman Begins" film was released; also, the company published several limited series establishing increasingly escalated conflicts among DC's heroes, with events climaxing in the "Infinite Crisis" limited series. Immediately after this event, DC's ongoing series jumped forward a full year in their in-story continuity, as DC launched a weekly series, "52", to gradually fill in the missing time. Concurrently, DC lost the copyright to "Superboy" (while retaining the trademark) when the heirs of Jerry Siegel used a provision of the 1976 revision to the copyright law to regain ownership.
In 2005, DC launched its "All-Star" line (evoking the title of the 1940s publication), designed to feature some of the company's best-known characters in stories that eschewed the long and convoluted continuity of the DC Universe. The line began with "All-Star Batman & Robin the Boy Wonder" and "All-Star Superman", with "All Star Wonder Woman" and "All Star Batgirl" announced in 2006 but neither being released nor scheduled as of the end of 2009.
DC licensed characters from the Archie Comics imprint Red Circle Comics by 2007. They appeared in the Red Circle line, based in the DC Universe, with a series of one-shots followed by a miniseries that lead into two ongoing titles, each lasting 10 issues.
2010s.
In 2011, DC rebooted all of its running titles following the Flashpoint story line. The reboot, called The New 52, gave new origin stories and costume designs to all of DC's characters.
In 2014, DC announced an eight-issue miniseries titled ""Convergence"" which began in April 2015.
DC Entertainment.
In September 2009, Warner Bros. announced that DC Comics would become a subsidiary of DC Entertainment, Inc., with Diane Nelson, President of Warner Premiere, becoming president of the newly formed holding company and DC Comics President and Publisher Paul Levitz moving to the position of Contributing Editor and Overall Consultant there.
On February 18, 2010, DC Entertainment named Jim Lee and Dan DiDio as Co-Publishers of DC Comics, Geoff Johns as Chief Creative Officer, John Rood as EVP of Sales, Marketing and Business Development, and Patrick Caldon as EVP of Finance and Administration.
DC licensed pulp characters including Doc Savage and the Spirit which it then used, along with some DC heroes, as part the First Wave comics line launched in 2010 and lasting through fall 2011.
In May 2011, DC announced it would begin releasing digital versions of their comics on the same day as paper versions.
On June 1, 2011, DC announced that it would end all ongoing series set in the DC Universe in August and relaunch its comic line with 52 issue #1s, starting with "Justice League" on August 31 (written by Geoff Johns and drawn by Jim Lee), with the rest to follow later on in September.
On June 4, 2013, DC unveiled two new digital comic innovations to enhance interactivity: "DC2" and "DC2 Multiverse". "DC2" layers dynamic artwork onto digital comic panels, adding a new level of dimension to digital storytelling, while "DC2 Multiverse" allows readers to determine a specific story outcome by selecting individual characters, storylines and plot developments while reading the comic, meaning one digital comic has multiple outcomes. "DC2" will first appear in the upcoming digital-first title, "Batman '66", based on the 1960s television series and "DC2 Multiverse" will first appear in "", a digital-first title based on the .
In October 2013, DC Entertainment (DCE) announced that the DC Comics offices would be moved from New York City to Warner Bros. Burbank, California, headquarters in 2015 joining the other DCE units, animation, movie, TV and portfolio planning, that moved there in 2010.
Logo.
DC's first logo appeared on the April 1940 issues of its titles. The letters "DC" stood for "Detective Comics", the name of Batman's flagship title. The small logo, with no background, read simply, "A DC Publication".
The November 1941 DC titles introduced an updated logo. This version was almost twice the size of the previous one, and was the first version with a white background. The name "Superman" was added to "A DC Publication", effectively acknowledging both Superman and Batman. This logo was the first to occupy the top-left corner of the cover, where the logo has usually resided since. The company now referred to itself in its advertising as "Superman-DC".
In November 1949, the logo was modified to incorporate the company's formal name, National Comics Publications. This logo would also serve as the round body of Johnny DC, DC's mascot in the 1960s.
In October 1970, DC briefly retired the circular logo in favor of a simple "DC" in a rectangle with the name of the title, or the star of the book; the logo on many issues of "Action Comics", for example, read "DC Superman". An image of the lead character either appeared above or below the rectangle. For books that did not have a single star, such as anthologies like "House of Mystery" or team series such as "Justice League of America", the title and "DC" appeared in a stylized logo, such as a bat for "House of Mystery". This use of characters as logos helped to establish the likenesses as trademarks, and was similar to Marvel's contemporaneous use of characters as part of its cover branding.
DC's "100 Page Super-Spectacular" titles and later 100-page and "Giant" issues published from 1972 to 1974 featured a logo exclusive to these editions: the letters "DC" in a simple sans-serif typeface within a circle. A variant had the letters in a square.
The July 1972 DC titles featured a new circular logo. The letters "DC" were rendered in a block-like typeface that would remain through later logo revisions until 2005. The title of the book usually appeared inside the circle, either above or below the letters.
In December 1973, this logo was modified with the addition of the words "The Line of DC Super-Stars" and the star motif that would continue in later logos. This logo was placed in the top center of the cover from August 1975 to October 1976.
When Jenette Kahn became DC's publisher in late 1976, she commissioned graphic designer Milton Glaser to design a new logo. Popularly referred to as the "DC bullet", this logo premiered on the February 1977 titles. Although it varied in size and color and was at times cropped by the edges of the cover, or briefly rotated 4 degrees, it remained essentially unchanged for nearly three decades. Despite logo changes since 2005, the old "DC bullet" continues to be used only on the DC Archive Editions series.
In July 1987, DC released variant editions of "Justice League" #3 and "The Fury of Firestorm" #61 with a new DC logo. It featured a picture of Superman in a circle surrounded by the words "SUPERMAN COMICS". The company released these variants to newsstands in certain markets as a marketing test.
On May 8, 2005, a new logo (dubbed the "DC spin") was unveiled, debuting on DC titles in June 2005 with "DC Special: The Return of Donna Troy" #1 and the rest of the titles the following week. In addition to comics, it was designed for DC properties in other media, which was used for movies since "Batman Begins", with "Superman Returns" showing the logo's normal variant, and the TV series "Smallville", the animated series "Justice League Unlimited" and others, as well as for collectibles and other merchandise. The logo was designed by Josh Beatman of Brainchild Studios and DC executive Richard Bruning.
In March 2012, DC unveiled a new logo consisting of the letter “D” flipping back to reveal the letter “C” and "DC ENTERTAINMENT". "The Dark Knight Rises" was the first film to use the new logo, while the TV series "Arrow" is the first series to feature the new logo.

</doc>
<doc id="9109" url="https://en.wikipedia.org/wiki?curid=9109" title="Diophantine equation">
Diophantine equation

In mathematics, a Diophantine equation is a polynomial equation, usually in two or more unknowns, such that only the integer solutions are sought or studied (an integer solution is a solution such that all the unknowns take integer values). A linear Diophantine equation is an equation between two sums of monomials of degree zero or one. An exponential Diophantine equation is one in which exponents on terms can be unknowns.
Diophantine problems have fewer equations than unknown variables and involve finding integers that work correctly for all equations. In more technical language, they define an algebraic curve, algebraic surface, or more general object, and ask about the lattice points on it.
The word "Diophantine" refers to the Hellenistic mathematician of the 3rd century, Diophantus of Alexandria, who made a study of such equations and was one of the first mathematicians to introduce symbolism into algebra. The mathematical study of Diophantine problems that Diophantus initiated is now called Diophantine analysis.
While individual equations present a kind of puzzle and have been considered throughout history, the formulation of general theories of Diophantine equations (beyond the theory of quadratic forms) was an achievement of the twentieth century.
Examples.
In the following Diophantine equations, , , , and are the unknowns and the other letters are given constants:
Linear Diophantine equations.
One equation.
The simplest linear Diophantine equation takes the form , where , and are given integers. The solutions are described by the following theorem:
Proof: If is this greatest common divisor, Bézout's identity asserts the existence of integers and such that . If is a multiple of , then for some integer , and is a solution. On the other hand, for every pair of integers and , the greatest common divisor of and divides . Thus, if the equation has a solution, then must be a multiple of . If and , then for every solution , we have
showing that is another solution. Finally, given two solutions such that , one deduces that . As and are coprime, Euclid's lemma shows that there exists an integer such that and . Therefore, and , which completes the proof.
Chinese remainder theorem.
The Chinese remainder theorem describes an important class of linear Diophantine systems of equations: let be pairwise coprime integers greater than one, be arbitrary integers, and be the product . The Chinese remainder theorem asserts that the following linear Diophantine system has exactly one solution such that , and that the other solutions are obtained by adding to a multiple of :
System of linear Diophantine equations.
More generally, every system of linear Diophantine equations may be solved by computing the Smith normal form of its matrix, in a way that is similar to the use of the reduced row echelon form to solve a system of linear equations over a field. Using matrix notation every system of linear Diophantine equations may be written
where is an matrix of integers, is an column matrix of unknowns and is an column matrix of integers.
The computation of the Smith normal form of provides two unimodular matrices (that is matrices that are invertible over the integers and have ±1 as determinant) and of respective dimensions and , such that the matrix
is such that is not zero for not greater than some integer , and all the other entries are zero. The system to be solved may thus be rewritten as
Calling the entries of and those of , this leads to the system
This system is equivalent to the given one in the following sense: A column matrix of integers is a solution of the given system if and only if for some column matrix of integers such that .
It follows that the system has a solution if and only if divides for and for . If this condition is fulfilled, the solutions of the given system are
where are arbitrary integers.
Hermite normal form may also be used for solving systems of linear Diophantine equations. However, Hermite normal form does not directly provide the solutions; to get the solutions from the Hermite normal form, one has to successively solve several linear equations. Nevertheless, Richard Zippel wrote that the Smith normal form "is somewhat more than is actually needed to solve linear diophantine equations. Instead of reducing the equation to diagonal form, we only need to make it triangular, which is called the Hermite normal form. The Hermite normal form is substantially easier to compute than the Smith normal form."
Integer linear programming amounts to finding some integer solutions (optimal in some sense) of linear systems that include also inequations. Thus systems of linear Diophantine equations are basic in this context, and textbooks on integer programming usually have a treatment of systems of linear Diophantine equations.
Diophantine analysis.
Typical questions.
The questions asked in Diophantine analysis include:
These traditional problems often lay unsolved for centuries, and mathematicians gradually came to understand their depth (in some cases), rather than treat them as puzzles.
Typical problem.
The given information is that a father's age is 1 less than twice that of his son, and that the digits making up the father's age are reversed in the son's age (i.e. ). This leads to the equation , thus . Inspection gives the result , , and thus equals 73 years and equals 37 years. One may easily show that there is not any other solution with and positive integers less than 10.
17th and 18th centuries.
In 1637, Pierre de Fermat scribbled on the margin of his copy of "Arithmetica": "It is impossible to separate a cube into two cubes, or a fourth power into two fourth powers, or in general, any power higher than the second into two like powers." Stated in more modern language, "The equation has no solutions for any higher than 2." And then he wrote, intriguingly: "I have discovered a truly marvelous proof of this proposition, which this margin is too narrow to contain." Such a proof eluded mathematicians for centuries, however, and as such his statement became famous as Fermat's Last Theorem. It wasn't until 1995 that it was proven by the British mathematician Andrew Wiles.
In 1657, Fermat attempted to solve the Diophantine equation (solved by Brahmagupta over 1000 years earlier). The equation was eventually solved by Euler in the early 18th century, who also solved a number of other Diophantine equations. The smallest solution of this equation in positive integers is , (see Chakravala method).
Hilbert's tenth problem.
In 1900, David Hilbert proposed the solvability of all Diophantine equations as the tenth of his fundamental problems. In 1970, Yuri Matiyasevich solved it negatively, by proving that it cannot exist any general algorithm for solving all Diophantine equations.
Diophantine geometry.
Diophantine geometry, which is the application of techniques from algebraic geometry in this field, has continued to grow as a result; since treating arbitrary equations is a dead end, attention turns to equations that also have a geometric meaning. The central idea of Diophantine geometry is that of a rational point, namely a solution to a polynomial equation or a system of polynomial equations, which is a vector in a prescribed field , when is "not" algebraically closed.
Modern research.
One of the few general approaches is through the Hasse principle. Infinite descent is the traditional method, and has been pushed a long way.
The depth of the study of general Diophantine equations is shown by the characterisation of Diophantine sets as equivalently described as recursively enumerable. In other words, the general problem of Diophantine analysis is blessed or cursed with universality, and in any case is not something that will be solved except by re-expressing it in other terms.
The field of Diophantine approximation deals with the cases of "Diophantine inequalities". Here variables are still supposed to be integral, but some coefficients may be irrational numbers, and the equality sign is replaced by upper and lower bounds.
The most celebrated single question in the field, the conjecture known as Fermat's Last Theorem, was solved by Andrew Wiles but using tools from algebraic geometry developed during the last century rather than within number theory where the conjecture was originally formulated. Other major results, such as Faltings' theorem, have disposed of old conjectures.
Infinite Diophantine equations.
An example of an infinite diophantine equation is:
which can be expressed as "How many ways can a given integer be written as the sum of a square plus twice a square plus thrice a square and so on?" The number of ways this can be done for each forms an integer sequence. Infinite Diophantine equations are related to theta functions and infinite dimensional lattices. This equation always has a solution for any positive . Compare this to:
which does not always have a solution for positive .
Exponential Diophantine equations.
If a Diophantine equation has as an additional variable or variables occurring as exponents, it is an exponential Diophantine equation. Examples include the Ramanujan–Nagell equation, , and the equation of the Fermat-Catalan conjecture and Beal's conjecture, with inequality restrictions on the exponents. A general theory for such equations is not available; particular cases such as Catalan's conjecture have been tackled. However, the majority are solved via ad hoc methods such as Størmer's theorem or even trial and error.

</doc>
<doc id="9110" url="https://en.wikipedia.org/wiki?curid=9110" title="Diophantus">
Diophantus

Diophantus of Alexandria (; born probably sometime between AD 201 and 215; died aged 84, probably sometime between AD 285 and 299), sometimes called "the father of algebra", was an Alexandrian Greek mathematician and the author of a series of books called "Arithmetica", many of which are now lost. These texts deal with solving algebraic equations. While reading Claude Gaspard Bachet de Méziriac's edition of Diophantus' "Arithmetica," Pierre de Fermat concluded that a certain equation considered by Diophantus had no solutions, and noted in the margin without elaboration that he had found "a truly marvelous proof of this proposition," now referred to as Fermat's Last Theorem. This led to tremendous advances in number theory, and the study of Diophantine equations ("Diophantine geometry") and of Diophantine approximations remain important areas of mathematical research. Diophantus coined the term παρισότης (parisotes) to refer to an approximate equality. This term was rendered as "adaequalitas" in Latin, and became the technique of adequality developed by Pierre de Fermat to find maxima for functions and tangent lines to curves. Diophantus was the first Greek mathematician who recognized fractions as numbers; thus he allowed positive rational numbers for the coefficients and solutions. In modern use, Diophantine equations are usually algebraic equations with integer coefficients, for which integer solutions are sought. Diophantus also made advances in mathematical notation.
Biography.
Little is known about the life of Diophantus. He lived in Alexandria, Egypt, probably from between AD 200 and 214 to 284 or 298. Much of our knowledge of the life of Diophantus is derived from a 5th-century Greek anthology of number games and puzzles created by Metrodorus. One of the problems (sometimes called his epitaph) states:
This puzzle implies that Diophantus' age can be expressed as
which gives a value of 84 years. However, the accuracy of the information cannot be independently confirmed.
In popular culture, this puzzle was the Puzzle No.142 in "Professor Layton and Pandora's Box" as one of the hardest solving puzzles in the game, which needed to be unlocked by solving other puzzles first.
"Arithmetica".
The Arithmetica is the major work of Diophantus and the most prominent work on algebra in Greek mathematics. It is a collection of problems giving numerical solutions of both determinate and indeterminate equations. Of the original thirteen books of which Arithmetica consisted only six have survived, though there are some who believe that four Arab books discovered in 1968 are also by Diophantus. Some Diophantine problems from Arithmetica have been found in Arabic sources.
It should be mentioned here that Diophantus never used general methods in his solutions. Hermann Hankel, renowned German mathematician made the following remark regarding Diophantus.
“Our author (Diophantos) not the slightest trace of a general, comprehensive method is discernible; each problem calls for some special method which refuses to work even for the most closely related problems. For this reason it is difficult for the modern scholar to solve the 101st problem even after having studied 100 of Diophantos’s solutions” 
History.
Like many other Greek mathematical treatises, Diophantus was forgotten in Western Europe during the so-called Dark Ages, since the study of ancient Greek, and literacy in general, had greatly declined. The portion of the Greek "Arithmetica" that survived, however, was, like all ancient Greek texts transmitted to the early modern world, copied by, and thus known to, medieval Byzantine scholars. In addition, some portion of the "Arithmetica" probably survived in the Arab tradition (see above). In 1463 German mathematician Regiomontanus wrote:
"Arithmetica" was first translated from Greek into Latin by Bombelli in 1570, but the translation was never published. However, Bombelli borrowed many of the problems for his own book "Algebra". The "editio princeps" of "Arithmetica" was published in 1575 by Xylander. The best known Latin translation of "Arithmetica" was made by Bachet in 1621 and became the first Latin edition that was widely available. Pierre de Fermat owned a copy, studied it, and made notes in the margins.
Margin-writing by Fermat and Chortasmenos.
The 1621 edition of Arithmetica by Bachet gained fame after Pierre de Fermat wrote his famous "Last Theorem" in the margins of his copy:
Fermat's proof was never found, and the problem of finding a proof for the theorem went unsolved for centuries. A proof was finally found in 1994 by Andrew Wiles after working on it for seven years. It is believed that Fermat did not actually have the proof he claimed to have. Although the original copy in which Fermat wrote this is lost today, Fermat's son edited the next edition of Diophantus, published in 1670. Even though the text is otherwise inferior to the 1621 edition, Fermat's annotations—including the "Last Theorem"—were printed in this version.
Fermat was not the first mathematician so moved to write in his own marginal notes to Diophantus; the Byzantine scholar John Chortasmenos (1370–1437) had written "Thy soul, Diophantus, be with Satan because of the difficulty of your theorems" next to the same problem.
Other works.
Diophantus wrote several other books besides "Arithmetica", but very few of them have survived.
The "Porisms".
Diophantus himself refers to a work which consists of a collection of lemmas called "The Porisms" (or "Porismata"), but this book is entirely lost.
Although "The Porisms" is lost, we know three lemmas contained there, since Diophantus refers to them in the "Arithmetica". One lemma states that the difference of the cubes of two rational numbers is equal to the sum of the cubes of two other rational numbers, i.e. given any and , with , there exist , all positive and rational, such that
Polygonal numbers and geometric elements.
Diophantus is also known to have written on polygonal numbers, a topic of great interest to Pythagoras and Pythagoreans. Fragments of a book dealing with polygonal numbers are extant.
A book called "Preliminaries to the Geometric Elements" has been traditionally attributed to Hero of Alexandria. It has been studied recently by Wilbur Knorr, who suggested that the attribution to Hero is incorrect, and that the true author is Diophantus.
Influence.
Diophantus' work has had a large influence in history. Editions of Arithmetica exerted a profound influence on the development of algebra in Europe in the late sixteenth and through the 17th and 18th centuries. Diophantus and his works have also influenced Arab mathematics and were of great fame among Arab mathematicians. Diophantus' work created a foundation for work on algebra and in fact much of advanced mathematics is based on algebra. As far as we know Diophantus did not affect the lands of the Orient much and how much he affected India is a matter of debate. 
Diophantus is often called “the father of algebra" because he contributed greatly to number theory, mathematical notation, and because Arithmetica contains the earliest known use of syncopated notation.
Diophantine analysis.
Today, Diophantine analysis is the area of study where integer (whole-number) solutions are sought for equations, and Diophantine equations are polynomial equations with integer coefficients to which only integer solutions are sought. It is usually rather difficult to tell whether a given Diophantine equation is solvable. Most of the problems in Arithmetica lead to quadratic equations. Diophantus looked at 3 different types of quadratic equations: , , and . The reason why there were three cases to Diophantus, while today we have only one case, is that he did not have any notion for zero and he avoided negative coefficients by considering the given numbers , , to all be positive in each of the three cases above. Diophantus was always satisfied with a rational solution and did not require a whole number which means he accepted fractions as solutions to his problems. Diophantus considered negative or irrational square root solutions "useless", "meaningless", and even "absurd". To give one specific example, he calls the equation 'absurd' because it would lead to a negative value for . One solution was all he looked for in a quadratic equation. There is no evidence that suggests Diophantus even realized that there could be two solutions to a quadratic equation. He also considered simultaneous quadratic equations.
Mathematical notation.
Diophantus made important advances in mathematical notation, becoming the first person known to use algebraic notation and symbolism. Before him everyone wrote out equations completely. Diophantus introduced an algebraic symbolism that used an abridged notation for frequently occurring operations, and an abbreviation for the unknown and for the powers of the unknown. Mathematical historian Kurt Vogel states:
“The symbolism that Diophantus introduced for the first time, and undoubtedly devised himself, provided a short and readily comprehensible means of expressing an equation... Since an abbreviation is also employed for the word ‘equals’, Diophantus took a fundamental step from verbal algebra towards symbolic algebra.”
Although Diophantus made important advances in symbolism, he still lacked the necessary notation to express more general methods. This caused his work to be more concerned with particular problems rather than general situations. Some of the limitations of Diophantus' notation are that he only had notation for one unknown and, when problems involved more than a single unknown, Diophantus was reduced to expressing "first unknown", "second unknown", etc. in words. He also lacked a symbol for a general number . Where we would write , Diophantus has to resort to constructions like: "... a sixfold number increased by twelve, which is divided by the difference by which the square of the number exceeds three".
Algebra still had a long way to go before very general problems could be written down and solved succinctly.

</doc>
<doc id="9111" url="https://en.wikipedia.org/wiki?curid=9111" title="Dong">
Dong

Dong or DONG may refer to:

</doc>
<doc id="9118" url="https://en.wikipedia.org/wiki?curid=9118" title="Duke Kahanamoku">
Duke Kahanamoku

Duke Paoa Kahinu Mokoe Hulikohola Kahanamoku (August 24, 1890 – January 22, 1968) was a Kānaka Maoli (Native Hawaiian) competition swimmer, who is widely credited with popularizing the ancient Hawaiian sport of surfing. He was born towards the end of the Kingdom of Hawaii, just before the overthrow, living well into statehood as a United States citizen. He was a five-time Olympic medalist in swimming. Duke was also a law enforcement officer, an actor, a beach volleyball player and businessman.
His name "Duke" has often been confused with a royal title; however, it was his given name, passed down from his father. His grandparents were attendants to the Kamehamehas. While not a formal member of the royal family, he was descended from Alapainui, who ruled Hawaii Island.
Early years.
According to Kahanamoku, he was born in Honolulu at Haleʻākala, the home of Bernice Pauahi Bishop which was later converted into the Arlington Hotel. He had five brothers and three sisters, including Samuel Kahanamoku. In 1893, the family moved to Kālia, Waikiki (near the present site of the Hilton Hawaiian Village), to be closer to his mother's parents and family. Duke grew up with his siblings and 31 Paoa cousins. Duke attended the Waikiki Grammar School, Kaahumanu School, and the Kamehameha Schools, although he never graduated because he had quit to help support the family.
"Duke" was not a title or a nickname, but a given name. He was named after his father, Duke Halapu Kahanamoku, who was christened by Bernice Pauahi Bishop in honor of Prince Alfred, Duke of Edinburgh, who was visiting Hawaii at the time. His father was a policeman. His mother Julia Paakonia Lonokahikina Paoa was a deeply religious woman with a strong sense of family ancestry.
Even though not of the formal Hawaiian Royal Family, his parents were from prominent Hawaiian ohana (family); the Kahanamoku and the Paoa ohana were considered to be lower-ranking nobles, who were in service to the "aliʻi nui" or royalty. His paternal grandfather was Kahanamoku and his grandmother, Kapiolani Kaoeha (sometimes spelled "Kahoea"), a descendant of Alapainui. They were "kahu", retainers and trusted advisors of the Kamehamehas, to whom they were related. His maternal grandparents Paoa, son of Paoa Hoolae and Hiikaalani, and Mele Uliama were also of aliʻi descent.
Growing up on the outskirts of Waikiki, Kahanamoku spent his youth as a bronzed beach boy. At Waikiki Beach he developed his surfing and swimming skills. In his youth, Kahanamoku preferred a traditional surf board, which he called his "papa nui", constructed after the fashion of ancient Hawaiian "olo" boards. Made from the wood of a koa tree, it was long and weighed . The board was without a skeg, which had yet to be invented. In his later career, he would often use smaller boards but always preferred those made of wood.
On August 11, 1911, Kahanamoku was timed at 55.4 seconds in the freestyle, beating the existing world record by 4.6 seconds, in the salt water of Honolulu Harbor. He also broke the record in the and equaled it in the . But the Amateur Athletic Union (AAU), in disbelief, would not recognize these feats until many years later. The AAU initially claimed that the judges must have been using alarm clocks rather than stopwatches and later claimed that ocean currents aided Kahanamoku.
Career.
Kahanamoku easily qualified for the U.S. Olympic swimming team in 1912. At the 1912 Summer Olympics in Stockholm, he won a gold medal in the 100-meter freestyle, and a silver medal with the second-place U.S. team in the men's 4×200-meter freestyle relay. During the 1920 Olympics in Antwerp, he won gold medals both in the 100 meters (bettering fellow Hawaiian Pua Kealoha) and in the relay. He finished the 100 meters with a silver medal during the 1924 Olympics in Paris, with the gold going to Johnny Weissmuller and the bronze to Duke's brother, Samuel Kahanamoku. At age 34, this was Kahanamoku's last Olympic medal. He also was an alternate for the U.S. water polo team at the 1932 Summer Olympics.
Between Olympic competitions, and after retiring from the Olympics, Kahanamoku traveled internationally to give swimming exhibitions. It was during this period that he popularized the sport of surfing, previously known only in Hawaii, by incorporating surfing exhibitions into these visits as well. His surfing exhibition at Sydney's Freshwater Beach on December 24, 1914 is widely regarded as a seminal event in the development of surfing in Australia. The board that Kahanamoku built from a piece of pine from a local hardware store is retained by the Freshwater Surf Club. There is a statue of Kahanamoku on the Northern headland of Freshwater Beach, New South Wales. He made surfing popular in mainland America first in 1912 while in Southern California.
During his time living in Southern California, Kahanamoku performed in Hollywood as a background actor and a character actor in several films. In this way, he made connections with people who could further publicize the sport of surfing. Kahanamoku was involved with the Los Angeles Athletic Club, acting as lifeguard and competing on both swimming and water polo teams.
While living in Newport Beach, California on June 14, 1925, Kahanamoku rescued eight men from a fishing vessel that capsized in heavy surf while attempting to enter the city's harbor. 29 fishermen went into the water and 17 perished. Using his surfboard, he was able to make quick trips back and forth to shore to increase the number of sailors rescued. Two other surfers saved four more fishermen. Newport's police chief at the time called Duke's efforts "the most superhuman surfboard rescue act the world has ever seen". It also caused U.S. lifeguards to begin using surfboards in their water rescues.
In 1940, he married Nadine Alexander, who accompanied him when he traveled. Kahanamoku was the first person to be inducted into both the Swimming Hall of Fame and the Surfing Hall of Fame. The Duke Kahanamoku Invitational Surfing Championships are named in his honor. He is a member of the U.S. Olympic Hall of Fame. He served as sheriff of Honolulu, Hawaii from 1932 to 1961, serving 13 consecutive terms. During this period, he also appeared in a number of television programs and films, such as "Mister Roberts" (1955).
Kahanamoku was a friend and surfing companion of heiress Doris Duke, who built a home (now a museum) on Oahu named Shangri-la.
"Duncan v. Kahanamoku".
Kahanamoku was the "pro forma" defendant in the landmark Supreme Court case "Duncan v. Kahanamoku". While Kahanamoku was a military police officer during World War II, he arrested Duncan for public intoxication. At the time, Hawaii, not yet a state, was being administered under the Hawaiian Organic Act which effectively instituted martial law on the island. Duncan was therefore tried by a military tribunal and appealed to the Supreme Court. In a "post hoc" ruling, the court ruled that trial by military tribunal was, in this case, unconstitutional.
Death and legacy.
Kahanamoku died of a heart attack on January 22, 1968, at the age of 77. For his burial at sea a long motorcade of mourners, accompanied by a 30-man police escort, moved across town to Waikiki Beach. Reverend Abraham Akaka, the pastor of Kawaiahao Church, performed the service. A group of beach boys sang Hawaiian songs, including "Aloha Oe." His ashes were scattered into the ocean.
Hawaii music promoter Kimo Wilder McVay capitalized on Kahanamoku's popularity by naming his Waikiki showroom "Duke Kahanamoku's," and giving Kahanamoku a piece of the financial action in exchange for the use of his name. It was a major Waikiki showroom in the 1960s and is remembered as the home of Don Ho & The Aliis from 1964 through 1969.
Kahanamoku's name is also used by Duke's Canoe Club & Barefoot Bar, a beachfront bar and restaurant in the Outrigger Waikiki on the Beach Hotel. There is a chain of restaurants named after him in California and Hawaii called Duke's. A bronze statue at Waikiki beach in Honolulu honors his memory. It shows Kahanamoku standing in front of his surfboard with his arms outstretched. Many honor him by placing leis on his statue. There is a webcam watching the statue, allowing visitors from around the world to wave to their friends.
On August 24, 2002, which was also the 112th anniversary of the birth of Duke Kahanamoku, a 37-cent first-class letter rate postage stamp of the United States Postal Service with Duke's picture on it, was issued. The First Day Ceremony was held at the Hilton Hawaiian Village in Waikiki and was attended by thousands. At this ceremony, attendees could attach the Duke stamp to an envelope and get it canceled with a First Day of Issue postmark. These First Day Covers are very collectable.
On February 28, 2015, a monument featuring a replica of Kahanamoku's surfboard was unveiled at New Brighton beach, Christchurch, New Zealand in honour of the 100th anniversary of Kahanamoku's visit to New Brighton.
On August 24, 2015, a Google Doodle honored the 125th anniversary of Duke Kahanamoku's birthday.

</doc>
