<doc id="7039" url="https://en.wikipedia.org/wiki?curid=7039" title="Control theory">
Control theory

Control theory is an interdisciplinary branch of engineering and mathematics that deals with the behavior of dynamical systems with inputs, and how their behavior is modified by feedback. The usual objective of control theory is to control a system, often called the "plant", so its output follows a desired control signal, called the "reference", which may be a fixed or changing value. To do this a "controller" is designed, which monitors the output and compares it with the reference. The difference between actual and desired output, called the "error" signal, is applied as feedback to the input of the system, to bring the actual output closer to the reference. Some topics studied in control theory are stability (whether the output will converge to the reference value or oscillate about it), controllability and observability.
Extensive use is usually made of a diagrammatic style known as the block diagram. The transfer function, also known as the system function or network function, is a mathematical representation of the relation between the input and output based on the differential equations describing the system.
Although a major application of control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs. A few examples are in physiology, electronics, climate modeling, machine design, ecosystems, navigation, neural networks, predator-prey interaction, gene expression, and production theory.
Overview.
Control theory is
Control systems may be thought of as having four functions: measure, compare, compute and correct. These four functions are completed by five elements: detector, transducer, transmitter, controller and final control element. The measuring function is completed by the detector, transducer and transmitter. In practical applications these three elements are typically contained in one unit. A standard example of a measuring unit is a resistance thermometer. The compare and compute functions are completed within the controller, which may be implemented electronically by proportional control, a PI controller, PID controller, bistable, hysteretic control or programmable logic controller. Older controller units have been mechanical, as in a centrifugal governor or a carburetor. The correct function is completed with a final control element. The final control element changes an input or output in the control system that affects the manipulated or controlled variable.
An example.
An example of a control system is a car's cruise control, which is a device designed to maintain vehicle speed at a constant "desired" or "reference" speed provided by the driver. The "controller" is the cruise control, the "plant" is the car, and the "system" is the car and the cruise control. The system output is the car's speed, and the control itself is the engine's throttle position which determines how much power the engine delivers.
A primitive way to implement cruise control is simply to lock the throttle position when the driver engages cruise control. However, if the cruise control is engaged on a stretch of flat road, then the car will travel slower going uphill and faster when going downhill. This type of controller is called an "open-loop controller" because there is no feedback; no measurement of the system output (the car's speed) is used to alter the control (the throttle position.) As a result, the controller cannot compensate for changes acting on the car, like a change in the slope of the road.
In a "closed-loop control system", data from a sensor monitoring the car's speed (the system output) enters a controller which continuously subtracts the quantity representing the speed from the reference quantity representing the desired speed. The difference, called the error, determines the throttle position (the control). The result is to match the car's speed to the reference speed (maintain the desired system output). Now, when the car goes uphill, the difference between the input (the sensed speed) and the reference continuously determines the throttle position. As the sensed speed drops below the reference, the difference increases, the throttle opens, and engine power increases, speeding up the vehicle. In this way, the controller dynamically counteracts changes to the car's speed. The central idea of these control systems is the "feedback loop", the controller affects the system output, which in turn is measured and fed back to the controller.
Classification.
Linear versus nonlinear control theory.
The field of control theory can be divided into two branches:
Frequency domain versus time domain.
Mathematical techniques for analyzing and designing control systems fall into two different categories:
SISO vs MIMO.
Control systems can be divided into different categories depending on the number of inputs and outputs.
History.
Although control systems of various types date back to antiquity, a more formal analysis of the field began with a dynamics analysis of the centrifugal governor, conducted by the physicist James Clerk Maxwell in 1868, entitled "On Governors". This described and analyzed the phenomenon of self-oscillation, in which lags in the system may lead to overcompensation and unstable behavior. This generated a flurry of interest in the topic, during which Maxwell's classmate, Edward John Routh, abstracted Maxwell's results for the general class of linear systems. Independently, Adolf Hurwitz analyzed system stability using differential equations in 1877, resulting in what is now known as the Routh–Hurwitz theorem.
A notable application of dynamic control was in the area of manned flight. The Wright brothers made their first successful test flights on December 17, 1903 and were distinguished by their ability to control their flights for substantial periods (more so than the ability to produce lift from an airfoil, which was known). Continuous, reliable control of the airplane was necessary for flights lasting longer than a few seconds.
By World War II, control theory was an important part of fire-control systems, guidance systems and electronics.
Sometimes, mechanical methods are used to improve the stability of systems. For example, ship stabilizers are fins mounted beneath the waterline and emerging laterally. In contemporary vessels, they may be gyroscopically controlled active fins, which have the capacity to change their angle of attack to counteract roll caused by wind or waves acting on the ship.
The Sidewinder missile uses small control surfaces placed at the rear of the missile with spinning disks on their outer surfaces and these are known as rollerons. Airflow over the disks spins them to a high speed. If the missile starts to roll, the gyroscopic force of the disks drives the control surface into the airflow, cancelling the motion. Thus, the Sidewinder team replaced a potentially complex control system with a simple mechanical solution.
The Space Race also depended on accurate spacecraft control, and control theory has also seen an increasing use in fields such as economics.
People in systems and control.
Many active and historical figures made significant contribution to control theory including
Classical control theory.
To overcome the limitations of the open-loop controller, control theory introduces feedback.
A closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is "fed back" as input to the process, closing the loop.
Closed-loop controllers have the following advantages over open-loop controllers:
In some systems, closed-loop and open-loop control are used simultaneously. In such systems, the open-loop control is termed feedforward and serves to further improve reference tracking performance.
A common closed-loop controller architecture is the PID controller.
Closed-loop transfer function.
The output of the system "y(t)" is fed back through a sensor measurement "F" to the reference value "r(t)". The controller "C" then takes the error "e" (difference) between the reference and the output to change the inputs "u" to the system under control "P". This is shown in the figure. This kind of controller is a closed-loop controller or feedback controller.
This is called a single-input-single-output ("SISO") control system; "MIMO" (i.e., Multi-Input-Multi-Output) systems, with more than one input/output, are common. In such cases variables are represented through vectors instead of simple scalar values. For some distributed parameter systems the vectors may be infinite-dimensional (typically functions).
If we assume the controller "C", the plant "P", and the sensor "F" are linear and time-invariant (i.e., elements of their transfer function "C(s)", "P(s)", and "F(s)" do not depend on time), the systems above can be analysed using the Laplace transform on the variables. This gives the following relations:
Solving for "Y"("s") in terms of "R"("s") gives
The expression formula_5 is referred to as the "closed-loop transfer function" of the system. The numerator is the forward (open-loop) gain from "r" to "y", and the denominator is one plus the gain in going around the feedback loop, the so-called loop gain. If formula_6, i.e., it has a large norm with each value of "s", and if formula_7, then "Y(s)" is approximately equal to "R(s)" and the output closely tracks the reference input.
PID controller.
The PID controller is probably the most-used feedback control design. "PID" is an initialism for "Proportional-Integral-Derivative", referring to the three terms operating on the error signal to produce a control signal. If "u(t)" is the control signal sent to the system, "y(t)" is the measured output and "r(t)" is the desired output, and tracking error formula_8, a PID controller has the general form
The desired closed loop dynamics is obtained by adjusting the three parameters formula_10, formula_11 and formula_12, often iteratively by "tuning" and without specific knowledge of a plant model. Stability can often be ensured using only the proportional term. The integral term permits the rejection of a step disturbance (often a striking specification in process control). The derivative term is used to provide damping or shaping of the response. PID controllers are the most well established class of control systems: however, they cannot be used in several more complicated cases, especially if MIMO systems are considered.
Applying Laplace transformation results in the transformed PID controller equation
with the PID controller transfer function
There exists a nice example of the closed-loop system discussed above. If we take
PID controller transfer function in series form
1st order filter in feedback loop
linear actuator with filtered input
and insert all this into expression for closed-loop transfer function H(s), then tuning is very easy: simply put
and get H(s) = 1 identically.
For practical PID controllers, a pure differentiator is neither physically realisable nor desirable due to amplification of noise and resonant modes in the system. Therefore, a phase-lead compensator type approach is used instead, or a differentiator with low-pass roll-off.
Modern control theory.
In contrast to the frequency domain analysis of the classical control theory, modern control theory utilizes the time-domain state space representation, a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations. To abstract from the number of inputs, outputs and states, the variables are expressed as vectors and the differential and algebraic equations are written in matrix form (the latter only being possible when the dynamical system is linear). The state space representation (also known as the "time-domain approach") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With inputs and outputs, we would otherwise have to write down Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions. "State space" refers to the space whose axes are the state variables. The state of the system can be represented as a point within that space.
Topics in control theory.
Stability.
The "stability" of a general dynamical system with no input can be described with Lyapunov stability criteria. 
For simplicity, the following descriptions focus on continuous-time and discrete-time linear systems.
Mathematically, this means that for a causal linear system to be stable all of the poles of its transfer function must have negative-real values, i.e. the real part of each pole must be less than zero. Practically speaking, stability requires that the transfer function complex poles reside
The difference between the two cases is simply due to the traditional method of plotting continuous time versus discrete time transfer functions. The continuous Laplace transform is in Cartesian coordinates where the formula_20 axis is the real axis and the discrete Z-transform is in circular coordinates where the formula_21 axis is the real axis.
When the appropriate conditions above are satisfied a system is said to be asymptotically stable; the variables of an asymptotically stable control system always decrease from their initial value and do not show permanent oscillations. Permanent oscillations occur when a pole has a real part exactly equal to zero (in the continuous time case) or a modulus equal to one (in the discrete time case). If a simply stable system response neither decays nor grows over time, and has no oscillations, it is marginally stable; in this case the system transfer function has non-repeated poles at complex plane origin (i.e. their real and complex component is zero in the continuous time case). Oscillations are present when poles with real part equal to zero have an imaginary part not equal to zero.
If a system in question has an impulse response of
then the Z-transform (see this example), is given by
which has a pole in formula_24 (zero imaginary part). This system is BIBO (asymptotically) stable since the pole is "inside" the unit circle.
However, if the impulse response was
then the Z-transform is
which has a pole at formula_27 and is not BIBO stable since the pole has a modulus strictly greater than one.
Numerous tools exist for the analysis of the poles of a system. These include graphical systems like the root locus, Bode plots or the Nyquist plots.
Mechanical changes can make equipment (and control systems) more stable. Sailors add ballast to improve the stability of ships. Cruise ships use antiroll fins that extend transversely from the side of the ship for perhaps 30 feet (10 m) and are continuously rotated about their axes to develop forces that oppose the roll.
Controllability and observability.
Controllability and observability are main issues in the analysis of a system before deciding the best control strategy to be applied, or whether it is even possible to control or stabilize the system. Controllability is related to the possibility of forcing the system into a particular state by using an appropriate control signal. If a state is not controllable, then no signal will ever be able to control the state. If a state is not controllable, but its dynamics are stable, then the state is termed "stabilizable". Observability instead is related to the possibility of "observing", through output measurements, the state of a system. If a state is not observable, the controller will never be able to determine the behaviour of an unobservable state and hence cannot use it to stabilize the system. However, similar to the stabilizability condition above, if a state cannot be observed it might still be detectable.
From a geometrical point of view, looking at the states of each variable of the system to be controlled, every "bad" state of these variables must be controllable and observable to ensure a good behaviour in the closed-loop system. That is, if one of the eigenvalues of the system is not both controllable and observable, this part of the dynamics will remain untouched in the closed-loop system. If such an eigenvalue is not stable, the dynamics of this eigenvalue will be present in the closed-loop system which therefore will be unstable. Unobservable poles are not present in the transfer function realization of a state-space representation, which is why sometimes the latter is preferred in dynamical systems analysis.
Solutions to problems of uncontrollable or unobservable system include adding actuators and sensors.
Control specification.
Several different control strategies have been devised in the past years. These vary from extremely general ones (PID controller), to others devoted to very particular classes of systems (especially robotics or aircraft cruise control).
A control problem can have several specifications. Stability, of course, is always present. The controller must ensure that the closed-loop system is stable, regardless of the open-loop stability. A poor choice of controller can even worsen the stability of the open-loop system, which must normally be avoided. Sometimes it would be desired to obtain particular dynamics in the closed loop: i.e. that the poles have formula_28, where formula_29 is a fixed value strictly greater than zero, instead of simply asking that formula_30.
Another typical specification is the rejection of a step disturbance; including an integrator in the open-loop chain (i.e. directly before the system under control) easily achieves this. Other classes of disturbances need different types of sub-systems to be included.
Other "classical" control theory specifications regard the time-response of the closed-loop system. These include the rise time (the time needed by the control system to reach the desired value after a perturbation), peak overshoot (the highest value reached by the response before reaching the desired value) and others (settling time, quarter-decay). Frequency domain specifications are usually related to robustness (see after).
Modern performance assessments use some variation of integrated tracking error (IAE,ISA,CQI).
Model identification and robustness.
A control system must always have some robustness property. A robust controller is such that its properties do not change much if applied to a system slightly different from the mathematical one used for its synthesis. This specification is important, as no real physical system truly behaves like the series of differential equations used to represent it mathematically. Typically a simpler mathematical model is chosen in order to simplify calculations, otherwise the true system dynamics can be so complicated that a complete model is impossible.
The process of determining the equations that govern the model's dynamics is called system identification. This can be done off-line: for example, executing a series of measures from which to calculate an approximated mathematical model, typically its transfer function or matrix. Such identification from the output, however, cannot take account of unobservable dynamics. Sometimes the model is built directly starting from known physical equations, for example, in the case of a system we know that formula_31. Even assuming that a "complete" model is used in designing the controller, all the parameters included in these equations (called "nominal parameters") are never known with absolute precision; the control system will have to behave correctly even when connected to physical system with true parameter values away from nominal.
Some advanced control techniques include an "on-line" identification process (see later). The parameters of the model are calculated ("identified") while the controller itself is running. In this way, if a drastic variation of the parameters ensues, for example, if the robot's arm releases a weight, the controller will adjust itself consequently in order to ensure the correct performance.
Analysis of the robustness of a SISO (single input single output) control system can be performed in the frequency domain, considering the system's transfer function and using Nyquist and Bode diagrams. Topics include gain and phase margin and amplitude margin. For MIMO (multi input multi output) and, in general, more complicated control systems one must consider the theoretical results devised for each control technique (see next section). I.e., if particular robustness qualities are needed, the engineer must shift his attention to a control technique by including them in its properties.
A particular robustness issue is the requirement for a control system to perform properly in the presence of input and state constraints. In the physical world every signal is limited. It could happen that a controller will send control signals that cannot be followed by the physical system, for example, trying to rotate a valve at excessive speed. This can produce undesired behavior of the closed-loop system, or even damage or break actuators or other subsystems. Specific control techniques are available to solve the problem: model predictive control (see later), and anti-wind up systems. The latter consists of an additional control block that ensures that the control signal never exceeds a given threshold.
System classifications.
Linear systems control.
For MIMO systems, pole placement can be performed mathematically using a state space representation of the open-loop system and calculating a feedback matrix assigning poles in the desired positions. In complicated systems this can require computer-assisted calculation capabilities, and cannot always ensure robustness. Furthermore, all system states are not in general measured and so observers must be included and incorporated in pole placement design.
Nonlinear systems control.
Processes in industries like robotics and the aerospace industry typically have strong nonlinear dynamics. In control theory it is sometimes possible to linearize such classes of systems and apply linear techniques, but in many cases it can be necessary to devise from scratch theories permitting control of nonlinear systems. These, e.g., feedback linearization, backstepping, sliding mode control, trajectory linearization control normally take advantage of results based on Lyapunov's theory. Differential geometry has been widely used as a tool for generalizing well-known linear control concepts to the non-linear case, as well as showing the subtleties that make it a more challenging problem. Control theory has also been used to decipher the neural mechanism that directs cognitive states.
Decentralized systems control.
When the system is controlled by multiple controllers, the problem is one of decentralized control. Decentralization is helpful in many ways, for instance, it helps control systems to operate over a larger geographical area. The agents in decentralized control systems can interact using communication channels and coordinate their actions.
Deterministic and stochastic systems control.
A stochastic control problem is one in which the evolution of the state variables is subjected to random shocks from outside the system. A deterministic control problem is not subject to external random shocks.
Main control strategies.
Every control system must guarantee first the stability of the closed-loop behavior. For linear systems, this can be obtained by directly placing the poles. Non-linear control systems use specific theories (normally based on Aleksandr Lyapunov's Theory) to ensure stability without regard to the inner dynamics of the system. The possibility to fulfill different specifications varies from the model considered and the control strategy chosen.
Applications.
Optimal control theory has been applied for optimal development of oil and gas reservoirs.
Further reading.
For Chemical Engineering

</doc>
<doc id="7042" url="https://en.wikipedia.org/wiki?curid=7042" title="Cracking joints">
Cracking joints

Joint manipulation in humans can produce a sharp cracking or popping sound. This occurs during deliberate knuckle-cracking, and it is possible to crack many other joints, such as those in the back and neck vertebrae, hips, wrists, elbows, shoulders, toes, ankles, knees, jaws, feet, sternum, and the Achilles tendon area. The mechanism that produces the cracking sound was until recently not known.
According to a folk belief, the popping of joints, especially knuckles, leads to arthritis or other joint problems. However, medical research has so far failed to conclusively demonstrate any connection between knuckle cracking and long-term joint problems.
Causes.
The physical mechanism that causes the cracking sound as a result of bending, twisting, or compressing joints is uncertain. Suggested causes include:
There were several theories to explain the cracking of joints. Synovial fluid cavitation has some evidence to support it. When a spinal manipulation is performed, the applied force separates the articular surfaces of a fully encapsulated synovial joint, which in turn creates a reduction in pressure within the joint cavity. In this low-pressure environment, some of the gases that are dissolved in the synovial fluid (which are naturally found in all bodily fluids) leave the solution, making a bubble, or cavity, which rapidly collapses upon itself, resulting in a "clicking" sound. The contents of the resultant gas bubble are thought to be mainly carbon dioxide. The effects of this process will remain for a period of time known as the "refractory period," during which the joint cannot be "re-cracked," which lasts about twenty minutes, while the gases are slowly reabsorbed into the synovial fluid. There is some evidence that ligament laxity may be associated with an increased tendency to cavitate.
However, recent evidence demonstrates that the cracking sound is produced when the bubble within the joint is formed, not when it collapses.
The snapping of tendons or scar tissue over a prominence (as in snapping hip syndrome) can also generate a loud snapping or popping sound.
Effects.
The common claim that cracking one's knuckles causes arthritis appears unsupported. A study published in 2011 examined the hand radiographs of 215 people (aged 50 to 89) and compared the joints of those who regularly cracked their knuckles to those who did not. The study concluded that knuckle-cracking did not cause hand osteoarthritis, no matter how many years or how often a person cracked their knuckles. An earlier study also concluded that there was no increased preponderance of arthritis of the hand of chronic knuckle-crackers; however, habitual knuckle-crackers were more likely to have hand swelling and lowered grip strength. Habitual knuckle-cracking was associated with manual labour, biting of the nails, smoking, and drinking alcohol and was suggested to result in functional hand impairment. This early study has been criticized for not taking into consideration the possibility of confounding factors, such as whether the ability to crack one's knuckles is associated with impaired hand functioning rather than being a cause of it.
There are many ways people have learned to crack the joints of their fingers, one of the most common ways is to put pressure on the joint between the metacarpals and the proximal phalanges.
Medical doctor Donald Unger cracked the knuckles of his left hand every day for more than sixty years, but he did not crack the knuckles of his right hand. No arthritis or other ailments formed in either hand, earning him the 2009 Ig Nobel Prize in Medicine, a parody of the Nobel Prize.

</doc>
<doc id="7043" url="https://en.wikipedia.org/wiki?curid=7043" title="Chemical formula">
Chemical formula

A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using a single line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, commas and "plus" (+) and "minus" (−) signs. These are limited to a single typographic line of symbols, which may include subscripts and superscripts. A chemical formula is not a chemical name, and it contains no words. Although a chemical formula may imply certain simple chemical structures, it is not the same as a full chemical structural formula. Chemical formulas can fully specify the structure of only the simplest of molecules and chemical substances, and are generally more limited in power than are chemical names and structural formulas.
The simplest types of chemical formulas are called "empirical formulas", which use letters and numbers indicating the numerical "proportions" of atoms of each type. "Molecular formulas" indicate the simple "numbers" of each type of atom in a molecule, with no information on structure. For example, the empirical formula for glucose is CH2O (twice as many hydrogen atoms as carbon and oxygen), while its molecular formula is C6H12O6 (12 hydrogen atoms, six carbon and oxygen atoms).
Sometimes a chemical formula is complicated by being written as a condensed formula (or condensed molecular formula, occasionally called a "semi-structural formula"), which conveys additional information about the particular ways in which the atoms are chemically bonded together, either in covalent bonds, ionic bonds, or various combinations of these types. This is possible if the relevant bonding is easy to show in one dimension. An example is the condensed molecular/chemical formula for ethanol, which is CH3-CH2-OH or CH3CH2OH. However, even a condensed chemical formula is necessarily limited in its ability to show complex bonding relationships between atoms, especially atoms that have bonds to four or more different substituents.
Since a chemical formula must be expressed as a single line of chemical element symbols, it often cannot be as informative as a true structural formula, which is a graphical representation of the spatial relationship between atoms in chemical compounds (see for example the figure for butane structural and chemical formulas, at right). For reasons of structural complexity, there is no condensed chemical formula (or semi-structural formula) that specifies glucose (and there exist many different molecules, for example fructose and mannose, have the same molecular formula C6H12O6 as glucose). Linear equivalent chemical "names" exist that can and do specify any complex structural formula (see chemical nomenclature), but such names must use many terms (words), rather than the simple element symbols, numbers, and simple typographical symbols that define a chemical formula.
Chemical formulas may be used in chemical equations to describe chemical reactions and other chemical transformations, such as the dissolving of ionic compounds into solution. While, as noted, chemical formulas do not have the full power of structural formulas to show chemical relationships between atoms, they are sufficient to keep track of numbers of atoms and numbers of electrical charges in chemical reactions, thus balancing chemical equations so that these equations can be used in chemical problems involving conservation of atoms, and conservation of electric charge.
Overview.
A chemical formula identifies each constituent element by its chemical symbol and indicates the proportionate number of atoms of each element. In empirical formulas, these proportions begin with a key element and then assign numbers of atoms of the other elements in the compound, as ratios to the key element. For molecular compounds, these ratio numbers can all be expressed as whole numbers. For example, the empirical formula of ethanol may be written C2H6O because the molecules of ethanol all contain two carbon atoms, six hydrogen atoms, and one oxygen atom. Some types of ionic compounds, however, cannot be written with entirely whole-number empirical formulas. An example is boron carbide, whose formula of CBn is a variable non-whole number ratio with n ranging from over 4 to more than 6.5.
When the chemical compound of the formula consists of simple molecules, chemical formulas often employ ways to suggest the structure of the molecule. These types of formulas are variously known as molecular formulas and condensed formulas. A molecular formula enumerates the number of atoms to reflect those in the molecule, so that the molecular formula for glucose is C6H12O6 rather than the glucose empirical formula, which is CH2O. However, except for very simple substances, molecular chemical formulas lack needed structural information, and are ambiguous.
For simple molecules, a condensed (or semi-structural) formula is a type of chemical formula that may fully imply a correct structural formula. For example, ethanol may be represented by the condensed chemical formula CH3CH2OH, and dimethyl ether by the condensed formula CH3OCH3. These two molecules have the same empirical and molecular formulas (C2H6O), but may be differentiated by the condensed formulas shown, which are sufficient to represent the full structure of these simple organic compounds.
Condensed chemical formulas may also be used to represent ionic compounds that do not exist as discrete molecules, but nonetheless do contain covalently bound clusters within them. These polyatomic ions are groups of atoms that are covalently bound together and have an overall ionic charge, such as the sulfate ion. Each polyatomic ion in a compound is written individually in order to illustrate the separate groupings. For example, the compound dichlorine hexoxide has an empirical formula , and molecular formula , but in liquid or solid forms, this compound is more correctly shown by an ionic condensed formula , which illustrates that this compound consists of ions and ions. In such cases, the condensed formula only need be complex enough to show at least one of each ionic species.
Chemical formulas as described here are distinct from the far more complex chemical systematic names that are used in various systems of chemical nomenclature. For example, one systematic name for glucose is (2"R",3"S",4"R",5"R")-2,3,4,5,6-pentahydroxyhexanal. This name, interpreted by the rules behind it, fully specifies glucose's structural formula, but the name is not a chemical formula as usually understood, and uses terms and words not used in chemical formulas. Such names, unlike basic formulas, may be able to represent full structural formulas without graphs.
Empirical formula.
In chemistry, the empirical formula of a chemical is a simple expression of the relative number of each type of atom or ratio of the elements in the compound. Empirical formulas are the standard for ionic compounds, such as , and for macromolecules, such as . An empirical formula makes no reference to isomerism, structure, or absolute number of atoms. The term empirical refers to the process of elemental analysis, a technique of analytical chemistry used to determine the relative percent composition of a pure chemical substance by element.
For example, hexane has a molecular formula of , or structurally , implying that it has a chain structure of 6 carbon atoms, and 14 hydrogen atoms. However, the empirical formula for hexane is . Likewise the empirical formula for hydrogen peroxide, , is simply HO expressing the 1:1 ratio of component elements. Formaldehyde and acetic acid have the same empirical formula, . This is the actual chemical formula for formaldehyde, but acetic acid has double the number of atoms.
Hill System.
The Hill system writes empirical formulas in such order that the C, H symbols are in front. Then the number of all other chemical elements follow in alphabetical order. When the formula contains "no carbon", all the elements, including hydrogen, are ordered alphabetically. This deterministic system enables straightforward sorting and searching of compounds.
Molecular formula.
Molecular formulas indicate the simple numbers of each type of atom in a molecule of a molecular substance. They are the same as empirical formulas for molecules that only have one atom of a particular type, but otherwise have larger numbers. An example of the difference is the empirical formula for glucose, which is CH2O ("ratio" 1:2:1), while its molecular formula is C6H12O6 ("number of atoms" 6:12:6). For water, both formulas are H2O. A molecular formula provides more information about a molecule than its empirical formula, but is more difficult to establish.
Condensed formula.
In organic chemistry implying molecular geometry and structural formulas.
The connectivity of a molecule often has a strong influence on its physical and chemical properties and behavior. Two molecules composed of the same numbers of the same types of atoms (i.e. a pair of isomers) might have completely different chemical and/or physical properties if the atoms are connected differently or in different positions. In such cases, a structural formula is useful, as it illustrates which atoms are bonded to which other ones. From the connectivity, it is often possible to deduce the approximate shape of the molecule.
A condensed chemical formula may represent the types and spatial arrangement of bonds in a simple chemical substance, though it does not necessarily specify isomers or complex structures. For example, ethane consists of two carbon atoms single-bonded to each other, with each carbon atom having three hydrogen atoms bonded to it. Its chemical formula can be rendered as CH3CH3. In ethylene there is a double bond between the carbon atoms (and thus each carbon only has two hydrogens), therefore the chemical formula may be written: CH2CH2, and the fact that there is a double bond between the carbons is implicit because carbon has a valence of four. However, a more explicit method is to write H2C=CH2 or less commonly H2C::CH2. The two lines (or two pairs of dots) indicate that a double bond connects the atoms on either side of them.
A triple bond may be expressed with three lines or pairs of dots, and if there may be ambiguity, a single line or pair of dots may be used to indicate a single bond.
Molecules with multiple functional groups that are the same may be expressed by enclosing the repeated group in round brackets. For example, isobutane may be written (CH3)3CH. This condensed structural formula implies a different connectivity from other molecules that can be formed using the same atoms in the same proportions (isomers). The formula (CH3)3CH implies a central carbon atom connected to one hydrogen atom and three CH3 groups. The same number of atoms of each element (10 hydrogens and 4 carbons, or C4H10) may be used to make a straight chain molecule, "n"-butane: CH3CH2CH2CH3.
Law of Composition.
In any given chemical compound, the elements always combine in the same proportion with each other. This is the law of constant composition.
The law of constant composition says that, in any particular chemical compound, all samples of that compound will be made up of the same elements in the same proportion or ratio. For example, any water molecule is always made up of two hydrogen atoms and one oxygen atom in a 2:1 ratio. If we look at the relative masses of oxygen and hydrogen in a water molecule, we see that 94% of the mass of a water molecule is accounted for by oxygen and the remaining 6% is the mass of hydrogen. This mass proportion will be the same for any water molecule.
Chemical names in answer to limitations of chemical formulas.
The alkene called but-2-ene has two isomers, which the chemical formula CH3CH=CHCH3 does not identify. The relative position of the two methyl groups must be indicated by additional notation denoting whether the methyl groups are on the same side of the double bond ("cis" or "Z") or on the opposite sides from each other ("trans" or "E"). Such extra symbols violate the rules for chemical formulas, and begin to enter the territory of more complex naming systems.
As noted above, in order to represent the full structural formulas of many complex organic and inorganic compounds, chemical nomenclature may be needed which goes well beyond the available resources used above in simple condensed formulas. See IUPAC nomenclature of organic chemistry and IUPAC nomenclature of inorganic chemistry 2005 for examples. In addition, linear naming systems such as International Chemical Identifier (InChI) allow a computer to construct a structural formula, and simplified molecular-input line-entry system (SMILES) allows a more human-readable ASCII input. However, all these nomenclature systems go beyond the standards of chemical formulas, and technically are chemical naming systems, not formula systems.
Polymers in condensed formulas.
For polymers in condensed chemical formulas, parentheses are placed around the repeating unit. For example, a hydrocarbon molecule that is described as CH3(CH2)50CH3, is a molecule with fifty repeating units. If the number of repeating units is unknown or variable, the letter "n" may be used to indicate this formula: CH3(CH2)"n"CH3.
Ions in condensed formulas.
For ions, the charge on a particular atom may be denoted with a right-hand superscript. For example, Na+, or Cu2+. The total charge on a charged molecule or a polyatomic ion may also be shown in this way. For example: H3O+ or SO42−.
For more complex ions, brackets [ ] are often used to enclose the ionic formula, as in which is found in compounds such as . Parentheses ( ) can be nested inside brackets to indicate a repeating unit, as in Here (NH3)6 indicates that the ion contains six [ammine|NH3 groups, and [ ] encloses the entire formula of the ion with charge +3. 
Isotopes.
Although isotopes are more relevant to nuclear chemistry or stable isotope chemistry than to conventional chemistry, different isotopes may be indicated with a prefixed superscript in a chemical formula. For example, the phosphate ion containing radioactive phosphorus-32 is 32PO43−. Also a study involving stable isotope ratios might include the molecule 18O16O.
A left-hand subscript is sometimes used redundantly to indicate the atomic number. For example, 8O2 for dioxygen, and 168O2 for the most abundant isotopic species of dioxygen. This is convenient when writing equations for nuclear reactions, in order to show the balance of charge more clearly.
Trapped atoms.
The @ symbol (at sign) indicates an atom or molecule trapped inside a cage but not chemically bound to it. For example, a buckminsterfullerene (C60) with an atom (M) would simply be represented as MC60 regardless of whether M was inside the fullerene without chemical bonding or outside, bound to one of the carbon atoms. Using the @ symbol, this would be denoted M@C60 if M was inside the carbon network. A non-fullerene example is [As@Ni12As20]3−, an ion in which one As atom is trapped in a cage formed by the other 32 atoms.
This notation was proposed in 1991 with the discovery of fullerene cages (endohedral fullerenes), which can trap atoms such as La to form, for example, La@C60 or La@C82. The choice of the symbol has been explained by the authors as being concise, readily printed and transmitted electronically (the at sign is included in ASCII, which most modern character encoding schemes are based on), and the visual aspects suggesting the structure of an endohedral fullerene.
Non-stoichiometric chemical formulas.
Chemical formulas most often use integers for each element. However, there is a class of compounds, called non-stoichiometric compounds, that cannot be represented by small integers. Such a formula might be written using decimal fractions, as in Fe0.95O, or it might include a variable part represented by a letter, as in Fe1–xO, where x is normally much less than 1.
General forms for organic compounds.
A chemical formula used for a series of compounds that differ from each other by a constant unit is called general formula. Such a series is called the homologous series, while its members are called homologs.
For example, alcohols may be represented by: C"n"H(2n + 1)OH ("n" ≥ 1)

</doc>
<doc id="7044" url="https://en.wikipedia.org/wiki?curid=7044" title="Beetle">
Beetle

Beetles are a group of insects that form the order Coleoptera. The word "coleoptera" is from the Greek , "koleos", meaning "sheath"; and , "pteron", meaning "wing", thus "sheathed wing", because most beetles have two pairs of wings, the front pair, the "elytra", being hardened and thickened into a shell-like protection for the rear pair and the beetle's abdomen. The order contains more species than any other order, constituting almost 25% of all known life-forms. About 40% of all described insect species are beetles (about 400,000 species), and new species are discovered frequently. The largest taxonomic family, the Curculionidae (the weevils or snout beetles), also belongs to this order.
The diversity of beetles is very wide-ranging. They are found in almost all types of habitats, but are not known to occur in the sea or in the polar regions. They interact with their ecosystems in several ways. They often feed on plants and fungi, break down animal and plant debris, and eat other invertebrates. Some species are prey of various animals including birds and mammals. Certain species are agricultural pests, such as the Colorado potato beetle "Leptinotarsa decemlineata", the boll weevil "Anthonomus grandis", the red flour beetle "Tribolium castaneum", and the mungbean or cowpea beetle "Callosobruchus maculatus", while other species of beetles are important controls of agricultural pests. For example, beetles in the family Coccinellidae ("ladybirds" or "ladybugs") consume aphids, scale insects, thrips, and other plant-sucking insects that damage crops.
Species in the order Coleoptera are generally characterized by a particularly hard exoskeleton and hard forewings (elytra, singular elytron). These elytra distinguish beetles from most other insect species, except for a few species of Hemiptera. The beetle's exoskeleton is made up of numerous plates called sclerites, separated by thin sutures. This design creates the armored defenses of the beetle while maintaining flexibility. The general anatomy of a beetle is quite uniform, although specific organs and appendages may vary greatly in appearance and function between the many families in the order. Like all insects, beetles' bodies are divided into three sections: the head, the thorax, and the abdomen. Coleopteran internal morphology is similar to other insects, although there are several examples of novelty. Such examples include species of water beetle which use air bubbles in order to dive under the water, and can remain submerged thanks to passive diffusion as oxygen moves from the water into the bubble. 
Beetles are endopterygotes, which means that they undergo complete metamorphosis, a biological process by which an animal physically develops after birth or hatching, undergoing a series of conspicuous and relatively abrupt change in their body structure. Coleopteran species have an extremely intricate behavior when mating, using such methods as pheromones for communication to locate potential mates. Males may fight for females using very elongated mandibles, causing a strong divergence between males and females in sexual dimorphism.
Etymology.
Coleoptera comes from the Greek "koleopteros", literally "sheath-wing", from "koleos" meaning "sheath", and "pteron", meaning "wing". The name was given to the group by Aristotle for their elytra, hardened shield-like forewings. The English name "beetle" comes from the Old English word "bitela", literally meaning small biter, deriving from the word "bitel", which means biting. This word is related to the word "bītan" (to bite) The name also derives from the Middle English word "betylle" from Old English "bitula" (also meaning to bite). Another Old English name for beetle is "ceafor", chafer, used in names such as cockchafer, from the Proto-Germanic *kabraz- (compare German Käfer). These terms have been in use since the 12th century. In addition to names including the words "beetle" or "chafer", many groups of Coleoptera have common names such as fireflies, June bugs, ladybugs and weevils.
Taxonomy.
The Coleopterans include more species than any other order, constituting almost 25% of all known types of animal life forms. About 450,000 species of beetles occur – representing about 40% of all known insects. Such a large number of species poses special problems for classification, with some families consisting of thousands of species and needing further division into subfamilies and tribes. This immense number of species allegedly led evolutionary biologist J. B. S. Haldane to quip, when some theologians asked him what could be inferred about the mind of the Creator from the works of His Creation, that God displayed "an inordinate fondness for beetles".
Polyphaga is the largest suborder, containing more than 300,000 described species in more than 170 families, including rove beetles (Staphylinidae), scarab beetles (Scarabaeidae), blister beetles (Meloidae), stag beetles (Lucanidae) and true weevils (Curculionidae). These beetles can be identified by the presence of cervical sclerites (hardened parts of the head used as points of attachment for muscles) absent in the other suborders.
The suborder Adephaga contains about 10 families of largely predatory beetles, includes ground beetles (Carabidae), Dytiscidae and whirligig beetles (Gyrinidae). In these beetles, the testes are tubular and the first abdominal sternum (a plate of the exoskeleton) is divided by the hind coxae (the basal joints of the beetle's legs).
Archostemata contains four families of mainly wood-eating beetles, including reticulated beetles (Cupedidae) and the telephone-pole beetle.
Myxophaga contains about 100 described species in four families, mostly very small, including Hydroscaphidae and the genus "Sphaerius".
Evolution.
The oldest known insect that unequivocally resembles species of Coleoptera date back to the Lower Permian (270 mya), though it instead has 13-segmented antennae, elytra with more fully developed venation and more irregular longitudinal ribbing, and an abdomen and ovipositor extending beyond the apex of the elytra. At the end of the Permian, the biggest mass extinction in history took place, collectively called the Permian–Triassic extinction event (P-Tr): 30% of all insect species became extinct; however, it is the only mass extinction of insects in Earth's history until today.
Due to the P-Tr extinction, the fossil record of insects only includes beetles from the Lower Triassic (). Around this time, during the Late Triassic, mycetophagous, or fungus-feeding species (e.g. Cupedidae) appear in the fossil record. In the stages of the Upper Triassic, representatives of the algophagous, or algae-feeding species (e.g. Triaplidae and Hydrophilidae) begin to appear, as well as predatory water beetles. The first primitive weevils appear (e.g. Obrienidae), as well as the first representatives of the rove beetles (e.g. Staphylinidae), which show no marked difference in morphology compared to recent species.
During the Jurassic (), a dramatic increase in the known diversity of family-level Coleoptera occurred, including the development and growth of carnivorous and herbivorous species. Species of the superfamily Chrysomeloidea are believed to have developed around the same time, which include a wide array of plant hosts ranging from cycads and conifers, to angiosperms. Close to the Upper Jurassic, the portion of the Cupedidae decreased, but at the same time the diversity of the early plant-eating, or phytophagous species increased. Most of the recent phytophagous species of Coleoptera feed on flowering plants or angiosperms. The increase in diversity of the angiosperms is also believed to have influenced the diversity of the phytophagous species, which doubled during the Middle Jurassic. However, doubts have been raised recently, since the increase of the number of beetle families during the Cretaceous does not correlate with the increase of the number of angiosperm species. Also around the same time, numerous primitive weevils (e.g. Curculionoidea) and click beetles (e.g. Elateroidea) appeared. Also, the first jewel beetles (e.g. Buprestidae) are present, but they were rather rare until the Cretaceous. The first scarab beetles appeared around this time, but they were not coprophagous (feeding upon fecal matter), instead presumably feeding upon the rotting wood with the help of fungus; they are an early example of a mutualistic relationship.
The Cretaceous included the initiation of the most recent round of southern landmass fragmentation, via the opening of the southern Atlantic ocean and the isolation of New Zealand, while South America, Antarctica, and Australia grew more distant. During the Cretaceous, the diversity of Cupedidae and Archostemata decreased considerably. Predatory ground beetles (Carabidae) and rove beetles (Staphylinidae) began to distribute into different patterns; whereas the Carabidae predominantly occurred in the warm regions, the Staphylinidae and click beetles (Elateridae) preferred many areas with temperate climates. Likewise, predatory species of Cleroidea and Cucujoidea hunted their prey under the bark of trees together with the jewel beetles (Buprestidae). The jewel beetles' diversity increased rapidly during the Cretaceous, as they were the primary consumers of wood, while longhorn beetles (Cerambycidae) were rather rare, and their diversity increased only towards the end of the Upper Cretaceous. The first coprophagous beetles have been recorded from the Upper Cretaceous, and are believed to have lived on the excrement of herbivorous dinosaurs, but discussion is still ongoing as to whether the beetles were always tied to mammals during their development. Also, the first species with an adaption of both larvae and adults to the aquatic lifestyle are found. Whirligig beetles (Gyrinidae) were moderately diverse, although other early beetles (e.g. Dytiscidae) were less, with the most widespread being the species of Coptoclavidae, which preyed on aquatic fly larvae.
Between the Paleogene and the Neogene is when today's beetles developed. During this time, the continents began to be located closer to where they are today. Around , the land bridge between South America and North America was formed, and the fauna exchange between Asia and North America started. Though many recent genera and species already existed during the Miocene, their distribution differed considerably from today's.
Fossil record.
A 2007 study based on DNA of living beetles and maps of likely beetle evolution indicated beetles may have originated during the Lower Permian, up to 285 million years ago. In 2009, a fossil beetle was described from the Pennsylvanian of Mazon Creek, Illinois, pushing the origin of the beetles to an earlier date, . Fossils from this time have been found in Asia and Europe, for instance in the red slate fossil beds of Niedermoschel near Mainz, Germany. Further fossils have been found in Obora, Czech Republic and Tshekarda in the Ural mountains, Russia. However, there are only a few fossils from North America before the middle Permian, although both Asia and North America had been united to Euramerica. The first discoveries from North America made in the Wellington formation of Oklahoma were published in 2005 and 2008.
As a consequence of the Permian–Triassic extinction event, the fossil record of insects is scant, including beetles from the Lower Triassic. However, a few exceptions are noted, as in Eastern Europe; at the Babiy Kamen site in the Kuznetsk Basin, numerous beetle fossils were discovered, even entire specimen of the infraorders Archostemata (e.g. Ademosynidae, Schizocoleidae), Adephaga (e.., Triaplidae, Trachypachidae) and Polyphaga (e.g. Hydrophilidae, Byrrhidae, Elateroidea) and in nearly a perfectly preserved condition. However, species from the families Cupedidae and Schizophoroidae are not present at this site, whereas they dominate at other fossil sites from the Lower Triassic. Further records are known from Khey-Yaga, Russia, in the Korotaikha Basin. There are many important sites from the Jurassic, with more than 150 important sites with beetle fossils, the majority being situated in Eastern Europe and North Asia. In North America and especially in South America and Africa, the number of sites from that time period is smaller, and the sites have not been exhaustively investigated yet. Outstanding fossil sites include Solnhofen in Upper Bavaria, Germany, Karatau in South Kazakhstan, the Yixian formation in Liaoning, North China, as well as the Jiulongshan formation and further fossil sites in Mongolia. In North America there are only a few sites with fossil records of insects from the Jurassic, namely the shell limestone deposits in the Hartford basin, the Deerfield basin and the Newark basin.
A large number of important fossil sites worldwide contain beetles from the Cretaceous. Most are located in Europe and Asia and belong to the temperate climate zone during the Cretaceous. A few of the fossil sites mentioned in the chapter Jurassic also shed some light on the early Cretaceous beetle fauna (for example, the Yixian formation in Liaoning, North China). Further important sites from the Lower Cretaceous include the Crato fossil beds in the Araripe basin in the Ceará, North Brazil, as well as overlying Santana formation, with the latter was situated near the paleoequator, or the position of the earth's equator in the geologic past as defined for a specific geologic period. In Spain, important sites are located near Montsec and Las Hoyas. In Australia, the Koonwarra fossil beds of the Korumburra group, South Gippsland, Victoria, are noteworthy. Important fossil sites from the Upper Cretaceous include Kzyl-Dzhar in South Kazakhstan and Arkagala in Russia.
Phylogeny.
The superficial consistency of most beetles' morphology, in particular their possession of elytra, has long suggested that Coleoptera is a monophyletic group. Growing evidence indicates this is unjustified, there being arguments for example, in favor of allocating the current suborder Adephaga their own order, or very likely even more than one. The suborders diverged in the Permian and Triassic. Their phylogenetic relationship is uncertain, with the most popular hypothesis being that Polyphaga and Myxophaga are most closely related, with Adephaga as the sister group to those two, and Archostemata as sister to the other three collectively. Although six other competing hypotheses are noted, the other most widely discussed one has Myxophaga as the sister group of all remaining beetles rather than just of Polyphaga. Evidence for a close relationship of the two suborders, Polyphaga and Myxophaga, includes the shared reduction in the number of larval leg articles. Adephaga is further considered as sister to Myxophaga and Polyphaga, based on their completely sclerotized elytra, reduced number of crossveins in the hind wings, and the folded (as opposed to rolled) hind wings of those three suborders.
Recent cladistic analysis of some of the structural characteristics supports the Polyphaga and Myxophaga hypothesis. The membership of the clade Coleoptera is not in dispute, with the exception of the twisted-wing parasites, Strepsiptera. These odd insects have been regarded as related to the beetle families Rhipiphoridae and Meloidae, with which they share first-instar larvae that are active, host-seeking triungulins and later-instar larvae that are endoparasites of other insects, or the sister group of beetles, or more distantly related to insects. Recent molecular genetic analysis strongly supports the hypothesis that Strepsiptera is the sister group to beetles.
Distribution and diversity.
Beetles are by far the largest order of insects, with 350,000–400,000 species in four suborders (Adephaga, Archostemata, Myxophaga, and Polyphaga), making up about 40% of all insect species described, and about 30% of all animals. Though classification at the family level is a bit unstable, about 500 families and subfamilies are recognized. One of the first proposed estimates of the total number of beetle species on the planet is based on field data rather than on catalog numbers. The technique used for this original estimate, possibly as many as 12 million species, was criticized, and was later revised, with estimates of 850,000–4,000,000 species proposed. Some 70–95% of all beetle species, depending on the estimate, remain undescribed. The beetle fauna is not equally well known in all parts of the world. For example, the known beetle diversity of Australia is estimated at 23,000 species in 3265 genera and 121 families. This is slightly lower than reported for North America, a land mass of similar size with 25,160 species in 3526 genera and 129 families. While other predictions show there could be as many as 28,000 species in North America, including those currently undescribed, a realistic estimate of the little-studied Australian beetle fauna's true diversity could vary from 80,000 to 100,000.
Coleoptera are found in nearly all natural habitats, including freshwater and marine habitats, everywhere vegetative foliage is found, from trees and their bark to flowers, leaves, and underground near roots- even inside plants in galls, in every plant tissue, including dead or decaying ones.
External morphology.
Beetles are generally characterized by a particularly hard exoskeleton and hard forewings (elytra). The beetle's exoskeleton is made up of numerous plates, called sclerites, separated by thin sutures. This design provides armored defenses while maintaining flexibility. The general anatomy of a beetle is quite uniform, although specific organs and appendages may vary greatly in appearance and function between the many families in the order. Like all insects, beetles' bodies are divided into three sections: the head, the thorax, and the abdomen.
Head.
The head, having mouthparts projecting forward or sometimes downturned, is usually heavily sclerotized and varies in size. The eyes are compound and may display remarkable adaptability, as in the case of whirligig beetles (family Gyrinidae), where they are split to allow a view both above and below the waterline. Other species also have divided eyes – some longhorn beetles (family Cerambycidae) and weevils – while many have eyes that are notched to some degree. A few beetle genera also possess ocelli, which are small, simple eyes usually situated farther back on the head (on the vertex).
Beetles' antennae are primarily organs of smell, but may also be used to feel a beetle's environment physically. They may also be used in some families during mating, or among a few beetle species for defence. Antennae vary greatly in form within the Coleoptera, but are often similar within any given family. Males and females sometimes have different antennal forms. Antennae may be ( and are subforms of clavate, or clubbed antennae), , , , , or .
Beetles have mouthparts similar to those of grasshoppers. Of these parts, the most commonly known are probably the mandibles, which appear as large pincers on the front of some beetles. The mandibles are a pair of hard, often tooth-like structures that move horizontally to grasp, crush, or cut food or enemies (see defence, below). Two pairs of finger-like appendages, the maxillary and labial palpi, are found around the mouth in most beetles, serving to move food into the mouth. In many species, the mandibles are sexually dimorphic, with the males' enlarged enormously compared with those of females of the same species.
Thorax.
The thorax is segmented into the two discernible parts, the pro- and pterathorax. The pterathorax is the fused meso- and metathorax, which are commonly separated in other insect species, although flexibly articulate from the prothorax. When viewed from below, the thorax is that part from which all three pairs of legs and both pairs of wings arise. The abdomen is everything posterior to the thorax. When viewed from above, most beetles appear to have three clear sections, but this is deceptive: on the beetle's upper surface, the middle "section" is a hard plate called the pronotum, which is only the front part of the thorax; the back part of the thorax is concealed by the beetle's wings. This further segmentation is usually best seen on the abdomen.
Extremities.
The multisegmented legs end in two to five small segments called tarsi. Like many other insect orders, beetles bear claws, usually one pair, on the end of the last tarsal segment of each leg. While most beetles use their legs for walking, legs may be variously modified and adapted for other uses. Among aquatic families – Dytiscidae, Haliplidae, many species of Hydrophilidae and others – the legs, most notably the last pair, are modified for swimming and often bear rows of long hairs to aid this purpose. Other beetles have fossorial legs that are widened and often spined for digging. Species with such adaptations are found among the scarabs, ground beetles, and clown beetles (family Histeridae). The hind legs of some beetles, such as flea beetles (within Chrysomelidae) and flea weevils (within Curculionidae), are enlarged and designed for jumping.
Wings.
The elytra are connected to the pterathorax, so named because it is where the wings are connected ("pteron" meaning "wing" in Greek). The elytra are not used for flight, but tend to cover the hind part of the body and protect the second pair of wings ("alae"). They must be raised to move the hind flight wings. A beetle's flight wings are crossed with veins and are folded after landing, often along these veins, and stored below the elytra. A fold ("jugum") of the membrane at the base of each wing is a characteristic feature. In some beetles, the ability to fly has been lost. These include some ground beetles (family Carabidae) and some "true weevils" (family Curculionidae), but also desert- and cave-dwelling species of other families. Many have the two elytra fused together, forming a solid shield over the abdomen. In a few families, both the ability to fly and the elytra have been lost, with the best known example being the glow-worms of the family Phengodidae, in which the females are larviform throughout their lives.
Abdomen.
The abdomen is the section behind the metathorax, made up of a series of rings, each with a hole for breathing and respiration, called a spiracle, composing three different segmented sclerites: the tergum, pleura, and the sternum. The tergum in almost all species is membranous, or usually soft and concealed by the wings and elytra when not in flight. The pleura are usually small or hidden in some species, with each pleuron having a single spiracle. The sternum is the most widely visible part of the abdomen, being a more or less sclerotized segment. The abdomen itself does not have any appendages, but some (for example, Mordellidae) have articulating sternal lobes.
Internal morphology.
Digestive system.
The digestive system of beetles is primarily based on plants, upon which they, for the most part, feed, with mostly the anterior midgut performing digestion, although in predatory species (for example Carabidae), most digestion occurs in the crop by means of midgut enzymes. In Elateridae species, the predatory larvae defecate enzymes on their prey, with digestion being extraorally. The alimentary canal basically consists of a short, narrow pharynx, a widened expansion, the crop, and a poorly developed gizzard. After is the midgut, that varies in dimensions between species, with a large amount of cecum, with a hindgut, with varying lengths. Typically, four to six Malpighian tubules occur.
Nervous system.
The nervous system in beetles contains all the types found in insects, varying between different species, from three thoracic and seven or eight abdominal ganglia which can be distinguished to that in which all the thoracic and abdominal ganglia are fused to form a composite structure.
Respiratory system.
Like most insects, beetles inhale oxygen and exhale carbon dioxide via a tracheal system. Air enters the body through spiracles, and circulates within the haemocoel in a system of tracheae and tracheoles, through the walls of which the relevant gases can diffuse appropriately.
Diving beetles, such as the Dytiscidae, carry a bubble of air with them when they dive. Such a bubble may be contained under the elytra or against the body by specialized hydrophobic hairs. The bubble covers at least some of the spiracles, thereby permitting the oxygen to enter the tracheae.
The function of the bubble is not so much as to contain a store of air, but to act as a physical gill. The air that it traps is in contact with oxygenated water, so as the animal's consumption depletes the oxygen in the bubble, more oxygen can diffuse in to replenish it. Carbon dioxide is more soluble in water than either oxygen or nitrogen, so it readily diffuses out faster than in. Nitrogen is the most plentiful gas in the bubble, and the least soluble, so it constitutes a relatively static component of the bubble and acts as a stable medium for respiratory gases to accumulate in and pass through. Occasional visits to the surface are sufficient for the beetle to re-establish the constitution of the bubble.
Circulatory system.
Like other insects, beetles have open circulatory systems, based on hemolymph rather than blood. Also as in other insects, a segmented tube-like heart is attached to the dorsal wall of the hemocoel. It has paired inlets or "ostia" at intervals down its length, and circulates the hemolymph from the main cavity of the haemocoel and out through the anterior cavity in the head.
Specialized organs.
Different glands specialize for different pheromones produced for finding mates. Pheromones from species of Rutelinea are produced from epithelial cells lining the inner surface of the apical abdominal segments; amino acid-based pheromones of Melolonthinae are produced from eversible glands on the abdominal apex. Other species produce different types of pheromones. Dermestids produce esters, and species of Elateridae produce fatty acid-derived aldehydes and acetates. For means of finding a mate also, fireflies (Lampyridae) use modified fat body cells with transparent surfaces backed with reflective uric acid crystals to biosynthetically produce light, or bioluminescence. The light produce is highly efficient, as it is produced by oxidation of luciferin by enzymes (luciferases) in the presence of adenosine triphosphate (ATP) and oxygen, producing oxyluciferin, carbon dioxide, and light.
A notable number of species have developed special glands to produce chemicals for deterring predators (see Defense and predation). The ground beetle's (of Carabidae) defensive glands, located at the posterior, produce a variety of hydrocarbons, aldehydes, phenols, quinones, esters, and acids released from an opening at the end of the abdomen. African carabid beetles (for example, "Anthia" and "Thermophilum" – Thermophilum generally included within "Anthia") employ the same chemicals as ants: formic acid. Bombardier beetles have well-developed, like other carabid beetles, pygidial glands that empty from the lateral edges of the intersegment membranes between the seventh and eighth abdominal segments. The gland is made of two containing chambers. The first holds hydroquinones and hydrogen peroxide, with the second holding just hydrogen peroxide plus catalases. These chemicals mix and result in an explosive ejection, forming temperatures of around , with the breakdown of hydroquinone to H2 + O2 + quinone, with the O2 propelling the excretion.
Tympanal organs or hearing organs, which is a membrane (tympanum) stretched across a frame backed by an air sac and associated sensory neurons, are described in two families. Several species of the genus "Cicindela" (Cicindelidae) have ears on the dorsal surfaces of their first abdominal segments beneath the wings; two tribes in the subfamily Dynastinae (Scarabaeidae) have ears just beneath their pronotal shields or neck membranes. The ears of both families are sensitive to ultrasonic frequencies, with strong evidence indicating they function to detect the presence of bats by their ultrasonic echolocation. Though beetles constitute a large order and live in a variety of niches, examples of hearing are surprisingly lacking amongst species, though likely most simply remain undiscovered.
Reproduction and development.
Beetles are members of the superorder Endopterygota, and accordingly most of them undergo complete metamorphosis. The typical form of metamorphosis in beetles passes through four main stages: the egg, the larva, the pupa, and the imago or adult. The larvae are commonly called grubs and the pupa sometimes is called the chrysalis. In some species, the pupa may be enclosed in a cocoon constructed by the larva towards the end of its final instar. Going beyond "complete metamorphosis", however, some beetles, such as typical members of the families Meloidae and Rhipiphoridae, undergo hypermetamorphosis in which the first instar takes the form of a triungulin.
Mating.
Beetles may display extremely intricate behavior when mating. Pheromone communication is likely to be important in the location of a mate.
Different species use different chemicals for their pheromones. Some scarab beetles (for example, Rutelinae) utilize pheromones derived from fatty acid synthesis, while other scarab beetles use amino acids and terpenoid compounds (for example, Melolonthinae). Another way species of Coleoptera find mates is the use of biosynthesized light, or bioluminescence. This special form of a mating call is confined to fireflies (Lampyridae) by the use of abdominal light-producing organs. The males and females engage in complex dialogue before mating, identifying different species by differences in duration, flight patterns, composition, and intensity.
Before mating, males and females may engage in various forms of behavior. They may stridulate, or vibrate the objects they are on. In some species (for example, Meloidae), the male climbs onto the dorsum of the female and strokes his antennae on her head, palps, and antennae. In the genus "Eupompha" of said family, the male draws the antennae along his longitudinal vertex. They may not mate at all if they do not perform the precopulatory ritual.
Conflict can play a part in the mating rituals of species such as burying beetles (genus "Nicrophorus"), where conflicts between males and females rage until only one of each is left, thus ensuring reproduction by the strongest and fittest. Many male beetles are territorial and fiercely defend their small patches of territory from intruding males. In such species, the male often has horns on the head or thorax, making its body length greater than that of a female. Pairing is generally quick, but in some cases lasts for several hours. During pairing, sperm cells are transferred to the female to fertilize the egg.
Lifecycle.
Egg.
A single female may lay from several dozen to several thousand eggs during her lifetime. Eggs are usually laid according to the substrate on which the larvae feed upon hatching. Among others, they can be laid loose in the substrate (for example, flour beetle), laid in clumps on leaves (for example, Colorado potato beetle), individually attached (for example, mungbean beetle and other seed borers), or buried in the medium (for example, carrot weevil).
Parental care varies between species, ranging from the simple laying of eggs under a leaf to certain scarab beetles, which construct underground structures complete with a supply of dung to house and feed their young. Other beetles are leaf rollers, biting sections of leaves to cause them to curl inwards, then laying their eggs, thus protected, inside.
Larva.
The larva is usually the principal feeding stage of the beetle lifecycle. Larvae tend to feed voraciously once they emerge from their eggs. Some feed externally on plants, such as those of certain leaf beetles, while others feed within their food sources. Examples of internal feeders are most Buprestidae and longhorn beetles. The larvae of many beetle families are predatory like the adults (ground beetles, ladybirds, rove beetles). The larval period varies between species, but can be as long as several years. The larvae are highly varied amongst species, with well-developed and sclerotized heads, and have distinguishable thoracic and abdominal segments (usually the tenth, though sometimes the eighth or ninth).
Beetle larvae can be differentiated from other insect larvae by their hardened, often darkened heads, the presence of chewing mouthparts, and spiracles along the sides of their bodies. Like adult beetles, the larvae are varied in appearance, particularly between beetle families. Beetles with somewhat flattened, highly mobile larvae include the ground beetles, some rove beetles, and others; their larvae are described as campodeiform. Some beetle larvae resemble hardened worms with dark head capsules and minute legs. These are elateriform larvae, and are found in the click beetle (Elateridae) and darkling beetle (Tenebrionidae) families. Some elateriform larvae of click beetles are known as wireworms. Beetles in the Scarabaeoidea have short, thick larvae described as scarabaeiform, more commonly known as grubs.
All beetle larvae go through several instars, which are the developmental stages between each moult. In many species, the larvae simply increase in size with each successive instar as more food is consumed. In some cases, however, more dramatic changes occur. Among certain beetle families or genera, particularly those that exhibit parasitic lifestyles, the first instar (the planidium) is highly mobile to search out a host, while the following instars are more sedentary and remain on or within their host. This is known as hypermetamorphosis; examples include the blister beetles (family Meloidae) and some rove beetles, particularly those of the genus "Aleochara".
Pupa.
As with all endopterygotes, beetle larvae pupate, and from these pupae emerge fully formed, sexually mature adult beetles, or imagos. Adults have extremely variable lifespans, from weeks to years, depending on the species. In some species, the pupa may go through all four forms during its development, called hypermetamorphosis (for example, Meloidae). Pupae always have no mandibles (are adecticous). In most, the appendages are not attached to the pupae; ones that do have appendages are mostly obtect, and the rest are exarate.
Behavior.
Locomotion.
Aquatic beetles use several techniques for retaining air beneath the water's surface. Beetles of the family Dytiscidae hold air between the abdomen and the elytra when diving. Hydrophilidae have hairs on their under surface that retain a layer of air against their bodies. Adult crawling water beetles use both their elytra and their hind coxae (the basal segment of the back legs) in air retention, while whirligig beetles simply carry an air bubble down with them whenever they dive.
The elytra allow beetles and weevils to both fly and move through confined spaces, doing so by folding the delicate wings under the elytra while not flying, and folding their wings out just before take off. The unfolding and folding of the wings is operated by muscles attached to the wing base; as long as the tension on the radial and cubital veins remains, the wings remain straight. In day-flying species (for example, Buprestidae, Scarabaeidae), flight does not include large amounts of lifting of the elytra, having the metathorac wings extended under the lateral elytra margins.
Communication.
Beetles have a variety of ways to communicate, some of which include a sophisticated chemical language through the use of pheromones. From the host tree, mountain pine beetles have many forms of communication. They can emit both an aggregative pheromone and an anti-aggregative pheromone. The aggregative pheromone attracts other beetles to the tree, and the anti-aggregative pheromone neutralizes the aggregative pheromone. This helps to avoid the harmful effects of having too many beetles on one tree competing for resources. The mountain pine beetle can also stridulate to communicate, or rub body parts together to create sound, having a "scraper" on their abdomens that they rub against a grooved surface on the underside of their left wing cover to create a sound that is not audible to humans. Once the female beetles have arrived on a suitable pine tree host, they begin to stridulate and produce aggregative pheromones to attract other unmated males and females. New females arrive and do the same as they land and bore into the tree. As the males arrive, they enter the galleries that the females have tunneled, and begin to stridulate to let the females know they have arrived, and to also warn others that the female in that gallery is taken. At this point, the female stops producing aggregative pheromones and starts producing anti-aggregative pheromone to deter more beetles from coming.
Since species of Coleoptera use environmental stimuli to communicate, they are affected by the climate. Microclimates, such as wind or temperature, can disturb the use of pheromones; wind would blow the pheromones while they travel through the air. Stridulating can be interrupted when the stimulus is vibrated by something else.
Parental care.
Among insects, parental care is very uncommon, only found in a few species. Some beetles also display this unique social behavior. One theory states parental care is necessary for the survival of the larvae, protecting them from adverse environmental conditions and predators. One species, a rover beetle ("Bledius spectabilis") displays both causes for parental care: physical and biotic environmental factors. Said species lives in salt marshes, so the eggs and/or larvae are endangered by the rising tide. The maternal beetle patrols the eggs and larvae and applies the appropriate burrowing behavior to keep them from flooding and from asphyxiating. Another advantage is that the mother protects the eggs and larvae from the predatory carabid beetle "Dicheirotrichus gustavi" and from the parasitoid wasp "Barycnemis blediator". Up to 15% of larvae are killed by this parasitoid wasp, being only protected by maternal beetles in their dens.
Some species of dung beetle also display a form of parental care. Dung beetles collect animal feces, or "dung", from which their name is derived, and roll it into a ball, sometimes being up to 50 times their own weight; albeit sometimes it is also used to store food. Usually it is the male that rolls the ball, with the female hitch-hiking or simply following behind. In some cases the male and the female roll together. When a spot with soft soil is found, they stop and bury the dung ball. They then mate underground. After the mating, one or both of them prepares the brooding ball. When the ball is finished, the female lays eggs inside it, a form of mass provisioning. Some species do not leave after this stage, but remain to safeguard their offspring.
Feeding.
Besides being abundant and varied, beetles are able to exploit the wide diversity of food sources available in their many habitats. Some are omnivores, eating both plants and animals. Other beetles are highly specialized in their diet. Many species of leaf beetles, longhorn beetles, and weevils are very host-specific, feeding on only a single species of plant. Ground beetles and rove beetles (family Staphylinidae), among others, are primarily carnivorous and catch and consume many other arthropods and small prey, such as earthworms and snails. While most predatory beetles are generalists, a few species have more specific prey requirements or preferences.
Decaying organic matter is a primary diet for many species. This can range from dung, which is consumed by coprophagous species (such as certain scarab beetles of the family Scarabaeidae), to dead animals, which are eaten by necrophagous species (such as the carrion beetles of the family Silphidae). Some of the beetles found within dung and carrion are in fact predatory. These include the clown beetles, preying on the larvae of coprophagous and necrophagous insects.
Ecology.
Defense and predation.
Beetles and their larvae have a variety of strategies to avoid being attacked by predators or parasitoids. These include camouflage, mimicry, toxicity, and active defense. Camouflage involves the use of coloration or shape to blend into the surrounding environment. This sort of protective coloration is common and widespread among beetle families, especially those that feed on wood or vegetation, such as many of the leaf beetles (family Chrysomelidae) or weevils. In some of these species, sculpturing or various colored scales or hairs cause the beetle to resemble bird dung or other inedible objects. Many of those that live in sandy environments blend in with the coloration of the substrate. The giant African longhorn beetle ("Petrognatha gigas") resembles the moss and bark of the tree it feeds on. Another defense that often uses color or shape to deceive potential enemies is mimicry. Some longhorn beetles (family Cerambycidae) bear a striking resemblance to wasps, which helps them avoid predation even though the beetles are in fact harmless. This defense is an example of Batesian mimicry and, together with other forms of mimicry and camouflage occurs widely in other beetle families, such as the Scarabaeidae. Beetles may combine their color mimicry with behavioral mimicry, acting like the wasps they already closely resemble. Many beetle species, including ladybirds, blister beetles, and lycid beetles can secrete distasteful or toxic substances to make them unpalatable or even poisonous. These same species are often aposematic, where bright or contrasting color patterns warn away potential predators; many beetles and other insects mimic these chemically protected species.
Chemical defense is another important defense found amongst species of Coleoptera, usually being advertised by bright colors. Others may utilize behaviors that would be done when releasing noxious chemicals (for example, Tenebrionidae). Chemical defense may serve purposes other than just protection from vertebrates, such as protection from a wide range of microbes, and repellents. Some species release chemicals in the form of a spray with surprising accuracy, such as ground beetles (Carabidae), may spray chemicals from their abdomen to repel predators. Some species take advantage of the plants from which they feed, and sequester the chemicals from the plant that would protect it and incorporate into their own defense. African carabid beetles (for example, "Anthia" and "Thermophilum") employ the same chemicals used by ants, while bombardier beetles have a their own unique separate gland, spraying potential predators from far distances.
Large ground beetles and longhorn beetles may defend themselves using strong mandibles, spines or horns to forcibly persuade a predator to seek out easier prey. Many species such as the rhinoceros beetle have large protrusions from their thorax and head, which can be used to defend themselves from predators. Many species of weevil that feed out in the open on leaves of plants react to attack by employing a "drop-off reflex". Some combine it with thanatosis, in which they close up their appendages and "play dead".
Parasitism.
Over 1000 species of beetles are known to be either parasitic, predatory, or commensals in the nests of ants.
A few species of beetles are actually ectoparasitic on mammals. One such species, "Platypsyllus castoris", parasitises beavers ("Castor" spp.). This beetle lives as a parasite both as a larva and as an adult, feeding on epidermal tissue and possibly on skin secretions and wound exudates. They are strikingly flattened dorsoventrally, no doubt as an adaptation for slipping between the beavers' hairs. They also are wingless and eyeless, as are many other ectoparasites.
Other parasitic beetles include those that are kleptoparasites of other invertebrates, such as the small hive beetle ("Aethina tumida") that infests honey bee hives. The larvae tunnel through comb towards stored honey or pollen, damaging or destroying cappings and comb in the process. Larvae defecate in honey and the honey becomes discolored from the feces, which causes fermentation and a frothiness in the honey; the honey develops a characteristic odor of decaying oranges. Damage and fermentation cause honey to run out of combs, destroying large amounts of it both in hives and sometimes also in honey extracting rooms. Heavy infestations cause bees to abscond; though the beetle is only a minor pest in Africa, beekeepers in other regions have reported the rapid collapse of even strong colonies.
Pollination.
Beetle-pollinated flowers are usually large, greenish or off-white in color, and heavily scented. Scents may be spicy, fruity, or similar to decaying organic material. Most beetle-pollinated flowers are flattened or dish-shaped, with pollen easily accessible, although they may include traps to keep the beetle longer. The plants' ovaries are usually well protected from the biting mouthparts of their pollinators. Beetles may be particularly important in some parts of the world such as semiarid areas of southern Africa and southern California and the montane grasslands of KwaZulu-Natal in South Africa.
Mutualism.
[[File:Ambrosia beetle life cycle.svg|upright=1.2|thumb|right|
1: The adult beetle burrows hole into wood and lays eggs, carrying fungal spores in its mycangia.<br>
2: The larva feeds on the fungus, which digest the wood, removing toxins: they mutually benefit.<br>
3: The larva pupates and then ecloses.]]
Amongst most orders of insects, mutualism is not common, but some examples occur in species of Coleoptera, such as the ambrosia beetle, the ambrosia fungus, and probably bacteria. The beetles excavate tunnels in dead trees in which they cultivate fungal gardens, their sole source of nutrition. After landing on a suitable tree, an ambrosia beetle excavates a tunnel in which it releases spores of its fungal symbiont. The fungus penetrates the plant's xylem tissue, digests it, and concentrates the nutrients on and near the surface of the beetle gallery, so the weevils and the fungus both benefit. The beetles cannot eat the wood due to toxins, and uses its relationship with fungi to help overcome its host tree defenses and to provide nutrition for their larvae. Chemically mediated by a bacterially produced polyunsaturated peroxide, this mutualistic relationship between the beetle and the fungus is coevolved.
Commensalism.
Pseudoscorpions are small arachnids with flat, pear-shaped bodies and pincers that resemble those of scorpions (only distant relatives), usually ranging from in length. Their small size allows them to hitch rides under the elytra of giant harlequin beetles to be dispersed over wide areas while simultaneously being protected from predators. They may also find mating partners as other individuals join them on the beetle. This would be a form of parasitism if the beetle were harmed in the process, but the beetle is, presumably, unaffected by the presence of the hitchhikers.
Eusociality.
"Austroplatypus incompertus" is eusocial, one of the few organisms outside Hymenoptera to do so, and the only species of Coleoptera.
Relationship to humans.
As pests.
About 75% of beetle species are phytophagous in both the larval and adult stages, and live in or on plants, wood, fungi, and a variety of stored products, including cereals, tobacco, and dried fruits. Because many of these plants are important for agriculture, forestry, and the household, beetles can be considered pests. Some of these species cause significant damage, such as the boll weevil, which feeds on cotton buds and flowers. The boll weevil crossed the Rio Grande near Brownsville, Texas, to enter the United States from Mexico around 1892, and had reached southeastern Alabama by 1915. By the mid-1920s, it had entered all cotton-growing regions in the US, traveling per year. It remains the most destructive cotton pest in North America. Mississippi State University has estimated, since the boll weevil entered the United States, it has cost cotton producers about $13 billion, and in recent times about $300 million per year. Many other species also have done extensive damage to plant populations, such as the bark beetle, elm leaf beetle and Asian longhorned beetle. The bark beetle, elm leaf beetle and Asian longhorned beetle, among other species, have been known to nest in elm trees. Bark beetles in particular carry Dutch elm disease as they move from infected breeding sites to feed on healthy elm trees, which in turn allows the Asian longhorned beetle to continue killing more elms. The spread of Dutch elm disease by the beetle has led to the devastation of elm trees in many parts of the Northern Hemisphere, notably in Europe and North America.
Situations in which a species has developed immunity to pesticides are worse, as in the case of the Colorado potato beetle, "Leptinotarsa decemlineata", which is a notorious pest of potato plants. Crops are destroyed and the beetle can only be treated by employing expensive pesticides, to many of which it has begun to develop resistance. Suitable hosts can include a number of plants from the potato family (Solanaceae), such as nightshade, tomato, eggplant and capsicum, as well as potatoes. The Colorado potato beetle has developed resistance to all major insecticide classes, although not every population is resistant to every chemical.
Pests do not only affect agriculture, but can also even affect houses, such as the death watch beetle. The death watch beetle, "Xestobium rufovillosum" (family Anobiidae), is of considerable importance as a pest of older wooden buildings in Great Britain. It attacks hardwoods such as oak and chestnut, always where some fungal decay has taken or is taking place. The actual introduction of the pest into buildings is thought to take place at the time of construction.
Other pest include the coconut hispine beetle, "Brontispa longissima", which feeds on young leaves and damages seedlings and mature coconut palms. On September 27, 2007, Philippines' Metro Manila and 26 provinces were quarantined due to having been infested with this pest (to save the $800-million Philippine coconut industry). The mountain pine beetle normally attacks mature or weakened lodgepole pine. It can be the most destructive insect pest of mature pine forests. The current infestation in British Columbia is the largest Canada has ever seen.
As beneficial resources.
Beetles are not only pests, but can also be beneficial, usually by controlling the populations of pests. One of the best, and widely known, examples are the ladybugs or ladybirds (family Coccinellidae). Both the larvae and adults are found feeding on aphid colonies. Other ladybugs feed on scale insects and mealybugs. If normal food sources are scarce, they may feed on small caterpillars, young plant bugs, or honeydew and nectar. Ground beetles (family Carabidae) are common predators of many different insects and other arthropods, including fly eggs, caterpillars, wireworms, and others.
Dung beetles (Scarabidae) have been successfully used to reduce the populations of pestilent flies and parasitic worms that breed in cattle dung. The beetles make the dung unavailable to breeding pests by quickly rolling and burying it in the soil, with the added effect of improving soil fertility, tilth, and nutrient cycling. The Australian Dung Beetle Project (1965–1985), led by Dr. George Bornemissza of the Commonwealth Scientific and Industrial Research Organisation, introduced species of dung beetle to Australia from South Africa and Europe, and effectively reduced the bush fly ("Musca vetustissima") population by 90%.
Dung beetles play a remarkable role in agriculture. By burying and consuming dung, they improve nutrient recycling and soil structure. They also protect livestock, such as cattle, by removing dung, which, if left, could provide habitat for pests such as flies. Therefore, many countries have introduced the creatures for the benefit of animal husbandry. In developing countries, the beetle is especially important as an adjunct for improving standards of hygiene. The American Institute of Biological Sciences reports that dung beetles save the United States cattle industry an estimated US$380 million annually through burying above-ground livestock feces.
Some beetles help in a professional setting, doing things that people cannot; those of the family Dermestidae are often used in taxidermy and preparation of scientific specimens to clean bones of remaining soft tissue. The beetle larvae are used to clean skulls because they do a thorough job of cleaning, and do not leave the tool marks that taxidermists' tools do. Another benefit is, with no traces of meat remaining and no emulsified fats in the bones, the trophy does not develop the unpleasant dead odor. Using the beetle larvae means that all cartilage is removed along with the flesh, leaving the bones spotless.
As food.
Insects are used as human food in 80% of the world's nations. Beetles are the most widely eaten insects. About 344 species are known to be used as food, usually eaten in the larval stage. The mealworm is the most commonly eaten beetle species. The larvae of the darkling beetle and the rhinoceros beetle are also commonly eaten.
In art.
Many beetles have beautiful and durable elytra that have been used as material in arts, with beetlewing the best example. Sometimes, they are also incorporated into ritual objects for their religious significance. Whole beetles, either as-is or encased in clear plastic, are also made into objects varying from cheap souvenirs such as key chains to expensive fine-art jewelry. In parts of Mexico, beetles of the genus "Zopherus" are made into living brooches by attaching costume jewelry and golden chains, which is made possible by the incredibly hard elytra and sedentary habits of the genus.
In ancient cultures.
Some beetles were prominent in ancient cultures, the most prominent being the dung beetle in Ancient Egypt. Several species of dung beetle, especially the "sacred scarab" "Scarabaeus sacer", were revered by the ancient Egyptians. The hieroglyphic image of the beetle may have had existential, fictional, or ontologic significance. Images of the scarab in bone, ivory, stone, Egyptian faience, and precious metals are known from the Sixth Dynasty and up to the period of Roman rule. The scarab was of prime significance in the funerary cult of ancient Egypt.
The scarab was linked to Khepri, the god of the rising sun, from the supposed resemblance of the dung ball rolled by the beetle to the rolling of the sun by the god. Plutarch wrote:
In contrast to funerary contexts, some of ancient Egypt's neighbors adopted the scarab motif for seals of varying types. The best-known of these are the Judean LMLK seals (eight of 21 designs contained scarab beetles), which were used exclusively to stamp impressions on storage jars during the reign of Hezekiah.
In modern cultures.
Beetles still play roles in culture. One example is in insect fighting for entertainment and gambling. This sport exploits the territorial behavior and mating competition of certain species of large beetles. In the Chiang Mai district of northern Thailand, male "Xylotrupes" rhinoceros beetles are caught in the wild and trained for fighting. Females are held inside a log to stimulate the fighting males with their pheromones.

</doc>
<doc id="7045" url="https://en.wikipedia.org/wiki?curid=7045" title="Concorde">
Concorde

Aérospatiale/BAC Concorde is a turbojet-powered supersonic passenger jet that was operated until 2003. It had a maximum speed over twice the speed of sound at Mach 2.04 ( at cruise altitude), with seating for 92 to 128 passengers. First flown in 1969, Concorde entered service in 1976 and continued flying for the next 27 years. It is one of only two supersonic transports to have been operated commercially; the other is the Soviet-built Tupolev Tu-144, which was operated for a much shorter period of time.
Concorde was jointly developed and manufactured by Aérospatiale and the British Aircraft Corporation (BAC) under an Anglo-French treaty. Concorde's name, meaning harmony or union, reflects the co-operation on the project between the United Kingdom and France. In the UK, any or all of the type are known simply as "Concorde", without an article. Twenty aircraft were built including six prototypes and development aircraft. Air France (AF) and British Airways (BA) each received seven aircraft. The research and development failed to make a profit and the two then state-owned airlines bought the aircraft at a huge discount.
Among other destinations, Concorde flew regular transatlantic flights from London Heathrow and Paris Charles de Gaulle Airport to New York-JFK, Washington Dulles and Barbados; it flew these routes in less than half the time of other airliners. Over time, the aircraft became profitable when it found a customer base willing to pay for flights on what was for most of its career the fastest commercial airliner in the world. The aircraft is regarded by many as an aviation icon and an engineering marvel while it was also criticised for being uneconomical, and lacking a credible market.
Concorde was retired in 2003 due to a general downturn in the commercial aviation industry after the type's only crash in 2000, the September 11 attacks in 2001, and a decision by Airbus, the successor to Aérospatiale and BAC, to discontinue maintenance support.
Development.
Early studies.
The origins of the Concorde project date to the early 1950s, when Arnold Hall, director of the Royal Aircraft Establishment (RAE) asked Morien Morgan to form a committee to study the supersonic transport (SST) concept. The group met for the first time in February 1954 and delivered their first report in April 1955.
At the time it was known that the drag at supersonic speeds was strongly related to the span of the wing. This led to the use of very short-span, very thin rectangular wings such as those seen on the control surfaces of many missiles, or in aircraft like the Lockheed F-104 Starfighter or the Avro 730 that the team studied. The team outlined a baseline configuration that looked like an enlarged Avro 730.
This same short span produced very little lift at low speed, which resulted in extremely long take-off runs and frighteningly high landing speeds. In an SST design, this would have required enormous engine power to lift off from existing runways, and to provide the fuel needed, "some horribly large aeroplanes" resulted. Based on this, the group considered the concept of an SST unfeasible, and instead suggested continued low-level studies into supersonic aerodynamics.
Slender deltas.
Soon after, Johanna Weber and Dietrich Küchemann at the RAE published a series of reports on a new wing planform, known in the UK as the "slender delta" concept. The team, including Eric Maskell, worked with the fact that delta wings can produce strong vortexes on their upper surfaces at high angles of attack. The vortex will lower the air pressure and cause lift to be greatly increased. This effect had been noticed earlier, notably by Chuck Yeager in the Convair XF-92, but its qualities had not been fully appreciated. Weber suggested that this was no mere curiosity, and the effect could be deliberately used to improve low speed performance.
Küchemann's and Weber's papers changed the entire nature of supersonic design almost overnight. Although the delta had already been used on aircraft prior to this point, these designs used planforms that were not much different from a swept wing of the same span. Weber noted that the lift from the vortex was increased by the length of the wing it had to operate over, which suggested that the effect would be maximised by extending the wing along the fuselage as far as possible. Such a layout would still have good supersonic performance inherent to the short span, while also offering reasonable take-off and landing speeds using vortex generation. The only downside to such a design is that the aircraft would have to take off and land very "nose high" to generate the required vortex lift, which led to questions about the low speed handling qualities of such a design. It would also need to have long landing gear to produce the required angles while still on the runway.
Küchemann presented the idea at a meeting where Morgan was also present. Test pilot Eric Brown recalls Morgan's reaction to the presentation, saying that he immediately seized on it as the solution to the SST problem. Brown considers this moment as being the true birth of the Concorde project.
Supersonic Transport Advisory Committee.
On 1 October 1956 the Ministry of Supply asked Morgan to form a new study group, the "Supersonic Transport Advisory Committee" ("STAC"), with the explicit goal of developing a practical SST design and finding industry partners to build it. At the very first meeting, on 5 November 1956, the decision was made to fund the development of a test bed aircraft to examine the low speed performance of the slender delta, a contract that eventually produced the Handley Page HP.115. This aircraft would ultimately demonstrate safe control at speeds as low as 69 mph, about ⅓ that of the Starfighter.
STAC stated that an SST would have economic performance similar to existing subsonic types. Although they would burn more fuel in cruise, they would be able to fly more sorties in a given period of time, so fewer aircraft would be needed to service a particular route. This would remain economically advantageous as long as fuel represented a small percentage of operational costs, as it did at the time. STAC suggested that two designs naturally fell out of their work, a transatlantic model flying at about Mach 2, and a shorter-range version flying at perhaps Mach 1.2. Morgan suggested that a 150 passenger transatlantic SST would cost about £75 to £90 million to develop to production, and be in service in 1970. The smaller 100 passenger short-range version would cost perhaps £50 to £80 million, and be ready for service in 1968. To meet this schedule, development would need to begin in 1960, with production contracts let in 1962. Morgan strongly suggested that the US was already involved in a similar project, and that if the UK failed to respond it would be locked out of an airliner market that he believed would be dominated by SST aircraft.
In 1959, a study contract was awarded to Hawker Siddeley and Bristol for preliminary designs based on the slender delta concept, which developed as the HSA.1000 and Bristol 198. Armstrong Whitworth also responded with an internal design, the "M-Wing", for the lower-speed shorter-range category. Even at this early time, both the STAC group and the government were looking for partners to develop the designs. In September 1959, Hawker approached Lockheed, and after the creation of British Aircraft Corporation in 1960, the former Bristol team immediately started talks with Boeing, General Dynamics, Douglas Aircraft and Sud Aviation.
Ogee planform selected.
Küchemann and others at the RAE continued their work on the slender delta throughout, considering three basic shapes; the classic straight-edge delta, the "gothic delta" that was rounded outwards to appear like a gothic arch, and the "ogival wing" that was compound-rounded into the shape of an ogee. Each of these planforms had their own advantages and disadvantages in terms of aerodynamics. As they worked with these shapes, a practical concern grew to become so important that it forced selection of one of these designs.
Generally one wants to have the wing's centre of pressure (CP, or "lift point") close to the aircraft's centre of gravity (CG, or "balance point") to reduce the amount of control force required to pitch the aircraft. As the aircraft layout changes during the design phase, it is common for the CG to move fore or aft. With a normal wing design this can be addressed by moving the wing slightly fore or aft to account for this. With a delta wing running most of the length of the fuselage, this was no longer easy; moving the wing would leave it in front of the nose or behind the tail. Studying the various layouts in terms of CG changes, both during design and changes due to fuel use during flight, the ogee planform immediately came to the fore.
While the wing planform was evolving, so was the basic SST concept. Bristol's original Type 198 was a small design with an almost pure slender delta wing, but evolved into the larger Type 223 with an ogival wing and canards as well.
Partnership with Sud.
By this time similar political and economic concerns in France had led to their own SST plans. In the late 1950s the government requested designs from both the government-owned Sud and Nord, as well as Dassault. All three returned designs based on Küchemann and Weber's slender delta; Nord suggested a ramjet powered design flying at Mach 3, the other two were jet powered Mach 2 designs that were similar to each other. Of the three, the Sud Aviation Super-Caravelle won the design contest with a medium-range design deliberately sized to avoid competition with transatlantic US designs they assumed were already on the drawing board.
As soon as the design was complete, in April 1960, Pierre Satre, the company's technical director, was sent to Bristol to discuss a partnership. Bristol was surprised to find that the Sud team had designed a very similar aircraft after considering the SST problem and coming to the very same conclusions as the Bristol and STAC teams in terms of economics. It was later revealed that the original STAC report, marked "For UK Eyes Only", had secretly been passed to the French to win political favour. Sud made minor changes to the paper, and presented it as their own work.
Unsurprisingly, the two teams found much to agree on. The French had no modern large jet engines, and had already concluded they would buy a British design anyway (as they had on the earlier subsonic Caravelle). As neither company had experience in the use of high-heat metals for airframes, a maximum speed of around Mach 2 was selected so aluminium could be used – above this speed the friction with the air warms the metal so much that aluminium begins to soften. This lower speed would also speed development and allow their design to fly before the Americans. Finally, everyone involved agreed that Küchemann's ogee shaped wing was the right one.
The only disagreements were over the size and range. The UK team was still focused on a 150 passenger design serving transatlantic routes, while the French were deliberately avoiding these. However, this proved not to be the barrier it might seem; common components could be used in both designs, with the shorter range version using a clipped fuselage and four engines, the longer one with a stretched fuselage and six engines, leaving only the wing to be extensively re-designed. The teams continued to meet through 1961, and by this time it was clear that the two aircraft would be considerably more similar in spite of different range and seating arrangements. A single design emerged that differed primarily in fuel load. More powerful Bristol Siddeley Olympus engines, being developed for the TSR-2, allowed either design to be powered by only four engines.
Cabinet response, treaty.
While the development teams met, French Minister of Public Works and Transport Robert Buron was meeting with the UK Minister of Aviation Peter Thorneycroft, and Thorneycroft soon revealed to the cabinet that the French were much more serious about a partnership than any of the US companies. The various US companies had proved uninterested in such a venture, likely due to the belief that the government would be funding development and would frown on any partnership with a European company, and the risk of "giving away" US technological leadership to a European partner.
When the STAC plans were presented to the UK cabinet, a very negative reaction resulted. The economic considerations were considered highly questionable, especially as these were based on development costs, now estimated to be £150 million, which were repeatedly overrun in the industry. The Treasury Ministry in particular presented a very negative view, suggesting that there was no way the project would have any positive financial returns for the government, especially in light that "the industry's past record of over-optimistic estimating (including the recent history of the TSR.2) suggests that it would be prudent to consider the £150 million to turn out much too low."
This concern led to an independent review of the project by the Committee on Civil Scientific Research and Development, which met on topic between July and September 1962. The Committee ultimately rejected the economic arguments, including considerations of supporting the industry made by Thorneycroft. Their report in October stated that it was unlikely there would be any direct positive economic outcome, but that the project should still be considered for the simple reason that everyone else was going supersonic, and they were concerned they would be locked out of future markets. Conversely, it appeared the project would not be likely to significantly impact other, more important, research efforts.
After considerable argument, the decision to proceed ultimately fell to an unlikely political expediency. At the time, the UK was pressing for admission to the European Common Market, which was being controlled by Charles de Gaulle who felt the UK's Special Relationship with the US made them unacceptable in a pan-European group. Cabinet felt that signing a deal with Sud would pave the way for Common Market entry, and this became the main deciding reason for moving ahead with the deal. It was this belief that had led the original STAC documents being leaked to the French. However, De Gaulle spoke of the European origin of the design, and continued to block the UK's entry into the Common Market.
The development project was negotiated as an international treaty between the two countries rather than a commercial agreement between companies and included a clause, originally asked for by the UK, imposing heavy penalties for cancellation. A draft treaty was signed on 29 November 1962.
Naming.
Reflecting the treaty between the British and French governments that led to Concorde's construction, the name "Concorde" is from the French word "concorde" (), which has an English equivalent, "concord". Both words mean "agreement", "harmony" or "union". The name was officially changed to "Concord" by Harold Macmillan in response to a perceived slight by Charles de Gaulle. At the French roll-out in Toulouse in late 1967, the British Government Minister for Technology, Tony Benn, announced that he would change the spelling back to "Concorde". This created a nationalist uproar that died down when Benn stated that the suffixed 'e' represented "Excellence, England, Europe and Entente (Cordiale)." In his memoirs, he recounts a tale of a letter from an irate Scotsman claiming: "talk about 'E' for England, but part of it is made in Scotland." Given Scotland's contribution of providing the nose cone for the aircraft, Benn replied, "[It was also 'E' for 'Écosse' (the French name for Scotland)  – and I might have added 'e' for extravagance and 'e' for escalation as well!"
Concorde also acquired an unusual nomenclature for an aircraft. In common usage in the United Kingdom, the type is known as "Concorde" without an article, rather than "the Concorde" or "a Concorde".
Sales efforts.
At first, the new consortium intended to produce one long-range and one short-range version. However, prospective customers showed no interest in the short-range version and it was dropped.
An advertisement covering two full pages, promoting Concorde, ran in 29 May 1967 issue of "Aviation Week & Space Technology". The advertisement predicted a market for 350 aircraft by 1980 and boasted of Concorde's head start over the United States' SST project.
The consortium secured orders (i.e., non-binding options) for over 100 of the long-range version from the major airlines of the day: Pan Am, BOAC, and Air France were the launch customers, with six Concordes each. Other airlines in the order book included Panair do Brasil, Continental Airlines, Japan Airlines, Lufthansa, American Airlines, United Airlines, Air India, Air Canada, Braniff, Singapore Airlines, Iran Air, Olympic Airways, Qantas, CAAC, Middle East Airlines, and TWA. At the time of the first flight the options list contained 74 options from 16 airlines:
Testing.
The design work was supported by a preceding research programme studying the flight characteristics of low ratio delta wings. A supersonic Fairey Delta 2 was modified and – as the BAC 221 – was used for flight tests of the high speed flight envelope, the Handley Page HP.115 also provided valuable information on low speed performance.
Construction of two prototypes began in February 1965: 001, built by Aérospatiale at Toulouse, and 002, by BAC at Filton, Bristol. Concorde 001 made its first test flight from Toulouse on 2 March 1969, piloted by André Turcat, and first went supersonic on 1 October. The first UK-built Concorde flew from Filton to RAF Fairford on 9 April 1969, piloted by Brian Trubshaw. Both prototypes were presented to the public for the first time on 7–8 June 1969 at the Paris Air Show. As the flight programme progressed, 001 embarked on a sales and demonstration tour on 4 September 1971, which was also the first transatlantic crossing of Concorde. Concorde 002 followed suit on 2 June 1972 with a tour of the Middle and Far East. Concorde 002 made the first visit to the United States in 1973, landing at the new Dallas/Fort Worth Regional Airport to mark that airport's opening.
While Concorde had initially held a great deal of customer interest, the project was hit by a large number of order cancellations. The Paris Le Bourget air show crash of the competing Soviet Tupolev Tu-144 had shocked potential buyers, and public concern over the environmental issues presented by a supersonic aircraft – the sonic boom, take-off noise and pollution – had produced a shift in public opinion of SSTs. By 1976 four nations remained as prospective buyers: Britain, France, China, and Iran. Only Air France and British Airways (the successor to BOAC) took up their orders, with the two governments taking a cut of any profits made.
The United States cancelled the Boeing 2707, its rival supersonic transport programme, in 1971. Observers have suggested that opposition to Concorde on grounds of noise pollution had been encouraged by the United States Government, as it lacked its own competitor. The US, India, and Malaysia all ruled out Concorde supersonic flights over the noise concern, although some of these restrictions were later relaxed. Professor Douglas Ross characterised restrictions placed upon Concorde operations by President Jimmy Carter's administration as having been an act of protectionism of American aircraft manufacturers. Concorde flew to an altitude of 68,000 ft (20,700 m) during a test flight in June 1973.
Concorde had other considerable difficulties that led to its dismal sales performance. Costs had spiralled during development to more than six times the original projections, arriving at a unit cost of £23 million in 1977. World events had also dampened Concorde sales prospects, the 1973 oil crisis made many airlines think twice about aircraft with high fuel consumption rates; and new wide-body aircraft, such as the Boeing 747, had recently made subsonic aircraft significantly more efficient and presented a low-risk option for airlines. While carrying a full load, Concorde achieved 15.8 passenger miles per gallon of fuel, while the Boeing 707 reached 33.3 pm/g, the Boeing 747 46.4 pm/g, and the McDonnell Douglas DC-10 53.6 pm/g. An emerging trend in the industry in favour of cheaper airline tickets had also caused airlines such as Qantas to question Concorde's market suitability.
Design.
General features.
Concorde is an ogival delta winged aircraft with four Olympus engines based on those employed in the RAF's Avro Vulcan strategic bomber. It is one of the few commercial aircraft to employ a tailless design (the Tupolev Tu-144 being another). Concorde was the first airliner to have a (in this case, analogue) fly-by-wire flight-control system; the avionics system the Concorde used was unique because it was the first commercial aircraft to employ hybrid circuits. The principal designer for the project was Pierre Satre, with Sir Archibald Russell as his deputy.
Concorde pioneered the following technologies:
For high speed and optimisation of flight:
For weight-saving and enhanced performance:
Powerplant.
Concorde needed to fly long distances to be economically viable; this required high efficiency. Turbofan engines were rejected due to their larger cross-section producing excessive drag. Turbojets were found to be the best choice of engines. The engine used was the twin spool Rolls-Royce/Snecma Olympus 593, a development of the Bristol engine first used for the Avro Vulcan bomber, and developed into an afterburning supersonic variant for the BAC TSR-2 strike bomber. Rolls-Royce's own engine proposed for the aircraft at the time of Concorde's initial design was the RB.169.
The aircraft used reheat (afterburners) at take-off and to pass through the upper transonic regime and to supersonic speeds, between Mach 0.95 and Mach 1.7. The afterburners were switched off at all other times. Due to jet engines being highly inefficient at low speeds, Concorde burned two tonnes of fuel (almost 2% of the maximum fuel load) taxiing to the runway. Fuel used is Jet A-1. Due to the high thrust produced even with the engines at idle, only the two outer engines were run after landing for easier taxiing and less brake pad wear - at low weights after landing the aircraft would not remain stationary with all four engines idling requiring the brakes to be continuously applied to prevent the aircraft moving.
The intake design for Concorde's engines was especially critical. The intakes had to provide low distortion levels (to prevent engine surge) and high efficiency for all likely ambient temperatures to be met in cruise. They had to provide adequate subsonic performance for diversion cruise and low engine-face distortion at take-off. They also had to provide an alternate path for excess intake air during engine throttling or shutdowns. The variable intake features required to meet all these requirements consisted of front and rear ramps, a dump door, an auxiliary inlet and a ramp bleed to the exhaust nozzle.
As well as supplying air to the engine, the intake also supplied air through the ramp bleed to the propelling nozzle. The nozzle ejector (or aerodynamic) design, with variable exit area and secondary flow from the intake, contributed to good expansion efficiency from take-off to cruise.
Engine failure causes problems on conventional subsonic aircraft; not only does the aircraft lose thrust on that side but the engine creates drag, causing the aircraft to yaw and bank in the direction of the failed engine. If this had happened to Concorde at supersonic speeds, it theoretically could have caused a catastrophic failure of the airframe. Although computer simulations predicted considerable problems, in practice Concorde could shut down both engines on the same side of the aircraft at Mach 2 without the predicted difficulties. During an engine failure the required air intake is virtually zero so, on Concorde, engine failure was countered by the opening of the auxiliary spill door and the full extension of the ramps, which deflected the air downwards past the engine, gaining lift and minimising drag. Concorde pilots were routinely trained to handle double engine failure.
Concorde's Air Intake Control Units (AICUs) made use of a digital processor to provide the necessary accuracy for intake control. It was the world's first use of a digital processor to be given full authority control of an essential system in a passenger aircraft. It was developed by the Electronics and Space Systems (ESS) division of the British Aircraft Corporation after it became clear that the analogue AICUs fitted to the prototype aircraft and developed by Ultra Electronics were found to be insufficiently accurate for the tasks in hand.
Concorde's thrust-by-wire engine control system was also developed by Ultra Electronics.
Heating issues.
Air compression on the outer surfaces caused the cabin to heat up during flight. Every surface, such as windows and panels, was warm to the touch by the end of the flight. Besides engines, the hottest part of the structure of any supersonic aircraft, due to aerodynamic heating, is the nose. The engineers used Hiduminium R.R. 58, an aluminium alloy, throughout the aircraft due to its familiarity, cost and ease of construction. The highest temperature that aluminium could sustain over the life of the aircraft was , which limited the top speed to Mach 2.02. Concorde went through two cycles of heating and cooling during a flight, first cooling down as it gained altitude, then heating up after going supersonic. The reverse happened when descending and slowing down. This had to be factored into the metallurgical and fatigue modelling. A test rig was built that repeatedly heated up a full-size section of the wing, and then cooled it, and periodically samples of metal were taken for testing. The Concorde airframe was designed for a life of 45,000 flying hours.
Owing to air compression in front of the plane as it travelled at supersonic speed, the fuselage heated up and expanded by as much as 300 mm (almost 1 ft). The most obvious manifestation of this was a gap that opened up on the flight deck between the flight engineer's console and the bulkhead. On some aircraft that conducted a retiring supersonic flight, the flight engineers placed their caps in this expanded gap, wedging the cap when it shrank again. To keep the cabin cool, Concorde used the fuel as a heat sink for the heat from the air conditioning. The same method also cooled the hydraulics. During supersonic flight the surfaces forward from the cockpit became heated, and a visor was used to deflect much of this heat from directly reaching the cockpit.
Concorde had livery restrictions; the majority of the surface had to be covered with a highly reflective white paint to avoid overheating the aluminium structure due to heating effects from supersonic flight at Mach 2. The white finish reduced the skin temperature by 6 to 11 degrees Celsius. In 1996, Air France briefly painted F-BTSD in a predominantly blue livery, with the exception of the wings, in a promotional deal with Pepsi. In this paint scheme, Air France were advised to remain at Mach 2 for no more than 20 minutes at a time, but there was no restriction at speeds under Mach 1.7. F-BTSD was used because it was not scheduled for any long flights that required extended Mach 2 operations.
Structural issues.
Due to its high speeds, large forces were applied to the aircraft during banks and turns, and caused twisting and distortion of the aircraft's structure. In addition there were concerns over maintaining precise control at supersonic speeds. Both of these issues were resolved by active ratio changes between the inboard and outboard elevons, varying at differing speeds including supersonic. Only the innermost elevons, which are attached to the stiffest area of the wings, were active at high speed. Additionally, the narrow fuselage meant that the aircraft flexed. This was visible from the rear passengers' viewpoints.
When any aircraft passes the critical mach of that particular airframe, the centre of pressure shifts rearwards. This causes a pitch down force on the aircraft if the centre of mass remains where it was. The engineers designed the wings in a specific manner to reduce this shift, but there was still a shift of about 2 metres. This could have been countered by the use of trim controls, but at such high speeds this would have dramatically increased drag. Instead, the distribution of fuel along the aircraft was shifted during acceleration and deceleration to move the centre of mass, effectively acting as an auxiliary trim control.
Range.
To fly non-stop across the Atlantic Ocean, Concorde required the greatest supersonic range of any aircraft. This was achieved by a combination of engines which were highly efficient at supersonic speeds, a slender fuselage with high fineness ratio, and a complex wing shape for a high lift-to-drag ratio. This also required carrying only a modest payload and a high fuel capacity, and the aircraft was trimmed with precision to avoid unnecessary drag.
Nevertheless, soon after Concorde began flying, a Concorde "B" model was designed with slightly larger fuel capacity and slightly larger wings with leading edge slats to improve aerodynamic performance at all speeds, with the objective of expanding the range to reach markets in new regions. It featured more powerful engines with sound deadening and without the fuel-hungry and noisy afterburner. It was speculated that it was reasonably possible to create an engine with up to 25% gain in efficiency over the Rolls-Royce/Snecma Olympus 593. This would have given additional range and a greater payload, making new commercial routes possible. This was cancelled due in part to poor sales of Concorde, but also to the rising cost of aviation fuel in the 1970s.
Radiation concerns.
Concorde's high cruising altitude meant passengers received almost twice the flux of extraterrestrial ionising radiation as those travelling on a conventional long-haul flight. Upon Concorde's introduction, it was speculated that this exposure during supersonic travels would increase the likelihood of skin cancer. Due to the proportionally reduced flight time, the overall equivalent dose would normally be less than a conventional flight over the same distance. Unusual solar activity might lead to an increase in incident radiation. To prevent incidents of excessive radiation exposure, the flight deck had a radiometer and an instrument to measure the rate of decrease of radiation. If the radiation level became too high, Concorde would descend below .
Cabin pressurisation.
Airliner cabins were usually maintained at a pressure equivalent to 6,000–8,000 feet (1,800–2,400 m) elevation. Concorde's pressurisation was set to an altitude at the lower end of this range, . Concorde's maximum cruising altitude was ; subsonic airliners typically cruise below .
A sudden reduction in cabin pressure is hazardous to all passengers and crew. Above , a sudden cabin depressurisation would leave a "time of useful consciousness" up to 10–15 seconds for a conditioned athlete. At Concorde's altitude, the air density is very low; a breach of cabin integrity would result in a loss of pressure severe enough so that the plastic emergency oxygen masks installed on other passenger jets would not be effective and passengers would soon suffer from hypoxia despite quickly donning them. Concorde was equipped with smaller windows to reduce the rate of loss in the event of a breach, a reserve air supply system to augment cabin air pressure, and a rapid descent procedure to bring the aircraft to a safe altitude. The FAA enforces minimum emergency descent rates for aircraft and noting Concorde's higher operating altitude, concluded that the best response to pressure loss would be a rapid descent. Continuous positive airway pressure would have delivered pressurised oxygen directly to the pilots through masks.
Flight characteristics.
While subsonic commercial jets took eight hours to fly from New York to Paris, the average supersonic flight time on the transatlantic routes was just under 3.5 hours. Concorde had a maximum cruise altitude of and an average cruise speed of Mach 2.02, about 1155 knots (2140 km/h or 1334 mph), more than twice the speed of conventional aircraft.
With no other civil traffic operating at its cruising altitude of about , dedicated oceanic airways or "tracks" were used by Concorde to cross the Atlantic. Due to the nature of high altitude winds, these SST tracks were fixed in terms of their co-ordinates, unlike the North Atlantic Tracks at lower altitudes whose co-ordinates alter daily according to forecast weather patterns (jetstreams). Concorde would also be cleared in a block, allowing for a slow climb from 45,000 to during the oceanic crossing as the fuel load gradually decreased. In regular service, Concorde employed an efficient "cruise-climb" flight profile following take-off.
The delta-shaped wings required Concorde to adopt a higher angle of attack at low speeds than conventional aircraft, but it allowed the formation of large low pressure vortices over the entire upper wing surface, maintaining lift. The normal landing speed was . Because of this high angle, during a landing approach Concorde was on the "back side" of the drag force curve, where raising the nose would increase the rate of descent; the aircraft was thus largely flown on the throttle and was fitted with an autothrottle to reduce the pilot's workload.
Brakes and undercarriage.
Because of the way Concorde's delta-wing generated lift, the undercarriage had to be unusually strong. At rotation, Concorde would rise to a high angle of attack, about 18 degrees. Prior to rotation the wing generated almost no lift, unlike typical aircraft wings. Combined with the high airspeed at rotation (199 knots indicated airspeed), this increased the stresses on the rear undercarriage in a way that was initially unexpected during the development and required a major redesign. Due to the high angle needed at rotation, a small set of wheels were added aft to prevent tailstrikes. The rear main undercarriage units swing towards each other to be stowed but due to their great height also need to retract telescopically before swinging to clear each other when stowed. The four main wheel tyres on each bogie unit are inflated to 232 lb/sq in. The twin-wheel nose undercarriage retracts forwards and its tyres are inflated to a pressure of 191 lb/sq in, and the wheel assembly carries a spray deflector to prevent standing water being thrown up into the engine intakes. The tyres are rated to 250 mph. The starboard nose wheel carries a single disc brake to halt wheel rotation during retraction of the undercarriage. The port nose wheel carries speed generators for the anti-skid braking system which prevents brake activation until nose and main wheels rotate at the same rate.
Additionally, due to the high average take-off speed of , Concorde needed upgraded brakes. Like most airliners, Concorde has anti-skid braking – a system which prevents the tyres from losing traction when the brakes are applied for greater control during roll-out. The brakes, developed by Dunlop, were the first carbon-based brakes used on an airliner. The use of carbon over equivalent steel brakes provided a weight-saving of . Each wheel has multiple discs which are cooled by electric fans. Wheel sensors include brake overload, brake temperature, and tyre deflation. After a typical landing at Heathrow, brake temperatures were around 300–400 °C (572–752 °F). For landing Concorde required a minimum of 6,000 feet runway length, this in fact being considerably less than the shortest runway Concorde ever actually landed on, that of Cardiff Airport.
Droop nose.
Concorde's drooping nose, developed by Marshall Aerospace, enabled the aircraft to switch between being streamlined to reduce drag and achieve optimum aerodynamic efficiency, and not obstructing the pilot's view during taxi, take-off, and landing operations. Due to the high angle of attack, the long pointed nose obstructed the view and necessitated the capability to droop. The droop nose was accompanied by a moving visor that retracted into the nose prior to being lowered. When the nose was raised to horizontal, the visor would rise in front of the cockpit windscreen for aerodynamic streamlining.
A controller in the cockpit allowed the visor to be retracted and the nose to be lowered to 5° below the standard horizontal position for taxiing and take-off. Following take-off and after clearing the airport, the nose and visor were raised. Prior to landing, the visor was again retracted and the nose lowered to 12.5° below horizontal for maximum visibility. Upon landing the nose was raised to the five-degree position to avoid the possibility of damage.
The US Federal Aviation Administration had objected to the restrictive visibility of the visor used on the first two prototype Concordes and thus requiring alteration before the FAA would permit Concorde to serve US airports; this led to the redesigned visor used on the production and the four pre-production aircraft (101, 102, 201, and 202). The nose window and visor glass needed to endure temperatures in excess of 100 °C (212 °F) at supersonic flight were developed by Triplex.
Operational history.
Scheduled flights.
Scheduled flights began on 21 January 1976 on the London–Bahrain and Paris–Rio de Janeiro (via Dakar) routes, with BA flights using the "Speedbird Concorde" call sign to notify air traffic control of the aircraft's unique abilities and restrictions, but the French using their normal call signs. The Paris-Caracas route (via Azores) began on 10 April. The US Congress had just banned Concorde landings in the US, mainly due to citizen protest over sonic booms, preventing launch on the coveted North Atlantic routes. The US Secretary of Transportation, William Coleman, gave permission for Concorde service to Washington Dulles International Airport, and Air France and British Airways simultaneously began service to Dulles on 24 May 1976.
When the US ban on JFK Concorde operations was lifted in February 1977, New York banned Concorde locally. The ban came to an end on 17 October 1977 when the Supreme Court of the United States declined to overturn a lower court's ruling rejecting efforts by the Port Authority and a grass-roots campaign led by Carol Berman to continue the ban. In spite of complaints about noise, the noise report noted that Air Force One, at the time a Boeing VC-137, was louder than Concorde at subsonic speeds and during take-off and landing. Scheduled service from Paris and London to New York's John F. Kennedy Airport began on 22 November 1977.
In 1977, British Airways and Singapore Airlines shared a Concorde for flights between London and Singapore International Airport at Paya Lebar via Bahrain. The aircraft, BA's Concorde G-BOAD, was painted in Singapore Airlines livery on the port side and British Airways livery on the starboard side. The service was discontinued after three return flights because of noise complaints from the Malaysian government; it could only be reinstated on a new route bypassing Malaysian airspace in 1979. A dispute with India prevented Concorde from reaching supersonic speeds in Indian airspace, so the route was eventually declared not viable and discontinued in 1980.
During the Mexican oil boom, Air France flew Concorde twice weekly to Mexico City's Benito Juárez International Airport via Washington, DC, or New York City, from September 1978 to November 1982. The worldwide economic crisis during that period resulted in this route's cancellation; the last flights were almost empty. The routing between Washington or New York and Mexico City included a deceleration, from Mach 2.02 to Mach 0.95, to cross Florida subsonically and avoid creating a sonic boom over the state; Concorde then re-accelerated back to high speed while crossing the Gulf of Mexico. On 1 April 1989, on an around-the-world luxury tour charter, British Airways implemented changes to this routing that allowed G-BOAF to maintain Mach 2.02 by passing around Florida to the east and south. Periodically Concorde visited the region on similar chartered flights to Mexico City and Acapulco.
From December 1978 to May 1980, Braniff International Airways leased 11 Concordes, five from Air France and six from British Airways. These were used on subsonic flights between Dallas-Fort Worth and Washington Dulles International Airport, flown by Braniff flight crews. Air France and British Airways crews then took over for the continuing supersonic flights to London and Paris. The aircraft were registered in both the United States and their home countries; the European registration was covered while being operated by Braniff, retaining full AF/BA liveries. The flights were not profitable and typically less than 50% booked, forcing Braniff to end its tenure as the only US Concorde operator in May 1980.
In its early years, the British Airways Concorde service had a greater number of "no shows" (passengers who booked a flight and then failed to appear at the gate for boarding) than any other aircraft in the fleet.
British Caledonian interest.
Following the launch of British Airways Concorde services, Britain's other major airline, British Caledonian (BCal), set up a task force headed by Gordon Davidson, BA's former Concorde director, to investigate the possibility of their own Concorde operations. This was seen as particularly viable for the airline's long-haul network as there were two unsold aircraft then available for purchase.
One important reason for BCal's interest in Concorde was that the British Government's 1976 aviation policy review had opened the possibility of BA setting up supersonic services in competition with BCal's established sphere of influence. To counteract this potential threat, BCal considered their own independent Concorde plans, as well as a partnership with BA. BCal were considered most likely to have set up a Concorde service on the Gatwick–Lagos route, a major source of revenue and profits within BCal's scheduled route network; BCal's Concorde task force did assess the viability of a daily supersonic service complementing the existing subsonic widebody service on this route.
BCal entered into a bid to acquire at least one Concorde. However, BCal eventually arranged for two aircraft to be leased from BA and Aérospatiale respectively, to be maintained by either BA or Air France. BCal's envisaged two-Concorde fleet would have required a high level of aircraft utilisation to be cost-effective; therefore, BCal had decided to operate the second aircraft on a supersonic service between Gatwick and Atlanta, with a stopover at either Gander or Halifax. Consideration was given to services to Houston and various points on its South American network at a later stage. Both supersonic services were to be launched at some point during 1980; however, steeply rising oil prices caused by the 1979 energy crisis led to BCal shelving their supersonic ambitions.
British Airways buys its Concordes outright.
By around 1981 in the UK, the future for Concorde looked bleak. The British government had lost money operating Concorde every year, and moves were afoot to cancel the service entirely. A cost projection came back with greatly reduced metallurgical testing costs because the test rig for the wings had built up enough data to last for 30 years and could be shut down. Despite this, the government was not keen to continue. In 1983, BA's managing director, Sir John King, convinced the government to sell the aircraft outright to British Airways for £16.5 million plus the first year's profits.
King recognised that, in Concorde, BA had a premier product that was underpriced. Market research had revealed that many customers thought Concorde was more expensive than it actually was; thus ticket prices were progressively raised to match these perceptions. It is reported that British Airways then ran Concorde at a profit, unlike their French counterpart.
Between 1984 and 1991, British Airways flew a thrice-weekly Concorde service between London and Miami, stopping at Washington Dulles International Airport. Until 2003, Air France and British Airways continued to operate the New York services daily. Concorde routinely flew to Grantley Adams International Airport, Barbados, during the winter holiday season.
Prior to the Air France Paris crash, several UK and French tour operators operated charter flights to European destinations on a regular basis; the charter business was viewed as lucrative by British Airways and Air France.
In 1997, British Airways held a promotional contest to mark the 10th anniversary of the airline's move into the private sector. The promotion was a lottery to fly to New York held for 190 tickets valued at £5,400 each, to be offered at £10. Contestants had to call a special hotline to compete with up to 20 million people.
Retirement.
On 10 April 2003, Air France and British Airways simultaneously announced that they would retire Concorde later that year. They cited low passenger numbers following 25 July 2000 crash, the slump in air travel following the September 11 attacks, and rising maintenance costs. Although Concorde was technologically advanced when introduced in the 1970s, 30 years later, its analogue cockpit was dated. There had been little commercial pressure to upgrade Concorde due to a lack of competing aircraft, unlike other airliners of the same era such as the Boeing 747. By its retirement, it was the last aircraft in the British Airways fleet that had a flight engineer; other aircraft, such as the modernised 747-400, had eliminated the role.
On 11 April 2003, Virgin Atlantic founder Sir Richard Branson announced that the company was interested in purchasing British Airways' Concorde fleet for their nominal original price of £1 (US$1.57 in April 2003) each. British Airways dismissed the idea, prompting Virgin to increase their offer to £1 million each. Branson claimed that when BA was privatised, a clause in the agreement required them to allow another British airline to operate Concorde if BA ceased to do so, but the Government denied the existence of such a clause. In October 2003, Branson wrote in "The Economist" that his final offer was "over £5 million" and that he had intended to operate the fleet "for many years to come". The chances for keeping Concorde in service were stifled by Airbus's lack of support for continued maintenance.
It has been suggested that Concorde was not withdrawn for the reasons usually given but that it became apparent during the grounding of Concorde that the airlines could make more profit carrying first class passengers subsonically. A lack of commitment to Concorde from Director of Engineering Alan MacDonald was cited as having undermined BA's resolve to continue operating Concorde.
Air France.
Air France made its final commercial Concorde landing in the United States in New York City from Paris on 30 May 2003. Air France's final Concorde flight took place on 27 June 2003 when F-BVFC retired to Toulouse.
An auction of Concorde parts and memorabilia for Air France was held at Christie's in Paris on 15 November 2003; 1,300 people attended, and several lots exceeded their predicted values. French Concorde F-BVFC was retired to Toulouse and kept functional for a short time after the end of service, in case taxi runs were required in support of the French judicial enquiry into the 2000 crash. The aircraft is now fully retired and no longer functional.
French Concorde F-BTSD has been retired to the "Musée de l'Air" at Paris–Le Bourget Airport near Paris; unlike the other museum Concordes, a few of the systems are being kept functional. For instance, the famous "droop nose" can still be lowered and raised. This led to rumours that they could be prepared for future flights for special occasions.
French Concorde F-BVFB currently rests at the Auto & Technik Museum Sinsheim at Sinsheim, Germany, after its last flight from Paris to Baden-Baden, followed by a spectacular transport to Sinsheim via barge and road. The museum also has a Tupolev Tu-144 on display – this is the only place where both supersonic airliners can be seen together.
In 1989, Air France signed a letter of agreement to donate a Concorde to the National Air and Space Museum in Washington D.C. upon the aircraft's retirement. On 12 June 2003, Air France honoured that agreement, donating Concorde F-BVFA (serial 205) to the Museum upon the completion of its last flight. This aircraft was the first Air France Concorde to open service to Rio de Janeiro, Washington, D.C., and New York and had flown 17,824 hours. It is on display at the Smithsonian's Steven F. Udvar-Hazy Center at Dulles Airport.
British Airways.
British Airways conducted a North American farewell tour in October 2003. G-BOAG visited Toronto Pearson International Airport on 1 October, after which it flew to New York's John F. Kennedy International Airport. G-BOAD visited Boston's Logan International Airport on 8 October, and G-BOAG visited Washington Dulles International Airport on 14 October. It has been claimed that G-BOAD's flight from London Heathrow to Boston set a transatlantic flight record of 3 hours, 5 minutes, 34 seconds. However the fastest transatlantic flight was from New York JFK airport to Heathrow on 7 February 1996, taking 2 hours, 52 minutes, 59 seconds; 90 seconds less than a record set in April 1990.
In a week of farewell flights around the United Kingdom, Concorde visited Birmingham on 20 October, Belfast on 21 October, Manchester on 22 October, Cardiff on 23 October, and Edinburgh on 24 October. Each day the aircraft made a return flight out and back into Heathrow to the cities, often overflying them at low altitude. On 22 October, both Concorde flight BA9021C, a special from Manchester, and BA002 from New York landed simultaneously on both of Heathrow's runways. On 23 October 2003, the Queen consented to the illumination of Windsor Castle, an honour reserved for state events and visiting dignitaries, as Concorde's last west-bound commercial flight departed London.
British Airways retired its Concorde fleet on 24 October 2003. G-BOAG left New York to a fanfare similar to that given for Air France's F-BTSD, while two more made round trips, G-BOAF over the Bay of Biscay, carrying VIP guests including former Concorde pilots, and G-BOAE to Edinburgh. The three aircraft then circled over London, having received special permission to fly at low altitude, before landing in sequence at Heathrow. The captain of the New York to London flight was Mike Bannister. The final flight of a Concorde in the US occurred on 5 November 2003 when G-BOAG flew from New York's JFK Airport to Seattle's Boeing Field to join the Museum of Flight's permanent collection. The plane was piloted by Mike Bannister and Les Broadie who claimed a flight time of three hours, 55 minutes and 12 seconds, a record between the two cities. The museum had been pursuing a Concorde for their collection since 1984. The final flight of a Concorde world-wide took place on 26 November 2003 with a landing at Filton, Bristol, UK.
All of BA's Concorde fleet have been grounded, drained of hydraulic fluid and their airworthiness certificates withdrawn. Jock Lowe, ex-chief Concorde pilot and manager of the fleet estimated in 2004 that it would cost £10–15 million to make G-BOAF airworthy again. BA maintain ownership and have stated that they will not fly again due to a lack of support from Airbus. On 1 December 2003, Bonhams held an auction of British Airways Concorde artefacts, including a nose cone, at Kensington Olympia in London. Proceeds of around £750,000 were raised, with the majority going to charity. G-BOAD is currently on display at the Intrepid Sea, Air & Space Museum in New York. In 2007, BA announced that the advertising spot at Heathrow where a 40% scale model of Concorde was located would not be retained; the model is now on display at the Brooklands Museum.
Restoration.
Although only used for spares after being retired from test flying and trials work in 1981, Concorde G-BBDG was dismantled and transported by road from Filton then restored from essentially a shell at the Brooklands Museum in Surrey, where it remains open to visitors to the museum.
One of the youngest Concordes (F-BTSD) is on display at Le Bourget Air and Space Museum in Paris. In February 2010, it was announced that the museum and a group of volunteer Air France technicians intend to restore F-BTSD so it can taxi under its own power. In May 2010, it was reported that the British Save Concorde Group and French Olympus 593 groups had begun inspecting the engines of a Concorde at the French museum; their intent is to restore the airliner to a condition where it can fly in demonstrations. Save Concorde Group hoped to get F-BTSD flying for the 2012 London Olympics, but this never happened. The work for restoring F-BTSD to operating condition as of September 2015 is currently not very well known, and it is still being housed in Le Bourget as a museum exhibit. In 2015, the organisation Club Concorde announced that it had raised funds of £120 million for a static display and to buy the Concorde at Le Bourget, restore it and return it to service as a heritage aircraft for air displays and charter hire by 2019, to coincide with the 50th anniversary of Concorde's first flight.
In July 2015, it was reported that planning permission had been granted for the construction of the museum at Bristol Filton Airport to house G-BOAF the last Concorde where it is intended to form a key exhibit of the new Bristol Aviation Heritage Museum.
Return to service plan.
In September 2015, it was publicly revealed that the Club Concorde had secured over £160 million to return an aircraft to service.
Club Concorde president Paul James said: "The main obstacle to any Concorde project to date has been 'Where’s the money?' – a question we heard ad nauseam, until we found an investor. Now that money is no longer the problem it's over to those who can help us make it happen." The organisation aims to buy the Concorde currently on display at Le Bourget airport. A tentative date of 2019 has been put forward for the first flight – 50 years after its maiden journey.
Accidents and incidents.
Air France Flight 4590.
On 25 July 2000, Air France Flight 4590, registration F-BTSC, crashed in Gonesse, France after departing from Paris-Charles de Gaulle en route to John F. Kennedy International Airport in New York City, killing all 100 passengers and 9 crew members on board the flight, and 4 people on the ground. It was the only fatal accident involving Concorde.
According to the official investigation conducted by the "Bureau d'Enquêtes et d'Analyses pour la Sécurité de l'Aviation Civile" (BEA), the crash was caused by a metallic strip that fell from a Continental Airlines DC-10 that had taken off minutes earlier. This fragment punctured a tyre on Concorde's left main wheel bogie during take-off. The tyre exploded, and a piece of rubber hit the fuel tank, which caused a fuel leak and led to a fire. The crew shut down engine number 2 in response to a fire warning, and with engine number 1 surging and producing little power, the aircraft was unable to gain altitude or speed. The aircraft entered a rapid pitch-up then a violent descent, rolling left and crashing tail-low into the Hôtelissimo Les Relais Bleus Hotel in Gonesse.
The claim that a metallic strip caused the crash was disputed during the trial both by witnesses (including the pilot of Jacques Chirac's aircraft that had just landed on an adjacent runway when Flight 4590 caught fire) and by an independent French TV investigation that found a wheel spacer had not been installed in the left-side main gear and that the plane caught fire some 1,000 feet from where the metallic strip lay.
On 6 December 2010, Continental Airlines and John Taylor, one of their mechanics, were found guilty of involuntary manslaughter, but on 30 November 2012, a French court overturned the conviction, saying mistakes by Continental and Taylor did not make them criminally responsible. 
Prior to the accident, Concorde had been arguably the safest operational passenger airliner in the world in passenger deaths-per-kilometres travelled with zero, but there had been two prior non-fatal accidents and a rate of tyre damage some 30 times higher than subsonic airliners from 1995 to 2000. Safety improvements were made in the wake of the crash, including more secure electrical controls, Kevlar lining on the fuel tanks and specially developed burst-resistant tyres. The first flight with the modifications departed from London Heathrow on 17 July 2001, piloted by BA Chief Concorde Pilot Mike Bannister. During the 3-hour 20-minute flight over the mid-Atlantic towards Iceland, Bannister attained Mach 2.02 and before returning to RAF Brize Norton. The test flight, intended to resemble the London–New York route, was declared a success and was watched on live TV, and by crowds on the ground at both locations.
The first flight with passengers after the accident took place on 11 September 2001, landing shortly before the World Trade Center attacks in the United States. This was not a commercial flight: all the passengers were BA employees. Normal commercial operations resumed on 7 November 2001 by BA and AF (aircraft G-BOAE and F-BTSD), with service to New York JFK, where mayor Rudy Giuliani greeted the passengers.
Other accidents and incidents.
Concorde had suffered two previous non-fatal accidents that were similar to each other.
Comparable aircraft.
The only supersonic airliner in direct competition with Concorde was the Soviet Tupolev Tu-144, nicknamed "Concordski" by Western European journalists for its outward similarity to Concorde. It had been alleged that Soviet espionage efforts had resulted in the theft of Concorde blueprints, ostensibly to assist in the design of the Tu-144. As a result of a rushed development programme, the first Tu-144 prototype was substantially different from the preproduction machines, but both were cruder than Concorde. The Tu-144"S" had a significantly shorter range than Concorde, due to its low-bypass turbofan engines. The aircraft had poor control at low speeds because of a simpler supersonic wing design; in addition the Tu-144 required braking parachutes to land while Concorde used anti-lock brakes. The Tu-144 had two crashes, one at the 1973 Paris Air Show, and another during a pre-delivery test flight in May 1978.
Later production Tu-144 versions were more refined and competitive. They had retractable canards for better low-speed control, turbojet engines providing nearly the fuel efficiency and range of Concorde and a top speed of Mach 2.35. Passenger service commenced in November 1977, but after the 1978 crash the aircraft was taken out of service. The aircraft had an inherently unsafe structural design as a consequence of an automated production method chosen to simplify and speed up manufacturing.
The American designs, the Boeing 2707 and the Lockheed L-2000, were to have been larger, with seating for up to 300 people. Running a few years behind Concorde, the Boeing 2707 was redesigned to a cropped delta layout; the extra cost of these changes helped to kill the project. The operation of US military aircraft such as the XB-70 Valkyrie and B-58 Hustler had shown that sonic booms were quite capable of reaching the ground, and the experience from the Oklahoma City sonic boom tests led to the same environmental concerns that hindered the commercial success of Concorde. The American government cancelled its SST project in 1971, after having spent more than $1 billion.
The only other large supersonic aircraft comparable to Concorde are strategic bombers, principally the Russian Tu-22, Tu-22M, M-50 (experimental), T-4 (experimental), Tu-160 and the American XB-70 (experimental) and B-1.
Impact.
Environmental.
Before Concorde's flight trials, developments in the civil aviation industry were largely accepted by governments and their respective electorates. Opposition to Concorde's noise, particularly on the east coast of the United States, forged a new political agenda on both sides of the Atlantic, with scientists and technology experts across a multitude of industries beginning to take the environmental and social impact more seriously. Although Concorde led directly to the introduction of a general noise abatement programme for aircraft flying out of John F. Kennedy Airport, many found that Concorde was quieter than expected, partly due to the pilots temporarily throttling back their engines to reduce noise during overflight of residential areas. Even before commercial flights started, it had been claimed that Concorde was quieter than many other aircraft. In 1971, BAC's technical director was quoted as saying, "It is certain on present evidence and calculations that in the airport context, production Concordes will be no worse than aircraft now in service and will in fact be better than many of them."
Concorde produced nitrogen oxides in its exhaust, which, despite complicated interactions with other ozone-depleting chemicals, are understood to result in degradation to the ozone layer at the stratospheric altitudes it cruised. It has been pointed out that other, lower-flying, airliners produce ozone during their flights in the troposphere, but vertical transit of gases between the layers is restricted. The small fleet meant overall ozone-layer degradation caused by Concorde was negligible. In 1995, David Fahey, of the National Oceanic and Atmospheric Administration in the United States, warned that a fleet of 500 supersonic aircraft with exhausts similar to Concorde might produce in a 2 per cent drop in global ozone levels, much higher than previously thought. Each 1 per cent drop in ozone is estimated to increase the incidence of non-melanoma skin cancer worldwide by 2 per cent. Dr Fahey said if these particles are produced by highly oxidised sulphur in the fuel, as he believed, then removing sulphur in the fuel will reduce the ozone-destroying impact of supersonic transport.
Concorde's technical leap forward boosted the public's understanding of conflicts between technology and the environment as well as awareness of the complex decision analysis processes that surround such conflicts. In France, the use of acoustic fencing alongside TGV tracks might not have been achieved without the 1970s controversy over aircraft noise. In the UK, the CPRE has issued tranquillity maps since 1990.
Some sources say Concorde typically flew per passenger.
Public perception.
Concorde was normally perceived as a privilege of the rich, but special circular or one-way (with return by other flight or ship) charter flights were arranged to bring a trip within the means of moderately well-off enthusiasts.
The aircraft was usually referred to by the British as simply "Concorde". In France it was known as "le Concorde" due to "le", the definite article, used in French grammar to introduce the name of a ship or aircraft, and the capital being used to distinguish a proper name from a common noun of the same spelling. In French, the common noun "concorde" means "agreement, harmony, or peace". Concorde's pilots and British Airways in official publications often refer to Concorde both in the singular and plural as "she" or "her".
As a symbol of national pride, an example from the BA fleet made occasional flypasts at selected Royal events, major air shows and other special occasions, sometimes in formation with the Red Arrows. On the final day of commercial service, public interest was so great that grandstands were erected at Heathrow Airport. Significant numbers of people attended the final landings; the event received widespread media coverage.
In 2006, 37 years after its first test flight, Concorde was announced the winner of the Great British Design Quest organised by the BBC and the Design Museum. A total of 212,000 votes were cast with Concorde beating design icons such as the Mini, mini skirt, Jaguar E-type, Tube map and the Supermarine Spitfire.
Special missions.
Heads of France and the United Kingdom flew Concorde many times. Presidents Georges Pompidou, Valéry Giscard d'Estaing and François Mitterrand regularly used Concorde as French flagman aircraft in foreign visits. Queen Elizabeth II and Prime Ministers Edward Heath, Jim Callaghan, Margaret Thatcher, John Major, Tony Blair took Concorde in some charter flights such as the Queen's trips to Barbados on her Silver Jubilee in 1977, in 1987 and in 2003, to Middle East in 1984, to the United States in 1991, etc.
Pope John Paul II flew on Concorde in May 1989. The British Prime Minister flew in a British Airways Concorde (G-BOAC) to San Juan for the second G-6 Economic Summit, held in the United States and hosted by President Gerald Ford at the Dorado Beach Hotel in Dorado, Puerto Rico on 27–28 June 1976.
Concorde sometimes made special flights for its demonstration, for exhibit on airshows (Farnborough, Paris-LeBourget, MAKS, etc.) and other expositions, for taking part in parades and celebrations (as ex., of Zürich airport anniversary in 1998), for private charters (as ex., many times by President of Zaire Mobutu Sese Seko), for promo-advertising of companies (OKI, etc.), for Olympic torch relays (1992 Winter Olympics in Albertville), for observing of solar eclipse, etc.
Records.
The fastest transatlantic airliner flight was from New York JFK to London Heathrow on 7 February 1996 by the British Airways G-BOAD in 2 hours, 52 minutes, 59 seconds from take-off to touchdown aided by a 175 mph (282 km/h) tailwind. On 13 February 1985, a Concorde charter flight flew from London Heathrow to Sydney—on the opposite side of the world—in a time of 17 hours, 3 minutes and 45 seconds, including refuelling stops.
Concorde also set other records, including the official FAI "Westbound Around the World" and "Eastbound Around the World" world air speed records. On 12–13 October 1992, in commemoration of the 500th anniversary of Columbus’ first New World landing, Concorde Spirit Tours (USA) chartered Air France Concorde F-BTSD and circumnavigated the world in 32 hours 49 minutes and 3 seconds, from Lisbon, Portugal, including six refuelling stops at Santo Domingo, Acapulco, Honolulu, Guam, Bangkok, and Bahrain.
The eastbound record was set by the same Air France Concorde (F-BTSD) under charter to Concorde Spirit Tours in the USA on 15–16 August 1995. This promotional flight circumnavigated the world from New York/JFK International Airport in 31 hours 27 minutes 49 seconds, including six refuelling stops at Toulouse, Dubai, Bangkok, Andersen AFB in Guam, Honolulu, and Acapulco. By its 30th flight anniversary on 2 March 1999 Concorde had clocked up 920,000 flight hours, with more than 600,000 supersonic, many more than all of the other supersonic aircraft in the Western world combined.
On its way to the Museum of Flight in November 2003, G-BOAG set a New York City-to-Seattle speed record of 3 hours, 55 minutes, and 12 seconds.

</doc>
<doc id="7053" url="https://en.wikipedia.org/wiki?curid=7053" title="Cannon">
Cannon

A cannon (plural: "cannon" or "cannons") is any piece of artillery that uses gunpowder or other usually explosive-based propellants to launch a projectile. Cannon vary in calibre, range, mobility, rate of fire, angle of fire, and firepower; different forms of cannon combine and balance these attributes in varying degrees, depending on their intended use on the battlefield. The word "cannon" is derived from several languages, in which the original definition can usually be translated as "tube", "cane", or "reed". In the modern era, the term "cannon" has fallen into decline, replaced by "guns" or "artillery" if not a more specific term such as "mortar" or "howitzer", except for in the field of aerial warfare, where it is often used as shorthand for autocannon.
First invented in China, cannon were among the earliest forms of gunpowder artillery, and over time replaced siege engines—among other forms of ageing weaponry—on the battlefield. In the Middle East, the first use of the hand cannon is argued to be during the 1260 Battle of Ain Jalut between the Mamluks and Mongols. The first cannon in Europe were in use in Iberia by the mid-13th century. It was during this period, the Middle Ages, that cannon became standardized, and more effective in both the anti-infantry and siege roles. After the Middle Ages most large cannon were abandoned in favour of greater numbers of lighter, more maneuverable pieces. In addition, new technologies and tactics were developed, making most defences obsolete; this led to the construction of star forts, specifically designed to withstand artillery bombardment though these too (along with the Martello Tower) would find themselves rendered obsolete when explosive and armour piercing rounds made even these types of fortifications vulnerable.
Cannon also transformed naval warfare in the early modern period, as European navies took advantage of their firepower. As rifling became commonplace, the accuracy and destructive power of cannon was significantly increased, and they became deadlier than ever, both to infantry who belatedly had to adopt different tactics, and to ships, which had to be armoured. In World War I, the majority of combat fatalities were caused by artillery; they were also used widely in World War II. Most modern cannon are similar to those used in the Second World War, although the importance of the larger calibre weapons has declined with the development of missiles.
Cannon was widely known as the earliest form of a gun and artillery, before early firearms were invented.
Etymology and terminology.
"Cannon" is derived from the Old Italian word "cannone", meaning "large tube", which came from Latin "canna", in turn originating from the Greek κάννα ("kanna"), "reed", and then generalized to mean any hollow tube-like object; cognate with Akkadian term "qanu" and Hebrew "qāneh", meaning "tube" or "reed". The word has been used to refer to a gun since 1326 in Italy, and 1418 in England. Both "Cannons" and "Cannon" are correct and in common usage, with one or the other having preference in different parts of the English-speaking world. "Cannons" is more common in North America and Australia, while "cannon" as plural is more common in the United Kingdom.
Cannon materials, parts, and terms.
Cannon in general have the form of a truncated cone with an internal cylindrical bore for holding an explosive charge and a projectile. The thickest, strongest, and closed part of the cone is located near the explosive charge. As any explosive charge will dissipate in all directions equally, the thickest portion of the cannon is useful for containing and directing this force. The backward motion of the cannon as its projectile leaves the bore is termed its recoil and the effectiveness of the cannon can be measured in terms of how much this response can be diminished, though obviously diminishing recoil through increasing the overall mass of the cannon means decreased mobility.
Field artillery cannon in Europe and the Americas were initially made most often of bronze, though later forms were constructed of cast iron and eventually steel. Bronze has several characteristics that made it preferable as a construction material: although it is relatively expensive, does not always alloy well, and can result in a final product that is "spongy about the bore", bronze is more flexible than iron and therefore less prone to bursting when exposed to high pressure; cast iron cannon are less expensive and more durable generally than bronze and withstand being fired more times without deteriorating. However, cast iron cannon have a tendency to burst without having shown any previous weakness or wear, and this makes them more dangerous to operate.
The older and more-stable forms of cannon were muzzle-loading as opposed to breech-loading— in order to be used they had to have their ordnance packed down the bore through the muzzle rather than inserted through the breech.
The following terms refer to the components or aspects of a classical western cannon (c. 1850) as illustrated here. In what follows, the words "near", "close", and "behind" will refer to those parts towards the thick, closed end of the piece, and "far", "front", "in front of", and "before" to the thinner, open end.
Solid spaces.
The main body of a cannon consists of three basic extensions: the foremost and the longest is called the "chase", the middle portion is the "reinforce", and the closest and briefest portion is the "cascabel" or "cascable".
To pack a muzzle-loading cannon, first gunpowder is poured down the bore. This is followed by a layer of wadding (often nothing more than paper), and then the cannonball itself. A certain amount of windage allows the ball to fit down the bore, though the greater the windage the less efficient the propulsion of the ball when the gunpowder is ignited. To fire the cannon, the fuse located in the vent is lit, quickly burning down to the gunpowder, which then explodes violently, propelling wadding and ball down the bore and out of the muzzle. A small portion of exploding gas also escapes through the vent, but this does not dramatically affect the total force exerted on the ball.
Any large, smoothbore, muzzle-loading gun—used before the advent of breech-loading, rifled guns—may be referred to as a cannon, though once standardized names were assigned to different-sized cannon, the term specifically referred to a gun designed to fire a shot, as distinct from a demi-cannon – , culverin – , or demi-culverin – . "Gun" specifically refers to a type of cannon that fires projectiles at high speeds, and usually at relatively low angles; they have been used in warships, and as field artillery. The term "cannon" is also used for autocannon, a modern repeating weapon firing explosive projectiles. Cannon have been used extensively in fighter aircraft since World War II, and in place of machine guns on land vehicles.
History.
Development in China.
The invention of the cannon, driven by gunpowder, was first developed in China and later spread to the Islamic world and Europe. Like small arms, cannon are a descendant of the fire lance, a gunpowder-filled tube attached to the end of a spear and used as a flamethrower in China. Shrapnel was sometimes placed in the barrel, so that it would fly out along with the flames. The first documented battlefield use of fire lances took place in 1132 when Chen Gui used them to defend De'an from attack by the Jurchen Jin. Eventually, the paper and bamboo of which fire lance barrels were originally constructed came to be replaced by metal. It has been disputed at which point flame-projecting cannon were abandoned in favour of missile-projecting ones, as words meaning either "incendiary" or "explosive" are commonly translated as "gunpowder". The earliest known depiction of a gun is a sculpture from a cave in Sichuan, dating to the 12th century that portrays a figure carrying a vase-shaped bombard, firing flames and a ball. The oldest surviving gun, known as the Heilongjiang hand cannon and dated to no later than 1290, is 34 cm long with a muzzle bore diameter of . The second oldest, dated to 1332 is 35.3 cm long, a muzzle bore diameter of and weighs 6.94 kg; both are made of bronze.
The earliest known illustration of a cannon is dated to 1326. In his 1341 poem, "The Iron Cannon Affair", one of the first accounts of the use of gunpowder artillery in China, Xian Zhang wrote that a cannonball fired from an eruptor could "pierce the heart or belly when it strikes a man or horse, and can even transfix several persons at once."
Joseph Needham suggests that the proto-shells described in the "Huolongjing" may be among the first of their kind. The weapon was later taken up by both the Mongol conquerors and the Koreans. Chinese soldiers fighting under the Mongols appear to have used hand cannon in Manchurian battles during 1288, a date deduced from archaeological findings at battle sites. The Ming Chinese also mounted over 3,000 cast bronze and iron cannon on the Great Wall of China, to defend against the Mongols.
Cannon were used by Ming dynasty forces at the Battle of Lake Poyang. Ming dynasty era ships had bronze cannon. One shipwreck in Shandong had a cannon dated to 1377 and an anchor dated to 1372. From the 13th to 15th centuries cannon armed Chinese ships also travelled throughout Southeast Asia.
In the 1593 Siege of Pyongyang, 40,000 Ming troops deployed a variety of cannon to bombard an equally large Japanese army. Despite both forces having similar numbers, the Japanese were easily defeated due to the Ming cannon. Throughout the Seven Year War in Korea, the Chinese-Korean coalition used artillery widely, in both land and naval battles, including on the Turtle Ships of Yi Sun-sin.
Islamic world.
Arabic manuscripts dated from the 14th century document the use of the hand cannon, a forerunner of the handgun, in the Arabic world. Ahmad Y. al-Hassan argues that these manuscripts are copies of earlier manuscripts and reported on hand-held cannon being used by the Mamluks at the Battle of Ain Jalut in 1260. Al-Hassan also interprets Ibn Khaldun as reporting cannon being used as siege machines by the Marinid sultan Abu Yaqub Yusuf at the siege of Sijilmasa in 1274. Other historians urge caution regarding claims of Islamic firearms use in the 1204–1324 period as late medieval Arabic texts used the same word for gunpowder, "naft", that they used for an earlier incendiary naphtha. The Mamluks had certainly acquired siege cannon by the 1360s, and possibly as early as 1320.
Sixty-eight super-sized bombards referred to as Great Turkish Bombards were used by Mehmed II to capture Constantinople in 1453. Orban, a Hungarian cannon engineer, is credited with introducing the cannon from Central Europe to the Ottomans. These cannon could fire heavy stone balls a mile, and the sound of their blast could reportedly be heard from a distance of . Shkodran historian Marin Barleti discusses Turkish bombards at length in his book "De obsidione Scodrensi" (1504), describing the 1478–79 siege of Shkodra in which eleven bombards and two mortars were employed.
The similar Dardanelles Guns (for the location) were created by Munir Ali in 1464 and were still in use during the Anglo-Turkish War (1807–1809). These were cast in bronze into two parts, the chase (the barrel) and the breech, which combined weighed 18.4 tonnes. The two parts were screwed together using levers to facilitate moving it.
Fathullah Shirazi, a Persian-Indian who worked for Akbar the Great in the Mughal Empire, developed a volley gun in the 16th century.
Medieval Europe.
In Europe, one of the first mentions of gunpowder use appears in a passage found in Roger Bacon's "Opus Maius" and "Opus Tertium" in what has been interpreted as being firecrackers. In the early 20th century, a British artillery officer proposed that another work tentatively attributed to Bacon, "Epistola de Secretis Operibus Artis et Naturae, et de Nullitate Magiae" contained an encrypted formula for gunpowder. These claims have been disputed by historians of science. In any case, the formula claimed to have been decrypted is not useful for firearms use or even firecrackers, burning slowly and producing mostly smoke.
The first confirmed use of cannon in Europe was in southern Iberia, by the Moors, in the Siege of Cordoba in 1280. By this time, hand guns were probably in use, as "scopettieri"—"gun bearers"—were mentioned in conjunction with crossbowmen, in 1281. In Iberia, the "first artillery-masters on the Peninsula" were enlisted, at around the same time.
The first metal cannon was the "pot-de-fer". Loaded with an arrow-like bolt that was probably wrapped in leather to allow greater thrusting power, it was set off through a touch hole with a heated wire. This weapon, and others similar, were used by both the French and English during the Hundred Years' War, when cannon saw their first real use on the European battlefield. While still a relatively rarely-used weapon, cannon were employed in increasing numbers during the war. The Battle of Arnemuiden, fought on 23 September 1338, was the first naval battle using artillery, as the English ship "Christofer" had three cannon and one hand gun. "Ribaldis", which shot large arrows and simplistic grapeshot, were first mentioned in the English Privy Wardrobe accounts during preparations for the Battle of Crécy, between 1345 and 1346. The Florentine Giovanni Villani recounts their destructiveness, indicating that by the end of the battle, "the whole plain was covered by men struck down by arrows and cannon balls." Similar cannon were also used at the Siege of Calais, in the same year, although it was not until the 1380s that the "ribaudekin" clearly became mounted on wheels.
A small bronze cannon unearthed in Loshult, Scania in southern Sweden is considered to be one of the earliest surviving European guns. It dates from the early-mid 14th century, and is currently in the Swedish History Museum in Stockholm. They were used in Russia around 1380, though they were used only in sieges, often by the defenders. Large cannon known as bombards ranged from three to five feet in length and were used by Dubrovnik and Kotor in defence in the later 14th century. The first bombards were made of iron, but bronze was quickly recognized as being stronger and capable of propelling stones weighing as much as a hundred pounds (45 kg). Byzantine strategists did not have the money to invest in this technology. Around the same period, the Byzantine Empire began to accumulate its own cannon to face the Ottoman threat, starting with medium-sized cannon long and of 10 in calibre. The first definite use of artillery in the region was against the Ottoman siege of Constantinople, in 1396, forcing the Ottomans to withdraw. They acquired their own cannon, and laid siege to the Byzantine capital again, in 1422, using "falcons", which were short but wide cannon. By 1453, the Ottomans used 68 Hungarian-made cannon for the 55-day bombardment of the walls of Constantinople, "hurling the pieces everywhere and killing those who happened to be nearby." The largest of their cannon was the Great Turkish Bombard, which required an operating crew of 200 men and 70 oxen, and 10,000 men to transport it. Gunpowder made the formerly devastating Greek fire obsolete, and with the final fall of Constantinople—which was protected by what were once the strongest walls in Europe—on 29 May 1453, "it was the end of an era in more ways than one."
Early modern period.
By the 16th century, cannon were made in a great variety of lengths and bore diameters, but the general rule was that the longer the barrel, the longer the range. Some cannon made during this time had barrels exceeding in length, and could weigh up to . Consequently, large amounts of gunpowder were needed, to allow them to fire stone balls several hundred yards. By mid-century, European monarchs began to classify cannon to reduce the confusion. Henry II of France opted for six sizes of cannon, but others settled for more; the Spanish used twelve sizes, and the English sixteen. Better powder had been developed by this time as well. Instead of the finely ground powder used by the first bombards, powder was replaced by a "corned" variety of coarse grains. This coarse powder had pockets of air between grains, allowing fire to travel through and ignite the entire charge quickly and uniformly.
The end of the Middle Ages saw the construction of larger, more powerful cannon, as well their spread throughout the world. As they were not effective at breaching the newer fortifications resulting from the development of cannon, siege engines—such as siege towers and trebuchets—became less widely used. However, wooden "battery-towers" took on a similar role as siege towers in the gunpowder age—such as that used at siege of Kazan in 1552, which could hold ten large-calibre cannon, in addition to 50 lighter pieces. Another notable effect of cannon on warfare during this period was the change in conventional fortifications. Niccolò Machiavelli wrote, "There is no wall, whatever its thickness that artillery will not destroy in only a few days." Although castles were not immediately made obsolete by cannon, their use and importance on the battlefield rapidly declined. Instead of majestic towers and merlons, the walls of new fortresses were thick, angled, and sloped, while towers became low and stout; increasing use was also made of earth and brick in breastworks and redoubts. These new defences became known as "star forts", after their characteristic shape which attempted to force any advance toward it directly into the firing line of the guns. A few of these featured cannon batteries, such as the Tudors' Device Forts, in England. Star forts soon replaced castles in Europe, and, eventually, those in the Americas, as well.
By the end of the 15th century, several technological advancements made cannon more mobile. Wheeled gun carriages and trunnions became common, and the invention of the limber further facilitated transportation. As a result, field artillery became more viable, and began to see more widespread use, often alongside the larger cannon intended for sieges. Better gunpowder, cast-iron projectiles (replacing stone), and the standardization of calibres meant that even relatively light cannon could be deadly. In "The Art of War", Niccolò Machiavelli observed that "It is true that the arquebuses and the small artillery do much more harm than the heavy artillery." This was the case at Flodden, in 1513: the English field guns outfired the Scottish siege artillery, firing two or three times as many rounds. Despite the increased maneuverability, however, cannon were still the slowest component of the army: a heavy English cannon required 23 horses to transport, while a culverin needed nine. Even with this many animals pulling, they still moved at a walking pace. Due to their relatively slow speed, and lack of organization, and undeveloped tactics, the combination of pike and shot still dominated the battlefields of Europe.
Innovations continued, notably the German invention of the mortar, a thick-walled, short-barrelled gun that blasted shot upward at a steep angle. Mortars were useful for sieges, as they could hit targets behind walls or other defences. This cannon found more use with the Dutch, who learned to shoot bombs filled with powder from them. Setting the bomb fuse was a problem. "Single firing" was first used to ignite the fuse, where the bomb was placed with the fuse down against the cannon's propellant. This often resulted in the fuse being blown into the bomb, causing it to blow up as it left the mortar. Because of this, "double firing" was tried where the gunner lit the fuse and then the touch hole. This, however, required considerable skill and timing, and was especially dangerous if the gun misfired, leaving a lighted bomb in the barrel. Not until 1650 was it accidentally discovered that double-lighting was superfluous as the heat of firing would light the fuse.
Gustavus Adolphus of Sweden emphasized the use of light cannon and mobility in his army, and created new formations and tactics that revolutionized artillery. He discontinued using all 12 pounder—or heavier—cannon as field artillery, preferring, instead, to use cannon that could be manned by only a few men. One obsolete type of gun, the "leatheren" was replaced by 4 pounder and 9 pounder demi-culverins. These could be operated by three men, and pulled by only two horses. Adolphus's army was also the first to use a cartridge that contained both powder and shot which sped up reloading, increasing the rate of fire. Finally, against infantry he pioneered the use of canister shot - essentially a tin can filled with musket balls. Until then there was no more than one cannon for every thousand infantrymen on the battlefield but Gustavus Adolphus increased the number of cannon sixfold. Each regiment was assigned two pieces, though he often arranged then into batteries instead of distributing them piecemeal. He used these batteries to break his opponent's infantry line, while his cavalry would outflank their heavy guns.
At the Battle of Breitenfeld, in 1631, Adolphus proved the effectiveness of the changes made to his army, by defeating Johann Tserclaes, Count of Tilly. Although severely outnumbered, the Swedes were able to fire between three and five times as many volleys of artillery, and their infantry's linear formations helped ensure they didn't lose any ground. Battered by cannon fire, and low on morale, Tilly's men broke ranks and fled.
In England cannon were being used to besiege various fortified buildings during the English Civil War. Nathaniel Nye is recorded as testing a Birmingham cannon in 1643 and experimenting with a saker in 1645. From 1645 he was the master gunner to the Parliamentarian garrison at Evesham and in 1646 he successfully directed the artillery at the Siege of Worcester, detailing his experiences and in his 1647 book "The Art of Gunnery". Believing that war was as much a science as an art, his explanations focused on triangulation, arithmetic, theoretical mathematics, and cartography as well as practical considerations such as the ideal specification for gunpowder or slow matches. His book acknowledged mathematicians such as Robert Recorde and Marcus Jordanus as well as earlier military writers on artillery such as Niccolò Tartaglia and Thomas Malthus.
Around this time also came the idea of aiming the cannon to hit a target. Gunners controlled the range of their cannon by measuring the angle of elevation, using a "gunner's quadrant." Cannon did not have sights, therefore, even with measuring tools, aiming was still largely guesswork.
In the latter half of the 17th century, the French engineer Vauban introduced a more systematic and scientific approach to attacking gunpowder fortresses, in a time when many field commanders "were notorious dunces in siegecraft." Careful sapping forward, supported by enfilading ricochet fire, was a key feature of this system, and it even allowed Vauban to calculate the length of time a siege would take. He was also a prolific builder of star forts, and did much to popularize the idea of "depth in defence" in the face of cannon. These principles were followed into the mid-19th century, when changes in armaments necessitated greater depth defence than Vauban had provided for. It was only in the years prior to World War I that new works began to break radically away from his designs.
18th and 19th centuries.
The lower tier of 17th-century English ships of the line were usually equipped with demi-cannon, guns that fired a solid shot, and could weigh up to . Demi-cannon were capable of firing these heavy metal balls with such force that they could penetrate more than a metre of solid oak, from a distance of , and could dismast even the largest ships at close range. Full cannon fired a shot, but were discontinued by the 18th century, as they were too unwieldy. By the end of the 18th century, principles long adopted in Europe specified the characteristics of the Royal Navy's cannon, as well as the acceptable defects, and their severity. The United States Navy tested guns by measuring them, firing them two or three times—termed "proof by powder"—and using pressurized water to detect leaks.
The carronade was adopted by the Royal Navy in 1779; the lower muzzle velocity of the round shot when fired from this cannon was intended to create more wooden splinters when hitting the structure of an enemy vessel, as they were believed to be more deadly than the ball by itself. The carronade was much shorter, and weighed between a third to a quarter of the equivalent long gun; for example, a 32 pounder carronade weighed less than a ton, compared with a 32 pounder long gun, which weighed over 3 tons. The guns were, therefore, easier to handle, and also required less than half as much gunpowder, allowing fewer men to crew them. Carronades were manufactured in the usual naval gun calibres, but were not counted in a ship of the line's rated number of guns. As a result, the classification of Royal Navy vessels in this period can be misleading, as they often carried more cannon than were listed.
In the 1810s and 1820s, greater emphasis was placed on the accuracy of long-range gunfire, and less on the weight of a broadside. The carronade, although initially very successful and widely adopted, disappeared from the Royal Navy in the 1850s after the development of wrought-iron-jacketed steel cannon by William George Armstrong and Joseph Whitworth. Nevertheless, carronades were used in the American Civil War.
Western cannon during the 19th century became larger, more destructive, more accurate, and could fire at longer range. One example is the American wrought-iron, muzzle-loading rifle, or Griffen gun (usually called the 3-inch Ordnance Rifle), used during the American Civil War, which had an effective range of over . Another is the smoothbore 12 pounder Napoleon, which originated in France in 1853 and was widely used by both sides in the American Civil War. This cannon was renowned for its sturdiness, reliability, firepower, flexibility, relatively light weight, and range of .
Cannon were crucial in Napoleon Bonaparte's rise to power, and continued to play an important role in his army in later years. During the French Revolution, the unpopularity of the Directory led to riots and rebellions. When over 25,000 royalists led by General Danican assaulted Paris, Paul François Jean Nicolas, vicomte de Barras was appointed to defend the capital; outnumbered five to one and disorganized, the Republicans were desperate. When Napoleon arrived, he reorganized the defences but realized that without cannon the city could not be held. He ordered Joachim Murat to bring the guns from the Sablons artillery park; the Major and his cavalry fought their way to the recently captured cannon, and brought them back to Napoleon. When Danican's poorly trained men attacked, on 13 Vendémiaire, 1795 — 5 October 1795, in the calendar used in France at the time — Napoleon ordered his cannon to fire grapeshot into the mob, an act that became known as the "whiff of grapeshot". The slaughter effectively ended the threat to the new government, while, at the same time, made Bonaparte a famous—and popular—public figure. Among the first generals to recognize that artillery was not being used to its full potential, Napoleon often massed his cannon into batteries and introduced several changes into the French artillery, improving it significantly and making it among the finest in Europe. Such tactics were successfully used by the French, for example, at the Battle of Friedland, when sixty-six guns fired a total of 3,000 roundshot and 500 rounds of grapeshot, inflicting severe casualties to the Russian forces, whose losses numbered over 20,000 killed and wounded, in total. At the Battle of Waterloo—Napoleon's final battle—the French army had many more artillery pieces than either the British or Prussians. As the battlefield was muddy, recoil caused cannons to bury themselves into the ground after firing, resulting in slow rates of fire, as more effort was required to move them back into an adequate firing position; also, roundshot did not ricochet with as much force from the wet earth. Despite the drawbacks, sustained artillery fire proved deadly during the engagement, especially during the French cavalry attack. The British infantry, having formed infantry squares, took heavy losses from the French guns, while their own cannon fired at the cuirassiers and lancers, when they fell back to regroup. Eventually, the French ceased their assault, after taking heavy losses from the British cannon and musket fire.
The practice of rifling—casting spiralling lines inside the cannon's barrel—was applied to artillery more frequently by 1855, as it gave cannon projectiles gyroscopic stability, which improved their accuracy. One of the earliest rifled cannon was the breech-loading Armstrong Gun—also invented by William George Armstrong—which boasted significantly improved range, accuracy, and power than earlier weapons. The projectile fired from the Armstrong gun could reportedly pierce through a ship's side, and explode inside the enemy vessel, causing increased damage, and casualties. The British military adopted the Armstrong gun, and was impressed; the Duke of Cambridge even declared that it "could do everything but speak." Despite being significantly more advanced than its predecessors, the Armstrong gun was rejected soon after its integration, in favour of the muzzle-loading pieces that had been in use before. While both types of gun were effective against wooden ships, neither had the capability to pierce the armour of ironclads; due to reports of slight problems with the breeches of the Armstrong gun, and their higher cost, the older muzzle-loaders were selected to remain in service instead. Realizing that iron was more difficult to pierce with breech-loaded cannon, Armstrong designed rifled muzzle-loading guns, which proved successful; "The Times" reported: "even the fondest believers in the invulnerability of our present ironclads were obliged to confess that against such artillery, at such ranges, their plates and sides were almost as penetrable as wooden ships."
The superior cannon of the Western world brought them tremendous advantages in warfare. For example, in the Opium War in China, during the 19th century, British battleships bombarded the coastal areas and fortifications from afar, safe from the reach of the Chinese cannon. Similarly, the shortest war in recorded history, the Anglo-Zanzibar War of 1896, was brought to a swift conclusion by shelling from British cruisers. The cynical attitude toward recruited infantry in the face of ever more powerful field artillery is the source of the term "cannon fodder", first used by François-René de Chateaubriand, in 1814; however, the concept of regarding soldiers as nothing more than "food for powder" was mentioned by William Shakespeare as early as 1598, in Henry IV, Part 1.
20th and 21st centuries.
Cannon in the 20th and 21st centuries are usually divided into sub-categories and given separate names. Some of the most widely used types of modern cannon are howitzers, mortars, guns, and autocannon, although a few superguns—extremely large, custom-designed cannon—have also been constructed. Nuclear artillery was experimented with, but was abandoned as impractical. Modern artillery is used in a variety of roles, depending on its type. According to NATO, the general role of artillery is to provide fire support, which is defined as "the application of fire, coordinated with the manoeuvre of forces to destroy, neutralize, or suppress the enemy."
When referring to cannon, the term "gun" is often used incorrectly. In military usage, a gun is a cannon with a high muzzle velocity and a flat trajectory, useful for hitting the sides of targets such as walls, as opposed to howitzers or mortars, which have lower muzzle velocities, and fire indirectly, lobbing shells up and over obstacles to hit the target from above.
By the early 20th century, infantry weapons had become more powerful, forcing most artillery away from the front lines. Despite the change to indirect fire, cannon proved highly effective during World War I, directly or indirectly causing over 75% of casualties. The onset of trench warfare after the first few months of World War I greatly increased the demand for howitzers, as they were more suited at hitting targets in trenches. Furthermore, their shells carried more explosives than those of guns, and caused considerably less barrel wear. The German army had the advantage here as they began the war with many more howitzers than the French. World War I also saw the use of the Paris Gun, the longest-ranged gun ever fired. This calibre gun was used by the Germans against Paris and could hit targets more than away.
The Second World War sparked new developments in cannon technology. Among them were sabot rounds, hollow-charge projectiles, and proximity fuses, all of which increased the effectiveness of cannon against specific target. The proximity fuse emerged on the battlefields of Europe in late December 1944. Used to great effect in anti-aircraft projectiles, proximity fuses were fielded in both the European and Pacific Theatres of Operations; they were particularly useful against V-1 flying bombs and kamikaze planes. Although widely used in naval warfare, and in anti-air guns, both the British and Americans feared unexploded proximity fuses would be reverse engineered leading to them limiting its use in continental battles. During the Battle of the Bulge, however, the fuses became known as the American artillery's "Christmas present" for the German army because of their effectiveness against German personnel in the open, when they frequently dispersed attacks. Anti-tank guns were also tremendously improved during the war: in 1939, the British used primarily 2 pounder and 6 pounder guns. By the end of the war, 17 pounders had proven much more effective against German tanks, and 32 pounders had entered development. Meanwhile, German tanks were continuously upgraded with better main guns, in addition to other improvements. For example, the Panzer III was originally designed with a 37 mm gun, but was mass-produced with a 50 mm cannon. To counter the threat of the Russian T-34s, another, more powerful 50 mm gun was introduced, only to give way to a larger 75 mm cannon, which was in a fixed mount as the StuG III, the most-produced German World War II armoured fighting vehicle of any type. Despite the improved guns, production of the Panzer III was ended in 1943, as the tank still could not match the T-34, and was replaced by the Panzer IV and Panther tanks. In 1944, the 8.8 cm KwK 43 and many variations, entered service with the Wehrmacht, and was used as both a tank main gun, and as the PaK 43 anti-tank gun. One of the most powerful guns to see service in World War II, it was capable of destroying any Allied tank at very long ranges.
Despite being designed to fire at trajectories with a steep angle of descent, howitzers can be fired directly, as was done by the 11th Marine Regiment at the Battle of Chosin Reservoir, during the Korean War. Two field batteries fired directly upon a battalion of Chinese infantry; the Marines were forced to brace themselves against their howitzers, as they had no time to dig them in. The Chinese infantry took heavy casualties, and were forced to retreat.
The tendency to create larger calibre cannon during the World Wars has reversed since. The United States Army, for example, sought a lighter, more versatile howitzer, to replace their ageing pieces. As it could be towed, the M198 was selected to be the successor to the World War II–era cannon used at the time, and entered service in 1979. Still in use today, the M198 is, in turn, being slowly replaced by the M777 Ultralightweight howitzer, which weighs nearly half as much and can be more easily moved. Although land-based artillery such as the M198 are powerful, long-ranged, and accurate, naval guns have not been neglected, despite being much smaller than in the past, and, in some cases, having been replaced by cruise missiles. However, the 's planned armament includes the Advanced Gun System (AGS), a pair of 155 mm guns, which fire the Long Range Land-Attack Projectile. The warhead, which weighs , has a circular error of probability of , and will be mounted on a rocket, to increase the effective range to , further than that of the Paris Gun. The AGS's barrels will be water cooled, and will fire 10 rounds per minute, per gun. The combined firepower from both turrets will give a "Zumwalt"-class destroyer the firepower equivalent to 18 conventional M198 howitzers. The reason for the re-integration of cannon as a main armament in United States Navy ships is because satellite-guided munitions fired from a gun are less expensive than a cruise missile but have a similar guidance capability.
Autocannon.
Autocannons have an automatic firing mode, similar to that of a machine gun. They have mechanisms to automatically load their ammunition, and therefore have a higher rate of fire than artillery, often approaching, or, in the case of rotary autocannons, even surpassing the firing rate of a machine gun. While there is no minimum bore for autocannons, they are generally larger than machine guns, typically 20 mm or greater since World War II and are usually capable of using explosive ammunition even if it isn't always used. Machine guns in contrast are usually too small to use explosive ammunition.
Most nations use rapid-fire cannon on light vehicles, replacing a more powerful, but heavier, tank gun. A typical autocannon is the 25 mm "Bushmaster" chain gun, mounted on the LAV-25 and M2 Bradley armored vehicles. Autocannons may be capable of a very high rate of fire, but ammunition is heavy and bulky, limiting the amount carried. For this reason, both the 25 mm Bushmaster and the 30 mm RARDEN are deliberately designed with relatively low rates of fire. The typical rate of fire for a modern autocannon ranges from 90 to 1,800 rounds per minute. Systems with multiple barrels, such as a rotary autocannon, can have rates of fire of more than several thousand rounds per minute. The fastest of these is the GSh-6-23, which has a rate of fire of over 10,000 rounds per minute.
Autocannons are often found in aircraft, where they replaced machine guns and as shipboard anti-aircraft weapons, as they provide greater destructive power than machine guns.
Aircraft use.
The first documented installation of a cannon on an aircraft was on the Voisin Canon in 1911, displayed at the Paris Exposition that year.
By World War I, all of the major powers were experimenting with aircraft mounted cannon; however their low rate of fire and great size and weight precluded any of them from being anything other than experimental. The most successful (or least unsuccessful) was the SPAD 12 Ca.1 with a single 37mm Puteaux mounted to fire between the cylinder banks and through the propeller boss of the aircraft's Hispano-Suiza 8C. The pilot (by necessity an ace) had to manually reload each round.
The first autocannon were developed during World War I as anti-aircraft guns, and one of these - the Coventry Ordnance Works "COW 37 mm gun" was installed in an aircraft but the war ended before it could be given a field trial and never became standard equipment in a production aircraft. Later trials had it fixed at a steep angle upwards in both the Vickers Type 161 and the Westland C.O.W. Gun Fighter, an idea that would return later.
During this period autocannons became available and several fighters of the German "Luftwaffe" and the Imperial Japanese Navy Air Service were fitted with 20mm cannon. They continued to be installed as an adjunct to machine guns rather than as a replacement, as the rate of fire was still too low and the complete installation too heavy. There was a some debate in the RAF as to whether the greater number of possible rounds being fired from a machine gun, or a smaller number of explosive rounds from a cannon was preferable. Improvements during the war in regards to rate of fire allowed the cannon to displace the machine gun almost entirely. The cannon was more effective against armour so they were increasingly used during the course of World War II, and newer fighters such as the Hawker Tempest usually carried two or four versus the six .50 Browning machine guns for US aircraft or eight to twelve M1919 Browning machine guns on British aircraft. The Hispano-Suiza HS.404, Oerlikon 20 mm cannon, MG FF, and their numerous variants became among the most widely used autocannon in the war. cannon, as with machine guns, were generally fixed to fire forwards (mounted in the wings, in the nose or fuselage, or in a pannier under either); or were mounted in gun turrets on heavier aircraft. Both the Germans and Japanese mounted cannon to fire upwards and forwards for use against heavy bombers, with the Germans calling guns so-installed "Schräge Musik" . Schräge Musik derives from the German colloquialism for Jazz Music (the German word schräg literally means slanted or oblique)
Preceding the Vietnam War the high speeds aircraft were attaining led to a move to remove the cannon due to the mistaken belief that they would be useless in a dogfight, but combat experience during the Vietnam War showed conclusively that despite advances in missiles, there was still a need for them. Nearly all modern fighter aircraft are armed with an autocannon and they are also commonly found on ground-attack aircraft. One of the most powerful examples is the 30mm GAU-8/A Avenger Gatling-type rotary cannon, mounted exclusively on the Fairchild Republic A-10 Thunderbolt II. The Lockheed AC-130 gunship (a converted transport) can carry a 105mm howitzer as well as a variety of autocannons ranging up to 40mm. Both are used in the close air support role.
Operation.
In the 1770s, cannon operation worked as follows: each cannon would be manned by two gunners, six soldiers, and four officers of artillery. The right gunner was to prime the piece and load it with powder, and the left gunner would fetch the powder from the magazine and be ready to fire the cannon at the officer's command. On each side of the cannon, three soldiers stood, to ram and sponge the cannon, and hold the ladle. The second soldier on the left tasked with providing 50 bullets.
Before loading, the cannon would be cleaned with a wet sponge to extinguish any smouldering material from the last shot. Fresh powder could be set off prematurely by lingering ignition sources. The powder was added, followed by wadding of paper or hay, and the ball was placed in and rammed down. After ramming, the cannon would be aimed with the elevation set using a quadrant and a plummet. At 45 degrees, the ball had the utmost range: about ten times the gun's level range. Any angle above a horizontal line was called random-shot. Wet sponges were used to cool the pieces every ten or twelve rounds.
During the Napoleonic Wars, a British gun team consisted of five gunners to aim it, clean the bore with a damp sponge to quench any remaining embers before a fresh charge was introduced, and another to load the gun with a bag of powder and then the projectile. The fourth gunner pressed his thumb on the vent hole, to prevent a draught that might fan a flame. The charge loaded, the fourth would prick the bagged charge through the vent hole, and fill the vent with powder. On command, the fifth gunner would fire the piece with a slowmatch.
When a cannon had to be abandoned such as in a retreat or surrender, the touch hole of the cannon would be plugged flush with an iron spike, disabling the cannon (at least until metal boring tools could be used to remove the plug). This was called "spiking the cannon".
A gun was said to be "honeycombed" when the surface of the bore had cavities, or holes in it, caused either by corrosion or casting defects.
Deceptive use.
Historically, logs or poles have been used as decoys to mislead the enemy as to the strength of an emplacement. The "Quaker gun trick" was used by Colonel William Washington's Continentals, during the American Revolutionary War; in 1780, approximately 100 Loyalists surrendered to them, rather than face bombardment. During the American Civil War, Quaker guns were also used by the Confederates, to compensate for their shortage of artillery. The decoy cannon were painted black at the "muzzle", and positioned behind fortifications to delay Union attacks on those positions. On occasion, real gun carriages were used to complete the deception.
In popular culture.
Music.
Cannon sounds have sometimes been used in classical pieces with a military theme. Giuseppe Sarti is believed to be the first composer to orchestrate real cannon in a musical work. His "Te Deum" celebrates the Russian victory at Ochakov (1789) with the firing of a real cannon and the use of fireworks, to heighten the martial effect of the music.
One of the best known examples of such a piece is another Russian work, Pyotr Ilyich Tchaikovsky's "1812 Overture". The overture is properly performed using an artillery section together with the orchestra, resulting in noise levels requiring musicians to wear ear protection. The cannon fire simulates Russian artillery bombardments of the Battle of Borodino, a critical battle in Napoleon's invasion of Russia, whose defeat the piece celebrates. When the overture was first performed, the cannon were fired by an electric current triggered by the conductor. However, the overture was not recorded with real cannon fire until Mercury Records and conductor Antal Doráti's 1958 recording of the Minnesota Orchestra. Cannon fire is also frequently used annually in presentations of the "1812" on the American Independence Day, a tradition started by Arthur Fiedler of the Boston Pops in 1974.
The hard rock band AC/DC also used cannon in their song "For Those About to Rock (We Salute You)", and in live shows replica Napoleonic cannon and pyrotechnics were used to perform the piece.
Restoration.
Cannon recovered from the sea are often extensively damaged from exposure to salt water; because of this, electrolytic reduction treatment is required to forestall the process of corrosion. The cannon is then washed in deionized water to remove the electrolyte, and is treated in tannic acid, which prevents further rust and gives the metal a bluish-black colour. After this process, cannon on display may be protected from oxygen and moisture by a wax sealant. A coat of polyurethane may also be painted over the wax sealant, to prevent the wax-coated cannon from attracting dust in outdoor displays.
Recently archaeologists say six cannon recovered from a river in Panama that could have belonged to legendary pirate Henry Morgan are being studied and could eventually be displayed after going through a restoration process.

</doc>
<doc id="7056" url="https://en.wikipedia.org/wiki?curid=7056" title="Computer mouse">
Computer mouse

A computer mouse is a pointing device (hand control) that detects two-dimensional motion relative to a surface. This motion is typically translated into the motion of a pointer on a display, which allows a smooth control of the graphical user interface.
Physically, a mouse consists of an object held in one's hand, with one or more buttons. Mice often also feature other elements, such as touch surfaces and "wheels", which enable additional control and dimensional input.
Naming.
The earliest known publication of the term "mouse" as a computer pointing device is in Bill English's 1965 publication "Computer-Aided Display Control".
The online "Oxford Dictionaries" entry for "mouse" states the plural for the small rodent is "mice", while the plural for the small computer connected device is either "mice" or "mouses". The dictionary's usage section states that the more common plural is "mice" and claims the first recorded use of the plural is "mice" (though it cites a 1984 use of "mice" when there were actually several earlier ones, such as J. C. R. Licklider's "The Computer as a Communication Device" of 1968). According to the fifth edition of "The American Heritage Dictionary of the English Language" the plural can be either "mice" or "mouses".
History.
The trackball, a related pointing device, was invented in 1941 by Ralph Benjamin as part of a World War II-era fire-control radar plotting system called Comprehensive Display System (CDS). Benjamin was then working for the British Royal Navy Scientific Service. Benjamin's project used analog computers to calculate the future position of target aircraft based on several initial input points provided by a user with a joystick. Benjamin felt that a more elegant input device was needed and invented what they called a "roller ball" for this purpose.
The device was patented in 1947, but only a prototype using a metal ball rolling on two rubber-coated wheels was ever built, and the device was kept as a military secret.
Another early trackball was built by British electrical engineer Kenyon Taylor in collaboration with Tom Cranston and Fred Longstaff. Taylor was part of the original Ferranti Canada, working on the Royal Canadian Navy's DATAR (Digital Automated Tracking and Resolving) system in 1952.
DATAR was similar in concept to Benjamin's display. The trackball used four disks to pick up motion, two each for the X and Y directions. Several rollers provided mechanical support. When the ball was rolled, the pickup discs spun and contacts on their outer rim made periodic contact with wires, producing pulses of output with each movement of the ball. By counting the pulses, the physical movement of the ball could be determined. A digital computer calculated the tracks, and sent the resulting data to other ships in a task force using pulse-code modulation radio signals. This trackball used a standard Canadian five-pin bowling ball. It was not patented, as it was a secret military project as well.
On 2 October 1968, a mouse device named ' (German for "rolling ball") was released that had been developed and published by the German company Telefunken. As the name suggests and unlike Engelbart's mouse, the Telefunken model already had a ball. It was based on an earlier trackball-like device (also named ') that was embedded into radar flight control desks. This had been developed around 1965 by a team led by Rainer Mallebrein at Telefunken for the German "Bundesanstalt für Flugsicherung" as part of their TR 86 process computer system with its SIG 100-86 vector graphics terminal.
When the development for the Telefunken main frame began in 1965, and his team came up with the idea of "reversing" the existing into a moveable mouse-like device, so that customers did not have to be bothered with mounting holes for the earlier trackball device. Together with light pens and trackballs, it was offered as optional input device for their system since 1968. Some samples, installed at the in Munich in 1972, are still well preserved. Telefunken considered the invention too small to apply for a patent on their device.
A few months after Telefunken started to sell the Rollkugel, Engelbart released his demo on 9 December 1968.
Independently, Douglas Engelbart at the Stanford Research Institute (now SRI International) invented his first mouse prototype in the 1960s with the assistance of his lead engineer Bill English. They christened the device the "mouse" as early models had a cord attached to the rear part of the device looking like a tail and generally resembling the common mouse. Engelbart never received any royalties for it, as his employer SRI held the patent, which ran out before it became widely used in personal computers. The invention of the mouse was just a small part of Engelbart's much larger project, aimed at augmenting human intellect via the Augmentation Research Center.
Several other experimental pointing-devices developed for Engelbart's oN-Line System (NLS) exploited different body movements – for example, head-mounted devices attached to the chin or nose – but ultimately the mouse won out because of its speed and convenience. The first mouse, a bulky device (pictured) used two potentiometers perpendicular to each other and connected to wheels: the rotation of each wheel translated into motion along one axis. At the time of the "Mother of All Demos", Englebart's group had been using their second generation, 3-button mouse for about a year.
The Xerox Alto was one of the first computers designed for individual use in 1973, and is regarded as the grandfather of computers that utilize the mouse. Inspired by PARC's Alto, the Lilith, a computer which had been developed by a team around at ETH Zürich between 1978 and 1980, provided a mouse as well. The third marketed version of an integrated mouse shipped as a part of a computer and intended for personal computer navigation came with the Xerox 8010 Star Information System in 1981.
By 1982 the Xerox 8010 was probably the best-known computer with a mouse, and the forthcoming Apple Lisa was rumored to use one, but the peripheral remained obscure; Jack Hawley of The Mouse House reported that one buyer for a large organization believed at first that his company sold lab mice. Hawley, who manufactured mice for Xerox, stated that "Practically, I have the market all to myself right now"; a Hawley mouse cost $415. That year Microsoft made the decision to make the MS-DOS program Microsoft Word mouse-compatible, and developed the first PC-compatible mouse. Microsoft's mouse shipped in 1983, thus beginning Microsoft hardware. However, the mouse remained relatively obscure until the 1984 appearance of the Macintosh 128K, which included an updated version of the Lisa Mouse and the Atari ST in 1985.
Operation.
A mouse typically controls the motion of a pointer in two dimensions in a graphical user interface (GUI). The mouse turns movements of the hand backward and forward, left and right into equivalent electronic signals that in turn are used to move the pointer.
The relative movements of the mouse on the surface are applied to the position of the pointer on the screen, which signals the point where actions of the user take place, so that the hand movements are replicated by the pointer. Clicking or hovering (stopping movement while the cursor is within the bounds of an area) can select files, programs or actions from a list of names, or (in graphical interfaces) through small images called "icons" and other elements. For example, a text file might be represented by a picture of a paper notebook, and clicking while the cursor hovers this icon might cause a text editing program to open the file in a window.
Different ways of operating the mouse cause specific things to happen in the GUI:
Mouse gestures.
Users can also employ mice "gesturally"; meaning that a stylized motion of the mouse cursor itself, called a "gesture", can issue a command or map to a specific action. For example, in a drawing program, moving the mouse in a rapid "x" motion over a shape might delete the shape.
Gestural interfaces occur more rarely than plain pointing-and-clicking; and people often find them more difficult to use, because they require finer motor-control from the user. However, a few gestural conventions have become widespread, including the drag and drop gesture, in which:
For example, a user might drag-and-drop a picture representing a file onto a picture of a trash can, thus instructing the system to delete the file.
Standard semantic gestures include:
Specific uses.
Other uses of the mouse's input occur commonly in special application-domains. In interactive three-dimensional graphics, the mouse's motion often translates directly into changes in the virtual objects' or camera's orientation. For example, in the first-person shooter genre of games (see below), players usually employ the mouse to control the direction in which the virtual player's "head" faces: moving the mouse up will cause the player to look up, revealing the view above the player's head. A related function makes an image of an object rotate, so that all sides can be examined. 3D design and animation software often modally chords many different combinations to allow objects and cameras to be rotated and moved through space with the few axes of movement mice can detect.
When mice have more than one button, software may assign different functions to each button. Often, the primary (leftmost in a right-handed configuration) button on the mouse will select items, and the secondary (rightmost in a right-handed) button will bring up a menu of alternative actions applicable to that item. For example, on platforms with more than one button, the Mozilla web browser will follow a link in response to a primary button click, will bring up a contextual menu of alternative actions for that link in response to a secondary-button click, and will often open the link in a new tab or window in response to a click with the tertiary (middle) mouse button.
Variants.
Mechanical mice.
The German company Telefunken published on their early ball mouse on October 2, 1968. Telefunken's mouse was sold as optional equipment for their computer systems. Bill English, builder of Engelbart's original mouse, created a ball mouse in 1972 while working for Xerox PARC.
The ball mouse replaced the external wheels with a single ball that could rotate in any direction. It came as part of the hardware package of the Xerox Alto computer. Perpendicular chopper wheels housed inside the mouse's body chopped beams of light on the way to light sensors, thus detecting in their turn the motion of the ball. This variant of the mouse resembled an inverted trackball and became the predominant form used with personal computers throughout the 1980s and 1990s. The Xerox PARC group also settled on the modern technique of using both hands to type on a full-size keyboard and grabbing the mouse when required.
The ball mouse has two freely rotating rollers. They are located 90 degrees apart. One roller detects the forward–backward motion of the mouse and other the left–right motion. Opposite the two rollers is a third one (white, in the photo, at 45 degrees) that is spring-loaded to push the ball against the other two rollers. Each roller is on the same shaft as an encoder wheel that has slotted edges; the slots interrupt infrared light beams to generate electrical pulses that represent wheel movement. Each wheel's disc, however, has a pair of light beams, located so that a given beam becomes interrupted, or again starts to pass light freely, when the other beam of the pair is about halfway between changes.
Simple logic circuits interpret the relative timing to indicate which direction the wheel is rotating. This incremental rotary encoder scheme is sometimes called quadrature encoding of the wheel rotation, as the two optical sensor produce signals that are in approximately quadrature phase. The mouse sends these signals to the computer system via the mouse cable, directly as logic signals in very old mice such as the Xerox mice, and via a data-formatting IC in modern mice. The driver software in the system converts the signals into motion of the mouse cursor along X and Y axes on the computer screen.
The ball is mostly steel, with a precision spherical rubber surface. The weight of the ball, given an appropriate working surface under the mouse, provides a reliable grip so the mouse's movement is transmitted accurately. Ball mice and wheel mice were manufactured for Xerox by Jack Hawley, doing business as The Mouse House in Berkeley, California, starting in 1975. Based on another invention by Jack Hawley, proprietor of the Mouse House, Honeywell produced another type of mechanical mouse. Instead of a ball, it had two wheels rotating at off axes. Key Tronic later produced a similar product.
Modern computer mice took form at the École Polytechnique Fédérale de Lausanne (EPFL) under the inspiration of Professor Jean-Daniel Nicoud and at the hands of engineer and watchmaker André Guignard. This new design incorporated a single hard rubber mouseball and three buttons, and remained a common design until the mainstream adoption of the scroll-wheel mouse during the 1990s. In 1985, René Sommer added a microprocessor to Nicoud's and Guignard's design. Through this innovation, Sommer is credited with inventing a significant component of the mouse, which made it more "intelligent;" though optical mice from Mouse Systems had incorporated microprocessors by 1984.
Another type of mechanical mouse, the "analog mouse" (now generally regarded as obsolete), uses potentiometers rather than encoder wheels, and is typically designed to be plug compatible with an analog joystick. The "Color Mouse", originally marketed by RadioShack for their Color Computer (but also usable on MS-DOS machines equipped with analog joystick ports, provided the software accepted joystick input) was the best-known example.
Optical and laser mice.
Optical mice rely entirely on one or more light-emitting diodes (LEDs) and an imaging array of photodiodes to detect movement relative to the underlying surface, eschewing the internal moving parts a mechanical mouse uses in addition to its optics. A laser mouse is an optical mouse that uses coherent (laser) light.
The earliest optical mice detected movement on pre-printed mousepad surfaces, whereas the modern LED optical mouse works on most opaque diffuse surfaces; it is usually unable to detect movement on specular surfaces like polished stone. Laser diodes are also used for better resolution and precision, improving performance on opaque specular surfaces. Battery powered, wireless optical mice flash the LED intermittently to save power, and only glow steadily when movement is detected.
Inertial and gyroscopic mice.
Often called "air mice" since they do not require a surface to operate, inertial mice use a tuning fork or other accelerometer (US Patent 4787051, published in 1988) to detect rotary movement for every axis supported. The most common models (manufactured by Logitech and Gyration) work using 2 degrees of rotational freedom and are insensitive to spatial translation. The user requires only small wrist rotations to move the cursor, reducing user fatigue or "gorilla arm".
Usually cordless, they often have a switch to deactivate the movement circuitry between use, allowing the user freedom of movement without affecting the cursor position. A patent for an inertial mouse claims that such mice consume less power than optically based mice, and offer increased sensitivity, reduced weight and increased ease-of-use. In combination with a wireless keyboard an inertial mouse can offer alternative ergonomic arrangements which do not require a flat work surface, potentially alleviating some types of repetitive motion injuries related to workstation posture.
3D mice.
Also known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion/Logitech's SpaceMouse from the early 1990s. In the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.
A recent consumer 3D pointing device is the Wii Remote. While primarily a motion-sensing device (that is, it can determine its orientation and direction of movement), Wii Remote can also detect its spatial position by comparing the distance and position of the lights from the IR emitter using its integrated IR camera (since the nunchuk accessory lacks a camera, it can only tell its current heading and orientation). The obvious drawback to this approach is that it can only produce spatial coordinates while its camera can see the sensor bar.
A mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each. In November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.
Tactile mice.
In 2000, Logitech introduced a "tactile mouse" that contained a small actuator to make the mouse vibrate. Such a mouse can augment user-interfaces with haptic feedback, such as giving feedback when crossing a window boundary. To surf by touch requires the user to be able to feel depth or hardness; this ability was realized with the first electrorheological tactile mice but never marketed.
Pucks.
Tablet digitizers are sometimes used with accessories called pucks, devices which rely on absolute positioning, but can be configured for sufficiently mouse-like relative tracking that they are sometimes marketed as mice.
Ergonomic mice.
As the name suggests, this type of mouse is intended to provide optimum comfort and avoid injuries such as carpal tunnel syndrome, arthritis and other repetitive strain injuries. It is designed to fit natural hand position and movements, to reduce discomfort.
When holding a typical mouse, ulna and radius bones on the arm are crossed. Some designs attempt to place the palm more vertically, so the bones take more natural parallel position. Some limit wrist movement, encouraging to use arm instead that may be less precise but more optimal from the health point of view. A mouse may be angled from the thumb downward to the opposite side – this is known to reduce wrist pronation. However such optimizations make the mouse right or left hand specific, making more problematic to change the tired hand. Time magazine has criticised manufacturers for offering few or no left-handed ergonomic mice: "Oftentimes I felt like I was dealing with someone who’d never actually met a left-handed person before."
Another solution is a pointing bar device. The so-called "roller bar mouse" is positioned snuggly in front of the keyboard, thus allowing bi-manual accessibility.
Gaming mice.
These mice are specifically designed for use in computer games. They typically employ a wide array of controls and buttons and have designs that differ radically from traditional mice. It is also common for gaming mice, especially those designed for use in real-time strategy games such as "StarCraft", or in multiplayer online battle arena games such as "Dota 2" to have a relatively high sensitivity, measured in dots per inch (DPI). Some advanced mice from gaming manufacturers also allow users to customize the weight of the mouse by adding or subtracting weights to allow for easier control. Ergonomic quality is also an important factor in gaming mice, as extended gameplay times may render further use of the mouse to be uncomfortable. Some mice have been designed to have adjustable features such as removable and/or elongated palm rests, horizontally adjustable thumb rests and pinky rests. Some mice may include several different rests with their products to ensure comfort for a wider range of target consumers. Gaming mice are held by gamers in three styles of grip:
Connectivity and communication protocols.
To transmit their input, typical cabled mice use a thin electrical cord terminating in a standard connector, such as RS-232C, PS/2, ADB or USB. Cordless mice instead transmit data via infrared radiation (see IrDA) or radio (including Bluetooth), although many such cordless interfaces are themselves connected through the aforementioned wired serial buses.
While the electrical interface and the format of the data transmitted by commonly available mice is currently standardized on USB, in the past it varied between different manufacturers. A bus mouse used a dedicated interface card for connection to an IBM PC or compatible computer.
Mouse use in DOS applications became more common after the introduction of the Microsoft mouse, largely because Microsoft provided an open standard for communication between applications and mouse driver software. Thus, any application written to use the Microsoft standard could use a mouse with a driver that implements the same API, even if the mouse hardware itself was incompatible with Microsoft's. This driver provides the state of the buttons and the distance the mouse has moved in units that its documentation calls "mickeys", as does the Allegro library.
Serial interface and protocol.
Standard PC mice once used the RS-232C serial port via a D-subminiature connector, which provided power to run the mouse's circuits as well as data on mouse movements. The Mouse Systems Corporation version used a five-byte protocol and supported three buttons. The Microsoft version used a three-byte protocol and supported two buttons. Due to the incompatibility between the two protocols, some manufacturers sold serial mice with a mode switch: "PC" for MSC mode, "MS" for Microsoft mode.
PS/2 interface and protocol.
With the arrival of the IBM PS/2 personal-computer series in 1987, IBM introduced the eponymous PS/2 interface for mice and keyboards, which other manufacturers rapidly adopted. The most visible change was the use of a round 6-pin mini-DIN, in lieu of the former 5-pin connector. In default mode (called "stream mode") a PS/2 mouse communicates motion, and the state of each button, by means of 3-byte packets. For any motion, button press or button release event, a PS/2 mouse sends, over a bi-directional serial port, a sequence of three bytes, with the following format:
Here, XS and YS represent the sign bits of the movement vectors, XV and YV indicate an overflow in the respective vector component, and LB, MB and RB indicate the status of the left, middle and right mouse buttons (1 = pressed). PS/2 mice also understand several commands for reset and self-test, switching between different operating modes, and changing the resolution of the reported motion vectors.
A Microsoft IntelliMouse relies on an extension of the PS/2 protocol: the ImPS/2 or IMPS/2 protocol (the abbreviation combines the concepts of "IntelliMouse" and "PS/2"). It initially operates in standard PS/2 format, for backwards compatibility. After the host sends a special command sequence, it switches to an extended format in which a fourth byte carries information about wheel movements. The IntelliMouse Explorer works analogously, with the difference that its 4-byte packets also allow for two additional buttons (for a total of five).
Mouse vendors also use other extended formats, often without providing public documentation. The Typhoon mouse uses 6-byte packets which can appear as a sequence of two standard 3-byte packets, such that an ordinary PS/2 driver can handle them. For 3-D (or 6-degree-of-freedom) input, vendors have made many extensions both to the hardware and to software. In the late 1990s Logitech created ultrasound based tracking which gave 3D input to a few millimetres accuracy, which worked well as an input device but failed as a profitable product. In 2008, Motion4U introduced its "OptiBurst" system using IR tracking for use as a Maya (graphics software) plugin.
Apple Desktop Bus.
In 1986 Apple first implemented the Apple Desktop Bus allowing the daisy-chaining together of up to 16 devices, including arbitrarily many mice and other devices on the same bus with no configuration whatsoever. Featuring only a single data pin, the bus used a purely polled approach to computer/mouse communications and survived as the standard on mainstream models (including a number of non-Apple workstations) until 1998 when iMac joined the industry-wide switch to using USB. Beginning with the Bronze Keyboard PowerBook G3 in May 1999, Apple dropped the external ADB port in favor of USB, but retained an internal ADB connection in the PowerBook G4 for communication with its built-in keyboard and trackpad until early 2005.
USB.
The industry-standard USB (Universal Serial Bus) protocol and its connector have become widely used for mice; it is among the most popular types.
Cordless or wireless.
Cordless or wireless mice transmit data via infrared radiation (see IrDA) or radio (including Bluetooth and Wi-Fi). The receiver is connected to the computer through a serial or USB port, or can be built in (as is sometimes the case with Bluetooth and WiFi).
Modern non-Bluetooth and non-WiFi wireless mice use USB receivers. Some of these can be stored inside the mouse for safe transport while not in use, while other, newer mice use newer "nano" receivers, designed to be small enough to remain plugged into a laptop during transport, while still being large enough to easily remove.
Atari standard joystick connectivity.
The Amiga and the Atari ST use an Atari standard DE-9 connector for mice, the same connector that is used for joysticks on the same computers and numerous 8-bit systems, such as the Commodore 64 and the Atari 2600. However, the signals used for mice are different from those used for joysticks. As a result, plugging a mouse into a joystick port causes the "joystick" to continuously move in some direction, even if the mouse stays still, whereas plugging a joystick into a mouse port causes the "mouse" to only be able to move a single pixel in each direction.
Multiple-mouse systems.
Some systems allow two or more mice to be used at once as input devices. 16-bit era home computers such as the Amiga used this to allow computer games with two players interacting on the same computer (Lemmings and The Settlers for example). The same idea is sometimes used in collaborative software, e.g. to simulate a whiteboard that multiple users can draw on without passing a single mouse around.
Microsoft Windows, since Windows 98, has supported multiple simultaneous pointing devices. Because Windows only provides a single screen cursor, using more than one device at the same time requires cooperation of users or applications designed for multiple input devices.
Multiple mice are often used in multi-user gaming in addition to specially designed devices that provide several input interfaces.
Windows also has full support for multiple input/mouse configurations for multiuser environments.
Starting with Windows XP, Microsoft introduced a SDK for developing applications that allow multiple input devices to be used at the same time with independent cursors and independent input points.
The introduction of Vista and Microsoft Surface (now known as Microsoft PixelSense) introduced a new set of input APIs that were adopted into Windows 7, allowing for 50 points/cursors, all controlled by independent users. The new input points provide traditional mouse input; however, are designed for more advanced input technology like touch and image. They inherently offer 3D coordinates along with pressure, size, tilt, angle, mask, and even an image bitmap to see and recognize the input point/object on the screen.
As of 2009, Linux distributions and other operating systems that use X.Org, such as OpenSolaris and FreeBSD, support 255 cursors/input points through Multi-Pointer X. However, currently no window managers support Multi-Pointer X leaving it relegated to custom software usage.
There have also been propositions of having a single operator use two mice simultaneously as a more sophisticated means of controlling various graphics and multimedia applications.
Buttons.
Mouse buttons are microswitches which can be pressed to select or interact with an element of a graphical user interface, producing a distinctive clicking sound.
Since around the late 1990s, the three-button scrollmouse has become the de facto standard. Users most commonly employ the second button to invoke a contextual menu in the computer's software user interface, which contains options specifically tailored to the interface element over which the mouse cursor currently sits. By default, the primary mouse button sits located on the left-hand side of the mouse, for the benefit of right-handed users; left-handed users can usually reverse this configuration via software.
Scrolling.
Nearly all mice now have an integrated input primarily intended for scrolling on top, usually a single-axis digital wheel or rocker switch which can also be depressed to act as a third button. Though less common, many mice instead have two-axis inputs such as a tiltable wheel, trackball, or touchpad.
Mouse speed.
Mickeys per second is a unit of measurement for the speed and movement direction of a computer mouse. But speed can also refer to the ratio between how many pixels the cursor moves on the screen and how far the mouse moves on the mouse pad, which may be expressed as pixels per Mickey, or pixels per inch, or pixels per cm. The directional movement is called the horizontal mickey count and the vertical mickey count.
The computer industry often measures mouse sensitivity in terms of counts per inch (CPI), commonly expressed as dots per inch (DPI)the number of steps the mouse will report when it moves one inch. In early mice, this specification was called pulses per inch (ppi). The Mickey originally referred to one of these counts, or one resolvable step of motion. If the default mouse-tracking condition involves moving the cursor by one screen-pixel or dot on-screen per reported step, then the CPI does equate to DPI: dots of cursor motion per inch of mouse motion. The CPI or DPI as reported by manufacturers depends on how they make the mouse; the higher the CPI, the faster the cursor moves with mouse movement. However, software can adjust the mouse sensitivity, making the cursor move faster or slower than its CPI. software can change the speed of the cursor dynamically, taking into account the mouse's absolute speed and the movement from the last stop-point. In most software, an example being the Windows platforms, this setting is named "speed" referring to "cursor precision". However, some operating systems name this setting "acceleration", the typical Apple OS designation. This term is in fact incorrect. The mouse acceleration, in the majority of mouse software, refers to the setting allowing the user to modify the cursor acceleration: the change in speed of the cursor over time while the mouse movement is constant.
For simple software, when the mouse starts to move, the software will count the number of "counts" or "mickeys" received from the mouse and will move the cursor across the screen by that number of pixels (or multiplied by a rate factor, typically less than 1). The cursor will move slowly on the screen, having a good precision. When the movement of the mouse passes the value set for "threshold", the software will start to move the cursor more quickly, with a greater rate factor. Usually, the user can set the value of the second rate factor by changing the "acceleration" setting.
Operating systems sometimes apply acceleration, referred to as "ballistics", to the motion reported by the mouse. For example, versions of Windows prior to Windows XP doubled reported values above a configurable threshold, and then optionally doubled them again above a second configurable threshold. These doublings applied separately in the X and Y directions, resulting in very nonlinear response.
Mousepads.
Engelbart's original mouse did not require a mousepad; the mouse had two large wheels which could roll on virtually any surface. However, most subsequent mechanical mice starting with the steel roller ball mouse have required a mousepad for optimal performance.
The mousepad, the most common mouse accessory, appears most commonly in conjunction with mechanical mice, because to roll smoothly the ball requires more friction than common desk surfaces usually provide. So-called "hard mousepads" for gamers or optical/laser mice also exist.
Most optical and laser mice do not require a pad. Whether to use a hard or soft mousepad with an optical mouse is largely a matter of personal preference. One exception occurs when the desk surface creates problems for the optical or laser tracking, for example, a transparent or reflective surface.
In the marketplace.
Around 1981 Xerox included mice with its Xerox Star, based on the mouse used in the 1970s on the Alto computer at Xerox PARC. Sun Microsystems, Symbolics, Lisp Machines Inc., and Tektronix also shipped workstations with mice, starting in about 1981. Later, inspired by the Star, Apple Computer released the Apple Lisa, which also used a mouse. However, none of these products achieved large-scale success. Only with the release of the Apple Macintosh in 1984 did the mouse see widespread use.
The Macintosh design, commercially successful and technically influential, led many other vendors to begin producing mice or including them with their other computer products (by 1986, Atari ST, Amiga, Windows 1.0, GEOS for the Commodore 64, and the Apple IIGS).
The widespread adoption of graphical user interfaces in the software of the 1980s and 1990s made mice all but indispensable for controlling computers. In November 2008, Logitech built their billionth mouse.
Use in games.
The Mac OS Desk Accessory "Puzzle" in 1984 was the first game designed specifically for a mouse. The device often functions as an interface for PC-based computer games and sometimes for video game consoles.
First-person shooters.
FPSs naturally lend themselves to separate and simultaneous control of the player's movement and aim, and on computers this has traditionally been achieved with a combination of keyboard and mouse. Players use the X-axis of the mouse for looking (or turning) left and right, and the Y-axis for looking up and down; the keyboard is used for movement and supplemental inputs.
Many shooting genre players prefer a mouse over a gamepad analog stick because the mouse is a linear input device, which allows for fast and precise control. Holding a stick in a given position produces a corresponding constant movement or rotation, i.e. the output is an integral of the user's input, and requires that time be spent moving to or from its null position before this input can be given; in contrast, the output of a mouse directly and instantaneously corresponds to how far it is moved in a given direction (often multiplied by an "acceleration" factor derived from how quickly the mouse is moved). The effect of this is that a mouse is well suited to small, precise movements; large, quick movements; and immediate, responsive movements; all of which are important in shooter gaming. This advantage also extends in varying degrees to similar game styles such as third-person shooters.
Some incorrectly ported games or game engines have acceleration and interpolation curves which unintentionally produce excessive, irregular, or even negative acceleration when used with a mouse instead of their native platform's non-mouse default input device. Depending on how deeply hardcoded this misbehavior is, internal user patches or external 3rd-party software may be able to fix it.
Due to their similarity to the WIMP desktop metaphor interface for which mice were originally designed, and to their own tabletop game origins, computer strategy games are most commonly played with mice. In particular, real-time strategy and MOBA games usually require the use of a mouse.
The left button usually controls primary fire. If the game supports multiple fire modes, the right button often provides secondary fire from the selected weapon. Games with only a single fire mode will generally map secondary fire to "ironsights". In some games, the right button may also invoke accessories for a particular weapon, such as allowing access to the scope of a sniper rifle or allowing the mounting of a bayonet or silencer.
Gamers can use a scroll wheel for changing weapons (or for controlling scope-zoom magnification, in older games). On most first person shooter games, programming may also assign more functions to additional buttons on mice with more than three controls. A keyboard usually controls movement (for example, WASD for moving forward, left, backward and right, respectively) and other functions such as changing posture. Since the mouse serves for aiming, a mouse that tracks movement accurately and with less lag (latency) will give a player an advantage over players with less accurate or slower mice. In some cases the right mouse button may be used to move the player forward, either in lieu of, or in conjunction with the typical WASD configuration.
Many games provide players with the option of mapping their own choice of a key or button to a certain control.
An early technique of players, circle strafing, saw a player continuously strafing while aiming and shooting at an opponent by walking in circle around the opponent with the opponent at the center of the circle. Players could achieve this by holding down a key for strafing while continuously aiming the mouse towards the opponent.
Games using mice for input are so popular that many manufacturers make mice specifically for gaming. Such mice may feature adjustable weights, high-resolution optical or laser components, additional buttons, ergonomic shape, and other features such as adjustable CPI.
Many games, such as first- or third-person shooters, have a setting named "invert mouse" or similar (not to be confused with "button inversion", sometimes performed by left-handed users) which allows the user to look downward by moving the mouse forward and upward by moving the mouse backward (the opposite of non-inverted movement). This control system resembles that of aircraft control sticks, where pulling back causes pitch up and pushing forward causes pitch down; computer joysticks also typically emulate this control-configuration.
After id Software's commercial hit of "Doom", which did not support vertical aiming, competitor Bungie's "Marathon" became the first first-person shooter to support using the mouse to aim up and down. Games using the Build engine had an option to invert the Y-axis. The "invert" feature actually made the mouse behave in a manner that users regard as non-inverted (by default, moving mouse forward resulted in looking down). Soon after, id Software released "Quake", which introduced the invert feature as users know it.
Home consoles.
In 1988, the VTech Socrates educational video game console featured a wireless mouse with an attached mouse pad as an optional controller used for some games. In the early 1990s, the Super Nintendo Entertainment System video game system featured a mouse in addition to its controllers. The "Mario Paint" game in particular used the mouse's capabilities as did its successor on the N64. Sega released official mice for their Genesis/Mega Drive, Saturn and Dreamcast consoles. NEC sold official mice for its PC Engine and PC-FX consoles. Sony released an official mouse product for the PlayStation console, included one along with the Linux for PlayStation 2 kit, as well as allowing owners to use virtually any USB mouse with the PS2, PS3, and PS4. Nintendo's Wii also had this added on in a later software update, retained on the Wii U.

</doc>
<doc id="7059" url="https://en.wikipedia.org/wiki?curid=7059" title="Civil defense">
Civil defense

Civil defense, civil defence (see spelling differences) or civil protection is an effort to protect the citizens of a state (generally non-combatants) from military attack. It uses the principles of emergency operations: prevention, mitigation, preparation, response, or emergency evacuation and recovery. Programs of this sort were initially discussed at least as early as the 1920s and were implemented in some countries during the 1930s as the threat of war and aerial bombardment grew. It became widespread after the threat of nuclear weapons was realized.
Since the end of the Cold War, the focus of civil defense has largely shifted from military attack to emergencies and disasters in general. The new concept is described by a number of terms, each of which has its own specific shade of meaning, such as crisis management, emergency management, emergency preparedness, contingency planning, emergency services, and civil protection.
In some countries, civil defense is seen as a key part of "total defense". For example, in Sweden, the Swedish word "totalförsvar" refers to the commitment of a wide range of resources of the nation to its defense - including to civil protection. Respectively, some countries (notably the Soviet Union) may have or have had military-organized civil defense units (Civil Defense Troops) as part of their armed forces or as a paramilitary service.
History.
Origins.
United Kingdom.
The advent of civil defence was stimulated by the experience of the bombing of civilian areas during the First World War. The bombing of Britain began on 19 January 1915 when German zeppelins dropped bombs on the Great Yarmouth area, killing six people. German bombing operations of the First World War were surprisingly effective, especially after the Gotha bombers surpassed the zeppelins. The most devastating raids inflicted 121 casualties for each ton of bombs dropped; this figure was then used as a basis for predictions.
After the war, attention was turned toward civil defence in the event of war, and the Air Raid Precautions Committee (ARP) was established in 1924 to investigate ways for ensuring the protection of civilians from the danger of air-raids.
The Committee produced figures estimating that in London there would be 9,000 casualties in the first two days and then a continuing rate of 17,500 casualties a week. These rates were thought conservative. It was believed that there would be "total chaos and panic" and hysterical neurosis as the people of London would try to flee the city. To control the population harsh measures were proposed: bringing London under almost military control, and physically cordoning off the city with 120,000 troops to force people back to work. A different government department proposed setting up camps for refugees for a few days before sending them back to London.
A special government department, the Civil Defence Service, was established by the Home Office in 1935. Its remit included the pre-existing ARP as well as wardens, firemen (initially the Auxiliary Fire Service (AFS) and latterly the National Fire Service (NFS)), fire watchers, rescue, first aid post, stretcher party and industry. Over 1.9 million people served within the CD; nearly 2,400 lost their lives to enemy action.
The organisation of civil defence was the responsibility of the local authority. Volunteers were ascribed to different units depending on experience or training. Each local civil defence service was divided into several sections. Wardens were responsible for local reconnaissance and reporting, and leadership, organisation, guidance and control of the general public. Wardens would also advise survivors of the locations of rest and food centres, and other welfare facilities.
Rescue Parties were required to assess and then access bombed-out buildings and retrieve injured or dead people. In addition they would turn off gas, electricity and water supplies, and repair or pull down unsteady buildings. Medical services, including First Aid Parties, provided on the spot medical assistance.
The expected stream of information that would be generated during an attack was handled by 'Report and Control' teams. A local headquarters would have an ARP controller who would direct rescue, first aid and decontamination teams to the scenes of reported bombing. If local services were deemed insufficient to deal with the incident then the controller could request assistance from surrounding boroughs.
Fire Guards were responsible for a designated area/building and required to monitor the fall of incendiary bombs and pass on news of any fires that had broken out to the NFS. They could deal with an individual magnesium electron incendiary bomb by dousing it with buckets of sand or water or by smothering. Additionally, 'Gas Decontamination Teams' kitted out with gas-tight and waterproof protective clothing were to deal with any gas attacks. They were trained to decontaminate buildings, roads, rail and other material that had been contaminated by liquid or jelly gases.
Little progress was made over the issue of air-raid shelters, because of the apparently irreconcilable conflict between the need to send the public underground for shelter and the need to keep them above ground for protection against gas attacks. In February 1936 the Home Secretary appointed a technical Committee on Structural Precautions against Air Attack. During the Munich crisis, local authorities dug trenches to provide shelter. After the crisis, the British Government decided to make these a permanent feature, with a standard design of precast concrete trench lining. They also decided to issue the Anderson shelter free to poorer households and to provide steel props to create shelters in suitable basements.
During the Second World War, the ARP was responsible for the issuing of gas masks, pre-fabricated air-raid shelters (such as Anderson shelters, as well as Morrison shelters), the upkeep of local public shelters, and the maintenance of the blackout. The ARP also helped rescue people after air raids and other attacks, and some women became ARP Ambulance Attendants whose job was to help administer first aid to casualties, search for survivors, and in many grim instances, help recover bodies, sometimes those of their own colleagues.
As the war progressed, the effectiveness of aerial bombardment was, beyond the destruction of property, very limited. There were less than three casualties for each ton of bombs dropped by the Luftwaffe in many British cities and the expected social consequences hardly happened. The morale of the British people remained high, 'shell-shock' was not at all common, and the rates of other nervous and mental ailments declined.
United States.
In the United States, the Office of Civil Defense was established in May 1941 to coordinate civilian defense efforts. It coordinated with the Department of the Army and established similar groups to the British ARP. One of these groups that still exists today is the Civil Air Patrol, which was originally created as a civilian auxiliary to the Army. The CAP was created on December 1, 1941, with the main civil defense mission of search and rescue. The CAP also sank two Axis submarines and provided aerial reconnaissance for Allied and neutral merchant ships. In 1946, the Civil Air Patrol was barred from combat by Public Law 79-476. The CAP then received its current mission: search and rescue for downed aircraft. When the Air Force was created, in 1947, the Civil Air Patrol became the auxiliary of the Air Force.
In the United States a federal civil defense program existed under Public Law 920 of the 81st Congress, as amended, from 1951-1994. That statutory scheme was made so-called all-hazards by Public Law 103-160 in 1993 and largely repealed by Public Law 103-337 in 1994. Parts now appear in Title VI of the Robert T. Stafford Disaster Relief and Emergency Assistance Act, Public Law 100-107 as amended. The term EMERGENCY PREPAREDNESS was largely codified by that repeal and amendment. See 42 USC Sections 5101 and following.
Atomic Age.
In most of the states of the North Atlantic Treaty Organization, such as the United States, the United Kingdom and West Germany, as well as the Soviet Bloc, and especially in the neutral countries, such as Switzerland and in Sweden during the 1950s and 1960s, many civil defense practices took place to prepare for the aftermath of a nuclear war, which seemed quite likely at that time.
In the United Kingdom, the Civil Defence Service was disbanded in 1945, followed by the ARP in 1946. With the onset of the growing tensions between East and West, the service was revived in 1949 as the Civil Defence Corps. As a civilian volunteer organisation, it was tasked to take control in the aftermath of a major national emergency, principally envisaged as being a Cold War nuclear attack. Although under the authority of the Home Office, with a centralised administrative establishment, the corps was administered locally by Corps Authorities. In general every county was a Corps Authority, as were most county boroughs in England and Wales and large burghs in Scotland.
Each division was divided into several sections, including the Headquarters, Intelligence and Operations, Scientific and Reconnaissance, Warden & Rescue, Ambulance and First Aid and Welfare.
In the United States, the sheer power of nuclear weapons and the perceived likelihood of such an attack precipitated a greater response than had yet been required of civil defense. Civil defense, previously considered an important and commonsense step, became divisive and controversial in the charged atmosphere of the Cold War. In 1950, the National Security Resources Board created a 162-page document outlining a model civil defense structure for the U.S. Called the "Blue Book" by civil defense professionals in reference to its solid blue cover, it was the template for legislation and organization for the next 40 years.
Perhaps the most memorable aspect of the Cold War civil defense effort was the educational effort made or promoted by the government. In "Duck and Cover", Bert the Turtle advocated that children "duck and cover" when they "see the flash." Booklets such as "Survival Under Atomic Attack", "Fallout Protection" and "Nuclear War Survival Skills" were also commonplace. The transcribed radio program Stars for Defense combined hit music with civil defense advice. Government institutes created public service announcements including children's songs and distributed them to radio stations to educate the public in case of nuclear attack.
The US President Kennedy (1961–63) launched an ambitious effort to install fallout shelters throughout the United States. These shelters would not protect against the blast and heat effects of nuclear weapons, but would provide some protection against the radiation effects that would last for weeks and even affect areas distant from a nuclear explosion. In order for most of these preparations to be effective, there had to be some degree of warning. In 1951, CONELRAD (Control of Electromagnetic Radiation) was established. Under the system, a few primary stations would be alerted of an emergency and would broadcast an alert. All broadcast stations throughout the country would be constantly listening to an upstream station and repeat the message, thus passing it from station to station.
In a once classified US war game analysis, looking at varying levels of war escalation, warning and pre-emptive attacks in the late 1950s early 1960s, it was estimated that approximately 27 million US citizens would have been saved with civil defense education. At the time, however, the cost of a full-scale civil defense program was regarded as less effective in cost-benefit analysis than a ballistic missile defense (Nike Zeus) system, and as the Soviet adversary was increasing their nuclear stockpile, the efficacy of both would follow a diminishing returns trend.
Contrary to the largely noncommittal approach taken in NATO, with its stops and starts in civil defense depending on the whims of each newly elected government, the military strategy in the comparatively more ideologically consistent USSR held that, amongst other things, a winnable nuclear war was possible. To this effect the Soviets planned to minimize, as far as possible, the effects of nuclear weapon strikes on its territory, and therefore spent considerably more thought on civil defense preparations than in U.S., with defense plans that have been assessed to be far more effective than those in the U.S.
Soviet Civil Defense Troops played the main role in the massive disaster relief operation following the 1986 Chernobyl nuclear accident. Defense Troop reservists were officially mobilized (as in a case of war) from throughout the USSR to join the Chernobyl task force and formed on the basis of the Kiev Civil Defense Brigade. The task force performed some high-risk tasks including, with the failure of their robotic machinery, the manual removal of highly-radioactive debris. Many of their personnel were later decorated with medals for their work at containing the release of radiation into the environment, with a number of the 56 deaths from the accident being Civil defense troops.
Decline.
In Western countries, strong civil defense policies were never properly implemented, because it was fundamentally at odds with the doctrine of "mutual assured destruction" (MAD) by making provisions for survivors. It was also considered that a full-fledged total defense would have not been worth the very large expense. For whatever reason, the public saw efforts at civil defense as fundamentally ineffective against the powerful destructive forces of nuclear weapons, and therefore a waste of time and money, although detailed scientific research programmes did underlie the much-mocked government civil defence pamphlets of the 1950s and 1960s.
Governments in most Western countries, with the sole exception of Switzerland, generally sought to underfund Civil Defense due to its perceived pointlessness. Nevertheless, effective but commonly dismissed civil defense measures against nuclear attack were implemented, in the face of popular apathy and scepticism of authority. After the end of the Cold War, the focus moved from defense against nuclear war to defense against a terrorist attack possibly involving chemical or biological weapons.
The Civil Defence Corps was stood down in Great Britain in 1968 with the tacit realization that nothing practical could be done in the event of an unrestricted nuclear attack. Its neighbors, however, remained committed to Civil Defence, namely the Isle of Man Civil Defence Corps and Civil Defence Ireland (Republic of Ireland).
In the United States, the various civil defense agencies were replaced with the Federal Emergency Management Agency (FEMA) in 1979. In 2002 this became part of the Department of Homeland Security. The focus was shifted from nuclear war to an "all-hazards" approach of Comprehensive Emergency Management. Natural disasters and the emergence of new threats such as terrorism have caused attention to be focused away from traditional civil defense and into new forms of civil protection such as emergency management and homeland security.
Today.
Many countries still maintain a national Civil Defence Corps, usually having a wide brief for assisting in large scale civil emergencies such as flood, earthquake, invasion, or civil disorder.
After the September 11 attacks in 2001, in the United States the concept of civil defense has been revisited under the umbrella term of homeland security and all-hazards emergency management.
In Europe, the triangle CD logo continues to be widely used. The old U.S. civil defense logo was used in the FEMA logo until 2006 and is hinted at in the United States Civil Air Patrol logo. Created in 1939 by Charles Coiner of the N. W. Ayer Advertising Agency, it was used throughout World War II and the Cold War era. In 2006, the National Emergency Management Association—a U.S. organisation made up of state emergency managers—"officially" retired the Civil Defense triangle logo, replacing it with a stylised EM (standing for Emergency management). The name and logo, however, continue to be used by Hawaii State Civil Defense
The term "civil protection" is currently widely used within the European Union to refer to government-approved systems and resources tasked with protecting the non-combat population, primarily in the event of natural and technological disasters. In recent years there has been emphasis on preparedness for technological disasters resulting from terrorist attack. Within EU countries the term "crisis-management" emphasises the political and security dimension rather than measures to satisfy the immediate needs of the population.
In Australia, civil defence is the responsibility of the volunteer-based State Emergency Service.
In most former Soviet countries civil defence is the responsibility of governmental ministries, such as Russia's Ministry of Emergency Situations.
Importance.
Relatively small investments in preparation can speed up recovery by months or years and thereby prevent millions of deaths by hunger, cold and disease. According to human capital theory in economics, a country's population is more valuable than all of the land, factories and other assets that it possesses. People rebuild a country after its destruction, and it is therefore important for the economic security of a country that it protect its people. According to psychology, it is important for people to feel as though they are in control of their own destiny, and preparing for uncertainty via civil defense may help to achieve this.
In the United States, the federal civil defense program was authorised by statute and ran from 1951 to 1994. Originally authorised by Public Law 920 of the 81st Congress, it was repealed by Public Law 93-337 in 1994. Small portions of that statutory scheme were incorporated into the Robert T. Stafford Disaster Relief and Emergency Assistance Act (Public Law 100-707) which partly superseded in part, partly amended, and partly supplemented the Disaster Relief Act of 1974 (Public Law 93-288). In the portions of the civil defense statute incorporated into the Stafford Act, the primary modification was to use the term "Emergency Preparedness" wherever the term "Civil Defence" had previously appeared in the statutory language.
An important concept initiated by President Jimmy Carter was the so-called "Crisis Relocation Program" administered as part of the federal civil defense program. That effort largely lapsed under President Ronald Reagan, who discontinued the Carter initiative because of opposition from areas potentially hosting the relocated population.
Threat assessment.
Threats to civilians and civilian life include NBC (Nuclear, Biological, and Chemical warfare) and others, like the more modern term CBRN (Chemical Biological Radiological and Nuclear). Threat assessment involves studying each threat so that preventative measures can be built into civilian life.
Refers to conventional explosives. A blast shelter designed to protect only from radiation and fallout would be much more vulnerable to conventional explosives. See also fallout shelter.
Shelter intended to protect against nuclear blast effects would include thick concrete and other sturdy elements which are resistant to conventional explosives. The biggest threats from a nuclear attack are effects from the blast, fires and radiation. One of the most prepared countries for a nuclear attack is Switzerland. Almost every building in Switzerland has an "abri" (shelter) against the initial nuclear bomb and explosion followed by the fall-out. Because of this, many people use it as a safe to protect valuables, photos, financial information and so on. Switzerland also has air-raid and nuclear-raid sirens in every village.
A "radiologically enhanced weapon," or "dirty bomb", uses an explosive to spread radioactive material. This is a theoretical risk, and such weapons have not been used by terrorists. Depending on the quantity of the radioactive material, the dangers may be mainly psychological. Toxic effects can be managed by standard hazmat techniques.
The threat here is primarily from disease-causing microorganisms such as bacteria and viruses.
Various chemical agents are a threat, such as nerve gas (VX, Sarin, and so on.).
Stages.
Mitigation.
Mitigation is the process of actively preventing the war or the release of nuclear weapons. It includes policy analysis, diplomacy, political measures, nuclear disarmament and more military responses such as a National Missile Defense and air defense artillery. In the case of counter-terrorism, mitigation would include diplomacy, intelligence gathering and direct action against terrorist groups. Mitigation may also be reflected in long-term planning such as the design of the interstate highway system and the placement of military bases further away from populated areas.
Preparation.
Preparation consists of building blast shelters and pre-positioning information, supplies, and emergency infrastructure. For example, most larger cities in the U.S. now have underground emergency operations centres that can perform civil defense coordination. FEMA also has many underground facilities for the same purpose located near major railheads such as the ones in Denton, Texas and Mount Weather, Virginia.
Other measures would include continual government inventories of grain silos, the Strategic National Stockpile, the uncapping of the Strategic Petroleum Reserve, the dispersal of lorry-transportable bridges, water purification, mobile refineries, mobile de-contamination facilities, mobile general and special purpose disaster mortuary facilities such as Disaster Mortuary Operational Response Team (DMORT) and DMORT-WMD, and other aids such as temporary housing to speed civil recovery.
On an individual scale, one means of preparation for exposure to nuclear fallout is to obtain potassium iodide (KI) tablets as a safety measure to protect the human thyroid gland from the uptake of dangerous radioactive iodine. Another measure is to cover the nose, mouth and eyes with a piece of cloth and sunglasses to protect against alpha particles, which are only an internal hazard.
To support and supplement efforts at national, regional and local level with regard to disaster prevention, the preparedness of those responsible for civil protection and the intervention in the event of disaster
Preparing also includes sharing information:
Response.
Response consists first of warning civilians so they can enter Fallout Shelters and protect assets.
Staffing a response is always full of problems in a civil defense emergency. After an attack, conventional full-time emergency services are dramatically overloaded, with conventional fire fighting response times often exceeding several days. Some capability is maintained by local and state agencies, and an emergency reserve is provided by specialised military units, especially civil affairs, Military Police, Judge Advocates and combat engineers.
However, the traditional response to massed attack on civilian population centres is to maintain a mass-trained force of volunteer emergency workers. Studies in World War II showed that lightly trained (40 hours or less) civilians in organised teams can perform up to 95% of emergency activities when trained, liaised and supported by local government. In this plan, the populace rescues itself from most situations, and provides information to a central office to prioritize professional emergency services.
In the 1990s, this concept was revived by the Los Angeles Fire Department to cope with civil emergencies such as earthquakes. The program was widely adopted, providing standard terms for organization. In the U.S., this is now official federal policy, and it is implemented by community emergency response teams, under the Department of Homeland Security, which certifies training programmes by local governments, and registers "certified disaster service workers" who complete such training.
Recovery.
Recovery consists of rebuilding damaged infrastructure, buildings and production. The recovery phase is the longest and ultimately most expensive phase. Once the immediate "crisis" has passed, cooperation fades away and recovery efforts are often politicised or seen as economic opportunities.
Preparation for recovery can be very helpful. If mitigating resources are dispersed before the attack, cascades of social failures can be prevented. One hedge against bridge damage in riverine cities is to subsidise a "tourist ferry" that performs scenic cruises on the river. When a bridge is down, the ferry takes up the load.
Implementation.
Some advocates believe that government should change building codes to require autonomous buildings in order to reduce civil societies' dependence on complex, fragile networks of social services.
An example of a crucial need after a general nuclear attack would be the fuel required to transport every other item for recovery. However, oil refineries are large, immobile, and probable targets. One proposal is to pre-position truck-mounted fuel refineries near oil fields and bulk storage depots. Other critical infrastructure needs would include road and bridge repair, communications, electric power, food production, and potable water.
Civil defense organizations.
Civil Defense is also the name of a number of organizations around the world dedicated to protecting civilians from military attacks, as well as to providing rescue services after natural and human-made disasters alike.
Worldwide protection is managed by the United Nations Office for the Coordination of Humanitarian Affairs (OCHA).
In a few countries such as Jordan and Singapore (see Singapore Civil Defence Force), civil defense is essentially the same organisation as the fire brigade. In most countries, however, civil defense is a government-managed, volunteer-staffed organisation, separate from the fire brigade and the ambulance service.
As the threat of Cold War eased, a number of such civil defense organisations have been disbanded or mothballed (as in the case of the Royal Observer Corps in the United Kingdom and the United States civil defense), while others have changed their focuses into providing rescue services after natural disasters (as for the State Emergency Service in Australia). However, the ideals of Civil Defense have been brought back in the United States under FEMA's Citizen Corps and Community Emergency Response Team (CERT).
In the United Kingdom Civil Defence work is carried out by Emergency Responders under the Civil Contingencies Act 2004, with assistance from voluntary groups such as RAYNET, Search and Rescue Teams and 4x4 Response. In Ireland, the Civil Defence is still very much an active organisation and is occasionally called upon for its Auxiliary Fire Service and ambulance/rescue services when emergencies such as flash flooding occur and require additional manpower. The organisation has units of trained firemen and medical responders based in key areas around the country.
By country.
UK:
US:
See also.
General:

</doc>
<doc id="7060" url="https://en.wikipedia.org/wiki?curid=7060" title="Chymotrypsin">
Chymotrypsin

Chymotrypsin (, "chymotrypsins A and B", "alpha-chymar ophth", "avazyme", "chymar", "chymotest", "enzeon", "quimar", "quimotrase", "alpha-chymar", "alpha-chymotrypsin A", "alpha-chymotrypsin") is a digestive enzyme component of pancreatic juice acting in the duodenum where it performs proteolysis, the breakdown of proteins and polypeptides. Chymotrypsin preferentially cleaves peptide amide bonds where the carboxyl side of the amide bond (the P1 position) is a large hydrophobic amino acid (tyrosine, tryptophan, and phenylalanine). These amino acids contain an aromatic ring in their sidechain that fits into a 'hydrophobic pocket' (the S1 position) of the enzyme. It is activated in the presence of trypsin. The hydrophobic and shape complementarity between the peptide substrate P1 sidechain and the enzyme S1 binding cavity accounts for the substrate specificity of this enzyme. Chymotrypsin also hydrolyzes other amide bonds in peptides at slower rates, particularly those containing leucine and methionine at the P1 position.
Structurally, it is the archetypal structure for its superfamily, the PA clan of proteases.
Activation.
Chymotrypsin is synthesized in the pancreas by protein biosynthesis as a precursor called chymotrypsinogen that is enzymatically inactive. On cleavage by trypsin into two parts that are still connected via an S-S bond, cleaved chymotrypsinogen molecules can activate each other by removing two small peptides in a "trans"-proteolysis. The resulting molecule is active chymotrypsin, a three-polypeptide molecule interconnected via disulfide bonds.
Mechanism of action and kinetics.
"In vivo", chymotrypsin is a proteolytic enzyme (Serine protease) acting in the digestive systems of many organisms. It facilitates the cleavage of peptide bonds by a hydrolysis reaction, which despite being thermodynamically favorable occurs extremely slowly in the absence of a catalyst. The main substrates of chymotrypsin include tryptophan, tyrosine, phenylalanine, and leucine, which are cleaved at the carboxyl terminal. Like many proteases, chymotrypsin will also hydrolyse amide bonds "in vitro", a virtue that enabled the use of substrate analogs such as N-acetyl-L-phenylalanine p-nitrophenyl amide for enzyme assays.
Chymotrypsin cleaves peptide bonds by attacking the unreactive carbonyl group with a powerful nucleophile, the serine 195 residue located in the active site of the enzyme, which briefly becomes covalently bonded to the substrate, forming an enzyme-substrate intermediate. Along with histidine 57 and aspartic acid 102, this serine residue constitutes the catalytic triad of the active site.
These findings rely on inhibition assays and the study of the kinetics of cleavage of the aforementioned substrate, exploiting the fact that the enzyme-substrate intermediate "p"-nitrophenolate has a yellow colour, enabling us to measure its concentration by measuring light absorbance at 410 nm.
It was found that the reaction of chymotrypsin with its substrate takes place in two stages, an initial “burst” phase at the beginning of the reaction and a steady-state phase following Michaelis-Menten kinetics. It is also called "ping-pong" mechanism. The mode of action of chymotrypsin explains this as hydrolysis takes place in two steps. First acylation of the substrate to form an acyl-enzyme intermediate and then deacylation in order to return the enzyme to its original state. This occurs via the concerted action of the three amino acid residues in the catalytic triad. Aspartate hydrogen bonds to the N-δ hydrogen of histidine, increasing the pKa of its ε nitrogen and thus making it able to deprotonate serine. It is this deprotonation that allows the serine side chain to act as a nucleophile and bind to the electron-deficient carbonyl carbon of the protein main chain. Ionization of the carbonyl oxygen is stabilized by formation of two hydrogen bonds to adjacent main chain N-hydrogens. This occurs in the oxyanion hole. This forms a tetrahedral adduct and breakage of the peptide bond. An acyl-enzyme intermediate, bound to the serine, is formed, and the newly formed amino terminus of the cleaved protein can dissociate. In the second reaction step, a water molecule is activated by the basic histidine, and acts as a nucleophile. The oxygen of water attacks the carbonyl carbon of the serine-bound acyl group, resulting in formation of a second tetrahedral adduct, regeneration of the serine -OH group, and release of a proton, as well as the protein fragment with the newly formed carboxyl terminus 

</doc>
<doc id="7061" url="https://en.wikipedia.org/wiki?curid=7061" title="Community emergency response team">
Community emergency response team

In the United States a Community Emergency Response Team (CERT) can refer to
Sometimes programs and organizations take different names, such as Neighborhood Emergency Response Team (NERT), or Neighborhood Emergency Team (NET).
The concept of civilian auxiliaries is similar to civil defense, which has a longer history. The CERT concept differs because it includes nonmilitary emergencies, and is coordinated with all levels of emergency authorities, local to national, via an overarching incident command system.
CERT Organization.
A local government agency, often a fire department, police department, or emergency management agency, agrees to sponsor CERT within its jurisdiction. The sponsoring agency liaises with, deploys and may train or supervise the training of CERT members. The sponsoring agency receives and disburses federal and state Citizen Corps grant funds allocated to its CERT program. Many sponsoring agencies employ a full-time community-service person as liaison to the CERT members. In some communities, the liaison is a volunteer and CERT member. 
As people are trained and agree to join the community emergency response effort, a CERT is formed. Initial efforts may result in a team with only a few members from across the community. As the number of members grow, a single community-wide team may subdivide. Multiple CERTs are organized into a hierarchy of teams consistent with ICS principles. This follows the Incident Command System (ICS) principle of Span of control until the ideal distribution is achieved: one or more teams are formed at each neighborhood within a community.
A Teen Community Emergency Response Team (TEEN CERT), or Student Emergency Response Team (SERT), can be formed from any group of teens. A Teen Cert can be formed as a school club, service organization, Venturing Crew, Explorer Post, or the training can be added to a school's graduation curriculum. Some CERTs form a club or service corporation, and recruit volunteers to perform training on behalf of the sponsoring agency. This reduces the financial and human resource burden on the sponsoring agency.
When not responding to disasters or large emergencies, CERTs may
Some sponsoring agencies use Citizen Corps grant funds to purchase response tools and equipment for their members and team(s) (subject to Stafford Act limitations). Most CERTs also acquire their own supplies, tools, and equipment. As community members, CERTs are aware of the specific needs of their community and equip the teams accordingly.
CERT Response.
The basic idea is to use CERT to perform the large number of tasks needed in emergencies. This frees highly trained professional responders for more technical tasks. Much of CERT training concerns the Incident Command System and organization, so CERT members fit easily into larger command structures.
A team may self-activate (self-deploy) when their own neighborhood is affected by disaster. An effort is made to report their response status to the sponsoring agency. A self-activated team will size-up the loss in their neighborhood and begin performing the skills they have learned to minimize further loss of life, property, and environment. They will continue to respond safely until redirected or relieved by the sponsoring agency or professional responders on-scene.
Teams in neighborhoods not affected by disaster may be deployed or activated by the sponsoring agency. The sponsoring agency may communicate with neighborhood CERT leaders through an organic communication team. In some areas the communications may be by amateur radio, FRS, GMRS or MURS radio, dedicated telephone or fire-alarm networks. In other areas, relays of bicycle-equipped runners can effectively carry messages between the teams and the local emergency operations center.
The sponsoring agency may activate and dispatch teams in order to gather or respond to intelligence about an incident. Teams may be dispatched to affected neighborhoods, or organized to support operations. CERT members may augment support staff at an Incident Command Post or Emergency Operations Center. Additional teams may also be created to guard a morgue, locate supplies and food, convey messages to and from other CERT teams and local authorities, and other duties on an as-needed basis as identified by the team leader.
In the short term, CERTs perform data gathering, especially to locate mass-casualties requiring professional response, or situations requiring professional rescues, simple fire-fighting tasks (for example, small fires, turning off gas), light search and rescue, damage evaluation of structures, triage and first aid. In the longer term, CERTs may assist in the evacuation of residents, or assist with setting up a neighborhood shelter.
While responding, CERT members are temporary volunteer government workers. In some areas, (such as California, Hawaii and Kansas) registered, activated CERT members are eligible for worker's compensation for on-the-job injuries during declared disasters.
CERT Member Roles.
The Federal Emergency Management Agency (FEMA) recommends that the standard, ten-person team be comprised as follows:
Because every CERT member in a community receives the same core instruction, any team member has the training necessary to assume any of these roles. This is important during a disaster response because not all members of a regular team may be available to respond. Hasty teams may be formed by whichever members are responding at the time. Additionally, members may need to adjust team roles due to stress, fatigue, injury, or other circumstances.
CERT Training.
While state and local jurisdictions will implement training in the manner that best suits the community, the Citizen Corps CERT program has an established curriculum. Jurisdictions may augment the training, but are strongly encouraged to deliver the entire core content. The Citizen Corps CERT core curriculum for the basic course is composed of the following nine units (time is instructional hours):
Citizen Corps CERT training emphasizes safely "doing the most good for the most people as quickly as possible" when responding to a disaster. For this reason, cardiopulmonary resuscitation (CPR) training is not included in the core curriculum, as it is time and responder intensive in a mass-casualty incident. However, many jurisdictions encourage or require CERT members to obtain CPR training. Many CERT programs provide or encourage members to take additional first aid training. Some CERT members may also take training to become a certified first responder or emergency medical technician.
Many CERT programs also provide training in amateur radio operation, shelter operations, flood response, community relations, mass care, the incident command system (ICS, and the National Incident Management System(NIMS). 
Each unit of Citizen Corps CERT training is ideally delivered by professional responders or other experts in the field addressed by the unit. This is done to help build unity between CERT members and responders, keep the attention of students, and help the professional response organizations be comfortable with the training which CERT members receive. 
Each course of instruction is ideally facilitated by one or more instructors certified in the CERT curriculum by the state or sponsoring agency. Facilitating instructors provide continuity between units, and help ensure that the CERT core curriculum is being delivered successfully. Facilitating instructors also perform set-up and tear-down of the classroom, provide instructional materials for the course, record student attendance and other tasks which assist the professional responder in delivering their unit as efficiently as possible.
Citizen Corps CERT training is provided free to interested members of the community, and is delivered in a group classroom setting. People may complete the training without obligation to join a CERT. Citizen Corps grant funds can be used to print and provide each student with a printed manual. Some sponsoring agencies use Citizen Corps grant funds to purchase disaster response tool kits. These kits are offered as an incentive to join a CERT, and must be returned to the sponsoring agency when members resign from CERT.
Some sponsoring agencies require a criminal background-check of all trainees before allowing them to participate on a CERT. For example, the city of Albuquerque, New Mexico require all volunteers to pass a background check, while the city of Austin, Texas does not require a background check to take part in training classes but requires members to undergo a background check in order to receive a CERT badge and directly assist first reponders during an activation of the Emergency Operations Center in Austin. However, most programs do not require a criminal background check in order to participate.
The Citizen Corps CERT curriculum (including the Train-the-Trainer course) was updated during the last half of 2008 to reflect feedback from instructors across the nation. The update is in final review, and is scheduled for release during the first quarter of 2009.

</doc>
<doc id="7063" url="https://en.wikipedia.org/wiki?curid=7063" title="Catapult">
Catapult

A catapult is a ballistic device used to launch a projectile a great distance without the aid of explosive devices—particularly various types of ancient and medieval siege engines. Although the catapult has been used since ancient times, it has proven to be one of the most effective mechanisms during warfare. The word 'catapult' comes from the Latin 'catapulta', which in turn comes from the Greek ("katapeltēs"), itself from ("kata"), "downwards" + πάλλω ("pallō"), "to toss, to hurl". Catapults were invented by the ancient Greeks.
Greek and Roman catapults.
The catapult and crossbow in Greece are closely intertwined. Primitive catapults were essentially “the product of relatively straightforward attempts to increase the range and penetrating power of missiles by strengthening the bow which propelled them”. The historian Diodorus Siculus (fl. 1st century BC), described the invention of a mechanical arrow-firing catapult ("katapeltikon") by a Greek task force in 399 BC. The weapon was soon after employed against Motya (397 BC), a key Carthaginian stronghold in Sicily. Diodorus is assumed to have drawn his description from the highly rated history of Philistus, a contemporary of the events then. The introduction of crossbows however, can be dated further back: according to the inventor Hero of Alexandria (fl. 1st century AD), who referred to the now lost works of the 3rd-century BC engineer Ctesibius, this weapon was inspired by an earlier foot-held crossbow, called the "gastraphetes", which could store more energy than the Greek bows. A detailed description of the "gastraphetes", or the “belly-bow”, along with a watercolor drawing, is found in Heron's technical treatise "Belopoeica".
A third Greek author, Biton (fl. 2nd century BC), whose reliability has been positively reevaluated by recent scholarship, described two advanced forms of the "gastraphetes", which he credits to Zopyros, an engineer from southern Italy. Zopyrus has been plausibly equated with a Pythagorean of that name who seems to have flourished in the late 5th century BC. He probably designed his bow-machines on the occasion of the sieges of Cumae and Milet between 421 BC and 401 BC. The bows of these machines already featured a winched pull back system and could apparently throw two missiles at once.
Philo of Byzantium provides probably the most detailed account on the establishment of a theory of belopoietics (“belos” = projectile; “poietike” = (art) of making) circa 200 BC. The central principle to this theory was that “all parts of a catapult, including the weight or length of the projectile, were proportional to the size of the torsion springs”. This kind of innovation is indicative of the increasing rate at which geometry and physics were being assimilated into military enterprises.
From the mid-4th century BC onwards, evidence of the Greek use of arrow-shooting machines becomes more dense and varied: arrow firing machines ("katapaltai") are briefly mentioned by Aeneas Tacticus in his treatise on siegecraft written around 350 BC. An extant inscription from the Athenian arsenal, dated between 338 and 326 BC, lists a number of stored catapults with shooting bolts of varying size and springs of sinews. The later entry is particularly noteworthy as it constitutes the first clear evidence for the switch to torsion catapults which are more powerful than the flexible crossbows and came to dominate Greek and Roman artillery design thereafter. This move to torsion springs was likely spurred by the engineers of Philip II of Macedonia. Another Athenian inventory from 330 to 329 BC includes catapult bolts with heads and flights. As the use of catapults became more commonplace, so did the training required to operate them. Many Greek children were instructed in catapult usage, as evidenced by “a 3rd Century B.C. inscription from the island of Ceos in the Cyclades catapult shooting competitions for the young”. Arrow firing machines in action are reported from Philip II's siege of Perinth (Thrace) in 340 BC. At the same time, Greek fortifications began to feature high towers with shuttered windows in the top, which could have been used to house anti-personnel arrow shooters, as in Aigosthena. Projectiles included both arrows and (later) stones that were sometimes lit on fire. Onomarchus of Phocis first used catapults on the battlefield against Philip II of Macedon. Philip's son, Alexander the Great, was the next commander in recorded history to make such use of catapults on the battlefield as well as to use them during sieges.
The Romans started to use catapults as arms for their wars against Syracuse, Macedon, Sparta and Aetolia (3rd and 2nd centuries BC). The Roman machine known as an arcuballista was similar to a large crossbow. Later the Romans used ballista catapults on their warships.
Other ancient catapults.
Ajatshatru is recorded in Jaina texts as having used a catapult in his campaign against the Licchavis.
Medieval catapults.
Castles and fortified walled cities were common during this period – and catapults were used as a key siege weapon against them. As well as attempting to breach the walls, incendiary missiles could be thrown inside—or early biological warfare attempted with diseased carcasses or putrid garbage catapulted over the walls.
Defensive techniques in the Middle Ages progressed to a point that rendered catapults ineffective for the most part. The Viking siege of Paris (885–6 A.D.) “saw the employment by both sides of virtually every instrument of siege craft known to the classical world, including a variety of catapults,” to little effect, resulting in failure.
The most widely used catapults throughout the Middle Ages were as follows:
Modern use.
The last large scale military use of catapults was during the trench warfare of World War I. During the early stages of the war, catapults were used to throw hand grenades across no man's land into enemy trenches. They were eventually replaced by small mortars.
Special variants called "aircraft catapults" are used to launch planes from land bases and sea carriers when the takeoff runway is too short for a powered takeoff or simply impractical to extend. Ships also use them to launch torpedoes and deploy bombs against submarines. Small catapults, referred to as "traps", are still widely used to launch clay targets into the air in the sport of clay pigeon shooting.
Until recently, catapults were used by thrill-seekers to experience being catapulted through the air. The practice has been discontinued due to fatalities, when the participants failed to land onto the safety net.
"Pumpkin chunking" is another widely popularized use, in which people compete to see who can launch a pumpkin the farthest by mechanical means (although the world record is held by a pneumatic air cannon).
In January 2011, PopSci.com, the news blog version of "Popular Science" magazine, reported that a group of smugglers used a homemade catapult to deliver cannabis into the United States from Mexico. The machine was found 20 feet from the border fence with bales of cannabis ready to launch.
Models.
In the US, catapults of all types and sizes are being built for school science and history fairs, competitions or as a hobby. Catapult projects can inspire students to study different subjects including physics, engineering, science, math and history. These kits can be purchased from Renaissance Fairs, or from several online stores.

</doc>
<doc id="7066" url="https://en.wikipedia.org/wiki?curid=7066" title="Cinquain">
Cinquain

Cinquain is a class of poetic forms that employ a 5-line pattern. Earlier used to describe any five-line form, it now refers to one of several forms that are defined by specific rules and guidelines.
American Cinquain.
The modern form, known as American Cinquain inspired by Japanese haiku and tanka, akin in spirit to that of the Imagists.
In her 1915 collection titled "Verse", published one year after her death, Adelaide Crapsey included 28 cinquains.
Crapsey's American Cinquain form developed in two stages. The first, fundamental form is a stanza of five lines of accentual verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses. Then Crapsey decided to make the criterion a stanza of five lines of accentual-syllabic verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses and 2, 4, 6, 8, and 2 syllables. Iambic feet were meant to be the standard for the cinquain, which made the dual criteria match perfectly. Some resource materials define classic cinquains as solely iambic, but that is not necessarily so. In contrast to the Eastern forms upon which she based them, Crapsey always titled her cinquains, effectively utilizing the title as a sixth line. Crapsey's cinquain depends on strict structure and intense physical imagery to communicate a mood or feeling.
The form is illustrated by Crapsey's "November Night":
Listen... <br>
With faint dry sound, <br>
Like steps of passing ghosts, <br>
The leaves, frost-crisp'd, break from the trees <br>
And fall.
The Scottish poet William Soutar also wrote over one hundred American Cinquains (he labelled them Epigrams) between 1933 and 1940.
Cinquain variations.
The Crapsey cinquain has subsequently seen a number of variations by modern poets, including:
Didactic cinquain.
The didactic cinquain is closely related to the Crapsey cinquain. It is an informal cinquain widely taught in elementary schools and has been featured in, and popularized by, children's media resources, including Junie B. Jones and PBS Kids. This form is also embraced by young adults and older poets for its expressive simplicity. The prescriptions of this type of cinquain refer to word count, not syllables and stresses. Ordinarily, the first line is a one-word title, the subject of the poem; the second line is a pair of adjectives describing that title; the third line is a three-word phrase that gives more information about the subject (often a list of three gerunds); the fourth line consists of four words describing feelings related to that subject; and the fifth line is a single word synonym or other reference for the subject from line one.
For example:
Snow
Silent, white
Dancing, falling, drifting
Covering everything it touches
Blanket
External link.
Essay-Introduction to American cinquains

</doc>
<doc id="7067" url="https://en.wikipedia.org/wiki?curid=7067" title="Cook Islands">
Cook Islands

The Cook Islands (; Cook Islands Māori: "Kūki 'Āirani") is an island country in the South Pacific Ocean in free association with New Zealand. It comprises 15 islands whose total land area is . The Cook Islands' Exclusive Economic Zone (EEZ), however, covers of ocean.
The Cook Islands' defence and foreign affairs are the responsibility of New Zealand, but they are exercised in consultation with the Cook Islands. In recent times, the Cook Islands have adopted an increasingly independent foreign policy. Although Cook Islanders are citizens of New Zealand, they have the status of Cook Islands nationals, which is not given to other New Zealand citizens.
The Cook Islands' main population centres are on the island of Rarotonga (10,572 in 2011), where there is an international airport. There is a larger population of Cook Islanders in New Zealand, particularly the North Island. In the 2006 census, 58,008 self-identified as being of ethnic Cook Islands Māori descent.
With about 100,000 visitors travelling to the islands in the 2010–11 financial year, tourism is the country's main industry, and the leading element of the economy, ahead of offshore banking, pearls, and marine and fruit exports.
Geography.
The Cook Islands are in the South Pacific Ocean, northeast of New Zealand, between French Polynesia and American Samoa. There are 15 major islands spread over of ocean, divided into two distinct groups: the Southern Cook Islands and the Northern Cook Islands of coral atolls.
The islands were formed by volcanic activity; the northern group is older and consists of six atolls, which are sunken volcanoes topped by coral growth. The climate is moderate to tropical.
The 15 islands and two reefs are grouped as follows:
History.
The Cook Islands were first settled in the 6th century by Polynesian people who migrated from Tahiti, an island to the northeast.
Spanish ships visited the islands in the 16th century; the first written record of contact with the islands came in 1595 with the sighting of Pukapuka by Spanish sailor Álvaro de Mendaña de Neira, who called it "San Bernardo" (Saint Bernard). Pedro Fernandes de Queirós, a Portuguese captain working for the Spanish crown, made the first recorded European landing in the islands when he set foot on Rakahanga in 1606, calling it "Gente Hermosa" (Beautiful People).
British navigator Captain James Cook arrived in 1773 and 1777 and named the islands the "Hervey Islands"; the name "Cook Islands", in honour of Cook, appeared on a Russian naval chart published in the 1820s.
In 1813 John Williams, a missionary on the "Endeavour" (not the same ship as Cook's) made the first recorded sighting of Rarotonga. The first recorded landing on Rarotonga by Europeans was in 1814 by the "Cumberland"; trouble broke out between the sailors and the Islanders and many were killed on both sides. The islands saw no more Europeans until missionaries arrived from England in 1821. Christianity quickly took hold in the culture and many islanders continue to be Christian believers today.
The Cook Islands became a British protectorate in 1888, due largely to community fears that France might occupy the territory as it had Tahiti. On 6September 1900, the leading islanders presented a petition asking that the islands (including Niue "if possible") should be annexed as British territory. On 8–9 October 1900 seven instruments of cession of Rarotonga and other islands were signed by their chiefs and people; and by a British Proclamation issued at the same time the cessions were accepted, the islands being declared parts of Her Britanic Majesty's dominions. These instruments did not include Aitutaki. It appears that, though the inhabitants regarded themselves as British subjects, the Crown's title was uncertain, and the island was formally annexed by Proclamation dated 9October 1900. The islands were included within the boundaries of the Colony of New Zealand in 1901 by Order in Council under the Colonial Boundaries Act, 1895 of the United Kingdom. The boundary change became effective on 11June 1901 and the Cook Islands have had a formal relationship with New Zealand ever since.
When the British Nationality and New Zealand Citizenship Act 1948 came into effect on 1January 1949, Cook Islanders who were British subjects gained New Zealand citizenship. The country remained a New Zealand dependent territory until 1965, when the New Zealand Government decided to offer self-governing status to its colony. In that year, Albert Henry of the Cook Islands Party was elected as the first Premier. Henry led the country until he was accused of vote-rigging. He was succeeded in 1978 by Tom Davis of the Democratic Party.
Politics and foreign relations.
The Cook Islands is a representative democracy with a parliamentary system in an associated state relationship with New Zealand. Executive power is exercised by the government, with the Chief Minister as head of government. Legislative power is vested in both the government and the Parliament of the Cook Islands. There is a pluriform multi-party system. The Judiciary is independent of the executive and the legislature. The Head of State is the Queen of New Zealand, who is represented in the Cook Islands by the Queen's Representative.
The islands are self-governing in "free association" with New Zealand. New Zealand retains primary responsibility for external affairs, with consultation with the Cook Islands government. Cook Islands nationals are citizens of New Zealand and can receive New Zealand government services, but the reverse is not true; New Zealand citizens are not Cook Islands nationals. Despite this, , the Cook Islands had diplomatic relations in its own name with 43 other countries. The Cook Islands is not a United Nations member state, but, along with Niue, has had their "full treaty-making capacity" recognised by United Nations Secretariat, and is a full member of the WHO and UNESCO UN specialised agencies, is an associate member of the Economic and Social Commission for Asia and the Pacific (UNESCAP) and a Member of the Assembly of States of the International Criminal Court.
On 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing any American claims to Penrhyn, Pukapuka, Manihiki, and Rakahanga. In 1990 the Cook Islands and France signed a treaty that delimited the boundary between the Cook Islands and French Polynesia. As competition between the US and China heated up in the South China Sea and other areas closer to the mainland, the Cook Islands began to feel the results. In late August 2012, for instance, United States Secretary of State Hillary Clinton visited the islands.
Administrative subdivisions.
There are island councils on all of the inhabited outer islands (Outer Islands Local Government Act 1987 with amendments up to 2004, and Palmerston Island Local Government Act 1993) except Nassau, which is governed by Pukapuka (Suwarrow, with only one caretaker living on the island, also governed by Pukapuka, is not counted with the inhabited islands in this context). Each council is headed by a mayor.
The three "Vaka" councils of Rarotonga established in 1997 ("Rarotonga Local Government Act 1997"), also headed by mayors, were abolished in February 2008, despite much controversy.
On the lowest level, there are village committees. Nassau, which is governed by Pukapuka, has an island committee (Nassau Island Committee), which advises the Pukapuka Island Council on matters concerning its own island.
Demographics.
Births and deaths 
Economy.
The economy is strongly affected by geography. It is isolated from foreign markets, and has some inadequate infrastructure; it lacks major natural resources, has limited manufacturing and suffers moderately from natural disasters. Tourism provides the economic base which makes up approximately 67.5% of GDP.
Additionally, the economy is supported by foreign aid, largely from New Zealand. The Peoples' Republic of China has also contributed foreign aid which has resulted in, among other projects, the Police Headquarters building. The Cook Islands is expanding its agriculture, mining and fishing sectors, with varying success.
Since approximately 1989, the Cook Islands have become a location specialising in so-called asset protection trusts, by which investors shelter assets from the reach of creditors and legal authorities. According to "The New York Times", the Cooks have "laws devised to protect foreigners' assets from legal claims in their home countries" which were apparently crafted specifically to thwart the long arm of American justice; creditors must travel to the Cook Islands and argue their cases under Cooks law, often at prohibitive expense. Unlike other foreign jurisdictions such as the British Virgin Islands, the Cayman Islands and Switzerland, the Cooks "generally disregard foreign court orders" and do not require that bank accounts, real estate, or other assets protected from scrutiny (it is illegal to disclose names or any information about Cooks trusts) be physically located within the archipelago. Taxes on trusts and trust employees account for some 8% of the Cook Islands economy, behind tourism but ahead of fishing.
Culture.
Language.
The languages of the Cook Islands include English, Cook Islands Māori, or "Rarotongan," and Pukapukan. Dialects of Cook Islands Maori include Penrhyn; Rakahanga-Manihiki; the Ngaputoru dialect of Atiu, Mitiaro, and Mauke; the Aitutaki dialect; and the Mangaian dialect. Cook Islands Maori and its dialectic variants are closely related to both Tahitian and to New Zealand Māori. Pukapukan is considered closely related to the Samoan language. English and Cook Islands Maori are official languages of the Cook Islands.
Music.
Music in the Cook Islands is varied, with Christian songs being quite popular, but traditional dancing and songs in Polynesian languages remain popular.
Art.
Carving.
Woodcarving is a common art form in the Cook Islands. The proximity of islands in the southern group helped produce a homogeneous style of carving but which had special developments in each island. Rarotonga is known for its fisherman's gods and staff-gods, Atiu for its wooden seats, Mitiaro, Mauke and Atiu for mace and slab gods and Mangaia for its ceremonial adzes. Most of the original wood carvings were either spirited away by early European collectors or were burned in large numbers by missionaries. Today, carving is no longer the major art form with the same spiritual and cultural emphasis given to it by the Maori in New Zealand. However, there are continual efforts to interest young people in their heritage and some good work is being turned out under the guidance of older carvers. Atiu, in particular, has a strong tradition of crafts both in carving and local fibre arts such as tapa. Mangaia is the source of many fine adzes carved in a distinctive, idiosyncratic style with the so-called double-k design. Mangaia also produces food pounders carved from the heavy calcite found in its extensive limestone caves.
Weaving.
The outer islands produce traditional weaving of mats, basketware and hats. Particularly fine examples of rito hats are worn by women to church. They are made from the uncurled immature fibre of the coconut palm and are of very high quality. The Polynesian equivalent of Panama hats, they are highly valued and are keenly sought by Polynesian visitors from Tahiti. Often, they are decorated with hatbands made of minuscule pupu shells which are painted and stitched on by hand. Although pupu are found on other islands the collection and use of them in decorative work has become a speciality of Mangaia. The weaving of rito is a speciality of the northern island of Penrhyn.
Tivaevae.
A major art form in the Cook Islands is tivaevae. This is, in essence, the art of handmade Island scenery patchwork quilts. Introduced by the wives of missionaries in the 19th century, the craft grew into a communal activity and is probably one of the main reasons for its popularity.
Contemporary art.
The Cook Islands has produced internationally recognised contemporary artists, especially in the main island of Rarotonga. Artists include painter (and photographer) Mahiriki Tangaroa, sculptors Eruera (Ted) Nia (originally a film maker) and master carver Mike Tavioni, painter (and Polynesian tattoo enthusiast) Upoko'ina Ian George, Aitutakian-born painter Tim Manavaroa Buchanan, Loretta Reynolds, Judith Kunzlé, Joan Rolls Gragg, Kay George (who is also known for her fabric designs), Apii Rongo, Varu Samuel, and multi-media, installation and community-project artist Ani O'Neill, all of whom currently live on the main island of Rarotonga. Atiuan-based Andrea Eimke is an artist who works in the medium of tapa and other textiles, and also co-authored the book 'Tivaivai – The Social Fabric of the Cook Islands' with British academic Susanne Kuechler. Many of these artists have studied at university art schools in New Zealand and continue to enjoy close links with the New Zealand art scene.
New Zealand-based Cook Islander artists include Michel Tuffery, print-maker David Teata, Richard Shortland Cooper, Sylvia Marsters and Jim Vivieaere.
On Rarotonga, the main commercial galleries are Beachcomber Contemporary Art (Taputapuatea, Avarua) run by Ben & Trevon Bergman, and The Art Studio Gallery (Arorangi) run by Ian and Kay George. The Cook Islands National Museum also exhibits art.
Sport.
Rugby league is the most popular sport in the Cook Islands. Rugby union, Association football (soccer), Netball, and Cricket are also popular.

</doc>
<doc id="7068" url="https://en.wikipedia.org/wiki?curid=7068" title="History of the Cook Islands">
History of the Cook Islands

The Cook Islands are named after Captain James Cook, who visited the islands in 1773 and 1777. The Cook Islands became a British protectorate in 1888.
By 1900, the islands were annexed as British territory. In 1901, the islands were included within the boundaries of the Colony of New Zealand.
The Cook Islands contain 15 islands in the group spread over a vast area in the South Pacific. The majority of islands are low coral atolls in the Northern Group, with Rarotonga, a volcanic island in the Southern Group, as the main administration and government centre. The main Cook Islands language is Rarotongan Māori. There are some variations in dialect in the 'outer' islands.
Early Settlers of the Cooks.
It is thought that the Cook Islands may have been settled between the years 900 - 1200 AD. Early settlements suggest that were generally great warriors migrating from Tahiti, to the north east of the Cooks. The Cook Islands continue to hold important connections with Tahiti, and this is generally found in the two countries culture, tradition and language. It is also thought that the early settlers were true Tahitians, who landed in Rarotonga (Takitumu city). There are notable historic epics of great warriors that travel between the two nations for a wide variety of reasons. These missions are still unclear but recent research indicate that large to small groups often flee their island due to local wars being forced upon them. For each group to travel and to survive, they would normally rely on a warrior to lead them. Outstanding warriors are still mentioned in the countries traditions and stories.
These arrivals are evidenced by an older road in Toi, the "Ara Metua", which runs around most of Rarotonga, and is believed to be at least 1200 years old. This 29 km long, paved road is a considerable achievement of ancient engineering, possibly unsurpassed elsewhere in Polynesia. The islands of Manihiki and Rakahanga trace their origins to the arrival of Toa, an outcast from Rarotonga, and Tupaeru, a high-ranking woman from the Puaikura tribe of Rarotonga. The remainder of the northern islands were probably settled by expeditions from Samoa and Tonga.
Early European contact.
Spanish ships visited the islands in the 16th century; the first written record of contact from Europeans with the native inhabitants of the Cook Islands came with the sighting of Pukapuka by Spanish sailor Álvaro de Mendaña in 1595 who called it "San Bernardo" (Saint Bernard). Portuguese-Spaniard Pedro Fernández de Quirós made the first recorded European landing in the islands when he set foot on Rakahanga in 1606, calling it "Gente Hermosa" (Beautiful People).
British navigator Captain James Cook arrived in 1773 and 1777; Cook named the islands the 'Hervey Islands' to honour a British Lord of the Admiralty; Half a century later the Russian Baltic German Admiral Adam Johann von Krusenstern published the "Atlas de l'Ocean Pacifique", in which he renamed the islands the Cook Islands to honour Cook. Captain Cook navigated and mapped much of the group. Surprisingly, Cook never sighted the largest island, Rarotonga, and the only island that he personally set foot on was tiny, uninhabited Palmerston Atoll.
The first recorded landing by Europeans was in 1814 by the Cumberland; trouble broke out between the sailors and the Islanders and many were killed on both sides.
The islands saw no more Europeans until missionaries arrived from England in 1821. Christianity quickly took hold in the culture and remains the predominant religion today.
In 1823, Captain John Dibbs of the colonial barque Endeavour made the first official sighting of the island Rarotonga. The Endeavour was transporting Rev. John Williams on a missionary voyage to the islands.
Brutal Peruvian slave traders, known as blackbirders, took a terrible toll on the islands of the Northern Group in 1862 and 1863. At first the traders may have genuinely operated as labour recruiters, but they quickly turned to subterfuge and outright kidnapping to round up their human cargo. The Cook Islands was not the only island group visited by the traders, but Penrhyn Atoll was their first port of call and it has been estimated that three-quarters of the population was taken to Callao, Peru. Rakahanga and Pukapuka also suffered tremendous losses.
Islands become British.
The Cook Islands became a British protectorate in 1888, due largely to community fears that France might occupy the territory as it had Tahiti. On 6 September 1900, the leading islanders presented a petition asking that the islands (including Niue "if possible") should be annexed as British territory. On 8–9 October 1900 seven instruments of cession of Rarotonga and other islands were signed by their chiefs and people; and by a British Proclamation issued at the same time the cessions were accepted, the islands being declared parts of Her Britanic Majesty's dominions. These instruments did not include Aitutaki. It appears that, though the inhabitants regarded themselves as British subjects, the Crown's title was uncertain, and the island was formally annexed by Proclamation dated 9 October 1900. The islands were included within the boundaries of the Colony of New Zealand in 1901 by Order in Council under the Colonial Boundaries Act, 1895 of the United Kingdom.
Recent history.
They remained a New Zealand dependent territory until 1965, at which point they became a self-governing territory in free association with New Zealand. The first Prime Minister Sir Albert Henry led the county until 1978 when he was accused of vote-rigging.
Today, the Cook Islands are essentially independent (self-governing in free association with New Zealand), but are still officially placed under New Zealand sovereignty. New Zealand is tasked with overseeing the country's foreign relations and defense. The Cook Islands, Niue, New Zealand (and its territories: Tokelau and the Ross Dependency) make up the Realm of New Zealand.
After achieving autonomy in 1965, the Cook Islands elected Albert Henry of the Cook Islands Party as their first Prime Minister. He was succeeded in 1978 by Tom Davis of the Democratic Party.
On 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing its claim to the islands of Penrhyn, Pukapuka, Manihiki, and Rakahanga. In 1990 the Cook Islands signed a treaty with France which delimited the maritime boundary between the Cook Islands and French Polynesia.
On June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. "Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers," chief Makea Vakatini Joseph Ariki explained. The "Cook Islands Herald" suggested that the "ariki" were attempting thereby to regain some of their traditional prestige or "mana". Prime Minister Jim Marurai described the take-over move as "ill-founded and nonsensical". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.
The emigration of skilled workers to New Zealand and government deficits are continuing problems.
Timeline.
1595 — Spaniard Álvaro de Mendaña de Neira is the first European to sight the islands.
1606 — Portuguese-Spaniard Pedro Fernández de Quirós made the first recorded European landing in the islands when he set foot on Rakahanga.
1773 — Captain James Cook explored the islands and named them the Hervey Islands. Fifty years later they are renamed in his honour by Russian Admiral Adam Johann von Krusenstern.
1821 — English and Tahitian missionaries land in Aitutaki, become the first non-Polynesian settlers.
1823 — English missionary John Williams lands in Rarotonga, converting Makea Pori Ariki to Christianity.
1858 — The Cook Islands become united as a state, the Kingdom of Rarotonga.
1862 — Peruvian slave traders took a terrible toll on the islands of Penrhyn, Rakahanga and Pukapuka in 1862 and 1863.
1888 — Cook Islands are proclaimed a British protectorate and a single federal parliament is established.
1900 — The Cook Islands are ceded to the United Kingdom as British territory, except for Aitutaki which was annexed by the United Kingdom at the same time.
1901 — The boundaries of the Colony of New Zealand are extended by the United Kingdom to include the Cook Islands.
1924 — The All Black "Invincibles" stop in Rarotonga on their way to the United Kingdom and play a friendly match against a scratch Rarotongan team.
1946 — Legislative Council is established. For the first time since 1912, the territory has direct representation.
1965 — The Cook Islands become a self-governing territory in free association with New Zealand. Albert Henry, leader of the Cook Islands Party, is elected as the territory's first prime minister.
1974 — Albert Henry is knighted by Queen Elizabeth II
1979 — Sir Albert Henry is found guilty of electoral fraud and stripped of his premiership and his knighthood. Tom Davis becomes Premier.
1980 — Cook Islands – United States Maritime Boundary Treaty establishes the Cook Islands – American Samoa boundary
1981 — Constitution is amended. Parliament grows from 22 to 24 seats and the parliamentary term is extended from four to five years. Tom Davis is knighted.
1984 — The country's first coalition government, between Sir Thomas and Geoffrey Henry, is signed in the lead up to hosting regional Mini Games in 1985. Shifting coalitions saw ten years of political instability. At one stage, all but two MPs were in government.
1985 — Rarotonga Treaty is opened for signing in the Cook Islands, creating a nuclear free zone in the South Pacific.
1986 — In January 1986, following the rift between New Zealand and the USA in respect of the ANZUS security arrangements Prime Minister Tom Davis declared the Cook Islands a neutral country, because he considered that New Zealand (which has control over the islands' defence and foreign policy) was no longer in a position to defend the islands. The proclamation of neutrality meant that the Cook Islands would not enter into a military relationship with any foreign power, and, in particular, would prohibit visits by US warships. Visits by US naval vessels were allowed to resume by Henry's Government.
1990 — Cook Islands – France Maritime Delimitation Agreement establishes the Cook Islands – French Polynesia boundary
1991 — The Cook Islands signed a treaty of friendship and co-operation with France, covering economic development, trade and surveillance of the islands' EEZ. The establishment of closer relations with France was widely regarded as an expression of the Cook Islands' Government's dissatisfaction with existing arrangements with New Zealand which was no longer in a position to defend the Cook Islands.
1995 — The French Government resumed its Programme of nuclear-weapons testing at Mururoa Atoll in September 1995 upsetting the Cook Islands. New Prime Minister Geoffrey Henry was fiercely critical of the decision and dispatched a vaka (traditional voyaging canoe) with a crew of Cook Islands' traditional warriors to protest near the test site. The tests were concluded in January 1996 and a moratorium was placed on future testing by the French government.
1997 — Full diplomatic relations established with China.
1997 — In November, Cyclone Martin in Manihiki kills at least six people; 80% of buildings are damaged and the black pearl industry suffered severe losses.
1999 — A second era of political instability begins, starting with five different coalitions in less than nine months, and at least as many since then.
2000 — Full diplomatic relations concluded with France.
2002 — Prime Minister Terepai Maoate is ousted from government following second vote of no-confidence in his leadership.
2004 — Prime Minister Robert Woonton visits China; Chinese Premier Wen Jiabao grants $16 million in development aid.
2006 — Parliamentary elections held. The Democratic Party keeps majority of seats in parliament, but is unable to command a majority for confidence, forcing a coalition with breakaway MPs who left, then rejoined the "Demos."
2008 — Pacific Island nations imposed a series of measures aimed at halting overfishing.

</doc>
<doc id="7069" url="https://en.wikipedia.org/wiki?curid=7069" title="Geography of the Cook Islands">
Geography of the Cook Islands

The Cook Islands can be divided into two groups: the Southern Cook Islands and the Northern Cook Islands.
Location.
Oceania, group of islands in the South Pacific Ocean, about one-half of the way from Hawaii to New Zealand

</doc>
<doc id="7070" url="https://en.wikipedia.org/wiki?curid=7070" title="Demographics of the Cook Islands">
Demographics of the Cook Islands

This article is about the demographic features of the population of the Cook Islands, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

</doc>
<doc id="7071" url="https://en.wikipedia.org/wiki?curid=7071" title="Politics of the Cook Islands">
Politics of the Cook Islands

The politics of the Cook Islands, an associated state, takes place in a framework of a parliamentary representative democracy within a constitutional monarchy. The Queen of New Zealand, represented in the Cook Islands by the Queen's Representative, is the Head of State; the Chief Minister is the head of government and of a multi-party system. The Islands are self-governing in free association with New Zealand and are fully responsible for internal affairs. New Zealand retains some responsibility for external affairs, in consultation with the Cook Islands. In recent years, the Cook Islands have taken on more of its own external affairs; as of 2005, it has diplomatic relations in its own name with eighteen other countries. Executive power is exercised by the government, while legislative power is vested in both the government and the islands' parliament. The judiciary is independent of the executive and the legislatures.
Constitution.
The Constitution of the Cook Islands took effect on August 4, 1965, when the Cook Islands became a self-governing territory in free association with New Zealand. The anniversary of these events in 1965 is commemorated annually on Constitution Day, with week long activities known as "Te Maevea Nui Celebrations" locally.
Executive.
The monarch is hereditary; her representative is appointed by the monarch on the recommendation of the Cook Islands Government. The cabinet is chosen by the prime minister and collectively responsible to Parliament.
Ten years of rule by the Cook Islands Party (CIP) came to an end 18 November 1999 with the resignation of Prime Minister Joe Williams. Williams had led a minority government since October 1999 when the New Alliance Party (NAP) left the government coalition and joined the main opposition Democratic Party (DAP). On 18 November 1999, DAP leader Dr. Terepai Maoate was sworn in as prime minister. He was succeeded by his co-partisan Robert Woonton. When Dr Woonton lost his seat in the 2004 elections, Jim Marurai took over. In the 2010 elections, the CIP regained power and Henry Puna was sworn in as prime minister on 30 November 2010.
Following uncertainty about the ability of the government to maintain its majority, the Queen's representative dissolved parliament mid-way through its term and a 'snap' election was held on 26 September 2006. Jim Marurai's Democratic Party retained the Treasury benches with an increased majority.
The New Zealand High Commissioner is appointed by the New Zealand Government.
Legislature.
The Parliament of the Cook Islands has 24 members, elected for a five-year term in single-seat constituencies. There is also a House of Ariki, composed of chiefs, which has a purely advisory role. The Koutu Nui is a similar organization consisting of sub-chiefs. It was established by an amendment in 1972 of the 1966 House of Ariki Act. The current President is Te Tika Mataiapo Dorice Reid.
On June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. "Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers," chief Makea Vakatini Joseph Ariki explained. The "Cook Islands Herald" suggested that the "ariki" were attempting thereby to regain some of their traditional prestige or "mana". Prime Minister Jim Marurai described the take-over move as "ill-founded and nonsensical". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.
Recent political history.
The 1999 election produced a hung Parliament. Cook Islands Party leader Geoffrey Henry remained Prime Minister, but was replaced after a month by Joe Williams following a coalition realignment. A further realignment three months later saw Williams replaced by Democratic Party leader Terepai Maoate. A third realignment saw Maoate replaced mid-term by his deputy Robert Woonton in 2002, who ruled with the backing of the CIP.
The Democratic Party won a majority in the 2004 election, but Woonton lost his seat, and was replaced by Jim Marurai. In 2005 Marurai left the Democrats due to an internal disputes, founding his own Cook Islands First Party. He continued to govern with the support of the CIP, but in 2005 returned to the Democrats. The loss of several by-elections forced a snap-election in 2006, which produced a solid majority for the Democrats and saw Marurai continue as Prime Minister.
In December 2009, Marurai sacked his Deputy Prime Minister, Terepai Maoate, sparking a mass-resignation of Democratic Party cabinet members He and new Deputy Prime Minister Robert Wigmore were subsequently expelled from the Democratic Party. Marurai appointed three junior members of the Democratic party to Cabinet, but on 31 December 2009 the party withdrew its support.
In May 2014 a new party was formed by Teina Bishop of Aitutaki "One Cook Islands" Party.

</doc>
<doc id="7072" url="https://en.wikipedia.org/wiki?curid=7072" title="Economy of the Cook Islands">
Economy of the Cook Islands

The economy of the Cook Islands, as in many other South Pacific nations, is hindered by the isolation of the country from foreign markets, lack of natural resources, periodic devastation from natural disasters, and inadequate infrastructure.
Tourism provides the economic base with minor exports made up of tropical and citrus fruit. Manufacturing activities are limited to fruit-processing, clothing, and handicrafts.
Trade deficits are made up for by remittances from emigrants and by foreign aid, overwhelmingly from New Zealand. Efforts to exploit tourism potential, encourage offshore banking, and expand the mining and fishing industries have been partially successful in stimulating investment and growth.
Banking and finance.
The Cook Islands has "Home Rule" with respect to banking, similar to Guernsey, Jersey and the Isle of Man.
This "Home Rule" banking confuses New Zealanders on vacation in the Cooks. Cook automated teller machines often fail to fully disclose the fact that the Cooks are not part of the New Zealand banking system, thus legally requiring banks to charge the same fees for withdrawing or transferring money as if the person was in Australia or the EU. The New Zealand dollar is the official currency of the Cook Islands, adding to the confusion. Cook Islanders are NZ citizens.
The banking and incorporation laws of the Cook Islands make it an important centre for setting up companies that are involved in global trade.
Telecommunications.
Telecom Cook Islands Ltd (TCI) is the sole provider of telecommunications in the Cook Islands. TCI is a private company owned by Spark New Zealand Ltd (60%) and the Cook Islands Government (40%). In operation since July 1991, TCI provides local, national and international telecommunications as well as internet access on all islands except Suwarrow. Communications to Suwarrow is via HF radio.

</doc>
<doc id="7073" url="https://en.wikipedia.org/wiki?curid=7073" title="Telecommunications in the Cook Islands">
Telecommunications in the Cook Islands

Like most countries and territories in Oceania, telecommunications in the Cook Islands is limited by its isolation and low population, with only one major television broadcasting station and six radio stations. However, most residents have a main line or mobile phone. Its telecommunications are mainly provided by Telecom Cook Islands, who is currently working with O3b Networks, Ltd. for faster Internet connection.
Telephone.
In July 2012, there were about 7,500 main line telephones, which covers about 98% of the country's population, while there were approximately 7,800 mobile phones in 2009. Telecom Cook Islands, owned by Spark New Zealand, is the islands' main telephone system and offers international direct dialling, Internet, email, fax, and Telex. The individual islands are connected by a combination of satellite earth stations, microwave systems, and very high frequency and high frequency radiotelephone; within the islands, service is provided by small exchanges connected to subscribers by open wire, cable, and fibre optic cable. For international communication, they rely on the satellite earth station Intelsat.
In 2003, the largest island of Rarotonga started using a GSM/GPRS mobile data service system with GSM 900/1900 networks by 2006.
The Cook Islands uses the country calling code +682.
Broadcasting.
There are six radio stations in the Cook Islands, with one reaching all islands. there were 14,000 radios.
Cook Islands Television broadcasts from Rarotonga, providing a mix of local news and overseas-sourced programs. there were 4,000 television sets.
Internet.
There were 6,000 Internet users in 2009 and 3,562 Internet hosts as of 2012. The country code top-level domain for the Cook Islands is .ck.
In June 2010, Telecom Cook Islands partnered with O3b Networks, Ltd. to provide faster Internet connection to the Cook Islands. On 25 June 2013 the O3b satellite constellation was launched from an Arianespace Soyuz ST-B rocket in French Guinea. The medium Earth orbit satellite orbits at and uses the Ka band. It has a latency of about 100 milliseconds because it is much closer to Earth than standard geostationary satellites, whose latencies can be over 600 milliseconds. Although the initial launch consisted of 4 satellites, as many as 20 may be launched eventually to serve various areas with little or no optical fibre service, the first of which is the Cook Islands.
In December 2015, Alcatel-Lucent and Bluesky Pacific Group announced that they would build the Moana Cable system connecting New Zealand to Hawaii with a single fibre pair branching off to the Cook Islands. The Moana Cable is expected to be completed in 2018.

</doc>
<doc id="7074" url="https://en.wikipedia.org/wiki?curid=7074" title="Transport in the Cook Islands">
Transport in the Cook Islands

This article lists transport in the Cook Islands.

</doc>
<doc id="7077" url="https://en.wikipedia.org/wiki?curid=7077" title="Computer file">
Computer file

A computer file is a resource for storing information, which is available to a computer program and is usually based on some kind of durable storage. A file is "durable" in the sense that it remains available for other programs to use after the program that created it has finished executing. Computer files can be considered as the information technology counterpart of paper documents which traditionally are kept in office and library files, and this is the source of the term.
Etymology.
The word "file" was used publicly in the context of computer storage as early as February, 1950. In an RCA (Radio Corporation of America) advertisement in "Popular Science Magazine" describing a new "memory" vacuum tube it had developed, RCA stated:
In 1952, "file" was used in referring to information stored on punched cards.
In early usage, people regarded the underlying hardware (rather than the contents) as a file. For example, the IBM 350 disk drives were called "disk files". In about 1961 the Burroughs MCP and the MIT Compatible Time-Sharing System introduced the concept of a "file system", which managed several virtual "files" on one storage device, giving the term its present-day meaning. Although the current term "register file" shows the early concept of files, it has largely disappeared.
The word ultimately comes from the Latin filum "a thread".
File contents.
On most modern operating systems, files are organized into one-dimensional arrays of bytes. The format of a file is defined by its content since a file is solely a container for data, although, on some platforms the format is usually indicated by its filename extension, specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain text file (.txt in Windows) are associated with either ASCII or UTF-8 characters, while the bytes of image, video, and audio files are interpreted otherwise. Most file types also allocate a few bytes for metadata, which allows a file to carry some basic information about itself.
Some file systems can store arbitrary (not interpreted by the file system) file-specific data outside of the file format, but linked to the file, for example extended attributes or forks. On other file systems this can be done via sidecar files or software-specific databases. All those methods, however, are more susceptible to loss of metadata than are container and archive file formats.
File size.
At any instant in time, a file might have a size, normally expressed as number of bytes, that indicates how much storage is associated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system limit. Many older operating systems kept track only of the number of blocks or tracks occupied by a file on a physical storage device. In such systems, software employed other methods to track the exact byte count (e.g., CP/M used a special control character, Ctrl-Z, to signal the end of text files).
The general definition of a file does not require that its size have any real meaning, however, unless the data within the file happens to correspond to data within a pool of persistent storage. A special case is a zero byte file; these files can be newly created files that have not yet had any data written to them, or may serve as some kind of flag in the file system, or are accidents (the results of aborted disk operations). For example, the file to which the link /bin/ls points in a typical Unix-like system probably has a defined size that seldom changes. Compare this with /dev/null which is also a file, but its size may be obscure. (This is misleading because /dev/null is not really a file: in Unix-like systems, all resources, including devices, are accessed like files, but there is still a real distinction between files and devices--at core, they behave differently--and the obscurity of the "size" of /dev/null is one manifestation of this. As a character device, /dev/null "has" no size.)
Organizing the data in a file.
Information in a computer file can consist of smaller packets of information (often called "records" or "lines") that are individually different but share some common traits. For example, a payroll file might contain information concerning all the employees in a company and their payroll details; each record in the payroll file concerns just one employee, and all the records have the common trait of being related to payroll—this is very similar to placing all payroll information into a specific filing cabinet in an office that does not have a computer. A text file may contain lines of text, corresponding to printed lines on a piece of paper. Alternatively, a file may contain an arbitrary binary image (a BLOB) or it may contain an executable.
The way information is grouped into a file is entirely up to how it is designed. This has led to a plethora of more or less standardized file structures for all imaginable purposes, from the simplest to the most complex. Most computer files are used by computer programs which create, modify or delete the files for their own use on an as-needed basis. The programmers who create the programs decide what files are needed, how they are to be used and (often) their names.
In some cases, computer programs manipulate files that are made visible to the computer user. For example, in a word-processing program, the user manipulates document files that the user personally names. Although the content of the document file is arranged in a format that the word-processing program understands, the user is able to choose the name and location of the file and provide the bulk of the information (such as words and text) that will be stored in the file.
Many applications pack all their data files into a single file called an archive file, using internal markers to discern the different types of information contained within. The benefits of the archive file are to lower the number of files for easier transfer, to reduce storage usage, or just to organize outdated files. The archive file must often be unpacked before next using.
File operations.
The most basic operations that programs can perform on a file are:
Files on a computer can be created, moved, modified, grown, shrunk, and deleted. In most cases, computer programs that are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For instance, Microsoft Word files are normally created and modified by the Microsoft Word program in response to user commands, but the user can also move, rename, or delete these files directly by using a file manager program such as Windows Explorer (on Windows computers) or by command lines (CLI).
In Unix-like systems, user-space processes do not normally deal with files at all; the operating system provides a level of abstraction which means that almost all interaction with files from user-space is through hard links. For example, a user space program cannot delete a file; it can delete a link to a file, and if the kernel determines that there are no hard links to the file, it may then allow the memory location for the deleted file to be allocated for another file. The resulting free space, is commonly considered a security risk due to the existence of file recovery software. Such a risk has given rise to secure deletion programs. Only the kernel deals with files, but it handles all user-space interaction with (virtual) files in a manner that is transparent to the user-space programs.
Identifying and organizing files.
In modern computer systems, files are typically accessed using names (filenames). In some operating systems, the name is associated with the file itself. In others, the file is anonymous, and is pointed to by links that have names. In the latter case, a user can identify the name of the link with the file itself, but this is a false analogue, especially where there exists more than one link to the same file.
Files (or links to files) can be located in directories. However, more generally, a directory can contain either a list of files or a list of links to files. Within this definition, it is of paramount importance that the term "file" includes directories. This permits the existence of directory hierarchies, i.e., directories containing sub-directories. A name that refers to a file within a directory must be typically unique. In other words, there must be no identical names within a directory. However, in some operating systems, a name may include a specification of type that means a directory can contain an identical name for more than one type of object such as a directory and a file.
In environments in which a file is named, a file's name and the path to the file's directory must uniquely identify it among all other files in the computer system—no two files can have the same name and path. Where a file is anonymous, named references to it will exist within a namespace. In most cases, any name within the namespace will refer to exactly zero or one file. However, any file may be represented within any namespace by zero, one or more names.
Any string of characters may or may not be a well-formed name for a file or a link depending upon the context of application. Whether or not a name is well-formed depends on the type of computer system being used. Early computers permitted only a few letters or digits in the name of a file, but modern computers allow long names (some up to 255 characters) containing almost any combination of unicode letters or unicode digits, making it easier to understand the purpose of a file at a glance. Some computer systems allow file names to contain spaces; others do not. Case-sensitivity of file names is determined by the file system. Unix file systems are usually case sensitive and allow user-level applications to create files whose names differ only in the case of characters. Microsoft Windows supports multiple file systems, each with different policies regarding case-sensitivity. The common FAT file system can have multiple files whose names differ only in case if the user uses a disk editor to edit the file names in the directory entries. User applications, however, will usually not allow the user to create multiple files with the same name but differing in case.
Most computers organize files into hierarchies using folders, directories, or catalogs. The concept is the same irrespective of the terminology used. Each folder can contain an arbitrary number of files, and it can also contain other folders. These other folders are referred to as subfolders. Subfolders can contain still more files and folders and so on, thus building a tree-like structure in which one "master folder" (or "root folder" — the name varies from one operating system to another) can contain any number of levels of other folders and files. Folders can be named just as files can (except for the root folder, which often does not have a name). The use of folders makes it easier to organize files in a logical way.
When a computer allows the use of folders, each file and folder has not only a name of its own, but also a path, which identifies the folder or folders in which a file or folder resides. In the path, some sort of special character—such as a slash—is used to separate the file and folder names. For example, in the illustration shown in this article, the path /Payroll/Salaries/Managers uniquely identifies a file called Managers in a folder called Salaries, which in turn is contained in a folder called Payroll. The folder and file names are separated by slashes in this example; the topmost or root folder has no name, and so the path begins with a slash (if the root folder had a name, it would precede this first slash).
Many (but not all) computer systems use extensions in file names to help identify what they contain, also known as the file type. On Windows computers, extensions consist of a dot (period) at the end of a file name, followed by a few letters to identify the type of file. An extension of .txt identifies a text file; a .doc extension identifies any type of document or documentation, commonly in the Microsoft Word file format; and so on. Even when extensions are used in a computer system, the degree to which the computer system recognizes 
and heeds them can vary; in some systems, they are required, while in other systems, they are completely ignored if they are presented.
Protecting files.
Many modern computer systems provide methods for protecting files against accidental and deliberate damage. Computers that allow for multiple users implement file permissions to control who may or may not modify, delete, or create files and folders. For example, a given user may be granted only permission to read a file or folder, but not to modify or delete it; or a user may be given permission to read and modify files or folders, but not to execute them. Permissions may also be used to allow only certain users to see the contents of a file or folder. Permissions protect against unauthorized tampering or destruction of information in files, and keep private information confidential from unauthorized users.
Another protection mechanism implemented in many computers is a "read-only flag." When this flag is turned on for a file (which can be accomplished by a computer program or by a human user), the file can be examined, but it cannot be modified. This flag is useful for critical information that must not be modified or erased, such as special files that are used only by internal parts of the computer system. Some systems also include a "hidden flag" to make certain files invisible; this flag is used by the computer system to hide essential system files that users should not alter.
Storing files.
The discussion above describes a file as a concept presented to a user or a high-level operating system. However, any file that has any useful purpose, outside of a thought experiment, must have some physical manifestation. That is, a file (an abstract concept) in a real computer system must have a real physical analogue if it is to exist at all.
In physical terms, most computer files are stored on some type of data storage device. For example, there is a hard disk, from which most operating systems run and on which most store their files. Hard disks have been the ubiquitous form of non-volatile storage since the early 1960s. Where files contain only temporary information, they may be stored in RAM. Computer files can be also stored on other media in some cases, such as magnetic tapes, compact discs, Digital Versatile Discs, Zip drives, USB flash drives, etc. The use of solid state drives is also beginning to rival the hard
disk drive.
In Unix-like operating systems, many files have no direct association with a physical storage device: /dev/null is a prime example, as are just about all files under /dev, /proc and /sys. These can be accessed as files in user space. They are really virtual files that exist, in reality, as objects within the operating system kernel.
As seen by a running user program, files are usually represented either by a File Control Block or by a file handle. A File Control Block (FCB) is an area of memory which is manipulated to establish a filename etc. and then passed to the operating system as a parameter, it was used by older IBM operating systems and by early PC operating systems including CP/M and early versions of MS-DOS. A file handle is generally either an opaque data type or an integer, it was introduced in around 1961 by the ALGOL-based Burroughs MCP running on the Burroughs B5000 but is now ubiquitous.
Backing up files.
When computer files contain information that is extremely important, a "back-up" process is used to protect against disasters that might destroy the files. Backing up files simply means making copies of the files in a separate location so that they can be restored if something happens to the computer, or if they are deleted accidentally.
There are many ways to back up files. Most computer systems provide utility programs to assist in the back-up process, which can become very time-consuming if there are many files to safeguard. Files are often copied to removable media such as writable CDs or cartridge tapes. Copying files to another hard disk in the same computer protects against failure of one disk, but if it is necessary to protect against failure or destruction of the entire computer, then copies of the files must be made on other media that can be taken away from the computer and stored in a safe, distant location.
The grandfather-father-son backup method automatically makes three back-ups; the grandfather file is the oldest copy of the file and the son is the current copy.
File systems and file managers.
The way a computer organizes, names, stores and manipulates files is globally referred to as its "file system." Most computers have at least one file system. Some computers allow the use of several different file systems. For instance, on newer MS Windows computers, the older FAT-type file systems of MS-DOS and old versions of Windows are supported, in addition to the NTFS file system that is the normal file system for recent versions of Windows. Each system has its own advantages and disadvantages. Standard FAT allows only eight-character file names (plus a three-character extension) with no spaces, for example, whereas NTFS allows much longer names that can contain spaces. You can call a file "Payroll records" in NTFS, but in FAT you would be restricted to something like payroll.dat (unless you were using VFAT, a FAT extension allowing long file names).
File manager programs are utility programs that allow users to manipulate files directly. They allow you to move, create, delete and rename files and folders, although they do not actually allow you to read the contents of a file or store information in it. Every computer system provides at least one file-manager program for its native file system. For example, File Explorer (formerly Windows Explorer) is commonly used in Microsoft Windows operating systems, and Nautilus is common under several distributions of Linux.

</doc>
<doc id="7079" url="https://en.wikipedia.org/wiki?curid=7079" title="CID">
CID

CID may refer to:

</doc>
<doc id="7080" url="https://en.wikipedia.org/wiki?curid=7080" title="Christian Doppler">
Christian Doppler

Christian Andreas Doppler (; ; 29 November 1803 – 17 March 1853) was an Austrian mathematician and physicist. He is celebrated for his principle — known as the Doppler effect — that the observed frequency of a wave depends on the relative speed of the source and the observer. He used this concept to explain the color of binary stars.
Biography.
Doppler was born and raised in Salzburg, Austria, the son of a stonemason. He could not work in his father's business because of his generally weak physical condition. After completing high school, Doppler studied philosophy in Salzburg and mathematics and physics at the Imperial–Royal Polytechnic Institute (now Vienna University of Technology) where he began work as an assistant in 1829. In 1835 he began work at the Prague Polytechnic (now Czech Technical University), where he received an appointment in 1841.
Only a year later, at the age of 38, Doppler gave a lecture to the Royal Bohemian Society of Sciences and subsequently published his most notable work, "Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels" "(On the coloured light of the binary stars and some other stars of the heavens)". There is a facsimile edition with an English translation by Alec Eden. In this work, Doppler postulated his principle (later coined the Doppler effect) that the observed frequency of a wave depends on the relative speed of the source and the observer, and he tried to use this concept for explaining the colour of binary stars. While he worked at the Prague Polytechnic as a professor he published over 50 articles on mathematics, physics and astronomy. 
In 1847 he left Prague for the professorship of mathematics, physics, and mechanics at the Academy of Mines and Forests (its successor is the present day University of Miskolc) in Selmecbánya (then Kingdom of Hungary, now Banská Štiavnica, Slovakia), and in 1849 he moved to Vienna.
Doppler's research was interrupted by the revolutionary incidents of 1848. During the Hungarian Revolution, he fled to Vienna. There he was appointed head of the Institute for Experimental Physics at the University of Vienna in 1850. During his time there, Doppler, along with Franz Unger, played an influential role in the development of young Gregor Mendel, known as the founding father of genetics, who was a student at the University of Vienna from 1851 to 1853.
Doppler died on 17 March 1853 at age 49 from a pulmonary disease in Venice (at that time part of the Austrian Empire). His tomb, found by Dr. Peter M. Schuster is just inside the entrance of the Venetian island cemetery of San Michele.
Full name.
Some confusion exists about Doppler's full name. Doppler referred to himself as Christian Doppler. The records of his birth and baptism stated Christian "Andreas" Doppler. Forty years after Doppler's death the misnomer "Johann" Christian Doppler was introduced by the astronomer Julius Scheiner. Scheiner's mistake has since been copied by many.

</doc>
<doc id="7081" url="https://en.wikipedia.org/wiki?curid=7081" title="Clerihew">
Clerihew

A clerihew () is a whimsical, four-line biographical poem invented by Edmund Clerihew Bentley. The first line is the name of the poem's subject, usually a famous person put in an absurd light. The rhyme scheme is AABB, and the rhymes are often forced. The line length and metre are irregular. Bentley invented the clerihew in school and then popularized it in books. One of his best known is this (1905):
Form.
A clerihew has the following properties:
Clerihews are not satirical or abusive, but they target famous individuals and reposition them in an absurd, anachronistic or commonplace setting, often giving them an over-simplified and slightly garbled description (not unlike the schoolboy style of "1066 and All That").
The unbalanced and unpolished poetic meter and line length parody the limerick, and the clerihew in form also parodies the eulogy.
Practitioners.
The form was invented by and is named after Edmund Clerihew Bentley. When he was a 16-year-old pupil at St Paul's School in London, the lines about Humphry Davy came into his head during a science class. Together with his schoolfriends, he filled a notebook with examples. The first use of the word in print was in 1928. Clerihew published three volumes of his own clerihews: "Biography for Beginners" (1905), published as "edited by E. Clerihew"; "More Biography" (1929); and "Baseless Biography" (1939), a compilation of clerihews originally published in "Punch" illustrated by the author's son Nicolas Bentley.
G. K. Chesterton, a friend of Bentley, was also a practitioner of the clerihew and one of the sources of its popularity. Chesterton provided verses and illustrations for the original schoolboy notebook and illustrated "Biography for Beginners". Other serious authors also produced clerihews, including W. H. Auden, and it remains a popular humorous form among other writers and the general public. Among contemporary writers, the satirist Craig Brown has made considerable use of the clerihew in his columns for "The Daily Telegraph".
Examples.
Bentley's first clerihew, published in 1905, was written about Sir Humphry Davy:
The original poem had the second line “Was not fond of gravy"; but the better-known published version has the more succinct “Abominated gravy”.
Other classic clerihews by Bentley include:
and
Auden's "Literary Graffiti" includes:
The subject of a clerihew written by the students of his "alma mater", Sherborne School in England, was one of the founders of computing:
A clerihew much appreciated by chemists is cited in "Dark Sun" by Richard Rhodes, and regards the inventor of the thermos bottle (or Dewar flask): 
In 1983, "Games" Magazine ran a contest titled "Do You Clerihew?" The winning entry was:
Other uses of the form.
The clerihew form has also occasionally been used for non-biographical verses. Bentley opened his 1905 "Biography for Beginners" with an example, entitled "Introductory Remarks", on the theme of biography itself:
The third edition of the same work, published in 1925, included a "Preface to the New Edition" in 11 stanzas, each in clerihew form. One stanza ran:

</doc>
<doc id="7085" url="https://en.wikipedia.org/wiki?curid=7085" title="Civil war">
Civil war

A civil war is a war between organized groups within the same state or country, or, less commonly, between two countries created from a formerly united state.
The aim of one side may be to take control of the country or a region, to achieve independence for a region or to change government policies.
The term is a calque of the Latin "bellum civile" which was used to refer to the various civil wars of the Roman Republic in the 1st century BC.
A civil war is a high-intensity conflict, often involving regular armed forces, that is sustained, organized and large-scale. Civil wars may result in large numbers of casualties and the consumption of significant resources. Most modern civil wars involve intervention by outside powers. According to Patrick M. Regan in his book "Civil Wars and Foreign Powers" (2000) about two thirds of the 138 intrastate conflicts between the end of World War II and 2000 saw international intervention, with the United States intervening in 35 of these conflicts.
Civil wars since the end of World War II have lasted on average just over four years, a dramatic rise from the one-and-a-half-year average of the 1900–1944 period. While the rate of emergence of new civil wars has been relatively steady since the mid-19th century, the increasing length of those wars has resulted in increasing numbers of wars ongoing at any one time. For example, there were no more than five civil wars underway simultaneously in the first half of the 20th century while there were over 20 concurrent civil wars close to the end of the Cold War. Since 1945, civil wars have resulted in the deaths of over 25 million people, as well as the forced displacement of millions more. Civil wars have further resulted in economic collapse; Somalia, Burma (Myanmar), Uganda and Angola are examples of nations that were considered to have promising futures before being engulfed in civil wars.
Formal classification.
James Fearon, a scholar of civil wars at Stanford University, defines a civil war as "a violent conflict within a country fought by organized groups that aim to take power at the center or in a region, or to change government policies". Ann Hironaka further specifies that one side of a civil war is the state. The intensity at which a civil disturbance becomes a civil war is contested by academics. Some political scientists define a civil war as having more than 1000 casualties, while others further specify that at least 100 must come from each side. The Correlates of War, a dataset widely used by scholars of conflict, classifies civil wars as having over 1000 war-related casualties per year of conflict. This rate is a small fraction of the millions killed in the Second Sudanese Civil War and Cambodian Civil War, for example, but excludes several highly publicized conflicts, such as The Troubles of Northern Ireland and the struggle of the African National Congress in Apartheid-era South Africa.
Based on the 1000 casualties per year criterion, there were 213 civil wars from 1816 to 1997, 104 of which occurred from 1944 to 1997. If one uses the less-stringent 1000 casualties total criterion, there were over 90 civil wars between 1945 and 2007, with 20 ongoing civil wars as of 2007.
The Geneva Conventions do not specifically define the term "civil war", nevertheless they do outline the responsibilities of parties in "armed conflict not of an international character". This includes civil wars, however no specific definition of civil war is provided in the text of the Conventions.
Nevertheless, the International Committee of the Red Cross has sought to provide some clarification through its commentaries on the Geneva Conventions, noting that the Conventions are "so general, so vague, that many of the delegations feared that it might be taken to cover any act committed by force of arms". Accordingly, the commentaries provide for different 'conditions' on which the application of the Geneva Convention would depend, the commentary however points out that these should not be interpreted as rigid conditions. The conditions listed by the ICRC in its commentary are as follows:
That the Party in revolt against the de jure Government possesses an organized military force, an authority responsible for its acts, acting within a determinate territory and having the means of respecting and ensuring respect for the Convention.
That the legal Government is obliged to have recourse to the regular military forces against insurgents organized as military and in possession of a part of the national territory.
(a) That the de jure Government has recognized the insurgents as belligerents; or
(b) That it has claimed for itself the rights of a belligerent; or
(c) That it has accorded the insurgents recognition as belligerents for the purposes only of the present Convention; or
(d) That the dispute has been admitted to the agenda of the Security
Council or the General Assembly of the United Nations as being a
threat to international peace, a breach of the peace, or an act
of aggression.
(a) That the insurgents have an organization purporting to have the
characteristics of a State.
(b) That the insurgent civil authority exercises de facto authority
over the population within a determinate portion of the national
territory.
(c) That the armed forces act under the direction of an organized
authority and are prepared to observe the ordinary laws of war.
(d) That the insurgent civil authority agrees to be bound by the
provisions of the Convention.
Causes of civil war in the Collier–Hoeffler Model.
Scholars investigating the cause of civil war are attracted by two opposing theories, greed versus grievance. Roughly stated: are conflicts caused by who people are, whether that be defined in terms of ethnicity, religion or other social affiliation, or do conflicts begin because it is in the economic best interests of individuals and groups to start them? Scholarly analysis supports the conclusion that economic and structural factors are more important than those of identity in predicting occurrences of civil war.
A comprehensive study of civil war was carried out by a team from the World Bank in the early 21st century. The study framework, which came to be called the Collier–Hoeffler Model, examined 78 five-year increments when civil war occurred from 1960 to 1999, as well as 1,167 five-year increments of "no civil war" for comparison, and subjected the data set to regression analysis to see the effect of various factors. The factors that were shown to have a statistically significant effect on the chance that a civil war would occur in any given five-year period were:
A high proportion of primary commodities in national exports significantly increases the risk of a conflict. A country at "peak danger", with commodities comprising 32% of gross domestic product, has a 22% risk of falling into civil war in a given five-year period, while a country with no primary commodity exports has a 1% risk. When disaggregated, only petroleum and non-petroleum groupings showed different results: a country with relatively low levels of dependence on petroleum exports is at slightly less risk, while a high level of dependence on oil as an export results in slightly more risk of a civil war than national dependence on another primary commodity. The authors of the study interpreted this as being the result of the ease by which primary commodities may be extorted or captured compared to other forms of wealth; for example, it is easy to capture and control the output of a gold mine or oil field compared to a sector of garment manufacturing or hospitality services.
A second source of finance is national diasporas, which can fund rebellions and insurgencies from abroad. The study found that statistically switching the size of a country's diaspora from the smallest found in the study to the largest resulted in a sixfold increase in the chance of a civil war.
Higher male secondary school enrollment, per capita income and economic growth rate all had significant effects on reducing the chance of civil war. Specifically, a male secondary school enrollment 10% above the average reduced the chance of a conflict by about 3%, while a growth rate 1% higher than the study average resulted in a decline in the chance of a civil war of about 1%. The study interpreted these three factors as proxies for earnings forgone by rebellion, and therefore that lower forgone earnings encourage rebellion. Phrased another way: young males (who make up the vast majority of combatants in civil wars) are less likely to join a rebellion if they are getting an education or have a comfortable salary, and can reasonably assume that they will prosper in the future.
Low per capita income has been proposed as a cause for grievance, prompting armed rebellion. However, for this to be true, one would expect economic inequality to also be a significant factor in rebellions, which it is not. The study therefore concluded that the economic model of opportunity cost better explained the findings.
High levels of population dispersion and, to a lesser extent, the presence of mountainous terrain, increased the chance of conflict. Both of these factors favor rebels, as a population dispersed outward toward the borders is harder to control than one concentrated in a central region, while mountains offer terrain where rebels can seek sanctuary.
Most proxies for "grievance" – the theory that civil wars begin because of issues of identity, rather than economics – were statistically insignificant, including economic equality, political rights, ethnic polarization and religious fractionalization. Only ethnic dominance, the case where the largest ethnic group comprises a majority of the population, increased the risk of civil war. A country characterized by ethnic dominance has nearly twice the chance of a civil war. However, the combined effects of ethnic and religious fractionalization, i.e. the greater chance that any two randomly chosen people will be from separate ethnic or religious groups, the less chance of a civil war, were also significant and positive, as long as the country avoided ethnic dominance. The study interpreted this as stating that minority groups are more likely to rebel if they feel that they are being dominated, but that rebellions are more likely to occur the more homogeneous the population and thus more cohesive the rebels. These two factors may thus be seen as mitigating each other in many cases.
The various factors contributing to the risk of civil war rise increase with population size. The risk of a civil war rises approximately proportionately with the size of a country's population.
The more time that has elapsed since the last civil war, the less likely it is that a conflict will recur. The study had two possible explanations for this: one opportunity-based and the other grievance-based. The elapsed time may represent the depreciation of whatever capital the rebellion was fought over and thus increase the opportunity cost of restarting the conflict. Alternatively, elapsed time may represent the gradual process of healing of old hatreds. The study found that the presence of a diaspora substantially reduced the positive effect of time, as the funding from diasporas offsets the depreciation of rebellion-specific capital.
Other causes.
Evolutionary psychologist Satoshi Kanazawa has argued that an important cause of intergroup conflict may be the relative availability of women of reproductive age. He found that polygyny greatly increased the frequency of civil wars but not interstate wars. Gleditsch et al. did not find a relationship between ethnic groups with polygyny and increased frequency of civil wars but nations having legal polygamy may have more civil wars. They argued that misogyny is a better explanation than polygyny. They found that increased women's rights were are associated with less civil wars and that legal polygamy had no effect after women’s rights were controlled for.
Duration of civil wars.
Ann Hironaka, author of "Neverending Wars", divides the modern history of civil wars into the pre-19th century, 19th century to early 20th century, and late 20th century. In 19th-century Europe, the length of civil wars fell significantly, largely due to the nature of the conflicts as battles for the power center of the state, the strength of centralized governments, and the normally quick and decisive intervention by other states to support the government. Following World War II the duration of civil wars grew past the norm of the pre-19th century, largely due to weakness of the many postcolonial states and the intervention by major powers on both sides of conflict. The most obvious commonality to civil wars are that they occur in fragile states.
Civil wars in the 19th and early 20th centuries.
Civil wars in the 19th century and in the early 20th century tended to be short; civil wars between 1900 and 1944 lasted on average one and half years. The state itself formed the obvious center of authority in the majority of cases, and the civil wars were thus fought for control of the state. This meant that whoever had control of the capital and the military could normally crush resistance. A rebellion which failed to quickly seize the capital and control of the military for itself normally found itself doomed to rapid destruction. For example, the fighting associated with the 1871 Paris Commune occurred almost entirely in Paris, and ended quickly once the military sided with the government at Versailles and conquered Paris.
The power of non-state actors resulted in a lower value placed on sovereignty in the 18th and 19th centuries, which further reduced the number of civil wars. For example, the pirates of the Barbary Coast were recognized as "de facto" states because of their military power. The Barbary pirates thus had no need to rebel against the Ottoman Empire – their nominal state government – to gain recognition for their sovereignty. Conversely, states such as Virginia and Massachusetts in the United States of America did not have sovereign status, but had significant political and economic independence coupled with weak federal control, reducing the incentive to secede.
The two major global ideologies, monarchism and democracy, led to several civil wars. However, a bi-polar world, divided between the two ideologies, did not develop, largely due to the dominance of monarchists through most of the period. The monarchists would thus normally intervene in other countries to stop democratic movements taking control and forming democratic governments, which were seen by monarchists as being both dangerous and unpredictable. The Great Powers (defined in the 1815 Congress of Vienna as the United Kingdom, Habsburg Austria, Prussia, France, and Russia) would frequently coordinate interventions in other nations' civil wars, nearly always on the side of the incumbent government. Given the military strength of the Great Powers, these interventions nearly always proved decisive and quickly ended the civil wars.
There were several exceptions from the general rule of quick civil wars during this period. The American Civil War (1861–1865) was unusual for at least two reasons: it was fought around regional identities as well as political ideologies, and it ended through a war of attrition, rather than with a decisive battle over control of the capital, as was the norm. The Spanish Civil War (1936–1939) proved exceptional because "both" sides in the struggle received support from intervening great powers: Germany, Italy, and Portugal supported opposition leader Francisco Franco, while France and the Soviet Union supported the government (see proxy war).
Civil wars since 1945.
In the 1990s, about twenty civil wars were occurring concurrently during an average year, a rate about ten times the historical average since the 19th century. However, the rate of new civil wars had not increased appreciably; the drastic rise in the number of ongoing wars after World War II was a result of the tripling of the average duration of civil wars to over four years. This increase was a result of the increased number of states, the fragility of states formed after 1945, the decline in interstate war, and the Cold War rivalry.
Following World War II, the major European powers divested themselves of their colonies at an increasing rate: the number of ex-colonial states jumped from about 30 to almost 120 after the war. The rate of state formation leveled off in the 1980s, at which point few colonies remained. More states also meant more states in which to have long civil wars. Hironaka statistically measures the impact of the increased number of ex-colonial states as increasing the post-WWII incidence of civil wars by +165% over the pre-1945 number.
While the new ex-colonial states appeared to follow the blueprint of the idealized state – centralized government, territory enclosed by defined borders, and citizenry with defined rights -, as well as accessories such as a national flag, an anthem, a seat at the United Nations and an official economic policy, they were in actuality far weaker than the Western states they were modeled after. In Western states, the structure of governments closely matched states' actual capabilities, which had been arduously developed over centuries. The development of strong administrative structures, in particular those related to extraction of taxes, is closely associated with the intense warfare between predatory European states in the 17th and 18th centuries, or in Charles Tilly's famous formulation: "War made the state and the state made war". For example, the formation of the modern states of Germany and Italy in the 19th century is closely associated with the wars of expansion and consolidation led by Prussia and Sardinia-Piedmont, respectively. The Western process of forming effective and impersonal bureaucracies, developing efficient tax systems, and integrating national territory continued into the 20th century. Nevertheless, Western states that survived into the latter half of the 20th century were considered "strong" by simple reason that they had managed to develop the institutional structures and military capability required to survive predation by their fellow states.
In sharp contrast, decolonization was an entirely different process of state formation. Most imperial powers had not foreseen a need to prepare their colonies for independence; for example, Britain had given limited self-rule to India and Sri Lanka, while treating British Somaliland as little more than a trading post, while all major decisions for French colonies were made in Paris and Belgium prohibited any self-government up until it suddenly granted independence to its colonies in 1960. Like Western states of previous centuries, the new ex-colonies lacked autonomous bureaucracies, which would make decisions based on the benefit to society as a whole, rather than respond to corruption and nepotism to favor a particular interest group. In such a situation, factions manipulate the state to benefit themselves or, alternatively, state leaders use the bureaucracy to further their own self-interest. The lack of credible governance was compounded by the fact that most colonies were economic loss-makers at independence, lacking both a productive economic base and a taxation system to effectively extract resources from economic activity. Among the rare states profitable at decolonization was India, to which scholars credibly argue that Uganda, Malaysia and Angola may be included. Neither did imperial powers make territorial integration a priority, and may have discouraged nascent nationalism as a danger to their rule. Many newly independent states thus found themselves impoverished, with minimal administrative capacity in a fragmented society, while faced with the expectation of immediately meeting the demands of a modern state. Such states are considered "weak" or "fragile". The "strong"-"weak" categorization is not the same as "Western"-"non-Western", as some Latin American states like Argentina and Brazil and Middle Eastern states like Egypt and Israel are considered to have "strong" administrative structures and economic infrastructure.
Historically, the international community would have targeted weak states for territorial absorption or colonial domination or, alternatively, such states would fragment into pieces small enough to be effectively administered and secured by a local power. However, international norms towards sovereignty changed in the wake of WWII in ways that support and maintain the existence of weak states. Weak states are given "de jure" sovereignty equal to that of other states, even when they do not have "de facto" sovereignty or control of their own territory, including the privileges of international diplomatic recognition and an equal vote in the United Nations. Further, the international community offers development aid to weak states, which helps maintain the facade of a functioning modern state by giving the appearance that the state is capable of fulfilling its implied responsibilities of control and order. The formation of a strong international law regime and norms against territorial aggression is strongly associated with the dramatic drop in the number of interstate wars, though it has also been attributed to the effect of the Cold War or to the changing nature of economic development. Consequently, military aggression that results in territorial annexation became increasingly likely to prompt international condemnation, diplomatic censure, a reduction in international aid or the introduction of economic sanction, or, as in the case of 1990 invasion of Kuwait by Iraq, international military intervention to reverse the territorial aggression. Similarly, the international community has largely refused to recognize secessionist regions, while keeping some secessionist self-declared states such as Somaliland in diplomatic recognition limbo. While there is not a large body of academic work examining the relationship, Hironaka's statistical study found a correlation that suggests that every major international anti-secessionist declaration increased the number of ongoing civil wars by +10%, or a total +114% from 1945 to 1997. The diplomatic and legal protection given by the international community, as well as economic support to weak governments and discouragement of secession, thus had the unintended effect of encouraging civil wars.
Interventions by outside powers.
There has been an enormous amount of international intervention in civil wars since 1945 that some have argued served to extend wars. According to Patrick M. Regan in his book "Civil Wars and Foreign Powers" (2000) about 2/3rds of the 138 intrastate conflicts between the end of WWII and 2000 saw international intervention, with the United States intervening in 35 of these conflicts. While intervention has been practiced since the international system has existed, its nature changed substantially. It became common for both the state and opposition group to receive foreign support, allowing wars to continue well past the point when domestic resources had been exhausted. Superpowers, such as the European great powers, had always felt no compunction in intervening in civil wars that affected their interests, while distant regional powers such as the United States could declare the interventionist Monroe Doctrine of 1821 for events in its Central American "backyard". However, the large population of weak states after 1945 allowed intervention by former colonial powers, regional powers and neighboring states who themselves often had scarce resources. On average, a civil war with interstate intervention was 300% longer than those without. When disaggregated, a civil war with intervention on only one side is 156% longer, while when intervention occurs on both sides the average civil war is longer by an additional 92%. If one of the intervening states was a superpower, a civil war is a further 72% longer; a conflict such as the Angolan Civil War, in which there is two-sided foreign intervention, including by a superpower (actually, two superpowers in the case of Angola), would be 538% longer on average than a civil war without any international intervention.
Effect of the Cold War.
The Cold War (1945–1989) provided a global network of material and ideological support that often helped perpetuate civil wars, which were mainly fought in weak ex-colonial states rather than the relatively strong states that were aligned with the Warsaw Pact and North Atlantic Treaty Organization. In some cases, superpowers would superimpose Cold War ideology onto local conflicts, while in others local actors using Cold War ideology would attract the attention of a superpower to obtain support. Using a separate statistical evaluation than used above for interventions, civil wars that included pro- or anti-communist forces lasted 141% longer than the average non-Cold War conflict, while a Cold War civil war that attracted superpower intervention resulted in wars typically lasting over three times as long as other civil wars. Conversely, the end of the Cold War marked by the fall of the Berlin Wall in 1989 resulted in a reduction in the duration of Cold War civil wars of 92% or, phrased another way, a roughly ten-fold increase in the rate of resolution of Cold War civil wars. Lengthy Cold War-associated civil conflicts that ground to a halt include the wars of Guatemala (1960–1996), El Salvador (1979–1991) and Nicaragua (1970–1990).

</doc>
<doc id="7088" url="https://en.wikipedia.org/wiki?curid=7088" title="List of cryptographers">
List of cryptographers

List of cryptographers.
Modern.
See also: Category:Modern cryptographers for an exhaustive list.

</doc>
<doc id="7089" url="https://en.wikipedia.org/wiki?curid=7089" title="Chocolate">
Chocolate

Chocolate is a typically sweet, usually brown, food preparation of "Theobroma cacao" seeds, roasted and ground, often flavored, as with vanilla. It is made in the form of a liquid, paste, or in a block, or used as a flavoring ingredient in other foods. Cacao has been cultivated by many cultures for at least three millennia in Mesoamerica. The earliest evidence of use traces to the Mokaya (Mexico and Guatemala), with evidence of chocolate beverages dating back to 1900 BC. In fact, the majority of Mesoamerican people made chocolate beverages, including the Maya and Aztecs, who made it into a beverage known as "xocolātl" , a Nahuatl word meaning "bitter water". The seeds of the cacao tree have an intense bitter taste and must be fermented to develop the flavor.
After fermentation, the beans are dried, cleaned, and roasted. The shell is removed to produce cacao nibs, which are then ground to cocoa mass, pure chocolate in rough form. Because the cocoa mass is usually liquefied before being molded with or without other ingredients, it is called chocolate liquor. The liquor also may be processed into two components: cocoa solids and cocoa butter. Unsweetened baking chocolate (bitter chocolate) contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate consumed today is in the form of sweet chocolate, a combination of cocoa solids, cocoa butter or other fat, and sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk, but no cocoa solids.
Cocoa solids are a source of flavonoids and alkaloids, such as theobromine, phenethylamine and caffeine.
Chocolate has become one of the most popular food types and flavors in the world, and a vast number of foodstuffs involving chocolate have been created, particularly desserts including cakes, pudding, mousse, chocolate brownies, and chocolate chip cookies. Many candies are filled with or coated with sweetened chocolate, and bars of solid chocolate and candy bars coated in chocolate are eaten as snacks. Gifts of chocolate molded into different shapes (e.g., eggs, hearts) have become traditional on certain Western holidays, such as Easter and Valentine's Day. Chocolate is also used in cold and hot beverages such as chocolate milk and hot chocolate and in some alcoholic drinks, such as creme de cacao.
Although cocoa originated in the Americas, in the 2000s, Western Africa produces almost two-thirds of the world's cocoa, with Ivory Coast growing almost half of it. In the 2000s, concerns about the use of child labor and the human trafficking and slavery of child laborers in African cocoa cultivation were raised.
Etymology.
The word "chocolate" entered the English language from Spanish in about 1600. How the word came into Spanish is less certain, and there are competing explanations. Perhaps the most cited explanation is that "chocolate" comes from Nahuatl, the language of the Aztecs, from the word "chocolātl", which many sources say derived from "xocolātl" , combining "xococ", sour or bitter, and "ātl", water or drink. The word "chocolatl" does not occur in central Mexican colonial sources, making this an unlikely derivation. Another derivation comes from the Yucatec Mayan word "chokol" meaning hot, and the Nahuatl "atl" meaning water. The Nahuatl term, "chicolatl", meaning "beaten drink", may derive from the word for the frothing stick, "chicoli".The term "chocolate chip" was first used in 1940. The term "chocolatier", for a chocolate confection maker, is attested from 1888.
History.
Mesoamerican usage.
Chocolate has been prepared as a drink for nearly all of its history. For example, one vessel found at an Olmec archaeological site on the Gulf Coast of Veracruz, Mexico, dates chocolate's preparation by pre-Olmec peoples as early as 1750 BC. On the Pacific coast of Chiapas, Mexico, a Mokaya archaeological site provides evidence of cacao beverages dating even earlier, to 1900 BC. The residues and the kind of vessel in which they were found indicate the initial use of cacao was not simply as a beverage, but the white pulp around the cacao beans was likely used as a source of fermentable sugars for an alcoholic drink.
An early Classic-period (460–480 AD) Mayan tomb from the site in Rio Azul had vessels with the Maya glyph for cacao on them with residue of a chocolate drink, suggests the Maya were drinking chocolate around 400 AD. Documents in Maya hieroglyphs stated chocolate was used for ceremonial purposes, in addition to everyday life. The Maya grew cacao trees in their backyards, and used the cacao seeds the trees produced to make a frothy, bitter drink.
By the 15th century, the Aztecs gained control of a large part of Mesoamerica and adopted cacao into their culture. They associated chocolate with Quetzalcoatl, who, according to one legend, was cast away by the other gods for sharing chocolate with humans, and identified its extrication from the pod with the removal of the human heart in sacrifice. In contrast to the Maya, who liked their chocolate warm, the Aztecs drank it cold, seasoning it with a broad variety of additives, including the petals of the "Cymbopetalum penduliflorum" tree, chile pepper, allspice, vanilla, and honey.
The Aztecs were not able to grow cacao themselves, as their home in the Mexican highlands was unsuitable for it, so chocolate was a luxury imported into the empire. Those who lived in areas ruled by the Aztecs were required to offer cacao seeds in payment of the tax they deemed "tribute". Cocoa beans were often used as currency. For example, the Aztecs used a system in which one turkey cost 100 cacao beans and one fresh avocado was worth three beans.
European adaptation.
Until the 16th century, no European had ever heard of the popular drink from the Central and South American peoples. Christopher Columbus and his son Ferdinand encountered the cacao bean on Columbus's fourth mission to the Americas on 15 August 1502, when he and his crew seized a large native canoe that proved to contain cacao beans among other goods for trade. Spanish conquistador Hernán Cortés may have been the first European to encounter it, as the frothy drink was part of the after-dinner routine of Montezuma. Jose de Acosta, a Spanish Jesuit missionary who lived in Peru and then Mexico in the later 16th century, wrote of its growing influence on the Spaniards:
Loathsome to such as are not acquainted with it, having a scum or froth that is very unpleasant taste. Yet it is a drink very much esteemed among the Indians, where with they feast noble men who pass through their country. The Spaniards, both men and women that are accustomed to the country are very greedy of this Chocolate. They say they make diverse sorts of it, some hot, some cold, and some temperate, and put therein much of that "chili"; yea, they make paste thereof, the which they say is good for the stomach and against the catarrh.
While Columbus had taken cacao beans with him back to Spain, chocolate made no impact until Spanish friars introduced it to the Spanish court. After the Spanish conquest of the Aztecs, chocolate was imported to Europe. There, it quickly became a court favorite. It was still served as a beverage, but the Spanish added sugar, as well as honey, to counteract the natural bitterness. Vanilla was also a popular additive, with pepper and other spices sometimes used to give the illusion of a more potent vanilla flavor. Unfortunately, these spices had the tendency to unsettle the European constitution; the "Encyclopédie" states, "The pleasant scent and sublime taste it imparts to chocolate have made it highly recommended; but a long experience having shown that it could potentially upset one's stomach," which is why chocolate without vanilla was sometimes referred to as "healthy chocolate." By 1602, chocolate had made its way from Spain to Austria. By 1662, the bishop of Rome had declared that religious fasts were not broken by consuming chocolate drinks. Within about a hundred years, chocolate established a foothold throughout Europe.
The new craze for chocolate brought with it a thriving slave market, as between the early 1600s and late 1800s, the laborious and slow processing of the cacao bean was manual. Cacao plantations spread, as the English, Dutch, and French colonized and planted. With the depletion of Mesoamerican workers, largely to disease, cacao production was often the work of poor wage laborers and African slaves. Wind-powered and horse-drawn mills were used to speed production, augmenting human labor. Heating the working areas of the table-mill, an innovation that emerged in France in 1732, also assisted in extraction.
New processes that sped the production of chocolate emerged early in the Industrial Revolution. In 1815, Dutch chemist Coenraad van Houten introduced alkaline salts to chocolate, which reduced its bitterness. A few years thereafter, in 1828, he created a press to remove about half the natural fat (cocoa butter or cacao butter) from chocolate liquor, which made chocolate both cheaper to produce and more consistent in quality. This innovation introduced the modern era of chocolate. Known as "Dutch cocoa", this machine-pressed chocolate was instrumental in the transformation of chocolate to its solid form when, in 1847, Joseph Fry learned to make chocolate moldable by adding back melted cacao butter. Milk had sometimes been used as an addition to chocolate beverages since the mid-17th century, but in 1875 Daniel Peter invented milk chocolate by mixing a powdered milk developed by Henri Nestlé with the liquor. In 1879, the texture and taste of chocolate was further improved when Rudolphe Lindt invented the conching machine.
Besides Nestlé, a number of notable chocolate companies had their start in the late 19th and early 20th centuries. Rowntree's of York set up and began producing chocolate in 1862, after buying out the Tuke family business. Cadbury was manufacturing boxed chocolates in England by 1868. In 1893, Milton S. Hershey purchased chocolate processing equipment at the World's Columbian Exposition in Chicago, and soon began the career of Hershey's chocolates with chocolate-coated caramels.
Types.
Several types of chocolate can be distinguished. Pure, unsweetened chocolate contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate consumed today is in the form of sweet chocolate, combining chocolate with sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. In the U.K. and Ireland, milk chocolate must contain a minimum of 20% total dry cocoa solids; in the rest of the European Union, the minimum is 25%. "White chocolate" contains cocoa butter, sugar, and milk, but no cocoa solids. Chocolate contains alkaloids such as theobromine and phenethylamine, which may have physiological effects in humans, but the presence of theobromine renders it toxic to some animals, such as dogs and cats. Dark chocolate has been promoted for unproven health benefits.
White chocolate, although similar in texture to that of milk and dark chocolate, does not contain any cocoa solids. Because of this, many countries do not consider white chocolate as chocolate at all. Because it does not contain any cocoa solids, white chocolate does not contain any theobromine, so it can be consumed by animals.
Dark chocolate is produced by adding fat and sugar to the cacao mixture. The U.S. Food and Drug Administration calls this "sweet chocolate", and requires a 15% concentration of chocolate liquor. European rules specify a minimum of 35% cocoa solids. Semisweet chocolate is a dark chocolate with a low sugar content. Bittersweet chocolate is chocolate liquor to which some sugar (typically a third), more cocoa butter, vanilla, and sometimes lecithin have been added. It has less sugar and more liquor than semisweet chocolate, but the two are interchangeable in baking.
Unsweetened chocolate is pure chocolate liquor, also known as bitter or baking chocolate. It is unadulterated chocolate: the pure, ground, roasted chocolate beans impart a strong, deep chocolate flavor. Raw chocolate, often referred to as raw cacao, is always dark and a minimum of 75% cacao.
Chocolate may have whitish spots on the dark chocolate part, called chocolate bloom; it is an indication that sugar and/or fat has separated due to poor storage. It is not toxic.
Production.
Roughly two-thirds of the entire world's cocoa is produced in West Africa, with 43% sourced from Ivory Coast, where child labor is a common practice to obtain the product. According to the World Cocoa Foundation, some 50 million people around the world depend on cocoa as a source of livelihood. In the UK, most chocolatiers purchase their chocolate from them, to melt, mold and package to their own design. According to the WCF's 2012 report, the Ivory Coast is the largest producer of cocoa in the world.
Production costs can be decreased by reducing cocoa solids content or by substituting cocoa butter with another fat. Cocoa growers object to allowing the resulting food to be called "chocolate", due to the risk of lower demand for their crops. The sequencing in 2010 of the genome of the cacao tree may allow yields to be improved.
The two main jobs associated with creating chocolate candy are chocolate makers and chocolatiers. Chocolate makers use harvested cacao beans and other ingredients to produce couverture chocolate (covering). Chocolatiers use the finished couverture to make chocolate candies (bars, truffles, etc.).
Cacao varieties.
Chocolate is made from cocoa beans, the dried and partially fermented seeds of the cacao tree ("Theobroma cacao"), a small (4– to 8-m-tall (15– to 26-ft-tall) evergreen tree native to the deep tropical region of the Americas. Recent genetic studies suggest the most common genotype of the plant originated in the Amazon basin and was gradually transported by humans throughout South and Central America. Early forms of another genotype have also been found in what is now Venezuela. The scientific name, "Theobroma", means "food of the deities". The fruit, called a cacao pod, is ovoid, long and wide, ripening yellow to orange, and weighing about when ripe.
Cacao trees are small, understory trees that need rich, well-drained soils. They naturally grow within 20° of either side of the equator because they need about 2000 mm of rainfall a year, and temperatures in the range of . Cacao trees cannot tolerate a temperature lower than .
The three main varieties of cacao beans used in chocolate are criollo, forastero, and trinitario.
Criollo.
Representing only 5% of all cocoa beans grown, criollo is the rarest and most expensive cocoa on the market, and is native to Central America, the Caribbean islands and the northern tier of South American states. The genetic purity of cocoas sold today as criollo is disputed, as most populations have been exposed to the genetic influence of other varieties.
Criollos are particularly difficult to grow, as they are vulnerable to a variety of environmental threats and produce low yields of cocoa per tree. The flavor of criollo is described as delicate yet complex, low in classic chocolate flavor, but rich in "secondary" notes of long duration.
Forastero.
The most commonly grown bean is forastero, a large group of wild and cultivated cacaos, most likely native to the Amazon basin. The African cocoa crop is entirely of the forastero variety. They are significantly hardier and of higher yield than criollo. The source of most chocolate marketed, forastero cocoas are typically strong in classic "chocolate" flavor, but have a short duration and are unsupported by secondary flavors, producing "quite bland" chocolate.
Trinitario.
Trinitario is a natural hybrid of criollo and forastero. Trinitario originated in Trinidad after an introduction of forastero to the local criollo crop. Nearly all cacao produced over the past five decades is of the forastero or lower-grade trinitario varieties.
Processing.
Cacao pods are harvested by cutting them from the tree using a machete, or by knocking them off the tree using a stick. The beans with their surrounding pulp are removed from the pods and placed in piles or bins, allowing access to micro-organisms so fermentation of the pectin-containing material can begin. Yeasts produce ethanol, lactic acid bacteria produce lactic acid, and acetic acid bacteria produce acetic acid. The fermentation process, which takes up to seven days, also produces several flavor precursors, eventually resulting in the familiar chocolate taste.
It is important to harvest the pods when they are fully ripe, because if the pod is unripe, the beans will have a low cocoa butter content, or sugars in the white pulp will be insufficient for fermentation, resulting in a weak flavor. After fermentation, the beans must be quickly dried to prevent mold growth. Climate and weather permitting, this is done by spreading the beans out in the sun from five to seven days.
The dried beans are then transported to a chocolate manufacturing facility. The beans are cleaned (removing twigs, stones, and other debris), roasted, and graded. Next, the shell of each bean is removed to extract the nib. Finally, the nibs are ground and liquefied, resulting in pure chocolate in fluid form: chocolate liquor. The liquor can be further processed into two components: cocoa solids and cocoa butter.
Blending.
Chocolate liquor is blended with the cocoa butter in varying quantities to make different types of chocolate or couvertures. The basic blends of ingredients for the various types of chocolate (in order of highest quantity of cocoa liquor first), are:
Usually, an emulsifying agent, such as soy lecithin, is added, though a few manufacturers prefer to exclude this ingredient for purity reasons and to remain GMO-free, sometimes at the cost of a perfectly smooth texture. Some manufacturers are now using PGPR, an artificial emulsifier derived from castor oil that allows them to reduce the amount of cocoa butter while maintaining the same mouthfeel.
The texture is also heavily influenced by processing, specifically conching (see below). The more expensive chocolate tends to be processed longer and thus have a smoother texture and mouthfeel, regardless of whether emulsifying agents are added.
Different manufacturers develop their own "signature" blends based on the above formulas, but varying proportions of the different constituents are used. The finest, plain dark chocolate couvertures contain at least 70% cocoa (both solids and butter), whereas milk chocolate usually contains up to 50%. High-quality white chocolate couvertures contain only about 35% cocoa butter.
Producers of high-quality, small-batch chocolate argue that mass production produces bad-quality chocolate. Some mass-produced chocolate contains much less cocoa (as low as 7% in many cases), and fats other than cocoa butter. Vegetable oils and artificial vanilla flavor are often used in cheaper chocolate to mask poorly fermented and/or roasted beans.
In 2007, the Chocolate Manufacturers Association in the United States, whose members include Hershey, Nestlé, and Archer Daniels Midland, lobbied the Food and Drug Administration (FDA) to change the legal definition of chocolate to let them substitute partially hydrogenated vegetable oils for cocoa butter, in addition to using artificial sweeteners and milk substitutes. Currently, the FDA does not allow a product to be referred to as "chocolate" if the product contains any of these ingredients. In the EU a product can be sold as chocolate if it contains up to 5% vegetable oil, and must be labelled as 'family milk chocolate' rather than 'milk chocolate' if it contains 20% milk.
Conching.
The penultimate process is called conching. A conche is a container filled with metal beads, which act as grinders. The refined and blended chocolate mass is kept in a liquid state by frictional heat. Chocolate prior to conching has an uneven and gritty texture. The conching process produces cocoa and sugar particles smaller than the tongue can detect, hence the smooth feel in the mouth. The length of the conching process determines the final smoothness and quality of the chocolate. High-quality chocolate is conched for about 72 hours, and lesser grades about four to six hours. After the process is complete, the chocolate mass is stored in tanks heated to about until final processing.
Tempering.
The final process is called tempering. Uncontrolled crystallization of cocoa butter typically results in crystals of varying size, some or all large enough to be clearly seen with the naked eye. This causes the surface of the chocolate to appear mottled and matte, and causes the chocolate to crumble rather than snap when broken. The uniform sheen and crisp bite of properly processed chocolate are the result of consistently small cocoa butter crystals produced by the tempering process.
The fats in cocoa butter can crystallize in six different forms (polymorphous crystallization). The primary purpose of tempering is to assure that only the best form is present. The six different crystal forms have different properties.
As a solid piece of chocolate, the cocoa butter fat particles are in a crystalline rigid structure that gives the chocolate its solid appearance. Once heated, the crystals of the polymorphic cocoa butter are able to break apart from the rigid structure and allow the chocolate to obtain a more fluid consistency as the temperature increases – the melting process. When the heat is removed, the cocoa butter crystals become rigid again and come closer together, allowing the chocolate to solidify.
The temperature in which the crystals obtain enough energy to break apart from their rigid conformation would depend on the milk fat content in the chocolate and the shape of the fat molecules, as well as the form of the cocoa butter fat. Chocolate with a higher fat content will melt at a lower temperature.
Making chocolate considered "good" is about forming as many type V crystals as possible. This provides the best appearance and texture and creates the most stable crystals, so the texture and appearance will not degrade over time. To accomplish this, the temperature is carefully manipulated during the crystallization.
Generally, the chocolate is first heated to to melt all six forms of crystals. Next, the chocolate is cooled to about , which will allow crystal types IV and V to form. At this temperature, the chocolate is agitated to create many small crystal "seeds" which will serve as nuclei to create small crystals in the chocolate. The chocolate is then heated to about to eliminate any type IV crystals, leaving just type V. After this point, any excessive heating of the chocolate will destroy the temper and this process will have to be repeated. However, other methods of chocolate tempering are used. The most common variant is introducing already tempered, solid "seed" chocolate. The temper of chocolate can be measured with a chocolate temper meter to ensure accuracy and consistency. A sample cup is filled with the chocolate and placed in the unit which then displays or prints the results.
Two classic ways of manually tempering chocolate are:
Chocolate tempering machines (or temperers) with computer controls can be used for producing consistently tempered chocolate, particularly for large volume applications.
Storage.
Chocolate is very sensitive to temperature and humidity. Ideal storage temperatures are between , with a relative humidity of less than 50%. Various types of "blooming" effects can occur if chocolate is stored or served improperly. Fat bloom is caused by storage temperature fluctuating or exceeding , while sugar bloom is caused by temperature below or excess humidity. To distinguish between different types of bloom, one can rub the surface of the chocolate lightly, and if the bloom disappears, it is fat bloom. One can get rid of bloom by retempering the chocolate or using it for any use that requires melting the chocolate.
Chocolate is generally stored away from other foods, as it can absorb different aromas. Ideally, chocolates are packed or wrapped, and placed in proper storage with the correct humidity and temperature. Additionally, chocolate is frequently stored in a dark place or protected from light by wrapping paper.
If refrigerated or frozen without containment, chocolate can absorb enough moisture to cause a whitish discoloration, the result of fat or sugar crystals rising to the surface. Moving chocolate from one temperature extreme to another, such as from a refrigerator on a hot day, can result in an oily texture. Although visually unappealing, chocolate suffering from bloom is perfectly safe for consumption.
Nutrition and research.
Nutrition.
A 100 gram serving of milk chocolate supplies 540 calories. It is 59% carbohydrates (52% as sugar and 3% as dietary fiber), 30% fat and 8% protein (table). Approximately 65% of the fat in milk chocolate is saturated, composed mainly of palmitic acid and stearic acid, while the predominant unsaturated fat is oleic acid (table, see USDA reference for full report).
In 100 gram amounts, milk chocolate is an excellent source (> 19% of the Daily Value, DV) of riboflavin, vitamin B12 and the dietary minerals, manganese, phosphorus and zinc (table). Chocolate is a good source (10-19% DV) of calcium, magnesium and iron (table).
Research.
Chocolate and cocoa are under preliminary research to determine if consumption affects the risk of certain cardiovascular diseases or cognitive abilities.
Chocolate may be a factor for heartburn in some people because one of its constituents, theobromine, may affect the oesophageal sphincter muscle, hence permitting stomach acidic contents to enter into the oesophagus. Theobromine is also toxic to some animals unable to metabolize it (see theobromine poisoning).
Excessive consumption of large quantities of any energy-rich food, such as chocolate, without a corresponding increase in activity to expend the associated calories, can increase the risk of weight gain and possibly obesity. Raw chocolate is high in cocoa butter, a fat which is removed during chocolate refining, then added back in varying proportions during the manufacturing process. Manufacturers may add other fats, sugars, and milk as well, all of which increase the caloric content of chocolate.
Chocolate and cocoa contain moderate to high amounts of oxalate, which may increase risk for kidney stones. During cultivation and production, chocolate may absorb lead from the environment, but the total amounts typically eaten are less than the tolerable daily limit for lead consumption, according to the World Health Organization.
A few studies have documented allergic reactions from chocolate in children.
Labeling.
Some manufacturers provide the percentage of chocolate in a finished chocolate confection as a label quoting percentage of "cocoa" or "cacao". It should be noted that this refers to the combined percentage of both cocoa solids and cocoa butter in the bar, not just the percentage of cocoa solids. The Belgian AMBAO certification mark indicates that no non-cocoa vegetable fats have been used in making the chocolate.
Chocolates that are organic or fair trade certified carry labels accordingly.
In the United States, some large chocolate manufacturers lobbied the federal government to permit confections containing cheaper hydrogenated vegetable oil in place of cocoa butter to be sold as "chocolate". In June 2007, as a response to consumer concern after the proposed change, the FDA reiterated "Cacao fat, as one of the signature characteristics of the product, will remain a principal component of standardized chocolate."
Industry.
The chocolate industry is a steadily growing, $50 billion-a-year worldwide business centered on the sale and consumption of chocolate. It is prevalent throughout most of the world. Europe accounts for 45% of the world's chocolate revenue and the US $20 billion. Big Chocolate is the grouping of major international chocolate companies in Europe and the U.S. The U.S. companies, such as Mars and Hershey’s alone, generate $13 billion a year in chocolate sales and account for two-thirds of U.S. production. Despite the expanding reach of the chocolate industry internationally, cocoa farmers and labourers in the Ivory Coast are unaware of the uses of the beans. The high cost of chocolate in the Ivory Coast also means that it is inaccessible to the majority of the population, who are unaware of what it tastes like.
Manufacturers.
Chocolate manufacturers produce a range of products from chocolate bars to fudge. Large manufacturers of chocolate products include Cadbury (the world's largest confectionery manufacturer), Guylian, The Hershey Company, Lindt & Sprüngli, Mars, Incorporated, Milka, Neuhaus and Suchard.
Guylian is best known for its chocolate sea shells; Cadbury for its Dairy Milk and Creme Egg. The Hershey Company, the largest chocolate manufacturer in North America, produces the Hershey Bar and Hershey's Kisses. Mars Incorporated, a large privately owned U.S. corporation, produces Mars Bar, Milky Way, M&M's, Twix, and Snickers. Lindt is known for its truffle balls and gold foil-wrapped Easter bunnies.
Food conglomerates Nestlé SA and Kraft Foods both have chocolate brands. Nestlé acquired Rowntree's in 1988 and now markets chocolates under their own brand, including Smarties (a chocolate candy) and Kit Kat (a candy bar); Kraft Foods through its 1990 acquisition of Jacobs Suchard, now owns Milka and Suchard. In February 2010, Kraft also acquired British-based Cadbury.; Fry's, Trebor Basset and the fair trade brand Green & Black's also belongs to the group.
Human trafficking of child labourers.
The widespread use of children in cocoa production is controversial, not only for the concerns about child labor and exploitation, but also because up to 12,000 of the 200,000 children working in Côte d'Ivoire, the world's biggest producer of cocoa, may be victims of trafficking or slavery. Most attention on this subject has focused on West Africa, which collectively supplies 69 percent of the world's cocoa, and Côte d'Ivoire in particular, which supplies 35 percent of the world's cocoa. Thirty percent of children under age 15 in sub-Saharan Africa are child laborers, mostly in agricultural activities including cocoa farming. It is estimated that more than 1.8 million children in West Africa are involved in growing cocoa. Major chocolate producers, such as Nestlé, buy cocoa at commodities exchanges where Ivorian cocoa is mixed with other cocoa.
In 2009, Salvation Army International Development (SAID) UK stated that 12,000 children have been trafficked on cocoa farms in the Ivory Coast of Africa, where half of the world's chocolate is made. SAID UK states that it is these child slaves who are likely to be working in "harsh and abusive" conditions for the production of chocolate, and an increasing number of health-food and anti-slavery organisations are now highlighting and campaigning against the use of trafficking in the chocolate industry.
Fair trade.
In the 2000s, some chocolate producers began to engage in fair trade initiatives, to address concerns about the marginalization of cocoa laborers in developing countries. Traditionally, Africa and other developing countries received low prices for their exported commodities such as cocoa, which caused poverty to abound. Fair trade seeks to establish a system of direct trade from developing countries to counteract this unfair system. One solution for fair labor practices is for farmers to become part of an Agricultural cooperative. Cooperatives pay farmers a fair price for their cocoa so farmers have enough money for food, clothes, and school fees. One of the main tenets of fair trade is that farmers receive a fair price, but this does not mean that the larger amount of money paid for fair trade cocoa goes directly to the farmers. The effectiveness of fair trade has been questioned. In a 2014 article, "The Economist" stated that workers on fair trade farms have a lower standard of living than on similar farms outside the fair trade system.
Usage and consumption.
Chocolate is sold in chocolate bars, which come in dark chocolate, milk chocolate and white chocolate varieties. Some bars that are mostly chocolate have other ingredients blended into the chocolate, such as nuts, raisins or crisped rice. Chocolate is used as an ingredient in a huge variety of candy bars, which typically contain various confectionary ingredients (e.g., nougat, wafers, caramel, nuts, etc.) which are coated in chocolate. Chocolate is used as a flavouring product in many desserts, such as chocolate cakes, chocolate brownies, chocolate mousse and chocolate chip cookies. Numerous types of candy and snacks contain chocolate, either as a filling (e.g., M&Ms) or as a coating (e.g., chocolate-coated raisins or chocolate-coated peanuts. Some non-alcoholic beverages contain chocolate, such as chocolate milk, hot chocolate and chocolate milkshakes. Some alcoholic liqueurs are flavoured with chocolate, such as chocolate liqueur and creme de cacao. Chocolate is a popular flavour of ice cream and pudding and chocolate sauce is a commonly added as a topping on ice cream sundaes.
Popular culture.
Religious and cultural links.
Chocolate is associated with festivals such as Easter, when moulded chocolate rabbits and eggs are traditionally given in Christian communities, and Hanukkah, when chocolate coins are given in Jewish communities. Chocolate hearts and chocolate in heart-shaped boxes are popular on Valentine's Day and are often presented along with flowers and a greeting card. Chocolate is an acceptable gift on other holidays and on occasions such as birthdays.
Many confectioners make holiday-specific chocolate candies. Chocolate Easter eggs or rabbits and Santa Claus figures are two examples. Such confections can be solid, hollow, or filled with sweets or fondant.
Books and film.
Chocolate has been the center of several successful book and film adaptations. In 1964, Roald Dahl published a children's novel titled "Charlie and the Chocolate Factory". The novel centers on a poor boy named Charlie Bucket who takes a tour through the greatest chocolate factory in the world, owned by Willy Wonka. Two film adaptations of the novel were produced. The first was "Willy Wonka & the Chocolate Factory", a 1971 film which later became a cult classic, and spawned the real world Willy Wonka Candy Company, which produces chocolate products to this day. Thirty-four years later, a second film adaptation was produced, titled "Charlie and the Chocolate Factory". The 2005 film was very well received by critics and was one of the highest grossing films that year, earning over US$470,000,000 worldwide. "Charlie and the Chocolate Factory" was also recognized at the 78th Academy Awards, where it was nominated for Best Costume Design for Gabriella Pesucci.
"Like Water for Chocolate" ("Como agua para chocolate"), a 1989 love story by novelist Laura Esquivel, was adapted to film in 1992. The plot incorporates magical realism with Mexican cuisine, and the title is a double entendre in its native language, referring both to a recipe for hot chocolate and to an idiom that is a metaphor for sexual arousal. The film earned 11 Ariel Awards from the Academia Mexicana de Artes y Ciencias Cinematográficas, including Best Picture.
"Chocolat", a 1999 novel by Joanne Harris, tells the story of Vianne Rocher, a young mother, whose confections change the lives of the townspeople. The 2000 film adaptation, "Chocolat", also proved successful, grossing over US$150,000,000 worldwide, and receiving Academy Award and Golden Globe nominations for Best Picture, Best Actress, and Best Original Score.

</doc>
<doc id="7100" url="https://en.wikipedia.org/wiki?curid=7100" title="Cornet">
Cornet

The cornet is a brass instrument very similar to the trumpet, distinguished by its conical bore, compact shape, and mellower tone quality. The most common cornet is a transposing instrument in B, though there is also a soprano cornet in E. Both are unrelated to the renaissance and early baroque cornett.
History.
The cornet was initially derived from the post horn around 1820 in France. Among the first manufacturers of modern cornets was Parisian Jean Asté in 1828. Cornets first appeared as separate instrumental parts in 19th century French compositions.
This instrument could not have been developed without the improvement of piston valves by Heinrich Stölzel and Friedrich Blühmel. In the early 19th century these two instrument makers almost simultaneously invented the valves still used today. They jointly applied for a patent and were granted this for a period of ten years. The first notable virtuoso player was Jean-Baptiste Arban, who studied the cornet extensively and published "La grande méthode complète de cornet à piston et de saxhorn", commonly referred to as the "Arban method", in 1864. Up until the early 20th century, the trumpet and cornet coexisted in musical ensembles. Symphonic repertoire often involves separate parts for trumpet and cornet. As several instrument builders made improvements to both instruments, they started to look and sound more alike. The modern day cornet is used in brass bands, concert bands, and in specific orchestral repertoire that requires a more mellow sound.
The name cornet derives from corne, meaning "horn", itself from Latin cornus. While not musically related, instruments of the Zink family (which includes serpents) are named "cornetto", with a tonal or pitch related Latin word following the hyphen to describe the particular variant. The 11th edition of the "Encyclopædia Britannica" referred to serpents as "old wooden cornets". The Roman/Etruscan cornu (or simply "horn") is the lingual ancestor of these. It is a predecessor of the post horn from which the cornet evolved and was used like a bugle to signal orders on the battlefield.
The instrument was once sometimes referred to as a cornopean, referencing the earliest cornets with the Stölzel valve system.
Relationship to trumpet.
The cornet was invented by adding valves to the post horn in 1814. The valves allowed for melodic playing throughout the register of the cornet. Trumpets were slower to adopt the new valve technology, so for the next 100 years or more, composers often wrote separate parts for trumpet and cornet. The trumpet would play fanfare-like passages, while the cornet played more melodic passages. The modern trumpet has valves that allow it to play the same notes and fingerings as the cornet.
Cornets and trumpets made in a given key (usually the key of B) play at the same pitch, and the technique for playing the instruments is nearly identical. However, cornets and trumpets are not entirely interchangeable, as they differ in timbre. Also available, but usually seen only in the brass band, is an E soprano model, pitched a fourth above the standard B.
Unlike the trumpet, which has a cylindrical bore up until the bell section, the tubing of the cornet has a mostly conical bore, starting very narrow at the mouthpiece and gradually widening towards the bell. Cornets following the 1913 patent of E.A. Couturier can have a continuously conical bore. The conical bore of the cornet is primarily responsible for its characteristic warm, mellow tone, which can be distinguished from the more penetrating sound of the trumpet. The conical bore of the cornet also makes it more agile than the trumpet when playing fast passages, but correct pitching is often less assured. The cornet is often preferred for young beginners as it is easier to hold, with its centre of gravity much closer to the player.
The cornet mouthpiece has a shorter and narrower shank than that of a trumpet so it can fit the cornet's smaller mouthpiece receiver. The cup size is often deeper than that of a trumpet mouthpiece.
One variety is the short model traditional cornet, also known as a ""Shepherd's Crook"" shaped model. These are most often large–bore instruments with a rich mellow sound. There is also a long-model cornet, usually with a smaller bore and a brighter sound, which is closer to a trumpet in appearance. The Shepherd's Crook model is preferred by cornet traditionalists. The long-model cornet is generally used in concert bands in the United States, but has found little following in British-style brass and concert bands.
Echo cornet.
The echo cornet is an obsolete variant which has a mute chamber (or echo chamber) mounted to the side acting as a second bell when the fourth valve is pressed. The second bell has a sound similar to that of a Harmon mute and is typically used to play echo phrases, whereupon the player imitates the sound from the primary bell using the echo chamber.
Playing technique.
Like the trumpet and all other modern brass wind instruments, the cornet makes a sound when the player vibrates ("buzzes") the lips in the mouthpiece, creating a vibrating column of air in the tubing. The frequency of the air column's vibration can be modified by changing the lip tension and aperture or "embouchure", and by altering the tongue position to change the shape of the oral cavity, thereby increasing or decreasing the speed of the airstream. In addition, the column of air can be lengthened by engaging one or more valves, thus lowering the pitch. Double and triple tonguing are also possible.
Without valves, the player could only produce a harmonic series of notes like those played by the bugle and other "natural" brass instruments. These notes are far apart for most of the instrument's range, making diatonic and chromatic playing impossible except in the extreme high register. The valves change the length of the vibrating column and provide the cornet with the ability to play chromatically.
Ensembles with cornets.
Brass band.
British brass band ensembles consist completely of brass instruments (except for the percussion section). The cornet is the leading melodic instrument in this ensemble and trumpets are never used. The ensemble consists of about thirty musicians, including nine B cornets and one E cornet (soprano cornet) in the higher registers. In England, companies such as Besson and Boosey and Hawkes specialized in these instruments. In America, 19th century manufacturers such as Graves and Company, Hall and Quinby, E.G. Wright and the Boston Musical Instrument Manufactury built lines of instruments for this format of ensemble.
Concert band.
Early American concert band pieces, particularly those written or transcribed before 1960, often feature distinct, separate parts for trumpets and cornets. Cornet parts are rarely ever included in later pieces, however. The cornet also features in the British-style concert band, unlike the contemporary American concert band or wind band, where it is replaced by the trumpet. This slight difference in instrumentation derives from the British concert band's heritage in military bands, where the highest brass instrument is always the cornet. There are usually four to six B cornets present in a concert band, but no E instrument, as this role is taken by the E clarinet.
Fanfare orkest.
Fanfare orkesten ("fanfare orchestras"), only found in the Netherlands, Belgium, Northern France and Lithuania use the complete saxhorn family of instruments. The standard instrumentation includes both the cornet and the trumpet; however, in recent decades, the cornet has largely been replaced by the trumpet.
Jazz ensemble.
In old style jazz bands the cornet was preferred to the trumpet, but from the swing era onwards it has been largely replaced by the louder, more piercing trumpet. Likewise the cornet has been largely phased out of big bands by a growing taste for louder and more aggressive instruments, especially since the advent of bebop in the post World War II era.
Legendary jazz pioneer Buddy Bolden played the cornet, and Louis Armstrong started off on the cornet but later switched to the trumpet. Cornetists such as Bubber Miley and Rex Stewart contributed substantially to the Duke Ellington Orchestra's early sound. Other influential jazz cornetists include King Oliver, Bix Beiderbecke, Ruby Braff and Nat Adderley. Notable performances on cornet by players generally associated with the trumpet include Freddie Hubbard's on "Empyrean Isles" by Herbie Hancock and Don Cherry's on "The Shape of Jazz to Come" by Ornette Coleman.
Notable cornetists.
Today.
Influential contemporary cornet players include:

</doc>
<doc id="7102" url="https://en.wikipedia.org/wiki?curid=7102" title="CAMP">
CAMP

CAMP or cAMP may stand for:

</doc>
<doc id="7103" url="https://en.wikipedia.org/wiki?curid=7103" title="CGMP">
CGMP

CGMP is an initialism. It can refer to:

</doc>
<doc id="7104" url="https://en.wikipedia.org/wiki?curid=7104" title="Cotton Mather">
Cotton Mather

Cotton Mather, FRS (February 12, 1663 – February 13, 1728; A.B. 1678, Harvard College; A.M. 1681, honorary doctorate 1710, University of Glasgow) was a socially and politically influential New England Puritan minister, prolific author, and pamphleteer. Known for his vigorous support for the Salem witch trials, he also left a scientific legacy due to his hybridization experiments and his promotion of inoculation for disease prevention. He was subsequently denied the Presidency of Harvard College which his father, Increase, had held.
Life and work.
Mather was born in Boston, Massachusetts Bay Colony, the son of Maria (née Cotton) and Increase Mather, and grandson of both John Cotton and Richard Mather, all also prominent Puritan ministers. Mather was named after his maternal grandfather, John Cotton. He attended Boston Latin School, where his name was posthumously added to its Hall of Fame, and graduated from Harvard in 1678 at age 15. After completing his post-graduate work, he joined his father as assistant pastor of Boston's original North Church (not to be confused with the Anglican/Episcopal Old North Church of Paul Revere fame). In 1685 Mather assumed full responsibilities as pastor of the church.
Mather wrote more than 450 books and pamphlets, and his ubiquitous literary works made him one of the most influential religious leaders in America. Mather set the moral tone in the colonies, and sounded the call for second- and third-generation Puritans, whose parents had left England for the New England colonies of North America, to return to the theological roots of Puritanism. The most important of these, "Magnalia Christi Americana" (1702), comprises seven distinct books, many of which depict biographical and historical narratives.
From his religious training, Mather viewed the importance of texts for elaborating meaning and for bridging different moments of history—linking, for instance, the Biblical stories of Noah and Abraham with the arrival of such eminent leaders as John Eliot, John Winthrop, and his own father, Increase. Highly influential, Mather was a force to be reckoned with in secular, as well as in spiritual, matters. After the fall of James II of England, in 1688, Mather was among the leaders of the successful revolt against James' governor of the consolidated Dominion of New England, Sir Edmund Andros.
Mather was not known for writing in a neutral, unbiased perspective. Many, if not all, of his writings had bits and pieces of his own personal life in them or were written for personal reasons. According to literary historian Sacvan Bercovitch:
Mather influenced early American science. In 1716, because of observations of corn varieties, he conducted one of the first recorded experiments with plant hybridization. This observation was memorialized in a letter to his friend James Petiver:
In November 1713, Mather's wife, newborn twins, and two-year-old daughter all succumbed during a measles epidemic. Twice widowed, of his 15 children, only two survived Mather, who died on the day after his 65th birthday and was buried on Copp's Hill, near Old North Church.
Boyle's influence on Mather.
A huge influence throughout Mather's career was Robert Boyle. Mather read "The Usefulness of Experimental Natural Philosophy", which Boyle had written. Mather read Boyle's work closely throughout the 1680s and his early works on science and religion borrowed greatly from it. He used almost identical language to Boyle.
Increase Mather.
Mather's relationship with his own father, Increase Mather, is thought by some to have been strained and difficult. Increase was a pastor of the North Square Church and president of Harvard College; he led an accomplished life. Despite Cotton's efforts, he never became quite as well known and successful in politics as his father. He did surpass his father's output as a writer, writing over 400 books. One of the most public displays of their strained relationship emerged during the witch trials, which Increase Mather reportedly did not support.
Yale College.
Cotton Mather helped convince Elihu Yale to make a donation to a new college in New Haven that would come to be Yale College.
Salem witch trials of 1692, The Mather Influence.
In 1689, Mather published "Memorable Providences," detailing the supposed afflictions of several children in the Goodwin family in Boston. Catholic washerwoman Goody Glover was convicted of witchcraft and executed in this case. Robert Calef, a contemporary critic of Mather, considered this book responsible for laying the groundwork for the Salem witch trials three years later: 
Nineteenth-century historian Charles Wentworth Upham shared the view that the afflicted in Salem were imitating the Goodwin children, but put the blame on both Cotton and his father, Increase Mather: 
Mather was influential in the construction of the court for the trials from the beginning. Sir William Phips, governor of the newly chartered Province of Massachusetts Bay, appointed his lieutenant governor, William Stoughton, as head of a special witchcraft tribunal and then as chief justice of the colonial courts, where he presided over the witch trials. According to George Bancroft, Mather had been influential in gaining the politically unpopular Stoughton his appointment as lieutenant governor under Phips through the intervention of Mather's own politically powerful father, Increase. "Intercession had been made by Cotton Mather for the advancement of Stoughton, a man of cold affections, proud, self-willed and covetous of distinction." Apparently Mather saw in Stoughton, a bachelor who had never wed, an ally for church-related matters. Bancroft quotes Mather's reaction to Stoughton's appointment as follows:
Mather claimed not to have attended the trials in Salem (although his father attended the trial of George Burroughs). Two contemporaries, Calef and Thomas Brattle, place him at the executions (see below). Mather began to publicize and celebrate the trials well before they were put to an end: "If in the midst of the many Dissatisfaction among us, the publication of these Trials may promote such a pious Thankfulness unto God, for Justice being so far executed among us, I shall Re-joyce that God is Glorified..." Mather called himself a historian not an advocate, but his writing largely presumes the guilt of the accused and includes such venomous comments as calling Martha Carrier "a rampant hag". Mather referred to George Burroughs—a Harvard alumnus, survivor of Indian attacks in Maine, and unordained minister hanged the same day as Martha Carrier, John Proctor, George Jacobs and John Willard—as a "very puny man" whose "tergiversations, contradictions, and falsehoods" made his testimony not "worth considering".
Use of spectral evidence.
Mather's most fatal influence over the trials was in composing the answer to the question of whether or not to allow spectral evidence, that is, allowing the afflicted girls to claim that some invisible ghost of the defendant was tormenting them, and for this to be considered evidence of witchcraft by the defendant, even if the defendant denied it and professed their own strongly held Christian beliefs. An opinion on the matter was sought from the most esteemed ministers of the area and Mather took credit for their response when anonymously celebrating himself years later: "drawn up at their desire, by Cotton Mather the younger, as I have been informed."
On May 31, 1692, Mather wrote to one of the judges, John Richards, a member of his congregation, expressing his support of the prosecutions, but cautioning; "do not lay more stress on pure spectral evidence than it will bear ... It is very certain that the Devils have sometimes represented the Shapes of persons not only innocent, but also very virtuous. Though I believe that the just God then ordinarily provides a way for the speedy vindication of the persons thus abused."
Hutchinson sums the letter, "The two first and the last sections of this advice took away the force of all the others, and the prosecutions went on with more vigor than before." (Reprinting the letter years later in "Magnalia", Mather notably omitted the fateful "two first and the last" sections.) The original full version of the letter, called "Return of the Several Ministers", was dated June 15, 1692, and had been reprinted in late 1692 in the final two pages of Increase Mather's "Cases of Conscience". It is a curious document and remains a source of confusion and argument. Calef calls it "perfectly Ambidexter, giving as great as greater Encouragement to proceed in those dark methods, then cautions against them... indeed the Advice then given, looks most like a thing of his Composing, as carrying both Fire to increase and Water to quench the Conflagration."
Regarding spectral evidence, Upham concludes that "Cotton Mather never in any public writing 'denounced the admission' of it, never advised its absolute exclusion; but on the contrary recognized it as a ground of 'presumption' ... once admitted nothing could stand against it. Character, reason, common sense, were swept away." The later rejection of spectral evidence by Governor Phips and its exclusion from trial beginning in January 1693 immediately brought about far fewer convictions. Due to a reprieve by Phips, there were no more executions. 
Bancroft notes that Mather considered witches "among the poor, and vile, and ragged beggars upon Earth", and Bancroft asserts that Mather considered the people against the witch trials to be witch advocates. Calef places Mather at the scene of the execution of George Burroughs (and four others who were executed after Mather spoke) and shows him playing a direct and influential role:
Post-trial.
In the years after the trials, of the principal actors in the trial, whose lives are recorded after, neither he nor Stoughton ever admitted to any misgivings. In the years after the trials, he became an increasingly vehement defender of them. At the request of then Lt.-Gov. Stoughton, Mather wrote "Wonders of the Invisible World" during the trials, which were published in 1693. The book contained a few of Mather's sermons, the conditions of the colony and a description of witch trials in Europe. He somewhat clarified the contradictory advice he had given in "Return of the Several Ministers", by defending the use of spectral evidence. "Wonders of the Invisible World" appeared at the same time as Increase Mather's "Cases of Conscience.""
Mather did not sign his name or support his father's book initially:
The last event in Mather's involvement with witchcraft was his attempt to cure Mercy Short and Margaret Rule. Boston merchant Robert Calef began his eight-year campaign against the influential Mathers. Calef's book was inspired by the fear that Mather would succeed in once again stirring up new witchcraft trials, and the need to bear witness to the horrible experiences of 1692. He quotes the public apologies of the men on the jury and one of the judges. Upon reading Calef's "More Wonders of the Invisible World", Increase Mather, Cotton's father, publicly burned the book in Harvard Yard.
Poole vs. Upham.
In 1869, William Frederick Poole quoted from various school textbooks of the time demonstrating they were in agreement on Cotton Mather's role in the Witch Trials:
Poole was a librarian, and a lover of literature, including Mather's "Magnalia" "and other books and tracts, numbering nearly 400 were never so prized by collectors as today." Poole announced his intention to redeem Mather's name, using as a springboard a harsh critique of a recently published tome by Charles Wentworth Upham, "Salem Witchcraft Volumes I and II With an Account of Salem Village and a History of Opinions on Witchcraft and Kindred Subjects", which runs to almost 1,000 pages, and a quick search of the name Mather (referring to either father, son, or ancestors) shows that it occurs 96 times. Poole's critique, in book form, runs less than 70 pages but the name "Mather" occurs many more times than the other book, which is more than ten times as long. Upham shows a balanced and complicated view of Cotton Mather, such as this first mention: "One of Cotton Mather's most characteristic productions is the tribute to his venerated master. It flows from a heart warm with gratitude." Upham's book refers to Robert Calef no fewer than 25 times with the majority of these regarding documents compiled by Calef in the mid-1690s and stating: "Although zealously devoted to the work of exposing the enormities connected with the witchcraft prosecutions, there is no ground to dispute the veracity of Calef as to matters of fact." He goes on to say that Calef's collection of writings "gave a shock to Mather's influence, from which it never recovered."
Calef produced only the one book; he is self-effacing and apologetic for his limitations, and on the title page he is listed not as author but "collector". Poole, champion of literature, cannot accept Calef whose "faculties, as indicated by his writings appear to us to have been of an inferior order;..." and his book "in our opinon, has a reputation much beyond its merits." Poole refers to Calef as Mather's "personal enemy" and opens a line, "Without discussing the character and motives of Calef..." but does not follow up on this suggestive comment to discuss any actual or purported motive or reason to impune Calef. Upham responded to Poole in a book running five times as long and sharing the same title (referring to Poole as "the Reviewer.") Many of Poole's arguments were addressed, but both authors emphasize the importance of Cotton Mather's difficult and contradictory view on spectral evidence, as copied in the final pages, called "The Return of Several Ministers", of Increase Mather's "Cases of Conscience".
In 1914, historian George Lincoln Burr sided with Upham in a note on Thomas Brattle's letter, "The strange suggestion of W. F. Poole that Brattle here means Cotton Mather himself, is adequately answered by Upham..." Burr reprinted Calef in full and dug deep into the historical record for information on the man and concludes "...that he had else any grievance against the Mathers or their colleagues there is no reason to think." Burr finds that a comparison between Calef's work and original documents in the historical record collections "testify to the care and exactness..."
Ongoing Revision.
Chadwick Hansen's "Witchcraft at Salem", published in 1969, defined Mather as a positive influence on the Salem Trials. Hansen considered Mather's handling of the Goodwin children to be sane and temperate. Hansen posited that Mather was more concerned with helping the affected children than witch-hunting. Mather treated the affected children through prayer and fasting. In 1688, Mather tried to convert Goodwife Glover, a Catholic, before she was executed, accused of practicing witchcraft on the Goodwin children. Hansen claimed Mather acted as a moderating influence in the trials by opposing the death penalty for those who confessed — or feigned confession — such as Tituba and Dorcas Good. Hansen claims that most negative impressions of Mather stem from his defense of the ongoing trials in "Wonders of the Invisible World". After some others had lamented the roles they played in the executions and imprisonments, Mather and Stoughton remained the chief defenders of the trials, which diminishes the view of him as a moderate influence.
Historian Dr. Larry Gragg highlights Mather's cloudy thinking and confusion between sympathy for the possessed, and the boundlessness of spectral evidence when Mather stated, "the devil have sometimes represented the shapes of persons not only innocent, but also the very virtuous."
Writing in the early 1980s, historian John Demos imputed to Mather a purportedly moderating influence on the trials.
Smallpox Inoculation Controversy.
The practice of smallpox inoculation (as opposed to the later practice of vaccination) was developed possibly in 8th-century India or 10th-century China. Spreading its reach in seventeenth-century Turkey, inoculation or, rather, variolation, involved infecting a person via a cut in the skin with exudate from a patient with a relatively mild case of smallpox (variola), in order to bring about a manageable and recoverable infection that would provide later immunity. By the beginning of the 18th century, the Royal Society in England was discussing the practice of inoculation, and the smallpox epidemic in 1713 spurred further interest. It was not until 1721, however, that England recorded its first case of inoculation. 
Early New England.
Smallpox was a serious threat in colonial America, most devastating to Native Americans, but also to Anglo-American settlers. New England suffered smallpox epidemics in 1677, 1689–90, and 1702. It was highly contagious, and mortality could reach as high as 30 percent. Boston had been plagued by smallpox outbreaks in 1690 and 1702. During this era, public authorities in Massachusetts dealt with the threat primarily by means of quarantine. Incoming ships were quarantined in Boston harbor, and any smallpox patients in town were held under guard or in a "pesthouse".
In 1706, Mather's slave, Onesimus, explained to Mather how he had been inoculated as a child in Africa. Mather was fascinated by the idea. By July 1716, he had read an endorsement of inoculation by Dr Emanuel Timonius of Constantinople in the "Philosophical Transactions". Mather then declared, in a letter to Dr John Woodward of Gresham College in London, that he planned to press Boston's doctors to adopt the practice of inoculation should smallpox reach the colony again.
By 1721, a whole generation of young Bostonians was vulnerable and memories of the last epidemic's horrors had by and large disappeared. On April 22 of that year, the HMS "Seahorse" arrived from the West Indies carrying smallpox on board. Despite attempts to protect the town through quarantine, eight known cases of smallpox appeared in Boston by May 27, and by mid-June, the disease was spreading at an alarming rate. As a new wave of smallpox hit the area and continued to spread, many residents fled to outlying rural settlements. The combination of exodus, quarantine, and outside traders' fears disrupted business in the capital of the Bay Colony for weeks. Guards were stationed at the House of Representatives to keep Bostonians from entering without special permission. The death toll reached 101 in September, and the Selectmen, powerless to stop it, "severely limited the length of time funeral bells could toll." As one response, legislators delegated a thousand pounds from the treasury to help the people who, under these conditions, could no longer support their families.
On June 6, 1721, Mather sent an abstract of reports on inoculation by Timonius and Jacobus Pylarinus to local physicians, urging them to consult about the matter. He received no response. Next, Mather pleaded his case to Dr. Zabdiel Boylston, who tried the procedure on his only son and two slaves—one grown and one a boy. All recovered in about a week. Boylston inoculated seven more people by mid-July. The epidemic peaked in October 1721, with 411 deaths; by February 26, 1722, Boston was again free from smallpox. The total number of cases since April 1721 came to 5,889, with 844 deaths—more than three-quarters of all the deaths in Boston during 1721. Meanwhile, Boylston had inoculated 287 people, with six resulting deaths.
Inoculation debate.
Boylston and Mather's inoculation crusade "raised a horrid Clamour" among the people of Boston. Both Boylston and Mather were "Object of their Fury; their furious Obloquies and Invectives", which Mather acknowledges in his diary. Boston's Selectmen, consulting a doctor who claimed that the practice caused many deaths and only spread the infection, forbade Boylston from performing it again.
"The New-England Courant" published writers who opposed the practice. The editorial stance was that the Boston populace feared that inoculation spread, rather than prevented, the disease; however, some historians, notably H. W. Brands, have argued that this position was a result of the contrarian positions of editor-in-chief James Franklin (a brother of Benjamin Franklin).
Public discourse ranged in tone from organized arguments by Reverend John Williams from Boston, who posted that "several arguments proving that inoculating the smallpox is not contained in the law of Physick, either natural or divine, and therefore unlawful", to those put forth in a pamphlet by Dr. William Douglass of Boston, entitled "The Abuses and Scandals of Some Late Pamphlets in Favour of Inoculation of the Small Pox" (1721), on the qualifications of inoculation's proponents. (Douglass was exceptional at the time for holding a medical degree from Europe.) At the extreme, in November 1721, someone hurled a lighted grenade into Mather's home.
Medical opposition.
Several opponents of smallpox inoculation, among them Rev. John Williams, stated that there were only two laws of physick (medicine): sympathy and antipathy. In his estimation, inoculation was neither a sympathy toward a wound or a disease, or an antipathy toward one, but the creation of one. For this reason, its practice violated the natural laws of medicine, transforming health care practitioners into those who harm rather than heal.
As with many colonists, Williams' Puritan beliefs were enmeshed in every aspect of his life, and he used the Bible to state his case. He quoted , when Jesus said: "It is not the healthy who need a doctor, but the sick." Dr. William Douglass proposed a more secular argument against inoculation, stressing the importance of reason over passion and urging the public to be pragmatic in their choices. In addition, he demanded that ministers leave the practice of medicine to physicians, and not meddle in areas where they lacked expertise. According to Douglass, smallpox inoculation was "a medical experiment of consequence," one not to be undertaken lightly. He believed that not all learned individuals were qualified to doctor others, and while ministers took on several roles in the early years of the colony, including that of caring for the sick, they were now expected to stay out of state and civil affairs. Douglass felt that inoculation caused more deaths than it prevented. The only reason Mather had had success in it, he said, was because Mather had used it on children, who are naturally more resilient. Douglass vowed to always speak out against "the wickedness of spreading infection". Speak out he did: "The battle between these two prestigious adversaries and Mather lasted far longer than the epidemic itself, and the literature accompanying the controversy was both vast and venomous."
Puritan resistance.
Generally, Puritan pastors favored the inoculation experiments. Increase Mather, Cotton's father, was joined by prominent pastors Benjamin Colman and William Cooper in openly propagating the use of inoculations. "One of the classic assumptions of the Puritan mind was that the will of God was to be discerned in nature as well as in revelation." Nevertheless, Williams questioned whether the smallpox "is not one of the strange works of God; and whether inoculation of it be not a fighting with the most High." He also asked his readers if the smallpox epidemic may have been given to them by God as "punishment for sin," and warned that attempting to shield themselves from God's fury (via inoculation), would only serve to "provoke him more".
Puritans found meaning in affliction, and they did not yet know why God was showing them disfavor through smallpox. Not to address their errant ways before attempting a cure could set them back in their "errand". Many Puritans believed that creating a wound and inserting poison was doing violence and therefore was antithetical to the healing art. They grappled with adhering to the Ten Commandments, with being proper church members and good caring neighbors. The apparent contradiction between harming or murdering a neighbor through inoculation and the Sixth Commandment – "thou shalt not kill" — seemed insoluble and hence stood as one of the main objections against the procedure. Williams maintained that because the subject of inoculation could not be found in the Bible, it was not the will of God, and therefore "unlawful." He explained that inoculation violated The Golden Rule, because if one neighbor voluntarily infected another with disease, he was not doing unto others as he would have done to him. With the Bible as the Puritans' source for all decision-making, lack of scriptural evidence concerned many, and Williams vocally scorned Mather for not being able to reference an inoculation edict directly from the Bible.
Inoculation defended.
With the smallpox epidemic catching speed and racking up a staggering death toll, a solution to the crisis was becoming more urgently needed by the day. The use of quarantine and various other efforts, such as balancing the body's humors, did not slow the spread of the disease. As news rolled in from town to town and correspondence arrived from overseas, reports of horrific stories of suffering and loss due to smallpox stirred mass panic among the people. "By circa 1700, smallpox had become among the most devastating of epidemic diseases circulating in the Atlantic world."
Mather strongly challenged the perception that inoculation was against the will of God and argued that the procedure was not outside of Puritan principles. He wrote that "whether a Christian may not employ this Medicine (let the matter of it be what it will) and humbly give Thanks to God's good Providence in discovering of it to a miserable World; and humbly look up to His Good Providence (as we do in the use of any other Medicine) It may seem strange, that any wise Christian cannot answer it. And how strangely do Men that call themselves Physicians betray their Anatomy, and their Philosophy, as well as their Divinity in their invectives against this Practice?" The Puritan minister began to embrace the sentiment that smallpox was an inevitability for anyone, both the good and the wicked, yet God had provided them with the means to save themselves. Mather reported that, from his view, "none that have used it ever died of the Small Pox, tho at the same time, it were so malignant, that at least half the People died, that were infected With it in the Common way."
While Mather was experimenting with the procedure, prominent Puritan pastors Benjamin Colman and William Cooper expressed public and theological support for them. The practice of smallpox inoculation was eventually accepted by the general population due to first-hand experiences and personal relationships. Although many were initially wary of the concept, it was because people were able to witness the procedure's consistently positive results, within their own community of ordinary citizens, that it became widely utilized and supported. One important change in the practice after 1721 was regulated quarantine of innoculees.
The aftermath.
Although Mather and Boylston were able to demonstrate the efficacy of the practice, the debate over inoculation would continue even beyond the epidemic of 1721–22. After overcoming considerable difficulty and achieving notable success, Boylston traveled to London in 1725 where he published his results and was elected to the Royal Society in 1726. 
Works.
"Boston Ephemeris".
The Boston Ephemeris was an almanac written by Mather in 1686. The content was similar to what is known today as the "Farmer's Almanac". This was particularly important because it shows that Cotton Mather had influence in mathematics during the time of Puritan New England. This almanac contained a significant amount of astronomy, celestial within the text of the almanac the positions and motions of these celestial bodies, which he must have calculated by hand.
"The Biblia Americana".
When Mather died, he left behind an abundance of unfinished writings, including one entitled "The Biblia Americana". Mather believed that "Biblia Americana" was the best thing he had ever written; his masterwork. "Biblia Americana" contained Mather's thoughts and opinions on the Bible and how he interpreted it. "Biblia Americana" is incredibly large, and Mather worked on it from 1693 until 1728, when he died. Mather tried to convince others that philosophy and science could work together with religion instead of against it. People did not have to choose one or the other. In "Biblia Americana", Mather looked at the Bible through a scientific perspective, completely opposite to his perspective in "The Christian Philosopher", in which he approached science in a religious manner.
"Pillars of Salt".
Mather's first published sermon, printed in 1686, concerned the execution of James Morgan, convicted of murder. Thirteen years later, Mather published the sermon in a compilation, along with other similar works, called "Pillars of Salt".
"Magnalia Christi Americana".
"Magnalia Christi Americana", considered Mather's greatest work, was published in 1702, when he was 39. The book includes several biographies of saints and describes the process of the New England settlement. In this context "saints" does not refer to the canonized saints of the Catholic church, but to those Puritan divines about whom Mather is writing. It comprises seven total books, including "Pietas in Patriam: The life of His Excellency Sir William Phips", originally published anonymously in London in 1697. Despite being one of Mather's best-known works, many have openly criticized it, labeling it as hard to follow and understand, and poorly paced and organized. However, other critics have praised Mather's work, citing it as one of the best efforts at properly documenting the establishment of America and growth of the people.
"The Christian Philosopher".
In 1721, Mather published "The Christian Philosopher", the first systematic book on science published in America. Mather attempted to show how Newtonian science and religion were in harmony. It was in part based on Robert Boyle's "The Christian Virtuoso" (1690). Mather reportedly took inspiration from "Hayy ibn Yaqdhan", by the 12th-century Islamic philosopher Abu Bakr Ibn Tufail.
Despite condemning the "Mahometans" as infidels, Mather viewed the novel's protagonist, Hayy, as a model for his ideal Christian philosopher and monotheistic scientist. Mather viewed Hayy as a noble savage and applied this in the context of attempting to understand the Native American Indians, in order to convert them to Puritan Christianity. Mather's short treatise on the Lord's Supper was later translated by his nephew Josiah Cotton in the Massachusett.
In popular culture.
Cotton Mather is played by Seth Gabel (as "Reverend Cotton Mather") in the 2014 TV series "Salem".

</doc>
<doc id="7105" url="https://en.wikipedia.org/wiki?curid=7105" title="Cordwainer Smith">
Cordwainer Smith

Cordwainer Smith (, ) was the pen-name used by American author Paul Myron Anthony Linebarger (July 11, 1913 – August 6, 1966) for his science fiction works. Linebarger was a noted East Asia scholar and expert in psychological warfare. ("Cordwainer" is an archaic word for "a worker in cordwain or cordovan leather; a shoemaker", and a "smith" is "one who works in iron or other metals; esp. a blacksmith or farrier": two kinds of skilled workers with traditional materials.)
Linebarger also employed the literary pseudonyms "Carmichael Smith" (for his political thriller "Atomsk"), "Anthony Bearden" (for his poetry) and "Felix C. Forrest" (for the novels "Ria" and "Carola"). 
He died of a heart attack in 1966 at Johns Hopkins University Medical Center in Baltimore, Maryland, at age 53.
Early life and education.
Linebarger was born in Milwaukee, Wisconsin. His father was Paul M. W. Linebarger, a lawyer and political activist with close ties to the leaders of the Chinese revolution of 1911. As a result of those connections, Linebarger's godfather was Sun Yat-sen, considered the father of Chinese nationalism.
As a child, Linebarger was blinded in his right eye; the vision in his remaining eye was impaired by infection. His father moved his family to France and then Germany while Sun Yat-sen was struggling against contentious warlords in China. As a result, Linebarger was familiar with six languages by adulthood.
At the age of 23, he received a PhD in Political Science from Johns Hopkins University.
Career.
From 1937 to 1946, Linebarger held a faculty appointment at Duke University, where he began producing highly regarded works on Far Eastern affairs.
While retaining his professorship at Duke after the beginning of World War II, Linebarger began serving as a second lieutenant of the United States Army, where he was involved in the creation of the Office of War Information and the Operation Planning and Intelligence Board. He also helped organize the Army's first psychological warfare section. In 1943, he was sent to China to coordinate military intelligence operations. When he later pursued his interest in China, Linebarger became a close confidant of Chiang Kai-shek. By the end of the war, he had risen to the rank of major.
In 1947, Linebarger moved to the Johns Hopkins University's School of Advanced International Studies in Washington, DC, where he served as Professor of Asiatic Studies. He used his experiences in the war to write the book "Psychological Warfare" (1948), regarded by many in the field as a classic text.
He eventually rose to the rank of colonel in the reserves. He was recalled to advise the British forces in the Malayan Emergency and the U.S. Eighth Army in the Korean War. While he was known to call himself a "visitor to small wars", he refrained from becoming involved in Vietnam, but is known to have done undocumented work for the Central Intelligence Agency. He traveled extensively and became a member of the Foreign Policy Association, and was called upon to advise President John F. Kennedy.
Marriage and family.
In 1936, Linebarger married Margaret Snow. They had a daughter in 1942 and another in 1947. They divorced in 1949.
In 1950, Linebarger married again to Genevieve Collins; they had several children. They were married until his death from a heart attack in 1966, in Baltimore, Maryland. Linebarger had expressed a wish to retire to Australia, which he had visited in his travels, but died at age 53.
Colonel Linebarger is buried in Arlington National Cemetery, Section 35, Grave Number 4712. His widow, Genevieve Collins Linebarger, was interred with him on November 16, 1981.
Case history debate.
Linebarger was long rumored to have been the original for "Kirk Allen," the fantasy-haunted subject of "The Jet-Propelled Couch," a chapter in psychologist Robert M. Lindner's best-selling 1954 collection "The Fifty-Minute Hour." According to Cordwainer Smith scholar Alan C. Elms, this speculation first reached print in Brian Aldiss's 1973 history of science fiction, "Billion Year Spree"; Aldiss, in turn, claimed to have gotten the information from Leon Stover. More recently, both Elms and librarian Lee Weinstein have gathered circumstantial evidence to support the case for Linebarger's being Allen, but both concede there is no direct proof that Linebarger was ever a patient of Lindner's or that he suffered from a disorder similar to that of Kirk Allen.
Science fiction style.
A notable characteristic of Linebarger's science fiction is that most of his stories are set in the same fictional universe, with a unified chronology. Some anthologies of Linebarger's fiction include a chart, with each of his stories inserted into the appropriate slot in the timeline. All his writings suggest a rich universe developing over a long period of time, but leave much to be guessed at by the reader.
Linebarger's stories are unusual, sometimes being written in narrative styles closer to traditional Chinese stories than to most English-language fiction, as well as reminiscent of the Genji tales of Lady Murasaki. The total volume of his science fiction output is relatively small, because of his time-consuming profession and his early death.
Smith's works consist of: a single novel, originally published in two volumes in edited form as "The Planet Buyer", also known as "The Boy Who Bought Old Earth" (1964) and "The Underpeople" (1968), and later restored to its original form as "Norstrilia" (1975); and 32 short stories (collected in "The Rediscovery of Man" (1993), including two versions of the short story "War No. 81-Q").
Linebarger's cultural links to China are partially expressed in the pseudonym "Felix C. Forrest", which he used in addition to "Cordwainer Smith": his godfather Sun Yat-Sen suggested to Linebarger that he adopt the Chinese name "Lin Bai-lo" (), which may be roughly translated as "Forest of Incandescent Bliss". ("Felix" is Latin for "happy".) In his later years, Linebarger proudly wore a tie with the Chinese characters for this name embroidered on it.
As an expert in psychological warfare, Linebarger was very interested in the newly developing fields of psychology and psychiatry. He used many of their concepts in his fiction. His fiction often has religious overtones or motifs, particularly evident in characters who have no control over their actions. James P. Jordan argued for the importance of Anglicanism to Linebarger's works back to 1949. But Linebarger's daughter Rosana Hart has indicated that he did not become an Anglican until 1950, and was not strongly interested in religion until later still. The introduction to the collection "Rediscovery of Man" notes that from around 1960 Linebarger became more devout and expressed this in his writing. Linebarger's works are sometimes included in analyses of Christianity in fiction, along with the works of authors such as C. S. Lewis and J.R.R. Tolkien.
Most of Smith's stories are set in an era starting some 14,000 years in the future. The Instrumentality of Mankind rules Earth and goes on to control other planets later inhabited by humanity. The Instrumentality attempts to revive old cultures and languages in a process known as the Rediscovery of Man. This rediscovery can be seen as the initial period when humankind emerges from a mundane utopia and the nonhuman Underpeople gain freedom from slavery. It may also be viewed as part of a continuing process begun by the Instrumentality, encompassing the whole cycle, where mankind is constantly at risk of falling back into bad old ways.
For years, Cordwainer Smith had a pocket notebook which he had filled with ideas about The Instrumentality and additional stories in the series. But while in a small boat in a lake or bay in the mid 60s, he leaned over the side, and his notebook fell out of his breast pocket into the water, where it was lost forever. Another story claims that he accidentally left the notebook in a restaurant in Rhodes in 1965. With the book gone, he felt empty of ideas, and decided to start a new series which was an allegory of Mid-Eastern politics.
Smith's stories describe a long future history of Earth. The settings range from a postapocalyptic landscape with walled cities, defended by agents of the Instrumentality, to a state of sterile utopia, in which freedom can be found only deep below the surface, in long-forgotten and buried anthropogenic strata. These features may place Smith's works within the Dying Earth subgenre of science fiction. They are ultimately more optimistic and distinctive.
Smith's most celebrated short story is his first-published, "Scanners Live in Vain", which led many of its earliest readers to assume that "Cordwainer Smith" was a new pen name for one of the established giants of the genre. It was selected as one of the best science fiction short stories of the pre-Nebula Award period by the Science Fiction and Fantasy Writers of America. It was selected for "The Science Fiction Hall of Fame Volume One, 1929-1964".
Linebarger's stories feature strange and vivid creations, such as:
Published fiction.
Short stories.
Titles marked with an asterisk * are independent stories not related to the Instrumentality universe.

</doc>
<doc id="7110" url="https://en.wikipedia.org/wiki?curid=7110" title="CSS (disambiguation)">
CSS (disambiguation)

CSS is Cascading Style Sheets, a language used to describe the style of document presentations in web development.
CSS may also refer to:

</doc>
<doc id="7112" url="https://en.wikipedia.org/wiki?curid=7112" title="Colorado Front Range">
Colorado Front Range

The Colorado Front Range is a colloquial geographic term for the most populous region of the state of Colorado in the United States. The area is located just east of the foothills of the Front Range, aligned in a north-south configuration on the western edge of the Great Plains, where they meet the Rockies. Geologically, the region lies mostly within the Colorado Piedmont, in the valley of the South Platte and Arkansas rivers on the east side of the Rockies. The region contains the largest cities and the majority of the population of Colorado.
The Colorado Front Range communities include (in a roughly north-to-south order):
The addition of the adjacent Wyoming city of Cheyenne to the Colorado Front Range forms the full Front Range Urban Corridor.

</doc>
<doc id="7118" url="https://en.wikipedia.org/wiki?curid=7118" title="Churnsike Lodge">
Churnsike Lodge

Churnsike Lodge () was an early Victorian hunting lodge situated in the parish of Greystead, West Northumberland, England. It was built in 1850 as a shooting lodge and was part of the Hesleyside estate (Hesleyside house is situated in the North Tyne valley near Bellingham). When the estate was sold in 1889, Churnsike Lodge was purchased by the Chesters Estate (near Hexham, Northumberland). The "cairnsyke" estate comprised several thousand acres of grouse moor and is referred to in the sale catalogue of 1889 as the "Finest grouse moor in the Kingdom". The property included stables for 6 horses, a gamekeepers bothy and well-appointed dog kennels which housed the Irthing head and Kielder hounds (headed by famous fox hunter William Dodd, as referred to in "Wanny Blossoms"). Situated 10 miles north of Gilsland and 13 miles west of Bellingham, the former grouse moor is now part of the Wark forest.

</doc>
