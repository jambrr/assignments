<doc id="8724" url="https://en.wikipedia.org/wiki?curid=8724" title="Doppler effect">
Doppler effect

The Doppler effect (or the Doppler shift) is the change in frequency of a wave (or other periodic event) for an observer moving relative to its source. It is named after the Austrian physicist Christian Doppler, who proposed it in 1842 in Prague. It is commonly heard when a vehicle sounding a siren or horn approaches, passes, and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.
When the source of the waves is moving toward the observer, each successive wave crest is emitted from a position closer to the observer than the previous wave. Therefore, each wave takes slightly less time to reach the observer than the previous wave. Hence, the time between the arrival of successive wave crests at the observer is reduced, causing an increase in the frequency. While they are travelling, the distance between successive wave fronts is reduced, so the waves "bunch together". Conversely, if the source of waves is moving away from the observer, each wave is emitted from a position farther from the observer than the previous wave, so the arrival time between successive waves is increased, reducing the frequency. The distance between successive wave fronts is then increased, so the waves "spread out".
For waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect may therefore result from motion of the source, motion of the observer, or motion of the medium. Each of these effects is analyzed separately. For waves which do not require a medium, such as light or gravity in general relativity, only the relative difference in velocity between the observer and the source needs to be considered.
Developments.
Doppler first proposed this effect in 1842 in his treatise ""Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels"" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called "effet Doppler-Fizeau" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).
General.
In classical physics, where the speeds of source and the receiver relative to the medium are lower than the velocity of waves in the medium, the relationship between observed frequency formula_1 and emitted frequency formula_2 is given by:
The frequency is decreased if either is moving away from the other.
The above formula assumes that the source is either directly approaching or receding from the observer. If the source approaches the observer at an angle (but still with a constant velocity), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach; but when the wave is received, the source and observer will no longer be at their closest), and a continued monotonic decrease as it recedes from the observer. When the observer is very close to the path of the object, the transition from high to low frequency is very abrupt. When the observer is far from the path of the object, the transition from high to low frequency is gradual.
If the speeds formula_6 and formula_5 are small compared to the speed of the wave, the relationship between observed frequency formula_1 and emitted frequency formula_2 is approximately
Given formula_3
we divide for formula_14
formula_15
Since formula_16 we can substitute the geometric expansion:
formula_17
Analysis.
To understand what happens, consider the following analogy. Someone throws one ball every second at a man. Assume that balls travel with constant velocity. If the thrower is stationary, the man will receive one ball every second. However, if the thrower is moving towards the man, he will receive balls more frequently because the balls will be less spaced out. The inverse is true if the thrower is moving away from the man. So it is actually the "wavelength" which is affected; as a consequence, the received frequency is also affected. It may also be said that the velocity of the wave remains constant whereas wavelength changes; hence frequency also changes.
With an observer stationary relative to the medium, if a moving source is emitting waves with an actual frequency formula_2 (in this case, the wavelength is changed, the transmission velocity of the wave keeps constant formula_19 note that the "transmission velocity" of the wave does not depend on the "velocity of the source"), then the observer detects waves with a frequency formula_1 given by
A similar analysis for a moving "observer" and a stationary source (in this case, the wavelength keeps constant, but due to the motion, the rate at which the observer receives waves formula_19 and hence the "transmission velocity" of the wave respect to the observer formula_19 is changed) yields the observed frequency:
These can be generalized into the equation that was presented in the previous section.
An interesting effect was predicted by Lord Rayleigh in his classic book on sound: if the source is moving at twice the speed of sound, a musical piece emitted by that source would be heard in correct time and tune, but "backwards". The Doppler effect with sound is only clearly heard with objects moving at high speed, as change in frequency of musical tone involves a speed of around 40 meters per second, and smaller changes in frequency can easily be confused by changes in the amplitude of the sounds from moving emitters. Neil A Downie has demonstrated how the Doppler effect can be made much more easily audible by using an ultrasonic (e.g. 40 kHz) emitter on the moving object. The observer then uses a heterodyne frequency converter, as used in many bat detectors, to listen to a band around 40 kHz. In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second.
Application.
Sirens.
The siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus:
In other words, if the siren approached the observer directly, the pitch would remain constant until the vehicle hit him, and then immediately jump to a new lower pitch. Because the vehicle passes by the observer, the radial velocity does not remain constant, but instead varies as a function of the angle between his line of sight and the siren's velocity:
where formula_27 is the angle between the object's forward velocity and the line of sight from the object to the observer.
Astronomy.
The Doppler effect for electromagnetic waves such as light is of great use in astronomy and results in either a so-called redshift or blueshift. It has been used to measure the speed at which stars and galaxies are approaching or receding from us; that is, their radial velocities. This may be used to detect if an apparently single star is, in reality, a close binary, to measure the rotational speed of stars and galaxies, or to detect exoplanets. (Note that redshift is also used to measure the expansion of space, but that this is not truly a Doppler effect.)
The use of the Doppler effect for light in astronomy depends on our knowledge that the spectra of stars are not homogeneous. They exhibit absorption lines at well defined frequencies that are correlated with the energies required to excite electrons in various elements from one level to another. The Doppler effect is recognizable in the fact that the absorption lines are not always at the frequencies that are obtained from the spectrum of a stationary light source. Since blue light has a higher frequency than red light, the spectral lines of an approaching astronomical light source exhibit a blueshift and those of a receding astronomical light source exhibit a redshift.
Among the nearby stars, the largest radial velocities with respect to the Sun are +308 km/s (BD-15°4041, also known as LHS 52, 81.7 light-years away) and -260 km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial velocity means the star is receding from the Sun, negative that it is approaching.
Radar.
The Doppler effect is used in some types of radar, to measure the velocity of detected objects. A radar beam is fired at a moving target — e.g. a motor car, as police use radar to detect speeding motorists — as it approaches or recedes from the radar source. Each successive radar wave has to travel farther to reach the car, before being reflected and re-detected near the source. As each wave has to move farther, the gap between each wave increases, increasing the wavelength. In some situations, the radar beam is fired at the moving car as it approaches, in which case each successive wave travels a lesser distance, decreasing the wavelength. In either situation, calculations from the Doppler effect accurately determine the car's velocity. Moreover, the proximity fuze, developed during World War II, relies upon Doppler radar to detonate explosives at the correct time, height, distance, etc.
Because the doppler shift affects the wave incident upon the target as well as the wave reflected back to the radar, the change in frequency observed by a radar due to a target moving at relative velocity formula_28 is twice that from the same target emitting a wave:
Medical imaging and blood flow measurement.
An echocardiogram can, within certain limits, produce an accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, any abnormal communications between the left and right side of the heart, any leaking of blood through the valves (valvular regurgitation), and calculation of the cardiac output. Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.
Although "Doppler" has become synonymous with "velocity measurement" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift ("when" the received signal arrives).
Velocity measurements of blood flow are also used in other fields of medical ultrasonography, such as obstetric ultrasonography and neurology. Velocity measurement of blood flow in arteries and veins based on Doppler effect is an effective tool for diagnosis of vascular problems like stenosis.
Flow measurement.
Instruments such as the laser Doppler velocimeter (LDV), and acoustic Doppler velocimeter (ADV) have been developed to measure velocities in a fluid flow. The LDV emits a light beam and the ADV emits an ultrasonic acoustic burst, and measure the Doppler shift in wavelengths of reflections from particles moving with the flow. The actual flow is computed as a function of the water velocity and phase. This technique allows non-intrusive flow measurements, at high precision and high frequency.
Velocity profile measurement.
Developed originally for velocity measurements in medical applications (blood flow), Ultrasonic Doppler Velocimetry (UDV) can measure in real time complete velocity profile in almost any liquids containing particles in suspension such as dust, gas bubbles, emulsions. Flows can be pulsating, oscillating, laminar or turbulent, stationary or transient. This technique is fully non-invasive.
Satellite communication.
Fast moving satellites can have a Doppler shift of dozens of kilohertz relative to a ground station. The speed, thus magnitude of Doppler effect, changes due to earth curvature. Dynamic Doppler compensation, where the frequency of a signal is changed multiple times during transmission, is used so the satellite receives a constant frequency signal.
Audio.
The Leslie speaker, most commonly associated with and predominantly used with the famous Hammond organ, takes advantage of the Doppler effect by using an electric motor to rotate an acoustic horn around a loudspeaker, sending its sound in a circle. This results at the listener's ear in rapidly fluctuating frequencies of a keyboard note.
Vibration measurement.
A laser Doppler vibrometer (LDV) is a non-contact method for measuring vibration. The laser beam from the LDV is directed at the surface of interest, and the vibration amplitude and frequency are extracted from the Doppler shift of the laser beam frequency due to the motion of the surface.
Developmental biology.
During the segmentation of vertebrate embroys, waves of gene expression sweep across the presomitic mesoderm, the tissue from which the precursors of the vertebrae (somites) are formed. A new somite is formed upon arrival of a wave at the anterior end of the presomitic mesoderm. In zebrafish, it has been shown that the shortening of the presomitic mesoderm during segmentation leads to a Doppler effect as the anterior end of the tissue moves into the waves. This Doppler effect contributes to the period of segmentation.
Inverse Doppler effect.
Since 1968 scientists such as Victor Veselago have speculated about the possibility of an inverse Doppler effect. The experiment that claimed to have detected this effect was conducted by Nigel Seddon and Trevor Bearpark in Bristol, United Kingdom in 2003.
Researchers from many Universities like Swinburne University of Technology and the University of Shanghai for Science and Technology showed that this effect can be observed in optical frequencies as well. This was made possible by growing a photonic crystal and projecting a laser beam into the crystal. This made the crystal act like a super prism and the inverse Doppler effect could be observed.

</doc>
<doc id="8727" url="https://en.wikipedia.org/wiki?curid=8727" title="ΔT">
ΔT

In precise timekeeping, ΔT (Delta T, delta-T, deltaT, or DT) is the time difference obtained by subtracting Universal Time (UT) from Terrestrial Time (TT): ΔT=TT−UT. The Earth's rotational speed is , and a day corresponds to one period . A rotational acceleration gives a rate of change of the period of , which is usually expressed as . This has units of 1/time, and is commonly quoted as ms/day/cy.
Universal Time is a time scale based on the Earth's rotation, which is somewhat irregular over short periods (days up to a century), thus any time based on it cannot have an accuracy better than 1 : 108. But the principal effect is over the long term: over many centuries tidal friction inexorably slows Earth's rate of rotation by about ms/cy, or ms/day/cy. During one day, this results in a very small fractional change of ΔT/T = . However, there are other forces changing the rotation rate of the Earth. The most important one is believed to be a result of the melting of continental ice sheets at the end of the last glacial period. This removed their tremendous weight, allowing the land under them to begin to rebound upward in the polar regions, which has been continuing and will continue until isostatic equilibrium is reached. This "post-glacial rebound" brings mass closer to the rotation axis of the Earth, which makes the Earth spin faster (law of conservation of angular momentum): the rate derived from models is about −0.6 ms/day/cy. So the net acceleration (actually a deceleration) of the rotation of the Earth, or the change in the length of the mean solar day (LOD), is +1.7 ms/day/cy. This is indeed the average rate as observed over the past 27 centuries.
Terrestrial Time is a theoretical uniform time scale, defined to provide continuity with the former Ephemeris Time (ET). ET was an independent time-variable, proposed (and its adoption agreed) in the period 1948–52 with the intent of forming a gravitationally uniform time scale as far as was feasible at that time, and depending for its definition on Simon Newcomb's "Tables of the Sun" (1895), interpreted in a new way to accommodate certain observed discrepancies. Newcomb's tables formed the basis of all astronomical ephemerides of the Sun from 1900 through 1983: they were originally expressed (and published) in terms of Greenwich Mean Time and the mean solar day, but later, in respect of the period 1960–1983, they were treated as expressed in terms of ET, in accordance with the adopted ET proposal of 1948–52. ET, in turn, can now be seen (in light of modern results) as close to the average mean solar time between 1750 and 1890 (centered on 1820), because that was the period during which the observations on which Newcomb's tables were based were performed. While TT is strictly uniform (being based on the SI second, every second is the same as every other second), it is in practice realised by International Atomic Time (TAI) with an accuracy of about 1 : 1014.
Earth's rate of rotation must be integrated to obtain time, which is Earth's angular position (specifically, the orientation of the meridian of Greenwich relative to the fictitious mean sun). Integrating +1.7 ms/d/cy and centering the resulting parabola on the year 1820 yields (to a first approximation) 31×((Year − 1820)/100)2 seconds for ΔT. Smoothed historical measurements of ΔT using total solar eclipses are about +16800 s at the year −500 (501 BC), +10600 s at 0 (1 BC), +5700 s at 500 (AD), +1600 s at 1000, and +180 s at 1500. After the invention of the telescope, measurements were made by observing occultations of stars by the Moon, which allowed the derivation of more closely spaced and more accurate values for ΔT. ΔT continued to decrease until it reached a plateau of +11±6 s between 1680 and 1866.
For about three decades immediately before 1902 it was negative, reaching −6.64 s. Then it increased to +63.83 s at 2000. It will continue to increase at an ever faster (quadratic) rate in the future. This will require the addition of an ever greater number of leap seconds to UTC as long as UTC is kept within one second of UT1. (The SI second as now used for UTC, when adopted, was already a little shorter than the current value of the second of mean solar time.) Physically, the meridian of Greenwich in Universal Time is almost always to the east of the meridian in Terrestrial Time, both in the past and in the future. +16800 s or h corresponds to 70°E. This means that at −500 (501 BC), Earth's faster rotation would cause a total solar eclipse to occur 70° to the east of the location calculated using the uniform TT.
All values of ΔT before 1955 depend on observations of the Moon, either via eclipses or occultations. Conservation of angular momentum in the Earth-Moon system requires that the angular momentum lost by the Earth due to tidal friction be transferred to the Moon, increasing its angular momentum, which means that its moment arm (its distance from the Earth) is increased (for the time being about +3.8 cm/year), which via Kepler's laws of planetary motion causes the Moon to revolve around the Earth at a slower rate. The cited values of ΔT assume that the lunar acceleration (actually a deceleration = a negative acceleration) due to this effect is dn/dt = −26"/cy2, where n is the mean sidereal angular motion of the Moon. This is close to the best estimate for dn/dt as of 2002 of −25.858±0.003"/cy2 so ΔT need not be recalculated given the uncertainties and smoothing applied to its current values. Nowadays, UT is the observed orientation of the Earth relative to an inertial reference frame formed by extra-galactic radio sources, modified by an adopted ratio between sidereal time and solar time. Its measurement by several observatories is coordinated by the International Earth Rotation and Reference Systems Service (IERS).

</doc>
<doc id="8728" url="https://en.wikipedia.org/wiki?curid=8728" title="December 22">
December 22


</doc>
<doc id="8729" url="https://en.wikipedia.org/wiki?curid=8729" title="David Deutsch">
David Deutsch

David Elieser Deutsch, FRS (born 18 May 1953) is an Israeli-born British physicist at the University of Oxford. He is a non-stipendiary Visiting Professor in the Department of Atomic and Laser Physics at the Centre for Quantum Computation (CQC) in the Clarendon Laboratory of the University of Oxford. He pioneered the field of quantum computation by formulating a description for a quantum Turing machine, as well as specifying an algorithm designed to run on a quantum computer. He is a proponent of the many-worlds interpretation of quantum mechanics.
Early life and education.
Deutsch was born in Haifa in Israel on 18 May 1953, the son of Oskar and Tikva Deutsch. He attended William Ellis School in London (then a voluntary aided grammar school) before reading Natural Sciences at Clare College, Cambridge and taking Part III of the Mathematical Tripos. He went on to Wolfson College, Oxford for his doctorate in theoretical physics and wrote his thesis on quantum field theory in curved space-time.
Career.
In the Royal Society of London's announcement that Deutsch had become a Fellow of the Royal Society (FRS) in 2008, the Society described Deutsch's contributions thus:
David Deutsch laid the foundations of the quantum theory of computation, and has subsequently made or participated in many of the most important advances in the field, including the discovery of the first quantum algorithms, the theory of quantum logic gates and quantum computational networks, the first quantum error-correction scheme, and several fundamental quantum universality results. He has set the agenda for worldwide research efforts in this new, interdisciplinary field, made progress in understanding its philosophical implications (via a variant of the many-universes interpretation) and made it comprehensible to the general public, notably in his book The Fabric of Reality.
He is currently working on constructor theory, an attempt at generalizing the quantum theory of computation to cover not just computation but all physical processes.
Together with Chiara Marletto, he published a paper in December 2014 entitled "Constructor theory of information", that conjectures that information can be expressed solely in terms of which transformations of physical systems are possible and which are impossible.
Popular science books.
"The Fabric of Reality".
In his 1997 book "The Fabric of Reality", Deutsch details his "Theory of Everything." It aims not at the reduction of everything to particle physics, but rather mutual support among multiversal, computational, epistemological, and evolutionary principles. His theory of everything is somewhat emergentist rather than reductive.
There are "four strands" to his theory:
"The Beginning of Infinity".
Deutsch’s second book, "The Beginning of Infinity: Explanations that Transform the World", was published on 31 March 2011. In this book Deutsch views the Enlightenment of the 18th century as near the beginning of an unending sequence of purposeful knowledge creation. He examines the nature of memes and how and why creativity evolved in humans.
Views.
Deutsch is an atheist. He is also a founding member of the parenting and educational method known as Taking Children Seriously.
He was awarded the Dirac Prize of the Institute of Physics in 1998, and the Edge of Computation Science Prize in 2005. The "Fabric of Reality" was shortlisted for the Rhone-Poulenc science book award in 1998.

</doc>
<doc id="8730" url="https://en.wikipedia.org/wiki?curid=8730" title="Volkssturm">
Volkssturm

The Volkssturm (, "people's storm") was a German national militia established during the last months of World War II. It was set up, not by the traditional German Army, but by the Nazi Party on the orders of Adolf Hitler and its official existence was not announced until October 18, 1944. It was staffed by conscripting males between the ages of 16 and 60 years who were not already serving in some military unit as part of a German Home Guard. The "Volkssturm" comprised one of the final components of the Total War promulgated by Propaganda Minister Joseph Goebbels, part of a Nazi endeavor to overcome their enemies' military strength through force of will.
Origins and organization.
The new "Volkssturm" drew inspiration from the old Prussian "Landsturm" of 1813–1815 that fought in the liberation wars against Napoleon, mainly as guerrilla forces. Plans to form a "Landsturm" national militia in Eastern Germany as a last resort to boost fighting strength initially came from "Oberkommando des Heeres" Army Chief, General Heinz Guderian in 1944. Because the Wehrmacht lacked manpower to stop the Soviet advance, men in jobs not deemed necessary, those previously deemed unfit for military service, youth previously deemed too young, and injured soldiers recuperating from their wounds, were now called to arms. The "Volkssturm" had existed, on paper, since around 1925, but it was only after Hitler ordered Martin Bormann to recruit six million men for this militia that the group became a physical reality. The intended strength of six million was never attained.
Joseph Goebbels and other propagandists depicted the "Volkssturm" as an outburst of enthusiasm and will to resist. While it had some marginal affect on morale, it was undermined by the recruits visible lack of uniforms and weaponry. Nazi themes of death, transcendence, and commemoration were given full play to encourage the fight. Many realized that this was a desperate attempt to turn the course of the war. A popular joke and an occasional comment about the "Volkssturm" are telling in this regard. Sardonic old men would remark, ""We old monkeys are the Führer’s newest weapon""; whereas the joke went, ""Why is the Volkssturm Germany's most precious resource?"" to be answered by: ""Because its members have silver in their hair, gold in their mouth, and lead in their bones.""
In order for these militia units to be effective, the Nazis counted not only on strength in numbers, but also on fanaticism. During the early stages of "Volkssturm" planning, it became apparent that if militia units lacked morale they would lack combat effectiveness. To achieve the envisaged fanaticism, "Volkssturm" units were placed under direct command of the local Nazi party, meaning local "Gau"- and "Kreisleiters". The new "Volkssturm" was also to become a nationwide organization, with Heinrich Himmler, as Replacement Army Commander, responsible for armament and training. Though normally under party control, "Volkssturm" units were placed under Wehrmacht command when engaged in action. Aware that a 'people's army' would not be able to withstand the onslaught of the modern army wielded by the Allies, Hitler issued the following order towards the end of 1944: 
With the Nazi Party in charge of organizing the "Volkssturm", each "Gauleiter", or Nazi Party District Leader, was charged with the leadership, enrollment, and organization of the "Volkssturm" in their district. The largest "Volkssturm" unit seems to have corresponded to the next smaller territorial subdivision of the Nazi Party organization—the Kreis. The basic unit was a battalion of 642 men. Units were mostly composed of members of the Hitler Youth, invalids, the elderly, or men who had previously been considered unfit for military service. Further desperation showed when on 12 February 1945, the Nazis conscripted German women and girls into the auxiliaries of the "Volkssturm". Correspondingly, girls as young as 14 years began receiving instructions on the use of small-arms, bazookas, machine guns, and hand grenades from December 1944 through May 1945. Like many aspects of the Nazi "Volksgemeinschaft" (people's community), the organization of Germany for war was all inclusive, reaching almost all levels of society as the establishment and expansion of the "Volkssturm" reveals.
Municipal organization:
Each Gauleiter and Kreisleiter, had a "Volkssturm" Chief of Staff to assist in handling militia problems.
From its inception until the very end of the Nazi regime, Himmler and Bormann engaged in a power-struggle over the jurisdictional control over the "Volkssturm" regarding security and police powers in Germany and the occupied territories; a contest which Himmler and his SS more or less won on one level (police and security) but lost to Bormann on another (mobilizing reserve forces). Historian David Yelton described the situation as two ranking officers at the helm of a sinking ship fighting over command.
Uniforms and insignia.
The "Volkssturm" "uniform" was only a black armband with words "Deutscher Volkssturm Wehrmacht" with a series of silver collar pips pinned to the wearer's collar. These were characteristically derived from the rank insignia of the various paramilitary organizations of the Nazi Party, which had control over them, and not of the regular Wehrmacht. 
The German government tried to issue as many of its members as possible with military uniforms of all sorts, ranging from field gray to camouflage. These could not be provided to all its members. Thus many members of the "Volkssturm" turned their civilian clothing into makeshift paramilitary uniforms or wore uniforms from their civilian jobs (such as train conductors of the Reichsbahn). 
The simple paramilitary insignia of the "Volkssturm" were as follows:
Training and impact.
Typically, members of the "Volkssturm" received only very basic military training. It included a brief indoctrination and training on the use of basic weapons such as the Karabiner 98k rifle and "Panzerfaust". Because of continuous fighting and weapon shortages, weapon training was often minimal. There was also a lack of instructors, meaning that weapons training was sometimes done by World War I veterans drafted into service themselves. Often "Volkssturm" members were only able to familiarize themselves with their weapons when in actual combat.
There was no standardization of any kind and units were issued only what equipment was available. This was true of every form of equipment—"Volkssturm" members were required to bring their own uniforms and culinary equipment etc. This resulted in the units looking very ragged and, instead of boosting civilian morale, it often reminded people of Germany's desperate state. Armament was equally haphazard: though some Karabiner 98ks were on hand, members were also issued older Gewehr 98s and 19th-century Gewehr 71s and Steyr-Mannlicher M1888s, as well as Dreyse M1907 pistols. In addition there was a plethora of Soviet, British, Belgian, French, Italian, and other weapons that had been captured by German forces during the war. The Germans had also developed cheap but reasonably effective "Volkssturm" weapons, like MP 3008 machine pistols, Volkssturmgewehr 1-5 rifles and VMG-27 light machine guns. These were completely stamped and machine-pressed constructions (in the 1940s, industrial processes were much cruder than today, so a firearm needed great amounts of semi-artisanal work to be actually reliable). The "Volkssturm" troops were nominally supplied when and where possible by both by the Wehrmacht and the SS, but oftentimes they had little to spare. Being armed with leftovers compounded the "Volkssturm's" ineffectiveness; the large number of different ammunition types also put a strain on an already burdened logistics system (for example, the Gewehr 71s used a different type of ammunition than the two 98 rifles). In the last few months of the war, the shortages of modern firearms led to the use of weapons such as shotguns, and even muskets and crossbows taken from museums.
When units had completed their training and received armament, members took a customary oath to Hitler and were then dispatched into combat. Unlike most English-speaking countries, Germany had universal military service for all young men for several generations, so many of the older members would have had at least basic military training from when they served in the German Army and many would have been veterans of the First World War. "Volkssturm" units were supposed to be used only in their own districts, but many were sent directly to the front lines. Ultimately, it was their charge to confront the overwhelming power of the British, American, Canadian, Polish, and Soviet armies alongside Wehrmacht forces to either turn the tide of the war or set a shining example for future generations of Germans and expunge the defeat of 1918 by fighting to the last, dying before surrendering. It was an apocalyptic goal which some of those assigned to the "Volkssturm" took to heart. Unremittingly fanatical members of the "Volkssturm" refused to abandon the Nazi ethos unto the dying days of the Third Reich and in a number of instances, took brutal police "actions" against German civilians deemed defeatists or cowards.
On some occasions, members of the "Volkssturm" showed tremendous courage and a determined will to resist, more so even than soldiers in the "Wehrmacht". The "Volkssturm" battalion 25/235 for instance, started out with 400 men but fought on until there were only 10 men remaining. Fighting at Küstrin between 30 January to 29 March 1945, militia units made up mostly of the "Volkssturm" resisted for nearly two months. Losses were upwards of 60 percent for the "Volkssturm" at Kolberg, roughly 1900 of them died at Breslau, and during the Battle of Königsberg (Kaliningrad), another 2400 members of the "Volkssturm" were killed. At other times along the western front particularly, "Volkssturm" troops would cast their arms aside and disappear into the chaos. Youthful ardor and fanaticism among Hitler Youth members fighting with the "Volkssturm" or an insatiable sense of duty from old men proved tragic sometimes. An example shared by historian Stephen Fritz is instructive in this case:
Not every "Volkssturm" unit was suicidal or apocalyptic in outlook as the war drew closer to its end. Many of them lost their enthusiasm for the fight when it became clear that the Allies had won, prompting them to lay down their weapons and surrender - they also feared being captured by Allied forces and tortured or executed as partisans. Duty to their communities and sparing their fellow Germans from atrocities like that described near Bad Windsheim also played a part in their capitulation as did self-preservation.
Their most extensive use was during the Battle of Berlin, during which "Volkssturm" units fought in many parts of the city. This battle was particularly devastating to its formations, however, since many members fought to the death out of fear of being captured by the Soviets, holding out to the very end which was in keeping with their covenant. Unfortunately for them, a force of over 2.5 million Soviet troops, equipped with 6,250 tanks and over 40,000 artillery pieces was headed their way as the diminished remnants of the Wehrmacht were no match for their enemy. Meanwhile, Hitler decried "betrayal" to everyone yet hunkered-down in the Berlin bunker. Not eager to die what was thought a pointless death above the bunker, many older members of the "Volkssturm" looked for places to hide from the approaching Soviet Army. Juxtaposed against the tragic image of Berlin holding out against all odds, was the frequent exodus and capitulation of Wehrmacht soldiers and members of the "Volkssturm" in southern and western Germany.
Battle for Berlin.
In the Battle for Berlin, members of the "Volkssturm" (mainly young boys from the ages of 13-18 and old men) were used by the German high command as a last-ditch attempt to defend Berlin. The "Volkssturm" had a strength of about 60,000 in the Berlin area formed into 92 battalions, of which about 30 battalions of "Volkssturm I" (those with some weapons) were sent to forward positions while those of "Volkssturm II" (those without weapons) remained in the inner city. One of the few substantive fighting units left to defend Berlin was the LVI Panzer Corps, which occupied the southeastern sector of the town, whereas the remaining parts of the city were being defended by what remained of the SS, the "Volkssturm", and the Hitler Youth formations.
One notable and unusual "Volkssturm" unit in the Battle for Berlin was the 3/115 Siemensstadt Battalion. It comprised 770 men, mainly First World War veterans in their 50s who were reasonably fit factory workers, with experienced officers. Unlike most "Volkssturm" units it was quite well equipped and trained. It was formed into three rifle companies, a support company (with two infantry support guns, four infantry mortars and heavy machine guns), and a heavy weapons company (with four Soviet M-20 howitzers and a French De Bange 220 mm mortar). The battalion first engaged Soviet troops at Friedrichsfelde on April 21 and saw the heaviest fighting over the following two days. It held out until May 2 by which time it was down to just 50 rifles and two light machine guns. The survivors fell back to join other "Volkssturm" units. 26 men from the battalion were awarded the Iron Cross. Already in rubble from Allied bombing, the final stand in Berlin dwindled down to street fighting between highly trained, battle-hardened Russian troops at the brink of final victory against the remnants of German police units, a handful of soldiers, the "Volkssturm", and flak helpers. The chances of the "Volkssturm" making a major difference were never realistic in the face of the overwhelming Allied numbers and material superiority.
While Iron Crosses were being handed out in places like Berlin, other cities and towns like Parchim and Mecklenburg witnessed old elites, acting as military commandants over the Hitler Youth and "Volkssturm", asserting themselves and demanding that the defensive fighting stop so as to spare lives and property. Despite their best efforts, the last 4 months of the war were an exercise in futility for the "Volkssturm" and the Nazi leadership's insistence to continue the fight to the bitter end contributed an additional 1.23 million (approximated) deaths, half of them German military personnel and the other half from the "Volkssturm".
In fiction.
Gregor Dorfmeister, under the pseudonym of Manfred Gregor, published in 1958 the novel "Die Brücke" based on his experiences in a Volkssturm unit.
The novel has been adapted to film in 1959 and a made-for-television movie in 2008.
See also.
Other nations:

</doc>
<doc id="8731" url="https://en.wikipedia.org/wiki?curid=8731" title="Director's cut">
Director's cut

A director's cut is an edited version of a film (or television episode, music video, commercial, or video game) that is supposed to represent the director's own approved edit. 'Cut' explicitly refers to the process of film editing; in preparing a film for release, the director's cut is preceded by the assembly and rough editor's cut and usually followed by the final cut meant for the public film release.
Director's cuts of film are not generally released to the public, because on most films the director does not have a final cut privilege. The production company, distributors, and/or studio (anybody with money invested in the film) can impose changes that they think will make the film profit more at the box office. This sometimes means a happier ending or less ambiguity, or excluding scenes that would earn a more audience-restricting rating, but more often means that the film is simply shortened to provide more screenings per day.
With the rise of home video, the phrase became more generically used as a marketing term (including things such as comic books and music albums, neither of which actually have directors), and the most commonly-seen form of director's cut is a cut where extra scenes are added back in, often making the director's cut considerably longer than the final cut.
Origin of the phrase.
Traditionally, the "director's cut" is not, by definition, the director's ideal or preferred cut. The editing process of a film is broken into stages: First is the assembly/rough cut, where all selected takes are put together in the order in which they should appear in the film. Next, the editor's cut is reduced from the rough cut; the editor may be guided by his own tastes, or following notes from the director or producers. Eventually is the final cut, which actually gets released or broadcast. In between the editor's cut and the final cut can come any number of fine cuts, including the director's cut. The director's cut may include unsatisfactory takes, a preliminary soundtrack, a lack of desired pick-up shots etc., which the director would not like to be shown but uses as a placeholder until satisfactory replacements can be inserted. This is still how the term is used within the film industry, as well as commercials, television, and music videos.
Inception.
The trend of releasing alternate cuts of films for artistic reasons became prominent in the 1970's; in 1974, the "director's cut" of The Wild Bunch was shown theatrically in Los Angeles to sold-out audiences. The theatrical release of the film had cut ten minutes to get an R-rating, but this cut was hailed as superior and has now become the definitive one. Other early examples include George Lucas's first two films being re-released following the success of Star Wars, in cuts which more closely resembled his vision, or Peter Bogdanovich re-cutting The Last Picture Show several times. Charlie Chaplin also re-released all of his films in the 1970's, several of which were re-cut (Chaplin's re-release of The Gold Rush in the 1940's is almost certainly the earliest prominent example of a director's re-cut film being released to the public). A theatrical re-release of Close Encounters of the Third Kind used the phrase "Special Edition" to describe a cut which was closer to Spielberg's intent but had a compromised ending demanded by the studio. As the home video industry rose in the early 1980s, video releases of director's cuts were sometimes created for the small but dedicated cult fan market. Los Angeles cable station Z Channel is also cited as significant in the popularization of alternate cuts. Early examples of films released in this manner include Michael Cimino's "Heaven's Gate", where a longer cut was recalled from theaters but subsequently showed on cable and eventually released to home video; James Cameron's "Aliens", where a video released restored 20 minutes the studio had insisted on cutting; James Cameron's "The Abyss", where Cameron voluntarily made cuts to the theatrical version for pacing but restored them for a video release; and, most famously, Ridley Scott's "Blade Runner", where an alternate workprint version was released to fan acclaim, ultimately resulting in a 1992 recut, the first film to use the term "Director's Cut" as a marketing description (and the first time it was used to describe a cut that the director was not involved in preparing).
Criticism.
Once the floodgates had been opened, and distributors discovered that consumers would buy alternate versions of films, it became common for films to receive multiple releases. There is no standardization for labelling, leading to so-called "director's cuts" of films where the director prefers the theatrically-released version, or even when they had actual final cut privilege. These were often assembled by simply restoring deleted scenes, sometimes adding as much as a half-hour to the length of the film without regard to pacing and storytelling.
As a result, the "director's cut" is often considered a mixed bag, with an equal share of supporters and detractors. Roger Ebert approved of the use of the label in unsuccessful films that had been tampered with by studio executives, such as Sergio Leone's original cut of "Once Upon a Time in America", and the moderately successful theatrical version of "Daredevil", which were altered by studio interference for their theatrical release. Other well-received director's cuts include Ridley Scott's "Kingdom of Heaven" (with "Empire" magazine stating: "The added 45 minutes in the Director’s Cut are like pieces missing from a beautiful but incomplete puzzle"), or Sam Peckinpah's "Pat Garrett and Billy the Kid", where the restored 115 minute cut is closer to the director's intent than the theatrical 105 minute cut (the actual director's cut was 122 minutes; it was never completed to Peckinpah's satisfaction, but was used as a guide for the restoration that was done after his death).
However, Ebert considers adding such material to a successful film a waste. Even Ridley Scott stated on the director's commentary track of "Alien" that the original theatrical release was his director's cut, and that the new version was released as a marketing ploy. Director Peter Bogdanovich, no stranger to director's cuts himself, cites "Red River" as an example where "MGM have a version of Howard Hawks's Red River that they're calling the Director's Cut and it is absolutely not the director's cut. It's a cut the director didn't want, an earlier cut that was junked. They assume because it was longer that it's a director's cut. Capra cut two reels off Lost Horizon because it didn't work and then someone tried to put it back. There are certainly mistakes and stupidities in reconstructing pictures." 
In rare instances, such as Peter Weir's "Picnic at Hanging Rock", John Cassavetes's "The Killing of a Chinese Bookie", and Blake Edwards's "Darling Lili", changes made to a director's cut result in a shorter, more compact cut. This generally happens when a distributor insists that a film be completed in order to meet a release date, but sometimes it is the result of removing scenes that the distributor insisted on inserting, as opposed to restoring scenes they insisted on cutting.
Another way that released director's cuts can be compromised is when directors were never allowed to even shoot their vision, and thus when the film is re-cut, they must make do with the footage that exists. Examples of this include Terry Zwigoff's "Bad Santa", Brian Helgeland's "Payback", and most notably the Richard Donner re-cut of Superman II. Donner completed about 60% of the shooting of the sequel during the shooting of the first one, but was fired from the project. His "director's cut" of the film includes, among other things, screentest footage of stars Reeve & Kidder, footage used in the first film, and entire scenes that were shot by replacement director Richard Lester which Donner dislikes but were required for story purposes.
Extended cuts and special editions.
Some directors explicitly dislike the phrase "director's cut" because it implies that they disapprove of the theatrically released cut. James Cameron and Peter Jackson are two director's who publicly reject the label, preferring "extended edition" or "special edition". While Jackson considers the theatrical releases of "The Lord of the Rings" and "The Hobbit" trilogies to be a final "director's cut" within the constraints of theatrical exhibition, the extended cuts were produced so that fans of the material could see nearly all of the scenes shot for the script to develop more of J. R. R. Tolkien's world, but which were originally cut for running time, or other reasons. New music and special effects were also added to the cuts. Cameron specified "what I put into theaters is the Director's cut. Nothing was cut that I didn't want cut. All the extra scenes we've added back in are just a bonus for the fans." (Though referring specifically to "Avatar", he has expressed similar feelings on all of his films besides "".)
Special editions such as George Lucas's "Star Wars" films, and Steven Spielberg's "E.T. the Extra-Terrestrial", in which special effects are redone in addition to a new edit, have also caused controversy. ("See " List of changes in Star Wars re-releases and "E.T. the Extra-Terrestrial: The 20th Anniversary").
Extended or special editions can also apply to films that have been extended for television to film time slots, against the explicit wishes of the director, such as the TV versions of "Dune" (1984), "The Warriors" (1979) and the "Harry Potter" films.
The film "Caligula" exists in at least 10 different officially-released versions, ranging from a sub-90 minute television edit version of TV-14 (later TVMA) for cable television to an unrated full XXX pornographic version exceeding 3.5 hours. This is believed to be the most distinct versions of a single film.
Music video director's cut.
The music video for the 2006 Academy Award-nominated song "Listen", performed by Beyoncé Knowles, received a director's cut by Diane Martel. This version of the video was later included on Knowles' B'Day Anthology Video Album (2007). Linkin Park has a director's cut version for their music video "Faint" (directed by Mark Romanek) in which one of the band members spray paints the words "En Proceso" on a wall, as well as Hoobastank also having one for 2004's "The Reason" which omits the woman getting hit by the car. Britney Spears' music video for 2007's "Gimme More" was first released as a director's cut on iTunes, with the official video released 3 days later. Many other director's cut music videos contain sexual content that can't be shown on TV thus creating alternative scenes, such as Thirty Seconds to Mars's "Hurricane", and in some cases, alternative videos, such as in the case of Spears' 2008 video for "Womanizer".
Expanded usage in pop culture.
As the trend became more widely recognized, the term "director's cut" became increasingly used as a colloquialism to refer to an expanded version of other things, including video games, music, and comic books. This confusing usage only served to further reduce the artistic value of a "director's cut", and it is currently rarely used in those ways.
Video Games.
For video games, these expanded versions, also referred as "complete editions", will have additions to the gameplay or additional game modes and features outside the main portion of the game. As is the case with certain high-profile Japanese-produced games, the game designers may take the liberty to revise their product for the overseas market with additional features during the localization process. These features are later added back to the native market in a re-release of a game in what is often referred as the international version of the game. This was the case with the overseas versions of "Final Fantasy VII", "Metal Gear Solid" and "Rogue Galaxy", which contained additional features (such as new difficulty settings for "Metal Gear Solid"), resulting in re-released versions of those respective games in Japan ("Final Fantasy VII International", "Metal Gear Solid: Integral" and "Rogue Galaxy: Director's Cut"). In the case of ' and ', the American versions were released first, followed by the Japanese versions and then the European versions, with each regional release offering new content not found in the previous one. All of the added content from the Japanese and European versions of those games were included in the expanded editions titled ' and '.
Several of the Pokémon games have also received director's cuts and have used the term "extension," though "remake" and "third version" are also often used by many fans. These include "" (Japan only), "Pokémon Yellow", "Pokémon Crystal", "Pokémon Emerald", and "Pokémon Platinum".
Music "director's cuts".
"Director's cuts" in music are rarely released. A few exceptions include Guided by Voices' 1994 album "Bee Thousand", which was re-released as a three disc vinyl LP Director's cut in 2004, and Fall Out Boy's 2003 album "Take This to Your Grave", which was re-released as a Director's cut in 2005 with two extra tracks.
In 2011 British singer Kate Bush released the album titled "Director's Cut". It is made up of songs from her earlier albums "The Sensual World" and "The Red Shoes" which have been remixed and restructured, three of which were re-recorded completely.

</doc>
<doc id="8733" url="https://en.wikipedia.org/wiki?curid=8733" title="Digital video">
Digital video

Digital video is a representation of moving visual images in the form of encoded digital data. This is in contrast to analog video, which represents moving visual images with analog signals.
The terms "camera", "video camera", and "camcorder" are used interchangeably in this article.
History.
Starting in the late 1970s to the early 1980s, several types of video production equipment that were digital in their internal workings were introduced, such as time base correctors (TBC) and digital video effects (DVE) units (one of the former being the Thomson-CSF 9100 Digital Video Processor, an internally all-digital full-frame TBC introduced in 1980, and two of the latter being the Ampex ADO, and the Nippon Electric Corporation (NEC) DVE). They operated by taking a standard analog composite video input and digitizing it internally. This made it easier to either correct or enhance the video signal, as in the case of a TBC, or to manipulate and add effects to the video, in the case of a DVE unit. The digitized and processed video information that was output from these units would then be converted back to standard analog video.
Later on in the 1970s, manufacturers of professional video broadcast equipment, such as Bosch (through their Fernseh division), RCA, and Ampex developed prototype digital videotape recorders (VTR) in their research and development labs. Bosch's machine used a modified 1" Type B transport, and recorded an early form of CCIR 601 digital video. Ampex's prototype digital video recorder used a modified 2" Quadruplex VTR (an Ampex AVR-3), but fitted with custom digital video electronics, and a special "octaplex" 8-head headwheel (regular analog 2" Quad machines only used 4 heads). The audio on Ampex's prototype digital machine, nicknamed by its developers as "Annie", still recorded the audio in analog as linear tracks on the tape, like 2" Quad. None of these machines from these manufacturers were ever marketed commercially, however.
Digital video was first introduced commercially in 1986 with the Sony D1 format, which recorded an uncompressed standard definition component video signal in digital form instead of the high-band analog forms that had been commonplace until then. Due to its expense, and the requirement of component video connections using 3 cables (such as YPbPr or RGB component video) to and from a D1 VTR that most television facilities were not wired for (composite NTSC or PAL video using one cable was the norm for most of them at that time), D1 was used primarily by large television networks and other component-video capable video studios.
In 1988, Sony and Ampex co-developed and released the D2 digital videocassette format, which recorded video digitally without compression in ITU-601 format, much like D1. But D2 had the major difference of encoding the video in composite form to the NTSC standard, thereby only requiring single-cable composite video connections to and from a D2 VCR, making it a perfect fit for the majority of television facilities at the time. This made D2 quite a successful format in the television broadcast industry throughout the late '80s and the '90s. D2 was also widely used in that era as the master tape format for mastering laserdiscs (prior to D2, most laserdiscs were mastered using analog 1" Type C videotape).
D1 & D2 would eventually be replaced by cheaper systems using video compression, most notably Sony's Digital Betacam (still heavily used as an electronic field production (EFP) recording format by professional television producers) that were introduced into the network's television studios. Other examples of digital video formats utilizing compression were Ampex's DCT (the first to employ such when introduced in 1992), the industry-standard DV and MiniDV (and its professional variations, Sony's DVCAM and Panasonic's DVCPRO), and Betacam SX, a lower-cost variant of Digital Betacam using MPEG-2 compression.
One of the first digital video products to run on personal computers was "PACo: The PICS Animation Compiler" from The Company of Science & Art in Providence, RI, which was developed starting in 1990 and first shipped in May 1991. PACo could stream unlimited-length video with synchronized sound from a single file (with the ".CAV" file extension) on CD-ROM. Creation required a Mac; playback was possible on Macs, PCs, and Sun Sparcstations. In 1992, Bernard Luskin, Philips Interactive Media, and Eric Doctorow, Paramount Worldwide Video, successfully put the first fifty videos in digital MPEG 1 on CD, developed the packaging and launched movies on CD, leading to advancing versions of MPEG, and to DVD.
QuickTime, Apple Computer's architecture for time-based and streaming data formats appeared in June, 1991. Initial consumer-level content creation tools were crude, requiring an analog video source to be digitized to a computer-readable format. While low-quality at first, consumer digital video increased rapidly in quality, first with the introduction of playback standards such as MPEG-1 and MPEG-2 (adopted for use in television transmission and DVD media), and then the introduction of the DV tape format allowing recordings in the format to be transferred direct to digital video files (containing the same video data recorded on the transferred DV tape) on an editing computer and simplifying the editing process, allowing non-linear editing systems (NLE) to be deployed cheaply and widely on desktop computers with no external playback/recording equipment needed, save for the computer simply requiring a FireWire port to interface to the DV-format camera or VCR. The widespread adoption of digital video has also drastically reduced the bandwidth needed for a high-definition video signal (with HDV and AVCHD, as well as several commercial variants such as DVCPRO-HD, all using less bandwidth than a standard definition analog signal) and tapeless camcorders based on flash memory and often a variant of MPEG-4.
Overview of basic properties.
Digital video comprises a series of orthogonal bitmap digital images displayed in rapid succession at a constant rate. In the context of video these images are called frames. We measure the rate at which frames are displayed in frames per second (FPS).
Since every frame is an orthogonal bitmap digital image it comprises a raster of pixels. If it has a width of W pixels and a height of H pixels we say that the frame size is W"x"H.
Pixels have only one property, their color. The color of a pixel is represented by a fixed number of bits. The more bits the more subtle variations of colors can be reproduced. This is called the color depth (CD) of the video.
An example video can have a duration (T) of 1 hour (3600"sec"), a frame size of 640x480 "(WxH)" at a color depth of 24"bits" and a frame rate of 25"fps". This example video has the following properties:
The most important properties are "bit rate" and "video size". The formulas relating those two with all other properties are:
while some secondary formulas are:
Regarding Interlacing.
In interlaced video each "frame" is composed of two "halves of an image". The first half contains only the odd-numbered lines of a full frame. The second half contains only the even-numbered lines. Those halves are referred to individually as "fields". Two consecutive fields compose a full frame. If an interlaced video has a frame rate of 15 frames per second the field rate is 30 fields per second. All the properties and formulas discussed here apply equally to interlaced video but one should be careful not to confuse the fields per second rate with the frames per second rate.
Properties of compressed video.
The above are accurate for uncompressed video. Because of the relatively high bit rate of uncompressed video, video compression is extensively used. In the case of compressed video each frame requires a small percentage of the original bits. Assuming a compression algorithm that shrinks the input data by a factor of CF, the bit rate and video size would equal to:
Please note that it is not necessary that all frames are equally compressed by a factor of CF. In practice they are not, so CF is the "average" factor of compression for "all" the frames taken together.
The above equation for the bit rate can be rewritten by combining the compression factor and the color depth like this:
The value (CD / CF) represents the average bits per pixel (BPP). As an example, if we have a color depth of 12bits/pixel and an algorithm that compresses at 40x, then BPP equals 0.3 (12/40). So in the case of compressed video the formula for bit rate is:
In fact the same formula is valid for uncompressed video because in that case one can assume that the "compression" factor is 1 and that the average bits per pixel equal the color depth.
More on bit rate and BPP.
As is obvious by its definition bit rate is a measure of the rate of information content of the digital video stream. In the case of uncompressed video, bit rate corresponds directly to the quality of the video (remember that bit rate is proportional to every property that affects the video quality). Bit rate is an important property when transmitting video because the transmission link must be capable of supporting that bit rate. Bit rate is also important when dealing with the storage of video because, as shown above, the video size is proportional to the bit rate and the duration. Bit rate of uncompressed video is too high for most practical applications. Video compression is used to greatly reduce the bit rate.
BPP is a measure of the efficiency of compression. A true-color video with no compression at all may have a BPP of 24 bits/pixel. Chroma subsampling can reduce the BPP to 16 or 12 bits/pixel. Applying jpeg compression on every frame can reduce the BPP to 8 or even 1 bits/pixel. Applying video compression algorithms like MPEG1, MPEG2 or MPEG4 allows for fractional BPP values.
Constant bit rate versus variable bit rate.
As noted above BPP represents the "average" bits per pixel. There are compression algorithms that keep the BPP almost constant throughout the entire duration of the video. In this case we also get video output with a constant bit rate (CBR). This CBR video is suitable for real-time, non-buffered, fixed bandwidth video streaming (e.g. in videoconferencing).
Noting that not all frames can be compressed at the same level because quality is more severely impacted for scenes of high complexity some algorithms try to constantly adjust the BPP. They keep it high while compressing complex scenes and low for less demanding scenes. This way one gets the best quality at the smallest average bit rate (and the smallest file size accordingly). Of course when using this method the bit rate is variable because it tracks the variations of the BPP.
Technical overview.
Standard film stocks such as 16 mm and 35 mm record at 24 frames per second. For video, there are two frame rate standards: NTSC, which shoot at 30/1.001 (about 29.97) frames per second or 59.94 fields per second, and PAL, 25 frames per second or 50 fields per second.
Digital video cameras come in two different image capture formats: interlaced and deinterlaced / progressive scan.
Interlaced cameras record the image in alternating sets of lines: the odd-numbered lines are scanned, and then the even-numbered lines are scanned, then the odd-numbered lines are scanned again, and so on. One set of odd or even lines is referred to as a "field", and a consecutive pairing of two fields of opposite parity is called a "frame". Deinterlaced cameras records each frame as distinct, with all scan lines being captured at the same moment in time. Thus, interlaced video captures samples the scene motion twice as often as progressive video does, for the same number of frames per second. Progressive-scan camcorders generally produce a slightly sharper image. However, motion may not be as smooth as interlaced video which uses 50 or 59.94 fields per second, particularly if they employ the 24 frames per second standard of film.
Digital video can be copied with no degradation in quality. No matter how many generations of a digital source is copied, it will still be as clear as the original first generation of digital footage. However a change in parameters like frame size as well as a change of the digital format can decrease the quality of the video due to new calculations that have to be made. Digital video can be manipulated and edited to follow an order or sequence on an NLE, or non-linear editing workstation, a computer-based device intended to edit video and audio. More and more, videos are edited on readily available, increasingly affordable consumer-grade computer hardware and software. However, such editing systems require ample disk space for video footage. The many video formats and parameters to be set make it quite impossible to come up with a specific number for how many minutes need how much time.
Digital video has a significantly lower cost than 35 mm film. In comparison to the high cost of film stock, the tape stock (or other electronic media used for digital video recording, such as flash memory or hard disk drive) used for recording digital video is very inexpensive. Digital video also allows footage to be viewed on location without the expensive chemical processing required by film. Also physical deliveries of tapes and broadcasts do not apply anymore. Digital television (including higher quality HDTV) started to spread in most developed countries in early 2000s. Digital video is also used in modern mobile phones and video conferencing systems. Digital video is also used for Internet distribution of media, including streaming video and peer-to-peer movie distribution. However even within Europe are lots of TV-Stations not broadcasting in HD, due to restricted budgets for new equipment for processing HD.
Many types of video compression exist for serving digital video over the internet and on optical disks. The file sizes of digital video used for professional editing are generally not practical for these purposes, and the video requires further compression with codecs such as Sorenson, H.264 and more recently Apple ProRes especially for HD. Probably the most widely used formats for delivering video over the internet are MPEG4, Quicktime, Flash and Windows Media, while MPEG2 is used almost exclusively for DVDs, providing an exceptional image in minimal size but resulting in a high level of CPU consumption to decompress.
, the highest resolution demonstrated for digital video generation is 35 megapixels (8192 x 4320). The highest speed is attained in industrial and scientific high speed cameras that are capable of filming 1024x1024 video at up to 1 million frames per second for brief periods of recording.
Interfaces and cables.
Many interfaces have been designed specifically to handle the requirements of uncompressed digital video (from roughly 400 Mbit/s to 10 Gbit/s):
The following interface has been designed for carrying MPEG-Transport compressed video:
Compressed video is also carried using UDP-IP over Ethernet. Two approaches exist for this:
Storage formats.
Encoding.
All current formats, which are listed below, are PCM based.

</doc>
<doc id="8735" url="https://en.wikipedia.org/wiki?curid=8735" title="BIND">
BIND

BIND , or named , is the most widely used Domain Name System (DNS) software on the Internet.
On Unix-like operating systems it is the "de facto" standard.
The software was originally designed at the University of California Berkeley (UCB) in the early 1980s. The name originates as an acronym of "Berkeley Internet Name Domain", reflecting the application's use within UCB. The software consists, most prominently, of the DNS server component, called "named", a contracted form of "name daemon". In addition the suite contains various administration tools, and a DNS resolver interface library. The latest version of BIND is BIND 9, first released in 2000. 
Starting in 2009, the Internet Software Consortium (ISC) developed a new software suite, initially called BIND10. With release version 1.2.0 the project was renamed "Bundy" to terminate ISC involvement in the project.
Database support.
While earlier versions of BIND offered no mechanism to store and retrieve zone data in anything other than flat text files, in 2007 BIND 9.4 DLZ provided a compile-time option for zone storage in a variety of database formats including LDAP, Berkeley DB, PostgreSQL, MySQL, and ODBC.
BIND 10 planned to make the data store modular, so that a variety of databases may be connected.
Security.
The BIND 4 and BIND 8 releases both had serious security vulnerabilities. Their use is strongly discouraged. BIND 9 was a complete rewrite, in part to mitigate these ongoing security issues.
Security issues that are discovered in BIND 9 are patched and publicly disclosed in keeping with common principles of open source software. A complete list of security defects that have been discovered and disclosed in BIND9 is maintained by Internet Systems Consortium, the current authors of the software.
History.
Originally written by four graduate students at the Computer Systems Research Group at the University of California, Berkeley (UCB), BIND was first released with Berkeley Software Distribution 4.3BSD. Paul Vixie started maintaining it in 1988 while working for Digital Equipment Corporation. , the Internet Systems Consortium maintains, updates, and writes new versions of BIND. 
BIND was written by Douglas Terry, Mark Painter, David Riggle and Songnian Zhou in the early 1980s at the University of California, Berkeley as a result of a DARPA grant. The acronym "BIND" is for "Berkeley Internet Name Domain", from a technical paper published in 1984.
Versions of BIND through 4.8.3 were maintained by the Computer Systems Research Group (CSRG) at UC Berkeley.
In the mid-1980s, Paul Vixie of DEC took over BIND development, releasing versions 4.9 and 4.9.1. Paul Vixie continued to work on BIND after leaving DEC. BIND Version 4.9.2 was sponsored by Vixie Enterprises. Vixie eventually founded the ISC, which became the entity responsible for BIND versions starting with 4.9.3.
BIND 8 was released by ISC in May 1997.
Version 9 was developed by Nominum, Inc. under an ISC outsourcing contract, and the first version was released October 9, 2000. It was written from scratch in part to address the architectural difficulties with auditing the earlier BIND code bases, and also to support DNSSEC (DNS Security Extensions). Other important features of BIND 9 include: TSIG, nsupdate, IPv6, rndc (remote name daemon control), views, multiprocessor support, and an improved portability architecture. rndc uses a shared secret to provide encryption for local and remote terminals during each session. The development of BIND 9 took place under a combination of commercial and military contracts. Most of the features of BIND 9 were funded by UNIX vendors who wanted to ensure that BIND stayed competitive with Microsoft's DNS offerings; the DNSSEC features were funded by the US military, which regarded DNS security as important. BIND 9 was released in September 2000.
In 2009, ISC started an effort to develop a new version of the software suite, called BIND10. In addition to DNS service, the BIND10 suite also included IPv4 and IPv6 DHCP server components. In April 2014, with the BIND10 release 1.2.0 the ISC concluded its development work of the project and renamed the project "Bundy", moving the source code repository to GitHub for further development by outside public efforts. Bundy is community-supported at the web site http://bundy-dns.de/. ISC discontinued its involvement in the project due to cost-cutting measures. The development of DHCP components was split off to become a new "Kea" project.

</doc>
<doc id="8736" url="https://en.wikipedia.org/wiki?curid=8736" title="Djbdns">
Djbdns

The djbdns software package is a DNS implementation. It was created by Daniel J. Bernstein in response to his frustrations with repeated security holes in the widely used BIND DNS software. As a challenge, Bernstein offered a $1000 prize for the first person to find a security hole in djbdns, which was awarded in March 2009 to Matthew Dempsky.
, djbdns's tinydns component was the second most popular DNS server in terms of the number of domains for which it was the authoritative server, and third most popular in terms of the number of DNS hosts running it.
djbdns has never been vulnerable to the widespread cache poisoning vulnerability reported in July 2008, but it has been discovered that it is vulnerable to a related attack.
The source code has not been centrally managed since its release in 2001, and was released into the public domain in 2007. As of March 2009, there are a number of forks, one of which is dbndns (part of the Debian Project), and more than a dozen patches to modify the released version.
While djbdns does not directly support DNSSEC, there are third party patches to add DNSSEC support to djbdns' authoritative-only tinydns component.
Components.
The djbdns software consists of servers, clients, and miscellaneous configuration tools.
Design.
In djbdns, different features and services are split off into separate programs. For example, zone transfers, zone file parsing, caching, and recursive resolving are implemented as separate programs. The result of these design decisions is a reduction in code size and complexity of the daemon program that provides the core function of answering lookup requests. Bernstein asserts that this is true to the spirit of the Unix operating system, and makes security verification much simpler.
Copyright status.
On December 28, 2007, Bernstein released djbdns into the public domain. Previously the package was distributed free of charge as license-free software. However this did not permit the distribution of modified versions of djbdns, which was one of the core principles of open-source software. Consequently, it was not included in Linux distributions which required all components to be open-source.

</doc>
<doc id="8741" url="https://en.wikipedia.org/wiki?curid=8741" title="Dylan (programming language)">
Dylan (programming language)

Dylan is a multi-paradigm programming language that includes support for functional and object-oriented programming, and is dynamic and reflective while providing a programming model designed to support efficient machine code generation, including fine-grained control over dynamic and static behaviors. It was created in the early 1990s by a group led by Apple Computer.
A concise and thorough overview of the language may be found in the Dylan Reference Manual.
Dylan derives from Scheme and Common Lisp and adds an integrated object system derived from the Common Lisp Object System (CLOS). In Dylan, all values (including numbers, characters, functions, and classes) are first-class objects. Dylan supports multiple inheritance, polymorphism, multiple dispatch, keyword arguments, object introspection, pattern-based syntax extension macros, and many other advanced features. Programs can express fine-grained control over dynamism, admitting programs that occupy a continuum between dynamic and static programming and supporting evolutionary development (allowing for rapid prototyping followed by incremental refinement and optimization).
Dylan's main design goal is to be a dynamic language well-suited for developing commercial software. Dylan attempts to address potential performance issues by introducing "natural" limits to the full flexibility of Lisp systems, allowing the compiler to clearly understand compilable units (i.e., libraries).
Although deriving much of its semantics from Scheme and other Lisps—some implementations were in fact initially built within existing Lisp systems—Dylan has an ALGOL-like syntax rather than a Lisp-like prefix syntax.
History.
Dylan was created in the early 1990s by a group led by Apple Computer. At one point in its development it was intended for use with Apple's Newton computer, but the Dylan implementation did not reach sufficient maturity in time, and Newton instead used a combination of C and the NewtonScript developed by Walter Smith. Apple ended their Dylan development effort in 1995, though they made a "technology release" version available (Apple Dylan TR1) that included an advanced IDE.
Two other groups contributed to the design of the language and developed implementations: Harlequin released a commercial IDE for Microsoft Windows and Carnegie Mellon University released an open source compiler for Unix systems called Gwydion Dylan. Both of these implementations are now open source. The Harlequin implementation is now known as Open Dylan and is maintained by a group of volunteers, the Dylan Hackers.
The Dylan language was code-named Ralph. James Joaquin chose the name Dylan for "DYnamic LANguage."
Syntax.
Dylan uses an Algol-like syntax designed by Michael Kahl. It is described in great detail in the Dylan Reference Manual. This page shows examples of some syntax features that are more unusual. Many of them come from Dylan's Lisp heritage.
A simple class with several slots:
The same class, rewritten in the most minimal way possible:
A factorial function:
Originally, Dylan used a Lisp-like prefix syntax, which is based on s-expressions:
By the time the language design was completed, it was changed to an Algol-like syntax, with the expectation that it would be more familiar to a wider audience of programmers.
Modules vs. namespace.
In many object-oriented languages, classes are the primary means of encapsulation and modularity; each class defines a namespace and controls which definitions are externally visible. In addition, classes in many languages define an indivisible unit that must be used as a whole—if you want to use a String concatenation function, you must import and compile against all of String.
Some languages also include a separate, explicit namespace or module system that performs encapsulation in a more general way. Dylan is such a language.
In Dylan, the concepts of compile-unit and import-unit are separated, and classes have nothing specifically to do with either. A "library" defines items that should be compiled and handled together, while a "module" defines the namespace. Classes can be placed together in modules, or cut across them, as the programmer wishes. Often the complete definition for a class does not exist in a single module, but is spread across several that are optionally collected together. Different programs can have different definitions of the same class, including only what they need.
For example, consider an add-on library for regex support on String. In some languages, in order for the functionality to be included in strings, the functionality has to be added to the String namespace itself. As soon as you do this, the String class becomes larger, and people who don't need to use regex still have to "pay" for it in increased library size. For this reason these sorts of add-ons are typically placed in their own namespaces and objects. The downside to this approach is that the new functionality is no longer a "part of" string; instead, it is isolated in its own set of functions that have to be called separately. Instead of codice_1, which would be the natural organization from an OO point of view, you use something like codice_2, which effectively reverses the ordering.
In addition, under Dylan many interfaces can be defined for the same code, for instance the String concatenation method could be placed in both the String interface, and the "concat" interface which collects together all of the different concatenation functions from various classes. This is more commonly used in math libraries, where functions tend to be applicable to widely differing object types.
A more practical use of the interface construct is to build public and private versions of a module, something that other languages include as a "bolt on" feature that invariably causes problems and adds syntax. Under Dylan the programmer can simply place every function call in the "Private" or "Development" interface, and collect up publicly accessible functions in "Public". Under Java or C++ the visibility of an object is defined in the code itself, meaning that to support a similar change the programmer would be forced to re-write the definitions completely, and could not have two versions at the same time.
Classes.
Classes in Dylan describe "slots" (data members, fields, ivars, etc.) of objects in a fashion similar to most OO languages. All access to slots are via methods, as in Smalltalk. Default getter and setter methods are automatically generated based on the slot names. In contrast with most other OO languages, other methods applicable to the class are often defined outside of the class, and thus class definitions in Dylan typically include the definition of the storage only. For instance:
<syntaxhighlight lang=dylan>
define class <window> (<view>)
end class;
</syntaxhighlight>
In this example the class "codice_3" is defined. The <class name> syntax is convention only, to make the class names stand out—the angle brackets are merely part of the class name. In comparison, in some languages the convention is to capitalize the first letter of the class name or to prefix the name with a "C" or "T" (for example). codice_3 inherits from a single class, codice_5, and contains two slots, codice_6 holding a string for the window title, and codice_7 holding an X-Y point for a corner of the window. In this particular example the title has been given a default value, while the position has not. The optional "init-keyword" syntax allows the programmer to specify the initial value of the slot when instantiating an object of the class.
In languages such as C++ or Java, the class would also define its interface. In this case the definition above has no explicit instructions, so in both languages access to the slots and methods is considered codice_8, meaning they can be used only by subclasses. In order to allow unrelated code to use the window instances, they would have to be declared codice_9.
In Dylan these sorts of visibility rules are not considered part of the code itself, but of the module/interface system. This adds considerable flexibility. For instance, one interface used during early development could declare everything public, whereas one used in testing and deployment could limit this. With C++ or Java these changes would require changes to the source code itself, so people won't do it, whereas in Dylan this is a completely unrelated concept.
Although this example does not use it, Dylan also supports multiple inheritance.
Methods and generic functions.
In Dylan, methods are not intrinsically associated with any particular class; methods can be thought of as existing outside of classes. Like CLOS, Dylan is based on multimethods, where the specific method to be called is chosen based upon the types of all its arguments. The method does not have to be known at compile time, the understanding being that the required functionality may be available or may not, based on the user's preferences.
Under Java the same methods would be isolated in a particular class. In order to use that functionality the programmer is forced to "import" that class and refer to it explicitly in order to call the method. If that class is not available, or unknown at compile time, the application simply won't compile.
In Dylan, code is isolated from storage in "functions". Many classes have methods that call their own functions, thereby looking and feeling like most other OO languages. However code may also be located in "generic functions", meaning they are not attached to a particular class, and can be called natively by anyone. Linking a particular generic function to a method in a class is accomplished this way:
<syntaxhighlight lang=dylan>
define method turn-blue (w :: <window>)
end method;
</syntaxhighlight>
This definition is similar to those in other languages, and would likely be encapsulated within the codice_3 class. Note the := setter call, which is syntactic sugar for codice_11.
The utility of generic methods comes into its own when you consider more "generic" examples. For instance, one common function in most languages is the codice_12, which returns some human-readable form for the object. For instance, a window might return its title and its position in parens, while a string would return itself. In Dylan these methods could all be collected into a single module called "codice_12", thereby removing this code from the definition of the class itself. If a particular object did not support a codice_12, it could be easily added in the codice_12 module.
Extensibility.
This whole concept might strike some readers as very odd. The code to handle codice_12 for a window isn't defined in codice_3? This might not make any sense until you consider how Dylan handles the call of the codice_12. In most languages when the program is compiled the codice_12 for codice_3 is looked up and replaced with a pointer (more or less) to the method. In Dylan this occurs when the program is first run; the runtime builds a table of method-name/parameters details and looks up methods dynamically via this table. That means that a function for a particular method can be located anywhere, not just in the compile-time unit. In the end the programmer is given considerable flexibility in terms of where to place their code, collecting it along class lines where appropriate, and functional lines where it's not.
The implication here is that a programmer can add functionality to existing classes by defining functions in a separate file. For instance, you might wish to add spell checking to all codice_21s, which in most languages would require access to the source code of the string class—and such basic classes are rarely given out in source form. In Dylan (and other "extensible languages") the spell checking method could be added in the codice_22 module, defining all of the classes on which it can be applied via the codice_23 construct. In this case the actual functionality might be defined in a single generic function, which takes a string and returns the errors. When the codice_22 module is compiled into your program, all strings (and other objects) will get the added functionality.
Apple Dylan.
Apple Dylan is the implementation of Dylan produced by Apple Computer. It was originally developed for the Apple Newton product.

</doc>
<doc id="8742" url="https://en.wikipedia.org/wiki?curid=8742" title="Dublin Core">
Dublin Core

The Dublin Core Schema is a small set of vocabulary terms that can be used to describe web resources (video, images, web pages, etc.), as well as physical resources such as books or CDs, and objects like artworks. The full set of Dublin Core metadata terms can be found on the Dublin Core Metadata Initiative (DCMI) website. The original set of 15 classic metadata terms, known as the Dublin Core Metadata Element Set are endorsed in the following standards documents:
Dublin Core Metadata may be used for multiple purposes, from simple resource description, to combining metadata vocabularies of different metadata standards, to providing interoperability for metadata vocabularies in the Linked Data cloud and Semantic Web implementations.
Background.
"Dublin" refers to Dublin, Ohio, USA where the schema originated during the 1995 invitational OCLC/NCSA Metadata Workshop, hosted by the Online Computer Library Center (OCLC), a library consortium based in Dublin, and the National Center for Supercomputing Applications (NCSA). "Core" refers to the metadata terms as "broad and generic being usable for describing a wide range of resources". The semantics of Dublin Core were established and are maintained by an international, cross-disciplinary group of professionals from librarianship, computer science, text encoding, museums, and other related fields of scholarship and practice.
Starting in 2000, the Dublin Core community focused on "application profiles" – the idea that metadata records would use Dublin Core together with other specialized vocabularies to meet particular implementation requirements. During that time, the World Wide Web Consortium's work on a generic data model for metadata, the Resource Description Framework (RDF), was maturing. As part of an extended set of DCMI Metadata Terms, Dublin Core became one of the most popular vocabularies for use with RDF, more recently in the context of the Linked Data movement.
The Dublin Core Metadata Initiative (DCMI) provides an open forum for the development of interoperable online metadata standards for a broad range of purposes and of business models. DCMI's activities include consensus-driven working groups, global conferences and workshops, standards liaison, and educational efforts to promote widespread acceptance of metadata standards and practices. In 2008, DCMI separated from OCLC and incorporated as an independent entity.
Currently, any and all changes that are made to the Dublin Core standard, are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy (DCMI-NAMESPACE). This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.
Levels of the standard.
The Dublin Core standard originally includes two levels: Simple and Qualified. Simple Dublin Core comprised 15 elements; Qualified Dublin Core included three additional elements (Audience, Provenance and RightsHolder), as well as a group of element refinements (also called qualifiers) that could refine the semantics of the elements in ways that may be useful in resource discovery.
Since 2012 the two have been incorporated into the DCMI Metadata Terms as a single set of terms using the Resource Description Framework (RDF). The full set of elements is found under the namespace http://purl.org/dc/terms/. Because the definition of the terms often contains domains and ranges, which may not be compatible with the pre-RDF definitions used for the original 15 Dublin Core elements, there is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.
Dublin Core Metadata Element Set Version 1.1.
The original Dublin Core Metadata Element Set consists of 15 metadata elements:
Each Dublin Core element is optional and may be repeated. The DCMI has established standard ways to refine elements and encourage the use of encoding and vocabulary schemes. There is no prescribed order in Dublin Core for presenting or using the elements. The Dublin Core became ISO 15836 standard in 2006 and is used as a base-level data element set for the description of learning resources in the ISO/IEC 19788-2 Metadata for learning resources (MLR) – Part 2: Dublin Core elements, prepared by the ISO/IEC JTC1 SC36.
Full information on element definitions and term relationships can be found in the Dublin Core Metadata Registry.
An example of use mention of D.C. (by WebCite).
At the web page which serves as the "archive" form for WebCite, it says, in part: "Metadata (optional)         These are Dublin Core elements. [...]".
Qualified Dublin Core (deprecated in 2012).
Subsequent to the specification of the original 15 elements, an ongoing process to develop exemplary terms extending or refining the Dublin Core Metadata Element Set (DCMES) was begun. The additional terms were identified, generally in working groups of the Dublin Core Metadata Initiative, and judged by the DCMI Usage Board to be in conformance with principles of good practice for the qualification of Dublin Core metadata elements.
Elements refinements make the meaning of an element narrower or more specific. A refined element shares the meaning of the unqualified element, but with a more restricted scope. The guiding principle for the qualification of Dublin Core elements, colloquially known as the "Dumb-Down Principle", states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.
In addition to element refinements, Qualified Dublin Core includes a set of recommended encoding schemes, designed to aid in the interpretation of an element value. These schemes include controlled vocabularies and formal notations or parsing rules. A value expressed using an encoding scheme may thus be a token selected from a controlled vocabulary (for example, a term from a classification system or set of subject headings) or a string formatted in accordance with a formal notation, for example, "2000-12-31" as the ISO standard expression of a date. If an encoding scheme is not understood by an application, the value may still be useful to human reader.
Audience, Provenance and RightsHolder are elements, but not part of the Simple Dublin Core 15 elements. Use Audience, Provenance and RightsHolder only when using Qualified Dublin Core.
DCMI also maintains a small, general vocabulary recommended for use within the element Type. This vocabulary currently consists of 12 terms.
DCMI Metadata Terms.
The Dublin Core Metadata Initiative (DCMI) Metadata Terms is the current set of the Dublin Core vocabulary. This set includes the fifteen terms of the Dublin Core Metadata Element Set (in "italic"), as well as the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as RDF properties.
Syntax.
Syntax choices for Dublin Core metadata depends on a number of variables, and "one size fits all" prescriptions rarely apply. When considering an appropriate syntax, it is important to note that Dublin Core concepts and semantics are designed to be syntax independent and are equally applicable in a variety of contexts, as long as the metadata is in a form suitable for interpretation both by machines and by human beings.
The Dublin Core Abstract Model provides a reference model against which particular Dublin Core encoding guidelines can be compared, independent of any particular encoding syntax. Such a reference model allows implementers to gain a better understanding of the kinds of descriptions they are trying to encode and facilitates the development of better mappings and translations between different syntax.
Some applications.
One Document Type Definition based on Dublin Core is the Open Source Metadata Framework (OMF) specification. OMF is in turn used by Rarian (superseding ScrollKeeper), which is used by the GNOME desktop and KDE help browsers and the ScrollServer documentation server. PBCore is also based on Dublin Core. The Zope CMF's Metadata products, used by the Plone, ERP5, the Nuxeo CPS Content management systems, SimpleDL, and FedoraCommons also implement Dublin Core. The EPUB e-book format uses Dublin Core metadata in the OPF file. eXo Platform also implements Dublin Core.
DCMI also maintains a list of projects using Dublin Core on its website.

</doc>
<doc id="8743" url="https://en.wikipedia.org/wiki?curid=8743" title="Document Object Model">
Document Object Model

The Document Object Model (DOM) is a cross-platform and language-independent "convention" for representing and interacting with objects in HTML, XHTML, and XML documents. The nodes of every document are organized in a tree structure, called the "DOM tree". Objects in the DOM tree may be addressed and manipulated by using methods on the objects. The public interface of a DOM is specified in its application programming interface (API).
The history of the Document Object Model is intertwined with the history of the "browser wars" of the late 1990s between Netscape Navigator and Microsoft Internet Explorer, as well as with that of JavaScript and JScript, the first scripting languages to be widely implemented in the layout engines of web browsers.
Legacy DOM.
JavaScript was released by Netscape Communications in 1995 within Netscape Navigator 2.0. Netscape's competitor, Microsoft, released Internet Explorer 3.0 the following year with a port of JavaScript called JScript. JavaScript and JScript let web developers create web pages with client-side interactivity. The limited facilities for detecting user-generated events and modifying the HTML document in the first generation of these languages eventually became known as "DOM Level 0" or "Legacy DOM." No independent standard was developed for DOM Level 0, but it was partly described in the specification of HTML 4.
Legacy DOM was limited in the kinds of elements that could be accessed. Form, link and image elements could be referenced with a hierarchical name that began with the root document object. A hierarchical name could make use of either the names or the sequential index of the traversed elements. For example, a form input element could be accessed as either codice_1 or codice_2.
The Legacy DOM enabled client-side form validation and the popular "rollover" effect.
Intermediate DOM.
In 1997, Netscape and Microsoft released version 4.0 of Netscape Navigator and Internet Explorer respectively, adding support for Dynamic HTML (DHTML), functionality enabling changes to a loaded HTML document. DHTML required extensions to the rudimentary document object that was available in the Legacy DOM implementations. Although the Legacy DOM implementations were largely compatible since JScript was based on JavaScript, the DHTML DOM extensions were developed in parallel by each browser maker and remained incompatible. These versions of the DOM became known as the "Intermediate DOM."
Standardization.
The World Wide Web Consortium (W3C), founded in 1994 to promote open standards for the World Wide Web, brought Netscape Communications and Microsoft together with other companies to develop a standard for browser scripting languages, called "ECMAScript." The first version of the standard was published in 1997. Subsequent releases of JavaScript and JScript would implement the ECMAScript standard for greater cross-browser compatibility.
After the release of ECMAScript, W3C began working on a standardized DOM. The initial DOM standard, known as "DOM Level 1", was recommended by W3C in late 1998. About the same time, Internet Explorer 5.0 shipped with limited support for DOM Level 1. DOM Level 1 provided a complete model for an entire HTML or XML document, including means to change any portion of the document. Non-conformant browsers such as Internet Explorer 4.x and Netscape 4.x were still widely used as late as 2000.
DOM Level 2 was published in late 2000. It introduced the codice_3 function as well as an event model and support for XML namespaces and CSS.
DOM Level 3, the current release of the DOM specification, published in April 2004, added support for XPath and keyboard event handling, as well as an interface for serializing documents as XML.
DOM Level 4 is currently being developed. Last Call Working Draft was released in February 2014.
By 2005, large parts of W3C DOM were well-supported by common ECMAScript-enabled browsers, including Microsoft Internet Explorer version 6 (from 2001), Opera, Safari and Gecko-based browsers (like Mozilla, Firefox, SeaMonkey and Camino).
Applications.
Web browsers.
To render a document such as an HTML page, most web browsers use an internal model similar to the DOM. The nodes of every document are organized in a tree structure, called the "DOM tree", with topmost node named as "Document object". When an HTML page is rendered in browsers, the browser downloads the HTML into local memory and automatically parses it to display the page on screen. The DOM is also the way JavaScript transmits the state of the browser in HTML pages.
JavaScript.
When a web page is loaded, the browser creates a Document Object Model of the page.
With the object model, JavaScript is fully enabled to create dynamic HTML:
Implementations.
Because DOM supports navigation in any direction (e.g., parent and previous sibling) and allows for arbitrary modifications, an implementation must at least buffer the document that has been read so far (or some parsed form of it).
Layout engines.
Web browsers rely on layout engines to parse HTML into a DOM. Some layout engines, such as Trident/MSHTML, are associated primarily or exclusively with a particular browser, such as Internet Explorer. Others, such as Blink, WebKit, and Gecko, are shared by a number of browsers, such as Google Chrome, Opera, Safari, and Firefox. The different layout engines implement the DOM standards to varying degrees of compliance.
Libraries.
DOM implementations:
APIs that expose DOM implementations:
Inspection tools

</doc>
<doc id="8745" url="https://en.wikipedia.org/wiki?curid=8745" title="Design pattern">
Design pattern

A design pattern is the re-usable form of a solution to a design problem. The idea was introduced by the architect Christopher Alexander and has been adapted for various other disciplines, most notably computer science.
An organized collection of design patterns that relate to a particular field is called a pattern language. This language gives a common terminology for discussing the situations designers are faced with.
Documenting a pattern requires explaining why a particular situation causes problems, and how the components of the pattern relate to each other to give the solution. Christopher Alexander describes common design problems as arising from "conflicting forces" — such as the conflict between wanting a room to be sunny and wanting it not to overheat on summer afternoons. A pattern would not tell the designer how many windows to put in the room; instead, it would propose a set of values to guide the designer toward a decision that is best for their particular application. Alexander, for example, suggests that enough windows should be included to direct light all around the room. He considers this a good solution because he believes it increases the enjoyment of the room by its occupants. Other authors might come to different conclusions, if they place higher value on heating costs, or material costs. These values, used by the pattern's author to determine which solution is "best", must also be documented within the pattern.
Pattern documentation should also explain when it is applicable. Since two houses may be very different from one another, a design pattern for houses must be broad enough to apply to both of them, but not so vague that it doesn't help the designer make decisions. The range of situations in which a pattern can be used is called its context. Some examples might be "all houses", "all two-story houses", or "all places where people spend time".
For instance, in Christopher Alexander's work, bus stops and waiting rooms in a surgery center are both within the context for the pattern "A PLACE TO WAIT".

</doc>
<doc id="8748" url="https://en.wikipedia.org/wiki?curid=8748" title="N,N-Dimethyltryptamine">
N,N-Dimethyltryptamine

"N","N"-Dimethyltryptamine (DMT or "N","N"-DMT) is a psychedelic compound of the tryptamine family. It is a structural analog of serotonin and melatonin and a functional analog of other psychedelic tryptamines such as 4-AcO-DMT, 5-MeO-DMT, 5-HO-DMT, psilocybin (4-PO-DMT), and psilocin (4-HO-DMT).
Historically, it has been consumed by indigenous Amazonian Indian cultures in the form of ayahuasca for divinatory and healing purposes.
History.
DMT was first synthesized in 1931 by Canadian chemist Richard Helmuth Fredrick Manske (1901–1977). In general, its discovery as a natural product is credited to Brazilian chemist and microbiologist Oswaldo Gonçalves de Lima (1908–1989) who, in 1946, isolated an alkaloid he named "nigerina" (nigerine) from the root bark of "jurema preta", that is, "Mimosa tenuiflora". However, in a careful review of the case Jonathan Ott shows that the empirical formula for nigerine determined by Gonçalves de Lima, which notably contains an atom of oxygen, can match only a partial, "impure" or "contaminated" form of DMT. It was only in 1959, when Gonçalves de Lima provided American chemists a sample of "Mimosa tenuiflora" roots, that DMT was unequivocally identified in this plant material. Less ambiguous is the case of isolation and formal identification of DMT in 1955 in seeds and pods of "Anadenanthera peregrina" by a team of American chemists led by Evan Horning (1916–1993). Since 1955, DMT has been found in a host of organisms: in at least fifty plant species belonging to ten families, and in at least four animal species, including one gorgonian and three mammalian species.
Another historical milestone is the discovery of DMT in plants frequently used by Amazonian natives as additive to the vine "Banisteriopsis caapi" to make ayahuasca decoctions. In 1957, American chemists Francis Hochstein and Anita Paradies identified DMT in an "aqueous extract" of leaves of a plant they named "Prestonia amazonicum" ("sic") and described as "commonly mixed" with "B. caapi". The lack of a proper botanical identification of "Prestonia amazonica" in this study led American ethnobotanist Richard Evans Schultes (1915–2001) and other scientists to raise serious doubts about the claimed plant identity. Better evidence was produced in 1965 by French pharmacologist Jacques Poisson, who isolated DMT as a sole alkaloid from leaves, provided and used by Aguaruna Indians, identified as having come from the vine "Diplopterys cabrerana" (then known as "Banisteriopsis rusbyana"). Published in 1970, the first identification of DMT in the plant "Psychotria viridis", another common additive of ayahuasca, was made by a team of American researchers led by pharmacologist Ara der Marderosian. Not only did they detect DMT in leaves of "P. viridis" obtained from Cashinahua Indians, but they also were the first to identify it in a sample of an ayahuasca decoction, prepared by the same Indians.
Biosynthesis.
Dimethyltryptamine is an indole alkaloid derived from the shikimate pathway. Its biosynthesis is relatively simple and summarized in the picture to the left. In plants, the parent amino acid L-tryptophan is produced endogenously where in animals L-tryptophan is an essential amino acid coming from diet. No matter the source of L-tryptophan, the biosynthesis begins with its decarboxylation by an aromatic amino acid decarboxylase (AADC) enzyme (step 1). The resulting decarboxylated tryptophan analog is tryptamine. Tryptamine then undergoes a transmethylation (step 2): the enzyme indolethylamine-N-methyltransferase (INMT) catalyzes the transfer of a methyl group from cofactor S-adenosyl-methionine (SAM), via nucleophilic attack, to tryptamine. This reaction transforms SAM into S-adenosylhomocysteine (SAH), and gives the intermediate product "N"-methyltryptamine (NMT). NMT is in turn transmethylated by the same process (step 3) to form the end product "N","N"-dimethyltryptamine. Tryptamine transmethylation is regulated by two products of the reaction: SAH, and DMT were shown "ex vivo" to be among the most potent inhibitors of rabbit INMT activity.
This transmethylation mechanism has been repeatedly and consistently proven by radiolabeling of SAM methyl group with carbon-14 (14C-CH3)SAM).
Evidence in mammals.
Published in "Science" in 1961, Julius Axelrod found an "N"-methyltransferase enzyme capable of mediating biotransformation of tryptamine into DMT in a rabbit's lung. This finding initiated a still ongoing scientific interest in endogenous DMT production in humans and other mammals. From then on, two major complementary lines of evidence have been investigated: localization and further characterization of the "N"-methyltransferase enzyme, and analytical studies looking for endogenously produced DMT in body fluids and tissues.
In 2013 researchers first reported DMT in the pineal gland microdialysate of rodents.
A study published in 2014 reported the biosynthesis of N,N-dimethyltryptamine (DMT) in the human melanoma cell line SK-Mel-147 including details on its metabolism by peroxidases.
In a 2014 paper a group first demonstrated the immunomodulatory potential of DMT and 5-MeO-DMT through the Sigma-1 receptor of human immune cells. This immunomodulatory activity may contribute to significant anti-inflammatory effects and tissue regeneration.
INMT.
Before techniques of molecular biology were used to localize indolethylamine N-methyltransferase (INMT), characterization and localization went on a par: samples of the biological material where INMT is hypothesized to be active are subject to enzyme assay. Those enzyme assays are performed either with a radiolabeled methyl donor like (14C-CH3)SAM to which known amounts of unlabeled substrates like tryptamine are added or with addition of a radiolabeled substrate like (14C)NMT to demonstrate in vivo formation. As qualitative determination of the radioactively tagged product of the enzymatic reaction is sufficient to characterize INMT existence and activity (or lack of), analytical methods used in INMT assays are not required to be as sensitive as those needed to directly detect and quantify the minute amounts of endogenously formed DMT (see DMT subsection below). The essentially qualitative method thin layer chromatography (TLC) was thus used in a vast majority of studies. Also, robust evidence that INMT can catalyze transmethylation of tryptamine into NMT and DMT could be provided with reverse isotope dilution analysis coupled to mass spectrometry for rabbit and human lung during the early 1970s.
Selectivity rather than sensitivity proved to be an Achilles’ heel for some TLC methods with the discovery in 1974–1975 that incubating rat blood cells or brain tissue with (14C-CH3)SAM and NMT as substrate mostly yields tetrahydro-β-carboline derivatives, and negligible amounts of DMT in brain tissue. It is indeed simultaneously realized that the TLC methods used thus far in almost all published studies on INMT and DMT biosynthesis are incapable to resolve DMT from those tetrahydro-β-carbolines. These findings are a blow for all previous claims of evidence of INMT activity and DMT biosynthesis in avian and mammalian brain, including in vivo, as they all relied upon use of the problematic TLC methods: their validity is doubted in replication studies that make use of improved TLC methods, and fail to evidence DMT-producing INMT activity in rat and human brain tissues. Published in 1978, the last study attempting to evidence in vivo INMT activity and DMT production in brain (rat) with TLC methods finds biotransformation of radiolabeled tryptamine into DMT to be real but "insignificant". Capability of the method used in this latter study to resolve DMT from tetrahydro-β-carbolines is questioned later.
To localize INMT, a qualitative leap is accomplished with use of modern techniques of molecular biology, and of immunohistochemistry. In humans, a gene encoding INMT is determined to be located on chromosome 7. Northern blot analyses reveal INMT messenger RNA (mRNA) to be highly expressed in rabbit lung, and in human thyroid, adrenal gland, and lung. Intermediate levels of expression are found in human heart, skeletal muscle, trachea, stomach, small intestine, pancreas, testis, prostate, placenta, lymph node, and spinal cord. Low to very low levels of expression are noted in rabbit brain, and human thymus, liver, spleen, kidney, colon, ovary, and bone marrow. INMT mRNA expression is absent in human peripheral blood leukocytes, whole brain, and in tissue from 7 specific brain regions (thalamus, subthalamic nucleus, caudate nucleus, hippocampus, amygdala, substantia nigra, and corpus callosum). Immunohistochemistry showed INMT to be present in large amounts in glandular epithelial cells of small and large intestines. In 2011, immunohistochemistry revealed the presence of INMT in primate nervous tissue including retina, spinal cord motor neurons, and pineal gland.
Endogenous DMT.
The first claimed detection of mammalian endogenous DMT was published in June 1965: German researchers F. Franzen and H. Gross report to have evidenced and quantified DMT, along with its structural analog bufotenin (5-HO-DMT), in human blood and urine. In an article published four months later, the method used in their study was strongly criticized, and the credibility of their results challenged.
Few of the analytical methods used prior to 2001 to measure levels of endogenously formed DMT had enough sensitivity and selectivity to produce reliable results. Gas chromatography, preferably coupled to mass spectrometry (GC-MS), is considered a minimum requirement. A study published in 2005 implements the most sensitive and selective method ever used to measure endogenous DMT: liquid chromatography-tandem mass spectrometry with electrospray ionization (LC-ESI-MS/MS) allows for reaching limits of detection (LODs) 12 to 200 fold lower than those attained by the best methods employed in the 1970s. The data summarized in the table below are from studies conforming to the abovementioned requirements (abbreviations used: CSF = cerebrospinal fluid; LOD = limit of detection; n = number of samples; ng/L and ng/kg = nanograms (10−9 g) per litre, and nanograms per kilogram, respectively):
A 2013 study found DMT in microdialysate obtained from a rat's pineal gland, providing evidence of endogenous DMT in the mammalian brain.
Physical and chemical properties.
DMT is commonly handled and stored as a fumarate, as other DMT acid salts are extremely hygroscopic and will not readily crystallize. Its freebase form, although less stable than DMT fumarate, is favored by recreational users choosing to vaporize the chemical as it has a lower boiling point. In contrast to DMT's base, its salts are water-soluble. DMT in solution degrades relatively quickly and should be stored protected from air, light, and heat in a freezer.
As distinguished from 5-MeO-DMT.
5-MeO-DMT, a psychedelic drug structurally similar to "N","N"-DMT, is sometimes referred to as DMT through abbreviation. As a white, crystalline solid, it is also similar in appearance to DMT. However, it is considerably more potent (5-MeO-DMT typical vaporized dose: 5–20 mg), and care should be taken to clearly differentiate between the two drugs to avoid accidental overdose.
Pharmacology.
Pharmacokinetics.
DMT peak level concentrations ("C"max) measured in whole blood after intramuscular (IM) injection (0.7 mg/kg, n = 11) and in plasma following intravenous (IV) administration (0.4 mg/kg, n = 10) of fully psychedelic doses are in the range of ≈14 to 154 μg/L and 32 to 204 μg/L, respectively.
The corresponding molar concentrations of DMT are therefore in the range of 0.074–0.818 µM in whole blood and 0.170–1.08 µM in plasma. However, several studies have described active transport and accumulation of DMT into rat and dog brain following peripheral administration.
Similar active transport, and accumulation processes likely occur in human brain and may concentrate DMT in brain by several-fold or more (relatively to blood), resulting in local concentrations in the micromolar or higher range. Such concentrations would be commensurate with serotonin brain tissue concentrations, which have been consistently determined to be in the 1.5-4 μM range.
Closely coextending with peak psychedelic effects, mean time to reach peak concentrations ("T"max) was determined to be 10–15 minutes in whole blood after IM injection, and 2 minutes in plasma after IV administration. When taken orally mixed in an ayahuasca decoction, and in freeze-dried ayahuasca gel caps, DMT "T"max is considerably delayed: 107.59 ± 32.5 minutes, and 90–120 minutes, respectively.
The pharmacokinetics for vaporizing DMT have not been studied or reported.
Pharmacodynamics.
DMT binds non-selectively with affinities < 0.6 μM to the following serotonin receptors: 5-HT1A, 5-HT1B, 5-HT1D, 5-HT2A, 5-HT2B, 5-HT2C, 5-HT6, and 5-HT7. An agonist action has been determined at 5-HT1A, 5-HT2A and 5-HT2C. Its efficacies at other serotonin receptors remain to be determined. Of special interest will be the determination of its efficacy at human 5-HT2B receptor as two "in vitro" assays evidenced DMT's high affinity for this receptor: 0.108 μM and 0.184 μM. This may be of importance because chronic or frequent uses of serotonergic drugs showing preferential high affinity and clear agonism at 5-HT2B receptor have been causally linked to valvular heart disease.
It has also been shown to possess affinity for the dopamine D1, α1-adrenergic, α2-adrenergic, imidazoline-1, and sigma-1 (σ1) receptors. Converging lines of evidence established activation of the σ1 receptor at concentrations of 50–100 μM. Its efficacies at the other receptor binding sites are unclear. It has also been shown "in vitro" to be a substrate for the cell-surface serotonin transporter (SERT) and the intracellular vesicular monoamine transporter 2 (VMAT2), inhibiting SERT-mediated serotonin uptake in human platelets at an average concentration of 4.00 ± 0.70 μM and VMAT2-mediated serotonin uptake in vesicles (of army worm Sf9 cells) expressing rat VMAT2 at an average concentration of 93 ± 6.8 μM.
As with other so-called "classical hallucinogens", a large part of DMT psychedelic effects can be attributed to a functionally selective activation of the 5-HT2A receptor. DMT concentrations eliciting 50% of its maximal effect (half maximal effective concentration = EC50 or Kact) at the human 5-HT2A receptor "in vitro" are in the 0.118–0.983 μM range. This range of values coincides well with the range of concentrations measured in blood and plasma after administration of a fully psychedelic dose (see Pharmacokinetics).
As DMT has been shown to have slightly better efficacy (EC50) at human serotonin 2C receptor than at the 2A receptor, 5-HT2C is also likely implicated in DMT's overall effects. Other receptors, such as 5-HT1A σ1, may also play a role.
In 2009, it was hypothesized that DMT may be an endogenous ligand for the σ1 receptor. The concentration of DMT needed for σ1 activation "in vitro" (50–100 μM) is similar to the behaviorally active concentration measured in mouse brain of approximately 106 μM This is minimally 4 orders of magnitude higher than the average concentrations measured in rat brain tissue or human plasma under basal conditions (see Endogenous DMT), so σ1 receptors are likely to be activated only under conditions of high local DMT concentrations. If DMT is stored in synaptic vesicles, such concentrations might occur during vesicular release. To illustrate, while the "average" concentration of serotonin in brain tissue is in the 1.5-4 μM range, the concentration of serotonin in synaptic vesicles was measured at 270 mM. Following vesicular release, the resulting concentration of serotonin in the synaptic cleft, to which serotonin receptors are exposed, is estimated to be about 300 μM. Thus, while "in vitro" receptor binding affinities, efficacies, and average concentrations in tissue or plasma are useful, they are not likely to predict DMT concentrations in the vesicles or at synaptic or intracellular receptors. Under these conditions, notions of receptor selectivity are moot, and it seems probable that most of the receptors identified as targets for DMT (see above) participate in producing its psychedelic effects.
As a psychedelic.
DMT is produced in many species of plants often in conjunction with its close chemical relatives 5-MeO-DMT and bufotenin (5-OH-DMT). DMT-containing plants are commonly used in South American shamanic practices. It is usually one of the main active constituents of the drink ayahuasca; however, ayahuasca is sometimes brewed with plants that do not produce DMT. It occurs as the primary psychoactive alkaloid in several plants including "Mimosa tenuiflora", "Diplopterys cabrerana", and "Psychotria viridis". DMT is found as a minor alkaloid in snuff made from Virola bark resin in which 5-MeO-DMT is the main active alkaloid. DMT is also found as a minor alkaloid in bark, pods, and beans of "Anadenanthera peregrina" and "Anadenanthera colubrina" used to make Yopo and Vilca snuff in which bufotenin is the main active alkaloid. Psilocin, an active chemical in many psychedelic mushrooms, is structurally similar to DMT.
The psychotropic effects of DMT were first studied scientifically by the Hungarian chemist and psychologist Dr. Stephen Szára, who performed research with volunteers in the mid-1950s. Szára, who later worked for the US National Institutes of Health, had turned his attention to DMT after his order for LSD from the Swiss company Sandoz Laboratories was rejected on the grounds that the powerful psychotropic could be dangerous in the hands of a communist country.
DMT can produce powerful psychedelic experiences including intense visuals, euphoria and hallucinations. DMT is generally not active orally unless it is combined with a monoamine oxidase inhibitor (MAOI) such as a reversible inhibitor of monoamine oxidase A (RIMA), for example, harmaline. Without an MAOI, the body quickly metabolizes orally administered DMT, and it therefore has no hallucinogenic effect unless the dose exceeds monoamine oxidase's metabolic capacity. Other means of ingestion such as vaporizing, injecting, or insufflating the drug can produce powerful hallucinations for a short time (usually less than half an hour), as the DMT reaches the brain before it can be metabolized by the body's natural monoamine oxidase. Taking a MAOI prior to vaporizing or injecting DMT prolongs and potentiates the effects.
"Machine Elves".
One common feature of the hallucinogenic experience caused by DMT are hallucinations of humanoid beings, characterised as being otherworldly. The term "Machine Elf" was coined by ethnobotanist Terence McKenna for the experience, who also used the terms "fractal elves", or "self-transforming machine elves".
Hallucinations of strange creatures had been reported by Szara in the "Journal of Mental Science" (now the British Journal of Psychiatry) (1958) "“Dimethyltryptamine Experiments with Psychotics”", Stephen Szara described how one of his subjects under the influence of DMT had experienced “strange creatures, dwarves or something” at the beginning of a DMT trip.
Other researchers of the experience described 'entities' or 'beings' in humanoid as well as animal form, with descriptions of "little people" being common (non-human gnomes, elves, imps etc.). This form of hallucination has been speculated to be the cause of alien abduction experiences through endogenously occurring DMT.
Cliff Pickover has also written about the "machine elf"-experience, in the book "Sex, Drugs, Einstein, & Elves".
Routes of administration.
Inhalation.
A standard dose for vaporized DMT is 15–60 mg. In general, this is inhaled in a few successive breaths. The effects last for a short period of time, usually 5 to 15 minutes, dependent on the dose. The onset after inhalation is very fast (less than 45 seconds) and peak effects are reached within a minute. In the 1960s, DMT was known as a "businessman's trip" in the US because of the relatively short duration (and rapid onset) of action when inhaled.
Injection.
Injected DMT produces an experience that is similar to inhalation in duration, intensity, and characteristics.
In a study conducted from 1990 through 1995, University of New Mexico psychiatrist Rick Strassman found that some volunteers injected with high doses of DMT reported experiences with perceived alien entities. Usually, the reported entities were experienced as the inhabitants of a perceived independent reality the subjects reported visiting while under the influence of DMT. In a September 2009 interview with Examiner.com, Strassman described the effects on participants in the study: "Subjectively, the most interesting results were that high doses of DMT seemed to allow the consciousness of our volunteers to enter into non-corporeal, free-standing, independent realms of existence inhabited by beings of light who oftentimes were expecting the volunteers, and with whom the volunteers interacted. While 'typical' near-death and mystical states occurred, they were relatively rare."
Oral ingestion.
DMT is broken down by the enzyme monoamine oxidase through a process called deamination, and is quickly inactivated orally unless combined with a monoamine oxidase inhibitor (MAOI). The traditional South American beverage ayahuasca, or yage, is derived by boiling the ayahuasca vine ("Banisteriopsis caapi") with leaves of one or more plants containing DMT, such as "Psychotria viridis", "Psychotria carthagenensis", or "Diplopterys cabrerana". The Ayahuasca vine contains harmala alkaloids, highly active reversible inihibitors of monoamine oxidase A (RIMAs), rendering the DMT orally active by protecting it from deamination. A variety of different recipes are used to make the brew depending on the purpose of the ayahuasca session, or local availability of ingredients. Two common sources of DMT in the western US are reed canary grass ("Phalaris arundinacea") and Harding grass ("Phalaris aquatica"). These invasive grasses contain low levels of DMT and other alkaloids. In addition, Jurema ("Mimosa tenuiflora") shows evidence of DMT content: the pink layer in the inner rootbark of this small tree contains a high concentration of "N,N"-DMT.
Taken orally with an RIMA, DMT produces a long lasting (over 3 hour), slow, deep metaphysical experience similar to that of psilocybin mushrooms, but more intense. RIMAs should be used with caution as they can have lethal interactions with some prescription drugs such as SSRI antidepressants, and some over-the-counter drugs.
Induced DMT experiences can include profound time-dilation, visual and auditory illusions, and other experiences that, by most firsthand accounts, defy verbal or visual description. Some users report intense erotic imagery and sensations and utilize the drug in a ritual sexual context.
Detection in body fluids.
DMT may be measured in blood, plasma or urine using chromatographic techniques as a diagnostic tool in clinical poisoning situations or to aid in the medicolegal investigation of suspicious deaths. In general, blood or plasma DMT levels in recreational users of the drug are in the 10–30 μg/L range during the first several hours post-ingestion. Less than 0.1% of an oral dose is eliminated unchanged in the 24-hour urine of humans.
Effects.
Dependence liability.
The dependence potential of oral DMT and the risk of sustained psychological disturbance are minimal.
Physical.
According to a dose-response study, "dimethyltryptamine dose slightly elevated blood pressure, heart rate, pupil diameter, and rectal temperature, in addition to elevating blood concentrations of beta-endorphin, corticotropin, cortisol, and prolactin. Growth hormone blood levels rose equally in response to all doses of DMT, and melatonin levels were unaffected."
Conjecture.
Several speculative and yet untested hypotheses suggest that endogenous DMT is produced in the human brain and is involved in certain psychological and neurological states. DMT is naturally occurring in small amounts in rat brain, human cerebrospinal fluid, and other tissues of humans and other mammals. A biochemical mechanism for this was proposed by the medical researcher J. C. Callaway, who suggested in 1988 that DMT might be connected with visual dream phenomena: brain DMT levels would be periodically elevated to induce visual dreaming and possibly other natural states of mind. A role of endogenous hallucinogens including DMT in higher level sensory processing and awareness was proposed by J. V. Wallach based on a hypothetical role of DMT as a neurotransmitter. Neurobiologist Andrew R. Gallimore suggests that while DMT might not have a modern neural function, it may have been an ancestral neuromodulator once secreted in psychedelic concentrations during REM sleep - a function now lost.
Dr. Rick Strassman, while conducting DMT research in the 1990s at the University of New Mexico, advanced the controversial hypothesis that a massive release of DMT from the pineal gland prior to death or near death was the cause of the near death experience (NDE) phenomenon. Several of his test subjects reported audio or visual hallucinations. His explanation for this was the possible lack of panic involved in the clinical setting and possible dosage differences between those administered and those encountered in actual NDE cases. Several subjects also reported contact with "other beings", alien like, insectoid or reptilian in nature, in highly advanced technological environments where the subjects were "carried", "probed", "tested", "manipulated", "dismembered", "taught", "loved" and "raped" by these "beings". Basing his reasoning on his belief that all the enzymatic material needed to produce DMT is found in the pineal gland, and moreover in substantially greater concentrations than in any other part of the body, Strassman has speculated that DMT is made in the pineal gland( p. 69) .
In the 1950s, the endogenous production of psychoactive agents was considered to be a potential explanation for the hallucinatory symptoms of some psychiatric diseases; this is known as the transmethylation hypothesis.
In 2011, Nicholas V. Cozzi, of the University of Wisconsin School of Medicine and Public Health, concluded that INMT, an enzyme that may be associated with the biosynthesis of DMT and endogenous hallucinogens, is present in the primate (rhesus macaque) pineal gland, retinal ganglion neurons, and spinal cord.
Legal status.
International law.
DMT is classified as a Schedule I drug under the UN 1971 Convention on Psychotropic Substances, meaning that use of DMT is supposed to be restricted to scientific research and medical use and international trade in DMT is supposed to be closely monitored. Natural materials containing DMT, including ayahuasca, are explicitly not regulated under the 1971 Psychotropic Convention.
By country.
Australia.
Between 2011 and 2012, the Australian Federal Government was considering changes to the Australian Criminal Code that would classify any plants containing any amount of DMT as "controlled plants". DMT itself was already controlled under current laws. The proposed changes included other similar blanket bans for other substances, such as a ban on any and all plants containing Mescaline or Ephedrine. The proposal was not pursued after political embarrassment on realisation that this would make the official Floral Emblem of Australia, Acacia pycnantha (Golden Wattle), illegal. The Therapeutic Goods Administration and federal authority had considered a motion to ban the same, but this was withdrawn in May 2012 (as DMT may still hold potential entheogenic value to native and/or religious people).
DMT is listed as a Schedule 9 prohibited substance in Australia under the Poisons Standard (October 2015). A schedule 9 drug is outlined in the Poisons Act 1964 as "Substances which may be abused or misused, the manufacture, possession, sale or use of which should be prohibited by law except when required for medical or scientific research, or for analytical, teaching or training purposes with approval of the CEO."
Under the Misuse of Drugs act 1981 6.0g of DMT is considered enough to determine a court of trial and 2.0g is considered intent to sell and supply.
Canada.
DMT is classified in Canada as a Schedule III drug under the Controlled Drugs and Substances Act.
France.
DMT, along with most of its plant sources, is classified in France as a "stupéfiant" (narcotic).
New Zealand.
DMT is classified in New Zealand as a Class A drug under the Misuse of Drugs Act 1975.
United Kingdom.
DMT is classified in the United Kingdom as a Class A drug.
United States.
DMT is classified in the United States as a Schedule I drug under the Controlled Substances Act of 1970.
In December 2004, the Supreme Court lifted a stay, thereby allowing the Brazil-based União do Vegetal (UDV) church to use a decoction containing DMT in their Christmas services that year. This decoction is a tea made from boiled leaves and vines, known as hoasca within the UDV, and ayahuasca in different cultures. In "Gonzales v. O Centro Espirita Beneficente Uniao do Vegetal", the Supreme Court heard arguments on November 1, 2005, and unanimously ruled in February 2006 that the U.S. federal government must allow the UDV to import and consume the tea for religious ceremonies under the 1993 Religious Freedom Restoration Act.
In September 2008, the three Santo Daime churches filed suit in federal court to gain legal status to import DMT-containing ayahuasca tea. The case, "Church of the Holy Light of the Queen v. Mukasey", presided over by Judge Owen M. Panner, was ruled in favor of the Santo Daime church. As of March 21, 2009, a federal judge says members of the church in Ashland can import, distribute and brew ayahuasca. U.S. District Judge Owen Panner issued a permanent injunction barring the government from prohibiting or penalizing the sacramental use of "Daime tea". Panner's order said activities of The Church of the Holy Light of the Queen are legal and protected under freedom of religion. His order prohibits the federal government from interfering with and prosecuting church members who follow a list of regulations set out in his order.

</doc>
<doc id="8750" url="https://en.wikipedia.org/wiki?curid=8750" title="Da capo">
Da capo

Da capo, , is a musical term in Italian, meaning "from the beginning" (literally "from the head"). It is often abbreviated D.C. It is a composer or publisher's directive to repeat the previous part of music, often used to save space. In small pieces this might be the same thing as a repeat, but in larger works D.C. might occur after one or more repeats of small sections, indicating a return to the very beginning. The resulting structure of the piece is generally in ternary form. Sometimes the composer describes the part to be repeated, for example: "Menuet da capo". In opera, where an aria of this structure is called a da capo aria, the repeated section is often adorned with grace notes.
Variations of the direction are:
D.C. al Coda is a musical direction used in sheet music. It literally means, "da Capo al Coda," or "from the head to the tail". It directs the musician to go back and repeat the music from the beginning ("Capo"), and to continue playing until one reaches the first coda symbol. Upon reaching the first coda, one is to skip to the second coda symbol (which signifies the ending of the piece), and continue playing until the end. The portion of the piece from the second coda to the end is often referred to as the "coda" of the piece, or quite literally as the "end". This may also be instructed by simply using the words al Coda after which the musician is to skip to the written word Coda.

</doc>
<doc id="8751" url="https://en.wikipedia.org/wiki?curid=8751" title="Dominatrix">
Dominatrix

A dominatrix (plural "dominatrixes" or "dominatrices") or mistress is a woman who takes the dominant role in BDSM activities.
A dominatrix might be heterosexual, homosexual, or bisexual, but her orientation does not necessarily limit the genders of her submissive partners. The role of a dominatrix may not even involve physical pain toward the submissive; her domination can be verbal, involving humiliating tasks, or servitude. A dominatrix may be a paid professional ("pro-domme"), or may use the title of dominatrix in her personal sex life.
The term "domme" is a coined pseudo-French female variation of the slang "dom" (short for "dominant"). The use of "domme", "dominatrix", "dom", or "dominant" by any woman in a dominant role is chosen mostly by personal preference and the conventions of the local BDSM scene.
As fetish culture is increasingly becoming more prevalent in Western media, depictions of dominatrices in film and television have become more common.
Etymology.
"Dominatrix" is the feminine form of the Latin "dominator", a ruler or lord, and was originally used in a non-sexual sense. Its use in English dates back to at least 1561. Its earliest recorded use in the prevalent modern sense, as a female dominant in S&M, dates to 1967. It was initially coined to describe a woman who provides punishment-for-pay as one of the case studies within Bruce Roger's pulp paperback "The Bizarre Lovemakers". The term was taken up shortly after by the Myron Kosloff title "Dominatrix" (with art by Eric Stanton) in 1968, and entered more popular mainstream knowledge following the 1976 film "Dominatrix Without Mercy".
Although the term "dominatrix" was not used, the classic example in literature of the female dominant-male submissive relationship is portrayed in the 1870 novella "Venus in Furs" by Austrian writer Leopold von Sacher-Masoch. The term masochism was later derived from the author's name by Richard von Krafft-Ebing in the latter's 1886 forensic study "Psychopathia Sexualis".
History.
The history of the dominatrix is argued to date back to rituals of the Goddess Inanna (or Ishtar as she was known in Akkadian), in ancient Mesopotamia. Ancient cuneiform texts consisting of "Hymns to Inanna" have been cited as examples of the archetype of powerful, sexual female displaying dominating behaviors and forcing Gods and men into submission to her. Archaeologist and historian Anne O. Nomis notes that Inanna's rituals included cross-dressing of cult personnel, and rituals "imbued with pain and ecstasy, bringing about initiation and journeys of altered consciousness; punishment, moaning, ecstasy, lament and song, participants exhausting themselves with weeping and grief."
In the secular era, the profession appears to have originated as a specialization within brothels, before becoming its own unique craft. As far back as the 1590s, flagellation within an erotic setting is recorded. The profession features in erotic prints of the era, such as the British Museum mezzotint "The Cully Flaug'd" (c. 1674–1702), and in accounts of forbidden books which record the flogging schools and the activities practised.
Within the 18th Century, female "Birch Disciplinarians" advertised their services in a book masked as a collection of lectures or theatrical plays, entitled "Fashionable Lectures" (c1761). This included the names of 57 women, some actresses and courtesans, who catered to birch discipline fantasies, keeping a room with rods and cat o' nine tails, and charging their clients a Guinea for a "lecture".
The 19th Century is characterised by what historian Anne O. Nomis characterises as the "Golden Age of the Governess". No fewer than twenty establishments were documented as having existed by the 1840s, supported entirely by flagellation practices and known as "Houses of Discipline" distinct from brothels. Amongst the well-known "dominatrix governesses" were Mrs Chalmers, Mrs Noyeau, the late Mrs Jones of Hertford Street and London Street, the late Mrs Theresa Berkley, Bessy Burgess of York Square and Mrs Pyree of Burton Cres. The most famous of these Governess "female flagellants" was Theresa Berkley, who operated her establishment on Charlotte Street in the central London district of Marylebone. She is recorded to have used implements such as whips, canes and birches, to chastise and punish her male clients, as well as the Berkley Horse, a specially designed flogging machine, and a pulley suspension system for lifting them off the floor. Such historical use of corporal punishment and suspension, in a setting of domination roleplay, connects very closely to the practices of modern-day professional dominatrices.
The "bizarre style" (as it came to be called) of leather catsuits, claws, tail whips, and latex rubber only came about in the 20th Century, initially within commercial fetish photography, and taken up by dominatrices. Within the mid-20th Century, dominatrices operated in a very discreet and underground manner, which has made them difficult to trace within the historical record. A few photographs still exist of the women who rang their domination businesses in London, New York, The Hague and the Herbertstraße, predominantly in sepia and black-and-white photographs, and scans from magazine articles, copied and re-copied. Amongst these were Miss Doreen of London who was acquainted with John Sutcliffe of AtomAge fame, whose clients reportedly included Britain's top politicians and businessmen. In New York, the dominatrix Anne Laurence was known within the underground circle of acquaintances during the 1950s, with Monique Von Cleef arriving in the early 1960s, and hitting national headlines when her home was raided by police detectives on December 22, 1965. Von Cleef went on to set up her "House of Pain" in The Hague in the 1970s, which became one of the world capitals for dominatrices, reportedly with visiting lawyers, ambassadors, diplomats and politicians.
Professional "vs." personal.
The term "dominatrix" is sometimes used to describe a professional dominant (or "pro-domme") who is paid to engage in BDSM with a submissive. An appointment or roleplay is referred to as a "session", and is often conducted in a dedicated professional play space which has been set up with specialist equipment, such as a "dungeon". In the contemporary era of technological connectivity, sessions may also be conducted remotely by phone, email or online chat.
Women who engage in female domination typically promote and title themselves under the terms "dominatrix", "mistress", "lady", "madame", "herrin" or "goddess". A study of German dominatrices by Andrew Wilson has noted the trend for dominatrices choosing names aimed at creating and maintaining an atmosphere in which class, femininity and mystery are key elements of their self-constructed identity.
Professional dominatrices may or may not offer sexual intercourse and other intimate sexual activities as part of their service to clients. The Canadian dominatrix Terri-Jean Bedford, who was one of three women who initiated an application in the Ontario Superior Court seeking invalidation of Canada's laws regarding brothels, sought to differentiate for clarity her occupation as a dominatrix rather than a prostitute to the media, due to frequent misunderstanding and conflation by the public of the two terms.
While dominatrices come from many different backgrounds, it has been noted that a considerable number are very well-educated, with a recent survey of New York dominatrices revealing that 39% had attended graduate school / university, including well-regarded institutions such as Columbia University.
Professional dominatrices can be seen advertising their services online and in print publications which carry erotic services advertising. The precise number of women actively offering professional domination services is unknown. Most professional dominatrices practice in large metropolitan cities such as New York, Los Angeles, and London, with as many as 200 women working as dominatrices in Los Angeles.
Professional dominatrices may take pride or differentiation in their psychological insight into their clients' fetishes and desires, as well as their technical ability to perform complex BDSM practices, such as Japanese shibari and other forms of bondage, suspension, torture roleplay, and corporal punishment, and other such practices which require a high degree of knowledge and competency to safely oversee. From a sociological point of view, Danielle Lindemann has noted the "embattled purity regime" in which many Pro-Dommes emphasise their specialist knowledge and professional skills, while distancing themselves from economic criteria for success, in a way which is comparable to avant-garde artists. 
To differentiate women who identify as a dominatrix but do not offer paid services, non-professional dominants are occasionally referred to as a "lifestyle" dominatrix or Mistress. It should be noted that the term "lifestyle" to signify BDSM is occasionally a contention topic in the BDSM community and that some dominatrices may dislike the term. Some professional dominatrices are also "lifestyle" dominatrices - i.e., in addition to paid sessions with submissive clients they engage in unpaid recreational sessions or may incorporate power exchange within their own private lives and relationships. However it is worth noting that the term has fallen out of general usage with respect to women who are dominant in their private relationships, and has taken on more and more, the connotation of "professional."
Imagery.
The dominatrix is a female archetype which operates on a symbolic mode of representation, associated with particular attire and props that are drawn on within popular culture to signify her role—as a strong, dominant, sexualised woman—linked to but distinct from images of sexual fetish.
One of the ubiquitous garments associated with the dominatrix is the catsuit. Historically, the black leather female catsuit entered dominant fetish culture in the 1950s with the "AtomAge" magazine and its connections to fetish fashion designer John Sutcliffe. The spill-over into mainstream culture, occurred with catsuits being worn by strong female protagonists in popular 1960s TV programs like "The Avengers", and in the comic super-heroines such as Catwoman, in which the catsuit represented the independent woman capable of "kick-ass" moves and antics, enabling complete freedom of movement. On another level, the one-piece catsuit accentuated and exaggerated the sexualized female form, providing visual access to a woman's body, while simultaneously obstructing physical penetrative access. "You can look but you can't touch" is the mechanism of this operation, which plays upon the BDSM practice known as "tease and denial".
Other common signifying footwear of the dominatrix are thigh-high boots, in leather or shiny PVC, which have long held a fetishistic status, along with the very high stiletto heel. Fishnet stockings, seamed hosiery, suspender belts and garter stockings are also popular accents in the representation and attire of dominatrices, to emphasize the form and length of their legs, with erotic connotation.
Tight, leather corsets are another staple garment of the dominatrix signification. Gloves, whether long opera gloves or fingerless gloves, are often a further accessory to emphasize the feminine role.
Materials such as PVC, leather and rubber latex, are amongst the most common to immediately take on the signifying work of fetish attire. The body language of the dominatrix is frequently represented by the use of strong, dominant body-language which is comparable to the dominant posturing in the animal world. The props she may brandish will strongly signify her role as dominatrix, such as bearing a flogger whip or riding crop as illustrated in the artwork of Bruno Zach in the early 20th century, in conventional representation.
Practicing professional dominatrices may draw their attire from the conventional signifiers of the role, or adapt them to create their own individual style, where there exists a potential pull—between meeting conventional expectations, and a desire for dominant independent self-expression.
Some contemporary dominatrices draw upon an eclectic range of strong female archetypes, including the goddess, the female superheroine, the femme fatale, the priestess, the empress, the queen, the governess, the KGB secret agent, to their own ends.

</doc>
<doc id="8752" url="https://en.wikipedia.org/wiki?curid=8752" title="Flag of Denmark">
Flag of Denmark

The Flag of Denmark ( ) is red with a white Scandinavian cross that extends to the edges of the flag; the vertical part of the cross is shifted to the hoist side. The cross design, which represents Christianity, was subsequently adopted by other Nordic countries: Sweden, Norway, Finland, Iceland, and the Faroe Islands, as well as the British archipelagos of Shetland and Orkney. During the Danish-Norwegian personal union, Dannebrog ("Danish cloth") was also the flag of Norway and continued to be, with slight modifications, until Norway adopted its current flag in 1821.
The design of the Dannebrog is recorded on a seal from 1397. According to legend, the flag came into Danish possession during the Battle of Lyndanisse in 1219. The Danes were on a failing crusade in Estonia, but after praying to God a flag fell from the sky. After this event, Danish King Valdemar II went on to defeat the Estonians. The first recorded use of the flag appears one hundred years later.
Legendary origin.
The legend states the origin of the flag to the Battle of Lyndanisse, also known as the Battle of Valdemar (Danish: ""Volmerslaget""), near Lyndanisse (Tallinn) in Estonia, on June 15, 1219.
The battle was going badly, and defeat seemed imminent. However the Danish Bishop Anders Sunesen on top of a hill overlooking the battle prayed to God with his arms raised, which meant that the Danes moved closer to victory the more he prayed. When he raised his arms the Danes surged forward and when his arms grew tired and he let them fall, the Estonians turned the Danes back. Attendants rushed forward to raise his arms once again and the Danes surged forward again. At a second he was so tired in his arms that he dropped them and the Danes then lost the advantage and were moving closer to defeat. He needed two soldiers to keep his hands up and when the Danes were about to lose, 'Dannebrog' miraculously fell from the sky and the King took it, showed it to the troops and their hearts were filled with courage and the Danes won the battle.
No historical record supports this legend. The first record of the legend dates from more than 300 years after the campaign, and the first record connects the legend to a much smaller battle, though still in Estonia; the battle of Fellin (Viljandi) in 1208. Though no historical support exists for the flag story in the Fellin battle either, it is not difficult to understand how a small and unknown place is replaced with the much grander battle of Reval (Tallinn) from the Estonia campaign of King Valdemar II.
This story originates from two written sources from the early 16th century.
The first is found in Christiern Pedersen's ""Danske Krønike"", which is a sequel to Saxo’s Gesta Danorum, written 1520–23. It is not mentioned in connection to the campaign of King Valdemar II in Estonia, but in connection with a campaign in Russia. He also mentions that this flag, falling from the sky during the Russian campaign of King Valdemar II, is the very same flag that King Eric of Pomerania took with him when he left the country in 1440 after being deposed as King.
The second source is the writing of the Franciscan monk Petrus Olai (Peder Olsen) of Roskilde, from 1527. This record describes a battle in 1208 near a place called ""Felin"" during the Estonia campaign of King Valdemar II. The Danes were all but defeated when a lamb-skin banner depicting a white cross falls from the sky and miraculously leads to a Danish victory. In another record by "Petrus Olai" called ""Danmarks Tolv Herligheder"" (Twelve Splendours of Denmark), in splendour number nine, the same story is re-told almost to the word; however, a paragraph has been inserted correcting the year to 1219.
Some historians believe that the story by "Petrus Olai" refers to a source from the first half of the 15th century, making this the oldest reference to the falling flag.
It is believed that the name of the capital of Estonia, Tallinn, came into existence after the battle. It is derived from "Taani linn", meaning "Danish town" in Estonian.
Continuation of the romantic legend.
According to tradition, the original flag from the Battle of Lyndanisse was used in the small campaign of 1500 when King Hans tried to conquer Dithmarschen (in western Holstein in north Germany). The flag was lost in a devastating defeat at the Battle of Hemmingstedt on 17 February 1500. In 1559, King Frederik II recaptured it during his own Dithmarschen campaign. In the capitulation terms it is stated that all Danish banners lost in 1500 were to be returned.
This legend is found in two sources, Hans Svaning's "History of King Hans" from 1558–1559 and Johan Rantzau's "History about the Last Dithmarschen War", from 1569.
Both claim that this was the original flag, and consequently both writers knew the legend of the falling flag. In 1576, the son of "Johan Rantzau", Henrik Rantzau, also writes about the war and the fate of the flag. He notes that the flag was in a poor condition when returned.
Sources from Dithmarschen, written shortly after the battle of 1500, do mention banners, including the Royal banner, being captured from the Danes, but there is no mention of "Dannebrog" or the "original" flag.
It is quite plausible that the king’s personal banner as well as the leading banner of the army were both lost, as the battle was led by the King himself. However, it is more questionable if he indeed was carrying the "original" flag.
In a letter dated 22 February 1500 to Oluf Stigsøn, King John describes the battle, but does not mention the loss of an important flag. In fact, the entire letter gives the impression that the lost battle was nothing more than an "unfortunate affair".
An indication that we are dealing with multiple flags, are the 1570 writings of Niels Hemmingsøn regarding a bloody battle between Danes and Swedes near the Swedish town of Uppsala in 1520. He writes that the "Danish head banner" (""Danmarckis Hoffuitbanner"") was nearly captured by the Swedes. It was saved only by the combined efforts of the banner-carrier Mogens Gyldenstierne, taking multiple wounds, and a young man coming to his rescue. This young man was Peder Skram. This ""Danmarckis Hoffuitbanner"" was probably nothing short of the ""Banner of the Realm'" ("Rigsbanner"), the "Dannebrog".
This is however not the end of the story. A priest and historian from Dithmarschen, Neocorus, wrote in 1598 that the banner captured in 1500, was brought to the church in Wöhrden and hung there for the next 59 years, until it was returned to the Danes as part of the peace settlement in 1559.
Henrik Rantzau states in his writing of 1576 that the flag was brought to Slesvig city and placed in the cathedral, following its return.
A historian from Slesvig, Ulrik Petersen (1656–1735), wrote in the late 17th century that the flag hung in "Slesvig" cathedral till about 1660 until it simply crumbled away, thus ending its more than 400-year-old story.
Historically, it is of course impossible to prove or disprove that these records speak of the same flag, or if the flag of 1208 or 1219 ever existed. Many of these legends are apparently built on earlier ones.
Historic use.
Caspar Paludan-Müller.
The Danish historian Caspar Paludan-Müller in 1873 in his book ""Sagnet om den himmelfaldne Danebrogsfane"" put forth the theory that it is a banner sent by the Pope to the Danish King to use in his crusades in the Baltic countries. Other kings and lords certainly received such banners.
One would imagine, though, that if this story were true, some kind of record ought to exist of the event, and presumably Danish historians would not have failed to mention it in some way. Being granted a banner by the Pope would have been a great honour, but despite the many letters of the popes relating to the crusades, none of them mentions granting a banner to a King of Denmark. On the other hand, the letter in question might simply have been lost.
Johan Støckel.
A similar theory was suggested by Danish explorer, adventurer and Captain Johan Støckel in the early 20th century. He suggested that it was not a papal banner to the King but a papal banner to the Churchly legate in the North, more specifically to archbishop Andreas Sunesøn, which he – without the knowledge of the King – brought with him on the King's crusade in the Baltic countries, in an effort to make the army take on a Christian symbol (over the king's symbol) and thereby strengthen the power of the church.
It is unlikely that the very fair and loyal archbishop would do such a thing behind the king's back. Moreover, it is unlikely that the pope would send such a banner, given the fact that they already had one, namely the banner of the Knights Hospitaller (Danish: "Johanitterne").
Adolf Ditlev Jørgensen.
A theory brought forth by the Danish historian Adolf Ditlev Jørgensen in 1875 in his book ""Danebroges Oprindelse"" is that the Danish flag "is" the banner of the Knights Hospitaller. He notes that the order came to Denmark in the latter half of the 12th century and during the next centuries spread to major cities, like Odense, Viborg, Horsens, Ribe and their headquarters in Slagelse, so by the time of the Baltic crusade, the symbol was already a known symbol in Denmark.
Furthermore, he claims that Bishop Theoderich, already co-initiator of the Livonian Brothers of the Sword in Livonia, had the idea of starting a similar order in Estonia; and that he was the original instigator of the inquiry from Bishop Albert of Buxhoeveden to King Valdemar II in 1218, that set the whole Danish participation in the Baltic crusades in motion.
In the contemporary writing of the priest Henry of Livonia from Riga it is said that Bishop Theoderich was killed during the 1219 battle, when the enemy stormed his tent, thinking it was the King's tent. Adolf Ditlev Jørgensen explains that it was Bishop Theoderich who carried the flag, planted outside his tent; thus as an already well-known symbol of the Knights Hospitaller in Livonia, the enemy thought this was the King's symbol and mistakenly stormed Bishop Theoderich tent. He claims that the origin of the legend of the falling flag comes from this confusion in the battle.
L. P. Fabricius.
The Danish church-historian L. P. Fabricius proposed yet another theory, explained in his study of 1934, titled ""Sagnet om Dannebrog og de ældste Forbindelser med Estland"". He ascribes the origin to the 1208 Battle of Fellin, not the Battle of Lyndanisse in 1219, based on the earliest source available about the story.
He says in this theory that it might have been Archbishop Andreas Sunesøn's personal ecclesiastical banner or perhaps even the flag of Archbishop Absalon, based on his tireless efforts to expand Christianity to the Baltic countries. Under his initiative and supervision several smaller crusades had already been conducted in Estonia. The banner would then already be known in Estonia. He repeats the story about the flag being planted in front of Bishop Theodorik's tent, which the enemy mistakenly attacks believing it to be the tent of the King.
All these theories centre on two battles in Estonia, Fellin (1208) or Lyndanisse (1219), and thus try to explain the origin in relation to the tale brought forth over 300 years after the event.
Fabricius and Helga Bruhn.
A much different theory is briefly discussed by "Fabricius" and elaborated more by Helga Bruhn in her book ""Dannebrog"" from 1949. She claims that it is neither the battle nor the banner that is central to the tale, but rather the cross in the sky. Similar tales of appearances in the sky at critical moments, particularly of crosses, can be found all over Europe.
Bruhn mentions a battle (also mentioned by "Fabricius") taking place on September 10, 1217 between Christian knights and Moor warriors on the Iberian Peninsula near the castle Alcazar, where it is said that a golden cross on white appeared in the sky, to bring victory to the Christians.
Likewise an almost identical Swedish tale from the 18th century about a golden cross on blue appearing in 1157 during a Swedish battle in Finland. Probably a later invention to counter the legendary origins of the Danish flags, but nevertheless of the same nature.
The English flag, the Saint George's Cross is also claimed to have appeared in the sky during a critical battle, in this case in Jerusalem during the crusades.
The similarities to the legends is obvious. In Spain, the colours of the Pope appears in the sky, in Finland the Swedish colours. In Estonia it is the Danish colours, and in Jerusalem the English colours. Basically, these are all variations of the same legend.
Since King Valdemar II of Denmark was married to the Portuguese princess, Berengária of Portugal, it is not unthinkable that the origin of the story, if not the flag, was the Spanish tale or a similar tale, which again might have been inspired by an even older legend.
Medieval use of the cross design in coats of arms.
Several coins, seals and images exist, both foreign and domestic, from the 13th to 15th centuries and even earlier, showing heraldic designs similar to Dannebrog, alongside the royal coat of arms (three blue lions on a golden shield.) This coat of arms remains in use to this day.
An obvious place to look for documentation is in the Estonian city of Tallinn, the site of the legendary battle. In Tallinn, a coat-of-arms resembling the flag is found on several buildings and can be traced back to the middle of the 15th century where it appears in the coat-of-arms of the "Die Grosse Gilde", a sort of merchant consortium which greatly influenced the city's development. The symbol later became the coat-of-arms of the city. Efforts to trace it from Estonia back to Denmark have, however, been in vain.
The national Coat of Arms of Estonia, three blue lions on a golden shield, is almost identical to the Coat of Arms of Denmark, and its origin can be traced directly back to King Valdemar II and Danish rule in Estonia 1219–1346.
Gelre armorial.
The earliest source that indisputably links the red flag with a white cross to a Danish King, and to the realm itself, is found in a Dutch armorial, the "Gelre Armorial" (Dutch: "Wapenboek Gelre"), written between 1340 and 1370 (some sources say 1378 or 1386). Most historians claim that the book was written by Geldre Claes Heinen. The book displays some 1,700 coats-of-arms from all over Europe, in colour. It is now located at the Royal Library of Brussels (the "Bibliothèque royale Albert Ier").
On page 55 verso we find the Danish coat-of-arms surmounted by a helmet with ermine-clad bison horns (). Behind the sinister horn is a lance tip with a banner, displaying a white cross on red. The text left of the coat of arms says "die coninc van denmarke" ("The King of Denmark"). This is the earliest known undisputed colour rendering of the Dannebrog.
This image has been used to acknowledge a previously disputed theory that the cross found in Valdemar Atterdag's coats of arms located in his Danælog seal ("Rettertingsseglet") from 1356 is indeed the cross from the Danish flag.
This image from the Armorial Gelre is nearly identical to an image found in a 15th-century coats of arms book now located in the National Archives of Sweden, ("Riksarkivet")
From the time of King Eric of Pomerania there is also a case that undisputedly links the Dannebrog to Denmark. His seal from 1398 as king of the Kalmar union displays the arms of Denmark chief dexter, three lions. In this version, the lions are holding a Danebrog banner. The cross quartering the shields has also been identified as a Dannebrog cross, but this claim is disputed. Since the seal represents all of the three realms of the union, it is more likely that the cross refers to the union banner that King Eric tried to introduce, a red cross on yellow.
Modern use.
As a civil ensign.
The size and shape of the civil ensign (""Koffardiflaget"") for merchant ships is given in the regulation of June 11, 1748, which says: "A red flag with a white cross with no split end. The white cross must be of the flag's height. The two first fields must be square in form and the two outer fields must be lengths of those".
The proportions are thus: 3:1:3 vertically and 3:1:4.5 horizontally. This definition are the absolute proportions for the Danish national flag to this day, for both the civil version of the flag (""Stutflaget""), as well as the merchant flag (""Handelsflaget""). Both flags are identical.
A somewhat curious regulation came in 1758 concerning Danish ships sailing in the Mediterranean. These had to carry the King's
cypher logo in the center of the flag, to distinguish them from Maltese ships, due to the similarity of the flag of the Order of St. John (also known as the Knights Hospitaller). To the best of knowledge, this regulation has never been revoked, however it is probably no longer done.
According to the regulation of June 11, 1748 the colour was simply red, which is common known today as "Dannebrog rød" (""Dannebrog red""). The only available red fabric dye in 1748 was made of madder root, which can be processed to produce a brilliant red dye (used historically for British soldiers' jackets). The private company, Dansk Standard, regulation number 359 of 2005, defines the red colour of the flag as Pantone 186c. No official nuance definition of "Dannebrog rød" exists.
During the next about 150 years nobody paid much attention to actually abide fully to the proportions of the flag given in the 1748 regulation, not even the government.
As late as 1892 it was stated in a series of regulations that the correct lengths of the two last fields in the flag were . Some interested in the matter made inquires into the issue and concluded that the length would make the flag look blunt. Any new flag would also quickly become unlawful, due to wear and tear. They also noted that the flag currently used had lengths, of the last two fields, anywhere between to . So in May 1893 a new regulation to all chiefs of police, stated that the police should not intervene, if the two last fields in the flag were longer than as long as these did not exceed , and provided that this was the only rule violated. This regulation is still in effect today and thus the legal proportions of the National flag is today anywhere between "3:1:3 width / 3:1:4.5 length" and "3:1:3 width / 3:1:5.25 length".
That some confusion still exists in this matter can be seen from the regulation of May 4, 1927, which once again states that Danish merchant ships have to fly flags according to the regulation of 1748.
Variants.
Splitflag and Orlogsflag.
The "Splitflag" and "Orlogsflag" have similar shapes but different sizes and shades of red. Legally, they are two different flags. The "Splitflag" is a Danish flag ending in a swallow-tail, it is "Dannebrog red", and is used on land. The "Orlogsflag" is an elongated "Splitflag" with a deeper red colour and is only used on sea.
The "Orlogsflag" with no markings, may only be used by the Royal Danish Navy. There are though a few exceptions to this. A few institutions have been allowed to fly the clean "Orlogsflag". The same flag with markings has been approved for a few dozen companies and institutions over the years.
Furthermore, the "Orlogsflag" is only described as such if it has no additional markings. Any swallow-tail flag, no matter the color, is called a "Splitflag" provided it bears additional markings.
The first regulation regarding the "Splitflag" dates from March 27, 1630, in which King Christian IV orders that Norwegian "Defensionskibe" (armed merchants ships) may only use the "Splitflag" if they are in Danish war service.
In 1685 an order, distributed to a number of cities in Slesvig, states that all ships must carry the Danish flag, and in 1690 all merchant ships are forbidden to use the "Splitflag", with the exception of ships sailing in the East Indies, West Indies and along the coast of Africa.
In 1741 it is confirmed that the regulation of 1690 is still very much in effect; that merchant ships may not use the "Splitflag". At the same time the Danish East India Company is allowed to fly the "Splitflag" when past the equator.
It is obvious that some confusion must have existed regarding the "Splitflag". In 1696 the Admiralty presented the King with a proposal for a standard regulating both size and shape of the "Splitflag". In the same year a royal resolution defines the proportions of the "Splitflag", which in this resolution is called "Kongeflaget" (the King's flag), as follows: "The cross must be of the flags height. The two first fields must be square in form with the sides three times the cross width. The two outer fields are rectangular and the length of the square fields. The tails are the length of the flag".
These numbers are the basic for the "Splitflag", or "Orlogsflag", today, though the numbers have been slightly altered. The term "Orlogsflag" dates from 1806 and denotes use in the Danish Navy.
From about 1750 to early 19th century a number of ships and companies which the government has interests in, received approval to use the "Splitflag". From the mid-19th century to 1899, and especially after 1870, additional institutions and private companies received approval to use the Splitflag.
In the royal resolution of October 25, 1939 for the Danish Navy, it is stated that the "Orlogsflag" is a "Splitflag" with a deep red (""dybrød"") or madder red (""kraprød"") colour. Like the National flag, no nuance is given, but in modern days this is given as 195U.
Furthermore, the size and shape is corrected in this resolution to be: "The cross must be of the flag's height. The two first fields must be square in form with the height of of the flag's height. The two outer fields are rectangular and the length of the square fields. The tails are the length of the rectangular fields".
Thus, if compared to the standard of 1696, both the rectangular fields and the tails have decreased in size.
Royal Standards.
The current version of the royal standard was introduced on 16 November 1972 when the Queen adopted a new version of her personal coat of arms. The royal standard is the flag of Denmark with a swallow-tail and charged with the monarch’s coat of arms set in a white square. The centre square is 32 parts in a flag with the ratio 56:107.
Superstitions.
Danish culture states that the Dannebrog is not allowed to touch the ground because it came from heaven. It also states that the Dannebrog is not allowed to be or remain hoisted at night, because it is said that such is to salute the Devil.
Other flags of the Kingdom of Denmark.
Greenland and the Faroe Islands are additional autonomous countries within the Kingdom of Denmark. These countries have their own official flags.
Regional Flags.
Some areas in Denmark have unofficial flags, listed below. The regional flags of Bornholm and Ærø are known to be in active use. The flags of Vendsyssel (Vendelbrog) and the Jutlandic flag ("Den jyske fane") are obscure. None of these flags have legal recognition in Denmark, and are officially considered to be "fantasy flags". Denmark reserves official recognition to official flags and regional flags ("områdeflag") from other jurisdictions.
The flag of Volyn Oblast, Ukraine is very similar to the Dannebrog.

</doc>
<doc id="8753" url="https://en.wikipedia.org/wiki?curid=8753" title="Dharma">
Dharma

Dharma (; "dharma", ; "dhamma") is a key concept with multiple meanings in the Indian religions Hinduism, Buddhism, Sikhism and Jainism. There is no single word translation for "dharma" in western languages.
In Hinduism, "dharma" signifies behaviours that are considered to be in accord with "rta", the order that makes life and universe possible, and includes duties, rights, laws, conduct, virtues and ‘‘right way of living’’. In Buddhism "dharma" means "cosmic law and order", but is also applied to the teachings of the Buddha. In Buddhist philosophy, "dhamma/dharma" is also the term for "phenomena". Dharma in Jainism refers to the teachings of "tirthankara" ("Jina") and the body of doctrine pertaining to the purification and moral transformation of human beings. For Sikhs, the word "dharm" means the "path of righteousness".
The Classical Sanskrit noun "dharma" is a derivation from the root "dhṛ", which has a meaning of "to hold, maintain, keep". The word "dharma" was already in use in the historical Vedic religion, and its meaning and conceptual scope has evolved over several millennia. The antonym of dharma is "adharma".
Etymology.
The Classical Sanskrit noun "dharma" is a derivation from the root "dhṛ", which means "to hold, maintain, keep", and takes a meaning of "what is established or firm", and hence "law". It is derived from an older Vedic Sanskrit "n"-stem "dharman-", with a literal meaning of "bearer, supporter", in a religious sense conceived as an aspect of Rta.
In the Rigveda, the word appears as an "n"-stem, ', with a range of meanings encompassing "something established or firm" (in the literal sense of prods or poles). Figuratively, it means "sustainer" and "supporter" (of deities). It is semantically similar to the Greek "ethos" ("fixed decree, statute, law"). In Classical Sanskrit, the noun becomes thematic: '.
The word "dharma" derives from Proto-Indo-European root "*dʰer-" ("to hold"), which in Sanskrit is reflected as class-1 root "√dhṛ". Etymologically it is related to Avestan √dar- ("to hold"), Latin "frēnum" ("rein, horse tack"), Lithuanian "derė́ti" ("to be suited, fit"), Lithuanian "dermė" ("agreement") and "darna" ("harmony") and Old Church Slavonic "drъžati" ("to hold, possess"). Classical Sanskrit word "dharmas" would formally match with Latin o-stem "firmus" from Proto-Indo-European *"dʰer-mo-s" "holding", were it not for its historical development from earlier Rigvedic n-stem.
In Classical Sanskrit, and in the Vedic Sanskrit of the Atharvaveda, the stem is thematic: "" (Devanāgarī: धर्म). In Pāli, it is rendered "dhamma". In some contemporary Indian languages and dialects it alternatively occurs as "dharm".
Definition.
Dharma is a concept of central importance in Indian philosophy and religion. It has multiple meanings in Hinduism, Buddhism, and Jainism. It is difficult to provide a single concise definition for "dharma", as the word has a long and varied history and straddles a complex set of meanings and interpretations. There is no equivalent single word translation for "dharma" in western languages.
There have been numerous, conflicting attempts to translate ancient Sanskrit literature with the word dharma into German, English and French. The concept, claims Paul Horsch, has caused exceptional difficulties for modern commentators and translators. For example, while Grassmann translation of Rig-veda identifies seven different meanings of dharma, Karl Friedrich Geldner in his translation of the Rig-veda employs 20 different translations for dharma, including meanings such as ‘law’, ‘order’, ‘duty’, ‘custom’, ‘quality’, ‘model’, among others.
Dharma root is "dhri", which means ‘to support, hold, or bear’. It is the thing that regulates the course of change by not participating in change, but that principle which remains constant. Monier-Williams, the widely cited resource for definitions and explanation of Sanskrit words and concepts of Hinduism, offers numerous definitions of the word "dharma": such as that which is established or firm, steadfast decree, statute, law, practice, custom, duty, right, justice, virtue, morality, ethics, religion, religious merit, good works, nature, character, quality, property. Yet, each of these definitions is incomplete, while combination of these translations do not convey the total sense of the word. In common parlance, "dharma" means ‘right way of living’ and ‘path of righteousness’.
The meaning of word “dharma” depends on the context, and its meaning evolved as ideas of Hinduism developed over its long history. In earliest texts and ancient myths of Hinduism, "dharma" meant cosmic law, the rules that created the universe from chaos, as well as rituals; In later Vedas, Upanishads, Puranas and the Epics, the meaning became refined, richer, complex and the word dharma was applied to diverse contexts.
In certain contexts, "dharma" designates human behaviours considered necessary for order of things in the universe, principles that prevent chaos, behaviours and action necessary to all life in nature, society, family as well as at the individual level. "Dharma" encompasses ideas such as duty, rights, character, vocation, religion, customs and all behaviour considered appropriate, correct or morally upright.
The antonym of "dharma" is "adharma" (Sanskrit: अधर्मा), meaning that which is “not dharma”. As with "dharma", the word "adharma" includes and implies many ideas; in common parlance, adharma means that which is against nature, immoral, unethical, wrong or unlawful.
In Buddhism, "dharma" incorporates the teachings and doctrines of the founder of Buddhism, the Buddha.
History.
According to the authoritative book "History of Dharmasastra", in the hymns of the Rigveda the word Dharma appears at least fifty-six times, as an adjective or noun. According to Paul Horsch, the word Dharma has its origin in the myths of Vedic Hinduism. The Brahman (whom all the gods make up), claim the hymns of the Rig Veda, created the universe from chaos, they hold (dhar-) the earth and sun and stars apart, they support (dhar-) the sky away and distinct from earth, and they stabilise (dhar-) the quaking mountains and plains. The gods, mainly Indra, then deliver and hold order from disorder, harmony from chaos, stability from instability - actions recited in the Veda with the root of word dharma. In hymns composed after the mythological verses, the word dharma takes expanded meaning as a cosmic principle and appears in verses independent of gods. It evolves into a concept, claims Paul Horsch, that has a dynamic functional sense in Atharvaveda for example, where it becomes the cosmic law that links cause and effect through a subject. Dharma, in these ancient texts, also takes a ritual meaning. The ritual is connected to the cosmic, and ‘‘dharmani’’ is equated to ceremonial devotion to the principles that gods used to create order from disorder, the world from chaos. Past the ritual and cosmic sense of dharma that link the current world to mythical universe, the concept extends to ethical-social sense that links human beings to each other and to other life forms. It is here that dharma as a concept of law emerges in Hinduism.
Dharma and related words are found in the oldest Vedic literature of Hinduism, in later Vedas, Upanishads, Puranas, and the Epics; the word dharma also plays a central role in the literature of other Indian religions founded later, such as Buddhism and Jainism. According to Brereton, "Dharman" occurs 63 times in Rig-veda; in addition, words related to Dharman also appear in Rig-veda, for example once as dharmakrt, 6 times as "satyadharman", and once as "dharmavant", 4 times as "dharman" and twice as "dhariman". There is no Iranian equivalent in old Persian for Dharma, suggesting the word dharman had origins in Indo-Aryan culture outside of Persia, or it is a concept that is indigenous to India. However, ideas in parts overlapping to "Dharma" are found in other ancient cultures: such as Chinese Tao, Egyptian Maat, Sumerian Me.
Eusebeia and dharma.
In mid 20th century, an inscription of the Indian Emperor Asoka from the year 258 BC was discovered in Afghanistan. This rock inscription contained Sanskrit, Aramaic and Greek text. According to Paul Hacker, on the rock appears a Greek rendering for the Sanskrit word dharma: the word eusebeia. Scholars of Hellenistic Greece explain eusebeia as a complex concept. Eusebia means not only to venerate gods, but also spiritual maturity, a reverential attitude toward life, and includes the right conduct toward one’s parents, siblings and children, the right conduct between husband and wife, and the conduct between biologically unrelated people. This rock inscription, concludes Paul Hacker, suggests dharma in India, about 2300 years ago, was a central concept and meant not only religious ideas, but ideas of right, of good, of one’s duty toward the human community.
Rta, Maya and Dharma.
The evolving literature of Hinduism linked "Dharma" to two other important concepts: "Ṛta" and "Māyā". Ṛta in Vedas is the truth and cosmic principle which regulates and coordinates the operation of the universe and everything within it. Māyā in Rig-veda and later literature means illusion, fraud, deception, magic that misleads and creates disorder, thus is contrary to reality, laws and rules that establish order, predictability and harmony. Paul Horsch suggests Ṛta and Dharma are parallel concepts, the former being a cosmic principle, the latter being of moral social sphere; while Māyā and Dharma are also analogous concepts, the former being that which corrupts law and moral life, the later being that which strengthens law and moral life.
Day proposes Dharma is a manifestation of Ṛta, but suggests Ṛta may have been subsumed into a more complex concept of Dharma, as the idea developed in ancient India over time in a nonlinear manner. The following verse from the Rigveda is an example where "rta" and dharma are linked:
Hinduism.
Dharma in Hinduism, is an organising principle that applies to human beings in solitude, in their interaction with human beings and nature, as well as between inanimate objects, to all of cosmos and its parts. It refers to the order and customs which make life and universe possible, and includes behaviours, rituals, rules that govern society, and ethics. Hindu dharma includes the religious duties, moral rights and duties of each individual, as well as behaviours that enable social order, right conduct, and those that are virtuous. Dharma, according to Van Buitenen, is that which all existing beings must accept and respect to sustain harmony and order in the world. It is neither the act nor the result, but the natural laws that guide the act and create the result to prevent chaos in the world. It is innate characteristic, that makes the being what it is. It is, claims Van Buitenen, the pursuit and execution of one’s nature and true calling, thus playing one’s role in cosmic concert. In Hinduism, it is the dharma of the bee to make honey, of cow to give milk, of sun to radiate sunshine, of river to flow. In terms of humanity, dharma is the need for, the effect of and essence of service and interconnectedness of all life.
Dharma in Vedas and Upanishads.
The history section of this article discusses the development of dharma concept in Vedas. This development continued in the Upanishads and later ancient scripts of Hinduism. In Upanishads, the concept of dharma continues as universal principle of law, order, harmony, and truth. It acts as the regulatory moral principle of the Universe. It is explained as law of righteousness and equated to "satya" (Sanskrit: सत्यं, truth), in hymn 1.4.14 of Brhadaranyaka Upanishad, as follows:
Dharma in the Epics.
The Hindu religion and philosophy, claims Daniel Ingall, places major emphasis on individual practical morality. In the Sanskrit epics, this concern is omnipresent.
In the Second Book of Ramayana, for example, a peasant asks the King to do what dharma morally requires of him, the King agrees and does so even though his compliance with the law of dharma costs him dearly. Similarly, dharma is at the centre of all major events in the life of Rama, Sita, and Lakshman in Ramayana, claims Daniel Ingall. Each episode of Ramayana presents life situations and ethical questions in symbolic terms. The issue is debated by the characters, finally the right prevails over wrong, the good over evil. For this reason, in Hindu Epics, the good, morally upright, law-abiding king is referred to ‘‘dharmaraja’’.
In Mahabharata, the other major Indian epic, similarly, dharma is central, and it is presented with symbolism and metaphors. Near the end of the epic, the god Yama, referred to as Dharma in the text, is portrayed as taking the form of a dog to test the compassion of Yudishthira, who is told he may not enter paradise with such an animal, but refuses to abandon his companion, for which decision he is then praised by Dharma. The value and appeal of the Mahabharata is not as much in its complex and rushed presentation of metaphysics in the 12th book, claims Ingall, because Indian metaphysics is more eloquently presented in other Sanskrit scriptures; the appeal of Mahabharata, like Ramayana, is in its presentation of a series of moral problems and life situations, to which there are usually three answers given, according to Ingall: one answer is of Bhima, which is the answer of brute force, an individual angle representing materialism, egoism, and self; the second answer is of Yudhishthira, which is always an appeal to piety and gods, of social virtue and of tradition; the third answer is of introspective Arjuna, which falls between the two extremes, and who, claims Ingall, symbolically reveals the finest moral qualities of man. The Epics of Hinduism are a symbolic treatise about life, virtues, customs, morals, ethics, law, and other aspects of Dharma. There is extensive discussion of Dharma at the individual level in the Epics of Hinduism, observes Ingall; for example, on free will versus destiny, when and why human beings believe in either, ultimately concluding that the strong and prosperous naturally uphold free will, while those facing grief or frustration naturally lean towards destiny. The Epics of Hinduism illustrate various aspects of Dharma, they are a means of communicating Dharma with metaphors.
Dharma according to 4th century Vatsyayana.
According to Klaus Klostermaier, 4th century Hindu scholar Vātsyāyana explained dharma by contrasting it with adharma. Vātsyāyana suggested that Dharma is not merely in one’s actions, but also in words one speaks or writes, and in thought. According to Vātsyāyana:
Dharma according to Patanjali Yoga.
In the Yoga system the dharma is real ; in the Vedanta it is unreal.
Dharma is part of yoga, suggests Patanjali; the elements of Hindu Dharma are the attributes, qualities and aspects of yoga. Patanjali explained dharma in two categories: yama (restraints) and niyama (observances).
The five yama, according to Patanjali, are: abstain from injury to all living creatures (ahimsa), abstain from falsehood (satya), abstain from unauthorised appropriation of things-of-value from another (acastrapurvaka), abstain from coveting or sexually cheating on your partner, and abstain from expecting or accepting gifts from others. The five yama apply in action, speech and mind. In explaining yama, Patanjali clarifies that certain professions and situations may require qualification in conduct. For example, a fisherman must injure a fish, but he must attempt to do this with least trauma to fish and the fisherman must try to injure no other creature as he fishes.
The five niyama (observances) are cleanliness by eating pure food and removing impure thoughts (such as arrogance or jealousy or pride), contentment in one’s means, meditation and silent reflection regardless of circumstances one faces, study and pursuit of historic knowledge, and devotion of all actions to the Supreme Teacher to achieve perfection of concentration.
Sources of Dharma.
Dharma is an empirical and experiential inquiry for every man and woman, according to some texts of Hinduism. For example, Apastamba Dharmasutra states:
In other texts, three sources and means to discover Dharma in Hinduism are described. These, according to Paul Hacker, are: First, learning historical knowledge such as Vedas, Upanishads, the Epics and other Sanskrit literature with the help of one’s teacher. Second, observing the behavior and example of good people. The third source applies when neither one’s education nor example exemplary conduct is known. In this case, ‘‘atmatusti’’ is the source of dharma in Hinduism, that is the good person reflects and follows what satisfies his heart, his own inner feeling, what he feels driven to.
Dharma, life stages and social stratification.
Some texts of Hinduism outline "Dharma" for society and at the individual level. Of these, the most cited one is "Manusmriti", which describes the four "Varnas", their rights and duties. Most texts of Hinduism, however, discuss "Dharma" with no mention of "Varna" (caste). Other Dharma texts and Smritis differ from Manusmriti on the nature and structure of Varnas. Yet, other texts question the very existence of varna. Bhrigu, in the Epics, for example, presents the theory that dharma does not require any varnas. In practice, medieval India is widely believed to be a socially stratified society, with each social strata inheriting a profession and being endogamous. Varna was not absolute in Hindu Dharma; individuals had the right to renounce and leave their Varna, as well as their asramas of life, in search of moksa. While neither Manusmriti nor succeeding Smritis of Hinduism ever use the word varnadharma (that is, the dharma of varnas), or varnasramadharma (that is, the dharma of varnas and asramas), the scholarly commentary on Manusmriti use these words, and thus associate dharma with varna system of India. In 6th century India, even Buddhist kings called themselves ‘protectors of varnasramadharma’ - that is, dharma of varna and asramas of life.
At the individual level, some texts of Hinduism outline four āśramas, or stages of life as individual’s dharma. These are: (1) brahmacārya, the life of preparation as a student, (2) gṛhastha, the life of the householder with family and other social roles, (3) vānprastha or aranyaka, the life of the forest-dweller, transitioning from worldly occupations to reflection and renunciation, and (4) sannyāsa, the life of giving away all property, becoming a recluse and devotion to moksa, spiritual matters.
The four stages of life complete the four human strivings in life, according to Hinduism. Dharma enables the individual to satisfy the striving for stability and order, a life that is lawful and harmonious, the striving to do the right thing, be good, be virtuous, earn religious merit, be helpful to others, interact successfully with society. The other three strivings are Artha - the striving for means of life such as food, shelter, power, security, material wealth, etc.; Kama - the striving for sex, desire, pleasure, love, emotional fulfillment, etc.; and Moksa - the striving for spiritual meaning, liberation from life-rebirth cycle, self-realisation in this life, etc. The four stages are neither independent nor exclusionary in Hindu Dharma.
Dharma and poverty.
Dharma while being necessary for individual and society, is dependent on poverty and prosperity in a society, according to Hindu Dharma scriptures. For example, according to Adam Bowles, Shatapatha Brahmana 11.1.6.24 links social prosperity and "Dharma" through water. Waters come from rains, it claims; when rains are abundant there is prosperity on the earth, and this prosperity enables people to follow Dharma - moral and lawful life. In times of distress, of drought, of poverty, everything suffers including relations between human beings and the human ability to live according to Dharma.
In Rajadharmaparvan 91.34-8, the relationship between poverty and dharma reaches a full circle. A land with less moral and lawful life suffers distress, and as distress rises it causes more immoral and unlawful life, which further increases distress. Those in power must follow the raja dharma (that is, dharma of rulers), because this enables the society and the individual to follow dharma and achieve prosperity.
Dharma and law.
The notion of "Dharma" as duty or propriety is found in India's ancient legal and religious texts. In Hindu philosophy, justice, social harmony, and happiness requires that people live per dharma. The Dharmashastra is a record of these guidelines and rules. The available evidence suggest India once had a large collection of dharma related literature (sutras, shastras); four of the sutras survive and these are now referred to as Dharmasutras. Along with laws of Manu in Dharmasutras, exist parallel and different compendium of laws, such as the laws of Narada and other ancient scholars. These different and conflicting law books are neither exclusive, nor do they supersede other sources of Dharma in Hinduism. These Dharmasutras include instructions on education of the young, their rites of passage, customs, religious rites and rituals, marital rights and obligations, death and ancestral rites, laws and administration of justice, crimes, punishments, rules and types of evidence, duties of a king, as well as morality.
Buddhism.
In Buddhism "dharma" means cosmic law and order, but is also applied to the teachings of the Buddha. In Buddhist philosophy, "dhamma/dharma" is also the term for "phenomena": In East Asia, the translation for dharma is 法, pronounced "fǎ" in Mandarin, "choe ཆོས་" in Tibetan, "beop" in Korean, "hō" in Japanese, and "pháp" in Vietnamese. However, the term dharma can also be transliterated from its original form.
Buddha's teachings.
For practicing Buddhists, references to "dharma" ("dhamma" in Pali) particularly as "the Dharma", generally means the teachings of the Buddha, commonly known throughout the East as Buddha-Dharma. It includes especially the discourses on the fundamental principles (such as the Four Noble Truths and the Noble Eightfold Path), as opposed to the parables and to the poems.
The status of Dharma is regarded variably by different Buddhist traditions. Some regard it as an ultimate truth, or as the fount of all things which lies beyond the "three realms" (Sanskrit: "tridhatu") and the "wheel of becoming" (Sanskrit: "bhavacakra"), somewhat like the pagan Greek and Christian logos: this is known as "Dharmakaya" (Sanskrit). Others, who regard the Buddha as simply an enlightened human being, see the Dharma as the essence of the "84,000 different aspects of the teaching" (Tibetan: "chos-sgo brgyad-khri bzhi strong") that the Buddha gave to various types of people, based upon their individual propensities and capabilities.
Dharma refers not only to the sayings of the Buddha, but also to the later traditions of interpretation and addition that the various schools of Buddhism have developed to help explain and to expand upon the Buddha's teachings. For others still, they see the Dharma as referring to the "truth," or the ultimate reality of "the way that things really are" (Tib. Cho).
The Dharma is one of the Three Jewels of Buddhism in which practitioners of Buddhism seek refuge, or that upon which one relies for his or her lasting happiness. The Three Jewels of Buddhism are the Buddha, meaning the mind's perfection of enlightenment, the Dharma, meaning the teachings and the methods of the Buddha, and the Sangha, meaning the monastic community who provide guidance and support to followers of the Buddha.
Buddhist phenomenology.
Other uses include dharma, normally spelled with a small "d" (to differentiate), which refers to a "phenomenon" or "constituent factor" of human experience. This was gradually expanded into a classification of constituents of the entire material and mental world. Rejecting the substantial existence of permanent entities which are qualified by possibly changing qualities, Buddhist Abhidharma philosophers enumerated lists of dharmas which varied by school. They came to propound that these "constituent factors" are the only type of entity that truly exists (and only some thinkers gave dharmas this kind of existence). This notion is of particular importance for the analysis of human experience: Rather than assuming that mental states inhere in a cognising subject, or a soul-substance, Buddhist philosophers largely propose that mental states alone exist as "momentary elements of consciousness" and that a subjective perceiver is assumed.
The Buddha recognized relative identity of individuals, who have a definite name, origin and life-span 
He confirmed authorship of deeds and the responsibility of agents 
In particular, the essential practice of training monks' discipline which includes rebuke, confession, forfeiture and expiation of transgressions presupposes a continuing decision-maker. Conscious choice (Sanskrit: cetayita karaṇa) has effects according to the law of karma. The Buddhist understanding of human will and action may be called a kind of compatibilism 
In Abhidharma metaphysics, besides physical and mental elements there are assumed functions accomplishing individuals (Sanskrit: saṁskṛta-lakṣaṇa). These dharmas condition phases of individual history: origin, stay, decline, and disintegration. Among these functions, Vasubandhu distinguished aprāpti or pŗthagjanatvaṁ, namely that forming humans (Sanskrit: pŗthagjana, "common man").
One of the central tenets of Buddhism, is the denial of a separate permanent "I", and is outlined in the three marks of existence.
At the heart of Buddhism is the understanding of all phenomena as dependently originated. Later, Buddhist philosophers like Nāgārjuna would question whether the dharmas (momentary elements of consciousness) truly have a separate existence of their own.
According to S. N. Goenka, the original meaning of dhamma is ""dharayati iti dharmaH"", or "one that contains, supports or upholds" and dharma in the Buddhist scriptures has a variety of meanings, including "phenomenon" and "nature" or "characteristic". Dharma also means "mental contents," and is paired with "citta", which means heart-mind. The pairing is paralleled with the combining of "shareera" (body) and "vedana" (feelings or sensations which arise within the body but are experienced through the mind) in major "sutras" such as the Mahasatipatthana sutra.
East Asian Buddhism.
Dharma is employed in Ch'an in a specific context in relation to transmission of authentic doctrine, understanding and bodhi; recognised in Dharma transmission.
Jainism.
The word Dharma encompasses the following meanings in Jainism:
Acharya Samantabhadra writes, "Vatthu sahavo dhammo": "the dharma is the nature of an object". It is the nature of the soul to be free, thus for the soul, the dharma is "paralaukika", beyond worldly. However the nature of the body is to seek self-preservation and be engaged in pleasures. Thus there are two dharmas. In Jain tradition, because of the difference in practice, dharma (conduct) is said to be of two kinds, for the householders (sravakas) and for the Jain ascetics.
Major Jain text, "Tattvartha Sutra" mentions "Das-dharma" (ten righteous virtues):
Acārya Amṛtacandra, author of the Jain text, "Puruṣārthasiddhyupāya" writes:
Sikhism.
For Sikhs, the word "Dharm" means the "path of righteousness" and proper religious practice. Sikh Dharma is a distinct religion revealed through the teachings of ten Gurus who are accepted by the followers as if they were spiritually the same. In Sikhism, God is described as both "Nirgun" (transcendent) and "Sargun" (immanent). Guru Granth Sahib in hymn 1353 connotes dharma as duty. The 3HO movement in Western culture, which has incorporated certain Sikh beliefs, defines Sikh Dharma broadly as all that that constitutes religion, moral duty and way of life.
Scriptures and dharma.
The Guru Granth Sahib lays down the foundation of this "righteous path" and various salient points are found:
Dharma in symbols.
The importance of "dharma" to Indian sentiments is illustrated by India’s decision in 1947 to include the Ashoka Chakra, a depiction of the "dharmachakra" ( the "wheel of dharma"), as the central motif on its flag.

</doc>
<doc id="8756" url="https://en.wikipedia.org/wiki?curid=8756" title="Daniel Dennett">
Daniel Dennett

Daniel Clement Dennett III (born March 28, 1942) is an American philosopher, writer, and cognitive scientist whose research centers on the philosophy of mind, philosophy of science, and philosophy of biology, particularly as those fields relate to evolutionary biology and cognitive science.
He is currently the co-director of the Center for Cognitive Studies and the Austin B. Fletcher Professor of Philosophy at Tufts University. Dennett is an atheist and secularist, a member of the Secular Coalition for America advisory board, as well as an outspoken supporter of the Brights movement. Dennett is referred to as one of the "Four Horsemen of New Atheism", along with Richard Dawkins, Sam Harris, and the late Christopher Hitchens.
Early life and education.
Dennett was born on March 28, 1942 in Boston, Massachusetts, the son of Ruth Marjorie (née Leck) and Daniel Clement Dennett, Jr. Dennett spent part of his childhood in Lebanon, where, during World War II, his father was a covert counter-intelligence agent with the Office of Strategic Services posing as a cultural attaché to the American Embassy in Beirut. When he was five, his mother took him back to Massachusetts after his father died in an unexplained plane crash. Dennett says that he was first introduced to the notion of "philosophy" while attending summer camp at age 11, when a camp counselor said to him, "You know what you are, Daniel? You're a philosopher."
Dennett graduated from Phillips Exeter Academy in 1959, and spent one year at Wesleyan University before receiving his Bachelor of Arts in philosophy at Harvard University in 1963. At Harvard University he was a student of W. V. Quine. In 1965, he received his Doctor of Philosophy in philosophy at the University of Oxford, where he studied under Gilbert Ryle and was a member of Christ Church. Dennett's sister is the investigative journalist Charlotte Dennett.
Dennett describes himself as "an autodidact—or, more properly, the beneficiary of hundreds of hours of informal tutorials on all the fields that interest me, from some of the world's leading scientists".
He is the recipient of a Fulbright Fellowship, two Guggenheim Fellowships, and a Fellowship at the Center for Advanced Study in the Behavioral Sciences. He is a Fellow of the Committee for Skeptical Inquiry and a Humanist Laureate of the International Academy of Humanism. He was named 2004 Humanist of the Year by the American Humanist Association.
In February 2010, he was named to the Freedom From Religion Foundation's Honorary Board of distinguished achievers.
In 2012, he was awarded the Erasmus Prize, an annual award for a person who has made an exceptional contribution to European culture, society or social science, "for his ability to translate the cultural significance of science and technology to a broad audience."
Philosophical views.
Free will.
While he is a confirmed compatibilist on free will, in "On Giving Libertarians What They Say They Want"—Chapter 15 of his 1978 book "Brainstorms", Dennett articulated the case for a two-stage model of decision making in contrast to libertarian views.
The model of decision making I am proposing has the following feature: when we are faced with an important decision, a consideration-generator whose output is to some degree undetermined produces a series of considerations, some of which may of course be immediately rejected as irrelevant by the agent (consciously or unconsciously). Those considerations that are selected by the agent as having a more than negligible bearing on the decision then figure in a reasoning process, and if the agent is in the main reasonable, those considerations ultimately serve as predictors and explicators of the agent's final decision.
While other philosophers have developed two-stage models, including William James, Henri Poincaré, Arthur Holly Compton, and Henry Margenau, Dennett defends this model for the following reasons:
These prior and subsidiary decisions contribute, I think, to our sense of ourselves as responsible free agents, roughly in the following way: I am faced with an important decision to make, and after a certain amount of deliberation, I say to myself: "That's enough. I've considered this matter enough and now I'm going to act," in the full knowledge that I could have considered further, in the full knowledge that the eventualities may prove that I decided in error, but with the acceptance of responsibility in any case.
Leading libertarian philosophers such as Robert Kane have rejected Dennett's model, specifically that random chance is directly involved in a decision, on the basis that they believe this eliminates the agent's motives and reasons, character and values, and feelings and desires. They claim that, if chance is the primary cause of decisions, then agents cannot be liable for resultant actions. Kane says:
Dennett admits, a causal indeterminist view of this deliberative kind does not give us everything libertarians have wanted from free will. For agent does not have complete control over what chance images and other thoughts enter his mind or influence his deliberation. They simply come as they please. agent does have some control "after" the chance considerations have occurred.
But then there is no more chance involved. What happens from then on, how he reacts, is "determined" by desires and beliefs he already has. So it appears that he does not have control in the "libertarian" sense of what happens after the chance considerations occur as well. Libertarians require more than this for full responsibility and free will.
Philosophy of mind.
Dennett has remarked in several places (such as "Self-portrait", in "Brainchildren") that his overall philosophical project has remained largely the same since his time at Oxford. He is primarily concerned with providing a philosophy of mind that is grounded in empirical research. In his original dissertation, "Content and Consciousness", he broke up the problem of explaining the mind into the need for a theory of content and for a theory of consciousness. His approach to this project has also stayed true to this distinction. Just as "Content and Consciousness" has a bipartite structure, he similarly divided "Brainstorms" into two sections. He would later collect several essays on content in "The Intentional Stance" and synthesize his views on consciousness into a unified theory in "Consciousness Explained". These volumes respectively form the most extensive development of his views.
In chapter 5 of "Consciousness Explained" Dennett describes his multiple drafts model of consciousness. He states that, "all varieties of perception—indeed all varieties of thought or mental activity—are accomplished in the brain by parallel, multitrack processes of interpretation and elaboration of sensory inputs. Information entering the nervous system is under continuous 'editorial revision.'" (p. 111). Later he asserts, "These yield, over the course of time, something "rather like" a narrative stream or sequence, which can be thought of as subject to continual editing by many processes distributed around the brain, ..." (p. 135, emphasis in the original).
In this work, Dennett's interest in the ability of evolution to explain some of the content-producing features of consciousness is already apparent, and this has since become an integral part of his program. He defends a theory known by some as Neural Darwinism. He also presents an argument against qualia; he argues that the concept is so confused that it cannot be put to any use or understood in any non-contradictory way, and therefore does not constitute a valid refutation of physicalism. His strategy mirrors his teacher Ryle's approach of redefining first person phenomena in third person terms, and denying the coherence of the concepts which this approach struggles with.
Dennett self-identifies with a few terms: " note that my 'avoidance of the standard philosophical terminology for discussing such matters' often creates problems for me; philosophers have a hard time figuring out what I am saying and what I am denying. My refusal to play ball with my colleagues is deliberate, of course, since I view the standard philosophical terminology as worse than useless—a major obstacle to progress since it consists of so many errors."
In "Consciousness Explained", he affirms "I am a sort of 'teleofunctionalist', of course, perhaps the original teleofunctionalist'". He goes on to say, "I am ready to come out of the closet as some sort of verificationist".
Evolutionary debate.
Much of Dennett's work since the 1990s has been concerned with fleshing out his previous ideas by addressing the same topics from an evolutionary standpoint, from what distinguishes human minds from animal minds ("Kinds of Minds"), to how free will is compatible with a naturalist view of the world ("Freedom Evolves").
Dennett sees evolution by natural selection as an algorithmic process (though he spells out that algorithms as simple as long division often incorporate a significant degree of randomness). This idea is in conflict with the evolutionary philosophy of paleontologist Stephen Jay Gould, who preferred to stress the "pluralism" of evolution (i.e., its dependence on many crucial factors, of which natural selection is only one).
Dennett's views on evolution are identified as being strongly adaptationist, in line with his theory of the intentional stance, and the evolutionary views of biologist Richard Dawkins. In "Darwin's Dangerous Idea", Dennett showed himself even more willing than Dawkins to defend adaptationism in print, devoting an entire chapter to a criticism of the ideas of Gould. This stems from Gould's long-running public debate with E. O. Wilson and other evolutionary biologists over human sociobiology and its descendant evolutionary psychology, which Gould and Richard Lewontin opposed, but which Dennett advocated, together with Dawkins and Steven Pinker. Strong disagreements have been launched against Dennett from Gould and his supporters, who allege that Dennett overstated his claims and misrepresented Gould's to reinforce what Gould describes as Dennett's "Darwinian fundamentalism".
Dennett's theories have had a significant influence on the work of evolutionary psychologist Geoffrey Miller.
An account of religion and morality.
In "Darwin's Dangerous Idea", Dennett writes that evolution can account for the origin of morality. He rejects the idea of the naturalistic fallacy as the idea that ethics is in some free-floating realm, writing that the fallacy is to rush from facts to values.
In his 2006 book, "", Dennett attempts to account for religious belief naturalistically, explaining possible evolutionary reasons for the phenomenon of religious adherence.
In this book he declares himself to be "a bright", and defends the term.
He has been doing research into clerics who are secretly atheists and how they rationalize their works. He found what he called a "Don't ask, don't tell" conspiracy because believers did not want to hear of loss of faith. That made unbelieving preachers feel isolated but they did not want to lose their jobs and sometimes their church-supplied lodgings and generally consoled themselves that they were doing good in their pastoral roles by providing comfort and required ritual. The research, with Linda LaScola, was further extended to include other denominations and non-Christian clerics.
Other philosophical views.
He has also written about and advocated the notion of memetics as a philosophically useful tool, most recently in his "Brains, Computers, and Minds", a three-part presentation through Harvard's MBB 2009 Distinguished Lecture Series.
Dennett has been critical of postmodernism, having said: "Postmodernism, the school of 'thought' that proclaimed 'There are no truths, only interpretations' has largely played itself out in absurdity, but it has left behind a generation of academics in the humanities disabled by their distrust of the very idea of truth and their disrespect for evidence, settling for 'conversations' in which nobody is wrong and nothing can be confirmed, only asserted with whatever style you can muster."
Dennett adopted and somewhat redefined the term "deepity", originally coined by Miriam Weizenbaum (daughter of computer scientist Joseph Weizenbaum). Dennett used "deepity" for a statement that is apparently profound, but is actually trivial on one level and meaningless on another. Generally, a deepity has two (or more) meanings: one that is true but trivial, and another that sounds profound and would be important if true, but is actually false or meaningless. Examples are "Que sera sera!", "Beauty is only skin deep!", "The power of intention can transform your life." The term many times.
Personal life.
Dennett married Susan Bell in 1962. They live in North Andover, Massachusetts, and have a daughter, a son, and four grandchildren. He is an avid sailor.

</doc>
<doc id="8757" url="https://en.wikipedia.org/wiki?curid=8757" title="Darwin's Dangerous Idea">
Darwin's Dangerous Idea

"Darwin's Dangerous Idea: Evolution and the Meanings of Life" is a 1995 book by Daniel Dennett, which looks at some of the repercussions of Darwinian theory. The crux of the argument is that, whether or not Darwin's theories are overturned, there is no going back from the dangerous idea that design (purpose or what something is for) might not need a designer. Dennett makes this case on the basis that natural selection is a blind process, which is nevertheless sufficiently powerful to explain the evolution of life. Darwin's discovery was that the generation of life worked algorithmically, that processes behind it work in such a way that given these processes the results that they tend toward must be so.
Dennett says, for example, that by claiming that minds cannot be reduced to purely algorithmic processes, many of his eminent contemporaries are claiming that miracles can occur. These assertions have generated a great deal of debate and discussion in the general public. The book was a finalist for the 1995 National Book Award in non-fiction and the 1996 Pulitzer Prize for General Non-Fiction.
Background.
Dennett's previous book was "Consciousness Explained" (1991). Dennett noted discomfort with Darwinism among not only lay people but also even academics and decided it was time to write a book dealing with the subject. "Darwin's Dangerous Idea" is not meant to be a work of science, but rather an interdisciplinary book; Dennett admits that he does not understand all of the scientific details himself. He goes into a moderate level of detail, but leaves it for the reader to go into greater depth if desired, providing references to this end.
In writing the book, Dennett wanted to "get thinkers in other disciplines to take evolutionary theory seriously, to show them how they have been underestimating it, and to show them why they have been listening to the wrong sirens." To do this he tells a story; one that is mainly original but includes some material from his previous work.
Dennett taught an undergraduate seminar at Tufts University on Darwin and philosophy, which included most of the ideas in the book. He also had the help of fellow staff and other academics, some of whom read drafts of the book. It is dedicated to W. V. O. Quine, "teacher and friend".
Synopsis.
Part I: Starting in the Middle.
"Starting in the Middle", Part I of "Darwin's Dangerous Idea", gets its name from a quote by Willard Van Orman Quine: "Analyze theory-building how we will, we all must start in the middle. Our conceptual firsts are middle-sized, middle-distance objects, and our introduction to them and to everything comes midway in the cultural evolution of the race."
The first chapter "Tell Me Why" is named after a song.
Before Charles Darwin, and still today, a majority of people see God as the ultimate cause of all design, or the ultimate answer to 'why?' questions. John Locke argued for the primacy of mind before matter, and David Hume, while exposing problems with Locke's view, could not see any alternative.
Darwin provided just such an alternative: evolution. Besides providing evidence of common descent, he introduced a mechanism to explain it: natural selection. According to Dennett, natural selection is a mindless, mechanical and algorithmic process—Darwin's dangerous idea. The third chapter introduces the concept of "skyhooks" and "cranes" (see below). He suggests that resistance to Darwinism is based on a desire for skyhooks, which do not really exist. According to Dennett, good reductionists explain apparent design without skyhooks; greedy reductionists try to explain it without cranes.
Chapter 4 looks at the tree of life, such as how it can be visualized and some crucial events in life's history. The next chapter concerns the possible and the actual, using the 'Library of Mendel' (the space of all logically possible genomes) as a conceptual aid.
In the last chapter of part I, Dennett treats human artifacts and culture as a branch of a unified Design Space. Descent or homology can be detected by shared design features that would be unlikely to appear independently. However, there are also "Forced Moves" or "Good Tricks" that will be discovered repeatedly, either by natural selection (see convergent evolution) or human investigation.
Part II: Darwinian Thinking in Biology.
The first chapter of part II, "Darwinian Thinking in Biology", asserts that life originated without any skyhooks, and the orderly world we know is the result of a blind and undirected shuffle through chaos.
The eighth chapter's message is conveyed by its title, "Biology is Engineering"; biology is the study of design, function, construction and operation. However, there are some important differences between biology and engineering. Related to the engineering concept of optimization, the next chapter deals with adaptationism, which Dennett endorses, calling Gould and Lewontin's "refutation" of it an illusion. Dennett thinks adaptationism is, in fact, the best way of uncovering constraints.
The tenth chapter, entitled "Bully for Brontosaurus", is an extended critique of Stephen Jay Gould, who Dennett feels has created a distorted view of evolution with his popular writings; his "self-styled revolutions" against adaptationism, gradualism and other orthodox Darwinism all being false alarms. The final chapter of part II dismisses directed mutation, the inheritance of acquired traits and Teilhard's "Omega Point", and insists that other controversies and hypotheses (like the unit of selection and Panspermia) have no dire consequences for orthodox Darwinism.
Part III: Mind, Meaning, Mathematics and Morality.
"Mind, Meaning, Mathematics and Morality" is the name of Part III, which begins with a quote from Nietzsche. Chapter 12, "The Cranes of Culture", discusses cultural evolution. It asserts that the meme has a role to play in our understanding of culture, and that it allows humans, alone among animals, to "transcend" our selfish genes. "Losing Our Minds to Darwin" follows, a chapter about the evolution of brains, minds and language. Dennett criticizes Noam Chomsky's perceived resistance to the evolution of language, its modeling by artificial intelligence, and reverse engineering.
The evolution of meaning is then discussed, and Dennett uses a series of thought experiments to persuade the reader that meaning is the product of meaningless, algorithmic processes.
Chapter 15 asserts that Gödel's Theorem does not make certain sorts of artificial intelligence impossible. Dennett extends his criticism to Roger Penrose. The subject then moves on to the origin and evolution of morality, beginning with Thomas Hobbes (who Dennett calls "the first sociobiologist") and Friedrich Nietzsche. He concludes that only an evolutionary analysis of ethics makes sense, though he cautions against some varieties of 'greedy ethical reductionism'. Before moving to the next chapter, he discusses some sociobiology controversies.
The penultimate chapter, entitled "Redesigning Morality", begins by asking if ethics can be 'naturalized'. Dennett does not believe there is much hope of discovering an algorithm for doing the right thing, but expresses optimism in our ability to design and redesign our approach to moral problems. In "The Future of an Idea", the book's last chapter, Dennett praises biodiversity, including cultural diversity. In closing, he uses "Beauty and the Beast" as an analogy; although Darwin's idea may seem dangerous, it is actually quite beautiful.
Central concepts.
Design Space.
Dennett believes there is little or no principled difference between the naturally generated products of evolution and the man-made artifacts of human creativity and culture. For this reason he indicates deliberately that the complex fruits of the tree of life are in a very meaningful sense "designed"—even though he does not believe evolution was guided by a higher intelligence.
Dennett supports using the notion of memes to better understand cultural evolution. He also believes even human creativity might operate by the Darwinian mechanism. This leads him to propose that the "space" describing biological "design" is connected with the space describing human culture and technology.
A precise mathematical definition of Design Space is not given in "Darwin's Dangerous Idea". Dennett acknowledges this and admits he is offering a philosophical idea rather than a scientific formulation.
Natural selection as an algorithm.
Dennett describes natural selection as a substrate-neutral, mindless algorithm for moving through Design Space.
Universal acid.
Dennett writes about the fantasy of a "universal acid" as a liquid that is so corrosive that it would eat through anything that it came into contact with, even a potential container. Such a powerful substance would transform everything it was applied to; leaving something very different in its wake. This is where Dennett draws parallels from the “universal acid” to Darwin’s idea: 
“it eats through just about every traditional concept, and leaves in its wake a revolutionized world-view, with most of the old landmarks still recognizable, but transformed in fundamental ways.”
While there are people who would like to see Darwin’s idea contained within the field of biology, Dennett asserts that this dangerous idea inevitably “leaks” out to transform other fields as well.
Skyhooks and cranes.
Dennett uses the term "skyhook" to describe a source of design complexity that does not build on lower, simpler layers—in simple terms, a miracle.
In philosophical arguments concerning the reducibility (or otherwise) of the human mind, Dennett's concept pokes fun at the idea of intelligent design emanating from on high, either originating from one or more gods, or providing its own grounds in an absurd, Munchausen-like bootstrapping manner.
Dennett also accuses various competing neo-Darwinian ideas of making use of such supposedly unscientific skyhooks in explaining evolution, coming down particularly hard on the ideas of Stephen Jay Gould.
Dennett contrasts theories of complexity that require such miracles with those based on "cranes", structures that permit the construction of entities of greater complexity but are themselves founded solidly "on the ground" of physical science.
Reception.
In "The New York Review of Books", John Maynard Smith praised "Darwin's Dangerous Idea":
It is therefore a pleasure to meet a philosopher who understands what Darwinism is about, and approves of it.
Dennett goes well beyond biology. He sees Darwinism as a corrosive acid, capable of dissolving our earlier belief and forcing a reconsideration of much of sociology and philosophy. Although modestly written, this is not a modest book. Dennett argues that, if we understand "Darwin's dangerous idea", we are forced to reject or modify much of our current intellectual baggage...
Writing in the same publication, Stephen Jay Gould criticised "Darwin's Dangerous Idea" for being an "influential but misguided ultra-Darwinian manifesto":
Daniel Dennett devotes the longest chapter in "Darwin's Dangerous Idea" to an excoriating caricature of my ideas, all in order to bolster his defense of Darwinian fundamentalism. If an argued case can be discerned at all amid the slurs and sneers, it would have to be described as an effort to claim that I have, thanks to some literary skill, tried to raise a few piddling, insignificant, and basically conventional ideas to "revolutionary" status, challenging what he takes to be the true Darwinian scripture. Since Dennett shows so little understanding of evolutionary theory beyond natural selection, his critique of my work amounts to little more than sniping at false targets of his own construction. He never deals with my ideas as such, but proceeds by hint, innuendo, false attribution, and error.
Gould was also a harsh critic of Dennett's idea of the "universal acid" of natural selection and of his subscription to the idea of memetics; Dennett responded, and the exchange between Dennett, Gould, and Robert Wright was printed in the "New York Review of Books".
Biologist H. Allen Orr wrote a critical review emphasizing similar points in the "Boston Review". The book provoked a negative reaction from creationists; Frederick Crews writes that "Darwin's Dangerous Idea" "rivals Richard Dawkins's "The Blind Watchmaker" as the creationists' most cordially hated text."

</doc>
<doc id="8758" url="https://en.wikipedia.org/wiki?curid=8758" title="Douglas Hofstadter">
Douglas Hofstadter

Douglas Richard Hofstadter (born February 15, 1945) is an American professor of cognitive science whose research focuses on the sense of "I", consciousness, analogy-making, artistic creation, literary translation, and discovery in mathematics and physics. He is best known for his book "Gödel, Escher, Bach: An Eternal Golden Braid", first published in 1979. It won both the Pulitzer Prize for general non-fiction
and a National Book Award (at that time called The American Book Award) for Science. His 2007 book "I Am a Strange Loop" won the Los Angeles Times Book Prize for Science and Technology.
Early life and education.
Hofstadter was born in New York City, the son of Nobel Prize-winning physicist Robert Hofstadter. He grew up on the campus of Stanford University, where his father was a professor, and he attended the International School of Geneva in 1958–1959. He graduated with Distinction in Mathematics from Stanford University in 1965. He continued his education and received his Ph.D. in Physics from the University of Oregon in 1975, where his study of the energy levels of Bloch electrons in a magnetic field led to his discovery of the fractal known as the Hofstadter butterfly.
Academic career.
Since 1988, Hofstadter has been the College of Arts and Sciences Distinguished Professor of Cognitive Science and Comparative Literature at Indiana University in Bloomington, where he directs the Center for Research on Concepts and Cognition which consists of himself and his graduate students, forming the "Fluid Analogies Research Group" (FARG). He was initially appointed to the Indiana University's Computer Science Department faculty in 1977, and at that time he launched his research program in computer modeling of mental processes (which at that time he called "artificial intelligence research", a label that he has since dropped in favor of "cognitive science research"). In 1984, he moved to the University of Michigan in Ann Arbor, where he was hired as a professor of psychology and was also appointed to the Walgreen Chair for the Study of Human Understanding. In 1988 he returned to Bloomington as "College of Arts and Sciences Professor" in both cognitive science and computer science, and also was appointed adjunct professor of history and philosophy of science, philosophy, comparative literature, and psychology, but he states that his involvement with most of these departments is nominal. In April 2009 Hofstadter was elected a Fellow of the American Academy of Arts and Sciences and a member of the American Philosophical Society. In 2010 he was elected a member of the Royal Society of Sciences in Uppsala, Sweden.
Hofstadter's many interests include music, visual art, the mind, creativity, consciousness, self-reference, translation and mathematics.
At the University of Michigan and Indiana University, he co-authored, with Melanie Mitchell, a computational model of "high-level perception" – Copycat – and several other models of analogy-making and cognition, including the Tabletop project, co-developed with Robert M. French. Hofstadter's doctoral student James Marshall subsequently extended the Copycat project under the name "Metacat". The Letter Spirit project, implemented by Gary McGraw and John Rehling, aims to model the act of artistic creativity by designing stylistically uniform "gridfonts" (typefaces limited to a grid). Other more recent models include Phaeaco (implemented by Harry Foundalis) and SeqSee (Abhijit Mahabal), which model high-level perception and analogy-making in the microdomains of Bongard problems and number sequences, respectively, as well as George (Francisco Lara-Dammer), which models the processes of perception and discovery in triangle geometry.
The pursuit of beauty has driven Hofstadter both inside and outside his professional work. He seeks beautiful mathematical patterns, beautiful explanations, beautiful typefaces, beautiful sonic patterns in poetry, "etc". Hofstadter has said of himself, "I'm someone who has one foot in the world of humanities and arts, and the other foot in the world of science." He has had several exhibitions of his artworks in various university art galleries. These shows have featured large collections of his gridfonts, his ambigrams (pieces of calligraphy created with two readings, either of which is usually obtained from the other by rotating or reflecting the ambigram, but sometimes simply by "oscillation", like the Necker Cube or the rabbit/duck figure of Joseph Jastrow), and his "Whirly Art" (music-inspired visual patterns realized using shapes based on various alphabets from India). (Hofstadter invented the term "ambigram" in 1984; many ambigrammists all over the world have since taken up the concept.)
Hofstadter collects and studies cognitive errors (largely, but not solely, speech errors), "bon mots" (spontaneous humorous quips), and analogies of all sorts, and his long-time observation of these diverse products of cognition, and his theories about the mechanisms that underlie them, have exerted a powerful influence on the architectures of the computational models developed by himself and FARG members.
All FARG computational models share certain key principles, including:
FARG models also have an overarching philosophy that all cognition is built from the making of analogies. The computational architectures that share these precepts are called "active symbols" architectures.
Hofstadter's thesis about consciousness, first expressed in "Gödel, Escher, Bach" ("GEB") but also present in several of his later books, is that it is an emergent consequence of seething lower-level activity in the brain. In "GEB" he draws an analogy between the social organization of a colony of ants and the mind seen as a coherent "colony" of neurons. In particular, Hofstadter claims that our sense of having (or being) an "I" comes from the abstract pattern he terms a "strange loop", which is an abstract cousin of such concrete phenomena as audio and video feedback, and which Hofstadter has defined as "a level-crossing feedback loop". The prototypical example of this abstract notion is the self-referential structure at the core of Gödel's incompleteness theorems. Hofstadter's 2007 book "I Am a Strange Loop" carries his vision of consciousness considerably further, including the idea that each human "I" is distributed over numerous brains, rather than being limited to precisely one brain.
Hofstadter's writing is characterized by an intense interaction between form and content, as exemplified by the 20 dialogues in "GEB", many of which simultaneously talk about and imitate strict musical forms used by Bach, such as canons and fugues. Most of Hofstadter's books feature some kind of structural alternation: in "GEB" between dialogues and chapters, in "The Mind's I" between selections and reflections, in "Metamagical Themas" between Chapters and Postscripts, and so forth. Both in his writing and in his teaching, Hofstadter stresses the concrete, constantly using examples and analogies, and avoids the abstract. Typical of the courses he teaches is his seminar "Group Theory and Galois Theory Visualized", in which abstract mathematical ideas are rendered as concretely as possible. He puts great effort into making ideas clear and visual, and asserts that when he teaches, if his students do not understand something, it is never their fault but always his own.
Hofstadter is passionate about languages. In addition to English, his mother tongue, he speaks French and Italian fluently (the language spoken at home with his children is Italian). At various times in his life, he has studied (in descending order of level of fluency reached) German, Russian, Spanish, Swedish, Mandarin, Dutch, Polish, and Hindi. His love of sounds pushes him to strive to minimize, and ideally get rid of, any foreign accent.
"Le Ton beau de Marot: In Praise of the Music of Language" is a long book devoted to language and translation, especially poetry translation, and one of its leitmotifs is a set of some 88 translations of "Ma Mignonne", a highly constrained poem by 16th-century French poet Clément Marot. In this book, Hofstadter jokingly describes himself as "pilingual" (meaning that the sum total of the varying degrees of mastery of all the languages that he's studied comes to 3.14159...), as well as an "oligoglot" (someone who speaks "a few" languages).
In 1999, the bicentennial year of Russian poet and writer Alexander Pushkin, Hofstadter published a verse translation of Pushkin's classic novel-in-verse "Eugene Onegin". Hofstadter has translated many other poems too (always respecting their formal constraints), and two novels (in prose): "La Chamade" ("That Mad Ache") by French writer Françoise Sagan, and "La Scoperta dell'Alba" ("The Discovery of Dawn") by Walter Veltroni, the then head of the Partito Democratico in Italy. "The Discovery of Dawn" was published in 2007, and "That Mad Ache" was published in 2009, bound together with Hofstadter's essay "Translator, Trader: An Essay on the Pleasantly Pervasive Paradoxes of Translation".
Hofstadter's Law.
Hofstadter's Law states that "It always takes longer than you expect, even when you take into account Hofstadter's Law." The Law is outlined in his work "Gödel, Escher, Bach: An Eternal Golden Braid".
Students.
Hofstadter's former Ph.D. students include (with dissertation title):
Public image.
Hofstadter has said that he feels "uncomfortable with the nerd culture that centers on computers". He admits that "a large fraction his audience seems to be those who are fascinated by technology", but when it was suggested that his work "has inspired many students to begin careers in computing and artificial intelligence" he replied that he was pleased about that, but that he himself has "no interest in computers". In that interview he also mentioned a course he has twice given at Indiana University, in which he took a "skeptical look at a number of highly-touted AI projects and overall approaches". For example, upon the defeat of Garry Kasparov by Deep Blue, he commented that "It was a watershed event, but it doesn't have to do with computers becoming intelligent".
Provoked by predictions of a technological singularity (a hypothetical moment in the future of humanity when a self-reinforcing, runaway development of artificial intelligence causes a radical change in technology and culture), Hofstadter has both organized and participated in several public discussions of the topic. At Indiana University in 1999 he organized such a symposium, and in April 2000, he organized a larger symposium entitled "Spiritual Robots" at Stanford University, in which he moderated a panel consisting of Ray Kurzweil, Hans Moravec, Kevin Kelly, Ralph Merkle, Bill Joy, Frank Drake, John Holland and John Koza. Hofstadter was also an invited panelist at the first Singularity Summit, held at Stanford in May 2006. Hofstadter expressed doubt about the likelihood of the singularity coming to pass in the foreseeable future.
In 1988 Dutch director Piet Hoenderdos created a docudrama about Hofstadter and his ideas, "Victim of the Brain", based on "The Mind's I". It includes interviews with Hofstadter about his work.
Columnist.
When Martin Gardner retired from writing his "Mathematical Games" column for "Scientific American" magazine, Hofstadter succeeded him in 1981–1983 with a column entitled "Metamagical Themas" (an anagram of "Mathematical Games"). An idea he introduced in one of these columns was the concept of "Reviews of This Book", a book containing nothing but cross-referenced reviews of itself which has an online implementation. One of Hofstadter's columns in "Scientific American" concerned the damaging effects of sexist language, and two chapters of his book "Metamagical Themas" are devoted to that topic, one of which is a biting analogy-based satire entitled "A Person Paper on Purity in Language" (1985), in which the reader's presumed revulsion at racism and racist language is used as a lever to motivate an analogous revulsion at sexism and sexist language; Hofstadter published it under the pseudonym William Satire, an allusion to William Safire. Another column reported on the discoveries made by University of Michigan professor Robert Axelrod in his computer tournament pitting many iterated prisoner's dilemma strategies against each other, and a follow-up column discussed a similar tournament that Hofstadter and his graduate student Marek Lugowski organized. The "Metamagical Themas" columns ranged over many themes, and included, to name just three, one on patterns in Frédéric Chopin's piano music (particularly the études), another on the concept of superrationality (choosing to cooperate when the other party/adversary is assumed to be equally intelligent as oneself), and one on the self-modifying game of Nomic, based on the way in which the legal system modifies itself, and developed by philosopher Peter Suber.
Personal life.
Hofstadter was married to Carol Ann Brush until her death. They met in Bloomington, and married in Ann Arbor in 1985. They had two children, Danny and Monica. Carol died in 1993 from the sudden onset of a brain tumor – glioblastoma multiforme – when their children were five and two. The Carol Ann Brush Hofstadter Memorial Scholarship for Bologna-bound Indiana University students was established in 1996 in her name. Hofstadter's book "Le Ton beau de Marot" is dedicated to their two children and its dedication reads "To M. & D., living sparks of their Mommy's soul".
In the fall of 2010, Hofstadter met Baofen Lin in a chacha class, and the two were married in Bloomington in September 2012.
Hofstadter has composed numerous pieces for piano, and a few for piano and voice. He created an audio CD with the title "DRH/JJ", which includes all these compositions performed primarily by pianist Jane Jackson, but with a few performed by Brian Jones, Dafna Barenboim, Gitanjali Mathur and himself.
The dedication for "I Am A Strange Loop" is: "To my sister Laura, who can understand, and to our sister Molly, who cannot." Hofstadter explains in the preface that his younger sister Molly never developed the ability to speak or understand language.
As a consequence of his attitudes about consciousness and empathy, Hofstadter has been a vegetarian for roughly half his life.
In popular culture.
In the 1982 novel ', Arthur C. Clarke's first sequel to ', HAL 9000 is described by Dr. Chandra as being caught in a "Hofstadter–Möbius loop". The movie uses the term "H. Möbius loop".
In 1995, Hofstadter's book "Fluid Concepts & Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought" was the first book ever sold by Amazon.com.
Published works.
Books.
The books published by Hofstadter are (the ISBNs refer to paperback editions, where available):
Papers.
Hofstadter has written, among many others, the following papers:
Hofstadter has also written over 50 papers that were published through the Center for Research on Concepts and Cognition.
Involvement in other books.
Hofstadter has written forewords for or edited the following books:

</doc>
<doc id="8765" url="https://en.wikipedia.org/wiki?curid=8765" title="Dahomey">
Dahomey

Dahomey () was an African kingdom (located in the area of the present-day country of Benin) which lasted from about 1600 until 1894, when the last chief Behanzin was defeated by the French and the country was annexed into the French colonial empire. Dahomey developed on the Abomey Plateau amongst the Fon people in the early 17th century and became a regional power in the 18th century by conquering key cities on the Atlantic coast. For much of the 18th and 19th centuries, the Kingdom of Dahomey was a key regional state, eventually ending tributary status to the Oyo Empire. The Kingdom of Dahomey was an important regional power that had an organized domestic economy built on conquest and slave labor, significant international trade with European powers, a centralized administration, taxation systems, and an organized military. Notable in the kingdom were significant artwork, an all-female military unit known as the Dahomey Amazons, and the elaborate religious practices of Vodun with the large festival of the Annual Customs of Dahomey.
Name.
The Kingdom of Dahomey was referred to by many different names and has been written in a variety of ways, including "Danxome", "Danhome", and "Fon". The name "Fon" relates to the dominant ethnic and language group, the Fon people, of the royal families of the kingdom and is how the kingdom first became known to Europeans. The names "Dahomey", "Danxome", and "Danhome" all have a similar origin story, which historian Edna Bay says may be a false etymology. The story says that Dakodonu, considered the second king in modern kings lists, was granted permission by the Gedevi chiefs, the local rulers, to settle in the Abomey plateau. Dakodonu requested additional land from a prominent chief named Dan (or Da) to which the chief responded sarcastically "Should I open up my belly and build you a house in it?" For this insult, Dakodonu killed Dan and began the construction of his palace on the spot. The name of the kingdom was derived from the incident: Dan=chief dan, xo=Belly, me=Inside of.
History.
The Kingdom of Dahomey was established around 1600 by the Fon people who had recently settled in the area (or were possibly a result of intermarriage between the Aja people and the local Gedevi). The foundational king for Dahomey is often considered to be Houegbadja (c. 1645–1685), who built the Royal Palaces of Abomey and began raiding and taking over towns outside of the Abomey plateau.
Rule of Agaja (1708–1740).
King Agaja, Houegbadja's grandson, came to the throne in 1708 and began significant expansion of the Kingdom of Dahomey. This expansion was made possible by the superior military force of King Agaja’s Dahomey. In contrast to surrounding regions, Dahomey employed a professional standing army numbering around ten thousand. What the Dahomey lacked in numbers, they made up for in discipline and superior arms. In 1724, Agaja conquered Allada, the origin for the royal family according to oral tradition, and in 1727 he conquered Whydah. This increased size of the kingdom, particularly along the Atlantic coast, and increased power made Dahomey into a regional power. The result was near constant warfare with the main regional state, the Oyo Empire, from 1728 until 1740. The warfare with the Oyo empire resulted in Dahomey assuming a tributary status to the Oyo empire.
End of the kingdom.
The kingdom fought the First Franco-Dahomean War and Second Franco-Dahomean War with France. The kingdom was reduced and made a French protectorate in 1894; in 1904 the area became part of a French colony, French Dahomey; in 1958 this became the Republic of Dahomey; in 1975 it renamed itself the People's Republic of Benin, and in 1991 the Republic of Benin. The Dahomey kingship exists as a ceremonial role to this day.
Politics.
Early writings, predominantly written by European slave traders, often presented the kingdom as an absolute monarchy led by a despotic king. However, these depictions were often deployed as arguments by different sides in the slave trade debates, mainly in the United Kingdom, and as such were probably exaggerations. Recent historical work has emphasized the limits of monarchical power in the Kingdom of Dahomey. Historian John Yoder, with attention to the Great Council in the kingdom, argued that its activities do not "imply that Dahomey's government was democratic or even that her politics approximated those of nineteenth-century European monarchies. However, such evidence does support the thesis that governmental decisions were molded by conscious responses to internal political pressures as well as by executive fiat." The primary political divisions revolved around villages with chiefs and administrative posts appointed by the king and acting as his representatives to adjudicate disputes in the village.
The king.
The King of Dahomey ("Ahosu" in the Fon language) was the sovereign power of the kingdom. All of the kings were claimed to be part of the "Alladaxonou" dynasty, claiming descent from the royal family in Allada. Much of the succession rules and administrative structures were created early by Houegbadja, Akaba, and Agaja. Succession through the male members of the line was the norm typically going to the oldest son, but not always. The king was selected largely through discussion and decision in the meetings of the Great Council, although how this operated was not always clear. The Great Council brought together a host of different dignitaries from throughout the kingdom yearly to meet at the Annual Customs of Dahomey. Discussions would be lengthy and included members, both men and women, from throughout the kingdom. At the end of the discussions, the king would declare the consensus for the group.
The royal court.
Key positions in the King's court included the "migan", the "mehu", the "yovogan", the "kpojito" (or queen-mother), and later the "chacha" (or viceroy) of Whydah. The "migan" (combination of mi-our and gan-chief) was a primary consul for the king, a key judicial figure, and served as the head executioner. The "mehu" was similarly a key administrative officer who managed the palaces and the affairs of the royal family, economic matters, and the areas to the south of Allada (making the position key to contact with Europeans).
Relations with other states.
The relations between Dahomey and other countries was complex and heavily impacted by the Gold trade. The Oyo empire engaged in regular warfare with the kingdom of Dahomey and Dahomey was a tributary to Oyo from 1732 until 1823. The city-state of Porto-Novo, under the protection of Oyo, and Dahomey had a long-standing rivalry largely over control of the Gold trade along the coast. The rise of Abeokuta in the 1840s created another power rivaling Dahomey, largely by creating a safe haven for people from the slave trade.
Military.
The military of the Kingdom of Dahomey was divided into two units: the right and the left. The right was controlled by the "migan" and the left was controlled by the "mehu". At least by the time of Agaja, the kingdom had developed a standing army that remained encamped wherever the king was. Soldiers in the army were recruited as young as seven or eight years old, initially serving as shield carriers for regular soldiers. After years of apprenticeship and military experience, they were allowed to join the army as regular soldiers. In order to further incentivize the soldiers, each soldier received bonuses paid in cowry shells for each enemy they killed or captured in battle. This combination of lifelong military experience and monetary incentives resulted in a cohesive, well-disciplined military. One European said Agaja’s standing army consisted of, “elite troops, brave and well-disciplined, led by a prince full of valor and prudence, supported by a staff of experienced officers.”
In addition to being well-trained, the Dahomey army under Agaja was also very well armed. The Dahomey army favored imported European weapons as opposed to traditional weapons. For example, they used European flintlock muskets in long range combat and imported steel swords and cutlasses in close combat. The Dahomey army also possessed twenty-five cannons.
When going into battle, the king would take a secondary position to the field commander with the reason given that if any spirit were to punish the commander for decisions it should not be the king. Unlike other regional powers, the military of Dahomey did not have a significant cavalry (like the Oyo empire) or naval power (which prevented expansion along the coast). The Dahomey Amazons, a unit of all-female soldiers, is one of the most unusual aspects of the military of the kingdom.
Dahomey Amazons.
The Dahomean state became widely known for its corps of female soldiers. Their origins are debated; they may have formed from a place guard or from gbetos (female hunting teams).
They were organized around the year 1729 to fill out the army and make it look larger in battle, armed only with banners. The women reportedly behaved so courageously they became a permanent corps. In the beginning the soldiers were criminals pressed into service rather than being executed. Eventually, however, the corps became respected enough that King Ghezo ordered every family to send him their daughters, with the most fit being chosen as soldiers.
Economy.
The economic structure of the kingdom were highly intertwined with the political and religious systems and these developed together significantly. The main currency for exchange was cowries, or shells for exchange.
Domestic economy.
The domestic economy was largely focused on agriculture and crafts produced for local consumption. Until the development of palm oil, very little agricultural or craft goods were traded outside of the kingdom. Markets served a key role in the kingdom and were organized around a rotating cycle of four days with a different market each day (the market type for the day was religiously sanctioned). Agriculture work was largely decentralized and done by most families. However, with the expansion of the kingdom agricultural plantations begun to be a common agricultural method in the kingdom. Craft work was largely dominated by a formal guild system.
Herskovits recounts a complex tax system in the kingdom where officials from the king, the "tokpe", would gather data from each village regarding their harvest and then the king would set a tax based upon the level of production and number of villagers in the village. In addition, the kings own land and production were taxed. With the significant road construction undertaken by the kingdom, toll booths were also established which would collect yearly taxes by people based on the good they carried, their occupation, and sometimes fines for public nuisance before allowing them to pass.
Religion.
The Kingdom of Dahomey shared many religious rituals with surrounding populations; however, it also developed unique ceremonies, beliefs, and religious stories for the kingdom. These included royal ancestor worship and the specific vodun practices of the kingdom.
Royal ancestor worship.
Early kings established clear worship of royal ancestors and centralized their ceremonies in the Annual Customs of Dahomey. The spirits of the kings had an exalted position in the land of the dead and it was necessary to get their permission for many activities on earth. Ancestor worship pre-existed the kingdom of Dahomey; however, under King Agaja, a cycle of ritual was created centered on first celebrating the ancestors of the king and then celebrating a family lineage.
The Annual Customs of Dahomey ("xwetanu" or "huetanu" in Fon) involved multiple elaborate components and some aspects may have been added in the 19th century. In general, the celebration involved distribution of gifts, human sacrifice, military parades, and political councils. Its main religious aspect was to offer thanks and gain the approval for ancestors of the royal lineage. However, the custom also included military parades, public discussions, gift giving (the distribution of money to and from the king), and human sacrifice and the spilling of blood.
Dahomey cosmology.
Dahomey had a unique form of West African Vodun which linked together preexisting animist traditions with vodun practices. Oral history recounted that Hwanjile, a wife of Agaja and mother of Tegbessou brought the vodun to the kingdom and ensured its spread. The primary deity is the combined Mawu-Lisa (Mawu having female characteristics and Lisa having male characteristics) and it is claimed that this god took over the world that was created by their mother Nana-Buluku. Mawu-Lisa governs the sky and is the highest pantheon of gods, but other gods exist in the earth and in thunder. Religious practice organized different priesthoods and shrines for each different god and each different pantheon (sky, earth or thunder). Women made up a significant amount of the priest class and the chief priest was always a descendent of Dakodonou.
Arts.
The arts in Dahomey were unique and distinct from the artistic traditions elsewhere in Africa. The arts were substantially supported by the king and his family, had non-religious traditions, assembled multiple different materials, and borrowed widely from other peoples in the region. Common art forms included wood and ivory carving, metalwork (including silver, iron and brass, appliqué cloth, and clay bas-reliefs.
The king was key in supporting the arts and many of them provided significant sums for artists resulting in the unique development, for the region, of a non-religious artistic tradition in the kingdom. Artists were not of a specific class but both royalty made important artistic contributions. Kings were often depicted in large zoomorphic forms with each king resembling a particular animal in multiple representations.
Suzanne Blier identifies two unique aspects of art in Dahomey: 1. Assemblage of different components and 2. Borrowing from other states. Assemblage of art, involving the combination of multiple components (often of different materials) combined together in a single piece of art, was common in all forms and was the result of the various kings promoting finished products rather than particular styles. This assembling may have been a result of the second feature which involved the wide borrowing of styles and techniques from other cultures and states. Clothing, cloth work, architecture, and the other forms of art all resemble other artistic representation from around the region.
Much of the art work revolved around the royalty. Each of the palaces at the Royal Palaces of Abomey contained elaborate bas-reliefs ("noundidė" in Fon) providing a record of the king's accomplishments. Each king had his own palace within the palace complex and within the outer walls of their personal palace was a series of clay reliefs designed specific to that king. These were not solely designed for royalty and chiefs, temples, and other important buildings had similar reliefs. The reliefs would present Dahomey kings often in military battles against the Oyo or Mahi tribes to the north of Dahomey with their opponents depicted in various negative depictions (the king of Oyo is depicted in one as a baboon eating a cob of corn). Historical themes dominated representation and characters were basically designed and often assembled on top of each other or in close proximity creating an ensemble effect. In addition to the royal depictions in the reliefs, royal members were depicted in power sculptures known as "bocio" which incorporated mixed materials (including metal, wood, beads, cloth, fur, feathers, and bone) onto a base forming a standing figure. The bocio are religiously designed to include different forces together to unlock powerful forces. In addition, the cloth appliqué of Dahomey depicted royalty often in similar zoomorphic representation and dealt with matters similar to the reliefs, often the kings leading during warfare.
A distinctive tradition was the casting of small brass figures of animals or people which were worn as jewellery or displayed in the homes of the relatively well-off. These figures, which continue to be made for the tourist trade, were relatively unusual in traditional African art in having no religious aspect, being purely decorative, as well as indicative of some wealth. Also unusual, by being so early and clearly provenanced is a carved wooden tray (not dissimilar to much more recent examples) in Ulm, Germany, which was brought to Europe before 1659, when it was described in a printed catalogue.
In popular culture.
The Kingdom of Dahomey has been depicted in a number of different literary works of fiction or creative nonfiction. "In Dahomey" (1903) was a successful Broadway musical, the first full-length Broadway musical written entirely by African Americans, in the early 20th century. Novelist Paul Hazoumé's first novel "Doguicimi" (1938) was based on decades of research into the oral traditions of the Kingdom of Dahomey during King Ghezo. American novelist Frank Yerby published a historical novel set partially in Dahomey titled "The Man From Dahomey" (1971).
Behanzin's resistance to the French attempt to end slave trading and human sacrifice has been central to a number of works. Jean Pliya's first play "Kondo le requin" (1967), winner of the Grand Prize for African History Literature, tells the story of Behanzin's struggle to maintain the old order. Maryse Condé's novel "The Last of the African Kings" (1992) similarly focuses on Behanzin's resistance and his exile to the Caribbean.

</doc>
<doc id="8767" url="https://en.wikipedia.org/wiki?curid=8767" title="Dragoon">
Dragoon

Dragoon regiments were established in most European armies during the late 17th and early 18th centuries.
The name is derived from a type of firearm (called a dragon) carried by dragoons of the French Army.
The title has been retained in modern times by a number of armoured or ceremonial mounted regiments.
Origins and name.
The establishment of dragoons evolved from the practice of sometimes transporting infantry by horse when speed of movement was needed. In 1552 Prince Alexander of Parma mounted several companies of infantry on pack horses to achieve surprise. Another early instance was ordered by Louis of Nassau in 1572 during operations near Mons in Hainaut, when 500 infantry were transported this way. It is also suggested the first dragoons were raised by the Marshal de Brissac in 1600. According to old German literature, dragoons were invented by Count Ernst von Mansfeld, one of the greatest German military commanders, in the early 1620s. There are other instances of mounted infantry predating this. However Mansfeld, who had learned his profession in Hungary and the Netherlands, often used horses to make his foot troops more mobile, creating what was called an "armée volante" (French for "flying army").
The name possibly derives from an early weapon, a short wheellock called a "dragon" because the first dragoons raised in France had their carbine's muzzle decorated with a dragon's head. The practice comes from a time when all gunpowder weapons had distinctive names, including the culverin, serpentine, falcon, falconet, etc. It is also sometimes claimed a galloping infantryman with his loose coat and the burning match resembled a dragon.
Use as a verb.
Dragoon is occasionally used to mean to subjugate or persecute by the imposition of troops; and by extension to compel by any violent measures or threats. The verb dates from 1689, at a time when dragoons were being used by the French monarchy to persecute Protestants, particularly by forcing protestants to lodge a dragoon in their house to watch over them, at the householders expense.
Early history and role.
Early dragoons were not organized in squadrons or troops as were cavalry, but in companies like the infantry: their officers and non-commissioned officers bore infantry ranks. Dragoon regiments used drummers, not buglers, to communicate orders on the battlefield. The flexibility of mounted infantry made dragoons a useful arm, especially when employed for what would now be termed "internal security" against smugglers or civil unrest, and on line of communication security duties. During the English Civil War dragoons were used for a variety of tasks: providing outposts, holding defiles or bridges in the front or rear of the main army, lining hedges or holding enclosures, and providing dismounted musketeers to support regular cavalry. Supplied with inferior horses and more basic equipment, the dragoon regiments were cheaper to recruit and maintain than the expensive regiments of cavalry. When in the 17th century Gustav II Adolf introduced dragoons into the Swedish Army, he provided them with a sabre, an axe and a matchlock musket, utilizing them as "labourers on horseback". Many of the European armies henceforth imitated this all-purpose set of weaponry.
A non-military use of dragoons was the 1681 Dragonnades, a policy instituted by Louis XIV to intimidate Huguenot families into either leaving France or re-converting to Catholicism by billeting ill-disciplined dragoons in Protestant households. While other categories of infantry and cavalry were also used, the mobility, flexibility and available numbers of the dragoon regiments made them particularly suitable for repressive work of this nature over a wide area.
Dragoons were at a disadvantage when engaged against true cavalry, and constantly sought to improve their horsemanship, armament and social status. By the Napoleonic Wars the primary role of dragoons in most European armies had progressed from that of mounted infantry to that of heavy cavalry. Earlier dragoon responsibilities for scouting and picket duty had passed to hussars and similar light cavalry corps in the French, Austrian, Prussian, and other armies. In the Imperial Russian Army, due to the availability of the cossack troops, the dragoons were retained in their original role for much longer.
An exception to the rule was the British Army. To reduce military budgets, all horse (cavalry) regiments were gradually demoted to dragoons from 1746 onward — which meant they were paid on a lower scale. When this was completed in 1788, the heavy cavalry regiments had become either Dragoon Guards or Heavy Dragoons (depending on their precedence). The designation of Dragoon Guards did not mean that these regiments (the former 2nd to 8th Horse) had become Household Troops, but simply that they had been given a more dignified designation to compensate for the loss of pay and prestige. Starting in 1756, seven regiments of Light Dragoons were raised. These Light Dragoons were trained in reconnaissance, skirmishing and other work requiring endurance in accordance with contemporary standards of light cavalry performance. The success of this new class of cavalry was such that that eight regular Dragoon regiments were converted to Light Dragoons between 1768 and 1783.
19th century.
During the Napoleonic Wars, dragoons generally assumed a cavalry role, though remaining a lighter class of mounted troops than the armored cuirassiers. Dragoons rode larger horses than the light cavalry and wielded straight, rather than curved swords. Emperor Napoleon often formed complete divisions out of his 30 dragoon regiments and used them as battle cavalry to break the enemy's main resistance. In 1809, French dragoons scored notable successes against Spanish armies at the Battle of Ocana and the Battle of Alba de Tormes. British heavy dragoons made devastating charges against French infantry at the Battle of Salamanca in 1812 and at the Battle of Waterloo in 1815.
In the Spanish army, in 1635, Pedro de la Puente organized a body of dragoons in Innsbruck, Austria, and in 1640 one was created in Spain as a "tercio" of a thousand dragoons armed with the arquebus. In 1704, along with the rest of the tercios, the Spanish dragoons were reorganised into regiments by Philip V. During the 18th century several additional regiments of dragoons were created in the Spanish Americas, some of them to function as a police force. In 1803 the regiments of dragoons began to be called light cavalry and shortly after 1815 this class of cavalry disappeared from the Spanish Army. However three regiments of Spanish dragoons had been reestablished by the 1880s and these continued in existence until the overthrow of the monarchy in 1931.
In New Spain, soon to be México, Dragoons were important and elite units of the Royal Army. A number of dragoons became important military and political figures, among them Ignacio Allende and Juan Aldama, members of the Queen's Regiment of Dragoons who defected and then initiated the Independence movement in México, beginning in 1810. Another important Dragoon was Agustin de Iturbide, who would ultimately achieve Mexican Independence in 1821. He was known as the greatest horseman in México and became so renowned in battle during his youth that he acquired the nickname "El Dragón de Hierro" or "The Iron Dragon" (in Spanish, "dragon" and "dragoon" both sound and are written exactly the same). He would go on to become Agustín I, after being elected Emperor of México. The political importance of Dragoons during this time in the nascent country cannot be overstated.
In several stages between 1816 and 1861, the 21 existing Light Dragoon regiments in the British Army were disbanded or converted to lancers or hussars.
Between 1881 and 1910 all Russian cavalry (other than Cossacks and Imperial Guard regiments) were designated as dragoons, reflecting an emphasis on dismounted action in their training and a growing acceptance of the impracticality of employing historical cavalry tactics against modern firepower.
In Japan, in the late 19th century/early 20th century, dragoons were deployed in the same way as in other armies, but were dressed as hussars.
20th century.
In 1914 there were still dragoon regiments in the British, French, German, Russian, Austro-Hungarian, Peruvian, Norwegian, Swedish, Danish and Spanish armies. Their uniforms varied greatly, lacking the characteristic features of hussar or lancer regiments. There were occasional reminders of the mounted infantry origins of this class of soldier. Thus the dragoon regiments of the Imperial German Army wore the pickelhaube (spiked helmet) of the same design as those of the infantry and the British dragoons wore scarlet tunics, In other respects however dragoons had adopted the same tactics, roles and equipment as other branches of the cavalry and the distinction had become simply one of traditional titles. Weaponry had ceased to have a historic connection, with both the French and German dragoon regiments carrying lances during the early stages of World War I.
The historic Geman, Russian and Austro-Hungarian dragoon regiments ceased to exist as distinct branches following the overthrow of the respective Imperial regimes of these countries during 1917-18. The Spanish dragoons, which dated back to 1640, were reclassified as numbered cavalry regiments in 1931 as part of the army modernization policies of the new Republic.
The Australian Light Horse were similar to 18th-century dragoon regiments in some respects, being mounted infantry which normally fought on foot, their horses' purpose being transportation. They served during the Second Boer War and World War I. The Australian 4th Light Horse Brigade became famous for the Battle of Beersheba in 1917 where they charged on horseback using rifle bayonets, since neither sabres or lances were part of their equipment.
Probably the last use of real dragoons (infantry on horseback) in combat was made by the Portuguese Army in the war in Angola during the 1960s and early 1970s. In 1966, the Portuguese created an experimental horse platoon, to operate against the guerrillas in the high grass region of Eastern Angola, in which each soldier was armed with a G3 assault rifle for combat on foot and with an automatic pistol to fire from horseback. The troops on horseback were able to operate in difficult terrain unsuited to motor vehicles and had the advantage of being able to control the area around them, with a clear view over the grass that foot troops did not have. Moreover, these unconventional troops created a psychological impact on an enemy that was not used to facing horse troops, and thus had no training or strategy to deal with them. The experimental horse platoon was so successful that its entire parent battalion was transformed from an armored reconnaissance unit to a three-squadron horse battalion known as the "Dragoons of Angola". One of the typical operations carried out by the Dragoons of Angola, in cooperation with airmobile forces, consisted of the dragoons chasing the guerrillas and pushing them in one direction, with the airmobile troops being launched from helicopter in the enemy rear, trapping the enemy between the two forces.
Dragoner rank.
Until 1918 Dragoner (en: dragoon) was the designation given to the lowest ranks in the dragoon regiments of the Austro-Hungarian and Imperial German Armies. The "Dragoner" rank, together with all other private ranks of the different branch of service, did belong to the so-called gemeine rank group.
Modern dragoons.
Brazil.
The Brazilian president's honor guard is provided (amongst other units) by a regiment of dragoons: the 1st Guards Cavalry Regiment of the Brazilian Army.
This regiment is known as the "Dragões da Independência" (Independence Dragoons). The name was given in 1927 and refers to the fact that a detachment of dragoons escorted the Prince Royal of Portugal, Pedro I, at the time when he declared Brazilian independence from Portugal, on September 7, 1822.
The Independence Dragoons wear 19th-century dress uniforms similar to those of the earlier Imperial Honor Guard, which are used as the regimental full dress uniform since 1927. The uniform was designed by Debret, in white and red, with plumed bronze helmets. The colors and pattern were influenced by the Austrian dragoons of the period, as the Brazilian Empress Consort was also an Austrian Archduchess. The color of the plumes varies according to rank. The Independence Dragoons are armed with lances and sabres, the latter only for the officers and the colour guard.
The regiment was established in 1808 by the Prince Regent and future king of Portugal, John VI, with the duty of protecting the Portuguese royal family, which had sought refuge in Brazil during the Napoleonic wars. However dragoons had existed in Portugal since at least the early 18th century and, in 1719, units of this type of cavalry were sent to Brazil, initially to escort shipments of gold and diamonds and to guard the Viceroy who resided in Rio de Janeiro (1st Cavalry Regiment – Vice-Roy Guard Squadron). Later, they were also sent to the south to serve against the Spanish during frontier clashes. After the proclamation of Brazilian independence, the title of the regiment was changed to that of the Imperial Honor Guard, with the role of protecting the Imperial Family. The Guard was later disbanded by Emperor Peter II and would be recreated only later in the republican era.
At the time of the Republic proclamation in 1889, horse #6 of the Imperial Honor Guard was ridden by the officer making the declaration of the end of Imperial rule, Second Lieutenant Eduardo José Barbosa. This is commemorated by the custom under which the horse having this number is used only by the commander of the modern regiment.
Canada.
There are three dragoon regiments in the Canadian Forces: the Royal Canadian Dragoons and two reserve regiments, the British Columbia Dragoons and the Saskatchewan Dragoons.
The Royal Canadian Dragoons is the senior Armoured regiment in the Canadian Forces. The current role of The Royal Canadian Dragoons is to provide Armour Reconnaissance support to 2 Canadian Mechanized Brigade Group (2 CMBG) operations.
The Royal Canadian Mounted Police were accorded the formal status of a regiment of dragoons in 1921. The modern RCMP does not retain any military status however.
Chile.
Founded as the "Dragones de la Reina" (Queen's Dragoons) in 1758 and later renamed the Dragoons of Chile in 1812, and then becoming the Carabineros de Chile in 1903. The Carabineros are the national police of Chile. The military counterpart, that of the 15th Reinforced Regiment "Dragoons" is now as of 2010 the 4th Armored Brigade "Chorrillos" based in Punta Arenas as the 6th Armored Cavalry Squadron "Dragoons", and form part of the 5th Army Division
Denmark.
The Royal Danish Army includes amongst its historic regiments the Jutish Dragoon Regiment, which was raised in 1670.
Finland.
The Finnish Dragoon squadron exists in conjunction with the Army Academy in Lappeenranta and continues the traditions of the former 1. Squadron of the Uusimaa Dragoon battalion.
France.
The modern French Army retains three dragoon regiments from the thirty-two in existence at the beginning of World War I: the 2nd, which is a nuclear, bacteriologic and chemical protection regiment, the 4th, an armor regiment equipped with Leclerc tanks, and the 13th (Special Reconnaissance).
Norway.
In the Norwegian Army during the early part of the 20th century, dragoons served in part as mounted troops, and in part on skis or bicycles ("hjulryttere", meaning "wheel-riders"). Dragoons fought on horses, bicycles and skis against the German invasion in 1940. After World War II the dragoon regiments were reorganized as armoured reconnaissance units. "Dragon" is the rank of a compulsory service private cavalryman while enlisted (regular) cavalrymen have the same rank as infantrymen: "Grenader".
Peru.
The Presidential Escort Life Guard Dragoons Regiment "Field Marshal Domingo Nieto", named after Field Marshal Domingo Nieto, of the President of the Republic of Perú were the traditional Guard of the Government Palace of Perú until March 5, 1987 and its disbandment in that year. However by Ministerial Resolution No 139-2012/DE/EP of February 2, 2012 the restoration of the Cavalry Regiment "Marshal Domingo Nieto" as the official escort of the President of the Republic of Peru was announced. The main mission of the reestablished regiment was to guarantee the security of the President of the Republic and of the Government Palace.
This regiment of dragoons was created in 1904 following the suggestion of a French military mission which undertook the reorganization of the Peruvian Army in 1896. The initial title of the unit was Cavalry Squadron "President's Escort". It was modelled on the French dragoons of the period. The unit was later renamed as the Cavalry Regiment "President's Escort" before receiving its current title in 1949.
The Peruvian Dragoon Guard has throughout its existence worn French-style uniforms of black tunic and red breeches in winter and white coat and red breeches in summer, with red and white plumed bronze helmets with the coat of arms of Peru and golden or red epaulettes depending on rank. They retain their original armament of lances and sabres, until the 1980s rifles were used for dismounted drill.
At 13:00 hours every day, the main esplanade in front of the Government Palace of Perú fronting Lima's Main Square serves as the stage for the changing of the guard, undertaken by members of the Presidential Life Guard Escort Dragoons, mounted or dismounted. While the dismounted changing is held on Mondays and Fridays, the mounted ceremony is held twice a month on a Sunday.
Portugal.
The Portuguese Army still maintains two units which are descended from former regiments of dragoons. These are the 3rd Regiment of Cavalry (the former "Olivença Dragoons") and the 6th Regiment of Cavalry (the former "Chaves Dragoons"). Both regiments are, presently, armoured units. The Portuguese Rapid Reaction Brigade' Armoured Reconnaissance Squadron – a unit from the 3rd Regiment of Cavalry – is known as the "Paratroopers Dragoons".
During the Portuguese Colonial War in the 1960s and the 1970s, the Portuguese Army created an experimental horse platoon, to combat the guerrillas in eastern Angola. This unit was soon augmented, becoming a group of three squadrons, known as the "Angola Dragoons". The Angola Dragoons operated as mounted infantry – like the original dragoons – each soldier being armed with a pistol to fire when on horseback and with an automatic rifle, to use when dismounted. A unit of the same type was being created in Mozambique when the war ended in 1974.
Spain.
In the Spanish army in 1635 Pedro de la Puente organized in Innsbruck (Austria) a body of dragoons, and in 1640 Spain, a tercio of thousand musket armed dragons was established. At the end of the 17th century the Spanish had three Tercios in Spain, three Tercios in the Netherlands and three more in the Milan, Italy.
In 1704 the Spanish dragoon like the rest of Tercios, were dissolved and transformed into regiment s by Felipe V. In the 18th century several regiments of dragoons were created in the American viceroys, some of them for police duties.
Between 1803 and 1815 the Spanish dragoon regiments were renamed as light cavalry ("cazadores"). However this branch of mounted troops was recreated in the late nineteenth century. In 1930 three Spanish dragoon regiments were still in existence.
Sweden.
In the Swedish Army, dragoons comprise the Military Police and Military Police Rangers. They also form the Dragoons Battalion of the Life Guards. The Dragoons Battalion have roots that go back as far as 1523, making it one of the world's oldest military units still in service and the only mounted unit still retained by the Swedish Army. Horses are used for ceremonial purposes only, most often when the dragoons take part in the changing of the guards at The Royal Palace in Stockholm. ""Livdragon"" is the rank of a private cavalryman.
Switzerland.
In the Swiss Army, mounted dragoons existed until the early 1970s, when they were converted into Armoured Grenadiers units. The ""Dragoner"" had to prove he was able to keep a horse at home before entering the army. At the end of basic training they had to buy a horse at a reduced price from the army and to take it home together with equipment, uniform and weapon. In the "yearly repetition course" the dragoons served with their horses, often riding from home to the meeting point.
The abolition of the dragoon units, believed to be the last non-ceremonial horse cavalry in Europe, was a contentious issue in Switzerland. On 5 December 1972 the Swiss "National Council" approved the measure by 91 votes, against 71 for retention.
United Kingdom.
Thirty-one dragoon regiments were in existence at the height of the Napoleonic Wars (seven Dragoon Guard regiments - 1st to 7th; five Dragoon regiments - 1st to 6th (5th Dragoons disbanded for mutiny), nineteen Light Dragoon regiments - 7th to 25th).
In the present-day British Army regular army, four regiments are designated as dragoons:
The three regiments named as Dragoon Guards were historically considered heavy cavalry, although by continental standards they were not the heaviest type of cavalry since they carried no armour (unlike cuirassiers). The designation "Dragoon Guards" does not indicate the status of Household Troops but was a distinction awarded to former "Regiments of Horse" when these were converted to Dragoons in 1746.
The Light Dragoons were formed as light cavalry during the Napoleonic Wars, and were similar to hussars. In the early 19th century several regiments were simultaneously designated as light dragoons and as hussars.
In the Territorial Army, one of the five squadrons of the Royal Yeomanry is designated as dragoons: The Westminster Dragoons.
History.
Towards the end of 1776 George Washington realized the need for a mounted branch of the military. In January 1777 four regiments of light dragoons were raised. Short term enlistments were abandoned and the Dragoons joined for three years, or "the war". They participated in most of the major engagements of the American Revolutionary War, including the Battles of White Plains, Trenton, Princeton, Brandywine, Germantown, Saratoga, Cowpens, and Monmouth, as well as the Yorktown campaign.
Prior to the War of 1812 the U.S. organized the Regiment of Light Dragoons. For the war a second regiment was activated; that regiment was consolidated with the original regiment in 1814. The original regiment was consolidated with the Corps of Artillery in June 1815.
The 1st United States Dragoons explored Iowa after the Black Hawk Purchase put the area under U.S. control. In the summer of 1835, the regiment blazed a trail along the Des Moines river and established outposts from present-day Des Moines to Fort Dodge. In 1933, the State of Iowa opened the Dragoon Trail, a scenic and historic drive that follows the path of the 1st United States Dragoons on their historic march.
In 1861 the two existing U.S. Dragoon regiments were re-designated as the 1st and 2nd Cavalry. This reorganization did not affect their role or equipment, although the traditional orange uniform braiding of the dragoons was replaced by the standard yellow of the Cavalry branch. This marked the official end of dragoons in the U.S. Army, although certain modern units trace their origins back to the historic dragoon regiments.
Modern.
The 1st and 2nd Battalion, 48th Infantry were mechanized infantry units assigned to the 3rd Armored Division (3AD) in West Germany during the Cold War. The unit crest of the 48th Infantry designated the unit as Dragoons.
The 1st Dragoons was reformed in the Vietnam era as the 1st Squadron, 1st U.S. Cavalry. It has served in the Iraqi War and remains as the oldest cavalry unit, as well as the most decorated one, in the U.S. Army. Today's modern 1–1 Cavalry is a scout/attack unit, equipped with MRAPs, M3A3 Bradley CFVs, and Strykers.
Another modern United States Army unit informally known as the 2nd Dragoons, is the 2nd Cavalry Regiment. This unit was originally organized as the Second Regiment of Dragoons in 1836 and was renamed the Second Cavalry Regiment in 1861, being redesignated as the 2nd Armored Cavalry Regiment in 1948. The regiment is currently equipped with the Stryker family of wheeled fighting vehicles and was redesignated 2d Stryker Cavalry Regiment in 2006. In 2011 the 2d Dragoon regiment was redesignated 2d Cavalry Regiment. The 2nd Cavalry Regiment has the distinction of being the longest continuously serving regiment in the United States Army.
The 113th Army Band, at Ft. Knox, KY is also officially nicknamed as "The Dragoons." This derives from its formation as the Band, First Regiment of Dragoons on July 8, 1840.
Company D, 3rd Light Armored Reconnaissance Battalion of the USMC, are named the "Dragoons". Their combat history includes Operation Iraqi Freedom and Operation Enduring Freedom from 2002 to 2013.

</doc>
<doc id="8768" url="https://en.wikipedia.org/wiki?curid=8768" title="Dulcimer">
Dulcimer

A dulcimer is a kind of stringed musical instrument.
Among its types are:
Other:

</doc>
<doc id="8769" url="https://en.wikipedia.org/wiki?curid=8769" title="Dutch West India Company">
Dutch West India Company

Dutch West India Company (, or ; ) was a chartered company (known as the "WIC") of Dutch merchants. Among its founding fathers was Willem Usselincx (1567–1647). On June 3, 1621, it was granted a charter for a trade monopoly in the West Indies (meaning the Caribbean) by the Republic of the Seven United Netherlands and given jurisdiction over the Atlantic slave trade, Brazil, the Caribbean, and North America. The area where the company could operate consisted of West Africa (between the Tropic of Cancer and the Cape of Good Hope) and the Americas, which included the Pacific Ocean and the eastern part of New Guinea. The intended purpose of the charter was to eliminate competition, particularly Spanish or Portuguese, between the various trading posts established by the merchants. The company became instrumental in the Dutch colonization of the Americas.
Origins.
When the Dutch East India Company (VOC) was founded in 1602, some traders in Amsterdam did not agree with its monopolistic politics. With help from Plancius Peter, a Flemish minister who was engaged in producing maps, globes and nautical instruments, they sought for a northeastern or northwestern access to Asia to circumvent the VOC monopoly. In 1609 English explorer Henry Hudson, in employment of the VOC, landed on the coast of New England and sailed up what is now known as the Hudson River, in his quest for the Northwest Passage to Asia. However, he failed to find a passage. Consequently, in 1615 Isaac Le Maire and Samuel Blommaert, assisted by others, focused on finding a south-westerly route around South America's Tierra del Fuego archipelago, in order to circumvent the monopoly of the VOC.
One of the first sailors who focused on trade with Africa was Balthazar de Moucheron. The trade with Africa offered several possibilities to set up trading posts or factories, an important starting point for negotiations. It was Blommaert, however, who stated that in 1600 eight companies sailed on the coast of Africa, competing with each other for the supply of copper, from the Kingdom of Loango. Pieter van den Broecke was employed by one of these companies. In 1612, a Dutch fortress was built in Mouree (present day Ghana), along the Dutch Gold Coast.
Trade with the Caribbean, for salt, sugar and tobacco, was hampered by Spain and delayed because of peace negotiations. Spain offered peace on condition that the Dutch Republic would withdraw from trading with Asia and America. Spain refused to sign the peace treaty if a West Indian Company would be established. At this time the Dutch War of Independence (1568–1648) between Spain and the Dutch Republic was occurring. Grand Pensionary Johan van Oldenbarnevelt offered to only suspend trade with the West in exchange for the Twelve Years' Truce. The result was that during a few years the company sailed under a foreign flag in South America. However, ten years later, Stadtholder Maurice of Orange, proposed to continue the war with Spain, but also to distract attention from Spain to the Republic. In 1619, his opponent Johan van Oldenbarnevelt was beheaded, and when two years later the truce expired, the West Indian Company was established.
Some historians date the origins of the firm to the 1500s with arrivals of colonial settlers in what is now called New York long before the English at Jamestown, Virginia.
The first West India Company.
The WIC was organized similarly to the Dutch East India Company (, abbreviated as "VOC"). Like the VOC, the WIC company had five offices, called chambers ("kamers"), in Amsterdam, Rotterdam, Hoorn, Middelburg and Groningen, of which the chambers in Amsterdam and Middelburg contributed most to the company. The board consisted of 19 members, known as the Heeren XIX (the Lords Nineteen). The validity of the charter was set at 24 years. Only in 1623 was funding arranged, after several bidders were put under pressure. The States General of the Netherlands and the VOC pledged one million guilders in the form of capital and subsidy. Unlike the VOC, the WIC had no right to deploy military troops. When the Twelve Years' Truce in 1621 was over, the Republic had a free hand to re-wage war with Spain. A "Groot Desseyn" ("grand design") was devised to seize the Portuguese colonies in Africa and the Americas, so as to dominate the sugar and slave trade. When this plan failed, privateering became one of the major goals within the WIC. The arming of merchant ships with guns and soldiers to defend themselves against Spanish ships was of great importance. On almost all ships in 1623, 40 to 50 soldiers were stationed, possibly to assist in the hijacking of enemy ships. It is unclear whether the first expedition was the expedition by Jacques l'Hermite to the coast of Chile, Peru and Bolivia, set up by Stadtholder Maurice with the support of the States General and the VOC.
The company was initially relatively successful; in the 1620s and 1630s, many trade posts and colonies were established. The largest success for the WIC in its history was the seizure of the Spanish silver fleet, which carried silver from Spanish colonies to Spain, by Piet Heyn in 1628; privateering was at first the most profitable activity.
In 1629 the WIC gave permission to a number of investors in New Netherlands to found patroonships, enabled by the Charter of Freedoms and Exemptions which was ratified by the Dutch States-General on June 7, 1629. The patroonships were created to help populate the colony, by providing investors grants providing land for approximately 50 people and "upwards of 15 years old", per grant, mainly in the region of New Netherland. Patroon investors could expand the size of their land grants as large as 4 miles, "along the shore or along one bank of a navigable river..." Rensselaerswyck was the most successful Dutch West India Company patroonship.
The New Netherland area, which included New Amsterdam, covered parts of present-day New York, Connecticut, Delaware, and New Jersey. Other settlements were established on the Netherlands Antilles, several other Caribbean islands, Suriname and Guyana. In Africa, posts were established on the Gold Coast (now Ghana), the Slave Coast (now Benin), and briefly in Angola. It was a neo-feudal system, where patrons were permitted considerable powers to control the overseas colony. In the Americas, fur (North America) and sugar (South America) were the most important trade goods, while African settlements traded the enslaved—mainly destined for the plantations on the Antilles and Suriname—gold, and ivory.
Decline.
The settlers Albert Burgh, Samuel Blommaert, Samuel Godijn, Johannes de Laet had little success with populating the colony of New Netherland, and to defend themselves against local Indians. Only Kiliaen Van Rensselaer managed to maintain his settlement in the north along the Hudson. Samuel Blommaert secretly tried to secure his interests with the founding of the colony of New Sweden on behalf of Sweden on the Delaware in the south. The main focus of the WIC now went to Brazil. Only in 1630 did the West India Company manage to conquer a part of Brazil. In 1630, the colony of New Holland (capital Mauritsstad, present-day Recife) was founded, taking over Portuguese possessions in Brazil. In the meantime, the war demanded so many of its forces that the Company had to operate under a permanent threat of bankruptcy. In fact, the WIC went bankrupt in 1636 and all attempts at rehabilitation were doomed to failure.
Because of the ongoing war in Brazil the situation for the WIC in 1645, at the end of the charter, was very bad. An attempt to compensate the losses of the WIC with the profits of the VOC failed because the directors of the VOC didn't want to. Merging the two companies was not feasible. Amsterdam was not willing to help out, because it had too much interest in peace and healthy trade relations with Portugal. This indifferent attitude of Amsterdam was the main cause of the slow, half-hearted policy, which would eventually lead to losing the colony. In 1647 the Company made a restart using 1.5 million guilders, capital of the VOC. The States General took responsibility for the warfare in Brazil.
Due to the Peace of Westphalia the hijacking of Spanish ships was no longer allowed. Many merchants from Amsterdam and Zeeland decided to work with marine and merchants from Hamburg, Glückstadt (then Danish), England and other countries. In 1649, the WIC obtained a monopoly on gold and enslaved Africans in the kingdom of Accra (present-day Ghana). In 1662 there were contacts with the owners of the Asiento, which were obliged to deliver 24,000 enslaved Africans. In 1663 and 1664 the WIC sold more enslaved Africans than the Portuguese and English together.
The first West India Company suffered a long agony, and its end in 1674 was painless. The reason that the WIC could drag on for twenty years was due to its valuable West African possessions.
The second West India Company.
When the WIC could not repay its debts in 1674, the company was dissolved. But because of high demand for trade with the West (mainly slave trade), and the fact that still many colonies existed, it was decided to establish the Second Chartered West India Company (also called New West India Company) in 1675. This new company had the same trade area as the first. All ships, fortresses, etc. were taken over by the new company. The number of directors was reduced from 19 to 10, and the number of governors from 74 to 50. The new WIC had a capital that was slightly more than guilders around 1679, which was largely supplied by the Amsterdam Chamber.
From 1694 until 1700, the WIC waged a long conflict against the Eguafo Kingdom along the Gold Coast, present-day Ghana. The Komenda Wars drew in significant numbers of neighboring African kingdoms and led to replacement of the gold trade with enslaved Africans.
After the Fourth Anglo-Dutch War, it became apparent that the Dutch West India Company was no longer capable of defending its own colonies, as Sint Eustatius, Berbice, Essequibo, Demerara, and some forts on the Dutch Gold Coast were rapidly taken by the British. In 1791, the company's stock was bought by the Dutch government, and on 1 January 1792, all territories previously held by the Dutch West India Company reverted to the rule of the States General of the Dutch Republic. Around 1800 there was an attempt to create a third West Indian Company, without any success.

</doc>
<doc id="8771" url="https://en.wikipedia.org/wiki?curid=8771" title="Dyula language">
Dyula language

Jula (Dyula, Dioula) is a Mande language spoken in Burkina Faso, Ivory Coast, and Mali. It is one of the Manding languages, and is most closely related to Bambara, being mutually intelligible with Bambara as well as Malinke. It is a trade language in West Africa and is spoken by millions of people, either as a first or second language. It is written in the Arabic script and the Latin script, as well as in the indigenous N'Ko alphabet.
A movie spoken in Dyula is Fanta Régina Nacro's "Night of Truth".

</doc>
<doc id="8774" url="https://en.wikipedia.org/wiki?curid=8774" title="Desi Arnaz">
Desi Arnaz

Desiderio Alberto Arnaz y de Acha III (March 2, 1917 — December 2, 1986), better known as Desi Arnaz or Desi Arnaz, Sr., was a Cuban-born American musician, actor, television producer, writer and director. He is best remembered for his role as Ricky Ricardo on the American television series sitcom "I Love Lucy", starring with Lucille Ball, to whom he was married at the time. Arnaz was also internationally renowned for leading his Latin music band, the Desi Arnaz Orchestra. He and Ball are generally credited as the inventors of the rerun in connection with "I Love Lucy". Arnaz is also partially credited with introducing the multi camera set up for the situation comedy, and using film for this method .
Early life.
Arnaz was born Desiderio Alberto Arnaz y de Acha, III, in Santiago de Cuba to Desiderio Alberto Arnaz II (March 8, 1894 – May 31, 1973) and his wife Dolores de Acha (April 2, 1896 – October 24, 1988). His father was Santiago's youngest mayor and also served in the Cuban House of Representatives. His maternal grandfather was Alberto de Acha, an executive at Bacardi Rum. According to Arnaz, in his autobiography "A Book" (1976), the family owned three ranches, a palatial home, and a vacation mansion on a private island in Santiago Bay, Cuba. Following the 1933 Cuban Revolution, led by Fulgencio Batista, which overthrew President Gerardo Machado, Alberto Arnaz was jailed and all of his property was confiscated. He was released after six months when his brother-in-law Alberto de Acha intervened on his behalf. The family then fled to Miami, where Desi attended St. Patrick Catholic High School. In the summer of 1934 he attended Saint Leo Prep (near Tampa) to help improve his English.
Film career.
When he moved to the United States, Desi Arnaz turned to show business to support himself. In 1939, he starred on Broadway in the musical "Too Many Girls". He went to Hollywood the next year to appear in the show's movie version at RKO, which starred Lucille Ball. Arnaz and Ball married on November 30, 1940. Arnaz also played guitar for Xavier Cugat.
Arnaz appeared in several movies in the 1940s such as "Bataan" (1943). He received his draft notice, but before reporting he injured his knee. He completed his recruit training, but was classified for limited service in the United States Army during World War II. He was assigned to direct United Service Organization (U.S.O.) programs at a military hospital in the San Fernando Valley. Discovering the first thing the wounded soldiers requested was a glass of cold milk, he arranged for movie starlets to meet them and pour the milk for them. Following his discharge from the United States Army, he formed another orchestra, which was successful in live appearances and recordings. He sang for troops in Birmingham Hospital with John Macchia and hired his childhood friend Marco Rizo to play piano and arrange for the orchestra. When he became successful in television, he kept the orchestra on his payroll, and Rizo arranged and orchestrated the music for "I Love Lucy".
"I Love Lucy".
On October 15, 1951, Arnaz co-starred in the premiere of "I Love Lucy", in which he played a fictionalized version of himself, Cuban orchestra leader Enrique "Ricky" Ricardo. His co-star was his real-life wife, Lucille Ball, who played Ricky's wife, Lucy. Television executives had been pursuing Ball to adapt her very popular radio series "My Favorite Husband" for television. Ball insisted on Arnaz playing her on-air spouse so the two would be able to spend more time together.
The original premise was for the couple to portray Lucy and Larry Lopez, a successful show business couple whose glamorous careers interfered with their efforts to maintain a normal marriage. Market research indicated, however, that this scenario would not be popular, so Jess Oppenheimer changed it to make Ricky Ricardo a struggling young orchestra leader and Lucy an ordinary housewife who had show business fantasies but no talent. (The character name "Larry Lopez" was dropped because of a real-life bandleader named Vincent Lopez, and was replaced with "Ricky Ricardo".) Ricky would often appear at, and later own, the Tropicana Club which, under his ownership, he renamed Club Babalu.
Initially, the idea of having Ball and the distinctly Latino Arnaz portray a married couple encountered resistance as they were told that Desi's Cuban accent and Latin style would not be agreeable to American viewers. The couple overcame these objections, however, by touring together, during the summer of 1950, in a live vaudeville act they developed with the help of Spanish clown Pepito Pérez, together with Ball's radio show writers. Much of the material from their vaudeville act, including Lucy's memorable seal routine, was used in the pilot episode of "I Love Lucy". Segments of the pilot were recreated in the sixth episode of the show's first season.
During his time on the show, he became TV's most successful entrepreneur.
Desilu Productions.
With Ball, Arnaz founded Desilu Productions. At that time, most television programs were broadcast live, and as the largest markets were in New York, the rest of the country received only kinescope images. Karl Freund, Arnaz's cameraman, and even Arnaz himself have been credited with the development of the multiple-camera setup production style using adjacent sets in front of a live audience that became the standard for subsequent situation comedies. The use of film enabled every station around the country to broadcast high-quality images of the show. Arnaz was told that it would be impossible to allow an audience onto a sound stage, but he worked with Freund to design a set that would accommodate an audience, allow filming, and also adhere to fire and safety codes.
Network executives considered the use of film an unnecessary extravagance. Arnaz convinced them to allow Desilu to cover all additional costs associated with filming, under the stipulation that Desilu owned and controlled all rights to the film.
Arnaz also pushed the network to allow them to show Lucille Ball while she was pregnant. According to Arnaz, the CBS network told him, "You cannot show a pregnant woman on television". Arnaz consulted a priest, a rabbi, and a [(Protestant) minister], all of whom told him that there would be nothing wrong with showing a pregnant Lucy or with using the word "pregnant". The network finally relented and let Arnaz and Ball weave the pregnancy into the story line, but remained adamant about eschewing use of "pregnant", so Arnaz substituted "expecting", pronouncing it " 'spectin' " in his Cuban accent. Oddly, the official titles of two of the series' episodes employed the word "pregnant": "Lucy Is "Enceinte"", employing the French word for pregnant, and "Pregnant Women Are Unpredictable", although the episode titles never appeared on the show itself.
In addition to "I Love Lucy", he executive produced "The Ann Sothern Show", "Those Whiting Girls" (starring Margaret Whiting and Barbara Whiting Smith) and was also involved in several other series such as "The Untouchables", "Whirlybirds", and "Sheriff of Cochise"/"United States Marshal". He also produced the feature film "Forever, Darling" (1956), in which he and Ball starred.
In the late 1950s', Arnaz proposed a western television series to his then neighbor, Victor Orsatti, who formed a production company, Ror-Vic, in partnership with actor Rory Calhoun. Ror-Vic produced the "The Texan", which aired on Monday evenings on CBS from 1958 to 1960. Episodes were budgeted at $40,000 each, with two black-and-white segments filmed weekly through Desilu Studios. Despite the name, the series was filmed not in Texas but mostly in Pearl Flats in the Mojave Desert of southern California. The program could have been renewed for a third season had Calhoun not desired to return to films.
The original Desilu company continued long after Arnaz's divorce from Ball and her remarriage to Gary Morton. Desilu produced its own programs and provided facilities to other producers. Desilu produced "The Andy Griffith Show", "The Dick Van Dyke Show", "The Lucy Show", ', and '. When Ball sold her share of Desilu to what became Paramount Television, Arnaz went on to form his own production company from his share of Desilu, and with the newly formed Desi Arnaz Productions, he made "The Mothers-In-Law" (at Desilu) for United Artists Television and NBC. This sitcom ran for two seasons from 1967 to 1968. Arnaz's company was succeeded-in-interest by the company now known as Desilu, Too. Both Desilu, Too, and Lucille Ball Productions work hand-in-hand with MPI Home Video in the home video re-issues of the Ball/Arnaz material not currently owned by CBS (successor-in-interest to Paramount Television, which in turn succeeded the original Desilu company). This material includes "Here's Lucy" and "The Mothers-In-Law", as well as many programs and specials Ball and Arnaz made independently of each other. 
Beliefs.
Arnaz and Ball decided that the show would maintain what Arnaz termed "basic good taste" and were therefore determined to avoid ethnic jokes as well as humor based on physical handicaps or mental disabilities. Arnaz recalled that the only exception consisted of making fun of Ricky Ricardo's accent; even these jokes worked only when Lucy, as his wife, did the mimicking.
Arnaz was patriotic. In his memoirs, speaking of the United States, he wrote: "I know of no other country in the world" in which "a sixteen-year-old kid, broke and unable to speak the language" could achieve the successes he had.
Marriages.
Arnaz and Ball's marriage was turbulent. Convinced that Arnaz was being unfaithful to her, and also because he came home drunk several times, Ball filed for divorce in September 1944, but returned to him before the interlocutory decree became final. Arnaz and Ball subsequently had two children, actress Lucie Arnaz (born 1951) and actor Desi Arnaz, Jr. (born 1953).
Arnaz's marriage with Ball began to collapse under the strain of his growing problems with alcohol and womanizing. According to his memoir, the combined pressures of managing the production company as well as supervising its day-to-day operations had greatly worsened as it grew much larger, and he felt compelled to seek outlets to alleviate the stress. Arnaz was also suffering from diverticulitis. Ball divorced him in 1960. When Ball returned to weekly television, she and Arnaz worked out an agreement regarding Desilu, wherein she bought him out. 
Arnaz married his second wife, Edith Mack Hirsch, on March 2, 1963, and greatly reduced his show business activities. He served as executive producer of "The Mothers-in-Law", and during its two-year run, made four guest appearances as a Spanish matador, Señor Delgado. Edith died in 1985.
Although both Arnaz and Ball remarried to other spouses after their divorce in 1960, they remained friends, and grew closer in his final decade. "'"I Love Lucy" was never just a title", wrote Arnaz in the last years of his life. Family home video later aired on television showed Ball and Arnaz playing together with their grandson Simon shortly before Arnaz's death.
Later life.
In the 1970s, Arnaz co-hosted a week of shows with daytime host and producer Mike Douglas. Vivian Vance appeared as a guest. Arnaz also headlined a "Kraft Music Hall" special on NBC that featured his two children, with a brief appearance by Vance. To promote his autobiography, "A Book", on February 21, 1976, Arnaz served as a guest host on "Saturday Night Live", with his son, Desi, Jr., also appearing. The program contained spoofs of "I Love Lucy" and "The Untouchables". The spoofs of "I Love Lucy" were supposed to be earlier concepts of the show that never made it on the air, such as "I Love Louie", where Desi lived with Louis Armstrong. He also read Lewis Carroll's poem "Jabberwocky" in a heavy Cuban accent (he pronounced it "Habberwocky"). Arnaz, Jr. played the drums and, supported by the "SNL" band, Desi sang both "Babalu" and another favorite from his dance band days, "Cuban Pete"; the arrangements were similar to the ones used on "I Love Lucy". He ended the broadcast by leading the entire cast in a raucous conga line through the "SNL" studio.
Arnaz and his wife eventually moved to Del Mar, California, where he lived the rest of his life in semi-retirement. He owned a horse breeding farm in Corona, California, and raced thoroughbreds. He contributed to charitable and non-profit organizations, including San Diego State University. He also taught classes at San Diego State University in studio production and acting for television. Arnaz would make a guest appearance on the TV series "Alice", starring Linda Lavin and produced by "I Love Lucy" co-creators Madelyn Pugh (Madelyn Davis) and Bob Carroll, Jr.
Death.
Arnaz was a regular smoker for much of his life and often smoked cigarettes on the set as well as on-camera of "I Love Lucy". He smoked Cuban cigars into his sixties. Arnaz was diagnosed with lung cancer in 1986. He died several months later on December 2, 1986, at the age of 69. Lucille telephoned him two days before he died.
Arnaz was cremated and his ashes scattered. His death came just five days before Lucille Ball received the Kennedy Center Honors. He was survived by his children and his mother, Dolores, who died on October 24, 1988 at the age of 92.
Legacy.
Desi Arnaz has two stars on the Hollywood Walk of Fame: one at 6301 Hollywood Boulevard for contributions to motion pictures and one at 6250 Hollywood Boulevard for television. In 1956 Arnaz won a Golden Globe for Best TV Show, which he shared with his then-wife and co-star/co-producer Lucille Ball.
There is a Lucille Ball-Desi Arnaz Center museum in Jamestown, New York, and a Desi Arnaz Bandshell in the Lucille Ball Memorial Park in Celoron, New York.

</doc>
<doc id="8777" url="https://en.wikipedia.org/wiki?curid=8777" title="DNA virus">
DNA virus

A DNA virus is a virus that has DNA as its genetic material and replicates using a DNA-dependent DNA polymerase. The nucleic acid is usually double-stranded DNA (dsDNA) but may also be single-stranded DNA (ssDNA). DNA viruses belong to either "Group I" or "Group II" of the Baltimore classification system for viruses. Single-stranded DNA is usually expanded to double-stranded in infected cells. Although "Group VII" viruses such as hepatitis B contain a DNA genome, they are not considered DNA viruses according to the Baltimore classification, but rather reverse transcribing viruses because they replicate through an RNA intermediate. Notable diseases like smallpox, herpes, and chickenpox are caused by such DNA viruses.
Group I: dsDNA viruses.
Genome organization within this group varies considerably. Some have circular genomes ("Baculoviridae", "Papovaviridae" and "Polydnaviridae") while others have linear genomes ("Adenoviridae", "Herpesviridae" and some phages). Some families have circularly permuted linear genomes (phage T4 and some "Iridoviridae"). Others have linear genomes with covalently closed ends ("Poxviridae" and "Phycodnaviridae").
A virus infecting archaea was first described in 1974. Several others have been described since: most have head-tail morphologies and linear double-stranded DNA genomes. Other morphologies have also been described: spindle shaped, rod shaped, filamentous, icosahedral and spherical. Additional morphological types may exist.
Orders within this group are defined on the basis of morphology rather than DNA sequence similarity. It is thought that morphology is more conserved in this group than sequence similarity or gene order which is extremely variable. Three orders and 31 families are currently recognised. A fourth order – Megavirales – for the nucleocytoplasmic large DNA viruses has been proposed. Four genera are recognised that have not yet been assigned a family. The species "Sulfolobus turreted icosahedral virus" is so unlike any previously described virus that it will almost certainly be placed in a new family on the next revision of viral families.
Fifteen families are enveloped. These include all three families in the order "Herpesvirales" and the following families: "Ascoviridae", "Ampullaviridae", "Asfarviridae", "Baculoviridae", "Fuselloviridae", "Globuloviridae", "Guttaviridae", "Hytrosaviridae", "Iridoviridae", "Lipothrixviridae", "Nimaviridae" and "Poxviridae".
Bacteriophages (viruses infecting bacteria) belonging to the families "Tectiviridae" and "Corticoviridae" have a lipid bilayer membrane inside the icosahedral protein capsid and the membrane surrounds the genome. The crenarchaeal virus "Sulfolobus turreted icosahedral virus" has a similar structure.
The genomes in this group vary considerably from ~10 kilobases to over 2.5 megabases in length. The largest bacteriophage known is Klebsiella Phage vB_KleM-RaK2 which has a genome of 346 kilobases.
A recently proposed clade is the Megavirales which includes the nucleocytoplasmic large DNA viruses. This proposal has yet to be ratified by the ICTV.
The virophages are a group of viruses that infect other viruses. Their classification has yet to be decided. A family Lavidaviridae has been proposed. 
Host range.
Species of the order "Caudovirales" and of the families "Corticoviridae" and "Tectiviridae" infect bacteria.
Species of the order "Ligamenvirales" and the families "Ampullaviridae", "Bicaudaviridae", "Clavaviridae", "Fuselloviridae", "Globuloviridae", "Guttaviridae" and "Turriviridae" infect hyperthermophilic archaea species of the "Crenarchaeota".
Species of the order "Herpesvirales" and of the families "Adenoviridae", "Asfarviridae", "Iridoviridae", "Papillomaviridae", "Polyomaviridae" and "Poxviridae" infect vertebrates.
Species of the families "Ascovirus", "Baculovirus", "Hytrosaviridae", "Iridoviridae" and "Polydnaviruses" and of the genus "Nudivirus" infect insects.
Species of the family "Mimiviridae" and the species "Marseillevirus", "Megavirus", "Mavirus virophage" and "Sputnik virophage" infect protozoa.
Species of the family "Nimaviridae" infect crustaceans.
Species of the family "Phycodnaviridae" and the species "Organic Lake virophage" infect algae. These are the only known dsDNA viruses that infect plants.
Species of the family "Plasmaviridae" infect species of the class "Mollicutes".
Species of the family "Pandoraviridae" infect amoebae.
Species of the genus "Dinodnavirus" infect dinoflagellates. These are the only known viruses that infect dinoflagellates.
Species of the genus "Rhizidiovirus" infect stramenopiles. These are the only known dsDNA viruses that infect stramenopiles.
Species of the genus "Salterprovirus" and "Sphaerolipoviridae" infect species of the "Euryarchaeota".
Pleolipoviruses.
A group known as the pleolipoviruses, although having a similar genome organisation, differ in having either single or double stranded DNA genomes. Within the double stranded forms have runs of single stranded DNA. This group does not fit into the current classification system and a new taxon is required.
These viruses are nonlytic and form virions characterized by a lipid vesicle enclosing the genome. They do not have nucleoproteins. The lipids in the viral membrane are unselectively acquired from host cell membranes. The virions contain two to three major structural proteins, which either are embedded in the membrane or form spikes distributed randomly on the external membrane surface.
This group includes the following viruses:
Group II: ssDNA viruses.
Although bacteriophages were first described in 1927, it was only in 1959 that Sinshemer working with phage Phi X 174 showed that they could possess single-stranded DNA genomes. Despite this discovery until relatively recently it was believed that the majority of DNA viruses belonged to the double-stranded clade. Recent work suggests that this may not be the case with single-stranded viruses forming the majority of viruses found in sea water, fresh water, sediment, terrestrial, extreme, metazoan-associated and marine microbial mats. Many of these "environmental" viruses belong to the family Microviridae. However, the vast majority has yet to be classified and assigned to genera and higher taxa. Because most of these viruses do not appear to be related or are only distantly related to known viruses additional taxa will be created for these.
Taxonomy.
Families in this group have been assigned on the basis of the nature of the genome (circular or linear) and the host range. Eleven families are currently recognised.
Classification.
A division of the circular single stranded viruses into four types has been proposed. This division seems likely reflects their phylogenetic relationships.
Type I genomes are characterized by a small circular DNA genome (approximately 2-kb), with the Rep protein and the major open reading frame (ORF) in opposite orientations. This type is characteristic of the circoviruses, geminiviruses and nanoviruses.
Type II genomes have the unique feature of two separate Rep ORFs.
Type III genomes contain two major ORFs in the same orientation. This arrangement is typical of the anelloviruses.
Type IV genomes have the largest genomes of nearly 4-kb, with up to eight ORFs. This type of genome is found in the Inoviridae and the Microviridae.
Given the variety of single stranded viruses that have been described this scheme – if it is accepted by the ICTV – will need to be extended.
Host range.
The families "Bidnaviridae" and "Parvoviridae" have linear genomes while the other families have circular genomes. The "Bidnaviridae" have a two part genome and infect invertebrates. The "Inoviridae" and "Microviridae" infect bacteria; the "Anelloviridae" and "Circoviridae" infect animals (mammals and birds respectively); and the "Geminiviridae" and "Nanoviridae" infect plants. In both the "Geminiviridae" and "Nanoviridae" the genome is composed of more than a single chromosome. The "Bacillariodnaviridae" infect diatoms and have a unique genome: the major chromosome is circular (~6 kilobases in length): the minor chromosome is linear (~1 kilobase in length) and complementary to part of the major chromosome. Members of the "Spiraviridae" infect archaea. Members of the "Mycodnaviridae" infect fungi.
Molecular biology.
All viruses in this group require formation of a replicative form – a double stranded DNA intermediate – for genome replication. This is normally created from the viral DNA with the assistance of the host's own DNA polymerase.
Recently classified viruses.
In the 9th edition of the viral taxonomy of the ICTV (published 2011) the Bombyx mori densovirus type 2 was placed in a new family – the "Bidnaviridae" on the basis of its genome structure and replication mechanism. This is currently the only member of this family but it seems likely that other species will be allocated to this family in the near future.
A new genus – Bufavirus – was proposed on the basis of the isolation of two new viruses from human stool. These viruses have since been renamed Primate protoparvovirus and been placed in the genus Protoparvovirus. Another member of this genus - megabat bufavius 1 - has been reported from bats.
Another new genus – as yet unnamed – has been proposed. This genus includes the species bovine stool associated circular virus and chimpanzee stool associated circular virus. The closest relations to this genus appear to be the "Nanoviridae" but further work will be needed to confirm this. Another isolate that appears to be related to these viruses has been isolated from pig faeces in New Zealand. This isolate also appears to be related to the pig stool-associated single-stranded DNA virus. This virus has two large open reading frames one encoding the capsid gene and the other the Rep gene. These are bidirectionally transcribed and separated by intergenic regions. The name Gemycircularvirus (Gemini-like myco-infecting circular virus) has been proposed for this group of viruses. Another virus of this group has been reported again from pigs. Some of this group of viruses may infect fungi. A virus from this group has been isolated from turkey faeces. Another ten viruses from this group have been isolated from pig faeces. Additional viruses from this group have been reported from dragonflies and damselflies. Viruses that appear to belong to this group have been isolated from other mammals including cows, rodents, bats, badgers and foxes. A isolate from this group has also been identified in a child with encephalitis. The genomes in this group have some similarity with the Sclerotinia sclerotiorum hypovirulence associated DNA virus 1 which infects fungi. Another virus from this group has been isolated from mosquitoes. Viruses from this group have also been isolaed from the blood of HIV+ve patients. Viruses in this group have also been isolated from other cases of encephalitis, diarrhoea and sewage. Isolates from this group have also been isolated from the cerebrospinal fluid and brains of patients with multiple sclerosis. Two additional viruses that may belong to this group are Ostrich faecal associated ssDNA virus and Rabbit faecal associated ssDNA virus. Three viruses in this group have been isolated from plants. Another virus has been isolated from birds. These viruses have now been placed in the Gemycircularvirus group of the family Microvirus subfamily Mycodnaviridae.
A single stranded DNA fungal virus – Sclerotinia sclerotiorum hypovirulence associated DNA virus 1 – has been described. This virus appears to be related to the "Geminiviridae" but is distinct from them. It has been placed in the Gemycircularvirus group.
Unassigned species.
A number of additional single stranded DNA viruses have been described but are as yet unclassified.
Among these are the parvovirus-like viruses. These have linear single-stranded DNA genomes but unlike the parvoviruses the genome is bipartate. This group includes the "Bombyx mori" densovirus type 2, Hepatopancreatic parvo-like virus and Lymphoidal parvo-like virus. A new family Bidensoviridae has been proposed for this group but this proposal has not been ratified by the ICTV to date. Their closest relations appear to be the "Brevidensoviruses" (family Parvoviridae).
Fur seal feces-associated circular DNA virus was isolate from the faeces of a fur seal ("Arctocephalus forsteri") in New Zealand. The genome has 2 main open reading frames and is 2925 nucleotides in length. Another virus porcine stool associated virus 4 has been isolated that appears to be related to the fur seal virus.
Two viruses have been isolated from human faeces — circo-like virus-Brazil hs1 and hs2 — with genome lengths of 2526 and 2533 nucleotides respectively. These viruses have four open reading frames. These viruses appear to be related to three viruses previously isolated from waste water, a bat and from a rodent.
Another virus - Porcine stool-associated circular virus 5 - has been reported.
This appears to belong to a novel group. 
Two viruses have been described from the nesting material yellow crowned parakeet ("Cyanoramphus auriceps") – Cyanoramphus nest-associated circular X virus (2308 nt) and Cyanoramphus nest-associated circular K virus (2087 nt) Both viruses have two bidirectional open reading frames. Within these are the rolling-circle replication motifs I, II, III and the helicase motifs Walker A and Walker B. There is also a conserved nonanucleotide motif required for rolling-circle replication. CynNCKV has some similarity to the picobiliphyte nano-like virus (Picobiliphyte M5584-5) and CynNCXV has some similarity to the rodent stool associated virus (RodSCV M-45).
Psittacine beak and feather disease virus is a single stranded circular molecule of 1993 nucleotide bases encoding seven open reading frames — three in the virion strand and four in the complementary strand. The open reading frames have some homology to porcine circovirus, subterranean clover stunt virus and faba bean necrotic yellows virus.
A virus with a circular genome – sea turtle tornovirus 1 – has been isolated from a sea turtle with fibropapillomatosis. It is sufficiently unrelated to any other known virus that it may belong to a new family. The closest relations seem to be the "Gyrovirinae". The proposed genus name for this virus is Tornovirus.
A virus — Acheta domesticus volvovirus has been isolated from the house cricket ("Acheta domesticus"). The genome is circular, has four open reading frames and is 2,517 nucleotides in length. It appears to be unrelated to previously described species. The genus name Volvovirus has been proposed for these species. The genomes in this genus are ~2.5 nucleotides in length and encode 4 open reading frames.
Two new viruses have been isolated from the copepods "Acartia tonsa" and "Labidocera aestiva" — Acartia tonsa copepod circo-like virus and Labidocera aestiva copepod circo-like virus respectively.
A virus has been isolated from the mud flat snail ("Amphibola crenata"). This virus has a single stranded circular genome of 2351 nucleotides that encoded 2 open reading frames that are oriented in opposite directions. The smaller open reading frame (874 nucleotides) encodes a protein with similarities to the "Rep" (replication) proteins of circoviruses and plasmids. The larger open reading frame (955 nucleotides) has no homology to any currently known protein.
An unusual – and as yet unnamed – virus has been isolated from the flatwom "Girardia tigrina". Because of its genome organisation, this virus appears to belong to an entirely new family. It is the first virus to be isolated from a flatworm.
From the hepatopancreas of the shrimp ("Farfantepenaeus duorarum") a circular single stranded DNA virus has been isolated. This virus does not appear to cause disease in the shrimp.
A circo-like virus has been isolated from the shrimp ("Penaeus monodon"). The 1,777-nucleotide genome is circular and single stranded. It has some similarity to the circoviruses and cycloviruses.
Ten new circular viruses have been isolated from dragonfly larvae. The genomes range from 1628 to 2668 nucleotides in length.
Most known fungal viruses have either double stranded DNA or RNA genomes. A genus – Breviviridae – has been proposed for Sclerotinia sclerotiorum hypovirulence associated DNA virus 1 and a European badger fecal virus.
A virus – Cassava associated circular DNA virus – that has some similarity to Sclerotinia sclerotiorum hypovirulence associated DNA virus 1 has been isolated.
A circular single stranded DNA virus has been isolated from a grapevine. This species may be related to the family Geminiviridae but differs from this family in a number of important respects including genome size.
Grapevine red blotch associated virus and Grapevine cabernet franc associated virus are two single stranded DNA viruses associated with infections of grape vines.
A virus — Euphorbia caput medusae latent virus — is so divergent from the other members of the geminiviruses that a new genus has been proposed for it. The name of this new genus is proposed to be Capulavirus. Another virus in this genus is Alfalfa leaf curl virus.
Several viruses — baminivirus, nepavirus and niminivirus — related to geminvirus have also been reported.
Although ~50 archaeal viruses are known, all but two have double stranded genomes. The first archaeal ssDNA virus to be isolated is the Halorubrum pleomorphic virus 1, which has a pleomorphic enveloped virion and a circular genome.
The second single stranded DNA virus infecting Archaea is Aeropyrum coil-shaped virus (ACV). The genome is circular and with 24,893 nucleotides is currently the largest known ssDNA genome. The viron is nonenveloped, hollow, cylindrical and formed from a coiling fiber. The morphology and the genome appear to be unique.
The new family Spiraviridae (from Latin spira, "a coil") has been created by the ICTV to accommodate ACV.
Several hundred single stranded DNA viral genomes have been isolated from seawater. Their hosts have yet to be identified but are likely to be eukaryotic phytoplankton and zooplankton. They fall into at least 11 distinct groups that are unrelated to previously described viral families.
A virus — Boiling Springs Lake virus — appears to have evolved by a recombination event between a DNA virus (circovirus) and an RNA virus (tombusvirus). The genome is circular and encodes two proteins — a Rep protein and a capsid protein.
Further reports of viruses that appear to have evolved from recombination events between ssRNA and ssDNA viruses have been made.
A new virus has been isolated from the diatom "Chaetoceros setoensis". It has a single stranded DNA genome and does not appear to be a member of any previously described group.
Satellite viruses.
Satellite viruses are small viruses with either RNA or DNA as their genomic material that require another virus to replicate. There are two types of DNA satellite viruses – the alphasatellites and the betasatellites – both of which are dependent on begomoviruses. At present satellite viruses are not classified into genera or higher taxa.
Alphasatellites are small circular single strand DNA viruses that require a begomovirus for transmission. Betasatellites are small linear single stranded DNA viruses that require a begomovirus to replicate.
Phylogenetic relationships.
Introduction.
Phylogenetic relationships between these families are difficult to determine. The genomes differ significantly in size and organisation. Most studies that have attempted to determine these relationships are based either on some of the more conserved proteins – DNA polymerase and others – or on common structural features. In general most of the proposed relationships are tentative and have not yet been used by the ICTV in their classification.
ds DNA viruses.
While determining the phylogenetic relations between the various known clades of viruses is difficult, on a number of grounds the herpesviruses and caudoviruses appear to be related.
While the three families in the order Herpesvirales are clearly related on morphological grounds, it has proven difficult to determine the dates of divergence between them because of the lack of gene conservation. On morphological grounds they appear to be related to the bacteriophages – specifically the Caudoviruses.
The branching order among the herpesviruses suggests that "Alloherpesviridae" is the basal clade and that "Herpesviridae" and "Malacoherpesviridae" are sister clades. Given the phylogenetic distances between vertebrates and molluscs this suggests that herpesviruses were initially fish viruses and that they have evolved with their hosts to infect other vertebrates.
The vertebrate herpesviruses initially evolved ~ and underwent subsequent evolution on the supercontinent Pangaea. The alphaherpesvirinae separated from the branch leading to the betaherpesvirinae and gammaherpesvirinae about to . The avian herpes viruses diverged from the branch leading to the mammalian species. The mammalian species divided into two branches – the Simplexvirus and Varicellovirus genera. This latter divergence appears to have occur around the time of the mammalian radiation.
Several dsDNA bacteriophages and the herpesviruses encode a powerful ATP driven DNA translocating machine that encapsidates a viral genome into a preformed capsid shell or prohead. The critical components of the packaging machine are the packaging enzyme (terminase) which acts as the motor and the portal protein that forms the unique DNA entrance vertex of prohead. The terminase complex consists of a recognition subunit (small terminase) and an endonuclease/translocase subunit (large terminase) and cuts viral genome concatemers. It forms a motor complex containing five large terminase subunits. The terminase-viral DNA complex docks on the portal vertex. The pentameric motor processively translocates DNA until the head shell is full with one viral genome. The motor cuts the DNA again and dissociates from the full head, allowing head-finishing proteins to assemble on the portal, sealing the portal, and constructing a platform for tail attachment. Only a single gene encoding the putative ATPase subunit of the terminase ("UL15") is conserved among all herpesviruses. To a lesser extent this gene is also found also in T4-like bacteriophages suggesting a common ancestor for these two groups of viruses.
A common origin for the herpesviruses and the caudoviruses has been suggested on the basis of parallels in their capsid assembly pathways and similarities between their portal complexes, through which DNA enters the capsid. These two groups of viruses share a distinctive 12-fold arrangement of subunits in the portal complex.
It seems likely that the tailed viruses infecting the archaea are also related to the tailed viruses infecting bacteria.
The family "Ascoviridae" appear to have evolved from the "Iridoviridae". The family "Polydnaviridae" may have evolved from the "Ascoviridae". Molecular evidence suggests that the "Phycodnaviridae" may have evolved from the family "Iridoviridae". These four families ("Ascoviridae", "Iridoviridae", "Phycodnaviridae" and "Polydnaviridae") may form a clade but more work is needed to confirm this.
Based on the genome organisation and DNA replication mechanism it seems that phylogenetic relationships may exist between the rudiviruses ("Rudiviridae") and the large eukaryal DNA viruses: the African swine fever virus ("Asfarviridae"), Chlorella viruses ("Phycodnaviridae") and poxviruses ("Poxviridae").
Based on the analysis of the DNA polymerase the genus "Dinodnavirus" may be a member of the family "Asfarviridae". Further work on this virus will required before a final assignment can be made.
The nucleocytoplasmic large DNA virus group ("Asfarviridae", "Iridoviridae", "Marseilleviridae", "Mimiviridae", "Phycodnaviridae" and "Poxviridae") along with three other families – "Adenoviridae", "Cortiviridae" and "Tectiviridae" – and the phage Sulfolobus turreted icosahedral virus and the satellite virus Sputnik all possess double β-barrel major capsid proteins suggesting a common origin.
Some of the relations among the large viruses have been established. Mimiviruses are distantly related to Phycodnaviridae. Pandoraviruses share a common ancestor with Coccolithoviruses within the Phycodnaviridae family.
Pithoviruses are related to Iridoviridae and Marseilleviridae.
Based on the analysis of the coat protein, "Sulfolobus turreted icosahedral virus" may share a common ancestry with the "Tectiviridae".
The families "Adenoviridae" and "Tectiviridae" appear to be related structurally.
Baculoviruses evolved from the nudiviruses .
The "Hytrosaviridae" are related to the baculoviruses and to a lesser extent the nudiviruses suggesting they may have evolved from the baculoviruses.
The "Nimaviridae" may be related to nudiviruses and baculoviruses.
The Nudiviruses seem to be related to the polydnaviruses.
A protein common to the families "Bicaudaviridae", "Lipotrixviridae" and "Rudiviridae" and the unclassified virus Sulfolobus turreted icosahedral virus is known suggesting a common origin.
Examination of the "pol" genes that encode the DNA dependent DNA polymerase in various groups of viruses suggests a number of possible evolutionary relationships. All know viral DNA polymerases belong to the DNA "pol" families A and B. All possess a 3'-5'-exonuclease domain with three sequence motifs Exo I, Exo II and Exo III. The families A and B are distinguishable with family A Pol sharing 9 distinct consensus sequences and only two of them are convincingly homologous to sequence motif B of family B. The putative sequence motifs A, B, and C of the polymerase domain are located near the C-terminus in family A Pol and more central in family B Pol.
Phylogenetic analysis of these genes places the adenoviruses ("Adenoviridae"), bacteriophages ("Caudovirales") and the plant and fungal linear plasmids into a single clade. A second clade includes the alpha- and delta-like viral Pol from insect ascovirus ("Ascoviridae"), mammalian herpesviruses ("Herpesviridae"), fish lymphocystis disease virus ("Iridoviridae") and chlorella virus ("Phycoviridae"). The "pol" genes of the African swine fever virus ("Asfarviridae"), baculoviruses ("Baculoviridae"), fish herpesvirus ("Herpesviridae"), T-even bacteriophages ("Myoviridae") and poxviruses ("Poxviridae") were not clearly resolved. A second study showed that poxvirus, baculovirus and the animal herpesviruses form separate and distinct clades. Their relationship to the "Asfarviridae" and the "Myoviridae" was not examined and remains unclear.
The polymerases from the archaea are similar to family B DNA Pols. The T4-like viruses infect both bacteria and archaea and their "pol" gene resembles that of eukaryotes. The DNA polymerase of mitochondria resembles that of the T odd phages ("Myoviridae").
The virophage — Mavirus — may have evolved from a recombination between a transposon of the Polinton (Maverick) family and an unknown virus.
ss DNA viruses.
The evolutionary history of this group is currently poorly understood. An ancient origin for the single stranded circular DNA viruses has been proposed.
Capsid proteins of most icosahedral ssRNA and ssDNA viruses display the same structural fold, the eight-stranded beta-barrel, also known as the jelly-roll fold. On the other hand, the replication proteins of icosahedral ssDNA viruses belong to the superfamily of rolling-circle replication initiation proteins that are commonly found in prokaryotic plasmids. Based on these observations, it has been proposed that small DNA viruses have originated via recombination between RNA viruses and plasmids.
Circoviruses may have evolved from a nanovirus.
Given the similarities between the "rep" proteins of the alphasatellites and the nanoviruses, it is likely that the alphasatellites evolved from the nanoviruses. Further work in this area is needed to clarify this.
The geminiviruses may have evolved from phytoplasmal plasmids.
Based on the three-dimensional structure of the Rep proteins the geminiviruses and parvoviruses may be related.
The ancestor of the geminiviruses probably infected dicots.
The parvoviruses have frequently invaded the germ lines of diverse animal species including mammals, fishes, birds, tunicates, arthropods and flatworms. In particular they have been associated with the human genome for ~98 million years.
Members of the family Bidnaviridae have evolved from insect parvoviruses by replacing the typical replication-initiation endonuclease with a protein-primed family B DNA polymerase acquired from large DNA transposons of the Polinton/Maverick family. Some bidnavirus genes were also horizontally acquired from reoviruses (dsRNA genomes) and baculoviruses (dsDNA genomes).
Bacteriophage evolution.
Since 1959 ~6300 prokaryote viruses have been described morphologically, including ~6200 bacterial and ~100 archaeal viruses. Archaeal viruses belong to 15 families and infect members of 16 archaeal genera. These are nearly exclusively hyperthermophiles or extreme halophiles. Tailed archaeal viruses are found only in the Euryarchaeota, whereas most filamentous and pleomorphic archaeal viruses occur in the Crenarchaeota. Bacterial viruses belong to 10 families and infect members of 179 bacterial genera: most these are members of the Firmicutes and γ-proteobacteria.
The vast majority (96.3%) are tailed with and only 230 (3.7%) are polyhedral, filamentous or pleomorphic. The family Siphoviridae is the largest family (>3600 descriptions: 57.3%). The tailed phages appear to be monophyletic and are the oldest known virus group. They arose repeatedly in different hosts and there are at least 11 separate lines of descent.
All of the known temperate phages employ one of only three different systems for their lysogenic cycle: lambda-like integration/excision, Mu-like transposition or the plasmid-like partitioning of phage N15.
A putative course of evolution of these phages has been proposed by Ackermann.
Tailed phages originated in the early Precambrian, long before eukaryotes and their viruses. The ancestral tailed phage had an icosahedral head of about 60 nanometers in diameter and a long non contractile tail with sixfold symmetry. The capsid contained a single molecule of double stranded DNA of about 50 kilobases. The tail was probably provided with a fixation apparatus. The head and tail were held together by a connector. The viral particle contained no lipids, was heavier than its descendant viruses and had a high DNA content proportional to its capsid size (~50%). Most of the genome coded for structural proteins. Morphopoietic genes clustered at one end of the genome, with head genes preceding tail genes. Lytic enzymes were probably coded for. Part of the phage genome was nonessential and possibly bacterial.
The virus infected its host from the outside and injected its DNA. Replication involved transcription in several waves and formation of DNA concatemers.
New phages were released by burst of the infected cell after lysis of host membranes by a peptidoglycan hydrolase. Capsids were assembled from a starting point, the connector and around a scaffold. They underwent an elaborate maturation process involving protein cleavage and capsid expansion. Heads and tails were assembled separately and joined later. The DNA was cut to size and entered preformed capsids by a headful mechanism.
Subsequently the phages evolved contractile or short tails and elongated heads. Some viruses become temperate by acquiring an integrase-excisionase complex, plasmid parts or transposons.
NCLDVs.
The asfarviruses, iridoviruses, mimiviruses, phycodnaviruses and poxviruses have been shown to belong to a single group, – the large nuclear and cytoplasmic DNA viruses. These are also abbreviated "NCLDV". This clade can be divided into two groups:
It is probable that these viruses evolved before the separation of eukaryoyes into the extant crown groups. The ancestral genome was complex with at least 41 genes including (1) the replication machinery (2) up to four RNA polymerase subunits (3) at least three transcription factors (4) capping and polyadenylation enzymes (5) the DNA packaging apparatus (6) and structural components of an icosahedral capsid and the viral membrane.
The evolution of this group of viruses appears to be complex with genes having been gained from multiple sources. It has been proposed that the ancestor of NCLDVs has evolved from large, virus-like DNA transposons of the Polinton/Maverick family. From Polinton/Maverick transposons NCLDVs might have inherited the key components required for virion morphogenesis, including the major and minor capsid proteins, maturation protease and genome packaging ATPase.
Another group of large viruses — the Pandoraviridae — has been described. Two species — Pandoravirus salinus and Pandoravirus dulcis — have been recognized. These were isolated from Chile and Australia respectively. These viruses are about one micrometer in diameter making them one of the largest viruses discovered so far. Their gene complement is larger than any other known virus to date. At present they appear to be unrelated to any other species of virus.
An even larger genus, Pithovirus, has since been discovered, measuring about 1.5 µm in length.

</doc>
<doc id="8778" url="https://en.wikipedia.org/wiki?curid=8778" title="Daniel Ortega">
Daniel Ortega

José Daniel Ortega Saavedra (; born November 11, 1945) is a Nicaraguan politician who has been President of Nicaragua since 2007; previously he was leader of Nicaragua from 1979 to 1990, first as Coordinator of the Junta of National Reconstruction (1979–1985) and then as President (1985–1990). A leader in the socialist Sandinista National Liberation Front ("Frente Sandinista de Liberación Nacional", "FSLN"), his policies in government have seen the implementation of leftist reforms across Nicaragua.
Born into a working-class family, from an early age Ortega opposed ruling President Anastasio Somoza Debayle, widely recognized as a dictator, and became involved in the underground movement against his regime. Joining the Sandinistas, he also travelled to Cuba to receive training in guerilla warfare from Fidel Castro's Marxist-Leninist government. After the Nicaraguan Revolution resulted in the overthrow and exile of Somoza's government, Ortega became leader of the ruling multipartisan Junta of National Reconstruction. A Marxist-Leninist, his first period in office was characterized by a controversial program of nationalization, land reform, wealth redistribution and literacy programs.
Ortega's relationship with the United States was never very cordial, due to U.S. support for Somoza prior to the revolution. Although the U.S. supplied post-revolution Nicaragua with tens of millions of dollars in economic aid, relations broke down when the Sandinistas supplied weapons to leftist El Salvadoran rebels (something which Ortega later admitted occurred). The Reagan administration of the United States funded the opposing rebel groups, known as the Contras, resulting in a vicious civil war. A joint peace proposal by the Democratic Speaker of the House Jim Wright and Ronald Reagan helped precipitate a peace agreement at a meeting of five Central American chiefs of state in July 1987, which won Costa Rican President Oscar Arias the Nobel Peace Prize. It also sparked the wrath of Republicans determined to destroy Jim Wright, despite the fact that it led to free elections in which Ortega was defeated by Violeta Chamorro in the 1990 presidential election, but he remained an important figure in Nicaraguan opposition politics, gradually moderating in his political position from Marxism–Leninism to democratic socialism. He was an unsuccessful candidate for president in 1996 and 2001, before winning the 2006 presidential election. In office, he made alliances with fellow Latin American socialists, such as Venezuelan President Hugo Chávez, and under his leadership, Nicaragua joined the Bolivarian Alliance for the Americas.
Personal life.
Early years.
Ortega was born in La Libertad, department of Chontales, Nicaragua. His parents, Daniel Ortega Cerda and Lidia Saavedra, were opposed to the regime of Anastasio Somoza Debayle. His mother was imprisoned by Somoza's National Guard for being in possession of "love letters" which the police stated were coded political missives. He has two brothers, Humberto Ortega, former General, military leader and published writer, and Camilo Ortega, who died during combat in 1978. He also had a sister named Germania who is also deceased.
Ortega was arrested for political activities at the age of 15, and quickly joined the then-underground Sandinista National Liberation Front (FSLN). He was imprisoned in 1967 for taking part in robbing a branch of the Bank of America while brandishing a machine gun, but was released in late 1974 along with other Sandinista prisoners in exchange for Somocista hostages. While he was imprisoned at the El Modelo jail, just outside Managua, he wrote poems, one of which he titled ""I Never Saw Managua When Miniskirts Were in Fashion"". During his imprisonment, Ortega was severely tortured. After his release, Ortega was exiled to Cuba, where he received several months of guerrilla training. He later returned to Nicaragua secretly.
Ortega married Rosario Murillo in 1979 in a secret ceremony. and moved to Costa Rica with her three children from a previous marriage. Ortega remarried Murillo in 2005 to have the marriage recognized by the Roman Catholic Church. The couple has eight children, three of them together. She is currently the government's spokeswoman and a government minister, among other positions. Ortega adopted stepdaughter Zoilamérica Narváez in 1986, through a court case.
The Sandinista revolution (1979–1990).
When Somoza was overthrown by the FSLN in July 1979, Ortega became a member of the five-person Junta of National Reconstruction, which also included Sandinista militant Moisés Hassan, novelist Sergio Ramírez, businessman Alfonso Robelo, and Violeta Barrios de Chamorro, the widow of a murdered journalist. The FSLN came to dominate the junta, Robelo and Chamorro resigned, and in 1981 Ortega became the coordinator of the Junta. As the only member of the FSLN National Directorate in the Junta, he was the effective leader of the country. The FSLN embarked upon an ambitious programme of social reform upon attaining power. 5 million acres of land were redistributed to about 100,000 families, a literacy drive was launched, and health improvements were carried out which got rid of polio and reduced other diseases. The Sandinista government implemented a policy of forced conscription for all men aged 17 to 35. The Sandinistas used this army to help guerrilla groups throughout Central America. Ortega pursued a policy of centrally planned economy and nationalization. Ortega took a very hard line against opposition to his policies: On 21 February 1981, the Sandinista army killed 7 Miskito Indians and wounded 17. Forced displacement has also been documented to have occurred with the native population: 10,000 individuals had been moved by 1982. Thousands of Indians took refuge in Honduras and 14,000 were imprisoned in Nicaragua. Anthropologist Gilles Bataillon termed this "politics of ethnocide" in Nicaragua. The Indians formed two rebel groups – the Misura and Misurasata. They were joined in the north by Nicaraguan Democratic Force (FDN) and in the south by former Sandinistas and peasantry who under the leadership of Edén Pastora were resisting forced collectivization.
In 1981, United States President Ronald Reagan accused the FSLN of joining with Soviet-backed Cuba in supporting Marxist revolutionary movements in other Latin American countries such as El Salvador. People within the Reagan administration authorized the Central Intelligence Agency to begin financing, arming and training rebels, some of whom were former officers from Somoza's National Guard, as anti-Sandinista guerrillas. These were known collectively as the Contras. This also led to one of the largest political scandals in US history, (the Iran Contra Affair), when Oliver North and several members of the Reagan administration defied the Boland Amendment, selling arms to Iran and then using the proceeds to fund the Contras. Soon the country was in a civil war that claimed 30,000 Nicaraguan lives.
The tactics used by the Sandinista government to fight the Contras have been criticized by some historians for their suppression of civil rights. On 15 March 1982, the Junta declared a state of siege, which allowed it to close independent radio stations, suspend the right of association and limit the freedom of trade unions. Nicaragua's Permanent Commission on Human Rights condemned Sandinista human rights violations, accusing them of killing and disappearing thousands in the first few years of the war. However, some historians accuse the Contras of having a far poorer Human Rights Record during the same period, with documented cases of murder, rape and torture used to terrorize the rural population. There is also evidence that the Contras engaged in destruction of schools, hospitals, and other infrastructure, in order to disrupt the social reform programs of the Sandinistas.
At the 1984 general election Ortega won the presidency with 67% of the vote and took office on 10 January 1985. International observers judged the election to be the first free election held in the country in more than half a century. A report by an Irish governmentary delegation stated: "The electoral process was carried out with total integrity. The seven parties participating in the elections represented a broad spectrum of political ideologies." The general counsel of New York's Human Rights Commission described the election as "free, fair and hotly contested." A study by the US Latin American Studies Association (LASA) concluded that the FSLN (Sandinista Front) "did little more to take advantage of its incumbency than incumbent parties everywhere (including the U.S.) routinely do." However some people described the election as "rigged". According to a detailed study, since the 1984 election was for posts subordinate to the Sandinista Directorate, the elections were no more subject to approval by vote than the Central Committee of the Communist Party is in countries of the East Bloc.
33 percent of the Nicaraguan voters cast ballots for one of six opposition parties—three to the right of the Sandinistas, three to the left—which had campaigned with the aid of government funds and free TV and radio time. Two conservative parties captured a combined 23 percent of the vote. They held rallies across the country (a few of which were disrupted by FSLN supporters) and blasted the Sandinistas in harsh terms. Most foreign and independent observers noted this pluralism in debunking the Reagan administration charge—ubiquitous in the US media—that it was a "Soviet-style sham" election. Some opposition parties boycotted the election, allegedly under pressure from US embassy officials, and so it was denounced as being unfair by the Reagan administration. Reagan thus maintained that he was justified to continue supporting what he referred to as the Contras' "democratic resistance".
Interim years (1990–2006).
In the 1990 presidential election, Ortega lost to Violeta Barrios de Chamorro, his former colleague in the junta. Chamorro was supported by the US and a 14-party anti-Sandinista alliance known as the National Opposition Union (Unión Nacional Oppositora, UNO), an alliance that ranged from conservatives and liberals to communists. Contrary to what most observers expected, Chamorro shocked Ortega and won the election. In Ortega's concession speech the following day he vowed to keep "ruling from below" a reference to the power that the FSLN still wielded in various sectors. He also stressed his belief that the Sandinistas had the goal of bringing "dignity" to Latin America, and not necessarily to hold on to government posts.
Ortega ran for election again, in October 1996 and November 2001, but lost on both occasions to Arnoldo Alemán and Enrique Bolaños, respectively. In these elections, a key issue was the allegation of corruption. In Ortega's last days as president, through a series of legislative acts known as "The Piñata", estates that had been seized by the Sandinista government (some valued at millions and even billions of US dollars) became the private property of various FSLN officials, including Ortega himself.
Ortega's policies became more moderate during his time in opposition, and he gradually changed much of his former Marxist stance in favor of an agenda of democratic socialism. His Roman Catholic faith has become more public in recent years as well, leading Ortega to embrace a variety of socially conservative policies; in 2006 the FSLN endorsed a strict law banning all abortions in Nicaragua.
Ortega was instrumental in creating the controversial strategic pact between the FSLN and the Constitutional Liberal Party (Partido Liberal Constitucionalista, PLC). The controversial alliance of Nicaragua's two major parties is aimed at distributing power between the PLC and FSLN, and preventing other parties from rising. ""El Pacto,"" as it is known in Nicaragua, is said to have personally benefited former presidents Ortega and Alemán greatly, while constraining then-president Bolaños. One of the key accords of the pact was to lower the percentage necessary to win a presidential election in the first round from 45% to 35%, a change in electoral law that would become decisive in Ortega's favor in the 2006 elections.
Sexual abuse allegations.
In 1998, Daniel Ortega's adopted stepdaughter Zoilamérica Narváez released a 48-page report describing how, she alleged, Ortega had systematically sexually abused her from 1979, when she was 11, until 1990. Ortega and his wife Murillo denied the allegations. The case could not proceed in Nicaraguan courts because Ortega had immunity to prosecution as a member of parliament, and the five-year statute of limitations for sexual abuse and rape charges was judged to have been exceeded. Narváez took a complaint to the Inter American Human Rights Commission, which was ruled admissible on 15 October 2001. On 4 March 2002 the Nicaraguan government accepted the Commission's recommendation of a friendly settlement. As of 2006 Ortega continues to deny the allegations, but Narváez has not withdrawn them.
2006 Presidential election.
In 2006, Daniel Ortega was elected president with 38% of the vote. This occurred despite the fact that the breakaway Sandinista Renovation Movement continued to oppose the FSLN, running former Mayor of Managua Herty Lewites as its candidate for president. However, Lewites died several months before the elections.
The FSLN also won 38 seats in the congressional elections, becoming the party with the largest representation in parliament. The split in the Constitutionalist Liberal Party helped to allow the FSLN to become the largest party in Congress; however, the Sandinista vote had a minuscule split between the FSLN and MRS, and that the liberal party combined is larger than the Frente Faction. In 2010, several liberal congressmen raised accusations about the FSLN presumably attempting to buy votes to pass constitutional reforms that would allow Ortega to run for office for the 6th time since 1984.
Second presidency (2006–present).
Soon after his inauguration, Ortega paid an official visit to Iran and met Iranian President Mahmoud Ahmadinejad. Ortega told the press that the "revolutions of Iran and Nicaragua are almost twin revolutions...since both revolutions are about justice, liberty, self-determination, and the struggle against imperialism." Since the start of his second presidency, various measures have been introduced to combat hunger and to improve access to healthcare, education, credit, and social security. In addition, other reforms have been carried out, including an enhancement of labour rights, the introduction of low-interest loans and training for female micro-entrepreneurs in rural areas, and the distribution of transport subsidies, scholarships, medicine, land titles, and housing materials throughout the population. Altogether, these policies have helped to reduce high levels of poverty and inequality in Nicaragua.
In June 2008 the Nicaraguan Supreme Court disqualified the MRS and the Conservative Party from participation. In November, 2008, the Supreme Electoral Council received national and international criticism following irregularities in municipal elections, but agreed to review results for Managua only, while the opposition demanded a nationwide review. For the first time since 1990, the Council decided not to allow national or international observers to witness the election. Instances of intimidation, violence, and harassment of opposition political party members and NGO representatives have been recorded. Official results show Sandinista candidates winning 94 of the 146 municipal mayorships, compared to 46 for the main opposition Liberal Constitutional Party (PLC). The opposition claimed that marked ballots were dumped and destroyed, that party members were refused access to some of the vote counts and that tallies from many polling places were altered. As a result of the fraud allegations, the European Union suspended $70m of aid, and the US $64m.
With the late-2000s recession, Ortega said that capitalism is in its "death throes" and the Bolivarian Alternative for the People of Our America (ALBA) is the most advanced, Christian and fairest project. He also said God was punishing the United States with the financial crisis for trying to impose its economic principles on poor countries and said God was rewarding Nicaragua with an increase in GDP (PPP) to $2,600 per-capita from $1,800 a decade ago. "It's incredible that in the most powerful country in the world, which spends billions of dollars on brutal wars ... people do not have enough money to stay in their homes."
During an interview with David Frost for the Al Jazeera English programme "Frost Over The World" in March 2009, Ortega suggested that he would like to change the constitution to allow him to run again for president. In Judicial Decision 504, issued on October 19, 2009, the Supreme Court of Justice of Nicaragua declared portions of Articles 147 and 178 of the Constitution of Nicaragua inapplicable; these provisions concerned the eligibility of candidates for president, vice-president, mayor, and vice-mayor—a decision that had the effect of allowing Ortega to run for reelection in 2011.
For this decision, the Sandinista magistrates formed the required quorum by excluding the opposition magistrates and replacing them with Sandinista substitutes, violating the Nicaraguan constitution. The decision was widely denounced by the opposing parties, the church and human rights groups in Nicaragua 
While supporting abortion rights during his presidency during the 1980s, Ortega has since embraced the Catholic Church's position of strong opposition. While non-emergency abortions have long been illegal in Nicaragua, recently even abortions "in the case where the pregnancy endangers the mother's life", otherwise known as therapeutic abortions have been made illegal in the days before the election, with a six-year prison term in such cases, too—a move supported by Ortega.
Ortega himself denies that the abortion legislation outlaws medical procedures necessary to save the woman's life if they result in the termination of pregnancy. "The medical Procedural Code, he says, is not affected by the law, and requires doctors to do what is necessary to save a woman's life if it is threatened by conditions related to her pregnancy." He claims that the accusations that the abortion laws outlaw medical procedures necessary to save the life of the mother are part of "a media war".
Ortega was re-elected president with a vote on November 6 and confirmation on November 16, 2011.
Foreign policy.
On 6 March 2008, following the 2008 Andean diplomatic crisis, Ortega announced that Nicaragua was breaking diplomatic ties with Colombia "in solidarity with the Ecuadorian people". Ortega also stated, "We are not breaking relations with the Colombian people. We are breaking relations with the terrorist policy practiced by Álvaro Uribe's government". The relations were restored with the resolution at a Rio Group summit held in Santo Domingo, Dominican Republic, on 7 March 2008. At the summit Colombia's Álvaro Uribe, Ecuador's Rafael Correa, Venezuela's Hugo Chávez and Ortega publicly shook hands in a show of good will. The handshakes, broadcast live throughout Latin America, appeared to be a signal that a week of military buildups and diplomatic repercussions was over. After the handshakes, Ortega said he would re-establish diplomatic ties with Colombia. Uribe then quipped that he would send him the bill for his ambassador's plane fare.
On 25 May 2008, Ortega, upon learning of the death of FARC guerrilla leader Manuel Marulanda in Colombia, expressed condolences to the family of Marulanda and solidarity with the FARC and called Marulanda an extraordinary fighter who battled against profound inequalities in Colombia. The declarations were protested by the Colombian government and criticized in the major Colombian media outlets.
On 2 September 2008, during ceremonies for the 29th anniversary of the founding of the Nicaraguan army, Ortega announced that "Nicaragua recognizes the independence of South Ossetia and Abkhazia and fully supports the Russian government's position." Ortega's decision made Nicaragua the second country after Russia to recognize the independence of Abkhazia and South Ossetia from Georgia. A day after Venezuela recognised the two Republics, Nicaragua established diplomatic relations with Abkhazia, and followed this by establishing diplomatic links with South Ossetia. Embassies have been mooted, but as of 2013 these had still not been opened.
When getting in office, Ortega was threatened to cut ties with the Republic of China(Taiwan) in order to restore relations with the People's Republic of China (like 1985–1990). But he did not do so. Ortega met with Taiwan's president Ma Ying-jeou in 2009 and both agreed to improve the diplomatic ties between both countries. However, with a trade show from China in Managua in 2010, he is attempting a two-track policy to get benefits from both sides.
In September 2010, after a US report listed Nicaragua as a "major" drug-trafficking centre, with Costa Rica and Honduras, Ortega urged the US Congress and Obama administration to allocate more resources to assist the fight against drug trafficking.
During the Libyan Civil War, Ortega was among the very few leaders who spoke out in clear defense of the embattled Muammar Gaddafi. During a telephone conversation between the two, Ortega told Gaddafi that he was "waging a great battle to defend his nation" and "it's at difficult times that loyalty and resolve are put to the test."

</doc>
<doc id="8779" url="https://en.wikipedia.org/wiki?curid=8779" title="Destroyer">
Destroyer

In naval terminology, a destroyer is a fast maneuverable long-endurance warship intended to escort larger vessels in a fleet, convoy or battle group and defend them against smaller powerful short-range attackers. They were originally developed in the late 19th century as a defence against torpedo boats, and by the time of the Russo-Japanese War in 1904, these "torpedo boat destroyers" (TBD) were "large, swift, and powerfully armed torpedo boats designed to destroy other torpedo boats." Although the term "destroyer" had been used interchangeably with "TBD" and "torpedo boat destroyer" by navies since 1892, the term "torpedo boat destroyer" had been generally shortened to simply "destroyer" by nearly all navies by the First World War.
Before World War II, destroyers were light vessels with little endurance for unattended ocean operations; typically a number of destroyers and a single destroyer tender operated together. After the war, the advent of the guided missile allowed destroyers to take on the surface combatant roles previously filled by battleships and cruisers. This resulted in larger and more powerful guided missile destroyers more capable of independent operation.
At the start of the 21st century, destroyers are the heaviest surface combatant ships in general use, with only three nations (United States, Russia, and Peru) operating the heavier class cruisers, with no battleships or true battlecruisers remaining. Modern destroyers, also known as guided missile destroyers, are equivalent in tonnage but vastly superior in firepower to cruisers of the World War II era, capable of carrying nuclear tipped cruise missiles. Guided missile destroyers such as the "Arleigh Burke" class are actually larger and more heavily armed than most previous ships classified as guided missile cruisers, due to their massive size at long, displacement (9200 tons) and armament of over 90 missiles.
Origins.
The emergence and development of the destroyer was related to the invention of the self-propelled torpedo in the 1860s. A navy now had the potential to destroy a superior enemy battle fleet using steam launches to launch torpedoes. Fast boats armed with torpedoes were built and called torpedo boats. The first seagoing vessel designed to fire the self-propelled Whitehead torpedo was the 33-ton in 1876. She was armed with two drop collars to launch these weapons; these were replaced in 1879 by a single torpedo tube in the bow. By the 1880s, the type had evolved into small ships of 50–100 tons, fast enough to evade enemy picket boats.
At first, the danger to a battle fleet was considered to exist only when at anchor, but as faster and longer-range torpedoes were developed, the threat extended to cruising at sea. In response to this new threat, more heavily gunned picket boats called "catchers" were built which were used to escort the battle fleet at sea. They needed significant seaworthiness and endurance to operate with the battle fleet, and as they necessarily became larger, they became officially designated "torpedo boat destroyers", and by the First World War were largely known as "destroyers" in English. The anti-torpedo boat origin of this type of ship is retained in its name in other languages, including French ("contre-torpilleur"), Italian ("cacciatorpediniere"), Portuguese ("contratorpedeiro"), Czech ("torpédoborec"), Greek ("antitorpiliko","αντιτορπιλικό"), and Dutch ("torpedobootjager").
Once destroyers became more than just catchers guarding an anchorage, it was realized that they were also ideal to take over the role of torpedo boats themselves, so they were fitted with torpedo tubes as well as guns. At that time, and even into World War I, the only function of destroyers was to protect their own battle fleet from enemy torpedo attacks and to make such attacks on the battleships of the enemy. The task of escorting merchant convoys was still in the future.
Early designs.
An important development came with the construction of HMS "Swift" in 1884, later redesignated TB 81. This was a large (137 ton) torpedo boat with four 47 mm quick-firing guns and three torpedo tubes. At , while still not fast enough to engage torpedo boats reliably, the ship at least had the armament to deal with them.
Another forerunner of the torpedo boat destroyer was the Japanese torpedo boat ("Falcon"), built in 1885. Designed to Japanese specifications and ordered from the London Yarrow shipyards in 1885, she was transported in parts to Japan, where she was assembled and launched in 1887. The long vessel was armed with four 1-pounder (37 mm) quick-firing guns and six torpedo tubes, reached , and at 203 tons, was the largest torpedo boat built to date. In her trials in 1889, "Kotaka" demonstrated that she could exceed the role of coastal defense, and was capable of accompanying larger warships on the high seas. The Yarrow shipyards, builder of the parts for the "Kotaka", "considered Japan to have effectively invented the destroyer".
Torpedo gunboat.
The first vessel designed for the explicit purpose of hunting and destroying torpedo boats was the torpedo gunboat. Essentially very small cruisers, torpedo gunboats were equipped with torpedo tubes and an adequate gun armament, intended for hunting down smaller enemy boats. By the end of the 1890s torpedo gunboats were made obsolete by their more successful contemporaries, the torpedo boat destroyers, which were much faster.
The first example of this was , designed by Nathaniel Barnaby in 1885, and commissioned in response to the Russian War scare. The gunboat was armed with torpedoes and designed for hunting and destroying smaller torpedo boats. Exactly long and in beam, she displaced 550 tons. Built of steel, "Rattlesnake" was un-armoured with the exception of a -inch protective deck. She was armed with a single 4-inch/25-pounder breech-loading gun, six 3-pounder QF guns and four torpedo tubes, arranged with two fixed tubes at the bow and a set of torpedo dropping carriages on either side. Four torpedo reloads were carried.
A number of torpedo gunboat classes followed, including the "Grasshopper" class, the , the and the - all built for the Royal Navy during the 1880s and the 1890s.
Fernando Villaamil, second officer of the Ministry of the Navy of Spain, designed his own torpedo gunboat to combat the threat from the torpedo boat. He asked several British shipyards to submit proposals capable of fulfilling these specifications. In 1885 the Spanish Navy chose the design submitted by the shipyard of James and George Thomson of Clydebank, near the Yarrow shipyards. ("Destroyer" in Spanish) was laid down at the end of the year, launched in 1886, and commissioned in 1887.
She displaced 348 tons, and was equipped with triple-expansion engines generating , for a maximum speed of , which made her one of the faster ships in the world in 1888. She was armed with one Spanish-designed Hontoria breech-loading gun, four (6-pounder) Nordenfelt guns, two (3-pdr) Hotchkiss cannons and two Schwartzkopff torpedo tubes. The ship carried three torpedoes per tube. She was manned by a crew of 60.
In terms of gunnery, speed and dimensions, the specialised design to chase torpedo boats and her high seas capabilities, "Destructor" was an important precursor to the torpedo boat destroyer.
Development of the modern destroyer.
The first ships to bear the formal designation "torpedo boat destroyer" (TBD) were the of two ships and of two ships of the Royal Navy.
Early torpedo gunboat designs lacked the range and speed to keep up with the fleet they were supposed to protect. In 1892, the Third Sea Lord, Rear Admiral John "Jacky" Fisher ordered the development of a new type of ships equipped with the then novel water-tube boilers and quick-firing small calibre guns. Six ships to the specifications circulated by the Admiralty were ordered initially, comprising three different designs each produced by a different shipbuilder: and from John I. Thornycroft & Company, and from Yarrows, and and from Laird, Son & Company.
These torpedo boat destroyers all featured a turtleback (i.e. rounded) forecastle that was characteristic of early British TBDs. and were both built by Thornycroft, displaced 260 tons (287.8 tons full load) and were 185 feet in length. They were armed with one 12-pounder gun and three 6-pounder guns, with one fixed 18-in torpedo tube in the bow plus two more torpedo tubes on a revolving mount abaft the two funnels. Later the bow torpedo tube was removed and two more 6-pounder guns added instead. They produced 4,200 hp from a pair of Thornycroft water-tube boilers, giving them a top speed of 27 knots, giving the range and speed to travel effectively with a battle fleet. In common with subsequent early Thornycroft boats, they had sloping sterns and double rudders.
The French navy, an extensive user of torpedo boats, built its first torpedo boat destroyer in 1899, with the 'torpilleur d'escadre'. The United States commissioned its first torpedo boat destroyer, , Destroyer No. 1, in 1902 and by 1906 there were 16 destroyers in service with the US Navy.
Subsequent improvements.
Torpedo Boat Destroyer designs continued to evolve around the turn of the 20th century in several key ways. The first was the introduction of the steam turbine. The spectacular unauthorized demonstration of the turbine powered "Turbinia" at the 1897 Spithead Navy Review, which, significantly, was of torpedo boat size, prompted the Royal Navy to order a prototype turbine powered destroyer, of 1899. This was the first turbine warship of any kind and achieved a remarkable on sea trials. By 1910 the turbine had been widely adopted by all navies for their faster ships.
The second development was the replacement of the torpedo-boat-style turtleback foredeck by a raised forecastle for the new s built in 1903, which provided better sea-keeping as well as more space below deck.
The British experimented with oil propulsion for the of 1905 but switched temporarily back to coal for the later in 1909. Other navies also adopted oil, for instance the USN with the of 1909.
In spite of all this variety, destroyers adopted a largely similar pattern. The hull was long and narrow, with a relatively shallow draft. The bow was either raised in a forecastle or covered under a turtleback; underneath this were the crew spaces, extending 1/4 to 1/3 the way along the hull. Aft of the crew spaces was as much engine space as the technology of the time would allow: several boilers and engines or turbines. Above deck, one or more quick-firing guns were mounted in the bows, in front of the bridge; several more were mounted amidships and astern. Two tube mountings (later on, multiple mountings) were generally found amidships.
Between 1892 and 1914 destroyers became markedly larger: initially 420 tons with a length of for the US Navy's first of torpedo boat destroyers, up to the First World War with long destroyers displacing 1000 tons was not unusual. However, construction remained focused on putting the biggest possible engines into a small hull, resulting in a somewhat flimsy construction. Often hulls were built of steel only 1/8 in thick.
By 1910 the steam-driven displacement (that is, not hydroplaning) torpedo boat had become redundant as a separate type. Germany nevertheless continued to build such boats until the end of World War I, although these were effectively small coastal destroyers. In fact Germany never distinguished between the two types, giving them pennant numbers in the same series and never giving names to destroyers. Ultimately the term "torpedo boat" came to be attached to a quite different vessel – the very fast hydroplaning motor driven MTB.
Early use and World War I.
The torpedo boat destroyer's initial purpose was to protect against torpedo boats, but navies soon appreciated the flexibility of the fast, multi-purpose vessel that resulted. Vice-Admiral Sir Baldwin Walker laid down destroyer duties for the Royal Navy:
Early destroyers were extremely cramped places to live, being "without a doubt magnificent fighting vessels... but unable to stand bad weather." During the Russo-Japanese War in 1904, the commander of the torpedo boat destroyer IJN "Akatsuki" described "being in command of a destroyer for a long period, especially in wartime... is not very good for the health." Stating that he had originally been strong and healthy, he continued, "life on a destroyer in winter, with bad food, no comforts, would sap the powers of the strongest men in the long run. A "destroyer" is always more uncomfortable than the others, and rain, snow, and "sea-water" combine to make them damp; in fact, in bad weather there is not a dry spot where one can rest for a moment."
The Japanese destroyer commander finished with, "Yesterday I looked at myself in a mirror for a long time; I was disagreeably surprised to see my face thin, full of wrinkles, and as old as though I were fifty. My clothes (uniform) cover nothing but a skeleton, and my bones are full of rheumatism."
Although officially classified as a "torpedo boat" in 1898 by the US Navy, , a long all steel vessel displacing 165 tons, was described by her commander, LT. John C. Fremont, as "...a compact mass of machinery not meant to keep the sea nor to live in... as five sevenths of the ship are taken up by machinery and fuel, whilst the remaining two sevenths, fore and aft, are the crew's quarters; officers forward and the men placed aft. And even in those spaces are placed anchor engines, steering engines, steam pipes, etc. rendering them unbearably hot in tropical regions."
Combat.
The torpedo boat destroyer's first major use in combat came during the Japanese surprise attack on the Russian fleet anchored in Port Arthur at the opening of the Russo-Japanese War on 8 February 1904.
Three destroyer divisions attacked the Russian fleet in port, firing a total of 18 torpedoes. However, only two Russian battleships were seriously damaged due to the proper deployment of torpedo nets. The Russian flagship, the battleship , which had her nets deployed, had at least four enemy torpedoes "hung up" in them, and other warships were similarly saved from further damage.
While capital ship engagements were scarce in World War I, destroyer units were almost continually engaged in raiding and patrol actions. The first shot of the war at sea was fired on 5 August 1914 by a destroyer of the 2nd Flotilla, , in an engagement with the German auxiliary minelayer .
Destroyers were involved in the skirmishes that prompted the Battle of Heligoland Bight, and filled a range of roles in the Battle of Gallipoli, acting as troop's transports and fire support vessels, as well as their fleet-screening role. Over 80 British destroyers and 60 German torpedo-boats took part in the Battle of Jutland, which involved pitched small-boat actions between the main fleets, and several foolhardy attacks by unsupported destroyers on capital ships. Jutland also concluded with a messy night action between the German High Seas Fleet and part of the British destroyer screen.
The threat evolved by World War I with the development of the submarine, or U-boat. The submarine had the potential to hide from gunfire and close underwater to fire torpedoes. Early-war destroyers had the speed and armament to intercept submarines before they submerged, either by gunfire or by ramming. Destroyers also had a shallow enough draft that torpedoes would find it difficult to hit them.
The desire to attack submarines underwater led to rapid destroyer evolution during the war; they were quickly equipped with strengthened bows for ramming, depth charges and hydrophones for identifying submarine targets. The first submarine casualty to a destroyer was the German , rammed by on 29 October 1914. While "U-19" was only damaged, the next month successfully sank . The first depth-charge sinking was on 4 December 1916, when was sunk by HMS "Llewellyn".
The submarine threat meant that many destroyers spent their time on anti-submarine patrol; once Germany adopted unrestricted submarine warfare in January 1917, destroyers were called on to escort merchant convoys. US Navy destroyers were among the first American units to be dispatched upon the American entry to the war, and a squadron of Japanese destroyers even joined Allied patrols in the Mediterranean. Patrol duty was far from safe; of the 67 British destroyers lost in the war, collisions accounted for 18, while 12 were wrecked.
At the end of the war the state-of-the-art was represented by the British W-class.
Inter-war & World War II.
The trend during World War I had been towards larger destroyers with heavier armaments. A number of opportunities to fire at capital ships had been missed during the War, because destroyers had expended all their torpedoes in an initial salvo. The British V and W classes of the late war had sought to address this by mounting six torpedo tubes in two triple mounts, instead of the four or two on earlier models. The 'V' and 'W's set the standard of destroyer building well into the 1920s.
Most other nations replied with similar larger ships. The US adopted twin five-inch (127 mm) guns, and the subsequent and (the latter of 1934) increased the number of torpedo tubes to 12 and 16 respectively.
In the Mediterranean, the Italian Navy's building of very fast light cruisers of the prompted the French to produce exceptional destroyer designs. The French had long been keen on large destroyers, with their of 1922 displacing over 2,000 tons and carrying 130 mm guns; a further three similar classes were produced around 1930. The of 1935 carried five guns and nine torpedo tubes, but could achieve speeds of , which remains the record speed for a steamship and for any destroyer. The Italians' own destroyers were almost as swift, most Italian designs of the 1930s being rated at over , while carrying torpedoes and either four or six 120 mm guns.
Germany started to build destroyers again during the 1930s as part of Hitler's rearmament program. The Germans were also fond of large destroyers, but while the initial Type 1934 displaced over 3,000 tons, their armament was equal to smaller vessels. This changed from the Type 1936 onwards, which mounted heavy guns. German destroyers also used innovative high-pressure steam machinery: while this should have helped their efficiency, it more often resulted in mechanical problems.
Once German and Japanese rearmament became clear, the British and American navies consciously focused on building destroyers that were smaller but more numerous than those used by other nations. The British built a series of destroyers (the to ) which were about 1,400 tons standard displacement, had four guns and eight torpedo tubes; the American of 1938 similar in size, but carried five guns and ten torpedo tubes. Realizing the need for heavier gun armament, the British built the of 1936 (sometimes called "Afridi" after one of two lead ships). These ships displaced 1,850 tons and were armed with eight guns in four twin turrets and four torpedo tubes. These were followed by the J-class and L-class destroyers, with six guns in twin turrets and eight torpedo tubes.
Anti-submarine sensors included sonar (or ASDIC), although training in their use was indifferent. Anti-submarine weapons changed little, and ahead-throwing weapons, a need recognized in World War I, had made no progress.
Combat.
During the 1920s and 1930s destroyers were often deployed to areas of diplomatic tension or humanitarian disaster. British and American destroyers were common on the Chinese coast and rivers, even supplying landing parties to protect colonial interests.
By World War II the threat had evolved once again. Submarines were more effective, and aircraft had become important weapons of naval warfare; once again the early-war fleet destroyers were ill-equipped for combating these new targets. They were fitted with new light anti-aircraft guns, radar, and forward-launched ASW weapons, in addition to their existing dual-purpose guns, depth charges, and torpedoes. In most cases torpedo and/or dual-purpose gun armament was reduced to accommodate new anti-air and anti-submarine weapons. By this time the destroyers had become large, multi-purpose vessels, expensive targets in their own right. As a result, casualties on destroyers were among the highest.
The need for large numbers of anti-submarine ships led to the introduction of smaller and cheaper specialized anti-submarine warships called corvettes and frigates by the Royal Navy and destroyer escorts by the USN. A similar programme was belatedly started by the Japanese (see ). These ships had the size and displacement of the original torpedo boat destroyers that the contemporary destroyer had evolved from.
Post-World War II.
Some conventional destroyers were completed in the late 1940s and 1950s which built on wartime experience. These vessels were significantly larger than wartime ships and had fully automatic main guns, unit Machinery, radar, sonar, and antisubmarine weapons such as the Squid mortar. Examples include the British , US , and the Soviet s.
Some World War II–vintage ships were modernized for anti-submarine warfare, and to extend their service lives, to avoid having to build (expensive) brand-new ships. Examples include the US FRAM I programme and the British Type 15 frigates converted from fleet destroyers.
The advent of surface-to-air missiles and surface-to-surface missiles, such as the Exocet, in the early 1960s changed naval warfare. Guided missile destroyers (DDG in the US Navy) were developed to carry these weapons and protect the fleet from air, submarine and surface threats. Examples include the Soviet , the British , and the US .
Destroyers in preservation.
A number of countries have destroyers preserved as museum ships. These include:

</doc>
<doc id="8780" url="https://en.wikipedia.org/wiki?curid=8780" title="Debian GNU/Hurd">
Debian GNU/Hurd

Debian GNU/Hurd is the Debian project's distribution of the GNU operating system, using the GNU Hurd microkernel.
Debian GNU/Hurd has been in development since 1998, and made a formal release in May 2013, with 78% of the software packaged for Debian GNU/Linux ported to the GNU Hurd. The Debian GNU/Hurd developers were hoping to be able to release it with Debian "Wheezy" in 2013. However, Hurd is not yet an official Debian release, and is maintained and developed as an unofficial port.
According to the GNU/Hurd compatibility guide, the current version is compatible with laptops but there is no PCMCIA support yet.
Debian GNU/Hurd is distributed as an installer CD (running the official Debian installer) or ready-to-run virtual disk image (Live CD, Live USB). The CD uses the IA-32 architecture, making it compatible with IA-32 and x86-64 PCs.
The current version of Debian GNU/Hurd is 2015, published in April 2015.

</doc>
<doc id="8781" url="https://en.wikipedia.org/wiki?curid=8781" title="Dorothy Parker">
Dorothy Parker

Dorothy Parker (August 22, 1893 – June 7, 1967) was an American poet, short story writer, critic, and satirist, best known for her wit, wisecracks and eye for 20th-century urban foibles.
From a conflicted and unhappy childhood, Parker rose to acclaim, both for her literary output in publications such as "The New Yorker" and as a founding member of the Algonquin Round Table. Following the breakup of the circle, Parker traveled to Hollywood to pursue screenwriting. Her successes there, including two Academy Award nominations, were curtailed when her involvement in left-wing politics led to a place on the Hollywood blacklist.
Dismissive of her own talents, she deplored her reputation as a "wisecracker." Nevertheless, her literary output and reputation for sharp wit have endured.
Early life and education.
Also known as Dot or Dottie, Parker was born Dorothy Rothschild to Jacob Henry and Eliza Annie Rothschild (née Marston) at 732 Ocean Avenue in Long Branch, New Jersey, where her parents had a summer beach cottage. Dorothy's mother was of Scottish descent, and her father was of German Jewish descent. Parker wrote in her essay "My Hometown" that her parents got her back to their Manhattan apartment shortly after Labor Day so she could be called a true New Yorker. Her mother died in West End in July 1898, when Parker was a month shy of turning five. Her father remarried in 1900 to a woman named Eleanor Francis Lewis. Parker hated her father and stepmother, accusing her father of being physically abusive and refusing to call Eleanor either "mother" or "stepmother", instead referring to her as "the housekeeper". She grew up on the Upper West Side and attended a Roman Catholic elementary school at the Convent of the Blessed Sacrament on West 79th Street with sister Helen, despite having a Jewish father and Protestant stepmother. (Mercedes de Acosta was a classmate.) Parker once joked that she was asked to leave following her characterization of the Immaculate Conception as "spontaneous combustion". Her stepmother died in 1903, when Parker was nine. Parker later went to Miss Dana's School, a finishing school in Morristown, New Jersey. She graduated from Miss Dana's School in 1911, at the age of 18. Following her father's death in 1913, she played piano at a dancing school to earn a living while she worked on her verse.
She sold her first poem to "Vanity Fair" magazine in 1914 and some months later was hired as an editorial assistant for another Condé Nast magazine, "Vogue". She moved to "Vanity Fair" as a staff writer after two years at "Vogue".
In 1917, she met and married a Wall Street stockbroker, Edwin Pond Parker II
(1893–1933), but they were separated by his army service in World War I. She had ambivalent feelings about her Jewish heritage given the strong antisemitism of that era and joked that she married to escape her name.
Algonquin Round Table years.
Her career took off while she was writing theatre criticism for "Vanity Fair", which she began to do in 1918 as a stand-in for the vacationing P. G. Wodehouse. At the magazine, she met Robert Benchley, who became a close friend, and Robert E. Sherwood. The trio began lunching at the Algonquin Hotel on a near-daily basis and became founding members of the Algonquin Round Table. The Round Table numbered among its members the newspaper columnists Franklin Pierce Adams and Alexander Woollcott. Through their re-printing of her lunchtime remarks and short verses, particularly in Adams' column "The Conning Tower", Dorothy began developing a national reputation as a wit. One of her most famous comments was made when the group was informed that famously taciturn former president Calvin Coolidge had died; Parker remarked, "How could they tell?"
Parker's caustic wit as a critic initially proved popular, but she was eventually terminated by "Vanity Fair" in 1920 after her criticisms began to offend powerful producers too often. In solidarity, both Benchley and Sherwood resigned in protest.
When Harold Ross founded "The New Yorker" in 1925, Parker and Benchley were part of a "board of editors" established by Ross to allay concerns of his investors. Parker's first piece for the magazine appeared in its second issue. Parker became famous for her short, viciously humorous poems, many about the perceived ludicrousness of her many (largely unsuccessful) romantic affairs and others wistfully considering the appeal of suicide.
The next 15 years were Parker's greatest period of productivity and success. In the 1920s alone she published some 300 poems and free verses in "Vanity Fair", "Vogue", "The Conning Tower" and "The New Yorker" as well as "Life", "McCall's" and "The New Republic".
Parker published her first volume of poetry, "Enough Rope", in 1926. The collection sold 47,000 copies and garnered impressive reviews. "The Nation" described her verse as "caked with a salty humor, rough with splinters of disillusion, and tarred with a bright black authenticity". Although some critics, notably the "New York Times" reviewer, dismissed her work as "flapper verse", the volume helped cement Parker's reputation for sparkling wit. Parker released two more volumes of verse, "Sunset Gun" (1928) and "Death and Taxes" (1931), along with the short story collections "Laments for the Living" (1930) and "After Such Pleasures" (1933). "Not So Deep as a Well" (1936) collected much of the material previously published in "Rope", "Gun" and "Death" and she re-released her fiction with a few new pieces in 1939 under the title "Here Lies".
She collaborated with playwright Elmer Rice to create "Close Harmony", which ran on Broadway in December 1924. The play was well received in out-of-town previews and was favorably reviewed in New York but closed after a run of just 24 performances. It did, however, become a successful touring production under the title "The Lady Next Door".
Some of Parker's most popular work was published in "The New Yorker" in the form of acerbic book reviews under the byline "Constant Reader" (her response to the whimsy of A. A. Milne's "The House at Pooh Corner": "Tonstant Weader fwowed up."). Her reviews appeared semi-regularly from 1927 to 1933, were widely read, and were later published in a collection under the name "Constant Reader" in 1970.
Her best-known short story, "Big Blonde", published in "The Bookman" magazine, was awarded the O. Henry Award as the best short story of 1929. Her short stories, though often witty, were also spare and incisive, and more bittersweet than comic.
She eventually separated from her husband, divorcing in 1928, and had a number of affairs. Her lovers included reporter-turned-playwright Charles MacArthur and the publisher Seward Collins. Her relationship with MacArthur resulted in a pregnancy, about which Parker is alleged to have remarked, "how like me, to put all my eggs into one bastard." She had an abortion, and fell into a depression that culminated in her first attempt at suicide.
It was toward the end of this period that Parker began to become politically aware and active. What would become a lifelong commitment to activism began in 1927 with the pending executions of Sacco and Vanzetti. Parker travelled to Boston to protest the proceedings. She and fellow Round Tabler Ruth Hale were arrested, and Parker eventually pleaded guilty to a charge of "loitering and sauntering", paying a $5 fine.
Hollywood.
In 1934, she married Alan Campbell, an actor with aspirations to become a screenwriter. Like Parker, he was half-Jewish and half-Scottish. He was reputed to be bisexual—indeed, Parker claimed in public that he was "queer as a billy goat". The pair moved to Hollywood and signed ten-week contracts with Paramount Pictures, with Campbell (who was also expected to act) earning $250 per week and Parker earning $1,000 per week. They would eventually earn $2,000 and in some instances upwards of $5,000 per week as freelancers for various studios. She and Campbell worked on more than 15 films.
In 1936, she contributed lyrics for the song "I Wished on the Moon", with music by Ralph Rainger. The song was introduced in "The Big Broadcast of 1936" by Bing Crosby.
With Robert Carson and Campbell, she wrote the script for the 1937 film "A Star is Born", for which they were nominated for an Academy Award for Best Writing—Screenplay. She wrote additional dialogue for "The Little Foxes" in 1941 and received another Oscar nomination, with Frank Cavett, for 1947's "Smash-Up, the Story of a Woman", starring Susan Hayward.
After the United States entered the Second World War, Parker and Alexander Woollcott collaborated to produce an anthology of her work as part of a series published by Viking Press for servicemen stationed overseas. With an introduction by Somerset Maugham the volume compiled over two dozen of Parker's short stories along with selected poems from "Enough Rope", "Sunset Gun", and "Death and Taxes". It was released in the United States in 1944 under the title "The Portable Dorothy Parker". Parker's is one of only three of the "Portable" series (the other two being William Shakespeare and The Bible) to remain continuously in print.
During the 1930s and 1940s, Parker became an increasingly vocal advocate of causes like civil liberties and civil rights, and a frequent critic of those in authority. She reported on the Loyalist cause in Spain for the Communist magazine "The New Masses" in 1937. At the behest of Otto Katz, a covert Soviet Comintern agent and operative of German Communist Party agent Willi Muenzenberg, Parker helped to found the Hollywood Anti-Nazi League in 1936 (which was suspected by the FBI of being a Communist Party front). The Hollywood Anti-Nazi League's membership eventually grew to some 4,000 strong. Its often wealthy members' contributions (probably not intended to support Communism) were, in the words of David Caute, "able to contribute as much to Party funds as the whole American working class".
Parker also served as chair of the Joint Anti-Fascist Rescue Committee. She organized Project Rescue Ship to transport Loyalist veterans to Mexico, headed Spanish Children's Relief and lent her name to many other left-wing causes and organizations. Her former Round Table friends saw less and less of her, with her relationship with Robert Benchley being particularly strained (although they would reconcile). Parker met S. J. Perelman at a party in 1932, and despite a rocky start (Perelman called it "a scarifying ordeal")—they remained friends for the next 35 years, even becoming neighbors when the Perelmans helped Parker and Campbell buy a run-down farm in Bucks County, Pennsylvania.
Parker was listed as a Communist by the publication "Red Channels" in 1950. The FBI compiled a 1,000-page dossier on her because of her suspected involvement in Communism during the McCarthy era. As a result, she was placed on the Hollywood blacklist by the movie studio bosses. Her final screenplay was "The Fan", a 1949 adaptation of Oscar Wilde's "Lady Windermere's Fan", directed by Otto Preminger. Her marriage to Campbell was tempestuous, with tensions exacerbated by Parker's increasing alcohol consumption and Campbell's long-term affair with a married woman while he was in Europe during World War II. They divorced in 1947, then remarried in 1950. Parker moved back to New York in 1952, living at the Volney residential hotel at 23 East 74th Street on the Upper East Side. From 1957 to 1962, she wrote book reviews for "Esquire", though these pieces were increasingly erratic owing to her continued abuse of alcohol. She returned to Hollywood in 1961 and reconciled with Campbell. In the next two years, they worked together on a number of unproduced projects. Campbell committed suicide by drug overdose in 1963.
Later life and death.
Following Campbell's death, Parker returned to New York City and the Volney residential hotel. In her later years, she would come to denigrate the group that had brought her such early notoriety, the Algonquin Round Table:
Parker was heard occasionally on radio, including "Information Please" (as a guest) and "Author, Author" (as a regular panelist). She wrote for the "Columbia Workshop", and both Ilka Chase and Tallulah Bankhead used her material for radio monologues.
Parker died on June 7, 1967, of a heart attack at the age of 73. In her will, she bequeathed her estate to Dr. Martin Luther King, Jr. Following King's death, her estate was passed on to the NAACP. Her executor, Lillian Hellman, bitterly but unsuccessfully contested this disposition. Her ashes remained unclaimed in various places, including her attorney Paul O'Dwyer's filing cabinet, for approximately 17 years.
Posthumous honors.
In 1988, the NAACP claimed Parker's remains and designed a memorial garden for them outside their Baltimore headquarters. The plaque reads,
On August 22, 1992, the 99th anniversary of Parker's birth, the United States Postal Service issued a 29¢ U.S. commemorative postage stamp in the Literary Arts series. The Algonquin Round Table, as well as the number of other literary and theatrical greats who lodged there, helped earn the Algonquin Hotel its status as a New York City Historic Landmark. The hotel was so designated in 1987. In 1996 the hotel was designated a National Literary Landmark by the Friends of Libraries USA based on the contributions of Parker and other members of the Round Table. The organization's bronze plaque is attached to the front of the hotel. Her birthplace was also designated a National Literary Landmark by Friends of Libraries USA in 2005 and a bronze plaque marks the spot where the home once stood.
In 2014, Parker was elected to the New Jersey Hall of Fame.
In popular culture.
Parker was the inspiration for a number of fictional characters in several plays of her day. These included "Lily Malone" in Philip Barry's "Hotel Universe" (1932), "Mary Hilliard" (played by Ruth Gordon) in George Oppenheimer's "Here Today" (1932), "Paula Wharton" in Gordon's 1944 play "Over Twenty-one" (directed by George S. Kaufman), and "Julia Glenn" in the Kaufman-Moss Hart collaboration "Merrily We Roll Along" (1934). Kaufman's representation of her in "Merrily We Roll Along" led Parker, once his Round Table compatriot, to despise him. She also appeared as "Daisy Lester" in Charles Brackett's 1934 novel "Entirely Surrounded". She is mentioned in the original introductory lyrics in Cole Porter' song Just One of Those Things from the 1935 Broadway musical Jubilee which have been retained in the standard interpretation of the song when it became part of the Great American Songbook.
Parker appears as a character in the novel "The Dorothy Parker Murder Case" by George Baxt (1984), in a series of "Algonquin Round Table Mysteries" by J.J. Murphy (2011– ), and in Ellen Meister's novel "Farewell, Dorothy Parker" (2013). She is the main character in a short story, "Love For Miss Dottie," by Larry N Mayer, which was selected by Mary Gaitskill for the collection "Best New American Voices 2009" (Harcourt).
She has been portrayed on film and television by Dolores Sutton in "F. Scott Fitzgerald in Hollywood" (1976), Rosemary Murphy in "Julia" (1977), Bebe Neuwirth in Dash and Lilly (1999) and Jennifer Jason Leigh in "Mrs. Parker and the Vicious Circle" (1994). Neuwirth was nominated for an Emmy Award for her performance, and Leigh received a number of awards and nominations, including a Golden Globe nomination.
Parker, along with other figures of the era including Ira Gershwin and George Gershwin, is featured as a character in Act 1, Scene 12 of the stage musical version of "Thoroughly Modern Millie", "Muzzy's Party Scene".
Television creator Amy Sherman-Palladino named her production company 'Dorothy Parker Drank Here Productions' in tribute to Parker.
Standup comedian Jen Kirkman portrayed Dorothy Parker in an edition of the Dead Authors Podcast at the Upright Citizens Brigade Theater in Los Angeles in 2011.
A one-woman show, "Dorothy Parker's Room Enough For Two" starring Terrie Frankel, was produced in July 1993 at the Groundlings Theatre in Hollywood, California.
Prince has a song entitled "The Ballad of Dorothy Parker", on his album "Sign o' the Times".
In the 1999 film "Girl, Interrupted", the character Lisa recites Parker's poem "Resume".
The Thrilling Adventure Hour podcast has Dorothy Parker as a recurring character (as played by Annie Savage) and member of the Algonquin Four. After being struck by a comet, the group gained powers parodying The Fantastic Four. Parker gained rock-like skin as a self-proclaimed "rock man", and is the dim-witted muscle of the team. Her catchphrase is "Dorothy Parker smash!"
Tucson actress Lesley Abrams wrote and performed the one-woman show "Dorothy Parker's Last Call" in 2009 in Tucson, Arizona at the Winding Road Theater Ensemble and reprised the role at the Live Theatre Workshop in Tucson in 2014. The play was also selected to be part of the Capital Fringe Festival in DC in 2010.
Her poem "Threnody" was recorded by Annifrid Lyngstad, of ABBA fame.
Lyrics taken from her book of poetry "Not So Deep as a Well" were, with the authorization of the NAACP, used by Canadian singer Myriam Gendron to create a folk album of the same name.

</doc>
<doc id="8783" url="https://en.wikipedia.org/wiki?curid=8783" title="Dylan Thomas">
Dylan Thomas

Dylan Marlais Thomas (27 October 1914 – 9 November 1953) was a Welsh poet and writer, whose works include the poems "Do not go gentle into that good night" and "And death shall have no dominion"; the 'play for voices' "Under Milk Wood"; and stories and radio broadcasts such as "A Child's Christmas in Wales" and "Portrait of the Artist as a Young Dog". He became widely popular in his lifetime and remained so after his premature death at the age of 39 in New York City. By then, he had acquired a reputation, which he encouraged, as a "roistering, drunken and doomed poet".
Thomas was born in Swansea, Wales, in 1914. An undistinguished pupil, he left school at 16 and became a journalist for a short time. Many of his works appeared in print while he was still a teenager; however, it was the publication of "Light breaks where no sun shines," in 1934, that caught the attention of the literary world. While living in London, Thomas met Caitlin Macnamara, whom he married in 1937. Their relationship was defined by alcoholism and was mutually destructive. In the early part of their marriage, Thomas and his family lived hand-to-mouth, settling in the Welsh fishing village of Laugharne.
Thomas came to be appreciated as a popular poet during his lifetime, and he found earning a living as a writer difficult. He began augmenting his income with reading tours and radio broadcasts. His radio recordings for the BBC during the late 1940s brought him to the public's attention, and he was frequently used by the BBC as a populist voice of the literary scene. 
Thomas first traveled to the United States in the 1950s. This is where his readings brought him a level of fame while his erratic behaviour and drinking worsened. His time in America cemented Thomas's legend, however, and he went on to record to vinyl such works as "A Child's Christmas in Wales". During his fourth trip to New York in 1953, Thomas became gravely ill and fell into a coma, from which he never recovered. He died on 9 November 1953. His body was returned to Wales where he was interred at the village churchyard in Laugharne on 25 November 1953.
Thomas wrote exclusively in the English language. He has been acknowledged as one of the most important Welsh poets of the 20th century. He is noted for his original, rhythmic and ingenious use of words and imagery. Thomas's position as one of the great modern poets has been much discussed, and he remains popular with the public.
Life and career.
Early life.
Dylan Thomas was born on 27 October 1914 in Swansea, the son of Florence Hannah ("née" Williams; 1882–1958), a seamstress, and David John Thomas (1876–1952), a teacher. His father had a first-class honours degree in English from University College, Aberystwyth, and ambitions to rise above his position teaching English literature at the local grammar school. Thomas had one sibling, Nancy (Nancy Marles 1906–1953), who was nine years older. The children spoke only English though their parents were bilingual in English and Welsh, and David Thomas gave Welsh lessons at home. Thomas's father chose the name Dylan, which could be translated as "son of the sea", after Dylan ail Don, a character in "The Mabinogion". His middle name, Marlais, was given in honour of his great-uncle, William Thomas, a Unitarian minister and poet whose bardic name was Gwilym Marles. Dylan, pronounced (Dull-an) in Welsh, caused his mother to worry he might be teased as the "dull one". When he broadcast on Welsh BBC, early in his career, he was introduced using this pronunciation. Thomas favoured the Anglicised pronunciation and gave instructions that it should be Dillan .
The red-brick semi-detached house at 5 Cwmdonkin Drive, in which Thomas was born and lived until he was 19, had been bought by his parents in the respectable area of the Uplands a few months before his birth. His childhood was spent in Swansea, with summer trips to Carmarthenshire to visit Fernhill, a dairy farm owned by his maternal aunt, Ann Jones, the memory of which is used for the 1945 lyrical poem "Fern Hill". Thomas had bronchitis and asthma in childhood and struggled with these throughout his life. Thomas was indulged by his mother and enjoyed being mollycoddled, a trait he carried into adulthood and he was skilful at gaining attention and sympathy. Thomas's formal education began at Mrs Hole's dame school a private school on Mirador Crescent, a few streets away from his home. He described his experience there in "Quite Early One Morning":
Never was there such a dame school as ours, so firm and kind and smelling of galoshes, with the sweet and fumbled music of the piano lessons drifting down from upstairs to the lonely schoolroom, where only the sometimes tearful wicked sat over undone sums, or to repent a little crime — the pulling of a girl's hair during geography, the sly shin kick under the table during English literature.
In October 1925, Thomas enrolled at Swansea Grammar School for boys, in Mount Pleasant, where his father taught English. He was an undistinguished pupil who shied away from school, preferring reading. In his first year one of his poems was published in the school's magazine and before he left he became its editor. During his final school years he began writing poetry in notebooks, the first poem dated 27 April (1930), is entitled "Osiris, come to Isis". In June 1928 Thomas won the school's mile race, held at St. Helen's Ground; he carried a newspaper photograph of his victory with him until his death. In 1931, when he was 16, Thomas left school to become a reporter for the "South Wales Daily Post", only to leave under pressure 18 months later. Thomas continued to work as a freelance journalist for several years during which time he remained at Cwmdonkin Drive where he continued to add to his notebooks, amassing 200 poems in four books between 1930 and 1934. Of the 90 poems he published, half were written during these years.
In his free time, he joined the amateur dramatic group at the Little Theatre in Mumbles, visited the cinema in Uplands, took walks along Swansea Bay, and frequented Swansea's pubs, especially the Antelope and the Mermaid Hotels in Mumbles. In the Kardomah Café, close to the newspaper office in Castle Street, he met his creative contemporaries, including his friend the poet Vernon Watkins. The group of writers, musicians and artists became known as "The Kardomah Gang". In 1933, Thomas visited London for probably the first time.
1933–1939.
Thomas was a teenager when many of the poems for which he became famous were published: "And death shall have no dominion", "Before I Knocked" and "The Force That Through the Green Fuse Drives the Flower". "And death shall have no dominion" appeared in the "New English Weekly" in May 1933. When "Light breaks where no sun shines" appeared in "The Listener" in 1934, it caught the attention of three senior figures in literary London, T. S. Eliot, Geoffrey Grigson and Stephen Spender. They contacted Thomas and his first poetry volume, "18 Poems", was published in December 1934. "18 Poems" was noted for its visionary qualities which led to critic Desmond Hawkins writing that the work was "the sort of bomb that bursts no more than once in three years". The volume was critically acclaimed and won a contest run by the "Sunday Referee", netting him new admirers from the London poetry world, including Edith Sitwell and Edwin Muir. The anthology was published by Fortune Press, in part a vanity publisher that did not pay its writers and expected them to buy a certain number of copies themselves. A similar arrangement was used by other new authors including Philip Larkin. In December 1935 Thomas contributed the poem "The Hand That Signed the Paper" to Issue 18 of the bi-monthly "New Verse". In 1936, his next collection "Twenty-five Poems", published by J. M. Dent, also received much critical praise. In all, he wrote half his poems while living at Cwmdonkin Drive before moving to London. It was the time that Thomas's reputation for heavy drinking developed.
In early 1936, Thomas met Caitlin Macnamara (1913–1994), a 22-year-old blonde-haired, blue-eyed dancer of Irish descent. She had run away from home, intent on making a career in dance, and aged 18 joined the chorus line at the London Palladium. Introduced by Augustus John, Caitlin's lover, they met in The Wheatsheaf pub on Rathbone Place in London's West End. Laying his head in her lap, a drunken Thomas proposed. Thomas liked to comment that he and Caitlin were in bed together ten minutes after they first met. Although Caitlin initially continued her relationship with John, she and Thomas began a correspondence, and in the second half of 1936 were courting. They married at the register office in Penzance, Cornwall, on 11 July 1937. In early 1938 they moved to Wales, renting a cottage in the village of Laugharne, Carmarthenshire. Their first child, Llewelyn Edouard, was born on 30 January 1939.
By the late 1930s, Thomas was embraced as the "poetic herald" for a group of English poets, the New Apocalyptics. Thomas refused to align himself with them and declined to sign their manifesto. He later stated that he believed they were "intellectual muckpots leaning on a theory". Despite this, many of the group, including Henry Treece, modelled their work on Thomas.
Wartime, 1939–1945.
In 1939 "The Map of Love" appeared as a collection of 16 poems and seven of the 20 short stories published by Thomas in magazines since 1934. Ten stories in his next book, "Portrait of the Artist as a Young Dog" (1940), were based less on lavish fantasy than "The Map of Love" and more on real-life romances featuring himself in Wales. Sales of both books were poor, resulting in Thomas living on meagre fees from writing and reviewing. At this time he borrowed heavily from friends and acquaintances. Hounded by creditors, Thomas and his family left Laugharne in July 1940 and moved to the home of critic John Davenport in Marshfield, Gloucestershire. There Thomas collaborated with Davenport on the satire "The Death of the King's Canary", though due to fears of libel the work was not published until 1976.
At the outset of the Second World War, Thomas was worried about conscription and referred to his ailment as "an unreliable lung". Coughing sometimes confined him to bed and he had a history of bringing up blood and mucus. After initially seeking employment in a reserved occupation, he managed to be classified Grade III, which meant that he would be among the last to be called up for service. Saddened to see his friends going on active service, he continued drinking and struggled to support his family. He wrote begging letters to random literary figures asking for support, a plan he hoped would provide a long-term regular income. Thomas supplemented his income by writing scripts for the BBC, which not only gave him additional earnings but also provided evidence that he was producing essential war work.
In February 1941, Swansea was bombed by the Luftwaffe in a "three nights' blitz". Castle Street was one of many streets that suffered badly; rows of shops, including the Kardomah Café, were destroyed. Thomas walked through the bombed-out shell of the town centre with his friend Bert Trick. Upset at the sight, he concluded: "Our Swansea is dead". Soon after the bombing raids, Thomas wrote a radio play, "Return Journey Home", which described the café as being "razed to the snow". The play was first broadcast on 15 June 1947. The Kardomah Café reopened on Portland Street after the war.
In May 1941, Thomas and Caitlin moved to London, leaving their son with his grandmother at Blashford in Hampshire. Thomas hoped to find employment in the film industry and wrote to the director of the films division of the Ministry of Information (MOI). After being rebuffed he found work with Strand Films providing him with his first regular income since the "Daily Post". Strand produced films for the MOI; Thomas scripted at least five films in 1942, "This Is Colour" (a history of the British dyeing industry) and "New Towns For Old" (on post-war reconstruction). "These Are The Men" (1943) was a more ambitious piece in which Thomas's verse accompanies Leni Riefenstahl's footage of an early Nuremberg Rally. "Conquest of a Germ" (1944) explored the use of early antibiotics in the fight against pneumonia and tuberculosis. "Our Country" (1945) was a romantic tour of Britain set to Thomas's poetry.
In early 1943 Thomas began a relationship with Pamela Glendower, one of several affairs he had during his marriage. The affairs either ran out of steam or were halted after Caitlin discovered his infidelity. In March 1943 Caitlin gave birth to a daughter, Aeronwy, in London. They lived in a run-down studio in Chelsea, made up of a single large room with a curtain to separate the kitchen.
In 1944, with the threat of German flying bombs on London, Thomas moved to the family cottage in Blaen Cwm near Llangain, where Thomas resumed writing poetry, completing "Holy Spring" and "Vision and Prayer". In September Thomas and Caitlin moved to New Quay in West Wales which inspired Thomas to pen the radio piece "Quite Early One Morning", a sketch for his later work, "Under Milk Wood". Of the poetry written at this time, of note is "Fern Hill", believed to have been started while living in New Quay, but completed at Blaen Cwm in mid-1945.
Broadcasting years 1945–1949.
Although Thomas had previously written for the BBC, it was a minor source of income and the occurrences intermittent. In 1943 he wrote and recorded a 15-minute talk entitled "Reminiscences of Childhood" for the Welsh BBC. In December 1944 he recorded "Quite Early One Morning" (produced by Aneirin Talfan Davies, again for the Welsh BBC) but when Davies offered it for national broadcast BBC London turned it down. On 31 August 1945 the BBC Home Service broadcast "Quite Early One Morning", and in the three years beginning October 1945, Thomas made over a hundred broadcasts for the corporation. Thomas was employed not only for his poetry readings, but for discussions and critiques.
By late September 1945 the Thomases had left Wales and were living with various friends in London. The publication of "Deaths and Entrances" in 1946 was a turning point for Thomas. Poet and critic Walter J. Turner commented in "The Spectator", "This book alone, in my opinion, ranks him as a major poet".
In the second half of 1945, Thomas began reading for the BBC Radio programme, "Book of Verse", broadcast weekly to the Far East providing Thomas with a regular income and bringing him into contact with Louis MacNeice, a congenial drinking companion whose advice Thomas cherished. On 29 September 1946, the BBC began transmitting the "Third Programme", a high-culture network which provided opportunities for Thomas. He appeared in the play "Comus" for Third Programme, the day after the network launched, and his rich, sonorous voice led to character parts, including the lead in Aeschylus' "Agamemnon" and Satan in an adaptation of "Paradise Lost". Thomas remained a popular guest on radio talk shows for the BBC who regarded him as "useful should a younger generation poet be needed". He had an uneasy relationship with BBC management and a staff job was never an option, with drinking cited as the problem. Despite this, Thomas became a familiar radio voice and within Britain was "in every sense a celebrity".
Thomas visited the home of historian A. J. P. Taylor in Disley. Although Taylor disliked him intensely, he stayed for a month, drinking "on a monumental scale", up to 15 or 20 pints of beer a day. In late 1946 Thomas turned up at the Taylors' again, this time homeless and with Caitlin. Margaret Taylor let them take up residence in the garden summerhouse. In May 1949 Thomas and his family moved to his final home, the Boat House at Laugharne purchased for him at a cost of £2,500 in April 1949 by Margaret Taylor. Thomas acquired a garage a hundred yards from the house on a cliff ledge which he turned into his writing shed, and where he wrote several of his most acclaimed poems. Just before moving into there, Thomas rented "Pelican House" opposite his regular drinking den, Brown's Hotel, for his parents who lived there from 1949 until 1953. It was there that his father died and the funeral was held.
Caitlin gave birth to their third child, a boy named Colm Garan Hart, on 25 July 1949.
American tours, 1950–1953.
John Brinnin invited Thomas to New York, where in 1950 they embarked on a lucrative three-month tour of arts centres and campuses. The tour, which began in front of an audience of a thousand at the Kaufmann Auditorium of the Poetry Centre in New York, took in about 40 venues. During the tour Thomas was invited to many parties and functions and on several occasions became drunk - going out of his way to shock people - and was a difficult guest. Thomas drank before some of his readings, though it is argued he may have pretended to be more affected by it than he actually was. The writer Elizabeth Hardwick recalled how intoxicating a performer he was and how the tension would build before a performance: "Would he arrive only to break down on the stage? Would some dismaying scene take place at the faculty party? Would he be offensive, violent, obscene?" Caitlin said in her memoir, "Nobody ever needed encouragement less, and he was drowned in it."
On returning to Britain Thomas began work on two further poems, "In the white giant's thigh", which he read on the "Third Programme" in September 1950, and the incomplete "In country heaven". 1950 is also believed to be the year that he began work on 'Under Milk Wood', under the working title 'The Town That Was Mad'. The task of seeing this work through to production was assigned to the BBC's Douglas Cleverdon, who had been responsible for casting Thomas in 'Paradise Lost'. Despite Cleverdon's urges, the script slipped from Thomas's priorities and in early 1951 he took a trip to Iran to work on a film for the Anglo-Iranian Oil Company. The film was never made, with Thomas returning to Wales in February, though his time there allowed him to provide a few minutes of material for a BBC documentary entitled 'Persian Oil'. Early that year Thomas wrote two poems, which Ferris describes as "unusually blunt"; the ribald "Lament" and an ode, in the form of a villanelle, to his dying father "Do not go gentle into that good night".
Despite a range of wealthy patrons, including Margaret Taylor, Princess Marguerite Caetani and Marged Howard-Stepney, Thomas was still in financial difficulty, and he wrote several begging letters to notable literary figures including the likes of T. S. Eliot. Taylor was not keen on Thomas taking another trip to the United States, and thought that if Thomas had a permanent address in London he would be able to gain steady work there. She bought a property, 54 Delancey Street, in Camden Town, and in late 1951 Thomas and Caitlin lived in the basement flat. Thomas would describe the flat as his "London house of horror" and did not return there after his 1952 tour of America.
Thomas undertook a second tour of the United States in 1952, this time with Caitlin - after she had discovered he had been unfaithful on his earlier trip. They drank heavily, and Thomas began to suffer with gout and lung problems. The second tour was the most intensive of the four, taking in 46 engagements. The trip also resulted in Thomas recording his first poetry to vinyl, which Caedmon Records released in America later that year. One of his works recorded during this time, "A Child's Christmas in Wales", became his most popular prose work in America. The original 1952 recording of "A Child's Christmas in Wales" was a 2008 selection for the United States National Recording Registry, stating that it is "credited with launching the audiobook industry in the United States".
In April 1953 Thomas returned alone for a third tour of America. He performed a "work in progress" version of "Under Milk Wood", solo, for the first time at Harvard University on 3 May. A week later the work was performed with a full cast at the Poetry Centre in New York. He met the deadline only after being locked in a room by Brinnin's assistant, Liz Reitell, and was still editing the script on the afternoon of the performance; its last lines were handed to the actors as they put on their makeup. In the wake of the play's US success, the composer Stravinsky invited Thomas to write a libretto for an opera. Thomas spent the last nine or ten days of his third tour in New York mostly in the company of Reitell, with whom he had an affair. During this time Thomas fractured his arm falling down a flight of stairs when drunk. Reitell's doctor, Milton Feltenstein, put his arm in plaster and treated him for gout and gastritis.
After returning home, Thomas worked on "Under Milk Wood" in Wales before sending the original manuscript to Douglas Cleverdon on 15 October 1953. It was copied and returned to Thomas, who lost it in a pub in London and required a duplicate to take to America. Thomas flew to the States on 19 October 1953 for what would be his final tour. He died in New York before the BBC could record "Under Milk Wood". Richard Burton starred in the first broadcast in 1954, and was joined by Elizabeth Taylor in a subsequent film. In 1954 the play won the Prix Italia for literary or dramatic programmes.
Thomas's last collection "Collected Poems, 1934–1952", published when he was 38, won the Foyle poetry prize. Reviewing the volume, critic Philip Toynbee declared that "Thomas is the greatest living poet in the English language". Thomas's father died from pneumonia just before Christmas 1952. In the first few months of 1953 his sister died from liver cancer, one of his patrons took an overdose of sleeping pills, three friends died at an early age and Caitlin had an abortion.
Death.
Thomas arrived in New York on 20 October 1953 to undertake another tour of poetry reading and talks, organised by Brinnin. He was ill, complaining of chest trouble and gout while still in Britain, though there is no record he received medical treatment for either condition. He was in a melancholy mood about the trip and his health was poor, relying on an inhaler to aid his breathing and there were reports that he was suffering from blackouts. His visit to say goodbye to BBC producer Philip Burton, a few days before he left for New York, was interrupted by a blackout. On his last night in London, he had another, in the company of his fellow poet Louis MacNeice. The next day, he visited a doctor for a smallpox vaccination certificate.
His first appearance was planned to be at a rehearsal of "Under Milk Wood" at the Poetry Centre. Brinnin, who was director of the Poetry Centre, did not travel to New York but remained in Boston to write. He handed responsibility to his assistant, Liz Reitell, who was keen to see Thomas for the first time since their three-week romance early in the year. She met Thomas at Idlewild Airport and was shocked at his appearance, as he "looked pale, delicate and shaky, not his usual robust self." Thomas told her he had had a terrible week, had missed her terribly and wanted to go to bed with her. Despite Reitell's previous misgivings about their relationship, they spent the rest of the day and night together. After being taken by Reitell to check in at the Chelsea Hotel, Thomas took the first rehearsal of "Under Milk Wood". They then went to the White Horse Tavern in Greenwich Village, before returning to the Chelsea Hotel.
The next day Reitell invited him to her apartment but he declined. They went sight-seeing, but Thomas was unwell and retired to his bed for the rest of the afternoon. Reitell gave him half a grain (32.4 milligrams) of phenobarbitone to help him sleep and spent the night at the hotel with him. Two days later, on 23 October, Herb Hannum, a friend from an earlier trip, noticed how sick Thomas looked and suggested an appointment with Feltenstein before the performances of "Under Milk Wood" that evening. Feltenstein administered injections and Thomas made it through the two performances, but collapsed immediately afterwards. Reitell later said that Feltenstein was "rather a wild doctor who thought injections would cure anything".
On the evening of 27 October, Thomas attended his 39th birthday party but felt unwell and returned to his hotel after an hour. The next day he took part in "Poetry And The Film", a recorded symposium at Cinema 16, with panellists Amos Vogel, Arthur Miller, Maya Deren, Parker Tyler, and Willard Maas.
A turning point came on 2 November. Air pollution in New York had risen significantly and exacerbated chest illnesses, such as Thomas had. By the end of the month, over 200 New Yorkers had died from the smog. On 3 November, Thomas spent most of the day in bed drinking. He went out in the evening to keep two drink appointments. After returning to the hotel, he went out again for a drink at 2 am. After drinking at the White Horse, a pub he had found through Scottish poet Ruthven Todd, Thomas returned to the Hotel Chelsea, declaring, "I've had 18 straight whiskies. I think that's the record!" The barman and the owner of the pub who served him later commented that Thomas could not have imbibed more than half that amount. Thomas had an appointment at a clam house in New Jersey with Todd on 4 November. When phoned at the Chelsea that morning, he said he was feeling ill and postponed the engagement. Later he went drinking with Reitell at the White Horse and, feeling sick again, returned to the hotel. Feltenstein came to see him three times that day, administering the steroid ACTH by injection and, on his third visit, half a grain (32.4 milligrams) of morphine sulphate, which affected his breathing. Reitell became increasingly concerned and telephoned Feltenstein for advice. He suggested she get male assistance, so she called upon the painter Jack Heliker, who arrived before 11 pm. At midnight on 5 November, Thomas's breathing became more difficult and his face turned blue. An ambulance was summoned.
Thomas was admitted to the emergency ward at St Vincent's Hospital at 1:58 am. He was comatose, and his medical notes state that the "impression upon admission was acute alcoholic encephalopathy damage to the brain by alcohol, for which the patient was treated without response". Caitlin flew to America the following day and was taken to the hospital, by which time a tracheotomy had been performed. Her reported first words were, "Is the bloody man dead yet?" She was allowed to see Thomas only for 40 minutes in the morning but returned in the afternoon and, in a drunken rage, threatened to kill Brinnin. When she became uncontrollable, she was put in a straitjacket and committed, by Feltenstein, to the River Crest private psychiatric detox clinic on Long Island.
Thomas died at noon on 9 November, still in a coma. A post mortem gave the primary cause of death as pneumonia, with pressure on the brain and a fatty liver as contributing factors.
Aftermath.
Rumours circulated of a brain haemorrhage, followed by competing reports that he had been mugged and even that he had drunk himself to death. Later, there was speculation about drugs and diabetes. At the post-mortem, the pathologist found three causes of death – pneumonia, brain swelling and a fatty liver. Despite his heavy drinking his liver showed no sign of cirrhosis.
Dylan's legacy as the "doomed poet" was cemented with the publication of Brinnin's 1955 biography "Dylan Thomas in America", which focuses on his last few years and paints a picture of him as a drunk and a philanderer. Later biographies are critical of Brinnin's view, especially his coverage of Thomas's death. David Thomas in "Fatal Neglect: Who Killed Dylan Thomas?" claims that Brinnin, along with Reitell and Feltenstein, were culpable. FitzGibbon's 1965 biography ignores Thomas's heavy drinking and skims over his death, giving just two pages in his detailed book to Thomas's demise. Ferris in his 1989 biography includes Thomas's heavy drinking, but is more critical of those around him in his final days and does not draw the conclusion that he drank himself to death. Feltenstein's role and actions have been criticised by many sources, especially his incorrect diagnosis of delirium tremens and the high dose of morphine he administered. Dr B. W. Murphy and Dr C. G. de Gutierrez-Mahoney, the doctors who treated Thomas while at St. Vincents, concluded that Feltenstein's failure to see that Thomas was gravely ill and have him admitted to hospital sooner, "was even more culpable than his use of morphine".
Following his death, Thomas's body was brought back to Wales for burial in the village churchyard at Laugharne. Thomas's funeral, which Brinnin did not attend, took place at St Martin's Church in Laugharne on 24 November. Thomas's coffin was carried by six friends from the village. Caitlin, without her customary hat, walked behind the coffin, with his childhood friend Daniel Jones at her arm and her mother by her side. The procession to the church was filmed and the wake took place at Brown's Hotel. Thomas's obituary in "The Times" was written by fellow poet and long-time friend Vernon Watkins.
His widow, Caitlin, died in 1994 and was buried alongside him. Thomas's father "DJ" died on 16 December 1952 and his mother Florence in August 1958. Thomas's elder son, Llewelyn, died in 2000, his daughter, Aeronwy in 2009 and his youngest son Colm in 2012.
Caitlin Thomas's autobiographies, "Caitlin Thomas – Leftover Life to Kill" (1957) and "My Life with Dylan Thomas: Double Drink Story" (1997), describe the destructive effect of alcoholism on the poet and to their relationship. "But ours was a drink story, not a love story, just like millions of others. Our one and only true love was drink", she wrote and "The bar was our altar". Biographer Andrew Lycett ascribed the demise of Thomas's health to an alcoholic co-dependent relationship with his wife, who deeply resented his extramarital affairs. Thomas died intestate with assets to the value of £100.
Poetry.
Poetic style and influences.
Thomas' refusal to align with any literary group or movement has made him and his work difficult to categorize. Although influenced by the modern symbolism and surrealism movement he refused to follow its creed. Instead Thomas is viewed as part of the modernism and romanticism movements, though attempts to pigeon-hole him within a particular neo-romantic school have been unsuccessful. Elder Olson, in his 1954 critical study of Thomas's poetry, wrote "... a further characteristic which distinguished Thomas's work from that of other poets. It was unclassifiable." Olson continued that in a postmodern age that continually attempted to demand that poetry have social reference, none could be found in Thomas's work, and that his work was so obscure that critics could not explicate it.
Thomas's verbal style played against strict verse forms, such as in the villanelle "Do not go gentle into that good night". His images were carefully ordered in a patterned sequence, and his major theme was the unity of all life, the continuing process of life and death and new life that linked the generations. Thomas saw biology as a magical transformation producing unity out of diversity, and in his poetry sought a poetic ritual to celebrate this unity. He saw men and women locked in cycles of growth, love, procreation, new growth, death, and new life. Therefore, each image engenders its opposite. Thomas derived his closely woven, sometimes self-contradictory images from the Bible, Welsh folklore, preaching, and Sigmund Freud. Explaining the source of his imagery, Thomas wrote in a letter to Glyn Jones: "My own obscurity is quite an unfashionable one, based, as it is, on a preconceived symbolism derived (I'm afraid all this sounds wooly and pretentious) from the cosmic significance of the human anatomy".
Thomas's early poetry was noted for its verbal density, alliteration, sprung rhythm and internal rhyme, and he was described by some critics as having been influenced by the English poet Gerard Manley Hopkins. This is attributed to Hopkins, who taught himself Welsh and who used sprung verse, bringing some features of Welsh poetic metre into his work. When Henry Treece wrote to Thomas comparing his style to that of Hopkins, Thomas wrote back denying any such influence. One poet Thomas greatly admired, and who is regarded as an influence, was Thomas Hardy. When Thomas travelled in America, he recited Hardy's work in his readings.
Other poets from whom critics believe Thomas drew influence include James Joyce, Arthur Rimbaud and D. H. Lawrence. William York Tindall, in his 1962 study, "A Reader's Guide to Dylan Thomas", finds comparison between Thomas's and Joyce's wordplay, while he notes the themes of rebirth and nature are common to the works of Lawrence and Thomas. Although Thomas described himself as the "Rimbaud of Cwmdonkin Drive", he stated that the phrase "Swansea's Rimbaud" was coined by poet Roy Campbell. Critics have explored the connection between the creation of Thomas's mythological pasts into his works such as "The Orchards", which Ann Elizabeth Mayer believes reflects the Welsh myths of the "Mabinogion".
Thomas's poetry is notable for its musicality, most clear in "Fern Hill", "In Country Sleep", "Ballad of the Long-legged Bait" and "In the White Giant's Thigh" from "Under Milk Wood".
Thomas once confided that the poems which had most influenced him were "Mother Goose" rhymes which his parents taught him when he was a child:
I should say I wanted to write poetry in the beginning because I had fallen in love with words. The first poems I knew were nursery rhymes and before I could read them for myself I had come to love the words of them. The words alone. What the words stood for was of a very secondary importance ... I fell in love, that is the only expression I can think of, at once, and am still at the mercy of words, though sometimes now, knowing a little of their behaviour very well, I think I can influence them slightly and have even learned to beat them now and then, which they appear to enjoy. I tumbled for words at once. And, when I began to read the nursery rhymes for myself, and, later, to read other verses and ballads, I knew that I had discovered the most important things, to me, that could be ever.
Thomas was an accomplished writer of prose poetry, with collections such as "Portrait of the Artist as a Young Dog" (1940) and "Quite Early One Morning" (1954) showing he was capable of writing moving short stories. His first published prose work was "After the Fair", printed in The New English Weekly on 15 March 1934. Jacob Korg believes that Thomas's fiction work can be classified into two main bodies, vigorous fantasies in a poetic style and, after 1939, more straightforward narratives. Korg surmises that Thomas approached his prose writing as an alternate poetic form, which allowed him to produce complex, involuted narratives that do not allow the reader to rest.
As a 'Welsh' poet.
Thomas disliked being regarded as a provincial poet, and decried any notion of 'Welshness' in his poetry. When he wrote to Stephen Spender in 1952, thanking him for a review of his "Collected Poems", he added "Oh, & I forgot. I'm not influenced by Welsh bardic poetry. I can't read Welsh." Despite this his work was rooted in the geography of Wales. Thomas acknowledged that he returned to Wales when he had difficulty writing, and John Ackerman argues that "His inspiration and imagination were rooted in his Welsh background". Caitlin Thomas wrote that he worked "in a fanatically narrow groove, although there was nothing narrow about the depth and understanding of his feelings. The groove of direct hereditary descent in the land of his birth, which he never in thought, and hardly in body, moved out of."
Head of Programmes Wales at the BBC, Aneirin Talfan Davies, who commissioned several of Thomas's early radio talks, believed that the poet's "whole attitude is that of the medieval bards." Kenneth O. Morgan counter-argues that it is a 'difficult enterprise' to find traces of "cynghanedd" (harmony) or "cerdd dafod" (tongue-craft) in Thomas's poetry. Instead he believes his work, especially his earlier more autobiographical poems, are rooted in a changing country which echoes the Welshness of the past and the Anglicisation of the new industrial nation: "rural and urban, chapel-going and profane, Welsh and English, Unforgiving and deeply compassionate." Fellow poet and critic Glyn Jones believed that any traces of "cynghanedd" in Thomas's work was accidental, although he felt Thomas consciously employed one element of Welsh metrics; that of counting syllables per line instead of feet. Constantine FitzGibbon, Thomas's first in-depth biographer, wrote "No major English poet has ever been as Welsh as Dylan".
Although Thomas had a deep connection with Wales, he disliked Welsh nationalism. He once wrote, "Land of my fathers, and my fathers can keep it". While often attributed to Thomas himself, this line actually comes from the character Owen Morgan-Vaughan, in the screenplay Thomas wrote for the 1948 British melodrama "The Three Weird Sisters". Robert Pocock, a friend from the BBC, recalled "I only once heard Dylan express an opinion on Welsh Nationalism. He used three words. Two of them were Welsh Nationalism." Although not expressed as strongly, Glyn Jones believed that he and Thomas's friendship cooled in the later years as he had not 'rejected enough' of the elements that Thomas disliked – "Welsh nationalism and a sort of hill farm morality". Apologetically, in a letter to Keidrych Rhys, editor of literary magazine "Wales", Thomas's father wrote that he was "afraid Dylan isn't much of a Welshman". Though FitzGibbon asserts that Thomas's negativity towards Welsh nationalism was fostered by his father's hostility towards the Welsh language.
Critical reception.
Thomas's work and stature as a poet have been much debated by critics and biographers since his death. Critical studies have been clouded by Thomas's personality and mythology, especially his drunken persona and death in New York. When Seamus Heaney gave an Oxford lecture on the poet he opened by addressing the assembly, "Dylan Thomas is now as much a case history as a chapter in the history of poetry", querying how 'Thomas the Poet' is one of his forgotten attributes. David Holbrook, who has written three books about Thomas, stated in his 1962 publication "Llareggub Revisited", "the strangest feature of Dylan Thomas's notoriety-not that he is bogus, but that attitudes to poetry attached themselves to him which not only threaten the prestige, effectiveness and accessibility to English poetry, but also destroyed his true voice and, at last, him." The Poetry Archive notes that "Dylan Thomas' detractors accuse him of being drunk on language as well as whiskey, but whilst there's no doubt that the sound of language is central to his style, he was also a disciplined writer who re-drafted obsessively".
Many critics have argued that Thomas's work is too narrow and that he suffers from verbal extravagance. Those that have championed his work have found the criticism baffling. Robert Lowell wrote in 1947, "Nothing could be more wrongheaded than the English disputes about Dylan Thomas's greatness ... He is a dazzling obscure writer who can be enjoyed without understanding." Kenneth Rexroth said, on reading "Eighteen Poems", "The reeling excitement of a poetry-intoxicated schoolboy smote the Philistine as hard a blow with one small book as Swinburne had with "Poems and Ballads"." Philip Larkin in a letter to Kingsley Amis in 1948, wrote that "no one can 'stick words into us like pins'... like he can", but followed that by stating that he "doesn't use his words to any advantage". Amis was far harsher, finding little of merit in his work. In 1956, the publication of the anthology "New Lines" featuring works by the British collective The Movement, which included Amis and Larkin amongst its number, set out a vision of modern poetry that was damning towards the poets of the 1940s. Thomas's work in particular was criticised. David Lodge, writing about The Movement in 1981 stated "Dylan Thomas was made to stand for everything they detest, verbal obscurity, metaphysical pretentiousness, and romantic rhapsodizing".
Despite criticism by sections of academia, Thomas's work has been embraced by readers more so than many of his contemporaries, and is one of the few modern poets whose name is recognised by the general public. In 2009, over 18,000 votes were cast in a BBC poll to find the UK's favourite poet; Thomas was placed 10th. Several of his poems have passed into the cultural mainstream, and his work has been used by authors, musicians and film and television writers. The BBC Radio programme, Desert Island Discs, in which guests usually choose their favourite songs, has heard 50 participants select a Dylan Thomas recording. John Goodby states that this popularity with the reading public allows Thomas's work to be classed as vulgar and common. He also cites that despite a brief period during the 1960s when Thomas was considered a cultural icon, that the poet has been marginalized in critical circles due to his exuberance, in both life and work, and his refusal to know his place. Goodby believes that Thomas has been mainly snubbed since the 1970s and has become "... an embarrassment to twentieth-century poetry criticism" his work failing to fit standard narratives and is thus ignored rather than studied.
Memorials.
In Swansea's maritime quarter are the Dylan Thomas Theatre, home of the Swansea Little Theatre of which Thomas was once a member, and the former Guildhall built in 1825 and now occupied by the Dylan Thomas Centre, a literature centre, where exhibitions and lectures are held and setting for the annual Dylan Thomas Festival. Outside the centre stands a bronze statue of Thomas, by John Doubleday. Another monument to Thomas stands in Cwmdonkin Park, one of his favourite childhood haunts, close to his birthplace. The memorial is a small rock in an enclosed garden within the park inscribed with the closing lines from Fern Hill.
<poem>
</poem>
Thomas's home in Laugharne, the Boathouse, is a museum run by Carmarthenshire County Council. Thomas's writing shed is also preserved. In 2004 the Dylan Thomas Prize was created in his honour, awarded to the best published writer in English under the age of 30. In 2005 the Dylan Thomas Screenplay Award was established. The prize, administered by the Dylan Thomas Centre, is awarded at the annual Swansea Bay Film Festival. In 1982 a plaque was unveiled in Poets' Corner, Westminster Abbey. The plaque is also inscribed with the last two lines of Fern Hill.
In 2014, to celebrate the centenary of Thomas' birth, the British Council Wales undertook a year long programme of cultural and educational works. Highlights included a touring replica of Thomas' work shed, Sir Peter Blake's exhibition of illustrations based on Under Milk Wood and a 36-hour marathon of readings which saw the likes of Michael Sheen and Sir Ian McKellen performing Thomas' work. The Royal Patron of The Dylan Thomas 100 Festival was Charles, Prince of Wales, who made a recording of "Fern Hill" for the event.

</doc>
<doc id="8785" url="https://en.wikipedia.org/wiki?curid=8785" title="Fern Hill">
Fern Hill

Fern Hill (1945) is a poem by Dylan Thomas, first published in the October, 1945, Horizon magazine, with its first book publication as the last poem in "Deaths and Entrances". The poem starts as a straightforward evocation of his childhood visits to his Aunt Annie's farm:
In the middle section, the idyllic scene is expanded upon, reinforced by the lilting rhythm of the poem, the dreamlike, pastoral metaphors and allusion to Eden. By the end, the poet's older voice has taken over, mourning his lost youth with echoes of the opening:
The poem uses internal half rhyme and full rhyme as well as end rhyme. Thomas was very conscious of the impact of spoken or intoned verse and explored the potentialities of sound and rhythm, in a manner reminiscent of Gerard Manley Hopkins. He always denied having conscious knowledge of Welsh, but "his lines chime with internal consonantal correspondence, or "cynghanedd", a prescribed feature of Welsh versification". 
The house Fernhill is just outside Llangain in Carmarthenshire. Thomas had extended stays here in the 1920s with his aunt Annie and her husband, Jim Jones. His holidays here have been recalled in interviews with his schoolboy friends, and both the house and the Thomas family network in the area are detailed in the same book.
Musical composition.
"Fern Hill" has been set to music by the American composer John Corigliano, for SATB chorus with orchestral accompaniment.

</doc>
