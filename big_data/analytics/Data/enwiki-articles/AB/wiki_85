<doc id="11955" url="https://en.wikipedia.org/wiki?curid=11955" title="George H. W. Bush">
George H. W. Bush

George Herbert Walker Bush (born June 12, 1924) is an American politician who was 41st President of the United States from 1989 to 1993 and 43rd Vice President of the United States from 1981 to 1989. A member of the U.S. Republican Party, he was previously a congressman, ambassador, and Director of Central Intelligence. He is the oldest living former President and Vice President. He is also the last living former President who is a veteran of World War II. Bush is often referred to as "George H. W. Bush", "Bush 41", "Bush the Elder", or "George Bush Sr." to distinguish him from his eldest son, George W. Bush, who was the 43rd President of the United States. Prior to his son's presidency, he was known simply as George Bush or President Bush.
Bush was born in Milton, Massachusetts, to Prescott Bush and Dorothy Walker Bush. Following the attack on Pearl Harbor in 1941, Bush postponed college, enlisted in the U.S. Navy on his 18th birthday, and became the youngest aviator in the U.S. Navy at the time. He served until the end of the war, then attended Yale University. Graduating in 1948, he moved his family to West Texas and entered the oil business, becoming a millionaire by the age of 40.
He became involved in politics soon after founding his own oil company, serving as a member of the House of Representatives and Director of Central Intelligence, among other positions. He failed to win the Republican nomination for President in 1980, but was chosen by party nominee Ronald Reagan to be his running mate, and the two were elected. During his tenure, Bush headed administration task forces on deregulation and fighting the "War on Drugs".
In 1988, Bush ran a successful campaign to succeed Reagan as President, defeating Democratic opponent Michael Dukakis. Foreign policy drove the Bush presidency: military operations were conducted in Panama and the Persian Gulf; the Berlin Wall fell in 1989, and the Soviet Union dissolved two years later. Domestically, Bush and, after a struggle with Congress, signed an increase in taxes that Congress had passed. In the wake of a weak recovery from an economic recession, along with continuing budget deficits, he lost the 1992 presidential election to Democrat Bill Clinton.
Bush left office in 1993. His presidential library was dedicated in 1997, and he has been active—often alongside Bill Clinton—in various humanitarian activities. Besides being the 43rd president (2001–09), his son George also served as the 46th Governor of Texas (1995–2000) and is one of only two presidents—the other being John Quincy Adams—to be the son of a former president. His second son, Jeb Bush, served as the 43rd Governor of Florida (1999–2007) and made an unsuccessful run for president in 2016.
Early years.
George Herbert Walker Bush was born at 173 Adams Street in Milton, Massachusetts, on June 12, 1924 to Prescott Sheldon Bush and Dorothy (Walker) Bush. The Bush family moved from Milton to Greenwich, Connecticut, shortly after his birth.
Bush began his formal education at the Greenwich Country Day School in Greenwich. Beginning in 1936, he attended Phillips Academy in Andover, Massachusetts, where he held a number of leadership positions including president of the senior class, secretary of the student council, president of the community fund-raising group, a member of the editorial board of the school newspaper, and captain of both the varsity baseball and soccer teams.
World War II.
Following the attack on Pearl Harbor in December 1941, Bush decided to join the US. Navy, so after graduating from Phillips Academy in 1942, he became a naval aviator at the age of 18. After completing the 10-month course, he was commissioned as an ensign in the United States Naval Reserve at Naval Air Station Corpus Christi on June 9, 1943, just three days before his 19th birthday, which made him the youngest naval aviator to that date.
He was assigned to Torpedo Squadron (VT-51) as the photographic officer in September 1943. The following year, his squadron was based on the as a member of Air Group 51, where his lanky physique earned him the nickname "Skin". During this time, the task force was victorious in one of the largest air battles of World War II: the Battle of the Philippine Sea.
After Bush's promotion to Lieutenant (junior grade) on August 1, 1944, the "San Jacinto" commenced operations against the Japanese in the Bonin Islands. Bush piloted one of four Grumman TBM Avenger aircraft from VT-51 that attacked the Japanese installations on Chichijima. His crew for the mission, which occurred on September 2, 1944, included Radioman Second Class John Delaney and Lieutenant Junior Grade William White. During their attack, the Avengers encountered intense anti-aircraft fire; Bush's aircraft was hit by flak and his engine caught on fire. Despite his plane being on fire, Bush completed his attack and released bombs over his target, scoring several damaging hits. With his engine ablaze, Bush flew several miles from the island, where he and one other crew member on the TBM Avenger bailed out of the aircraft; the other man's parachute did not open. Bush waited for four hours in an inflated raft, while several fighters circled protectively overhead until he was rescued by the lifeguard submarine . For the next month he remained on the "Finback", and participated in the rescue of other pilots. Several of those shot down during the attack were executed and eaten by their captors.
Bush subsequently returned to "San Jacinto" in November 1944 and participated in operations in the Philippines until his squadron was replaced and sent home to the United States. Through 1944, he flew 58 combat missions for which he received the Distinguished Flying Cross, three Air Medals, and the Presidential Unit Citation awarded to "San Jacinto".
Because of his valuable combat experience, Bush was reassigned to Norfolk Navy Base and put in a training wing for new torpedo pilots. He was later assigned as a naval aviator in a new torpedo squadron, VT-153, based at Naval Air Station Grosse Ile, Michigan. Upon the Japanese surrender in 1945, Bush was honorably discharged in September of that year.
Marriage and college years.
George Bush married Barbara Pierce on January 6, 1945, only weeks after his return from the Pacific. The couple's first residence was a small rented apartment in Trenton, Michigan, near Bush's Navy assignment at NAS Grosse Ile. Their marriage produced six children: George Walker Bush (born 1946), Pauline Robinson "Robin" Bush (1949–1953, died of leukemia), John Ellis "Jeb" Bush (born 1953), Neil Mallon Pierce Bush (born 1955), Marvin Pierce Bush (born 1956), and Dorothy Bush Koch (born 1959).
Bush had been accepted to Yale University prior to his enlistment in the military, and took up the offer after his discharge and marriage. While at Yale, he was enrolled in an accelerated program that allowed him to graduate in two and a half years, rather than four. He was a member of the Delta Kappa Epsilon fraternity and was elected its president. He also captained the Yale baseball team, and as a left-handed first baseman, played in the first two College World Series. As the team captain, Bush met Babe Ruth before a game during his senior year. He was also, like his father, a member of the Yale cheerleading squad. Late in his junior year he was, like his father Prescott Bush (1917), initiated into the Skull and Bones secret society. He graduated as a member of the Phi Beta Kappa from Yale in 1948 with a Bachelor of Arts degree in economics.
Business career.
After graduating from Yale, Bush moved his young family to West Texas. His father's business connections proved useful as he ventured into the oil business, starting as a sales clerk with Dresser Industries, a subsidiary of Brown Brothers Harriman (where Prescott Bush had served on the board of directors for 22 years). While working for Dresser, Bush lived in various places with his family: Odessa, Texas; Ventura, Bakersfield and Compton, California; and Midland, Texas. (According to eldest son George W. Bush, then age two, the family lived in one of the few duplexes in Odessa with an indoor bathroom, which they "shared with a couple of hookers".) Bush started the Bush-Overbey Oil Development company in 1951 and in 1953 co-founded the Zapata Petroleum Corporation, an oil company that drilled in the Permian Basin in Texas. In 1954 he was named president of the Zapata Offshore Company, a subsidiary which specialized in offshore drilling.
In 1959, shortly after the subsidiary became independent, Bush moved the company and his family from Midland to Houston. He continued serving as president of the company until 1964, and later chairman until 1966, but his ambitions turned political. By that time, Bush had become a millionaire. According to Time.com, Bush had a net worth of $20 million in 2015.
Political career, 1964–80.
Congressional years, 1967–71.
Bush served as Chairman of the Republican Party for Harris County, Texas in 1964, but wanted to be more involved in policy making, so he set his sights high: he aimed for a U.S. Senate seat from Texas. After winning the Republican primary, Bush faced his opponent, incumbent Democrat Ralph W. Yarborough, who attacked Bush as a right-wing extremist. Bush was a strong supporter of Republican Senator Barry Goldwater, who headed the Republican ticket as the presidential candidate. Like Goldwater, Bush strongly opposed civil rights legislation in the name of states rights. Yarborough, a leading Texas liberal, supported the civil rights legislation and was reelected by 56% - 44%. The Republican candidate for governor, Jack Crichton of Dallas, who often campaigned alongside Bush before the election, lost by a much wider margin to Governor John B. Connally Jr. Bush and the Harris County Republicans played a role in the development of the new Republican Party of the late 20th century. First, Bush worked to absorb the John Birch Society members, who were trying to take over the Republican Party. Second, during and after the Civil Rights Movement, Democrats in the South who were committed to segregation left their party, and although the "country club Republicans" had differing ideological beliefs, they found common ground in hoping to expel the Democrats from power.
Bush was elected in 1966 to a House of Representatives seat from the 7th District of Texas, defeating with 57 percent of the ballots cast the Democrat Frank Briscoe, the district attorney of Harris County known for his law and order credentials and a cousin of later Governor Dolph Briscoe. Bush was the first Republican to represent Houston in the U.S. House. Bush's representative district included Tanglewood, the Houston neighborhood that was his residence; his family had moved into Tanglewood in the 1960s. His voting record in the House was generally conservative: Bush voted for the Civil Rights Act of 1968, although it was generally unpopular in his district. He supported the Nixon administration's Vietnam policies, but broke with Republicans on the issue of birth control, which he supported. Despite being a first-term congressman, Bush was appointed to the powerful House Ways and Means Committee, where he voted to abolish the military draft. He was elected to a second term in 1968.
In 1970 Nixon convinced Bush to relinquish his House seat to run for the Senate against Ralph Yarborough, a fierce Nixon critic. In the Republican primary, Bush easily defeated conservative Robert J. Morris, by a margin of 87.6% to 12.4%. Nixon came to Texas to campaign in Longview for Bush and gubernatorial candidate Paul Eggers, a Dallas lawyer who was a close friend of U.S. Senator John G. Tower. Former Congressman Lloyd Bentsen, a more moderate Democrat and native of Mission in south Texas, defeated Yarborough in the Democratic primary. Yarborough endorsed Bentsen, who defeated Bush, 53.4 to 46.6%. As Bush's political career waned, he moved out of Houston and sold his first Tanglewood house, but for periods of time continued to reside in Tanglewood.
Ambassador to the United Nations (1971–73).
Following his 1970 loss, Bush was well known as a prominent Republican businessman from the "Sun Belt", a group of states in the Southern part of the country. Nixon noticed and appreciated the sacrifice Bush had made of his Congressional position, so he appointed him Ambassador to the United Nations. He was confirmed unanimously by the Senate, and served for two years, beginning in 1971.
Chairman of the Republican National Committee (1973–74).
Amidst the Watergate scandal, Nixon asked Bush to become chairman of the Republican National Committee in 1973. Bush accepted, and held this position when the popularity of both Nixon and the Republican Party plummeted. He defended Nixon steadfastly, but later as Nixon's complicity became clear, Bush focused more on defending the Republican Party, while still maintaining loyalty to Nixon. As chairman, Bush formally requested that Nixon eventually resign for the good of the Republican party. Nixon did this on August 9, 1974; Bush noted in his diary that "There was an aura of sadness, like somebody died... The speech was vintage Nixon—a kick or two at the press—enormous strains. One couldn't help but look at the family and the whole thing and think of his accomplishments and then think of the shame... [President Gerald Ford's swearing-in offered indeed a new spirit, a new lift."
Envoy to China, 1974–75.
Gerald Ford, Nixon's successor, appointed Bush to be Chief of the U.S. Liaison Office in the People's Republic of China. Since the United States at the time maintained official relations with the Republic of China on Taiwan and not the People's Republic of China, the Liaison Office did not have the official status of an embassy and Bush did not formally hold the position of "ambassador", though he unofficially acted as one. The 14 months that he spent in China were largely seen as beneficial for U.S.-China relations.
After Ford's accession to the presidency, Bush was under serious consideration for being nominated as Vice President. Ford eventually narrowed his list to Nelson Rockefeller and Bush. White House Chief of Staff Donald Rumsfeld reportedly preferred Rockefeller over Bush. Rockefeller was finally named and confirmed.
Bush was again passed over for the vice presidency by Ford when the president chose Bush's future presidential rival, Senator Bob Dole, to replace Rockefeller on the 1976 presidential ticket.
Director of Central Intelligence (1976–77).
In 1976 Ford brought Bush back to Washington to become Director of Central Intelligence (DCI), replacing William Colby. He served in this role for 357 days, from January 30, 1976, to January 20, 1977. The CIA had been rocked by a series of revelations, including those based on investigations by the Church Committee regarding illegal and unauthorized activities by the CIA, and Bush was credited with helping to restore the agency's morale. In his capacity as DCI, Bush gave national security briefings to Jimmy Carter both as a Presidential candidate and as President-elect, and discussed the possibility of remaining in that position in a Carter administration, but did not do so. He was succeeded by Deputy Director of Central Intelligence E. Henry Knoche, who served as acting Director of Central Intelligence until Stansfield Turner was confirmed.
Other positions, 1977–80.
After a Democratic administration took power in 1977, Bush became chairman on the Executive Committee of the First International Bank in Houston. He later spent a year as a part-time professor of Administrative Science at Rice University's Jones School of Business beginning in 1978, the year it opened; Bush said of his time there, "I loved my brief time in the world of academia." Between 1977 and 1979, he was a director of the Council on Foreign Relations foreign policy organization.
1980 presidential campaign.
Bush had decided in the late 1970s that he was going to run for president in 1980; in 1979, he attended 850 political events and traveled more than to campaign for the nation's highest office. In the contest for the Republican Party nomination, Bush stressed his wide range of government experience, while competing against rivals Senator Howard Baker of Tennessee, Senator Bob Dole of Kansas, Congressman John Anderson of Illinois (who would later run as an independent), Congressman Phil Crane, also of Illinois, former Governor John Connally of Texas, former Minnesota Governor Harold Stassen, and the front-runner Ronald Reagan, former actor, and Governor of California.
In the primary election, Bush focused almost entirely on the Iowa caucuses, while Reagan ran a more traditional campaign. Bush represented the centrist wing in the GOP, whereas Reagan represented conservatives. Bush famously labeled Reagan's supply side-influenced plans for massive tax cuts "voodoo economics". His strategy proved useful, to some degree, as he won in Iowa with 31.5% to Reagan's 29.4%. After the win, Bush stated that his campaign was full of momentum, or "Big Mo". As a result of the loss, Reagan replaced his campaign manager, reorganized his staff, and concentrated on the New Hampshire primary. The two men agreed to a debate in the state, organized by the "Nashua Telegraph", but paid for by the Reagan campaign. Reagan invited the other four candidates as well, but Bush refused to debate them, and eventually they left. The debate proved to be a pivotal moment in the campaign; when the moderator, John Breen, ordered Reagan's microphone turned off, his angry response, "I am paying for this microphone," struck a chord with the public. Bush ended up losing New Hampshire's primary with 23% to Reagan's 50%. Bush lost most of the remaining primaries as well, and formally dropped out of the race in May of that year.
With his political future seeming dismal, Bush sold his house in Houston and bought his grandfather's estate in Kennebunkport, Maine, known as "Walker's Point". At the Republican Convention, Reagan selected Bush as his Vice Presidential nominee, placing him on the winning Republican presidential ticket of 1980.
Vice Presidency (1981–89).
First term, 1981–85.
As Vice President, Bush generally took on a low profile while recognizing the constitutional limits of the office; he avoided decision-making or criticizing Reagan in any way. As had become customary, he and his wife moved into the Vice President's residence at Number One Observatory Circle, about two miles from the White House. After selling the house in the Tanglewood, the Bushes declared a room in The Houstonian Hotel in Houston as their official voting address. The Bushes attended a large number of public and ceremonial events in their positions, including many state funerals, which became a common joke for comedians. Mrs. Bush found the funerals largely beneficial, saying, "George met with many current or future heads of state at the funerals he attended, enabling him to forge personal relationships that were important to President Reagan." As the President of the Senate, Bush stayed in contact with members of Congress, and kept the president informed on occurrences on Capitol Hill.
On March 30, 1981, early into the administration, Reagan was shot and seriously wounded in Washington, D.C. Bush, second in command by the presidential line of succession, was in Fort Worth, Texas, and flew back to Washington immediately. Reagan's cabinet convened in the White House Situation Room, where they discussed various issues, including the availability of the Nuclear Football. When Bush's plane landed, his aides advised him to proceed directly to the White House by helicopter, as an image of the government still functioning despite the attack. Bush rejected the idea, responding, "Only the President lands on the South Lawn." This made a positive impression on Reagan, who recovered and returned to work within two weeks. From then on, the two men would have regular Thursday lunches in the Oval Office.
In December 1983 Bush flew to El Salvador and warned that country's military leaders to end their death squads and hold fully free elections or face the loss of U.S. aid. Bush's aides feared for his safety and thought about calling the meeting off when they discovered apparent blood stains on the floor of the presidential palace of Álvaro Magaña. Bush was never told of the aides' concerns and a tense meeting was held in which some of Magaña's personnel brandished semiautomatic weapons and refused requests to take them outside.
Bush was assigned by Reagan to chair two special task forces, on deregulation and international drug smuggling. The deregulation task force reviewed hundreds of rules, making specific recommendations on which ones to amend or revise, in order to curb the size of the federal government. The drug smuggling task force coordinated federal efforts to reduce the quantity of drugs entering the United States. Both were popular issues with conservatives, and Bush, largely a moderate, began courting them through his work.
Second term, 1985–89.
Reagan and Bush ran for reelection in 1984. The Democratic opponent, Walter Mondale, made history by choosing a woman as his running mate, New York Representative Geraldine Ferraro. She and Bush squared off in a single televised Vice Presidential debate. Serving as a contrast to the Ivy-League educated Bush, Ferraro represented a "blue-collar" district in Queens, New York; this, coupled with her popularity among female journalists, left Bush at a disadvantage. The Reagan-Bush ticket won in a landslide against the Mondale-Ferraro ticket. Early into his second term as Vice President, Bush and his aides were planning a run for the presidency in 1988. By the end of 1985, a committee had been established and over two million dollars raised for Bush.
Bush became the first Vice President to serve as Acting President when, on July 13, 1985, Reagan underwent surgery to remove polyps from his colon, making Bush acting president for approximately eight hours.
The Reagan administration was shaken by a scandal in 1986, when it was revealed that administration officials had secretly arranged weapon sales to Iran, and had used the proceeds to fund the anticommunist Contras in Nicaragua, a direct violation of the law. When the Iran-Contra Affair, as it became known, broke to the media, Bush, like Reagan, stated that he had been "out of the loop" and unaware of the diversion of funds, although this was later questioned. His diaries from that time stated "I'm one of the few people that know fully the details." Ailes and others were concerned that Bush was seen as a "wimp", an image put to rest by his evident fury in an interview with Dan Rather.
As Vice President, Bush officially opened the 1987 Pan American Games in Indianapolis.
In 1988 the USS Vincennes accidentally shot down Iran Air Flight 655 killing 290 passengers. Bush said that he would "never apologize for the United States of America. Ever. I don't care what the facts are."
1988 presidential campaign.
In the January 26, 1987, issue of "Time" magazine, in an article entitled "Where Is the Real George Bush?" journalist Robert Ajemian reported that a friend of Bush's had urged him to spend several days at Camp David thinking through his plans for his prospective presidency, to which Bush is said to have responded in exasperation, "Oh, the vision thing." This oft-cited quote became a shorthand for the charge that Bush failed to contemplate or articulate important policy positions in a compelling and coherent manner. The phrase has since become a metonym for any politician's failure to incorporate a greater vision in a campaign, and has often been applied in the media to other politicians or public figures.
Bush had been planning a presidential run since as early as 1985, and entered the Republican primary for President of the United States in October 1987. His challengers for the Republican presidential nomination included U.S. Senator Bob Dole of Kansas, U.S. Representative Jack Kemp of New York, former Governor Pete DuPont of Delaware, and conservative Christian televangelist Pat Robertson.
Though considered the early frontrunner for the nomination, Bush came in third in the Iowa caucus, behind winner Dole and runner-up Robertson. Much as Reagan did in 1980, Bush reorganized his staff and concentrated on the New Hampshire primary. With Dole ahead in New Hampshire, Bush ran television commercials portraying the senator as a tax raiser; he rebounded to win the state's primary. Following the primary, Bush and Dole had a joint media appearance, when the interviewer asked Dole if he had anything to say to Bush, Dole said, in response to the ads, "yeah, stop lying about my record" in an angry tone. This is thought to have hurt Dole's campaign to Bush's benefit. Bush continued seeing victory, winning many Southern primaries as well. Once the multiple-state primaries such as Super Tuesday began, Bush's organizational strength and fundraising lead were impossible for the other candidates to match, and the nomination was his.
Leading up to the 1988 Republican National Convention, there was much speculation as to Bush's choice of running mate. Bush chose little-known U.S. Senator Dan Quayle of Indiana, favored by conservatives. Despite Reagan's popularity, Bush trailed Democratic nominee Michael Dukakis, then Governor of Massachusetts, in most polls.
Bush, occasionally criticized for his lack of eloquence when compared to Reagan, delivered a well-received speech at the 1988 Republican National Convention. Known as the "thousand points of light" speech, it described Bush's vision of America: he endorsed the Pledge of Allegiance, prayer in schools, capital punishment, gun rights, and opposed abortion. The speech at the convention included Bush's famous pledge: "."
The general election campaign between the two men had been described as one of the nastiest in modern times, but is now seen to be considerably moderate when compared to the more recent 2016 presidential election season. Bush blamed Dukakis for polluting the Boston Harbor as the Massachusetts governor. Bush also pointed out that Dukakis was opposed to the law that would require all students to say the Pledge of Allegiance, a topic well covered in Bush's nomination acceptance speech.
Dukakis's unconditional opposition to capital punishment led to a pointed question being asked during the presidential debates. Moderator Bernard Shaw asked Dukakis if Dukakis would hypothetically support the death penalty if his wife, Kitty, were raped and murdered. Dukakis's response of no, as well as a provocative ad about convicted felon Willie Horton, contributed toward Bush's characterization of Dukakis as "soft on crime".
Bush defeated Dukakis and his running mate, Lloyd Bentsen, in the Electoral College, by 426 to 111 (Bentsen received one vote from a faithless elector). In the nationwide popular vote, Bush took 53.4% of the ballots cast while Dukakis received 45.6%. Bush became the first serving Vice President to be elected President since Martin Van Buren in 1836 as well as the first person to succeed someone from his own party to the Presidency via election to the office in his own right since Herbert Hoover in 1929.
Presidency (1989–93).
Bush was inaugurated on January 20, 1989, succeeding Ronald Reagan. He entered office at a period of change in the world; the fall of the Berlin Wall and the collapse of Soviet Union came early in his presidency. He ordered military operations in Panama and the Persian Gulf, and, at one point, was recorded as having a record-high approval rating of 89%.
In his Inaugural Address, Bush said:
Domestic policy.
Economy.
Early in his term, Bush faced the problem of what to do with leftover deficits spawned by the Reagan years. At $220 billion in 1990, the deficit had grown to three times its size since 1980. Bush was dedicated to curbing the deficit, believing that America could not continue to be a leader in the world without doing so. He began an effort to persuade the Democratic controlled Congress to act on the budget; with Republicans believing that the best way was to cut government spending, and Democrats convinced that the only way would be to raise taxes, Bush faced problems when it came to consensus building.
In the wake of a struggle with Congress, Bush was forced by the Democratic majority to raise tax revenues; as a result, many Republicans felt betrayed because Bush had promised "no new taxes" in his 1988 campaign. Perceiving a means of revenge, Republican congressmen defeated Bush's proposal which would enact spending cuts and tax increases that would reduce the deficit by $500 billion over five years. Scrambling, Bush accepted the Democrats' demands for higher taxes and more spending, which alienated him from Republicans and gave way to a sharp decrease in popularity. Bush would later say that he wished he had never signed the bill. Near the end of the 101st Congress, the president and congressional members reached a compromise on a budget package that increased the marginal tax rate and phased out exemptions for high-income taxpayers. Although he originally demanded a reduction in the capital gains tax, Bush relented on this issue as well. This agreement with the Democratic leadership in Congress proved to be a turning point in the Bush presidency; his popularity among Republicans never fully recovered.
Coming at around the same time as the budget deal, America entered into a mild recession, lasting for six months. Many government programs, such as welfare, increased. As the unemployment rate edged upward in 1991, Bush signed a bill providing additional benefits for unemployed workers. The year 1991 was marked by many corporate reorganizations, which laid off a substantial number of workers. Many now unemployed were Republicans and independents, who had believed that their jobs were secure.
By his second year in office, Bush was told by his economic advisors to stop dealing with the economy, as they believed that he had done everything necessary to ensure his reelection. By 1992, interest and inflation rates were the lowest in years, but by midyear the unemployment rate reached 7.8%, the highest since 1984. In September 1992, the Census Bureau reported that 14.2% of all Americans lived in poverty. At a press conference in 1990, Bush told reporters that he found foreign policy more enjoyable.
Major initiatives.
During a speech to commemorate the 20th anniversary of the Apollo 11 moon landing, Bush announced a vision to complete Space Station "Freedom", resume exploration of the Moon and begin exploration of Mars. Although a space station was eventually constructed–work on the International Space Station began in 1998–other work has been confounded by NASA budgetary issues. In 1998, Bush received the Rotary National Award for Space Achievement's National Space Trophy for his pioneering leadership of the U.S. space program.
Bush signed a number of major laws in his presidency, including the Americans with Disabilities Act of 1990; this was one of the most pro-civil rights bills in decades. He is also the only President to successfully veto a civil rights act, having vetoed the job-discrimination protection Civil Rights Act of 1990. Bush feared racial quotas would be imposed, but later approved watered-down Civil Rights Act of 1991. He worked to increase federal spending for education, childcare, and advanced technology research. He also signed the Radiation Exposure Compensation Act which provides monetary compensation of people who had contracted cancer and a number of other specified diseases as a direct result of their exposure to atmospheric nuclear testing undertaken by the United States during the Cold War, or their exposure to high levels of radon while doing uranium mining. In dealing with the environment, Bush reauthorized the Clean Air Act, requiring cleaner burning fuels. He quarreled with Congress over an eventually signed bill to aid police in capturing criminals, and signed into law a measure to improve the nation's highway system. Bush signed the Immigration Act of 1990, which led to a 40 percent increase in legal immigration to the United States.
Bush became a life member of the National Rifle Association early in 1988 and had campaigned as a "pro-gun" candidate with the NRA's endorsement. In March 1989, he placed a temporary ban on the import of certain semiautomatic rifles. This action cost him endorsement from the NRA in 1992. Bush publicly resigned his life membership in the organization after receiving a form letter from NRA depicting agents of the Bureau of Alcohol, Tobacco, and Firearms as "jack-booted thugs." He called the NRA letter a "vicious slander on good people."
Points of Light.
President Bush devoted attention to voluntary service as a means of solving some of America's most serious social problems. He often used the "thousand points of light" theme to describe the power of citizens to solve community problems. In his 1989 inaugural address, President Bush said, "I have spoken of a thousand points of light, of all the community organizations that are spread like stars throughout the Nation, doing good."
Four years later in his report to the nation on The Points of Light Movement, President Bush said, "Points of Light are the soul of America. They are ordinary people who reach beyond themselves to touch the lives of those in need, bringing hope and opportunity, care and friendship. By giving so generously of themselves, these remarkable individuals show us not only what is best in our heritage but what all of us are called to become."
In 1990 the Points of Light Foundation was created as a nonprofit organization in Washington to promote this spirit of volunteerism. In 2007, the Points of Light Foundation merged with the Hands On Network with the goal of strengthening volunteerism, streamlining costs and services and deepening impact. Points of Light, the organization created through this merger, has approximately 250 affiliates in 22 countries and partnerships with thousands of nonprofits and companies dedicated to volunteer service around the world. In 2012, Points of Light mobilized 4 million volunteers in 30 million hours of service worth $635 million.
On October 16, 2009, President Barack Obama held a Presidential Forum on Service hosted by former President George H. W. Bush and Points of Light at the George Bush Presidential Library Center on the campus of Texas A&M University. The event celebrated the contributions of more than 4,500 Daily Point of Light award winners and honored President Bush's legacy of service and civic engagement.
In 2011 Points of Light paid tribute to President George H. W. Bush and volunteer service at Washington, D.C.'s Kennedy Center. President Bush was joined by Presidents Jimmy Carter, Bill Clinton, and George W. Bush to highlight the role volunteer service plays in people's lives.
Daily Point of Light Award.
President Bush created the Daily Point of Light Award in 1989 to recognize ordinary Americans from all walks of life taking direct and consequential voluntary action in their communities to solve serious social problems. The President focused great attention on these individuals and organizations, both to honor them for their tremendous work and to call the nation to join them and multiply their efforts. By the end of his administration, President Bush had recognized 1,020 Daily Points of Light representing all 50 states and addressing issues ranging from care for infants and teenagers with AIDS to adult illiteracy and from gang violence to job training for the homeless. The Daily Point of Light continues to be awarded by Points of Light and President Bush continues to sign all of the awards.
On July 15, 2013, President Barack Obama welcomed President Bush to the White House to celebrate the 5,000th Daily Point of Light Award. They bestowed the award on Floyd Hammer and Kathy Hamilton of Union, Iowa, for their work founding Outreach, a nonprofit that delivers free meals to hungry children in 15 countries.
Judicial appointments.
Supreme Court.
Bush appointed the following Justices to the Supreme Court of the United States:
Other courts.
In addition to his two Supreme Court appointments, Bush appointed 42 judges to the United States Courts of Appeals, and 148 judges to the United States district courts. Among these appointments was Vaughn R. Walker, who would later be revealed to be the earliest known gay federal judge. Bush also experienced a number of judicial appointment controversies, as 11 nominees for 10 federal appellate judgeships were not processed by the Democratically-controlled Senate Judiciary Committee.
Foreign policy.
Panama.
In the 1980s, Panamanian leader Manuel Noriega, a once U.S.-supportive leader who was later accused of spying for Fidel Castro and using Panama to traffic drugs into the United States, was one of the most recognizable names in America and was constantly in the press. The struggle to remove him from power began in the Reagan administration, when economic sanctions were imposed on the country; this included prohibiting American companies and government from making payments to Panama and freezing $56 million in Panamanian funds in American banks. Reagan sent more than 2,000 American troops to Panama as well. Unlike Reagan, Bush was able to remove Noriega from power, but his administration's unsuccessful post-invasion planning hindered the needs of Panama during the establishment of the young democratic government.
In May 1989, Panama held democratic elections, in which Guillermo Endara was elected president; the results were then annulled by Noriega's government. In response, Bush sent 2,000 more troops to the country, where they began conducting regular military exercises in Panamanian territory (in violation of prior treaties). Bush then removed an embassy and ambassador from the country, and dispatched additional troops to Panama to prepare the way for an upcoming invasion. Noriega suppressed an October military coup attempt and massive protests in Panama against him, but after a U.S. serviceman was shot by Panamanian forces in December 1989, Bush ordered 24,000 troops into the country with an objective of removing Noriega from power; "Operation Just Cause" was a large-scale American military operation, and the first in more than 40 years that was not related to the Cold War.
The mission was controversial, but American forces achieved control of the country and Endara assumed the Presidency. Noriega surrendered to the United States and was convicted and imprisoned on racketeering and drug trafficking charges in April 1992. President Bush and First Lady Barbara Bush visited Panama in June 1992, to give support to the first post-invasion Panamanian government.
Soviet Union.
In 1989, just after the fall of the Berlin Wall, Bush met with Soviet General Secretary Mikhail Gorbachev in a conference on the Mediterranean island of Malta. The administration had been under intense pressure to meet with the Soviets, but not all initially found the Malta Summit to be a step in the right direction; General Brent Scowcroft, among others, was apprehensive about the meeting, saying that it might be "premature" due to concerns where, according to Condoleezza Rice, "expectations be set that something was going to happen, where the Soviets might grandstand and force U.S. into agreements that would ultimately not be good for the United States." But European leaders, including François Mitterrand and Margaret Thatcher, encouraged Bush to meet with Gorbachev, something that he did December 2 and 3, 1989. Though no agreements were signed, the meeting was viewed largely as being an important one; when asked about nuclear war, Gorbachev responded, "I assured the President of the United States that the Soviet Union would never start a hot war against the United States of America. And we would like our relations to develop in such a way that they would open greater possibilities for cooperation... This is just the beginning. We are just at the very beginning of our road, long road to a long-lasting, peaceful period." The meeting was received as a very important step to the end of the Cold War.
Another summit was held in July 1991, where the Strategic Arms Reduction Treaty (START I) was signed by Bush and Gorbachev in Moscow. The treaty took nine years in the making and was the first major arms agreement since the signing of the Intermediate Ranged Nuclear Forces Treaty by Reagan and Gorbachev in 1987. The contentions in START would reduce the strategic nuclear weapons of the United States and the USSR by about 35% over seven years, and the Soviet Union's land-based intercontinental ballistic missiles would be cut by 50%. Bush described START as "a significant step forward in dispelling half a century of mistrust". After the dissolution of the USSR in 1991, President Bush and Gorbachev declared a U.S.-Russian strategic partnership, marking the end of the Cold War.
Gulf War.
On August 2, 1990, Iraq, led by Saddam Hussein, invaded its oil-rich neighbor to the south, Kuwait; Bush condemned the invasion and began rallying opposition to Iraq in the US and among European, Asian, and Middle Eastern allies. Secretary of Defense Richard Bruce "Dick" Cheney traveled to Saudi Arabia to meet with King Fahd; Fahd requested US military aid in the matter, fearing a possible invasion of his country as well. The request was met initially with Air Force fighter jets. Iraq made attempts to negotiate a deal that would allow the country to take control of half of Kuwait. Bush rejected this proposal and insisted on a complete withdrawal of Iraqi forces. The planning of a ground operation by US-led coalition forces began forming in September 1990, headed by General Norman Schwarzkopf. Bush spoke before a joint session of the U.S. Congress regarding the authorization of air and land attacks, laying out four immediate objectives: "Iraq must withdraw from Kuwait completely, immediately, and without condition. Kuwait's legitimate government must be restored. The security and stability of the Persian Gulf must be assured. And American citizens abroad must be protected." He then outlined a fifth, long-term objective: "Out of these troubled times, our fifth objective – a new world order – can emerge: a new era – freer from the threat of terror, stronger in the pursuit of justice, and more secure in the quest for peace. An era in which the nations of the world, East and West, North and South, can prosper and live in harmony... A world where the rule of law supplants the rule of the jungle. A world in which nations recognize the shared responsibility for freedom and justice. A world where the strong respect the rights of the weak." With the United Nations Security Council opposed to Iraq's violence, Congress authorized the use of Military force with a set goal of returning control of Kuwait to the Kuwaiti government, and protecting America's interests abroad.
Early on the morning of January 17, 1991, allied forces launched the first attack, which included more than 4,000 bombing runs by coalition aircraft. This pace would continue for the next four weeks, until a ground invasion was launched on February 24, 1991. Allied forces penetrated Iraqi lines and pushed toward Kuwait City while on the west side of the country, forces were intercepting the retreating Iraqi army. Bush made the decision to stop the offensive after a mere 100 hours. Critics labeled this decision premature, as hundreds of Iraqi forces were able to escape; Bush responded by saying that he wanted to minimize U.S. casualties. Opponents further charged that Bush should have continued the attack, pushing Hussein's army back to Baghdad, then removing him from power. Bush explained that he did not give the order to overthrow the Iraqi government because it would have "incurred incalculable human and political costs... We would have been forced to occupy Baghdad and, in effect, rule Iraq."
Bush's approval ratings skyrocketed after the successful offensive. Additionally, President Bush and Secretary of State Baker felt the coalition victory had increased U.S. prestige abroad and believed there was a window of opportunity to use the political capital generated by the coalition victory to revitalize the Arab-Israeli peace process. The administration immediately returned to Arab-Israeli peacemaking following the end of the Gulf War; this resulted in the Madrid Conference, later in 1991.
Somali Civil War.
Faced with a humanitarian disaster in Somalia, exacerbated by a complete breakdown in civil order, the United Nations had created the UNOSOM I mission in April 1992 to aid the situation through humanitarian efforts, though the mission failed. The Bush administration proposed American aid to the region by assisting in creating a secure environment for humanitarian efforts and UN Resolution 794 was unanimously adopted by the Security Council on December 3, 1992. A lame duck president, Bush launched Operation Restore Hope the following day under which the United States would assume command in accordance with Resolution 794. Fighting would escalate and continue into the Clinton administration.
NAFTA.
Bush's administration, along with the Progressive Conservative Canadian Prime Minister Brian Mulroney, spearheaded the negotiations of the North American Free Trade Agreement (NAFTA), which would eliminate the majority of tariffs on products traded among the United States, Canada, and Mexico, to encourage trade amongst the countries. The treaty also restricts patents, copyrights, and trademarks, and outlines the removal of investment restrictions among the three countries.
The agreement came under heavy scrutiny amongst mainly Democrats, who charged that NAFTA resulted in a loss of American jobs. NAFTA also contained no provisions for labor rights; according to the Bush administration, the trade agreement would generate economic resources necessary to enable Mexico's government to overcome problems of funding and enforcement of its labor laws. Bush needed a renewal of negotiating authority to move forward with the NAFTA trade talks. Such authority would enable the president to negotiate a trade accord that would be submitted to Congress for a vote, thereby avoiding a situation in which the president would be required to renegotiate with trading partners those parts of an agreement that Congress wished to change. While initial signing was possible during his term, negotiations made slow, but steady, progress. President Clinton would go on to make the passage of NAFTA a priority for his administration, despite its conservative and Republican roots—with the addition of two side agreements—to achieve its passage in 1993.
The treaty has since been defended as well as criticized further. The American economy has grown 54% since the adoption of NAFTA in 1993, with 25 million new jobs created; this was seen by some as evidence of NAFTA being beneficial to the United States. With talk in early 2008 regarding a possible American withdrawal from the treaty, Carlos M. Gutierrez, current United States Secretary of Commerce, writes, "Quitting NAFTA would send economic shock waves throughout the world, and the damage would start here at home." But John J. Sweeney, President of the AFL-CIO, wrote in "The Boston Globe" that "the U.S. trade deficit with Canada and Mexico ballooned to 12 times its pre-NAFTA size, reaching $111 billion in 2004."
On January 8, 1992, Bush fainted after vomiting at a banquet hosted by the then Prime Minister of Japan, Kiichi Miyazawa. Bush was suffering from gastroenteritis.
Pardons.
As other presidents have done, Bush issued a series of pardons during his last days in office. On December 24, 1992, he granted executive clemency to six former government employees implicated in the Iran-Contra scandal of the late 1980s, most prominently former Secretary of Defense Caspar Weinberger. Bush described Weinberger, who was scheduled to stand trial on January 5, 1993, for criminal charges related to Iran-Contra, as a "true American patriot".
In addition to Weinberger, Bush pardoned Duane R. Clarridge, Clair E. George, Robert C. McFarlane, Elliott Abrams, and Alan G. Fiers Jr., all of whom had been indicted and/or convicted of criminal charges by an Independent Counsel headed by Lawrence Walsh.
Awards and honors.
In 1990 "Time "magazine named him the Man of the Year. In 1991 the U.S. Navy Memorial Foundation awarded Bush its Lone Sailor award for his naval service and his subsequent government service. In 1993, he was made an Honorary Knight Grand Cross of the Order of the Bath by Queen Elizabeth II.
1992 presidential campaign.
Bush announced his reelection bid in early 1992; with a coalition victory in the Persian Gulf War and high approval ratings, reelection initially looked likely. As a result, many leading Democrats declined to seek their party's presidential nomination. But an economic recession, and doubts of whether Bush ended the Gulf War properly, reduced his popularity.
Conservative political columnist Pat Buchanan challenged Bush for the Republican nomination, and shocked political pundits by finishing second, with 37% of the vote, in the New Hampshire primary. Bush responded by adopting more conservative positions on issues, in an attempt to undermine Buchanan's base. Once he had secured the nomination, Bush faced his challenger, Democrat and Governor of Arkansas William Jefferson "Bill" Clinton. Clinton attacked Bush as not doing enough to assist the working middle-class and being "out of touch" with the common man, a notion reinforced by reporter Andrew Rosenthal's false report that Bush was "astonished" to see a demonstration of a supermarket scanner.
In early 1992, the race took an unexpected twist when Texas billionaire H. Ross Perot launched a third party bid, claiming that neither Republicans nor Democrats could eliminate the deficit and make government more efficient. His message appealed to voters across the political spectrum disappointed with both parties' perceived fiscal irresponsibility. Perot later bowed out of the race for a short time, then reentered.
Clinton had originally been in the lead, until Perot reentered, tightening the race significantly. Nearing election day, polls suggested that the race was a dead-heat, but Clinton pulled out on top, defeating Bush in a 43% to 38% popular vote margin. Perot won 19% of the popular vote, one of the highest totals for a third party candidate in U.S. history, drawing equally from both major candidates, according to exit polls. Bush received 168 electoral votes to Clinton's 370.
Several factors were key in Bush's defeat. The ailing economy which arose from recession may have been the main factor in Bush's loss, as 7 in 10 voters said on election day that the economy was either "not so good" or "poor". On the eve of the 1992 election, after unemployment reports of 7.8% appeared (the highest since 1984), Economic recession had contributed to a sharp decline in his approval rating – to just 37%.
Conservative Republicans point to Bush's 1990 agreement to raise taxes in contradiction of his famous "" pledge. In doing so, Bush alienated many members of his conservative base, losing their support for his re-election. According to one survey, of the voters who cited Bush's broken "No New Taxes" pledge as "very important", two thirds voted for Bill Clinton. Bush had raised taxes in an attempt to address an increasing budget deficit, which has largely been attributed to the Reagan tax cuts and military spending of the 1980s. The tax revenue increase had not hurt his approval ratings to the extent that it prevented it from reaching 89% during the Gulf War, four months after the tax vote. By February 1991 his approval rating rose to its highest level—89%.
Public image.
George Bush was widely seen as a "pragmatic caretaker" president who lacked a unified and compelling long-term theme in his efforts. Indeed, Bush's sound bite where he refers to the issue of overarching purpose as "the vision thing" has become a metonym applied to other political figures accused of similar difficulties. "He does not say why he wants to be there", wrote columnist George Will, "so the public does not know why it should care if he gets his way".
His Ivy League and prep school education led to warnings by advisors that his image was too "preppy" in 1980, which resulted in deliberate efforts in his 1988 campaign to shed the image, including meeting voters at factories and shopping malls, abandoning set speeches.
His ability to gain broad international support for the Gulf War and the war's result were seen as both a diplomatic and military triumph, rousing bipartisan approval, though his decision to withdraw without removing Saddam Hussein left mixed feelings, and attention returned to the domestic front and a souring economy. A "New York Times" article mistakenly depicted Bush as being surprised to see a supermarket barcode reader; the report of his reaction exacerbated the notion that he was "out of touch". Amid the early 1990s recession, his image shifted from "conquering hero" to "politician befuddled by economic matters".
Although Bush became the first elected Republican president since Hoover in 1932 to lose a reelection bid (facing a 34% approval rating leading up to the 1992 election), the mood did not last. Despite his defeat, Bush climbed back from election day approval levels to leave office in 1993 with a 56% job approval rating. By December 2008, 60% of Americans gave Bush's presidency a positive rating.
Post-presidency (1993–present).
Upon leaving office, Bush retired with his wife, Barbara, and temporarily moved into a friend's house near the Tanglewood community of Houston as they prepared to build a permanent retirement house nearby. Ultimately they built their retirement house in the community of West Oaks, near Tanglewood. They had a presidential office within the Park Laureate Building on Memorial Drive. Mimi Swartz of "National Geographic" wrote that "The Bushes are too studiously sedate to live in River Oaks". They spend the summer at Walker's Point in Kennebunkport, Maine. On January 10, 1999, the Bushes became the longest-married Presidential couple in history, outlasting John and Abigail Adams, who were married for 54 years and 3 days. At 70 years as of January 2015, they still hold the record, by a year and a half, over Jimmy and Rosalynn Carter. Bush holds his own fishing tournament in Islamorada, an island in the Florida Keys.
In 1993, Bush was awarded an honorary knighthood (GCB) by Queen Elizabeth II. He was the third American president to receive the honor, the others being Dwight D. Eisenhower and Ronald Reagan.
In 1993, Bush visited Kuwait to commemorate the coalition's victory over Iraq in the Gulf War, where he was targeted in an assassination plot. Kuwaiti authorities arrested 17 people allegedly involved in using a car bomb to kill Bush. Through interviews with the suspects and examinations of the bomb's circuitry and wiring, the FBI established that the plot had been directed by the Iraqi Intelligence Service. A Kuwaiti court later convicted all but one of the defendants. Two months later, in retaliation, Clinton ordered the firing of 23 cruise missiles at Iraqi Intelligence Service headquarters in Baghdad. The day before the strike, U.S. Ambassador to the UN Madeleine Albright went before the Security Council to present evidence of the Iraqi plot. After the missiles were fired, Vice President Al Gore said the attack "was intended to be a proportionate response at the place where this plot" to assassinate Bush "was hatched and implemented".
From 1993 to 1999 he served as the chairman to the board of trustees for Eisenhower Fellowships, and from 2007 to 2009 was chairman of the National Constitution Center.
In 1997, the same year as the opening of his Presidential Library, the Houston international airport was renamed George Bush Intercontinental Airport.
President Bush is Honorary Chairman of Points of Light, an international nonprofit dedicated to engaging more people and resources in solving serious social problems through voluntary service.
His eldest son, George W. Bush, was inaugurated as the 43rd President of the United States on January 20, 2001, and re-elected in 2004. Through previous administrations, the elder Bush had ubiquitously been known as "George Bush" or "President Bush", but following his son's election the need to distinguish between them has made retronymic forms such as "George H. W. Bush" and "George Bush senior"—and colloquialisms such as "Bush 41" and "Bush the Elder" much more common.
Presidential library.
The George Bush Presidential Library is the presidential library named for Bush. This tenth presidential library was built between 1995 and 1997 and contains the presidential and vice-presidential papers of Bush and the vice-presidential papers of Dan Quayle. It was dedicated on November 6, 1997, and opened to the public shortly thereafter; the architectural firm of Hellmuth, Obata and Kassabaum designed the complex.
The George Bush Presidential Library and Museum is located on a site on the west campus of Texas A&M University in College Station, Texas, on a plaza adjoining the Presidential Conference Center and the Texas A&M Academic Center. The Library operates under NARA's administration and the Presidential Libraries Act of 1955's provisions.
The George Bush School of Government and Public Service is a graduate public policy school at Texas A&M University in College Station, Texas. The graduate school is part of the presidential library complex, and offers four programs: two master's degree programs ("Public Service Administration" and "International Affairs") and two certificate programs ("Advanced International Affairs" and "Homeland Security"). The master's program in International Affairs (MPIA) program offers concentration on either National Security Affairs or International Economics and Development.
Later activities.
Bush continues to make many public appearances. He and Mrs. Bush attended the state funeral of Ronald Reagan in June 2004, and of Gerald Ford in January 2007. One month later, he was awarded the Ronald Reagan Freedom Award in Beverly Hills, California, by former First Lady Nancy Reagan. Despite his political differences with Bill Clinton, it has been acknowledged that the two former presidents have become friends. He and Clinton appeared together in television ads in 2005, encouraging aid for victims of Hurricane Katrina and the 2004 Indian Ocean tsunami.
In October 2006, Bush was honored by the National Italian American Foundation (NIAF) with the NIAF One America Award for fundraising, with Bill Clinton, for the victims of the 2004 tsunami and Hurricane Katrina.
Upon the death of Gerald Ford, Bush became the oldest living (former) president, 111 days older than Jimmy Carter.
On February 18, 2008, Bush formally endorsed Senator John McCain for the presidency of the United States. The endorsement offered a boost to McCain's campaign, as the Arizona Senator had been facing criticism among many conservatives.
On January 10, 2009, both George H. W. and George W. Bush were present at the commissioning of the , the tenth and last supercarrier of the United States Navy. Bush paid a visit to the carrier again on May 26, 2009.
On February 15, 2011, he was awarded the Presidential Medal of Freedom—the highest civilian honor in the United States—by President Barack Obama.
Bush suffers from Vascular Parkinsonism, a form of Parkinson's disease which has forced him to use a motorized scooter or wheelchair since at least 2012.
In July 2013, Bush had his head shaved in a show of support for the two-year-old son of a member of his security detail, who had leukemia.
In April 2014, Frederick D. McClure, chief executive of the Bush library foundation, organized a three-day gathering in College Park, Texas, to mark the 25th anniversary of the Bush administration. Also in early 2014, the John F. Kennedy Library Foundation presented the Profile in Courage Award to Bush and Mount Vernon awarded him its first Cyrus A. Ansary Prize. The Kennedy foundation award was presented by Jack Schlossberg, the late president's grandson, to Lauren Bush Lauren, who accepted on her grandfather's behalf. The Ansary prize was presented in Houston with Ansary, Barbara Lucas, Ryan C. Crocker, dean of the Bush school since January 2010, Barbara Bush, and Curt Viebranz in attendance with the former president. Fifty thousand dollars of the prize was directed by Bush to the Bush school at Texas A&M and $25,000 will fund an animation about the Siege of Yorktown for Mt. Vernon. Viebranz and Lucas represented Mount Vernon at the presentation.
On June 12, 2014, Bush fulfilled a long-standing promise by skydiving on his 90th birthday. He made the parachute jump from a helicopter near his home at 11:15 a.m. in Kennebunkport, Maine. The jump marked the eighth time the former president had skydived, including jumps on his 80th and 85th birthday as well. He had tweeted about the incident prior to the jump, saying "It's a wonderful day in Maine — in fact, nice enough for a parachute jump."
In July 2015, Bush suffered a severe neck injury. Wearing a neck brace in October in his first public engagement since the accident, he threw the ceremonial first pitch for the Houston Astros at Minute Maid Park, at the age of 91.

</doc>
<doc id="11956" url="https://en.wikipedia.org/wiki?curid=11956" title="GPS (disambiguation)">
GPS (disambiguation)

GPS is the Global Positioning System, a satellite-based navigation system.
GPS may also refer to:

</doc>
<doc id="11958" url="https://en.wikipedia.org/wiki?curid=11958" title="George Berkeley">
George Berkeley

George Berkeley (; 12 March 168514 January 1753) — known as Bishop Berkeley (Bishop of Cloyne) — was an Anglo-Irish philosopher whose primary achievement was the advancement of a theory he called "immaterialism" (later referred to as "subjective idealism" by others). This theory denies the existence of material substance and instead contends that familiar objects like tables and chairs are only ideas in the minds of perceivers, and as a result cannot exist without being perceived. Berkeley is also known for his critique of abstraction, an important premise in his argument for immaterialism.
In 1709, Berkeley published his first major work, "", in which he discussed the limitations of human vision and advanced the theory that the proper objects of sight are not material objects, but light and colour. This foreshadowed his chief philosophical work, "A Treatise Concerning the Principles of Human Knowledge", in 1710, which, after its poor reception, he rewrote in dialogue form and published under the title "Three Dialogues between Hylas and Philonous" in 1713. Repr. in In this collection of essays, Turbayne's work comprised two papers that had been published in Philosophy and Phenomenological Research:
</ref>
In this book, Berkeley's views were represented by Philonous (Greek: "lover of mind"), while Hylas (Greek: "matter") embodies the Irish thinker's opponents, in particular John Locke.
Berkeley argued against Sir Isaac Newton's doctrine of absolute space, time and motion in "De Motu" (On Motion), published 1721. His arguments were a precursor to the views of Mach and Einstein. In 1732, he published "Alciphron", a Christian apologetic against the free-thinkers, and in 1734, he published "The Analyst", a critique of the foundations of calculus, which was influential in the development of mathematics.
His last major philosophical work, "Siris" (1744), begins by advocating the medicinal use of tar water and then continues to discuss a wide range of topics, including science, philosophy, and theology. Interest in Berkeley's work increased after World War II because he tackled many of the issues of paramount interest to philosophy in the 20th century, such as the problems of perception, the difference between primary and secondary qualities, and the importance of language.
Biography.
Ireland.
Berkeley was born at his family home, Dysart Castle, near Thomastown, County Kilkenny, Ireland, the eldest son of William Berkeley, a cadet of the noble family of Berkeley. He was educated at Kilkenny College and attended Trinity College, Dublin, earning a bachelor's degree in 1704 and completing a master's degree in 1707. He remained at Trinity College after completion of his degree as a tutor and Greek lecturer.
His earliest publication was on mathematics, but the first that brought him notice was his "", first published in 1709. In the essay, Berkeley examines visual distance, magnitude, position and problems of sight and touch. While this work raised much controversy at the time, its conclusions are now accepted as an established part of the theory of optics.
The next publication to appear was the "Treatise Concerning the Principles of Human Knowledge" in 1710, which had great success and gave him a lasting reputation, though few accepted his theory that nothing exists outside the mind. This was followed in 1713 by "Three Dialogues between Hylas and Philonous", in which he propounded his system of philosophy, the leading principle of which is that the world, as represented by our senses, depends for its existence on being perceived.
For this theory, the "Principles" gives the exposition and the "Dialogues" the defence. One of his main objectives was to combat the prevailing materialism of his time. The theory was largely received with ridicule, while even those such as Samuel Clarke and William Whiston, who did acknowledge his "extraordinary genius," were nevertheless convinced that his first principles were false.
England and Europe.
Shortly afterwards, Berkeley visited England and was received into the circle of Addison, Pope and Steele. In the period between 1714 and 1720, he interspersed his academic endeavours with periods of extensive travel in Europe, including one of the most extensive Grand Tours of the length and breadth of Italy ever undertaken. In 1721, he took Holy Orders in the Church of Ireland, earning his doctorate in divinity, and once again chose to remain at Trinity College Dublin, lecturing this time in Divinity and in Hebrew. In 1721/2 he was made Dean of Dromore and, in 1724, Dean of Derry.
In 1723, following her violent quarrel with Jonathan Swift, who had been her intimate friend for many years, Esther Vanhomrigh (for whom Swift had created the nick-name "Vanessa") named Berkeley her co-heir along with the barrister Robert Marshall; her choice of legatees caused a good deal of surprise since she did not know either of them well, although Berkeley as a very young man had known her father. Swift said generously that he did not grudge Berkeley his inheritance, much of which vanished in a lawsuit in any event. A story that Berkeley and Marshall disregarded a condition of the inheritance that they must publish the correspondence between Swift and Vanessa is probably untrue.
In 1725, he began the project of founding a college in Bermuda for training ministers and missionaries in the colony, in pursuit of which he gave up his deanery with its income of £1100.
Marriage and America.
In 1728, he married Anne Forster, daughter of John Forster, Chief Justice of the Irish Common Pleas. He then went to America on a salary of £100 per annum. He landed near Newport, Rhode Island, where he bought a plantation at Middletownthe famous "Whitehall". It has been claimed that "he introduced Palladianism into America by borrowing a design from Kent's "Designs of Inigo Jones" for the door-case of his house in Rhode Island [Whitehall". He also brought to New England John Smibert, the British artist he "discovered" in Italy, who is generally regarded as the founding father of American portrait painting. Meanwhile, he drew up plans for the ideal city he planned to build on Bermuda. He lived at the plantation while he waited for funds for his college to arrive. The funds, however, were not forthcoming, and in 1732 he left America and returned to London.
Humanitarian work.
While living in London's Saville Street, he took part in efforts to create a home for the city's abandoned children. The Foundling Hospital was founded by Royal Charter in 1739, and Berkeley is listed as one of its original governors. In 1734, he was appointed Bishop of Cloyne in Ireland, a position he was to hold until his death. Soon afterwards, he published "Alciphron, or The Minute Philosopher", directed against both Shaftesbury and Bernard de Mandeville; and in 1735–37 "The Querist".
Last works.
His last two publications were "Siris: Philosophical reflexions and inquiries concerning the virtues of tar-water, and divers other subjects connected together and arising from one another" (1744) and "Further Thoughts on Tar-water" (1752). Pine tar is an effective antiseptic and disinfectant when applied to cuts on the skin, but Berkeley argued for the use of pine tar as a broad panacea for diseases. His 1744 work on tar-water sold more copies than any of his other books during Berkeley's lifetime.
He remained at Cloyne until 1752, when he retired and went to Oxford to live with his son. He died soon afterward and was buried in Christ Church Cathedral, Oxford. His affectionate disposition and genial manners made him much loved and held in warm regard by many of his contemporaries.
Contributions to philosophy.
The use of the concepts of "spirit" and "idea" is central in Berkeley's philosophy. As used by him, these concepts are difficult to translate into modern terminology. His concept of "spirit" is close to the concept of "conscious subject" or of "mind", and the concept of "idea" is close to the concept of "sensation" or "state of mind" or "conscious experience".
Thus Berkeley denied the existence of matter as a metaphysical substance, but did not deny the existence of physical objects such as apples or mountains. (""I do not argue against the existence of any one thing that we can apprehend, either by sense or reflection. That the things I see with mine eyes and touch with my hands do exist, really exist, I make not the least question. The only thing whose existence we deny, is that which philosophers call matter or corporeal substance. And in doing of this, there is no damage done to the rest of mankind, who, I dare say, will never miss it."", "Principles "#35) This basic claim of Berkeley's thought, his "idealism", is sometimes and somewhat derisively called "immaterialism" or, occasionally, subjective idealism. In "Principles #3," he wrote, using a combination of Latin and English, "esse is percipi" (to be is to be perceived), most often if slightly inaccurately attributed to Berkeley as the pure Latin phrase "esse est percipi." The phrase appears associated with him in authoritative philosophical sources, e.g., "Berkeley holds that there are no such mind-independent things, that, in the famous phrase, esse est percipi (aut percipere) – to be is to be perceived (or to perceive)."
Hence, human knowledge is reduced to two elements: that of spirits and of ideas ("Principles" #86). In contrast to ideas, a spirit cannot be perceived. A person's spirit, which perceives ideas, is to be comprehended intuitively by inward feeling or reflection ("Principles" #89). For Berkeley, we have no direct 'idea' of spirits, albeit we have good reason to believe in the existence of other spirits, for their existence explains the purposeful regularities we find in experience. (""It is plain that we cannot know the existence of other spirits otherwise than by their operations, or the ideas by them excited in us"", Dialogues #145). This is the solution that Berkeley offers to the problem of other minds. Finally, the order and purposefulness of the whole of our experience of the world and especially of nature overwhelms us into believing in the existence of an extremely powerful and intelligent spirit that causes that order. According to Berkeley, reflection on the attributes of that external spirit leads us to identify it with God. Thus a material thing such as an apple consists of a collection of ideas (shape, color, taste, physical properties, etc.) which are caused in the spirits of humans by the spirit of God.
Theology.
A convinced adherent of Christianity, Berkeley believed God to be present as an immediate cause of all our experiences.
Here is Berkeley's proof of the existence of God:
As T.I. Oizerman explained:
Berkeley believed that God is not the distant engineer of Newtonian machinery that in the fullness of time led to the growth of a tree in the university quadrangle. Rather, the perception of the tree is an idea that God's mind has produced in the mind, and the tree continues to exist in the quadrangle when "nobody" is there, simply because God is an infinite mind that perceives all.
The philosophy of David Hume concerning causality and objectivity is an elaboration of another aspect of Berkeley's philosophy. A.A. Luce, the most eminent Berkeley scholar of the 20th century, constantly stressed the continuity of Berkeley's philosophy. The fact that Berkeley returned to his major works throughout his life, issuing revised editions with only minor changes, also counts against any theory that attributes to him a significant volte-face.
Relativity arguments.
John Locke (Berkeley's predecessor) states that we define an object by its primary and secondary qualities. He takes heat as an example of a secondary quality. If you put one hand in a bucket of cold water, and the other hand in a bucket of warm water, then put both hands in a bucket of lukewarm water, one of your hands is going to tell you that the water is cold and the other that the water is hot. Locke says that since two different objects (both your hands) perceive the water to be hot "and" cold, then the heat is not a quality of the water.
While Locke used this argument to distinguish primary from secondary qualities, Berkeley extends it to cover primary qualities in the same way. For example, he says that size is not a quality of an object because the size of the object depends on the distance between the observer and the object, or the size of the observer. Since an object is a different size to different observers, then size is not a quality of the object. Berkeley rejects shape with a similar argument and then asks: if neither primary qualities nor secondary qualities are of the object, then how can we say that there is anything more than the qualities we observe?
New theory of vision.
In his "Essay Towards a New Theory of Vision", Berkeley frequently criticised the views of the Optic Writers, a title that seems to include Molyneux, Wallis, Malebranche and Descartes. In sections 1–51, Berkeley argued against the classical scholars of optics by holding that: "spatial depth, as the distance that separates the perceiver from the perceived object is itself invisible"; namely, that space is perceived by experience instead of the senses "per se".
Berkeley goes on to argue that visual cues, such as the perceived extension or 'confusion' of an object, can only be used to indirectly judge distance, because the viewer learns to associate visual cues with tactile sensations. Berkeley gives the following analogy regarding indirect distance perception: one perceives distance indirectly just as one perceives a person's embarrassment indirectly. When looking at an embarrassed person, we infer indirectly that the person is embarrassed by observing the red color on the person's face. We know through experience that a red face tends to signal embarrassment, as we've learned to associate the two.
The question concerning the visibility of space was central to the Renaissance perspective tradition and its reliance on classical optics in the development of pictorial representations of spatial depth. This matter was debated by scholars since the 11th-century Arab polymath and mathematician Alhazen (al-Hasan Ibn al-Haytham) affirmed in experimental contexts the visibility of space. This issue, which was raised in Berkeley's theory of vision, was treated at length in the "Phenomenology of Perception" of Maurice Merleau-Ponty, in the context of confirming the visual perception of spatial depth ("la profondeur"), and by way of refuting Berkeley's thesis.
Berkeley wrote about the perception of size in addition to that of distance. He is frequently misquoted as believing in size-distance invariance – a view held by the Optic Writers. This idea is that we scale the image size according to distance in a geometrical manner. The error may have become commonplace because the eminent historian and psychologist E. G. Boring perpetuated it. In fact, Berkeley argued that the same cues that evoke distance also evoke size, and that we do not first see size and then calculate distance. It is worth quoting Berkeley's words on this issue (Section 53):
What inclines men to this mistake (beside the humour of making one see by geometry) is, that the same perceptions or ideas which suggest distance, do also suggest magnitude... I say they do not first suggest distance, and then leave it to the judgement to use that as a medium, whereby to collect the magnitude; but they have as close and immediate a connexion with the magnitude as with the distance; and suggest magnitude as independently of distance, as they do distance independently of magnitude.
Philosophy of physics.
"Berkeley's works display his keen interest in natural philosophy [...] from his earliest writings ("Arithmetica", 1707) to his latest ("Siris", 1744). Moreover, much of his philosophy is shaped fundamentally by his engagement with the science of his time." The profundity of this interest can be judged from numerous entries in Berkeley's "Philosophical Commentaries" (1707–1708), e.g. "Mem. to Examine & accurately discuss the scholium of the 8th Definition of Mr Newton's Principia." (#316)
Berkeley argued that forces and gravity, as defined by Newton, constituted "occult qualities" that "expressed nothing distinctly". He held that those who posited "something unknown in a body of which they have no idea and which they call the principle of motion, are in fact simply stating that the principle of motion is unknown." Therefore, those who "affirm that active force, action, and the principle of motion are really in bodies are adopting an opinion not based on experience." Forces and gravity existed nowhere in the phenomenal world. On the other hand, if they resided in the category of "soul" or "incorporeal thing", they "do not properly belong to physics" as a matter. Berkeley thus concluded that forces lay beyond any kind of empirical observation and could not be a part of proper science. He proposed his theory of signs as a means to explain motion and matter without reference to the "occult qualities" of force and gravity.
Philosophy of mathematics.
In addition to his contributions to philosophy, Berkeley was also very influential in the development of mathematics, although in a rather indirect sense. "Berkeley was concerned with mathematics and its philosophical interpretation from the earliest stages of his intellectual life."
Berkeley's "Philosophical Commentaries" (1707–1708) witness to his interest in mathematics:
Axiom. No reasoning about things whereof we have no idea. Therefore no reasoning about Infinitesimals. (#354)
Take away the signs from Arithmetic & Algebra, & pray what remains? (#767)
These are sciences purely Verbal, & entirely useless but for Practise in Societys of Men. No speculative knowledge, no comparison of Ideas in them. (#768) 
In 1707, Berkeley published two treatises on mathematics. In 1734, he published "The Analyst", subtitled "A DISCOURSE Addressed to an Infidel Mathematician", a critique of the Calculus. Florian Cajori called this treatise "the most spectacular event of the century in the history of British mathematics." However, a recent study suggests that Berkeley misunderstood Leibnizian calculus. The mathematician in question is believed to have been either Edmond Halley, or Isaac Newton himself—though if to the latter, then the discourse was posthumously addressed, as Newton died in 1727. "The Analyst" represented a direct attack on the foundations and principles of calculus and, in particular, the notion of fluxion or infinitesimal change, which Newton and Leibniz used to develop the calculus. Berkeley coined the phrase ghosts of departed quantities, familiar to students of calculus. Ian Stewart's book From Here to Infinity, (chapter 6), captures the gist of his criticism.
Berkeley regarded his criticism of calculus as part of his broader campaign against the religious implications of Newtonian mechanicsas a defence of traditional Christianity against deism, which tends to distance God from His worshipers. Specifically, he observed that both Newtonian and Leibnizian calculus employed infinitesimals sometimes as positive, nonzero quantities and other times as a number explicitly equal to zero. Berkeley's key point in "The Analyst" was that Newton's calculus (and the laws of motion based in calculus) lacked rigorous theoretical foundations. He claimed that
In every other Science Men prove their Conclusions by their Principles, and not their Principles by the Conclusions. But if in yours you should allow your selves this unnatural way of proceeding, the Consequence would be that you must take up with Induction, and bid adieu to Demonstration. And if you submit to this, your Authority will no longer lead the way in Points of Reason and Science.
Berkeley did not doubt that calculus produced real world truth; simple physics experiments could verify that Newton's method did what it claimed to do. "The cause of Fluxions cannot be defended by reason", but the results could be defended by empirical observation, Berkeley's preferred method of acquiring knowledge at any rate. Berkeley, however, found it paradoxical that "Mathematicians should deduce true Propositions from false Principles, be right in Conclusion, and yet err in the Premises." In "The Analyst" he endeavoured to show "how Error may bring forth Truth, though it cannot bring forth Science." Newton's science, therefore, could not on purely scientific grounds justify its conclusions, and the mechanical, deistic model of the universe could not be rationally justified.
The difficulties raised by Berkeley were still present in the work of Cauchy whose approach to calculus was a combination of infinitesimals and a notion of limit, and were eventually sidestepped by Weierstrass by means of his (ε, δ) approach, which eliminated infinitesimals altogether. More recently, Abraham Robinson restored infinitesimal methods in his 1966 book "Non-standard analysis" by showing that they can be used rigorously.
Moral philosophy.
The tract "Passive Obedience" (1712) is "Berkeley's main contribution to moral and political philosophy. [...] Other important sources for Berkeley's views on morality are "Alciphron" (1732), especially dialogues I–III, and the "Discourse to Magistrates" (1738)." "Passive Obedience" is notable partly for containing one of the earliest statements of rule utilitarianism.
Place in the history of philosophy.
Berkeley's "Treatise Concerning the Principles of Human Knowledge" was published three years before the publication of Arthur Collier's "Clavis Universalis", which made assertions similar to those of Berkeley's. However, there seemed to have been no influence or communication between the two writers.
German philosopher Arthur Schopenhauer once wrote of him: "Berkeley was, therefore, the first to treat the subjective starting-point really seriously and to demonstrate irrefutably its absolute necessity. He is the father of idealism...".
George Berkeley has gone down in the handbooks as a great spokesman of British empiricism.
Today, every student of the history of philosophy is familiar with the view that there was a sort of linear development involving three great "British Empiricists", leading from Locke through Berkeley to Hume. 
Berkeley influenced many modern philosophers, especially David Hume. Thomas Reid admitted that he put forward a drastic criticism of Berkeleianism after he had been an admirer of Berkeley's philosophical system for a long time. Berkeley's "thought made possible the work of Hume and thus Kant, notes Alfred North Whitehead." Some authors draw a parallel between Berkeley and Edmund Husserl.
When Berkeley visited America, the American educator Samuel Johnson visited him, and the two later corresponded. Johnson convinced Berkeley to establish a scholarship program at Yale, and to donate a large number of books as well as his plantation to the college when the philosopher returned to England. It was one of Yale's largest and most important donations; it doubled its library holdings, improved the college's financial position and brought Anglican religious ideas and English culture into New England. Johnson also took Berkeley's philosophy and used parts of it as a framework for his own American Practical Idealism school of philosophy. As Johnson's philosophy was taught to about half the graduates of American colleges between 1743 and 1776, and over half of the contributors to the "Declaration of Independence "were connected to it, Berkeley's ideas were indirectly a foundation of the American Mind.
Outside of America, during Berkeley's lifetime his philosophical ideas were comparatively uninfluential. But interest in his doctrine grew from the 1870s when Alexander Campbell Fraser, "the leading Berkeley scholar of the nineteenth century", published "The Works of George Berkeley." A powerful impulse to serious studies in Berkeley's philosophy was given by A. A. Luce and Thomas Edmund Jessop, "two of the twentieth century's foremost Berkeley scholars," thanks to whom Berkeley scholarship was raised to the rank of a special area of historico-philosophical science.
The proportion of Berkeley scholarship, in literature on the history of philosophy, is increasing. This can be judged from the most comprehensive bibliographies on George Berkeley. During the period of 1709–1932, about 300 writings on Berkeley were published. That amounted to 1.5 publication per annum. During the course of 1932–79, over one thousand works were brought out, i.e., 20 works per annum. Since then, the number of publications has reached 30 per annum. In 1977 publication began in Ireland of a special journal on Berkeley's life and thought ("Berkeley Studies").
Commemoration.
The University of California, Berkeley was named after him, although the pronunciation has evolved to suit American English: ( ). The naming was suggested in 1866 by Frederick Billings, a trustee of the then College of California. Billings was inspired by Berkeley's "Verses on the Prospect of Planting Arts and Learning in America", particularly the final stanza: "Westward the course of empire takes its way; The first four Acts already past, A fifth shall close the Drama with the day; Time's noblest offspring is the last."
On 18 April 1735, The Town of Berkley, in Bristol County Massachusetts, was founded and named after him. Located 40 miles south of Boston and 25 miles north of Middletown Rhode Island where Berkeley lived at his farmhouse "Whitehall" . Whitehall Museum House is the farmhouse modified by Dean George Berkeley, when he lived in the northern section of Newport, Rhode Island that comprises present-day Middletown, Rhode Island in 1729–31, while working to open his planned St Paul's College on Bermuda. It is also known as Berkeley House or Bishop George Berkeley House and was listed on the National Register of Historic Places in 1970.
A residential college and an Episcopal seminary at Yale University also bear Berkeley's name, as does the Berkeley Library at Trinity College, Dublin.
Also named for him is Berkeley Preparatory School in Tampa, Florida. This leading private school is affiliated with the Episcopal Church, has almost 1300 students from pre-kindergarten through twelfth grade, and was founded in 1960.
An Ulster History Circle blue plaque commemorating him is located in Bishop Street Within, city of Derry.
Veneration.
Berkeley is honoured together with Joseph Butler with a feast day on the liturgical calendar of the Episcopal Church (USA) on 16 June.
Further reading.
Primary.
"The Works of George Berkeley". Ed. by Alexander Campbell Fraser. In 4 Volumes. Oxford: Clarendon Press, 1901.
Ewald, William B., ed., 1996. "From Kant to Hilbert: A Source Book in the Foundations of Mathematics", 2 vols. Oxford Uni. Press.

</doc>
<doc id="11959" url="https://en.wikipedia.org/wiki?curid=11959" title="G. E. Moore">
G. E. Moore

George Edward "G. E." Moore (; 4 November 1873 – 24 October 1958) was an English philosopher. He was, with Bertrand Russell, Ludwig Wittgenstein, and (before them) Gottlob Frege, one of the founders of the analytic tradition in philosophy. Along with Russell, he led the turn away from idealism in British philosophy, and became well known for his advocacy of common sense concepts, his contributions to ethics, epistemology, and metaphysics, and "his exceptional personality and moral character." He was Professor of Philosophy at the University of Cambridge, highly influential among (though not a member of) the Bloomsbury Group, and the editor of the influential journal "Mind". He was elected a fellow of the British Academy in 1918. He was a member of the Cambridge Apostles, the intellectual secret society, from 1894 to 1901, and the Cambridge University Moral Sciences Club.
Life and work.
Moore was born in Upper Norwood, Croydon, Greater London, on 4 November 1873, the middle child of seven of Dr Daniel Moore and Henrietta Sturge. His grandfather was the author Dr George Moore. His eldest brother was Thomas Sturge Moore, a poet, writer and engraver.
He was educated at Dulwich College and in 1892 went up to Trinity College Cambridge to study classics for moral sciences. He became a Fellow of Trinity in 1898, and went on to hold the University of Cambridge chair of Mental Philosophy and Logic, from 1925 to 1939.
Moore is best known today for his defence of ethical non-naturalism, his emphasis on common sense in philosophical method, and the paradox that bears his name. He was admired by and influential among other philosophers, and also by the Bloomsbury Group, but is (unlike his colleague Russell) mostly unknown today outside of academic philosophy. Moore's essays are known for their clear, circumspect writing style, and for his methodical and patient approach to philosophical problems. He was critical of modern philosophy for its lack of progress, which he believed was in stark contrast to the dramatic advances in the natural sciences since the Renaissance. Among Moore's most famous works are his book Principia Ethica, and his essays, "The Refutation of Idealism", "A Defence of Common Sense", and "A Proof of the External World".
He was president of the Aristotelian Society from 1918-19.
Paul Levy wrote in "Moore: G. E. Moore and the Cambridge Apostles" (1979) that Moore was an important member of the secretive Cambridge Apostles.
G. E. Moore died on 24 October 1958; he was cremated at Cambridge Crematorium on 28 October 1958 and his ashes interred at the Parish of the Ascension Burial Ground in Cambridge, England; his wife, Dorothy Ely (1892-1977) was buried there. Together they had two sons, the poet Nicholas Moore and the composer Timothy Moore.
Ethics.
His influential work "Principia Ethica" is one of the main inspirations of the movement against ethical naturalism (see ethical non-naturalism) and is partly responsible for the twentieth-century concern with meta-ethics.
The naturalistic fallacy.
Moore asserted that philosophical arguments can suffer from a confusion between the use of a term in a particular argument and the definition of that term (in all arguments). He named this confusion the naturalistic fallacy. For example, an ethical argument may claim that if a thing has certain properties, then that thing is 'good.' A hedonist may argue that 'pleasant' things are 'good' things. Other theorists may argue that 'complex' things are 'good' things. Moore contends that even if such arguments are correct, they do not provide definitions for the term 'good.' The property of 'goodness' cannot be defined. It can only be shown and grasped. Any attempt to define it (X is good if it has property Y) will simply shift the problem (Why is Y-ness good in the first place?).
Open-question argument.
Moore's argument for the indefinability of "good" (and thus for the fallaciousness of the "naturalistic fallacy") is often called the open-question argument; it is presented in §13 of "Principia Ethica". The argument hinges on the nature of statements such as "Anything that is pleasant is also good" and the possibility of asking questions such as "Is it "good" that x is pleasant?" According to Moore, these questions are "open" and these statements are "significant"; and they will remain so no matter what is substituted for "pleasure". Moore concludes from this that any analysis of value is bound to fail. In other words, if value could be analysed, then such questions and statements would be trivial and obvious. Since they are anything but trivial and obvious, value must be indefinable.
Critics of Moore's arguments sometimes claim that he is appealing to general puzzles concerning analysis (cf. the paradox of analysis), rather than revealing anything special about value. The argument clearly depends on the assumption that if "good" were definable, it would be an analytic truth about "good," an assumption many contemporary moral realists like Richard Boyd and Peter Railton reject. Other responses appeal to the Fregean distinction between sense and reference, allowing that value concepts are special and "sui generis", but insisting that value properties are nothing but natural properties (this strategy is similar to that taken by non-reductive materialists in philosophy of mind).
Good as indefinable.
Moore contended that goodness cannot be analysed in terms of any other property. In "Principia Ethica, "he writes:
Therefore, we cannot define "good" by explaining it in other words. We can only point to an "action" or a "thing" and say "That is good." Similarly, we cannot describe to a blind person exactly what yellow is. We can only show a sighted person a piece of yellow paper or a yellow scrap of cloth and say "That is yellow."
Good as a non-natural property.
In addition to categorising "good" as indefinable, Moore also emphasized that it is a non-natural property. This means that it cannot be empirically or scientifically tested or verified - it is not within the bounds of "natural science".
Moral knowledge.
Moore argued that once arguments based on the naturalistic fallacy had been discarded, questions of intrinsic goodness could only be settled by appeal to what he (following Sidgwick) called "moral intuitions:" self-evident propositions which recommend themselves to moral reflection, but which are not susceptible to either direct proof or disproof (PE § 45). As a result of his view, he has often been described by later writers as an advocate of ethical intuitionism. Moore, however, wished to distinguish his view from the views usually described as "Intuitionist" when "Principia Ethica" was written:
Moore distinguished his view from the view of deontological intuitionists, who held that "intuitions" could determine questions about what "actions" are right or required by duty. Moore, as a consequentialist, argued that "duties" and moral rules could be determined by investigating the "effects" of particular actions or kinds of actions (PE § 89), and so were matters for empirical investigation rather than direct objects of intuition (PE § 90). On Moore's view, "intuitions" revealed not the rightness or wrongness of specific actions, but only what things were good in themselves, as "ends to be pursued".
Proof of an external world.
One of the most important parts of Moore's philosophical development was his break from the idealism that dominated British philosophy (as represented in the works of his former teachers F. H. Bradley and John McTaggart), and his defence of what he regarded as a "common sense" form of realism. In his 1925 essay "A Defence of Common Sense", he argued against idealism and scepticism toward the external world, on the grounds that they could not give reasons to accept that their metaphysical premises were more plausible than the reasons we have to accept the common sense claims about our knowledge of the world, which sceptics and idealists must deny. He famously put the point into dramatic relief with his 1939 essay "Proof of an External World", in which he gave a common sense argument against scepticism by raising his right hand and saying "Here is one hand," and then raising his left and saying "And here is another," then concluding that there are at least two external objects in the world, and therefore that he knows (by this argument) that an external world exists. Not surprisingly, not everyone inclined to sceptical doubts found Moore's method of argument entirely convincing; Moore, however, defends his argument on the grounds that sceptical arguments seem invariably to require an appeal to "philosophical intuitions" that we have considerably less reason to accept than we have for the common sense claims that they supposedly refute. (In addition to fueling Moore's own work, the "Here is one hand" argument also deeply influenced Wittgenstein, who spent his last years working out a new approach to Moore's argument in the remarks that were published posthumously as "On Certainty".)
Moore's paradox.
Moore is also remembered for drawing attention to the peculiar inconsistency involved in uttering a sentence such as "It is raining but I do not believe it is raining."—a puzzle which is now commonly called "Moore's paradox." The puzzle arises because it seems impossible for anyone to consistently "assert" such a sentence; but there doesn't seem to be any "logical contradiction" between "It is raining" and "I don't believe that it is raining." because the former is a statement about the weather and the latter a statement about a person's belief about the weather, and it is perfectly logically possible that it may rain whilst a person does not believe that it is raining.
In addition to Moore's own work on the paradox, the puzzle also inspired a great deal of work by Ludwig Wittgenstein, who described the paradox as the most impressive philosophical insight that Moore had ever introduced. It is said that when Wittgenstein first heard this paradox one evening (which Moore had earlier stated in a lecture), he rushed round to Moore's lodgings, got him out of bed and insisted that Moore repeat the entire lecture to him.
Organic wholes.
Moore’s description of the principle of organic unity is extremely straightforward; nonetheless, it is a principle that seems to have generally escaped ethical philosophers and ontologists before his time:
According to Moore, a moral actor cannot survey the "goodness" inherent in the various parts of a situation, assign a value to each of them, and then generate a sum in order to get an idea of its total value. A moral scenario is a complex assembly of parts, and its total value is often created by the relations between those parts, and not by their individual value. The organic metaphor is thus very appropriate: biological organisms seem to have emergent properties which cannot be found anywhere in their individual parts. For example, a human brain seems to exhibit a capacity for thought when none of its neurons exhibit any such capacity. In the same way, a moral scenario can have a value far greater than the sum of its component parts.
To understand the application of the organic principle to questions of value, it is perhaps best to consider Moore’s primary example, that of a consciousness experiencing a beautiful object. To see how the principle works, a thinker engages in "reflective isolation", the act of isolating a given concept in a kind of null-context and determining its intrinsic value. In our example, we can easily see that per "sui", beautiful objects and consciousnesses are not particularly valuable things. They might have some value, but when we consider the total value of a consciousness experiencing a beautiful object, it seems to exceed the simple sum of these values (Principia 18:2).

</doc>
<doc id="11964" url="https://en.wikipedia.org/wiki?curid=11964" title="Genus–differentia definition">
Genus–differentia definition

A genus–differentia definition is a type of intensional definition, and it is composed of two parts:
For example, consider these two definitions:
Those definitions can be expressed as one genus and two "differentiae":
Differentiation and Abstraction.
The process of producing new definitions by "extending" existing definitions is commonly known as differentiation (and also as derivation). The reverse process, by which just part of an existing definition is used itself as a new definition, is called abstraction; the new definition is called "an abstraction" and it is said to have been "abstracted away from" the existing definition.
For instance, consider the following:
A part of that definition may be singled out (using parentheses here):
and with that part, an abstraction may be formed:
Then, the definition of "a square" may be recast with that abstraction as its genus:
Similarly, the definition of "a square" may be rearranged and another portion singled out:
leading to the following abstraction:
Then, the definition of "a square" may be recast with that abstraction as its genus:
In fact, the definition of "a square" may be recast in terms of both of the abstractions, where one acts as the genus and the other acts as the differentia:
Hence, abstraction is crucial in simplifying definitions.
Multiplicity.
When multiple definitions could serve equally well, then all such definitions apply simultaneously. Thus, "a square" is a member of both the genus "[a] rectangle" and the genus "[a] rhombus". In such a case, it is notationally convenient to consolidate the definitions into one definition that is expressed with multiple genera (and possibly no differentia, as in the following):
or completely equivalently:
More generally, a collection of formula_1 equivalent definitions (each of which is expressed with one unique genus) can be recast as one definition that is expressed with formula_2 genera. Thus, the following:
could be recast as:
Structure.
A genus of a definition provides a means by which to specify an "is-a relationship":
The non-genus portion of the differentia of a definition provides a means by which to specify a "has-a relationship":
When a system of definitions is constructed with genera and differentiae, the definitions can be thought of as nodes forming a hierarchy or—more generally—a directed acyclic graph; a node that has no predecessor is "a most general definition"; each node along a directed path is "more differentiated" (or "more derived") than any one of its predecessors, and a node with no successor is "a most differentiated" (or "a most derived") definition.
When a definition, "S", is the tail of each of its successors (that is, "S" has at least one successor and each direct successor of "S" is a most differentiated definition), then "S" is often called "the species" of each of its successors, and each direct successor of "S" is often called "an individual" (or "an entity") of the species "S"; that is, the genus of an individual is synonymously called "the species" of that individual. Furthermore, the differentia of an individual is synonymously called "the identity" of that individual. For instance, consider the following definition:
In this case:
As in that example, the identity itself (or some part of it) is often used to refer to the entire individual, a phenomenon that is known in linguistics as a "pars pro toto synecdoche".

</doc>
<doc id="11966" url="https://en.wikipedia.org/wiki?curid=11966" title="Firearm">
Firearm

A firearm is a portable gun, being a barreled weapon that launches one or more projectiles often driven by the action of an explosive force. The first primitive firearms were invented in 13th century China when the one-person-portable fire lance was combined with projectiles. The technology gradually spread through the rest of East Asia, South Asia, Middle East and then into Europe. In older firearms, the propellant was typically black powder, but modern firearms use smokeless powder or other propellants. Most modern firearms (with the notable exception of smoothbore shotguns) have rifled barrels to impart spin to the projectile for improved flight stability.
Modern firearms are usually described by their caliber (i.e. their bore diameter, this is given in millimeters or inches e.g. 7.5mm, .357) or in the case of shotguns their gauge (e.g. 12 ga.); the type of action employed (muzzle, breech, lever, bolt, pump, revolver, semi-automatic, automatic etc.) together with the usual means of deportment (hand-held or mechanical mounting). They may be further distinguished by reference to the type of barrel used (rifled) and the barrel length (19 inch), the design's primary intended use (e.g. hunting rifle), or the commonly accepted name for a particular variation (e.g. Gatling gun). The word "firearms" usually is used in a sense restricted to small arms (weapons that can be carried by a single person), whereas the word "artillery" covers larger gunpowder-fired weapons.
Firearms are aimed visually at their targets by hand using either iron sights or optical sights. The accurate range of pistols is generally limited to , while most rifles are accurate to using iron sights, or longer ranges using optical sights (firearm rounds may be dangerous or lethal well beyond their accurate range; minimum distance for safety is much greater than specified range). Some purpose-built sniper rifles are accurate to ranges of more than .
Types of firearms.
Configuration.
Handguns.
The smallest of all firearms is the handgun. There are three common types of handguns: single-shot pistols (more common historically), revolvers, and semi-automatic pistols. Revolvers have a number of firing chambers or "charge holes" in a revolving cylinder; each chamber in the cylinder is loaded with a single cartridge or charge. Semi-automatic pistols have a single fixed firing chamber machined into the rear of the barrel, and a magazine so they can be used to fire more than one round. Each press of the trigger fires a cartridge, using the energy of the cartridge to activate the mechanism so that the next cartridge may be fired immediately. This is opposed to "double-action" revolvers which accomplish the same end using a mechanical action linked to the trigger pull.
Prior to the 19th century, all handguns were single-shot muzzleloaders. With the invention of the revolver in 1818, handguns capable of holding multiple rounds became popular. Certain designs of auto-loading pistol appeared beginning in the 1870s and had largely supplanted revolvers in military applications by the end of World War I. By the end of the 20th century, most handguns carried regularly by military, police and civilians were semi-automatic, although revolvers were still widely used. Generally speaking, military and police forces use semi-automatic pistols due to their high magazine capacities (10 to 17 or, in some cases, over 25 rounds of ammunition) and ability to rapidly reload by simply removing the empty magazine and inserting a loaded one. Revolvers are very common among handgun hunters because revolver cartridges are usually more powerful than similar caliber semi-automatic pistol cartridges (which are designed for self-defense) and the strength, simplicity and durability of the revolver design is well-suited to outdoor use. Revolvers, especially in .22LR and 38 Special/357 Magnum, are also common concealed weapons in jurisdictions allowing this practice because their simple mechanics make them smaller than many autoloaders while remaining reliable. Both designs are common among civilian gun owners, depending on the owner's intention (self-defense, hunting, target shooting, competitions, collecting, etc.).
Long guns.
A long gun is generally any firearm that is larger than a handgun and is designed to be held and fired with both hands, either from the hip or the shoulder. Long guns typically have a barrel between 10 and 30 inches (there are restrictions on minimum barrel length in many jurisdictions; maximum barrel length is usually a matter of practicality), that along with the receiver and trigger group is mounted into a wood, plastic, metal or composite "stock", composed of one or more pieces that form a foregrip, rear grip, and optionally (but typically) a shoulder mount called the "butt". Early long arms, from the Renaissance up to the mid-19th century, were generally smoothbore firearms that fired one or more ball shot, called muskets.
Rifles and shotguns.
Most modern long guns are either rifles or shotguns. Both are the successors of the musket, diverging from their parent weapon in distinct ways. A rifle is so named for the spiral fluting (Rifling) carved into the inner surface of its barrel, which imparts a self-stabilizing spin to the single bullets it fires. Shotguns are predominantly smoothbore firearms designed to fire a number of "shot"; pellet sizes commonly ranging between 2 mm #9 birdshot and 8.4 mm #00 (double-aught) buckshot. Shotguns are also capable of firing single slugs, or specialty (often "less lethal") rounds such as bean bags, tear gas or breaching rounds. Rifles have a very small impact area but a long range and high accuracy. Shotguns have a large impact area with considerably less range and accuracy. However, the larger impact area can compensate for reduced accuracy, since shot spreads during flight; consequently, in hunting, shotguns are generally used for flying game.
Rifles and shotguns are commonly used for hunting and often to defend a home or place of business. Usually, large game are hunted with rifles (although shotguns can be used, particularly with slugs), while birds are hunted with shotguns. Shotguns are sometimes preferred for defending a home or business due to their wide impact area, multiple wound tracks (when using buckshot), shorter range, and reduced penetration of walls (when using lighter shot), which significantly reduces the likelihood of unintended harm, although the handgun is also common.
There are a variety of types of rifles and shotguns based on the method they are reloaded. Bolt-action and lever-action rifles are manually operated. Manipulation of the bolt or the lever causes the spent cartridge to be removed, the firing mechanism recocked, and a fresh cartridge inserted. These two types of action are almost exclusively used by rifles. Slide-action (commonly called 'pump-action') rifles and shotguns are manually cycled by shuttling the foregrip of the firearm back and forth. This type of action is typically used by shotguns, but several major manufacturers make rifles that use this action.
Both rifles and shotguns also come in break-action varieties that do not have any kind of reloading mechanism at all but must be hand-loaded after each shot. Both rifles and shotguns come in single- and double-barreled varieties; however due to the expense and difficulty of manufacturing, double-barreled rifles are rare. Double-barreled rifles are typically intended for African big-game hunts where the animals are dangerous, ranges are short, and speed is of the essence. Very large and powerful calibers are normal for these firearms.
Rifles have been in nationally featured marksmanship events in Europe and the United States since at least the 18th century, when rifles were first becoming widely available. One of the earliest purely "American" rifle-shooting competitions took place in 1775, when Daniel Morgan was recruiting sharpshooters in Virginia for the impending American Revolutionary War. In some countries, rifle marksmanship is still a matter of national pride. Some specialized rifles in the larger calibers are claimed to have an accurate range of up to about , although most have considerably less. In the second half of the 20th century, competitive shotgun sports became perhaps even more popular than riflery, largely due to the motion and immediate feedback in activities such as skeet, trap and sporting clays.
In military use, bolt-action rifles with high-power scopes are common as sniper rifles, however by the Korean War the traditional bolt-action and semi-automatic rifles used by infantrymen had been supplemented by select-fire designs known as "automatic rifles".
Carbines.
A carbine is a firearm similar to a rifle in form and intended usage, but generally shorter or smaller than the typical "full-size" hunting or battle rifle of similar time period, and sometimes using a smaller or less-powerful cartridge. Carbines were and are typically used by members of the military in roles that are expected to engage in combat, but where a full-size rifle would be an impediment to the primary duties of that soldier (vehicle drivers, field commanders and support staff, airborne troops, engineers, etc.). Carbines are also common in law enforcement and among civilian owners where similar size, space and/or power concerns may exist. Carbines, like rifles, can be single-shot, repeating-action, semi-automatic or select-fire/fully automatic, generally depending on the time period and intended market. Common historical examples include the Winchester Model 1892, Lee–Enfield "Jungle Carbine", SKS, M1 carbine (no relation to the larger M1 Garand) and M4 carbine (a more compact variant of the current M16 rifle). Modern U.S. civilian carbines include compact customizations of the AR-15, Ruger Mini-14, Beretta Cx4 Storm, Kel-Tec SUB-2000, bolt-action rifles generally falling under the specifications of a scout rifle, and aftermarket conversion kits for popular pistols including the M1911 and Glock models.
Function.
Firearms are also categorized by their functioning cycle or "action" which describes its loading, firing, and unloading cycle.
Manual.
The earliest evolution of the firearm, there are many types of manual action firearms. These can be divided into two basic categories: single shot and repeating.
A single shot firearm can only be fired once per equipped barrel before it must be reloaded or charged via an external mechanism or series of steps. A repeating firearm can be fired multiple times, but can only be fired once with each subsequent pull of the trigger. Between trigger pulls, the firearm's action must be reloaded or charged via an internal mechanism.
Semi-automatic.
A semi-automatic, or self-loading, firearm is one that performs all steps necessary to prepare the it to discharge again after firing—assuming cartridges remain in the weapon's feed device or magazine. While the rifle variety of semi-automatic firearms may resemble military-style firearms they are not equally classified with the former as "Assault Weapons".
Automatic.
An "automatic" firearm is generally defined as one that continues to load and fire cartridges from its magazine as long as the trigger is depressed (or until the magazine is depleted). The first weapon generally considered in this category is the Gatling gun, originally a carriage-mounted, crank-operated firearm with multiple rotating barrels that was fielded in the American Civil War. The modern trigger-actuated machine gun began with various designs developed in the late 19th century and fielded in World War I, such as the Maxim gun, Lewis Gun, and MG 08 "Spandau". Most automatic weapons are classed as long guns (as the ammunition used is of similar type as for rifles, and the recoil of the weapon's rapid fire is better controlled with two hands), but handgun-sized automatic weapons also exist, generally in the "submachine gun" or "machine pistol" class.
Machine guns.
A machine gun is a fully automatic emplaceable weapon, most often separated from other classes of automatic weapon by the use of belt-fed ammunition (though some designs employ drum, pan or hopper magazines), generally in a rifle-inspired caliber ranging between 5.56×45mm NATO (.223 Remington) for a light machine gun to as large as .50 BMG or even larger for crewed or aircraft weapons. Although not widely fielded until World War I, early machine guns were being used by militaries in the second half of the 19th century. Notables in the U.S. arsenal during the 20th century included the M2 Browning .50 caliber heavy machine gun and M1919 Browning .30 caliber medium machine gun, and the M60 7.62×51mm NATO general-purpose machine gun which came into use around the Vietnam War. Machine guns of this type were originally defensive firearms crewed by at least two men, mainly because of the difficulties involved in moving and placing them, their ammunition, and their tripod. In contrast, modern light machine guns such as the FN Minimi are often wielded by a single infantryman. They provide a large ammunition capacity and a high rate of fire, and are typically used to give suppressing fire during infantry movement. Accuracy on machine guns varies based on a wide number of factors from design to manufacturing tolerances, most of which have been improved over time. Machine guns are often mounted on vehicles or helicopters, and have been used since World War I as offensive firearms in fighter aircraft and tanks (e.g. for air combat or suppressing fire for ground troop support).
The definition of machine gun is different in U.S. law. The National Firearms Act and Firearm Owners Protection Act define a "machine gun" in the United States code "Title 26, Subtitle E, Chapter 53, Subchapter B, Part 1, § 5845" as:
"... any firearm which shoots ... automatically more than one shot, without manual reloading, by a single function of the trigger". "Machine gun" is therefore largely synonymous with "automatic weapon" in the U.S. civilian parlance, covering all automatic firearms.
Submachine guns.
A submachine gun is a magazine-fed firearm, usually smaller than other automatic firearms, that fires pistol-caliber ammunition; for this reason certain submachine guns can also be referred to as "machine pistols", especially when referring to handgun-sized designs such as the Škorpion vz. 61 and Glock 18. Well-known examples are the Israeli Uzi and Heckler & Koch MP5 which use the 9×19mm Parabellum cartridge, and the American Thompson submachine gun which fires .45 ACP. Because of their small size and limited projectile penetration compared to high-power rifle rounds, submachine guns are commonly favored by military, paramilitary and police forces for close-quarters engagements such as inside buildings, in urban areas or in trench complexes.
Submachine guns were originally about the size of carbines. Because they fire pistol ammunition, they have limited long-range use, but in close combat can be used in fully automatic in a controllable manner due to the lighter recoil of the pistol ammunition. They are also extremely inexpensive and simple to build in time of war, enabling a nation to quickly arm its military. In the latter half of the 20th century, submachine guns were being miniaturized to the point of being only slightly larger than some large handguns. The most widely used submachine gun at the end of the 20th century was the Heckler & Koch MP5. The MP5 is actually designated as a "machine pistol" by Heckler & Koch (MP5 stands for "Maschinenpistole 5", or Machine Pistol 5), although some reserve this designation for even smaller submachine guns such as the MAC-10 and Glock 18, which are about the size and shape of pistols.
Personal defense weapons.
A related class of firearm to the submachine gun is the "Personal Defense Weapon" or PDW, which is in simplest terms a submachine gun designed to fire rounds similar to rifle cartridges. A submachine gun is desirable for its compact size and ammunition capacity, however a pistol round lacks the penetrating capability of a rifle round. Conversely, rifle bullets can pierce light armor and are easier to shoot accurately, but even a carbine such as the Colt M4 is larger and/or longer than a submachine gun, making it harder to maneuver in close quarters. The solution many firearms manufacturers have presented is a weapon resembling a submachine gun in size and general configuration, but which fires a higher-powered armor-penetrating round (often specially designed for the weapon), thus combining the advantages of a carbine and submachine gun. The FN P90 and Heckler & Koch MP7 are examples.
Automatic rifles.
An automatic rifle is a magazine-fed firearm, wielded by a single infantryman, that is chambered for rifle cartridges and capable of automatic fire. The M1918 Browning Automatic Rifle was the first U.S. infantry weapon of this type, and was generally used for suppressive or support fire in the role now usually filled by the light machine gun. Other early automatic rifles include the Fedorov Avtomat and the Huot Automatic Rifle. Later, German forces fielded the Sturmgewehr 44 during World War II, a light automatic rifle firing a reduced power "intermediate cartridge". This design was to become the basis for the "assault rifle" subclass of automatic weapons, as contrasted with "battle rifles", which generally fire a traditional "full-power" rifle cartridge.
Military style semi or full automatic rifles.
In World War II, Germany introduced the StG 44, and brought to the forefront of firearm technology what eventually became the class of firearm most widely adopted by the military, the assault rifle. An assault rifle is usually slightly smaller than a battle rifle such as the Karabiner 98k, but the chief differences defining an assault rifle are select-fire capability and the use of a rifle round of lesser power, known as an intermediate cartridge. This reduces recoil allowing for controllable bursts at short range like a submachine gun, while retaining rifle-like accuracy at medium ranges. Generally, assault rifles have mechanisms that allow the user to select between single shots, fully automatic bursts, or fully automatic fire. Universally, civilian versions of military assault rifles are strictly semi-automatic.
Soviet engineer Mikhail Kalashnikov quickly adapted the German concept, using a less-powerful 7.62×39mm cartridge derived from the standard 7.62×54mmR Russian battle rifle round, to produce the AK-47, which has become the world's most widely used assault rifle. Soon after World War II, the Automatic Kalashnikov AK-47 assault rifle began to be fielded by the Soviet Union and its allies in the Eastern Bloc, as well as by nations such as China, North Korea, and North Vietnam.
In the United States, the assault rifle design was later in coming; the replacement for the M1 Garand of WWII was another John Garand design chambered for the new 7.62×51mm NATO cartridge; the select-fire M14, which was used by the U.S. military until the 1960s. The significant recoil of the M14 when fired in full-automatic mode was seen as a problem as it reduced accuracy, and in the 1960s it was replaced by Eugene Stoner's AR-15, which also marked a switch from the powerful .30 caliber cartridges used by the U.S. military up until early in the Vietnam War to the much less powerful but far lighter and light recoiling .223 caliber (5.56mm) intermediate cartridge. The military later designated the AR-15 as the "M16". The civilian version of the M16 continues to be known as the AR-15 and looks exactly like the military version, although to conform to B.A.T.F.E. regulations in the U.S., it lacks the mechanism that permits fully automatic fire.
Variants of both of the M16 and AK-47 are still in wide international use today, though other automatic rifle designs have since been introduced. A smaller version of the M16A2, the M4 carbine, is widely used by U.S. and NATO tank and vehicle crews, airbornes, support staff, and in other scenarios where space is limited. The IMI Galil, an Israeli-designed weapon based on the action of the AK-47, is in use by Israel, Italy, Burma, the Philippines, Peru, and Colombia. Swiss Arms of Switzerland produces the SIG SG 550 assault rifle used by France, Chile, and Spain among others, and Steyr Mannlicher produces the AUG, a bullpup rifle in use in Austria, Australia, New Zealand, Ireland, and Saudi Arabia among other nations.
Modern designs call for compact weapons retaining firepower. The bullpup design, by mounting the magazine behind the trigger, unifies the accuracy and firepower of the traditional assault rifle with the compact size of the submachine gun (though submachine guns are still used); examples are the French FAMAS and the British SA80. Recently, smaller but exceedingly penetrative ammunition types have been introduced to counter ballistic armour. Such designs are the basis for the FN P90 and Heckler & Koch MP7.
Avenues for improvements over traditional ammunition, include caseless ammunition (an example being the German Heckler & Koch G11) and the flechette (allowing for extreme penetration abilities and a very flat trajectory, gained however at the cost of stopping power).
History.
Some say the first primitive firearms were invented about 1250 A.D. in China when the man-portable fire lance (a bamboo or metal tube that could shoot ignited gunpowder) was combined with projectiles such as scrap metal, broken porcelain, or darts/arrows. Historian W.H.B. Smith says that Greek fire predates the early Chinese technology by 600 years and that the origin of gunpowder and firearms are unknown because records have been mistranslated and misquoted.
The earliest depiction of a firearm is a sculpture from a cave in Sichuan, China. The sculpture dates to the 12th century and is of a figure carrying a vase-shaped bombard, with flames and a cannonball coming out of it. The oldest surviving gun, made of bronze, has been dated to 1288 because it was discovered at a site in modern-day Acheng District, Heilongjiang, China, where the "Yuan Shi" records that battles were fought at that time. The firearm had a 6.9 inch barrel of a 1-inch diameter, a 2.6 inch chamber for the gunpowder and a socket for the firearm's handle. It is 13.4 inches long and 7.8 pounds without the handle, which would have been made of wood.
The Europeans and Arabs (first Mamluks) obtained firearms in the 14th century. "The Europeans certainly had firearms by the first half of the 14th century. The Arabs obtained firearms in the 14th century too, and the Turks, Iranians, and Indians all got them no later than the 15th century, in each case directly or indirectly from the Europeans. The Koreans adopted firearms from the Chinese in the 14th century, but the Japanese did not acquire them until the 16th century, and then from the Portuguese rather than the Chinese." Turks, Iranians (first Aq Qoyunlu and Safavids), and Indians (first Mughals) all had firearms no later than the 15th century, in each case directly or indirectly from the Europeans. The Japanese did not acquire firearms until the 16th century, and then from the Portuguese rather than the Chinese.
The development behind firearms accelerated during the 19th and 20th centuries. Breech-loading became more or less a universal standard for the reloading of most hand-held firearms and continues to be so with some notable exceptions (such as mortars). Instead of loading individual rounds into weapons, magazines holding multiple munitions were adopted—these aided rapid reloading. Automatic and semi-automatic firing mechanisms meant that a single soldier could fire many more rounds in a minute than a vintage weapon could fire over the course of a battle. Polymers and alloys in firearm construction made weaponry progressively lighter and thus easier to deploy. Ammunition changed over the centuries from simple metallic ball-shaped projectiles that rattled down the barrel to bullets and cartridges manufactured to high precision. Especially in the past century has particular attention been devoted to accuracy and sighting to make firearms altogether far more accurate than ever before. More than any single factor though, firearms have proliferated due to the advent of mass production—enabling arms manufacturers to produce large quantities of weaponry to a consistent standard.
The force of a projectile is related to the kinetic energy imparted to it, given by the formula formula_1 where formula_2 is the mass and formula_3 is the velocity of the projectile.
Generally, kinetic energy can be enhanced in two ways:
Velocities of bullets increased with the use of a "jacket" of a metal such as copper or copper alloys that covered a lead core and allowed the bullet to glide down the barrel more easily than exposed lead. Such bullets are designated as "full metal jacket" (FMJ). Such FMJ bullets are less likely to fragment on impact and are more likely to traverse through a target while imparting less energy. Hence, FMJ bullets impart less tissue damage than non-jacketed bullets that expand. (Dougherty and Eidt, 2009) This led to their adoption for military use by countries adhering to the Hague Convention in 1899.
That said, the basic principle behind firearm operation remains unchanged to this day. A musket of several centuries ago is still similar in principle to a modern-day assault rifle—using the expansion of gases to propel projectiles over long distances—albeit less accurately and rapidly.
Evolution.
Early models.
Fire lances.
The Chinese fire lance was the direct predecessor to the modern concept of the firearm. It was not a gun itself, but an addition to the soldiers' spears. Originally it consisted of paper or bamboo barrels that would have incendiary gunpowder within it, that could be lit one time and would project flames at the enemy. Sometimes the Chinese troops would place small projectiles within the barrel that would also be projected when the gunpowder was lit, but most of the explosive force would create flames. Later, the barrel was changed to be made of metal, so that a more explosive gunpowder could be used and put more force into the propulsion of the projectile.
Hand cannons.
The original predecessor of all firearms, the Chinese fire lance and hand cannon were loaded with gunpowder and the shot (initially lead shot, later replaced by cast iron) through the muzzle, while a fuse was placed at the rear. This fuse was lit, causing the gunpowder to ignite and propel the cannonball. In military use, the standard hand cannon was tremendously powerful, while also being somewhat useless due to relative inability of the gunner to aim the weapon, or control the ballistic properties of the projectile. Recoil could be absorbed by bracing the barrel against the ground using a wooden support, the forerunner of the stock. Neither the amount of gunpowder, nor the consistency in projectile dimensions were controlled, with resulting inaccuracy in firing due to windage, and due to the difference in diameter between the bore and the shot. The hand cannons were replaced by lighter carriage-mounted artillery pieces, and ultimately the arquebus.
Muskets.
Muzzle-loading muskets (smooth-bored long guns) were among the first firearms developed. The firearm was loaded through the muzzle with gunpowder, optionally some wadding and then a bullet (usually a solid lead ball, but musketeers could shoot stones when they ran out of bullets). Greatly improved muzzleloaders (usually rifled instead of smooth-bored) are manufactured today and have many enthusiasts, many of whom hunt large and small game with their guns. Muzzleloaders have to be manually reloaded after each shot; a skilled archer could fire multiple arrows faster than most early muskets could be reloaded and fired, although by the mid-18th century, when muzzleloaders became the standard small armament of the military, a well-drilled soldier could fire six rounds in a minute using prepared cartridges in his musket. Before then, effectiveness of muzzleloaders was hindered by both the low reloading speed and, before the firing mechanism was perfected, the very high risk posed by the firearm to the person attempting to fire it.
One interesting solution to the reloading problem was the "Roman Candle Gun" with superposed loads. This was a muzzleloader in which multiple charges and balls were loaded one on top of the other, with a small hole in each ball to allow the subsequent charge to be ignited after the one ahead of it was ignited. It was neither a very reliable nor popular firearm, but it enabled a form of "automatic" fire long before the advent of the machine gun.
Loading techniques.
Most early firearms were muzzle-loading. This form of loading has several disadvantages, such as a slow rate of fire and having to expose oneself to enemy fire to reload as the weapon had to be pointed upright so the powder could be poured through the muzzle into the breech followed by the ramming the projectile into the breech. As effective methods of sealing the breech were developed through the development of sturdy, weatherproof, self-contained metallic cartridges, muzzle-loaders were replaced by single-shot breech loaders. Eventually single-shot weapons were replaced by the following repeater type weapons.
Internal magazines.
Many firearms made in the late 19th century through the 1950s used internal magazines to load the cartridge into the chamber of the weapon. The most notable and revolutionary weapons of this period appeared during the U.S. Civil War and they were the Spencer and Henry repeating rifles. Both used fixed tubular magazines, the former having the magazine in the buttstock and the latter under the barrel which allowed a larger capacity. Later weapons used fixed box magazines that could not be removed from the weapon without dissembling the weapon itself. Fixed magazines permitted the use of larger cartridges and eliminated the hazard of having the bullet of one cartridge butting next to the primer or rim of another cartridge. These magazines are loaded while they are in the weapon, often using a stripper clip. A clip is used to transfer cartridges into the magazine. Some notable weapons that use internal magazines include the Mosin–Nagant, the Mauser Kar 98k, the Springfield M1903, the M1 Garand, and the SKS. Firearms that have internal magazines are usually, but not always, rifles. Some exceptions to this include the Mauser C96 pistol, which uses an internal magazine, and the Breda 30, an Italian light machine gun.
Detachable magazines.
Many modern firearms use what are called detachable or box magazines as their method of chambering a cartridge. Detachable magazines can be removed from the weapon without disassembling the firearms, usually by pushing the magazine release. Some notable weapons that use detachable magazines include the AK-47, the M14, the AR-15, and the Glock 17.
Belt-fed weapons.
A belt or ammunition belt is a device used to retain and feed cartridges into a firearm commonly used on machine guns. Belts were originally composed of canvas or cloth with pockets spaced evenly to allow the belt to be mechanically fed into the gun. These designs were prone to malfunctions due to the effects of oil and other contaminants altering the belt. Later belt designs used permanently connected metal links to retain the cartridges during feeding. These belts were more tolerant to exposure to solvents and oil. Some notable weapons that use belts are the M240, the M249, the M134 Minigun, and the PK Machine Gun.
Firing mechanisms.
Matchlock.
Matchlocks were the first and simplest firearms firing mechanisms developed. Using the matchlock mechanism, the powder in the gun barrel was ignited by a piece of burning cord called a "match". The match was wedged into one end of an S-shaped piece of steel. As the trigger (often actually a lever) was pulled, the match was brought into the open end of a "touch hole" at the base of the gun barrel, which contained a very small quantity of gunpowder, igniting the main charge of gunpowder in the gun barrel. The match usually had to be relit after each firing. The main parts to the matchlock firing mechanism are the pan, match, arm and trigger. A benefit of the pan and arm swivel being moved to the side of the gun was it gave a clear line of fire. An advantage to the matchlock firing mechanism is that it did not misfire. However, it also came with some disadvantages. One disadvantage was if it was raining the match could not be kept lit to fire the weapon. Another issue with the match was it could give away the position of soldiers because of the glow, sound, and smell.
Wheellock.
The wheellock action, a successor to the matchlock, predated the flintlock. Despite its many faults, the wheellock was a significant improvement over the matchlock in terms of both convenience and safety, since it eliminated the need to keep a smoldering match in proximity to loose gunpowder. It operated using a small wheel much like that on cigarette lighters which was wound up with a key before use and which, when the trigger was pulled, spun against a flint, creating the shower of sparks that ignited the powder in the touch hole. Supposedly invented by Leonardo da Vinci, the Italian Renaissance man, the wheellock action was an innovation that was not widely adopted due to the high cost of the clockwork mechanism.
Flintlock.
The flintlock action was a major innovation in firearm design. The spark used to ignite the gunpowder in the touch hole was supplied by a sharpened piece of flint clamped in the jaws of a "cock" which, when released by the trigger, struck a piece of steel called the "frizzen" to create the necessary sparks. (The spring-loaded arm that holds a piece of flint or pyrite is referred to as a cock because of its resemblance to a rooster.) The cock had to be manually reset after each firing, and the flint had to be replaced periodically due to wear from striking the frizzen. (See also flintlock mechanism, snaphance, Miquelet lock) The flintlock was widely used during the 18th and 19th centuries in both muskets and rifles.
Percussion cap.
Percussion caps (caplock mechanisms), coming into wide service in the 19th century, were a dramatic improvement over flintlocks. With the percussion cap mechanism, the small primer charge of gunpowder used in all preceding firearms was replaced by a completely self-contained explosive charge contained in a small brass "cap". The cap was fastened to the touch hole of the gun (extended to form a "nipple") and ignited by the impact of the gun's "hammer". (The hammer is roughly the same as the cock found on flintlocks except that it doesn't clamp onto anything.) In the case of percussion caps the hammer was hollow on the end to fit around the cap in order to keep the cap from fragmenting and injuring the shooter.
Once struck, the flame from the cap in turn ignited the main charge of gunpowder, as with the flintlock, but there was no longer any need to charge the touch hole with gunpowder, and even better, the touch hole was no longer exposed to the elements. As a result, the percussion cap mechanism was considerably safer, far more weatherproof, and vastly more reliable (cloth-bound cartridges containing a premeasured charge of gunpowder and a ball had been in regular military service for many years, but the exposed gunpowder in the entry to the touch hole had long been a source of misfires). All muzzleloaders manufactured since the second half of the 19th century use percussion caps except those built as replicas of the flintlock or earlier firearms.
Cartridges.
A major innovation in firearms and light artillery came in the second half of the 19th century when ammunition, previously delivered as separate bullets and powder, was combined in a single metallic (usually brass) cartridge containing a percussion cap, powder, and a bullet in one weatherproof package. The main technical advantage of the brass cartridge case was the effective and reliable sealing of high pressure gasses at the breech, as the gas pressure forces the cartridge case to expand outward, pressing it firmly against the inside of the gun barrel chamber. This prevents the leakage of hot gas which could injure the shooter. The brass cartridge also opened the way for modern repeating arms, by uniting the bullet, gunpowder and primer into one assembly that could be fed reliably into the breech by a mechanical action in the firearm.
Before this, a "cartridge" was simply a premeasured quantity of gunpowder together with a ball in a small cloth bag (or rolled paper cylinder), which also acted as wadding for the charge and ball. This early form of cartridge had to be rammed into the muzzleloader's barrel, and either a small charge of gunpowder in the touch hole or an external percussion cap mounted on the touch hole ignited the gunpowder in the cartridge. Cartridges with built-in percussion caps (called "primers") continue to this day to be the standard in firearms. In cartridge-firing firearms, a hammer (or a firing pin struck by the hammer) strikes the cartridge primer, which then ignites the gunpowder within. The primer charge is at the base of the cartridge, either within the rim (a "rimfire" cartridge) or in a small percussion cap embedded in the center of the base (a "centerfire" cartridge). As a rule, centerfire cartridges are more powerful than rimfire cartridges, operating at considerably higher pressures than rimfire cartridges. Centerfire cartridges are also safer, as a dropped rimfire cartridge has the potential to discharge if its rim strikes the ground with sufficient force to ignite the primer. This is practically impossible with most centerfire cartridges.
Nearly all contemporary firearms load cartridges directly into their breech. Some additionally or exclusively load from a magazine that holds multiple cartridges. A magazine is defined as a part of the firearm which exists to store ammunition and assist in its feeding by the action into the breech (such as through the rotation of a revolver's cylinder or by spring-loaded platforms in most pistol and rifle designs). Some magazines, such as that of most centerfire hunting rifles and all revolvers, are internal to and inseparable from the firearm, and are loaded by using a "clip". A clip, often mistakingly used to refer to a detachable "magazine", is a device that holds the ammunition by the rim of the case and is designed to assist the shooter in reloading the firearm's magazine. Examples include revolver speedloaders, the stripper clip used to aid loading rifles such as the Lee–Enfield or Mauser 98, and the en-bloc clip used in loading the M1 Garand. In this sense, "magazines" and "clips", though often used synonymously, refer to different types of devices.
Repeating, semi-automatic, and automatic firearms.
Many firearms are "single shot": i.e., each time a cartridge is fired, the operator must manually re-cock the firearm and load another cartridge. The classic single-barreled shotgun is a good example. A firearm that can load multiple cartridges as the firearm is re-cocked is considered a "repeating firearm" or simply a "repeater". A lever-action rifle, a pump-action shotgun, and most bolt-action rifles are good examples of repeating firearms. A firearm that automatically re-cocks and reloads the next round with each trigger pull is considered a semi-automatic or autoloading firearm.
The first "rapid firing" firearms were usually similar to the 19th century Gatling gun, which would fire cartridges from a magazine as fast as and as long as the operator turned a crank. Eventually, the "rapid" firing mechanism was perfected and miniaturized to the extent that either the recoil of the firearm or the gas pressure from firing could be used to operate it, thus the operator needed only to pull a trigger (which made the firing mechanisms truly "automatic"). An automatic (or "fully automatic") firearm is one that automatically re-cocks, reloads, and fires as long as the trigger is depressed. An automatic firearm is capable of firing multiple rounds with one pull of the trigger. The Gatling gun may have been the first automatic weapon, though the modern trigger-actuated machine gun was not widely introduced until the First World War with the German "Spandau" and British Lewis Gun. Automatic rifles such as the Browning Automatic Rifle were in common use by the military during the early part of the 20th century, and automatic rifles that fired handgun rounds, known as submachine guns, also appeared in this time. Many modern military firearms have a selective fire option, which is a mechanical switch that allows the firearm be fired either in the semi-automatic or fully automatic mode. In the current M16A2 and M16A4 variants of the U.S.-made M16, continuous fully automatic fire is not possible, having been replaced by an automatic burst of three cartridges (this conserves ammunition and increases controllability).
Automatic weapons are largely restricted to military and paramilitary organizations, though many automatic designs are infamous for their use by civilians. Automatic firearms have long been available to U.S. civilians, under increasingly restrictive conditions. Importation of machine guns for civilian sale in the U.S. was banned by the Gun Control Act of 1968. The now prohibits United States civilian ownership or transfer of automatic weapons unless they were registered before May 19, 1986. Non-prohibited automatic weapons can be legally owned by civilians who pay a $200 tax to the Bureau of Alcohol, Tobacco, Firearms and Explosives (BATFE), pass a background investigation, and, in some jurisdictions, receive approval from local law enforcement. Permission must be received directly from the BATFE to move a machine gun between states, even if it does not change ownership. An extremely limited number of U.S. citizens have special permits from the BATFE to buy, and even import, automatic weapons produced and registered after 1986. The use of such weapons is tightly restricted to the film industry under direct supervision of the master of arms holding the permit, and the weapons are often altered so they will not fire "factory" ammunition, but rather only special "light-primer" blank cartridges produced specifically for the film industry. This arrangement allows weapons first produced after 1986 to be used by actors in films and T.V. series produced inside the U.S

</doc>
<doc id="11968" url="https://en.wikipedia.org/wiki?curid=11968" title="George Washington">
George Washington

George Washington ( – , 1799) was the first President of the United States (1789–97), the Commander-in-Chief of the Continental Army during the American Revolutionary War, and one of the Founding Fathers of the United States. He presided over the convention that drafted the current United States Constitution and during his lifetime was called the "father of his country".
Widely admired for his strong leadership qualities, Washington was unanimously elected president in the first two national elections. He oversaw the creation of a strong, well-financed national government that maintained neutrality in the French Revolutionary Wars, suppressed the Whiskey Rebellion, and won acceptance among Americans of all types. Washington's incumbency established many precedents, still in use today, such as the cabinet system, the inaugural address, and the title Mr. President. His retirement from office after two terms established a tradition that lasted until 1940, when Franklin Delano Roosevelt won an unprecedented third term. The 22nd Amendment (1951) now limits the president to two elected terms.
Born into the provincial gentry of Colonial Virginia, his family were wealthy planters who owned tobacco plantations and slaves which he inherited. He owned hundreds of slaves throughout his lifetime, but his views on slavery evolved to support abolition. In his youth he became a senior British officer in the colonial militia during the first stages of the French and Indian War. In 1775 the Second Continental Congress commissioned Washington as commander-in-chief of the Continental Army in the American Revolution. In that command, Washington forced the British out of Boston in 1776, but was defeated and nearly captured later that year when he lost New York City. After crossing the Delaware River in the middle of winter, he defeated the British in two battles (Trenton and Princeton), retook New Jersey and restored momentum to the Patriot cause.
His strategy enabled Continental forces to capture two major British armies at Saratoga in 1777 and Yorktown in 1781. Historians laud Washington for the selection and supervision of his generals, preservation and command of the army, coordination with the Congress, with state governors and their militia, and attention to supplies, logistics, and training. In battle, however, Washington was repeatedly outmaneuvered by British generals with larger armies. After victory had been finalized in 1783, Washington resigned as commander-in-chief rather than seize power, proving his opposition to dictatorship and his commitment to American republicanism.
Washington presided over the Constitutional Convention in 1787, which devised a new form of federal government for the United States. Following unanimous election as president in 1789, he worked to unify rival factions in the fledgling nation. He supported Alexander Hamilton's programs to satisfy all debts, federal and state, established a permanent seat of government, implemented an effective tax system, and created a national bank. In avoiding war with Great Britain, he guaranteed a decade of peace and profitable trade by securing the Jay Treaty in 1795, despite intense opposition from the Jeffersonians. Although he remained nonpartisan, never joining the Federalist Party, he largely supported its policies. Washington's Farewell Address was an influential primer on civic virtue, warning against partisanship, sectionalism, and involvement in foreign wars. He retired from the presidency in 1797, returning to his home and plantation at Mount Vernon.
While in power, his use of national authority pursued many ends, especially the preservation of liberty, reduction of regional tensions, and promotion of a spirit of American nationalism. Upon his death, Washington was eulogized as "first in war, first in peace, and first in the hearts of his countrymen" by Henry Lee. Revered in life and in death, scholarly and public polling consistently ranks him among the top three presidents in American history; he has been depicted and remembered in monuments, currency, and other dedications to the present day.
Early life (1732–1753).
The first child of Augustine Washington (1694–1743) and his second wife, Mary Ball Washington (1708–1789), George Washington was born on their Pope's Creek Estate near present-day Colonial Beach in Westmoreland County, Virginia. According to the Julian calendar and Annunciation Style of enumerating years (then in use in the British Empire), Washington was born on February 11, 1731; the Gregorian calendar, adopted later within the British Empire in 1752, renders a birth date of February 22, 1732.
Washington was of primarily English gentry descent, especially from Sulgrave, England. His great-grandfather, John Washington, emigrated to Virginia in 1656 and began accumulating land and slaves, as did his son Lawrence and his grandson, George's father, Augustine. Augustine was a tobacco planter who also tried his hand in iron-mining ventures. In George's youth, the Washingtons were moderately prosperous members of the Virginia gentry, of "middling rank" rather than one of the leading planter families. At this time, Virginia and other southern colonies had become a slave society, in which slaveholders formed the ruling class and the economy was based upon slave labor.
Six of George's siblings reached maturity, including two older half-brothers, Lawrence and Augustine, from his father's first marriage to Jane Butler Washington, and four full siblings, Samuel, Elizabeth (Betty), John Augustine and Charles. Three siblings died before adulthood: his full sister Mildred died when she was about one, his half-brother Butler died in infancy, and his half-sister Jane died aged of twelve, when George was about two. His father died of a sudden illness in April 1743 when George was eleven years old, and his half-brother Lawrence became a surrogate father and role model. William Fairfax, Lawrence's father-in-law and cousin of Virginia's largest landowner, Thomas, Lord Fairfax, was also a formative influence.
Washington spent much of his boyhood at Ferry Farm in Stafford County near Fredericksburg. Lawrence Washington inherited another family property from his father, a plantation on the Potomac River at Little Hunting Creek, which he named Mount Vernon, in honor of his commanding officer, Admiral Edward Vernon. George inherited Ferry Farm upon his father's death and eventually acquired Mount Vernon after Lawrence's death.
The death of his father prevented Washington from an education at England's Appleby School, as his older brothers had received. He achieved the equivalent of an elementary school education from a variety of tutors, as well as from school run by an Anglican clergyman in or near Fredericksburg. Talk of securing an appointment in the Royal Navy for him when he was 15 was dropped when his widowed mother objected. At the age of 17, in 1749, Washington would receive his surveyor's license from the College of William & Mary. Thanks to Lawrence's connection to the powerful Fairfax family, Washington was appointed official surveyor for Culpeper County, a well-paid position which enabled him to purchase land in the Shenandoah Valley, the first of his many land acquisitions in western Virginia. Thanks also to Lawrence's involvement in the Ohio Company, a land investment company funded by Virginia investors, and Lawrence's position as commander of the Virginia militia, Washington came to the notice of the new lieutenant governor of Virginia, Robert Dinwiddie. Washington was hard to miss: At exactly six feet, he towered over most of his contemporaries.
In 1751 Washington traveled to Barbados with Lawrence, who was suffering from tuberculosis, with the hope that the climate would be beneficial to Lawrence's health. Washington contracted smallpox during the trip, which left his face slightly scarred, but immunized him against future exposures to the dreaded disease. However, Lawrence's health failed to improve, and he returned to Mount Vernon, where he would die in the summer of 1752. Lawrence's position as Adjutant General (militia leader) of Virginia was divided into four district offices after his death. Washington was appointed by Governor Dinwiddie as one of the four district adjutants in February 1753, with the rank of major in the Virginia militia. During this period, Washington became a Freemason while in Fredericksburg, although his involvement was minimal.
French and Indian War (or 'Seven Years' War', 1754–1758).
The Ohio Company was an important vehicle through which British investors planned to expand into the Ohio Valley, opening new settlements and trading posts for the Indian trade. In 1753 the French themselves began expanding their military control into the Ohio Country, a territory already claimed by the British colonies of Virginia and Pennsylvania. These competing claims led to a war in the colonies called the French and Indian War (1754–62), and contributed to the start of the global Seven Years' War (1756–63). By chance, Washington became involved in its beginning.
Robert Dinwiddie, lieutenant governor of colonial Virginia, was ordered by the British government to guard the British territorial claims including the Ohio River basin. In late 1753 Dinwiddie ordered Washington to deliver a letter asking the French to vacate the Ohio Valley; he was eager to prove himself as the new adjutant general of the militia, appointed by the Lieutenant Governor himself only a year before. During his trip Washington met with Tanacharison (also called "Half-King") and other Iroquois chiefs allied with England at Logstown to secure their support in case of a military conflict with the French—indeed Washington and Tanacharison became friends. He delivered the letter to the local French commander Jacques Legardeur de Saint-Pierre, who politely refused to leave. Washington kept a diary during his expedition which was printed by William Hunter on Dinwiddie's order and which made Washington's name recognizable in Virginia. This increased notoriety helped him to obtain a commission to raise a company of 100 men and start his military career.
Dinwiddie sent Washington back to the Ohio Country to safeguard an Ohio Company's construction of a fort at present-day Pittsburgh, Pennsylvania. However, before he reached the area, a French force drove out colonial traders and began construction of Fort Duquesne. A small detachment of French troops led by Joseph Coulon de Jumonville, was discovered by Tanacharison and a few warriors east of present-day Uniontown, Pennsylvania. On May 28, 1754, Washington and some of his militia unit, aided by their Mingo allies, ambushed the French in what has come to be called the Battle of Jumonville Glen. Exactly what happened during and after the battle is a matter of contention, but several primary accounts agree that the battle lasted about 15 minutes, that Jumonville was killed, and that most of his party were either killed or taken prisoner. Whether Jumonville died at the hands of Tanacharison in cold blood or was somehow shot by an onlooker with a musket as he sat with Washington or by another means, is not completely clear. He was given the epithet Town Destroyer by Tanacharison.
The French responded by attacking and capturing Washington at Fort Necessity in July 1754. However, he was allowed to return with his troops to Virginia. Historian Joseph Ellis concludes that the episode demonstrated Washington's bravery, initiative, inexperience and impetuosity. These events had international consequences; the French accused Washington of assassinating Jumonville, who they claimed was on a diplomatic mission. Both France and Great Britain were ready to fight for control of the region and both sent troops to North America in 1755; war was formally declared in 1756.
Braddock disaster 1755.
In 1755 Washington became the senior American aide to British General Edward Braddock on the ill-fated Braddock expedition. This was the largest British expedition to the colonies, and was intended to expel the French from the Ohio Country; the first objective was the capture of Fort Duquesne. Washington initially sought an appointment as a major from Braddock, but upon advice that no rank above captain could be given except by London, he agreed to serve as a staff volunteer. During the passage of the expedition, Washington fell ill with severe headaches and fever; nevertheless, when the pace of the troops continued to slow, Washington recommended to Braddock that the army be split into two divisions – a primary and more lightly, but adequately equipped, "flying column" offensive which could move at a more rapid pace, to be followed by a more heavily armed reinforcing division. Braddock accepted the recommendation (likely made in a council of war including other officers) and took command of the lead division.
In the Battle of the Monongahela the French and their Indian allies ambushed Braddock's reduced forces and the general was mortally wounded. After suffering devastating casualties, the British panicked and retreated in disarray; however, Washington rode back and forth across the battlefield, rallying the remnants of the British and Virginian forces into an organized retreat. In the process, despite his lingering illness, he demonstrated much bravery and stamina—he had two horses shot from underneath him, while his coat was pierced with four bullets. In his report, Washington chiefly blamed the disaster on the conduct of the redcoats while praising that of the Virginia contingent. Whatever responsibility rested on him for the defeat as a result of his recommendation to Braddock, Washington was not included by the succeeding commander, Col. Thomas Dunbar, in planning subsequent force movements.
Commander of Virginia Regiment.
Lt. Governor Dinwiddie rewarded Washington in 1755 with a commission as "Colonel of the Virginia Regiment and Commander in Chief of all forces now raised in the defense of His Majesty's Colony" and gave him the task of defending Virginia's frontier. The Virginia Regiment was the first full-time American military unit in the colonies (as opposed to part-time militias and the British regular units). Washington was ordered to "act defensively or offensively" as he thought best. While Washington happily accepted the commission, the coveted redcoat of a British officer as well as the accompanying pay continued to elude him. Dinwiddie as well pressed in vain for the British military to incorporate the Virginia regiment into its ranks.
In command of a thousand soldiers, Washington was a disciplinarian who emphasized training. He led his men in brutal campaigns against the Indians in the west; in 10 months his regiment fought 20 battles, and lost a third of its men. Washington's strenuous efforts meant that Virginia's frontier population suffered less than that of other colonies; Ellis concludes "it was his only unqualified success" in the war.
In 1758 Washington participated in the Forbes Expedition to capture Fort Duquesne. He was embarrassed by a friendly fire episode in which his unit and another British unit thought the other was the French enemy and opened fire, with 14 dead and 26 wounded in the mishap. Washington was not involved in any other major fighting on the expedition, and the British scored a major strategic victory, gaining control of the Ohio Valley, when the French abandoned the fort. Following the expedition, he retired from his Virginia Regiment commission in December 1758. Washington did not return to military life until the outbreak of the revolution in 1775.
Lessons learned.
Although Washington never gained the commission in the British army he yearned for, in these years the young man gained valuable military, political, and leadership skills. He closely observed British military tactics, gaining a keen insight into their strengths and weaknesses that proved invaluable during the Revolution. Washington learned to organize, train, drill, and discipline his companies and regiments. From his observations, readings and conversations with professional officers, he learned the basics of battlefield tactics, as well as a good understanding of problems of organization and logistics. He gained an understanding of overall strategy, especially in locating strategic geographical points.
He demonstrated his toughness and courage in the most difficult situations, including disasters and retreats. He developed a command presence—given his size, strength, stamina, and bravery in battle, he appeared to soldiers to be a natural leader and they followed him without question. However Washington's fortitude in his early years was sometimes manifested in less constructive ways. Biographer John R. Alden contends Washington offered "fulsome and insincere flattery to British generals in vain attempts to win great favor" and on occasion showed youthful arrogance, as well as jealousy and ingratitude in the midst of impatience.
Historian Ron Chernow is of the opinion that his frustrations in dealing with government officials during this conflict led him to advocate the advantages of a strong national government and a vigorous executive agency that could get results; other historians tend to ascribe Washington's position on government to his later American Revolutionary War service. He developed a very negative idea of the value of militia, who seemed too unreliable, too undisciplined, and too short-term compared to regulars. On the other hand, his experience was limited to command of at most 1000 men, and came only in remote frontier conditions that were far removed from the urban situations he faced during the Revolution at Boston, New York, Trenton and Philadelphia.
Between the wars: Mount Vernon (1759–1774).
On January 6, 1759, Washington married the wealthy widow Martha Dandridge Custis, then 28 years old. Surviving letters suggest that he may have been in love at the time with Sally Fairfax, the wife of a friend. Nevertheless, George and Martha made a compatible marriage, because Martha was intelligent, gracious, and experienced in managing a planter's estate.
Together the two raised her two children from her previous marriage, John Parke Custis and Martha Parke Custis; later the Washingtons raised two of Mrs. Washington's grandchildren, Eleanor Parke Custis and George Washington Parke Custis. George and Martha never had any children together—his earlier bout with smallpox in 1751 may have made him sterile. The newlywed couple moved to Mount Vernon, near Alexandria, where he took up the life of a planter and political figure.
Washington's marriage to Martha greatly increased his property holdings and social standing, and made him one of Virginia's wealthiest men. He acquired one-third of the Custis estate upon his marriage, worth approximately $100,000, and managed the remainder on behalf of Martha's children, for whom he sincerely cared.
In 1754 Lieutenant Governor Dinwiddie had promised land bounties to the soldiers and officers who volunteered to serve during the French and Indian War. Lord Botetourt, the new governor, finally fulfilled Dinwiddie's promise in 1769–1770, with Washington subsequently receiving title to where the Kanawha River flows into the Ohio River, in what is now western West Virginia. He also frequently bought additional land in his own name. By 1775 Washington had doubled the size of Mount Vernon to , and had increased its slave population to over 100. As a respected military hero and large landowner, he held local office and was elected to the Virginia provincial legislature, representing Frederick County in the House of Burgesses for seven years, beginning in 1758.
Washington lived an aristocratic lifestyle—fox hunting was a favorite leisure activity. He also enjoyed going to dances and parties, in addition to the theater, races, and cockfights. Washington also was known to play cards, backgammon, and billiards. Like most Virginia planters, he imported luxuries and other goods from England and paid for them by exporting his tobacco crop.
Washington began to pull himself out of debt in the mid-1760s by diversifying his previously tobacco-centric business interests into other ventures and paying more attention to his affairs. In 1766 he started switching Mount Vernon's primary cash crop away from tobacco to wheat, a crop that could be processed and then sold in various forms in the colonies, and further diversified operations to include flour milling, fishing, horse breeding, spinning, weaving and (in the 1790s) whiskey production. Patsy Custis's death in 1773 from epilepsy enabled Washington to pay off his British creditors, since half of her inheritance passed to him.
A successful planter, he was a leader in the social elite in Virginia. From 1768 to 1775, he invited some 2000 guests to his Mount Vernon estate, mostly those he considered "people of rank". As for people not of high social status, his advice was to "treat them civilly" but "keep them at a proper distance, for they will grow upon familiarity, in proportion as you sink in authority". In 1769 he became more politically active, presenting the Virginia Assembly with legislation to ban the importation of goods from Great Britain.
American Revolution (1775–1783).
Washington opposed the 1765 Stamp Act, the first direct tax on the colonies imposed by the English Parliament which included no representatives from the colonies; he began taking a leading role in the growing colonial resistance when protests against the Townshend Acts (enacted in 1767) became widespread. In May 1769 Washington introduced a proposal, drafted by his friend George Mason, calling for Virginia to boycott English goods until the Acts were repealed. Parliament repealed the Townshend Acts in 1770. However, Washington regarded the passage of the Intolerable Acts in 1774 as "an Invasion of our Rights and Privileges". Washington told friend Bryan Fairfax, "I think the Parliament of Great Britain has no more right to put their hands in my pocket without my consent than I have to put my hands into yours for money." He also said that Americans must not submit to acts of tyranny "till custom and use shall make us as tame and abject slaves, as the blacks we rule over with such arbitrary sway."
In July 1774 he chaired the meeting at which the "Fairfax Resolves" were adopted, which called for the convening of a Continental Congress, among other things. In August, Washington attended the First Virginia Convention, where he was selected as a delegate to the First Continental Congress.
Commander in chief.
After the Battles of Lexington and Concord near Boston in April 1775, the colonies went to war. Washington appeared at the Second Continental Congress in a military uniform, signaling that he was prepared for war. Washington had the prestige, military experience, charisma and military bearing of a military leader and was known as a strong patriot. Virginia, the largest colony, deserved recognition, and New England—where the fighting began—realized it needed Southern support. Washington did not explicitly seek the office of commander and said that he was not equal to it, but there was no serious competition. Congress created the Continental Army on June 14, 1775. Nominated by John Adams of Massachusetts, Washington was then appointed as a full General and Commander-in-chief. The British then articulated the peril of Washington and his army—on August 23, 1775 Britain issued a Royal proclamation labeling American rebels as traitors; if they resorted to force, they faced confiscation of their property. Their leaders were subject to execution upon the scaffold.
General Washington essentially assumed three roles during the war. First, in 1775–77, and again in 1781 he provided leadership of troops against the main British forces. Although he lost many of his battles, he never surrendered his army during the war, and he continued to fight the British relentlessly until the war's end. He plotted the overall strategy of the war, in cooperation with Congress.
Secondly, he was charged with organizing and training the army. He recruited regulars and assigned Baron von Steuben, a veteran of the Prussian general staff, to train them. The war effort and getting supplies to the troops were under the purview of Congress, but Washington pressured the Congress to provide the essentials. In June 1776 Congress' first attempt at running the war effort was established with the committee known as "Board of War and Ordnance", succeeded by the Board of War in July 1777, a committee which eventually included members of the military. The command structure of the armed forces was a hodgepodge of Congressional appointees (and Congress sometimes made those appointments without Washington's input) with state-appointments filling the lower ranks and of all of the militia-officers. The results of his general staff were mixed, as some of his favorites (like John Sullivan) never mastered the art of command.
Eventually, he found capable officers, such as General Nathanael Greene, General Daniel Morgan—"the old wagoner" that he had served with in The French and Indian War, Colonel Henry Knox—chief of artillery, and Colonel Alexander Hamilton—chief-of-staff. The American officers never equaled their opponents in tactics and maneuver, and consequently they lost most of the pitched battles. The great successes, at Boston (1776), Saratoga (1777) and Yorktown (1781), came from trapping the British far from base with much larger numbers of troops. Daniel Morgan's annihilation of Banastre Tarleton's legion of dragoons at Cowpens in February 1781, came as a result of Morgan's employment of superior line tactics against his British opponent, resulting in one of the very few double envelopments in military history, another being Hannibal's defeat of the Romans at Cannae in 216 b.c. The decisive defeat of Col. Patrick Ferguson's Tory Regiment at King's Mountain demonstrated the superiority of the riflery of American "over mountain men" over British-trained troops armed with musket and bayonet. These "over-mountain men" were led by a variety of elected officers, including the 6'6" William Campbell who had become one of Washington's officers by the time of Yorktown. Similarly, Morgan's Virginia riflemen proved themselves superior to the British at Saratoga, a post-revolutionary war development being the creation of trained "rifle battalions" in the European armies.
Washington's third, and most important role in the war effort, was the embodiment of armed resistance to the Crown—the representative man of the Revolution. His long-term strategy was to maintain an army in the field at all times, and eventually this strategy worked. His enormous personal and political stature and his political skills kept Congress, the army, the French, the militias, and the states all pointed toward a common goal. Furthermore, by voluntarily resigning his commission and disbanding his army when the war was won (rather than declaring himself monarch), he permanently established the principle of civilian supremacy in military affairs. Yet his constant reiteration of the point that well-disciplined professional soldiers counted for twice as much as erratic militias (clearly demonstrated in the rout at Camden, where only the Maryland and Delaware Continentals under Baron DeKalb held firm), helped overcome the ideological distrust of a standing army.
Victory at Boston.
Washington assumed command of the Continental Army in the field at Cambridge, Massachusetts, in July 1775, during the ongoing siege of Boston. Realizing his army's desperate shortage of gunpowder, Washington asked for new sources. American troops raided British arsenals, including some in the Caribbean, and some manufacturing was attempted. They obtained a barely adequate supply (about 2.5 million pounds) by the end of 1776, mostly from France.
Washington reorganized the army during the long standoff, and forced the British to withdraw by putting artillery on Dorchester Heights overlooking the city. The British evacuated Boston in March 1776 and Washington moved his army to New York City.
Although highly disparaging toward most of the Patriots, British newspapers routinely praised Washington's personal character and qualities as a military commander. These articles were bold, as Washington was an enemy general who commanded an army in a cause that many Britons believed would ruin the empire.
Defeat at New York City and Fabian tactics.
In August 1776 British General William Howe launched a massive naval and land campaign designed to seize New York. The Continental Army under Washington engaged the enemy for the first time as an army of the newly independent United States at the Battle of Long Island, the largest battle of the entire war. The Americans were heavily outnumbered, many men deserted, and Washington was badly beaten. Subsequently, Washington was forced to retreat across the East River at night. He did so without loss of life or materiel. Washington, heeding Greene's recommendation to attempt a defense of Ft. Washington, belatedly retreated further across the Hudson to Ft. Lee, to avoid encirclement, but thereby enabled Howe to take the offensive and capture Fort Washington on November 16 with high Continental casualties. Biographer Alden opines that "although Washington was responsible for the decision to delay the patriots' retreat, he tried to ascribe blame for the decision to defend Fort Washington to the wishes of Congress and the bad advice of Nathaniel Greene."
Washington then continued his flight across New Jersey; the future of the Continental Army was in doubt due to expiring enlistments and the string of losses. On the night of December 25, 1776, Washington staged a comeback with a surprise attack on a Hessian outpost in western New Jersey. He led his army across the Delaware River to capture nearly 1,000 Hessians in Trenton, New Jersey. Washington followed up his victory at Trenton with another over British regulars at Princeton in early January. The British retreated to New York City and its environs, which they held until the peace treaty of 1783. Washington's victories wrecked the British carrot-and-stick strategy of showing overwhelming force then offering generous terms. The Americans would not negotiate for anything short of independence. These victories alone were not enough to ensure ultimate Patriot victory, however, since many soldiers did not reenlist or deserted during the harsh winter. Washington and Congress reorganized the army with increased rewards for staying and punishment for desertion, which raised troop numbers effectively for subsequent battles.
In February 1777 while encamped at Morristown, New Jersey Washington became convinced that only smallpox inoculation would prevent the destruction of his Army, by using variolation. Washington ordered the inoculation of all troops and by some reports, death by smallpox in the ranks dropped from 17% of all deaths to 1% of all reported deaths.
Historians debate whether or not Washington preferred a Fabian strategy to harass the British with quick, sharp attacks followed by a retreat so the larger British army could not catch him, or whether he preferred to fight major battles. While his southern commander Greene in 1780–81 did use Fabian tactics, Washington did so only in fall 1776 to spring 1777, after losing New York City and seeing much of his army melt away. Trenton and Princeton were Fabian examples. By summer 1777, however, Washington had rebuilt his strength and his confidence; he stopped using raids and went for large-scale confrontations, as at Brandywine, Germantown, Monmouth and Yorktown.
1777 campaigns.
In late summer of 1777, British General John Burgoyne led a major invasion army south from Quebec, with the intention of splitting off rebellious New England; but General Howe in New York took his army south to Philadelphia instead of going up the Hudson River to join with Burgoyne near Albany—a major strategic mistake. Meanwhile, Washington rushed to Philadelphia to engage Howe, while closely following the action in upstate New York, where the patriots were led by General Philip Schuyler and his successor Horatio Gates. The ensuing pitched battles at Philadelphia were too complex for Washington's relatively inexperienced men and they were defeated. At the Battle of Brandywine on September 11, 1777, Howe outmaneuvered Washington, and marched into the American capital at Philadelphia unopposed on September 26. Washington's army unsuccessfully attacked the British garrison at Germantown in early October. Meanwhile to the north, Burgoyne, beyond the reach of help from Howe, was trapped and forced to surrender after the Battles of Saratoga. This was a major turning point militarily and diplomatically—the French responded to Burgoyne's defeat by entering the war, allying with America and expanding the Revolutionary War into a major worldwide affair.
Washington's loss at Philadelphia prompted some members of Congress to consider removing Washington from command. This movement, termed the Conway Cabal, failed after Washington's supporters rallied behind him. Biographer Alden relates, "it was inevitable that the defeats of Washington's forces and the concurrent victory of the forces in upper New York should be compared." The zealous admiration of Washington indeed inevitably waned. John Adams (never a fan of the southern delegation to the Continental Congress) wrote "Congress will appoint a thanksgiving; and one cause of it ought to be that the glory of turning the tide of arms is not immediately due to the commander-in-chief nor to southern troops. If it had been, idolatry and adulation would have been unbounded...Now we can allow a certain citizen to be wise, virtuous, and good, without thinking him a deity or a savior."
Valley Forge.
Washington's army of 11,000 went into winter quarters at Valley Forge north of Philadelphia in December 1777. Over the next six months, the deaths in camp numbered in the thousands (the majority being from disease), with historians' death toll estimates ranging from 2,000 to 2,500, to over 3,000 men. The next spring, however, the army emerged from Valley Forge in good order, thanks in part to a full-scale training program supervised by General von Steuben. The British evacuated Philadelphia to New York in 1778, shadowed by Washington. Washington attacked them at Monmouth, fighting to an effective draw in one of the war's largest battles. Afterwards, the British continued to head towards New York, and Washington moved his army outside of New York.
Sullivan Expedition.
In the summer of 1779 Washington and Congress decided to strike the Iroquois warriors of the "Six Nations" in a campaign to force Britain's Indian allies out of New York, which they had used as a base to attack American settlements across New England. In June 1779 the warriors had joined with Tory rangers led by Colonel William Butler, using barbarities normally shunned, slew over 200 frontiersmen and laid waste to the Wyoming Valley in Pennsylvania. Indeed, one British officer who witnessed the Tory brutality said the redcoats on return to England would "scalp every son of a bitch of them." In August of 1779 General John Sullivan led a military operation that destroyed at least 40 Iroquois villages, burned all available crops. Few people were killed as the Indians fled to British protection in Canada. Sullivan later reported that "the immediate objects of this expedition are accomplished, viz: total ruin of the Indian settlements and the destruction of their crops, which were designed for the support of those inhuman barbarians."
Hudson River and Southern battles.
Washington at this time moved his headquarters from Middlebrook to New Windsor on the Hudson, with an army of 10,000. The British, led by Howe's successor, Sir Henry Clinton made a move up the Hudson against American posts at Verplanck's Point and Stony Point and both places succumbed, but a counter-offensive by the patriots led by General Anthony Wayne was briefly successful. Clinton was in the end able to shut off Kings Ferry but it was a strategic loss; he could proceed no further up the river, due to American fortifications and Washington's army. The skirmishes at Verplanck's Point and at Stony Point demonstrated that the continental infantry had become quite formidable and were an enormous boost to morale.
The winter of 1779–1780, when Washington went into quarters at Morristown, represented the worst suffering for the army during the war. The temperatures fell to 16 below zero, the New York Harbor was frozen over, and snow and ice covered the ground for weeks, with the troops again lacking provisions for a time as at Valley Forge. In late 1779 Clinton moved his forces south to Charleston for an offensive against the patriots, led by Benjamin Lincoln. After his success there Clinton returned victorious to New York, leaving Cornwallis in the south. Congress replaced Lincoln with Gates, despite Washington's recommendation of Greene. When Gates failed in South Carolina, he was then replaced by Greene. The British at the time seemed to have the South almost in their grasp. Despite this news, Washington was encouraged to learn in mid-1780 that Lafayette had returned from France with additional naval assets and forces.
Treachery and mutiny.
Washington was shocked to learn of the treason of Benedict Arnold, who had contributed significantly to the war effort. Embittered by his dealings with Congress over rank and finances, as well as the alliance with France, Arnold joined the British cause; he conspired with the British in a plan to seize the post he commanded at West Point. Washington just missed apprehending him, but did capture his conspirator, Major John Andre, a British intelligence officer under Clinton, who was later hanged by order of a court-martial called by Washington.
Washington's army went into winter quarters at New Windsor in 1780 and suffered again for lack of supplies. There resulted a considerable mutiny by Pennsylvania troops; Washington prevailed upon Congress as well as state officials to come to their aid with provisions. He very much sympathized with their suffering, saying he hoped the army would not "continue to struggle under the same difficulties they have hitherto endured, which I cannot help remarking seem to reach the bounds of human patience".
Victory at Yorktown.
In July 1780, 5,000 veteran French troops led by the "comte" de Rochambeau arrived at Newport, Rhode Island to aid in the war effort; French naval forces then landed, led by Admiral François Joseph Paul de Grasse. Though it was Washington's hope initially to bring the allied fight to New York and to end the war there, de Grasse was advised by Rochambeau that Cornwallis in Virginia was the better target. de Grasse followed Rochambeau's advice and arrived off the Virginia Coast. Washington immediately saw the advantage created, made a feinting move with his force towards Clinton in New York and then headed south to Virginia.
Washington's Continental Army, also newly funded by $20,000 in French gold, delivered the final blow to the British in 1781, after a French naval victory allowed American and French forces to trap a British army in Virginia, preventing reinforcement by Clinton from the North. The surrender at Yorktown on October 19, 1781, marked the end of major fighting in continental North America. Cornwallis failed to appear at the official surrender ceremony, and sent General Charles Oharrow as his proxy; Washington then assigned his role to Benjamin Lincoln of equal rank.
Demobilization.
Though substantial combat had ended, the war had not, and a formal treaty of peace was months away, creating tension. The British still had 26,000 troops occupying New York City, Charleston and Savannah, together with a powerful fleet. The French army and navy departed, so the Americans were on their own in 1782–83. Money matters fed the anxiety—the treasury was empty, and the unpaid soldiers were growing restive, almost to the point of mutiny. At one point the mutineers forced an adjournment of the Congress from Philadelphia to Princeton. Washington dispelled unrest among officers by suppressing the Newburgh Conspiracy in March 1783, and Congress came up with the promise of a five-year bonus.
With the initial peace treaty articles ratified in April, a recently formed Congressional committee under Hamilton was considering needs and plans for a peacetime army. On May 2, 1783, the Commander in Chief submitted his "Sentiments on a Peace Establishment" to the Committee, essentially providing an official Continental Army position. The original proposal was defeated in Congress in two votes (May 1783, October 1783) with a truncated version also being rejected in April 1784.
By the Treaty of Paris (signed that September), Great Britain recognized the independence of the United States. Washington disbanded his army and, on November 2, gave an eloquent farewell address to his soldiers. On November 25, the British evacuated New York City, and Washington and the governor took possession. At Fraunces Tavern on December 4, Washington formally bade his officers farewell and on December 23, 1783, he resigned his commission as commander-in-chief, saying "I consider it an indispensable duty to close this last solemn act of my official life, by commending the interests of our dearest country to the protection of Almighty God, and those who have the superintendence of them, to his holy keeping." Historian Gordon Wood concludes that the greatest act in his life was his resignation as commander of the armies. King George III called Washington "the greatest character of the age" because of this.
Washington later submitted a formal account of the expenses he had personally advanced the army over the eight year conflict, of about $450,000. It is said to have been detailed regarding small items, vague concerning large ones and included the expenses incurred from Martha's visits to his headquarters, as well as his compensation for service, none of which had been drawn during the war.
Historian John Shy says that by 1783 Washington was "a mediocre military strategist but had become a master political tactician with an almost perfect sense of timing and a developed capacity to exploit his charismatic reputation, using people who thought they were using him".
Constitutional Convention.
Washington's retirement to personal business at Mount Vernon was short-lived. Making an exploratory trip to the western frontier in 1784, he inspected his land holdings in Western Pennsylvania that had been earned decades earlier for his service in the French and Indian War. There, he confronted squatters, including David Reed and the Covenanters, who vacated, but only after losing a court decision heard in Washington, Pennsylvania in 1786.
After much reluctance, he was persuaded to attend the Constitutional Convention in Philadelphia during the summer of 1787 as a delegate from Virginia, where he was elected in unanimity as president of the Convention. He held considerable criticism of the Articles of Confederation of the thirteen colonies, for the weak central government it established, referring to the Articles as no more than "a rope of sand" to support the new nation. His participation in the debates was minor, although he casted his vote when called upon; his prestige facilitated the collegiality and productivity of the delegates. After a couple of months into the task, Washington told Alexander Hamilton, "I almost despair of seeing a favorable issue to the proceedings of our convention and do therefore repent having had any agency in the business." In the end agreements were hatched however, and Washington thought the achievement monumental.
Following the Convention, his support convinced many, but not all of his colleagues, to vote for ratification. He unsuccessfully lobbied Patrick Henry, saying that "the adoption of it under the present circumstances of the union is in my opinion desirable;" he declared that the only alternative would be anarchy. Nevertheless, he did not consider it appropriate to cast his vote in favor of adoption for Virginia, since he was expected to be nominated president thereunder. The new Constitution was subsequently ratified by all thirteen states. The delegates to the convention designed the presidency with Washington in mind, allowing him to define the office by establishing precedent once elected.
Presidency (1789–1797).
The Electoral College unanimously elected Washington as the first president in 1789, and again 1792; He remains the only president to receive the totality of electoral votes. John Adams, who received the next highest vote total, was elected vice president. On April 30, 1789, Washington was inaugurated, taking the first presidential oath of office on the balcony of Federal Hall in New York City. The oath, as follows, was administered by Chancellor Robert R. Livingston: "I do solemnly swear that I will faithfully execute the Office of President of the United States, and will to the best of my ability, preserve, protect and defend the Constitution of the United States." Historian John R. Alden indicates that Washington added the words "So help me God."
The 1st United States Congress voted to pay Washington a salary of $25,000 a year—a large sum in 1789, valued at about $340,000 in 2015 dollars. Washington, despite facing financial troubles then, initially declined the salary, valuing his image as a selfless public servant. At the urging of Congress, however, he ultimately accepted the payment, to avoid setting a precedent whereby the presidency would be perceived as limited only to independently wealthy individuals who could serve without any salary. The president, aware that everything he did set a precedent, attended carefully to the pomp and ceremony of office, making sure that the titles and trappings were suitably republican and never emulated European royal courts. To that end, he preferred the title "Mr. President" to the more majestic names proposed by the Senate.
Washington proved an able administrator, and established many precedents in the functions of the presidency, including messages to Congress and the cabinet form of government. Despite fears that a democratic system would lead to political violence, he set the standard for tolerance of opposition voices and conducted a smooth transition of power to his successor. An excellent delegator and judge of talent and character, he talked regularly with department heads and listened to their advice before making a final decision. In handling routine tasks, he was "systematic, orderly, energetic, solicitous of the opinion of others ... but decisive, intent upon general goals and the consistency of particular actions with them." After reluctantly serving a second term, Washington refused to run for a third, establishing the tradition of a maximum of two terms for a president, which was solidified by Thomas Jefferson and James Madison.
Domestic issues.
Washington was not a member of any political party and hoped that they would not be formed, fearing conflict that would undermine republicanism. His closest advisors formed two factions, setting the framework for the future First Party System. Secretary of Treasury Alexander Hamilton had bold plans to establish the national credit and build a financially powerful nation, and formed the basis of the Federalist Party. Secretary of the State Thomas Jefferson, founder of the Jeffersonian Republicans, strenuously opposed Hamilton's agenda, but Washington typically favored Hamilton over Jefferson, and it was Hamilton's agenda that went into effect. Jefferson's political actions, his support of Philip Freneau's "National Gazette", and his attempt to undermine Hamilton, nearly led George Washington to dismiss Jefferson from his cabinet. Though Jefferson left the cabinet voluntarily, Washington never forgave him, and never spoke to him again.
In early 1790 Hamilton devised a plan with the approval of Washington, culminating in The Residence Act of 1790, that established the creditworthiness of the new government, as well as its permanent location. Congress had previously issued almost $22 million in certificates of debt during the war to suppliers; some of the states had incurred debt as well (more so in the north). In accordance with the plan, Congress authorized the "assumption" and payment of these debts, and provided funding through customs duties and excise taxes. The proposal was largely favored in the north and opposed in the south. Hamilton obtained the approval of the southern states in exchange for an agreement to place the new national capitol on the Potomac River. While the national debt increased as a result during Hamilton's service as Secretary of the Treasury, the nation established its good credit. Many in the Congress and elsewhere in the government profited from trading in the debt paper which was assumed. Though many of Washington's fellow Virginians, as well as others, were vexed by this, he considered they had adequate redress through their Congressional representatives.
The Revenue Act authorized the president to select the specific location of the seat of the government on the Potomac; the president was to appoint three commissioners to survey and acquire property for this seat. Washington personally oversaw this effort throughout his term in office. In 1791 the commissioners named the permanent seat of government "The City of Washington in the Territory of Columbia" to honor Washington. In 1800, the Territory of Columbia became the District of Columbia when the federal government moved to the site according to the provisions of the Residence Act.
In 1791 partly as a result of the Copper Panic of 1789, Congress imposed an excise tax on distilled spirits, which led to protests in frontier districts, especially Pennsylvania. By 1794 after Washington ordered the protesters to appear in U.S. district court, the protests turned into full-scale defiance of federal authority known as the Whiskey Rebellion. The federal army was too small to be used, so Washington invoked the Militia Act of 1792 to summon militias from Pennsylvania, Virginia, Maryland and New Jersey. The governors sent the troops, with Washington taking initial command. He subsequently named Henry "Lighthorse Harry" Lee as field commander to lead the troops into the rebellious districts. The rebels dispersed and there was no fighting, as Washington's forceful action proved the new government could protect itself. This represented the premier instance of the federal government using military force to exert authority over the states and citizens and is also the only time a sitting U.S. president personally commanded troops in the field.
Foreign affairs.
In February 1793 the French Revolutionary Wars broke out between Great Britain and its allies and revolutionary France, and engulfed Europe until 1815; Washington, with cabinet approval, proclaimed American neutrality. The revolutionary government of France sent diplomat Edmond-Charles Genêt, called "Citizen Genêt", to America. Genêt was welcomed with great enthusiasm, and began promoting the case for France using a network of new Democratic Societies in major cities. He even issued French letters of marque and reprisal to French ships manned by American sailors so they could capture British merchant ships. Washington denounced the societies and demanded the French government recall Genêt, which they did.
Hamilton formulated the Jay Treaty to normalize trade relations with Great Britain, remove them from western forts, and resolve financial debts remaining from the Revolution; John Jay negotiated and signed the treaty on November 19, 1794. Jeffersonians supported France and strongly attacked the treaty. Washington listened to both sides then announced his strong support, which mobilized public opinion and was pivotal in securing ratification in the Senate by the requisite two-thirds majority. The British agreed to depart from their forts around the Great Lakes and the United States-Canadian boundary had to be re-adjusted; numerous pre-Revolutionary debts were liquidated, and the British opened their West Indies colonies to American trade. Most importantly, the treaty delayed war with Great Britain and instead brought a decade of prosperous trade with the British. The treaty angered the French and became a central issue in many political debates. Relations with France deteriorated after the treaty was signed, leaving the succeeding president, John Adams, with the prospect of war.
Farewell Address.
Washington's Farewell Address (issued as a public letter in 1796) was one of the most influential statements of republicanism. Drafted primarily by Washington himself, with help from Hamilton, it gives advice on the necessity and importance of national union, the value of the Constitution and the rule of law, the evils of political parties and the proper virtues of a republican people. He referred to morality as "a necessary spring of popular government", and said, "Whatever may be conceded to the influence of refined education on minds of peculiar structure, reason and experience both forbid us to expect that national morality can prevail in exclusion of religious principle."
The address warned against foreign influence in domestic affairs and American meddling in European affairs, and as well against bitter partisanship in domestic politics; he also called for men to move beyond partisanship and serve the common good. He cautioned against "permanent alliances with any portion of the foreign world", saying the United States must concentrate primarily on American interests. He counseled friendship and commerce with all nations, but advised against involvement in European wars and entering into long-term "entangling" alliances. The address quickly set American values regarding foreign affairs.
Retirement (1797–1799).
After retiring from the presidency in March 1797, Washington returned to Mount Vernon with a profound sense of relief. He devoted much time to his plantations and other business interests, including his distillery which produced its first batch of spirits in February 1797. As explains, his plantation operations were only minimally profitable. The lands out west yielded little income because they were under attack by Indians and the squatters living there refused to pay him rent. Most Americans assumed he was rich because of the well-known "glorified façade of wealth and grandeur" at Mount Vernon. Historians estimate his estate was worth about $1 million in 1799 dollars, equivalent to about $19.9 million in 2014 purchasing power.
By 1798 relations with France had deteriorated to the point that war seemed imminent, and on July 4, 1798, President Adams offered Washington a commission as lieutenant general and Commander-in-chief of the armies raised or to be raised for service in a prospective war. He accepted, and served as the senior officer of the United States Army from July 13, 1798 until his death seventeen months later. He participated in the planning for a Provisional Army to meet any emergency that might arise, but avoided involvement in details as much as possible; he delegated most of the work, including leadership of the army, to Hamilton.
Comparisons with Cincinnatus.
During the Revolutionary and Early Republican periods of American history, many commentators compared Washington with the Roman aristocrat and statesman Cincinnatus. The comparison arose as Washington, like Cincinnatus, remained in command of the Continental Army only until the British had been defeated. Thereafter, instead of seeking great political power, he returned as quickly as possible to cultivating his lands. Remarking on Washington's resignation in December 1783, and his decision to retire to Mount Vernon, poet Philip Freneau wrote: "Thus He, whom Rome's proud legions sway'd/Return'd, and sought his sylvan shade." Lord Byron's "Ode to Napoleon" also lionized Washington as "the Cincinnatus of the West".
Death.
On Thursday, December 12, 1799, Washington spent several hours inspecting his plantation on horseback, in snow, hail, and freezing rain; later that evening he ate his supper without changing from his wet clothes. That Friday he awoke with a severe sore throat and became increasingly hoarse as the day progressed, yet still rode out in the heavy snow, marking trees on the estate that he wanted cut. Sometime around 3 a.m. that Saturday, he suddenly awoke with severe difficulty breathing and almost completely unable to speak or swallow. A firm believer in bloodletting, a standard medical practice of that era which he had used to treat various ailments of slaves on his plantation, he ordered estate overseer Albin Rawlins to remove half a pint of his blood.
A total of three physicians were sent for, including Washington's personal physician Dr. James Craik along with Dr. Gustavus Brown and Dr. Elisha Dick. Craik and Brown thought that Washington had "quinsey" or "quincy", while Dick, the younger man, thought the condition was more serious or a "violent inflammation of the throat". By the time the three physicians finished their treatments and bloodletting of the president, there had been a massive volume of blood loss—half or more of his total blood content was removed over the course of just a few hours. Recognizing that the bloodletting and other treatments were failing, Dr. Dick proposed performing an emergency tracheotomy, a procedure that few American physicians were familiar with at the time, as a last-ditch effort to save Washington's life, but the other two doctors disapproved.
Washington died at home around 10 p.m. on Saturday, December 14, 1799, aged 67. In his journal, Lear recorded Washington's last words as being "'Tis well."
The diagnosis of Washington's final illness and the immediate cause of his death have been subjects of debate since the day he died. In the days immediately following his death, Craik and Dick's published account stated that they felt his symptoms had been consistent with ""cynanche trachealis"", a term of that period used to describe severe inflammation of the structures of the upper airway. Even at that early date, there were accusations of medical malpractice, with some believing that Washington had been bled to death. Various modern medical authors have speculated that Washington probably died from a severe case of epiglottitis which was complicated by the given treatments (all of which were accepted medical practice in Washington's day)—most notably the massive deliberate blood loss, which almost certainly caused hypovolemic shock.
Throughout the world, men and women were saddened by Washington's death. In France, First Consul Napoleon Bonaparte ordered ten days of mourning throughout the country; in the United States, memorial processions were held in major cities and thousands wore mourning clothes for months.
To protect their privacy, Martha Washington burned the correspondence they had exchanged; only five letters between the couple are known to have survived, two letters from Martha to George and three from him to Martha.
On December 18, 1799, a funeral was held at Mount Vernon, where his body was interred. Congress passed a joint resolution to construct a marble monument in the planned crypt below the rotunda of the center section of the Capitol (then still under construction) for his body, a plan supported by Martha. In December 1800, the House passed an appropriations bill for $200,000 to build the mausoleum, which was to be a pyramid with a square base. Southern representatives and senators, in later opposition to the plan, defeated the measure because they felt it was best to have Washington's body remain at Mount Vernon.
In 1831, for the centennial of his birth, a new tomb was constructed to receive his remains. That year, an unsuccessful attempt was made to steal the body of Washington. Despite this, a joint Congressional committee in early 1832, debated the removal of President Washington's body from Mount Vernon to a crypt in the Capitol, built by architect Charles Bulfinch in the 1820s during the reconstruction of the burned-out structure after the British set it afire in August 1814, during the "Burning of Washington". Southern opposition was intense, antagonized by an ever-growing rift between North and South. Congressman Wiley Thompson of Georgia expressed the fear of Southerners when he said, "Remove the remains of our venerated Washington from their association with the remains of his consort and his ancestors, from Mount Vernon and from his native State, and deposit them in this capitol, and then let a severance of the Union occur, and behold the remains of Washington on a shore foreign to his native soil."
His remains were moved on October 7, 1837 to the new tomb constructed at Mount Vernon, presented by John Struthers of Philadelphia. After the ceremony, the inner vault's door was closed and the key was thrown into the Potomac.
Personal life.
Along with Martha's biological family, George Washington had a close relationship with his nephew and heir, Bushrod Washington, son of George's younger brother, John Augustine Washington. The year before his uncle's death, Bushrod became an Associate Justice of the Supreme Court of the United States. George, however, apparently did not get along well with his mother, Mary Ball Washington (Augustine's second wife), who was a very demanding and difficult person.
As a young man, Washington had red hair. A popular myth is that he wore a wig, as was the fashion among some at the time. However, Washington did not wear a wig; instead, he powdered his hair, as is represented in several portraits, including the well-known, unfinished Gilbert Stuart depiction called the "Athenaeum Portrait."
Washington's height was variously recorded as to , and he had unusually great physical strength that amazed younger men. Jefferson called Washington "the best horseman of his age", and both American and European observers praised his riding; the horsemanship benefited his hunting, a favorite hobby. Washington was an excellent dancer and frequently attended the theater, often referencing Shakespeare in letters. He drank in moderation and precisely recorded gambling wins and losses, but Washington disliked the excessive drinking, gambling, smoking, and profanity that was common in colonial Virginia. Although he grew tobacco, he eventually stopped smoking, and considered drunkenness a man's worst vice; Washington was glad that post-Revolutionary Virginia society was less likely to "force to drink and to make it an honor to send them home drunk."
Washington suffered from problems with his teeth throughout his life, and historians have tracked his experiences in great detail. He lost his first adult tooth when he was twenty-two and had only one left by the time he became president. John Adams claims he lost them because he used them to crack Brazil nuts but modern historians suggest the mercury oxide, which he was given to treat illnesses such as smallpox and malaria, probably contributed to the loss. He had several sets of false teeth made, four of them by a dentist named John Greenwood. None of the sets were made from wood. The set made when he became president was carved from hippopotamus and elephant ivory, held together with gold springs. Prior to these, he had a set made with real human teeth, likely ones he purchased from "several unnamed 'Negroes,' presumably Mount Vernon slaves" in 1784. Dental problems left Washington in constant pain, for which he took laudanum. This distress may be apparent in many of the portraits painted while he was still in office, including the one still used on the $1 bill.
Slavery.
Washington was the only prominent Founding Father to arrange in his will for the manumission of all his slaves following his death. He privately opposed slavery as an institution which he viewed as economically unsound and morally indefensible. He also regarded the divisiveness of his countrymen's feelings about slavery as a potentially mortal threat to the unity of the nation. Yet, as general of the army, president of the Constitutional Convention, and the first president of the United States, he never publicly challenged the institution of slavery, possibly because he wanted to avoid provoking a split in the new republic over so inflammatory an issue.
Washington had owned slaves since the death of his father in 1743, when at the age of eleven, he inherited 10 slaves. At the time of his marriage to Martha Custis in 1759, he personally owned at least 36 slaves, which meant he had achieved the status of a major planter. The wealthy widow Martha brought at least 85 "dower slaves" to Mount Vernon by inheriting a third of her late husband's estate. Using his wife's great wealth, Washington bought more land, tripling the size of the plantation at Mount Vernon, and purchased the additional slaves needed to work it. By 1774 he paid taxes on 135 slaves (this figure does not include the "dowers"). The last record of a slave purchase by him was in 1772, although he later received some slaves in repayment of debts. Washington also used some hired staff and white indentured servants; in April 1775, he offered a reward for the return of two runaway white servants.
Washington came to oppose slavery on both moral and economic grounds. Before the American Revolution, he had expressed no moral reservations about slavery. But by 1779 he would tell his manager at Mount Vernon that he wished to sell his slaves when the war ended, if the Americans were victorious. He concluded that maintaining a large, and increasingly elderly, slave population at Mount Vernon was no longer economically profitable, and that people who were compelled to work would never work hard. Washington could not legally sell the "dower slaves", and because they had long intermarried with his own slaves, he could not sell his slaves without breaking up families, which he wanted to avoid. In 1786 Washington wrote to Robert Morris, saying, "There is not a man living who wishes more sincerely than I do, to see a plan adopted for the abolition of slavery."
As president, following the transfer of the national capital to Pennsylvania in 1790, Washington brought eight enslaved people to work for him in the President's House in Philadelphia, where state law would have automatically granted freedom to any slaves who had resided in the state for more than 6 months. He circumvented that provision of the law by maintaining that he was not a Pennsylvania resident and ensuring that neither he nor any of his slaves stayed in the state for more than six months at a time. When one of the slaves, Oney Judge, a personal attendant to Martha, escaped, Washington complained that the slave had fled "without the least provocation," and he secretly sent agents to hunt her down. Washington could not legally free Judge, since she was Martha's dower slave. Martha urged Washington to advertise a reward for her capture, and was placed in the "Pennsylvania Gazette" on May 24, 1796. When the escaped former slave was spotted in New Hampshire, she said that she would agree to return out of affection for the Washington family, but only if they would guarantee her freedom, a proposal the Washingtons refused. They were still trying, surreptitiously, to recapture her two years later. Another slave, Hercules, who served as Washington's chef in the Presidential House in Philadelphia, managed to escape from Mount Vernon despite Washington's suspicions that he had been planning it. Washington would eventually replace the slaves at the President's House with immigrant German indentured servants.
By 1794, as he contemplated retirement, Washington began organizing his affairs so that in his will he could free all the slaves he owned outright. As historian Gordon S. Wood writes in his review of Joseph Ellis' biography of Washington, "He did this in the teeth of opposition from his relatives, his neighbors, and perhaps even Martha. It was a courageous act, and one of his greatest legacies." At the time of Washington's death in 1799, 317 slaves lived at Mount Vernon: 123 were owned by Washington himself, 154 were held by his wife as "dower slaves", and 40 others were rented from a neighbor. Washington's will provided for all of his slaves to be unconditionally freed upon the death of his widow, his heirs being expressly forbidden from selling or transporting those slaves out of Virginia. Hercules, who had earlier escaped Washington, was freed and no longer a fugitive slave. The will also provided for the training of the younger former-slaves in useful skills and for the creation of an old-age pension fund for the older ones. George and Martha emancipated no slaves of their own during their lifetimes and when Martha died on May 22, 1802, all of the slaves she was legally responsible for were not freed. Her human property Elisha went to her grandson George Washington Custis, the slaves from her first husband's estate—the dower slaves as well as the slaves she held in trust—went to his inheritors.
Religion.
The exact nature of Washington's religious beliefs has been debated by historians and biographers for over two hundred years. For his entire life he was affiliated with the Anglican Church, later called the Episcopal Church. He served as a vestryman and as church warden for both Fairfax Parish in Alexandria and Truro Parish, administrative positions that, like all positions in Virginia while it had an official religion, required one to swear they would not speak or act in a way that did not conform to the tenets of the Church. Numerous historians have suggested that theologically, Washington agreed largely with the Deists. However, he never made a statement one way or the other. He often used words for the deity, such as "God" and "Providence," while avoiding using the words "Jesus" and "Christ." In his collected works, they appear in an official letter to Indians that might have been drafted by an aide. At the time, Deism was a theological outlook, not an organized denomination, and was compatible with being an Episcopalian. Historian Gregg Frazer argues that Washington was not a deist but a "theistic rationalist." This theological position rejected core beliefs of Christianity, such as the divinity of Christ, the Trinity and original sin. However, unlike the deists, the theological rationalists believed in the efficacy of prayer to God. Historian Peter A. Lillback argues that Washington was neither a deist nor a "theistic rationalist" but a Christian who believed in the core beliefs of Christianity.
Washington, as commander of the army and as president, was a vigorous promoter of tolerance for all religious denominations. He believed religion was an important support for public order, morality and virtue. He often attended services of different denominations. He suppressed anti-Catholic celebrations in the Army.
Washington frequently accompanied his wife to church services. Although third-hand reports say he took communion, he is usually characterized as never or rarely participating in the rite. He would regularly leave services before communion with the other non-communicants (as was the custom of the day), until, after being admonished by a rector, he ceased attending at all on communion Sundays.
Chernow, in a 2010 podcast, summed up Washington's religious views:
There has been a huge controversy, to put it mildly, about Washington's religious beliefs. Before the Revolutionary War he was Anglican—Church of England—which meant after the war, he was Episcopalian. So, he was clearly Christian ... He was quite intensely religious, because even though he uses the word Providence, he constantly sees Providence as an active force in life, particularly in American life. I mean, every single victory in war he credits to Providence. The miracle of the Constitutional Convention he credits to Providence. The creation of the federal government and the prosperity of the early republic, he credits to Providence ... I was struck at how frequently in his letters he's referring to Providence, and it's Providence where there's a sense of design and purpose, which sounds to me very much like religion ... Unfortunately, this particular issue has become very very politicized.
Michael Novak and Jana Novak suggest that it may have been "Washington's intention to maintain a studied ambiguity (and personal privacy) regarding his own deepest religious convictions, so that all Americans, both in his own time and for all time to come, might feel free to approach him on their own terms—and might also feel like full members of the new republic, equal with every other". They conclude: "He was educated in the Episcopal Church, to which he always adhered; and my conviction is, that he believed in the fundamental doctrines of Christianity as usually taught in that Church, according to his understanding of them; but without a particle of intolerance, or disrespect for the faith and modes of worship adopted by Christians of other denominations."
Freemasonry.
Washington was initiated into Freemasonry in 1752. He had a high regard for the Masonic Order and often praised it, but he seldom attended lodge meetings. He was attracted by the movement's dedication to the Enlightenment principles of rationality, reason and fraternalism; the American lodges did not share the anti-clerical perspective that made the European lodges so controversial. In 1777 a convention of Virginia lodges recommended Washington to be the Grand Master of the newly established Grand Lodge of Virginia; however, Washington declined, due to his necessity to lead the Continental Army at a critical stage, and because he had never been installed as Master or Warden of a lodge, he did not consider it Masonically legal to serve as Grand Master. In 1788 Washington, with his personal consent, was named Master in the Virginia charter of Alexandria Lodge No. 22.
Legacy.
As Commander-in-Chief of the Continental Army, hero of the revolution and the first president of the United States, George Washington's legacy remains among the greatest in American history. Congressman Henry "Light-Horse Harry" Lee, a Revolutionary War comrade, , "First in war—first in peace—and first in the hearts of his countrymen".
Lee's words set the standard by which Washington's overwhelming reputation was impressed upon the American memory. Washington set many precedents for the national government, and the presidency in particular, and was called the "Father of His Country" as early as 1778. Washington's Birthday, is a federal holiday in the United States.
As the leader of the first successful revolution against a colonial empire in world history, Washington became an international icon for liberation and nationalism. The Federalists made him the symbol of their party but for many years, the Jeffersonians continued to distrust his influence and delayed building the Washington Monument. After Yorktown, his service as Commander in Chief brought him election as a Fellow of the American Academy of Arts and Sciences.
During the United States Bicentennial year, George Washington was posthumously appointed to the grade of General of the Armies of the United States by the congressional joint resolution passed on January 19, 1976, with an effective appointment date of July 4, 1976. This restored Washington's position as the highest-ranking military officer in U.S. history.
Papers.
The serious collection and publication of Washington's documentary record began with the pioneer work of Jared Sparks in the 1830s, "Life and Writings of George Washington" (12 vols., 1834–1837). "The Writings of George Washington from the Original Manuscript Sources, 1745–1799" (1931–44) is a 37 volume set edited by John C. Fitzpatrick. It contains over 17,000 letters and documents and is available online from the University of Virginia.
The definitive letterpress edition of his writings was begun by the University of Virginia in 1968, and today comprises 52 published volumes, with more to come. It contains everything written by Washington, or signed by him, together with most of his incoming letters. Part of the collection is available online from the University of Virginia.
Monuments and memorials.
Starting with victory in their Revolution, there were many proposals to build a monument to Washington. After his death, Congress authorized a suitable memorial in the national capital, but the decision was reversed when the Democratic-Republicans took control of Congress in 1801. The Democratic-Republicans were dismayed that Washington had become the symbol of the Federalist Party.
In 1932, the shipping company, United States Lines constructed their second American-built luxury ocean liner, named the SS Washington after the first president with the nickname "President Washington's Ship" in which become the flagship of the company between 1932 to 1940.
Washington, together with Theodore Roosevelt, Thomas Jefferson, and Lincoln, is depicted in stone at the Mount Rushmore Memorial. The Washington Monument, one of the best known American landmarks, was built in his honor. The George Washington Masonic National Memorial in Alexandria, Virginia, was constructed between 1922 and 1932 with voluntary contributions from all 52 local governing bodies of the Freemasons in the United States.
Many places and entities have been named in honor of Washington. Washington's name became that of the nation's capital, Washington, D.C., one of two national capitals across the globe to be named after an American president (the other is Monrovia, Liberia). The state of Washington is the only state to be named after a United States president. Mount Washington in New Hampshire, the tallest mountain in the Northeastern United States.
There are many other "Washington Monuments" in the United States, including two well-known equestrian statues: one in Manhattan and one in Richmond, Virginia. The first statue to show Washington on horseback was dedicated in 1856 and is located in Manhattan's Union Square.
Postage and currency.
George Washington appears on contemporary U.S. currency, including the one-dollar bill and the quarter-dollar coin (the Washington quarter).
Washington, along with Benjamin Franklin, appeared on the nation's first postage stamps in 1847. Since that time Washington has appeared on many postage issues, more than all other presidents combined.
Washington's victory over Cornwallis at the Battle of Yorktown was commemorated with a two-cent stamp on the battle's 150th anniversary on October 19, 1931. The 150th anniversary of the signing of the Constitution with George Washington as presiding officer was celebrated with a three-cent issue on September 17, 1937, was adapted from the painting by Julius Brutus Stearns. Washington's presidential inauguration at Federal Hall in New York City was celebrated on its 150th anniversary on April 30, 1939.
Selected Issues :
Selected currency :
Cherry tree.
Perhaps the best known story about Washington's childhood is that he chopped down his father's favorite cherry tree and admitted the deed when questioned: "I can't tell a lie, Pa." The anecdote was first reported by biographer Parson Weems, who after Washington's death interviewed people who knew him as a child over a half-century earlier. The Weems text was very widely reprinted throughout the 19th century, for example in McGuffey "Readers." Adults wanted children to learn moral lessons from history, especially as taught by example from the lives of great national heroes like Washington. After 1890 however, historians insisted on scientific research methods to validate every statement, and there was no documentation for this anecdote apart from Weems' report that he learned it in an interview with an old person. Joseph Rodman in 1904 noted that Weems plagiarized other Washington tales from published fiction set in England, but no one has found an alternative source for the cherry tree story.
Personal property auction record.
On June 22, 2012, George Washington's personal annotated copy of the "Acts Passed at a Congress of the United States of America" from 1789, which includes the Constitution of the United States and a draft of the Bill of Rights, was sold at Christie's for a $9,826,500 (with fees added to the final cost), to The Mount Vernon Ladies' Association. This was the record for a document sold at auction.

</doc>
<doc id="11969" url="https://en.wikipedia.org/wiki?curid=11969" title="Gulf Coast of the United States">
Gulf Coast of the United States

The Gulf Coast of the United States is the coastline along which the Southern United States meets the Gulf of Mexico. The coastal states that have a shoreline on the Gulf of Mexico are Texas, Louisiana, Mississippi, Alabama, and Florida, and these are known as the "Gulf States". 
The economy of the Gulf Coast area is dominated by industries related to energy, petrochemicals, fishing, aerospace, agriculture, and tourism. The large cities of the region are (from west to east) Brownsville, Corpus Christi, Houston, Galveston, Beaumont, Lafayette, Baton Rouge, New Orleans, Biloxi, Mobile, Pensacola, St. Petersburg, Tampa, and increasingly, Sarasota; all are the centers of their respective metropolitan areas and contain large ports. (Baton Rouge is relatively far from the Gulf of Mexico; its port is on the Mississippi River, as is the port of New Orleans.)
Geography.
The Gulf Coast is made of many inlets, bays, and lagoons. The coast is also intersected by numerous rivers, the largest of which is the Mississippi River. Much of the land along the Gulf Coast is, or was, marshland. Ringing the Gulf Coast is the Gulf Coastal Plain which reaches from Southern Texas to the western Florida Panhandle while the western portions of the Gulf Coast are made up of many barrier islands and peninsulas, including the Padre Island and Galveston Island located in the U.S. State of Texas. These landforms protect numerous bays and inlets providing as a barrier to oncoming waves. The central part of the Gulf Coast, from eastern Texas through Louisiana, consists primarily of marshland. The eastern part of the Gulf Coast, predominantly Florida, is dotted with many bays and inlets.
Climate.
The Gulf Coast climate is humid subtropical. Much of the year is warm to hot along the Gulf Coast, while the 3 winters months bring periods of cool and wet weather mixed with mild temperatures. The area is vulnerable to hurricanes as well as floods and severe thunderstorms. Tornadoes are infrequent at the coast but do occur, however the frequency at which they occur in inland portions of Gulf Coast states is much greater. Earthquakes are extremely rare to the area, but a surprising 6.0 earthquake in the Gulf of Mexico on September 10, 2006, could be felt from the cities of New Orleans to Tampa.
Economic activities.
The Gulf Coast is a major center of economic activity. The marshlands along the Louisiana and Texas coasts provide breeding grounds and nurseries for ocean life that drive the fishing and shrimping industries. The Port of South Louisiana (Metropolitan New Orleans in Laplace) and the Port of Houston are two of the ten busiest ports in the world by cargo volume. As of 2004, seven of the top ten busiest ports in the U.S. are on the Gulf Coast.
The discovery of oil and gas deposits along the coast and offshore, combined with easy access to shipping, have made the Gulf Coast the heart of the U.S. petrochemical industry. The coast contains nearly 4,000 oil platforms.
Besides the above, the region features other important industries including aerospace and biomedical research, as well as older industries such as agriculture and — especially since the development of the Gulf Coast beginning in the 1920s and the increase in wealth throughout the United States — tourism.
History.
The history of the Gulf Coast is an important part of United States history; as economically important as the Gulf Coast is to the United States today, it arguably once held an even greater position of prominence in the U.S. Before Europeans arrived in the region, the region was home to several pre-Columbian kingdoms that had extensive trade networks with empires such as the Aztecs and the Mississippi Mound Builders. Shark and alligator teeth and shells from the Gulf have been found as far North as Ohio, in the mounds of the Hopewell culture.
The first Europeans to settle the Gulf Coast were primarily the French and the Spanish. The Louisiana Purchase and the Texas Revolution made the Gulf Coast a part of the United States during the first half of the 19th century. As the U.S. population continued to expand its frontiers westward, the Gulf Coast was a natural magnet in the South providing access to shipping lanes and both national and international commerce. The development of sugar and cotton production (enabled by slavery) allowed the South to prosper. By the mid 19th century the city of New Orleans, being situated as a key to commerce on the Mississippi River and in the Gulf, had become the largest U.S. city not on the Atlantic seaboard and the fourth largest in the U.S. overall.
Two major events were turning points in the earlier history of the Gulf Coast region. The first was the American Civil War, which caused severe damage to some economic sectors in the South, including the Gulf Coast. The second event was the Galveston Hurricane of 1900. At the end of the 19th century Galveston was, with New Orleans, one of the most developed cities in the region. The city had the third busiest port in the U.S. and its financial district was known as the ""Wall Street of the South."" The storm mostly destroyed the city, which has never regained its former glory, and set back development in the region.
Since these darker times the Gulf Coast has been hit with numerous other hurricanes. In 2005, Hurricane Katrina struck the Gulf Coast as a Category 5 hurricane. It was the most damaging storm in the history of the United States, causing upwards of $80 billion in damages, and leaving over 1,800 dead. Again in 2008 the Gulf Coast was struck by a catastrophic hurricane. Due to its immense size, Hurricane Ike caused devastation from the Louisiana coastline all the way to the Kenedy County, Texas region near Corpus Christi. In addition, Ike caused flooding and significant damage along the Mississippi coastline and the Florida Panhandle Ike killed 112 people and left upwards of 300 people missing, never to be found. Hurricane Ike was the third most damaging storm in the history of the United States, causing more than $25 billion in damage along the coast, leaving hundreds of thousands of people homeless, and sparking the largest search-and-rescue operation in U.S. history.
Other than the hurricanes, the Gulf Coast has redeveloped dramatically over the course of the 20th century. The gulf coast is highly populated. The petrochemical industry, launched with the major discoveries of oil in Texas and spurred on by further discoveries in the Gulf waters, has been a vehicle for development in the central and western Gulf which has spawned development on a variety of fronts in these regions. Texas in particular has benefited tremendously from this industry over the course of the 20th century and economic diversification has made the state a magnet for population and home to more Fortune 500 companies than any other U.S. state. Florida has grown as well, driven to a great extent by its long established tourism industry but also by its position as a gateway to the Caribbean and Latin America. As of 2006, these two states are the second and fourth most populous states in the nation, respectively (see this article). Other areas of the Gulf Coast have benefited less, though economic development fueled by tourism has greatly increased property values along the coast, and is now a severe danger to the valuable but fragile ecosystems of the Gulf Coast.
Metropolitan areas.
The following table lists the 15 largest MSAs along the Gulf Coast.

</doc>
<doc id="11971" url="https://en.wikipedia.org/wiki?curid=11971" title="Galaxy formation and evolution">
Galaxy formation and evolution

The study of galaxy formation and evolution is concerned with the processes that formed a heterogeneous universe from a homogeneous beginning, the formation of the first galaxies, the way galaxies change over time, and the processes that have generated the variety of structures observed in nearby galaxies.
Galaxy formation is hypothesized to occur, from structure formation theories, as a result of tiny quantum fluctuations in the aftermath of the Big Bang. The simplest model for this that is in general agreement with observed phenomena is the Λ-Cold Dark Matter cosmology; that is to say that clustering and merging is how galaxies gain in mass, and can also determine their shape and structure.
Commonly observed properties of galaxies.
Because of the inability to conduct experiments in outer space, the only way to “test” theories and models of galaxy evolution is to compare them with observations. Explanations for how galaxies formed and evolved must be able to predict the observed properties and types of galaxies.
Edwin Hubble created the first galaxy classification scheme known as the Hubble tuning-fork diagram. It partitioned galaxies into ellipticals, normal spirals, barred spirals (such as the Milky Way), and irregulars. These galaxy types exhibit the following properties which can be explained by current galaxy evolution theories:
Hubble thought incorrectly that the tuning fork diagram described an evolutionary sequence for galaxies, from elliptical galaxies through lenticulars to spiral galaxies. However, astronomers now believe that disk galaxies likely formed first, then evolved into elliptical galaxies through galaxy mergers.
Formation of disk galaxies.
The earliest stage in the evolution of galaxies is the formation. When a galaxy forms, it has a disk shape and is called a spiral galaxy due to spiral-like "arm" structures located on the disk. There are different theories on how these disk-like distributions of stars develop from a cloud of matter, and at this point, one cannot say which is "right" because no current theory exactly predicts everything correctly as compared to what we observe.
Top-down theories.
Olin Eggen, Donald Lynden-Bell, and Allan Sandage in 1962, proposed a theory that disk galaxies form through a monolithic collapse of a large gas cloud. The distribution of matter in the early universe was in clumps that consisted mostly of dark matter. These clumps interacted gravitationally, putting tidal torques on each other that acted to give them some angular momentum. As the baryonic matter cooled, it dissipated some energy and contracted toward the center. With angular momentum conserved, the matter near the center speeds up its rotation. Then, like a spinning ball of pizza dough, the matter forms into a tight disk. Once the disk cools, the gas is not gravitationally stable, so it cannot remain a singular homogeneous cloud. It breaks, and these smaller clouds of gas form stars. Since the dark matter does not dissipate as it only interacts gravitationally, it remains distributed outside the disk in what is known as the dark halo. Observations show that there are stars located outside the disk, which does not quite fit the "pizza dough" model. It was first proposed by Leonard Searle and Robert Zinn that galaxies form by the coalescence of smaller progenitors. Known as a top-down formation scenario, this theory is quite simple yet no longer widely accepted.
Bottom-up theories.
More recent theories include the clustering of dark matter halos in the bottom-up process. Instead of large gas clouds collapsing to form a galaxy in which the gas breaks up into smaller clouds, it is proposed that matter started out in these “smaller” clumps (mass on the order of globular clusters), and then many of these clumps merged to form galaxies, which then were drawn by gravitation to form galaxy clusters. This still results in disk-like distributions of baryonic matter with dark matter forming the halo for all the same reasons as in the top-down theory. Models using this sort of process predict more small galaxies than large ones, which matches observations.
Astronomers do not currently know what process stops the contraction. In fact, theories of disk galaxy formation are not successful at producing the rotation speed and size of disk galaxies. It has been suggested that the radiation from bright newly formed stars, or from an active galactic nuclei can slow the contraction of a forming disk. It has also been suggested that the dark matter halo can pull the galaxy, thus stopping disk contraction.
The Lambda-CDM model is a cosmological model that explains the formation of the universe after the Big Bang. It is a relatively simple model that predicts many properties observed in the universe, including the relative frequency of different galaxy types; however, it underestimates the number of thin disk galaxies in the universe. The reason is that these galaxy formation models predict a large number of mergers. If disk galaxies merge with another galaxy of comparable mass (at least 15 percent of its mass) the merger will likely destroy, or at a minimum greatly disrupt the disk, and the resulting galaxy is not expected to be a disk galaxy (see next section). While this remains an unsolved problem for astronomers, it does not necessarily mean that the Lambda-CDM model is completely wrong, but rather that it requires further refinement to accurately reproduce the population of galaxies in the universe.
Galaxy mergers and the formation of elliptical galaxies.
Elliptical galaxies are among some of the largest known (IC 1101) thus far. Their stars are on orbits that are randomly oriented within the galaxy (i.e. they are not rotating like disk galaxies). A distinguishing feature of elliptical galaxies is that the velocity of the stars does not necessarily contribute to flattening of the galaxy, such as in spiral galaxies. Based on current observations, it can be seen that elliptical galaxies have supermassive black holes at their center, and the mass of these black holes correlates with the galaxy’s mass.
Elliptical galaxies have two main stages of evolution. The first is due to the supermassive black hole increasing in size from accreting cooling gas. The second stage of the elliptical galaxy can be marked by the black hole stabilizing by suppressing gas cooling, thus leaving the elliptical galaxy in a stable state. The mass of the black hole is also correlated to a property called sigma which is the dispersion of the velocities of stars in the elliptical galaxies. This relationship, known as the M-sigma relation, was discovered in 2000. Elliptical galaxies do not have disks around them, although some bulges of disk galaxies look similar to elliptical galaxies.One is more likely to find elliptical galaxies in more crowded regions of the universe (such as galaxy clusters).
Astronomers now see elliptical galaxies as some of the most evolved systems in the universe. It is widely accepted that the main driving force for the evolution of elliptical galaxies is mergers of smaller galaxies. Many galaxies in the universe are gravitationally bound to other galaxies, which means that they will never escape the pull of the other galaxy. If the galaxies are of similar size, the resultant galaxy will appear similar to neither of the two galaxies merging, but will instead be an elliptical galaxy. It is important to note that there are many types of galaxy mergers, which do not necessarily result in elliptical galaxies, but result in a change in the structure of the mergers. For example, a minor merger event is thought to be occurring between the Milky Way and the Magellanic Clouds.
The merging between such large galaxies is regarded as violent, but because of the vast distances between stars, there are essentially no stellar collisions involved in a collision between two galaxies. However, the frictional interaction of the gas between the two galaxies can cause gravitational shock waves, which are capable of forming new stars as the elliptical galaxy forms. By sequencing several images of different galactic collisions, one can observe the timeline of two spiral galaxies merging into a single elliptical galaxy.
In the Local Group, the Milky Way and M31 (the Andromeda Galaxy) are gravitationally bound, and currently approaching each other at high speed. From simulations, it can be seen that the Milky Way and Andromeda are on a collision course and are expected to collide in less than five billion years. During this collision, it is expected that the Sun and the rest of the Solar System will be ejected from its current path around the Milky Way. After the collision takes place, the merging of these two galaxies would cause a giant elliptical galaxy to form.
While scientists have learned a great deal about The Milky Way and other galaxies, the most fundamental questions about formation and evolution remain only tentatively answered.
Galaxy quenching.
As described in previous sections, galaxies tend to evolve from spiral to elliptical structure via mergers. However, the current rate of galaxy mergers does not explain how all galaxies move from the "blue cloud" to the "red sequence." It also does not explain how star formation ceases in galaxies. Theories of galaxy evolution must therefore be able to explain how star formation turns off in galaxies. This phenomenon is called galaxy "quenching".
Stars form out of cold gas (see also the Kennicutt-Schmidt law), so a galaxy is quenched when it has no more cold gas. However, it is thought that quenching occurs relatively quickly (within 1 billion years), which is much longer than the time it would take for a galaxy to simply use up its reservoir of cold gas. Galaxy evolution models explain this by hypothesizing other physical mechanisms that remove or shut off the supply of cold gas in a galaxy. These mechanisms can be broadly classified into two categories: (1) preventive feedback mechanisms that stop cold gas from entering a galaxy or stop it from producing stars, and (2) ejective feedback mechanisms that remove gas so that it cannot form stars.
One theorized preventive mechanism called “strangulation” keeps cold gas from entering the galaxy. Strangulation is likely the main mechanism for quenching star formation in nearby low-mass galaxies. The exact physical explanation for strangulation is still unknown, but it may have to do with a galaxy’s interactions with other galaxies. As a galaxy falls into a galaxy cluster, gravitational interactions with other galaxies can strangle it by preventing it from accreting more gas. For galaxies with massive dark matter halos, another preventive mechanism called “virial shock heating” may also prevent gas from becoming cool enough to form stars.
Ejective processes, which expel cold gas from galaxies, may explain how more massive galaxies are quenched. One ejective mechanism is caused by supermassive black holes found in the centers of galaxies. Simulations have shown that gas accreting onto supermassive black holes in galactic centers produces high-energy jets; the released energy can expel enough cold gas to quench star formation.
Our own Milky Way and the nearby Andromeda Galaxy currently appear to be undergoing the quenching transition from star-forming blue galaxies to passive red galaxies. This may offer us a unique opportunity to observe star formation quenching up close, and to better understand this important stage in galaxy evolution.

</doc>
<doc id="11973" url="https://en.wikipedia.org/wiki?curid=11973" title="Generation X">
Generation X

Generation X, commonly abbreviated to Gen X, is the generation born after the Western Post–World War II baby boom. Most demographers and commentators use birth dates ranging from the early 1960s to the early 1980s.
Origin of term.
The term "Generation X" was coined by the Magnum photographer Robert Capa in the early 1950s. He used it later as a title for a photo-essay about young men and women growing up immediately after the Second World War. The project first appeared in "Picture Post" in the UK and "Holiday" in the U.S. in 1953. Describing his intention, Capa said "We named this unknown generation, The Generation X, and even in our first enthusiasm we realised that we had something far bigger than our talents and pockets could cope with." 
The name was popularized by Canadian author Douglas Coupland's 1991 novel "", concerning young adults during the late 1980s and their lifestyles. While Coupland's book helped to popularize the phrase Generation X, he erroneously attributed it to English rock musician Billy Idol in a 1989 magazine article. In fact, Idol had been a member of the punk band Generation X from 1976 to 1981, which was named after Deverson and Hamblett's 1965 sociology book on British youth, "Generation X"—a copy of which was owned by Idol's mother.
Characteristics and demographics.
Gen X is the generation born after the Western World War II baby boom describing a generational change from the Baby Boomers.
In a 2012 article for the Joint Center for Housing Studies of Harvard University, George Masnick wrote that the "Census counted 82.1 million" Gen Xers in the U.S. The Harvard Center uses 1965 to 1984 to define Gen X so that Boomers, Xers, and Millennials "cover equal 20-year age spans". Masnick concluded that immigration filled in any birth year deficits during low fertility years of the late 1960s and early 1970s.
Jon Miller at the Longitudinal Study of American Youth at the University of Michigan wrote that "Generation X refers to adults born between 1961 and 1981" and it "includes 84 million people" in the U.S.
John Markert at Cumberland University employs a similar approach, but wrote a 2004 article in which "Generations should be discrete twenty-year periods" but with "ten-year cohorts" and 5-year "bihorts" (his word) non-simultaneously, classifies Generation X as those born in the years 1966 to 1985. Markert censures other methods and tactics to define Generation X in his article stating that "inconsistent use of dates by the same author" simply results "in an apple to lemon measurement standard".
In contrast to this 20-year approach to defining generations, which, when applied to Generation X, can push the age of the conventionally post-war baby boomers well into or before World War II, or, conversely, set the upper-age of generation X as 1986 or later, many writers have adopted a more conservative span of 15 years or fewer. Some, such as Tamara Erickson, in "What's Next Gen X?", and Elwood Watson, in "Generation X Professors Speak", use the dates of 1965-1979. While in at least one of their studies, the Pew Research Center defines Generation X births as from 1965 to 1980.
In 2011, "The Generation X Report", based on annual surveys used in the longitudinal study of today's adults, found Gen Xers, defined in the report as people born between 1961 and 1981, to be highly educated, active, balanced, happy, and family-oriented. The study contrasted with the slacker, disenfranchised stereotype associated with youth in the 1970s and 1980s. Various questions and responses from approximately 4,000 people who were surveyed each year from 1987 through 2010 made up the study. Clive Thompson, writing in "Wired" in 2014 claimed that the differences between Generation X and its predecessors had been over-hyped, quoting Kali Trzesniewski, a scholar of life-span changes, as saying that the basic personality metrics of Americans had remained stable for decades.
In 2012, the Corporation for National and Community Service ranked Gen X volunteer rates in the U.S. at "29.4% per year", the highest compared with other generations. The rankings were based on a three-year moving average between 2009 and 2011.
In the preface to "Generation X Goes Global: Mapping a Youth Culture in Motion", a collection of global essays, Professor Christine Henseler summarizes it as "a generation whose worldview is based on change, on the need to combat corruption, dictatorships, abuse, AIDS, a generation in search of human dignity and individual freedom, the need for stability, love, tolerance, and human rights for all".
In cinema, directors Quentin Tarantino, David Fincher, Jane Campion, Steven Soderbergh, Kevin Smith, Richard Linklater and Todd Solondz have been called Generation X filmmakers. Smith is most known for his View Askewniverse films, the flagship film being "Clerks", which is set in New Jersey circa 1994, and focuses on two bored, convenience-store clerks in their twenties. Linklater's "Slacker" similarly explores young adult characters who were more interested in philosophizing than settling with a long-term career and family. Solondz' "Welcome to the Dollhouse" touched on themes of school bullying, school violence, teen drug use, peer pressure and broken or dysfunctional families, set in a junior high school environment in New Jersey during the early to mid-1990s. While not a member of Gen X himself, director John Hughes has been recognized as having created a series of classics "that an entire generation took ownership of with films like "The Breakfast Club", "Sixteen Candles" and "Weird Science"".
Gen Xers are often called the MTV Generation. They experienced the emergence of music videos, grunge, alternative rock and hip hop. Some called Xers the "latchkey generation" because their personal identity was in part shaped by the independence of being left alone after school when they were children.
Compared with previous generations, Generation X represents a more heterogeneous generation, embracing social diversity in terms of such characteristics as race, class, religion, ethnicity, culture, language, gender identity, and sexual orientation.
Unlike their parents who challenged leaders with an intent to replace them, Gen Xers are less likely to idolize leaders and are more inclined to work toward long-term institutional and systematic change through economic, media and consumer actions.
The U.S. Census Bureau reports that Generation X holds the highest education levels when looking at current age groups.
Pursuant to a study by Elwood Carlson on "how different generations respond in unique ways to common problems in some political, social, and consumption choices", the Population Reference Bureau, a private demographic research organization based in Washington, D.C., cited Generation X birth years as falling between 1965 and 1982. On the first page of the study, authors William Strauss and Neil Howe's definition of a "cohort generation" is cited. They define Generation X by the years 1961 to 1981.
In 2008, "Details" magazine editor-at-large Jeff Gordinier released his book "X Saves the World -- How Generation X Got the Shaft but Can Still Keep Everything from Sucking".
Economy.
Studies done by the Pew Charitable Trusts, the American Enterprise Institute, the Brookings Institution, the Heritage Foundation and the Urban Institute challenged the notion that each generation will be better off than the one that preceded it.
A report titled "Economic Mobility: Is the American Dream Alive and Well?" focused on the income of males 30–39 in 2004 (those born April 1964March 1974). The study was released on May 25, 2007 and emphasized that this generation's men made less (by 12%) than their fathers had at that same age in 1974, thus reversing a historical trend. It concluded that per year increases in household income generated by fathers/sons have slowed (from an average of 0.9% to 0.3%), barely keeping pace with inflation. "Family incomes have risen though (over the period 1947 to 2005) because more women have gone to work, supporting the incomes of men, by adding a second earner to the family. And as with male income, the trend is downward".
"Generation Flux" is a neologism and psychographic designation coined by "Fast Company" for American employees who need to make several changes in career throughout their working lives because of the chaotic nature of the job market following the Financial crisis of 2007–08. Those in "Generation Flux" have birth years in the ranges of Gen X and Millennials.
Entrepreneurship.
According to authors Michael Hais and Morley Winograd: "Small businesses and the entrepreneurial spirit that Gen Xers embody have become one of the most popular institutions in America. There's been a recent shift in consumer behavior and Gen Xers will join the “idealist generation” in encouraging the celebration of individual effort and business risk-taking. As a result, Xers will spark a renaissance of entrepreneurship in economic life, even as overall confidence in economic institutions declines. Customers, and their needs and wants (including Millennials) will become the North Star for an entire new generation of entrepreneurs".
Inclusion & self-identification debate.
Due in part to the frequent birth-year overlap and resulting incongruence existing between attempts to define Generation X and Millennials, a growing number of individuals born in the late-1970s and the early-1980s, see themselves as either belonging to, and identifying with, neither of these traditional generations, or, rather, as trans-generational, included and identified with both, either in whole or to a degree.
Some attempts to define those individuals born in the Generation X and Millennial overlapped years have given rise to inter- or micro-generations, such as Xennials, The Lucky Ones, and others.
United Kingdom.
A 2008 article by "The Observer", cites the Generation X birth years as falling between 1965 and 1982; the same article later describes Millennials as being born between 1982 and 2002. The writer states that Generation Xers were "labelled by some" as the me-generation of the Eighties." Another piece written by a "Guardian" journalist in 2011 uses 1961 to 1981 for this generation.
"The Telegraph" cited Gen X birth dates as falling between a longer time span (1965–1985), In 2007, "The Independent" estimated an earlier range of birth dates (1963–1978) compared to other writers or researchers. However, the newspaper's 2010 article titled "Generation X: A mid-life crisis" uses the 1961 to 1981 date range. The "BBC News" article about a lack of "mid-career volunteers" in their 20s provides a Generation X age range, which, being written in 2007, would suggest birth years that fall between 1962 and 1982.
A "Daily Express" article in December 2013 discusses the impact the recession has had on the generation "born between 1961 and 1981". Despite a good degree and desired job skills, Jan Etherington writes: "They discovered that there is no job security and everywhere there are cutbacks on staff, salaries and benefits."
Canada.
David Foot, author and University of Toronto professor, divides the generation born after the Baby Boomers into two groups in his book "Boom Bust & Echo: How to Profit from the Coming Demographic Shift": Generation X, born between 1960 and 1966; and the "Bust Generation", born between 1967 and 1979. Foot contends that those born between the periods of 1947–1966 were the Baby Boomers, where in Canada they were the largest boom of the industrialized world (relative to population). This large boom complicated the job market for the upcoming generation. However, it is also common in Canada to represent Generation Xers using the date ranges 1961–1981 or 1965–1981.
Australia and New Zealand.
A "Sydney Morning Herald" article defined Gen X as "those born roughly between 1963–1980." The Australian Bureau of Statistics use a 1965–1981 birth range to define Generation X. According to generational demographers Bill Strauss and Neil Howe, "shorter birth year definitions are shorthand for fertility rates. Gen Xers (as a cultural generation) look beyond demographics to define themselves by a shared location in history, common beliefs, attitudes and values (and a common perceived membership). Defining Gen X purely by demographic bulges and busts (like the Census) misses key cultural indicators that a very different set of young people has come along. Commentators who set Millennial birth boundaries starting in the late-70s often make the same assumptions using fertility rates to define birth dates rather than shared beliefs, attitudes and values. Children born in the early 1960s and after had a very different coming of age experience than those born in the late 1950s. Some of the most influential cultural definers of Gen X were born during the period between 1961 and 1964."
Sources in New Zealand, including the country's labour statistics, define Gen X between the years 1965 and 1981. According to a December 2013 article from "The New Zealand Herald", a study done by researcher Dr. Kristin Murray of Massey University claims to have "debunked stereotypes about workers of different generations" who "may have more in common than we may think". She found that, although there were cultural differences between those in their twenties and those in their mid-thirties: "Those cultural differences weren't reflected in underlying values and motivations." But, she found that Generation X-ers (1965–1981) and Baby Boomers (1946–1964) were "most alike". However, Dr. Murray clarifies that her study "focused on values, so there could still be differences in behavior between age groups." Jason Walker, who is the New Zealand managing director for job recruitment company Hays, disagrees with Dr. Murray's findings. His company's research showed that Generation X members worked their way up the corporate ladder, advancing by learning new job skills. The Baby Boomers were dependent on their employers to take care of them if they worked hard. The "technologically savvy" Millennials, conversely, were "more risk-taking in their careers" and expected "fast-paced results". If they weren't challenging enough, or if they felt like they were in a dead-end job, they would move on.

</doc>
<doc id="11974" url="https://en.wikipedia.org/wiki?curid=11974" title="Guam">
Guam

Guam ( or ; Chamorro: '; formally the Territory of Guam'") is an unincorporated and organized territory of the United States. Located in the northwestern Pacific Ocean, Guam is one of five American territories with an established civilian government. The capital city is Hagåtña, and the most populous city is Dededo. In 2015, 161,785 people resided on Guam. Guamanians are American citizens by birth. Guam has an area of and a density of 297/km² (770/sq mi). It is the largest and southernmost of the Mariana Islands, and the largest island in Micronesia. Among its municipalities, Mongmong-Toto-Maite has the highest density at 1,425/km² (3,691/sq mi), whereas Inarajan and Umatac have the lowest density at 47/km² (119/sq mi). The highest point is Mount Lamlam at above sea level.
The Chamorros, Guam's indigenous people, settled the island approximately 4,000 years ago. Portuguese explorer Ferdinand Magellan was the first European to visit the island on March 6, 1521. Guam was colonized in 1668 with settlers, like Diego Luis de San Vitores, a Catholic missionary. Between the 1500s and the 1700s, Guam was an important stopover for the Spanish Manila Galleons. During the Spanish–American War, the United States captured Guam on June 21, 1898. Under the Treaty of Paris, Spain ceded Guam to the United States on December 10, 1898. Guam is amongst the seventeen Non-Self-Governing Territories of the United Nations.
Before World War II, Guam and three other territories – American Samoa, Hawaii, and the Philippines – were the only American jurisdictions in the Pacific Ocean. On December 7, 1941, hours after the attack on Pearl Harbor, Guam was captured by the Japanese, and was occupied for thirty months. During the occupation, Guamanians were subjected to culture alignment, forced labor, beheadings, rape, and torture. Guam endured hostilities when American forces recaptured the island on July 21, 1944; Liberation Day commemorates the victory. Since the 1960s, the economy is supported by two industries: tourism and the United States Armed Forces.
History.
The original inhabitants of Guam and the Northern Mariana Islands are believed to be descendants of Austronesian people originating from Southeast Asia as early as 2000 BC. They evolved into the Chamorro people.
The ancient-Chamorro society had four classes: "chamorri" (chiefs), "matua" (upper class), "achaot" (middle class), and "mana'chang" (lower class). The "matua" were located in the coastal villages, which meant they had the best access to fishing grounds, whereas the "mana'chang" were located in the interior of the island. "Matua" and "mana'chang" rarely communicated with each other, and "matua" often used "achaot" as intermediaries. There were also ""makåhna"" (similar to shamans), skilled in healing and medicine. Belief in spirits of ancient Chamorros called ""Taotao mo'na"" still persists as a remnant of pre-European culture. Their society was organized along matrilineal clans.
"Latte" stones are stone pillars that are found only in the Mariana Islands; and, they are a recent development in Pre-Contact Chamorro society. The latte-stone was used as a foundation on which thatched huts were built. Latte stones consist of a base shaped from limestone called the "haligi" and with a capstone, or "tåsa", made either from a large brain coral or limestone, placed on top. A possible source for these stones, the Rota Latte Stone Quarry, was discovered in 1925 on Rota.
Magellan's encounter with Guam.
The first European to discover Guam was Portuguese navigator Ferdinand Magellan, sailing for the King of Spain, when he sighted the island on March 6, 1521 during his fleet's circumnavigation of the globe. When Magellan arrived on Guam, he was greeted by hundreds of small outrigger canoes that appeared to be flying over the water, due to their considerable speed. These outrigger canoes were called Proas, and resulted in Magellan naming Guam "Islas de las Velas Latinas" ("Islands of the Lateen sails"). Antonio Pigafetta, one of Magellan's original 18 the name "Island of Sails", but he also writes that the inhabitants "entered the ships and stole whatever they could lay their hands on", including "the small boat that was fastened to the poop of the flagship." "Those people are poor, but ingenious and very thievish, on account of which we called those three islands "Islas de los Ladrones" ("Islands of thieves")."
Spanish colonization and the Manila galleons.
Despite Magellan's visit, Guam was not officially claimed by Spain until January 26, 1565 by General Miguel López de Legazpi. From 1565 to 1815, Guam and the Northern Mariana Islands, the only Spanish outpost in the Pacific Ocean east of the Philippines, was an important resting stop for the Manila galleons, a fleet that covered the Pacific trade route between Acapulco and Manila. To protect these Pacific fleets, Spain built several defensive structures which are still standing today, such as Fort Nuestra Señora de la Soledad in Umatac. It is the biggest single segment of Micronesia, the largest islands between the island of Kyushu (Japan), New Guinea, the Philippines, and the Hawaiian Islands.
Spanish colonization commenced on June 15, 1668 with the arrival of Diego Luis de San Vitores and Pedro Calungsod, who established the first Catholic church. The islands were part of the Spanish East Indies governed from the Philippines, which were in turn part of the Viceroyalty of New Spain based in Mexico City. Other reminders of colonial times include the old Governor's Palace in Plaza de España and the Spanish Bridge, both in Hagatña. Guam's Cathedral Dulce Nombre de Maria was formally opened on February 2, 1669, as was the Royal College of San Juan de Letran. Guam, along with the rest of the Mariana and Caroline Islands, were treated as part of Spain's colony in the Philippines. While Guam's Chamorro culture has indigenous roots, the cultures of both Guam and the Northern Marianas have many similarities with Spanish and Mexican culture due to three centuries of Spanish rule.
Internal conflicts.
Intermittent warfare lasting from July 23, 1670 until July 1695, plus the typhoons of 1671 and 1693, and in particular the smallpox epidemic of 1688, reduced the Chamorro population from 50,000 to 10,000 to less than 5,000. Precipitated by the death of Quipuha, and the murder of Father San Vitores and Pedro Calungsod by local rebel chief Matapang, tensions led to a number of conflicts. Captain Juan de Santiago started a campaign to pacify the island, which was continued by the successive commanders of the Spanish forces.
After his arrival in 1674, Captain Damian de Esplana ordered the arrest of rebels who attacked the population of certain towns. Hostilities eventually led to the destruction of villages such as Chochogo, Pepura, Tumon, Sidia-Aty, Sagua, Nagan and Ninca. Starting in June 1676, the first Spanish Governor of Guam, Capt. Francisco de Irrisarri y Vinar controlled internal affairs more strictly than his predecessors in order to curb tensions. He also ordered the construction of schools, roads and other infrastructure. Later, Capt. Jose de Quiroga arrived in 1680 and continued some of the development projects started by his predecessors. He also continued the search for the rebels who had assassinated Father San Vitores, resulting in campaigns against the rebels which were hiding out in some islands, eventually leading to the death of Matapang, Hurao and Aguarin. Quiroga brought some natives from the northern islands to Guam, ordering the population to live in a few large villages. These included Jinapsan, Umatac, Pago, Agat and Inarajan, where he built a number of churches. By July 1695, Quiroga had completed the pacification process in Guam, Rota, Tinian and Aguigan.
Expulsion of the Jesuits.
On February 26, 1767, Charles III of Spain issued a decree confiscating the property of the Jesuits and banishing them from Spain and her possessions. As a consequence, the Jesuit fathers on Guam departed on November 2, 1769 on the schooner "Nuestra Senora de Guadalupe", abandoning their churches, rectories and ranches.
The arrival of Governor Don Mariano Tobias, on September 15, 1771, brought agricultural reforms, including making land available to the islanders for cultivation, encouraged the development of cattle raising, imported deer and water buffalos from Manila, donkeys and mules from Acapulco, established cotton mills and salt pans, free public schools, and the first Guam militia. Later, he was transferred to Manila in June 1774.
Post-Napoleonic Era.
Following the Napoleonic Wars, many Spanish colonies in the Western Hemisphere had become independent, shifting the economy dependence of Guam from Mexico to the Philippines. Don Francisco Ramon de Villalobos, who became governor in 1831, improved economic conditions including the promotion of rice cultivation and the establishment of a leper hospital.
Otto von Kotzebue visited the island in November 1817, and Louis de Freycinet in March 1819. Jules Dumont d'Urville made two visits, the first in May 1828. The island became a rest stop for whalers starting in 1823.
A devastating typhoon struck the island on August 10, 1848 and then a severe earthquake on January 25, 1849, which resulted in many refugees from the Caroline Islands, victims of the resultant tsunami. After a smallpox epidemic killed 3,644 Guamanians in 1856, Carolinians and Japanese were permitted to settle in the Marianas. Guam received nineteen Filipinos prisoners after their failed 1872 Cavite mutiny.
Spanish–American War and World War II.
The United States took control of the island in the 1898 Spanish–American War, as part of the Treaty of Paris. Guam was transferred to U.S. Navy control on 23 December 1898 by . Guam came to serve as a station for American ships traveling to and from the Philippines, while the Northern Mariana Islands passed to Germany, and then to Japan. A U.S. Navy yard was established at Piti in 1899, and a marine barracks at Sumay in 1901. Following the Philippine–American War, Emilio Aguinaldo and Apolinario Mabini were exiled on Guam in 1901.
A marine seaplane unit was stationed in Guam from 1921 to 1930, the first in the Pacific. Pan American Airways established a seaplane base on the island for its San Francisco-Manila-Hong Kong route, and the Commercial Pacific Cable Company built a telegraph/telephone station in 1903. During World War II, Guam was attacked and invaded by the Empire of Japan on December 8, 1941 shortly after the attack on Pearl Harbor.
The Northern Mariana Islands had become a Japanese protectorate before the war. It was the Chamorros from the Northern Marianas who were brought to Guam to serve as interpreters and in other capacities for the occupying Japanese force. The Guamanian Chamorros were treated as an occupied enemy by the Japanese military. After the war, this would cause resentment between the Guamanian Chamorros and the Chamorros of the Northern Marianas. Guam's Chamorros believed their northern brethren should have been compassionate towards them, whereas having been occupied for over 30 years, the Northern Mariana Chamorros were loyal to Japan.
Guam's Japanese occupation lasted for approximately thirty-one months. During this period, the indigenous people of Guam were subjected to forced labor, family separation, incarceration, execution, concentration camps and forced prostitution. Approximately one thousand people died during the occupation, according to Congressional testimony in 2004. Some historians estimate that war violence killed 10% of Guam's some 20,000 population.
The United States returned and fought the Battle of Guam from July 21 to August 10, 1944, to recapture the island from Japanese military occupation. More than 18,000 Japanese were killed as only 485 surrendered. Sergeant Shoichi Yokoi, who surrendered in January 1972, appears to have been the last confirmed Japanese holdout in Guam. The United States also captured and occupied the Northern Marianas.
North Field was established in 1944, and was renamed for Brigadier General James Roy Andersen (1904–1945) as Andersen Air Force Base.
Post-war.
After World War II, the Guam Organic Act of 1950 established Guam as an unincorporated organized territory of the United States, provided for the structure of the island's civilian government, and granted the people U.S. citizenship. The Governor of Guam was federally appointed until 1968, when the Guam Elective Governor Act provided for the office's popular election. Since Guam is not a U.S. state, U.S. citizens residing on Guam are not allowed to vote for president and their congressional representative is a non-voting member.
Vietnam War and later.
Andersen Air Force Base played a major role in the Vietnam War. The host unit was later designated the 36th Wing (36 WG), assigned to the Pacific Air Forces (PACAF) Thirteenth Air Force (13AF). In September 2012, 13 AF was inactivated and its functions merged into PACAF. 
On August 6, 1997, Guam was the site of the Korean Air Flight 801 aircraft accident. The Boeing 747–300 jetliner was preparing to land when it crashed into a hill, killing 228 of the 254 people on board. Since 1974, about 124 historic sites in Guam have been recognized under the U.S. National Register of Historic Places. Guam temporarily hosted 100,000 Vietnamese refugees in 1975, and 6,600 Kurdish refugees in 1996.
Geography.
Guam lies between 13.2°N and 13.7°N and between 144.6°E and 145.0°E, and has an area of , making it the 32nd largest island of the United States. It is the southernmost and largest island in the Mariana island chain and is also the largest island in Micronesia. This island chain was created by the colliding Pacific and Philippine Sea tectonic plates. Guam is the closest land mass to the Mariana Trench, a deep subduction zone, that lies beside the island chain to the east. Challenger Deep, the deepest surveyed point in the Oceans, is southwest of Guam at deep. The highest point in Guam is Mount Lamlam at an elevation of .
The island of Guam is long and wide, the size of Singapore. The island experiences occasional earthquakes due to its location on the western edge of the Pacific Plate and near the Philippine Sea Plate. In recent years, earthquakes with epicenters near Guam have had magnitudes ranging from 5.0 to 8.7. Unlike the Anatahan volcano in the Northern Mariana Islands, Guam is not volcanically active. However, due to its proximity to Anatahan, vog (i.e. volcanic smog) does occasionally affect Guam.
A coral table reef surrounds most of Guam, and the limestone plateau provide the source for most of the island's fresh water. Steep coastal cliffs dominate the north, while the southern end of the island is mountainous, with lower hills in between.
Climate.
Guam's climate is characterized as tropical marine moderated by seasonal northeast trade winds. The weather is generally very warm and humid with little seasonal temperature variation. The mean high temperature is 86 °F (30 °C) and mean low is 76 °F (24 °C) with an average annual rainfall of 96 inches (2,180 mm). The dry season runs from December to June. The remaining months (July to November) constitute the rainy season. The months of January and February are considered the coolest months of the year with overnight low temperatures of 70–75 °F (21–24 °C) and low humidity levels. The highest temperature ever recorded in Guam was on April 18, 1971 and April 1, 1990, and the lowest temperature ever recorded was on February 8, 1973.
Guam is located in "Typhoon Alley" and it is common for the island to be threatened by tropical storms and possible typhoons during the wet season. The highest risk of typhoons is during September and October. They can, however, occur year-round. The most intense typhoon to pass over Guam recently was Super Typhoon Pongsona, with sustained winds of , gusts to , which slammed Guam on December 8, 2002, leaving massive destruction.
Since Super Typhoon Pamela in 1976, wooden structures have been largely replaced by concrete structures. During the 1980s wooden utility poles began to be replaced by typhoon-resistant concrete and steel poles. After the local Government enforced stricter construction codes, many home and business owners built their structures out of reinforced concrete with installed typhoon shutters.
Demographics.
Based on a 2010 estimate, the largest ethnic group are the native Chamorros, accounting for 37.3% of the total population. Other significant ethnic groups include those of Filipino (26.3%), White (7.1%), and Chuukese (7%) ethnicities. The rest are from other Pacific Islands or of Asian ancestry. Roman Catholicism is the predominant religion, with 85% of the population stating an affiliation with it. A small Jewish community exists as well.
The official languages of the island are English and Chamorro.
Culture.
Post-European-contact Chamorro culture is a combination of American, Spanish, Filipino, other Micronesian Islander and Mexican traditions, with few remaining indigenous pre-Hispanic customs. These influences are manifested in the local language, music, dance, sea navigation, cuisine, fishing, games (such as batu, chonka, estuleks, and bayogu), songs and fashion. During Spanish colonial rule (1668–1898) the majority of the population was converted to Roman Catholicism and religious festivities such as Easter and Christmas became widespread. Post-contact Chamorro cuisine is largely based on corn, and includes tortillas, tamales, atole and chilaquiles, which are a clear influence from Spanish trade between Mesoamerica and Asia. The modern Chamorro language is a Malayo-Polynesian language with much Spanish and Filipino influence. Many Chamorros also have Spanish surnames because of their conversion to Roman Catholic Christianity and the adoption of names from the Catálogo alfabético de apellidos, a phenomenon also common to the Philippines.
Due to foreign cultural influence from Spain, most aspects of the early indigenous culture have been lost, though there has been a resurgence in preserving any remaining pre-Hispanic culture in the last few decades. Some scholars have traveled throughout the Pacific Islands conducting research to study what the original Chamorro cultural practices such as dance, language, and canoe building may have been like.
Two aspects of indigenous pre-Hispanic culture that withstood time are chenchule' and inafa'maolek. Chenchule' is the intricate system of reciprocity at the heart of Chamorro society. It is rooted in the core value of inafa'maolek. Historian Lawrence Cunningham in 1992 wrote, "In a Chamorro sense, the land and its produce belong to everyone. , or interdependence, is the key, or central value, in Chamorro culture ... depends on a spirit of cooperation and sharing. This is the armature, or core, that everything in Chamorro culture revolves around. It is a powerful concern for mutuality rather than individualism and private property rights."
The core culture or Pengngan Chamorro is based on complex social protocol centered upon respect: From sniffing over the hands of the elders (called mangnginge in Chamorro), the passing down of legends, chants, and courtship rituals, to a person asking for permission from spiritual ancestors before entering a jungle or ancient battle grounds. Other practices predating Spanish conquest include galaide' canoe-making, making of the belembaotuyan (a string musical instrument made from a gourd), fashioning of "" slings and slingstones, tool manufacture, burial rituals, and preparation of herbal medicines by Suruhanu.
Master craftsmen and women specialize in weavings, including plaited work (niyok- and åkgak-leaf baskets, mats, bags, hats, and food containments), loom-woven material (kalachucha-hibiscus and banana fiber skirts, belts and burial shrouds), and body ornamentation (bead and shell necklaces, bracelets, earrings, belts and combs made from tortoise shells and Spondylus).
The cosmopolitan and multicultural nature of modern Guam poses challenges for Chamorros struggling to preserve their culture and identity amidst forces of acculturation. The increasing numbers of Chamorros, especially Chamorro youth, relocating to the U.S. Mainland has further complicated both definition and preservation of Chamorro identity. While only a few masters exist to continue traditional art forms, the resurgence of interest among the Chamorros to preserve the language and culture has resulted in a growing number of young Chamorros who seek to continue the ancient ways of the Chamorro people.
Government and politics.
Guam is governed by a popularly elected governor and a unicameral 15-member legislature, whose members are known as senators. Guam elects one non-voting delegate, currently Democrat Madeleine Z. Bordallo, to the United States House of Representatives. U.S. citizens in Guam vote in a straw poll for their choice in the U.S. Presidential general election, but since Guam has no votes in the Electoral College, the poll has no real effect. However, in sending delegates to the Republican and Democratic national conventions, Guam does have influence in the national presidential race. These delegates are elected by local party conventions.
In the 1980s and early 1990s, there was a significant movement in favor of the territory becoming a commonwealth, which would give it a level of self-government similar to Puerto Rico and the Northern Mariana Islands. However, the federal government rejected the version of a commonwealth that the government of Guam proposed, due to it having clauses incompatible with the Territorial Clause (Art. IV, Sec. 3, cl. 2) of the U.S. Constitution. Other movements advocate U.S. statehood for Guam, union with the state of Hawaii, union with the Northern Mariana Islands as a single territory, or independence.
Villages and military bases.
Guam is divided into nineteen municipalities called villages: Agana Heights, Agat, Asan‑Maina, Barrigada, Chalan‑Pago‑Ordot, Dededo, Hagåtña, Inarajan, Mangilao, Merizo, Mongmong‑Toto‑Maite, Piti, Santa Rita, Sinajana, Talofofo, Tamuning, Umatac, Yigo, Yona.
The U.S. military maintains jurisdiction over its bases, which cover approximately , or 29% of the island's total land area:
In addition to on-shore military installations, Guam, along with the rest of the Mariana Islands, is being prepared to be the westernmost military training range for the U.S. Guam is currently viewed as a key military hub that will further allow U.S. military power to be projected via sea and sky.
The U.S. military has proposed building a new aircraft carrier berth on Guam and moving 8,600 Marines, and 9,000 of their dependents, to Guam from Okinawa, Japan. Including the required construction workers, this buildup would increase Guam's population by 45%. In a February 2010 letter, the United States Environmental Protection Agency sharply criticized these plans because of a water shortfall, sewage problems and the impact on coral reefs. By 2012, these plans had been cut to only have a maximum of 4,800 Marines stationed on the island, two thirds of which would be there on a rotational basis without their dependents.
With the proposed increased military presence stemming from the upcoming preparation efforts and relocation efforts of U.S. Marines from Okinawa, Japan to Guam slated to begin in 2010 and last for the next several years thereafter, the amounts of total land that the military will control or tenant may grow to or surpass 40% of the entire landmass of Guam.
In January 2011, the Ike Skelton National Defense Authorization Act for FY2011 indicated that recent significant events will delay the deadline for realigning U.S. Marine Corps service members and their families from Okinawa to Guam. The transfer may be as late as 2020. In addition, the Defense Authorization Act cut approximately $320 million from the 2011 budget request.
Villagers and the military community are interconnected in many ways. Many villagers serve in the military or are retired. Many active duty personnel and Defense Department civilians also live in the villages outside of the military installation areas. The military and village communities have "adoption" programs where Guam's population and military personnel stationed on Guam perform community service projects.
Economy.
Guam's economy depends primarily on tourism, Department of Defense installations and locally owned businesses. Despite paying no income or excise tax, it receives large transfer payments from the general revenues of the U.S. federal treasury. Under the provisions of a special law by Congress, it is Guam's treasury rather than the U.S. treasury that receives the federal income taxes paid by local taxpayers (including military and civilian federal employees assigned to Guam).
Lying in the western Pacific, Guam is a popular destination for Japanese tourists. Its tourist hub, Tumon, features over 20 large hotels, a Duty Free Shoppers Galleria, Pleasure Island district, indoor aquarium, Sandcastle Las Vegas–styled shows and other shopping and entertainment venues. It is a relatively short flight from Asia or Australia compared to Hawaii, with hotels and seven public golf courses accommodating over a million tourists per year. Although 75% of the tourists are Japanese, Guam receives a sizable number of tourists from South Korea, the U.S., the Philippines, and Taiwan. Significant sources of revenue include duty-free designer shopping outlets, and the American-style malls: Micronesia Mall, Guam Premier Outlets, the Agana Shopping Center, and the world's largest Kmart.
The economy had been stable since 2000 due to increased tourism, but took a recent downturn along with the rest of the global economy. It is expected to stabilize with the transfer of U.S. Marine Corps' 3rd Marine Expeditionary Force, currently in Okinawa, Japan, (approximately 8,000 Marines, along with their 10,000 dependents), to Guam between 2010 and 2015. In 2003, Guam had a 14% unemployment rate, and the government suffered a $314 million shortfall.
The Compacts of Free Association between the United States, the Federated States of Micronesia, the Republic of the Marshall Islands and the Republic of Palau accorded the former entities of the Trust Territory of the Pacific Islands a political status of "free association" with the United States. The Compacts give citizens of these island nations generally no restrictions to reside in the United States (also its territories), and many were attracted to Guam due to its proximity, environmental, and cultural familiarity. Over the years, it was claimed by some in Guam that the territory has had to bear the brunt of this agreement in the form of public assistance programs and public education for those from the regions involved, and the federal government should compensate the states and territories affected by this type of migration. Over the years, Congress had appropriated "Compact Impact" aids to Guam, the Northern Mariana Islands and Hawaii, and eventually this appropriation was written into each renewed Compact. Some, however, continue to claim the compensation is not enough or that the distribution of actual compensation received is significantly disproportionate.
Transportation and communications.
Most of the island has state-of-the-art mobile phone services and high-speed internet widely available through either cable or DSL. Guam was added to the North American Numbering Plan (NANP) in 1997 (country code 671 became NANP area code 671), removing the barrier of high cost international long-distance calls to the U.S. Mainland.
Guam is also a major hub for submarine cables between the Western U.S., Hawaii, Australia and Asia. Guam currently serves 12 submarine cables, with most continuing to China.
In 1899, the local postage stamps were overprinted "Guam" as was done for the other former Spanish colonies, but this was discontinued shortly thereafter and regular U.S. postage stamps have been used ever since. Because Guam is also part of the U.S. Postal System (postal abbreviation: GU, ZIP code range: 96910–96932), mail to Guam from the U.S. mainland is considered domestic and no additional charges are required. Private shipping companies, such as FedEx, UPS, and DHL, however, have no obligation to do so, and do not regard Guam as domestic.
The speed of mail traveling between Guam and the states varies depending on size and time of year. Light, first-class items generally take less than a week to or from the mainland, but larger first-class or Priority items can take a week or two. Fourth-class mail, such as magazines, are transported by sea after reaching Hawaii. Most residents use post office boxes or private mail boxes, although residential delivery is becoming increasingly available. Incoming mail not from the Americas should be addressed to "Guam" instead of "USA" to avoid being routed the long way through the U.S. mainland and possibly charged a higher rate (especially from Asia).
The Commercial Port of Guam is the island's lifeline because most products must be shipped into Guam for consumers. It receives the weekly calls of the Hawaii-based shipping line Matson, Inc. whose container ships connect Guam with Honolulu, Hawaii, Los Angeles, California, Oakland, California and Seattle, Washington. The port is also the regional transhipment hub for over 500,000 customers throughout the Micronesian region. The port is the shipping and receiving point for containers designated for the island's U.S. Department of Defense installations, Andersen Air Force Base and Commander, Naval Forces Marianas and eventually the Third Marine Expeditionary Force.
Guam is served by the Antonio B. Won Pat International Airport, which is a hub for United Airlines. The island is outside the United States customs zone so Guam is responsible for establishing and operating its own customs and quarantine agency and jurisdiction. Therefore, the U.S. Customs and Border Protection only carries immigration (but not customs) functions. Since Guam is under federal immigration jurisdiction, passengers arriving directly from the United States skip immigration and proceed directly to Guam Customs and Quarantine.
However, due to the Guam and CNMI visa waiver program for certain countries, an eligibility pre-clearance check is carried on Guam for flights to the States. For travel from the Northern Mariana Islands to Guam, a pre-flight passport and visa check is performed before boarding the flight to Guam. On flights from Guam to the Northern Mariana Islands, no immigration check is performed. Traveling between Guam and the States through a foreign point, however, does require a passport.
Most residents travel within Guam using personally owned vehicles. The local government currently outsources the only public bus system (Guam Regional Transit Authority), and some commercial companies operate buses between tourist-frequented locations
Ecological issues.
Brown tree snake.
Believed to be a stowaway on a U.S. military transport near the end of World War II, the brown tree snake ("Boiga irregularis") was accidentally introduced to Guam, that previously had no native species of snake. It nearly eliminated the native bird population. The problem was exacerbated because the reptile has no natural predators on the island. The brown tree snake, known locally as the "kulebla", is native to northern and eastern coasts of Australia, Papua New Guinea, and the Solomon Islands. While slightly venomous, the snake is relatively harmless to human beings. Although some studies have suggested a high density of these serpents on Guam, residents rarely see the nocturnal creatures. The United States Department of Agriculture has trained detector dogs to keep the snakes out of the island's cargo flow. The United States Geological Survey also has dogs capable of detecting snakes in forested environments around the region's islands.
Before the introduction of the brown tree snake, Guam was home to several endemic bird species. Among them were the Guam rail (or "ko'ko bird in Chamorro) and the Guam flycatcher, both common throughout the island. Today the flycatcher is entirely extinct while the Guam rail is extinct in the wild but bred in captivity by the Division of Aquatic and Wildlife Resources. The devastation caused by the snake has been significant over the past several decades. As many as twelve bird species are believed to have been driven to extinction. According to many elders, ko'ko' birds were common in Guam before World War II.
Other bird species threatened by the brown tree snake include the Mariana crow, the Mariana swiftlet, and the Micronesian starling, though populations are present on other islands, including Rota.
Guam is said to have 40 times more spiders than neighboring islands because the number of birds, their natural predators, are severely diminished.
Coconut rhinoceros beetle.
An infestation of the coconut rhinoceros beetle (CRB), "Oryctes rhinoceros", was detected on Guam on September 12, 2007. CRB is not known to occur in the United States except in American Samoa. Delimiting surveys performed September 13–25, 2007 indicated that the infestation was limited to Tumon Bay and Faifai Beach, an area of approximately . Guam Department of Agriculture (GDA) placed quarantine on all properties within the Tumon area on October 5 and later expanded the quarantine to about on October 25; approximately radius in all directions from all known locations of CRB infestation. CRB is native to Southern Asia and distributed throughout Asia and the Western Pacific including Sri Lanka, Upolu, Samoa, American Samoa, Palau, New Britain, West Irian, New Ireland, Pak Island and Manus Island (New Guinea), Fiji, Cocos (Keeling) Islands, Mauritius, and Reunion.
Other invasive animal species.
From the seventeenth through nineteenth centuries, the Spanish introduced pigs, dogs, chickens, the Philippine deer ("Rusa mariannus"), black francolins, and carabao (a subspecies of water buffalo), which have cultural significance. Herds of carabao obstruct military base operations and harm native ecosystems. After birth control and adoption efforts were ineffective, the U.S. military began culling the herds in 2002 leading to organized protests from island residents.
Other introduced species include cane toads imported in 1937, the giant African snail (an agricultural pest introduced during World War II by Japanese occupation troops) and more recently frog species which could threaten crops in addition to providing additional food for the brown tree snake population. Reports of loud chirping frogs native to Puerto Rico and known as coquí, that may have arrived from Hawaii, have led to fears that the noise could threaten Guam's tourism.
Introduced feral pigs and deer, over-hunting, and habitat loss from human development are also major factors in the decline and loss of Guam's native plants and animals.
Threats to indigenous plants.
Invading animal species are not the only threat to Guam's native flora. Tinangaja, a virus affecting coconut palms, was first observed on the island in 1917 when copra production was still a major part of Guam's economy. Though coconut plantations no longer exist on the island, the dead and infected trees that have resulted from the epidemic are seen throughout the forests of Guam.
During the past century, the dense forests of northern Guam have been largely replaced by thick "tangan-tangan" brush ("Leucaena leucocephala"). Much of Guam's foliage was lost during World War II. In 1947, the U.S. military is thought to have planted tangan-tangan by seeding the island from the air to prevent erosion. Tangan-tangan was present on the island before 1905.
In southern Guam, non-native grass species dominate much of the landscape. Although the colorful and impressive flame tree ("Delonix regia") is found throughout the Marianas, the tree on Guam has been largely decimated.
Wildfires.
Wildfires plague the forested areas of Guam every dry season despite the island's humid climate. Most fires are man-caused with 80% resulting from arson. Poachers often start fires to attract deer to the new growth. Invasive grass species that rely on fire as part of their natural life cycle grow in many regularly burned areas. Grasslands and "barrens" have replaced previously forested areas leading to greater soil erosion. During the rainy season sediment is carried by the heavy rains into the Fena Lake Reservoir and Ugum River, leading to water quality problems for southern Guam. Eroded silt also destroys the marine life in reefs around the island. Soil stabilization efforts by volunteers and forestry workers (planting trees) have had little success in preserving natural habitats.
Aquatic preserves.
Efforts have been made to protect Guam's coral reef habitats from pollution, eroded silt and overfishing, problems that have led to decreased fish populations. (Since Guam is a significant vacation spot for scuba divers, this is important.) In recent years, the Department of Agriculture, Division of Aquatic and Wildlife Resources has established several new marine preserves where fish populations are monitored by biologists. Before adopting U.S. Environmental Protection Agency standards, portions of Tumon Bay were dredged by the hotel chains to provide a better experience for hotel guests. Tumon Bay has since been made into a preserve. A federal Guam National Wildlife Refuge in northern Guam protects the decimated sea turtle population in addition to a small colony of Mariana fruit bats.
Harvest of sea turtle eggs was a common occurrence on Guam before World War II. The green sea turtle ("Chelonia mydas") was harvested legally on Guam before August 1978, when it was listed as threatened under the Endangered Species Act. The hawksbill sea turtle ("Eretmochelys imbricata") has been on the endangered list since 1970. In an effort to ensure protection of sea turtles on Guam, routine sightings are counted during aerial surveys and nest sites are recorded and monitored for hatchlings.
Education.
Colleges and universities.
The University of Guam (UOG) and Guam Community College, both fully accredited by the Western Association of Schools and Colleges, offer courses in higher education. UOG is a member of the exclusive group of only 76 U.S. land-grant institutions in the entire United States. Pacific Islands University is a small Christian liberal arts institution nationally accredited by the Transnational Association of Christian Colleges and Schools. They offer courses at both the undergraduate and graduate levels.
Primary and secondary schools.
The Guam Department of Education serves the entire island of Guam. In 2000, 32,000 students attended Guam's public schools. Guam Public Schools have struggled with problems such as high dropout rates and poor test scores. Guam's educational system has always faced unique challenges as a small community located from the U.S. mainland with a very diverse student body including many students who come from backgrounds without traditional American education. An economic downturn in Guam since the mid-1990s has compounded the problems in schools.
Before September 1997, the U.S. Department of Defense partnered with Guam Board of Education. In September 1997 the DoDEA opened its own schools for children of military personnel. DoDEA schools, which also serve children of some federal civilian employees, had an attendance of 2,500 in 2000. DoDEA Guam operates three elementary/middle schools and one high school.
Public libraries.
Guam Public Library System operates the Nieves M. Flores Memorial Library in Hagåtña and five branch libraries.
Health care.
The Government of Guam maintains the island's main health care facility, Guam Memorial Hospital, in Tamuning. U.S. board certified doctors and dentists practice in all specialties. In addition, the U.S. Naval Hospital in Agana Heights serves active-duty members and dependents of the military community. There is one subscriber-based air ambulance located on the island, CareJet, which provides emergency patient transportation across Guam and surrounding islands. A private hospital, the Guam Regional Medical City opened its doors in early 2016.
Film-making.
Over the years, a number of films have been shot on Guam, including "Shiro's Head" (directed by the Muna brothers) and the government-funded "" (2004). Although set on Guam, "No Man Is an Island" (1962) was not shot there, but in the Republic of the Philippines.
Sports.
Pacific Games.
Guam hosted the Pacific Games in 1975 and 1999. At the 2007 Games, Guam finished 7th of 22 countries and 14th at the 2011 Games.
Association Football.
The Guam national football team was founded in 1975 and joined FIFA in 1996. Guam was once considered one of FIFA's weakest teams, and experienced their first victory over a FIFA-registered side in 2009, when they defeated Mongolia in the East Asian Cup.
Guam entered the 2018 FIFA World Cup qualification Group D. Guam hosted qualifying games on island for the first time in 2015. During the qualifying round, Guam clinched their first FIFA World Cup Qualifying win by defeating Turkmenistan. Since then, the team has experienced moderate success in the Qualifying Round with a record of 2-1-1.
The national team plays at the Guam National Football Stadium, which has a capacity of 1,000. The men's national football team are known as the "matao" team. Matao is the definition of highest level or "noble" class; the matao team have done exceptionally well under the head coach Gary White. 
The top football division in Guam is the Guam Men's Soccer League. Rovers FC and Guam Shipyard are the league's most competitive and successful clubs, both have won nine championships in the past years. 
Swimming.
In the 2012 Summer Olympics in London, Pilar Shimizu competed for Guam and placed 42nd in the breaststroke competition.
Rugby union.
Guam is represented in rugby union by the Guam national rugby union team. The team has never qualified for a Rugby World Cup. Guam played their first match in 2005, an 8–8 draw with India. Guam's biggest win was a 74–0 thrashing of Brunei in June 2008.

</doc>
<doc id="11979" url="https://en.wikipedia.org/wiki?curid=11979" title="Game Boy line">
Game Boy line

The Game Boy line is a line of battery-powered handheld game consoles sold by Nintendo. The product line sold 200 million units worldwide.
The original and Game Boy Color combined sold 118.69 million units worldwide. All versions of the Game Boy Advance combined have sold 81.51 million units. All Game Boy systems combined have sold 200.20 million units worldwide.
The Game Boy line was succeeded by the Nintendo DS line. A number of Game Boy, Game Boy Color, and Game Boy Advance games have been re-released digitally through the Nintendo 3DS Virtual Console and Wii U Virtual Console.
History.
Nintendo's Game Boy handheld was first released in 1989. The gaming device was the brainchild of long-time Nintendo employee Gunpei Yokoi, who was the person behind the "Ultra Hand", an expanding arm toy created and produced by Nintendo in 1970, long before Nintendo would enter the video game market. Yokoi was also responsible for the Game & Watch series of handhelds when Nintendo made the move from toys to video games.
When Yokoi designed the original Game Boy, he knew that to be successful, the system needed to be small, light, inexpensive, and durable, as well as have a varied, recognizable library of games upon its release. By following this simple mantra, the Game Boy line managed to gain a vast following despite technically superior alternatives which would have color graphics instead. This is also apparent in the name (conceived by Shigesato Itoi), which connotes a smaller "sidekick" companion to Nintendo's consoles.
Game Boy continues its success to this day and many at Nintendo have dedicated the handheld in Yokoi's memory. Game Boy celebrated its 15th anniversary in 2004, which nearly coincided with the 20-year anniversary of the original Nintendo Entertainment System (NES). To celebrate, Nintendo released the Classic NES Series and an NES controller-themed color scheme for the Game Boy Advance SP.
In 2006, Nintendo president Satoru Iwata said on the rumored demise of the Game Boy brand: "No, it's not true after all. What we are repeatedly saying is that for whichever platform, we are always conducting research and development for the new system, be it the Game Boy, or new console or whatever. And what we just told the reporter was that in thinking about the current situation where we are enjoying great sales with the DS and that we are now trying to launch the Wii, it's unthinkable for us to launch any new platform for the handheld system, including the new version of the GBA... Perhaps they misunderstood a part of this story, but as far as the handheld market is concerned now we really want to focus on more sales of the DS; that's all."
Game Boy family.
Game Boy.
The original gray Game Boy was first released in Japan on April 21, 1989. Based on a Z80 processor, it has a black and green reflective LCD screen, an eight-way directional pad, two action buttons (A and B), and Start and Select buttons. It plays games from ROM-based media contained in small plastic detachable units called cartridges (sometimes called carts or Game Paks).
The game that pushed the Game Boy into the upper reaches of success was "Tetris". Tetris was widely popular, and on the handheld format could be played anywhere. It came packaged with the Game Boy, and broadened its reach; adults and children alike were buying Game Boys in order to play "Tetris". Releasing "Tetris" on the Game Boy was selected as #4 on GameSpy's "25 Smartest Moments in Gaming".
The original Game Boy was one of the first cartridge-based systems that supported more than four players at one time (via the link port). In fact, it has been shown that the system could support 16 simultaneous players. However, this feature was only supported in "Faceball 2000".
Game Boy Pocket.
The Game Boy Pocket is a redesigned version of the original Game Boy having the same features released in 1996. Notably, this variation is smaller and lighter.
Another notable improvement over the original Game Boy is a black-and-white display screen, rather than the green-tinted display of the original Game Boy, that also featured improved response time for less blurring during motion. The Game Boy Pocket takes two AAA batteries for roughly ten hours of gameplay. The first model of the Game Boy Pocket did not have an LED to show battery levels, but the feature was added due to public demand.
Game Boy Light.
On April 1998, a variant of the Game Boy Pocket named Game Boy Light was exclusively released in Japan. The differences between the original Game Boy Pocket and the Game Boy Light is that the Game Boy Light takes on two AA batteries and has a backlit screen that can be turned on or off. This backlit screen allowed the use of it in darkened areas. The Game Boy Light was superseded by the Game Boy Color six months later.
Game Boy Color.
First released in Japan on October 21, 1998, the Game Boy Color (abbreviated as GBC) added a (slightly smaller) color screen to a form factor similar in size to the Game Boy Pocket. It also has double the processor speed, three times as much memory, and an infrared communications port. Technologically, it was likened to the 8-bit NES video game console from the 1980s although the Game Boy Color has a much larger color palette (56 simultaneous colors out of 32,768 possible).
A major component of the Game Boy Color is its near-universal backward compatibility; that is, a Game Boy Color is able to read older Game Boy cartridges and even play them in a selectable color palette. The only black & white Game Boy games known to be incompatible are "Road Rash" and "Joshua & the Battle of Jericho". Backwards compatibility became a major feature of the Game Boy line, since it allowed each new launch to begin with a significantly larger library than any of its competitors. Some games written specifically for the Game Boy Color can be played on older model Game Boys, whereas others cannot (see the Game Paks section for more information).
Game Boy Advance family.
Game Boy Advance.
In Japan, on March 21, 2001, Nintendo released a significant upgrade to the Game Boy line. The Game Boy Advance (also referred to as GBA) featured a 32 bit 16.8 MHz ARM. It included a Z80 processor and a switch activated by inserting a Game Boy or Game Boy Color game into the slot for backward compatibility, and had a larger, higher resolution screen. Controls were slightly modified with the addition of "L" and "R" shoulder buttons. The system was technically likened to the SNES and showed its power with successful ports of SNES titles such as "Super Mario World", ', and . There were also new titles that you will only find on GBA, such as ' and "", and more. A widely-criticized drawback of the Game Boy Advance is that the screen is not backlit, making viewing difficult in some conditions. The Game Paks for the GBA are roughly half the length of original Game Boy cartridges, and so older Game Paks would stick out of the top of the unit. When playing older games, the GBA provides the option to play the game at the standard equal square resolution of the original screen or the option to stretch it over the wider GBA screen.
Game Boy Advance SP.
First released in Japan on February 14, 2003, the Game Boy Advance SP—Nintendo model AGS-001—resolved several problems with the original Game Boy Advance model. It featured a new smaller clamshell design with a flip-up screen, a switchable internal frontlight, a rechargeable battery, and the only problem is the omission of the headphone jack, which requires a special adapter, purchased separately. In some regions, owners of the original Game Boy Advance received a special limited offer to trade their old models into Nintendo and merely pay the difference on the Game Boy Advance SP. In September 2005, Nintendo released a significantly improved Game Boy Advance SP model known as Nintendo model number AGS-101, that featured a high quality backlit screen instead of a frontlit, similar to the Game Boy Micro screen but larger.
Game Boy Micro.
The third form of Game Boy Advance system, the Game Boy Micro is four and a half inches wide (10 cm), two inches tall (5 cm), and weighs 2.8 ounces (80g). By far the smallest Game Boy created, it has approximately the same dimensions as an original NES controller pad. Its screen is approximately 2/3 the size of the SP and GBA screens while maintaining the same resolution (240×160 pixels) but now has a higher quality (than the original SP, not the improved SP) backlit display with adjustable brightness. Included with the system are two additional faceplates which can be swapped to give the system a new look; Nintendo of America used to sell additional faceplates on its online store. In Europe, the Game Boy Micro comes with a single faceplate. In Japan, a special "Mother 3" limited edition Game Boy Micro was released with the game in the "Mother 3 Deluxe Box". The Game Boy Micro is unable to play any original Game Boy or Game Boy Color games, only playing Game Boy Advance titles (with the exception of the Nintendo e-Reader, discontinued in America, but still available in Japan).
Game Paks.
Each video game is stored on a plastic cartridge, officially called a "Game Pak" by Nintendo. All cartridges, excluding those for Game Boy Advance, measure 5.8 by 6.5 cm. The cartridge provides the code and game data to the console's CPU. Some cartridges include a small battery with SRAM, flash memory chip, or EEPROM, which allows game data to be saved when the console is turned off. If the battery runs out in a cartridge, then the save data will be lost, however, it is possible to replace the battery with a new battery. To do this, the cartridge must be unscrewed, opened up, and the old battery would be removed and replaced. This may require desoldering the dead battery and soldering the replacement in place. Before 2003, Nintendo used round, flat watch batteries for saving information on the cartridges. These batteries were replaced in newer cartridges because they could only live for a certain amount of time.
The cartridge is inserted into the console cartridge slot. If the cartridge is removed while the power is on, and the Game Boy does not automatically reset, the game freezes; the Game Boy may exhibit unexpected behavior, such as rows of zeros appearing on the screen, the sound remaining at the same pitch as was emitted the instant the game was pulled out, saved data may be corrupted, and hardware may be damaged. This applies to most video game consoles that use cartridges.
The original Game Boy power switch was designed to prevent the player from being able to remove the cartridge while the power is on. Cartridges intended only for Game Boy Color (and not for the original Game Boy) lack the "notch" for the locking mechanism present in the top of the original cartridges, preventing operation on an original Game Boy (the cartridge can be inserted, but the power switch cannot be moved to the "on" position). Even if this was bypassed by using a Game Boy Pocket, Game Boy Light, or Super Game Boy, the game would not run, and an image on the screen would inform the user that the game is only compatible with Game Boy Color systems. One exception would be the Kirby Tilt 'n' Tumble game: despite the game cartridge featuring a notch, enabling it to be inserted on the original Game Boy, the game displays an error message indicating that it only plays on Game Boy Color.
Game Boy Advance cartridges used a similar physical lock-out feature. Notches were located at the base of the cartridge's two back corners. One of these notches was placed as to avoid pressing a switch inside the cartridge slot. When an older Game Boy or Game Boy Color game was inserted into the cartridge slot, the switch would be pressed down and the Game Boy Advance would start in Game Boy Color mode, while a Game Boy Advance cartridge would not touch the switch and the system would start in Game Boy Advance mode. The Nintendo DS replaced the switch with a solid piece of plastic that would allow Game Boy Advance cartridges to be inserted into Slot 2, but would prevent an older Game Boy cartridge from being inserted fully into the slot.
Excluding game-specific variations, there are four types of cartridges compatible with Game Boy systems:
Accessories.
The Game Boy, as with many other consoles, has had a number of releases from both first-party and unlicensed third-party accessories. The most notable being the Game Boy Camera (left) and the Game Boy Printer (right), which were released in 1998.
Television adapters.
In addition to the Game Boy, special hardware has been released for various handhelds in the Game Boy line so they can be played on a television set.
Super Game Boy.
In 1994, a special adapter cartridge for Nintendo's Super Nintendo Entertainment System (SNES) was released called the Super Game Boy. The Super Game Boy allows game cartridges designed for use on the Game Boy to be played on a TV display using the SNES/Super Famicom controllers. When it was released in 1994, the Super Game Boy sold for about $60 in the United States. In the United Kingdom, it retailed for £49.99. The Super Game Boy's technical architecture is similar to that of a regular Game Boy, thus Game Boy games functioned on the native hardware rather than being emulated by the SNES. It was the precursor to the Game Boy Player on the Nintendo GameCube, which functioned in a similar manner.
Game Boy Player.
The Game Boy Player is a device released in 2003 by Nintendo for the GameCube which enables Game Boy (although Super Game Boy enhancements are ignored), Game Boy Color, or Game Boy Advance cartridges to be played on a television. It connects via the high speed parallel port at the bottom of the GameCube and requires use of a boot disc to access the hardware. Unlike devices such as Datel's Advance Game Port, the Game Boy Player does not use software emulation, but instead uses physical hardware nearly identical to that of a Game Boy Advance.
Reception.
Approximately two thousand games are available for the Game Boy, which can be attributed in part to its sales in the amount of millions, a well-documented design, and a typically short development cycle.
The Nintendo DS and Nintendo DS Lite are able to play the large library of Game Boy Advance games (though the Nintendo DSi, Nintendo DSi XL, Nintendo 3DS, and Nintendo 2DS lack a GBA game cartridge slot). However, the DS consoles do not have a GBA game link connector, and so cannot play multiplayer GBA games (except for the few that are multiplayer on a single GBA) or link to the GameCube. The DS is not backward-compatible with Game Paks for the original Game Boy or the Game Boy Color. With homebrew development on the Nintendo DS, full speed Game Boy and Game Boy Color emulation has been achieved as well as the ability to scale the smaller Game Boy screen image to the full DS screen.
Legacy.
Numerous musical acts have appropriated the Game Boy as a musical instrument (Game Boy music), using software such as nanoloop or Little Sound DJ.
Certain games released for the Game Boy and Game Boy Color handheld consoles are available via the Virtual Console service on the Nintendo 3DS. Game Boy Advance games were thought to be as well due to the 3DS not being compatible, but it was just a mistranslation. However, ten Game Boy Advance games will be released to Nintendo 3DS ambassadors, as in Nintendo 3DS owners who logged into the 3DS eShop before the major August 2011 price drop. The Virtual Console GBA features of releases are limited, and there are no plans to release them to the public. However, starting from April 2014, Nintendo has been releasing Game Boy Advance games as Virtual Console titles via the Nintendo eShop for the Wii U.

</doc>
<doc id="11982" url="https://en.wikipedia.org/wiki?curid=11982" title="Gemini 10">
Gemini 10

Gemini 10 (officially Gemini X) was a 1966 manned spaceflight in NASA's Gemini program. It was the 8th manned Gemini flight, the 16th manned American flight and the 24th spaceflight of all time (includes X-15 flights over ).
Objectives.
Gemini 10 established that radiation at high altitude was not a problem. After docking with their Agena booster in low orbit, Young and Collins used it to climb another to meet with the dead, drifting Agena left over from the aborted Gemini 8 flight—thus executing the program's first double rendezvous. With no electricity on board the second Agena, the rendezvous was accomplished with eyes only—no radar. After the rendezvous, Collins spacewalked over to the dormant Agena at the end of a tether, making Collins the first person to meet another spacecraft in orbit. He retrieved a cosmic dust-collecting panel from the side of the Agena, but was not able to take any pictures; in the complicated business of keeping his tether clear of the Gemini and Agena, his Hasselblad camera worked itself free and drifted away.
Gemini 10 was designed to achieve the objectives planned for the last two missions—rendezvous, docking and EVA. As well as this it was also hoped to dock with the Agena Target Vehicle from the Gemini 8 mission. This Agena's battery power had failed many months earlier and this would demonstrate the ability to rendezvous with a dormant object. It would be also the first mission to fire the Agena's own rocket, allowing them to reach higher orbits.
Flight.
The Agena launched perfectly for the second time, after problems had occurred with the targets for Gemini 6 and 9. Gemini 10 followed 100 minutes later and entered a orbit. They were behind the Agena. Two anomalous events occurred during the launch. At liftoff, a propellant fill umbilical became snared with its release lanyard. It ripped out of the LC-19 service tower and remained attached to the second stage during ascent. Tracking camera footage also showed that the first stage oxidizer tank dome ruptured after staging and released a cloud of nitrogen tetroxide. The telemetry package on the first stage had been disabled at staging, so visual evidence was the only data available. Film review of Titan II ICBM launches found at least seven other instances of post-staging tank rupture, most likely caused by flying debris, second stage engine exhaust, or structural bending. NASA finally decided that this phenomenon did not pose any safety risk to the astronauts and no corrective action had to be taken.
First rendezvous.
Collins discovered that he was unable to use the sextant for navigation as it did not seem to work as expected. At first he mistook airglow as the real horizon when trying to make some fixes on stars. Then the image didn't seem right. He tried another instrument that they had on board but this was not practical to use as it had a very small field of view.
They fortunately had a backup in the form of the computers on the ground. They made their first burn to put them into a orbit. However Young didn't realize that during the next burn, he had the spacecraft turned slightly, which meant that they introduced an out-of-plane error. This meant two extra burns were necessary, and by the time they had docked with the Agena, 60% of their fuel had been consumed. It was decided to keep the Gemini docked to the Agena as long as possible, as this would mean that they could use the fuel on board the Agena for attitude control.
The first burn of the Agena engine lasted 80 seconds and put them in a orbit. This was the highest a person had ever been (until the next mission when Gemini 11 went to over ). This burn was quite a ride for the crew. Because the Gemini and Agena docked nose-to-nose, the forces experienced were "eyeballs out" as opposed to "eyeballs in" for a launch from Earth. The crew took a couple of pictures when they reached apogee but were more interested in what was going on in the spacecraft — checking the systems and watching the radiation dosage meter.
After this they had their sleep period which lasted for eight hours and then they were ready for another busy day. The crew's first order of business was to make a second burn with the Agena engine to put them into the same orbit as the Gemini 8 Agena. This was at 20:58 UTC on July 19 and lasted 78 seconds and took off their speed, putting them into a orbit. They made one more burn of the Agena to circularize their orbit to .
EVA 1.
It was now time for the first of two EVAs on Gemini 10. This was to be just a standup EVA, where Collins would 'stand' in the open hatch and take some photographs of stars as part of experiment S-13. They used a 70 mm general purpose camera to image the Southern Milky Way in ultraviolet. After orbital sunrise, Collins then photographed a color plate on the side of the spacecraft (MSC-8) to see whether film reproduced colors accurately in space. They reentered the spacecraft six minutes early when they both found their eyes were irritated, which was caused by a minor leak of lithium hydroxide in the astronauts' oxygen supply. After repressurizing the cabin, they ran the oxygen at high rates and flushed the environment system.
After the exercise of the EVA Young and Collins slept in their second 'night' in space. The next 'morning' they started preparing for the second rendezvous and another EVA.
Second rendezvous.
After undocking from their Agena, the crew thought they sighted the Gemini 8 Agena. It however turned out to be their own Agena away, while their target was away. It wasn't until just over away that they saw it as a faint star. After a few more correction burns, they were station-keeping away from the Gemini 8 Agena. They found the Agena to be very stable and in good condition.
EVA 2.
At 48 hours and 41 minutes into the mission, the second EVA began. Collins' first task was to retrieve a Micrometeorite Collector (S-12) from the side of the spacecraft. This he accomplished with some difficulty (similar to that encountered by Eugene Cernan on Gemini 9A). However, the collector floated out of the cabin some time later during the EVA and was lost.
He next traveled over to the Agena and tried to grab onto the docking cone but found this impossible as it was smooth and had no grip. Collins used a nitrogen-propelled Hand-Held Maneuvering Unit (HHMU) to move himself towards the Gemini and then back to the Agena. This time he was able to grab hold of some wire bundles and retrieved the Micrometeorite Collector (S-10) from the Agena. He decided against replacing it as he could lose the one he had just retrieved.
His last task on this EVA was to test out the HHMU. However this stopped working and meant they finished the EVA after only 39 minutes. During this time, it took the crew eight minutes to close the hatch as they had some difficulty with the umbilical. It was jettisoned along with the chestpack used by Collins an hour later when they opened the hatch for the third and final time.
Experiments.
There were ten other experiments that the crew performed during the mission. Three were interested in radiation: MSC-3 was the Tri-Axis Magnetometer which measured levels in the South Atlantic Anomaly. There was also MSC-6, a beta spectrometer, which measured potential radiation doses for Apollo missions, and MSC-7, a bremsstrahlung spectrometer which detected radiation flux as a function of energy when the spacecraft passed through the South Atlantic Anomaly.
S-26 investigated the ion and electron wake of the spacecraft. This provided limited results due to the lack of fuel for attitude control, but found that electron and ion temperatures were higher than expected and it registered shock effects during docking and undocking.
The S-5 and S-6 experiments were performed, which were previously carried on Gemini 9A; these were Synoptic Terrain and Synoptic Weather photography respectively. There was also S-1 which was intended to image the Zodiacal light. All of these experiments were of little use as the film used was only half as sensitive as Gemini 9A and the dirty windows lowered the transmission of light by a factor of six.
The crew also tried to perform D-5, a navigation experiment. They were only able to track five stars, with six needed for accurate measurements. The last experiment, D-10, was to investigate an ion-sensing attitude control system. This experiment measured the attitude of the spacecraft from the flow of ions and electrons around the spacecraft in orbit. The results from this experiment showed the system to be accurate and responsive.
Re-entry.
The last day of the mission was short and retrofire came at 70 hours and 10 minutes into the mission. They landed only away from the intended landing site and were recovered by the USS "Guadalcanal".
The Gemini 10 mission was supported by the following U.S. Department of Defense resources; 9,067 personnel, 78 aircraft and 13 ships.
Insignia.
The patch is simple in design but highly symbolic. The main feature is a large X with a Gemini and Agena orbiting around it. The two stars have a variety of meanings: the two rendezvous attempts, Castor and Pollux in Gemini or the two crew members. This is one of the few crew patches without the crew's name. It is able to be displayed "upside down" but is correctly shown with the spacecraft to the right. It was designed by Young's first wife, Barbara.
Spacecraft location.
For many years the spacecraft was the centerpiece of a space exhibition at Norsk Teknisk Museum, Oslo, Norway. It was returned on request in 2002.
The spacecraft is currently on display at the Kansas Cosmosphere and Space Center, Hutchinson, Kansas. When the restoration of the Gemini 6A spacecraft is completed, then Gemini 10 will be restored in full view of the public. At the end of this restoration it will be put back on full display at the Cosmosphere. One of the hatches is displayed at Virginia Air and Space Center, Hampton, Virginia.

</doc>
<doc id="11984" url="https://en.wikipedia.org/wiki?curid=11984" title="Gardening">
Gardening

Gardening is the practice of growing and cultivating plants as part of horticulture. In gardens, ornamental plants are often grown for their flowers, foliage, or overall appearance; useful plants, such as root vegetables, leaf vegetables, fruits, and herbs, are grown for consumption, for use as dyes, or for medicinal or cosmetic use. Gardening is considered to be a relaxing activity for many people.
Gardening ranges in scale from fruit orchards, to long boulevard plantings with one or more different types of shrubs, trees, and herbaceous plants, to residential yards including lawns and foundation plantings, to plants in large or small containers grown inside or outside. Gardening may be very specialized, with only one type of plant grown, or involve a large number of different plants in mixed plantings. It involves an active participation in the growing of plants, and tends to be labor-intensive, which differentiates it from farming or forestry.
History.
Ancient times.
Forest gardening, a forest-based food production system, is the world's oldest form of gardening. Forest gardens originated in prehistoric times along jungle-clad river banks and in the wet foothills of monsoon regions. In the gradual process of families improving their immediate environment, useful tree and vine species were identified, protected and improved while undesirable species were eliminated. Eventually foreign species were also selected and incorporated into the gardens.
After the emergence of the first civilizations, wealthy individuals began to create gardens for aesthetic purposes. Egyptian tomb paintings from around 1500 BC provide some of the earliest physical evidence of ornamental horticulture and landscape design; they depict lotus ponds surrounded by symmetrical rows of acacias and palms. A notable example of ancient ornamental gardens were the Hanging Gardens of Babylon—one of the Seven Wonders of the Ancient World —while ancient Rome had dozens of gardens.
Wealthy ancient Egyptians used gardens for providing shade. Egyptians associated trees and gardens with gods as they believed that their deities were pleased by gardens. Gardens in ancient Egypt were often surrounded by walls with trees planted in rows. Among the most popular species planted were date palms, sycamores, fir trees, nut trees, and willows. These gardens were a sign of higher socioeconomic status. In addition, wealthy ancient Egyptians grew vineyards, as wine was a sign of the higher social classes. Roses, poppies, daisies and irises could all also be found in the gardens of the Egyptians.
The Assyrians were also renowned for their beautiful gardens. These tended to be wide and large, some of them used for hunting game—rather like a game reserve today—and others as leisure gardens. Cypresses and palms were some of the most frequently planted types of trees.
Ancient Roman gardens were laid out with hedges and vines and contained a wide variety of flowers—acanthus, cornflowers, crocus, cyclamen, hyacinth, iris, ivy, lavender, lilies, myrtle, narcissus, poppy, rosemary and violets—as well as statues and sculptures. Flower beds were popular in the courtyards of rich Romans.
The Middle Ages.
The Middle Age represented a period of decline in gardens for aesthetic purposes, in what concerns gardening. After the fall of Rome, gardening was done for the purpose of growing medicinal herbs and/or decorating church altars. Monasteries carried on a tradition of garden design and intense horticultural techniques during the medieval period in Europe. 
Generally, monastic garden types consisted of kitchen gardens, infirmary gardens, cemetery orchards, cloister garths and vineyards. Individual monasteries might also have had a "green court", a plot of grass and trees where horses could graze, as well as a cellarer's garden or private gardens for obedientiaries, monks who held specific posts within the monastery.
Islamic gardens were built after the model of Persian gardens and they were usually enclosed by walls and divided in 4 by watercourses. Commonly, the center of the garden would have a pool or pavilion. Specific to the Islamic gardens are the mosaics and glazed tiles used to decorate the rills and fountains that were built in these gardens.
By the late 13th century, rich Europeans began to grow gardens for leisure and for medicinal herbs and vegetables. They surrounded the gardens by walls to protect them from animals and to provide seclusion. During the next two centuries, Europeans started planting lawns and raising flowerbeds and trellises of roses. Fruit trees were common in these gardens and also in some, there were turf seats. At the same time, the gardens in the monasteries were a place to grow flowers and medicinal herbs but they were also a space where the monks could enjoy nature and relax.
The gardens in the 16th and 17th century were symmetric, proportioned and balanced with a more classical appearance. Most of these gardens were built around a central axis and they were divided into different parts by hedges. Commonly, gardens had flowerbeds laid out in squares and separated by gravel paths.
Gardens in Renaissance were adorned with sculptures, topiary and fountains. In the 17th century, knot gardens became popular along with the hedge mazes. By this time, Europeans started planting new flowers such as tulips, marigolds and sunflowers.
Cottage gardens.
Cottage gardens, which emerged in Elizabethan times, appear to have originated as a local source for herbs and fruits. One theory is that they arose out of the Black Death of the 1340s, when the death of so many laborers made land available for small cottages with personal gardens. According to the late 19th-century legend of origin, these gardens were originally created by the workers that lived in the cottages of the villages, to provide them with food and herbs, with flowers planted among them for decoration. Farm workers were provided with cottages that had architectural quality set in a small garden—about an acre—where they could grow food and keep pigs and chickens.
Authentic gardens of the yeoman cottager would have included a beehive and livestock, and frequently a pig and sty, along with a well. The peasant cottager of medieval times was more interested in meat than flowers, with herbs grown for medicinal use rather than for their beauty. By Elizabethan times there was more prosperity, and thus more room to grow flowers. Even the early cottage garden flowers typically had their practical use—violets were spread on the floor (for their pleasant scent and keeping out vermin); calendulas and primroses were both attractive and used in cooking. Others, such as sweet william and hollyhocks, were grown entirely for their beauty.
18th century.
In the 18th century, gardens were laid out more naturally, without any walls. This style of smooth undulating grass, which would run straight to the house, clumps, belts and scattering of trees and his serpentine lakes formed by invisibly damming small rivers, were a new style within the English landscape, a "gardenless" form of landscape gardening, which swept away almost all the remnants of previous formally patterned styles. The English garden usually included a lake, lawns set against groves of trees, and often contained shrubberies, grottoes, pavilions, bridges and follies such as mock temples, Gothic ruins, bridges, and other picturesque architecture, designed to recreate an idyllic pastoral landscape. This new style emerged in England in the early 18th century, and spread across Europe, replacing the more formal, symmetrical Garden à la française of the 17th century as the principal gardening style of Europe. The English garden presented an idealized view of nature. They were often inspired by paintings of landscapes by Claude Lorraine and Nicolas Poussin, and some were Influenced by the classic Chinese gardens of the East, which had recently been described by European travelers. The work of Lancelot 'Capability' Brown was particularly influential. Also, in 1804 the Horticultural Society was formed. Gardens of the 19th century contained plants such as the monkey puzzle or Chile pine. This is also the time when the so-called "gardenesque" style of gardens evolved. These gardens displayed a wide variety of flowers in a rather small space. Rock gardens increased in popularity in the 19th century.
Types.
Residential gardening takes place near the home, in a space referred to as the garden. Although a garden typically is located on the land near a residence, it may also be located on a roof, in an atrium, on a balcony, in a windowbox, or on a patio or vivarium.
Gardening also takes place in non-residential green areas, such as parks, public or semi-public gardens (botanical gardens or zoological gardens), amusement and amusement parks, along transportation corridors, and around tourist attractions and garden hotels. In these situations, a staff of gardeners or groundskeepers maintains the gardens.
Garden features and accessories.
There is a wide range of features and accessories available in the market for both the professional gardener and the amateur to exercise their creativity. These are used to add decoration or functionality, and may be made from a wide range of materials such as copper, stone, wood, bamboo, stainless steel, clay, stained glass, concrete, or iron. Examples include trellis, arbors, statues, benches, water fountains, urns, bird baths and feeders, and garden lighting such as candle lanterns and oil lamps. The use of these items can be part of the expression of a gardener's gardening personality.
Gardening departments and centers.
Gardening departments and centers mainly sell plants, sundries, and garden accessories, but in recent times, many now stock outdoor leisure products as diverse as spas, furniture, and barbecues. Many garden centers now include food halls, and sections for clothing, gifts, pets, and power tools. There are also a number of online garden centers that now deliver directly to customers' doors.
Comparison with farming.
Gardening for beauty is likely nearly as old as farming for food, however for most of history for the majority of people there was no real distinction since the need for food and other useful product trumped other concerns. Small-scale, subsistence agriculture (called hoe-farming) is largely indistinguishable from gardening. A patch of potatoes grown by a Peruvian peasant or an Irish smallholder for personal use could be described as either a garden or a farm. Gardening for average people evolved as a separate discipline, more concerned with aesthetics and recreation, under the influence of the pleasure gardens of the wealthy. Meanwhile, farming has evolved (in developed countries) in the direction of commercialization, economics of scale, and monocropping.
In respect to its food producing purpose, gardening is distinguished from farming chiefly by scale and intent. Farming occurs on a larger scale, and with the production of salable goods as a major motivation. Gardening is done on a smaller scale, primarily for pleasure and to produce goods for the gardener's own family or community. There is some overlap between the terms, particularly in that some moderate-sized vegetable growing concerns, often called market gardening, can fit in either category.
The key distinction between gardening and farming is essentially one of scale; gardening can be a hobby or an income supplement, but farming is generally understood as a full-time or commercial activity, usually involving more land and quite different practices. One distinction is that gardening is labor-intensive and employs very little infrastructural capital, sometimes no more than a few tools, e.g. a spade, hoe, basket and watering can. By contrast, larger-scale farming often involves irrigation systems, chemical fertilizers and harvesters or at least ladders, e.g. to reach up into fruit trees. However, this distinction is becoming blurred with the increasing use of power tools in even small gardens.
In part because of labor intensity and aesthetic motivations, gardening is very often much more productive per unit of land than farming. In the Soviet Union, half the food supply came from small peasants' garden plots on the huge government-run collective farms, although they were tiny patches of land. Some argue this as evidence of the superiority of capitalism, since the peasants were generally able to sell their produce. Others consider it to be evidence of a tragedy of the commons, since the large collective plots were often neglected, or fertilizers or water redirected to the private gardens.
The term precision agriculture is sometimes used to describe gardening using intermediate technology (more than tools, less than harvesters), especially of organic varieties. Gardening is effectively scaled up to feed entire villages of over 100 people from specialized plots. A variant is the community garden which offers plots to urban dwellers; see further in allotment (gardening).
Gardens as art.
Garden design is considered to be an art in most cultures, distinguished from gardening, which generally means "garden maintenance". Garden design can include different themes such as perennial, butterfly, wildlife, Japanese, water, tropical, or shade gardens. In Japan, Samurai and Zen monks were often required to build decorative gardens or practice related skills like flower arrangement known as "ikebana". In 18th-century Europe, country estates were refashioned by landscape gardeners into formal gardens or landscaped park lands, such as at Versailles, France, or Stowe, England. Today, landscape architects and garden designers continue to produce artistically creative designs for private garden spaces. In the USA, professional landscape designers are certified by the Association of Professional Landscape Designers.
Social aspects.
People can express their political or social views in gardens, intentionally or not. The lawn vs. garden issue is played out in urban planning as the debate over the "land ethic" that is to determine urban land use and whether hyper hygienist bylaws (e.g. weed control) should apply, or whether land should generally be allowed to exist in its natural wild state. In a famous Canadian Charter of Rights case, "Sandra Bell vs. City of Toronto", 1997, the right to cultivate all native species, even most varieties deemed noxious or allergenic, was upheld as part of the right of free expression.
Community gardening comprises a wide variety of approaches to sharing land and gardens.
People often surround their house and garden with a hedge. Common hedge plants are privet, hawthorn, beech, yew, leyland cypress, hemlock, arborvitae, barberry, box, holly, oleander, forsythia and lavender. The idea of open gardens without hedges may be distasteful to those who enjoy privacy.
The Slow Food movement has sought in some countries to add an edible school yard and garden classrooms to schools, e.g. in Fergus, Ontario, where these were added to a public school to augment the kitchen classroom. Garden sharing, where urban landowners allow gardeners to grow on their property in exchange for a share of the harvest, is associated with the desire to control the quality of one's food, and reconnect with soil and community.
In US and British usage, the production of ornamental plantings around buildings is called "landscaping", "landscape maintenance" or "grounds keeping", while international usage uses the term "gardening" for these same activities.
Also gaining popularity is the concept of "Green Gardening" which involves growing plants using organic fertilizers and pesticides so that the gardening process - or the flowers and fruits produced thereby - doesn't adversely affect the environment or people's health in any manner.
Garden pests.
Garden pests are generally plants, fungi, or animals (frequently insects) that engages in activity that the gardener considers undesirable. It may crowd out desirable plants, disturb soil, stunt the growth of young seedlings, steal or damage fruit, or otherwise kill plants, hamper their growth, damage their appearance, or reduce the quality of the edible or ornamental portions of the plant. Aphids, spider mites, slugs, snails, ants, birds, and even cats are commonly considered to be garden pests.
Because gardeners may have different goals, organisms considered "garden pests" vary from gardener to gardener. "Tropaeolum speciosum", for example, may be considered a desirable and ornamental garden plant, or it may be considered a pest if it seeds and starts to grow where it is not wanted. As another example, in lawns, moss can become dominant and be impossible to eradicate. In some lawns, lichens, especially very damp lawn lichens such as "Peltigera lactucfolia" and "P. membranacea", can become difficult to control and be considered pests.
Garden pest control.
There are many ways by which unwanted pests are removed from a garden. The techniques vary depending on the pest, the gardener's goals, and the gardener's philosophy. For example, snails may be dealt with through the use of a chemical pesticide, an organic pesticide, hand-picking, barriers, or simply growing snail-resistant plants.
Pest control is often done through the use of pesticides, which may be either organic or artificially synthesized. Pesticides may affect the ecology of a garden due to their effects on the populations of both target and non-target species. For example, unintended exposure to some neonicotinoid pesticides has been proposed as a factor in the recent decline in honey bee populations. A mole vibrator can deter mole activity in a garden.
Other means of control include the removal of infected plants, using fertilizers and biostimulants to improve the health and vigour of plants so they better resist attack, practising crop rotation to prevent pest build-up, using companion planting, and practising good garden hygiene, such as disinfecting tools and clearing debris and weeds which may harbour pests.

</doc>
<doc id="11985" url="https://en.wikipedia.org/wiki?curid=11985" title="Graffiti">
Graffiti

Graffiti (plural of "graffito": "a graffito", but "these graffiti") are writing or drawings that have been scribbled, scratched, or painted illicitly on a wall or other surface, often within public view. Graffiti range from simple written words to elaborate wall paintings, and they have existed since ancient times, with examples dating back to Ancient Egypt, Ancient Greece, and the Roman Empire.
In modern times, paint (particularly spray paint) and marker pens have become the most commonly used graffiti materials. In most countries, marking or painting property without the property owner's permission is considered defacement and vandalism, which is a punishable crime.
Graffiti may also express underlying social and political messages and a whole genre of artistic expression is based upon spray paint graffiti styles. Within hip hop culture, graffiti have evolved alongside hip hop music, b-boying, and other elements. Unrelated to hip-hop graffiti, gangs use their own form of graffiti to mark territory or to serve as an indicator of gang-related activities.
Controversies that surround graffiti continue to create disagreement amongst city officials, law enforcement, and writers who wish to display and appreciate work in public locations. There are many different types and styles of graffiti; it is a rapidly developing art form whose value is highly contested and reviled by many authorities while also subject to protection, sometimes within the same jurisdiction.
Etymology.
Both "graffiti" and its occasional singular form "graffito" are from the Italian word "graffiato" ("scratched"). "Graffiti" is applied in art history to works of art produced by scratching a design into a surface. A related term is "sgraffito", which involves scratching through one layer of pigment to reveal another beneath it. This technique was primarily used by potters who would glaze their wares and then scratch a design into it. In ancient times graffiti were carved on walls with a sharp object, although sometimes chalk or coal were used. The word originates from Greek — "graphein" — meaning "to write."
History.
The term "graffiti" referred to the inscriptions, figure drawings, and such, found on the walls of ancient sepulchres or ruins, as in the Catacombs of Rome or at Pompeii. Use of the word has evolved to include any graphics applied to surfaces in a manner that constitutes vandalism.
The only known source of the Safaitic language, a form of proto-Arabic, is from graffiti: inscriptions scratched on to the surface of rocks and boulders in the predominantly basalt desert of southern Syria, eastern Jordan and northern Saudi Arabia. Safaitic dates from the first century BC to the fourth century AD.
Modern-style graffiti.
The first known example of "modern style" graffiti survives in the ancient Greek city of Ephesus (in modern-day Turkey). Local guides say it is an advertisement for prostitution. Located near a mosaic and stone walkway, the graffiti shows a handprint that vaguely resembles a heart, along with a footprint and a number. This is believed to indicate that a brothel was nearby, with the handprint symbolizing payment.
The ancient Romans carved graffiti on walls and monuments, examples of which also survive in Egypt. Graffiti in the classical world had different connotations than they carry in today's society concerning content. Ancient graffiti displayed phrases of love declarations, political rhetoric, and simple words of thought, compared to today's popular messages of social and political ideals
The eruption of Vesuvius preserved graffiti in Pompeii, which includes Latin curses, magic spells, declarations of love, alphabets, political slogans, and famous literary quotes, providing insight into ancient Roman street life. One inscription gives the address of a woman named Novellia Primigenia of Nuceria, a prostitute, apparently of great beauty, whose services were much in demand. Another shows a phallus accompanied by the text, "mansueta tene" ("handle with care").
Disappointed love also found its way onto walls in antiquity:
<poem>Quisquis amat. veniat. Veneri volo frangere costas
fustibus et lumbos debilitare deae.
Si potest illa mihi tenerum pertundere pectus
Whoever loves, go to hell. I want to break Venus's ribs
with a club and deform her hips.
If she can break my tender heart
why can't I hit her over the head?
Ancient tourists visiting the 5th century citadel at Sigiriya in Sri Lanka scribbled over 1800 individual graffiti there between 6th and 18th centuries. Etched on the surface of the Mirror Wall, they contain pieces of prose, poetry, and commentary. The majority of these visitors appear to have been from the elite of society: royalty, officials, professions, and clergy. There were also soldiers, archers, and even some metalworkers. The topics range from love to satire, curses, wit, and lament. Many demonstrate a very high level of literacy and a deep appreciation of art and poetry. Most of the graffiti refer to the frescoes of semi-nude females found there. One reads:
<poem>Wet with cool dew drops
fragrant with perfume from the flowers
came the gentle breeze
jasmine and water lily
dance in the spring sunshine
side-long glances
of the golden hued ladies
stab into my thoughts
heaven itself cannot take my mind
as it has been captivated by one lass
among the five hundred I have seen here.</poem>
Among the ancient political graffiti examples were Arab satirist poems. Yazid al-Himyari, an Umayyad Arab and Persian poet, was most known for writing his political poetry on the walls between Sajistan and Basra, manifesting a strong hatred towards the Umayyad regime and its "walis", and people used to read and circulate them very widely.
Literacy or illiteracy often revealed in graffiti.
Historic forms of graffiti have helped gain understanding into the lifestyles and languages of past cultures. Errors in spelling and grammar in these graffiti offer insight into the degree of literacy in Roman times and provide clues on the pronunciation of spoken Latin. Examples are "CIL" IV, 7838: "Vettium Firmum / aed""quactiliar"[ii "rog". Here, "qu" is pronounced "co." The 83 pieces of graffiti found at "CIL" IV, 4706-85 are evidence of the ability to read and write at levels of society where literacy might not be expected. The graffiti appear on a peristyle which was being remodeled at the time of the eruption of Vesuvius by the architect Crescens. The graffiti were left by both the foreman and his workers. The brothel at "CIL" VII, 12, 18–20 contains more than 120 pieces of graffiti, some of which were the work of the prostitutes and their clients. The gladiatorial academy at "CIL" IV, 4397 was scrawled with graffiti left by the gladiator Celadus Crescens ("Suspirium puellarum Celadus thraex": "Celadus the Thracian makes the girls sigh.")
Another piece from Pompeii, written on a tavern wall about the owner of the establishment and his questionable wine:
<poem>Landlord, may your lies malign
Bring destruction on your head!
You yourself drink unmixed wine,
Water you sell your guests instead.</poem>
It was not only the Greeks and Romans who produced graffiti: the Maya site of Tikal in Guatemala contains examples of ancient Maya graffiti. Viking graffiti survive in Rome and at Newgrange Mound in Ireland, and a Varangian scratched his name (Halvdan) in runes on a banister in the Hagia Sophia at Constantinople. These early forms of graffiti have contributed to the understanding of lifestyles and languages of past cultures.
Graffiti, known as Tacherons, were frequently scratched on Romanesque Scandinavian church walls.
When Renaissance artists such as Pinturicchio, Raphael, Michelangelo, Ghirlandaio, or Filippino Lippi descended into the ruins of Nero's Domus Aurea, they carved or painted their names and returned to initiate the "grottesche" style of decoration.
There are also examples of graffiti occurring in American history, such as Signature Rock, a national landmark along the Oregon Trail.
Later, French soldiers carved their names on monuments during the Napoleonic in the 1790s. Lord Byron's survives on one of the columns of the Temple of Poseidon at Cape Sounion in Attica, Greece.
Contemporary graffiti.
Graffiti writing is often seen as having become intertwined with hip hop culture and the myriad international styles derived from New York City Subway graffiti. However, there are many other instances of notable graffiti in the twentieth century. Graffiti have long appeared on building walls, in latrines, railroad boxcars, subways, and bridges. The example with the longest known history, dating back to the 1920s and continuing into the present day, is Texino.
Some graffiti have their own poignancy. In World War II, an inscription on a wall at the fortress of Verdun was seen as an illustration of the US response twice in a generation to the wrongs of the Old World:
<poem>Austin White – Chicago, Ill – 1918
Austin White – Chicago, Ill – 1945
This is the last time I want to write my name here.</poem>
During World War II and for decades after, the phrase "Kilroy was here" with an accompanying illustration was widespread throughout the world, due to its use by American troops and ultimately filtering into American popular culture. Shortly after the death of Charlie Parker (nicknamed "Yardbird" or "Bird"), graffiti began appearing around New York with the words "Bird Lives". The student protests and general strike of May 1968 saw Paris bedecked in revolutionary, anarchistic, and situationist slogans such as "L'ennui est contre-révolutionnaire" ("Boredom is counterrevolutionary") expressed in painted graffiti, poster art, and stencil art. At the time in the US, other political phrases (such as "Free Huey" about Black Panther Huey Newton) became briefly popular as graffiti in limited areas, only to be forgotten. A popular graffito of the 1970s was the legend "Dick Nixon Before He Dicks You", reflecting the hostility of the youth culture to that US president.
Advent of aerosol paint.
Rock and roll graffiti is a significant subgenre. A famous graffito of the twentieth century was the inscription in the London tube reading "Clapton is God" in a link to the guitarist Eric Clapton. The phrase was spray-painted by an admirer on a wall in an Islington station on the Underground in the autumn of 1967. The graffito was captured in a photograph, in which a dog is urinating on the wall.
Graffiti also became associated with the anti-establishment punk rock movement beginning in the 1970s. Bands such as Black Flag and Crass (and their followers) widely stenciled their names and logos, while many punk night clubs, squats, and hangouts are famous for their graffiti. In the late 1980s the upside down Martini glass that was the tag for punk band Missing Foundation was the most ubiquitous graffito in lower Manhattan, and was copied by hard core punk fans throughout the US and West Germany.
Along similar lines was the legend "Frodo Lives", referring to the protagonist of "The Lord of the Rings".
Spread of hip hop culture.
In 1979, graffiti artist Lee Quinones and Fab 5 Freddy were given a gallery opening in Rome by art dealer Claudio Bruni. For many outside of New York, it was their first encounter with their art form. Fab 5 Freddy's friendship with Debbie Harry influenced Blondie's single "Rapture" (Chrysalis, 1981), the video of which featured Jean-Michel Basquiat, and offered many their first glimpse of a depiction of elements of graffiti in hip hop culture. JaJaJa toured Germany, Switzerland, Belgium, and Holland with a large graffiti canvas as a backdrop. Charlie Ahearn's independently released fiction film "Wild Style" (Wild Style, 1983), the early PBS documentary "Style Wars" (1983), hit songs such as "The Message" and "Planet Rock" and their accompanying music videos (both 1982) contributed to a growing interest outside New York in all aspects of hip hop.
"Style Wars" depicted not only famous graffiti artists such as Skeme, Dondi, MinOne, and ZEPHYR, but also reinforced graffiti's role within New York's emerging hip-hop culture by incorporating famous early break-dancing groups such as Rock Steady Crew into the film and featuring rap in the soundtrack. Style Wars is still recognized as the most prolific film representation of what was going on within the young hip hop culture of the early 1980s. Fab 5 Freddy and Futura 2000 took hip hop graffiti to Paris and London as part of the New York City Rap Tour in 1983. Hollywood also paid attention, consulting writers such as PHASE 2 as it depicted the culture and gave it international exposure in movies such as "Beat Street" (Orion, 1984).
Stencil graffiti emerges.
This period also saw the emergence of the new stencil graffiti genre. Some of the first examples were created in 1981 by graffiti artist Blek le Rat in Paris, in 1982 by Jef Aerosol in Tours (France); by 1985 stencils had appeared in other cities including New York City, Sydney, and Melbourne, where they were documented by American photographer Charles Gatewood and Australian photographer Rennie Ellis.
Graffiti as a memorial.
People often leave their traces in wet cement or concrete. This type of graffito often commemorates the mutual commitment of a couple, or simply records a person's presence at a particular moment. Often this type of graffito is dated and is left untouched for decades, offering a look into local historical minutiae.
Commercialization and entrance into mainstream pop culture.
With the popularity and legitimization of graffiti has come a level of commercialization. In 2001, computer giant IBM launched an advertising campaign in Chicago and San Francisco which involved people spray painting on sidewalks a peace symbol, a heart, and a penguin (Linux mascot), to represent "Peace, Love, and Linux." Due to laws forbidding it, some of the "street artists" were arrested and charged with vandalism, and IBM was fined more than US$120,000 for punitive damages and clean-up costs.
In 2005, a similar ad campaign was launched by Sony and executed by TATS CRU in New York, Chicago, Atlanta, Philadelphia, Los Angeles, and Miami to market its handheld PSP gaming system. In this campaign, taking notice of the legal problems of the IBM campaign, Sony paid building owners for the rights to paint on their buildings "a collection of dizzy-eyed urban kids playing with the PSP as if it were a skateboard, a paddle, or a rocking horse".
Gamer culture.
Along with the commercial growth has come the rise of video games also depicting graffiti, usually in a positive aspect – for example, the "Jet Set Radio" series (2000–2003) tells the story of a group of teens fighting the oppression of a totalitarian police force that attempts to limit the graffiti artists' freedom of speech. In plotlines mirroring the negative reaction of non-commercial artists to the commercialization of the art form by companies such as IBM (and, later, Sony itself) the "Rakugaki Ōkoku" series (2003–2005) for Sony's PlayStation 2 revolves around an anonymous hero and his magically imbued-with-life graffiti creations as they struggle against an evil king who only allows art to be produced which can benefit him. Following the original roots of modern graffiti as a political force came another game title, "" (2006), featuring a story line involving fighting against a corrupt city and its oppression of free speech, as in the "Jet Set Radio" series.
Other games which feature graffiti include "Bomb the World" (2004), an online graffiti simulation created by graffiti artist Klark Kent where users can paint trains virtually at 20 locations worldwide, and "Super Mario Sunshine" (2002), in which the hero, Mario must clean the city of graffiti left by the villain, Bowser Jr. in a plotline which evokes the successes of the Anti-Graffiti Task Force of New York's Mayor Rudolph Giuliani (a manifestation of the "broken window theory") or those of the "Graffiti Blasters" of Chicago's Mayor Richard M. Daley.
Numerous other non-graffiti-centric video games allow the player to produce graffiti (such as the "Half-Life" series, the "Tony Hawk's" series, ', "Rolling", and '). "Counter-Strike", which is a "Half-Life" mod, allows users to create their own graffiti tags to use in the game. Many other titles contain in-game depictions of graffiti, including "The Darkness", ', "NetHack", ', "The World Ends with You", "The Warriors", "Just Cause", "Portal", and various examples of Virtual Graffiti. There also exist games where the term "graffiti" is used as a synonym for "drawing" (such as "Yahoo! Graffiti", "Graffiti", etc.).
Advocates.
Marc Ecko, an urban clothing designer, has been an advocate of graffiti as an art form during this period, stating that "Graffiti is without question the most powerful art movement in recent history and has been a driving inspiration throughout my career."
Henry Chalfant is one of the foremost advocates of modern graffiti, having produced the documentary film Style Wars and co-authored the books "Subway Art" and "Spray Can Art". His most recent work, "Henry Chalfant's Graffiti Archive: New York City's Subway Art and Artists" displays his over 800 photographs of New York City Subway Graffiti Art.
Keith Haring was another well-known graffiti artist who brought Pop Art and graffiti to the commercial mainstream. In the 1980s, Haring opened his first Pop Shop: a store that offered everyone access to his works, which until then could only be found spray-painted on city walls. Pop Shop offered commodities such as bags and t-shirts. Haring explained that "The Pop Shop makes my work accessible. It's about participation on a big level, the point was that we didn't want to produce things that would cheapen the art. In other words, this was still art as statement."
Graffiti have become a common stepping stone for many members of both the art and design communities in North America and abroad. Within the United States graffiti artists such as Mike Giant, Pursue, Rime, Noah, and countless others have made careers in skateboard, apparel, and shoe design for companies such as DC Shoes, Adidas, Rebel8, Osiris, or Circa Meanwhile there are many others such as DZINE, Daze, Blade, and The Mac who have made the switch to being gallery artists, often not even using their initial medium, spray paint.
But perhaps the greatest example of graffiti artists infiltrating mainstream pop culture is the French crew 123Klan. Founded as a graffiti crew in 1989 by Scien and Klor, 123Klan has gradually turned their hands to illustration and design while still maintaining their graffiti practice and style. In doing so they have designed and produced logos and illustrations, shoes, and fashion for the likes of Nike, Adidas, Lamborghini, Coca Cola, Stussy, Sony, Nasdaq, and more.
Global developments.
South America.
There is a significant graffiti tradition in South America, especially in Brazil. Within Brazil, São Paulo is a significant centre of inspiration for many graffiti artists worldwide.
Tristan Manco wrote that Brazil "boasts a unique and particularly rich, graffiti scene ... it an international reputation as the place to go for artistic inspiration." Graffiti "flourishes in every conceivable space in Brazil's cities." Artistic parallels "are often drawn between the energy of São Paulo today and 1970s New York." The "sprawling metropolis," of São Paulo has "become the new shrine to graffiti;" Manco alludes to "poverty and unemployment ... [and the epic struggles and conditions of the country's marginalised peoples," and to "Brazil's chronic poverty," as the main engines that "have fuelled a vibrant graffiti culture." In world terms, Brazil has "one of the most uneven distributions of income. Laws and taxes change frequently." Such factors, Manco argues, contribute to a very fluid society, riven with those economic divisions and social tensions that underpin and feed the "folkloric vandalism and an urban sport for the disenfranchised," that is South American graffiti art.
Prominent Brazilian graffiti artists include Os Gêmeos, Boleta, Nunca, Nina, Speto, Tikka, and T.Freak. Their artistic success and involvement in commercial design ventures has highlighted divisions within the Brazilian graffiti community between adherents of the cruder transgressive form of "pichação" and the more conventionally artistic values of the practitioners of "grafite".
Middle East.
Graffiti in the Middle East is emerging slowly, with pockets of taggers operating in the various 'Emirates' of the United Arab Emirates, in Israel, and in Iran. The major Iranian newspaper "Hamshahri" has published two articles on illegal writers in the city with photographic coverage of Iranian artist A1one's works on Tehran walls. Tokyo-based design magazine, "PingMag", has interviewed A1one and featured photographs of his work. The Israeli West Bank barrier has become a site for graffiti, reminiscent in this sense of the Berlin Wall. Many graffiti artists in Israel come from other places around the globe, such as JUIF from Los Angeles and DEVIONE from London. The religious reference "נ נח נחמ נחמן מאומן" ("Na Nach Nachma Nachman Meuman") is commonly seen in graffiti around Israel.
Southeast Asia.
There are also a large number of graffiti influences in Southeast Asian countries that mostly come from modern Western culture, such as Malaysia, where graffiti have long been a common sight in Malaysia's capital city, Kuala Lumpur. Since 2010, the country has begun hosting a street festival to encourage all generations and people from all walks of life to enjoy and encourage Malaysian street culture.
Characteristics of common graffiti.
Methods and production.
The modern-day graffiti artist can be found with an arsenal of various materials that allow for a successful production of a piece. This includes such techniques as scribing. However, spray paint in aerosol cans is the number one medium for graffiti. From this commodity comes different styles, technique, and abilities to form master works of graffiti. Spray paint can be found at hardware and art stores and comes in virtually every color.
Stencil graffiti, originating in the early 1980s (Blek le Rat, Jef Aerosol, Speedy Graphito, Miss Tic...) is created by cutting out shapes and designs in a stiff material (such as cardboard or subject folders) to form an overall design or image. The stencil is then placed on the "canvas" gently and with quick, easy strokes of the aerosol can, the image begins to appear on the intended surface. This method of graffiti is popular amongst artists because of its swift technique that requires very little time. Time is always a factor with graffiti artists due to the constant threat of being caught by law enforcement.
Modern experimentation.
Modern graffiti art often incorporates additional arts and technologies. For example, Graffiti Research Lab has encouraged the use of projected images and magnetic light-emitting diodes (throwies) as new media for graffiti artists. Yarnbombing is another recent form of graffiti. Yarnbombers occasionally target previous graffiti for modification, which had been avoided among the majority of graffiti artists.
Tagging.
Some of the most common styles of graffiti have their own names. A tag is the most basic writing of an artist's name; it is simply a handstyle. A graffiti writer's tag is his or her personalized signature. Tagging is often the example given when opponents of graffiti refer to any acts of handstyle graffiti writing (it is by far the most common form of graffiti). Tags can contain subtle and sometimes cryptic messages, and may incorporate the artist's crew initials or other letters.
One form of tagging, known as pissing, involves taking a refillable fire-extinguisher and replacing the contents with paint, allowing for tags as high as approximately . Aiming and keeping a handstyle steady in this form of tagging is very difficult, usually coming out wavy and sloppy.
Another form is the throw-up, also known as a bombing, which is normally painted very quickly with two or three colors, sacrificing aesthetics for speed. Throw-ups can also be outlined on a surface with one color. A piece is a more elaborate representation of the artist's name, incorporating more stylized letters, usually incorporating a much larger range of colors. This is more time-consuming and increases the likelihood of the artist getting caught. A blockbuster or roller is a large piece, almost always done in a block-shaped style, done simply to cover a large area solidly with two contrasting colors, sometimes with the whole purpose of blocking other writers from painting on the same wall. These are usually accomplished with extended paint rollers and gallons of cheap exterior paint.
A more complex style is wildstyle, a form of graffiti usually involving interlocking letters and connecting points. These pieces are often harder to read by non-graffiti artists as the letters merge into one another in an often-undecipherable manner.
Some artists also use self-adhesive stickers as a quick way to do catch ups. While certain critics from within graffiti culture consider this lazy, stickers can be quite detailed in their own right and often, are used in conjunction with other materials. Sticker tags are commonly executed on blank postage stickers, as these can easily be acquired with no cost on the writer's part.
Many graffiti artists believe that doing complex pieces involves too great an investment of time to justify the practice. Doing a piece can take (depending on experience and size) from 30 minutes to months on end, as was the case for Saber MSK while working on the world's largest graffiti piece on the LA river.
Another graffiti artist can go over a piece in a matter of minutes with a simple throw-up. This was exemplified by the writer "CAP" in the documentary "Style Wars", who, other writers complain, ruins pieces with his quick throw ups. This became known as capping and often is done when there is a "beef", or conflict between writers.
A number of recent examples of graffiti make use of hashtags.
Uses.
Theories on the use of graffiti by avant-garde artists have a history dating back at least to the Scandinavian Institute of Comparative Vandalism in 1961.
Many contemporary analysts and even art critics have begun to see artistic value in some graffiti and to recognize it as a form of public art. According to many art researchers, particularly in the Netherlands and in Los Angeles, that type of public art is, in fact an effective tool of social emancipation or, in the achievement of a political goal.
The murals of Belfast and of Los Angeles offer another example of official recognition. In times of conflict, such murals have offered a means of communication and self-expression for members of these socially, ethnically, or racially divided communities, and have proven themselves as effective tools in establishing dialog and thus, of addressing cleavages in the long run. The Berlin Wall was also extensively covered by graffiti reflecting social pressures relating to the oppressive Soviet rule over the GDR.
Many artists involved with graffiti are also concerned with the similar activity of stenciling. Essentially, this entails stenciling a print of one or more colors using spray-paint. Recognized while exhibiting and publishing several of her coloured stencils and paintings portraying the Sri Lankan Civil War and urban Britain in the early 2000s, graffiti artist Mathangi Arulpragasam, aka M.I.A., has also become known for integrating her imagery of political violence into her music videos for singles "Galang" and "Bucky Done Gun", and her cover art. Stickers of her artwork also often appear around places such as London in Brick Lane, stuck to lamp posts and street signs, she having become a muse for other graffiti artists and painters worldwide in cities including Seville. Graffiti artist John Fekner, called "caption writer to the urban environment, adman for the opposition" by writer Lucy Lippard, was involved in direct art interventions within New York City's decaying urban environment in the mid-1970s through the 1980s. Fekner is known for his word installations targeting social and political issues, stenciled on buildings throughout New York.
Personal expression.
Graffiti artists constantly have the looming threat of facing consequences for displaying their graffiti. Many choose to protect their identities and reputation by remaining anonymous.
With the commercialization of graffiti (and hip hop in general), in most cases, even with legally painted "graffiti" art, graffiti artists tend to choose anonymity. This may be attributed to various reasons or a combination of reasons. Graffiti still remains the one of four hip hop elements that is not considered "performance art" despite the image of the "singing and dancing star" that sells hip hop culture to the mainstream. Being a graphic form of art, it might also be said that many graffiti artists still fall in the category of the introverted archetypal artist.
Banksy is one of the world's most notorious and popular street artists who continues to remain faceless in today's society. He is known for his political, anti-war stencil art mainly in Bristol, England, but his work may be seen anywhere from Los Angeles to Palestine. In the UK, Banksy is the most recognizable icon for this cultural artistic movement and keeps his identity a secret to avoid arrest. Much of Banksy's artwork may be seen around the streets of London and surrounding suburbs, although he has painted pictures throughout the world, including the Middle East, where he has painted on Israel's controversial West Bank barrier with satirical images of life on the other side. One depicted a hole in the wall with an idyllic beach, while another shows a mountain landscape on the other side. A number of exhibitions also have taken place since 2000, and recent works of art have fetched vast sums of money. Banksy's art is a prime example of the classic controversy: vandalism vs. art. Art supporters endorse his work distributed in urban areas as pieces of art and some councils, such as Bristol and Islington, have officially protected them, while officials of other areas have deemed his work to be vandalism and have removed it.
Pixnit is another artist who chooses to keep her identity from the general public. Her work focuses on beauty and design aspects of graffiti as opposed to Banksy's anti-government shock value. Her paintings are often of flower designs above shops and stores in her local urban area of Cambridge, Massachusetts. Some store owners endorse her work and encourage others to do similar work as well. "One of the pieces was left up above Steve's Kitchen, because it looks pretty awesome"- Erin Scott, the manager of New England Comics in Allston, Massachusetts.
Radical and political.
Graffiti often has a reputation as part of a subculture that rebels against authority, although the considerations of the practitioners often diverge and can relate to a wide range of attitudes. It can express a political practice and can form just one tool in an array of resistance techniques. One early example includes the anarcho-punk band Crass, who conducted a campaign of stenciling anti-war, anarchist, feminist, and anti-consumerist messages throughout the London Underground system during the late 1970s and early 1980s. In Amsterdam graffiti was a major part of the punk scene. The city was covered with names such as "De Zoot", "Vendex", and "Dr Rat". To document the graffiti a punk magazine was started that was called "Gallery Anus". So when hip hop came to Europe in the early 1980s there was already a vibrant graffiti culture.
The student protests and general strike of May 1968 saw Paris bedecked in revolutionary, anarchistic, and situationist slogans such as "L'ennui est contre-révolutionnaire" ("Boredom is counterrevolutionary") and "Lisez moins, vivez plus" ("Read less, live more"). While not exhaustive, the graffiti gave a sense of the 'millenarian' and rebellious spirit, tempered with a good deal of verbal wit, of the strikers.
The developments of graffiti art which took place in art galleries and colleges as well as "on the street" or "underground", contributed to the resurfacing in the 1990s of a far more overtly politicized art form in the subvertising, culture jamming, or tactical media movements. These movements or styles tend to classify the artists by their relationship to their social and economic contexts, since, in most countries, graffiti art remains illegal in many forms except when using non-permanent paint. Since the 1990s a growing number of artists are switching to non-permanent paints for a variety of reasons—but primarily because is it difficult for the police to apprehend them and for the courts to sentence or even convict a person for a protest that is as fleeting and less intrusive than marching in the streets. In some communities, such impermanent works survive longer than works created with permanent paints because the community views the work in the same vein as that of the civil protester who marches in the street—such protest are impermanent, but effective nevertheless.
In some areas where a number of artist share the impermanence ideal, there grows an informal competition. That is, the length of time that a work escapes destruction is related to the amount of respect the work garners in the community. A crude work that deserves little respect would be invariably removed immediately. The most talented artist might have works last for days.
Contemporary practitioners, accordingly, have varied and often, conflicting practices. Some individuals, such as Alexander Brener, have used the medium to politicize other art forms, and have used the prison sentences enforced on them as a means of further protest.
The practices of anonymous groups and individuals also vary widely, and practitioners by no means always agree with each other's practices. The anti-capitalist art group, the Space Hijackers, for example, did a piece in 2004 about the contradiction between the capitalistic elements of Banksy and his use of political imagery.
On top of the political aspect of graffiti as a movement, political groups and individuals may also use graffiti as a tool to spread their point of view. This practice, due to its illegality, has generally become favored by groups excluded from the political mainstream (e.g. far-left or far-right groups) who justify their activity by pointing out that they do not have the money – or sometimes the desire – to buy advertising to get their message across, and that a "ruling class" or "establishment" controls the mainstream press, systematically excluding the radical and alternative point of view. This type of graffiti can seem crude; for example fascist supporters often scrawl swastikas and other Nazi images.
One innovative form of graffiti that emerged in the UK in the 1970s was devised by the Money Liberation Front (MLF), essentially a loose affiliation of underground press writers such as the poet and playwright Heathcote Williams and magazine editor and playwright Jay Jeff Jones. They initiated the use of paper currency as a medium for counterculture propaganda, overprinting banknotes, usually with a John Bull printing set. Although short lived, the MLF was representative of London's Ladbroke Grove centered alternative and literary community of the period. The area was also a scene of considerable anti-establishment and humorous street graffiti, much of which is also produced by Williams. In 2009, following the elections in Iran, protesters (who regarded the electoral result as rigged) began to deface banknotes with slogans such as "Death to the dictator". In Colombia writing and drawing on banknotes has become increasingly popular, either to make political comments, for fun or as an artistic medium. The national government has run advertising campaigns in an attempt to discourage the practice. In the UK there have been signs of an MLF resurgence with a number of banknotes in circulation being over-marked with protest slogans such as "Banks=Robbers", relating to the perceived culpability of banks in the financial crisis.
Both sides of the conflict in Northern Ireland produce political graffiti. As well as slogans, Northern Irish political graffiti includes large wall paintings, referred to as "murals". Along with the flying of flags and the painting of kerb stones, the murals serve a territorial purpose, often associated with gang use. Artists paint them mostly on house gables or on the "Peace Lines", high walls that separate different communities.
The murals often develop over an extended period and tend to stylization, with a strong symbolic or iconographic content. Loyalist murals often refer to historical events dating from the war between James II and William III in the late seventeenth century, whereas Republican murals usually refer to the more recent troubles.
Territorial graffiti serves as marking ground to display tags and logos that differentiate certain groups from others. These images are meant to show outsiders a stern look at whose turf is whose. The subject matter of gang-related graffiti consists of cryptic symbols and initials strictly fashioned with unique calligraphies. Gang members use graffiti to designate membership throughout the gang, to differentiate rivals and associates and, most commonly, to mark borders which are both territorial and ideological.
As advertising.
Graffiti has been used as a means of advertising both legally and illegally. Bronx-based TATS CRU has made a name for themselves doing legal advertising campaigns for companies such as Coca Cola, McDonald's, Toyota, and MTV. In the UK, Covent Garden's Boxfresh used stencil images of a Zapatista revolutionary in the hopes that cross referencing would promote their store. Smirnoff hired artists to use reverse graffiti (the use of high pressure hoses to clean dirty surfaces to leave a clean image in the surrounding dirt) to increase awareness of their product. Shepard Fairey rose to fame after his "Andre the Giant Has a Posse" sticker campaign, in which his art was plastered in cities throughout America.
Many graffiti artists see legal advertising as no more than "paid for and legalised graffiti", and have risen against mainstream ads. The graffiti research lab crew have gone on to target several prominent ads in New York as a means of making a statement against this criteria.
Offensive graffiti.
Graffiti may also be used as an offensive expression. This form of graffiti may be difficult to identify, as it is mostly removed by the local authority (as councils which have adopted strategies of criminalization also strive to remove graffiti quickly). Therefore, existing racist graffiti is mostly more subtle and at first sight, not easily recognized as "racist". It can then only be understood if one knows the relevant "local code" (social, historical, political, temporal, and spatial), which is seen as heteroglot and thus an 'unique set of conditions' in a cultural context.
Hence, the lack of obvious racist graffiti does not necessarily mean that there is none. By making the graffiti less explicit (as adapted to social and legal constraints), these drawings are less likely to be removed, but do not lose their threatening and offensive character.
Decorative and high art.
In the early 1980s, the first art galleries to show graffiti artists to the public were Fashion Moda in the Bronx and Now Gallery in the East Village, Manhattan.
A 2006 exhibition at the Brooklyn Museum displayed graffiti as an art form that began in New York's outer boroughs and reached great heights in the early 1980s with the work of Crash, Lee, Daze, Keith Haring, and Jean-Michel Basquiat. It displayed 22 works by New York graffiti artists, including Crash, Daze, and Lady Pink. In an article about the exhibition in the magazine "Time Out", curator Charlotta Kotik said that she hoped the exhibition would cause viewers to rethink their assumptions about graffiti. Terrance Lindall, an artist and executive director of the Williamsburg Art and Historic Center, said regarding graffiti and the exhibition:
"Graffiti is revolutionary, in my opinion", he says, "and any revolution might be considered a crime. People who are oppressed or suppressed need an outlet, so they write on walls—it's free."
From the 1970s onwards, Burhan Dogancay photographed urban walls all over the world; these he then archived for use as sources of inspiration for his painterly works. The project today known as "Walls of the World" grew beyond even his own expectations and comprises about 30’000 individual images. It spans a period of 40 years across five continents and 114 countries. In 1982, photographs from this project comprised a one-man exhibition titled "Les murs murmurent, ils crient, ils chantent..." (The walls whisper, shout and sing...) at the Centre Georges Pompidou in Paris.
In Australia, art historians have judged some local graffiti of sufficient creative merit to rank them firmly within the arts. Oxford University Press's art history text "Australian Painting 1788–2000" concludes with a long discussion of graffiti's key place within contemporary visual culture, including the work of several Australian practitioners.
Between March and April 2009, 150 artists exhibited 300 pieces of graffiti at the Grand Palais in Paris — a clear acceptance of the art form into the French art world.
Many graffiti artists have used their design talents in other artistic endeavors. In 2009 graffiti artist "Scape" published "GRAFF; the Art & Technique of Graffiti", the world's first book dedicated to displaying the full techniques of creating graffiti art. Other books that focus on graffiti include "Faith of Graffiti" by Norman Mailer, "Trespass" by Taschen press, and the comic book by Elite Gudz, "Concrete Immortalz", which has a graffiti artist as its main character.
Figurines by KAWS, featuring icons of pop culture, often with crossed-out eyes, run in limited editions and sell for thousands of dollars. World-renowned street artist Banksy directed a film in 2010, "Exit Through the Gift Shop", which explored street art and commercialism.
Environmental effects.
Spray paint has many negative environmental effects. The paint contains toxic chemicals, and the can uses chlorofluorocarbons or volatile hydrocarbon gases to spray the paint unto a surface. As an alternative, moss graffiti is starting to catch on, which uses moss to create text or images. The moss is glued onto a surface by means of beer, buttermilk, or yogurt combined with sugar.
Government responses.
Asia.
In China, Mao Zedong in the 1920s used revolutionary slogans and paintings in public places to galvanise the country's communist revolution.
In Hong Kong, Tsang Tsou Choi was known as the "King of Kowloon" for his calligraphy graffiti over many years, in which he claimed ownership of the area. Now some of his work is preserved officially.
In Taiwan, the government has made some concessions to graffiti artists. Since 2005 they have been allowed to freely display their work along some sections of riverside retaining walls in designated "Graffiti Zones". From 2007, Taipei's department of cultural affairs also began permitting graffiti on fences around major public construction sites. Department head Yong-ping Lee (李永萍) stated, "We will promote graffiti starting with the public sector, and then later in the private sector too. It's our goal to beautify the city with graffiti". The government later helped organize a graffiti contest in Ximending, a popular shopping district. Graffiti artists caught working outside of these designated areas still face fines up to $6,000 TWD under a department of environmental protection regulation. However, Taiwanese authorities can be relatively lenient, one veteran police officer stating anonymously, "Unless someone complains about vandalism, we won't get involved. We don't go after it proactively."
In 1993 in Singapore after several expensive cars were spray-painted, the police arrested a student from the Singapore American School, Michael P. Fay, questioned him, and subsequently charged him with vandalism. Fay pleaded guilty to vandalizing a car in addition to stealing road signs. Under the 1966 Vandalism Act of Singapore, originally passed to curb the spread of communist graffiti in Singapore, the court sentenced him to four months in jail, a fine of S$3,500 (US$2,233), and a caning. "The New York Times" ran several editorials and op-eds that condemned the punishment and called on the American public to flood the Singaporean embassy with protests. Although the Singapore government received many calls for clemency, Fay's caning took place in Singapore on 5 May 1994. Fay had originally received a sentence of six strokes of the cane, but the presiding president of Singapore, Ong Teng Cheong, agreed to reduce his caning sentence to four lashes.
In South Korea, Park Jung-soo was fined 2 million South Korean won by the Seoul Central District Court for spray-painting a rat on posters of the G-20 Summit a few days before the event in November 2011. Park alleged that the initial in “G-20” sounds like the Korean word for “rat”, but Korean government prosecutors alleged that Mr. Park was making a derogatory statement about the president of ROK, Lee Myung-bak, the host of the summit. This case led to public outcry and debate on the lack of government tolerance and in support of freedom of expression. The court ruled that the painting, “an ominous creature like a rat” amounts to “an organized criminal activity" and upheld the fine while denying the prosecution's request for imprisonment for Park.
Europe.
In Europe, community cleaning squads have responded to graffiti, in some cases with reckless abandon, as when in 1992 in France a local Scout group, attempting to remove modern graffiti, damaged two prehistoric paintings of bison in the Cave of Mayrière supérieure near the French village of Bruniquel in Tarn-et-Garonne, earning them the 1992 Ig Nobel Prize in archeology.
In September 2006, the European Parliament directed the European Commission to create urban environment policies to prevent and eliminate dirt, litter, graffiti, animal excrement, and excessive noise from domestic and vehicular music systems in European cities, along with other concerns over urban life.
The Anti-Social Behaviour Act 2003 became Britain's latest anti-graffiti legislation. In August 2004, the Keep Britain Tidy campaign issued a press release calling for zero tolerance of graffiti and supporting proposals such as issuing "on the spot" fines to graffiti offenders and banning the sale of aerosol paint to anyone under the age of 16. The press release also condemned the use of graffiti images in advertising and in music videos, arguing that real-world experience of graffiti stood far removed from its often-portrayed 'cool' or 'edgy' image.
To back the campaign, 123 MPs (including then Prime Minister Tony Blair), signed a charter which stated: "Graffiti is not art, it's crime. On behalf of my constituents, I will do all I can to rid our community of this problem." However, since the early 1990s, the British graffiti scene has been struck by self-titled "art terrorist" Banksy, who has revolutionized the style of UK graffiti (bringing to the forefront stencils to aid the speed of painting), as well as the content; making his work largely satirical of the sociological state of cities, or the political climate of war, often using monkeys and rats as motifs.
In the UK, city councils have the power to take action against the owner of any property that has been defaced under the Anti-social Behaviour Act 2003 (as amended by the Clean Neighbourhoods and Environment Act 2005) or, in certain cases, the Highways Act. This is often used against owners of property that are complacent in allowing protective boards to be defaced so long as the property is not damaged.
In July 2008, a conspiracy charge was used to convict graffiti artists for the first time. After a three-month police surveillance operation, nine members of the DPM crew were convicted of conspiracy to commit criminal damage costing at least £1 million. Five of them received prison sentences, ranging from eighteen months to two years. The unprecedented scale of the investigation and the severity of the sentences rekindled public debate over whether graffiti should be considered art or crime.
Some councils, like those of Stroud and Loerrach, provide approved areas in the town where graffiti artists can showcase their talents, including underpasses, car parks, and walls that might otherwise prove a target for the 'spray and run.'
In Budapest, Hungary both a city-backed movement called "I Love Budapest" and a special police division tackle the problem, including the provision of approved areas.
Australia.
In an effort to reduce vandalism, many cities in Australia have designated walls or areas exclusively for use by graffiti artists. One early example is the "Graffiti Tunnel" located at the Camperdown Campus of the University of Sydney, which is available for use by any student at the university to tag, advertise, poster, and create "art". Advocates of this idea suggest that this discourages petty vandalism yet encourages artists to take their time and produce great art, without worry of being caught or arrested for vandalism or trespassing. Others disagree with this approach, arguing that the presence of legal graffiti walls does not demonstrably reduce illegal graffiti elsewhere. Some local government areas throughout Australia have introduced "anti-graffiti squads", who clean graffiti in the area, and such crews as BCW (Buffers Can't Win) have taken steps to keep one step ahead of local graffiti cleaners.
Many state governments have banned the sale or possession of spray paint to those under the age of 18 (age of majority). However, a number of local governments in Victoria have taken steps to recognize the cultural heritage value of some examples of graffiti, such as prominent political graffiti. Tough new graffiti laws have been introduced in Australia with fines of up to A$26,000 and two years in prison.
Melbourne is a prominent graffiti city of Australia with many of its lanes being tourist attractions, such as Hosier Lane in particular, a popular destination for photographers, wedding photography, and backdrops for corporate print advertising. The Lonely Planet travel guide cites Melbourne's street as a major attraction. All forms of graffiti, including sticker art, poster, stencil art, and wheatpasting, can be found in many places throughout the city. Prominent street art precincts include; Fitzroy, Collingwood, Northcote, Brunswick, St. Kilda, and the CBD, where stencil and sticker art is prominent. As one moves farther away from the city, mostly along suburban train lines, graffiti tags become more prominent. Many international artists such as Banksy have left their work in Melbourne and in early 2008 a perspex screen was installed to prevent a Banksy stencil art piece from being destroyed, it has survived since 2003 through the respect of local street artists avoiding posting over it, although it has recently had paint tipped over it.
New Zealand.
In February 2008 Helen Clark, the New Zealand prime minister at that time, announced a government crackdown on tagging and other forms of graffiti vandalism, describing it as a destructive crime representing an invasion of public and private property. New legislation subsequently adopted included a ban on the sale of paint spray cans to persons under 18 and increases in maximum fines for the offence from NZ$200 to NZ$2,000 or extended community service. The issue of tagging become a widely debated one following an incident in Auckland during January 2008 in which a middle-aged property owner stabbed one of two teenage taggers to death and was subsequently convicted of manslaughter.
United States.
Tracker databases.
Graffiti databases have increased in the past decade because they allow vandalism incidents to be fully documented against an offender and help the police and prosecution charge and prosecute offenders for multiple counts of vandalism. They also provide law enforcement the ability to rapidly search for an offender’s moniker or tag in a simple, effective, and comprehensive way. These systems can also help track costs of damage to city to help allocate an anti-graffiti budget. The theory is that when an offender is caught putting up graffiti, they are not just charged with one count of vandalism; they can be held accountable for all of the other damage for which they are responsible. This has two main benefits for law enforcement. One, it sends a signal to the offenders that their vandalism is being tracked. Two, a city can seek restitution from offenders for all of the damage that they have committed, not merely a single incident. These systems give law enforcement personnel real-time, street-level intelligence that allows them to not only focus on the worst graffiti offenders and their damage, but also to monitor potential gang violence that is associated with the graffiti.
Gang injunctions.
Many restrictions of civil gang injunctions are designed to help address and protect the physical environment and limit graffiti. Provisions of gang injunctions include things such as restricting the possession of marker pens, spray paint cans, or other sharp objects capable of defacing private or public property; spray painting, or marking with marker pens, scratching, applying stickers, or otherwise applying graffiti on any public or private property, including, but not limited to the street, alley, residences, block walls, and fences, vehicles and/or any other real or personal property. Some injunctions contain wording that restricts damaging or vandalizing the property of another, both public and private property, including, but limited to any vehicle, light fixture, door, fence, wall, gate, window, building, street sign, utility box, telephone box, trees, or power pole.
Hotlines and reward programs.
To help address many of these issues, many local jurisdictions have set up graffiti abatement hotlines, where citizens can call in and report vandalism and have it removed. San Diego’s hotline receives more than 5,000 calls per year, in addition to reporting the graffiti, callers can learn more about prevention. One of the complaints about these hotlines is the response time; there is often a lag time between a property owner calling about the graffiti and its removal. The length of delay should be a consideration for any jurisdiction planning on operating a hotline. Local jurisdictions must convince the callers that their complaint of vandalism will be a priority and cleaned off right away. If the jurisdiction does not have the resources to respond to complaints in a timely manner, the value of the hotline diminishes. Crews must be able to respond to individual service calls made to the graffiti hotline as well as focus on cleanup near schools, parks, and major intersections and transit routes to have the biggest impact. Some cities offer a reward for information leading to the arrest and prosecution of suspects for tagging or graffiti related vandalism. The amount of the reward is based on the information provided, and the action taken.
Search warrants.
When the police use search warrants in connection with a vandalism investigation they are often seeking judicial approval to look for items such as cans of spray paint and nozzles from other kinds of aerosol sprays, etching tools, or other sharp or pointed objects used to etch or scratch glass and other hard surfaces, such as permanent marking pens and markers or paint sticks; evidence of membership or affiliation with any gang or tagging crew, paraphernalia to include any reference to “(tagger’s name),” and any drawings, writings, objects, or graffiti depicting taggers’ names, initials, logos, monikers, slogans, or mention of tagging crew membership; any newspaper clippings relating details of or referring to any graffiti crime.

</doc>
<doc id="11986" url="https://en.wikipedia.org/wiki?curid=11986" title="Godzilla">
Godzilla

With the nuclear bombings of Hiroshima and Nagasaki and the "Lucky Dragon 5" incident still fresh in the Japanese consciousness, Godzilla was conceived as a metaphor for nuclear weapons. As the film series expanded, some stories took on less serious undertones portraying Godzilla as a hero while other plots still portrayed Godzilla as a destructive monster; sometimes the lesser of two threats who plays the defender by default but is still a danger to humanity.
Overview.
Name.
Godzilla's name was written in ateji as , where the kanji are used for phonetic value and not for meaning. The Japanese pronunciation of the name is ; the Anglicized form is , with the first syllable pronounced like the word "god", and the rest rhyming with "gorilla". In the Hepburn romanization system, Godzilla's name is rendered as "Gojira", whereas in the Kunrei romanization system it is rendered as "Gozira".
Characteristics.
Within the context of the Japanese films, Godzilla's exact origins vary, but it is generally depicted as an enormous, violent, prehistoric sea monster awakened and empowered by nuclear radiation. Although the specific details of Godzilla's appearance have varied slightly over the years, the overall impression has remained consistent. Inspired by the fictional "Rhedosaurus" created by animator Ray Harryhausen for the film "The Beast from 20,000 Fathoms", Godzilla's iconic character design was conceived as that of an amphibious reptilian monster based around the loose concept of a dinosaur with an erect standing posture, scaly skin, an anthropomorphic torso with muscular arms, spikes on its back and tail, and a furrowed brow. Art director Akira Watanabe combined attributes of a "Tyrannosaurus", an "Iguanodon", a "Stegosaurus" and an alligator to form a sort of blended chimera, inspired by illustrations from an issue of "Life" magazine. To emphasise the monster's relationship with the atomic bomb, its skin texture was inspired by the keloid scars seen on survivors in Hiroshima. The basic design has a reptilian visage, a robust build, an upright posture, a long tail and rows of serrated fins along the back. In the original film, the fins were added for purely aesthetic purposes, in order to further differentiate Godzilla from any other living or extinct creature.
Godzilla has a distinctive roar (transcribed in several comics as "Skreeeonk!"), which was created by composer Akira Ifukube, who produced the sound by rubbing a pine-tar-resin-coated glove along the string of a contrabass and then slowing down the playback. Godzilla is sometimes depicted as green in comics, cartoons and movie posters, but the costumes used in the movies were usually painted charcoal grey with bone-white dorsal fins up until the film "Godzilla 2000".
Godzilla's signature weapon is its "atomic breath," a nuclear blast that it generates inside of its body and unleashes from its jaws in the form of a blue or red radioactive heat ray. Toho’s special effects department has used various techniques to render the breath, from physical gas-powered flames to hand-drawn or computer-generated fire. Godzilla is shown to possess immense physical strength and muscularity. Haruo Nakajima, the actor who played Godzilla in the original films, was a black belt in Judo and used his expertise to choreograph the battle sequences. Godzilla can breathe underwater, and is described in the original film by the character Dr. Yamane as a transitional form between a marine and a terrestrial reptile. Godzilla is shown to have great vitality: it is immune to conventional weaponry thanks to its rugged hide and ability to regenerate, and as a result of surviving a nuclear explosion, it cannot be destroyed by anything less powerful. Various films, television shows, comics and games have depicted Godzilla with additional powers such as an atomic pulse, magnetism, precognition, fireballs, an electric bite, superhuman speed, eye beams and even flight.
Godzilla's allegiance and motivations have changed from film to film to suit the needs of the story. Although Godzilla does not like humans, it will fight alongside humanity against common threats. However, it makes no special effort to protect human life or property and will turn against its human allies on a whim. It is not motivated to attack by predatory instinct: it doesn't eat people, and instead sustains itself on radiation and an omnivorous diet. When inquired if Godzilla was "good or bad", producer Shogo Tomiyama likened it to a Shinto "God of Destruction" which lacks moral agency and cannot be held to human standards of good and evil. "He totally destroys everything and then there is a rebirth. Something new and fresh can begin."
In the original Japanese films, Godzilla and all the other monsters are referred to with gender-neutral pronouns equivalent to "it", while in the English dubbed versions, Godzilla is explicitly described as a male, such as in the title of "Godzilla, King of the Monsters!". The creature in the 1998 Godzilla film was depicted laying eggs through parthenogenesis.
In the various stories it has appeared in, Godzilla has been featured alongside many supporting characters. It has faced human opponents such as the JSDF, and other giant monsters, from recurring characters like King Ghidorah, Gigan and Mechagodzilla to one-shot characters like Megalon, Biollante and Megaguirus. Godzilla is also shown to have allies, such as Mothra, Rodan and Anguirus (though these characters were initially portrayed as Godzilla's rivals), and offspring, such as Minilla. Godzilla has even fought against fictional characters from other franchises in crossover media, such as King Kong and the Fantastic Four.
Size.
Godzilla's size is inconsistent, changing from film to film and even from scene to scene for the sake of artistic license. The miniature sets and costumes are typically built at a – scale and filmed at 240 frames per second, to create the illusion of great size. In the original 1954 film, Godzilla was scaled to be tall. This was done so Godzilla could just peer over the largest buildings in Tokyo at the time. In the 1956 American version, Godzilla is estimated to be over tall, because producer Joseph E. Levine felt that 50 m didn't sound "powerful enough". As the series progressed Toho would rescale the character, eventually making Godzilla as tall as . This was so that it wouldn't be dwarfed by the newer bigger buildings in Tokyo's skyline such as the Tokyo Metropolitan Government Building which Godzilla destroyed in the film "Godzilla vs. King Ghidorah" (1991). Supplementary information such as character profiles would also depict Godzilla as weighing between . In the American film "Godzilla" (2014) from Legendary Pictures, Godzilla was scaled to be , and weighing , making him the largest film incarnation of the character. Director Gareth Edwards wanted Godzilla "to be so big as to be seen from anywhere in the city, but not too big that he couldn’t be obscured". The producers of the upcoming "Godzilla Resurgence" made Godzilla even taller than the Legendary version, at .
Special effects details.
Godzilla's appearance has traditionally been portrayed in the films by an actor wearing a latex costume, though the character has also been rendered in animatronic, stop-motion and computer-generated form. 
Taking inspiration from "King Kong", special effects artist Eiji Tsuburaya had initially wanted Godzilla to be portrayed via stop-motion, but prohibitive deadlines and a lack of experienced animators in Japan at the time made suitmation more practical. The first suit consisted of a body cavity made of thin wires and bamboo wrapped in chicken wire for support, and covered in fabric and cushions, which were then coated in latex. The first suit was held together by small hooks on the back, though subsequent Godzilla suits incorporated a zipper. Its weight was in excess of . Prior to 1984, most Godzilla suits were made from scratch, thus resulting in slight design changes in each film appearance. The most notable changes during the 1960s-70s were the reduction in Godzilla's number of toes and the removal of the character's external ears and prominent fangs, features which would later be reincorporated in the Godzilla designs from "The Return of Godzilla" (1984) onwards. The most consistent Godzilla design was maintained from "Godzilla vs Biollante" (1989) to "Godzilla vs Destoroyah" (1995), when the suit was given a cat-like face and double rows of teeth. Several suit actors had difficulties in performing as Godzilla, due to the suits' weight, lack of ventilation and diminished visibility. Kenpachiro Satsuma in particular, who portrayed Godzilla from 1984 to 1995, described how the Godzilla suits he wore were even heavier and hotter than their predecessors, because of the incorporation of animatronics. Satsuma himself suffered numerous medical issues during his tenure, including oxygen deprivation, near drowning, concussions, electric shocks, and lacerations to the legs from the suits' steel wire reinforcements wearing through the rubber padding. The ventilation problem was partially solved in the suit used in 1994's "Godzilla vs. SpaceGodzilla", which was the first to include an air duct, which allowed suit actors to last longer during performances.
In the 1998 Godzilla film, special effects artist Patrick Tatopoulos was instructed to redesign Godzilla as an incredibly fast runner. At one point, it was planned to use motion capture from a human to create the movements of the computer-generated Godzilla, but it ended up looking too much like a human in a suit. Tatopoulos subsequently reimagined the creature as a lean, digitigrade bipedal iguana that stood with its back and tail parallel to the ground, rendered via CGI. Several scenes had the monster portrayed by stuntmen in suits. The suits were similar to those used in the Toho films, with the actors' heads being located in the monster's neck region, and the facial movements controlled via animatronics. However, because of the creature's horizontal posture, the stuntmen had to wear metal leg extenders, which allowed them to stand off the ground with their feet bent forward. The film's special effects crew also built a scale animatronic Godzilla for close-up scenes, whose size outmatched that of Stan Winston's "T. rex" in "Jurassic Park". Kurt Carley performed the suitmation sequences for the adult Godzilla.
In the 2014 Godzilla film, the character was portrayed entirely via CGI. Godzilla's design in the reboot was intended to stay true to that of the original series, though the film's special effects team strove to make the monster "more dynamic than a guy in a big rubber suit." To create a CG version of Godzilla, The Moving Picture Company (MPC) studied various animals such as bears, Komodo dragons, lizards, lions and wolves which helped the visual effects artists visualize Godzilla's body structure like that of its underlying bone, fat and muscle structure as well as the thickness and texture of its scale. Motion capture was also used for some of Godzilla's movements. TJ Storm provided the motion capture performance for Godzilla by wearing sensors in front of a green screen.
Cultural impact.
Godzilla is one of the most recognizable symbols of Japanese popular culture worldwide and remains an important facet of Japanese films, embodying the kaiju subset of the tokusatsu genre. Godzilla’s vaguely humanoid appearance and strained, lumbering movements endeared it to Japanese audiences, who could relate to Godzilla as a sympathetic character despite its wrathful nature. Audiences respond positively to the character because it acts out of rage and self-preservation and shows where science and technology can go wrong. 
Godzilla has been considered a filmographic metaphor for the United States, as well as an allegory of nuclear weapons in general. The earlier "Godzilla" films, especially the original, portrayed Godzilla as a frightening, nuclear monster. Godzilla represented the fears that many Japanese held about the atomic bombings of Hiroshima and Nagasaki, and the possibility of recurrence. As the series progressed, so did Godzilla, changing into a less destructive and more heroic character as the films became geared towards children. Since then, the character has fallen somewhere in the middle, sometimes portrayed as a protector of the world from external threats and other times as a bringer of destruction. 
In 1996, Godzilla received the MTV Lifetime Achievement Award, as well, Godzilla was given a star on the Hollywood Walk of Fame in 2004 to celebrate the premiere of the character's 50th anniversary film, "". Godzilla's pop-cultural impact has led to the creation of numerous parodies and tributes, as seen in media such as "Bambi Meets Godzilla", which was ranked as one of the "50 greatest cartoons", various episodes of "Mystery Science Theater 3000," and the song "Godzilla", by "Blue Öyster Cult". Godzilla has also been used in advertisements, such as in a commercial for Nike, where Godzilla lost a game of basketball to NBA player Charles Barkley. The commercial was subsequently adapted into a comic book illustrated by Jeff Butler. Godzilla has also appeared in a commercial for Snickers candy bar, which served as an indirect promo for the 2014 movie. Godzilla's success inspired the creation of numerous other monster characters, such as Gamera, Yonggary and Gorgo.
Godzilla's fame and saurian appearance has had an impact on the scientific community. "Gojirasaurus" is a dubious genus of coelophysid dinosaur, named by paleontologist and admitted Godzilla fan Kenneth Carpenter. "Dakosaurus" is an extinct marine crocodile of the Jurassic Period, which researchers informally nicknamed "Godzilla". Paleontologists have written tongue-in-cheek speculative articles about Godzilla's biology, with Ken Carpenter tentatively classifying it as a ceratosaur based on its skull shape, four fingered hands and dorsal scutes, and paleontologist Darren Naish expressing skepticism while commenting on Godzilla's unusual morphology.
Godzilla's ubiquity in pop-culture has led to the mistaken assumption that the character is in the public domain, resulting in litigation by Toho to protect their corporate asset from becoming a generic trademark. In April 2008, Subway depicted a giant monster in a commercial for their Five Dollar Footlong sandwich promotion. Toho filed a lawsuit against Subway for using the character without permission, demanding $150,000 in compensation. In February 2011, Toho sued Honda for depicting a fire-breathing monster in a commercial for the Honda Odyssey. The monster was never mentioned by name, being seen briefly on a video screen inside the minivan. The Sea Shepherd Conservation Society christened a vessel "Gojira". Its purpose is to target and harass Japanese whalers in defense of whales in the Southern Ocean Whale Sanctuary. The "Gojira" was renamed in May 2011 due to legal pressure from Toho. Gojira is the name of a French death metal band, formerly known as Godzilla; legal problems forced the band to change their name. In May 2015, Toho launched a lawsuit against Voltage Pictures over a planned picture starring Anne Hathaway. Promotional material released at the Cannes Film Festival used images of Godzilla.
Cultural ambassador.
To encourage tourism in April 2015 the central Shinjuku ward of Tokyo named Godzilla an official cultural ambassador. During an unveiling of a giant Godzilla bust at Toho headquarters, Shinjuki mayor Kenichi Yoshizumi stated "Godzillia is a character that is the pride of Japan." The mayor extended a residency certificate to an actor in a rubber suit representing Godzilla, but as the suit's hands were not designed for grasping it was accepted on Godzilla's behalf by a Toho executive. Reporters noted that Shinjuku ward has been flattened by Godzilla in three Toho movies.

</doc>
<doc id="11988" url="https://en.wikipedia.org/wiki?curid=11988" title="King Kong vs. Godzilla">
King Kong vs. Godzilla

An American production team produced a heavily altered English version that used new scenes, sound and dubbing. The American production was released theatrically in the United States in the summer of 1963 by Universal Pictures.
Plot.
Mr. Tako, head of Pacific Pharmaceuticals, is frustrated with the television shows his company is sponsoring and wants something to boost his ratings. When a doctor tells Tako about a giant monster he discovered on the small Faro Island, Tako believes that it would be a brilliant idea to use the monster to gain publicity. Tako immediately sends two men, Sakurai and Kinsaburo, to find and bring back the monster. Meanwhile, the American submarine Seahawk gets caught in an iceberg. The iceberg collapses, unleashing Godzilla (who, in the Japanese version, had been trapped within since 1955), who then destroys the submarine and a nearby military base.
On Faro Island, a giant octopus attacks the native village. The mysterious Faro monster arrives, revealed to be King Kong and defeats the octopus. Kong then drinks some red berry juice that immediately puts him to sleep. Sakurai and Kinsaburo place Kong on a large raft and begin to transport him back to Japan. Mr. Tako arrives on the ship transporting Kong, but a JSDF ship stops them and orders them to return Kong to Faro Island. Meanwhile, Godzilla arrives in Japan and begins terrorizing the countryside. Kong wakes up and breaks free from the raft. Reaching the mainland, Kong engages Godzilla in a brief battle but retreats after Godzilla nearly burns him alive.
The JSDF dig a large pit laden with explosives and lure Godzilla into it, but Godzilla is unharmed. They next string up a barrier of power lines around the city filled with a 1,000,000 volts of electricity (50,000 volts were tried in the first film but failed to turn the monster back), which prove effective against Godzilla. Kong then approaches Tokyo and tears through the power lines, feeding off the electricity which seems to make him stronger. Kong then enters Tokyo and captures Fumiko, Sakurai's sister. The JSDF launch capsules full of the Faro Island berry juice and put Kong to sleep. The JSDF then decide to transport Kong via balloons to Godzilla, in hopes that they will kill each other.
The next morning, Kong is dropped next to Godzilla at the summit of Mt. Fuji and the two engage in a final battle. Godzilla initially has the advantage and nearly kills King Kong, but Kong regains his strength after absorbing electricity from a nearby lightning cloud. The monsters continue their fight and, after tearing through Atami Castle, fall off a cliff together into the Pacific Ocean. After an underwater battle, only King Kong resurfaces and begins to swim towards his island home. There is no sign of Godzilla, but the JSDF speculate it to be possible that he survived. The JSDF decide not to pursue Kong but rather, let him return home.
Production.
The film had its roots in an earlier concept for a new "King Kong" feature developed by Willis O'Brien, animator of the original stop-motion Kong. Around 1960, O'Brien came up with a proposed treatment, "King Kong meets Frankenstein", where Kong would fight against a giant version of Frankenstein's monster in San Francisco. O'Brien took the project (which consisted of some concept art and a screenplay treatment) to RKO to secure permission to use the King Kong character. During this time the story was renamed "King Kong vs. the Ginko" when it was believed that Universal had the rights to the Frankenstein name (they actually only had the rights to the monster's makeup design). O'Brien was introduced to producer John Beck who promised to find a studio to make the film (at this point in time RKO was no longer a production company). Beck took the story treatment and had George Worthing Yates flesh it out into a screenplay. The story was slightly altered and the title changed to "King Kong vs. Prometheus", returning the name to the original Frankenstein concept ("The Modern Prometheus" was the alternate name of Frankenstein in the original novel). Unfortunately, the cost of stop animation discouraged potential studios from putting the film into production. After shopping the script around overseas, Beck eventually attracted the interest of the Japanese studio Toho. Toho had long wanted to make a "King Kong" film and decided to replace the Frankenstein creature with Godzilla. They thought it would be the perfect way to celebrate their thirtieth year in production. John Beck's dealings with Willis O'Brien's project were done behind his back, and O'Brien was never credited for his idea. In 1963, Merian C. Cooper attempted to file a lawsuit against John Beck claiming that he outright owned the King Kong character, but the lawsuit never went through as it turned out he was not Kong's sole legal owner as he had previously believed.
Special effects director Eiji Tsuburaya was planning on working on other projects at this point in time such as a new version of a fairy tale film script called "Kaguyahime" ("Princess Kaguya"), but he postponed those to work on this project with Toho instead since he was such a huge fan of King Kong. He stated in an early 1960s interview with the Mainichi Newspaper, "But my movie company has produced a very interesting script that combined King Kong and Godzilla, so I couldn't help working on this instead of my other fantasy films. The script is special to me; it makes me emotional because it was "King Kong" that got me interested in the world of special photographic techniques when I saw it in 1933." 
Eiji Tsuburaya had a stated intention to move the Godzilla series in a lighter direction. This approach was not favoured by most of the effects crew, who "couldn't believe" some of the things Tsuburaya asked them to do, such as Kong and Godzilla volleying a giant boulder back and forth. But Tsuburaya wanted to appeal to children's sensibilities and broaden the genre's audience. This approach was favoured by Toho and to this end, "King Kong vs. Godzilla" has a much lighter tone than the previous two "Godzilla" films and contains a great deal of humor within the action sequences. With the exception of the next film, "Mothra vs Godzilla", this film began the trend to portray Godzilla and the monsters with more and more anthropomorphism as the series progressed, to appeal more to younger children. Ishirô Honda was not a fan of the dumbing down of the monsters. Years later Honda stated in an interview. "I don't think a monster should ever be a comical character." "The public is more entertained when the great King Kong strikes fear into the hearts of the little characters." The decision was also taken to shoot the film in a (2.35:1) scope ratio (Tohoscope) and to film in color (Eastman Color), marking both monsters' first widescreen and color portrayals. Additionally, the theatrical release was accompanied by both a true 4.0 stereophonic soundtrack, and a regular monaural mix.
Toho had planned to shoot this film on location in Sri Lanka, but had to forgo that (and scale back on production costs) because they ended up paying RKO roughly $200,000 (US) for the rights to the King Kong character. The bulk of the film was shot on Oshima (an island near Japan) instead. The movie's production budget came out to ¥5,000,000.
Suit actors Shoichi Hirose (King Kong) and Haruo Nakajima (Godzilla) were given a mostly free rein by Eiji Tsuburaya to choreograph their own moves. The men would rehearse for hours and would base their moves on that from professional wrestling (a sport that was growing in popularity in Japan), in particular the movies of Toyonobori.
During pre-production, Ishirō Honda had toyed with the idea of using Willis O'Brien's stop motion technique instead of the suitmation process used in the first two "Godzilla" films, but budgetary concerns prevented him from using the process, and the more cost efficient suitmation was used instead. However, some brief stop motion was used in a couple of quick sequences. One of these sequences was animated by Koichi Takano who was a member of Eiji Tsuburaya's crew.
A brand new Godzilla suit was designed for this film and some slight alterations were done to its overall appearance. These alterations included the removal of its tiny ears, three toes on each foot rather than four, enlarged central dorsal fins and a bulkier body. These new features gave Godzilla a more reptilian/dinosaurian appearance. Outside of the suit, a meter high model and a small puppet were also built. Another puppet (from the waist up) was also designed that had a nozzle in the mouth to spray out liquid mist simulating Godzilla's atomic breath. However the shots in the film where this prop was employed (far away shots of Godzilla breathing its atomic breath during its attack on the Arctic Military base) were ultimately cut from the film. These cut scenes can be seen in the Japanese theatrical trailer. Finally a separate prop of Godzilla's tail was also built for closeup practical shots when its tail would be used (such as the scene where Godzilla trips Kong with its tail). The tail prop would be swung offscreen by a stage hand.
The King Kong suit for this film has widely been considered to be one of the least appealing and most insipid gorilla suits in film history Sadamasa Arikawa (who worked with Eiji Tsuburaya) said that the sculptors had a hard time coming up with a King Kong suit that appeased Tsuburaya. The first suit was rejected for being too fat with long legs giving Kong an almost cute look. A few other designs were done before Tsuburaya would approve the final look that was ultimately used in the film. The suit was given two separate masks and two separate pairs of arms. Long arm extensions which contained poles inside the arms for Hirose to grab onto and with static immovable hands was used for long shots of Kong, while short human length arms were added to the suit for scenes that required Kong to grab items and wrestle with Godzilla. Besides the suit with the two separate arm attachments, a meter high model and a puppet of Kong (used for closeups) were also built. As well, a huge prop of Kong's hand was built for the scene where he grabs Mie Hama (Fumiko) and carries her off.
For the attack of the giant octopus, four live octopuses were used. They were forced to move among the miniature huts by having hot air blown onto them. After the filming of that scene was finished, three of the four were released. The fourth became special effects director Eiji Tsuburaya's dinner. Along with the live animals, two rubber octopus props were built, with the larger one being covered with plastic wrap to simulate mucus. Some stop motion tentacles were also created for the scene where the octopus grabs a native and tosses him.
Since King Kong was seen as the bigger draw (at the time, he was even more popular in Japan than Godzilla), and since Godzilla was still a villain at this point in the series, it led to the decision to not only give King Kong top billing, but also to present him as the winner of the climactic fight. While the ending of the film does look somewhat ambiguous, Toho confirmed that King Kong was indeed the winner in their 1962/63 English-language film brochure Toho Films Vol. 8, which states in the film's plot synopsis, "A spectacular duel is arranged on the summit of Mt. Fuji, and King Kong is victorious. But after he has won..."
English-language version.
When John Beck sold the "King Kong vs. Prometheus" script to Toho (which became "King Kong vs. Godzilla"), he was given exclusive rights to produce a version of the film for release in non-Asian territories. He was able to line up a couple of potential distributors in Warner Bros. and Universal-International even before the film began production. Beck, accompanied by two Warner Bros. representatives, attended at least two private screenings of the film on the Toho Studios lot before it was released in Japan.
John Beck enlisted the help of two Hollywood writers, Paul Mason and Bruce Howard, to write a new screenplay. After having discussions with Beck in regard to how the film would be handled, the two wrote the American version and worked with editor Peter Zinner to remove scenes, recut others, and change the sequence of several events. Mason and Howard, in order to give the film more of an American feel, came up with the idea of inserting newly-shot footage of stage and television actor Michael Keith playing newscaster Eric Carter, a UN reporter who spends much of the time commenting on the action from the UN Headquarters via an International Communication Satellite (ICS) broadcast, and Arnold Johnson, the head of the Museum of Natural History in New York, who tries to explain Godzilla's origin and his and Kong's motivations. The new footage was directed by Thomas Montgomery and shot in three days.
Beck and his crew were able to obtain library music from a host of older films (music tracks that had been composed by Henry Mancini, Hans J. Salter, and even a track from Heinz Roemheld). These films include "Creature from the Black Lagoon", "Bend of the River", "Untamed Frontier", "The Golden Horde", "Frankenstein Meets the Wolfman", "Man Made Monster", "Thunder on the Hill", "While the City Sleeps", "Against All Flags", "The Monster That Challenged the World", "The Deerslayer" and music from the TV series "Wichita Town". Cues from these scores were used to almost completely replace the original Japanese score by Akira Ifukube and give the film a more Western sound. They also obtained stock footage from the film "The Mysterians" from RKO (the film's US copyright holder at the time) which was used to not only represent the ICS, but which was also utilized during the film's climax. Stock footage of a massive Earthquake from "The Mysterians" was employed to make the earthquake caused by Kong and Godzilla's plummet into the ocean much more violent than the tame tremor seen in the Japanese version. This added footage features massive tidal waves, flooded valleys, and the ground splitting open swallowing up various huts.
Beck spent roughly $15,500 making his English version and sold the film to Universal-International for roughly $200,000 on April 29, 1963. The film opened in New York on June 26 of that year.
Starting in 1963, Toho's international sales booklets began advertising an English dub of "King Kong vs. Godzilla" alongside Toho-commissioned, uncut international dubs of movies such as "Matango" and "Atragon". By association, it is thought that this "King Kong vs. Godzilla" dub is part of an uncut, English language international version not known to have been released on home video.
Release.
This film was released in Germany as "Die Rückkehr des King Kong (The Return of King Kong)" and in Italy as "Il Trionfo Di King Kong (The Triumph of King Kong)"
In Japan, this film has the highest box office attendance figures of all of the "Godzilla" series to date. It sold 11.2 million tickets during its initial theatrical run accumulating ¥350,000,000 in grosses. The film was the 4th highest grossing film in Japan that year and was Toho's second biggest moneymaker. The film was re-released twice as part of the "Champion Matsuri" (東宝チャンピオンまつり), a film festival that ran from 1969 through 1978 that featured numerous films packaged together and aimed at children in 1970 and again in 1977. After these 2 theatrical re-releases the film accumulated a lifetime figure of 12,550,000 tickets sold.
After its theatrical re-releases, the film was screened two more times at specialty festivals. In 1979, to celebrate Godzilla's 25th anniversary, the film was reissued as part of a triple bill festival known as "The Godzilla Movie Collection" ("Gojira Eiga Zenshu"). It played alongside "Invasion of Astro-Monster" and "Godzilla vs Mechagodzilla". This release is known among fans for its exciting and dynamic movie poster featuring all the main Kaiju from these three films engaged in battle. Then in 1983, the film was screened as part of "The Godzilla Resurrection Festival" ("Gojira no Fukkatsu"). This large festival featured ten Godzilla/kaiju films in all. ("Godzilla", "King Kong vs Godzilla", "Mothra vs Godzilla", "Ghidorah, the Three-Headed Monster", "Invasion of Astro-Monster", "Godzilla vs Mechagodzilla", "Rodan", "Mothra", "Atragon", and "King Kong Escapes").
In North America, "King Kong vs Godzilla" premiered in New York City on June 26, 1963. After its theatrical run it accumulated a profit of $1.25 million via theatrical rentals.
Home media releases.
The Japanese version of this film was released numerous times through the years by Toho on different home video formats. The film was first released on VHS in 1985 and again in 1991. It was released on Laserdisc in 1986 and 1991, and then again in 1992 as part of a laserdisc box set called the "Godzilla Toho Champion Matsuri". Toho then released the film on DVD in 2001. They released it again in 2005 as part of the Godzilla Final Box DVD set, and again in 2010 as part of the Toho Tokusatsu DVD Collection. This release was volume #8 of the series and came packaged with a collectible magazine that featured stills, behind the scenes photos, interviews, and more. In the Summer of 2014, the film was released for the first time on Blu-Ray as part of the company releasing the entire series on the Blu-Ray format for Godzilla's 60th anniversary.
The American version was released on VHS by GoodTimes Entertainment (which acquired the license of some of Universal's film catalogue) in 1987, and then on DVD to commemorate the 35th anniversary of the film's U.S release in 1998. Both these releases were full-frame. Universal Studios itself released the English-language version of the film on DVD in widescreen as part of a two-pack bundle with "King Kong Escapes" in 2005, and then re-released the film on Blu-Ray on April 1, 2014, along with "King Kong Escapes".
DVD
R1 America - Goodtimes Home Video - 35th Anniversary Edition
R1 America - Universal Pictures
Blu-ray
Preservation.
The original Japanese version of this film is infamous for being one of the most poorly preserved tokusatsu films. In 1970, director Ishiro Honda prepared an edited version of the film for the Champion Festival, a children's matinee program that showcased edited re-releases of older kaiju films along with cartoons and then-new kaiju films. In total, Honda cut twenty-four minutes. Unfortunately, the cuts were done to the film's original negative, and as a result, the highest quality source for the cut footage was lost. For years all that was thought to remain of the uncut 1962 version was a faded, heavily damaged 16mm element from which rental prints had been made. 1980s restorations for home video integrated the 16mm deleted scenes into the 35mm Champion cut, resulting in wildly inconsistent picture quality.
In 1991, Toho issued a restored laserdisc incorporating the rediscovery of a reel of 35mm trims of the deleted footage. The resultant quality was far superior to previous reconstructions, but not perfect; an abrupt cut caused by missing frames at the beginning or end of a trim is evident whenever the master switches between the Champion cut and a 35mm trim within the same shot. This laserdisc master was utilized for Toho's 2001 DVD release with few changes.
In 2014, Toho released a new restoration of the film on Blu-Ray, which utilized the 35mm trims once again, but only those available for reels 2-7 of the film were able to be located. The remainder of video for the deleted portions was sourced from the earlier Blu-Ray of the U.S. version, in addition to the previous 480i 1991 laserdisc master.
Legacy.
Due to this film's great box office success, Toho planned to do a sequel almost immediately. The sequel was simply called "Continuation: King Kong vs. Godzilla". Apparently though, the project never evolved past that announcement.
Also due to the great box office success of this film, Toho was convinced to build a franchise around the character of Godzilla and started producing sequels on a yearly basis. The next project was to pit Godzilla against another famous movie monster icon: a giant version of the Frankenstein monster. In 1963, Kaoru Mabuchi (a.k.a. Takeshi Kimura) wrote a script called "Frankenshutain tai Gojira". Ultimately, Toho rejected the script and the next year pitted Mothra against Godzilla instead, in the 1964 film "Mothra vs. Godzilla". This began an intra-company style crossover where kaiju from other Toho kaiju films would be brought into the Godzilla series. Frankenstein himself would later be introduced into the Toho canon with "Frankenstein Conquers the World", which was followed by a sequel, "War of the Gargantuas".
Toho was eager to build a series around their version of King Kong but were refused by RKO. They worked with the character again in 1967 though, when they helped Rankin/Bass co produce their film "King Kong Escapes" (which was loosely based on a cartoon series R/B had produced). That film, however, was not a sequel to "King Kong vs. Godzilla".
Henry Saperstein (whose company UPA co-produced the 1965 film "Frankenstein Conquers the World" and the 1966 film "War of the Gargantuas" with Toho) was so impressed with the octopus sequence that he requested the creature to appear in these two productions. The giant octopus appeared in an alternate ending in "Frankenstein Conquers the World" that was intended specifically for the American market but was ultimately never used. The creature did reappear at the beginning of the film's sequel "War of the Gargantuas" this time being retained in the finished film.
Even though it was only featured in this one film (although it was used for a couple of brief shots in "Mothra vs. Godzilla"), this Godzilla suit was always one of the more popular designs among fans from both sides of the Pacific. It formed the basis for some early merchandise in the US in the 1960s, such as a popular model kit by Aurora Plastics Corporation, and a popular board game by Ideal Toys. This game was released alongside a King Kong game in 1963 to coincide with the US theatrical release of the film.
The King Kong suit from this film was redressed into the giant monkey Goro for episode 2 ("GORO and Goro") of the television show Ultra Q. Afterwards it was reused for the water scenes (although it was given a new mask/head) for the film "King Kong Escapes".
Scenes of the giant octopus attack were reused in black and white for episode 23 ("Fury of the South Seas") of the television show Ultra Q.
A scene from this film was reused as stock footage in the 1972 film "Godzilla vs. Gigan". The scene of the construction vehicles digging the giant pit to trap Godzilla, was reused to portray the construction vehicles building the World Children's Land theme park in "Godzilla vs Gigan".
In 1992 (to coincide with the company's 60th anniversary), Toho wanted to remake this film as "Godzilla vs. King Kong" as part of the Heisei series of "Godzilla" films. However, according to the late Tomoyuki Tanaka, it proved to be difficult to obtain permission to use King Kong. Next, Toho thought to make "Godzilla vs. Mechani-Kong" but, (according to Koichi Kawakita), it was discovered that obtaining permission even to use the likeness of King Kong would be difficult. Mechani-Kong was replaced by Mechagodzilla, and the project eventually evolved into "Godzilla vs. Mechagodzilla II" in 1993.
In making "", the special effects crew was instructed to watch the giant octopus scene to get reference for the Kraken.
Through the years the film has been referenced in various songs, advertising, television shows and comic books. It was referenced in Da Lench Mob's 1992 single "Guerillas in tha Mist". It was spoofed in advertising for a Bembos burger commercial from Peru, for Ridsect Lizard Repellant, and for the board game Connect 4. It was paid homage to in comic books by DC Comics, Bongo Comics, and Disney Comics. It was even spoofed in "The Simpsons" episode "Wedding for Disaster". 
In 2015, Legendary Pictures announced plans for a "Godzilla vs. Kong" film of their own (unrelated to Toho's version) targeted for a 2020 release date.
Dual ending myth.
For many years a popular myth has persisted that in the Japanese version of this film, Godzilla emerges as the winner. The myth originated in the pages of "Spacemen" magazine, a 1960s sister magazine to the influential publication "Famous Monsters of Filmland". In an article about the film, it is incorrectly stated that there were two endings and "If you see "King Kong vs Godzilla" in Japan, Hong Kong or some Oriental sector of the world, Godzilla wins!" The article was reprinted in various issues of "Famous Monsters of Filmland" in the years following such as issues 51, and 114. This bit of incorrect info would be accepted as fact and persist for decades, transcending the medium and into the mainstream. For example, decades later in the 1980s the myth was still going strong. The Genus III edition of the popular board game Trivial Pursuit had a question that asked "Who wins in the Japanese version of "King Kong vs. Godzilla"?", and states that the correct answer is "Godzilla". As well, through the years, this myth has been misreported by various members of the media, and has been misreported by reputable news organizations such as The LA Times. Since seeing the original Japanese-language versions of Godzilla movies was very hard to come by from a Western standpoint during this time period, it became easily believable.
However, as more Westerners were able to view the original version of the film (especially after its availability on home video during the late 1980s), and gain access to Japanese publications about the film, the myth was dispelled. There is only one ending of this film. Both versions of the film end the same way: Kong and Godzilla crash into the ocean, and Kong is the only monster to emerge and swims home. The only differences between the two endings of the film are extremely minor and trivial ones:
In 1993, comic book artist Arthur Adams wrote and drew a one-page story that appeared in the anthology "Urban Legends #1", published by Dark Horse Comics, which dispels the popular misconception about the two versions of "King Kong vs. Godzilla".

</doc>
<doc id="11992" url="https://en.wikipedia.org/wiki?curid=11992" title="Godzilla vs. the Sea Monster">
Godzilla vs. the Sea Monster

Godzilla vs. The Sea Monster (also known as Ebirah, Horror of the Deep and released in Japan as is a 1966 Japanese science fiction kaiju film produced by Toho. Directed by Jun Fukuda with special effects by Sadamasa Arikawa (supervised by Eiji Tsuburaya), the film starred Akira Takarada, Akihiko Hirata, and Eisei Amamoto. The seventh film in the Godzilla series, this was the first of two island-themed adventure films starring Godzilla.
The film was released direct to television in the United States in 1967 by the Walter Reade organization as Godzilla versus The Sea Monster.
Plot.
After Yata (Toru Ibuki) is lost at sea, his brother Ryota (Toru Watanabe) steals a yacht with his two friends and a bank robber, the crew runs afoul of the giant lobster Ebirah, and washes up on the shore of an island, where a terrorist organization manufactures heavy water for their purposes, as well as a chemical that keeps Ebirah at bay. The organization, known as the Red Bamboo, has enslaved natives from Infant Island to help them, but the natives hope to awaken Mothra (now a full-grown moth metamorphosed from the larva that appeared in Ghidorah, the Three-Headed Monster) to rescue them.
In their efforts to avoid capture, Ryota and his friends, aided by a beautiful native girl, stumble across Godzilla sleeping within a cliffside cavern. The group devises a plan to defeat the Red Bamboo and escape from the island. In the process, they wake Godzilla using a lightning rod. Godzilla fights Ebirah, but the giant crustacean escapes. Godzilla is then attacked by a giant condor and a squadron of Red Bamboo fighter jets, but destroys them.
The humans retrieve the missing Yata, free the enslaved natives and Godzilla begins to destroy the base. Godzilla smashes a tower that has a self-destruct button that makes the island unstable. Godzilla fights Ebirah and defeats it, ripping off both Ebirah's claws and causing it to retreat into the sea. The natives summon Mothra to save everyone, however, Godzilla challenges Mothra when she gets to the island. Mothra manages to push Godzilla away and carry the people off. Godzilla escapes the island just before it explodes.
Production.
The film was originally written for King Kong (the project was tentatively
titled "Operation Robinson Crusoe:King Kong vs Ebirah"), but Toho switched Kong with another popular character at the time, Godzilla. This explains why Godzilla displays uncharacteristic behavior in the film, such as drawing strength from electricity, its curiosity to Kumi Mizuno's character, the usage of boulders to destroy the Red Bamboo Base, and its hostilities with Mothra (despite them now being allies). Toho would later use Kong for "King Kong Escapes".
The US television version and early video versions have a different opening to the film. The opening scenes of Ryota at the Maritime Safety Agency searching for news of his brother have been replaced with a scene supposedly showing Ebirah destroying Yata's boat. This sequence was created by editing a later scene in the movie. The current DVD version of the film restores the Japanese cut.
In 1991, "Godzilla vs. the Sea Monster" was distributed under the Film Ventures International name. The company replaced the opening with a generic credit sequence, using footage from "Son of Godzilla". This version was aired on "Mystery Science Theater 3000".
English version.
In 1967, the film was released directly to television in North America by the Walter Reade Organization. It was the first Godzilla film to not receive North American theatrical distribution. As with Walter Reade Organizations' next Godzilla release, "Son of Godzilla", this movie was dubbed by Titra Studios.
There were several alterations made for this release:
The English version runs 83 minutes, four minutes shorter than the Japanese version.
In 2005, Sony released the film on DVD. This was the first time the original Japanese version had been officially released on home video in the United States. The original American version, however, was replaced by Toho's international version, which is uncut. The dubbing in this version is often cited as inferior to the original Titra Studios dub.
Box office.
In Japan, the film was released on December 17, 1966 and sold approximately 3,450,000 tickets. It was re-released on July 22, 1972 and sold approximately 760,000 tickets.
Home media releases.
"Kraken Releasing" - Blu-ray
Sony Pictures - DVD

</doc>
<doc id="11993" url="https://en.wikipedia.org/wiki?curid=11993" title="Son of Godzilla">
Son of Godzilla

Son of Godzilla, released in Japan as , is a 1967 Japanese science fiction kaiju film produced by Toho. Directed by Jun Fukuda with special effects by Sadamasa Arikawa (supervised by Eiji Tsuburaya), the film starred Tadao Takashima, Akira Kubo, and Akihiko Hirata. The eighth film in the Godzilla series, it was also the second of two island-themed Godzilla adventures that Toho produced with slightly smaller budgets than most of the Godzilla films from this time period. Continuing the trend of shifting the series towards younger audiences, the film introduced an infant Godzilla named Minilla.
The film was released straight to television in the United States in 1969 by the Walter Reade organization.
Plot.
A team of scientists are trying to perfect a weather-controlling system. Their efforts are hampered by the arrival of a nosy reporter and by the sudden presence of 2-meter tall giant praying mantises. The first test of the weather control system goes awry when the remote control for a radioactive balloon is jammed by an unexplained signal coming from the center of the island. The balloon detonates prematurely, creating a radioactive storm that causes the giant mantises to grow to enormous sizes. Investigating the mantises, which are named Kamacuras (Gimantis in the English-dubbed version), the scientists find the monstrous insects digging an egg out from under a pile of earth. The egg hatches, revealing a baby Godzilla. The scientists realize that the baby's telepathic cries for help were the cause of the interference that ruined their experiment. Shortly afterwards, Godzilla arrives on the island in response to the infant's cries, demolishing the scientist's base while rushing to defend the baby. Godzilla kills two of the Kamacuras during the battle while one manages to fly away to safety, Godzilla then adopts the baby.
The baby Godzilla, named Minilla, quickly grows to about half the size of the adult Godzilla and Godzilla instructs it on the important monster skills of roaring and using its atomic ray. At first, Minilla has difficulty producing anything more than atomic smoke rings, but Godzilla discovers that stressful conditions (i.e. stomping on its tail) or motivation produces a true radioactive blast. Minilla comes to the aid of Reiko when she is attacked by a Kamacuras, but inadvertently awakens Kumonga (Spiga in the English-dubbed version), a giant spider that was sleeping in a valley. Kumonga attacks the caves where the scientists are hiding, and Minilla stumbles into the fray.
Kumonga traps Minilla and the final Kamacuras with his webbing, but as Kumonga begins to feed on the deceased Kamacuras, Godzilla arrives to save the day. Godzilla saves Minilla and they work together to defeat Kumonga by using their atomic rays on the giant spider. Hoping to keep the monsters from interfering in their attempt to escape the island, the scientists finally use their perfected weather altering device on the island and the once tropical island becomes buried in snow and ice. As the scientists are saved by an American submarine, Godzilla and Minilla begin to hibernate as they wait for the island to become tropical again.
Box office.
In Japan, the film sold approximately 2,480,000 tickets.
Reception.
"Son of Godzilla" has received generally positive reviews. The film currently holds a 63% rating on Rotten Tomatoes.
English version.
Shortly after the film's Japanese release, Toho had "Son of Godzilla" dubbed into English by Frontier Enterprises in Tokyo. As with nearly all Toho international versions, the dubbed version corresponds directly to uncut Japanese film. Frontier Enterprises owner William Ross dubs Dr. Kusumi (Tadao Takashima), while the part of Goro Maki (Akira Kubo) is dubbed by Burr Middleton, son of Charles B. Middleton. This version of the film was released on video in 1992 by PolyGram Video, Ltd. in the United Kingdom.
In the United States, "Son of Godzilla" was distributed directly to television by the Walter Reade Organization in 1969. The movie was re-dubbed by Titan Productions, Inc in New York. Peter Fernandez wrote and directed the dubbing script and voiced Goro Maki. Walter Reade Organization deleted almost all of the pre-credit sequence. All that remains in this version is a brief shot of Godzilla roaring and approaching the camera. The opening credits are also deleted, although the underlying footage is still present. In both English dubs, the monsters Kamacuras and Kumonga are called "Gimantis" and "Spiega", respectively. The character, "Saeko", is also called "Reiko" in both dubbed versions.
The original US version of the film was the one seen on American television and home video for over thirty years. 
In 2004, TriStar released the international version on DVD with the original Japanese audio included as an extra audio track. However, this DVD (along with their Godzilla vs. Mechagodzilla DVD) has been out of print for years, causing the price to shoot up. There are currently no plans for a future Region 1 DVD/Blu-ray release.
DVD release.
Sony Pictures (Out of print)

</doc>
<doc id="11994" url="https://en.wikipedia.org/wiki?curid=11994" title="Destroy All Monsters">
Destroy All Monsters

Destroy All Monsters, released in Japan as , is a 1968 Japanese science fiction Kaiju film produced by Toho. The ninth entry in the original "Godzilla" series, it stars Akira Kubo, Jun Tazaki, Yukiko Kobayashi and Yoshio Tsuchiya. Produced in celebration as Toho's 20th "kaiju" film, it was also originally intended to be the final "Godzilla" film, and as such, was given a bigger budget than the past few productions. Set at the end of the 20th century, the film features many of Toho's earlier monsters, eleven in all. The film was also the last to be produced by the main creators of the Godzilla character, with Ishirō Honda directing, Eiji Tsuburaya supervising the special effects, Tomoyuki Tanaka producing, and Akira Ifukube handling the film's score.
The film was released theatrically in the United States starting in May 1969 by American International Pictures.
Plot.
At the close of the 20th century, all of the Earth's kaiju have been collected by the United Nations Science Committee and confined in an area known as Monsterland, located in the Ogasawara island chain. A special control center is constructed underneath the island to ensure that the monsters stay secure, and to serve as a research facility to study them.
When communications with Monsterland are suddenly and mysteriously severed, and all of the monsters begin attacking world capitals, Dr. Yoshida of the UNSC orders Captain Yamabe and the crew of his spaceship, Moonlight SY-3, to investigate Ogasawara. There, they discover that the scientists, led by Dr. Otani, have become mind-controlled slaves of a feminine alien race identifying themselves as the Kilaaks, who reveal that they are in control of the monsters. Their leader demands that the human race surrender, or face total annihilation.
Godzilla attacks New York City, Rodan invades Moscow, Mothra (a larvae offspring) lays waste to Beijing, Gorosaurus (wrongly identified as Baragon...who might have dug the tunnel the former emerges from) destroys Paris, and Manda attacks London. These attacks were set in to motion to draw attention away from Japan, so that the aliens can establish an underground stronghold near Mt. Fuji in Japan. The Kilaaks then turn their next major attack on to Tokyo and, without serious opposition, become arrogant in their aims, until the UNSC discover that the Kilaaks have switched to broadcasting the control signals from their base under the Moon's surface. In a desperate battle, the crew of the SY-3 destroys the Kilaak's lunar outpost and returns the alien control system to Earth.
With all of the monsters under the control of the UNSC, the Kilaaks unleash their hidden weapon, King Ghidorah. The three-headed space monster is dispatched to protect the alien stronghold at Mt. Fuji, and battles Godzilla, Minilla, Mothra, Rodan, Gorosaurus, Anguirus, and Kumonga (Manda, Baragon and an unnamed Varan are also present but do not take part in the battle). While seemingly invincible, King Ghidorah is eventually overpowered by the combined strength of the Earth monsters and is killed. Refusing to admit defeat, the Kilaaks produce their trump card, a burning monster they call the Fire Dragon, which begins to torch cities and destroys the control center on Ogasawara. Suddenly, Godzilla attacks and destroys the Kilaak's underground base, revealing that the Earth's monsters instinctively know who their enemies are. Captain Yamabe then pursues the Fire Dragon in the SY-3, and narrowly achieves victory for the human race. The Fire Dragon is revealed to be a flaming Kilaak saucer and is destroyed. Godzilla and the other monsters are eventually returned to Monsterland to live in peace.
Production.
Original screenplay.
There was an initial screenplay with the preliminary title "All Monsters Attack Directive", which would have many of the same elements used in the final product. The difference, however, was in the monster line-up. This first draft included several monsters that would appear in the final film, such as Godzilla, Mothra, King Ghidorah, Rodan, Baragon, Varan, Kumonga, and Manda. The final two monsters were Maguma (from 1962's "Gorath") and Ebirah (from 1966's "Godzilla vs. the Sea Monster"). Maguma was to be one of the guardians of the Kilaak base with Baragon, who would have been the ones to fend off the SDF. Ebirah's role is unknown. The film’s title was later changed to "Kaiju Soshingeki" ("Charge of the Monsters"), and Ebirah and Maguma were replaced with Anguirus, Minilla and Gorosaurus.
U.S. version.
American International Pictures released the film theatrically in North America in 1969. The Americanization was handled by Titan Productions (formerly Titra Studios).
Among the changes for the U.S. release:
In the Japanese version, the credits come right after the Moonlight SY-3 blasts off at the beginning of the movie. The American version moved the credits to the end of the picture.
This version has been replaced on home video and television by Toho's international version. While uncut and letterboxed, it features an English dub track produced by William Ross' Tokyo-based "Frontier Enterprises" in 1968.
Critical reception.
"Destroy All Monsters" has been generally well received. "The New York Times" did not review the film upon release, but film critic Howard Thompson gave it a positive review on a re-release at a children's matinee in December 1970. He commented that "the feature wasn't bad at all of this type. The trick photography and especially the blended sweep and skill of the miniature settings provided the visual splash. The human beings, with good dubbed English voices, were a personable lot as they wrestled with some outer space culprits who had rounded up Japan's favorite monsters and turned them against the planet earth."
Among modern critics, Steve Biodrowski of "Cinefantastique" wrote, "In the end, "Destroy All Monsters" is too slim in its storyline, too thin in its characterizations, to be considered a truly great film. It is not as impressive as the original "Godzilla", and it is not as hip as "Monster Zero". But for the ten-year-old living inside us all, it is entertainment of the most awesome sort." Matt Paprocki of "Blogcritics" said the film is "far from perfect" and "can be downright boring at times" but felt that "the destruction scenes make up for everything else" and "the final battle is an epic that simply can't be matched".
On the review aggregator website Rotten Tomatoes, the film has a "certified fresh" rating of 80%.
Home Media releases.
ADV Films
ADV Films
Media Blasters/Tokyo Shock (Also on Blu-ray)
Legacy.
In "", the three-part "Monster Wars" story appears to have been inspired by "Destroy All Monsters", with aliens taking control of Godzilla and other giant creatures and using them to attack the world's cities in preparation for invasion. At the end, their island hideout is used as a secret reserve for the surviving monsters.
"Godzilla" director Gareth Edwards has expressed an interest in making a sequel to his 2014 movie inspired by "Destroy All Monsters".

</doc>
<doc id="11998" url="https://en.wikipedia.org/wiki?curid=11998" title="Godzilla vs. Megalon">
Godzilla vs. Megalon

The film was released theatrically in the United States in the summer of 1976 by Cinema Shares. Afterwards it became the only Godzilla film to receive a television premiere on a major U.S network, as NBC aired it on prime time television in the summer of 1977, where it was hosted by actor John Belushi dressed in a Godzilla costume.
Plot.
In the year 1973, the most recent underground nuclear test, set off near the Aleutians, sends shockwaves as far south as Monster Island, disturbing the monsters, causing Anguirus to fall into a fault opened up by the consequential earthquakes and Rodan to fly off, while Godzilla decides to stay put.
For years, Seatopia, an undersea civilization, has been heavily affected by this nuclear testing conducted by the surface nations of the world. Upset by these tests, the Seatopians plan to unleash their civilization's beetle-like god, Megalon, to destroy the surface world out of vengeance.
On the surface, an inventor named Goro Ibuki, his nephew Rokuro and their friend Hiroshi Jinkawa are off on an outing near a lake when Seatopia makes itself known to the Earth by drying up the lake the trio was relaxing nearby and using it as a base of operation. As they return home they are ambushed by agents of Seatopia who are trying to steal Jet Jaguar, a humanoid robot under construction by the trio of inventors. However the Agents' first attempt is botched and they are forced to flee to safety.
Some time later, Jet Jaguar is completed but the trio of inventors are knocked unconscious by the returning Seatopian agents. The agents' plan is to use Jet Jaguar to guide and direct Megalon to destroy whatever city Seatopia commands. Goro and Rokuro are sent to be killed, while Hiroshi is taken hostage. Megalon is finally released to the surface while Jet Jaguar is put under the control of the Seatopians and is used to guide Megalon to attack Tokyo with the Japan Self Defense Forces failing to defeat the monster. Eventually, the trio of heroes manage to escape their situation with the Seatopians and reunite to devise a plan to send Jet Jaguar to get Godzilla's help using Jet Jaguar's secondary control system.
After uniting with Japan's Defense Force, Goro manages to regain control of Jet Jaguar and sends the robot to Monster Island to bring Godzilla to fight Megalon. Without a guide to control its actions, Megalon flails around relentlessly and aimlessly fighting with the Defense Force and destroying the outskirts of Tokyo. The Seatopians learn of Jet Jaguar's turn and thus send out a distress call to the Nebula M aliens (from the previous film) to send Gigan to assist them.
As Godzilla journeys to fight Megalon, Jet Jaguar programs into a safeguard mode and grows to gigantic proportions to face Megalon itself until Godzilla arrives. The battle is roughly at a standstill between robot and monster, until Gigan arrives and both Megalon and Gigan double team Jet Jaguar. Godzilla finally arrives to assist Jet Jaguar and the odds become evened. After a long and brutal fight, Gigan and Megalon both retreat and Godzilla and Jet Jaguar shake hands on a job well done. Godzilla returns to Monster Island, and Jet Jaguar returns to its previous, human-sized state and reunites with its inventors.
Production.
"Godzilla Vs. Megalon" was originally planned as a non-"Godzilla" film, a solo vehicle for Jet Jaguar, which was the result of a contest Toho had for children in mid-to-late 1972. The winner of the contest was an elementary school student, who submitted the drawing of a robot called Red Arone, which superficially resembled both Ultraman and Mazinger Z. The robot was renamed Jet Jaguar and was set to star in "Jet Jaguar vs. Megalon", which pitted him against Megalon. However, after doing some screen tests and storyboards, Toho figured Jet Jaguar would not be able to carry the film on his own, either in screen appearance or marketing value, so they shut the project down during pre-production. Nearly a month later, producer Tomoyuki Tanaka called in screenwriter Shinichi Sekizawa to revise the script to add Godzilla and Gigan. To make up for lost production time, the film was shot in a hasty three weeks. The production time totaled at nearly six months, from planning to finish.
According to Teruyoshi Nakano, the Godzilla suit made for this film (known as the Megaro-Goji) was made in a week, the fastest featured Godzilla suit ever made to date. Godzilla was portrayed by stunt actor Shinji Takagi.
There are three notable deleted scenes. A scene towards the end of the film in which Antonio ponders aloud if sending Megalon to destroy the world above is really any different from what the people above are doing with atomic testing. Another is a roughly minute-long "conversation" between Gigan and Megalon that consists of quirky gestures and bodily movements. One that can be seen in the Japanese trailer has Jet Jaguar blinding Megalon with his flashlight eyes right before Megalon starts to kick at him while Gigan holds him down.
There are, interestingly, no major female characters in the movie, making this the only "Godzilla" film without a female lead.
Toho's popular kaiju character Anguirus appears in some stock footage from "Destroy All Monsters" and in two newly filmed scenes on Monster Island. In the second new scene, Anguirus appears largely as he did in the previous film, "Godzilla vs. Gigan". The first brief scene of Anguirus shows the monster without his fangs. This modification would carry over into his last appearance the next year in "Godzilla vs. Mechagodzilla".
English versions.
In 1976, Cinema Shares released "Godzilla vs. Megalon" theatrically. Riding the coattails of Dino De Laurentiis' big-budget "King Kong" remake, the poster art showed Godzilla and Megalon battling on top of the World Trade Center, despite the fact that no scenes were set in New York City. As it would with its later Godzilla releases, Cinema Shares opted to use the English dub created by Toho.
Cinema Shares originally released the film with very few edits. Eventually, more cuts were made, supposedly to keep a "G"-rating from the MPAA. Edits include:
With this being the first of the three Cinema Shares "Godzilla" releases, the publicity factor was high. Along with the poster, buttons with one of the four monsters' faces on them were released. Given away at theatrical showings was a comic herald that told a simplified version of the film. There were several errors like monster's names and locations and events. The theatrical trailer for the film also contain these errors, most notably Jet Jaguar being called "Robotman."
The press kit also included Godzilla, Megalon, Gigan, and Jet Jaguar in cars. This is a reflection of the Aurora kits with Godzilla riding a race car. Along with the press kit was a "Vote Godzilla for President" ad that, if mailed in, resulted in the receipt of a free ticket to the film.
"Godzilla vs. Megalon" was given a high-profile prime-time NBC network premiere in 1977, with an introduction and bumper segments by John Belushi in a Godzilla suit also used on Saturday Night Live. NBC extensively cut the film so that it would fit in a one-hour time slot.
Box office.
In Japan, "Godzilla vs. Megalon" sold approximately 980,000 tickets. It was the first "Godzilla" film to sell less than one million admissions.
The film was a huge success in American theaters, earning $383,744 in its first three days in Texas and Louisiana alone.
Critical reception.
"Godzilla vs. Megalon" was released theatrically in America on May 9, 1976, though the "San Francisco Chronicle" indicates that it opened there in June, and "The New York Times" indicates that it opened in New York City on July 11. "New York Times" film critic Vincent Canby, who a decade before had given a negative review to "Ghidorah, the Three-Headed Monster", gave "Godzilla vs. Megalon" a generally positive review. In his review on July 12, 1976, Canby said, ""Godzilla vs. Megalon" completes the canonization of Godzilla...It's been a remarkable transformation of character - the dragon has become St. George...It's wildly preposterous, imaginative and funny (often intentionally). It demonstrates the rewards of friendship, between humans as well as monsters, and it is gentle."
While Megalon and Gigan are still remembered by fans and reused in many videogames, Jet Jaguar is sometimes seen as more of a joke or a spinoff of Ultraman.
Legacy.
"Godzilla vs. Megalon" has attracted the ire of many Godzilla fans in the decades since its original release. The film contributed to the reputation of "Godzilla" films in the United States as cheap children's entertainment that should not be taken seriously. It's been described as "incredibly, undeniably, mind-numbingly bad" and one of the "poorer moments" in the history of kaiju films.
In particular, the special effects of the film have been heavily criticized. One review described the Godzilla costume as appearing to be "crossed with Kermit the Frog" and another sneeringly compared it to "Godzilla vs. Gigan", stating that it did ""everything wrong that Gigan did, and then some."" However, most of the criticism is of the lack of actual special effects work, as most of it consists of stock footage from previous films, including "Godzilla vs. Gigan" and "Ghidorah, the Three-Headed Monster", and a few pieces of effects work has garnered praise, specifically a scene where Megalon breaks through a dam and the draining of the lake.
The other aspects of the film have been similarly skewered. The acting is usually described as flat and generally poor, and as not improving, or sometimes, worsening, the already weak script. One part of the film, on the other hand, has garnered almost universal praise: Godzilla's final attack on Megalon, a flying kick. It has been called the saving grace of the film, and was made famous by the mock exclamations of shock and awe displayed on "Godzilla vs. Megalon"'s appearance on "Mystery Science Theater 3000". Through the end of season three to the middle of season five, that clip would be shown at the opening of each show.
Despite all this, the film is also one of the most widely seen Godzilla films in the United States — it was popular in its initial theatrical release, largely due to an aggressive marketing campaign, including elaborate posters of the two title monsters battling atop New York City's World Trade Center towers, presumably to capitalize on the hype surrounding the Dino De Laurentiis remake of "King Kong", which used a similar image for its own poster.
Home media releases.
Media Blasters (Tokyo Shock) has acquired the DVD rights to "Godzilla vs. Megalon" and "Destroy All Monsters". Both films were released under the company's division, Tokyo Shock. Media Blasters originally planned to release "Godzilla vs. Megalon" on DVD and Blu-ray on December 20, 2011; however, due to technical difficulties with the dubbing and Toho yet to give its approval for the release, the DVD / Blu-ray release was delayed. Media Blasters finally released the film on August 14, 2012 but only on a bare-bones DVD. Also, a manufacturing error led to the originally planned version featuring bonus content to be released by accident. These special feature versions are incredibly rare, and are not labelled differently from the standard version, making them nearly impossible to find.
Blu-ray-Media Blasters-Tokyo Shock
Alpha Video
Pulf Video
Madman Entertainment
The Mystery Science Theater 3000 Collection, Volume 10 (Rhino Home Video)
Media Blasters-Tokyo Shock

</doc>
<doc id="12000" url="https://en.wikipedia.org/wiki?curid=12000" title="Godzilla vs. Biollante">
Godzilla vs. Biollante

The film originated from a public story-writing contest, and set a trend common to all Heisei era movies of Godzilla facing off against opponents capable of metamorphosing into new, progressively more powerful forms. Although it was very positively received and maintained the dark, anti-nuclear atmosphere of its immediate predecessor, "The Return of Godzilla", it performed much poorer at the Japanese box-office, thus prompting Toho to make a shift from a realistic science fiction line to a more family-orientated set of films featuring more iconic and familiar monsters.
Plot.
In the aftermath of Godzilla's attack on Tokyo and his later imprisonment at Mt. Mihara, a team of scientists discovers cells left in the rubble by the monster and collects them. A group of American militants takes a sample of their own. They attempt to escape with them, but are killed by a lone assassin codenamed SSS9, who steals the sample and departs. He boards a cruise liner and heads to the Republic of Saradia to deliver the cells to the Saradia Institute of Technology and Science. The president of the institute wants to use the cells to merge with genetically modified plants, hoping to transform Saradia's deserts to vast greenery and end the country's dependence on oil wells for wealth. Dr. Genshiro Shiragami and his daughter, Erika, are enlisted to aid with the ambitious project. However, a terrorist bombing destroys the institute's laboratory, ruining the cells and killing Erika.
Five years later, Shiragami has returned to Japan and has merged some of Erika's cells with those of a rose. He hopes to have Erika's soul continue living in the plant. Psychic Miki Saegusa, who heads an institution for intuitive children, aids him in his research. The JSDF are using the Godzilla cells they collected to create "Anti-Nuclear Energy Bacteria", hoping it can serve as a fatal weapon against Godzilla should he return. Scientist Kazuhito Kirishima and Lieutenant Goro Gondo are assigned to lead the project. They attempt to recruit Shiragami to aid them, but a mournful Shiragami declines. International tensions are increasing over the Godzilla cells: an American biological corporation called Bio-Major wants the cells for its own personal gains, as does the Saradia Institute of Technology and Science. Bio-Major sends two agents to Japan, while Saradia sends SSS9. An explosion from Mt. Mihara causes tremors across the area, including Shiragami's home. The roses are badly damaged, and Shiragami realizes he needs a drastic plan to save Erika's soul. He agrees to join the JSDF's effort, giving him access to the Godzilla cells. He merges them with the roses at his lab. A night later, Bio-Major breaks into Shiragami's lab to steal the cells, but are ambushed by SSS9. A large, tentacle-like vine emerges and attacks the trio, killing a Bio-Major agent before the other two escape. The creature escapes to a nearby lake, transforming to a giant plant-like being that Shiragami names "Biollante."
The Bio-Major agent, in a desperate attempt to procure the cells, plants explosives around Mt. Mihara and issues an ultimatum to the Diet of Japan, warning that if he does not get the cells, he will detonate the explosives and free Godzilla. Kirishima and Gondo attempt to trade, but SSS9 thwarts the attempt and runs off with the cells. The explosives go off, and Godzilla escapes. He attempts to reach the nearest power plant to replenish his supply of nuclear energy, but Biollante calls out to him. Godzilla arrives at the lake to engage Biollante in a vicious battle, and emerges as the victor. Godzilla proceeds toward the power plant at Tsuruga, but Miki uses her psychic powers to divert him toward Osaka instead. The city is quickly evacuated, and a team of militants, led by Gondo, meet Godzilla at the central district and fire rockets infused with the anti-nuclear bacteria into his body. Gondo is killed in the process, and an unaffected Godzilla leaves the city.
Kirishima recovers the cells and returns them to the JSDF. Shiragami theorizes that if Godzilla's body temperature is increased, the bacteria should work against him. The JSDF forms microwave-emitting plates during an artificial thunderstorm, hitting Godzilla with lightning and heating up his body temperature during a battle in the mountains outside Osaka. Godzilla is only moderately affected, but Biollante arrives to engage him in battle once again. The fight ends after Godzilla fires his atomic heat ray inside Biollante's mouth. An exhausted Godzilla collapses on the beach, and Biollante disintegrates and her spores float to the sky, forming an image of Erika amongst the stars. Shiragami, watching the scene, is shot dead by SSS9. Kirishima chases the assassin and, after a brief scuffle, SSS9 is killed by a microwave-emitting plate. The sea water having cooled him and so stopped the bacteria, Godzilla reawakens and leaves for the ocean.
Production.
Pre-production.
Tomoyuki Tanaka announced a sequel to "The Return of Godzilla" in 1985, but was skeptical of its possibilities, as the film had been of little financial benefit to Toho, and the failure of "King Kong Lives" convinced him that audiences were not ready for a continuation of the Godzilla series. He relented after the success of "Little Shop of Horrors", and proceeded to hold a public story contest for a possible script. In consideration of "The Return of Godzillas marginal success in Japan, Tanaka insisted that the story focus on a classic monster vs. monster theme. Tanaka handed the five finalist entries to director Kazuki Ōmori, despite the two's initially hostile relationship; the latter had previously held Tanaka responsible for the decline in the Godzilla series' quality during the 1970s. Ōmori chose the entry of dentist Shinichiro Kobayashi, who wrote his story with the hypothetical death of his daughter in mind.
Kobayashi's submission was notable for its emphasis on dilemmas concerning biotechnology rather than nuclear energy, and revolved around a scientist grieving for his deceased daughter and attempting to keep her soul alive by merging her genes with those of a plant. The scientist's initial experiments would have resulted in the creation of a giant rat-like amphibian called Deutalios, which would have landed in Tokyo Bay and been killed by Godzilla. A female reporter investigating the scientist's activities would have suffered from psychic visions of plants with humanoid faces compelling her to infiltrate the scientist's laboratory. The scientist would have later confessed his intentions, and the finale would have had Godzilla battling a human-faced Biollante who defeats him by searing his flesh with acid.
Ōmori proceeded to modify the story into a workable script over a period of three years, using his background as a biologist to create a plausible plot involving genetic engineering and botany. In order to preserve the series' anti-nuclear message, he linked the creation of Biollante to the use of Godzilla cells, and replaced Kobayashi's journalist character with Miki Saegusa. He openly admitted that directing a Godzilla film was secondary to his desire to make a James Bond movie, and thus added elements of the spy film genre into the plot. Unlike the case with later, more committee-driven Godzilla films, Ōmori was given considerable leeway in writing and directing the film, which Toho staff later judged to have been an error resulting in a movie with a very narrow audience.
Special effects.
Koichi Kawakita, who had previously worked for Tsuburaya Productions, replaced Teruyoshi Nakano as head of the series' special effects unit after Toho became impressed at his work in "Gunhed". Kawakita made use of "Gunhead"'s special effects team Studio OX, and initially wanted to make Godzilla more animal-like, using crocodiles as references, but was berated by Tanaka, who declared Godzilla to be "a monster" rather than an animal. Kenpachiro Satsuma returned to portray Godzilla, hoping to improve his performance by making it less anthropomorphic than in previous films. Suitmaker Noboyuki Yasamaru created a Godzilla suit made specifically with Satsuma's measurements in mind, unlike the previous one which was initially built for another performer and caused Satsuma discomfort. The resulting 242 lb suit proved more comfortable than the last, having a lower center of gravity and more mobile legs. A second 176 lb suit was built for outdoor underwater scenes. The head's size was reduced, and the whites around the eyes removed. On the advice of story finalist Shinichiro Kobayashi, a double row of teeth was incorporated in the jaws. As with the previous film, animatronic models were used for close-up shots. These models were an improvement over the last, as they were made from the same molds used for the main costume, and included an articulated tongue and intricate eye motion. The suit's dorsal plates were filled with light bulbs for scenes in which Godzilla uses his atomic ray, thus lessening reliance on optical animation, though they electrocuted Satsuma the first time they were activated. Satsuma was also obliged to wear protective goggles when in the suit during scenes in which Godzilla battles the JSDF, as real explosives were used on set.
Designing and building the Biollante props proved problematic, as traditional suitmation techniques made realizing the requested design of the creature's first form difficult, and the resulting cumbersome model for Biollante's final form was met with disbelief from the special effects team. Biollante's first form was performed by Masao Takegami, who sat within the model's trunk area on a platform just above water level. While the creature's head movements were simple to operate, its vines were controlled by an intricate array of overhead wires which proved difficult for Satsuma to react to during combat scenes as they offered no tension, thus warranting Satsuma to feign receiving blows from them, despite not being able to perceive them. Biollante's final form was even more difficult to operate, as its vine network took hours to rig up on set. Visibility in both the Godzilla and final form Biollante suits was poor, thus causing difficulties for Takegami in aiming the creature's head when firing sap, which permanently stained anything it landed on.
While it was initially decided to incorporate stop motion animation into the film, the resulting sequences were scrapped, as Kawakita felt they failed to blend in with the live-action footage effectively. The film however became the first of its kind to use CGI, though its usage was limited to scenes involving computer generated schematics. The original cut of the movie had the first battle culminating in Biollante's spores falling around the hills surrounding Lake Ashino and blooming into fields of flowers, though this was removed as the flowers were out of scale.
Score.
Unlike the previous film, "Godzilla vs. Biollante" incorporates themes from Akira Ifukube's original "Gojira" theme, though the majority of the soundtrack was composed of original themes by Koichi Sugiyama. The score was orchestrated by conductor David Howell through the Kansai Philarmonic, though Howell himself had never viewed the movie, and thus was left to interpret what the scenes would consist of when conducting the orchestra.
English version.
After the film was released in Japan, Toho commissioned a Hong Kong company to dub the film into English. In Early 1990, Toho entered discussions with Miramax to distribute the film. When talks broke off, Toho filed a lawsuit in Los Angeles Federal Court, accusing Miramax of entering an oral agreement in June to pay Toho $500,000 to distribute the film. This lawsuit delayed the film's release for two years. An out of court settlement was reached with Miramax buying the rights to the film for an unreported figure. Miramax would've had entertained thoughts of releasing the film in theaters, but in the end it was decided to release the film straight to home video instead. HBO released the film on VHS in 1992 and Laserdisc in 1993. Miramax utilized the uncut English international version of the film for this release. "Godzilla vs. Biollante" was later released on DVD and Blu-ray Disc in North America by "Echo Bridge Entertainment" through Miramax on December 4, 2012.
Reception.
Box office.
In Japan, the film sold approximately 2 million tickets, earning $7,000,000 (U.S).
Critical reaction.
Godzilla vs. Biollante has received very positive reviews, with praise for the story, music and visuals.
Ed Godziszewski of Monster Zero said the film is "by no means a classic" but felt that "for the first time in well over 20 years, a script is presented with some fresh, original ideas and themes." Joseph Savitski of Beyond Hollywood said the film's music is "a major detraction", but added that it's "not only one of the most imaginative films in the series, but also the most enjoyable to watch." Japan Hero said, "is definitely a Godzilla movie not to be missed."
Composer Akira Ifukube, who had refused to compose the film's score, stated on interview that he disliked the way Koichi Sugiyama had modernized his Godzilla theme, and defined the Saradia theme as "ridiculous", on account of it sounding more European than Middle Eastern.
Home media releases.
Echo Bridge Home Entertainment/Miramax - Blu-ray A (Region Free) America

</doc>
<doc id="12001" url="https://en.wikipedia.org/wiki?curid=12001" title="Terror of Mechagodzilla">
Terror of Mechagodzilla

Terror of Mechagodzilla (released in Japan as ) is a 1975 Japanese science fiction kaiju film directed by Ishirō Honda. It is the 15th film in the "Godzilla" franchise, the final film in the Shōwa series, and serves as a direct sequel to 1974's "Godzilla vs. Mechagodzilla". The film was the least successful of the entire Godzilla franchise.
In the United States, it received a very limited theatrical release in the summer of 1978 by Bob Conn Enterprises as "The Terror of Godzilla".
Plot.
Continuing after the end of the previous film, Interpol agents, led by Inspector Kusaka, search for the wreck of Mechagodzilla at the bottom of the Okinawan Sea. Using the submarine, "Akatsuki", they hope to gather information on the robot's builders, the alien simians. The "Akatsuki" is suddenly attacked by a giant aquatic dinosaur called Titanosaurus, and the crew vanishes.
Interpol starts an investigation into the incident. With the help of marine biologist Akira Ichinose, they trace the Titanosaurus to a reclusive, mad scientist named Shinzô Mafune, who wants to destroy all mankind. While Ichinose is visiting his old home in the seaside forest of Manazuru, they meet Mafune's lone daughter, Katsura. She tells them that not only is her father dead, but she burned all of the notes about the giant dinosaur (at her father's request). Unknownst to them, Mafune is still alive and well. He is visited by his friend Tsuda, who is an aide to the simian alien leader Mugal. He is leading the project to quickly rebuild Mechagodzilla. Mugal offers their services to Mafune, so that his Titanosaurus and their Mechagodzilla 2 will be the ultimate weapons. They hope to wipe out mankind and rebuild the world for themselves.
But things are complicated for both factions when Ichinose falls in love with Katsura and unwittingly gives her Interpol's information against Titanosaurus, the new Mechagodzilla, and the aliens. It's also discovered that Katsura is actually a cyborg, due to undergoing surgery, and Mugal still has uses for her. Meanwhile, Mafune is desperate to unleash Titanosaurus without the aliens' permission, so he releases it on Yokosuka one night. By then, Interpol discovers that supersonic waves are the Titanosaurus' weakness. They had a supersonic wave oscillator ready, but Katsura sabotaged the machine before they could use it. Fortunately, Godzilla arrives to fight off Titanosaurus.
Later, when Ichinose visits Katsura, he is captured by the aliens. Tied up, Ichinose can only watch as Mafune and the aliens unleash Mechagodzilla and Titanosaurus on Tokyo, while Interpol struggles to repair their sonic wave machine and the Japanese armed forces struggle to keep the two monsters at bay. Katsura, while being controlled by Mugal, ignores Ichinose and controls both the dinosaur and the robot as they destroy the city.
Godzilla comes to the rescue, although it is outmatched by the two titans. While Interpol distracts Titanosaurus with the supersonic wave oscillator, Godzilla is able to focus on attacking Mechagodzilla. Interpol agents infiltrate the aliens' hideout, rescue Ichinose, and kill Mafune and many of the aliens. The remaining aliens attempt to escape in their ships, but Godzilla shoots them down. Katsura, while being embraced by Ichinose, shoots herself to destroy Mechagodzilla. Godzilla, with the help of the oscillator, defeats Titanosaurus, and heads back to sea.
Production.
The original screenplay that Yukiko Takayama created after winning Toho's story contest for the next establishment in the Godzilla series, which was picked by assistant producer Kenji Tokoro and was submitted for approval on July 1, 1974, less than four months after "Godzilla vs. Mechagodzilla" (1974) was released.
The original concept is similar to the finished version of "Terror of Mechagodzilla", with many of the changes being budgetary in nature. The most obvious alteration is the removal of the two monsters called the Titans, which merged to create Titanosaurus in the first draft. It was an interesting concept, although something that was also under explained considering the magnitude of such an occurrence of the creatures merging. Another noticeable change to the script is that of the final battle, which doesn't move to the countryside but instead would have reduced Tokyo to rubble during the ensuing conflict between the three monsters.
After her initial draft, Takayama submitted a revised version on October 14, 1974. This went through a third revision on December 4, and then yet another on December 28 of that same year before it was met with approval and filming began.
English version.
Toho titled its English version of the film "Terror of Mechagodzilla" and had it dubbed into English in Hong Kong. This “international version” has never seen wide release in the United States but has been issued on VHS in the United Kingdom by PolyGram Video Ltd. and on DVD in Taiwan by Power Multimedia.
The film was given a North American theatrical release in March 1978 by independent distributor Bob Conn Enterprises under the title "The Terror of Godzilla". Just as Cinema Shares had done with the previous three Godzilla movies, Bob Conn Enterprises chose to utilize the Toho-commissioned English dub instead of hiring a new crew to re-dub the film. "The Terror of Godzilla" was heavily edited to obtain a "G" rating from the MPAA. Several scenes with violent content were entirely removed, disrupting the flow of the narrative.
Henry Saperstein, who sold the theatrical rights to Bob Conn Enterprises, also released the film to television in late 1978, this time under Toho’s international title, "Terror of Mechagodzilla". Unlike "The Terror of Godzilla", the television version remained mostly uncut, with only a shot of Katsura’s breasts excised. Saperstein’s editors also added a prologue that serves as a brief history of Godzilla, with footage from Saperstein’s English versions of "Monster Zero" and "Godzilla's Revenge" (which itself utilized stock footage from "Godzilla vs. the Sea Monster" and "Son of Godzilla").
In the mid-1980s, the U.S. television version, "Terror of Mechagodzilla", was replaced by the theatrical edit, "The Terror of Godzilla", on television and home video. For some reason, the title was also changed to "Terror of Mechagodzilla". The 1994 Paramount release of "Terror of Mechagodzilla" listed a running time of 89 minutes on the slipcase, implying that this release would be the longer version first shown on American TV. The actual video cassette featured the edited theatrical version. In a 1995 interview with "G-Fan" magazine, Henry Saperstein was surprised to hear about this mistake. In the mid-2000s, the television version showed up again on Monsters HD, and in 2007, it made its home video debut as the U.S. version on the Classic Media DVD. Although the added prologue was originally framed for fullscreen television, it was cropped and shown in widescreen on the disc. The rest of the movie featured the audio from Saperstein's television version synced to video from the Japanese version.
The first article about the movie's storyline was published in a 1977 issue of "Japanese Giants" (published by Brad Boyle) and was written by Richard H. Campbell, creator of The Godzilla Fan News Letter (a.k.a. "The Gang").
Box office.
In Japan, the film sold 980,000 tickets. It would be the least-attended "Godzilla" film in Japan and also one of only two "Godzilla" films to sell less than one million tickets. This was part of a decline in attendance for monster-movies as a whole and Toho put the production of monster-movies on hold. Toho had no intention of permanently ending the Godzilla series. Throughout the remainder of the decade, several new stories were submitted by writers and producers. None of these films, however, were made. It wasn't until 1984 and "Godzilla's" 30th anniversary that Toho would start production on a new Godzilla movie.
DVD release.
Second Classic Media Release
First Classic Media Release
Simitar Entertainment

</doc>
<doc id="12002" url="https://en.wikipedia.org/wiki?curid=12002" title="Godzilla vs. King Ghidorah">
Godzilla vs. King Ghidorah

Although the film's production crew remained largely unchanged from that of "Godzilla vs. Biollante", the previous film's financial failure, due to a lack of child viewership and alleged competition with the contemporary "Back to the Future" franchise, compelled the producers to create a more fantasy orientated picture with elements of time travel included. It was the first Godzilla film to feature a newly orchestrated score by Akira Ifukube since "Terror of Mechagodzilla", and although much more successful than its predecessor, the film became controversial in America, due to perceived anti-Americanism stemming from a scene where Godzilla kills several American soldiers. Columbia TriStar Home Entertainment released "Godzilla vs. King Ghidorah" and "Godzilla vs. Mothra" on DVD in 1998.
Plot.
In 1992, science fiction writer Kenichiro Terasawa (Kosuke Toyohara) is writing a book about Godzilla and learns of a group of Japanese soldiers stationed on Lagos Island during the Gilbert and Marshall Islands campaign. In February 1944, while threatened by American soldiers, the Japanese soldiers were saved by a mysterious dinosaur. He theorizes that the dinosaur was subsequently mutated into Godzilla in 1954 after a hydrogen bomb test on the island. Yasuaki Shindo (Yoshio Tsuchiya), a wealthy businessman who commanded the Japanese soldiers on Lagos Island, confirms that the dinosaur did indeed exist.
Meanwhile, a UFO lands on Mount Fuji. When the Japanese army investigates, they are greeted by Wilson (Chuck Wilson), Grenchko (Richard Berger), Emmy Kano (Anna Nakagawa) and an android, M-11 (Robert Scott Field). The visitors, known as the "Futurians", explain that they are humans from the year 2204, where Godzilla has completely destroyed Japan. The Futurians plan to travel back in time to 1944 to remove the dinosaur from Lagos Island before the island is destroyed, thus preventing the mutation of the creature into Godzilla. As proof of their story, Emmy presents a copy of Terasawa's book, which has not yet been completed in the present.
The Futurians, Terasawa, Miki Saegusa (Megumi Odaka), and Professor Mazaki (Katsuhiko Sasaki), board a time shuttle and travel back to 1944 to Lagos Island. There, as American forces land and engage the Japanese forces commanded by Shindo, the dinosaur attacks and kills the American soldiers. The American navy then bombs the dinosaur from the sea and it is gravely wounded.
After Shindo and his men leave the island, M-11 teleports the dinosaur from Lagos Island to a place in the Bering Strait. Before returning to 1992, the Futurians leave three small creatures called Dorats on Lagos Island, which are exposed to radiation from the hydrogen bomb test in 1954. The creatures merge to become King Ghidorah, which then appears in present-day Japan. After returning to 1992, the Futurians use King Ghidorah to subjugate Japan. They issue an ultimatum, but Japan refuses to surrender.
Feeling sympathy for the Japanese people, Emmy reveals to Teresawa the truth behind the Futurians' mission: in the future, Japan is an economic superpower that has surpassed the United States, Russia, and China. The Futurians traveled back in time in order to change history and prevent Japan's future economic dominance by creating King Ghidorah and using it to destroy present day Japan. At the same time, they also planned to erase Godzilla from history so it wouldn't pose a threat to their plans. After M-11 brings Emmy back to the UFO, she reprograms the android so it will help her. With M-11 and Terasawa's aid, Emmy sabotages the UFO's control over King Ghidorah.
At the same time, Shindo plans to use his nuclear submarine to recreate Godzilla. On route to the Bering Strait, Shindo's submarine is destroyed by an already mutated Godzilla, who absorbs its radiation and becomes even larger and more powerful. Terasawa discovers that a Russian nuclear submarine sank in the Bering Strait in the 1970s. The Russian submarine released enough radiation to mutate the dinosaur into Godzilla, effectively setting in motion the events of "The Return of Godzilla" onwards.
Godzilla arrives in Japan and is met by King Ghidorah. They fight at equal strength, each immune to the other's attacks. Godzilla eventually ends the battle by blasting off Ghidorah's middle head. Before the final blow, Godzilla destroys the UFO, killing Wilson and Grenchko. Godzilla then turns its attention on Tokyo, destroying the metropolis and killing Shindo.
Emmy travels to the future with M-11 and then returns to the present day with Mecha-King Ghidorah, a cybernetic version of King Ghidorah. The cybernetic Ghidorah blasts Godzilla with energy beams, which proves useless. Godzilla then counters by relentlessly blasting Ghidorah with its atomic ray, almost decapitating Ghidorah. Ghidorah survives but then Godzilla prevails, knocking Ghidorah down. Emmy carries off Godzilla and drops it and Ghidorah into the ocean. Emmy then returns to the future but not before informing Terasawa that she is his descendant.
Production.
Conception.
Although the previously filmed "Godzilla vs. Biollante" had been the most expensive Godzilla film produced at the time, its low audience attendance and loss of revenue convinced executive producer and Godzilla series creator Tomoyuki Tanaka to revitalize the series by bringing back iconic monsters from pre-1984 Godzilla movies, specifically Godzilla's archenemy King Ghidorah.
"Godzilla vs. Biollante" director and writer Kazuki Ōmori had initially hoped to start a standalone series centered on Mothra, and was in the process of rewriting a 1980 script for the unrealized movie "Mothra vs. Bagan". The film was ultimately scrapped by Toho, under the assumption that, unlike Godzilla, Mothra would have been a difficult character to market overseas. The planning stages for a sequel to "Godzilla vs. Biollante" were initially hampered by Tanaka's deteriorating health, thus prompting the takeover of Shōgo Tomiyama as producer. The new producer felt that the financial failure of "Godzilla vs. Biollante" was due to the plot being too sophisticated for child audiences, and thus intended to return some of the fantasy elements of the pre-1984 Godzilla films to the series. Ōmori himself blamed the lackluster performance of "Godzilla vs. Biollante" to competition with "Back to the Future Part II", and thus concluded that audiences wanted plots involving time travel. His approach to the film also differed from "Godzilla vs. Biollante" in his greater emphasis on developing the personalities of the monsters rather than the human characters.
Akira Ifukube agreed to compose the film's score on the insistence of his daughter, after as he was dissatisfied with the way his compositions had been treated in "Godzilla vs. Biollante".
Special effects.
The Godzilla suits used in "Godzilla vs. Biollante" were reused in "Godzilla vs. King Ghidorah", though with slight modifications. The original suit used for land-based and full body shots had its head replaced with a wider and flatter one, and the body cut in half. The upper half was used in scenes where Godzilla emerges from the sea and during close-ups during the character's first fight with King Ghidorah. The suit used previously for scenes set at sea was modified with rounder shoulders, a more prominent chest, and an enhanced face, and was used throughout the majority of the film's Godzilla scenes.
The redesigned King Ghidorah featured much more advanced wirework puppetry than its predecessors, and effects team leader Koichi Kawakita designed the "Godzillasaurus" as a more paleontologically accurate-looking dinosaur than Godzilla itself as a nod to American filmmakers aspiring to direct their own Godzilla films with the intention of making the monster more realistic. Ōmori's original draft specified that the dinosaur that would become Godzilla was a "Tyrannosaurus", though this was rejected by creature designer Shinji Nishikawa, who stated that he "couldn't accept that a tyrannosaur could become Godzilla". The final suit combined features of "Tyrannosaurus" with Godzilla, and real octopus blood was used during the bombardment scene. Because the Godzillasaurus' arms were much smaller than Godzilla's, suit performer Wataru Fukuda had to operate them with levers within the costume. The creature's distress calls were recycled Gamera cries.
Home media release.
The Columbia/TriStar Home Video DVD version was released in 1998 as a double feature, with "Godzilla vs. Mothra", on a single disc. The picture was full frame (1.33:1) and the audio in English (2.0). There were no subtitles. Extras included the trailer for "Godzilla vs. King Ghidorah" and "Godzilla vs. Mothra".
The Sony - Blu-ray version was released on May 6, 2014 as a two disc double feature with "Godzilla vs. Mothra". The picture was MPEG-4 AVC (1.85:1) and the audio was in Japanese and English (DTS-HD Master Audio 2.0). Subtitles were added in English, English SDH and French. Extras included the theatrical trailer and three teasers in HD with English subtitles.
Controversy.
The film was considered controversial at the time of its release, being contemporary to a period of between America and Japan, but mainly due to its fictional World War II depictions. Gerald Glaubitz of the Pearl Harbor Survivors Association appeared alongside director Kazuki Ōmori on "Entertainment Tonight" and condemned the film as being in "very poor taste" and detrimental to American-Japanese relations, allegations that Ōmori denied, stating that the American extras in the film had been "happy about being crushed and squished by Godzilla." Ishiro Honda also criticized Ōmori, stating that the scene in question went "too far".

</doc>
