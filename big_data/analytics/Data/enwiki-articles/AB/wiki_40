<doc id="9230" url="https://en.wikipedia.org/wiki?curid=9230" title="English Channel">
English Channel

The English Channel (, "the Sleeve"; , "Sea of Brittany"; , "British Sea"), also called simply the Channel, is the body of water that separates southern England from northern France, and joins the southern part of the North Sea to the rest of the Atlantic Ocean.
It is about long and varies in width from at its widest to in the Strait of Dover. It is the smallest of the shallow seas around the continental shelf of Europe, covering an area of some .
Geography.
The International Hydrographic Organization defines the limits of the English Channel as follows:
The IHO defines the southwestern limit of the North Sea as "a line joining the Walde Lighthouse (France, 1°55'E) and Leathercoat Point (England, 51°10'N)". The Walde Lighthouse is 6 km east of Calais (), and Leathercoat Point is at the north end of St Margaret's Bay, Kent ().
The Strait of Dover (French: "Pas de Calais"), at the Channel's eastern end is its narrowest point, while its widest point lies between Lyme Bay and the Gulf of Saint Malo near its midpoint. It is relatively shallow, with an average depth of about at its widest part, reducing to a depth of about between Dover and Calais. Eastwards from there the adjoining North Sea reduces to about in the Broad Fourteens where it lies over the watershed of the former land bridge between East Anglia and the Low Countries. It reaches a maximum depth of in the submerged valley of Hurd's Deep, west-northwest of Guernsey.
The eastern region along the French coast between Cherbourg and the mouth of the Seine river at Le Havre is frequently referred to as the "Bay of the Seine ()".
There are several major islands in the Channel, the most notable being the Isle of Wight off the English coast, and the Channel Islands, British Crown Dependencies off the coast of France. The coastline, particularly on the French shore, is deeply indented; several small islands close to the coastline, including Chausey and Mont Saint-Michel, are within French jurisdiction. The Cotentin Peninsula in France juts out into the Channel, whilst on the English side there is a small parallel channel known as the Solent between the Isle of Wight and the mainland. The Celtic Sea is to the west of the Channel.
The Channel is of geologically recent origins, having been dry land for most of the Pleistocene period. It is thought to have been created between 450,000 and 180,000 years ago by two catastrophic glacial lake outburst floods caused by the breaching of the Weald–Artois anticline, a ridge that held back a large proglacial lake in the Doggerland region, now submerged under the North Sea. The flood would have lasted for several months, releasing as much as one million cubic metres of water per second. The cause of the breach is not known but may have been an earthquake or the build-up of water pressure in the lake. The flood carved a large bedrock-floored valley down the length of the Channel, leaving behind streamlined islands and longitudinal erosional grooves characteristic of catastrophic megaflood events. It destroyed the isthmus that connected Britain to continental Europe, although a land bridge across the southern North Sea would have existed intermittently at later times after periods of glaciation resulted in lowering of sea levels.
The Channel acts as a funnel that amplifies the tidal range from less than a metre as observed at sea to more than 6 metres as observed in the Channel Islands, the west coast of the Cotentin Peninsula and the north cost of Britanny. The time difference of about 6 hours between high water at the eastern and western limits of the Channel is indicative of the tidal range being amplified further by resonance.
For the UK Shipping Forecast the Channel is divided into the following areas, from the west:
Name.
The name "English Channel" has been widely used since the early 18th century, possibly originating from the designation in Dutch sea maps from the 16th century onwards. In modern Dutch, however, it is known as (with no reference to the word "English"). Later, it has also been known as the "British Channel" or the "British Sea" having been called the by the 2nd-century geographer Ptolemy. The same name is used on an Italian map of about 1450, which gives the alternative name of —possibly the first recorded use of the "Channel" designation. The Anglo-Saxon texts often call it "Sūð-sǣ" ("South Sea") as opposed to "Norð-sǣ" ("North Sea" = Bristol Channel). The word "channel" was first recorded in Middle English in the 13th century and was borowed from Old French "chanel", variant form of "chenel" "canal".
The French name has been in use since at least the 17th century. The name is usually said to refer to the Channel's sleeve () shape. However, it is sometimes claimed to derive from a Celtic word meaning "channel" that is also the source of the name for the Minch in Scotland.
In Spain and most Spanish-speaking countries the Channel is referred to as . In Portuguese it is known as . This is not a translation from French: in Portuguese and Spanish, means "stain", while the word for sleeve is – which suggests either a phonetic borrowing from French or a common source. Other languages also use this name, such as Greek () and Italian (). The German name is , literally "sleeve-channel", or more generally .
The name in Breton ("Mor Breizh") means "Breton Sea", and its Cornish name ("Mor Bretannek") means "British Sea".
History.
Before the Devensian glaciation (the most recent ice age that ended around 10,000 years ago), Britain and Ireland were part of continental Europe, linked by an unbroken Weald-Artois Anticline, which acted as a natural dam that held back a large freshwater pro-glacial lake in the Doggerland region, now submerged under the North Sea. During this period the North Sea and almost all of the British Isles were covered with ice. The lake was fed by meltwater from the Baltic and from the Caledonian and Scandinavian ice sheets that joined to the north, blocking its exit. The sea level was about lower than it is today. Then, more than 200,000 years ago a single catastrophic glacial lake outburst flood overtopped the Weald-Artois Anticline and scoured a channel through an expanse of low-lying tundra, right down to the underlying chalk bedrock. In a study published in 2007 high-resolution sonar revealed the unexpectedly well-preserved scourmarks and the telltale lenticular island forms characteristic of torrential flood. Through the scoured channel passed a river which now drained the combined Rhine and Thames towards the Atlantic to the west. As the ice sheet melted, a large freshwater lake formed in the southern part of what is now the North Sea. As the meltwater could still not escape to the north (as the northern North Sea was still frozen) the outflow channel from the lake entered the Atlantic Ocean in the region of Dover and Calais.
The Channel, which delayed human reoccupation of Great Britain for more than 100,000 years, has in historic times been both an easy entry for seafaring people and a key natural defence, halting invading armies while in conjunction with control of the North Sea allowing Britain to blockade the continent. The most significant failed invasion threats came when the Dutch and Belgian ports were held by a major continental power, e.g. from the Spanish Armada in 1588, Napoleon during the Napoleonic Wars, and Nazi Germany during World War II. Successful invasions include the Roman conquest of Britain, the Norman Conquest in 1066 and the invasion by the Dutch in 1688, while the concentration of excellent harbours in the Western Channel on Britain's south coast made possible the largest invasion of all time, the Normandy Landings in 1944. Channel naval battles include the Battle of the Downs (1639), Battle of Goodwin Sands (1652), the Battle of Portland (1653), the Battle of La Hougue (1692) and the engagement between USS "Kearsarge" and CSS "Alabama" (1864).
In more peaceful times the Channel served as a link joining shared cultures and political structures, particularly the huge Angevin Empire from 1135 to 1217. For nearly a thousand years, the Channel also provided a link between the Modern Celtic regions and languages of Cornwall and Brittany. Brittany was founded by Britons who fled Cornwall and Devon after Anglo-Saxon encroachment. In Brittany, there is a region known as "Cornouaille" (Cornwall) in French and "Kernev" in Breton In ancient times there was also a "Domnonia" (Devon) in Brittany as well.
In February 1684, ice formed on the sea in a belt wide off the coast of Kent and wide on the French side.
Route to the British Isles.
Remnants of a mesolithic boatyard have been found on the Isle of Wight. Wheat was traded across the Channel about 8,000 years ago. "... Sophisticated social networks linked the Neolithic front in southern Europe to the Mesolithic peoples of northern Europe." The Ferriby Boats, Hanson Log Boats and the later Dover Bronze Age Boat could carry a substantial cross-Channel cargo.
Diodorus Siculus and Pliny both suggest trade between the rebel Celtic tribes of Armorica and Iron Age Britain flourished. In 55 BC Julius Caesar invaded, claiming that the Britons had aided the Veneti against him the previous year. He was more successful in 54 BC, but Britain was not fully established as part of the Roman Empire until completion of the invasion by Aulus Plautius in 43 AD. A brisk and regular trade began between ports in Roman Gaul and those in Britain. This traffic continued until the end of Roman rule in Britain in 410 AD, after which the early Anglo-Saxons left less clear historical records.
In the power vacuum left by the retreating Romans, the Germanic Angles, Saxons, and Jutes began the next great migration across the North Sea. Having already been used as mercenaries in Britain by the Romans, many people from these tribes crossed during the Migration Period, conquering and perhaps displacing the native Celtic populations.
Norsemen and Normans.
The attack on Lindisfarne in 793 is generally considered the beginning of the Viking Age. For the next 250 years the Scandinavian raiders of Norway, Sweden, and Denmark dominated the North Sea, raiding monasteries, homes, and towns along the coast and along the rivers that ran inland. According to the "Anglo-Saxon Chronicle" they began to settle in Britain in 851. They continued to settle in the British Isles and the continent until around 1050.
The fiefdom of Normandy was created for the Viking leader Rollo (also known as Robert of Normandy). Rollo had besieged Paris but in 911 entered vassalage to the king of the West Franks Charles the Simple through the Treaty of St.-Claire-sur-Epte. In exchange for his homage and fealty, Rollo legally gained the territory he and his Viking allies had previously conquered. The name "Normandy" reflects Rollo's Viking (i.e. "Northman") origins.
The descendants of Rollo and his followers adopted the local Gallo-Romance language and intermarried with the area's inhabitants and became the Normans – a Norman French-speaking mixture of Scandinavians, Hiberno-Norse, Orcadians, Anglo-Danish, and indigenous Franks and Gauls.
Rollo's descendant William, Duke of Normandy became king of England in 1066 in the Norman Conquest beginning with the Battle of Hastings, while retaining the fiefdom of Normandy for himself and his descendants. In 1204, during the reign of King John, mainland Normandy was taken from England by France under Philip II, while insular Normandy (the Channel Islands) remained under English control. In 1259, Henry III of England recognised the legality of French possession of mainland Normandy under the Treaty of Paris. His successors, however, often fought to regain control of mainland Normandy.
With the rise of William the Conqueror the North Sea and Channel began to lose some of their importance. The new order oriented most of England and Scandinavia's trade south, toward the Mediterranean and the Orient.
Although the British surrendered claims to mainland Normandy and other French possessions in 1801, the monarch of the United Kingdom retains the title Duke of Normandy in respect to the Channel Islands. The Channel Islands (except for Chausey) are Crown dependencies of the British Crown. Thus the Loyal toast in the Channel Islands is "La Reine, notre Duc" ("The Queen, our Duke"). The British monarch is understood to "not" be the Duke of Normandy in regards of the French region of Normandy described herein, by virtue of the Treaty of Paris of 1259, the surrender of French possessions in 1801, and the belief that the rights of succession to that title are subject to Salic Law which excludes inheritance through female heirs.
French Normandy was occupied by English forces during the Hundred Years' War in 1346–1360 and again in 1415–1450.
England and Britain: Naval superpower.
From the reign of Elizabeth I, English foreign policy concentrated on preventing invasion across the Channel by ensuring no major European power controlled the potential Dutch and Flemish invasion ports. Her climb to the pre-eminent sea power of the world began in 1588 as the attempted invasion of the Spanish Armada was defeated by the combination of outstanding naval tactics by the English and the Dutch under command of Charles Howard, 1st Earl of Nottingham with Sir Francis Drake second in command, and the following stormy weather. Over the centuries the Royal Navy slowly grew to be the most powerful in the world.
The building of the British Empire was possible only because the Royal Navy eventually managed to exercise unquestioned control over the seas around Europe, especially the Channel and the North Sea. During the Seven Years' War, France attempted to launch an invasion of Britain. To achieve this France needed to gain control of the Channel for several weeks, but was thwarted following the British naval victory at the Battle of Quiberon Bay in 1759.
Another significant challenge to British domination of the seas came during the Napoleonic Wars. The Battle of Trafalgar took place off the coast of Spain against a combined French and Spanish fleet and was won by Admiral Horatio Nelson, ending Napoleon's plans for a cross-Channel invasion and securing British dominance of the seas for over a century.
First World War.
The exceptional strategic importance of the Channel as a tool for blockade was recognised by the First Sea Lord Admiral Fisher in the years before World War I. "Five keys lock up the world! Singapore, the Cape, Alexandria, Gibraltar, Dover." However, on 25 July 1909 Louis Blériot successfully made the first Channel crossing from Calais to Dover in an aeroplane. Blériot's crossing signalled the end of the Channel as a barrier-moat for England against foreign enemies.
Because the Kaiserliche Marine surface fleet could not match the British Grand Fleet, the Germans developed submarine warfare, which was to become a far greater threat to Britain. The Dover Patrol was set up just before war started to escort cross-Channel troopships and to prevent submarines from accessing the Channel, thereby obliging them to travel to the Atlantic via the much longer route around Scotland.
On land, the German army attempted to capture Channel ports (see "Race to the Sea"), but although the trenches are often said to have stretched "from the frontier of Switzerland to the English Channel", they reached the coast at the North Sea. Much of the British war effort in Flanders was a bloody but successful strategy to prevent the Germans reaching the Channel coast.
At the outset of the war, an attempt was made to block the path of U-boats through the Dover Strait with naval minefields. By February 1915, this had been augmented by a 25 kilometre stretch of light steel netting called the Dover Barrage, which it was hoped would ensnare submerged submarines. After initial success, the Germans learned how to pass through the barrage, aided by the unreliability of British mines. 31 January 1917, the Germans restarted unrestricted submarine warfare leading to dire Admiralty predictions that submarines would defeat Britain by November, the most dangerous situation Britain faced in either World War.
The Battle of Passchendaele in 1917 was fought to reduce the threat by capturing the submarine bases on the Belgian coast, though it was the introduction of convoys and not capture of the bases that averted defeat. In April 1918 the Dover Patrol carried out the famous Zeebrugge Raid against the U-boat bases. During 1917, the Dover Barrage was re-sited with improved mines and more effective nets, aided by regular patrols by small warships equipped with powerful searchlights. A German attack on these vessels resulted in the Battle of Dover Strait in 1917. A much more ambitious attempt to improve the barrage by installing eight massive concrete towers across the strait was called the Admiralty M–N Scheme, but only two towers were nearing completion at the end of the war and the project was abandoned.
The naval blockade in the Channel and North Sea was one of the decisive factors in the German defeat in 1918.
Second World War.
During the Second World War, naval activity in the European theatre was primarily limited to the Atlantic. During the Battle of France in May 1940, the Germans succeeded in capturing both Boulogne and Calais, thereby threatening the line of retreat for the British Expeditionary Force. By a combination of hard fighting and German indecision, the port of Dunkirk was kept open allowing 338,000 Allied troops to be evacuated in Operation Dynamo. More than 11,000 were evacuated from Le Havre during Operation Cycle and a further 192,000 were evacuated from ports further down the coast in Operation Ariel in June 1940. The early stages of the Battle of Britain featured air attacks on Channel shipping and ports, and until the Normandy Landings (with the exception of the Channel Dash) the narrow waters were too dangerous for major warships. Despite these early successes against shipping, the Germans did not win the air supremacy necessary for Operation Sealion, the projected cross-Channel invasion.
The Channel subsequently became the stage for an intensive coastal war, featuring submarines, minesweepers, and Fast Attack Craft.
Dieppe was the site of an ill-fated raid by Canadian and British armed forces. More successful was the later Operation Overlord (D-Day), a massive invasion of German-occupied France by Allied troops. Caen, Cherbourg, Carentan, Falaise and other Norman towns endured many casualties in the fight for the province, which continued until the closing of the so-called Falaise gap between Chambois and Montormel, then liberation of Le Havre.
The Channel Islands were the only part of the British Commonwealth occupied by Germany (excepting the part of Egypt occupied by the Afrika Korps at the time of the Second Battle of El Alamein, which was a protectorate and not part of the Commonwealth). The German occupation of 1940–1945 was harsh, with some island residents being taken for slave labour on the Continent; native Jews sent to concentration camps; partisan resistance and retribution; accusations of collaboration; and slave labour (primarily Russians and eastern Europeans) being brought to the islands to build fortifications. The Royal Navy blockaded the islands from time to time, particularly following the liberation of mainland Normandy in 1944. Intense negotiations resulted in some Red Cross humanitarian aid, but there was considerable hunger and privation during the occupation, particularly in the final months, when the population was close to starvation. The German troops on the islands surrendered on 9 May 1945, a few days after the final surrender in mainland Europe.
Population.
The English Channel is far more densely populated on the English shore. The most significant towns and cities along both the English and French sides of the Channel (each with more than 20,000 inhabitants, ranked in descending order; populations are the urban area populations from the 1999 French census, 2001 UK census, and 2001 Jersey census) are as follows:
Shipping.
The Channel has traffic on both the UK-Europe and North Sea-Atlantic routes, and is the world's busiest seaway, with over 500 ships per day. Following an accident in January 1971 and a series of disastrous collisions with wreckage in February, the Dover TSS the world's first radar-controlled Traffic Separation Scheme was set up by the International Maritime Organization. The scheme mandates that vessels travelling north must use the French side, travelling south the English side. There is a separation zone between the two lanes.
In December 2002 the MV "Tricolor", carrying £30m of luxury cars sank northwest of Dunkirk after collision in fog with the container ship "Kariba". The cargo ship "Nicola" ran into the wreckage the next day. There was no loss of life.
The shore-based long range traffic control system was updated in 2003 and there is a series of Traffic Separation Systems in operation. Though the system is inherently incapable of reaching the levels of safety obtained from aviation systems such as the Traffic Collision Avoidance System, it has reduced accidents to one or two per year.
Marine GPS systems allow ships to be preprogrammed to follow navigational channels accurately and automatically, further avoiding risk of running aground, but following the fatal collision between Dutch Aquamarine and Ash in October 2001, Britain's Marine Accident Investigation Branch (MAIB) issued a safety bulletin saying it believed that in these most unusual circumstances GPS use had actually contributed to the collision. The ships were maintaining a very precise automated course, one directly behind the other, rather than making use of the full width of the traffic lanes as a human navigator would.
A combination of radar difficulties in monitoring areas near cliffs, a failure of a CCTV system, incorrect operation of the anchor, the inability of the crew to follow standard procedures of using a GPS to provide early warning of the ship dragging the anchor and reluctance to admit the mistake and start the engine led to the MV "Willy" running aground in Cawsand bay, Cornwall in January 2002. The MAIB report makes it clear that the harbour controllers were informed of impending disaster by shore observers before the crew were themselves aware. The village of Kingsand was evacuated for three days because of the risk of explosion, and the ship was stranded for 11 days.
Ecology.
As a busy shipping lane, the Channel experiences environmental problems following accidents involving ships with toxic cargo and oil spills. Indeed, over 40% of the UK incidents threatening pollution occur in or very near the Channel. One of the recent occurrences was the MSC "Napoli", which on 18 January 2007 was beached with nearly 1700 tonnes of dangerous cargo in Lyme Bay, a protected World Heritage Site coastline. The ship had been damaged and was en route to Portland Harbour.
Transport.
Ferry.
The number of ferry routes crossing the Strait of Dover has reduced since the Channel Tunnel opened. Current cross-channel ferry routes are:
Channel Tunnel.
Many travellers cross beneath the Channel using the Channel Tunnel, first proposed in the early 19th century and finally opened in 1994, connecting the UK and France by rail. It is now routine to travel between Paris or Brussels and London on the Eurostar train. Cars can also be carried on special trains between Folkestone and Calais.
Economy.
Tourism.
The coastal resorts of the Channel, such as Brighton and Deauville, inaugurated an era of aristocratic tourism in the early 19th century, which developed into the seaside tourism that has shaped resorts around the world. Short trips across the Channel for leisure purposes are often referred to as Channel Hopping.
Culture and languages.
The two dominant cultures are English on the north shore of the Channel, French on the south. However, there are also a number of minority languages that are or were found on the shores and islands of the English Channel, which are listed here, with the Channel's name following them.
Dutch previously had a larger range, and extended into parts of modern-day France. For more information, please see French Flemish.
Most other languages tend towards variants of the French and English forms, but notably Welsh has "Môr Udd".
Channel crossings.
As one of the narrowest and most well-known international waterways lacking dangerous currents, the Channel has been the first objective of numerous innovative sea, air, and human powered crossing technologies.
Pre-historic people sailed from the mainland to England for millennia. At the end of the last Ice Age, lower sea levels even permitted walking across.
By boat.
Pierre Andriel crossed the English Channel aboard the "Élise", ex the Scottish p.s. "Margery" in March 1816, one of the earliest seagoing voyages by steam ship.
The paddle steamer "Defiance", Captain William Wager, was the first steamer to cross the Channel to Holland, arriving there on 9 May 1816.
On 10 June 1821, English-built paddle steamer "Rob Roy" was the first passenger ferry to cross channel. The steamer was purchased subsequently by the French postal administration and renamed "Henri IV" and put into regular passenger service a year later. It was able to make the journey across the Straits of Dover in around three hours.
In June 1843, because of difficulties with Dover harbour, the South Eastern Railway company developed the Boulogne-sur-Mer-Folkestone route as an alternative to Calais-Dover. The first ferry crossed under the command of Captain Hayward.
In 1974 a Welsh coracle piloted by Bernard Thomas of Llechryd crossed the English Channel to France in 13½ hours. The journey was undertaken to demonstrate how the Bull Boats of the Mandan Indians of North Dakota could have been copied from coracles introduced by Prince Madog in the 12th century.
The Mountbatten class hovercraft (MCH) entered commercial service in August 1968, initially between Dover and Boulogne but later also Ramsgate (Pegwell Bay) to Calais. The journey time Dover to Boulogne was roughly 35 minutes, with six trips per day at peak times. The fastest crossing of the English Channel by a commercial car-carrying hovercraft was 22 minutes, recorded by the "Princess Anne" MCH SR-N4 Mk3 on 14 September 1995,
By air.
The first aircraft to cross the Channel was a balloon in 1785, piloted by Jean Pierre François Blanchard (France) and John Jeffries (US).
Louis Blériot (France) piloted the first airplane to cross in 1909.
By swimming.
The sport of Channel swimming traces its origins to the latter part of the 19th century when Captain Matthew Webb made the first observed and unassisted swim across the Strait of Dover, swimming from England to France on 24–25 August 1875 in 21 hours 45 minutes.
In 1927, at a time when fewer than ten swimmers (including the first woman, Gertrude Ederle in 1926) had managed to emulate the feat and many dubious claims were being made, the Channel Swimming Association (CSA) was founded to authenticate and ratify swimmers' claims to have swum the Channel and to verify crossing times. The CSA was dissolved in 1999 and was succeeded by two separate organisations: CSA (Ltd) and the Channel Swimming and Piloting Federation (CSPF). Both observe and authenticate cross-Channel swims in the Strait of Dover. The Channel Crossing Association was set up at about this time to cater for unorthodox crossings.
The team with the most number of Channel swims to its credit is the Serpentine Swimming Club in London, followed by the International Sri Chinmoy Marathon Team.
By the end of 2005, 811 people had completed 1,185 verified crossings under the rules of the CSA, the CSA (Ltd), the CSPF and Butlins.
The number of swims conducted under and ratified by the Channel Swimming Association to 2005 was 982 by 665 people. This includes 24 two-way crossings and three three-way crossings.
The number of ratified swims to 2004 was 948 by 675 people (456 men, 214 women). There have been 16 two-way crossings (9 by men and 7 by women). There have been three three-way crossings (2 by men and 1 by a woman). (It is unclear whether this last set of data is comprehensive or CSA only.)
The Strait of Dover is the busiest stretch of water in the world. It is governed by International Law as described in "Unorthodox Crossing of the Dover Strait Traffic Separation Scheme". It states: " exceptional cases the French Maritime Authorities may grant authority for unorthodox craft to cross French territorial waters within the Traffic Separation Scheme when these craft set off from the British coast, on condition that the request for authorisation is sent to them with the opinion of the British Maritime Authorities."
The CCA, CSA, and CS&PF are the organisations escorting channel swims, because their pilots have the experience, qualifications, and equipment to guarantee the safety of the swimmers they escort.
The fastest verified swim of the Channel was by the Australian Trent Grimsey on 8 September 2012, in 6 hours 55 minutes, beating the previous record set in 2007 by Bulgarian swimmer Petar Stoychev.
There may have been some unreported swims of the Channel, by people intent on entering Britain in circumvention of immigration controls. A failed attempt to cross the Channel by two Syrian refugees in October 2014 only came to light when their bodies were later discovered on the shores of the North Sea in Norway and the Netherlands.
By car.
On 16 September 1965, two Amphicars crossed from Dover to Calais. One was crewed by two British Army officers, Captain Mike Bailey REME and Captain Peter Tappenden RAOC, the other by Tim Dill-Russell and Sgt Joe Minto RASC. The crossing took 7 hours 20 minutes, with mid-Channel wind conditions reaching force 5 on the Beaufort scale. The cars went on to the Frankfurt Motor Show that year, where they were put on display.
In 2007, the presenters of the BBC programme "Top Gear" (Jeremy Clarkson, Richard Hammond and James May) "drove" across the Channel from England to France. They did it by designing "amphibious cars" that could be driven on land and also operate in water.
After four attempts – twice failing to leave Dover Harbour – they reached the coast of France in a Nissan pick-up with an outboard motor and oil drums attached to the back to aid stability in open water. The other two vehicles that attempted the crossing (a Triumph Herald with a sail and a Volkswagen Campervan with a propeller attached to the flywheel) both sank. Clarkson believed it might be possible to break the world record for crossing the Channel in this manner, but the team was unsuccessful. The "Daily Mail" claimed that the BBC received criticism from a coastguard who claimed that they had not been told that the stunt was going to take place, and allegedly branded it "completely irresponsible"; however this was not reported by any other media sources and the aired episode showed the full co-operation of the coastguard.

</doc>
<doc id="9232" url="https://en.wikipedia.org/wiki?curid=9232" title="Eiffel Tower">
Eiffel Tower

The Eiffel Tower ( ; ) is a wrought iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.
Constructed in 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognisable structures in the world. The Eiffel Tower is the most-visited paid monument in the world; 6.91 million people ascended it in 2015.
The tower is tall, about the same height as an 81- building, and the tallest structure in Paris. Its base is square, measuring on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by . Excluding transmitters, the Eiffel Tower is the second-tallest structure in France after the Millau Viaduct.
The tower has three levels for visitors, with restaurants on the first and second levels. The top level's upper platform is above the ground – the highest observation deck accessible to the public in the European Union. Tickets can be purchased to ascend by stairs or lift (elevator) to the first and second levels. The climb from ground level to the first level is over 300 steps, as is the climb from the first level to the second. Although there is a staircase to the top level, it is usually only accessible by lift.
History.
Origin.
The design of the Eiffel Tower was the product of Maurice Koechlin and Émile Nouguier, two senior engineers working for the Compagnie des Établissements Eiffel, after discussion about a suitable centrepiece for the proposed 1889 Exposition Universelle, a world's fair to celebrate the centennial of the French Revolution. Eiffel openly acknowledged that inspiration for a tower came from the Latting Observatory built in New York City in 1853. In May 1884, working at home, Koechlin made a sketch of their idea, described by him as "a great pylon, consisting of four lattice girders standing apart at the base and coming together at the top, joined together by metal trusses at regular intervals". Eiffel initially showed little enthusiasm, but he did approve further study, and the two engineers then asked Stephen Sauvestre, the head of company's architectural department, to contribute to the design. Sauvestre added decorative arches to the base of the tower, a glass pavilion to the first level, and other embellishments.
The new version gained Eiffel's support: he bought the rights to the patent on the design which Koechlin, Nougier, and Sauvestre had taken out, and the design was exhibited at the Exhibition of Decorative Arts in the autumn of 1884 under the company name. On 30 March 1885, Eiffel presented his plans to the ; after discussing the technical problems and emphasising the practical uses of the tower, he finished his talk by saying the tower would symbolise, 
Little progress was made until 1886, when Jules Grévy was re-elected as president of France and Édouard Lockroy was appointed as minister for trade. A budget for the exposition was passed and, on 1 May, Lockroy announced an alteration to the terms of the open competition being held for a centrepiece to the exposition, which effectively made the selection of Eiffel's design a foregone conclusion, as entries had to include a study for a four-sided metal tower on the Champ de Mars. On 12 May, a commission was set up to examine Eiffel's scheme and its rivals, which, a month later, decided that all the proposals except Eiffel's were either impractical or lacking in details.
After some debate about the exact location of the tower, a contract was signed on 8 January 1887. This was signed by Eiffel acting in his own capacity rather than as the representative of his company, and granted him 1.5 million francs toward the construction costs: less than a quarter of the estimated 6.5 million francs. Eiffel was to receive all income from the commercial exploitation of the tower during the exhibition and for the next 20 years. He later established a separate company to manage the tower, putting up half the necessary capital himself.
The artists' protest.
The proposed tower had been a subject of controversy, drawing criticism from those who did not believe it was feasible and those who objected on artistic grounds. These objections were an expression of a long-standing debate in France about the relationship between architecture and engineering. It came to a head as work began at the Champ de Mars: a "Committee of Three Hundred" (one member for each metre of the tower's height) was formed, led by the prominent architect Charles Garnier and including some of the most important figures of the arts, such as Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet. A petition called "Artists against the Eiffel Tower" was sent to the Minister of Works and Commissioner for the Exposition, Charles Alphand, and it was published by "Le Temps" on 14 February 1887:
Gustave Eiffel responded to these criticisms by comparing his tower to the Egyptian pyramids: "My tower will be the tallest edifice ever erected by man. Will it not also be grandiose in its way? And why would something admirable in Egypt become hideous and ridiculous in Paris?" These criticisms were also dealt with by Édouard Lockroy in a letter of support written to Alphand, ironically saying, "Judging by the stately swell of the rhythms, the beauty of the metaphors, the elegance of its delicate and precise style, one can tell this protest is the result of collaboration of the most famous writers and poets of our time", and he explained that the protest was irrelevant since the project had been decided upon months before, and construction on the tower was already under way.
Indeed, Garnier was a member of the Tower Commission that had examined the various proposals, and had raised no objection. Eiffel was similarly unworried, pointing out to a journalist that it was premature to judge the effect of the tower solely on the basis of the drawings, that the Champ de Mars was distant enough from the monuments mentioned in the protest for there to be little risk of the tower overwhelming them, and putting the aesthetic argument for the tower: "Do not the laws of natural forces always conform to the secret laws of harmony?"
Some of the protesters changed their minds when the tower was built; others remained unconvinced. Guy de Maupassant supposedly ate lunch in the tower's restaurant every day because it was the one place in Paris where the tower was not visible.
By 1918, it had become a symbol of Paris and of France after Guillaume Apollinaire wrote a nationalist poem in the shape of the tower (a calligram) to express his feelings about the war against Germany. Today, it is widely considered to be a remarkable piece of structural art, and is often featured in films and literature.
Construction.
Work on the foundations started on 28 January 1887. Those for the east and south legs were straightforward, with each leg resting on four concrete slabs, one for each of the principal girders of each leg. The west and north legs, being closer to the river Seine, were more complicated: each slab needed two piles installed by using compressed-air caissons long and in diameter driven to a depth of to support the concrete slabs, which were thick. Each of these slabs supported a block of limestone with an inclined top to bear a supporting shoe for the ironwork.
Each shoe was anchored to the stonework by a pair of bolts in diameter and long. The foundations were completed on 30 June, and the erection of the ironwork began. The visible work on-site was complemented by the enormous amount of exacting preparatory work that took place behind the scenes: the drawing office produced 1,700 general drawings and 3,629 detailed drawings of the 18,038 different parts needed. The task of drawing the components was complicated by the complex angles involved in the design and the degree of precision required: the position of rivet holes was specified to within and angles worked out to one second of arc. The finished components, some already riveted together into sub-assemblies, arrived on horse-drawn carts from a factory in the nearby Parisian suburb of Levallois-Perret and were first bolted together, with the bolts being replaced with rivets as construction progressed. No drilling or shaping was done on site: if any part did not fit, it was sent back to the factory for alteration. In all, 18,038 pieces were joined together using 2.5 million rivets.
At first the legs were constructed as cantilevers, but about halfway to the first level, construction was paused in order to create a substantial timber scaffold. This renewed concerns about the structural integrity of the tower, and sensational headlines such as "Eiffel Suicide!" and "Gustave Eiffel Has Gone Mad: He Has Been Confined in an Asylum" appeared in the tabloid press. At this stage, a small "creeper" crane designed to move up the tower was installed in each leg. They made use of the guides for the lifts which were to be fitted in the four legs. The critical stage of joining the legs at the first level was completed by the end of March 1888. Although the metalwork had been prepared with the utmost attention to detail, provision had been made to carry out small adjustments in order to precisely align the legs; hydraulic jacks were fitted to the shoes at the base of each leg, capable of exerting a force of 800 tonnes, and the legs were intentionally constructed at a slightly steeper angle than necessary, being supported by sandboxes on the scaffold. Although construction involved 300 on-site employees, only one person died thanks to Eiffel's stringent safety precautions and the use of movable gangways, guardrails and screens.
Lifts.
Equipping the tower with adequate and safe passenger lifts was a major concern of the government commission overseeing the Exposition. Although some visitors could be expected to climb to the first level, or even the second, lifts clearly had to be the main means of ascent.
Constructing lifts to reach the first level was relatively straightforward: the legs were wide enough at the bottom and so nearly straight that they could contain a straight track, and a contract was given to the French company Roux, Combaluzier & Lepape for two lifts to be fitted in the east and west legs. Roux, Combaluzier & Lepape used a pair of endless chains with rigid, articulated links to which the car was attached. Lead weights on some links of the upper or return sections of the chains counterbalanced most of the car's weight. The car was pushed up from below, not pulled up from above: to prevent the chain buckling, it was enclosed in a conduit. At the bottom of the run, the chains passed around diameter sprockets. Smaller sprockets at the top guided the chains.
Installing lifts to the second level was more of a challenge because a straight track was impossible. No French company wanted to undertake the work. The European branch of Otis Brothers & Company submitted a proposal but this was rejected: the fair's charter ruled out the use of any foreign material in the construction of the tower. The deadline for bids was extended but still no French companies put themselves forward, and eventually the contract was given to Otis in July 1887. Otis were confident they would eventually be given the contract and had already started creating designs.
The car was divided into two superimposed compartments, each holding 25 passengers, with the lift operator occupying an exterior platform on the first level. Motive power was provided by an inclined hydraulic ram long and in diameter in the tower leg with a stroke of : this moved a carriage carrying six sheaves. Five fixed sheaves were mounted higher up the leg, producing an arrangement similar to a block and tackle but acting in reverse, multiplying the stroke of the piston rather than the force generated. The hydraulic pressure in the driving cylinder was produced by a large open reservoir on the second level. After being exhausted from the cylinder, the water was pumped back up to the reservoir by two pumps in the machinery room at the base of the south leg. This reservoir also provided power to the lifts to the first level.
The original lifts for the journey between the second and third levels were supplied by Léon Edoux. A pair of hydraulic rams were mounted on the second level, reaching nearly halfway up to the third level. One lift car was mounted on top of these rams: cables ran from the top of this car up to sheaves on the third level and back down to a second car. Each car only travelled half the distance between the second and third levels and passengers were required to change lifts halfway by means of a short gangway. The 10-ton cars each held 65 passengers.
Inauguration and the 1889 exposition.
The main structural work was completed at the end of March 1889 and, on 31 March, Eiffel celebrated by leading a group of government officials, accompanied by representatives of the press, to the top of the tower. Because the lifts were not yet in operation, the ascent was made by foot, and took over an hour, with Eiffel stopping frequently to explain various features. Most of the party chose to stop at the lower levels, but a few, including the structural engineer, Émile Nouguier, the head of construction, Jean Compagnon, the President of the City Council, and reporters from "Le Figaro" and "Le Monde Illustré", completed the ascent. At 2:35 pm, Eiffel hoisted a large Tricolour to the accompaniment of a 25-gun salute fired at the first level.
There was still work to be done, particularly on the lifts and facilities, and the tower was not opened to the public until nine days after the opening of the exposition on 6 May; even then, the lifts had not been completed. The tower was an instant success with the public, and nearly 30,000 visitors made the 1,710-step climb to the top before the lifts entered service on 26 May.
Tickets cost 2 francs for the first level, 3 for the second, and 5 for the top, with half-price admission on Sundays, and by the end of the exhibition there had been 1,896,987 visitors.
After dark, the tower was lit by hundreds of gas lamps, and a beacon sent out three beams of red, white and blue light. Two searchlights mounted on a circular rail were used to illuminate various buildings of the exposition. The daily opening and closing of the exposition were announced by a cannon at the top.
On the second level, the French newspaper "Le Figaro" had an office and a printing press, where a special souvenir edition, "Le Figaro de la Tour", was made. There was also a pâtisserie.
At the top, there was a post office where visitors could send letters and postcards as a memento of their visit. Graffitists were also catered for: sheets of paper were mounted on the walls each day for visitors to record their impressions of the tower. Gustave Eiffel described some of the responses as "vraiment curieuse" ("truly curious").
Famous visitors to the tower included the Prince of Wales, Sarah Bernhardt, "Buffalo Bill" Cody (his Wild West show was an attraction at the exposition) and Thomas Edison. Eiffel invited Edison to his private apartment at the top of the tower, where Edison presented him with one of his phonographs, a new invention and one of the many highlights of the exposition. Edison signed the guestbook with this message: 
Eiffel had a permit for the tower to stand for 20 years. It was to be dismantled in 1909, when its ownership would revert to the City of Paris. The City had planned to tear it down (part of the original contest rules for designing a tower was that it should be easy to dismantle) but as the tower proved to be valuable for communication purposes, it was allowed to remain after the expiry of the permit.
Eiffel made use of his apartment at the top of the tower to carry out meteorological observations, and also used the tower to perform experiments on the action of air resistance on falling bodies.
Subsequent events.
For the 1900 "Exposition Universelle", the lifts in the east and west legs were replaced by lifts running as far as the second level constructed by the French firm Fives-Lille. These had a compensating mechanism to keep the floor level as the angle of ascent changed at the first level, and were driven by a similar hydraulic mechanism to the Otis lifts, although this was situated at the base of the tower. Hydraulic pressure was provided by pressurised accumulators located near this mechanism. At the same time the lift in the north pillar was removed and replaced by a staircase to the first level. The layout of both first and second levels was modified, with the space available for visitors on the second level. The original lift in the south pillar was removed 13 years later.
On 19 October 1901, Alberto Santos-Dumont, flying his No.6 airship, won a 100,000-franc prize offered by Henri Deutsch de la Meurthe for the first person to make a flight from St. Cloud to the Eiffel Tower and back in less than half an hour.
Many innovations took place at the Eiffel Tower in the early 20th century. In 1910, Father Theodor Wulf measured radiant energy at the top and bottom of the tower. He found more at the top than expected, incidentally discovering what are known today as cosmic rays. Just two years later, on 4 February 1912, Austrian tailor Franz Reichelt died after jumping from the first level of the tower (a height of 57 metres) to demonstrate his parachute design. In 1914, at the outbreak of World War I, a radio transmitter located in the tower jammed German radio communications, seriously hindering their advance on Paris and contributing to the Allied victory at the First Battle of the Marne. From 1925 to 1934, illuminated signs for Citroën adorned three of the tower's sides, making it the tallest advertising space in the world at the time. In April 1935, the tower was used to make experimental low-resolution television transmissions, using a shortwave transmitter of 200 watts power. On 17 November, an improved 180-line transmitter was installed.
On two separate but related occasions in 1925, the con artist Victor Lustig "sold" the tower for scrap metal. A year later, in February 1926, pilot Leon Collet was killed trying to fly under the tower. His aircraft became entangled in an aerial belonging to a wireless station. On 2 May 1929, a bust of Gustave Eiffel by Antoine Bourdelle was unveiled at the base of the north leg. In 1930, the tower lost the title of the world's tallest structure when the Chrysler Building in New York City was completed. In 1938, the decorative arcade around the first level was removed.
Upon the German occupation of Paris in 1940, the lift cables were cut by the French. The tower was closed to the public during the occupation and the lifts were not repaired until 1946. In 1940, German soldiers had to climb the tower to hoist the swastika, but the flag was so large it blew away just a few hours later, and was replaced by a smaller one. When visiting Paris, Hitler chose to stay on the ground. In August 1944, when the Allies were nearing Paris, Hitler ordered General Dietrich von Choltitz, the military governor of Paris, to demolish the tower along with the rest of the city. Von Choltitz disobeyed the order. On 25 June, before the Germans had been driven out of Paris, the Nazi flag was replaced with a Tricolour by two men from the French Naval Museum, who narrowly beat three men led by Lucien Sarniguet, who had lowered the Tricolour on 13 June 1940 when Paris fell to the Germans.
On 3 January 1956, a fire started in the television transmitter, damaging the top of the tower. Repairs took a year, and in 1957, the present radio aerial was added to the top. In 1964, the Eiffel Tower was officially declared to be a historical monument by the Minister of Cultural Affairs, André Malraux. A year later, due to increasing visitor numbers, an additional lift system was installed in the north pillar.
According to interviews, in 1967, Montreal Mayor Jean Drapeau negotiated a secret agreement with Charles de Gaulle for the tower to be dismantled and temporarily relocated to Montreal to serve as a landmark and tourist attraction during Expo 67. The plan was allegedly vetoed by the company operating the tower out of fear that the French government could refuse permission for the tower to be restored in its original location.
In 1982, the original lifts between the second and third levels were replaced after 97 years in service. These had been closed to the public between November and March because the water in the hydraulic drive tended to freeze. The new cars operate in pairs, with one counterbalancing the other, and perform the journey in one stage, reducing the journey time from eight minutes to less than two minutes. At the same time, two new emergency staircases were installed, replacing the original spiral staircases. In 1983, the south pillar was fitted with an electrically driven Otis lift to serve the Jules Verne restaurant. The Fives-Lille lifts in the east and west legs, fitted in 1899, were extensively refurbished in 1986. The cars were replaced, and a computer system was installed to completely automate the lifts. The motive power was moved from the water hydraulic system to a new electrically driven oil-filled hydraulic system, and the original water hydraulics were retained solely as a counterbalance system. A service lift was added to the south pillar for moving small loads and maintenance personnel three years later.
On 31 March 1984, Robert Moriarty flew a Beechcraft Bonanza under the tower. In 1987, A.J. Hackett made one of his first bungee jumps from the top of the Eiffel Tower, using a special cord he had helped develop. Hackett was arrested by the police. On 27 October 1991, Thierry Devaux, along with mountain guide Hervé Calvayrac, performed a series of acrobatic figures while bungee jumping from the second floor of the tower. Facing the Champ de Mars, Devaux used an electric winch between figures to go back up to the second floor. When firemen arrived, he stopped after the sixth jump.
On 31 December 1999, for its "Countdown to the Year 2000" celebration, flashing lights and high-powered searchlights were installed on the tower. Fireworks were set off all over it. An exhibition above a cafeteria on the first floor commemorates this event. The searchlights on top of the tower made it a beacon in Paris's night sky, and 20,000 flashing bulbs gave the tower a sparkly appearance for five minutes every hour on the hour.
On 31 December 2000, the lights sparkled blue for several nights to herald the new millennium. The sparkly lighting continued for 18 months until July 2001. The sparkling lights were turned on again on 21 June 2003, and the display was planned to last for 10 years before they needed replacing.
The tower received its th guest on 28 November 2002. The tower has operated at its maximum capacity of about 7 million visitors since 2003. In 2004, the Eiffel Tower began hosting a seasonal ice rink on the first level. A glass floor was installed on the first level during the 2014 refurbishment.
Design.
Material.
The puddled iron (wrought iron) of the Eiffel Tower weighs 7,300 tons, and the addition of lifts, shops and antennae have brought the total weight to approximately 10,100 tons. As a demonstration of the economy of design, if the 7,300 tons of metal in the structure were melted down, it would fill the 125 m2 base to a depth of only , assuming the density of the metal to be 7.8 tons per cubic metre. Additionally, a cubic box surrounding the tower (324 m x 125 m x 125 m) would contain  tons of air, weighing almost as much as the iron itself. Depending on the ambient temperature, the top of the tower may shift away from the sun by up to due to thermal expansion of the metal on the side facing the sun.
Wind considerations.
When it was built, many were shocked by the tower's daring form. Eiffel was accused of trying to create something artistic with no regard to the principles of engineering. However, Eiffel and his team – experienced bridge builders – understood the importance of wind forces, and knew that if they were going to build the tallest structure in the world, they had to be sure it could withstand them. In an interview with the newspaper "Le Temps" published on 14 February 1887, Eiffel said:
He used graphical methods to determine the strength of the tower and empirical evidence to account for the effects of wind, rather than a mathematical formula. Close examination of the tower reveals a basically exponential shape. All parts of the tower were over-designed to ensure maximum resistance to wind forces. The latticework on the top half of the tower and its four legs was even assumed to have no gaps. In the years since it was completed, engineers have put forward various mathematical hypotheses in an attempt to explain the success of the design. The most recent, devised in 2004 after letters sent by Eiffel to the French Society of Civil Engineers in 1885 were translated into English, is described as a non-linear integral equation based on counteracting the wind pressure on any point of the tower with the tension between the construction elements at that point.
The Eiffel Tower sways by up to 9 centimetres (3.5 in) in the wind.
Accommodation.
When originally built, the first level contained three restaurants—one French, one Russian and one Flemish—and an "Anglo-American Bar". After the exposition closed, the Flemish restaurant was converted to a 250-seat theatre. A promenade wide ran around the outside of the first level. At the top, there were laboratories for various experiments, and a small apartment reserved for Gustave Eiffel to entertain guests, which is now open to the public, complete with period decorations and lifelike mannequins of Eiffel and some of his notable guests.
Passenger lifts.
The arrangement of the lifts has been changed several times during the tower's history. Given the elasticity of the cables and the time taken to align the cars with the landings, each lift, in normal service, takes an average of 8 minutes and 50 seconds to do the round trip, spending an average of 1 minute and 15 seconds at each level. The average journey time between levels is 1 minute. The original hydraulic mechanism is on public display in a small museum at the base of the east and west legs. Because the mechanism requires frequent lubrication and maintenance, public access is often restricted. The rope mechanism of the north tower can be seen as visitors exit the lift.
Engraved names.
Gustave Eiffel engraved on the tower the names of 72 French scientists, engineers and mathematicians in recognition of their contributions to the building of the tower. Eiffel chose this "invocation of science" because of his concern over the artists' protest. At the beginning of the 20th century, the engravings were painted over, but they were restored in 1986–87 by the , a company operating the tower.
Aesthetics.
The tower is painted in three shades: lighter at the top, getting progressively darker towards the bottom to perfectly complement the Parisian sky. It was originally reddish-brown; this changed in 1968 to a bronze colour known as "Eiffel Tower Brown".
The only non-structural elements are the four decorative grill-work arches, added in Sauvestre's sketches, which served to make the tower look more substantial and to make a more impressive entrance to the exposition.
One of the great Hollywood movie clichés is that the view from a Parisian window always includes the tower. In reality, since zoning restrictions limit the height of most buildings in Paris to seven storeys, only a small number of tall buildings have a clear view of the tower.
Maintenance.
Maintenance of the tower includes applying 60 tons of paint every seven years to prevent it from rusting. The tower has been completely repainted at least 19 times since it was built. Lead paint was still being used as recently as 2001 when the practice was stopped out of concern for the environment.
Tourism.
Transport.
The nearest Paris Métro station is Bir-Hakeim and the nearest RER station is Champ de Mars-Tour Eiffel. The tower itself is located at the intersection of the quai Branly and the Pont d'Iéna.
Popularity.
More than 250 million people have visited the tower since it was completed in 1889. In 2015, there were 6.91 million visitors. The tower is the most-visited paid monument in the world. An average of 25,000 people ascend the tower every day which can result in long queues. Tickets can be purchased online to avoid the long queues.
Restaurants.
The tower has two restaurants: on the first level, and , a gourmet restaurant with its own lift on the second level. This restaurant has one star in the Michelin Red Guide. It is run by the multi-Michelin star chef Alain Ducasse and owes its name to the famous science-fiction writer Jules Verne.
Replicas.
As one of the most iconic landmarks in the world, the Eiffel Tower has been the inspiration for the creation of many replicas and similar towers. An early example is Blackpool Tower in England. The mayor of Blackpool, Sir John Bickerstaffe, was so impressed on seeing the Eiffel Tower at the 1889 exposition that he commissioned a similar tower to be built in his town. It opened in 1894 and is 158 metres (518 ft) tall. Tokyo Tower in Japan, built as a communications tower in 1958, was also inspired by the Eiffel Tower.
There are various scale models of the tower in the United States, including one at Paris, Texas built in 1993, two 1:3 scale models in China, one in Durango, Mexico that was donated by the local French community, and several across Europe.
In 2011, the TV show "Pricing the Priceless" on the National Geographic Channel speculated that a full-size replica of the tower would cost approximately US$480 million to build.
Communications.
The tower has been used for making radio transmissions since the beginning of the 20th century. Until the 1950s, sets of aerial wires ran from the cupola to anchors on the Avenue de Suffren and Champ de Mars. These were connected to longwave transmitters in small bunkers. In 1909, a permanent underground radio centre was built near the south pillar, which still exists today. On 20 November 1913, the Paris Observatory, using the Eiffel Tower as an aerial, exchanged wireless signals with the United States Naval Observatory, which used an aerial in Arlington, Virginia. The object of the transmissions was to measure the difference in longitude between Paris and Washington, D.C. Today, radio and digital television signals are transmitted from the Eiffel Tower.
Digital television.
A television antenna was first installed on the tower in 1957, increasing its height by 18.7 m (61.4 ft). Work carried out in 2000 added a further 5.3 m (17.4 ft), giving the current height of 324 m (1,063 ft). Analogue television signals from the Eiffel Tower ceased on 8 March 2011.
Illumination copyright.
The tower and its image have long been in the public domain. In June 1990, however, a French court ruled that a special lighting display on the tower in 1989 to mark the tower's 100th anniversary was an "original visual creation" protected by copyright. The Court of Cassation, France's judicial court of last resort, upheld the ruling in March 1992. The (SETE) now considers any illumination of the tower to be a separate work of art that falls under copyright. As a result, it is illegal to publish contemporary photographs of the lit tower at night without permission in France and some other countries.
The imposition of copyright has been controversial. The Director of Documentation for what was then called the (SNTE), Stéphane Dieu, commented in 2005: "It is really just a way to manage commercial use of the image, so that it isn't used in ways which we don't approve". SNTE made just over €1 million from copyright fees in 2002. However, it could also be used to prohibit the publication of tourist photographs of the tower at night, as well as hindering non-profit and semi-commercial publication of images of the tower.
French doctrine and jurisprudence allows pictures incorporating a copyrighted work as long as their presence is incidental or accessory to the subject being represented, a reasoning akin to the "de minimis" rule. Therefore, SETE may be unable to claim copyright on photographs of Paris which happen to include the lit tower.
In popular culture.
As a global landmark, the Eiffel Tower is featured in films, video games and TV shows.
In a commitment ceremony in 2007, Erika Eiffel, an American woman, "married" the Eiffel Tower; her relationship with the tower has been the subject of extensive global publicity.
Taller structures.
Although it was the world's tallest structure when completed in 1889, the Eiffel Tower has lost its standing both as the tallest lattice tower and as the tallest structure in France.

</doc>
<doc id="9235" url="https://en.wikipedia.org/wiki?curid=9235" title="Ethical egoism">
Ethical egoism

Ethical egoism is the normative ethical position that moral agents ought to do what is in their own self-interest. It differs from psychological egoism, which claims that people can only act in their self-interest. Ethical egoism also differs from rational egoism, which holds that it is rational to act in one's self-interest.
Ethical egoism holds that actions whose consequences will benefit the doer can be considered ethical.
Ethical egoism contrasts with ethical altruism, which holds that moral agents have an obligation to help others. Egoism and altruism both contrast with ethical utilitarianism, which holds that a moral agent should treat one's self (also known as the subject) with no higher regard than one has for others (as egoism does, by elevating self-interests and "the self" to a status not granted to others). But it also holds that one should not (as altruism does) sacrifice one's own interests to help others' interests, so long as one's own interests (i.e. one's own desires or well-being) are substantially equivalent to the others' interests and well-being. Egoism, utilitarianism, and altruism are all forms of consequentialism, but egoism and altruism contrast with utilitarianism, in that egoism and altruism are both agent-focused forms of consequentialism (i.e. subject-focused or subjective). However, utilitarianism is held to be agent-neutral (i.e. objective and impartial): it does not treat the subject's (i.e. the self's, i.e. the moral "agent's") own interests as being more or less important than the interests, desires, or well-being of others.
Ethical egoism does not, however, require moral agents to harm the interests and well-being of others when making moral deliberation; e.g. what is in an agent's self-interest may be incidentally detrimental, beneficial, or neutral in its effect on others. Individualism allows for others' interest and well-being to be disregarded or not, as long as what is chosen is efficacious in satisfying the self-interest of the agent. Nor does ethical egoism necessarily entail that, in pursuing self-interest, one ought always to do what one wants to do; e.g. in the long term, the fulfillment of short-term desires may prove detrimental to the self. Fleeting pleasure, then, takes a back seat to protracted eudaimonia. In the words of James Rachels, "Ethical egoism [...] endorses selfishness, but it doesn't endorse foolishness."
Ethical egoism is often used as the philosophical basis for support of right-libertarianism and individualist anarchism. These are political positions based partly on a belief that individuals should not coercively prevent others from exercising freedom of action.
Forms of ethical egoism.
Ethical egoism can be broadly divided into three categories: individual, personal, and universal. An "individual ethical egoist" would hold that all people should do whatever benefits "my" ("the individual")" "self-interest; a "personal ethical egoist" would hold that he or she should act in "his or her" self-interest, but would make no claims about what anyone else ought to do; a "universal ethical egoist" would argue that everyone should act in ways that are in their self-interest.
History.
Ethical egoism was introduced by the philosopher Henry Sidgwick in his book "The Methods of Ethics", written in 1874. Sidgwick compared egoism to the philosophy of utilitarianism, writing that whereas utilitarianism sought to maximize overall pleasure, egoism focused only on maximizing individual pleasure.
Philosophers before Sidgwick have also retroactively been identified as ethical egoists. One ancient example is the philosophy of Yang Zhu (4th century B.C.), Yangism, who views "wei wo", or "everything for myself", as the only virtue necessary for self-cultivation. Ancient Greek philosophers like Plato, Aristotle and the Stoics were exponents of virtue ethics, and "did not accept the formal principle that whatever the good is, we should seek only our own good, or prefer it to the good of others." However, the beliefs of the Cyrenaics have been referred to as a "form of egoistic hedonism", and while some refer to Epicurus' hedonism as a form of virtue ethics, others argue his ethics are more properly described as ethical egoism.
Justifications.
Philosopher James Rachels, in an essay that takes as its title the theory's name, outlines the three arguments most commonly touted in its favor:
Notable proponents.
The term "ethical egoism" has been applied retroactively to philosophers such as Bernard de Mandeville and to many other materialists of his generation, although none of them declared themselves to be egoists. Note that materialism does not necessarily imply egoism, as indicated by Karl Marx, and the many other materialists who espoused forms of collectivism. It has been argued that ethical egoism can lend itself to individualist anarchism such as that of Benjamin Tucker, or the combined anarcho-communism and egoism of Emma Goldman, both of whom were proponents of many egoist ideas put forward by Max Stirner. In this context, egoism is another way of describing the sense that the common good should be enjoyed by all. However, most notable anarchists in history have been less radical, retaining altruism and a sense of the importance of the individual that is appreciable but does not go as far as egoism. Recent trends to greater appreciation of egoism within anarchism tend to come from less classical directions such as post-left anarchy or Situationism (e.g. Raoul Vaneigem). Egoism has also been referenced by anarcho-capitalists, such as Murray Rothbard.
Philosopher Max Stirner, in his book "The Ego and Its Own", was the first philosopher to call himself an egoist, though his writing makes clear that he desired not a new idea of morality (ethical egoism), but rather a rejection of morality (amoralism), as a nonexistent and limiting “spook”; for this, Stirner has been described as the first individualist anarchist. Other philosophers, such as Thomas Hobbes and David Gauthier, have argued that the conflicts which arise when people each pursue their own ends can be resolved for the best of each individual only if they all voluntarily forgo some of their aims — that is, one's self-interest is often best pursued by allowing others to pursue their self-interest as well so that liberty is equal among individuals. Sacrificing one's short-term self-interest to maximize one's long-term self-interest is one form of "rational self-interest" which is the idea behind most philosophers' advocacy of ethical egoism. Egoists have also argued that one's actual interests are not immediately obvious, and that the pursuit of self-interest involves more than merely the acquisition of some good, but the "maximizing" of one's chances of survival and/or happiness.
Philosopher Friedrich Nietzsche suggested that egoistic or "life-affirming" behavior stimulates jealousy or "ressentiment" in others, and that this is the psychological motive for the altruism in Christianity. Sociologist Helmut Schoeck similarly considered envy the motive of collective efforts by society to reduce the disproportionate gains of successful individuals through moral or legal constraints, with altruism being primary among these. In addition, Nietzsche (in "Beyond Good and Evil") and Alasdair MacIntyre (in "After Virtue") have pointed out that the ancient Greeks did not associate morality with altruism in the way that post-Christian Western civilization has done.
Aristotle's view is that we have duties to ourselves as well as to other people (e.g. friends) and to the "polis" as a whole. The same is true for Thomas Aquinas, Christian Wolff and Immanuel Kant, who claim that there are duties to ourselves as Aristotle did, although it has been argued that, for Aristotle, the duty to one's self is primary.
Ayn Rand argued that there is a positive harmony of interests among free, rational humans, such that no moral agent can rationally coerce another person consistently with his own long-term self-interest. Rand argued that other people are an enormous value to an individual's well-being (through education, trade and affection), but also that this value could be fully realized only under conditions of political and economic freedom. According to Rand, voluntary trade alone can assure that human interaction is "mutually" beneficial. Rand's student, Leonard Peikoff has argued that the identification of one's interests itself is impossible absent the use of principles, and that self-interest cannot be consistently pursued absent a consistent adherence to certain ethical principles. Recently, Rand's position has also been defended by such writers as Tara Smith, Tibor Machan, Allan Gotthelf, David Kelley, Douglas Rasmussen, Nathaniel Branden, Harry Binswanger, Andrew Bernstein, and Craig Biddle.
Philosopher David L. Norton identified himself an "ethical individualist," and, like Rand, saw a harmony between an individual's fidelity to his own self-actualization, or "personal destiny," and the achievement of society's well being.
Criticisms.
According to amoralism, there is nothing wrong with egoism, but there is also nothing ethical about it; one can adopt rational egoism and drop morality as a superfluous attribute of the egoism.
Ethical egoism has been alleged as the basis for immorality. Egoism has also been alleged as being outside the scope of moral philosophy. Thomas Jefferson writes in an 1814 letter to Thomas Law:
In contrast, Rand saw ethics as a necessity for human survival and well-being, and argued that the "social" implications of morality, including natural rights, were simply a subset of the wider field of ethics. Thus, for Rand, "virtue" included productiveness, honesty with oneself, and scrupulousness of thought. Although she greatly admired Jefferson, she also wrote:
In "The Moral Point of View", Kurt Baier objects that ethical egoism provides no moral basis for the resolution of conflicts of interest, which, in his opinion, form the only vindication for a moral code. Were this an ideal world, one in which interests and purposes never jarred, its inhabitants would have no need of a specified set of ethics, according to Baier. This, however, is not an "ideal world." Baier believes that ethical egoism fails to provide the moral guidance and arbitration that it necessitates. Far from resolving conflicts of interest, claimed Baier, ethical egoism all too often spawns them. To this, as Rachels has shown, the ethical egoist may object that he cannot admit a construct of morality whose aim is merely to forestall conflicts of interest. "On his view," he writes, "the moralist is not like a courtroom judge, who resolves disputes. Instead, he is like the Commissioner of Boxing, who urges each fighter to do his best."
Baiers is also part of a team of philosophers who hold that ethical egoism is paradoxical, implying that to do what is in one's best interests can be both wrong and right in ethical terms. Although a successful pursuit of self-interest may be viewed as a moral victory, it could also be dubbed immoral if it prevents another person from executing what is in "his" best interests. Again, however, the ethical egoists have responded by assuming the guise of the Commissioner of Boxing. His philosophy precludes empathy for the interests of others, so forestalling them is perfectly acceptable. "Regardless of whether we think this is a correct view," adds Rachels, "it is, at the very least, a "consistent" view, and so this attempt to convict the egoist of self-contradiction fails."
Finally, it has been averred that ethical egoism is no better than bigotry in that, like racism, it divides people into two types — themselves and others — and discriminates against one type on the basis of some arbitrary disparity. This, to Rachels's mind, is probably the best objection to ethical egoism, for it provides the soundest reason why the interests of others ought to concern the interests of the self. "What," he asks, "is the difference between myself and others that justifies placing myself in this special category? Am I more intelligent? Do I enjoy my life more? Are my accomplishments greater? Do I have needs or abilities that are so different from the needs and abilities of others? "What is it that makes me so special"? Failing an answer, it turns out that Ethical Egoism is an arbitrary doctrine, in the same way that racism is arbitrary. [...] We should care about the interests of other people "for the very same reason we care about our own interests"; for their needs and desires are comparable to our own."

</doc>
<doc id="9236" url="https://en.wikipedia.org/wiki?curid=9236" title="Evolution">
Evolution

Evolution is change in the heritable traits of biological populations over successive generations. Evolutionary processes give rise to diversity at every level of biological organisation, including the levels of species, individual organisms, and molecules.
All life on Earth shares a common ancestor known as the last universal ancestor, which lived approximately 3.5–3.8 billion years ago, although a study in 2015 found "remains of biotic life" from 4.1 billion years ago in ancient rocks in Western Australia. According to one of the researchers, "If life arose relatively quickly on Earth ... then it could be common in the universe."
Repeated formation of new species (speciation), change within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth are demonstrated by shared sets of morphological and biochemical traits, including shared DNA sequences. These shared traits are more similar among species that share a more recent common ancestor, and can be used to reconstruct a biological "tree of life" based on evolutionary relationships (phylogenetics), using both existing species and fossils. The fossil record includes a progression from early biogenic graphite, to microbial mat fossils, to fossilized multicellular organisms. Existing patterns of biodiversity have been shaped both by speciation and by extinction. More than 99 percent of all species that ever lived on Earth are estimated to be extinct. Estimates of Earth's current species range from 10 to 14 million, of which about 1.2 million have been documented.
In the mid-19th century, Charles Darwin formulated the scientific theory of evolution by natural selection, published in his book "On the Origin of Species" (1859). Evolution by natural selection is a process demonstrated by the observation that more offspring are produced than can possibly survive, along with three facts about populations: 1) traits vary among individuals with respect to morphology, physiology, and behaviour (phenotypic variation), 2) different traits confer different rates of survival and reproduction (differential fitness), and 3) traits can be passed from generation to generation (heritability of fitness). Thus, in successive generations members of a population are replaced by progeny of parents better adapted to survive and reproduce in the biophysical environment in which natural selection takes place. This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. Natural selection is the only known cause of adaptation but not the only known cause of evolution. Other, nonadaptive causes of microevolution include mutation and genetic drift.
In the early 20th century the modern evolutionary synthesis integrated classical genetics with Darwin's theory of evolution by natural selection through the discipline of population genetics. The importance of natural selection as a cause of evolution was accepted into other branches of biology. Moreover, previously held notions about evolution, such as orthogenesis, evolutionism, and other beliefs about innate "progress" within the largest-scale trends in evolution, became obsolete scientific theories. Scientists continue to study various aspects of evolutionary biology by forming and testing hypotheses, constructing mathematical models of theoretical biology and biological theories, using observational data, and performing experiments in both the field and the laboratory.
In terms of practical application, an understanding of evolution has been instrumental to developments in numerous scientific and industrial fields, including agriculture, human and veterinary medicine, and the life sciences in general. Discoveries in evolutionary biology have made a significant impact not just in the traditional branches of biology but also in other academic disciplines, including biological anthropology, and evolutionary psychology. Evolutionary Computation, a sub-field of Artificial Intelligence, is the result of the application of Darwinian principles to problems in Computer Science.
History of evolutionary thought.
The proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork "De rerum natura" ("On the Nature of Things"). In contrast to these materialistic views, Aristotle understood all natural things, not only living things, as being imperfect actualisations of different fixed natural possibilities, known as "forms," "ideas," or (in Latin translations) ""species"." This was part of his teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.
In the 17th century, the new method of modern science rejected Aristotle's approach. It sought explanations of natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences, the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, "species," to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. These species were designed by God, but showed differences caused by local conditions. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognized the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.
Other naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or "filament"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's "transmutation" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the "Natural Theology or Evidences of the Existence and Attributes of the Deity" (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.
The crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin in terms of variable populations. Partly influenced by "An Essay on the Principle of Population" (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a "struggle for existence" in which favorable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of "natural selection" from 1838 onwards and was writing up his "big book" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at a 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his "abstract" as "On the Origin of Species" explained natural selection in detail and in a way that led to an increasingly wide acceptance of concepts of evolution. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.
Precise mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cells structure. De Vries was also one of the researchers who made Mendel's work well-known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.
In the 1920s and 1930s a modern evolutionary synthesis connected natural selection, mutation theory, and Mendelian inheritance into a unified theory that applied generally to any branch of biology. The modern synthesis was able to explain patterns observed across species in populations, through fossil transitions in palaeontology, and even complex cellular mechanisms in developmental biology. The publication of the structure of DNA by James Watson and Francis Crick in 1953 demonstrated a physical basis for inheritance. Molecular biology improved our understanding of the relationship between genotype and phenotype. Advancements were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that "nothing in biology makes sense except in the light of evolution," because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.
Since then, the modern synthesis has been further extended to explain biological phenomena across the full and integrative scale of the biological hierarchy, from genes to species. This extension, known as evolutionary developmental biology and informally called "evo-devo," emphasises how changes between generations (evolution) acts on patterns of change within individual organisms (development).
Heredity.
Evolution in organisms occurs through changes in heritable traits—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the "brown-eye trait" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its genotype.
The complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. These traits come from the interaction of its genotype with the environment. As a result, many aspects of an organism's phenotype are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.
Heritable traits are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specify the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, the long strands of DNA form condensed structures called chromosomes. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are more complex and are controlled by quantitative trait loci (multiple interacting genes).
Recent findings have confirmed important examples of heritable changes that cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Descendants inherit genes plus environmental characteristics generated by the ecological actions of ancestors. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.
Variation.
An individual organism's phenotype results from both its genotype and the influence from the environment it has lived in. A substantial part of the phenotypic variation in a population is caused by genotypic variation. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.
Natural selection will only cause evolution if there is enough genetic variation in a population. Before the discovery of Mendelian genetics, one common hypothesis was blending inheritance. But with blending inheritance, genetic variance would be rapidly lost, making evolution by natural selection implausible. The Hardy–Weinberg principle provides the solution to how variation is maintained in a population with Mendelian inheritance. The frequencies of alleles (variations in a gene) will remain constant in the absence of selection, mutation, migration and genetic drift.
Variation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is identical in all individuals of that species. However, even relatively small differences in genotype can lead to dramatic differences in phenotype: for example, chimpanzees and humans differ in only about 5% of their genomes.
Mutation.
Mutations are changes in the DNA sequence of a cell's genome. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect. Based on studies in the fly "Drosophila melanogaster", it has been suggested that if a mutation changes a protein produced by a gene, this will probably be harmful, with about 70% of these mutations having damaging effects, and the remainder being either neutral or weakly beneficial.
Mutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.
New genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA.
The generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions. When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to one hundred independent domains that each catalyse one step in the overall process, like a step in an assembly line.
Sex and recombination.
In asexual organisms, genes are inherited together, or "linked", as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.
The two-fold cost of sex was first described by John Maynard Smith. The first cost is that only one of the two sexes can bear young. (This cost does not apply to hermaphroditic species, like most plants and many invertebrates.) The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. (Again, this applies mostly to the evolution of sexual dimorphism, which occurred long after the evolution of sex itself.) Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms (although more common than sexual dimorphism). The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment.
Gene flow.
Gene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy metal tolerant and heavy metal sensitive populations of grasses.
Gene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast "Saccharomyces cerevisiae" and the adzuki bean weevil "Callosobruchus chinensis" has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.
Large-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.
Mechanisms.
From a Neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms. For example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, genetic hitchhiking, mutation and gene flow.
Natural selection.
Evolution by means of natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It has often been called a "self-evident" mechanism because it necessarily follows from three simple facts:
More offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage.
The central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.
If an allele increases fitness more than the other alleles of that gene, then with each generation this allele will become more common within the population. These traits are said to be "selected "for"." Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele becoming rarer—they are "selected "against"." Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form (see Dollo's law).
[[File:Genetic Distribution.svg|thumb|left|315px|These charts depict the different types of genetic selection. On each graph, the x-axis variable is the type of phenotypic trait and the y-axis variable is the amount of organisms. Group A is the original population and Group B is the population after selection.
· Graph 1 shows directional selection, in which a single extreme phenotype is favored.
· Graph 2 depicts stabilizing selection, where the intermediate phenotype is favored over the extreme traits.
· Graph 3 shows disruptive selection, in which the extreme phenotypes are favored over the intermediate.]]
Natural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to slowly become all the same height.
A special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.
Natural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. "Nature" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: "Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity and material cycles (ie: exchange of materials between living and nonliving parts) within the system." Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.
Natural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation, as discussed below.
Biased mutation.
In addition to being a major source of variation, mutation may also function as a mechanism of evolution when there are different probabilities at the molecular level for different mutations to occur, a process known as mutation bias. If two genotypes, for example one with the nucleotide G and another with the nucleotide A in the same position, have the same fitness, but mutation from G to A happens more often than mutation from A to G, then genotypes with A will tend to evolve. Different insertion vs. deletion mutation biases in different taxa can lead to the evolution of different genome sizes. Developmental or mutational biases have also been observed in morphological evolution. For example, according to the phenotype-first theory of evolution, mutations can eventually cause the genetic assimilation of traits that were previously induced by the environment.
Mutation bias effects are superimposed on other processes. If selection would favor either one out of two mutations, but there is no extra advantage to having both, then the mutation that occurs the most frequently is the one that is most likely to become fixed in a population. Mutations leading to the loss of function of a gene are much more common than mutations that produce a new, fully functional gene. Most loss of function mutations are selected against. But when selection is weak, mutation bias towards loss of function can affect evolution. For example, pigments are no longer useful when animals live in the darkness of caves, and tend to be lost. This kind of loss of function can occur because of mutation bias, and/or because the function had a cost, and once the benefit of the function disappeared, natural selection leads to the loss. Loss of sporulation ability in "Bacillus subtilis" during laboratory evolution appears to have been caused by mutation bias, rather than natural selection against the cost of maintaining sporulation ability. When there is no selection for loss of function, the speed at which loss evolves depends more on the mutation rate than it does on the effective population size, indicating that it is driven more by mutation bias than by genetic drift. In parasitic organisms, mutation bias leads to selection pressures as seen in Ehrlichia. Mutations are biased towards antigenic variants in outer-membrane proteins.
Genetic drift.
Genetic drift is the change in allele frequency from one generation to the next that occurs because alleles are subject to sampling error. As a result, when selective forces are absent or relatively weak, allele frequencies tend to "drift" upward or downward randomly (in a random walk). This drift halts when an allele eventually becomes fixed, either by disappearing from the population, or replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that began with the same genetic structure to drift apart into two divergent populations with different sets of alleles.
It is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.
The neutral theory of molecular evolution proposed that most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. Hence, in this model, most genetic changes in a population are the result of constant mutation pressure and genetic drift. This form of the neutral theory is now largely abandoned, since it does not seem to fit the genetic variation seen in nature. However, a more recent and better-supported version of this model is the nearly neutral theory, where a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other alternative theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft.
The time for a neutral allele to become fixed by genetic drift depends on population size, with fixation occurring more rapidly in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.
Genetic hitchhiking.
Recombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.
Gene flow.
Gene flow involves the exchange of genes between populations and between species. The presence or absence of gene flow fundamentally changes the course of evolution. Due to the complexity of organisms, any two completely isolated populations will eventually evolve genetic incompatibilities through neutral processes, as in the Bateson-Dobzhansky-Muller model, even if both populations remain essentially identical in terms of their adaptation to the environment.
If genetic differentiation between populations develops, gene flow between populations can introduce traits or alleles which are disadvantageous in the local population and this may lead to organisms within these populations evolving mechanisms that prevent mating with genetically distant populations, eventually resulting in the appearance of new species. Thus, exchange of genetic information between individuals is fundamentally important for the development of the biological species concept.
During the development of the modern synthesis, Sewall Wright developed his shifting balance theory, which regarded gene flow between partially isolated populations as an important aspect of adaptive evolution. However, recently there has been substantial criticism of the importance of the shifting balance theory.
Outcomes.
Evolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed.
These outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction; whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in gene frequency and adaptation. In general, macroevolution is regarded as the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.
A common misconception is that evolution has goals, long-term plans, or an innate tendency for "progress," as expressed in beliefs such as orthogenesis and evolutionism; realistically however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size, and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to modern evolutionary research, since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.
Adaptation.
Adaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term "adaptation" for the evolutionary process and "adaptive trait" for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:
Adaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria "Escherichia coli" evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, "Flavobacterium" evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium "Sphingobium" evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).
Adaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.
During evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.
However, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard "Holaspis guentheri", which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.
An area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.
Coevolution.
Interactions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.
Cooperation.
Not all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.
Coalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.
Such cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the "helping" individual contains alleles which promote the helping activity, it is likely that its kin will "also" contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.
Speciation.
Speciation is the process where a species diverges into two or more descendant species.
There are multiple ways to define the concept of "species." The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by Ernst Mayr in 1942, the BSC states that "species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups." Despite its wide and long-term use, the BSC like others is not without controversy, for example because these concepts cannot be applied to prokaryotes, and this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.
Barriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the gray tree frog being a particularly well-studied example.
Speciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four mechanisms for speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.
The second mechanism of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.
The third mechanism of speciation is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass "Anthoxanthum odoratum", which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause "reinforcement", which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.
Finally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and non-random mating, to allow reproductive isolation to evolve.
One type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species "Arabidopsis thaliana" and "Arabidopsis arenosa" crossbred to give the new species "Arabidopsis suecica". This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.
Speciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short "bursts" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.
Extinction.
Extinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future.
The role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous "low-level" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.
Evolutionary history of life.
Origin of life.
The Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as "remains of biotic life" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, "If life arose relatively quickly on Earth … then it could be common in the universe." 
More than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described.
Highly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.
Common descent.
All organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits and finally, that organisms can be classified using these similarities into a hierarchy of nested groups—similar to a family tree. However, modern research has suggested that, due to horizontal gene transfer, this "tree of life" may be more complicated than a simple branching tree since some genes have spread independently between distantly related species.
Past species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, paleontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.
More recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.
Evolution of life.
Prokaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6–2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.
The history of life was that of the unicellular eukaryotes, prokaryotes and archaea until about 610 million years ago when multicellular organisms began to appear in the oceans in the Ediacaran period. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.
Soon after the emergence of these first multicellular organisms, a remarkable amount of biological diversity appeared over approximately 10 million years, in an event called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis. 
About 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from "reptile"-like lineages), mammals around 129 million years ago, homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.
Applications.
Concepts and models used in evolutionary biology, such as natural selection, have many applications.
Artificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.
Understanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.
Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.
In computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.
Social and cultural responses.
In the 19th century, particularly after the publication of "On the Origin of Species" in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.
While various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of "Vestiges of the Natural History of Creation" in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.
The teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes Trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 "Epperson v. Arkansas" decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 "Kitzmiller v. Dover Area School District" case.
Further reading.
Introductory reading
Advanced reading

</doc>
<doc id="9238" url="https://en.wikipedia.org/wiki?curid=9238" title="Ernst Mayr">
Ernst Mayr

Ernst Walter Mayr (; 5 July 1904 – 3 February 2005) was one of the 20th century's leading evolutionary biologists. He was also a renowned taxonomist, tropical explorer, ornithologist, philosopher of biology, and historian of science. His work contributed to the conceptual revolution that led to the modern evolutionary synthesis of Mendelian genetics, systematics, and Darwinian evolution, and to the development of the biological species concept.
Although Charles Darwin and others posited that multiple species could evolve from a single common ancestor, the mechanism by which this occurred was not understood, creating the "species problem". Ernst Mayr approached the problem with a new definition for species. In his book "Systematics and the Origin of Species" (1942) he wrote that a species is not just a group of morphologically similar individuals, but a group that can breed only among themselves, excluding all others. When populations within a species become isolated by geography, feeding strategy, mate choice, or other means, they may start to differ from other populations through genetic drift and natural selection, and over time may evolve into new species. The most significant and rapid genetic reorganization occurs in extremely small populations that have been isolated (as on islands).
His theory of peripatric speciation (a more precise form of allopatric speciation which he advanced), based on his work on birds, is still considered a leading mode of speciation, and was the theoretical underpinning for the theory of punctuated equilibrium, proposed by Niles Eldredge and Stephen Jay Gould. Mayr is sometimes credited with inventing modern philosophy of biology, particularly the part related to evolutionary biology, which he distinguished from physics due to its introduction of (natural) history into science.
Biography.
Mayr was the second son of Helene Pusinelli and Dr. Otto Mayr. His father was a jurist (District Prosecuting
Attorney at Würzburg) but took an interest in natural history and took the children out on field trips. He learnt all the local birds in Würzburg from his elder brother Otto. He also had access to a natural history magazine for amateurs, "Kosmos". His father died just before he was thirteen. The family then moved to Dresden and he studied at the Staatsgymnasium (“Royal Gymnasium” until 1918) in Dresden-Neustadt and completed his high school education there. In April 1922, while still in high school, he joined the newly founded Saxony Ornithologists’ Association. Here he met Rudolf Zimmermann, who became his ornithological mentor. In February 1923, Mayr passed his high school examination (Abitur) and his mother rewarded him with a pair of binoculars.
On 23 March 1923 on the lakes of Moritzburg, the Frauenteich, he spotted what he identified as a red-crested pochard. The species had not been seen in Saxony since 1845 and the local club argued about the identity. Raimund Schelcher (1891–1979) of the club then suggested that Mayr visit his classmate Erwin Stresemann on his way to Greifswald, where Mayr was to begin his medical studies. After a tough interrogation, Stresemann accepted and published the sighting as authentic. Stresemann was very impressed and suggested that, between semesters, Mayr could work as a volunteer in the ornithological section of the museum. Mayr wrote about this event, "It was as if someone had given me the key to heaven." He entered the University of Greifswald in 1923 and, according to Mayr himself, "took the medical curriculum (to satisfy a family tradition) but after only a year, he decided to leave medicine and enrolled at the Faculty of Biological Sciences." Mayr was endlessly interested in ornithology and "chose Greifswald at the Baltic for my studies for no other reason than that ... it was situated in the ornithologically most interesting area." Although he ostensibly planned to become a physician, he was "first and foremost an ornithologist." During the first semester break Stresemann gave him a test to identify treecreepers and Mayr was able to identify most of the specimens correctly. Stresemann declared that Mayr "was a born systematist". In 1925, Stresemann suggested that he give up his medical studies, in fact he should leave the faculty of medicine and enrole into the faculty of Biology and then join the Berlin Museum with the prospect of bird-collecting trips to the tropics, on the condition that he completed his doctoral studies in 16 months. Mayr completed his doctorate in ornithology at the University of Berlin under Dr. Carl Zimmer, who was a full professor (Ordentlicher Professor), on 24 June 1926 at the age of 21. On 1 July he accepted the position offered to him at the museum for a monthly salary of 330.54 Reichsmark.
At the International Zoological Congress at Budapest in 1927, Mayr was introduced by Stresemann to banker and naturalist Walter Rothschild, who asked him to undertake an expedition to New Guinea on behalf of himself and the American Museum of Natural History in New York. In New Guinea, Mayr collected several thousand bird skins (he named 26 new bird species during his lifetime) and, in the process also named 38 new orchid species. During his stay in New Guinea, he was invited to accompany the Whitney South Seas Expedition to the Solomon Islands. Also, while in New Guinea, he visited the Lutheran missionaries Otto Thiele and Christian Keyser, in the Finschhafen district; there, while in conversation with his hosts, he uncovered the discrepancies in Hermann Detzner's popular book "Four Years Among the Cannibals in German Guinea from 1914 to the Truce", in which Detzner claimed to have seen the interior, discovered several species of flora and fauna, while remaining only steps ahead of the Australian patrols sent to capture him.
He returned to Germany in 1930, and in 1931 he accepted a curatorial position at the American Museum of Natural History, where he played the important role of brokering and acquiring the Walter Rothschild collection of bird skins, which was being sold in order to pay off a blackmailer. During his time at the museum he produced numerous publications on bird taxonomy, and in 1942 his first book "Systematics and the Origin of Species", which completed the evolutionary synthesis started by Darwin.
After Mayr was appointed at the American Museum of Natural History, he influenced American ornithological research by mentoring young birdwatchers. Mayr was surprised at the differences between American and German birding societies. He noted that the German society was "far more scientific, far more interested in life histories and breeding bird species, as well as in reports on recent literature."
Mayr organized a monthly seminar under the auspices of the Linnean Society of New York. Under the influence of J.A. Allen, Frank Chapman, and Jonathan Dwight, the society concentrated on taxonomy and later became a clearing house for bird banding and sight records.
Mayr encouraged his Linnaean Society seminar participants to take up a specific research project of their own. Under Mayr's influence one of them, Joseph Hickey, went on to write "A Guide to Birdwatching" (1943). Hickey remembered later, "Mayr was our age and invited on all our field trips. The heckling of this German foreigner was tremendous, but he gave tit for tat, and any modern picture of Dr E. Mayr as a very formal person does not square with my memory of the 1930s. He held his own." A group of eight young birdwatchers from The Bronx later became the Bronx County Bird Club, led by Ludlow Griscom. "Everyone should have a problem" was the way one Bronx County Bird Club member recalled Mayr's refrain.
Mayr said of his own involvement with the local birdwatchers: "In those early years in New York when I was a stranger in a big city, it was the companionship and later friendship which I was offered in the Linnean Society that was the most important thing in my life."
Mayr also greatly influenced the American ornithologist Margaret Morse Nice. Mayr encouraged her to correspond with European ornithologists and helped her in her landmark study on song sparrows. Nice wrote to Joseph Grinnell in 1932, trying to get foreign literature reviewed in the "Condor": "Too many American ornithologists have despised the study of the living bird; the magazines and books that deal with the subject abound in careless statements, anthropomorphic interpretations, repetition of ancient errors, and sweeping conclusions from a pitiful array of facts.  ... in Europe the study of the living bird is taken seriously. We could learn a great deal from their writing." Mayr ensured that Nice could publish her two-volume "Studies in the Life History of the Song Sparrow". He found her a publisher, and her book was reviewed by Aldo Leopold, Joseph Grinnell, and Jean Delacour. Nice dedicated her book to "My Friend Ernst Mayr."
Mayr joined the faculty of Harvard University in 1953, where he also served as director of the Museum of Comparative Zoology from 1961 to 1970. He retired in 1975 as emeritus professor of zoology, showered with honors. Following his retirement, he went on to publish more than 200 articles, in a variety of journals—more than some reputable scientists publish in their entire careers; 14 of his 25 books were published after he was 65. Even as a centenarian, he continued to write books. On his 100th birthday, he was interviewed by "Scientific American" magazine. Mayr died on 3 February 2005 in his retirement home in Bedford, Massachusetts after a short illness. His wife, Margarete, died in 1990. He was survived by two daughters, five grandchildren and 10 great-grandchildren.
The awards that Mayr received include the National Medal of Science, the Balzan Prize, the Sarton Medal of the History of Science Society, the International Prize for Biology, the Loye and Alden Miller Research Award, and the Lewis Thomas Prize for Writing about Science. In 1939 he was elected a Corresponding Member of the Royal Australasian Ornithologists Union. He was awarded the 1946 Leidy Award from the Academy of Natural Sciences of Philadelphia. He was awarded the Linnean Society of London's prestigious Darwin-Wallace Medal in 1958 and the Linnaean Society of New York's inaugural Eisenmann Medal in 1983. For his work, "Animal Species and Evolution", he was awarded the Daniel Giraud Elliot Medal from the National Academy of Sciences in 1967. Mayr was elected a Foreign Member of the Royal Society (ForMemRS) in 1988. In 1995 he received the Benjamin Franklin Medal for Distinguished Achievement in the Sciences of the American Philosophical Society.
Mayr never won a Nobel Prize, but he noted that there is no prize for evolutionary biology and that Darwin would not have received one, either. (In fact, there is no Nobel Prize for biology.) Mayr did win a 1999 Crafoord Prize. It honors basic research in fields that do not qualify for Nobel Prizes and is administered by the same organization as the Nobel Prize.
Mayr was co-author of six global reviews of bird species new to science (listed below).
Mayr said he was an atheist towards "the idea of a personal God" because "there is nothing that supports "
Mayr's ideas.
As a traditionally trained biologist, Mayr was often highly critical of early mathematical approaches to evolution such as those of J.B.S. Haldane, famously calling such approaches "beanbag genetics" in 1959. He maintained that factors such as reproductive isolation had to be taken into account. In a similar fashion, Mayr was also quite critical of molecular evolutionary studies such as those of Carl Woese. Current molecular studies in evolution and speciation indicate that although allopatric speciation is the norm, there are numerous cases of sympatric speciation in groups with greater mobility (such as birds). The precise mechanisms of sympatric speciation, however, are usually a form of microallopatry enabled by variations in niche occupancy among individuals within a population.
In many of his writings, Mayr rejected reductionism in evolutionary biology, arguing that evolutionary pressures act on the whole organism, not on single genes, and that genes can have different effects depending on the other genes present. He advocated a study of the whole genome rather than of isolated genes only. After articulating the biological species concept in 1942, Mayr played a central role in the species problem debate over what was the best species concept. He staunchly defended the biological species concept against the many definitions of "species" that others proposed.
Mayr was an outspoken defender of the scientific method, and one known to sharply critique science on the edge. As a notable example, in 1995, he criticized the Search for Extra-Terrestrial Intelligence (SETI) as conducted by fellow Harvard professor Paul Horowitz as being a waste of university and student resources, for its inability to address and answer a scientific question. Over 60 eminent scientists led by Carl Sagan rebutted the criticism.
Mayr rejected the idea of a gene-centered view of evolution and starkly but politely criticized Richard Dawkins' ideas:
Mayr insisted that it is the entire genome, rather than individual genes that should be considered as the target of selection:

</doc>
<doc id="9239" url="https://en.wikipedia.org/wiki?curid=9239" title="Europe">
Europe

Europe is a continent that comprises the westernmost part of Eurasia. Europe is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean Sea to the south. To the east and southeast, Europe is generally considered as separated from Asia by the watershed divides of the Ural and Caucasus Mountains, the Ural River, the Caspian and Black Seas, and the waterways of the Turkish Straits. Yet the borders of Europe—a concept dating back to classical antiquity—are arbitrary, as the primarily physiographic term "continent" also incorporates cultural and political elements.
Europe is the world's second-smallest continent by surface area, covering about or 2% of the Earth's surface and about 6.8% of its land area. Of Europe's approximately 50 countries, Russia is the largest and most populous, spanning 39% of the continent and comprising 15% of its population, while Vatican City is the smallest both in terms area and population. Europe is the third-most populous continent after Asia and Africa, with a population of 739–743 million or about 11% of the world's population. Europe has a climate heavily affected by warm Atlantic currents that temper winters and summers on much of the continent, even at latitudes along which the climate in Asia and North America is severe. Further from the Atlantic, seasonal differences are mildly greater than close to the coast.
Europe, in particular ancient Greece, is the birthplace of Western civilization. The fall of the Western Roman Empire, during the migration period, marked the end of ancient history and the beginning of an era known as the "Middle Ages". The Renaissance humanism, exploration, art, and science led the "old continent", and eventually the rest of the world, to the modern era. From this period onwards, Europe played a predominant role in global affairs. Between the 16th and 20th centuries, European nations controlled at various times the Americas, most of Africa, Oceania, and the majority of Asia.
The Industrial Revolution, which began in the United Kingdom at the end of the 18th century, gave rise to radical economic, cultural, and social change in Western Europe, and eventually the wider world. Both world wars were largely focused upon Europe, contributing to a decline in Western European dominance in world affairs by the mid-20th century as the United States and Soviet Union took prominence. During the Cold War, Europe was divided along the Iron Curtain between NATO in the west and the Warsaw Pact in the east, until the revolutions of 1989 and fall of the Berlin Wall.
European integration led to the formation of the European Union, a political entity that lies between a confederation and a federation. The EU originated in Western Europe but has been expanding eastward since the fall of the Soviet Union in 1991. The currency of most countries of the European Union, the Euro, is the most commonly used among Europeans and the EU's Schengen Area abolishes border and immigration controls among most of its member states.
Definition.
Clickable map of Europe, showing one of the most commonly used continental boundaries Key: blue: states which straddle the border between Europe and Asia;
green: states not geographically in Europe, but closely associated with the continent
The use of the term "Europe" has developed gradually throughout history. In antiquity, the Greek historian Herodotus mentioned that the world had been divided by unknown persons into three parts, Europe, Asia, and Libya (Africa), with the Nile and the River Phasis forming their boundaries—though he also states that some considered the River Don, rather than the Phasis, as the boundary between Europe and Asia. Europe's eastern frontier was defined in the 1st century by geographer Strabo at the River Don. The "Book of Jubilees" described the continents as the lands given by Noah to his three sons; Europe was defined as stretching from the Pillars of Hercules at the Strait of Gibraltar, separating it from North Africa, to the Don, separating it from Asia.
A cultural definition of Europe as the lands of Latin Christendom coalesced in the 8th century, signifying the new cultural condominium created through the confluence of Germanic traditions and Christian-Latin culture, defined partly in contrast with Byzantium and Islam, and limited to northern Iberia, the British Isles, France, Christianised western Germany, the Alpine regions and northern and central Italy. The concept is one of the lasting legacies of the Carolingian Renaissance: "Europa" often figures in the letters of Charlemagne's court scholar, Alcuin. This division—as much cultural as geographical—was used until the Late Middle Ages, when it was challenged by the Age of Discovery. The problem of redefining Europe was finally resolved in 1730 when, instead of waterways, the Swedish geographer and cartographer von Strahlenberg proposed the Ural Mountains as the most significant eastern boundary, a suggestion that found favour in Russia and throughout Europe.
Europe is now generally defined by geographers as the western part of Eurasia, with its boundaries marked by large bodies of water to the north, west and south; Europe's limits to the far east are usually taken to be the Urals, the Ural River, and the Caspian Sea; to the southeast, including the Caucasus Mountains, the Black Sea and the waterways connecting the Black Sea to the Mediterranean Sea.
Islands are generally grouped with the nearest continental landmass, hence Iceland is generally considered to be part of Europe, while the nearby island of Greenland is usually assigned to North America. Nevertheless, there are some exceptions based on sociopolitical and cultural differences. Cyprus is closest to Anatolia (or Asia Minor), but is usually considered part of Europe both culturally and politically and currently is a member state of the EU. Malta was considered an island of North Africa for centuries.
The geographic boundary drawn between Europe and Asia in 1730 follows no international boundaries. As a result, attempts to organise Europe along political or economic lines have resulted in uses of the name in a geopolitically limiting way to refer only to the 28 member states of the European Union. Conversely, Europe has also been used in a very expansive way by the Council of Europe which has 47 member countries, some of which territorially over-reach the Ural and Bosphorus lines to include all of Russia and Turkey. In addition, people in the British Isles may refer to "continental" or "mainland" Europe as Europe.
Etymology.
In classical Greek mythology, Europa was a Phoenician princess whom Zeus abducted after assuming the form of a dazzling white bull. He took her to the island of Crete where she gave birth to Minos, Rhadamanthus, and Sarpedon. For Homer, Europe (, "Eurṓpē"; see also List of Greek place names) was a mythological queen of Crete, not a geographical designation.
The etymology of "Europe" is uncertain. One theory suggests that it is derived from the Greek εὐρύς ("eurus"), meaning "wide, broad" and ὤψ/ὠπ-/ὀπτ- ("ōps"/"ōp"-/"opt-"), meaning "eye, face, countenance", hence "Eurṓpē", "wide-gazing", "broad of aspect" (compare with "glaukōpis" (γλαυκῶπις 'grey-eyed') Athena or "boōpis" (βοὠπις 'ox-eyed') Hera). "Broad" has been an epithet of Earth herself in the reconstructed Proto-Indo-European religion. Another theory suggests that it is based on a Semitic word such as the Akkadian "erebu" meaning "to go down, set" (in reference to the sun), cognate to Phoenician " 'ereb" "evening; west" and Arabic Maghreb, Hebrew "ma'arav" (see also "Erebus", PIE "*h1regʷos", "darkness"). Martin Litchfield West states that "phonologically, the match between Europa's name and any form of the Semitic word is very poor". However, Michael A. Barry, professor in Princeton University's Near Eastern Studies Department, finds the mention of the word "Ereb" on an Assyrian stele with the meaning of "night", "country of sunset", in opposition to "Asu" "country of sunrise", i.e. Asia (Anatolia coming equally from Ἀνατολή, "(sun)rise", "east"). In the "Homeric Hymns" written in the seventh century BC, "Eurôpè" still represents, the western shore of the Aegean Sea.
Whatever the origin of the name of the mythological figure, Εὐρώπη is first used as a geographical term in the 6th century BC, by Greek geographers such as Anaximander and Hecataeus. Anaximander placed the boundary between Asia and Europe along the Phasis River (the modern Rioni) in the Caucasus, a convention still followed by Herodotus in the 5th century BC. But the convention received by the Middle Ages and surviving into modern usage is that of the Roman era used by Roman era authors such as Posidonius, Strabo and Ptolemy,
who took the Tanais (the modern Don River) as the boundary.
The term "Europe" is first used for a cultural sphere in the Carolingian Renaissance of the 9th century. From that time, the term designated the sphere of influence of the Western Church, as opposed to both the Eastern Orthodox churches and to the Islamic world. The modern convention, enlarging the area of "Europe" somewhat to the east and the southeast, develops in the 19th century.
Most major world languages use words derived from "Europa" to refer to the continent. Chinese, for example, uses the word "Ōuzhōu" (歐洲/欧洲); a similar Chinese-derived term is also sometimes used in Japanese such as in the Japanese name of the European Union, , despite the katakana being more commonly used. However, in some Turkic languages the originally Persian name "Frangistan" (land of the Franks) is used casually in referring to much of Europe, besides official names such as "Avrupa" or "Evropa".
History.
Prehistory.
[[File:Stonehenge, Condado de Wiltshire, Inglaterra, 2014-08-12, DD 09.JPG|thumb|Stonehenge monument, from United Kingdom (Late Neolithic from 3000 - 2000 BC).
"Homo erectus georgicus", which lived roughly 1.8 million years ago in Georgia, is the earliest hominid to have been discovered in Europe. Other hominid remains, dating back roughly 1 million years, have been discovered in Atapuerca, Spain. Neanderthal man (named after the Neandertal valley in Germany) appeared in Europe 150,000 years ago and disappeared from the fossil record about 28,000 BC, with this extinction probably due to climate change, and their final refuge being present-day Portugal. The Neanderthals were supplanted by modern humans (Cro-Magnons), who appeared in Europe around 43 to 40 thousand years ago.
The European Neolithic period—marked by the cultivation of crops and the raising of livestock, increased numbers of settlements and the widespread use of pottery—began around 7000 BC in Greece and the Balkans, probably influenced by earlier farming practices in Anatolia and the Near East. It spread from the Balkans along the valleys of the Danube and the Rhine (Linear Pottery culture) and along the Mediterranean coast (Cardial culture). Between 4500 and 3000 BC, these central European neolithic cultures developed further to the west and the north, transmitting newly acquired skills in producing copper artefacts. In Western Europe the Neolithic period was characterised not by large agricultural settlements but by field monuments, such as causewayed enclosures, burial mounds and megalithic tombs. The Corded Ware cultural horizon flourished at the transition from the Neolithic to the Chalcolithic. During this period giant megalithic monuments, such as the Megalithic Temples of Malta and Stonehenge, were constructed throughout Western and Southern Europe.
The European Bronze Age began c. 3200 BC in Greece with the Minoan civilization on Crete, the first advanced civilization in Europe. The Minoans were followed by the Myceneans, who collapsed suddenly around 1200 BC, ushering the European Iron Age. Iron Age colonisation by the Greeks and Phoenicians gave rise to early Mediterranean cities. Early Iron Age Italy and Greece from around the 8th century BC gradually gave rise to historical Classical antiquity, whose beginning is sometimes dated to 776 BC, the year the first Olympic Games.
Classical antiquity.
Ancient Greece was the founding culture of Western civilisation. Western democratic and individualistic culture are often attributed to Ancient Greece. The Greeks city-state, the polis, was the fundamental political unit of classical Greece. In 508 BC, Cleisthenes instituted the world's first democratic system of government in Athens. The Greek political ideals were rediscovered in the late 18th century by European philosophers and idealists. Greece also generated many cultural contributions: in philosophy, humanism and rationalism under Aristotle, Socrates and Plato; in history with Herodotus and Thucydides; in dramatic and narrative verse, starting with the epic poems of Homer; in drama with Sophocles and Euripides, in medicine with Hippocrates and Galen; and in science with Pythagoras, Euclid and Archimedes. In the course of the 5th century BC, several of the Greek city states would ultimately check the Achaemenid Persian advance in Europe through the Greco-Persian Wars, considered a pivotal moment in world history, as the 50 years of peace that followed are known as Golden Age of Athens, the seminal period of ancient Greece that laid many of the foundations of Western civilization.
Greece was followed by Rome, which left its mark on law, politics, language, engineering, architecture, government and many more key aspects in western civilisation. Expanding from their base in Italy beginning in the 3rd century BC, the Romans gradually expanded to eventually rule the entire Mediterranean basin and western Europe by the turn of the millennium. The Roman Republic ended in 27 BC, when Augustus proclaimed the Roman Empire. The two centuries that followed are known as the "pax romana", a period of unprecedented peace, prosperity, and political stability in most of Europe.
The empire continued to expand under emperors such as Hadrian, Antoninus Pius, and Marcus Aurelius, who all spent time on the Empire's northern border fighting Germanic, Pictish and Scottish tribes. The Empire began to decline in the 3rd century, particularly in the west. Christianity was legalised by Constantine I in 313 AD after three centuries of imperial persecution. Constantine also permanently moved the capital of the empire from Rome to the city of Byzantium, which was renamed Constantinople in his honour (modern-day Istanbul) in 330 AD. Christianity became the sole official religion of the empire in 380 AD, and in 391-392 AD, the emperor Theodosius outlawed pagan religions. This is sometimes considered to mark the end of antiquity; alternatively antiquity is considered to end with the fall of the Western Roman Empire in 476 AD; the closure of the pagan Platonic Academy of Athens in 529 AD; or the rise of Islam in the early 7th century AD.
Early Middle Ages.
During the decline of the Roman Empire, Europe entered a long period of change arising from what historians call the "Age of Migrations". There were numerous invasions and migrations amongst the Ostrogoths, Visigoths, Goths, Vandals, Huns, Franks, Angles, Saxons, Slavs, Avars, Bulgars and, later on, the Vikings, Pechenegs, Cumans and Magyars. Renaissance thinkers such as Petrarch would later refer to this as the "Dark Ages". Isolated monastic communities were the only places to safeguard and compile written knowledge accumulated previously; apart from this very few written records survive and much literature, philosophy, mathematics, and other thinking from the classical period disappeared from Western Europe though they were preserved in the east, in the Byzantine Empire.
While the Roman empire in the west continued to decline, Roman traditions and the Roman state remained strong in the predominantly Greek-speaking Eastern Roman Empire, also known as the Byzantine Empire. During most of its existence, the Byzantine Empire was the most powerful economic, cultural, and military force in Europe. Emperor Justinian I presided over Constantinople's first golden age: he established a legal code that forms the basis of many modern legal systems, funded the construction of the Hagia Sophia, and brought the Christian church under state control.
From the 7th century onwards, Muslim Arabs started to encroach on historically Roman territory. As the Byzantines and neighbouring Sasanid Persians were severely weakened by the time due the protracted, centuries-lasting and frequent Byzantine–Sasanian wars, the Muslims entirely toppled the Sasanids, and made inroads into Byzantine Asia Minor. In the mid 7th century AD, following the Muslim conquest of Persia, Islam penetrated into the Caucasus region. Over the next centuries Muslim forces took Cyprus, Malta, Crete, Sicily and parts of southern Italy. In the East, Volga Bulgaria became an Islamic state in the 10th century. Between 711 and 720, most of the Iberian Peninsula was brought under Muslim rule — save for small areas in the northwest (Asturias) and largely Basque regions in the Pyrenees. This territory, under the Arabic name Al-Andalus, became part of the expanding Umayyad Caliphate. The unsuccessful second siege of Constantinople (717) weakened the Umayyad dynasty and reduced their prestige. The Umayyads were then defeated by the Frankish leader Charles Martel at the Battle of Poitiers in 732, which ended their northward advance.
During the Dark Ages, the Western Roman Empire fell under the control of various tribes. The Germanic and Slav tribes established their domains over Western and Eastern Europe respectively. Eventually the Frankish tribes were united under Clovis I. Charlemagne, a Frankish king of the Carolingian dynasty who had conquered most of Western Europe, was anointed "Holy Roman Emperor" by the Pope in 800. This led in 962 to the founding of the Holy Roman Empire, which eventually became centred in the German principalities of central Europe.
East Central Europe saw the creation of the first Slavic states and the adoption of Christianity (circa 1000 AD). The powerful West Slavic state of Great Moravia spread its territory all the way south to the Balkans, reaching its largest territorial extent under Svatopluk I and causing a series of armed conflicts with East Francia. Further south, the first South Slavic states emerged in the late 7th and 8th century and adopted Christianity: the First Bulgarian Empire, the Serbian Principality (later Kingdom and Empire), and the Duchy of Croatia (later Kingdom of Croatia). To the East, the Kievan Rus expanded from its capital in Kiev to become the largest state in Europe by the 10th century. In 988, Vladimir the Great adopted Orthodox Christianity as the religion of state.
High and Late Middle Ages.
The period between the year 1000 and 1300 is known as the High Middle Ages, during which the population of Europe experienced significant growth, culminating in the Renaissance of the 12th century. Economic growth, together with the lack of safety on the mainland trading routes, made possible the development of major commercial routes along the coast of the Mediterranean and Baltic Seas. The growing wealth and independence acquired by some coastal cities gave the Maritime Republics a leading role in the European scene.
The Middle Ages on the mainland were dominated by the two upper echelons of the social structure: the nobility and the clergy. Feudalism developed in France in the Early Middle Ages and soon spread throughout Europe. A struggle for influence between the nobility and the monarchy in England led to the writing of the Magna Carta and the establishment of a parliament. The primary source of culture in this period came from the Roman Catholic Church. Through monasteries and cathedral schools, the Church was responsible for education in much of Europe.
The Papacy reached the height of its power during the High Middle Ages. An East-West Schism in 1054 split the former Roman Empire religiously, with the Eastern Orthodox Church in the Byzantine Empire and the Roman Catholic Church in the former Western Roman Empire. In 1095 Pope Urban II called for a crusade against Muslims occupying Jerusalem and the Holy Land. In Europe itself, the Church organised the Inquisition against heretics. In Spain, the Reconquista concluded with the fall of Granada in 1492, ending over seven centuries of Islamic rule in the Iberian Peninsula.
In the east a resurgent Byzantine Empire recaptured Crete and Cyprus from the Muslims and reconquered the Balkans. Constantinople was the largest and wealthiest city in Europe from the 9th to the 12th centuries, with a population of approximately 400,000. The Empire was weakened following the defeat at Manzikert and was weakened considerably by the sack of Constantinople in 1204, during the Fourth Crusade. Although it would recover Constantinople in 1261, Byzantium fell in 1453 when Constantinople was taken by the Ottoman Empire.
In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Pechenegs and the Cuman-Kipchaks, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north and temporarily halted the expansion of the Rus' state to the south and east. Like many other parts of Eurasia, these territories were overrun by the Mongols. The invaders, who became known as Tatars, were mostly Turkic-speaking peoples under Mongol suzerainty. They established the state of the Golden Horde with headquarters in Crimea, which later adopted Islam as a religion and ruled over modern-day southern and central Russia for more than three centuries. After the collapse of Mongol dominions, the first Romanian states (principalities) emerged in the 14th century: Moldova and Walachia. Previously, these territories were under the successive control of Pechenegs and Cumans. From the 12th to the 15th centuries, the Grand Duchy of Moscow grew from a small principality under Mongol rule to the largest state in Europe, overthrowing the Mongols in 1480 and eventually becoming the Tsardom of Russia. The state was consolidated under Ivan III the Great and Ivan the Terrible, steadily expanding to the east and south over the next centuries.
The Great Famine of 1315–1317 was the first crisis that would strike Europe in the late Middle Ages. The period between 1348 and 1420 witnessed the heaviest loss. The population of France was reduced by half. Medieval Britain was afflicted by 95 famines, and France suffered the effects of 75 or more in the same period. Europe was devastated in the mid-14th century by the Black Death, one of the most deadly pandemics in human history which killed an estimated 25 million people in Europe alone—a third of the European population at the time.
The plague had a devastating effect on Europe's social structure; it induced people to live for the moment as illustrated by Giovanni Boccaccio in "The Decameron" (1353). It was a serious blow to the Roman Catholic Church and led to increased persecution of Jews, foreigners, beggars and lepers. The plague is thought to have returned every generation with varying virulence and mortalities until the 18th century. During this period, more than 100 plague epidemics swept across Europe.
Early modern period.
The Renaissance was a period of cultural change originating in Florence and later spreading to the rest of Europe. The rise of a new humanism was accompanied by the recovery of forgotten classical Greek and Arabic knowledge from monastic libraries, often translated from Arabic into Latin. The Renaissance spread across Europe between the 14th and 16th centuries: it saw the flowering of art, philosophy, music, and the sciences, under the joint patronage of royalty, the nobility, the Roman Catholic Church, and an emerging merchant class. Patrons in Italy, including the Medici family of Florentine bankers and the Popes in Rome, funded prolific quattrocento and cinquecento artists such as Raphael, Michelangelo, and Leonardo da Vinci.
Political intrigue within the Church in the mid-14th century caused the Western Schism. During this forty-year period, two popes—one in Avignon and one in Rome—claimed rulership over the Church. Although the schism was eventually healed in 1417, the papacy's spiritual authority had suffered greatly.
The Church's power was further weakened by the Protestant Reformation (1517–1648), initially sparked by the works of German theologian Martin Luther, an attempt to start a reform within the Church. The Reformation also damaged the Holy Roman Emperor's influence, as German princes became divided between Protestant and Roman Catholic faiths. This eventually led to the Thirty Years War (1618–1648), which crippled the Holy Roman Empire and devastated much of Germany, killing between 25 and 40 percent of its population. In the aftermath of the Peace of Westphalia, France rose to predominance within Europe.
The 17th century in southern, central and eastern Europe was a period of general decline. Central and Eastern Europe experienced more than 150 famines in a 200-year period between 1501 and 1700. From the 15th to 18th centuries, when the disintegrating khanates of the Golden Horde were conquered by Russia, Tatars from the Crimean Khanate frequently raided Eastern Slavic lands to capture slaves. Further east, the Nogai Horde and Kazakh Khanate frequently raided the Slavic-speaking areas of Russia, Ukraine and Poland for hundreds of years, until the Russian expansion and conquest of most of northern Eurasia (i.e. Eastern Europe, Central Asia and Siberia). Meanwhile, in the south, the Ottomans had conquered the Balkans by the 15th century, laying siege to Vienna in 1529. In the Battle of Lepanto in 1571, the Holy League checked Ottoman power in the Mediterranean. The Ottomans again laid siege to Vienna in 1683, but the Battle of Vienna permanently ended their advance into Europe, and marked the political hegemony of the Habsburg dynasty in central Europe.
The Renaissance and the New Monarchs marked the start of an Age of Discovery, a period of exploration, invention, and scientific development. Among the great figures of the Western scientific revolution of the 16th and 17th centuries were Copernicus, Kepler, Galileo, and Isaac Newton. According to Peter Barrett, "It is widely accepted that 'modern science' arose in the Europe of the 17th century (towards the end of the Renaissance), introducing a new understanding of the natural world." In the 15th century, Portugal and Spain, two of the greatest naval powers of the time, took the lead in exploring the world. Christopher Columbus reached the New World in 1492 and Vasco da Gama opened the ocean route to the East in 1498, and soon after the Spanish and Portuguese began establishing colonial empires in the Americas and Asia. France, the Netherlands and England soon followed in building large colonial empires with vast holdings in Africa, the Americas, and Asia.
18th and 19th centuries.
The Age of Enlightenment was a powerful intellectual movement during the 18th century promoting scientific and reason-based thoughts. Discontent with the aristocracy and clergy's monopoly on political power in France resulted in the French Revolution and the establishment of the First Republic as a result of which the monarchy and many of the nobility perished during the initial reign of terror. Napoleon Bonaparte rose to power in the aftermath of the French Revolution and established the First French Empire that, during the Napoleonic Wars, grew to encompass large parts of Europe before collapsing in 1815 with the Battle of Waterloo. Napoleonic rule resulted in the further dissemination of the ideals of the French Revolution, including that of the nation-state, as well as the widespread adoption of the French models of administration, law, and education. The Congress of Vienna, convened after Napoleon's downfall, established a new balance of power in Europe centred on the five "Great Powers": the UK, France, Prussia, Austria, and Russia. This balance would remain in place until the Revolutions of 1848, during which liberal uprisings affected all of Europe except for Russia and the UK. These revolutions were eventually put down by conservative elements and few reforms resulted. The year 1859 saw the unification of Romania, as a nation-state, from smaller principalities. In 1867, the Austro-Hungarian empire was formed; and 1871 saw the unifications of both Italy and Germany as nation-states from smaller principalities.
In parallel, the Eastern Question grew more complex ever since the Ottoman defeat in the Russo-Turkish War (1768–1774). As the dissolution of the Ottoman Empire seemed imminent, the Great Powers struggled to safeguard their strategic and commercial interests in the Ottoman domains. The Russian Empire stood to benefit from the decline, whereas the Habsburg Empire and Britain perceived the preservation of the Ottoman Empire to be in their best interests. Meanwhile, the Serbian revolution (1804) and Greek War of Independence (1821) marked the beginning of the end of Ottoman rule in the Balkans, which ended with the Balkan Wars in 1912-1913. Formal recognition of the "de facto" independent principalities of Montenegro, Serbia and Romania ensued at the Congress of Berlin in 1878.
The Industrial Revolution started in Great Britain in the last part of the 18th century and spread throughout Europe. The invention and implementation of new technologies resulted in rapid urban growth, mass employment, and the rise of a new working class. Reforms in social and economic spheres followed, including the first laws on child labour, the legalisation of trade unions, and the abolition of slavery. In Britain, the Public Health Act of 1875 was passed, which significantly improved living conditions in many British cities. Europe's population increased from about 100 million in 1700 to 400 million by 1900. The last major famine recorded in Western Europe, the Irish Potato Famine, caused death and mass emigration of millions of Irish people. In the 19th century, 70 million people left Europe in migrations to various European colonies abroad and to the United States. Demographic growth meant that, by 1900, Europe's share of the world's population was 25%.
20th century to the present.
Two World Wars and an economic depression dominated the first half of the 20th century. World War I was fought between 1914 and 1918. It started when Archduke Franz Ferdinand of Austria was assassinated by the Yugoslav nationalist Gavrilo Princip. Most European nations were drawn into the war, which was fought between the Entente Powers (France, Belgium, Serbia, Portugal, Russia, the United Kingdom, and later Italy, Greece, Romania, and the United States) and the Central Powers (Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire). The War left more than 16 million civilians and military dead. Over 60 million European soldiers were mobilised from 1914 to 1918.
Russia was plunged into the Russian Revolution, which threw down the Tsarist monarchy and replaced it with the communist Soviet Union. Austria-Hungary and the Ottoman Empire collapsed and broke up into separate nations, and many other nations had their borders redrawn. The Treaty of Versailles, which officially ended World War I in 1919, was harsh towards Germany, upon whom it placed full responsibility for the war and imposed heavy sanctions.
Excess deaths in Russia over the course of World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million. In 1932–1933, under Stalin's leadership, confiscations of grain by the Soviet authorities contributed to the second Soviet famine which caused millions of deaths; surviving kulaks were persecuted and many sent to Gulags to do forced labour. Stalin was also responsible for the Great Purge of 1937–38 in which the NKVD executed 681,692 people; millions of people were deported and exiled to remote areas of the Soviet Union.
Economic instability, caused in part by debts incurred in the First World War and 'loans' to Germany played havoc in Europe in the late 1920s and 1930s. This and the Wall Street Crash of 1929 brought about the worldwide Great Depression. Helped by the economic crisis, social instability and the threat of communism, fascist movements developed throughout Europe placing Adolf Hitler of Nazi Germany, Francisco Franco of Spain and Benito Mussolini of Italy in power.
In 1933, Hitler became the leader of Germany and began to work towards his goal of building Greater Germany. Germany re-expanded and took back the Saarland and Rhineland in 1935 and 1936. In 1938, Austria became a part of Germany following the Anschluss. Later that year, following the Munich Agreement signed by Germany, France, the United Kingdom and Italy, Germany annexed the Sudetenland, which was a part of Czechoslovakia inhabited by ethnic Germans, and in early 1939, the remainder of Czechoslovakia was split into the Protectorate of Bohemia and Moravia, controlled by Germany, and the Slovak Republic. At the time, Britain and France preferred a policy of appeasement.
With tensions mounting between Germany and Poland over the future of Danzig, the Germans turned to the Soviets, and signed the Molotov–Ribbentrop Pact, which allowed the Soviets to invade the Baltic states and parts of Poland and Romania. Germany invaded Poland on 1 September 1939, prompting France and the United Kingdom to declare war on Germany on 3 September, opening the European Theatre of World War II. The Soviet invasion of Poland started on 17 September and Poland fell soon thereafter. On 24 September, the Soviet Union attacked the Baltic countries and later, Finland. The British hoped to land at Narvik and send troops to aid Finland, but their primary objective in the landing was to encircle Germany and cut the Germans off from Scandinavian resources. Around the same time, Germany moved troops into Denmark. The Phoney War continued.
In May 1940, Germany attacked France through the Low Countries. France capitulated in June 1940. By August Germany began a bombing offensive on Britain, but failed to convince the Britons to give up. In 1941, Germany invaded the Soviet Union in the Operation Barbarossa. On 7 December 1941 Japan's attack on Pearl Harbor drew the United States into the conflict as allies of the British Empire and other allied forces.
After the staggering Battle of Stalingrad in 1943, the German offensive in the Soviet Union turned into a continual fallback. The Battle of Kursk, which involved the largest tank battle in history, was the last major German offensive on the Eastern Front. In 1944, British and American forces invaded France in the D-Day landings, opening a new front against Germany. Berlin finally fell in 1945, ending World War II in Europe. The war was the largest and most destructive in human history, with 60 million dead across the world. More than 40 million people in Europe had died as a result of World War II, including between 11 and 17 million people who perished during the Holocaust. The Soviet Union lost around 27 million people (mostly civilians) during the war, about half of all World War II casualties. By the end of World War II, Europe had more than 40 million refugees. Several post-war expulsions in Central and Eastern Europe displaced a total of about 20 million people.
World War I and especially World War II diminished the eminence of Western Europe in world affairs. After World War II the map of Europe was redrawn at the Yalta Conference and divided into two blocs, the Western countries and the communist Eastern bloc, separated by what was later called by Winston Churchill an "Iron Curtain". The United States and Western Europe
established the NATO alliance and later the Soviet Union and Central Europe established the Warsaw Pact.
The two new superpowers, the United States and the Soviet Union, became locked in a fifty-year-long Cold War, centred on nuclear proliferation. At the same time decolonisation, which had already started after World War I, gradually resulted in the independence of most of the European colonies in Asia and Africa.
In the 1980s the reforms of Mikhail Gorbachev and the Solidarity movement in Poland accelerated the collapse of the Eastern bloc and the end of the Cold War. Germany was reunited, after the symbolic fall of the Berlin Wall in 1989, and the maps of Central and Eastern Europe were redrawn once more.
European integration also grew after World War II. The Treaty of Rome in 1957 established the European Economic Community between six Western European states with the goal of a unified economic policy and common market. In 1967 the EEC, European Coal and Steel Community and Euratom formed the European Community, which in 1993 became the European Union. The EU established a parliament, court and central bank and introduced the euro as a unified currency. In 2004 and 2007, more Central and Eastern European countries began joining, expanding the EU to its current size of 28 European countries, and once more making Europe a major economical and political centre of power.
Geography.
Europe makes up the western fifth of the Eurasian landmass. It has a higher ratio of coast to landmass than any other continent or subcontinent. Its maritime borders consist of the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean, Black, and Caspian Seas to the south.
Land relief in Europe shows great variation within relatively small areas. The southern regions are more mountainous, while moving north the terrain descends from the high Alps, Pyrenees, and Carpathians, through hilly uplands, into broad, low northern plains, which are vast in the east. This extended lowland is known as the Great European Plain, and at its heart lies the North German Plain. An arc of uplands also exists along the north-western seaboard, which begins in the western parts of the islands of Britain and Ireland, and then continues along the mountainous, fjord-cut spine of Norway.
This description is simplified. Sub-regions such as the Iberian Peninsula and the Italian Peninsula contain their own complex features, as does mainland Central Europe itself, where the relief contains many plateaus, river valleys and basins that complicate the general trend. Sub-regions like Iceland, Britain, and Ireland are special cases. The former is a land unto itself in the northern ocean which is counted as part of Europe, while the latter are upland areas that were once joined to the mainland until rising sea levels cut them off.
Climate.
[[File:Vegetation Europe.png|thumb|260px|Biomes of Europe and surrounding regions:
Europe lies mainly in the temperate climate zones, being subjected to prevailing westerlies. The climate is milder in comparison to other areas of the same latitude around the globe due to the influence of the Gulf Stream. The Gulf Stream is nicknamed "Europe's central heating", because it makes Europe's climate warmer and wetter than it would otherwise be. The Gulf Stream not only carries warm water to Europe's coast but also warms up the prevailing westerly winds that blow across the continent from the Atlantic Ocean.
Therefore, the average temperature throughout the year of Naples is 16 °C (60.8 °F), while it is only 12 °C (53.6 °F) in New York City which is almost on the same latitude. Berlin, Germany; Calgary, Canada; and Irkutsk, in the Asian part of Russia, lie on around the same latitude; January temperatures in Berlin average around 8 °C (15 °F) higher than those in Calgary, and they are almost 22 °C (40 °F) higher than average temperatures in Irkutsk. Similarly, northern parts of Scotland have a tempertate marine climate. The yearly average temperature in city of Inverness is 9.05 degrees Celsius (48.3 degrees Fahrenheit). However, Churchill, Manitoba, Canada, is on roughly the same latitude and has an average temperature of -6.5 degrees Celsius (20.3 degrees Fahrenheit), giving it a nearly subarctic climate.
Geology.
The geological history of Europe traces back to the formation of the Baltic Shield (Fennoscandia) and the Sarmatian craton, both around 2.25 billion years ago, followed by the Volgo–Uralia shield, the three together leading to the East European craton (≈ Baltica) which became a part of the supercontinent Columbia. Around 1.1 billion years ago, Baltica and Arctica (as part of the Laurentia block) became joined to Rodinia, later resplitting around 550 million years ago to reform as Baltica. Around 440 million years ago Euramerica was formed from Baltica and Laurentia; a further joining with Gondwana then leading to the formation of Pangea. Around 190 million years ago, Gondwana and Laurasia split apart due to the widening of the Atlantic Ocean. Finally, and very soon afterwards, Laurasia itself split up again, into Laurentia (North America) and the Eurasian continent. The land connection between the two persisted for a considerable time, via Greenland, leading to interchange of animal species. From around 50 million years ago, rising and falling sea levels have determined the actual shape of Europe, and its connections with continents such as Asia. Europe's present shape dates to the late Tertiary period about five million years ago.
The geology of Europe is hugely varied and complex, and gives rise to the wide variety of landscapes found across the continent, from the Scottish Highlands to the rolling plains of Hungary. Europe's most significant feature is the dichotomy between highland and mountainous Southern Europe and a vast, partially underwater, northern plain ranging from Ireland in the west to the Ural Mountains in the east. These two halves are separated by the mountain chains of the Pyrenees and Alps/Carpathians. The northern plains are delimited in the west by the Scandinavian Mountains and the mountainous parts of the British Isles. Major shallow water bodies submerging parts of the northern plains are the Celtic Sea, the North Sea, the Baltic Sea complex and Barents Sea.
The northern plain contains the old geological continent of Baltica, and so may be regarded geologically as the "main continent", while peripheral highlands and mountainous regions in the south and west constitute fragments from various other geological continents. Most of the older geology of western Europe existed as part of the ancient microcontinent Avalonia.
Flora.
Having lived side-by-side with agricultural peoples for millennia, Europe's animals and plants have been profoundly affected by the presence and activities of man. With the exception of Fennoscandia and northern Russia, few areas of untouched wilderness are currently found in Europe, except for various national parks.
The main natural vegetation cover in Europe is mixed forest. The conditions for growth are very favourable. In the north, the Gulf Stream and North Atlantic Drift warm the continent. Southern Europe could be described as having a warm, but mild climate. There are frequent summer droughts in this region. Mountain ridges also affect the conditions. Some of these (Alps, Pyrenees) are oriented east-west and allow the wind to carry large masses of water from the ocean in the interior. Others are oriented south-north (Scandinavian Mountains, Dinarides, Carpathians, Apennines) and because the rain falls primarily on the side of mountains that is oriented towards the sea, forests grow well on this side, while on the other side, the conditions are much less favourable. Few corners of mainland Europe have not been grazed by livestock at some point in time, and the cutting down of the pre-agricultural forest habitat caused disruption to the original plant and animal ecosystems.
Probably 80 to 90 percent of Europe was once covered by forest. It stretched from the Mediterranean Sea to the Arctic Ocean. Though over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadlef and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European forest dwelling species which require a mixture of tree species and diverse forest structure. The amount of natural forest in Western Europe is just 2–3% or less, in European Russia 5–10%. The country with the smallest percentage of forested area is Iceland (1%), while the most forested country is Finland (77%).
In temperate Europe, mixed forest with both broadleaf and coniferous trees dominate. The most important species in central and western Europe are beech and oak. In the north, the taiga is a mixed spruce–pine–birch forest; further north within Russia and extreme northern Scandinavia, the taiga gives way to tundra as the Arctic is approached. In the Mediterranean, many olive trees have been planted, which are very well adapted to its arid climate; Mediterranean Cypress is also widely planted in southern Europe. The semi-arid Mediterranean region hosts much scrub forest. A narrow east-west tongue of Eurasian grassland (the steppe) extends eastwards from Ukraine and southern Russia and ends in Hungary and traverses into taiga to the north.
Fauna.
Glaciation during the most recent ice age and the presence of man affected the distribution of European fauna. As for the animals, in many parts of Europe most large animals and top predator species have been hunted to extinction. The woolly mammoth was extinct before the end of the Neolithic period. Today wolves (carnivores) and bears (omnivores) are endangered. Once they were found in most parts of Europe. However, deforestation and hunting caused these animals to withdraw further and further. By the Middle Ages the bears' habitats were limited to more or less inaccessible mountains with sufficient forest cover. Today, the brown bear lives primarily in the Balkan peninsula, Scandinavia, and Russia; a small number also persist in other countries across Europe (Austria, Pyrenees etc.), but in these areas brown bear populations are fragmented and marginalised because of the destruction of their habitat. In addition, polar bears may be found on Svalbard, a Norwegian archipelago far north of Scandinavia. The wolf, the second largest predator in Europe after the brown bear, can be found primarily in Central and Eastern Europe and in the Balkans, with a handful of packs in pockets of Western Europe (Scandinavia, Spain, etc.).
European wild cat, foxes (especially the red fox), jackal and different species of martens, hedgehogs, different species of reptiles (like snakes such as vipers and grass snakes) and amphibians, different birds (owls, hawks and other birds of prey).
Important European herbivores are snails, larvae, fish, different birds, and mammals, like rodents, deer and roe deer, boars, and living in the mountains, marmots, steinbocks, chamois among others. A number of insects, such as the small tortoiseshell butterfly, add to the biodiversity.
The extinction of the dwarf hippos and dwarf elephants has been linked to the earliest arrival of humans on the islands of the Mediterranean.
Sea creatures are also an important part of European flora and fauna. The sea flora is mainly phytoplankton. Important animals that live in European seas are zooplankton, molluscs, echinoderms, different crustaceans, squids and octopuses, fish, dolphins, and whales.
Biodiversity is protected in Europe through the Council of Europe's Bern Convention, which has also been signed by the European Community as well as non-European states.
Politics.
The list below includes all entities falling even partially under any of the various common definitions of Europe, geographic or political. The data displayed are per sources in cross-referenced articles.
Within the above-mentioned states are several de facto independent countries with limited to no international recognition. None of them are members of the UN:
Several dependencies and similar territories with broad autonomy are also found in Europe. Note that the list does not include the constituent countries of the United Kingdom, federal states of Germany and Austria, and autonomous territories of Spain and the post-Soviet republics as well as the republic of Serbia.
Integration.
European integration is the process of political, legal, economic (and in some cases social and cultural) integration of states wholly or partially in Europe. While the Council of Europe—which includes almost all European states—has promoted pan-Europe cooperation, the European Union has been the focus of economic integration on the continent. More recently, the Eurasian Economic Union has been established as a counterpart comprising former Soviet states.
28 European states are members of the politico-economic European Union, 26 of the border-free Schengen Area and 19 of the monetary union Eurozone. Among the smaller European organizations are the Nordic Council, the Benelux, the Baltic Assembly and the Visegrád Group.
Economy.
As a continent, the economy of Europe is currently the largest on Earth and it is the richest region as measured by assets under management with over $32.7 trillion compared to North America's $27.1 trillion in 2008. In 2009 Europe remained the wealthiest region. Its $37.1 trillion in assets under management represented one-third of the world's wealth. It was one of several regions where wealth surpassed its precrisis year-end peak. As with other continents, Europe has a large variation of wealth among its countries. The richer states tend to be in the West; some of the Central and Eastern European economies are still emerging from the collapse of the Soviet Union and Yugoslavia.
The European Union, a political entity composed of 28 European states, comprises the largest single economic area in the world. 18 EU countries share the euro as a common currency.
Five European countries rank in the top ten of the world's largest national economies in GDP (PPP). This includes (ranks according to the CIA): Germany (5), the UK (6), Russia (7), France (8), and Italy (10).
There is huge disparity between many European countries in terms of their income. The richest in terms of GDP per capita is Monaco with its US$172,676 per capita (2009) and the poorest is Moldova with its GDP per capita of US$1,631 (2010). Monaco is the richest country in terms of GDP per capita in the world according to the World Bank report.
History.
Capitalism has been dominant in the Western world since the end of feudalism. From Britain, it gradually spread throughout Europe. The Industrial Revolution started in Europe, specifically the United Kingdom in the late 18th century, and the 19th century saw Western Europe industrialise. Economies were disrupted by World War I but by the beginning of World War II they had recovered and were having to compete with the growing economic strength of the United States. World War II, again, damaged much of Europe's industries.
After World War II the economy of the UK was in a state of ruin, and continued to suffer relative economic decline in the following decades. Italy was also in a poor economic condition but regained a high level of growth by the 1950s. West Germany recovered quickly and had doubled production from pre-war levels by the 1950s. France also staged a remarkable comeback enjoying rapid growth and modernisation; later on Spain, under the leadership of Franco, also recovered, and the nation recorded huge unprecedented economic growth beginning in the 1960s in what is called the Spanish miracle. The majority of Central and Eastern European states came under the control of the Soviet Union and thus were members of the Council for Mutual Economic Assistance (COMECON).
The states which retained a free-market system were given a large amount of aid by the United States under the Marshall Plan.
With the fall of communism in Central and Eastern Europe in 1991, the post-socialist states began free market reforms: Poland, Hungary, and Slovenia adopted them reasonably quickly, while Ukraine and Russia are still in the process of doing so.
After East and West Germany were reunited in 1990, the economy of West Germany struggled as it had to support and largely rebuild the infrastructure of East Germany.
By the millennium change, the EU dominated the economy of Europe comprising the five largest European economies of the time namely Germany, the United Kingdom, France, Italy, and Spain. In 1999, 12 of the 15 members of the EU joined the Eurozone replacing their former national currencies by the common euro. The three who chose to remain outside the Eurozone were: the United Kingdom, Denmark, and Sweden.
The European Union is now the largest economy in the world.
Figures released by Eurostat in 2009 confirmed that the Eurozone had gone into recession in 2008. It impacted much of the region. In 2010, fears of a sovereign debt crisis developed concerning some countries in Europe, especially Greece, Ireland, Spain, and Portugal. As a result, measures were taken, especially for Greece, by the leading countries of the Eurozone. The EU-27 unemployment rate was 10.3% in 2012. For those aged 15–24 it was 22.4%.
Demographics.
Since the Renaissance, Europe has had a major influence in culture, economics and social movements in the world. The most significant inventions had their origins in the Western world, primarily Europe and the United States. Approximately 70 million Europeans died through war, violence and famine between 1914 and 1945. Some current and past issues in European demographics have included religious emigration, race relations, economic immigration, a declining birth rate and an ageing population.
In some countries, such as Ireland and Poland, access to abortion is limited. It remains illegal on the island of Malta. Furthermore, three European countries (the Netherlands, Belgium, and Switzerland) and the Autonomous Community of Andalusia (Spain) have allowed a limited form of voluntary euthanasia for some terminally ill people.
In 2005, the population of Europe was estimated to be 731 million according to the United Nations, which is slightly more than one-ninth of the world's population. A century ago, Europe had nearly a quarter of the world's population. The population of Europe has grown in the past century, but in other areas of the world (in particular Africa and Asia) the population has grown far more quickly. Among the continents, Europe has a relatively high population density, second only to Asia. The most densely populated country in Europe (and in the world) is Monaco. Pan and Pfeil (2004) count 87 distinct "peoples of Europe", of which 33 form the majority population in at least one sovereign state, while the remaining 54 constitute ethnic minorities.
According to UN population projection, Europe's population may fall to about 7% of world population by 2050, or 653 million people (medium variant, 556 to 777 million in low and high variants, respectively). Within this context, significant disparities exist between regions in relation to fertility rates. The average number of children per female of child bearing age is 1.52. According to some sources, this rate is higher among Muslims in Europe. The UN predicts a steady population decline in Central and Eastern Europe as a result of emigration and low birth rates.
Europe is home to the highest number of migrants of all global regions at 70.6 million people, the IOM's report said. In 2005, the EU had an overall net gain from immigration of 1.8 million people. This accounted for almost 85% of Europe's total population growth. The European Union plans to open the job centres for legal migrant workers from Africa. In 2008, 696,000 persons were given citizenship of an EU27 member state, a decrease from 707,000 the previous year.
Emigration from Europe began with Spanish and Portuguese settlers in the 16th century, and French and English settlers in the 17th century. But numbers remained relatively small until waves of mass emigration in the 19th century, when millions of poor families left Europe.
Today, large populations of European descent are found on every continent. European ancestry predominates in North America, and to a lesser degree in South America (particularly in Uruguay, Argentina, Chile and Brazil, while most of the other Latin American countries also have a considerable population of European origins). Australia and New Zealand have large European derived populations. Africa has no countries with European-derived majorities (or with the exception of Cape Verde and probably São Tomé and Príncipe, depending on context), but there are significant minorities, such as the White South Africans. In Asia, European-derived populations predominate in Northern Asia (specifically Russians), some parts of Northern Kazakhstan and Israel.
Languages.
European languages mostly fall within three Indo-European language groups: the Romance languages, derived from the Latin of the Roman Empire; the Germanic languages, whose ancestor language came from southern Scandinavia; and the Slavic languages.
Slavic languages are most spoken by the number of native speakers in Europe, they are spoken in Central, Eastern, and Southeastern Europe. Romance languages are spoken primarily in south-western Europe as well as in Romania and Moldova, in Central or Eastern Europe. Germanic languages are spoken in Northern Europe, the British Isles and some parts of Central Europe.
Many other languages outside the three main groups exist in Europe. Other Indo-European languages include the Baltic group (that is, Latvian and Lithuanian), the Celtic group (that is, Irish, Scottish Gaelic, Manx, Welsh, Cornish, and Breton), Greek, Armenian, and Albanian. In addition, a distinct group of Uralic languages (Estonian, Finnish, and Hungarian) is spoken mainly in Estonia, Finland, and Hungary, while Kartvelian languages (Georgian, Mingrelian, and Svan), are spoken primarily in Georgia, and two other language families reside in the North Caucasus (termed Northeast Caucasian, most notably including Chechen, Avar and Lezgin and Northwest Caucasian, notably including Adyghe). Maltese is the only Semitic language that is official within the EU, while Basque is the only European language isolate. Turkic languages include Azerbaijani and Turkish, in addition to the languages of minority nations in Russia.
Multilingualism and the protection of regional and minority languages are recognised political goals in Europe today. The Council of Europe Framework Convention for the Protection of National Minorities and the Council of Europe's European Charter for Regional or Minority Languages set up a legal framework for language rights in Europe.
Religion.
Historically, religion in Europe has been a major influence on European art, culture, philosophy and law. The largest religion in Europe is Christianity, with 76.2% of Europeans considering themselves Christians, including Catholic, Eastern Orthodox and various Protestant denominations (especially historically state-supported European ones such as Lutheranism, Anglicanism and the Reformed faith). The notion of "Europe" and the "Western World" has been intimately connected with the concept of "Christianity and Christendom" many even attribute Christianity for being the link that created a unified European identity.
The second most popular religion is Islam (6%) concentrated mainly in the Balkans and eastern Europe (Bosnia and Herzegovina, Albania, Kosovo, Kazakhstan, North Cyprus, Turkey, Azerbaijan, North Caucasus, and the Volga-Ural region). Other religions, including Judaism, Hinduism, and Buddhism are minority religions (though Tibetan Buddhism is the majority religion of Russia's Republic of Kalmykia). The 20th century saw the revival of Neopaganism through movements such as Wicca and Druidry.
Europe has become a relatively secular continent, with an increasing number and proportion of irreligious, atheist and agnostic people which make up about 18.2% of Europeans population, actually the largest secular in the Western world. There are a particularly high number of self-described non-religious people in the Czech Republic, Estonia, Sweden, former East Germany, and France.
Culture.
The culture of Europe can be described as a series of overlapping cultures; cultural mixes exist across the continent. Scholar Andreas Kaplan describes Europe as "embracing maximum cultural diversity at minimal geographical distances". There are cultural innovations and movements, sometimes at odds with each other. Thus, the question of "common culture" or "common values" is complex.
According to historian Hilaire Belloc, for several centuries the peoples of Europe based their self-identification on the remaining traces of the Roman culture and on the concept of Christendom, because many European-wide military alliances were of religious nature: the Crusades (1095–1291), the Reconquista (711–1492), the Battle of Lepanto (1571).
External links.
Historical Maps

</doc>
<doc id="9240" url="https://en.wikipedia.org/wiki?curid=9240" title="Europa">
Europa

Europa may refer to:

</doc>
<doc id="9241" url="https://en.wikipedia.org/wiki?curid=9241" title="Euglenozoa">
Euglenozoa

The Euglenozoa are a large group of flagellate protozoa. They include a variety of common free-living species, as well as a few important parasites, some of which infect humans. There are two main subgroups, the euglenids and kinetoplastids. Euglenozoa are unicellular, mostly around 15-40 µm in size, although some euglenids get up to 500 µm long. 
Structure.
Most euglenozoa have two flagella, which are inserted parallel to one another in an apical or subapical pocket. In some these are associated with a cytostome or mouth, used to ingest bacteria or other small organisms. This is supported by one of three sets of microtubules that arise from the flagellar bases; the other two support the dorsal and ventral surfaces of the cell.
Some other euglenozoa feed through absorption, and many euglenids possess chloroplasts and so obtain energy through photosynthesis. These chloroplasts are surrounded by three membranes and contain chlorophylls "A" and "B", along with other pigments, so are probably derived from a captured green alga. Reproduction occurs exclusively through cell division. During mitosis, the nuclear membrane remains intact, and the spindle microtubules form inside of it.
The group is characterized by the ultrastructure of the flagella. In addition to the normal supporting microtubules or axoneme, each contains a rod (called "paraxonemal"), which has a tubular structure in one flagellum and a latticed structure in the other. Based on this, two smaller groups have been included here: the diplonemids and "Postgaardi".
Classification.
The euglenozoa are generally accepted as monophyletic. They are related to Percolozoa; the two share mitochondria with disk-shaped cristae, which only occurs in a few other groups.
Both probably belong to a larger group of eukaryotes called the excavates. This grouping, though, has been challenged.

</doc>
<doc id="9247" url="https://en.wikipedia.org/wiki?curid=9247" title="Epistemology">
Epistemology

Epistemology (; ) is a word first used by the Scottish philosopher James Frederick Ferrier to describe the branch of philosophy concerned with the theory of knowledge. It examines the nature of knowledge and how one can acquire it. Much of the debate in this field has focused on the philosophical analysis of the nature of knowledge and how it relates to such concepts as truth, belief, and justification.
The term was probably first introduced in Ferrier's "Institutes of Metaphysic: The Theory of Knowing and Being" (1854), p. 46.
Background and meaning.
The word "epistemology" is derived from the ancient Greek "epistēmē" meaning "scientific knowledge" and "logos" meaning "speech" or "word", in this context denoting "codified knowledge of". J.F. Ferrier coined "epistemology" on the model of 'ontology', to designate that branch of philosophy which aims to discover the meaning of knowledge, and called it the 'true beginning' of philosophy. The word is equivalent to the German concept "Wissenschaftslehre", which was used by Fichte and Bolzano for different projects before it was taken up again by Husserl. French philosophers then gave the term "épistémologie" a narrower meaning as 'theory of knowledge "de la connaissance".' E.g., Émile Meyerson opened his "Identity and Reality", written in 1908, with the remark that the word 'is becoming current' as equivalent to 'the philosophy of the sciences.'
Knowledge.
Knowledge that, knowledge how, and knowledge by acquaintance.
In epistemology in general, the kind of knowledge usually discussed is propositional knowledge, also known as "knowledge that." This is distinguished from "knowledge how" and "acquaintance-knowledge". For example: in mathematics, it is known "that" 2 + 2 = 4, but there is also knowing "how" to add two numbers and knowing a "person" (e.g., oneself), "place" (e.g., one's hometown), "thing" (e.g., cars), or "activity" (e.g., addition). Some philosophers think there is an important distinction between "knowing that," "knowing how," and "acquaintance-knowledge," with epistemology being primarily concerned with the first of these. It is sometimes suggested that these distinctions are defined linguistically in some languages, even if not in modern Standard English (N.B. some languages related to English have been said to retain these verbs, e.g. Scots: "wit" and "ken"). In French, Portuguese and Spanish, "to know (a person)" is translated using "connaître," "conhecer," and "conocer," respectively, whereas "to know (how to do something)" is translated using "savoir", "saber", and "saber". Modern Greek has the verbs "γνωρίζω" (gnorízo) and "ξέρω" (kséro). Italian has the verbs "conoscere" and "sapere" and the nouns for "knowledge" are "conoscenza" and "sapienza." German has the verbs "kennen" and "wissen." "Wissen" implies knowing a fact, "kennen" implies knowing in the sense of being acquainted with and having a working knowledge of; there is also a noun derived from "kennen," namely "Erkennen," which has been said to imply knowledge in the form of recognition or acknowledgment. The verb itself implies a process: you have to go from one state to another, from a state of "not-"erkennen"" to a state of true "erkennen." This verb seems to be the most appropriate in terms of describing the "episteme" in one of the modern European languages, hence the German name "Erkenntnistheorie." The theoretical interpretation and significance of these linguistic issues remains controversial.
In his paper "On Denoting" and his later book "Problems of Philosophy" Bertrand Russell stressed the distinction between "knowledge by description" and "knowledge by acquaintance". Gilbert Ryle is also credited with stressing the distinction between knowing how and knowing that in "The Concept of Mind." In "Personal Knowledge," Michael Polanyi argues for the epistemological relevance of knowledge how and knowledge that; using the example of the act of balance involved in riding a bicycle, he suggests that the theoretical knowledge of the physics involved in maintaining a state of balance cannot substitute for the practical knowledge of how to ride, and that it is important to understand how both are established and grounded. This position is essentially Ryle's, who argued that a failure to acknowledge the distinction between knowledge that and knowledge how leads to infinite regress.
In recent times, some epistemologists (Sosa, Greco, Kvanvig, Zagzebski) and Duncan Pritchard have argued that epistemology should evaluate people's "properties" (i.e., intellectual virtues) and not just the properties of propositions or of propositional mental attitudes.
Belief.
In common speech, a "statement of belief" is typically an expression of faith and/or trust in a person, power or other entity — while it includes such traditional views, epistemology is also concerned with what we believe. This includes 'the' truth, and everything else we accept as true for ourselves from a cognitive point of view.
Truth.
Whether someone's belief is true is not a prerequisite for (its) belief. On the other hand, if something is actually "known", then it categorically cannot be false. For example, if a person believes that a bridge is safe enough to support him, and attempts to cross it, but the bridge then collapses under his weight, it could be said that he "believed" that the bridge was safe but that his belief was mistaken. It would "not" be accurate to say that he "knew" that the bridge was safe, because plainly it was not. By contrast, if the bridge actually supported his weight, then he might say that he had believed that the bridge was safe, whereas now, after proving it to himself (by crossing it), he "knows" it was safe.
Epistemologists argue over whether belief is the proper truth-bearer. Some would rather describe knowledge as a system of justified true propositions, and others as a system of justified true sentences. Plato, in his Gorgias, argues that belief is the most commonly invoked truth-bearer.
Justification.
In many of Plato's dialogues, such as the "Meno" and, in particular, the "Theaetetus", Socrates considers a number of theories as to what knowledge is, the last being that knowledge is true belief that has been "given an account of" (meaning explained or defined in some way). According to the theory that knowledge is justified true belief, in order to know that a given proposition is true, one must not only believe the relevant true proposition, but one must also have a good reason for doing so. One implication of this would be that no one would gain knowledge just by believing something that happened to be true. For example, an ill person with no medical training, but with a generally optimistic attitude, might believe that he will recover from his illness quickly. Nevertheless, even if this belief turned out to be true, the patient would not have "known" that he would get well since his belief lacked justification.
The definition of knowledge as justified true belief was widely accepted until the 1960s. At this time, a paper written by the American philosopher Edmund Gettier provoked major widespread discussion. (See theories of justification for other views on the idea.)
Gettier problem.
Edmund Gettier is best known for a short paper entitled 'Is Justified True Belief Knowledge?' published in 1963, which called into question the theory of knowledge that had been dominant among philosophers for thousands of years. In a few pages, Gettier argued that there are situations in which one's belief may be justified and true, yet fail to count as knowledge. That is, Gettier contended that while justified belief in a true proposition is necessary for that proposition to be known, it is not sufficient. As in the diagram, a true proposition can be believed by an individual (purple region) but still not fall within the "knowledge" category (yellow region).
According to Gettier, there are certain circumstances in which one does not have knowledge, even when all of the above conditions are met. Gettier proposed two thought experiments, which have come to be known as "Gettier cases," as counterexamples to the classical account of knowledge. One of the cases involves two men, Smith and Jones, who are awaiting the results of their applications for the same job. Each man has ten coins in his pocket. Smith has excellent reasons to believe that Jones will get the job and, furthermore, knows that Jones has ten coins in his pocket (he recently counted them). From this Smith infers, "the man who will get the job has ten coins in his pocket." However, Smith is unaware that he also has ten coins in his own pocket. Furthermore, Smith, not Jones, is going to get the job. While Smith has strong evidence to believe that Jones will get the job, he is wrong. Smith has a justified true belief that a man with ten coins in his pocket will get the job; however, according to Gettier, Smith does not "know" that a man with ten coins in his pocket will get the job, because Smith's belief is "...true by virtue of the number of coins in "Jones's" pocket, while Smith does not know how many coins are in Smith's pocket, and bases his belief...on a count of the coins in Jones's pocket, whom he falsely believes to be the man who will get the job." (see p. 122.) These cases fail to be knowledge because the subject's belief is justified, but only happens to be true by virtue of luck. In other words, he made the correct choice (in this case predicting an outcome) for the wrong reasons. This example is similar to those often given when discussing belief and truth, wherein a person's belief of what will happen can coincidentally be correct without his or her having the actual knowledge to base it on.
Responses to Gettier.
The responses to Gettier have been varied. Usually, they have involved substantial attempts to provide a definition of knowledge different from the classical one, either by recasting knowledge as justified true belief with some additional fourth condition, or proposing a completely new set conditions, disregarding the classical ones entirely.
Yet a good counter-argument to Gettier's cases, is that unless they pulled out the coins and recounted them, they could not have "known", because the knowledge was based on past events. As time passes, so too does circumstances; for one could have dropped a coin after the other counted them, also without his knowledge, and he would be aware of this variable; thus couldn't truly believe this; ergo, the counter case is null.
Infallibilism, indefeasibility.
In one response to Gettier, the American philosopher Richard Kirkham has argued that the only definition of knowledge that could ever be immune to all counterexamples is the infallibilist one. To qualify as an item of knowledge, goes the theory, a belief must not only be true and justified, the justification of the belief must "necessitate" its truth. In other words, the justification for the belief must be infallible.
Yet another possible candidate for the fourth condition of knowledge is "indefeasibility." Defeasibility theory maintains that there should be no overriding or defeating truths for the reasons that justify one's belief. For example, suppose that person "S" believes he saw Tom Grabit steal a book from the library and uses this to justify the claim that Tom Grabit stole a book from the library. A possible defeater or overriding proposition for such a claim could be a true proposition like, "Tom Grabit's identical twin Sam is currently in the same town as Tom." When no defeaters of one's justification exist, a subject would be epistemelogically justified.
The Indian philosopher B K Matilal has drawn on the Navya-Nyāya fallibilism tradition to respond to the Gettier problem. Nyaya theory distinguishes between "know p" and "know that one knows p" – these are different events, with different causal conditions. The second level is a sort of implicit inference that usually follows immediately the episode of knowing p (knowledge "simpliciter"). The Gettier case is examined by referring to a view of Gangesha Upadhyaya (late 12th century), who takes any true belief to be knowledge; thus a true belief acquired through a wrong route may just be regarded as knowledge simpliciter on this view. The question of justification arises only at the second level, when one considers the knowledgehood of the acquired belief. Initially, there is lack of uncertainty, so it becomes a true belief. But at the very next moment, when the hearer is about to embark upon the venture of "knowing whether he knows p", doubts may arise. "If, in
some Gettier-like cases, I am wrong in my inference about the knowledgehood of the given occurrent belief (for the evidence may be pseudo-evidence), then I am mistaken about the truth of my belief – and this is in accordance with Nyaya fallibilism: not all knowledge-claims can be sustained."
Reliabilism.
Reliabilism has been a significant line of response to the Gettier problem among philosophers, originating with work by Alvin Goldman in the 1960s. According to reliabilism, a belief is justified (or otherwise supported in such a way as to count towards knowledge) only if it is produced by processes that typically yield a sufficiently high ratio of true to false beliefs. In other words, this theory states that a true belief counts as knowledge only if it is produced by a reliable belief-forming process.
Reliabilism has been challenged by Gettier cases. Another argument that challenges reliabilism, like the Gettier cases (although it was not presented in the same short article as the Gettier cases), is the case of Henry and the barn façades. In the thought experiment, a man, Henry, is driving along and sees a number of buildings that resemble barns. Based on his perception of one of these, he concludes that he has just seen barns. While he has seen one, and the perception he based his belief that the one he saw was of a real barn, all the other barn-like buildings he saw were façades. Theoretically, Henry does not know that he has seen a barn, despite both his belief that he has seen one being true and his belief being formed on the basis of a reliable process (i.e. his vision), since he only acquired his true belief by accident.
Other responses.
Robert Nozick has offered the following definition of knowledge:
"S" knows that "P" if and only if:
Nozick argues that the third of these conditions serves to address cases of the sort described by Gettier. Nozick further claims this condition addresses a case of the sort described by D. M. Armstrong: A father believes his daughter innocent of committing a particular crime, both because of faith in his baby girl and (now) because he has seen presented in the courtroom a conclusive demonstration of his daughter's innocence. His belief via the method of the courtroom satisfies the four subjunctive conditions, but his faith-based belief does not. If his daughter were guilty, he would still believe her innocent, on the basis of faith in his daughter; this would violate the third condition.
The British philosopher Simon Blackburn has criticized this formulation by suggesting that we do not want to accept as knowledge beliefs, which, while they "track the truth" (as Nozick's account requires), are not held for appropriate reasons. He says that "we do not want to award the title of knowing something to someone who is only meeting the conditions through a defect, flaw, or failure, compared with someone else who is not meeting the conditions." In addition to this, externalist accounts of knowledge, such as Nozick's, are often forced to reject closure in cases where it is intuitively valid.
Timothy Williamson has advanced a theory of knowledge according to which knowledge is not justified true belief plus some extra condition(s). In his book "Knowledge and its Limits," Williamson argues that the concept of knowledge cannot be broken down into a set of other concepts through analysis. Instead, Williamson argues, knowledge is "sui generis"—it is a basic, factive mental state. Williamson's theory suggests that the agent need neither to believe (or even know) that she knows in order for her to have knowledge nor to have justification in order to have knowledge. However, because knowledge is a factive mental state (FMSO), it entails truth. I.e. something must be true in order for us to say an agent has knowledge. Williamson keeps truth as a requirement for knowledge, but he does away with justification and belief as necessary conditions.
Alvin Goldman writes in his Causal Theory of Knowing that in order for knowledge to truly exist there must be a causal chain between the proposition and the belief of that proposition.
Externalism and internalism.
Part of the debate over the nature of knowledge is a debate between epistemological externalists on the one hand, and epistemological internalists on the other.
Externalists hold that factors deemed "external", meaning outside of the psychological states of those who gain knowledge, can be conditions of knowledge. For example, an externalist response to the Gettier problem is to say that, in order for a justified true belief to count as knowledge, there must be a link or dependency between the belief and the state of the external world. Usually this is understood to be a causal link. Such causation, to the extent that it is "outside" the mind, would count as an external, knowledge-yielding condition. Internalists, on the other hand, assert that all knowledge-yielding conditions are within the psychological states of those who gain knowledge.
Though unfamiliar with the internalist/externalist debate himself, many point to René Descartes as an early example of the internalist path to justification. He wrote that, because the only method by which we perceive the external world is through our senses, and that, because the senses are not infallible, we should not consider our concept of knowledge to be infallible. The only way to find anything that could be described as "indubitably true," he advocates, would be to see things "clearly and distinctly". He argued that if there is an omnipotent, good being who made the world, then it's reasonable to believe that people are made with the ability to know. However, this does not mean that man's ability to know is perfect. God gave man the ability to know, but not omniscience. Descartes said that man must use his capacities for knowledge correctly and carefully through methodological doubt. The dictum "Cogito ergo sum" (I think, therefore I am) is also commonly associated with Descartes' theory, because in his own methodological doubt, doubting everything he previously knew in order to start from a blank slate, the first thing that he could not logically bring himself to doubt was his own existence: "I do not exist" would be a contradiction in terms; the act of saying that one does not exist assumes that someone must be making the statement in the first place. Though Descartes could doubt his senses, his body and the world around him, he could not deny his own existence, because he was able to doubt and must exist in order to do so. Even if some "evil genius" were to be deceiving him, he would have to exist in order to be deceived. This one sure point provided him with what he would call his Archimedean point, in order to further develop his foundation for knowledge. Simply put, Descartes' epistemological justification depended upon his indubitable belief in his own existence and his clear and distinct knowledge of God.
Value problem.
A formulation of the value problem in epistemology first occurs in Plato's Meno. The problem is to identify what is it about knowledge (if anything) that makes it more valuable than mere true belief, or that makes knowledge more valuable than a more minimal conjunction of its components on a particular analysis of knowledge. The value problem re-emerged in the philosophical literature on epistemology in the twenty-first century following the rise of virtue epistemology in the 1980s, partly because of the obvious link with the concept of value in ethics.
The value problem has been presented as an argument against epistemic reliabilism by philosophers including Linda Zagzebski, Wayne Riggs and Richard Swinburne. Zagzebski gives a thought experiment to illustrate the unimportance of the belief being produced by a reliable process: imagine you go to a coffee machine and attempt to have it produce you a cup of coffee. The machine you use might reliably produce coffee, or it might not. Imagine one machine had a 90% chance of producing you coffee while another only had a 40% chance. If you happen to choose the 40% chance machine and it produces you a cup of coffee, the fact that it does not "reliably" produce coffee does not change the value that the coffee has to you. Similarly, if you have a true belief achieved through an unreliable process, Zagzebski argues that there's no particular reason that has "less" value than one produced through a reliable process. Advocates of virtue epistemology have argued that the value of knowledge comes from an internal relationship between the knower and the mental state of believing.
One of the more influential responses to the problem is that knowledge is not particularly valuable and is not what ought to be the main focus of epistemology. Instead, epistemologists ought to focus on other mental states, such as understanding.
Acquiring knowledge.
"A priori" and "a posteriori" knowledge.
The nature of this distinction has been disputed by various philosophers; however, the terms may be roughly defined as follows:
A priori knowledge is a way of gaining knowledge without the need of experience. In Bruce Russell's article "A Priori Justification and Knowledge" he says that it is "knowledge based on a priori justification," (1) which relies on intuition and the nature of these intuitions. A priori knowledge is often contrasted with posteriori knowledge, which is knowledge gained by experience. A way to look at the difference between the two is through an example. Bruce Russell gives two propositions in which the reader decides which one he believes more. Option A: All crows are birds. Option B: All crows are black. If you believe option A, then you are a priori justified in believing it because you don't have to see a crow to know it's a bird. If you believe in option B, then you are posteriori justified to believe it because you have seen many crows therefore knowing they are black. He goes on to say that it doesn't matter if the statement is true or not, only that if you believe in one or the other that matters.
The idea of a priori knowledge is that it is based on intuition or rational insights. Laurence BonJour says in his article "The Structure of Empirical Knowledge", that a "rational insight is an immediate, non-inferential grasp, apprehension or 'seeing' that some proposition is necessarily true." (3) Going back to the crow example, by Laurence BonJour's definition the reason you would believe in option A is because you have an immediate knowledge that a crow is a bird, without ever experiencing one.
Evolutionary psychology takes a novel approach to the problem. It says that there is an innate predisposition for certain types of learning. "Only small parts of the brain resemble a tabula rasa; this is true even for human beings. The remainder is more like an exposed negative waiting to be dipped into a developer fluid"
Analytic–synthetic distinction.
Immanuel Kant, in his "Critique of Pure Reason", drew a distinction between "analytic" and "synthetic" propositions. He contended that some propositions are such that we can know them to be true just by understanding their meaning. For example, consider, "My father's brother is my uncle." We can know it to be true solely by virtue of our understanding what its terms mean. Philosophers call such propositions "analytic." Synthetic propositions, on the other hand, have distinct subjects and predicates. An example would be, "My father's brother has black hair." Kant stated that all mathematical and scientific statements are synthetic a priori propositions because they are necessarily true but our knowledge about the attributes of the mathematical or physical subjects we can only get by logical inference.
The American philosopher W. V. O. Quine, in his "Two Dogmas of Empiricism", famously challenged the distinction, arguing that the two have a blurry boundary. Some contemporary philosophers have offered more sustainable accounts of the distinction.
Branches or 'tendencies' within epistemology.
Historical.
The historical study of philosophical epistemology is the historical study of efforts to gain philosophical understanding or knowledge of the nature and scope of human knowledge. Since efforts to get that kind of understanding have a history, the questions philosophical epistemology asks today about human knowledge are not necessarily the same as they once were. But that does not mean that philosophical epistemology is itself a historical subject, or that it pursues only or even primarily historical understanding.
Empiricism.
In philosophy, empiricism is generally a theory of knowledge focusing on the role of experience, especially experience based on perceptual observations by the senses. Certain forms treat all knowledge as empirical, while some regard disciplines such as mathematics and logic as exceptions.
There are many variants of empiricism, positivism, realism and common sense being among the most commonly expounded. But central to all empiricist epistemologies is the notion of the epistemologically privileged status of sense data.
Idealism.
Many idealists believe that knowledge is primarily (at least in some areas) acquired by "a priori" processes or is innate—for example, in the form of concepts not derived from experience. The relevant theoretical processes often go by the name "intuition". The relevant theoretical concepts may purportedly be part of the structure of the human mind (as in Kant's theory of transcendental idealism), or they may be said to exist independently of the mind (as in Plato's theory of Forms).
Rationalism.
By contrast with empiricism and idealism, which centres around the epistemologically privileged status of sense data (empirical) and the primacy of Reason (theoretical) respectively, modern rationalism adds a third 'system of thinking', (as Gaston Bachelard has termed these areas) and holds that all three are of equal importance: The empirical, the theoretical and the "abstract". For Bachelard, rationalism makes equal reference to all three systems of thinking.
Constructivism.
Constructivism is a view in philosophy according to which all "knowledge is a compilation of human-made constructions", "not the neutral discovery of an objective truth". Whereas objectivism is concerned with the "object of our knowledge", constructivism emphasises "how we construct knowledge". Constructivism proposes new definitions for knowledge and truth that form a new paradigm, based on inter-subjectivity instead of the classical objectivity, and on viability instead of truth. Piagetian constructivism, however, believes in objectivity—constructs can be validated through experimentation. The constructivist point of view is pragmatic; as Vico said: "The norm of the truth is to have made it."
Regress problem.
The regress problem is the problem of providing a complete logical foundation for human knowledge. The traditional way of supporting a rational argument is to appeal to other rational arguments, typically using chains of reason and rules of logic. A classic example that goes back to Aristotle is deducing that "Socrates is mortal." We have a logical rule that says "All humans are mortal" and an assertion that "Socrates is human" and we deduce that "Socrates is mortal". In this example how do we know that Socrates is human? Presumably we apply other rules such as: "All born from human females are human." Which then leaves open the question how do we know that all born from humans are human? This is the regress problem: how can we eventually terminate a logical argument with some statement(s) that do not require further justification but can still be considered rational and justified?
As John Pollock stated: ... to justify a belief one must appeal to a further justified belief. This means that one of two things can be the case. Either there are some beliefs that we can be justified for holding, without being able to justify them on the basis of any other belief, or else for each justified belief there is an infinite regress of (potential) justification nebula theory. On this theory there is no rock bottom of justification. Justification just meanders in and out through our network of beliefs, stopping nowhere.
The apparent impossibility of completing an infinite chain of reasoning is thought by some to support skepticism. It is also the impetus for Descartes' famous dictum: "I think therefore I am". Descartes was looking for some logical statement that could be true without appeal to other statements.
Response to the regress problem.
Many epistemologists studying justification have attempted to argue for various types of chains of reasoning that can escape the regress problem.
Foundationalism.
Foundationalists respond to the regress problem by asserting that certain "foundations" or "basic beliefs" support other beliefs but do not themselves require justification from other beliefs. These beliefs might be justified because they are self-evident, infallible, or derive from reliable cognitive mechanisms. Perception, memory, and a priori intuition are often considered to be possible examples of basic beliefs.
The chief criticism of foundationalism is that if a belief is not supported by other beliefs, accepting it may be arbitrary or unjustified.
Coherentism.
Another response to the regress problem is coherentism, which is the rejection of the assumption that the regress proceeds according to a pattern of linear justification. To avoid the charge of circularity, coherentists hold that an individual belief is justified circularly by the way it fits together (coheres) with the rest of the belief system of which it is a part. This theory has the advantage of avoiding the infinite regress without claiming special, possibly arbitrary status for some particular class of beliefs. Yet, since a system can be coherent while also being wrong, coherentists face the difficulty of ensuring that the whole system corresponds to reality. Additionally, most logicians agree that any argument that is circular is trivially valid. That is, to be illuminating, arguments must be linear with conclusions that follow from stated premises.
However, Warburton writes in 'Thinking from A to Z,' "Circular arguments are not invalid; in other words, from a logical point of view there is nothing intrinsically wrong with them. However, they are, when viciously circular, spectacularly uninformative. (Warburton 1996)."
Foundherentism.
A position known as "foundherentism", advanced by Susan Haack, is meant to be a unification of foundationalism and coherentism. One component of this theory is what is called the "analogy of the crossword puzzle." Whereas, for example, infinitists regard the regress of reasons as "shaped" like a single line, Susan Haack has argued that it is more like a crossword puzzle, with multiple lines mutually supporting each other.
Infinitism.
An alternative resolution to the regress problem is known as "infinitism". Infinitists take the infinite series to be merely potential, in the sense that an individual may have indefinitely many reasons available to them, without having consciously thought through all of these reasons when the need arises. This position is motivated in part by the desire to avoid what is seen as the arbitrariness and circularity of its chief competitors, foundationalism and coherentism.
Skepticism.
Skepticism is a position that questions the validity of some or all of human knowledge. Skepticism does not refer to any one specific school of philosophy, rather it is a thread that runs through many philosophical discussions of epistemology. The first well known sceptic was Socrates who claimed that his only knowledge was that he knew nothing with certainty. Descartes' most famous inquiry into mind and body also began as an exercise in skepticism. Descartes began by questioning the validity of all knowledge and looking for some fact that was irrefutable. In so doing, he came to his famous dictum: I think therefore I am.
Foundationalism and the other responses to the regress problem are essentially defenses against skepticism. Similarly, the pragmatism of William James can be viewed as a coherentist defense against skepticism. James discarded conventional philosophical views of truth and defined truth to be based on how well a concept works in a specific context rather than objective rational criteria. The philosophy of Logical Positivism and the work of philosophers such as Kuhn and Popper can be viewed as skepticism applied to what can truly be considered scientific knowledge.
External links.
"Stanford Encyclopedia of Philosophy" articles:
Other links:

</doc>
<doc id="9248" url="https://en.wikipedia.org/wiki?curid=9248" title="Esperanto">
Esperanto

Esperanto ( or ; ) is a constructed international auxiliary language. It is the most widely spoken constructed language in the world. The Polish ophthalmologist L. L. Zamenhof published the first book detailing Esperanto, the "," on 26 July 1887. The name of Esperanto derives from "" ("" translates as "one who hopes"), the pseudonym under which Zamenhof published Unua Libro. 
Zamenhof had three goals, as he wrote in 1887:
1) "To render the study of the language so easy as to make its acquisition mere play to the learner."
2) "To enable the learner to make direct use of his knowledge with persons of any nationality, whether the language be universally accepted or not; in other words, the language is to be directly a means of international communication."
3) "To find some means of overcoming the natural indifference of mankind, and disposing them, in the quickest manner possible, and en masse, to learn and use the proposed language as a living one, and not only in last extremities, and with the key at hand."
Up to 2,000,000 people worldwide, to varying degrees, speak Esperanto, including about 1,000 to 2,000 native speakers who learned Esperanto from birth. The World Esperanto Association has members in 120 countries. Its usage is highest in Europe, East Asia, and South America. , the most popular online learning platform for Esperanto, reported 150,000 registered users in 2013, and sees between 150,000 and 200,000 visitors each month. With about articles, Esperanto Wikipedia is the 32nd-largest Wikipedia as measured by the number of articles, and the largest Wikipedia in a constructed language. On 22 February 2012, Google Translate added Esperanto as its 64th language. On 28 May 2015, the language learning platform Duolingo launched an Esperanto course for English speakers. , over 340,000 users had signed up, with around 30 users completing the course every day.
The first World Congress of Esperanto was organized in France in 1905. Since then, congresses have been held in various countries every year, with the exceptions of years during the world wars. Although no country has adopted Esperanto officially, Esperanto was recommended by the French Academy of Sciences in 1921 and recognized by UNESCO in 1954, which recommended in 1985 that international non-governmental organizations use Esperanto. Esperanto was the 32nd language accepted as adhering to the "Common European Framework of Reference for Languages" in 2007.
Esperanto is currently the language of instruction of the International Academy of Sciences in San Marino.
Esperanto is seen by many of its speakers as an alternative or addition to the growing use of English throughout the world, offering a language that is easier to learn than English.
History.
Creation.
Esperanto was created in the late 1870s and early 1880s by L. L. Zamenhof, a Polish-Jewish ophthalmologist from Białystok, then part of the Russian Empire. According to Zamenhof, he created the language to reduce the "time and labour we spend in learning foreign tongues" and to foster harmony between people from different countries: "Were there but an international language, all translations would be made into it alone (...) and all nations would be united in a common brotherhood." His feelings and the situation in Białystok may be gleaned from an extract from his letter to Nikolai Borovko:
About his goals Zamenhof wrote that he wants mankind to "learn and use", "en masse", "the proposed language as a living one". The goal for Esperanto to become a general world language was not the only goal of Zamenhof; he also wanted to "enable the learner to make direct use of his knowledge with persons of any nationality, whether the language be universally accepted or not; in other words, the language is to be directly a means of international communication."
After some ten years of development, which Zamenhof spent translating literature into Esperanto as well as writing original prose and verse, the first book of Esperanto grammar was published in Warsaw on the 26th of July 1887. The number of speakers grew rapidly over the next few decades, at first primarily in the Russian Empire and Central Europe, then in other parts of Europe, the Americas, China, and Japan. In the early years, speakers of Esperanto kept in contact primarily through correspondence and periodicals, but in 1905 the first world congress of Esperanto speakers was held in Boulogne-sur-Mer, France. Since then world congresses have been held in different countries every year, except during the two World Wars. Since the Second World War, they have been attended by an average of more than 2,000 people and up to 6,000 people.
Zamenhof's name for the language was simply "" ("International Language").
Later history.
The autonomous territory of Neutral Moresnet, between what is today Belgium and Germany, had a sizable proportion of Esperanto-speakers among its small and multiethnic population. There was a proposal to make Esperanto its official language.
However, neither Belgium nor Prussia (now within Germany) had ever surrendered its original claim to it. Around 1900, Germany in particular was taking a more aggressive stance towards the territory and was accused of sabotage and of obstructing the administrative process in order to force the issue. It was the First World War, however, that was the catalyst that brought about the end of neutrality. On 4 August 1914, Germany invaded Belgium, leaving Moresnet at first "an oasis in a desert of destruction". In 1915, the territory was annexed by the Kingdom of Prussia, without international recognition.
After the Great War, there was a proposal for the League of Nations to accept Esperanto as their working language, following a report by Nitobe Inazō, an official delegate of League of Nations during the 13th World Congress of Esperanto in Prague. Ten delegates accepted the proposal with only one voice against, the French delegate, Gabriel Hanotaux. Hanotaux did not like how the French language was losing its position as the international language and saw Esperanto as a threat, effectively wielding his veto power to block the decision. However, two years later, the League recommended that its member states include Esperanto in their educational curricula. For this reason, many people see the 1920s as the heyday of the Esperanto movement. Anarchism as a political movement was very supportive during this time of anationalism as well as of the Esperanto language.
Esperanto attracted the suspicion of many states. The situation was especially pronounced in Nazi Germany, Francoist Spain up until the 1950s, and in the Soviet Union from 1937 to 1956.
In Nazi Germany, there was a motivation to forbid Esperanto because Zamenhof was Jewish, and due to the internationalist nature of Esperanto, which was perceived as "Bolshevist". In his work, "Mein Kampf", Adolf Hitler specifically mentioned Esperanto as an example of a language that could be used by an international Jewish conspiracy once they achieved world domination. Esperantists were killed during the Holocaust, with Zamenhof's family in particular singled out for being killed. The efforts of a minority of Esperantists to expel Jewish colleagues and align themselves with the Reich were futile and Esperanto was legally forbidden in 1935. Esperantists in German concentration camps taught the language to fellow prisoners, telling guards they were teaching Italian, the language of one of Germany's Axis allies.
In Imperial Japan, the left-wing of the Japanese Esperanto movement was forbidden, but its leaders were careful enough not to give the impression to the government that the Esperantists were socialist revolutionaries, which proved a successful strategy.
After the October Revolution of 1917, Esperanto was given a measure of government support by the new workers' states in the former Russian Empire and later by the Soviet Union government, with the Soviet Esperanto Association being established as an officially recognized organization. In his biography on Joseph Stalin, Leon Trotsky mentions that Stalin had studied Esperanto. However, in 1937, at the height of the Great Purge, Stalin completely reversed the Soviet government's policies on Esperanto, denouncing it as "the language of spies" and had Esperantists exiled or executed. The use of Esperanto was then banned in the Soviet Union until 1956.
Fascist Italy allowed the use of Esperanto, finding its phonology similar to that of Italian and publishing some tourist material in the language.
During and after the Spanish Civil War, Francoist Spain forbade anarchists, socialists and Catalan nationalists for many years, among whom the use of Esperanto was extensive, but in the 1950s the Esperanto movement was tolerated again.
Official use.
Esperanto has not been a secondary official language of any recognized country, but it entered the education system of several countries such as Hungary and China.
There were plans at the beginning of the 20th century to establish Neutral Moresnet as the world's first Esperanto state. In addition, the self-proclaimed artificial island micronation of Rose Island used Esperanto as its official language in 1968.
The Chinese government has used Esperanto since 2001 for daily news on china.org.cn. China also uses Esperanto in China Radio International and for the internet magazine "El Popola Ĉinio".
The Vatican Radio has an Esperanto version of its website.
The US Army has published military phrase books in Esperanto, to be used from the 1950s through the 1970s in war games by mock enemy forces.
Esperanto is the working language of several non-profit international organizations such as the "", a left-wing cultural association, or Education@Internet, which has developed from an Esperanto organization; most others are specifically Esperanto organizations. The largest of these, the World Esperanto Association, has an official consultative relationship with the United Nations and UNESCO, which recognized Esperanto as a medium for international understanding in 1954. Esperanto is also the first language of teaching and administration of one university, the International Academy of Sciences San Marino.
In the summer of 1924, the American Radio Relay League adopted Esperanto as its official international auxiliary language, and hoped that the language would be used by radio amateurs in international communications, but its actual use for radio communications was negligible.
All the personal documents issued by the World Service Authority, including the World Passport, are written in Esperanto, together with English, French, Spanish, Russian, Arabic, and Chinese.
Achievement of its creator's goals.
One common criticism made is that Esperanto has failed to live up to the hopes of its creator, who dreamed of it becoming a universal second language.
On the other hand, Zamenhof's goal to "enable the learner to make direct use of his knowledge with persons of any nationality, whether the language be universally accepted or not", as he wrote in 1887, has been achieved as the language is currently spoken by people living in more than one hundred countries.
Linguistic properties.
Alphabet.
The Esperanto alphabet is based on the Latin script, using a one-sound-one-letter principle, except for [d͡z]. It includes six letters with diacritics: ĉ, ĝ, ĥ, ĵ, ŝ (with circumflex), and ŭ (with breve). The alphabet does not include the letters "q, w, x," or "y", which are only used when writing unassimilated foreign terms or proper names.
The 28-letter alphabet is:
All unaccented letters are pronounced approximately as in the IPA, with the exception of "c". Esperanto "j" and "c" are used in a way familiar to speakers of many European languages, but which is largely unfamiliar to English speakers: "j" has a "y" sound, as in "yellow" and "boy," and "c" has a "ts" sound, as in "hits" or the "zz" in "pizza". The accented letters are a bit like "h"-digraphs in English: "Ĉ" is pronounced like English "ch", and "ŝ" like "sh". "Ĝ" is the "g" in "gem", "ĵ" a "zh" sound, as in "fusion" or French "Jacques", and the rare "ĥ" is like the German , older Scottish English "loch", or how Scouse people sometimes pronounce the 'k' in "book" and 'ck' in "chicken".
Writing diacritics.
Until the widespread adoption of Unicode, the letters with diacritics (found in the "Latin-Extended A" section of the Unicode Standard) caused problems with printing and computing. This was particularly true of the five letters with circumflexes, as they do not occur in any other language. These problems have abated, and are now normally seen only with computing applications that are limited to ASCII characters (typically internet chat systems and databases).
There are two principal workarounds to this problem, which substitute digraphs for the accented letters. Zamenhof, the inventor of Esperanto, created an "h-convention", which replaces "ĉ, ĝ, ĥ, ĵ, ŝ," and "ŭ" with "ch, gh, hh, jh, sh," and "u," respectively. If used in a database, a program in principle could not determine whether to render, for example, "ch" as "c" followed by "h" or as "ĉ", and would fail to render, for example, the word "" properly. A more recent "x-convention" has gained ground since the advent of computing. This system replaces each diacritic with an "x" (not part of the Esperanto alphabet) after the letter, producing the six digraphs "cx, gx, hx, jx, sx," and "ux."
There are computer keyboard layouts that support the Esperanto alphabet, and some systems use software that automatically replaces x- or h-convention digraphs with the corresponding diacritic letters ( for Microsoft Windows and for Windows Phone are examples). Another example is the Esperanto Wikipedia, which accepts the x-convention for input: when a contributor types "cx" when editing an article, it will appear as the correct ' in the article text. (The input pane also accepts '; when the page is saved, it will be changed to "cx", so that the x-convention applies uniformly in the wikitext.)
Criticisms are made of the letters with circumflex diacritics, which some find odd or cumbersome, along with their being invented specifically for Esperanto rather than borrowed from existing languages; as well as being arguably unnecessary, as for example with the use of "ŭ" instead of "w".
Classification.
Most scholars would say that as a constructed language, Esperanto is not genealogically related to any natural language. The phonology, grammar, vocabulary, and semantics are based on the Indo-European languages spoken in Europe. The sound inventory is essentially Slavic, as is much of the semantics, whereas the vocabulary derives primarily from the Romance languages, with a lesser contribution from Germanic languages and minor contributions from Slavic languages and Greek. Pragmatics and other aspects of the language not specified by Zamenhof's original documents were influenced by the native languages of early authors, primarily Russian, Polish, German, and French. However, Paul Wexler proposes that Esperanto is relexified Yiddish, which in turn he claims is a relexified Slavic language.
Esperanto has been described as "a language lexically predominantly Romanic, morphologically intensively agglutinative, and to a certain degree isolating in character". Typologically, Esperanto has prepositions and a pragmatic word order that by default is "subject–verb–object." Adjectives can be freely placed before or after the nouns they modify, though placing them before the noun is more common. New words are formed through extensive prefixing and suffixing.
Grammar.
Esperanto words are derived by stringing together prefixes, roots, and suffixes. This process is regular, so that people can create new words as they speak and be understood. Compound words are formed with a modifier-first, head-final order, as in English (compare "birdsong" and "songbird," and likewise, ' and ').
The different parts of speech are marked by their own suffixes: all common nouns end in ', all adjectives in ', all derived adverbs in ', and all verbs in one of six tense and mood suffixes, such as the present tense '. Nouns and adjectives have two cases: nominative for grammatical subjects and in general, and accusative for direct objects and (after a preposition) to indicate direction of movement.
Singular nouns used as grammatical subjects end in ', plural subject nouns in ' (pronounced like English "oy"). Singular direct object forms end in ', and plural direct objects with the combination ' (rhymes with "coin"): ' indicates that the word is a noun, ' indicates the plural, and ' indicates the accusative (direct object) case. Adjectives agree with their nouns; their endings are singular subject ' (rhymes with "ha!"), plural subject ' (pronounced "eye"), singular object ', and plural object "" (rhymes with "fine").
The suffix "", besides indicating the direct object, is used to indicate movement and a few other things as well.
The six verb inflections consist of three tenses and three moods. They are present tense ', future tense ', past tense ', infinitive mood ', conditional mood ' and jussive mood ' (used for wishes and commands). Verbs are not marked for person or number. Thus, ' means "to sing", ' means "I sing", ' means "you sing", and ' means "they sing".
Word order is comparatively free. Adjectives may precede or follow nouns; subjects, verbs and objects may occur in any order. However, the article ' "the", demonstratives such as ' "that" and prepositions (such as ' "at") must come before their related nouns. Similarly, the negative ' "not" and conjunctions such as ' "and" and ' "that" must precede the phrase or clause that they introduce. In copular (A = B) clauses, word order is just as important as in English: "people are animals" is distinguished from "animals are people".
Living language.
The Hungarian Academy of Sciences has found that Esperanto fulfills all the requirements of a living language.
Neutrality.
Origin.
This is most often noted in regard to the vocabulary, but applies equally to the orthography, phonology, and semantics, all of which are thoroughly European. The vocabulary, for example, draws about two-thirds from Romance and one-third from Germanic languages; the syntax is Romance; and the phonology and semantics are Slavic. The grammar is arguably more European than not, but Claude Piron among others argues that the derivation system is not particularly European, though the inflection is.
Gender.
Esperanto is frequently accused of being inherently sexist, because the default form of some nouns is masculine while a derived form is used for the feminine, which is said to retain traces of the male-dominated society of late 19th-century Europe of which Esperanto is a product. There are a couple dozen masculine nouns, primarily titles and kin terms, such as "sinjoro" "Mr, sir" vs. "sinjorino" "Mrs, lady" and "patro" "father" vs. "patrino" "mother". In addition, nouns that denote persons and whose definitions are not explicitly make are often assumed to be male unless explicitly made female, such as "doktoro," a PhD doctor (male or unspecified) versus "doktorino," a female PhD. This is analogous to the situation with the English suffix "-ess," as in baron/baroness, waiter/waitress etc. Esperanto pronouns are similar. As in English, "li" "he" may be used generically, whereas "ŝi" "she" is always female.
Phonology.
Esperanto has 23 consonants, five vowels, and two semivowels that combine with the vowels to form six diphthongs. (The consonant and semivowel are both written "j", and the uncommon consonant is written with the digraph "dz", which is the only consonant that doesn't have its own letter.) Tone is not used to distinguish meanings of words. Stress is always on the second-last vowel in fully Esperanto words unless a final vowel ' is elided, which occurs mostly in poetry. For example, ' "family" is , with the stress on the second "i", but when the word is used without the final " ()," the stress remains on the second ': .
Consonants.
The 23 consonants are:
The sound is usually trilled , but may be tapped . The is normally pronounced like English "v," but may be pronounced (between English "v" and "w") or , depending on the language background of the speaker. A semivowel normally occurs only in diphthongs after the vowels and , not as a consonant . Common, if debated, assimilation includes the pronunciation of ' as and ' as .
A large number of consonant clusters can occur, up to three in initial position (as in ', "strange") and four in medial position (as in ', "teach"). Final clusters are uncommon except in foreign names, poetic elision of final "," and a very few basic words such as ' "hundred" and ' "after".
Vowels.
Esperanto has the five vowels found in such languages as Spanish, Swahili, Modern Hebrew, and Modern Greek.
There are also two semivowels, and , which combine with the monophthongs to form six falling diphthongs: ", , , , ," and "".
Since there are only five vowels, a good deal of variation in pronunciation is tolerated. For instance, "e" commonly ranges from (French ') to (French '). These details often depend on the speaker's native language. A glottal stop may occur between adjacent vowels in some people's speech, especially when the two vowels are the same, as in ' "hero" ( or ) and ' "great-grandfather" ( or ).
Sample text.
The following short extract gives an idea of the character of Esperanto. (Pronunciation is covered above; the Esperanto letter "j" is pronounced like English "y".)
Simple phrases.
Below are listed some useful Esperanto words and phrases along with transcriptions:
Vocabulary.
The core vocabulary of Esperanto was defined by ', published by Zamenhof in 1887. This book listed 900 roots; these could be expanded into tens of thousands of words using prefixes, suffixes, and compounding. In 1894, Zamenhof published the first Esperanto dictionary, ', which had a larger set of roots. The rules of the language allowed speakers to borrow new roots as needed; it was recommended, however, that speakers use most international forms and then derive related meanings from these.
Since then, many words have been borrowed, primarily (but not solely) from the European languages. Not all proposed borrowings become widespread, but many do, especially technical and scientific terms. Terms for everyday use, on the other hand, are more likely to be derived from existing roots; ' "computer", for instance, is formed from the verb ' "compute" and the suffix ' "tool". Words are also calqued; that is, words acquire new meanings based on usage in other languages. For example, the word ' "mouse" has acquired the meaning of a computer mouse from its usage in English. Esperanto speakers often debate about whether a particular borrowing is justified or whether meaning can be expressed by deriving from or extending the meaning of existing words.
Some compounds and formed words in Esperanto are not entirely straightforward; for example, ', literally "give out", means "publish", paralleling the usage of certain European languages (such as German). In addition, the suffix "-um-" has no defined meaning; words using the suffix must be learned separately (such as ' "to the right" and "" "clockwise").
There are not many idiomatic or slang words in Esperanto, as these forms of speech tend to make international communication difficult—working against Esperanto's main goal.
Critics feel there are too many roots. Instead of derivations of Esperanto roots, new roots are taken from European languages in the endeavor to create an international language.
Education.
The majority of Esperanto speakers learn the language through self-directed study, online tutorials, and correspondence courses taught by volunteers. More recently, free teaching websites, like ' and ', have become popular.
Esperanto instruction is occasionally available at schools, including four primary schools in a pilot project under the supervision of the University of Manchester, and by one count at 69 universities. However, outside China and Hungary, these mostly involve informal arrangements rather than dedicated departments or state sponsorship. Eötvös Loránd University in Budapest had a department of Interlinguistics and Esperanto from 1966 to 2004, after which time instruction moved to vocational colleges; there are state examinations for Esperanto instructors. Additionally, Adam Mickiewicz University in Poland offers a diploma in Interlinguistics. The Senate of Brazil passed a bill in 2009 that would make Esperanto an optional part of the curriculum in public schools, although mandatory if there is demand for it. the bill is still under consideration by the Chamber of Deputies.
Various educators have estimated that Esperanto can be learned in anywhere from one quarter to one twentieth the amount of time required for other languages. Claude Piron, a psychologist formerly at the University of Geneva and Chinese–English–Russian–Spanish translator for the United Nations, argued that Esperanto is far more intuitive than many ethnic languages. "Esperanto relies entirely on innate reflexes differs from all other languages in that you can always trust your natural tendency to generalize patterns... The same neuropsychological law [—called by Jean Piaget "generalizing assimilation"—applies to word formation as well as to grammar."
The Institute of Cybernetic Pedagogy at Paderborn (Germany) has compared the length of study time it takes natively French-speaking high-school students to obtain comparable 'standard' levels in Esperanto, English, German, and Italian. The results were:
Third-language acquisition.
Four primary schools in Britain, with some 230 pupils, are currently following a course in "propaedeutic Esperanto"—that is, instruction in Esperanto to raise language awareness and accelerate subsequent learning of foreign languages—under the supervision of the University of Manchester. As they put it,
Studies have been conducted in New Zealand, United States, Germany, Italy and Australia. The results of these studies were favorable and demonstrated that studying Esperanto before another foreign language expedites the acquisition of the other, natural, language. This appears to be because learning subsequent foreign languages is easier than learning one's first foreign language, whereas the use of a grammatically simple and culturally flexible auxiliary language like Esperanto lessens the first-language learning hurdle. In one study, a group of European secondary school students studied Esperanto for one year, then French for three years, and ended up with a significantly better command of French than a control group, who studied French for all four years. 
Community.
Geography and demography.
Esperanto is by far the most widely spoken constructed language in the world. Speakers are most numerous in Europe and East Asia, especially in urban areas, where they often form Esperanto clubs. Esperanto is particularly prevalent in the northern and central countries of Europe; in China, Korea, Japan, and Iran within Asia; in Brazil, Argentina, and Mexico in the Americas; and in Togo in Africa.
Number of speakers.
An estimate of the number of Esperanto speakers was made by Sidney S. Culbert, a retired psychology professor at the University of Washington and a longtime Esperantist, who tracked down and tested Esperanto speakers in sample areas in dozens of countries over a period of twenty years. Culbert concluded that between one and two million people speak Esperanto at Foreign Service Level 3, "professionally proficient" (able to communicate moderately complex ideas without hesitation, and to follow speeches, radio broadcasts, etc.). Culbert's estimate was not made for Esperanto alone, but formed part of his listing of estimates for all languages of more than one million speakers, published annually in the World Almanac and Book of Facts. Culbert's most detailed account of his methodology is found in a 1989 letter to David Wolff. Since Culbert never published detailed intermediate results for particular countries and regions, it is difficult to independently gauge the accuracy of his results.
In the Almanac, his estimates for numbers of language speakers were rounded to the nearest million, thus the number for Esperanto speakers is shown as two million. This latter figure appears in "Ethnologue". Assuming that this figure is accurate, that means that about 0.03% of the world's population speak the language. Although it is not Zamenhof's goal of a universal language, it still represents a level of popularity unmatched by any other constructed language.
Marcus Sikosek (now Ziko van Dijk) has challenged this figure of 1.6 million as exaggerated. He estimated that even if Esperanto speakers were evenly distributed, assuming one million Esperanto speakers worldwide would lead one to expect about 180 in the city of Cologne. Van Dijk finds only 30 fluent speakers in that city, and similarly smaller-than-expected figures in several other places thought to have a larger-than-average concentration of Esperanto speakers. He also notes that there are a total of about 20,000 members of the various Esperanto organizations (other estimates are higher). Though there are undoubtedly many Esperanto speakers who are not members of any Esperanto organization, he thinks it unlikely that there are fifty times more speakers than organization members.
Finnish linguist Jouko Lindstedt, an expert on native-born Esperanto speakers, presented the following scheme to show the overall proportions of language capabilities within the Esperanto community:
In the absence of Dr. Culbert's detailed sampling data, or any other census data, it is impossible to state the number of speakers with certainty. According to the website of the World Esperanto Association:
In 2009 Lu Wunsch-Rolshoven used 2001 year census data from Hungary and Lithuania as a base for an estimate, resulting in approximately 160,000 to 300,000 to speak the language actively or fluently throughout the world, with about 80,000 to 150,000 of these being in the European Union.
Native speakers.
Native Esperanto speakers, "," have learned the language from birth from Esperanto-speaking parents. This usually happens when Esperanto is the chief or only common language in an international family, but sometimes occurs in a family of devoted Esperantists. The 15th edition of "Ethnologue" cited estimates that there were 200 to 2,000 native speakers in 1996, but these figures were removed from the 16th and 17th editions. 
As of 1996, there were approximately 350 attested cases of families with native Esperanto speakers.
Esperanto speaking users of Facebook.
Facebook has about 350,000 users who indicated Esperanto as one of their languages.
Culture.
Esperantists can access an international culture, including a large body of original as well as translated literature. There are more than 25,000 Esperanto books, both originals and translations, as well as several regularly distributed Esperanto magazines. In 2013 a museum about Esperanto opened in China. Esperantists use the language for free accommodations with Esperantists in 92 countries using the or to develop pen pals through "".
Every year, Esperantists meet for the World Congress of Esperanto "()".
Historically, much Esperanto music, such as ', has been in various folk traditions. There is also a variety of classical and semi-classical choral music, both original and translated, as well as large ensemble music that includes voices singing Esperanto texts. Lou Harrison, who incorporated styles and instruments from many world cultures in his music, used Esperanto titles and/or texts in several of his works, most notably ' (1973). David Gaines used Esperanto poems as well as an excerpt from a speech by Dr. Zamenhof for his "Symphony No. 1 (Esperanto)" for mezzo-soprano and orchestra (1994–98). He wrote original Esperanto text for his "" ("I Can Cry No Longer") for unaccompanied SATB choir (1994).
There are also shared traditions, such as Zamenhof Day, and shared behaviour patterns. Esperantists speak primarily in Esperanto at international Esperanto meetings.
Detractors of Esperanto occasionally criticize it as "having no culture". Proponents, such as Prof. Humphrey Tonkin of the University of Hartford, observe that Esperanto is "culturally neutral by design, as it was intended to be a facilitator between cultures, not to be the carrier of any one national culture". The late Scottish Esperanto author William Auld wrote extensively on the subject, arguing that Esperanto is "the expression of a common human culture, unencumbered by national frontiers. Thus it is considered a culture on its own."
Noted authors in Esperanto.
Some authors of works in Esperanto are:
Popular culture.
Esperanto has been used in a number of films and novels. Typically, this is done either to add the exotic flavour of a foreign language without representing any particular ethnicity, or to avoid going to the trouble of inventing a new language. The Charlie Chaplin film "The Great Dictator" (1940) showed Jewish ghetto shop signs in Esperanto. Two full-length feature films have been produced with dialogue entirely in Esperanto: "," in 1964, and "Incubus," a 1965 B-movie horror film. A language school teaching Esperanto is featured in Graham Greene's novel "The Confidential Agent", which was made into a film starring Charles Boyer and Lauren Bacall (1945). Other amateur productions have been made, such as a dramatization of the novel "" (Gerda Has Disappeared). In "Stamboul Train", Greene used Esperanto as the language on signs at the main train station in Budapest. A number of mainstream films in national languages have used Esperanto in some way.
Esperanto is used as the universal language in the far future of Harry Harrison's "Stainless Steel Rat" and "Deathworld" stories. Poul Anderson's story "High Treason" takes place in a future where Earth became united politically but was still divided into many languages and cultures, and Esperanto became the language of its space armed forces, fighting wars with various extraterrestrial races.
The opening song to the popular video game "Final Fantasy XI", "", was written in Esperanto. It was the first game in the series that was played online, and would have players from both Japan and North America (official European support was added after the North American launch) playing together on the same servers, using an auto-translate tool to communicate. The composer, Nobuo Uematsu, felt that Esperanto was a good language to symbolize worldwide unity.
In the geek fiction novel "Off to Be the Wizard", Esperanto is programmed as the language that triggers all of the wizard's spells. Philip, Martin's teacher, explains that this is because "no one really speaks Esperanto and it's easy to learn".
Esperanto is also found in the comic book series "Saga" as the language Blue, spoken by the inhabitants of Wreath. It is rendered in blue-colored text. Blue is generally only spoken by inhabitants of Wreath, while most other cultures use a universal language that appears to be simply named "Language." Some Wreath inhabitants use translator rings to communicate with those who don't speak Blue. Magic seems to be activated via the linguistic medium of blue.
In the television show "Red Dwarf", the bulk of which takes place more than three million years in the future, crewman Arnold Rimmer constantly spends his time trying to learn Esperanto and failing, even compared to his bunkmate Dave Lister who only maintains a casual interest. Additionally many of the signs around the ship "Red Dwarf" are written in both English and Esperanto. The novel "Infinity Welcomes Careful Drivers" states that, although not required, it is widely expected that officers in the Space Corps be fluent in the language, hence Rimmer's interest.
Science.
In 1921 the French Academy of Sciences recommended using Esperanto for international scientific communication. A few scientists and mathematicians, such as Maurice Fréchet (mathematics), John C. Wells (linguistics), Helmar Frank (pedagogy and cybernetics), and Nobel laureate Reinhard Selten (economics) have published part of their work in Esperanto. Frank and Selten were among the founders of the International Academy of Sciences in San Marino, sometimes called the "Esperanto University", where Esperanto is the primary language of teaching and administration.
A message in Esperanto was recorded and included in "Voyager 1"s Golden Record.
Commerce and trade.
Esperanto business groups have been active for many years. The French Chamber of Commerce did research in the 1920s and reported in "The New York Times" in 1921 that Esperanto seemed to be the best business language.
Goals of the movement.
Zamenhof's intention was to create an easy-to-learn language to foster international understanding. It was to serve as an international auxiliary language, that is, as a universal second language, not to replace ethnic languages. This goal was widely shared among Esperanto speakers in the early decades of the movement. Later, Esperanto speakers began to see the language and the culture that had grown up around it as ends in themselves, even if Esperanto is never adopted by the United Nations or other international organizations.
Esperanto speakers who want to see Esperanto adopted officially or on a large scale worldwide are commonly called ', from ', meaning "final victory", Those who focus on the intrinsic value of the language are commonly called "", from Rauma, Finland, where a declaration on the near-term unlikelihood of the "" and the value of Esperanto culture was made at the International Youth Congress in 1980.
Symbols and flags.
The earliest flag, and the one most commonly used today, features a green five-pointed star against a white canton, upon a field of green. It was proposed to Zamenhof by Irishman Richard Geoghegan, author of the first Esperanto textbook for English speakers, in 1887. The flag was approved in 1905 by delegates to the first conference of Esperantists at Boulogne-sur-Mer. A version with an "" superimposed over the green star is sometimes seen. Other variants include that for Christian Esperantists, with a white Christian cross superimposed upon the green star, and that for Leftists, with the color of the field changed from green to red.
In 1987, a second flag design was chosen in a contest organized by the UEA celebrating the first centennial of the language. It featured a white background with two stylised curved "E"s facing each other. Dubbed the "" (jubilee symbol), it attracted criticism from some Esperantists, who dubbed it the "" (melon) because of the design's elliptical shape. It is still in use, though to a lesser degree than the traditional symbol, known as the "" (green star).
Politics.
Esperanto has been placed in many proposed political situations. The most popular of these is the Europe—Democracy—Esperanto, which aims to establish Esperanto as the official language of the European Union. Grin's Report, published in 2005 by François Grin found that the use of English as the lingua franca within the European Union costs billions annually and significantly benefits English-speaking countries financially. The report considered a scenario where Esperanto would be the lingua franca and found that it would have many advantages, particularly economically speaking, as well as ideologically.
Religion.
Esperanto has served an important role in several religions, such as Oomoto from Japan and the Bahá'í Faith from Iran, and has been encouraged by others, like some Spiritist movements.
Oomoto.
The Oomoto religion encourages the use of Esperanto among its followers and includes Zamenhof as one of its deified spirits.
Bahá'í Faith.
The Bahá'í Faith encourages the use of an auxiliary international language. The Baha'i's believe that it will not be the language of the future, although it has great potential in this role, as it has not been chosen by the people. L. L. Zamenhof's daughter Lidja became a Bahá'í, and various volumes of the Bahá'í literatures and other Baha'i books have been translated into Esperanto. In 1973, the Bahá'í Esperanto-League for active Bahá'í supporters of Esperanto was founded.
Spiritism.
In 1908, spiritist Camilo Chaigneau wrote an article named "Spiritism and Esperanto" in the periodic "La Vie d'Outre-Tombe" recommending the use of Esperanto in a "central magazine" for all spiritists and esperantists. Esperanto then became actively promoted by spiritists, at least in Brazil, initially by Ismael Gomes Braga and František Lorenz; the latter is known in Brazil as Francisco Valdomiro Lorenz, and was a pioneer of both spiritist and Esperantist movements in this country.
The Brazilian Spiritist Federation publishes Esperanto coursebooks, translations of Spiritism's basic books, and encourages Spiritists to become Esperantists.
Bible translations.
The first translation of the Bible into Esperanto was a translation of the Tanakh or Old Testament done by L. L. Zamenhof. The translation was reviewed and compared with other languages' translations by a group of British clergy and scholars before its publication at the British and Foreign Bible Society in 1910. In 1926 this was published along with a New Testament translation, in an edition commonly called the "". In the 1960s, the ' tried to organize a new, ecumenical Esperanto Bible version. Since then, the Dutch Remonstrant pastor Gerrit Berveling has translated the Deuterocanonical or apocryphal books in addition to new translations of the Gospels, some of the New Testament epistles, and some books of the Tanakh or Old Testament. These have been published in various separate booklets, or serialized in ', but the Deuterocanonical books have appeared in recent editions of the Londona Biblio.
Christianity.
Christian Esperanto organizations include two that were formed early in the history of Esperanto:
Individual churches using Esperanto include:
Chick Publications, publisher of Protestant fundamentalist themed evangelistic tracts, has published a number of comic book style tracts by Jack T. Chick translated into Esperanto, including "This Was Your Life!" ("")
Islam.
Ayatollah Khomeini of Iran called on Muslims to learn Esperanto and praised its use as a medium for better understanding among peoples of different religious backgrounds. After he suggested that Esperanto replace English as an international lingua franca, it began to be used in the seminaries of Qom. An Esperanto translation of the Qur'an was published by the state shortly thereafter. In 1981, its usage became less popular when it became apparent that followers of the Bahá'í Faith were interested in it. However, during the recent decades, specially after the establishment of the Sabzandishan (Green-Thinkers) Institute in 1996, the first official Esperanto institute in Iran ever, and publication of its 56-page organ, called Payame Sabzandishan (Message of Green-Thinkers), a seasonal (quarterly) magazine in Esperanto and Persian from the autumn of 2002 till now, and recognition of the Iranian Esperanto-Association by the Universal Esperanto-Association (which enjoys official relations with UN and UNESCO) as its Iranian official branch in 2005, a new era started in Iran for spreading of Esperanto Movement as vastly as possible. During this new era, i.a. there have been speeches, lectures, seminars and courses in different cultural centers, universities and schools; publication of original and translated books and articles on Esperanto and specially its neutrality (politically, religiously, nationally, racially, etc.) by diverse publishers and in varied Persian newspapers and magazines; ... E.g. in the Persian translation of William Auld's book, called The Phenomenon Esperanto, 14 annexes were added to show more the history and neutrality of Esperanto language: as example, in the first annex, called The Views of World Celebrities on Esperanto, the Persian readers can read the positive views and opinions of 15 acclaimed and famous leaders and writers on Esperanto from different countries, religions, political backgrounds, languages and races, like Mahatma Gandhi, Leo Tolstoy, Romain Rolland, Umberto Eco, Rudolf Diesel, Rabindranath Tagore, Helen Keller, Lu Xun, J. R. R. Tolkien, ... (William Auld was nominated for the Nobel Prize in Literature in 1999, 2004, and 2006 making him the first person to be nominated for works in Esperanto.)
Modifications.
Though Esperanto itself has changed little since the publication of the "" (Foundation of Esperanto), a number of reform projects have been proposed over the years, starting with Zamenhof's proposals in 1894 and in 1907. Several later constructed languages, such as Universal, were based on Esperanto.
In modern times, attempts have been made to eliminate perceived sexism in the language, such as Riism.
Eponymous entities.
There are some geographical and astronomical features named after Esperanto, or after its creator L. L. Zamenhof. These include Esperanto Island in Zed Islands off Livingston Island, and the asteroids 1421 Esperanto and 1462 Zamenhof discovered by Finnish astronomer and Esperantist Yrjö Väisälä.

</doc>
<doc id="9251" url="https://en.wikipedia.org/wiki?curid=9251" title="Engineering">
Engineering

Engineering is the application of mathematics, empirical evidence and scientific, economic, social, and practical knowledge in order to invent, innovate, design, build, maintain, research, and improve structures, machines, tools, systems, components, materials, and processes.
The discipline of engineering is extremely broad, and encompasses a range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied science, technology and types of application.
The term "Engineering" is derived from the Latin "ingenium", meaning "cleverness" and "ingeniare", meaning "to contrive, devise".
Definition.
The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET) has defined "engineering" as:
The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation or safety to life and property.
History.
Engineering has existed since ancient times as humans devised fundamental inventions such as the wedge, lever, wheel, and pulley. Each of these inventions is essentially consistent with the modern definition of engineering.
The term "engineering" deriving from the word "engineer", which itself dates back to 1300, when an "engine'er" (literally, one who operates an "engine") originally referred to "a constructor of military engines." In this context, now obsolete, an "engine" referred to a military machine, "i.e.", a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, "e.g.", the U.S. Army Corps of Engineers.
The word "engine" itself is of even older origin, ultimately deriving from the Latin "ingenium" (c. 1250), meaning "innate quality, especially mental power, hence a clever invention."
Later, as the design of civilian structures such as bridges and buildings matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the older discipline of military engineering.
Ancient era.
The Pharos of Alexandria, the pyramids in Egypt, the Hanging Gardens of Babylon, the Acropolis and the Parthenon in Greece, the Roman aqueducts, Via Appia and the Colosseum, Teotihuacán and the cities and pyramids of the Mayan, Inca and Aztec Empires, the Great Wall of China, the Brihadeeswarar Temple of Thanjavur and Indian Temples, among many others, stand as a testament to the ingenuity and skill of the ancient civil and military engineers.
The earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630-2611 BC.
Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, the first known mechanical computer, and the mechanical inventions of Archimedes are examples of early mechanical engineering. Some of Archimedes' inventions as well as the Antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are still widely used today in diverse fields such as robotics and automotive engineering.
Chinese, Greek and Roman armies employed complex military machines and inventions such as artillery which was developed by the Greeks around the 4th century B.C., the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.
Renaissance era.
William Gilbert is considered to be the first electrical engineer with his 1600 publication of De Magnete. He coined the term "electricity".
The first steam engine was built in 1698 by Thomas Savery. The development of this device gave rise to the Industrial Revolution in the coming decades, allowing for the beginnings of mass production.
With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering the fields then known as the mechanic arts became incorporated into engineering.
Modern era.
The inventions of Thomas Newcomen and the Scottish engineer James Watt gave rise to modern mechanical engineering. The development of specialized machines and machine tools during the industrial revolution led to the rapid growth of mechanical engineering both in its birthplace Britain and abroad.
John Smeaton was the first self-proclaimed civil engineer, and is often regarded as the "father" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbours and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. His lighthouse remained in use until 1877 and was dismantled and partially rebuilt at Plymouth Hoe where it is known as Smeaton's Tower. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain "hydraulicity" in lime; work which led ultimately to the invention of Portland cement.
The United States census of 1850 listed the occupation of "engineer" for the first time with a count of 2,000. There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890 there were 6,000 engineers in civil, mining, mechanical and electrical.
There was no chair of applied mechanism and applied mechanics established at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.
The early stages of electrical engineering included the experiments of Alessandro Volta in the 1800s, the experiments of Michael Faraday, Georg Ohm and others and the invention of the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.
Chemical engineering developed in the late nineteenth century. Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants. The role of the chemical engineer was the design of these chemical plants and processes.
Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.
The first PhD in engineering (technically, "applied science and engineering") awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.
Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I . Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.
In 1990, with the rise of computer technology, the first search engine was built by computer engineer Alan Emtage.
Main branches of engineering.
Engineering is a broad discipline which is often broken down into several sub-disciplines. These disciplines concern themselves with differing areas of engineering work. Although initially an engineer will usually be trained in a specific discipline, throughout an engineer's career the engineer may become multi-disciplined, having worked in several of the outlined areas. Engineering is often characterized as having four main branches:
Beyond these four, a number of other branches are recognized. Historically, naval engineering and mining engineering were major branches. Other engineering fields sometimes included as major branches are manufacturing engineering, acoustical engineering, corrosion engineering, Instrumentation and control, aerospace, automotive, computer, electronic, petroleum, systems, audio, software, architectural, agricultural, biosystems, biomedical, geological, textile, industrial, materials, and nuclear engineering. These and other branches of engineering are represented in the 36 professional engineering institutions the UK Engineering Council.
New specialties sometimes combine with the traditional fields and form new branches - for example Earth Systems Engineering and Management involves a wide range of subject areas including anthropology, engineering studies, environmental science, ethics and philosophy. A new or emerging area of application will commonly be defined temporarily as a permutation or subset of existing disciplines; there is often gray area as to when a given sub-field warrants classification as a new "branch." One key indicator of such emergence is when major universities start establishing departments and programs in the new field.
For each of these fields there exists considerable overlap, especially in the areas of the application of fundamental sciences to their disciplines such as physics, chemistry, and mathematics.
Practice.
One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur or European Engineer, Designated Engineering Representative. In the UK many trades are called "Engineer" including gas, telephone, photocopy, maintenance, plumber-heating, drainage, sanitary, auto mechanic, TV, Refrigerator, electrician, washing machine, TV antenna installer (satellite) and many others.
Methodology.
Engineers apply mathematics and sciences such as physics to find suitable solutions to problems or to make improvements to the status quo. More than ever, engineers are now required to have knowledge of relevant sciences for their design projects. As a result, they may keep on learning new material throughout their career.
If multiple options exist, engineers weigh different design choices on their merits and choose the solution that best matches the requirements. The crucial and unique task of the engineer is to identify, understand, and interpret the constraints on a design in order to produce a successful result. It is usually not enough to build a technically successful product; it must also meet further requirements.
Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.
A general methodology and epistemology of engineering can be inferred from the historical case studies and comments provided by Walter Vincenti. Though Vincenti's case studies are from the domain of aeronautical engineering, his conclusions can be transferred into many other branches of engineering, too.
According to Billy Vaughn Koen, the ""engineering method" is the use of heuristics to cause the best change in a poorly understood situation within the available resources." Koen argues that the definition of what makes one an engineer should not be based on what he produces, but rather how he goes about it.
Problem solving.
Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a problem. Creating an appropriate mathematical model of a problem allows them to analyze it (sometimes definitively), and to test potential solutions.
Usually multiple reasonable solutions exist, so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of "low-level" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.
Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.
Engineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure. However, the greater the safety factor, the less efficient the design may be.
The study of failed products is known as forensic engineering, and can help the product designer in evaluating his or her design in the light of real conditions. The discipline is of greatest value after disasters, such as bridge collapses, when careful analysis is needed to establish the cause or causes of the failure.
Computer use.
As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.
One of the most widely used design tools in the profession is computer-aided design (CAD) software like CATIA, Autodesk Inventor, DSS SolidWorks or Pro Engineer which enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.
These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.
There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and AEC software for civil engineering.
In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).
Social context.
The engineering profession engages in a wide range of activities, from large collaboration at the societal level, and also smaller individual projects. Almost all engineering projects are obligated to some sort of financing agency: a company, a set of investors, or a government. The few types of engineering that are minimally constrained by such issues are "pro bono" engineering and open-design engineering.
By its very nature engineering has interconnections with society, culture and human behavior. Every product or construction used by modern society is influenced by engineering. The results of engineering activity influence changes to the environment, society and economies, and its application brings with it a responsibility and public safety. Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large.
Engineering projects can be subject to controversy. Examples from different engineering disciplines include the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some western engineering companies have enacted serious corporate and social responsibility policies.
Engineering is a key driver of innovation and human development. Sub-Saharan Africa in particular has a very small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid. The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.
All overseas development and relief NGOs make considerable use of engineers to apply solutions in disaster and development scenarios. A number of charitable organizations aim to use engineering directly for the good of mankind:
Engineering companies in many established economies are facing significant challenges with regard to the number of professional engineers being trained, compared with the number retiring. This problem is very prominent in the UK where engineering has a poor image and low status. There are many negative economic and political issues that this can cause, as well as ethical issues It is widely agreed that the engineering profession faces an "image crisis", rather than it being fundamentally an unattractive career. Much work is needed to avoid huge problems in the UK and other western economies.
Relationships with other disciplines.
Science.
There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.
Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely "engineering scientists".
In the book "What Engineers Know and How They Know It", Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.
Examples are the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of Miner's rule to calculate fatigue damage. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.
As stated by Fung "et al." in the revision to the classic engineering text "Foundations of Solid Mechanics":
Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.
Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.
Medicine and biology.
The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.
Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers. The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.
Conversely, some engineering disciplines view the human body as a biological machine worth studying, and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.
Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.
Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.
The heart for example functions much like a pump, the skeleton is like a linked structure with levers, the brain produces electrical signals etc. These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.
Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.
Art.
There are connections between engineering and art;
they are direct in some fields, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering); and indirect in others.
The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design. Robert Maillart's bridge design is perceived by some to have been deliberately artistic. At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.
Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.
Business Engineering and Engineering Management.
Business Engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management is a specialized field of management concerned with the engineering sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to the engineering sector. This work often deals with large scale complex business transformation or Business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical & electronics, power distribution & generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.
Other fields.
In political science, the term "engineering" has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Financial engineering has similarly borrowed the term.

</doc>
<doc id="9252" url="https://en.wikipedia.org/wiki?curid=9252" title="Education">
Education

Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, beliefs, and habits. Educational methods include storytelling, discussion, teaching, training, and directed research. Education frequently takes place under the guidance of educators, but learners may also educate themselves. Education can take place in formal or informal settings and any experience that has a formative effect on the way one thinks, feels, or acts may be considered educational. The methodology of teaching is called pedagogy.
Education commonly is divided formally into such stages as preschool or kindergarten, primary school, secondary school and then college, university, or apprenticeship.
A right to education has been recognized by some governments, including at the global level: Article 13 of the United Nations' 1966 International Covenant on Economic, Social and Cultural Rights recognizes a universal right to education. In most regions education is compulsory up to a certain age.
Etymology.
Etymologically, the word "education" is derived from the Latin "ēducātiō" ("A breeding, a bringing up, a rearing") from "ēdūcō" ("I educate, I train") which is related to the homonym "ēdūcō" ("I lead forth, I take out; I raise up, I erect") from "ē-" ("from, out of") and "dūcō" ("I lead, I conduct").
History.
Education began in prehistory, as adults trained the young in the knowledge and skills deemed necessary in their society. In pre-literate societies this was achieved orally and through imitation. Story-telling passed knowledge, values, and skills from one generation to the next. As cultures began to extend their knowledge beyond skills that could be readily learned through imitation, formal education developed. Schools existed in Egypt at the time of the Middle Kingdom.
Plato founded the Academy in Athens, the first institution of higher learning in Europe. The city of Alexandria in Egypt, established in 330 BCE, became the successor to Athens as the intellectual cradle of Ancient Greece. There, the great Library of Alexandria was built in the 3rd century BCE. European civilizations suffered a collapse of literacy and organization following the fall of Rome in AD 476.
In China, Confucius (551-479 BCE), of the State of Lu, was the country's most influential ancient philosopher, whose educational outlook continues to influence the societies of China and neighbors like Korea, Japan and Vietnam. Confucius gathered disciples and searched in vain for a ruler who would adopt his ideals for good governance, but his Analects were written down by followers and have continued to influence education in East Asia into the modern era.
After the Fall of Rome, the Catholic Church became the sole preserver of literate scholarship in Western Europe. The church established cathedral schools in the Early Middle Ages as centers of advanced education. Some of these establishments ultimately evolved into medieval universities and forebears of many of Europe's modern universities. During the High Middle Ages, Chartres Cathedral operated the famous and influential Chartres Cathedral School. The medieval universities of Western Christendom were well-integrated across all of Western Europe, encouraged freedom of inquiry, and produced a great variety of fine scholars and natural philosophers, including Thomas Aquinas of the University of Naples, Robert Grosseteste of the University of Oxford, an early expositor of a systematic method of scientific experimentation, and Saint Albert the Great, a pioneer of biological field research. Founded in 1088, the University of Bologne is considered the first, and the oldest continually operating university.
Elsewhere during the Middle Ages, Islamic science and mathematics flourished under the Islamic caliphate which was established across the Middle East, extending from the Iberian Peninsula in the west to the Indus in the east and to the Almoravid Dynasty and Mali Empire in the south.
The Renaissance in Europe ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly. The European Age of Empires saw European ideas of education in philosophy, religion, arts and sciences spread out across the globe. Missionaries and scholars also brought back new ideas from other civilisations — as with the Jesuit China missions who played a significant role in the transmission of knowledge, science, and culture between China and Europe, translating works from Europe like Euclid's Elements for Chinese scholars and the thoughts of Confucius for European audiences. The Enlightenment saw the emergence of a more secular educational outlook in Europe.
In most countries today, full-time education, whether at school or otherwise, is compulsory for all children up to a certain age. Due to this the proliferation of compulsory education, combined with population growth, UNESCO has calculated that in the next 30 years more people will receive formal education than in all of human history thus far.
Formal education.
Formal education occurs in a structured environment whose explicit purpose is teaching students. Usually, formal education takes place in a school environment with classrooms of multiple students learning together with a trained, certified teacher of the subject. Most school systems are designed around a set of values or ideals that govern all educational choices in that system. Such choices include curriculum, organizational models, design of the physical learning spaces (e.g. classrooms), student-teacher interactions, methods of assessment, class size, educational activities, and more.
Preschool.
Preschools provide education from ages approximately three to seven, depending on the country, when children enter primary education. These are also known as nursery schools and as kindergarten, except in the US, where kindergarten is a term used for primary education. Kindergarten "providea child-centered, preschool curriculum for three- to seven-year-old children that aim[s at unfolding the child's physical, intellectual, and moral nature with balanced emphasis on each of them."
Primary.
Primary (or elementary) education consists of the first five to seven years of formal, structured education. In general, primary education consists of six to eight years of schooling starting at the age of five or six, although this varies between, and sometimes within, countries. Globally, around 89% of children aged six to twelve are enrolled in primary education, and this proportion is rising. Under the Education For All programs driven by UNESCO, most countries have committed to achieving universal enrollment in primary education by 2015, and in many countries, it is compulsory. The division between primary and secondary education is somewhat arbitrary, but it generally occurs at about eleven or twelve years of age. Some education systems have separate middle schools, with the transition to the final stage of secondary education taking place at around the age of fourteen. Schools that provide primary education, are mostly referred to as "primary schools "or "elementary schools". Primary schools are often subdivided into infant schools and junior school.
In India, for example, compulsory education spans over twelve years, with eight years of elementary education, five years of primary schooling and three years of upper primary schooling. Various states in the republic of India provide 12 years of compulsory school education based on a national curriculum framework designed by the National Council of Educational Research and Training.
Secondary.
In most contemporary educational systems of the world, secondary education comprises the formal education that occurs during adolescence. It is characterized by transition from the typically compulsory, comprehensive primary education for minors, to the optional, selective tertiary, "postsecondary", or "higher" education (e.g. university, vocational school) for adults. Depending on the system, schools for this period, or a part of it, may be called secondary or high schools, gymnasiums, lyceums, middle schools, colleges, or vocational schools. The exact meaning of any of these terms varies from one system to another. The exact boundary between primary and secondary education also varies from country to country and even within them, but is generally around the seventh to the tenth year of schooling. Secondary education occurs mainly during the teenage years. In the United States, Canada and Australia, primary and secondary education together are sometimes referred to as K-12 education, and in New Zealand Year 1–13 is used. The purpose of secondary education can be to give common knowledge, to prepare for higher education, or to train directly in a profession.
Secondary education in the United States did not emerge until 1910, with the rise of large corporations and advancing technology in factories, which required skilled workers. In order to meet this new job demand, high schools were created, with a curriculum focused on practical job skills that would better prepare students for white collar or skilled blue collar work. This proved beneficial for both employers and employees, since the improved human capital lowered costs for the employer, while skilled employees received a higher wages.
Secondary education has a longer history in Europe, where grammar schools or academies date from as early as the 16th century, in the form of public schools, fee-paying schools, or charitable educational foundations, which themselves date even further back.
Community colleges offer another option at this transitional stage of education. They provide nonresidential junior college courses to people living in a particular area.
Tertiary (higher).
Higher education, also called tertiary, third stage, or postsecondary education, is the non-compulsory educational level that follows the completion of a school such as a high school or secondary school. Tertiary education is normally taken to include undergraduate and postgraduate education, as well as vocational education and training. Colleges and universities mainly provide tertiary education. Collectively, these are sometimes known as tertiary institutions. Individuals who complete tertiary education generally receive certificates, diplomas, or academic degrees.
Higher education typically involves work towards a degree-level or foundation degree qualification. In most developed countries a high proportion of the population (up to 50%) now enter higher education at some time in their lives. Higher education is therefore very important to national economies, both as a significant industry in its own right, and as a source of trained and educated personnel for the rest of the economy.
University education includes teaching, research, and social services activities, and it includes both the undergraduate level (sometimes referred to as tertiary education) and the graduate (or postgraduate) level (sometimes referred to as graduate school). Universities are generally composed of several colleges. In the United States, universities can be private and independent like Yale University; public and state-governed like the Pennsylvania State System of Higher Education; or independent but state-funded like the University of Virginia. A number of career specific courses are now available to students through the Internet.
One type of university education is a liberal arts education, which can be defined as a "college or university curriculum aimed at imparting broad general knowledge and developing general intellectual capacities, in contrast to a professional, vocational, or technical curriculum." Although what is known today as liberal arts education began in Europe, the term "liberal arts college" is more commonly associated with institutions in the United States.
Vocational.
Vocational education is a form of education focused on direct and practical training for a specific trade or craft. Vocational education may come in the form of an apprenticeship or internship as well as institutions teaching courses such as carpentry, agriculture, engineering, medicine, architecture and the arts.
Special.
In the past, those who were disabled were often not eligible for public education. Children with disabilities were repeatedly denied an education by physicians or special tutors. These early physicians (people like Itard, Seguin, Howe, Gallaudet) set the foundation for special education today. They focused on individualized instruction and functional skills. In its early years, special education was only provided to people with severe disabilities, but more recently it has been opened to anyone who has experienced difficulty learning.
Other educational forms.
Alternative.
While considered "alternative" today, most alternative systems have existed since ancient times. After the public school system was widely developed beginning in the 19th century, some parents found reasons to be discontented with the new system. Alternative education developed in part as a reaction to perceived limitations and failings of traditional education. A broad range of educational approaches emerged, including alternative schools, self learning, homeschooling and unschooling. Example alternative schools include Montessori schools, Waldorf schools (or Steiner schools), Friends schools, Sands School, Summerhill School, The Peepal Grove School, Sudbury Valley School, Krishnamurti schools, and open classroom schools. Charter schools are another example of alternative education, which have in the recent years grown in numbers in the US and gained greater importance in its public education system.
In time, some ideas from these experiments and paradigm challenges may be adopted as the norm in education, just as Friedrich Fröbel's approach to early childhood education in 19th-century Germany has been incorporated into contemporary kindergarten classrooms. Other influential writers and thinkers have included the Swiss humanitarian Johann Heinrich Pestalozzi; the American transcendentalists Amos Bronson Alcott, Ralph Waldo Emerson, and Henry David Thoreau; the founders of progressive education, John Dewey and Francis Parker; and educational pioneers such as Maria Montessori and Rudolf Steiner, and more recently John Caldwell Holt, Paul Goodman, Frederick Mayer, George Dennison and Ivan Illich.
Indigenous.
Indigenous education refers to the inclusion of indigenous knowledge, models, methods, and content within formal and non-formal educational systems. Often in a post-colonial context, the growing recognition and use of indigenous education methods can be a response to the erosion and loss of indigenous knowledge and language through the processes of colonialism. Furthermore, it can enable indigenous communities to "reclaim and revalue their languages and cultures, and in so doing, improve the educational success of indigenous students."
Informal learning.
Informal learning is one of three forms of learning defined by the Organisation for Economic Co-operation and Development (OECD). Informal learning occurs in a variety of places, such as at home, work, and through daily interactions and shared relationships among members of society. For many learners this includes language acquisition, cultural norms and manners. Informal learning for young people is an ongoing process that also occurs in a variety of places, such as out of school time, in youth programs at community centers and media labs.
Informal learning usually takes place outside educational establishments, does not follow a specified curriculum and may originate accidentally, sporadically, in association with certain occasions, from changing practical requirements. It is not necessarily planned to be pedagogically conscious, systematic and according to subjects, but rather unconsciously incidental, holistically problem-related, and related to situation management and fitness for life. It is experienced directly in its "natural" function of everyday life and is often spontaneous.
The concept of 'education through recreation' was applied to childhood development in the 19th century. In the early 20th century, the concept was broadened to include young adults but the emphasis was on physical activities. L.P. Jacks, also an early proponent of lifelong learning, described education through recreation: ""A master in the art of living draws no sharp distinction between his work and his play, his labour and his leisure, his mind and his body, his education and his recreation. He hardly knows which is which. He simply pursues his vision of excellence through whatever he is doing and leaves others to determine whether he is working or playing. To himself he always seems to be doing both. Enough for him that he does it well."" Education through recreation is the opportunity to learn in a seamless fashion through all of life's activities. The concept has been revived by the University of Western Ontario to teach anatomy to medical students.
Self-directed learning.
Autodidacticism (also autodidactism) is a contemplative, absorbing process, of "learning on your own" or "by yourself", or as a self-teacher. Some autodidacts spend a great deal of time reviewing the resources of libraries and educational websites. One may become an autodidact at nearly any point in one's life. While some may have been informed in a conventional manner in a particular field, they may choose to inform themselves in other, often unrelated areas. Notable autodidacts include Abraham Lincoln (U.S. president), Srinivasa Ramanujan (mathematician), Michael Faraday (chemist and physicist), Charles Darwin (naturalist), Thomas Alva Edison (inventor), Tadao Ando (architect), George Bernard Shaw (playwright), Frank Zappa (composer, recording engineer, film director), and Leonardo da Vinci (engineer, scientist, mathematician).
Open education and electronic technology.
"Main articles: Open education" and "Educational technology"
In 2012, the modern use of electronic educational technology (also called e-learning) had grown at 14 times the rate of traditional learning. Open education is fast growing to become the dominant form of education, for many reasons such as its efficiency and results compared to traditional methods. Cost of education has been an issue throughout history, and a major political issue in most countries today. Online courses often can be more expensive than face-to-face classes. Out of 182 colleges surveyed in 2009 nearly half said tuition for online courses was higher than for campus based ones. Many large university institutions are now starting to offer free or almost free full courses such as Harvard, MIT and Berkeley teaming up to form edX. Other universities offering open education are Stanford, Princeton, Duke, Johns Hopkins, Edinburgh, U. Penn, U. Michigan, U. Virginia, U. Washington, and Caltech. It has been called the biggest change in the way we learn since the printing press. Despite favorable studies on effectiveness, many people may still desire to choose traditional campus education for social and cultural reasons.
The conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the Open University in the United Kingdom. Presently, many of the major open education sources offer their own form of certificate. Due to the popularity of open education, these new kind of academic certificates are gaining more respect and equal "academic value" to traditional degrees. Many open universities are working to have the ability to offer students standardized testing and traditional degrees and credentials.
A culture is beginning to form around distance learning for people who are looking to social connections enjoyed on traditional campuses. For example, students may create study groups, meetups and movements such as UnCollege.
Development goals.
Since 1909, the ratio of children in the developing world attending school has increased. Before then, a small minority of boys attended school. By the start of the 21st century, the majority of all children in most regions of the world attended school.
Universal Primary Education is one of the eight international Millennium Development Goals, towards which progress has been made in the past decade, though barriers still remain. Securing charitable funding from prospective donors is one particularly persistent problem. Researchers at the Overseas Development Institute have indicated that the main obstacles to funding for education include conflicting donor priorities, an immature aid architecture, and a lack of evidence and advocacy for the issue. Additionally, Transparency International has identified corruption in the education sector as a major stumbling block to achieving Universal Primary Education in Africa. Furthermore, demand in the developing world for improved educational access is not as high as foreigners have expected. Indigenous governments are reluctant to take on the ongoing costs involved. There is also economic pressure from some parents, who prefer their children to earn money in the short term rather than work towards the long-term benefits of education.
A study conducted by the UNESCO International Institute for Educational Planning indicates that stronger capacities in educational planning and management may have an important spill-over effect on the system as a whole. Sustainable capacity development requires complex interventions at the institutional, organizational and individual levels that could be based on some foundational principles:
Internationalization.
Nearly every country now has Universal Primary Education.
Similarities — in systems or even in ideas — that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program facilitates exchanges across European universities. The Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.
Education and technology in developing countries.
Technology plays an increasingly significant role in improving access to education for people living in impoverished areas and developing countries. Charities like One Laptop per Child are dedicated to providing infrastructures through which the disadvantaged may access educational materials.
The OLPC foundation, a group out of MIT Media Lab and supported by several major corporations, has a stated mission to develop a $100 laptop for delivering educational software. The laptops were widely available as of 2008. They are sold at cost or given away based on donations.
In Africa, the New Partnership for Africa's Development (NEPAD) has launched an "e-school program" to provide all 600,000 primary and high schools with computer equipment, learning materials and internet access within 10 years. An International Development Agency project called nabuur.com, started with the support of former American President Bill Clinton, uses the Internet to allow co-operation by individuals on issues of social development.
India is developing technologies that will bypass land-based telephone and Internet infrastructure to deliver distance learning directly to its students. In 2004, the Indian Space Research Organisation launched EDUSAT, a communications satellite providing access to educational materials that can reach more of the country's population at a greatly reduced cost.
Private vs public funding in developing countries.
Research into LCPS (low cost private schools) found that over 5 years to July 2013, debate around LCPSs to achieving Education for All (EFA) objectives was polarised and finding growing coverage in international policy. The polarisation was due to disputes around whether the schools are affordable for the poor, reach disadvantaged groups, provide quality education, support or undermine equality, and are financially sustainable.
The report examined the main challenges encountered by development organisations which support LCPSs. Surveys suggest these types of schools are expanding across Africa and Asia. This success is attributed to excess demand. These surveys found concern for:
The report showed some cases of successful voucher and subsidy programmes; evaluations of international support to the sector are not widespread. Addressing regulatory ineffectiveness is a key challenge. Emerging approaches stress the importance of understanding the political economy of the market for LCPS, specifically how relationships of power and accountability between users, government, and private providers can produce better education outcomes for the poor.
Educational theory.
Purpose of schools.
Individual purposes for pursuing education can vary. Understanding the goals and means of educational socialization processes may also differ according to the sociological paradigm used.
The early years of schooling generally focus around developing basic interpersonal communication and literacy skills. This lays a foundation for more complex skills and subjects. Later, education usually turns toward gaining the knowledge and skills needed to create value and establish a livelihood.
People also pursue education for its own sake to satisfy innate curiosity, out of interest in a specific subject or skill, or for overall personal development.
Education is often understood as a means of overcoming handicaps, achieving greater equality, and acquiring wealth and status for all (Sargent 1994). Education is also often perceived as a place where children can develop according to their unique needs and potentials, with the purpose of developing every individual to their full potential.
Some claim that there is education inequality because children did not exceed the education of their parents. This education inequality is then associated with income inequality. Although critical thinking is a goal of education, criticism and blame are often the unintended by products of our current educational process. Students often blame their teachers and their textbooks, despite the availability of libraries and the internet. When someone tries to improve education, the educational establishment itself occasionally showers the person with criticism rather than gratitude. Better by products of an educational system would be gratitude and determination.
Developed countries have people with more resources (housing, food, transportation, water and sewage treatment, hospitals, health care, libraries, books, media, schools, the internet, education, etc.) than most of the world's population. One merely needs to see through travel or the media how many people in the undeveloped countries live to sense this. However, one can also use economic data to gain some insight into this. Yet criticism and blame are common among people in the developed countries.
Gratitude for all these resources and the determination to develop oneself would be more productive than criticism and blame because the resources are readily available and because, if you blame others, there is no need for you to do something different tomorrow or for you to change and improve. Where there is a will, there is a way. People in developed countries have the will and the way to do many things that they want to do. They sometimes need more determination and will to improve and to educate themselves with the resources that are abundantly available. They occasionally need more gratitude for the resources they have, including their teachers and their textbooks. The entire internet is also available to supplement these teachers and textbooks.
Educational psychology.
Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Although the terms "educational psychology" and "school psychology" are often used interchangeably, researchers and theorists are likely to be identified as educational psychologists, whereas practitioners in schools or school-related settings are identified as school psychologists. Educational psychology is concerned with the processes of educational attainment in the general population and in sub-populations such as gifted children and those with specific disabilities.
Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. Educational psychology in turn informs a wide range of specialties within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks (Lucas, Blazek, & Raley, 2006).
The intelligence–education relationship.
Intelligence is an important factor in how the individual responds to education. Those who have higher intelligence tend to perform better at school and go on to higher levels of education. This effect is also observable in the opposite direction, in that education increases measurable intelligence. Studies have shown that while educational attainment is important in predicting intelligence in later life, intelligence at 53 is more closely correlated to intelligence at 8 years old than to educational attainment.
Learning modalities.
There has been much interest in learning modalities and styles over the last two decades. The most commonly employed learning modalities are:
Other commonly employed modalities include musical, interpersonal, verbal, logical, and intrapersonal.
Dunn and Dunn focused on identifying relevant stimuli that may influence learning and manipulating the school environment, at about the same time as Joseph Renzulli recommended varying teaching strategies. Howard Gardner identified a wide range of modalities in his Multiple Intelligences theories. The Myers-Briggs Type Indicator and Keirsey Temperament Sorter, based on the works of Jung, focus on understanding how people's personality affects the way they interact personally, and how this affects the way individuals respond to each other within the learning environment. The work of David Kolb and Anthony Gregorc's Type Delineator follows a similar but more simplified approach.
Some theories propose that all individuals benefit from a variety of learning modalities, while others suggest that individuals may have preferred learning styles, learning more easily through visual or kinesthetic experiences. A consequence of the latter theory is that effective teaching should present a variety of teaching methods which cover all three learning modalities so that different students have equal opportunities to learn in a way that is effective for them. Guy Claxton has questioned the extent that learning styles such as Visual, Auditory and Kinesthetic(VAK) are helpful, particularly as they can have a tendency to label children and therefore restrict learning. Recent research has argued "there is no adequate evidence base to justify incorporating learning styles assessments into general educational practice."
Philosophy.
As an academic field, philosophy of education is "the philosophical study of education and its problems (...) its central subject matter is education, and its methods are those of philosophy". "The philosophy of education may be either the philosophy of the process of education or the philosophy of the discipline of education. That is, it may be part of the discipline in the sense of being concerned with the aims, forms, methods, or results of the process of educating or being educated; or it may be metadisciplinary in the sense of being concerned with the concepts, aims, and methods of the discipline." As such, it is both part of the field of education and a field of applied philosophy, drawing from fields of metaphysics, epistemology, axiology and the philosophical approaches (speculative, prescriptive, and/or analytic) to address questions in and about pedagogy, education policy, and curriculum, as well as the process of learning, to name a few. For example, it might study what constitutes upbringing and education, the values and norms revealed through upbringing and educational practices, the limits and legitimization of education as an academic discipline, and the relation between education theory and practice.
Curriculum.
In formal education, a curriculum is the set of courses and their content offered at a school or university. As an idea, curriculum stems from the Latin word for "race course", referring to the course of deeds and experiences through which children grow to become mature adults. A curriculum is prescriptive, and is based on a more general syllabus which merely specifies what topics must be understood and to what level to achieve a particular grade or standard.
An academic discipline is a branch of knowledge which is formally taught, either at the university–or via some other such method. Each discipline usually has several sub-disciplines or branches, and distinguishing lines are often both arbitrary and ambiguous. Examples of broad areas of academic disciplines include the natural sciences, mathematics, computer science, social sciences, humanities and applied sciences.
Educational institutions may incorporate fine arts as part of K-12 grade curricula or within majors at colleges and universities as electives. The various types of fine arts are music, dance, and theater.
Instruction.
Instruction is the facilitation of another's learning. Instructors in primary and secondary institutions are often called teachers, and they direct the education of students and might draw on many subjects like reading, writing, mathematics, science and history. Instructors in post-secondary institutions might be called teachers, instructors, or professors, depending on the type of institution; and they primarily teach only their specific discipline. Studies from the United States suggest that the quality of teachers is the single most important factor affecting student performance, and that countries which score highly on international tests have multiple policies in place to ensure that the teachers they employ are as effective as possible. With the passing of NCLB in the United States (No Child Left Behind), teachers must be highly qualified. A popular way to gauge teaching performance is to use student evaluations of teachers (SETS), but these evaluations have been criticized for being counterproductive to learning and inaccurate due to student bias.
Economics of education.
It has been argued that high rates of education are essential for countries to be able to achieve high levels of economic growth. Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting edge technologies already tried and tested by rich countries. However, technology transfer requires knowledgeable managers and engineers who are able to operate new machines or production practices borrowed from the leader in order to close the gap through imitation. Therefore, a country's ability to learn from the leader is a function of its stock of "human capital". Recent study of the determinants of aggregate economic growth have stressed the importance of fundamental economic institutions and the role of cognitive skills.
At the level of the individual, there is a large literature, generally related to the work of Jacob Mincer, on how earnings are related to the schooling and other human capital. This work has motivated a large number of studies, but is also controversial. The chief controversies revolve around how to interpret the impact of schooling. Some students who have indicated a high potential for learning, by testing with a high intelligence quotient, may not achieve their full academic potential, due to financial difficulties.
Economists Samuel Bowles and Herbert Gintis argued in 1976 that there was a fundamental conflict in American schooling between the egalitarian goal of democratic participation and the inequalities implied by the continued profitability of capitalist production.

</doc>
<doc id="9253" url="https://en.wikipedia.org/wiki?curid=9253" title="Encyclopedia">
Encyclopedia

An encyclopedia or encyclopaedia (also spelled encyclopædia, see spelling differences) is a type of reference work or compendium holding a comprehensive summary of information from either all branches of knowledge or a particular branch of knowledge.
Encyclopedias are divided into articles or entries, which are usually accessed alphabetically by article name. Encyclopedia entries are longer and more detailed than those in most dictionaries. Generally speaking, unlike dictionary entries, which focus on linguistic information about words, encyclopedia articles focus on factual information concerning the subject for which the article is named.
Encyclopedias have existed for around 2,000 years; the oldest still in existence, "Naturalis Historia", was written starting in ca. AD 77 by Pliny the Elder and was not fully revised at the time of his death in AD 79. The modern encyclopedia evolved out of dictionaries around the 17th century. Historically, some encyclopedias were contained in one volume, whereas others, such as the "Encyclopædia Britannica", the "Enciclopedia Italiana" (62 volumes, 56,000 pages) or the world's largest, "Enciclopedia universal ilustrada europeo-americana" (118 volumes, 105,000 pages), became huge multi-volume works. Some modern encyclopedias, such as Wikipedia, are electronic and often freely available.
Etymology.
The word "encyclopedia" comes from the Koine Greek , transliterated "enkyklios paideia", meaning "general education" from "enkyklios" (ἐγκύκλιος), meaning "circular, recurrent, required regularly, general" and "paideia" (παιδεία), meaning "education, rearing of a child"; it was reduced to a single word due to an error by copyists of Latin manuscripts. Together, the phrase literally translates as "complete instruction" or "complete knowledge".
Copyists of Latin manuscripts took this phrase to be a single Greek word, "enkyklopaidia", with the same meaning, and this spurious Greek word became the New Latin word "encyclopaedia", which in turn came into English. Though the notion of a compendium of knowledge dates back thousands of years, the term was first used in the title of a book in 1517 by Johannes Aventinus: "Encyclopedia orbisque doctrinarum, hoc est omnium artium, scientiarum, ipsius philosophiae index ac divisio", and in 1538 by Joachimus Fortius Ringelbergius, "Lucubrationes vel potius absolutissima kyklopaideia" (Basel, 1538).
The word "encyclopaedia" was first used as a noun in the title of his book by the Croatian encyclopedist Pavao Skalić in his "Encyclopaedia seu orbis disciplinarum tam sacrarum quam prophanarum epistemon" (Encyclopaedia, or Knowledge of the World of Disciplines, Basel, 1559). One of the oldest vernacular uses was by François Rabelais in his "Pantagruel" in 1532. Several encyclopedias have names that include the suffix "-p(a)edia", e.g., Banglapedia (on matters relevant for Bengal).
In British usage, the spellings "encyclopedia" and "encyclopaedia" are both current.
In American usage, only the former is commonly used. The spelling "encyclopædia"—with the "æ" ligature—was frequently used in the 19th century and is increasingly rare, although it is retained in product titles such as "Encyclopædia Britannica" and others. The "Oxford English Dictionary" (1989) records "encyclopædia" and "encyclopaedia" as equal alternatives (in that order), and notes the "æ" would be obsolete except that it is preserved in works that have Latin titles. "Webster's Third New International Dictionary" (1997–2002) features "encyclopedia" as the main headword and "encyclopaedia" as a minor variant. In addition, "cyclopedia" and "cyclopaedia" are now rarely used shortened forms of the word originating in the 17th century.
Characteristics.
The modern encyclopedia was developed from the dictionary in the 18th century. Historically, both encyclopedias and dictionaries have been researched and written by well-educated, well-informed content experts, but they are significantly different in structure. A dictionary is a linguistic work which primarily focuses on alphabetical listing of words and their definitions. Synonymous words and those related by the subject matter are to be found scattered around the dictionary, giving no obvious place for in-depth treatment. Thus, a dictionary typically provides limited information, analysis or background for the word defined. While it may offer a definition, it may leave the reader lacking in understanding the meaning, significance or limitations of a term, and how the term relates to a broader field of knowledge. An encyclopedia is, allegedly, not written in order to convince, although one of its goals is indeed to convince its reader about its own veracity. In the terms of Aristotle's Modes of persuasion, a dictionary should persuade the reader through "logos" (conveying only appropriate emotions); it will be expected to have a lack of pathos (it should not stir up irrelevant emotions), and to have little ethos except that of the dictionary itself.
To address those needs, an encyclopedia article is typically not limited to simple definitions, and is not limited to defining an individual word, but provides a more extensive meaning for a "subject or discipline". In addition to defining and listing synonymous terms for the topic, the article is able to treat the topic's more extensive meaning in more depth and convey the most relevant accumulated knowledge on that subject. An encyclopedia article also often includes many maps and illustrations, as well as bibliography and statistics.
Four major elements define an encyclopedia: its subject matter, its scope, its method of organization, and its method of production:
Some works entitled "dictionaries" are actually similar to encyclopedias, especially those concerned with a particular field (such as the "Dictionary of the Middle Ages", the "Dictionary of American Naval Fighting Ships", and "Black's Law Dictionary"). The "Macquarie Dictionary," Australia's national dictionary, became an encyclopedic dictionary after its first edition in recognition of the use of proper nouns in common communication, and the words derived from such proper nouns.
There are some broad differences between encyclopedias and dictionaries. Most noticeably, encyclopedia articles are longer, fuller and more thorough than entries in most general-purpose dictionaries. There are differences in content as well. Generally speaking, dictionaries provide linguistic information about words themselves, while encyclopedias focus more on the thing for which those words stand. Thus, while dictionary entries are inextricably fixed to the word described, encyclopedia articles can be given a different entry name. As such, dictionary entries are not fully translatable into other languages, but encyclopedia articles can be.
In practice, however, the distinction is not concrete, as there is no clear-cut difference between factual, "encyclopedic" information and linguistic information such as appears in dictionaries. Thus encyclopedias may contain material that is also found in dictionaries, and vice versa. In particular, dictionary entries often contain factual information about the thing named by the word.
History.
Encyclopedias have progressed from the beginning of history in written form, through medieval and modern times in print, and most recently, displayed on computer and distributed via computer networks.
Ancient times.
One of the earliest encyclopedic works to have survived to modern times is the "Naturalis Historia" of Pliny the Elder, a Roman statesman living in the 1st century AD. He compiled a work of 37 chapters covering natural history, architecture, medicine, geography, geology, and all aspects of the world around him. He stated in the preface that he had compiled 20,000 facts from 2000 works by over 200 authors, and added many others from his own experience. The work was published around AD 77-79, although he probably never finished proofing the work before his death in the eruption of Vesuvius in AD 79.
Middle Ages.
Isidore of Seville, one of the greatest scholars of the early Middle Ages, is widely recognized for writing the first encyclopedia of the Middle Ages, the "Etymologiae" ("The Etymologies") or "Origines" (around 630), in which he compiled a sizable portion of the learning available at his time, both ancient and contemporary. The work has 448 chapters in 20 volumes, and is valuable because of the quotes and fragments of texts by other authors that would have been lost had he not collected them.
The most popular encyclopedia of the Carolingian Age was the "De universo" or "De rerum naturis" by Rabanus Maurus, written about 830; it was based on "Etymologiae".
The encyclopedia of Suda, a massive 10th-century Byzantine encyclopedia, had 30 000 entries, many drawing from ancient sources that have since been lost, and often derived from medieval Christian compilers. The text was arranged alphabetically with some slight deviations from common vowel order and place in the Greek alphabet.
The early Muslim compilations of knowledge in the Middle Ages included many comprehensive works. Around year 960, the Brethren of Purity of Basra were engaged in their Encyclopedia of the Brethren of Purity. Notable works include Abu Bakr al-Razi's encyclopedia of science, the Mutazilite Al-Kindi's prolific output of 270 books, and Ibn Sina's medical encyclopedia, which was a standard reference work for centuries. Also notable are works of universal history (or sociology) from Asharites, al-Tabri, al-Masudi, Tabari's "History of the Prophets and Kings", Ibn Rustah, al-Athir, and Ibn Khaldun, whose Muqadimmah contains cautions regarding trust in written records that remain wholly applicable today.
The enormous encyclopedic work in China of the "Four Great Books of Song", compiled by the 11th century AD during the early Song dynasty (960–1279), was a massive literary undertaking for the time. The last encyclopedia of the four, the "Prime Tortoise of the Record Bureau", amounted to 9.4 million Chinese characters in 1000 written volumes.
In the late Middle Ages, several authors had the ambition of compiling the sum of human knowledge in a certain field or overall, for example Bartholomew of England, Vincent of Beauvais, Radulfus Ardens, Sydrac, Brunetto Latini, Giovanni da Sangiminiano, Pierre Bersuire. Some were women, like Hildegard of Bingen and Herrad of Landsberg. The most successful of those publications were the "Speculum maius (Great Mirror)" of Vincent of Beauvais and the "De proprietatibus rerum (On the Properties of Things)" by Bartholomew of England. The latter was translated (or adapted) into French, Provençal, Italian, English, Flemish, Anglo-Norman, Spanish and German during the Middle Ages. Both were written in the middle of the 13th century. No medieval encyclopedia bore the title "Encyclopaedia" – they were often called "On nature (De natura, De naturis rerum)", "Mirror (Speculum maius, Speculum universale)", "Treasure (Trésor)".
Renaissance.
These works were all hand copied and thus rarely available, beyond wealthy patrons or monastic men of learning: they were expensive, and usually written for those extending knowledge rather than those using it.
During the Renaissance the creation of printing allowed a wider diffusion of encyclopedias and every scholar could have his or her own copy. The "De expetendis et fugiendis rebus" by Giorgio Valla was posthumously printed in 1501 by Aldo Manuzio in Venice. This work followed the traditional scheme of liberal arts. However, Valla added the translation of ancient Greek works on mathematics (firstly by Archimedes), newly discovered and translated. The "Margarita Philosophica" by Gregor Reisch, printed in 1503, was a complete encyclopedia explaining the seven liberal arts.
The term encyclopaedia was coined by 16th century humanists who misread copies of their texts of Pliny and Quintilian, and combined the two Greek words ""enkyklios paideia"" into one word, έγκυκλοπαιδεία. The phrase "enkyklios paideia" (ἐγκύκλιος παιδεία) was used by Plutarch and the Latin word Encyclopedia came from him.
The first work titled in this way was the "Encyclopedia orbisque doctrinarum, hoc est omnium artium, scientiarum, ipsius philosophiae index ac divisio" written by Johannes Aventinus in 1517.
The English physician and philosopher, Sir Thomas Browne used the word 'encyclopaedia' in 1646 in the preface to the reader to define his "Pseudodoxia Epidemica", a major work of the 17th-century scientific revolution. Browne structured his encyclopaedia upon the time-honoured schemata of the Renaissance, the so-called 'scale of creation' which ascends through the mineral, vegetable, animal, human, planetary and cosmological worlds. "Pseudodoxia Epidemica" was a European best-seller, translated into French, Dutch and German as well as Latin it went through no less than five editions, each revised and augmented, the last edition appearing in 1672.
18th–19th centuries.
The beginnings of the modern idea of the general-purpose, widely distributed printed encyclopedia precede the 18th century encyclopedists. However, Chambers' "Cyclopaedia, or Universal Dictionary of Arts and Sciences" (1728), and the "Encyclopédie" of Denis Diderot and Jean le Rond d'Alembert (1751 onwards), as well as "Encyclopædia Britannica" and the "Conversations-Lexikon", were the first to realize the form we would recognize today, with a comprehensive scope of topics, discussed in depth and organized in an accessible, systematic method. Chambers, in 1728, followed the earlier lead of John Harris's "Lexicon Technicum" of 1704 and later editions (see also below); this work was by its title and content "A Universal English Dictionary of Arts and Sciences: Explaining not only the Terms of Art, but the Arts Themselves".
During the 19th and early 20th century, many smaller or less developed languages saw their first encyclopedias, using French, German, and English role models. While encyclopedias in larger languages, having large markets that could support a large editorial staff, churned out new 20-volume works in a few years and new editions with brief intervals, such publication plans often spanned a decade or more in smaller languages.
20th century.
Popular and affordable encyclopedias such as Harmsworth's Universal Encyclopaedia and the Children's Encyclopaedia appeared in the early 1920s.
In the United States, the 1950s and 1960s saw the introduction of several large popular encyclopedias, often sold on installment plans. The best known of these were "World Book" and "Funk and Wagnalls".
The second half of the 20th century also saw the proliferation of specialized encyclopedias that compiled topics in specific fields. This trend has continued. Encyclopedias of at least one volume in size now exist for most if not all academic disciplines, including such narrow topics such as bioethics.
By the late 20th century, encyclopedias were being published on CD-ROMs for use with personal computers. Microsoft's "Encarta", launched in 1993, was a landmark example as it had no printed equivalent. Articles were supplemented with both video and audio files as well as numerous high-quality images. After sixteen years, Microsoft discontinued the Encarta line of products in 2009.
21st century.
In 2001, Jimmy Wales and Larry Sanger launched Wikipedia, a collaboratively edited, multilingual, open-source, free Internet encyclopedia supported by the non-profit Wikimedia Foundation. As of , there are articles in the English Wikipedia. There are 287 different editions of Wikipedia. As of February 2014, it had 18 billion page views and nearly 500 million unique visitors each month. Wikipedia has more than 25 million accounts, out of which there were over 118,000 active editors globally, as of August 2015. Wikipedia's accuracy was found by a "Nature" study to be close to that of Encyclopædia Britannica, with Wikipedia being much larger. However, critics argue Wikipedia exhibits systemic bias, and its group dynamics hinder its goals. Many academics, historians, teachers, and journalists reject Wikipedia as a reliable source of information, primarily for being a mixture of truths, half truths, and some falsehoods, and that as a resource about many controversial topics, is notoriously subject to manipulation and spin.
While Wikipedia is by far the largest web-based encyclopedia, it is not the only one in existence. There are several much smaller, usually more specialized, encyclopedias on various themes, sometimes dedicated to a specific geographic region or time period.

</doc>
<doc id="9256" url="https://en.wikipedia.org/wiki?curid=9256" title="Enigma machine">
Enigma machine

The Enigma machines were a series of electro-mechanical rotor cipher machines developed and used in the early- to mid-twentieth century to protect commercial, diplomatic and military communication. Enigma was invented by the German engineer Arthur Scherbius at the end of World War I. Early models were used commercially from the early 1920s, and adopted by military and government services of several countries, most notably Nazi Germany before and during World War II. Several different Enigma models were produced, but the German military models are the most commonly recognised. However, Japanese and Italian models have been used.
German military messages enciphered on the Enigma machine were first broken by the Polish Cipher Bureau, beginning in December 1932. This success was a result of efforts by three Polish cryptologists, Marian Rejewski, Jerzy Różycki and Henryk Zygalski, working for Polish military intelligence. Rejewski reverse-engineered the device, using theoretical mathematics and material supplied by French military intelligence. Subsequently the three mathematicians designed mechanical devices for breaking Enigma ciphers, including the cryptologic bomb. From 1938 onwards, additional complexity was repeatedly added to the Enigma machines, making decryption more difficult and requiring further equipment and personnel—more than the Poles could readily produce.
On 26 and 27 July 1939, in Pyry near Warsaw, the Poles initiated French and British military intelligence representatives into their Enigma-decryption techniques and equipment, including Zygalski sheets and the cryptologic bomb, and promised each delegation a Polish-reconstructed Enigma. The demonstration represented a vital basis for the later British continuation and effort. During the war, British cryptologists decrypted a vast number of messages enciphered on Enigma. The intelligence gleaned from this source, codenamed "Ultra" by the British, was a substantial aid to the Allied war effort.
Though Enigma had some cryptographic weaknesses, in practice it was German procedural flaws, operator mistakes, failure to systematically introduce changes in encipherment procedures, and Allied capture of key tables and hardware that, during the war, enabled Allied cryptologists to succeed and "turned the tide" in the Allies' favor.
Design.
Like other rotor machines, the Enigma machine is a combination of mechanical and electrical subsystems. The mechanical subsystem consists of a keyboard; a set of rotating disks called "rotors" arranged adjacently along a spindle; and one of various stepping components to turn at least one rotor with each key press.
Electrical pathway.
The mechanical parts act in such a way as to form a varying electrical circuit. When a key is pressed, one or more rotors move to form a new rotor configuration, and a circuit is completed. Current flows through various components in the new configuration, ultimately lighting one display lamp, which shows the output letter. For example, when encrypting a message starting "ANX...", the operator would first press the "A" key, and the "Z" lamp might light, so "Z" would be the first letter of the ciphertext. The operator would next press "N", and then "X" in the same fashion, and so on.
The detailed operation of Enigma is shown in the wiring diagram to the left. To simplify the example, only four components of a complete Enigma machine are shown. In reality, there are 26 lamps and keys, rotor wirings inside the rotors (of which there are either three or four) and between six and ten plug leads.
Current flowed from the battery (1) through a depressed bi-directional keyboard switch (2) to the plugboard (3). Next, it passed through the (unused in this instance, so shown closed) plug "A" (3) via the entry wheel (4), through the wiring of the three (Wehrmacht Enigma) or four ("Kriegsmarine" M4 and "Abwehr" variants) installed rotors (5), and entered the reflector (6). The reflector returned the current, via an entirely different path, back through the rotors (5) and entry wheel (4), proceeding through plug "S" (7) connected with a cable (8) to plug "D", and another bi-directional switch (9) to light the appropriate lamp.
The repeated changes of electrical path through an Enigma scrambler implemented a polyalphabetic substitution cipher that provided Enigma's security. The diagram on the right shows how the electrical pathway changed with each key depression, which caused rotation of at least the right-hand rotor. Current passed into the set of rotors, into and back out of the reflector, and out through the rotors again. The greyed-out lines are other possible paths within each rotor; these are hard-wired from one side of each rotor to the other. The letter "A" encrypts differently with consecutive key presses, first to "G", and then to "C". This is because the right-hand rotor has stepped, sending the signal on a completely different route. Eventually other rotors step with a key press.
Rotors.
The rotors (alternatively "wheels" or "drums", "Walzen" in German) formed the heart of an Enigma machine. Each rotor was a disc approximately in diameter made from hard rubber or bakelite with 26 brass, spring-loaded, electrical contact pins arranged in a circle on one face; the other side housing the corresponding number of circular plate electrical contacts. The pins and contacts represent the alphabet—typically the 26 letters A–Z (this will be assumed for the rest of this description). When the rotors were mounted side-by-side on the spindle, the pins of one rotor rested against the plate contacts of the neighbouring rotor, forming an electrical connection. Inside the body of the rotor, 26 wires connected each pin on one side to a contact on the other in a complex pattern. Most of the rotors were identified by Roman numerals, and each issued copy of rotor I was wired identically to all others. The same was true for the special thin beta and gamma rotors used in the M4 naval variant.
By itself, a rotor performs only a very simple type of encryption—a simple substitution cipher. For example, the pin corresponding to the letter "E" might be wired to the contact for letter "T" on the opposite face, and so on. Enigma's security came from using several rotors in series (usually three or four) and the regular stepping movement of the rotors, thus implementing a polyalphabetic substitution cipher.
When placed in an Enigma, each rotor can be set to one of 26 possible positions. When inserted, it can be turned by hand using the grooved finger-wheel, which protrudes from the internal Enigma cover when closed. So that the operator can know the rotor's position, each had an "alphabet tyre" (or letter ring) attached to the outside of the rotor disk, with 26 characters (typically letters); one of these could be seen through the window, thus indicating the rotational position of the rotor. In early models, the alphabet ring was fixed to the rotor disk. A later improvement was the ability to adjust the alphabet ring relative to the rotor disk. The position of the ring was known as the "Ringstellung" ("ring setting"), and was a part of the initial setting prior to an operating session. In modern terms it was a part of the initialization vector.
Each rotor contained a notch (or more than one) that controlled rotor stepping. In the military variants, the notches are located on the alphabet ring.
The Army and Air Force Enigmas were used with several rotors, initially three. On 15 December 1938, this changed to five, from which three were chosen for a given session. Rotors were marked with Roman numerals to distinguish them: I, II, III, IV and V, all with single notches located at different points on the alphabet ring. This variation was probably intended as a security measure, but ultimately allowed the Polish Clock Method and British Banburismus attacks.
The Naval version of the "Wehrmacht" Enigma had always been issued with more rotors than the other services: at first six, then seven, and finally eight. The additional rotors were marked VI, VII and VIII, all with different wiring, and had two notches, resulting in more frequent turnover. The four-rotor Naval Enigma (M4) machine accommodated an extra rotor in the same space as the three-rotor version. This was accomplished by replacing the original reflector with a thinner one and by adding a thin fourth rotor. That fourth rotor was one of two types, "Beta" or "Gamma", and never stepped, but could be manually set to any of 26 positions. One of the 26 made the machine perform identically to the three-rotor machine.
Stepping.
To avoid merely implementing a simple (and easily breakable) substitution cipher, every key press caused one or more rotors to step by one twenty-sixth of a full rotation, before the electrical connections were made. This changed the substitution alphabet used for encryption, ensuring that the cryptographic substitution was different at each new rotor position, producing a more formidable polyalphabetic substitution cipher. The stepping mechanism varied slightly from model to model. The right-hand rotor stepped once with each keystroke, and other rotors stepped less frequently.
Turnover.
The advancement of a rotor other than the left-hand one was called a "turnover" by the British. This was achieved by a ratchet and pawl mechanism. Each rotor had a ratchet with 26 teeth and every time a key was pressed, the set of spring-loaded pawls moved forward in unison, trying to engage with a ratchet. The alphabet ring of the rotor to the right normally prevented this. As this ring rotated with its rotor, a notch machined into it would eventually align itself with the pawl, allowing it to engage with the ratchet, and advance the rotor on its left. The right-hand pawl, having no rotor and ring to its right, stepped its rotor with every key depression. For a single-notch rotor in the right-hand position, the middle rotor stepped once for every 26 steps of the right-hand rotor. Similarly for rotors two and three. For a two-notch rotor, the rotor to its left would turn over twice for each rotation.
The first five rotors to be introduced (I–V) contained one notch each, while the additional naval rotors VI, VII and VIII each had two notches. The position of the notch on each rotor was determined by the letter ring which could be adjusted in relation to the core containing the interconnections. The points on the rings at which they caused the next wheel to move were as follows.
The design also included a feature known as "double-stepping". This occurred when each pawl aligned with both the ratchet of its rotor and the rotating notched ring of the neighbouring rotor. If a pawl engaged with a ratchet through alignment with a notch, as it moved forward it pushed against both the ratchet and the notch, advancing both rotors. In a three-rotor machine, double-stepping affected rotor two only. If in moving forward the ratchet of rotor three was engaged, rotor two would move again on the subsequent keystroke, resulting in two consecutive steps. Rotor two also pushes rotor one forward after 26 steps, but since rotor one moves forward with every keystroke anyway, there is no double-stepping. This double-stepping caused the rotors to deviate from odometer-style regular motion.
With three wheels and only single notches in the first and second wheels, the machine had a period of 26 × 25 × 26 = 16,900 (not 26 × 26 × 26, because of double-stepping). Historically, messages were limited to a few hundred letters, and so there was no chance of repeating any combined rotor position during a single session, denying cryptanalysts valuable clues.
To make room for the Naval fourth rotors, the reflector was made much thinner. The fourth rotor fitted into the space made available. No other changes were made, which eased the changeover. Since there were only three pawls, the fourth rotor never stepped, but could be manually set into one of 26 possible positions.
A device that was designed, but not implemented before the war's end, was the "Lückenfüllerwalze" (gap-fill wheel) that implemented irregular stepping. It allowed field configuration of notches in all 26 positions. If the number of notches was a relative prime of 26 and the number of notches were different for each wheel, the stepping would be more unpredictable. Like the Umkehrwalze-D it also allowed the internal wiring to be reconfigured.
Entry wheel.
The current entry wheel ("Eintrittswalze" in German), or entry stator, connects the plugboard to the rotor assembly. If the plugboard is not present, the entry wheel instead connects the keyboard and lampboard to the rotor assembly. While the exact wiring used is of comparatively little importance to security, it proved an obstacle to Rejewski's progress during his study of the rotor wirings. The commercial Enigma connects the keys in the order of their sequence on a QWERTZ keyboard: "Q"formula_1"A", "W"formula_1"B", "E"formula_1"C" and so on. However, the military Enigma connects them in straight alphabetical order: "A"formula_1"A", "B"formula_1"B", "C"formula_1"C", and so on. It took inspired guesswork for Rejewski to penetrate the modification.
Reflector.
With the exception of models "A" and "B", the last rotor came before a 'reflector' (German: "Umkehrwalze", meaning 'reversal rotor'), a patented feature unique to Enigma among the period's various rotor machines. The reflector connected outputs of the last rotor in pairs, redirecting current back through the rotors by a different route. The reflector ensured that Enigma is self-reciprocal: conveniently, encryption was the same as decryption. However, the reflector also gave Enigma the property that no letter ever encrypted to itself. This was a severe conceptual flaw and a cryptological mistake subsequently exploited by codebreakers.
In Model 'C', the reflector could be inserted in one of two different positions. In Model 'D', the reflector could be set in 26 possible positions, although it did not move during encryption. In the "Abwehr" Enigma, the reflector stepped during encryption in a manner similar to the other wheels.
In the German Army and Air Force Enigma, the reflector was fixed and did not rotate; there were four versions. The original version was marked 'A', and was replaced by "Umkehrwalze B" on 1 November 1937. A third version, "Umkehrwalze C" was used briefly in 1940, possibly by mistake, and was solved by Hut 6. The fourth version, first observed on 2 January 1944, had a rewireable reflector, called "Umkehrwalze D", allowing the Enigma operator to alter the connections as part of the key settings.
Plugboard.
The plugboard ("Steckerbrett" in German) permitted variable wiring that could be reconfigured by the operator (visible on the front panel of Figure 1; some of the patch cords can be seen in the lid). It was introduced on German Army versions in 1930, and was soon adopted by the "Reichsmarine" (German Navy). The plugboard contributed more cryptographic strength than an extra rotor. Enigma without a plugboard (known as "unsteckered Enigma") can be solved relatively straightforwardly using hand methods; these techniques are generally defeated by the plugboard, driving Allied cryptanalysts to develop special machines to solve it.
A cable placed onto the plugboard connected letters in pairs; for example, "E" and "Q" might be a steckered pair. The effect was to swap those letters before and after the main rotor scrambling unit. For example, when an operator presses "E", the signal was diverted to "Q" before entering the rotors. Up to 13 steckered pairs might be used at one time, although only 10 were normally used.
Current flowed from the keyboard through the plugboard, and proceeded to the entry-rotor or "Eintrittswalze". Each letter on the plugboard had two jacks. Inserting a plug disconnected the upper jack (from the keyboard) and the lower jack (to the entry-rotor) of that letter. The plug at the other end of the crosswired cable was inserted into another letter's jacks, thus switching the connections of the two letters.
Accessories.
Other features made various Enigma machines more secure or more convenient.
"Schreibmax".
Some M4 Enigmas used the "Schreibmax", a small printer that could print the 26 letters on a narrow paper ribbon. This eliminated the need for a second operator to read the lamps and transcribe the letters. The "Schreibmax" was placed on top of the Enigma machine and was connected to the lamp panel. To install the printer, the lamp cover and light bulbs had to be removed. It improved both convenience and operational security; the printer could be installed remotely such that the signal officer operating the machine no longer had to see the decrypted plaintext.
"Fernlesegerät".
Another accessory was the remote lamp panel "Fernlesegerät". For machines equipped with the extra panel, the wooden case of the Enigma was wider and could store the extra panel. A lamp panel version could be connected afterwards, but that required, as with the "Schreibmax", that the lamp panel and lightbulbs be removed. The remote panel made it possible for a person to read the decrypted plaintext without the operator seeing it.
"Uhr".
In 1944, the "Luftwaffe" introduced a plugboard switch, called the "Uhr" (clock), a small box containing a switch with 40 positions. It replaced the standard plugs. After connecting the plugs, as determined in the daily key sheet, the operator turned the switch into one of the 40 positions, each producing a different combination of plug wiring. Most of these plug connections were, unlike the default plugs, not pair-wise. In one switch position, the "Uhr" did not swap letters, but simply emulated the 13 stecker wires with plugs.
Mathematical analysis.
The Enigma transformation for each letter can be specified mathematically as a product of permutations. Assuming a three-rotor German Army/Air Force Enigma, let formula_7 denote the plugboard transformation, formula_8 denote that of the reflector, and formula_9 denote those of the left, middle and right rotors respectively. Then the encryption formula_10 can be expressed as
After each key press, the rotors turn, changing the transformation. For example, if the right-hand rotor formula_12 is rotated formula_13 positions, the transformation becomes formula_14, where formula_15 is the cyclic permutation mapping "A" to "B", "B" to "C", and so forth. Similarly, the middle and left-hand rotors can be represented as formula_16 and formula_17 rotations of formula_18 and formula_19. The encryption transformation can then be described as
Combining three rotors from a set of five, the rotor settings with 26 positions, and the plugboard with ten pairs of letters connected, the military Enigma has 158,962,555,217,826,360,000 (nearly 159 quintillion) different settings.
Operation.
Basic operation.
A German Enigma operator would be given a plaintext message to encrypt. For each letter typed in, a lamp indicated a different letter according to a pseudo-random substitution, based upon the wiring of the machine. The letter indicated by the lamp would be recorded as the enciphered substitution. The action of pressing a key also moved the rotor so that the next key press used a different electrical pathway, and thus a different substitution would occur. For each key press there was rotation of at least the right hand rotor, giving a different substitution alphabet. This continued for each letter in the message until the message was completed and a series of substitutions, each different from the others, had occurred to create a cyphertext from the plaintext. The cyphertext would then be transmitted as normal to an operator of another Enigma machine. This operator would key in the cyphertext and—as long as all the settings of the deciphering machine were identical to those of the enciphering machine—for every key press the reverse substitution would occur and the plaintext message would emerge.
Details.
In use, the Enigma required a list of daily key settings and auxiliary documents. The procedures for German Naval Enigma were more elaborate and more secure than those in other services. Navy codebooks were printed in red, water-soluble ink on pink paper so that they could easily be destroyed if they were endangered.
In German military practice, communications were divided into separate networks, each using different settings. These communication nets were termed "keys" at Bletchley Park, and were assigned code names, such as "Red", "Chaffinch", and "Shark". Each unit operating in a network was assigned a settings list for its Enigma for a period of time. For a message to be correctly encrypted and decrypted, both sender and receiver had to configure their Enigma in the same way; rotor selection and order, starting position and plugboard connections must be identical. All these settings (together the key in modern terms) were established beforehand, distributed in codebooks.
An Enigma machine's initial state, the cryptographic key, has several aspects:
For example, the settings for the 18th day of the month in the German Luftwaffe Enigma key list number 649 (see image) were as follows:
Enigma was designed to be secure even if the rotor wiring was known to an opponent, although in practice considerable effort protected the wiring configuration. If the wiring is secret, the total number of possible configurations has been calculated to be around 10114 (approximately 380 bits); with known wiring and other operational constraints, this is reduced to around 1023 (76 bits). Users of Enigma were confident of its security because of the large number of possibilities; it was not then feasible for an adversary to even begin to try a brute force attack.
Indicator.
Most of the key was kept constant for a set time period, typically a day. However, a different initial rotor position was used for each message, a concept similar to an initialisation vector in modern cryptography. The reason is that encrypting many messages with identical or near-identical settings (termed in cryptanalysis as being "in depth"), would enable an attack using a statistical procedure such as Friedman's Index of coincidence. The starting position for the rotors was transmitted just before the ciphertext, usually after having been enciphered. The exact method used was termed the "indicator procedure". Design weakness and operator sloppiness in these indicator procedures were two of the main weaknesses that made cracking Enigma possible.
One of the earliest "indicator procedures" was used by Polish cryptanalysts to make the initial breaks into the Enigma. The procedure was for the operator to set up his machine in accordance with his settings list, which included a global initial position for the rotors (the "Grundstellung", meaning "ground setting"), say, "AOH". The operator turned his rotors until "AOH" was visible through the rotor windows. At that point, the operator chose his own arbitrary starting position for that particular message. An operator might select "EIN", and these became the "message settings" for that encryption session. The operator then typed "EIN" into the machine, twice, to allow for detection of transmission errors. The results were an encrypted indicator—the "EIN" typed twice might turn into "XHTLOA", which would be transmitted along with the message. Finally, the operator then spun the rotors to his message settings, "EIN" in this example, and typed the plaintext of the message.
At the receiving end, the operation was reversed. The operator set the machine to the initial settings and typed in the first six letters of the message ("XHTLOA"). In this example, "EINEIN" emerged on the lamps. After moving his rotors to "EIN", the receiving operator then typed in the rest of the ciphertext, deciphering the message.
The weakness in this indicator scheme came from two factors. First, use of a global ground setting—this was later changed so the operator selected his initial position to encrypt the indicator, and sent the initial position in the clear. The second problem was the repetition of the indicator, which was a serious security flaw. The message setting was encoded twice, resulting in a relation between first and fourth, second and fifth, and third and sixth character. This security problem enabled the Polish Cipher Bureau to break into the pre-war Enigma system as early as 1932. However, from 1940 on, the Germans changed procedure.
During World War II, codebooks were only used each day to set up the rotors, their ring settings and the plugboard. For each message, the operator selected a random start position, let's say "WZA", and a random message key, perhaps "SXT". He moved the rotors to the "WZA" start position and encoded the message key "SXT". Assume the result was "UHL". He then set up the message key, "SXT", as the start position and encrypted the message. Next, he transmitted the start position, "WZA", the encoded message key, "UHL", and then the ciphertext. The receiver set up the start position according to the first trigram, "WZA", and decoded the second trigram, "UHL", to obtain the "SXT" message setting. Next, he used this "SXT" message setting as the start position to decrypt the message. This way, each ground setting was different and the new procedure avoided the security flaw of double encoded message settings.
This procedure was used by "Wehrmacht" and "Luftwaffe" only. The "Kriegsmarine" procedures on sending messages with the Enigma were far more complex and elaborate. Prior to encryption the message was encoded using the "Kurzsignalheft" code book. The "Kurzsignalheft" contained tables to convert sentences into four-letter groups. A great many choices were included, for example, logistic matters such as refuelling and rendezvous with supply ships, positions and grid lists, harbour names, countries, weapons, weather conditions, enemy positions and ships, date and time tables. Another codebook contained the "Kenngruppen" and "Spruchschlüssel": the key identification and message key.
Additional details.
The Army Enigma machine used only the 26 alphabet characters. Punctuation was replaced with rare character combinations. A space was omitted or replaced with an X. The X was generally used as period or full-stop.
Some punctuation marks were different in other parts of the armed forces. The "Wehrmacht" replaced a comma with ZZ and the question mark with FRAGE or FRAQ.
The "Kriegsmarine" replaced the comma with Y and the question mark with UD. The combination CH, as in ""Acht"" (eight) or ""Richtung"" (direction), was replaced with Q (AQT, RIQTUNG). Two, three and four zeros were replaced with CENTA, MILLE and MYRIA.
The "Wehrmacht" and the "Luftwaffe" transmitted messages in groups of five characters.
The "Kriegsmarine", using the four rotor Enigma, had four-character groups. Frequently used names or words were varied as much as possible. Words like "Minensuchboot" (minesweeper) could be written as MINENSUCHBOOT, MINBOOT, MMMBOOT or MMM354. To make cryptanalysis harder, messages were limited to 250 characters. Longer messages were divided into several parts, each using a different message key.
History.
The Enigma family included multiple designs. The earliest were commercial models dating from the early 1920s. Starting in the mid-1920s, the German military began to use Enigma, making a number of security-related changes. Various nations either adopted or adapted the design for their own cipher machines.
An estimated 100,000 Enigma machines were constructed. After the end of World War II, the Allies sold captured Enigma machines, still widely considered secure, to developing countries.
Commercial Enigma.
On 23 February 1918, German engineer Arthur Scherbius applied for a patent for a cipher machine using rotors and, with E. Richard Ritter, founded the firm of Scherbius & Ritter. They approached the "Kaiserliche Marine", (German Navy) and Foreign Office with their design, but neither was interested. Scherbius & Ritter then assigned the patent rights to Gewerkschaft Securitas, who founded the "Chiffriermaschinen Aktien-Gesellschaft" (Cipher Machines Stock Corporation) on 9 July 1923; Scherbius and Ritter were on the board of directors.
Enigma model A (1923).
Chiffriermaschinen AG began advertising a rotor machine—"Enigma model A"—which was exhibited at the Congress of the International Postal Union in 1924. The machine was heavy and bulky, incorporating a typewriter. It measured 65×45×35 cm and weighed about .
In 1925 Enigma "model B" was introduced, and was of a similar construction. While bearing the Enigma name, both models "A" and "B" were quite unlike later versions: they differed in physical size and shape, but also cryptographically, in that they lacked the reflector.
Enigma C (1926).
The reflector—suggested by Scherbius's colleague Willi Korn—was introduced in "Enigma C" (1926).
"Model C" was smaller and more portable than its predecessors. It lacked a typewriter, relying on the operator; hence the informal name of "glowlamp Enigma" to distinguish it from models "A" and "B".
Enigma D (1927).
The "Enigma C" quickly gave way to "Enigma D" (1927). This version was widely used, with shipments to Sweden, the Netherlands, United Kingdom, Japan, Italy, Spain, United States and Poland. In 1927 Hugh Foss at the British Government Code and Cypher School was able to show that commercial Enigma machines could be broken provided that suitable cribs were available.
"Navy Cipher D" – Italian Navy.
Other countries used Enigma machines. The Italian Navy adopted the commercial Enigma as "Navy Cipher D". The Spanish also used commercial Enigma during their Civil War. British codebreakers succeeded in breaking these machines, which lacked a plugboard. Enigma were also used by diplomatic services.
Swiss K.
The Swiss used a version of Enigma called "model K" or "Swiss K" for military and diplomatic use, which was very similar to commercial Enigma D. The machine was cracked by Poland, France, the United Kingdom and the United States (the latter codenamed it INDIGO). An "Enigma T" model (codenamed "Tirpitz") was used by Japan.
Military Enigma.
Funkschlüssel C.
The Reichsmarine was the first military branch to adopt Enigma. This version, named "Funkschlüssel C" ("Radio cipher C"), had been put into production by 1925 and was introduced into service in 1926.
The keyboard and lampboard contained 29 letters—A-Z, Ä, Ö and Ü—which were arranged alphabetically, as opposed to the QWERTZUI ordering. The rotors had 28 contacts, with the letter "X" wired to bypass the rotors unencrypted.
Three rotors were chosen from a set of five and the reflector could be inserted in one of four different positions, denoted α, β, γ and δ. The machine was revised slightly in July 1933.
Enigma G (1928–1930).
By 15 July 1928, the German Army ("Reichswehr") had introduced their own exclusive version of the Enigma machine; the "Enigma G".
The "Abwehr" used the "Enigma G" (the "Abwehr" Enigma). This Enigma variant was a four-wheel unsteckered machine with multiple notches on the rotors. This model was equipped with a counter which incremented upon each key press, and so is also known as the "counter machine" or the "Zählwerk" Enigma.
Wehrmacht Enigma I (1930–1938).
Enigma machine G was modified to the "Enigma I" by June 1930. Enigma I is also known as the "Wehrmacht", or "Services" Enigma, and was used extensively by German military services and other government organisations (such as the railways) before and during World War II.
The major difference between "Enigma I", (German Army version from 1930), and commercial Enigma models was the addition of a plugboard to swap pairs of letters, greatly increasing cryptographic strength.
Other differences included the use of a fixed reflector and the relocation of the stepping notches from the rotor body to the movable letter rings. The machine measured and weighed around .
In August 1935, the Air Force introduced the Wehrmacht Enigma for their communications.
M3, (1934).
By 1930, the Reichswehr had suggested that the Navy adopt their machine, citing the benefits of increased security (with the plugboard) and easier interservice communications. The Reichsmarine eventually agreed and in 1934 brought into service the Navy version of the Army Enigma, designated "Funkschlüssel" ' or "M3". While the Army used only three rotors at that time, the Navy specified a choice of three from a possible five.
Two extra rotors (1938).
In December 1938, the Army issued two extra rotors so that the three rotors were chosen from a set of five. In 1938, the Navy added two more rotors, and then another in 1939 to allow a choice of three rotors from a set of eight.
M4 (1942).
A four-rotor Enigma was introduced by the Navy for U-boat traffic on 1 February 1942, called "M4" (the network was known as "Triton", or "Shark" to the Allies). The extra rotor was fitted in the same space by splitting the reflector into a combination of a thin reflector and a thin fourth rotor.
Enigma II.
There was also a large, eight-rotor printing model, the "Enigma II". In 1933 the Polish Cipher Bureau detected that it was in use for high-level military communications, but that it was soon withdrawn, as it was unreliable and jammed frequently.
Surviving machines.
The effort to break the Enigma was not disclosed until the 1970s. Since then, interest in the Enigma machine has grown. Enigmas are on public display in museums around the world, and several are in the hands of private collectors and computer history enthusiasts.
The "Deutsches Museum" in Munich has both the three- and four-rotor German military variants, as well as several civilian versions. Enigma machines are exhibited at the National Codes Centre in Bletchley Park, the Government Communications Headquarters, the Science Museum in London, the Polish Army Museum in Warsaw, the Swedish Army Museum ("Armémuseum") in Stockholm, the Nordland Red Cross War Memorial Museum in Narvik, Norway, The Artillery, Engineers and Signals Museum in Hämeenlinna, Finland the Technical University of Denmark in Lyngby, Denmark, and at the Australian War Memorial and in the foyer of the Defence Signals Directorate, both in Canberra, Australia. The Jozef Pilsudski Institute in London exhibits a rare Polish Enigma double assembled in France in1940.
In the United States, Enigma machines can be seen at the Computer History Museum in Mountain View, California, and at the National Security Agency's National Cryptologic Museum in Fort Meade, Maryland, where visitors can try their hand at enciphering and deciphering messages. Two machines that were acquired after the capture of during World War II are on display at the Museum of Science and Industry in Chicago, Illinois. A four rotor device is on display in the ANZUS Corridor of the Pentagon on the second floor, A ring, between corridors 9 and 10. This machine is on loan from Australia. The United States Air Force Academy in Colorado Springs has a machine on display in the Computer Science Department. There's also a machine located at the National World War II Museum in New Orleans. The Museum of World War II in Boston has seven Enigma machines on display, including a U-Boat four-rotor model, one of three surviving examples of an Enigma machine with a printer, one of fewer than ten surviving ten-rotor code machines, an example blown up by a retreating German Army unit, and two three-rotor Enigmas that visitors can operate to encode and decode messages themselves.
In Canada, a Swiss Army issue Enigma-K, is in Calgary, Alberta. It is on permanent display at the Naval Museum of Alberta inside the Military Museums of Calgary. A 3-rotor Enigma machine is on display at the Military Communications and Electronics Museum at Canadian Forces Base (CFB) Kingston in Kingston, Ontario.
Occasionally, Enigma machines are sold at auction; prices have in recent years ranged from US$40,000 to US$203,000 in 2011. Replicas are available in various forms, including an exact reconstructed copy of the Naval M4 model, an Enigma implemented in electronics (Enigma-E), various simulators and paper-and-scissors analogues.
A rare "Abwehr" Enigma machine, designated G312, was stolen from the Bletchley Park museum on 1 April 2000. In September, a man identifying himself as "The Master" sent a note demanding £25,000 and threatening to destroy the machine if the ransom was not paid. In early October 2000, Bletchley Park officials announced that they would pay the ransom, but the stated deadline passed with no word from the blackmailer. Shortly afterward, the machine was sent anonymously to BBC journalist Jeremy Paxman, missing three rotors.
In November 2000, an antiques dealer named Dennis Yates was arrested after telephoning "The Sunday Times" to arrange the return of the missing parts. The Enigma machine was returned to Bletchley Park after the incident. In October 2001, Yates was sentenced to 10 months in prison and served three months.
In October 2008, the Spanish daily newspaper "El País" reported that 28 Enigma machines had been discovered by chance in an attic of Army headquarters in Madrid. These 4-rotor commercial machines had helped Franco's Nationalists win the Spanish Civil War because, though the British cryptologist Alfred Dilwyn Knox in 1937 broke the cipher generated by Franco's Enigma machines, this was not disclosed to the Republicans, who failed to break the cipher. The Nationalist government continued using its 50 Enigmas into the 1950s. Some machines have gone on display in Spanish military museums, including one at the National Museum of Science and Technology (MUNCYT) in La Coruña. Two have been given to Britain's GCHQ.
The Bulgarian military used Enigma machines with a Cyrillic keyboard; one is on display in the National Museum of Military History in Sofia.
Derivatives.
The Enigma was influential in the field of cipher machine design, spinning off other rotor machines. The British Typex was originally derived from the Enigma patents; Typex even includes features from the patent descriptions that were omitted from the actual Enigma machine. The British paid no royalties for the use of the patents, to protect secrecy. The Typex implementation is not the same as that found in German or other Axis versions.
A Japanese Enigma clone was codenamed GREEN by American cryptographers. Little used, it contained four rotors mounted vertically. In the U.S., cryptologist William Friedman designed the M-325, a machine logically similar, although not in construction.
A unique rotor machine was constructed in 2002 by Netherlands-based Tatjana van Vark. This device makes use of 40-point rotors, allowing letters, numbers and some punctuation to be used; each rotor contains 509 parts.
Machines like the SIGABA, NEMA, Typex and so forth, are deliberately not considered to be Enigma derivatives as their internal ciphering functions are not mathematically identical to the Enigma transform.
Several software implementations exist, but not all exactly match Enigma behaviour. The most commonly used software derivative (that is not compliant with any hardware implementation of the Enigma) is at EnigmaCo.de. Many Java applet Enigmas only accept single letter entry, complicating use even if the applet is Enigma compliant. Technically, Enigma@home is the largest scale deployment of a software Enigma, but the decoding software does not implement encipherment making it a derivative (as all original machines could cipher and decipher).
A user-friendly 3-rotor simulator, where users can select rotors, use the plugboard and define new settings for the rotors and reflectors is available. The output appears in separate windows which can be independently made "invisible" to hide decryption. Another includes an "autotyping" function which takes plaintext from a clipboard and converts it to cyphertext (or vice versa) at one of four speeds. The "very fast" option produces 26 characters in less than one second.

</doc>
<doc id="9257" url="https://en.wikipedia.org/wiki?curid=9257" title="Enzyme">
Enzyme

Enzymes are macromolecular biological catalysts. Enzymes accelerate, or catalyze, chemical reactions. The molecules at the beginning of the process are called substrates and the enzyme converts these into different molecules, called products. Almost all metabolic processes in the cell need enzymes in order to occur at rates fast enough to sustain life. The set of enzymes made in a cell determines which metabolic pathways occur in that cell. The study of enzymes is called "enzymology".
Enzymes are known to catalyze more than 5,000 biochemical reaction types. Most enzymes are proteins, although a few are catalytic RNA molecules. Enzymes' specificity comes from their unique three-dimensional structures.
Like all catalysts, enzymes increase the rate of a reaction by lowering its activation energy. Some enzymes can make their conversion of substrate to product occur many millions of times faster. An extreme example is orotidine 5'-phosphate decarboxylase, which allows a reaction that would otherwise take millions of years to occur in milliseconds. Chemically, enzymes are like any catalyst and are not consumed in chemical reactions, nor do they alter the equilibrium of a reaction. Enzymes differ from most other catalysts by being much more specific. Enzyme activity can be affected by other molecules: inhibitors are molecules that decrease enzyme activity, and activators are molecules that increase activity. Many drugs and poisons are enzyme inhibitors. An enzyme's activity decreases markedly outside its optimal temperature and pH.
Some enzymes are used commercially, for example, in the synthesis of antibiotics. Some household products use enzymes to speed up chemical reactions: enzymes in biological washing powders break down protein, starch or fat stains on clothes, and enzymes in meat tenderizer break down proteins into smaller molecules, making the meat easier to chew.
Etymology and history.
By the late 17th and early 18th centuries, the digestion of meat by stomach secretions and the conversion of starch to sugars by plant extracts and saliva were known but the mechanisms by which these occurred had not been identified.
French chemist Anselme Payen was the first to discover an enzyme, diastase, in 1833. A few decades later, when studying the fermentation of sugar to alcohol by yeast, Louis Pasteur concluded that this fermentation was caused by a vital force contained within the yeast cells called "ferments", which were thought to function only within living organisms. He wrote that "alcoholic fermentation is an act correlated with the life and organization of the yeast cells, not with the death or putrefaction of the cells."
In 1877, German physiologist Wilhelm Kühne (1837–1900) first used the term "enzyme", which comes from Greek ἔνζυμον, "leavened", to describe this process. The word "enzyme" was used later to refer to nonliving substances such as pepsin, and the word "ferment" was used to refer to chemical activity produced by living organisms.
Eduard Buchner submitted his first paper on the study of yeast extracts in 1897. In a series of experiments at the University of Berlin, he found that sugar was fermented by yeast extracts even when there were no living yeast cells in the mixture. He named the enzyme that brought about the fermentation of sucrose "zymase". In 1907, he received the Nobel Prize in Chemistry for "his discovery of cell-free fermentation". Following Buchner's example, enzymes are usually named according to the reaction they carry out: the suffix "-ase" is combined with the name of the substrate (e.g., lactase is the enzyme that cleaves lactose) or to the type of reaction (e.g., DNA polymerase forms DNA polymers).
The biochemical identity of enzymes was still unknown in the early 1900s. Many scientists observed that enzymatic activity was associated with proteins, but others (such as Nobel laureate Richard Willstätter) argued that proteins were merely carriers for the true enzymes and that proteins "per se" were incapable of catalysis. In 1926, James B. Sumner showed that the enzyme urease was a pure protein and crystallized it; he did likewise for the enzyme catalase in 1937. The conclusion that pure proteins can be enzymes was definitively demonstrated by John Howard Northrop and Wendell Meredith Stanley, who worked on the digestive enzymes pepsin (1930), trypsin and chymotrypsin. These three scientists were awarded the 1946 Nobel Prize in Chemistry.
The discovery that enzymes could be crystallized eventually allowed their structures to be solved by x-ray crystallography. This was first done for lysozyme, an enzyme found in tears, saliva and egg whites that digests the coating of some bacteria; the structure was solved by a group led by David Chilton Phillips and published in 1965. This high-resolution structure of lysozyme marked the beginning of the field of structural biology and the effort to understand how enzymes work at an atomic level of detail.
Structure.
Enzymes are generally globular proteins, acting alone or in larger complexes. Like all proteins, enzymes are linear chains of amino acids that fold to produce a three-dimensional structure. The sequence of the amino acids specifies the structure which in turn determines the catalytic activity of the enzyme. Although structure determines function, a novel enzyme's activity cannot yet be predicted from its structure alone. Enzyme structures unfold (denature) when heated or exposed to chemical denaturants and this disruption to the structure typically causes a loss of activity. Enzyme denaturation is normally linked to temperatures above a species' normal level; as a result, enzymes from bacteria living in volcanic environments such as hot springs are prized by industrial users for their ability to function at high temperatures, allowing enzyme-catalysed reactions to be operated at a very high rate.
Enzymes are usually much larger than their substrates. Sizes range from just 62 amino acid residues, for the monomer of 4-oxalocrotonate tautomerase, to over 2,500 residues in the animal fatty acid synthase. Only a small portion of their structure (around 2–4 amino acids) is directly involved in catalysis: the catalytic site. This catalytic site is located next to one or more binding sites where residues orient the substrates. The catalytic site and binding site together comprise the enzyme's active site. The remaining majority of the enzyme structure serves to maintain the precise orientation and dynamics of the active site.
In some enzymes, no amino acids are directly involved in catalysis; instead, the enzyme contains sites to bind and orient catalytic cofactors. Enzyme structures may also contain allosteric sites where the binding of a small molecule causes a conformational change that increases or decreases activity.
A small number of RNA-based biological catalysts called ribozymes exist, which again can act alone or in complex with proteins. The most common of these is the ribosome which is a complex of protein and catalytic RNA components.
Mechanism.
Substrate binding.
Enzymes must bind their substrates before they can catalyse any chemical reaction. Enzymes are usually very specific as to what substrates they bind and then the chemical reaction catalysed. Specificity is achieved by binding pockets with complementary shape, charge and hydrophilic/hydrophobic characteristics to the substrates. Enzymes can therefore distinguish between very similar substrate molecules to be chemoselective, regioselective and stereospecific.
Some of the enzymes showing the highest specificity and accuracy are involved in the copying and expression of the genome. Some of these enzymes have "proof-reading" mechanisms. Here, an enzyme such as DNA polymerase catalyzes a reaction in a first step and then checks that the product is correct in a second step. This two-step process results in average error rates of less than 1 error in 100 million reactions in high-fidelity mammalian polymerases. Similar proofreading mechanisms are also found in RNA polymerase, aminoacyl tRNA synthetases and ribosomes.
Conversely, some enzymes display enzyme promiscuity, having broad specificity and acting on a range of different physiologically relevant substrates. Many enzymes possess small side activities which arose fortuitously (i.e. neutrally), which may be the starting point for the evolutionary selection of a new function.
"Lock and key" model.
To explain the observed specificity of enzymes, in 1894 Emil Fischer proposed that both the enzyme and the substrate possess specific complementary geometric shapes that fit exactly into one another. This is often referred to as "the lock and key" model. This early model explains enzyme specificity, but fails to explain the stabilization of the transition state that enzymes achieve.
Induced fit model.
In 1958, Daniel Koshland suggested a modification to the lock and key model: since enzymes are rather flexible structures, the active site is continuously reshaped by interactions with the substrate as the substrate interacts with the enzyme. As a result, the substrate does not simply bind to a rigid active site; the amino acid side-chains that make up the active site are molded into the precise positions that enable the enzyme to perform its catalytic function. In some cases, such as glycosidases, the substrate molecule also changes shape slightly as it enters the active site. The active site continues to change until the substrate is completely bound, at which point the final shape and charge distribution is determined.
Induced fit may enhance the fidelity of molecular recognition in the presence of competition and noise via the conformational proofreading mechanism.
Catalysis.
Enzymes can accelerate reactions in several ways, all of which lower the activation energy (ΔG‡, Gibbs free energy)
Enzymes may use several of these mechanisms simultaneously. For example, proteases such as trypsin perform covalent catalysis using a catalytic triad, stabilise charge build-up on the transition states using an oxyanion hole, complete hydrolysis using an oriented water substrate.
Dynamics.
Enzymes are not rigid, static structures; instead they have complex internal dynamic motions – that is, movements of parts of the enzyme's structure such as individual amino acid residues, groups of residues forming a protein loop or unit of secondary structure, or even an entire protein domain. These motions give rise to a conformational ensemble of slightly different structures that interconvert with one another at equilibrium. Different states within this ensemble may be associated with different aspects of an enzyme's function. For example, different conformations of the enzyme dihydrofolate reductase are associated with the substrate binding, catalysis, cofactor release, and product release steps of the catalytic cycle.
Allosteric modulation.
Allosteric sites are pockets on the enzyme, distinct from the active site, that bind to molecules in the cellular environment. These molecules then cause a change in the conformation or dynamics of the enzyme that is transduced to the active site and thus affects the reaction rate of the enzyme. In this way, allosteric interactions can either inhibit or activate enzymes. Allosteric interactions with metabolites upstream or downstream in an enzyme's metabolic pathway cause feedback regulation, altering the activity of the enzyme according to the flux through the rest of the pathway.
Cofactors.
Some enzymes do not need additional components to show full activity. Others require non-protein molecules called cofactors to be bound for activity. Cofactors can be either inorganic (e.g., metal ions and iron-sulfur clusters) or organic compounds (e.g., flavin and heme). Organic cofactors can be either coenzymes, which are released from the enzyme's active site during the reaction, or prosthetic groups, which are tightly bound to an enzyme. Organic prosthetic groups can be covalently bound (e.g., biotin in enzymes such as pyruvate carboxylase).
An example of an enzyme that contains a cofactor is carbonic anhydrase, which is shown in the ribbon diagram above with a zinc cofactor bound as part of its active site. These tightly bound ions or molecules are usually found in the active site and are involved in catalysis. For example, flavin and heme cofactors are often involved in redox reactions.
Enzymes that require a cofactor but do not have one bound are called "apoenzymes" or "apoproteins". An enzyme together with the cofactor(s) required for activity is called a "holoenzyme" (or haloenzyme). The term "holoenzyme" can also be applied to enzymes that contain multiple protein subunits, such as the DNA polymerases; here the holoenzyme is the complete complex containing all the subunits needed for activity.
Coenzymes.
Coenzymes are small organic molecules that can be loosely or tightly bound to an enzyme. Coenzymes transport chemical groups from one enzyme to another. Examples include NADH, NADPH and adenosine triphosphate (ATP). Some coenzymes, such as riboflavin, thiamine and folic acid, are vitamins, or compounds that cannot be synthesized by the body and must be acquired from the diet. The chemical groups carried include the hydride ion (H−) carried by NAD or NADP+, the phosphate group carried by adenosine triphosphate, the acetyl group carried by coenzyme A, formyl, methenyl or methyl groups carried by folic acid and the methyl group carried by S-adenosylmethionine.
Since coenzymes are chemically changed as a consequence of enzyme action, it is useful to consider coenzymes to be a special class of substrates, or second substrates, which are common to many different enzymes. For example, about 1000 enzymes are known to use the coenzyme NADH.
Coenzymes are usually continuously regenerated and their concentrations maintained at a steady level inside the cell. For example, NADPH is regenerated through the pentose phosphate pathway and "S"-adenosylmethionine by methionine adenosyltransferase. This continuous regeneration means that small amounts of coenzymes can be used very intensively. For example, the human body turns over its own weight in ATP each day.
Thermodynamics.
As with all catalysts, enzymes do not alter the position of the chemical equilibrium of the reaction. In the presence of an enzyme, the reaction runs in the same direction as it would without the enzyme, just more quickly. For example, carbonic anhydrase catalyzes its reaction in either direction depending on the concentration of its reactants:
The rate of a reaction is dependent on the activation energy needed to form the transition state which then decays into products. Enzymes increase reaction rates by lowering the energy of the transition state. First, binding forms a low energy enzyme-substrate complex (ES). Secondly the enzyme stabilises the transition state such that it requires less energy to achieve compared to the uncatalyzed reaction (ES‡). Finally the enzyme-product complex (EP) dissociates to release the products.
Enzymes can couple two or more reactions, so that a thermodynamically favorable reaction can be used to "drive" a thermodynamically unfavourable one so that the combined energy of the products is lower than the substrates. For example, the hydrolysis of ATP is often used to drive other chemical reactions.
Kinetics.
Enzyme kinetics is the investigation of how enzymes bind substrates and turn them into products. The rate data used in kinetic analyses are commonly obtained from enzyme assays. In 1913 Leonor Michaelis and Maud Leonora Menten proposed a quantitative theory of enzyme kinetics, which is referred to as Michaelis–Menten kinetics. The major contribution of Michaelis and Menten was to think of enzyme reactions in two stages. In the first, the substrate binds reversibly to the enzyme, forming the enzyme-substrate complex. This is sometimes called the Michaelis-Menten complex in their honor. The enzyme then catalyzes the chemical step in the reaction and releases the product. This work was further developed by G. E. Briggs and J. B. S. Haldane, who derived kinetic equations that are still widely used today.
Enzyme rates depend on solution conditions and substrate concentration. To find the maximum speed of an enzymatic reaction, the substrate concentration is increased until a constant rate of product formation is seen. This is shown in the saturation curve on the right. Saturation happens because, as substrate concentration increases, more and more of the free enzyme is converted into the substrate-bound ES complex. At the maximum reaction rate ("V"max) of the enzyme, all the enzyme active sites are bound to substrate, and the amount of ES complex is the same as the total amount of enzyme.
"V"max is only one of several important kinetic parameters. The amount of substrate needed to achieve a given rate of reaction is also important. This is given by the Michaelis-Menten constant ("K"m), which is the substrate concentration required for an enzyme to reach one-half its maximum reaction rate; generally, each enzyme has a characteristic "K"m for a given substrate. Another useful constant is "k"cat, also called the "turnover number", which is the number of substrate molecules handled by one active site per second.
The efficiency of an enzyme can be expressed in terms of "k"cat/"K"m. This is also called the specificity constant and incorporates the rate constants for all steps in the reaction up to and including the first irreversible step. Because the specificity constant reflects both affinity and catalytic ability, it is useful for comparing different enzymes against each other, or the same enzyme with different substrates. The theoretical maximum for the specificity constant is called the diffusion limit and is about 108 to 109 (M−1 s−1). At this point every collision of the enzyme with its substrate will result in catalysis, and the rate of product formation is not limited by the reaction rate but by the diffusion rate. Enzymes with this property are called "catalytically perfect" or "kinetically perfect". Example of such enzymes are triose-phosphate isomerase, carbonic anhydrase, acetylcholinesterase, catalase, fumarase, β-lactamase, and superoxide dismutase. The turnover of such enzymes can reach several million reactions per second.
Michaelis–Menten kinetics relies on the law of mass action, which is derived from the assumptions of free diffusion and thermodynamically driven random collision. Many biochemical or cellular processes deviate significantly from these conditions, because of macromolecular crowding and constrained molecular movement. More recent, complex extensions of the model attempt to correct for these effects.
Inhibition.
Enzyme reaction rates can be decreased by various types of enzyme inhibitors.
Functions of inhibitors.
In many organisms, inhibitors may act as part of a feedback mechanism. If an enzyme produces too much of one substance in the organism, that substance may act as an inhibitor for the enzyme at the beginning of the pathway that produces it, causing production of the substance to slow down or stop when there is sufficient amount. This is a form of negative feedback. Major metabolic pathways such as the citric acid cycle make use of this mechanism.
Since inhibitors modulate the function of enzymes they are often used as drugs. Many such drugs are reversible competitive inhibitors that resemble the enzyme's native substrate, similar to methotrexate above; other well-known examples include statins used to treat high cholesterol, and protease inhibitors used to treat retroviral infections such as HIV. A common example of an irreversible inhibitor that is used as a drug is aspirin, which inhibits the COX-1 and COX-2 enzymes that produce the inflammation messenger prostaglandin. Other enzyme inhibitors are poisons. For example, the poison cyanide is an irreversible enzyme inhibitor that combines with the copper and iron in the active site of the enzyme cytochrome c oxidase and blocks cellular respiration.
Biological function.
Enzymes serve a wide variety of functions inside living organisms. They are indispensable for signal transduction and cell regulation, often via kinases and phosphatases. They also generate movement, with myosin hydrolyzing ATP to generate muscle contraction, and also transport cargo around the cell as part of the cytoskeleton. Other ATPases in the cell membrane are ion pumps involved in active transport. Enzymes are also involved in more exotic functions, such as luciferase generating light in fireflies. Viruses can also contain enzymes for infecting cells, such as the HIV integrase and reverse transcriptase, or for viral release from cells, like the influenza virus neuraminidase.
An important function of enzymes is in the digestive systems of animals. Enzymes such as amylases and proteases break down large molecules (starch or proteins, respectively) into smaller ones, so they can be absorbed by the intestines. Starch molecules, for example, are too large to be absorbed from the intestine, but enzymes hydrolyze the starch chains into smaller molecules such as maltose and eventually glucose, which can then be absorbed. Different enzymes digest different food substances. In ruminants, which have herbivorous diets, microorganisms in the gut produce another enzyme, cellulase, to break down the cellulose cell walls of plant fiber.
Metabolism.
Several enzymes can work together in a specific order, creating metabolic pathways. In a metabolic pathway, one enzyme takes the product of another enzyme as a substrate. After the catalytic reaction, the product is then passed on to another enzyme. Sometimes more than one enzyme can catalyze the same reaction in parallel; this can allow more complex regulation: with, for example, a low constant activity provided by one enzyme but an inducible high activity from a second enzyme.
Enzymes determine what steps occur in these pathways. Without enzymes, metabolism would neither progress through the same steps and could not be regulated to serve the needs of the cell. Most central metabolic pathways are regulated at a few key steps, typically through enzymes whose activity involves the hydrolysis of ATP. Because this reaction releases so much energy, other reactions that are thermodynamically unfavorable can be coupled to ATP hydrolysis, driving the overall series of linked metabolic reactions.
Control of activity.
There are five main ways that enzyme activity is controlled in the cell.
Involvement in disease.
Since the tight control of enzyme activity is essential for homeostasis, any malfunction (mutation, overproduction, underproduction or deletion) of a single critical enzyme can lead to a genetic disease. The malfunction of just one type of enzyme out of the thousands of types present in the human body can be fatal. An example of a fatal genetic disease due to enzyme insufficiency is Tay-Sachs disease, in which patients lack the enzyme hexosaminidase.
One example of enzyme deficiency is the most common type of phenylketonuria. Many different single amino acid mutations in the enzyme phenylalanine hydroxylase, which catalyzes the first step in the degradation of phenylalanine, result in build-up of phenylalanine and related products. Some mutations are in the active site, directly disrupting binding and catalysis, but many are far from the active site and reduce activity by destabilising the protein structure, or affecting correct oligomerisation. This can lead to intellectual disability if the disease is untreated. Another example is pseudocholinesterase deficiency, in which the body's ability to break down choline ester drugs is impaired. 
Oral administration of enzymes can be used to treat some functional enzyme deficiencies, such as pancreatic insufficiency and lactose intolerance.
Another way enzyme malfunctions can cause disease comes from germline mutations in genes coding for DNA repair enzymes. Defects in these enzymes cause cancer because cells are less able to repair mutations in their genomes. This causes a slow accumulation of mutations and results in the development of cancers. An example of such a hereditary cancer syndrome is xeroderma pigmentosum, which causes the development of skin cancers in response to even minimal exposure to ultraviolet light.
Naming conventions.
An enzyme's name is often derived from its substrate or the chemical reaction it catalyzes, with the word ending in "-ase". Examples are lactase, alcohol dehydrogenase and DNA polymerase. Different enzymes that catalyze the same chemical reaction are called isozymes.
The International Union of Biochemistry and Molecular Biology have developed a nomenclature for enzymes, the EC numbers; each enzyme is described by a sequence of four numbers preceded by "EC". The first number broadly classifies the enzyme based on its mechanism.
The top-level classification is:
These sections are subdivided by other features such as the substrate, products, and chemical mechanism. An enzyme is fully specified by four numerical designations. For example, hexokinase (EC 2.7.1.1) is a transferase (EC 2) that adds a phosphate group (EC 2.7) to a hexose sugar, a molecule containing an alcohol group (EC 2.7.1).
Industrial applications.
Enzymes are used in the chemical industry and other industrial applications when extremely specific catalysts are required. Enzymes in general are limited in the number of reactions they have evolved to catalyze and also by their lack of stability in organic solvents and at high temperatures. As a consequence, protein engineering is an active area of research and involves attempts to create new enzymes with novel properties, either through rational design or "in vitro" evolution. These efforts have begun to be successful, and a few enzymes have now been designed "from scratch" to catalyze reactions that do not occur in nature.
Further reading.
General
Etymology and history
Enzyme structure and mechanism
Kinetics and inhibition

</doc>
<doc id="9258" url="https://en.wikipedia.org/wiki?curid=9258" title="Ethics">
Ethics

Ethics or moral philosophy is the branch of philosophy that involves systematizing, defending, and recommending concepts of right and wrong conduct. The term "ethics" derives from the Ancient Greek word ἠθικός "ethikos", which is derived from the word ἦθος "ethos" (habit, "custom"). The branch of philosophy axiology comprises the sub-branches of ethics and aesthetics, each concerned with values.
As a branch of philosophy, ethics investigates the questions "What is the best way for people to live?" and "What actions are right or wrong in particular circumstances?" In practice, ethics seeks to resolve questions of human morality, by defining concepts such as good and evil, right and wrong, virtue and vice, justice and crime. As a field of intellectual enquiry, moral philosophy also is related to the fields of moral psychology, descriptive ethics, and value theory.
Three major areas of study within ethics recognised today are:
Defining ethics.
Rushworth Kidder states that "standard definitions of "ethics" have typically included such phrases as 'the science of the ideal human character' or 'the science of moral duty' ". Richard William Paul and Linda Elder define ethics as "a set of concepts and principles that guide us in determining what behavior helps or harms sentient creatures". The "Cambridge Dictionary of Philosophy" states that the word ethics is "commonly used interchangeably with 'morality' ... and sometimes it is used more narrowly to mean the moral principles of a particular tradition, group or individual." Paul and Elder state that most people confuse ethics with behaving in accordance with social conventions, religious beliefs and the law and don't treat ethics as a stand-alone concept.
The word "ethics" in English refers to several things. It can refer to philosophical ethics or moral philosophy—a project that attempts to use reason in order to answer various kinds of ethical questions. As the English philosopher Bernard Williams writes, attempting to explain moral philosophy: "What makes an inquiry a philosophical one is reflective generality and a style of argument that claims to be rationally persuasive." And Williams describes the content of this area of inquiry as addressing the very broad question, "how one should live" Ethics can also refer to a common human ability to think about ethical problems that is not particular to philosophy. As bioethicist Larry Churchill has written: "Ethics, understood as the capacity to think critically about moral values and direct our actions in terms of such values, is a generic human capacity." Ethics can also be used to describe a particular person's own idiosyncratic principles or habits. For example: "Joe has strange ethics."
The English word ethics is derived from an Ancient Greek word "êthikos", which means "relating to one's character." The Ancient Greek adjective "êthikos" is itself derived from another Greek word, the noun "êthos" meaning "character, disposition."
Meta-ethics.
Meta-ethics asks how we understand, know about, and what we mean when we talk about what is right and what is wrong. An ethical question fixed on some particular practical question—such as, "Should I eat this particular piece of chocolate cake?"—cannot be a meta-ethical question. A meta-ethical question is abstract and relates to a wide range of more specific practical questions. For example, "Is it ever possible to have secure knowledge of what is right and wrong?" would be a meta-ethical question.
Meta-ethics has always accompanied philosophical ethics. For example, Aristotle implies that less precise knowledge is possible in ethics than in other spheres of inquiry, and he regards ethical knowledge as depending upon habit and acculturation in a way that makes it distinctive from other kinds of knowledge. Meta-ethics is also important in G.E. Moore's "Principia Ethica" from 1903. In it he first wrote about what he called "the naturalistic fallacy". Moore was seen to reject naturalism in ethics, in his Open Question Argument. This made thinkers look again at second order questions about ethics. Earlier, the Scottish philosopher David Hume had put forward a similar view on the difference between facts and values.
Studies of how we know in ethics divide into cognitivism and non-cognitivism; this is similar to the contrast between descriptivists and non-descriptivists. Non-cognitivism is the claim that when we judge something as right or wrong, this is neither true nor false. We may for example be only expressing our emotional feelings about these things. Cognitivism can then be seen as the claim that when we talk about right and wrong, we are talking about matters of fact.
The ontology of ethics is about value-bearing things or properties, i.e. the kind of things or stuff referred to by ethical propositions. Non-descriptivists and non-cognitivists believe that ethics does not need a specific ontology, since ethical propositions do not refer. This is known as an anti-realist position. Realists on the other hand must explain what kind of entities, properties or states are relevant for ethics, how they have value, and why they guide and motivate our actions.
Normative ethics.
Normative ethics is the study of ethical action. It is the branch of ethics that investigates the set of questions that arise when considering how one ought to act, morally speaking. Normative ethics is distinct from meta-ethics because it examines standards for the rightness and wrongness of actions, while meta-ethics studies the meaning of moral language and the metaphysics of moral facts. Normative ethics is also distinct from descriptive ethics, as the latter is an empirical investigation of people's moral beliefs. To put it another way, descriptive ethics would be concerned with determining what proportion of people believe that killing is always wrong, while normative ethics is concerned with whether it is correct to hold such a belief. Hence, normative ethics is sometimes called prescriptive, rather than descriptive. However, on certain versions of the meta-ethical view called moral realism, moral facts are both descriptive and prescriptive at the same time.
Traditionally, normative ethics (also known as moral theory) was the study of what makes actions right and wrong. These theories offered an overarching moral principle one could appeal to in resolving difficult moral decisions.
At the turn of the 20th century, moral theories became more complex and are no longer concerned solely with rightness and wrongness, but are interested in many different kinds of moral status. During the middle of the century, the study of normative ethics declined as meta-ethics grew in prominence. This focus on meta-ethics was in part caused by an intense linguistic focus in analytic philosophy and by the popularity of logical positivism.
In 1971 John Rawls published "A Theory of Justice", noteworthy in its pursuit of moral arguments and eschewing of meta-ethics. This publication set the trend for renewed interest in normative ethics.
Virtue ethics.
Virtue ethics describes the character of a moral agent as a driving force for ethical behavior, and is used to describe the ethics of Socrates, Aristotle, and other early Greek philosophers. Socrates (469–399 BC) was one of the first Greek philosophers to encourage both scholars and the common citizen to turn their attention from the outside world to the condition of humankind. In this view, knowledge bearing on human life was placed highest, while all other knowledge were secondary. Self-knowledge was considered necessary for success and inherently an essential good. A self-aware person will act completely within his capabilities to his pinnacle, while an ignorant person will flounder and encounter difficulty. To Socrates, a person must become aware of every fact (and its context) relevant to his existence, if he wishes to attain self-knowledge. He posited that people will naturally do what is good, if they know what is right. Evil or bad actions are the result of ignorance. If a criminal was truly aware of the intellectual and spiritual consequences of his actions, he would neither commit nor even consider committing those actions. Any person who knows what is truly right will automatically do it, according to Socrates. While he correlated knowledge with virtue, he similarly equated virtue with joy. The truly wise man will know what is right, do what is good, and therefore be happy.
Aristotle (384–323 BC) posited an ethical system that may be termed "self-realizationism." In Aristotle's view, when a person acts in accordance with his nature and realizes his full potential, he will do good and be content. At birth, a baby is not a person, but a potential person. To become a "real" person, the child's inherent potential must be realized. Unhappiness and frustration are caused by the unrealized potential of a person, leading to failed goals and a poor life. Aristotle said, "Nature does nothing in vain." Therefore, it is imperative for people to act in accordance with their nature and develop their latent talents in order to be content and complete. Happiness was held to be the ultimate goal. All other things, such as civic life or wealth, are merely means to the end. Self-realization, the awareness of one's nature and the development of one's talents, is the surest path to happiness.
Aristotle asserted that man had three natures: vegetable (physical/metabolism), animal (emotional/appetite) and rational (mental/conceptual). Physical nature can be assuaged through exercise and care, emotional nature through indulgence of instinct and urges, and mental through human reason and developed potential. Rational development was considered the most important, as essential to philosophical self-awareness and as uniquely human. Moderation was encouraged, with the extremes seen as degraded and immoral. For example, courage is the moderate virtue between the extremes of cowardice and recklessness. Man should not simply live, but live well with conduct governed by moderate virtue. This is regarded as difficult, as virtue denotes doing the right thing, to the right person, at the right time, to the proper extent, in the correct fashion, for the right reason.
Stoicism.
The Stoic philosopher Epictetus posited that the greatest good was contentment and serenity. Peace of mind, or Apatheia, was of the highest value; self-mastery over one's desires and emotions leads to spiritual peace. The "unconquerable will" is central to this philosophy. The individual's will should be independent and inviolate. Allowing a person to disturb the mental equilibrium is in essence offering yourself in slavery. If a person is free to anger you at will, you have no control over your internal world, and therefore no freedom. Freedom from material attachments is also necessary. If a thing breaks, the person should not be upset, but realize it was a thing that could break. Similarly, if someone should die, those close to them should hold to their serenity because the loved one was made of flesh and blood destined to death. Stoic philosophy says to accept things that cannot be changed, resigning oneself to existence and enduring in a rational fashion. Death is not feared. People do not "lose" their life, but instead "return", for they are returning to God (who initially gave what the person is as a person). Epictetus said difficult problems in life should not be avoided, but rather embraced. They are spiritual exercises needed for the health of the spirit, just as physical exercise is required for the health of the body. He also stated that sex and sexual desire are to be avoided as the greatest threat to the integrity and equilibrium of a man's mind. Abstinence is highly desirable. Epictetus said remaining abstinent in the face of temptation was a victory for which a man could be proud.
Contemporary virtue ethics.
Modern virtue ethics was popularized during the late 20th century in large part as a response to G. E. M. Anscombe's "Modern Moral Philosophy". Anscombe argues that consequentialist and deontological ethics are only feasible as universal theories if the two schools ground themselves in divine law. As a deeply devoted Christian herself, Anscombe proposed that either those who do not give ethical credence to notions of divine law take up virtue ethics, which does not necessitate universal laws as agents themselves are investigated for virtue or vice and held up to "universal standards," or that those who wish to be utilitarian or consequentialist ground their theories in religious conviction. Alasdair MacIntyre, who wrote the book "After Virtue", was a key contributor and proponent of modern virtue ethics, although MacIntyre supports a relativistic account of virtue based on cultural norms, not objective standards. Martha Nussbaum, a contemporary virtue ethicist, objects to MacIntyre's relativism, among that of others, and responds to relativist objections to form an objective account in her work "Non-Relative Virtues: An Aristotelian Approach."
"Complete Conduct Principles for the 21st Century" blended the Eastern virtue ethics and the Western virtue ethics, with some modifications to suit the 21st Century, and formed a part of contemporary virtue ethics.
Hedonism.
Hedonism posits that the principal ethic is maximizing pleasure and minimizing pain. There are several schools of Hedonist thought ranging from those advocating the indulgence of even momentary desires to those teaching a pursuit of spiritual bliss. In their consideration of consequences, they range from those advocating self-gratification regardless of the pain and expense to others, to those stating that the most ethical pursuit maximizes pleasure and happiness for the most people.
Cyrenaic hedonism.
Founded by Aristippus of Cyrene, Cyrenaics supported immediate gratification or pleasure. "Eat, drink and be merry, for tomorrow we die." Even fleeting desires should be indulged, for fear the opportunity should be forever lost. There was little to no concern with the future, the present dominating in the pursuit for immediate pleasure. Cyrenaic hedonism encouraged the pursuit of enjoyment and indulgence without hesitation, believing pleasure to be the only good.
Epicureanism.
Epicurean ethics is a hedonist form of virtue ethics. Epicurus "presented a sustained argument that pleasure, correctly understood, will coincide with virtue". He rejected the extremism of the Cyrenaics, believing some pleasures and indulgences to be detrimental to human beings. Epicureans observed that indiscriminate indulgence sometimes resulted in negative consequences. Some experiences were therefore rejected out of hand, and some unpleasant experiences endured in the present to ensure a better life in the future. To Epicurus the "summum bonum", or greatest good, was prudence, exercised through moderation and caution. Excessive indulgence can be destructive to pleasure and can even lead to pain. For example, eating one food too often will cause a person to lose taste for it. Eating too much food at once will lead to discomfort and ill-health. Pain and fear were to be avoided. Living was essentially good, barring pain and illness. Death was not to be feared. Fear was considered the source of most unhappiness. Conquering the fear of death would naturally lead to a happier life. Epicurus reasoned if there was an afterlife and immortality, the fear of death was irrational. If there was no life after death, then the person would not be alive to suffer, fear or worry; he would be non-existent in death. It is irrational to fret over circumstances that do not exist, such as one's state in death in the absence of an afterlife.
State consequentialism.
State consequentialism, also known as Mohist consequentialism, is an ethical theory that evaluates the moral worth of an action based on how much it contributes to the basic goods of a state. The "Stanford Encyclopedia of Philosophy" describes Mohist consequentialism, dating back to the 5th century BC, as "a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare." Unlike utilitarianism, which views pleasure as a moral good, "the basic goods in Mohist consequentialist thinking are ... order, material wealth, and increase in population". During Mozi's era, war and famines were common, and population growth was seen as a moral necessity for a harmonious society. The "material wealth" of Mohist consequentialism refers to basic needs like shelter and clothing, and the "order" of Mohist consequentialism refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability.
Stanford sinologist David Shepherd Nivison, in "The Cambridge History of Ancient China", writes that the moral goods of Mohism "are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth ... if people have plenty, they would be good, filial, kind, and so on unproblematically." The Mohists believed that morality is based on "promoting the benefit of all under heaven and eliminating harm to all under heaven." In contrast to Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain.
Consequentialism/Teleology.
Consequentialism refers to moral theories that hold that the consequences of a particular action form the basis for any valid moral judgment about that action (or create a structure for judgment, see rule consequentialism). Thus, from a consequentialist standpoint, a morally right action is one that produces a good outcome, or consequence. This view is often expressed as the aphorism ""The ends justify the means"".
The term "consequentialism" was coined by G. E. M. Anscombe in her essay "Modern Moral Philosophy" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick. Since then, the term has become common in English-language ethical theory.
The defining feature of consequentialist moral theories is the weight given to the consequences in evaluating the rightness and wrongness of actions. In consequentialist theories, the consequences of an action or rule generally outweigh other considerations. Apart from this basic outline, there is little else that can be unequivocally said about consequentialism as such. However, there are some questions that many consequentialist theories address:
One way to divide various consequentialisms is by the types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase in a positive effect, and the best action is one that results in that effect for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral "pleasure". Other theories adopt a package of several goods, all to be promoted equally. Whether a particular consequentialist theory focuses on a single good or many, conflicts and tensions between different good states of affairs are to be expected and must be adjudicated.
Utilitarianism.
Utilitarianism is an ethical theory that argues the proper course of action is one that maximizes a positive effect, such as "happiness", "welfare", or the ability to live according to personal preferences. Jeremy Bentham and John Stuart Mill are influential proponents of this school of thought. In "A Fragment on Government" Bentham says 'it is the greatest happiness of the greatest number that is the measure of right and wrong' and describes this as a fundamental axiom. In "An Introduction to the Principles of Morals and Legislation" he talks of 'the principle of utility' but later prefers "the greatest happiness principle".
Utilitarianism is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that what matters is the aggregate positive effect of everyone and not only of any one person. John Stuart Mill, in his exposition of utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. Other noteworthy proponents of utilitarianism are neuroscientist Sam Harris, author of The Moral Landscape, and moral philosopher Peter Singer, author of, amongst other works, Practical Ethics.
There are two types of utilitarianism, act utilitarianism and rule utilitarianism. In act utilitarianism the principle of utility is applied directly to each alternative act in a situation of choice. The right act is then defined as the one which brings about the best results (or the least amount of bad results). In rule utilitarianism the principle of utility is used to determine the validity of rules of conduct (moral principles). A rule like promise-keeping is established by looking at the consequences of a world in which people broke promises at will and a world in which promises were binding. Right and wrong are then defined as following or breaking those rules.
Deontology.
Deontological ethics or deontology (from Greek , "deon", "obligation, duty"; and , "-logia") is an approach to ethics that determines goodness or rightness from examining acts, or the rules and duties that the person doing the act strove to fulfill. This is in contrast to consequentialism, in which rightness is based on the consequences of an act, and not the act by itself. In deontology, an act may be considered right even if the act produces a bad consequence, if it follows the "rule" that "one should do unto others as they would have done unto them", and even if the person who does the act lacks virtue and had a bad intention in doing the act. According to deontology, we have a "duty" to act in a way that does those things that are inherently good as acts ("truth-telling" for example), or follow an objectively obligatory rule (as in rule utilitarianism). For deontologists, the ends or consequences of our actions are not important in and of themselves, and our intentions are not important in and of themselves.
Immanuel Kant's theory of ethics is considered deontological for several different reasons. First, Kant argues that to act in the morally right way, people must act from duty ("deon"). Second, Kant argued that it was not the consequences of actions that make them right or wrong but the motives (maxime) of the person who carries out the action.
Kant's argument that to act in the morally right way, one must act from duty, begins with an argument that the highest good must be both good in itself, and good without qualification. Something is 'good in itself' when it is intrinsically good, and 'good without qualification' when the addition of that thing never makes a situation ethically worse. Kant then argues that those things that are usually thought to be good, such as intelligence, perseverance and pleasure, fail to be either intrinsically good or good without qualification. Pleasure, for example, appears to not be good without qualification, because when people take pleasure in watching someone suffer, they make the situation ethically worse. He concludes that there is only one thing that is truly good: Nothing in the world—indeed nothing even beyond the world—can possibly be conceived which could be called good without qualification except a "good will". 
Pragmatic ethics.
Associated with the pragmatists, Charles Sanders Peirce, William James, and especially John Dewey, pragmatic ethics holds that moral correctness evolves similarly to scientific knowledge: socially over the course of many lifetimes. Thus, we should prioritize social reform over attempts to account for consequences, individual virtue or duty (although these may be worthwhile attempts, provided social reform is provided for).
Role ethics.
Role ethics is an ethical theory based on family roles. Unlike virtue ethics, role ethics is not individualistic. Morality is derived from a person's relationship with their community. Confucian ethics is an example of role ethics. Confucian roles center around the concept of filial piety or "xiao", a respect for family members. According to Roger Ames and Henry Rosemont, "Confucian normativity is defined by living one's family roles to maximum effect." Morality is determined through a person's fulfillment of a role, such as that of a parent or a child. Confucian roles are not rational, and originate through the "xin", or human emotions.
Anarchist ethics.
Anarchist ethics is an ethical theory based on the studies of anarchist thinkers. The biggest contributor to the anarchist ethics is the Russian zoologist, geographer, economist and political activist Peter Kropotkin. The anarchist ethics is a big and vague field which can depend upon different historical situations and different anarchist thinkers, but as Peter Kropotkin explains, "any “bourgeois” or “proletarian” ethics rests, after all, on the common basis, on the common ethnological foundation, which at times exerts a very strong inﬂuence on the principles of the class or group morality." Still, most of the anarchist ethics schools are based on three fundamental ideas, which are: "solidarity, equality and justice". Kropotkin argues that Ethics is evolutionary and is inherited as a sort of a social instinct through History, and by so, he rejects any religious and transcendental explanation of ethics. Kropotkin suggests that the principle of equality which lies at the basis of anarchism is the same as the Golden rule: This principle of treating others as one wishes to be treated oneself, what is it but the very same principle as equality, the fundamental principle of anarchism? And how can any one manage to believe himself an anarchist unless he practices it? We do not wish to be ruled. And by this very fact, do we not declare that we ourselves wish to rule nobody? We do not wish to be deceived, we wish always to be told nothing but the truth. And by this very fact, do we not de- clare that we ourselves do not wish to deceive anybody, that we promise to always tell the truth, nothing but the truth, the whole truth? We do not wish to have the fruits of our labor stolen from us. And by that very fact, do we not declare that we respect the fruits of others' labor? By what right indeed can we demand that we should be treated in one fashion, reserving it to ourselves to treat others in a fashion entirely different? Our sense of equality revolts at such an idea.
Postmodern ethics.
The 20th century saw a remarkable expansion and evolution of critical theory, following on earlier Marxist Theory efforts to locate individuals within larger structural frameworks of ideology and action.
Antihumanists such as Louis Althusser and Michel Foucault and structuralists such as Roland Barthes challenged the possibilities of individual agency and the coherence of the notion of the 'individual' itself. As critical theory developed in the later 20th century, post-structuralism sought to problematize human relationships to knowledge and 'objective' reality. Jacques Derrida argued that access to meaning and the 'real' was always deferred, and sought to demonstrate via recourse to the linguistic realm that "there is nothing outside context" (""il n'y a pas de hors-texte"" is often mistranslated as "there is nothing outside the text"); at the same time, Jean Baudrillard theorised that signs and symbols or simulacra mask reality (and eventually the absence of reality itself), particularly in the consumer world.
Post-structuralism and postmodernism argue that ethics must study the complex and relational conditions of actions. A simple alignment of ideas of right and particular acts is not possible. There will always be an ethical remainder that cannot be taken into account or often even recognized. Such theorists find narrative (or, following Nietzsche and Foucault, genealogy) to be a helpful tool for understanding ethics because narrative is always about particular lived experiences in all their complexity rather than the assignment of an idea or norm to separate and individuated actions.
Zygmunt Bauman says Postmodernity is best described as Modernity without illusion, the illusion being the belief that humanity can be repaired by some ethic principle. Postmodernity can be seen in this light as accepting the messy nature of humanity as unchangeable.
David Couzens Hoy states that Emmanuel Levinas's writings on the face of the Other and Derrida's meditations on the relevance of death to ethics are signs of the "ethical turn" in Continental philosophy that occurred in the 1980s and 1990s. Hoy describes post-critique ethics as the "obligations that present themselves as necessarily to be fulfilled but are neither forced on one or are enforceable" (2004, p. 103).
Hoy's post-critique model uses the term "ethical resistance". Examples of this would be an individual's resistance to consumerism in a retreat to a simpler but perhaps harder lifestyle, or an individual's resistance to a terminal illness. Hoy describes Levinas's account as "not the attempt to use power against itself, or to mobilize sectors of the population to exert their political power; the ethical resistance is instead the resistance of the powerless"(2004, p. 8).
Hoy concludes that
In present-day terms the powerless may include the unborn, the terminally sick, the aged, the insane, and non-human animals. It is in these areas that ethical action in Hoy's sense will apply. Until legislation or the state apparatus enforces a moral order that addresses the causes of resistance these issues will remain in the ethical realm. For example, should animal experimentation become illegal in a society, it will no longer be an ethical issue on Hoy's definition. Likewise one hundred and fifty years ago, not having a black slave in America would have been an ethical choice. This later issue has been absorbed into the fabric of an enforceable social order and is therefore no longer an ethical issue in Hoy's sense.
Applied ethics.
Applied ethics is a discipline of philosophy that attempts to apply ethical theory to real-life situations. The discipline has many specialized fields, such as engineering ethics, bioethics, geoethics, public service ethics and business ethics.
Specific questions.
Applied ethics is used in some aspects of determining public policy, as well as by individuals facing difficult decisions. The sort of questions addressed by applied ethics include: "Is getting an abortion immoral?" "Is euthanasia immoral?" "Is affirmative action right or wrong?" "What are human rights, and how do we determine them?" "Do animals have rights as well?" and "Do individuals have the right of self determination?"
A more specific question could be: "If someone else can make better out of his/her life than I can, is it then moral to sacrifice myself for them if needed?" Without these questions there is no clear fulcrum on which to balance law, politics, and the practice of arbitration — in fact, no common assumptions of all participants—so the ability to formulate the questions are prior to rights balancing. But not all questions studied in applied ethics concern public policy. For example, making ethical judgments regarding questions such as, "Is lying always wrong?" and, "If not, when is it permissible?" is prior to any etiquette.
People in-general are more comfortable with dichotomies (two opposites). However, in ethics the issues are most often multifaceted and the best proposed actions address many different areas concurrently. In ethical decisions the answer is almost never a "yes or no", "right or wrong" statement. Many buttons are pushed so that the overall condition is improved and not to the benefit of any particular faction.
Particular fields of application.
Bioethics.
Bioethics is the study of controversial ethics brought about by advances in biology and medicine. Bioethicists are concerned with the ethical questions that arise in the relationships among life sciences, biotechnology, medicine, politics, law, and philosophy. It also includes the study of the more commonplace questions of values ("the ethics of the ordinary") that arise in primary care and other branches of medicine.
Bioethics also needs to address emerging biotechnologies that affect basic biology and future humans. These developments include cloning, gene therapy, human genetic engineering, astroethics and life in space, and manipulation of basic biology through altered DNA, RNA and proteins,e.g.- "three parent baby,where baby is born from genetically modified embryos, would have DNA from a mother, a father and from a female donor. Correspondingly, new bioethics also need to address life at its core. For example, biotic ethics value organic gene/protein life itself and seek to propagate it. With such life-centered principles, ethics may secure a cosmological future for life.
Business ethics.
Business ethics (also corporate ethics) is a form of applied ethics or professional ethics that examines ethical principles and moral or ethical problems that arise in a business environment, including fields like Medical ethics. It applies to all aspects of business conduct and is relevant to the conduct of individuals and entire organizations.
Business ethics has both normative and descriptive dimensions. As a corporate practice and a career specialization, the field is primarily normative. Academics attempting to understand business behavior employ descriptive methods. The range and quantity of business ethical issues reflects the interaction of profit-maximizing behavior with non-economic concerns.
Interest in business ethics accelerated dramatically during the 1980s and 1990s, both within major corporations and within academia. For example, today most major corporations promote their commitment to non-economic values under headings such as ethics codes and social responsibility charters.
Adam Smith said, "People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices." Governments use laws and regulations to point business behavior in what they perceive to be beneficial directions. Ethics implicitly regulates areas and details of behavior that lie beyond governmental control. The emergence of large corporations with limited relationships and sensitivity to the communities in which they operate accelerated the development of formal ethics regimes.
Machine ethics.
In "Moral Machines: Teaching Robots Right from Wrong", Wendell Wallach and Colin Allen conclude that issues in machine ethics will likely drive advancement in understanding of human ethics by forcing us to address gaps in modern normative theory and by providing a platform for experimental investigation. The effort to actually program a machine or artificial agent to behave as though instilled with a sense of ethics requires new specificity in our normative theories, especially regarding aspects customarily considered common-sense. For example, machines, unlike humans, can support a wide selection of learning algorithms, and controversy has arisen over the relative ethical merits of these options. This may reopen classic debates of normative ethics framed in new (highly technical) terms.
Military ethics.
Military ethics are concerned with questions regarding the application of force and the ethos of the soldier and are often understood as applied professional ethics. Just war theory is generally seen to set the background terms of military ethics. However individual countries and traditions have different fields of attention.
Military ethics involves multiple subareas, including the following among others:
Political ethics.
Political ethics (also known as political morality or public ethics) is the practice of making moral judgements about political action and political agents.
Public sector ethics.
Public sector ethics is a set of principles that guide public officials in their service to their constituents, including their decision-making on behalf of their constituents. Fundamental to the concept of public sector ethics is the notion that decisions and actions are based on what best serves the public's interests, as opposed to the official's personal interests (including financial interests) or self-serving political interests.
Publication ethics.
Publication ethics is the set of principles that guide the writing and publishing process for all professional publications. In order to follow the set of principles, authors should verify that the publication does not contain plagiarism or publication bias. As a way to avoid misconduct in research these principles can also be applied to experiments which are referenced or analyzed in publications by ensuring the data is recorded, honestly and accurately.
Plagiarism is the failure to give credit to another author’s work or ideas, when it is used in the publication. It is the obligation of the editor of the journal to ensure the article does not contain any plagiarism before it is published. If a publication which has already been published is proven to contain plagiarism, then the editor of the journal can proceed to have the article retracted.
Publication bias occurs when the publication is one-sided or "prejudiced against results". In best practice, an author should try to include information from all parties involved, or affected by the topic. If an author is prejudiced against certain results, than it can "lead to erroneous conclusions being drawn.”
Misconduct in research can occur when information from an experiment is falsely recorded or altered. Falsely recorded information occurs when the researcher "fakes" information or data, which was not used when conducting the actual experiment. By faking the data, the researcher can alter the results from the experiment to better fit the hypothesis they originally predicted. When conducting medical research, it is important to honor the healthcare rights of a patient by protecting their anonymity in the publication.
Relational ethics.
Relational ethics are related to an ethics of care. They are used in qualitative research, especially ethnography and autoethnography. Researchers who employ relational ethics value and respect the connection between themselves and the people they study, and "between researchers and the communities in which they live and work" (Ellis, 2007, p. 4). Relational ethics also help researchers understand difficult issues such as conducting research on intimate others that have died and developing friendships with their participants. Relational ethics in close personal relationships form a central concept of contextual therapy.
Moral psychology.
Moral psychology is a field of study that began as an issue in philosophy and that is now properly considered part of the discipline of psychology. Some use the term "moral psychology" relatively narrowly to refer to the study of moral development. However, others tend to use the term more broadly to include any topics at the intersection of ethics and psychology (and philosophy of mind). Such topics are ones that involve the mind and are relevant to moral issues. Some of the main topics of the field are moral responsibility, moral development, moral character (especially as related to virtue ethics), altruism, psychological egoism, moral luck, and moral disagreement.
Evolutionary ethics.
Evolutionary ethics concerns approaches to ethics (morality) based on the role of evolution in shaping human psychology and behavior. Such approaches may be based in scientific fields such as evolutionary psychology or sociobiology, with a focus on understanding and explaining observed ethical preferences and choices.
Descriptive ethics.
Descriptive ethics is on the less philosophical end of the spectrum, since it seeks to gather particular information about how people live and draw general conclusions based on observed patterns. Abstract and theoretical questions that are more clearly philosophical—such as, "Is ethical knowledge possible?"—are not central to descriptive ethics. Descriptive ethics offers a value-free approach to ethics, which defines it as a social science rather than a humanity. Its examination of ethics doesn't start with a preconceived theory, but rather investigates observations of actual choices made by moral agents in practice. Some philosophers rely on descriptive ethics and choices made and unchallenged by a society or culture to derive categories, which typically vary by context. This can lead to situational ethics and situated ethics. These philosophers often view aesthetics, etiquette, and arbitration as more fundamental, percolating "bottom up" to imply the existence of, rather than explicitly prescribe, theories of value or of conduct. The study of descriptive ethics may include examinations of the following:

</doc>
<doc id="9259" url="https://en.wikipedia.org/wiki?curid=9259" title="Equivalence relation">
Equivalence relation

In mathematics, an equivalence relation is a binary relation that is at the same time a reflexive relation, a symmetric relation and a transitive relation. As a consequence of these properties an equivalence relation provides a partition of a set into equivalence classes.
Notation.
Although various notations are used throughout the literature to denote that two elements "a" and "b" of a set are equivalent with respect to an equivalence relation "R", the most common are ""a" ~ "b"" and ""a" ≡ "b"", which are used when "R" is the obvious relation being referenced, and variations of ""a" ~"R" "b"", ""a" ≡"R" "b"", or ""aRb"" otherwise.
Definition.
A given binary relation ~ on a set "X" is said to be an equivalence relation if and only if it is reflexive, symmetric and transitive. That is, for all "a", "b" and "c" in "X":
"X" together with the relation ~ is called a setoid. The equivalence class of formula_1 under ~, denoted formula_2.
Examples.
Simple example.
Let the set formula_3 have the equivalence relation formula_4. The following sets are equivalence classes of this relation:
formula_5.
The set of all equivalence classes for this relation is formula_6.
Equivalence relations.
The following are all equivalence relations:
Well-definedness under an equivalence relation.
If ~ is an equivalence relation on "X", and "P"("x") is a property of elements of "X", such that whenever "x" ~ "y", "P"("x") is true if "P"("y") is true, then the property "P" is said to be well-defined or a "class invariant" under the relation ~.
A frequent particular case occurs when "f" is a function from "X" to another set "Y"; if "x"1 ~ "x"2 implies "f"("x"1) = "f"("x"2) then "f" is said to be a "morphism" for ~, a "class invariant under" ~, or simply "invariant under" ~. This occurs, e.g. in the character theory of finite groups. The latter case with the function "f" can be expressed by a commutative triangle. See also invariant. Some authors use "compatible with ~" or just "respects ~" instead of "invariant under ~".
More generally, a function may map equivalent arguments (under an equivalence relation ~A) to equivalent values (under an equivalence relation ~B). Such a function is known as a morphism from ~A to ~B.
Equivalence class, quotient set, partition.
Let formula_7. Some definitions:
Equivalence class.
A subset "Y" of "X" such that "a" ~ "b" holds for all "a" and "b" in "Y", and never for "a" in "Y" and "b" outside "Y", is called an equivalence class of "X" by ~. Let formula_8 denote the equivalence class to which "a" belongs. All elements of "X" equivalent to each other are also elements of the same equivalence class.
Quotient set.
The set of all possible equivalence classes of "X" by ~, denoted formula_9, is the quotient set of "X" by ~. If "X" is a topological space, there is a natural way of transforming "X"/~ into a topological space; see quotient space for the details.
Projection.
The projection of ~ is the function formula_10 defined by formula_11 which maps elements of "X" into their respective equivalence classes by ~.
Equivalence kernel.
The equivalence kernel of a function "f" is the equivalence relation ~ defined by formula_12. The equivalence kernel of an injection is the identity relation.
Partition.
A partition of "X" is a set "P" of nonempty subsets of "X", such that every element of "X" is an element of a single element of "P". Each element of "P" is a "cell" of the partition. Moreover, the elements of "P" are pairwise disjoint and their union is "X".
Counting possible partitions.
Let "X" be a finite set with "n" elements. Since every equivalence relation over "X" corresponds to a partition of "X", and vice versa, the number of possible equivalence relations on "X" equals the number of distinct partitions of "X", which is the "nth" Bell number "Bn":
where the above is one of the ways to write the nth Bell number.
Fundamental theorem of equivalence relations.
A key result links equivalence relations and partitions:
In both cases, the cells of the partition of "X" are the equivalence classes of "X" by ~. Since each element of "X" belongs to a unique cell of any partition of "X", and since each cell of the partition is identical to an equivalence class of "X" by ~, each element of "X" belongs to a unique equivalence class of "X" by ~. Thus there is a natural bijection between the set of all possible equivalence relations on "X" and the set of all partitions of "X".
Comparing equivalence relations.
If ~ and ≈ are two equivalence relations on the same set "S", and "a"~"b" implies "a"≈"b" for all "a","b" ∈ "S", then ≈ is said to be a coarser relation than ~, and ~ is a finer relation than ≈. Equivalently,
The equality equivalence relation is the finest equivalence relation on any set, while the trivial relation that makes all pairs of elements related is the coarsest.
The relation "~ is finer than ≈" on the collection of all equivalence relations on a fixed set is itself a partial order relation, which makes the collection a geometric lattice.
Algebraic structure.
Much of mathematics is grounded in the study of equivalences, and order relations. Lattice theory captures the mathematical structure of order relations. Even though equivalence relations are as ubiquitous in mathematics as order relations, the algebraic structure of equivalences is not as well known as that of orders. The former structure draws primarily on group theory and, to a lesser extent, on the theory of lattices, categories, and groupoids.
Group theory.
Just as order relations are grounded in ordered sets, sets closed under pairwise supremum and infimum, equivalence relations are grounded in partitioned sets, which are sets closed under bijections and preserve partition structure. Since all such bijections map an equivalence class onto itself, such bijections are also known as permutations. Hence permutation groups (also known as transformation groups) and the related notion of orbit shed light on the mathematical structure of equivalence relations.
Let '~' denote an equivalence relation over some nonempty set "A", called the universe or underlying set. Let "G" denote the set of bijective functions over "A" that preserve the partition structure of "A": ∀"x" ∈ "A" ∀"g" ∈ "G" ("g"("x") ∈ ["x"]). Then the following three connected theorems hold:
In sum, given an equivalence relation ~ over "A", there exists a transformation group "G" over "A" whose orbits are the equivalence classes of "A" under ~.
This transformation group characterisation of equivalence relations differs fundamentally from the way lattices characterize order relations. The arguments of the lattice theory operations meet and join are elements of some universe "A". Meanwhile, the arguments of the transformation group operations composition and inverse are elements of a set of bijections, "A" → "A".
Moving to groups in general, let "H" be a subgroup of some group "G". Let ~ be an equivalence relation on "G", such that "a" ~ "b" ↔ ("ab"−1 ∈ "H"). The equivalence classes of ~—also called the orbits of the action of "H" on "G"—are the right cosets of "H" in "G". Interchanging "a" and "b" yields the left cosets.
‡"Proof". Let function composition interpret group multiplication, and function inverse interpret group inverse. Then "G" is a group under composition, meaning that ∀"x" ∈ "A" ∀"g" ∈ "G" (["g"("x")] = ["x"]), because "G" satisfies the following four conditions:
Let "f" and "g" be any two elements of "G". By virtue of the definition of "G", ["g"("f"("x"))] = ["f"("x")] and ["f"("x")] = ["x"], so that ["g"("f"("x"))] = ["x"]. Hence "G" is also a transformation group (and an automorphism group) because function composition preserves the partitioning of "A". formula_14
Related thinking can be found in Rosen (2008: chpt. 10).
Categories and groupoids.
Let "G" be a set and let "~" denote an equivalence relation over "G". Then we can form a groupoid representing this equivalence relation as follows. The objects are the elements of "G", and for any two elements "x" and "y" of "G", there exists a unique morphism from "x" to "y" if and only if "x"~"y".
The advantages of regarding an equivalence relation as a special case of a groupoid include:
Lattices.
The possible equivalence relations on any set "X", when ordered by set inclusion, form a complete lattice, called Con "X" by convention. The canonical map ker: "X"^"X" → Con "X", relates the monoid "X"^"X" of all functions on "X" and Con "X". ker is surjective but not injective. Less formally, the equivalence relation ker on "X", takes each function "f": "X"→"X" to its kernel ker "f". Likewise, ker(ker) is an equivalence relation on "X"^"X".
Equivalence relations and mathematical logic.
Equivalence relations are a ready source of examples or counterexamples. For example, an equivalence relation with exactly two infinite equivalence classes is an easy example of a theory which is ω-categorical, but not categorical for any larger cardinal number.
An implication of model theory is that the properties defining a relation can be proved independent of each other (and hence necessary parts of the definition) if and only if, for each property, examples can be found of relations not satisfying the given property while satisfying all the other properties. Hence the three defining properties of equivalence relations can be proved mutually independent by the following three examples:
Properties definable in first-order logic that an equivalence relation may or may not possess include:
Euclidean relations.
Euclid's "The Elements" includes the following "Common Notion 1":
Nowadays, the property described by Common Notion 1 is called Euclidean (replacing "equal" by "are in relation with"). By "relation" is meant a binary relation, in which "aRb" is generally distinct from "bRa". A Euclidean relation thus comes in two forms:
The following theorem connects Euclidean relations and equivalence relations:
with an analogous proof for a right-Euclidean relation. Hence an equivalence relation is a relation that is "Euclidean" and "reflexive". "The Elements" mentions neither symmetry nor reflexivity, and Euclid probably would have deemed the reflexivity of equality too obvious to warrant explicit mention.

</doc>
<doc id="9260" url="https://en.wikipedia.org/wiki?curid=9260" title="Equivalence class">
Equivalence class

In mathematics, when a set has an equivalence relation defined on its elements, there is a natural grouping of elements that are related to one another, forming what are called equivalence classes. Notationally, given a set and an equivalence relation on , the "equivalence class" of an element in is the subset of all elements in which are equivalent to . It follows from the definition of the equivalence relations that the equivalence classes form a partition of . The set of equivalence classes is sometimes called the quotient set or the quotient space of by and is denoted by .
When has some structure, and the equivalence relation is defined with some connection to this structure, the quotient set often inherits some related structure. Examples include quotient spaces in linear algebra, quotient spaces in topology, quotient groups, homogeneous spaces, quotient rings, quotient monoids, and quotient categories.
Notation and formal definition.
An equivalence relation is a binary relation satisfying three properties:
The equivalence class of an element is denoted and is defined as the set
of elements that are related to by . An alternative notation can be used to denote the equivalence class of the element , specifically with respect to the equivalence relation . This is said to be the -equivalence class of .
The set of all equivalence classes in with respect to an equivalence relation is denoted as and called modulo (or the quotient set of by ). The surjective map formula_2 from onto , which maps each element to its equivalence class, is called the canonical surjection or the canonical projection map.
When an element is chosen (often implicitly) in each equivalence class, this defines an injective map called a "section". If this section is denoted by , one has for every equivalence class . The element is called a representative of . Any element of a class may be chosen as a representative of the class, by choosing the section appropriately.
Sometimes, there is a section that is more "natural" than the other ones. In this case, the representatives are called "canonical representatives". For example, in modular arithmetic, consider the equivalence relation on the integers defined by if is a multiple of a given integer , called the "modulus". Each class contains a unique non-negative integer smaller than , and these integers are the canonical representatives. The class and its representative are more or less identified, as is witnessed by the fact that the notation may denote either the class or its canonical representative (which is the remainder of the division of by ).
Properties.
Every element of is a member of the equivalence class . Every two equivalence classes and are either equal or disjoint. Therefore, the set of all equivalence classes of forms a partition of : every element of belongs to one and only one equivalence class. Conversely every partition of comes from an equivalence relation in this way, according to which if and only if and belong to the same set of the partition.
It follows from the properties of an equivalence relation that
In other words, if is an equivalence relation on a set , and and are two elements of , then these statements are equivalent:
Graphical representation.
Any binary relation can be represented by a directed graph and symmetric ones, such as equivalence relations, by undirected graphs. If is an equivalence relation on a set , let the vertices of the graph be the elements of and join vertices and if and only if . The equivalence classes are represented in this graph by the maximal cliques forming the connected components of the graph.
Invariants.
If is an equivalence relation on , and is a property of elements of such that whenever , is true if is true, then the property is said to be an invariant of , or well-defined under the relation .
A frequent particular case occurs when is a function from to another set ; if whenever , then is said to be a "morphism" for , a "class invariant under" , or simply "invariant under" . This occurs, e.g. in the character theory of finite groups. Some authors use "compatible with " or just "respects " instead of "invariant under ".
Any function itself defines an equivalence relation on according to which if and only if . The equivalence class of is the set of all elements in which get mapped to , i.e. the class is the inverse image of . This equivalence relation is known as the kernel of .
More generally, a function may map equivalent arguments (under an equivalence relation on ) to equivalent values (under an equivalence relation on ). Such a function is known as a morphism from to .
Quotient space in topology.
In topology, a quotient space is a topological space formed on the set of equivalence classes of an equivalence relation on a topological space using the original space's topology to create the topology on the set of equivalence classes.
In abstract algebra, congruence relations on the underlying set of an algebra allow the algebra to induce an algebra on the equivalence classes of the relation, called a quotient algebra. In linear algebra, a quotient space is a vector space formed by taking a quotient group where the quotient homomorphism is a linear map. By extension, in abstract algebra, the term quotient space may be used for quotient modules, quotient rings, quotient groups, or any quotient algebra. However, the use of the term for the more general cases can as often be by analogy with the orbits of a group action.
The orbits of a group action on a set may be called the quotient space of the action on the set, particularly when the orbits of the group action are the right cosets of a subgroup of a group, which arise from the action of the subgroup on the group by left translations, or respectively the left cosets as orbits under right translation.
A normal subgroup of a topological group, acting on the group by translation action, is a quotient space in the senses of topology, abstract algebra, and group actions simultaneously.
Although the term can be used for any equivalence relation's set of equivalence classes, possibly with further structure, the intent of using the term is generally to compare that type of equivalence relation on a set "X" either to an equivalence relation that induces some structure on the set of equivalence classes from a structure of the same kind on "X", or to the orbits of a group action. Both the sense of a structure preserved by an equivalence relation and the study of invariants under group actions lead to the definition of invariants of equivalence relations given above.
Further reading.
This material is basic and can be found in any text dealing with the fundamentals of proof technique, such as any of the following:

</doc>
