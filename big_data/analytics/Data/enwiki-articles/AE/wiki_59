<doc id="27992" url="https://en.wikipedia.org/wiki?curid=27992" title="Slavery">
Slavery

Slavery is a legal or economic system in which principles of property law are applied to humans allowing them to be classified as property, to be owned, bought and sold accordingly, and they cannot withdraw unilaterally from the arrangement. While a person is a slave, the owner is entitled to the productivity of the slave's labour, without any remuneration. The rights and protection of the slave may be regulated by laws and customs in a particular time and place, and a person may become a slave from the time of their capture, purchase or birth.
Today, chattel slavery is unlawful in all countries, but a person may still be described as a slave if he or she is forced to work for another person without an ability on their part to unilaterally terminate the arrangement. Such situations are today commonly referred to as "practices similar to slavery". The present form of the slave trade is commonly referred to as human trafficking.
Slavery existed before written history and in many cultures. It was once institutionally recognized by most societies, but has now been outlawed in all countries, the last being Mauritania in 2007. However, it continues through such practices as debt bondage, serfdom, domestic servants kept in captivity, certain adoptions in which children are forced to work as slaves, child soldiers, human trafficking, and forced marriage. Accordingly, there are more slaves today than at any time in history, with an estimated 20 million to 36 million slaves worldwide.
Terminology.
The English word "slave" comes from Old French "sclave", from the Medieval Latin "sclavus", from the Byzantine Greek σκλάβος, which, in turn, comes from the ethnonym "Slav", because in some early Medieval wars many Slavs were captured and enslaved. An older theory connected it to the Greek verb "skyleúo" 'to strip a slain enemy'.
There is a dispute among modern historians about whether the term "enslaved person" rather than "slave" should be used when describing the victims of slavery. According to those proposing a change in terminology, "slave" perpetuates the crime of slavery in language, by reducing its victims to a nonhuman noun instead of, according to Andi Cumbo-Floyd, "carry them forward as people, not the property that they were". Other historians prefer "slave" because the term is familiar and shorter, or because it accurately reflects the inhumanity of slavery, with "person" implying a degree of autonomy that slavery did not allow for.
Types.
Chattel slavery.
Chattel slavery, also called traditional slavery, is so named because people are treated as the chattel (personal property) of the owner and are bought and sold as if they were commodities. It is the least prevalent form of slavery in the world today.
Bonded labor.
Debt bondage or bonded labor occurs when a person pledges himself or herself against a loan. The services required to repay the debt, and their duration, may be undefined. Debt bondage can be passed on from generation to generation, with children required to pay off their parents' debt. It is the most widespread form of slavery today. Debt bondage is most prevalent in South Asia.
Forced labor.
Forced labor occurs when an individual is forced to work against his or her will, under threat of violence or other punishment, with restrictions on their freedom. Human trafficking is primarily used for prostituting women and children and is the fastest growing form of forced labor, with Thailand, Cambodia, India, Brazil and Mexico having been identified as leading hotspots of commercial sexual exploitation of children.
The term 'forced labor' is also used to describe all types of slavery and may also include institutions not commonly classified as slavery, such as serfdom, conscription and penal labor.
Forced marriage.
A forced marriage may be regarded as a form of slavery by one or more of the parties involved in the marriage, as well as by people observing the marriage. People forced into marriage can be required to engage in sexual activity or to perform domestic duties or other work without any personal control. The customs of bride price and dowry that exist in many parts of the world can lead to buying and selling people into marriage. Forced marriage continues to be practiced in parts of the world including some parts of Asia and Africa. Forced marriages may also occur in immigrant communities in Europe, the United States, Canada and Australia. Marriage by abduction occurs in many places in the world today, with a national average of 69% of marriages in Ethiopia being through abduction.
The International Labour Organisation defines child and forced marriage as forms of modern-day slavery.
Dependency.
The word "slave" has also been used to express a general dependency from somebody else. In many cases, such as in ancient Persia, the situation and lives of such slaves could be better than those of other common citizens.
Contemporary slavery.
Even though slavery is now outlawed in every country, the number of slaves today is estimated as between 12 million and 29.8 million. Several estimates of the number of slaves in the world have been provided. According to a broad definition of slavery used by Kevin Bales of Free the Slaves (FTS), an advocacy group linked with Anti-Slavery International, there were 27 million people in slavery in 1999, spread all over the world. In 2005, the International Labour Organization provided an estimate of 12.3 million forced labourers. Siddharth Kara has also provided an estimate of 28.4 million slaves at the end of 2006 divided into three categories: bonded labour/debt bondage (18.1 million), forced labour (7.6 million), and trafficked slaves (2.7 million). Kara provides a dynamic model to calculate the number of slaves in the world each year, with an estimated 29.2 million at the end of 2009. According to a 2003 report by Human Rights Watch, an estimated 15 million children in debt bondage in India work in slavery-like conditions to pay off their family's debts.
Distribution.
A report by the Walk Free Foundation in 2013, found India had the highest number of slaves, nearly 14 million, followed by China (2.9 million), Pakistan (2.1 million), Nigeria, Ethiopia, Russia, Thailand, Democratic Republic of Congo, Myanmar and Bangladesh; while the countries with the highest of proportion of slaves were Mauritania, Haiti, Pakistan, India and Nepal.
In June 2013, U.S. State Department released a report on slavery, it placed Russia, China, Uzbekistan in the worst offenders category, Cuba, Iran, North Korea, Sudan, Syria, and Zimbabwe were also at the lowest level. The list also included Algeria, Libya, Saudi Arabia and Kuwait among a total of 21 countries.
Economics.
While American slaves in 1809 were sold for around $40,000 (in inflation adjusted dollars), a slave nowadays can be bought for just $90, making replacement more economical than providing long term care. Slavery is a multibillion-dollar industry with estimates of up to $35 billion generated annually.
Trafficking.
Trafficking in human beings (also called human trafficking) is one method of obtaining slaves. Victims are typically recruited through deceit or trickery (such as a false job offer, false migration offer, or false marriage offer), sale by family members, recruitment by former slaves, or outright abduction. Victims are forced into a "debt slavery" situation by coercion, deception, fraud, intimidation, isolation, threat, physical force, debt bondage or even force-feeding with drugs of abuse to control their victims. "Annually, according to U.S. government-sponsored research completed in 2006, approximately 800,000 people are trafficked across national borders, which does not include millions trafficked within their own countries. Approximately 80 percent of transnational victims are women and girls and up to 50 percent are minors", reports the U.S. State Department in a 2008 study.
While the majority of trafficking victims are women, and sometimes children, who are forced into prostitution (in which case the practice is called sex trafficking), victims also include men, women and children who are forced into manual labour. Due to the illegal nature of human trafficking, its exact extent is unknown. A U.S. government report published in 2005, estimates that 600,000 to 800,000 people worldwide are trafficked across borders each year. This figure does not include those who are trafficked internally. Another research effort revealed that between 1.5 million and 1.8 million individuals are trafficked either internally or internationally each year, 500,000 to 600,000 of whom are sex trafficking victims.
Examples.
Examples of modern slavery are numerous. Child slavery has commonly been used in the production of cash crops and mining.
Asia.
In 2008, the Nepalese government abolished the Haliya system, under which 20,000 people were forced to provide free farm labour. Though slavery was officially abolished in China in 1910, the practice continues unofficially in some regions of the country. In June and July 2007, 550 people who had been enslaved by brick manufacturers in Shanxi and Henan were freed by the Chinese government. Among those rescued were 69 children. In response, the Chinese government assembled a force of 35,000 police to check northern Chinese brick kilns for slaves, sent dozens of kiln supervisors to prison, punished 95 officials in Shanxi province for dereliction of duty, and sentenced one kiln foreman to death for killing an enslaved worker. The North Korean government operates six large political prison camps, where political prisoners and their families (around 200,000 people) in lifelong detention are subjected to hard slave labor, torture and inhumane treatment.
South America and Caribbean.
In 2008, in Brazil about 5,000 slaves were rescued by government authorities as part of an initiative to eradicate slavery, which was reported as ongoing in 2010. Poverty has forced at least 225,000 Haitian children to work as restavecs (unpaid household servants); the United Nations considers this to be a form of slavery.
Middle East.
Some tribal sheiks in Iraq still keep blacks, called "Abd", which means servant or slave in Arabic, as slaves.
According to media reports from late 2014 the Islamic State of Iraq and the Levant (ISIL) was selling Yazidi and Christian women as slaves. According to Haleh Esfandiari of the Woodrow Wilson International Center for Scholars, after ISIL militants have captured an area "usually take the older women to a makeshift slave market and try to sell them." In mid-October 2014, the UN estimated that 5,000 to 7,000 Yazidi women and children were abducted by ISIL and sold into slavery. In the digital magazine "Dabiq", ISIL claimed religious justification for enslaving Yazidi women whom they consider to be from a heretical sect. ISIL claimed that the Yazidi are idol worshipers and their enslavement part of the old shariah practice of spoils of war. According to "The Wall Street Journal", ISIL appeals to apocalyptic beliefs and claims "justification by a Hadith that they interpret as portraying the revival of slavery as a precursor to the end of the world".
Africa.
In Mauritania, the last country to abolish slavery (in 1981), it is estimated that up to 600,000 men, women and children, or 20% of the population, are enslaved with many used as bonded labour. Slavery in Mauritania was criminalized in August 2007. (although slavery as a practice was legally banned in 1981, it was not a crime to own a slave until 2007). Although many slaves have escaped or have been freed since 2007, , only one slave-owner had been sentenced to serve time in prison.
An article in the "Middle East Quarterly" in 1999 reported that slavery is endemic in Sudan. Estimates of abductions during the Second Sudanese Civil War range from 14,000 to 200,000 people.
In Niger, slavery is also a current phenomenon. A Nigerien study has found that more than 800,000 people are enslaved, almost 8% of the population. Niger installed anti slavery provision in 2003.
Many pygmies in the Republic of Congo and Democratic Republic of Congo belong from birth to Bantus in a system of slavery.
According to the U.S. State Department, more than 109,000 children were working on cocoa farms alone in Ivory Coast in "the worst forms of child labor" in 2002.
On the night of 14–15 April 2014, a group of militants attacked the Government Girls Secondary School in Chibok, Nigeria. They broke into the school, pretending to be guards, telling the girls to get out and come with them. A large number of students were taken away in trucks, possibly into the Konduga area of the Sambisa Forest where Boko Haram were known to have fortified camps. Houses in Chibok were also burned down in the incident. According to police, approximately 276 children were taken in the attack, of whom 53 had escaped as of 2 May. Other reports said that 329 girls were kidnapped, 53 had escaped and 276 were still missing. The students have been forced to convert to Islam and into marriage with members of Boko Haram, with a reputed "bride price" of ₦2,000 each ($12.50/£7.50). Many of the students were taken to the neighbouring countries of Chad and Cameroon, with sightings reported of the students crossing borders with the militants, and sightings of the students by villagers living in the Sambisa Forest, which is considered a refuge for Boko Haram.
On May 5, 2014 a video in which Boko Haram leader Abubakar Shekau claimed responsibility for the kidnappings emerged. Shekau claimed that "Allah instructed me to sell them...I will carry out his instructions" and ", and I shall capture people and make them slaves." He said the girls should not have been in school and instead should have been married since girls as young as nine are suitable for marriage.
History.
Early history.
Evidence of slavery predates written records, and has existed in many cultures. Graves dating to 8000 BC in Egypt may show the enslavement of a San-like tribe. Slavery is rare among hunter-gatherer populations. Mass slavery also requires economic surpluses and a high population density to be viable. Due to these factors, the practice of slavery would have only proliferated after the invention of agriculture during the Neolithic Revolution about 11,000 years ago.
In the earliest known records, slavery is treated as an established institution. The Code of Hammurabi (ca. 1760 BC), for example, prescribed death for anyone who helped a slave escape or who sheltered a fugitive. The Bible mentions slavery as an established institution.
Slavery was known in almost every ancient civilization and society including Sumer, Ancient Egypt, Ancient China, the Akkadian Empire, Assyria, Ancient India, Ancient Greece, the Roman Empire, the Hebrew kingdoms of the ancient Levant, and the pre-Columbian civilizations of the Americas. Such institutions included debt-slavery, punishment for crime, the enslavement of prisoners of war, child abandonment, and the birth of slave children to slaves.
Classical antiquity.
Records of slavery in Ancient Greece date as far back as Mycenaean Greece. It is certain that Classical Athens had the largest slave population, with as many as 80,000 in the 6th and 5th centuries BC; two to four-fifths of the population were slaves. As the Roman Republic expanded outward, entire populations were enslaved, thus creating an ample supply from all over Europe and the Mediterranean. Greeks, Illyrians, Berbers, Germans, Britons, Thracians, Gauls, Jews, Arabs, and many more were slaves used not only for labour, but also for amusement (e. g. gladiators and sex slaves). This oppression by an elite minority eventually led to slave revolts (see Roman Servile Wars); the Third Servile War led by Spartacus ("a Thracian") being the most famous and bitter.
By the late Republican era, slavery had become a vital economic pillar in the wealth of Rome, as well as a very significant part of Roman society. It is estimated that 25% or more of the population of Ancient Rome was enslaved. Slaves represented 15–25% of Italy's population, mostly captives in war especially from Gaul and Epirus. Estimates of the number of slaves in the Roman Empire suggest that the majority of slaves were scattered throughout the provinces outside of Italy. Generally, slaves in Italy were indigenous Italians, with a minority of foreigners (including both slaves and freedmen) born outside of Italy estimated at 5% of the total in the capital at its peak, where their number was largest. Those from outside of Europe were predominantly of Greek descent, while the Jewish ones never fully assimilated into Roman society, remaining an identifiable minority. These slaves (especially the foreigners) had higher death rates and lower birth rates than natives, and were sometimes even subjected to mass expulsions. The average recorded age at death for the slaves of the city of Rome was extraordinarily low: seventeen and a half years (17.2 for males; 17.9 for females).
Middle Ages.
Medieval and Early Modern Europe.
Large-scale trading in slaves was mainly confined to the South and East of early medieval Europe: the Byzantine Empire and the Muslim world were the destinations, while pagan Central and Eastern Europe (along with the Caucasus and Tartary) were important sources. Viking, Arab, Greek, and Radhanite Jewish merchants were all involved in the slave trade during the Early Middle Ages. The trade in European slaves reached a peak in the 10th century following the Zanj rebellion which dampened the use of African slaves in the Arab world.
Medieval Spain and Portugal were the scene of almost constant Muslim invasion of the predominantly Christian area. Periodic raiding expeditions were sent from Al-Andalus to ravage the Iberian Christian kingdoms, bringing back booty and slaves. In raid against Lisbon, Portugal in 1189, for example, the Almohad caliph Yaqub al-Mansur took 3,000 female and child captives, while his governor of Córdoba, in a subsequent attack upon Silves, Portugal in 1191, took 3,000 Christian slaves. From the 11th to the 19th century, North African Barbary Pirates engaged in "Razzias", raids on European coastal towns, to capture Christian slaves to sell at slave markets in places such as Algeria and Morocco.
In Britain, slavery continued to be practiced following the fall of Rome and sections of Hywel the Good's laws dealt with slaves in medieval Wales. The trade particularly picked up after the Viking invasions, with major markets at Chester and Bristol supplied by Danish, Mercian, and Welsh raiding of one another's borderlands. At the time of the "Domesday Book", nearly 10% of the English population were slaves. Slavery in early medieval Europe was so common that the Roman Catholic Church repeatedly prohibited it — or at least the export of Christian slaves to non-Christian lands was prohibited at e.g. the Council of Koblenz (922), the Council of London (1102), and the Council of Armagh (1171). In 1452, Pope Nicholas V issued the papal bull Dum Diversas, granting the kings of Spain and Portugal the right to reduce any "Saracens (antiquated term referring to Muslims), pagans and any other unbelievers" to perpetual slavery, legitimizing the slave trade as a result of war. The approval of slavery under these conditions was reaffirmed and extended in his Romanus Pontifex bull of 1455. However, Pope Paul III forbade enslavement of the Native Americans in 1537 in his papal bull Sublimus Dei. Dominican friars who arrived at the Spanish settlement at Santo Domingo strongly denounced the enslavement of the local Native Americans. Along with other priests, they opposed their treatment as unjust and illegal in an audience with the Spanish king and in the subsequent royal commission.
The Byzantine-Ottoman wars and the Ottoman wars in Europe brought large numbers of slaves into the Islamic world. To staff its bureaucracy, the Ottoman Empire established a janissary system which seized hundreds of thousands of Christian boys through the devşirme system. They were well cared for but were legally slaves owned by the government and were not allowed to marry. They were never bought or sold. The Empire gave them significant administrative and military roles. The system began about 1365; there were 135,000 janissaries in 1826, when the system ended. After the Battle of Lepanto, 12,000 Christian galley slaves were recaptured and freed from the Ottoman fleet. Eastern Europe suffered a series of Tatar invasions, the goal of which was to loot and capture slaves into "jasyr". Seventy-five Crimean Tatar raids were recorded into Poland–Lithuania between 1474 and 1569.
Approximately 10–20% of the rural population of Carolingian Europe consisted of slaves. Slavery largely disappeared from Western Europe by the later Middle Ages. The slave trade became illegal in England in 1102, but England went on to become very active in the lucrative Atlantic slave trade from the seventeenth to the early nineteenth century. In Scandinavia, thralldom was abolished in the mid-14th century. Slavery persisted longer in Eastern Europe. Slavery in Poland was forbidden in the 15th century; in Lithuania, slavery was formally abolished in 1588; they were replaced by the second serfdom. In Kievan Rus and Muscovy, slaves were usually classified as kholops.
Arab slave trade.
In early Islamic states of the Western Sudan (present-day West Africa), including Ghana (750–1076), Mali (1235–1645), Segou (1712–1861), and Songhai (1275–1591), about a third of the population was enslaved.
Slaves were purchased or captured on the frontiers of the Islamic world and then imported to the major centers, where there were slave markets from which they were widely distributed. In the 9th and 10th centuries, the black Zanj slaves may have constituted at least a half of the total population of lower Iraq. At the same time, many slaves in the region were also imported from Central Asia and the Caucasus. Many slaves were taken in the wars with the Christian nations of medieval Europe.
Modern history.
Europe.
Author David P. Forsythe has written: "In 1649 up to three-quarters of Muscovy's peasants, or 13 to 14 million people, were serfs whose material lives were barely distinguishable from slaves. Perhaps another 1.5 million were formally enslaved, with Russian slaves serving Russian masters." Slavery remained a major institution in Russia until 1723, when Peter the Great converted the household slaves into house serfs. Russian agricultural slaves were formally converted into serfs earlier in 1679. Russia's more than 23 million privately held serfs were freed by the Emancipation reform of 1861. State-owned serfs were emancipated in 1866.
Until the late 18th century, the Crimean Khanate (a Muslim Tatar state) maintained a massive slave trade with the Ottoman Empire and the Middle East, exporting about 2 million slaves from Poland-Lithuania and Russia over the period 1500–1700.
During the Second World War (1939–1945) Nazi Germany effectively enslaved about 12 million people, both those considered undesirable and citizens of countries they conquered.
Africa.
In Algiers, the capital of Algeria, Christians and Europeans that were captured had been forced into slavery. This eventually led to the Bombardment of Algiers in 1816.
Half the population of the Sokoto caliphate of the 19th century were slaves. The Swahili-Arab slave trade reached its height about years ago, when, for example, approximately 20,000 slaves were considered to be carried yearly from Nkhotakota on Lake Malawi to Kilwa. Roughly half the population of Madagascar was enslaved.
According to the "Encyclopedia of African History", "It is estimated that by the 1890s the largest slave population of the world, about 2 million people, was concentrated in the territories of the Sokoto Caliphate. The use of slave labor was extensive, especially in agriculture." The Anti-Slavery Society estimated there were 2 million slaves in Ethiopia in the early 1930s out of an estimated population of 8 to 16 million.
Hugh Clapperton in 1824 believed that half the population of Kano were enslaved people. W. A. Veenhoven wrote: "The German doctor, Gustav Nachtigal, an eye-witness, believed that for every slave who arrived at a market three or four died on the way ... Keltie ("The Partition of Africa", London, 1920) believes that for every slave the Arabs brought to the coast at least six died on the way or during the slavers' raid. Livingstone puts the figure as high as ten to one."
One of the most famous slave traders on the eastern Zanj (Bantu) coast was Tippu Tip, who was the grandson of a slave. The "prazeros" were slave traders along the Zambezi. North of the Zambezi, the waYao and Makua people played a similar role as professional slave raiders and traders. Still further north were the Nyamwezi slave traders.
Asia.
In Constantinople, about one-fifth of the population consisted of slaves. The city was a major center of the slave trade in the 15th and later centuries. By 1475 most of the slaves were provided by Tatar raids on Slavic villages. It has been estimated that some 200,000 slaves—mainly Circassians—were imported into the Ottoman Empire between 1800 and 1909. As late as 1908, women slaves were still sold in the Ottoman Empire. A slave market for captured Russian and Persian slaves was centred in the Central Asian khanate of Khiva. In the early 1840s, the population of the Uzbek states of Bukhara and Khiva included about 900,000 slaves. Darrel P. Kaiser wrote, "Kazakh-Kirghiz tribesmen kidnapped 1573 settlers from colonies settlements in Russia in 1774 alone and only half were successfully ransomed. The rest were killed or enslaved."
According to Sir Henry Bartle Frere (who sat on the Viceroy's Council), there were an estimated 8 or 9 million slaves in India in 1841. About 15% of the population of Malabar were slaves. Slavery was abolished in British India by the Indian Slavery Act V. of 1843.
In East Asia, the Imperial government formally abolished slavery in China in 1906, and the law became effective in 1910. The Nangzan in Tibetan history were, according to Chinese sources, hereditary household slaves.
Indigenous slaves existed in Korea. Slavery was abolished with the Gabo Reform of 1894, but continued in reality until 1930. During the Joseon Dynasty (1392–1910), about 30% to 50% of the Korean population were slaves. In late 16th century Japan, slavery as such was officially banned, but forms of contract and indentured labor persisted alongside the period penal codes' forced labor.
The hill tribe people in Indochina were "hunted incessantly and carried off as slaves by the Siamese (Thai), the Anamites (Vietnamese), and the Cambodians.." A Siamese military campaign in Laos in 1876 was described by a British observer as having been "transformed into slave-hunting raids on a large scale". The census, taken in 1879, showed that 6% of the population in the Malay sultanate of Perak were slaves. Enslaved people made up about two-thirds of the population in part of North Borneo in the 1880s.
Americas.
Slavery in the Americas had a contentious history, and played a major role in the history and evolution of some countries, triggering at least one revolution and one civil war, as well as numerous rebellions. The Aztecs had slaves. Other Amerindians, such as the Inca of the Andes, the Tupinambá of Brazil, the Creek of Georgia, and the Comanche of Texas, also owned slaves.
The maritime town of Lagos was the first slave market created in Portugal (one of the earliest colonizers of the Americas) for the sale of imported African slaves—the "Mercado de Escravos", opened in 1444. In 1441, the first slaves were brought to Portugal from northern Mauritania.
In 1519, Mexico's first Afro-Mexican slave was brought by Hernán Cortés.
By 1552, black African slaves made up 10% of the population of Lisbon. In the second half of the 16th century, the Crown gave up the monopoly on slave trade and the focus of European trade in African slaves shifted from import to Europe to slave transports directly to tropical colonies in the Americas—in the case of Portugal, especially Brazil. In the 15th century one-third of the slaves were resold to the African market in exchange of gold.
In order to establish itself as an American empire, Spain had to fight against the relatively powerful civilizations of the New World. The Spanish conquest of the indigenous peoples in the Americas included using the Natives as forced labour. The Spanish colonies were the first Europeans to use African slaves in the New World on islands such as Cuba and Hispaniola, see Atlantic slave trade.
Bartolomé de Las Casas a 16th-century Dominican friar and Spanish historian participated in campaigns in Cuba (at Bayamo and Camagüey) and was present at the massacre of Hatuey; his observation of that massacre led him to fight for a social movement away from the use of natives as slaves and towards the importation of African Blacks as slaves. Also, the alarming decline in the native population had spurred the first royal laws protecting the native population (Laws of Burgos, 1512–1513).
The first African slaves arrived in Hispaniola in 1501. In 1518, Charles I of Spain agreed to ship slaves directly from Africa. England played a prominent role in the Atlantic slave trade. The "slave triangle" was pioneered by Francis Drake and his associates. In 1640 a Virginia court sentenced John Punch to slavery, forcing him to serve his master, Hugh Gwyn, for the remainder of his life. This was the first legal sanctioning of slavery in the English colonies. In 1655, A black man, Anthony Johnson of Virginia, was granted ownership of John Casor as the result of a civil case.
The "Henrietta Marie" was probably built in France sometime in the 17th century and carried a crew of about eighteen men. The ship came into English possession late in the 17th century, possibly as a war prize during the War of the Grand Alliance. It was put to use in the Atlantic slave trade, making at least two voyages carrying Africans to slavery in the West Indies. On its first voyage, in 1697–1698, the ship carried more than 200 people from Africa that were sold as slaves in Barbados. In 1699 the "Henrietta Marie" sailed from England on the first leg of the triangular trade route with a load of trade goods, including iron and copper bars, pewter utensils, glass beads, cloth and brandy. The ship sailed under license from the Royal African Company (which held a monopoly on English trade with Africa), in exchange for ten percent of the profits of the voyage. It is known to have traded for African captives at New Calabar on the Guinea Coast. The ship then sailed on the second leg of its voyage, from Africa to the West Indies, and in May 1701 landed 191 Africans for sale in Port Royal, Jamaica. The "Henrietta Marie" then loaded a cargo of sugar, cotton, dyewoods (indigo) and ginger to take back to England on the third leg of the triangular route. After leaving Port Royal on 18 May 1701, the ship headed for the Yucatán Channel to pass around the western end of Cuba (thus avoiding the pirates infesting the passage between Cuba and Hispaniola) and catch the Gulf Stream, the preferred route for all ships leaving the Caribbean to return to Europe. A month later, the "Henrietta Marie" wrecked on New Ground Reef near the Marquesas Keys, approximately west of Key West. All aboard were lost.
Pirates often targeted slavers. For example, the 300 ton English frigate "Concord" launched in 1710 but was captured by the French one year later. She was modified to hold more cargo, including slaves, and renamed "La Concorde de Nantes". Sailing as a slave ship, she was captured by the pirate Captain Benjamin Hornigold on November 28, 1717, near the island of Martinique. Hornigold turned her over to one of his men, Edward Teach (later known as Blackbeard), and made him her captain. Teach then renamed her the "Queen Anne's Revenge". By 1750, slavery was a legal institution in all of the 13 American colonies, and the profits of the slave trade and of West Indian plantations amounted to 5% of the British economy at the time of the Industrial Revolution.
The trans-Atlantic slave trade peaked in the late 18th century, when the largest number of slaves were captured on raiding expeditions into the interior of West Africa. These expeditions were typically carried out by African kingdoms, such as the Oyo empire (Yoruba), the Ashanti Empire, the kingdom of Dahomey, and the Aro Confederacy. Europeans rarely entered the interior of Africa, due to fierce African resistance. The slaves were brought to coastal outposts where they were traded for goods. A significant portion of African Americans in North America are descended from Mandinka people. Through a series of conflicts, primarily with the Fulani Jihad States, about half of the Senegambian Mandinka were converted to Islam while as many as a third were sold into slavery to the Americas through capture in conflict.
An estimated 12 million Africans arrived in the Americas from the 16th to the 19th centuries. Of these, an estimated 645,000 were brought to what is now the United States. The usual estimate is that about 15% of slaves died during the voyage, with mortality rates considerably higher in Africa itself in the process of capturing and transporting indigenous peoples to the ships. Approximately 6 million black Africans were killed by others in tribal wars.
Many Europeans who arrived in North America during the 17th and 18th centuries came under contract as indentured servants. The transformation from indentured servitude to slavery was a gradual process in Virginia. The earliest legal documentation of such a shift was in 1640 where a negro, John Punch, was sentenced to lifetime slavery for attempting to run away. This case also marked the disparate treatment of Africans as held by the Virginia County Court, as two white runaways received far lesser sentences. After 1640, planters started to ignore the expiration of indentured contracts and kept their servants as slaves for life. This was demonstrated by the case Johnson v. Parker, where the court ruled that John Casor, an indentured servant, be returned to Johnson who claimed that Casor belonged to him for his life. According to the 1860 U. S. census, 393,975 individuals, representing 8% of all US families, owned 3,950,528 slaves. One-third of Southern families owned slaves.
The largest number of slaves were shipped to Brazil. In the Spanish viceroyalty of New Granada, corresponding mainly to modern Panama, Colombia, and Venezuela, the free black population in 1789 was 420,000, whereas African slaves numbered only 20,000. Free blacks also outnumbered slaves in Brazil. By contrast, in Cuba, free blacks made up only 15% in 1827; and in the French colony of Saint-Domingue (present-day Haiti) it was a mere 5% in 1789.
Author Charles Rappleye argued that
Although the trans-Atlantic slave trade ended shortly after the American Revolution, slavery remained a central economic institution in the Southern states of the United States, from where slavery expanded with the westward movement of population. Historian Peter Kolchin wrote, "By breaking up existing families and forcing slaves to relocate far from everyone and everything they knew" this migration "replicated (if on a reduced level) many of horrors" of the Atlantic slave trade.
Historian Ira Berlin called this forced migration the Second Middle Passage. Characterizing it as the "central event" in the life of a slave between the American Revolution and the Civil War, Berlin wrote that whether they were uprooted themselves or simply lived in fear that they or their families would be involuntarily moved, "the massive deportation traumatized black people, both slave and free.."
By 1860, 500,000 slaves had grown to 4 million. As long as slavery expanded, it remained profitable and powerful and was unlikely to disappear. Although complete statistics are lacking, it is estimated that 1,000,000 slaves moved west from the Old South between 1790 and 1860.
Most of the slaves were moved from Maryland, Virginia, and the Carolinas. Michael Tadman, in a 1989 book "Speculators and Slaves: Masters, Traders, and Slaves in the Old South", indicates that 60–70% of interregional migrations were the result of the sale of slaves. In 1820, a child in the Upper South had a 30% chance to be sold south by 1860.
In Puerto Rico, African slavery was finally abolished on March 22, 1873.
Middle East.
According to Robert Davis, between 1 million and 1.25 million Europeans were captured by Barbary pirates and sold as slaves in North Africa and Ottoman Empire between the 16th and 19th centuries. There was also an extensive trade in Christian slaves in the Black Sea region for several centuries until the Crimean Khanate was destroyed by the Russian Empire in 1783. In the 1570s close to 20,000 slaves a year were being sold in the Crimean port of Kaffa. The slaves were captured in southern Russia, Poland-Lithuania, Moldavia, Wallachia, and Circassia by Tatar horsemen. Some researchers estimate that altogether more than 3 million people were captured and enslaved during the time of the Crimean Khanate.
The Arab slave trade lasted more than a millennium. As recently as the early 1960s, Saudi Arabia's slave population was estimated at 300,000. Along with Yemen, the Saudis abolished slavery only in 1962. Slaves in the Arab World came from many different regions, including Sub-Saharan Africa (mainly "Zanj"), the Caucasus (mainly Circassians), Central Asia (mainly Tartars), and Central and Eastern Europe (mainly "Saqaliba").
Under Omani Arabs Zanzibar became East Africa's main slave port, with as many as 50,000 enslaved Africans passing through every year during the 19th century. Some historians estimate that between 11 and 18 million African slaves crossed the Red Sea, Indian Ocean, and Sahara Desert from 650 to 1900 AD. Eduard Rüppell described the losses of Sudanese slaves being transported on foot to Egypt: "after the Daftardar bey's 1822 campaign in the southern Nuba mountains, nearly 40,000 slaves were captured. However, through bad treatment, disease and desert travel barely 5000 made it to Egypt.."
The Moors, starting in the 8th century, also raided coastal areas around the Mediterranean and Atlantic Ocean, and became known as the Barbary pirates. It is estimated that they captured 1.25 million white slaves from Western Europe and North America between the 16th and 19th centuries. The mortality rate was very high. For instance, plague killed a third to two-thirds of the 30,000 occupants of the slave pens in Algiers in 1662.
Abolitionism.
Slavery has existed, in one form or another, through recorded human history—as have, in various periods, movements to free large or distinct groups of slaves.
Ashoka, who ruled the Maurya Empire from 269–232 BCE, abolished the slave trade but not slavery. The Qin dynasty, which ruled China from 221 to 206 BC, abolished slavery and discouraged serfdom. However, many of its laws were overturned when the dynasty was overthrown. Slavery was again abolished, by Wang Mang, in China in 17 C.E but was reinstituted after his assassination.
The Spanish colonization of the Americas sparked a discussion about the right to enslave Native Americans. A prominent critic of slavery in the Spanish New World colonies was Bartolomé de las Casas, who opposed the enslavement of Native Americans, and later also of Africans in America.
One of the first protests against slavery came from German and Dutch Quakers in Pennsylvania in 1688. One of the most significant milestones in the campaign to abolish slavery throughout the world occurred in England in 1772, with British judge Lord Mansfield, whose opinion in Somersett's Case was widely taken to have held that slavery was illegal in England. This judgement also laid down the principle that slavery contracted in other jurisdictions could not be enforced in England. In 1777, Vermont, at the time an independent nation, became the first portion of what would become the United States to abolish slavery. France abolished slavery in 1794.
British Member of Parliament William Wilberforce led the anti-slavery movement in the United Kingdom, although the groundwork was an anti-slavery essay by Thomas Clarkson. Wilberforce was also urged by his close friend, Prime Minister William Pitt the Younger, to make the issue his own, and was also given support by reformed Evangelical John Newton. The Slave Trade Act was passed by the British Parliament on March 25, 1807, making the slave trade illegal throughout the British Empire, Wilberforce also campaigned for abolition of slavery in the British Empire, which he lived to see in the Slavery Abolition Act 1833. After the 1807 act abolishing the slave trade was passed, these campaigners switched to encouraging other countries to follow suit, notably France and the British colonies. Between 1808 and 1860, the British West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard. Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against "the usurping King of Lagos", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.
In 1839, the world's oldest international human rights organization, Anti-Slavery International, was formed in Britain by Joseph Sturge, which campaigned to outlaw slavery in other countries. There were celebrations in 2007 to commemorate the 200th anniversary of the abolition of the slave trade in the United Kingdom through the work of the British Anti-Slavery Society.
In the United States, abolitionist pressure produced a series of small steps towards emancipation. After January 1, 1808, the importation of slaves into the United States was prohibited, but not the internal slave trade, nor involvement in the international slave trade externally. Legal slavery persisted; and those slaves already in the U.S. were legally emancipated only in 1863. Many American abolitionists took an active role in opposing slavery by supporting the Underground Railroad. Violent clashes between anti-slavery and pro-slavery Americans included Bleeding Kansas, a series of political and armed disputes in 1854–1861 as to whether Kansas would join the United States as a slave or free state. By 1860, the total number of slaves reached almost four million, and the American Civil War, beginning in 1861, led to the end of slavery in the United States. In 1863, Lincoln issued the Emancipation Proclamation, which freed slaves held in the Confederate States; the 13th Amendment to the U. S. Constitution prohibited slavery throughout the country.
In the 1860s, David Livingstone's reports of atrocities within the Arab slave trade in Africa stirred up the interest of the British public, reviving the flagging abolitionist movement. The Royal Navy throughout the 1870s attempted to suppress "this abominable Eastern trade", at Zanzibar in particular. In 1905, the French abolished indigenous slavery in most of French West Africa.
On December 10, 1948, the United Nations General Assembly adopted the Universal Declaration of Human Rights, which declared freedom from slavery is an internationally recognized human right. Article 4 of the Universal Declaration of Human Rights states:
In 2014, for the first time in history, major leaders of many religions, Buddhist, Anglican, Catholic, and Orthodox Christian, Hindu, Jewish, and Muslim, met to sign a shared commitment against modern-day slavery; the declaration they signed calls for the elimination of slavery and human trafficking by the year 2020. The signatories were: Pope Francis, Mātā Amṛtānandamayī, Bhikkhuni Thich Nu Chân Không (representing Zen Master Thích Nhất Hạnh), Datuk K Sri Dhammaratana, Chief High Priest of Malaysia, Rabbi Abraham Skorka, Rabbi David Rosen, Abbas Abdalla Abbas Soliman, Undersecretary of State of Al Azhar Alsharif (representing Mohamed Ahmed El-Tayeb, Grand Imam of Al-Azhar), Grand Ayatollah Mohammad Taqi al-Modarresi, Sheikh Naziyah Razzaq Jaafar, Special advisor of Grand Ayatollah (representing Grand Ayatollah Sheikh Basheer Hussain al Najafi), Sheikh Omar Abboud, Justin Welby, Archbishop of Canterbury, and Metropolitan Emmanuel of France (representing Ecumenical Patriarch Bartholomew.)
Groups such as the American Anti-Slavery Group, Anti-Slavery International, Free the Slaves, the Anti-Slavery Society, and the Norwegian Anti-Slavery Society continue to campaign to eliminate slavery.
Remnants of slavery.
In the case of freed slaves of the United States, many became share croppers and indentured servants. In this manner, some became tied to the very parcel of land into which they had been born a slave having little freedom or economic opportunity due to Jim Crow laws which perpetuated discrimination, limited education, promoted persecution without due process and resulted in continued poverty. Fear of reprisals such as unjust incarcerations and lynchings deterred upward mobility further.
Legal actions.
In November 2006, the International Labour Organization announced it will be seeking "to prosecute members of the ruling Myanmar junta for crimes against humanity" over the continuous unfree labour of its citizens by the military at the International Court of Justice. According to the International Labor Organization (ILO), an estimated 800,000 people are subject to forced labour in Myanmar.
The Ecowas Court of Justice is hearing the case of Hadijatou Mani in late 2008, where Ms. Mani hopes to compel the government of Niger to end slavery in its jurisdiction. Cases brought by her in local courts have failed so far.
Economics.
Economists have attempted to model the circumstances under which slavery (and variants such as serfdom) appear and disappear. One observation is that slavery becomes more desirable for landowners where land is abundant but labour is scarce, such that rent is depressed and paid workers can demand high wages. If the opposite holds true, then it becomes more costly for landowners to have guards for the slaves than to employ paid workers who can only demand low wages due to the amount of competition. Thus, first slavery and then serfdom gradually decreased in Europe as the population grew, but were reintroduced in the Americas and in Russia as large areas of new land with few people became available. In his books, "" and "Without Consent or Contract: the Rise and Fall of American Slavery, " Robert Fogel maintains that slavery was in fact a profitable method of production, especially on bigger plantations growing cotton that fetched high prices in the world market. It gave whites in the South higher average incomes than those in the North, but most of the money was spent on buying slaves and plantations.
Slavery is more common when the labour done is relatively simple and thus easy to supervise, such as large-scale growing of a single crop. It is much more difficult and costly to check that slaves are doing their best and with good quality when they are doing complex tasks. Therefore, slavery was seen as the most efficient method of production for large-scale crops like sugar and cotton, whose output was based on economies of scale. This enabled a gang system of labor to be prominent on large plantations where field hands were monitored and worked with factory-like precision. Each work gang was based on an internal division of labor that not only assigned every member of the gang to a precise task but simultaneously made his or her performance dependent on the actions of the others. The hoe hands chopped out the weeds that surrounded the cotton plants as well as excessive sprouts. The plow gangs followed behind, stirring the soil near the rows of cotton plants and tossing it back around the plants. Thus, the gang system worked like an early version of the assembly line later to be found in factories.
Critics since the 18th century have argued that slavery tends to retard technological advancement, since the focus is on increasing the number of slaves doing simple tasks rather than upgrading the efficiency of labour. Because of this, theoretical knowledge and learning in Greece—and later in Rome—was not applied to ease physical labour or improve manufacturing.
Adam Smith made the argument that free labor was economically better than slave labor, and argued further that slavery in Europe ended during the Middle Ages, and then only after both the church and state were separate, independent and strong institutions, that it is nearly impossible to end slavery in a free, democratic and republican forms of governments since many of its legislators or political figures were slave owners, and would not punish themselves, and that slaves would be better able to gain their freedom when there was centralized government, or a central authority like a king or the church. Similar arguments appear later in the works of Auguste Comte, especially when it comes to Adam Smith's belief in the separation of powers or what Comte called the "separation of the spiritual and the temporal" during the Middle Ages and the end of slavery, and Smith's criticism of masters, past and present. As Smith stated in the Lectures on Jurisprudence, "The great power of the clergy thus concurring with that of the king set the slaves at liberty. But it was absolutely necessary both that the authority of the king and of the clergy should be great. Where ever any one of these was wanting, slavery still continues.."
Slaves can be an attractive investment because the slave-owner only needs to pay for sustenance and enforcement. This is sometimes lower than the wage-cost of free labourers, because free workers earn more than sustenance, in these cases slaves have positive price. When the cost of sustenance and enforcement exceeds the wage rate, slave-owning would no longer be profitable, and owners would simply release their slaves. Slaves are thus a more attractive investment in high-wage environments, and environments where enforcement is cheap, and less attractive in environments where the wage-rate is low and enforcement is expensive.
Free workers also earn compensating differentials, whereby they are paid more for doing unpleasant work. Neither sustenance nor enforcement costs rise with the unpleasantness of the work, however, so slaves' costs do not rise by the same amount. As such, slaves are more attractive for unpleasant work, and less for pleasant work. Because the unpleasantness of the work is not internalised, being born by the slave rather than the owner, it is a negative externality and leads to over-use of slaves in these situations.
The weighted average global sales price of a slave is calculated to be approximately $340, with a high of $1,895 for the average trafficked sex slave, and a low of $40 to $50 for debt bondage slaves in part of Asia and Africa.
Worldwide slavery is a criminal offense but slave owners can get very high returns for their risk. According to researcher Siddharth Kara, the profits generated worldwide by all forms of slavery in 2007 were $91.2 billion. That is second only to drug trafficking in terms of global criminal enterprises. The weighted average annual profits generated by a slave in 2007 was $3,175, with a low of an average $950 for bonded labor and $29,210 for a trafficked sex slave. Approximately 40% of slave profits each year are generated by trafficked sex slaves, representing slightly more than 4% of the world's 29 million slaves.
Robert E. Wright has developed a model that helps to predict when firms (individuals, companies) will be more likely to use slaves rather than wage workers, indentured servants, family members, or other types of laborers.
Wage slavery.
The labour market, as institutionalised under today's market economic systems, has been criticised, especially by both mainstream socialists and anarcho-syndicalists, who utilise the term wage slavery as a pejorative for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.
For Marxists, labour-as-commodity, which is how they regard wage labour, provides an absolutely fundamental point of attack against capitalism. "It can be persuasively argued," noted one concerned philosopher, "that the conception of the worker's labour as a commodity confirms Marx's stigmatization of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it."
Clothing.
Throughout history, slaves were clothed in a distinctive fashion, particularly with respect to footwear or rather the lack thereof. This was both due to economic reasons as well as a distinguishing feature, especially in South Africa and South America. For example, the Cape Town slave code stated that ""Slaves must go barefoot and must carry passes."" This was the case in the majority of states that abolished slavery later in history, as most images from the respective historical period suggest that slaves were barefoot.
To quote Brother Riemer (1779): "slaves are, even in their most beautiful suit, obliged to go barefoot. Slaves were forbidden to wear shoes. This was a prime mark of distinction between the free and the bonded and no exceptions were permitted." 
As shoes have been considered badges of freedom since biblical times ""But the father said to his servants, Bring forth the best robe, and put on him; and put a ring on his hand, and shoes on [his feet ()"" this aspect has been an informal law wherever slavery existed. A barefoot person could therefore be clearly identified as a slave upon first sight.
In certain societies this rule is valid to this day, as with the Tuareg slavery is still unofficially practiced and their slaves have to go barefoot.
Apologies.
On May 21, 2001, the National Assembly of France passed the Taubira law, recognizing slavery as a crime against humanity. Apologies on behalf of African nations, for their role in trading their countrymen into slavery, remain an open issue since slavery was practiced in Africa even before the first Europeans arrived and the Atlantic slave trade was performed with a high degree of involvement of several African societies. The black slave market was supplied by well-established slave trade networks controlled by local African societies and individuals. Indeed, as already mentioned in this article, slavery persists in several areas of West Africa until the present day.
There is adequate evidence citing case after case of African control of segments of the trade. Several African nations such as the Calabar and other southern parts of Nigeria had economies depended solely on the trade. African peoples such as the Imbangala of Angola and the Nyamwezi of Tanzania would serve as middlemen or roving bands warring with other African nations to capture Africans for Europeans.
Several historians have made important contributions to the global understanding of the African side of the Atlantic slave trade. By arguing that African merchants determined the assemblage of trade goods accepted in exchange for slaves, many historians argue for African agency and ultimately a shared responsibility for the slave trade.
In 1999, President Mathieu Kerekou of Benin (formerly the Kingdom of Dahomey) issued a national apology for the central role Africans played in the Atlantic slave trade. Luc Gnacadja, minister of environment and housing for Benin, later said: "The slave trade is a shame, and we do repent for it." Researchers estimate that 3 million slaves were exported out of the Slave Coast bordering the Bight of Benin. President Jerry Rawlings of Ghana also apologized for his country's involvement in the slave trade.
The issue of an apology is linked to reparations for slavery and is still being pursued by a number of entities across the world. For example, the Jamaican Reparations Movement approved its declaration and action Plan.
In September 2006, it was reported that the UK government might issue a "statement of regret" over slavery. This was followed by a "public statement of sorrow" from Tony Blair on November 27, 2006, and a formal apology on March 14, 2007.
On February 25, 2007, the Commonwealth of Virginia resolved to 'profoundly regret' and apologize for its role in the institution of slavery. Unique and the first of its kind in the U. S., the apology was unanimously passed in both Houses as Virginia approached the 400th anniversary of the founding of Jamestown, where the first slaves were imported into North America in 1619.
Liverpool, which was a large slave trading port, apologized in 1999. On August 24, 2007, Mayor Ken Livingstone of London, United Kingdom, apologized publicly for Britain's role in colonial slave trade. "You can look across there to see the institutions that still have the benefit of the wealth they created from slavery," he said, pointing towards the financial district. He claimed that London was still tainted by the horrors of slavery. Specifically, London outfitted, financed, and insured many of the ships, which helped fund the building of London's docks. Jesse Jackson praised Livingstone, and added that reparations should be made, one of his common arguments.
On July 30, 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws. In June 2009, the US Senate passed a resolution apologizing to African-Americans for the "fundamental injustice, cruelty, brutality, and inhumanity of slavery". The news was welcomed by President Barack Obama, the nation's first President of African descent. Some of President Obama's ancestors were slave owners.
In 2010, Libyan leader Muammar Gaddafi apologized for Arab involvement in the slave trade, saying: "I regret the behavior of the Arabs… They brought African children to North Africa, they made them slaves, they sold them like animals, and they took them as slaves and traded them in a shameful way."
Reparations.
There have been movements to achieve reparations for those formerly held as slaves, or sometimes their descendants. Claims for reparations for being held in slavery are handled as a civil law matter in almost every country. This is often decried as a serious problem, since former slaves' relative lack of money means they often have limited access to a potentially expensive and futile legal process. Mandatory systems of fines and reparations paid to an as yet undetermined group of claimants from fines, paid by unspecified parties, and collected by authorities have been proposed by advocates to alleviate this "civil court problem.."Since in almost all cases there are no living ex-slaves or living ex-slave owners these movements have gained little traction. In nearly all cases the judicial system has ruled that the statute of limitations on these possible claims has long since expired.
Other uses of the term.
The word "slavery" is often used as a pejorative to describe any activity in which one is coerced into performing.
In film.
Film has been the most influential medium in the presentation of the history of slavery to the general public around the world. The American film industry has had a complex relationship with slavery and until recent decades often avoided the topic. Films such as "Birth of a Nation" (1915) and "Gone with the Wind" (1939) became controversial because they gave a favorable depiction. The last favorable treatment was "Song of the South" from Disney in 1946. In 1940 "The Santa Fe Trail" gave a liberal but ambiguous interpretation of John Brown's attacks on slavery—the film does not know what to do with slavery. The Civil Rights Movement in the 1950s made defiant slaves into heroes. The question of slavery in American memory necessarily involves its depictions in feature films. 
Most Hollywood films used American settings, although "Spartacus" (1960), dealt with an actual revolt in the Roman Empire known as the Third Servile War. It failed and all the rebels were executed, but their spirit lived on according to the film. "The Last Supper" ("La última cena" in Spanish) was a 1976 film directed by Cuban Tomás Gutiérrez Alea about the teaching of Christianity to slaves in Cuba, and emphasizes the role of ritual and revolt. "Burn!" takes place on the imaginary Portuguese island of Queimada (where the locals speak Spanish) and it merges historical events that took place in Brazil, Cuba, Santo Domingo, Jamaica, and elsewhere. "Spartacus" stays surprisingly close to the historical record.
Historians agree that films have largely shaped historical memories, but they debate issues of accuracy, plausibility, moralism, sensationalism, how facts are stretched in search of broader truths, and suitability for the classroom. Berlin argues that critics complain if the treatment emphasizes historical brutality, or if it glosses over the harshness to highlight the emotional impact of slavery.

</doc>
<doc id="27993" url="https://en.wikipedia.org/wiki?curid=27993" title="September 17">
September 17


</doc>
<doc id="27995" url="https://en.wikipedia.org/wiki?curid=27995" title="Supply chain management">
Supply chain management

Supply chain management (SCM) is the management of the flow of goods and services. It includes the movement and storage of raw materials, work-in-process inventory, and finished goods from point of origin to point of consumption. Interconnected or interlinked networks, channels and node businesses are involved in the provision of products and services required by end customers in a supply chain. Supply chain management has been defined as the "design, planning, execution, control, and monitoring of supply chain activities with the objective of creating net value, building a competitive infrastructure, leveraging worldwide logistics, synchronizing supply with demand and measuring performance globally."
SCM draws heavily from the areas of industrial engineering, systems engineering, operations management, logistics, procurement, and information technology, and strives for an integrated approach.
Origin of the term and definitions.
The term "supply chain management" entered the public domain when Keith Oliver, a consultant at Booz Allen Hamilton (now Strategy&), used it in an interview for the Financial Times in 1982. The term was slow to take hold. It gained currency in the mid-1990s, when a flurry of articles and books came out on the subject. In the late 1990s it rose to prominence as a management buzzword, and operations managers began to use it in their titles with increasing regularity.
Commonly accepted definitions of supply chain management include:
A supply chain, as opposed to supply chain management, is a set of organizations directly linked by one or more upstream and downstream flows of products, services, finances, or information from a source to a customer. Supply chain management is the management of such a chain.
Supply chain management software includes tools or modules used to execute supply chain transactions, manage supplier relationships, and control associated business processes.
Supply chain event management (SCEM) considers all possible events and factors that can disrupt a supply chain. With SCEM, possible scenarios can be created and solutions devised.
In many cases the supply chain includes the collection of goods after consumer use for recycling. Including third-party logistics or other gathering agencies as part of the RM re-patriation process is a way of illustrating the new endgame strategy.
Functions.
Supply chain management is a cross-functional approach that includes managing the movement of raw materials into an organization, certain aspects of the internal processing of materials into finished goods, and the movement of finished goods out of the organization and toward the end consumer. As organizations strive to focus on core competencies and become more flexible, they reduce their ownership of raw materials sources and distribution channels. These functions are increasingly being outsourced to other firms that can perform the activities better or more cost effectively. The effect is to increase the number of organizations involved in satisfying customer demand, while reducing managerial control of daily logistics operations. Less control and more supply chain partners lead to the creation of the concept of supply chain management. The purpose of supply chain management is to improve trust and collaboration among supply chain partners, thus improving inventory visibility and the velocity of inventory movement.
Importance.
Organizations increasingly find that they must rely on effective supply chains, or networks, to compete in the global market and networked economy. In Peter Drucker's (1998) new management paradigms, this concept of business relationships extends beyond traditional enterprise boundaries and seeks to organize entire business processes throughout a value chain of multiple companies.
In recent decades, globalization, outsourcing, and information technology have enabled many organizations, such as Dell and Hewlett Packard, to successfully operate collaborative supply networks in which each specialized business partner focuses on only a few key strategic activities (Scott, 1993). This inter-organisational supply network can be acknowledged as a new form of organisation. However, with the complicated interactions among the players, the network structure fits neither "market" nor "hierarchy" categories (Powell, 1990). It is not clear what kind of performance impacts different supply network structures could have on firms, and little is known about the coordination conditions and trade-offs that may exist among the players. From a systems perspective, a complex network structure can be decomposed into individual component firms (Zhang and Dilts, 2004). Traditionally, companies in a supply network concentrate on the inputs and outputs of the processes, with little concern for the internal management working of other individual players. Therefore, the choice of an internal management control structure is known to impact local firm performance (Mintzberg, 1979).
In the 21st century, changes in the business environment have contributed to the development of supply chain networks. First, as an outcome of globalization and the proliferation of multinational companies, joint ventures, strategic alliances, and business partnerships, significant success factors were identified, complementing the earlier "just-in-time", lean manufacturing, and agile manufacturing practices. Second, technological changes, particularly the dramatic fall in communication costs (a significant component of transaction costs), have led to changes in coordination among the members of the supply chain network (Coase, 1998).
Many researchers have recognized supply network structures as a new organisational form, using terms such as "Keiretsu", "Extended Enterprise", "Virtual Corporation", "Global Production Network", and "Next Generation Manufacturing System". In general, such a structure can be defined as "a group of semi-independent organisations, each with their capabilities, which collaborate in ever-changing constellations to serve one or more markets in order to achieve some business goal specific to that collaboration" (Akkermans, 2001).
The security management system for supply chains is described in ISO/IEC 28000 and ISO/IEC 28001 and related standards published jointly by the ISO and the IEC.Supply Chain Management draws heavily from the areas of operations management, logistics, procurement, and information technology, and strives for an integrated approach.
Historical developments.
Six major movements can be observed in the evolution of supply chain management studies: creation, integration, and globalization (Movahedi et al., 2009), specialization phases one and two, and SCM 2.0.
Creation era.
The term "supply chain management" was first coined by Keith Oliver in 1982. However, the concept of a supply chain in management was of great importance long before, in the early 20th century, especially with the creation of the assembly line. The characteristics of this era of supply chain management include the need for large-scale changes, re-engineering, downsizing driven by cost reduction programs, and widespread attention to Japanese management practices. However, the term became widely adopted after the publication of the seminal book "Introduction to Supply Chain Management" in 1999 by Robert B. Handfield and Ernest L. Nichols, Jr., which published over 25,000 copies and was translated into Japanese, Korean, Chinese, and Russian.
Integration era.
This era of supply chain management studies was highlighted with the development of electronic data interchange (EDI) systems in the 1960s, and developed through the 1990s by the introduction of enterprise resource planning (ERP) systems. This era has continued to develop into the 21st century with the expansion of Internet-based collaborative systems. This era of supply chain evolution is characterized by both increasing value added and cost reductions through integration.
A supply chain can be classified as a stage 1, 2 or 3 network. In a stage 1–type supply chain, systems such as production, storage, distribution, and material control are not linked and are independent of each other. In a stage 2 supply chain, these are integrated under one plan and is ERP enabled. A stage 3 supply chain is one that achieves vertical integration with upstream suppliers and downstream customers. An example of this kind of supply chain is Tesco.
Globalization era.
The third movement of supply chain management development, the globalization era, can be characterized by the attention given to global systems of supplier relationships and the expansion of supply chains beyond national boundaries and into other continents. Although the use of global sources in organisations' supply chains can be traced back several decades (e.g., in the oil industry), it was not until the late 1980s that a considerable number of organizations started to integrate global sources into their core business. This era is characterized by the globalization of supply chain management in organizations with the goal of increasing their competitive advantage, adding value, and reducing costs through global sourcing.
Specialization era (phase I): outsourced manufacturing and distribution.
In the 1990s, companies began to focus on "core competencies" and specialization. They abandoned vertical integration, sold off non-core operations, and outsourced those functions to other companies. This changed management requirements, by extending the supply chain beyond the company walls and distributing management across specialized supply chain partnerships.
This transition also refocused the fundamental perspectives of each organization. Original equipment manufacturers (OEMs) became brand owners that required visibility deep into their supply base. They had to control the entire supply chain from above, instead of from within. Contract manufacturers had to manage bills of material with different part-numbering schemes from multiple OEMs and support customer requests for work-in-process visibility and vendor-managed inventory (VMI).
The specialization model creates manufacturing and distribution networks composed of several individual supply chains specific to producers, suppliers, and customers that work together to design, manufacture, distribute, market, sell, and service a product. This set of partners may change according to a given market, region, or channel, resulting in a proliferation of trading partner environments, each with its own unique characteristics and demands.
Specialization era (phase II): supply chain management as a service.
Specialization within the supply chain began in the 1980s with the inception of transportation brokerages, warehouse management (storage and inventory), and non-asset-based carriers, and has matured beyond transportation and logistics into aspects of supply planning, collaboration, execution, and performance management.
Market forces sometimes demand rapid changes from suppliers, logistics providers, locations, or customers in their role as components of supply chain networks. This variability has significant effects on supply chain infrastructure, from the foundation layers of establishing and managing electronic communication between trading partners, to more complex requirements such as the configuration of processes and work flows that are essential to the management of the network itself.
Supply chain specialization enables companies to improve their overall competencies in the same way that outsourced manufacturing and distribution has done; it allows them to focus on their core competencies and assemble networks of specific, best-in-class partners to contribute to the overall value chain itself, thereby increasing overall performance and efficiency. The ability to quickly obtain and deploy this domain-specific supply chain expertise without developing and maintaining an entirely unique and complex competency in house is a leading reason why supply chain specialization is gaining popularity.
Outsourced technology hosting for supply chain solutions debuted in the late 1990s and has taken root primarily in transportation and collaboration categories. This has progressed from the application service provider (ASP) model from roughly 1998 through 2003, to the on-demand model from approximately 2003 through 2006, to the software as a service (SaaS) model currently in focus today.
Supply chain management 2.0 (SCM 2.0).
Building on globalization and specialization, the term "SCM 2.0" has been coined to describe both changes within supply chains themselves as well as the evolution of processes, methods, and tools to manage them in this new "era". The growing popularity of collaborative platforms is highlighted by the rise of TradeCard’s supply chain collaboration platform, which connects multiple buyers and suppliers with financial institutions, enabling them to conduct automated supply-chain finance transactions.
Web 2.0 is a trend in the use of the World Wide Web that is meant to increase creativity, information sharing, and collaboration among users. At its core, the common attribute of Web 2.0 is to help navigate the vast information available on the Web in order to find what is being bought. It is the notion of a usable pathway. SCM 2.0 replicates this notion in supply chain operations. It is the pathway to SCM results, a combination of processes, methodologies, tools, and delivery options to guide companies to their results quickly as the complexity and speed of the supply chain increase due to global competition; rapid price fluctuations; changing oil prices; short product life cycles; expanded specialization; near-, far-, and off-shoring; and talent scarcity.
SCM 2.0 leverages solutions designed to rapidly deliver results with the agility to quickly manage future change for continuous flexibility, value, and success. This is delivered through competency networks composed of best-of-breed supply chain expertise to understand which elements, both operationally and organizationally, deliver results, as well as through intimate understanding of how to manage these elements to achieve the desired results. The solutions are delivered in a variety of options, such as no-touch via business process outsourcing, mid-touch via managed services and software as a service (SaaS), or high-touch in the traditional software deployment model.
Business process integration.
Successful SCM requires a change from managing individual functions to integrating activities into key supply chain processes. In an example scenario, a purchasing department places orders as its requirements become known. The marketing department, responding to customer demand, communicates with several distributors and retailers as it attempts to determine ways to satisfy this demand. Information shared between supply chain partners can only be fully leveraged through process integration.
Supply chain business process integration involves collaborative work between buyers and suppliers, joint product development, common systems, and shared information. According to Lambert and Cooper (2000), operating an integrated supply chain requires a continuous information flow.
However, in many companies, management has concluded that optimizing product flows cannot be accomplished without implementing a process approach. The key supply chain processes stated by Lambert (2004) are:
Much has been written about demand management. Best-in-class companies have similar characteristics, which include the following:
One could suggest other critical supply business processes that combine these processes stated by Lambert, such as:
Theories.
There are gaps in the literature on supply chain management studies at present (2015): there is no theoretical support for explaining the existence or the boundaries of supply chain management. A few authors, such as Halldorsson et al. (2003), Ketchen and Hult (2006), and Lavassani et al. (2009), have tried to provide theoretical foundations for different areas related to supply chain by employing organizational theories, which may include the following:
However, the unit of analysis of most of these theories is not the supply chain but rather another system, such as the firm or the supplier-buyer relationship. Among the few exceptions is the relational view, which outlines a theory for considering dyads and networks of firms as a key unit of analysis for explaining superior individual firm performance (Dyer and Singh, 1998).
Supply chain centroids.
In the study of supply chain management, the concept of centroids has become an important economic consideration. A centroid is a location that has a high proportion of a country's population and a high proportion of its manufacturing, generally within . In the US, two major supply chain centroids have been defined, one near Dayton, Ohio, and a second near Riverside, California.
The centroid near Dayton is particularly important because it is closest to the population center of the US and Canada. Dayton is within 500 miles of 60% of the US population and manufacturing capacity, as well as 60% of Canada's population. The region includes the interchange between I-70 and I-75, one of the busiest in the nation, with 154,000 vehicles passing through per day, 30–35% of which are trucks hauling goods. In addition, the I-75 corridor is home to the busiest north-south rail route east of the Mississippi River.
Tax efficient supply chain management.
Tax efficient supply chain management is a business model that considers the effect of tax in the design and implementation of supply chain management. As the consequence of globalization, cross-national businesses pay different tax rates in different countries. Due to these differences, they may legally optimize their supply chain and increase profits based on tax efficiency.
Sustainability and social responsibility in supply chains.
Supply chain sustainability is a business issue affecting an organization's supply chain or logistics network, and is frequently quantified by comparison with SECH ratings, which uses a triple bottom line incorporating economic, social, and environmental aspects. SECH ratings are defined as social, ethical, cultural, and health' footprints. Consumers have become more aware of the environmental impact of their purchases and companies' SECH ratings and, along with non-governmental organizations (NGOs), are setting the agenda for transitions to organically grown foods, anti-sweatshop labor codes, and locally produced goods that support independent and small businesses. Because supply chains may account for over 75% of a company's carbon footprint, many organizations are exploring ways to reduce this and thus improve their SECH rating.
For example, in July 2009, Wal-Mart announced its intentions to create a global sustainability index that would rate products according to the environmental and social impacts of their manufacturing and distribution. The index is intended to create environmental accountability in Wal-Mart's supply chain and to provide motivation and infrastructure for other retail companies to do the same.
It has been reported that companies are increasingly taking environmental performance into account when selecting suppliers. A 2011 survey by the Carbon Trust found that 50% of multinationals expect to select their suppliers based upon carbon performance in the future and 29% of suppliers could lose their places on 'green supply chains' if they do not have adequate performance records on carbon.
The US Dodd–Frank Wall Street Reform and Consumer Protection Act, signed into law by President Obama in July 2010, contained a supply chain sustainability provision in the form of the Conflict Minerals law. This law requires SEC-regulated companies to conduct third party audits of their supply chains in order to determine whether any tin, tantalum, tungsten, or gold (together referred to as "conflict minerals") is mined or sourced from the Democratic Republic of the Congo, and create a report (available to the general public and SEC) detailing the due diligence efforts taken and the results of the audit. The chain of suppliers and vendors to these reporting companies will be expected to provide appropriate supporting information.
Incidents like the 2013 Savar building collapse with more than 1,100 victims have led to widespread discussions about corporate social responsibility across global supply chains. Wieland and Handfield (2013) suggest that companies need to audit products and suppliers and that supplier auditing needs to go beyond direct relationships with first-tier suppliers. They also demonstrate that visibility needs to be improved if supply cannot be directly controlled and that smart and electronic technologies play a key role to improve visibility. Finally, they highlight that collaboration with local partners, across the industry and with universities is crucial to successfully managing social responsibility in supply chains.
Components.
Management components.
SCM components are the third element of the four-square circulation framework. The level of integration and management of a business process link is a function of the number and level of components added to the link (Ellram and Cooper, 1990; Houlihan, 1985). Consequently, adding more management components or increasing the level of each component can increase the level of integration of the business process link.
Literature on business process re-engineering buyer-supplier relationships, and SCM suggests various possible components that should receive managerial attention when managing supply relationships. Lambert and Cooper (2000) identified the following components:
However, a more careful examination of the existing literature leads to a more comprehensive understanding of what should be the key critical supply chain components, or "branches" of the previously identified supply chain business processes—that is, what kind of relationship the components may have that are related to suppliers and customers. Bowersox and Closs (1996) state that the emphasis on cooperation represents the synergism leading to the highest level of joint achievement. A primary-level channel participant is a business that is willing to participate in responsibility for inventory ownership or assume other financial risks, thus including primary level components (Bowersox and Closs, 1996). A secondary-level participant (specialized) is a business that participates in channel relationships by performing essential services for primary participants, including secondary level components, which support primary participants. Third-level channel participants and components that support primary-level channel participants and are the fundamental branches of secondary-level components may also be included.
Consequently, Lambert and Cooper's framework of supply chain components does not lead to any conclusion about what are the primary- or secondary-level (specialized) supply chain components (see Bowersox and Closs, 1996, p. 93) —that is, which supply chain components should be viewed as primary or secondary, how these components should be structured in order to achieve a more comprehensive supply chain structure, and how to examine the supply chain as an integrative one (See above sections 2.1 and 3.1).
Reverse supply chain.
Reverse logistics is the process of managing the return of goods. It is also referred to as "aftermarket customer services". Any time money is taken from a company's warranty reserve or service logistics budget, one can speak of a reverse logistics operation.
Reverse logistics is also the process of managing the return of goods from store, which the returned goods are sent back to warehouse and after that either warehouse scrap the goods or send them back to supplier for replacement depending on the warranty of the merchandise.
Systems and value.
Supply chain systems configure value for those that organize the networks. Value is the additional revenue over and above the costs of building the network. Co-creating value and sharing the benefits appropriately to encourage effective participation is a key challenge for any supply system. Tony Hines defines value as follows: "Ultimately it is the customer who pays the price for service delivered that confirms value and not the producer who simply adds cost until that point".
Global applications.
Global supply chains pose challenges regarding both quantity and value. Supply and value chain trends include:
These trends have many benefits for manufacturers because they make possible larger lot sizes, lower taxes, and better environments (e.g., culture, infrastructure, special tax zones, or sophisticated OEM) for their products. There are many additional challenges when the scope of supply chains is global. This is because with a supply chain of a larger scope, the lead time is much longer, and because there are more issues involved, such as multiple currencies, policies, and laws. The consequent problems include different currencies and valuations in different countries, different tax laws, different trading protocols, and lack of transparency of cost and profit.
Supply chain consulting.
Supply-chain consulting is the providing of expert knowledge in order to assess the productivity of a supply-chain and, ideally, to enhance the productivity.
Supply chain Consulting is a service involved in transfer of knowledge on how to exploit existing assets through improved coordination and can hence be a source of competitive advantage; Hereby the role of the consultant is to help management by adding value to the whole process through the various sectors from the ordering of the raw materials to the final product.
On this regard, firms either build internal teams of consultants to tackle the issue or use external ones, (companies choose between these two approaches taking into consideration various factors).
The use of external consultants is a common practice among companies. The whole consulting process generally involves the analysis of the entire supply-chain process, including the countermeasures or correctives to take to achieve a better overall performance.
Companies in the field.
Supply-chain consultancies vary in their ranges of services and sizes, major players in the field include:
Certification.
Skills and competencies.
Supply chain professionals need to have knowledge of managing supply chain functions such as transportation, warehousing, inventory management, and production planning. In the past, supply chain professionals emphasized logistics skills, such as knowledge of shipping routes, familiarity with warehousing equipment and distribution center locations and footprints, and a solid grasp of freight rates and fuel costs. More recently, supply chain management extends to logistical support across firms and management of global supply chains. Supply chain professionals need to have an understanding of business continuity basics and strategies.
Roles and responsibilities.
Supply chain professionals play major roles in the design and management of supply chains. In the design of supply chains, they help determine whether a product or service is provided by the firm itself (insourcing) or by another firm elsewhere (outsourcing). In the management of supply chains, supply chain professionals coordinate production among multiple providers, ensuring that production and transport of goods happen with minimal quality control or inventory problems. One goal of a well-designed and maintained supply chain for a product is to successfully build the product at minimal cost. Such a supply chain could be considered a competitive advantage for a firm.
Beyond design and maintenance of a supply chain itself, supply chain professionals participate in aspects of business that have a bearing on supply chains, such as sales forecasting, quality management, strategy development, customer service, and systems analysis. Production of a good may evolve over time, rendering an existing supply chain design obsolete. Supply chain professionals need to be aware of changes in production and business climate that affect supply chains and create alternative supply chains as the need arises.
Individuals working in supply chain management can attain a professional certification by passing an exam developed by a third party certification organizations. The purpose of certification is to guarantee a certain level of expertise in the field.
Education.
The knowledge needed to pass a certification exam may be gained from several sources. Some knowledge may come from college courses, but most of it is acquired from a mix of on-the-job learning experiences, attending industry events, learning best practices with their peers, and reading books and articles in the field. Certification organizations may provide certification workshops tailored to their exams.
Organizations.
There are a number of organizations that provide certification exams, such as CSCMP (Council of Supply Chain Management Professionals), IIPMR (International Institute for Procurement and Market Research), APICS (the Association for Operations Management), ISCEA (The International Supply Chain Education Alliance) and IOSCM (Institute of Supply Chain Management). APICS' certification is called "Certified Supply Chain Professional", or CSCP, and ISCEA'S certification is called the "Certified Supply Chain Manager" (CSCM), CISCM (Chartered Institute of Supply Chain Management) awards certificate as "Chartered Supply Chain Management Professional" (CSCMP). Another, the Institute for Supply Management, is developing one called the "Certified Professional in Supply Management" (CPSM) focused on the procurement and sourcing areas of supply chain management. The Supply Chain Management Association (SCMA) is the main certifying body for Canada with the designations having global reciprocity. The designation Supply Chain Management Professional (SCMP) is the title of the supply chain leadership designation.
Topics addressed by selected professional supply chain certification programmes:
College-level education.
SCM programs are offered at undergraduate and graduate levels, and many are delivered 100% online.
United States
University of Alabama
Canada
France
Spain
United Kingdom

</doc>
<doc id="27998" url="https://en.wikipedia.org/wiki?curid=27998" title="Synchronised swimming">
Synchronised swimming

Synchronised swimming is a hybrid form of swimming, dance and gymnastics, consisting of swimmers (either solos, duets, trios, combos, or teams) performing a synchronised routine of elaborate moves in the water, accompanied by music. Athletes can perform solos and compete in most other competitions.
Synchronised swimming demands advanced water skills, and requires great strength, endurance, flexibility, grace, artistry and precise timing, as well as exceptional breath control when upside down underwater. During lifts, (where up to six people act as the platform, one person acts as a base, and one and/or two people act as flyers) swimmers are required not to touch the bottom - yet pull off an outstanding lift.
Following the addition new mixed-pair event, FINA World Aquatics competitions are open to men since the 16th 2015 championships in Kazan, and the other international and national competitions allow male competitors in every event. However, men are currently still barred from competing in the Olympics. Both USA Synchro and Synchro Canada allow men to compete with women. – Most European countries allow men to compete also, France even allows male only podiums, according to the number of participants. In the past decade more men are becoming involved in the sport and a global biannual competition called Men's Cup has been steadily growing.
Competitors show off their strength, flexibility, and aerobic endurance required to perform difficult routines. Swimmers perform two routines for the judges, one technical and one free, as well as age group routines and figures.
Synchronised swimming is both an individual and team sport. Swimmers compete individually during figures, and then as a team during the routine. Figures are made up of a combination of skills and positions that often require control, strength, and flexibility. Swimmers are ranked individually for this part of the competition. The routine involves teamwork and synchronization. It is choreographed to music and often has a theme.
Synchronised swimming is governed internationally by FINA (Federation Internationale de Natation).
History.
At the turn of the 20th century, synchronised swimming was known as water ballet. The first recorded competition was in 1891 in Berlin, Germany. Many swim clubs were formed around that time, and the sport simultaneously developed in Canada. As well as existing as a sport, it often constituted a popular addition to Music Hall evenings, in the larger variety theatres of London or Glasgow which were equipped with huge on-stage water tanks for the purpose.
In 1907, Australian Annette Kellerman popularized the sport when she performed in a glass tank as an underwater ballerina (the first water ballet in a glass tank) in the New York Hippodrome. After experimenting with various diving actions and stunts in the water, Katherine Curtis started one of the first water ballet clubs at the University of Chicago, where the team began executing strokes, "tricks," and floating formations. On May 27, 1939, the first U.S. synchronized swimming competition took place at Wright Junior College between Wright and the Chicago Teachers' College.
In 1924, the first competition in North America was in Montreal, with Peg Seller as the first champion.
Other important pioneers for the sport are Beulah Gundling, Käthe Jacobi, Marion Kane Elston, Dawn Bean, Billie MacKellar, Teresa Anderson, Gail Johnson, Gail Emery, Charlotte Davis, Mary Derosier, Norma Olsen and Clark Leach. Charlotte Davis coached Tracie Ruiz and Candy Costie, who won the gold medal in duet synchronized swimming at the 1984 Olympics in Los Angeles.
In the 1940s and 1950s, before men were banned from national competitions in World War II, Donn Squire and Bert Hubbard were important male synchronized swimmers in the USA.
In 1933 and 1934, Katherine Whitney Curtis organized a show, "The Kay Curtis Modern Mermaids", for the World Exhibition in Chicago. The announcer, Norman Ross, introduced the sport as "synchronized swimming" for the first time. The term eventually became standardized through the AAU, but Curtis still used the term "rhythmic swimming" in her book, "Rhythmic Swimming: A Source Book of Synchronized Swimming and Water Pageantry" (Minneapolis: Burgess Publishing Co., 1936).
Curtis persuaded the AAU to make synchronised swimming an officially recognized sport in December 1941, but she herself transferred overseas in 1943. She served as the Recreation Director of the Red Cross under Generals Patton and Eisenhower, during which time she produced the first international aquacade in Caserta, Italy. She was the Director of Travel in post-war Europe until 1962. In 1959 the Helms Hall of Fame officially recognized Curtis (along with Annette Kellerman) - ascribing to her the primary development of synchronised swimming. In 1979 the International Swimming Hall of Fame inducted Curtis with similar accolades.
A National A.A.U. champion swimmer, Esther Williams, popularized synchronised swimming during WWII and after, through (often elaborately staged) scenes in Hollywood films such as "Bathing Beauty" (1944), "Million Dollar Mermaid" (1952), and "Jupiter's Darling" (1955). In the 1970s and 1980s, Ft. Lauderdale swimming champion Charkie Phillips revived water ballet on television with The Krofftettes in "The Brady Bunch Hour" (1976–1977), NBC's "The Big Show" (1980), and then on screen with Miss Piggy in "The Great Muppet Caper" (1981).
Synchro as an Olympic sport in post war.
The first Olympic demonstration was at the 1952 Olympic Games, where the Helsinki officials welcomed Kay Curtis and lit a torch in her honor. Curtis died in 1980, but synchronised swimming did not become an official Olympic sport until the 1984 Summer Olympic Games. It was not until 1968 that synchronised swimming became officially recognized by FINA as the fourth water sport next to swimming, platform diving and water polo.
From 1984 through 1992, the Summer Olympic Games featured solo and duet competitions, but they both were dropped in 1996 in favor of team competition. At the 2000 Olympic Games, however, the duet competition was restored and is now featured alongside the team competition. 
World championships and synchro.
Synchronised swimming has been part of the World Aquatics Championships since the beginning. From 1973 through 2001, the World Aquatics Championships featured solo, duet and team competitions. In 2003, a free routine combination, comprising elements of solo, duet and team, was added. In 2005, it was renamed free combination. In 2007, solo, duet and team events were split between technical and free routines. Since 2007, seven World championship titles are at stake.
Basic skills.
Sculls.
Sculls (hand movements used to propel the body) are the most essential part to synchronised swimming. Commonly used sculls include support scull, stationary scull, propeller scull, alligator scull, torpedo scull, split-arm scull, barrel scull, and paddle scull. The support scull is used most often to support the body while a swimmer is performing upside down. Support scull is performed by holding the upper arms against the sides of the body and the fore arms at 90-degree angles to the body, with hands facing the bottom of the pool. The fore arms are then moved back and forth while maintaining the right angle. The resulting pressure against the hands allows the swimmer to hold their legs above water while swimming. Other sculls used in training include propeller and reverse propeller.
Eggbeater.
The "eggbeater kick" is another important skill of synchronised swimming. It is a form of treading water that allows for stability and height above the water while leaving the hands free to perform strokes. An average eggbeater height is usually around chest level. Eggbeater is used in all "arm" sections, a piece of choreography in which the swimmer is upright, often with one or both arms in the air. Another variation is a boost, which is executed through an eggbeater buildup and a strong whip kick, propelling the swimmer out of the water vertically.
Eggbeating for a considerable period is also referred to as an "aquabob" and is used to build propulsion under water prior to a boost or pop-up.
Lifts.
A lift is when members of the team propel another teammate relatively high out of the water. They are quite common in routines of older age groups and higher skill levels.
There are many variations on lifts, often dubbed "highlights". These can include partner lifts, float patterns or other areas of unique, artistic choreography intended to impress the judges and audience.
Parts of a successful lift.
There are three parts to every lift in synchronised swimming: The top (or "flyer"), the base, and the pushers. 
Positions.
There are hundreds of different regular positions that can be used to create seemingly infinite combinations. These are a few basic and commonly used ones:
Further descriptions of technical positions can be found on the International Olympic Committee website.
Routine.
Routines are composed of "figures" (leg movements) and arm or stroke sections. They often incorporate lifts or throws, an impressive move in which a group of swimmers lift or throw another swimmer out of the water. Swimmers are synchronised both to each other and to the music. During a routine swimmers can never use the bottom of the pool for support, but rather depend on sculling motions with the arms, and eggbeater kick to keep afloat. After the performance, the swimmers are judged and scored on their performance based on technical merit and artistic impression. Technical skill, patterns, expression, and synchronization are all critical to achieving a high score.
Technical vs. free routines.
Depending on the competition level, swimmers will perform a "technical" routine with predetermined elements that must be performed in a specific order. The technical routine acts as a replacement for the figure event, and is usually used only in senior and collegiate level meets. In addition to the technical routine, the swimmers will perform a longer "free" routine, which has no requirements and is a chance for the swimmers to get creative and innovative with their choreography.
Length of routines.
The type of routine and competition level determines the length of routines. Routines typically last two and a half to five minutes long, the shortest being solos, with length added as the number of swimmers are increased (duets, trios, teams, and combos). Age and skill level are other important factors in determining the required routine length.
Scoring.
Routines are scored on a scale of 100, with points for artistic impression, execution, and difficulty.
Preparation.
When performing routines in competition and practice, competitors wear a rubber noseclip to keep water from entering their nose when submerged. Some swimmers wear ear-plugs to keep the water out of their ears. Hair is worn in a bun and flavorless gelatin, Knox, is applied to keep hair in place; a decorative headpiece is bobby-pinned to the bun. Occasionally, swimmers wear custom-made swimming caps in place of their hair in buns.
Competitors wear custom swimsuits and headpieces, usually elaborately decorated with bright fabric and sequins to reflect the music to which they are swimming. The costume and music are not judged (but marks will be taken off if the headpiece falls off any swimmer while she is swimming the routine) but factor into the overall performance and "artistic impression." Heavy eye makeup is often worn to help portray the emotions involved with the routine; it helps to accentuate the eyes of each swimmer. (This makeup style is often the center of criticism and ridicule. Some argue that it shows a lack of taste and minimizes the athleticism of the sport. Other artistic sports, such as gymnastics and ice skating, do not employ the same makeup practices.)
Underwater speakers ensure that swimmers can hear the music and aid their ability to synchronize with each other. Routines are prepared and set to counts in the music, to further ensure synchronization. Coaches use underwater speakers to communicate with the swimmers during practice. Goggles, though worn during practice, are not permitted during routine competition.
Competitions.
Figures.
A standard meet begins with the swimmers doing "figures", which are progressions between positions performed individually without music. All swimmers must compete wearing the standard black swimsuit and white swimcap, as well as goggles and a noseclip. Figures are performed in front of a panel of 5 judges who score individual swimmers from 1 to 10 (10 being the best). The figure competition prefaces the routine events.
In the United States.
In the United States, competitors are divided into groups by age. The seven age groups are: 10 and Under, 11–12, 13–15, 16–17, 18–19, Junior (elite 15–18), Senior (elite 18+), Collegiate, and Master. In addition to these groups, younger swimmers may be divided by ability into 3 levels: Novice, Intermediate, and Age Group. Seasons range in length, and some swimmers participate year-round in competitions. There are many levels of competition, including but not limited to: State, Regional, Zone, Age Group National, and US Junior and Senior Opens. Each swimmer may compete in up to three of the following routine events: solo, duet, trio, combo (consisting of eight to ten swimmers), and team (consisting of four to eight swimmers). Figure scores are combined with routines to determine the final rankings. USA Synchro's annual intercollegiate championships have been dominated by The Ohio State University, Stanford University, and The University of the Incarnate Word.
In Canada.
In Canada, synchronized swimming has an age-based Structure system as of 2010 with age groups 10 & under, 12 & under, and 13-15 for the provincial levels. There is also a skill level which is 13-15 and juniors (16-18) known as national stream, as well as competition at the Masters and University levels. 13-15 age group and 16-18 age group are national stream athletes that fall in line with international age groups – 15 and Under and Junior (16–18) and Senior (18+) level athletes.
There are also the Wildrose age group. This is for competitors before they reach 13-15 national stream. Wildrose ranges from Tier 8 and under to 16 and over provincial/wildrose. These are also competitive levels. There are also the recreational levels which are called "stars". Synchro Canada requires that a competitor must pass Star 3 before entering Tier 1. To get into a Tier a swimmer must take a test for that Tier. In these tests, the swimmer must be able to perform the required movements for the level. (Canada no longer uses Tiers as a form of level placement).
The Canadian University synchronized Swimming League is intended for Canadian Swimmers who wish to continue their participation in the sport, as well as offering a "Novice" category for those new to the sport. Traditionally, the top teams hail from McGill University, Queens University and the University of Ottawa.
Injuries.
In their book 2012 "Concussions and Our Kids", Dr. Robert Cantu and Mark Hyman reported that Dr. Bill Moreau, who serves as medical director for the U.S. Olympic Committee (USOC), reported that during a two-week training session in Colorado Springs about a dozen women athletes participating suffered a 50% concussion rate. Dr. Moreau noted, "These women are superior athletes. They're in the pool eight hours a day. Literally, they're within inches of one another, sculling and paddling. As they go through their various routines, they're literally kicking each other in the head." As a result, the USOC initiated a process of reassessing concussion awareness and prevention for all sports. Dr. Moreau says that many athletes in non-collision sports "aren't thinking about head injury and don't know they've had concussions."

</doc>
<doc id="27999" url="https://en.wikipedia.org/wiki?curid=27999" title="Human swimming">
Human swimming

Human swimming is the self-propulsion of a person through water or another liquid, usually for recreation, sport, exercise, or survival. Locomotion is achieved through coordinated movement of the limbs, the body, or both. Humans can hold their breath underwater and undertake rudimentary locomotive swimming within weeks of birth, as an evolutionary response.
Swimming is consistently among top public recreational activities, and in some countries, swimming lessons are a compulsory part of the educational curriculum. As a formalized sport, swimming features in a range of local, national, and international competitions, including every modern summer Olympics, which takes place every four years.
Science.
Swimming relies on the natural buoyancy of the human body. On average, the body has a relative density of 0.98 compared to water, which causes the body to float. However, buoyancy varies on the basis of both body composition and the salinity of the water. Higher levels of body fat and saltier water both lower the relative density of the body and increase its buoyancy.
Since the human body is only slightly less dense than water, water supports the weight of the body during swimming. As a result, swimming is “low-impact” compared to land activities such as running. The density and viscosity of water also create resistance for objects moving through the water. Swimming strokes use this resistance to create propulsion, but this same resistance also generates drag on the body.
Hydrodynamics is important to stroke technique for swimming faster, and swimmers who want to swim faster or tire less try to reduce the drag of the body's motion through the water. To be more hydrodynamic, swimmers can either increase the power of their strokes or reduce water resistance, though power must increase by a factor of three to achieve the same effect as reducing resistance. Efficient swimming by reducing water resistance involves a horizontal water position, rolling the body to reduce the breadth of the body in the water, and extending the arms as far as possible to reduce wave resistance.
Just before plunging into the pool, swimmers may perform exercises such as squatting. Squatting helps in enhancing a swimmer’s start by warming up the thigh muscles.
Infant swimming.
Human babies demonstrate an innate swimming or diving reflex from newborn until the age of approximately 6 months. Other mammals also demonstrate this phenomenon (see mammalian diving reflex). The diving response involves apnea, reflex bradycardia, and peripheral vasoconstriction; in other words, babies immersed in water spontaneously hold their breath, slow their heart rate, and reduce blood circulation to the extremities (fingers and toes).
Technique.
Swimming can be undertaken using a wide range of styles, known as 'strokes,' and these strokes are used for different purposes, or to distinguish between classes in competitive swimming. It is not necessary to use a defined stroke for propulsion through the water, and untrained swimmers may use a 'doggy paddle' of arm and leg movements, similar to the way four-legged animals swim.
There are four main strokes used in competition and recreation swimming: the front crawl, also known as freestyle, the breaststroke, the backstroke and the butterfly. Competitive swimming in Europe started around 1800, mostly using the breaststroke. In 1873, John Arthur Trudgen introduced the trudgen to Western swimming competitions, after copying the front crawl he saw American Indians use, but substituting a scissor kick for the traditional flutter kick to reduce splashing. The butterfly stroke developed in the 1930s, and was considered a variant of the breaststroke until accepted as a separate style in 1952. Butterfly is also known as the hardest stroke to many, but it burns the most calories compared to the other 3 strokes. 
Other strokes exist for specific purposes, such as training or rescue, and it is also possible to adapt strokes to avoid using parts of the body, either to isolate certain body parts, such as swimming with arms only or legs only to train them harder, or for use by amputees or those affected by paralysis .
Historic record.
Swimming has been recorded since prehistoric times, and the earliest records of swimming date back to Stone Age paintings from around 7,000 years ago. Written references date from 2000 BC. Some of the earliest references include the Epic of Gilgamesh, the Iliad, the Odyssey, the Bible (Ezekiel 47:5, Acts 27:42, Isaiah 25:11), Beowulf, and other sagas.
The coastal tribes living in the volatile Low Countries were known as excellent swimmers by the Romans. Men and horses of the Batavi tribe could cross the Rhine without losing formation, according to Tacitus. Dio Cassius describes one surprise tactic employed by Aulus Plautius against the Celts at the Battle of the Medway:
The Celts thought that Romans would not be able to cross it without a bridge, and consequently bivouacked in rather careless fashion on the opposite bank; but he sent across a detachment of who were accustomed to swim easily in full armour across the most turbulent streams. . . . Thence the Britons retired to the river Thames at a point near where it empties into the ocean and at flood-tide forms a lake. This they easily crossed because they knew where the firm ground and the easy passages in this region were to be found; but the Romans in attempting to follow them were not so successful. However, the [Batavii swam across again and some others got over by a bridge a little way up-stream, after which they assailed the barbarians from several sides at once and cut down many of them."
In 1538, Nikolaus Wynmann, a German professor of languages, wrote the first swimming book, "The Swimmer or A Dialogue on the Art of Swimming" ("Der Schwimmer oder ein Zweigespräch über die Schwimmkunst").
Purpose.
There are many reasons why people swim, from swimming as a recreational pursuit to swimming as a necessary part of a job or other activity. Swimming may also be used to rehabilitate injuries, especially various cardiovascular injuries and muscle injuries.
Recreation.
Many swimmers swim for recreation, with swimming consistently ranking as one of the physical activities people are most likely to take part in. Recreational swimming can also be used for exercise, relaxation, or rehabilitation. The support of the water, and the reduction in impact, makes swimming accessible for people who are unable to undertake activities such as running.
Health.
Swimming is primarily a cardiovascular/aerobic exercise due to the long exercise time, requiring a constant oxygen supply to the muscles, except for short sprints where the muscles work anaerobically. As with most aerobic exercise, swimming is believed to reduce the harmful effects of stress. Swimming is also effective in improving health for people with cardiovascular problems and chronic illnesses. It is proven to positively impact the mental health of pregnant women and mothers. Swimming can even improve mood.
Disabled swimmers.
As of 2013, the Americans with Disabilities Act requires that swimming pools in the United States be accessible to disabled swimmers.
Elderly swimmers.
"Water-based exercise can benefit older adults by improving quality of life and decreasing disability. It also improves or maintains the bone health of post-menopausal women."
Sport.
Swimming as a sport predominantly involves participants competing to be the fastest over a given distance. Competitors swim different distances in different levels of competition. For example, swimming has been an Olympic sport since 1896, and the current program includes events from 50 m to 1500 m in length, across all four main strokes and medley.
The sport is governed internationally by the Fédération Internationale de Natation (FINA), and competition pools for FINA events are 25 or 50 meters in length. In the United States, a pool 25 yards in length is commonly used for competition.
Other swimming and water-related sporting disciplines include diving, synchronized swimming, water polo, triathlon, and the modern pentathlon.
Occupation.
Some occupations require workers to swim. For example, abalone and pearl diving, and spear fishing.
Swimming is used to rescue people in the water who are in distress, including exhausted swimmers, non-swimmers who have accidentally entered the water, and others who have come to harm on the water. Lifeguards or volunteer lifesavers are deployed at many pools and beaches worldwide to fulfill this purpose, and they, as well as rescue swimmers, may use specific swimming styles for rescue purposes.
Swimming is also used in marine biology to observe plants and animals in their natural habitat. Other sciences use swimming, for example Konrad Lorenz swam with geese as part of his studies of animal behavior.
Swimming also has military purposes. Military swimming is usually done by special operation forces, such as Navy SEALs and US Army Special Forces. Swimming is used to approach a location, gather intelligence, engage in sabotage or combat, and subsequently depart. This may also include airborne insertion into water or exiting a submarine while it is submerged. Due to regular exposure to large bodies of water, all recruits in the United States Navy, Marine Corps, and Coast Guard are required to complete basic swimming or water survival training.
Swimming is also a professional sport. Companies sponsor swimmers who have the skills to compete at the international level. Many swimmers compete competitively to represent their home country in the Olympics. Many major competitions give cash awards for breaking records. Professional swimmers may also earn a living as entertainers, performing in water ballets.
Locomotion.
Locomotion by swimming over brief distances is frequent when alternatives are precluded. There have been cases of political refugees swimming in the Baltic Sea and of people jumping in the water and swimming ashore from vessels not intended to reach land where they planned to go. Swimming travel is central to the plot of the motion picture "Welcome". US president John F. Kennedy led his sailors swimming island to island after his torpedo boat was sunk in World War II, and his senator brother Ted Kennedy claimed to have left Chappaquiddick Island by swimming.
Risks.
There are many risks associated with voluntary or involuntary human presence in water, which may result in death directly or through drowning asphyxiation. Swimming is both the goal of much voluntary presence, and the prime means of regaining land in accidental situations.
Most recorded water deaths fall into these categories:
Adverse effects of swimming can include:
Around any pool area, safety equipment is often important, and is a zoning requirement for most residential pools in the United States. Supervision by personnel trained in rescue techniques is required at most competitive swimming meets and public pools.
Lessons.
Traditionally, children were considered not able to swim independently until 4 years of age,
although now infant swimming lessons are recommended to prevent drowning.
In Sweden, Denmark, Norway, Estonia and Finland, the curriculum for the fifth grade (fourth grade in Estonia) states that all children should learn how to swim as well as how to handle emergencies near water. Most commonly, children are expected to be able to swim —of which at least on their back – after first falling into deep water and getting their head under water. Even though about 95 percent of Swedish school children know how to swim, drowning remains the third most common cause of death among children.
In both the Netherlands and Belgium swimming lessons under school time ("schoolzwemmen", school swimming) are supported by the government. Most schools provide swimming lessons. There is a long tradition of swimming lessons in the Netherlands and Belgium, the Dutch translation for the breaststroke swimming style is even "schoolslag" (schoolstroke). In France, swimming is a compulsory part of the curriculum for primary schools. Children usually spend one semester per year learning swimming during CE1/CE2/CM1 (2nd, 3rd and 4th grade). 
In many places, swimming lessons are provided by local swimming pools, both those run by the local authority and by private leisure companies. Many schools also include swimming lessons into their Physical Education curricula, provided either in the schools' own pool, or in the nearest public pool.
In the UK, the "Top-ups scheme" calls for school children who cannot swim by the age of 11 to receive intensive daily lessons. Children who have not reached Great Britain's National Curriculum standard of swimming 25 metres by the time they leave primary school receive a half-hour lesson every day for two weeks during term-time.
In Canada and Mexico there has been a call to include swimming in public school curriculum.
In the USA there is the Infant Swimming Resource (ISR) initiative that provides lessons for infant children, to cope with an emergency where they have fallen into water. They are taught how to roll-back-to-float (hold their breath underwater, to roll onto their back, to float unassisted, rest and breathe until help arrives).
Clothing and equipment.
Swimsuits.
Standard everyday clothing is usually impractical for swimming and is unsafe under some circumstances. Most cultures today expect swimmers to wear swimsuits.
Men's swimsuits commonly resemble shorts, or briefs. Casual men's swimsuits (for example, boardshorts) are rarely skintight, unlike competitive swimwear, like jammers or diveskins. In most cases, boys and men swim with their upper body exposed, except in countries where custom or law prohibits it in a public setting, or for practical reasons such as sun protection.
Modern women's swimsuits are generally skintight, covering the pubic region and the breasts (See bikini). Women's swimwear may also cover the midriff as well. Women's swimwear is often a fashion statement, and whether it is modest or not is a subject of debate by many groups, religious and secular.
Competitive swimwear is built so that the wearer can swim faster and more efficiently. Modern competitive swimwear is skintight and lightweight. There are many kinds of competitive swimwear for each gender. It is used in aquatic competitions, such as water polo, swim racing, diving, and rowing.
Wetsuits provide both thermal insulation and floatation. Many swimmers lack buoyancy in the leg. The wetsuit reduces density, and therefore improves buoyancy while swimming. It provides insulation by absorbing some of the surrounding water, which then heats up when in direct contact with skin. The wetsuit is the usual choice for those who swim in cold water for long periods of time, as it reduces susceptibility to hypothermia.
Some people also choose to wear no clothing while swimming. This is known as skinny dipping. It was common for males to swim naked in a public setting up to the early 20th century. Today, skinny dipping can be a rebellious activity, or merely a casual one.

</doc>
<doc id="28002" url="https://en.wikipedia.org/wiki?curid=28002" title="Simple machine">
Simple machine

A simple machine is a mechanical device that changes the direction or magnitude of a force. In general, they can be defined as the simplest mechanisms that use mechanical advantage (also called leverage) to multiply force. Usually the term refers to the six classical simple machines which were defined by Renaissance scientists: 
A simple machine uses a single applied force to do work against a single load force. Ignoring friction losses, the work done on the load is equal to the work done by the applied force. The machine can increase the amount of the output force, at the cost of a proportional decrease in the distance moved by the load. The ratio of the output to the applied force is called the "mechanical advantage".
Simple machines can be regarded as the elementary "building blocks" of which all more complicated machines (sometimes called "compound machines") are composed. For example, wheels, levers, and pulleys are all used in the mechanism of a bicycle. The mechanical advantage of a compound machine is just the product of the mechanical advantages of the simple machines of which it is composed.
Although they continue to be of great importance in mechanics and applied science, modern mechanics has moved beyond the view of the simple machines as the ultimate building blocks of which all machines are composed, which arose in the Renaissance as a neoclassical amplification of ancient Greek texts on technology. The great variety and sophistication of modern machine linkages, which arose during the Industrial Revolution, is inadequately described by these six simple categories. As a result, various post-Renaissance authors have compiled expanded lists of "simple machines", often using terms like "basic machines", "compound machines", or "machine elements" to distinguish them from the classical simple machines above. By the late 1800s, Franz Reuleaux had identified hundreds of machine elements, calling them "simple machines". Models of these devices may be found at Cornell University's Kinematic Models for Design (KMODDL) website.
History.
The idea of a simple machine originated with the Greek philosopher Archimedes around the 3rd century BC, who studied the Archimedean simple machines: lever, pulley, and screw. He discovered the principle of mechanical advantage in the lever. Archimedes' famous remark with regard to the lever: "Give me a place to stand on, and I will move the Earth." () expresses his realization that there was no limit to the amount of force amplification that could be achieved by using mechanical advantage. Later Greek philosophers defined the classic five simple machines (excluding the inclined plane) and were able to roughly calculate their mechanical advantage. For example, Heron of Alexandria (ca. 10–75 AD) in his work "Mechanics" lists five mechanisms that can "set a load in motion"; lever, windlass, pulley, wedge, and screw, and describes their fabrication and uses. However the Greeks' understanding was limited to the statics of simple machines; the balance of forces, and did not include dynamics; the tradeoff between force and distance, or the concept of work.
During the Renaissance the dynamics of the "Mechanical Powers", as the simple machines were called, began to be studied from the standpoint of how far they could lift a load, in addition to the force they could apply, leading eventually to the new concept of mechanical work. In 1586 Flemish engineer Simon Stevin derived the mechanical advantage of the inclined plane, and it was included with the other simple machines. The complete dynamic theory of simple machines was worked out by Italian scientist Galileo Galilei in 1600 in "Le Meccaniche" ("On Mechanics"), in which he showed the underlying mathematical similarity of the machines. He was the first to understand that simple machines do not create energy, only transform it.
The classic rules of sliding friction in machines were discovered by Leonardo da Vinci (1452–1519), but remained unpublished in his notebooks. They were rediscovered by Guillaume Amontons (1699) and were further developed by Charles-Augustin de Coulomb (1785).
Frictionless analysis.
Although each machine works differently mechanically, the way they function is similar mathematically. In each machine, a force formula_1 is applied to the device at one point, and it does work moving a load, formula_2 at another point. Although some machines only change the direction of the force, such as a stationary pulley, most machines multiply the magnitude of the force by a factor, the mechanical advantage 
that can be calculated from the machine's geometry and friction.
Simple machines do not contain a source of energy, so they cannot do more work than they receive from the input force. A simple machine with no friction or elasticity is called an "ideal machine". Due to conservation of energy, in an ideal simple machine, the power output (rate of energy output) at any time formula_4 is equal to the power input formula_5
The power output equals the velocity of the load formula_7 multiplied by the load force formula_8. Similarly the power input from the applied force is equal to the velocity of the input point formula_9 multiplied by the applied force formula_10.
Therefore,
Therefore, the mechanical advantage of a frictionless machine is equal to the "velocity ratio", the ratio of input velocity to output velocity
The "velocity ratio" of the machine is also equal to the ratio of the distance the output point moves to the corresponding distance the input point moves
This can be calculated from the geometry of the machine. For example, the velocity ratio of the lever is equal to the ratio of its lever arms.
The mechanical advantage can be greater or less than one: 
In the screw, which uses rotational motion, the input force should be replaced by the torque, and the velocity by the angular velocity the shaft is turned.
Friction and efficiency.
All real machines have friction, which causes some of the input power to be dissipated as heat. If formula_17 is the power lost to friction, from conservation of energy 
The efficiency formula_19 of a machine is a number between 0 and 1 defined as the ratio of power out to the power in, and is a measure of the energy losses
As above, the power is equal to the product of force and velocity, so
Therefore,
So in non-ideal machines, the mechanical advantage is always less than the velocity ratio by the product with the efficiency "η". So a machine that includes friction will not be able to move as large a load as a corresponding ideal machine using the same input force.
Compound machines.
A "compound machine" is a machine formed from a set of simple machines connected in series with the output force of one providing the input force to the next. For example, a bench vise consists of a lever (the vise's handle) in series with a screw, and a simple gear train consists of a number of gears (wheels and axles) connected in series.
The mechanical advantage of a compound machine is the ratio of the output force exerted by the last machine in the series divided by the input force applied to the first machine, that is
Because the output force of each machine is the input of the next, formula_24, this mechanical advantage is also given by
Thus, the mechanical advantage of the compound machine is equal to the product of the mechanical advantages of the series of simple machines that form it
Similarly, the efficiency of a compound machine is also the product of the efficiencies of the series of simple machines that form it
Self-locking machines.
In many simple machines, if the load force "Fout" on the machine is high enough in relation to the input force "Fin", the machine will move backwards, with the load force doing work on the input force. So these machines can be used in either direction, with the driving force applied to either input point. For example, if the load force on a lever is high enough, the lever will move backwards, moving the input arm backwards against the input force. These are called ""reversible"", ""non-locking"" or ""overhauling"" machines, and the backward motion is called ""overhauling"". However, in some machines, if the frictional forces are high enough, no amount of load force can move it backwards, even if the input force is zero. This is called a ""self-locking"", ""nonreversible"", or ""non-overhauling"" machine. These machines can only be set in motion by a force at the input, and when the input force is removed will remain motionless, "locked" by friction at whatever position they were left.
Self-locking occurs mainly in those machines with large areas of sliding contact between moving parts: the screw, inclined plane, and wedge:
A machine will be self-locking if and only if its efficiency "η" is below 50%:
Whether a machine is self-locking depends on both the friction forces (coefficient of static friction) between its parts, and the distance ratio "din/dout" (ideal mechanical advantage). If both the friction and ideal mechanical advantage are high enough, it will self-lock.
Proof.
When a machine moves in the forward direction from point 1 to point 2, with the input force doing work on a load force, from conservation of energy the input work formula_29 is equal to the sum of the work done on the load force formula_30 and the work lost to friction formula_31
If the efficiency is below 50%
formula_33
From (1)
When the machine moves backward from point 2 to point 1 with the load force doing work on the input force, the work lost to friction formula_31 is the same
So the output work is
Thus the machine self-locks, because the work dissipated in friction is greater than the work done by the load force moving it backwards even with no input force
Modern machine theory.
Kinematic chains.
Simple machines are elementary examples of kinematic chains that are used to model mechanical systems ranging from the steam engine to robot manipulators. The bearings that form the fulcrum of a lever and that allow the wheel and axle and pulleys to rotate are examples of a kinematic pair called a hinged joint. Similarly, the flat surface of an inclined plane and wedge are examples of the kinematic pair called a sliding joint. The screw is usually identified as its own kinematic pair called a helical joint.
Two levers, or cranks, are combined into a planar four-bar linkage by attaching a link that connects the output of one crank to the input of another. Additional links can be attached to form a six-bar linkage or in series to form a robot.
Classification of machines.
The identification of simple machines arises from a desire for a systematic method to invent new machines. Therefore, an important concern is how simple machines are combined to make more complex machines. One approach is to attach simple machines in series to obtain compound machines.
However, a more successful strategy was identified by Franz Reuleaux, who collected and studied over 800 elementary machines. He realized that a lever, pulley, and wheel and axle are in essence the same device: a body rotating about a hinge. Similarly, an inclined plane, wedge, and screw are a block sliding on a flat surface.
This realization shows that it is the joints, or the connections that provide movement, that are the primary elements of a machine. Starting with four types of joints, the revolute joint, sliding joint, cam joint and gear joint, and related connections such as cables and belts, it is possible to understand a machine as an assembly of solid parts that connect these joints.

</doc>
<doc id="28005" url="https://en.wikipedia.org/wiki?curid=28005" title="Semi-Automatic Ground Environment">
Semi-Automatic Ground Environment

The Semi-Automatic Ground Environment (SAGE) was a system of large computers and associated networking equipment that coordinated data from many radar sites and processed it to produce a single unified image of the airspace over a wide area. SAGE directed and controlled the NORAD response to a Soviet air attack, operating in this role from the late 1950s into the 1980s. Its enormous computers and huge displays remain a part of cold war lore, and a common prop in movies such as "Dr. Strangelove" and .
The processing power behind SAGE was supplied by the largest computer ever built, the AN/FSQ-7. Each SAGE Direction Center (DC) housed an FSQ-7 which occupied an entire floor, approximately 22,000 square feet, of the massive concrete blockhouse, not including supporting equipment. The upper two floors contained offices, operator stations, and a single two-story radar display visible to most of the DC's personnel. Information was fed to the DC's from a network of radar stations as well as readiness information from various defence sites. The computers, based on the raw radar data, developed "tracks" for the reported targets, and automatically calculated which defences were within range. Subsets of the data were then sent to the many operator consoles, where the operators used light guns to select targets onscreen for further information, select one of the available defences, and issue commands to attack. These commands would then be automatically sent to the defence site via teleprinter. Later additions to the system allowed SAGE's tracking data to be sent directly to CIM-10 Bomarc missiles and some of the US Air Force's interceptor aircraft in-flight, directly updating their autopilots to maintain an intercept course without operator intervention. Each SAGE DC also forwarded data to a Combat Center (CC) for "supervision of the several sectors within the division" ("each combat center the capability to coordinate defense for the whole nation"). Connecting the various sites was an enormous network of telephones, modems and teleprinters.
SAGE became operational in the late 1950s and early 1960s at a combined cost of billions of dollars. It was noted that the deployment cost more than the Manhattan Project, which it was, in a way, defending against. Throughout its development there were continual questions about its real ability to deal with large attacks, and several tests by Strategic Air Command bombers suggested the system was "leaky". Nevertheless, SAGE was the backbone of NORAD's air defence system into the 1980s, by which time the tube-based FSQ-7's were increasingly costly to maintain and completely outdated. Today the same command and control task is carried out by microcomputers, based on the same basic underlying data.
Background.
Computerized command and control for United States air defense was conceived in July 1945 during the Signal Corps' Project 414A contracted to Bell Laboratories after "employment of an American version of CDS", the British air defense C2 system, had been identified for air defense command and control on June 12.
The terms of the National Security Act were formulated during 1947, leading to the creation of the US Air Force out of the former US Army Air Force. During April of the same year, US Air Force staff were identifying specifically the requirement for the creation of automatic equipment for radar-detection which would relay information to an air defence control system, a system which would function without the inclusion of persons for its operation. The December 1949 "Air Defense Systems Engineering Committee" led by Dr. George Valley had recommended computerized networking for "radar stations guarding the northern air approaches to the United States" (e.g., in Canada). After a January 1950 meeting, Valley and Jay Forrester proposed using the Whirlwind I (completed 1951) for air defense. On August 18, 1950, when the "1954 Interceptor" requirements were issued, the USAF "noted that manual techniques of aircraft warning and control would impose "intolerable" delays" (Air Material Command (AMC) published "Electronic Air Defense Environment for 1954" in December .) During February–August 1951 at the new Lincoln Laboratory, the USAF conducted Project Claude which concluded an improved air defense system was needed.
In a test for the U.S. military at Bedford, during the 20th of April 1951, data produced by radar monitoring was transmitted through telephone lines to a computer for the first time, showing the detection of a mock enemy aircraft. This indicated the likelihood of possible future technology being produced capable of detecting and directing U.S. fighter planes to defend the security of the U.S.A. in the air, through the creation of an air-borne attack defence system. This first test was directed by C. Robert Wieser.
The "Summer Study Group" of scientists in 1952 recommended "computerized air direction centers…to be ready by 1954."
IBM's "Project High" assisted under their October 1952 Whirlwind subcontract with Lincoln Laboratory, and a 1952 USAF Project Lincoln "fullscale study" of "a large scale integrated ground control system" resulted in the SAGE approval "first on a trial basis in 1953". The USAF had decided by April 10, 1953, to cancel the competing ADIS (based on CDS), and the University of Michigan’s Aeronautical Research Center withdrew in the spring. Air Research and Development Command (ARDC) planned to "finalize a production contract for the Lincoln Transition System". Similarly, the July 22, 1953, report by the Bull Committee (NSC 159) identified completing the Mid-Canada Line radars as the top priority and "on a second-priority-basis: the Lincoln automated system" (the decision to control Bomarc with the automated system was also in 1953.)
The Priority Permanent System with the initial (priority) radar stations was completed in 1952 as a "manual air defense system" (e.g., NORAD/ADC used a "Plexiglas plotting board" at the Ent command center.) The Permanent System radar stations included 3 subsequent phases of deployments and by June 30, 1957, had 119 "Fixed CONUS" radars, 29 "Gap-filler low altitude" radars, and 23 control centers". At "the end of 1957, ADC operated 182 radar stations 17 control centers … 32 [stations had been added during the last half of the year as low-altitude, unmanned gap-filler radars. The total consisted of 47 gap-filler stations, 75 Permanent System radars, 39 semimobile radars, 19 Pinetree stations,…1 Lashup -era radar and a single Texas Tower". "On 31 December 1958, USAF ADC had 187 operational land-based radar stations" (74 were "P-sites", 29 "M-sites", 13 "SM-sites", & 68 "ZI Gap Fillers").
Development.
Jay Forrester was instrumental in directing the development of the key concept of an interception system during his work at Servomechanisms Laboratory of MIT. The concept of the system, according to the Lincoln Laboratory site was to:
The AN/FSQ-7 was developed by the Lincoln Laboratory's Digital Computer Laboratory and Division 6, working closely with IBM as the manufacturer. Each FSQ-7 actually consisted of two nearly identical computers operating in "duplex" for redundancy. The design used an improved version of the Whirlwind I magnetic core memory and was an extension of the Whirlwind II computer program, renamed AN/FSQ-7 in 1953 to comply with Air Force nomenclature. It has been suggested the FSQ-7 was based on the IBM 701 but, while the 701 was investigated by MIT engineers, its design was ultimately rejected due to high error rates and generally being "inadequate to the task." IBM's contributions were essential to the success of the FSQ-7 but IBM benefited immensely from its association with the SAGE project, most evidently during development of the IBM 704.
On October 28, 1953, the Air Force Council recommended 1955 funding for "ADC to convert to the Lincoln automated system" ("redesignated the SAGE System in 1954"). The ""experimental SAGE subsector, located in Lexington, Mass., was completed in 1955…with a prototype AN/FSQ-7…known as XD-1"" (single computer system in Building F). In 1955, Air Force personnel began IBM training at the Kingston, New York, prototype facility, and the "4620th Air Defense Wing (experimental SAGE) was established at Lincoln Laboratory"
On May 3, 1956, General Partridge presented "CINCNORAD’s Operational Concept for Control of Air Defense Weapons" to the Armed Forces Policy Council, and a June 1956 symposium presentation identified advanced programming methods of SAGE code. For SAGE consulting Western Electric and Bell Telephone Laboratories formed the Air Defense Engineering Service (ADES), which was contracted in January 1954. IBM delivered the FSQ-7 computer's prototype in June 1956, and Kingston's XD-2 with dual computers guided a Cape Canaveral BOMARC to a successful aircraft intercept on August 7, 1958. Initially contracted to RCA, the AN/FSQ-7 production units were started by IBM in 1958 (32 DCs were planned for networking NORAD regions.) IBM's production contract developed 56 SAGE computers for $½ billion (~$18 million per computer pair in each FSQ-7)—cf. the $2 billion WWII Manhattan Project.
General Operational Requirements (GOR) 79 and 97 were "the basic USAF documents guiding development and improvement of semi-automatic ground environment. Prior to fielding the AN/FSQ-7 centrals, the USAF initially deployed "pre-SAGE semiautomatic intercept systems" (AN/GPA-37) to Air Defense Direction Centers, ADDCs (e.g., at "NORAD Control Centers"). On April 22, 1958, NORAD approved Nike AADCPs to be collocated with the USAF manual ADDCs at Duncanville Air Force Station TX, Olathe Air Force Station KS, Belleville Air Force Station IL, and Osceola Air Force Station KS.
Deployment.
In 1957, SAGE System groundbreaking at McChord AFB was for DC-12 where the "electronic brain" began arriving in November 1958, and the "first SAGE regional battle post [CC-01] began operating in Syracuse, New York in early 1959". BOMARC "crew training was activated January 1, 1958", and AT&T "hardened many of its switching centers, putting them in deep underground bunkers", The North American Defense Objectives Plan (NADOP 59-63) submitted to Canada in December 1958 scheduled 5 Direction Centers and 1 Combat Center to be complete in Fiscal Year 1959, 12 DCs and 3 CCs complete at the end of FY 60, 19 DC/4 CC FY 61, 25/6 FY 62, and 30/10 FY 63. On June 30 NORAD ordered that "Air Defense Sectors (SAGE) were to be designated as NORAD sectors", (the military reorganization had begun when effective April 1, 1958, CONAD "designated four SAGE sectors -- New York, Boston, Syracuse, and Washington -- as CONAD Sectors".)
SAGE Geographic Reorganization: The SAGE Geographic Reorganization Plan of July 25, 1958, by NORAD was "to provide a means for the orderly transition and phasing from the manual to the SAGE system." The plan identified deactivation of the Eastern, Central, and Western Region/Defense Forces on July 1, 1960, and "current manual boundaries" were to be moved to the new "eight SAGE divisions" (1 in Canada, "the 35th") as soon as possible. Manual divisions "not to get SAGE computers were to be phased out" along with their Manual Air Defense Control Centers at the headquarters base: "9th Geiger Field… 32d, Syracuse AFS… 35th, Dobbins AFB… 58th, Wright-Patterson AFB… 85th, Andrews AFB". The 26th SAGE Division (New York, Boston, Syracuse & Bangor SAGE sectors)--the 1st of the SAGE divisions—became operational at Hancock Field on 1 January 1959 after the redesignation started for AC&W Squadrons (e.g., the Highlands P-9 unit became the 646th Radar Squadron (SAGE) October 1.) Additional sectors included the Los Angeles Air Defense Sector (SAGE) designated in February 1959. A June 23 JCS memorandum approved the new "March 1959 Reorganization Plan" for HQ NORAD/CONAD/ADC.
Project Wild Goose teams of Air Material Command personnel installed the Ground Air Transmit Receive stations for the SAGE TDDL (in April 1961, Sault Ste Marie was the first operational sector with TDDL.) … By the middle of 1960, AMC had determined that about 800,000 man-hours (involving 130 changes) would be required to bring the F-106 fleet to the point where it would be a valuable adjunct to the air defense system. Part of the work (Project Broad Jump) was accomplished by Sacramento Air Materiel Area. The remainder (Project Wild Goose) was done at ADC bases by roving AMC field assistance teams supported by ADC maintenance personnel. (cited by Volume I p. 271 & Schaffel p. 325) After a September 1959 experimental ATABE test between an "abbreviated" AN/FSQ-7 staged at Fort Banks and the Lexington XD-1, the 1961 "SAGE/Missile Master test program" conducted large-scale field testing of the ATABE "mathematical model" using radar tracks of actual SAC and ADC aircraft flying mock penetrations into defense sectors. Similarly conducted was the joint SAC-NORAD Sky Shield II exercise followed by Sky Shield III on 2 September 1962 On July 15, 1963, ESD's CMC Management Office assumed "responsibilities in connection with BMEWS, Space Track, SAGE, and BUIC." The Chidlaw Building's computerized NORAD/ADC Combined Operations Center in 1963 became the highest echelon of the SAGE computer network when operations moved from Ent AFB's 1954 manual Command Center to the partially underground "war room". Also in 1963, radar stations were renumbered (e.g., Cambria AFS was redesignated from P-2 to Z-2 on July 31) and the vacuum-tube SAGE System was completed (and obsolete).
On "June 26, 1958,…the New York sector became operational" and on December 1, 1958, the Syracuse sector's DC-03 was operational ("the SAGE system not become operational until January 1959.") Construction of CFB North Bay in Canada was started in 1959 for a bunker ~ underground (operational October 1, 1963), and by 1963 the system had 3 Combat Centers. The 23 SAGE centers included 1 in Canada, and the "SAGE control centers reached their full 22 site deployments in 1961 (out of 46 originally planned)." The completed Minot AFB blockhouse never received an AN/FSQ-7 (the April 1, 1959, Minot Air Defense Sector consolidated with the Grand Forks ADS on March 1, 1963).
Description.
The environment allowed radar station personnel to monitor the radar data and systems' status (e.g., Arctic Tower radome pressure) and to use the range height equipment to process height requests from Direction Center (DC) personnel. DCs received the Long Range Radar Input from the sector's radar stations, and DC personnel monitored the radar tracks and IFF data provided by the stations, requested height-finder radar data on targets, and monitored the computer's evaluation of which fighter aircraft or Bomarc missile site could reach the threat first. The DC's "NORAD sector commander's operational staff" could designate fighter intercept of a target or, using the Senior Director's keyed console in the Weapons Direction room, launch a Bomarc intercept with automatic Q-7 guidance of the surface-to-air missile to a final homing dive (equipped fighters eventually were automatically guided to intercepts).
The "NORAD sector direction center (NSDC) had air defense artillery director (ADAD) consoles an Army ADA battle staff officer", and the NSDC automatically communicated crosstelling of "SAGE reference track data" to/from adjacent sectors' DCs and to 10 Nike Missile Master AADCPs. Forwardtelling automatically communicated data from multiple DCs to a 3-story Combat Center (CC) usually at one of the sector's DCs (cf. planned Hamilton AFB CC-05 near the Beale AFB DC-18) for coordinating the air battle in the NORAD region (multiple sectors) and which forwarded data to the NORAD Command Center (Ent AFB, 1963 Chidlaw Building, & 1966 Cheyenne Mountain). NORAD's integration of air warning data (at the ADOC) along with space surveillance, intelligence, and other data allowed attack assessment of an Air Defense Emergency for alerting the SAC command centers (465L SACCS nodes at Offutt AFB & The Notch), The Pentagon/Raven Rock NMCC/ANMCC, and the public via CONELRAD radio stations.
SAGE System.
The Burroughs 416L SAGE System (ESD Project 416L, Semi Automatic Ground Environment System) was the Cold War network of computer sets and centrals that created the display and control environment (SAGE) for operation of the separate radars and to provide command guidance for ground-controlled interception by air defense aircraft in the "SAGE Defense System" ("Air Defense Weapons System"). Burroughs Corporation was the prime contractor for SAGE electronic equipment which included 134 Burroughs AN/FST-2 Coordinate Data Transmitting Sets (CDTS) at radar stations and other sites, the AN/FSQ-7 Combat Direction Central at 23 Direction Centers, and the AN/FSQ-8 Combat Control Central at 8 Combat Centers. The 2 computers of each AN/FSQ-7 together weighing used about ⅓ of the DC's 2nd floor space and at ~$50 per instruction had approximately 125,000 "computer instructions support actual operational air-defense mission" processing. The AN/FSQ-7 at Luke AFB had additional memory (32K total) and was used as a "computer center for all other" DCs. Project 416L was the USAF predecessor of NORAD, SAC, and other military organizations' "Big L" computer systems (e.g., 438L Air Force Intelligence Data Handling System & 496L Space Detection and Tracking System).
Network communications: The SAGE network of computers connected by a "Digital Radar Relay" (SAGE data system) used AT&T voice lines, microwave towers, switching centers (e.g., SAGE NNX 764 was at Delta, Utah & 759 at Mounds, Oklahoma), etc.; and AT&T's "main underground station" was in Kansas (Fairview) with other bunkers in Connecticut (Cheshire), California (Santa Rosa), Iowa (Boone) and Maryland (Hearthstone Mountain). CDTS modems at automated radar stations transmitted range and azimuth, and the Air Movements Identification Service (AMIS) provided air traffic data to the SAGE System. Radar tracks by telephone calls (e.g., from Manual Control Centers in the Albuquerque, Minot, and Oklahoma City sectors) could be entered via consoles of the 4th floor "Manual Inputs" room adjacent to the "Communication Recording-Monitoring and VHF" room. In 1966, SAGE communications were integrated into the AUTOVON Network.
SAGE Sector Warning Networks (cf. NORAD Division Warning Networks) provided the radar netting communications for each DC and eventually also allowed transfer of command guidance to autopilots of TDDL-equipped interceptors for vectoring to targets via the Ground to Air Data Link Subsystem and the Ground Air Transmit Receive (GATR) network of radio sites for "HF/VHF/UHF voice & TDDL" each generally co-located at a CDTS site. SAGE Direction Centers and Combat Centers were also nodes of NORAD's Alert Network Number 1, and SAC Emergency War Order Traffic included "Positive Control/Noah's Ark instructions" through northern NORAD radio sites to confirm or recall SAC bombers if "SAC decided to launch the alert force before receiving an execution order from the JCS".
A SAGE System ergonomic test at Luke AFB in 1964 ""showed conclusively that the wrong timing of human and technical operations was leading to frequent truncation of the flight path tracking system"" (Harold Sackman). SAGE software development was "grossly underestimated" (60,000 lines in September 1955): "the biggest mistake the SAGE computer program was [underestimating the jump from the 35,000 ] instructions … to the more than 100,000 instructions on the" AN/FSQ-8. NORAD conducted a "Sage/Missile Master Integration/ECM-ECCM Test" in 1963, and although SAGE used AMIS input of air traffic information, the 1959 plan developed by the July 1958 USAF Air Defense Systems Integration Division for SAGE Air Traffic Integration (SATIN) was cancelled by the DoD.
Radar stations.
SAGE radar stations, including 78 DEW Line sites in December 1961, provided radar tracks to DCs and had frequency diversity (FD) radars United States Navy picket ships also provided radar tracks, and seaward radar coverage was provided. By the late 1960s EC-121 Warning Star aircraft based at Otis AFB MA and McClellan AFB CA provided radar tracks via automatic data link to the SAGE System. Civil Aeronautics Administration radars were at some stations (e.g., stations of the Joint Use Site System), and the ARSR-1 Air Route Surveillance Radar rotation rate had to be modified "for " ("antenna gear box modification" for compatibility with FSQ-7 & FSG-1 centrals.)
Interceptors.
ADC aircraft such as the F-94 Starfire, F-89 Scorpion, F-101B Voodoo, and F-4 Phantom were controlled by SAGE GCI. The F-104 Starfighter was "too small to be equipped with data link equipment" and used voice-commanded GCI, but the F-106 Delta Dart was equipped for the automated data link (ADL). The ADL was designed to allow Interceptors that reached targets to transmit real-time tactical friendly and enemy movements and to determine whether sector defence reinforcement was necessary.
Familiarization flights allowed SAGE weapons directors to fly on two-seat interceptors to observe GCI operations. Surface-to-air missile installations for CIM-10 Bomarc interceptors were displayed on SAGE consoles.
Improvements.
Partially solid-state AN/FST-2B and later AN/FYQ-47 computers replaced the AN/FST-2, and sectors without AN/FSQ-7 centrals requiring a "weapon direction control device" for USAF air defense used the solid-state AN/GSG-5 CCCS instead of the AN/GPA-73 recommended by ADC in June 1958. Back-Up Interceptor Control (BUIC) with CCCS dispersed to radar stations for survivability allowed a diminished but functional SAGE capability. In 1962, Burroughs "won the contract to provide a military version of its D825" modular data processing system for BUIC II. BUIC II was 1st used at North Truro Z-10 in 1966, and the Hamilton AFB BUIC II was installed in the former MCC building when it was converted to a SAGE Combat Center in 1966 (CC-05). On June 3, 1963, the Direction Centers at Marysville CA, Marquette/K I Sawyer AFB (DC-14) MI, Stewart AFB NY (DC-02), and Moses Lake WA (DC-15) were planned for closing and at the end of 1969, only 6 CONUS SAGE DCs remained (DC-03, -04, -10, -12, -20, & -21) all with the vacuum tube AN/FSQ-7 centrals. In 1966, NORAD Combined Operations Center operations at Chidlaw transferred to the Cheyenne Mountain Operations Center (425L System) and in December 1963, the DoD approved solid state replacement of Martin AN/FSG-1 centrals with the AN/GSG-5 and subsequent Hughes AN/TSQ-51. The "416L/M/N Program Office" at Hanscom Field had deployed the BUIC III by 1971 (e.g., to Fallon NAS), and the initial BUIC systems were phased out 1974-5. ADC had been renamed Aerospace Defense Command on January 15, 1968, and its general surveillance radar stations transferred to ADTAC in 1979 when the ADC major command was broken up (space surveillance stations went to SAC and the Aerospace Defense Center was activated as a DRU.)
Replacement and disposition.
For airborne command posts, "as early as 1962 the Air Force began exploring possibilites for an Airborne Warning and Control System (AWACS)", and the Strategic Defense Architecture (SDA-2000) planned an integrated air defense and air traffic control network. The USAF declared full operational capability of the 1st 7 Joint Surveillance System ROCCs on December 23, 1980, with Hughes AN/FYQ-93 systems, and many of the SAGE radar stations became Joint Surveillance System (JSS) sites (e.g., San Pedro Hill Z-39 became FAA Ground Equipment Facility J-31.) The North Bay AN/FSQ-7 was dismantled and sent to Boston's Computer Museum. In 1996, AN/FSQ-7 components were moved to Moffett Federal Airfield for storage and later moved to the Computer History Museum in Mountain View, California. The last AN/FSQ-7 centrals were demolished at McChord AFB (August 1983) and Luke AFB (February 1984). AN/FSQ-7 equipment was used as TV/movie props (e.g., in The Time Tunnel and Voyage to the Bottom of the Sea).
Historiography.
SAGE histories include a 1983 special issue of the "Annals of the History of Computing", and various personal histories were published, e.g., Valley in 1985 and Jacobs in 1986. In 1998, the SAGE System was identified as 1 of 4 "Monumental Projects", and a SAGE lecture presented the vintage film "In Your Defense" followed by anecdotal information from Les Earnest, Jim Wong, and Paul Edwards. In 2013, a copy of a 1950s cover girl image programmed for SAGE display was identified as the "earliest known figurative computer art". Company histories identifying employees' roles in SAGE include the 1981 "System Builders: The Story of SDC" and the 1998 "Architects of Information Advantage: The MITRE Corporation Since 1958".
References.
</ref>

</doc>
<doc id="28009" url="https://en.wikipedia.org/wiki?curid=28009" title="Sydney underground railways">
Sydney underground railways

The city of Sydney, Australia has several sections of underground railway. These sections of railway are extensions of suburban main line services and are not a completely segregated true metro system. The underground sections, especially the City Circle, typically have frequent services. The railways are run by Sydney Trains, an agency of the government of New South Wales. An upcoming underground rapid transit metro will also form what will be the largest part of Sydney's underground railways and the first subway system in an Australian city.
Underground lines.
Sydney has four underground lines.
Currently commencing construction is: 
There are also plans for: 
There were previously plans for other lines, such as:
Disused tunnels.
Sydney has several disused tunnels. The best known of these are those leading out of St James station. There are also two instances of disused tunnels and platforms on the Eastern Suburbs line at Redfern and Central (see below). These stations have disused platforms adjacent (but walled off from) the platforms currently in use. There are also stub tunnels at North Sydney railway station for a never constructed Manly to Mona Vale line.
From the top of the northern stair to platform 10 at Redfern Station it is possible to view the unfinished structure for the low-level "up" (toward Central) Southern Suburbs platform. The associated never-used tunnels are quite complex. Immediately to your left is the (surface level) stub tunnel for the "down" Southern Suburbs track. This short tunnel exits on the northern side of Lawson Street road bridge. As a matter of interest, there are at least nine railway tunnels under the suburb of Redfern: some in use, some never used.
The never-used platforms at Central, numbered 26 and 27, lie above the Eastern Suburbs Railway platforms and have never been used for trains. Like St. James station, these stations have stub tunnels, although they are much shorter.
There are three tunnels formerly part of the Metropolitan Goods Lines. One runs underneath Railway Square, between the Central station railway yards and the Powerhouse Museum, the others underneath Pyrmont and Glebe. The first tunnel is now only used to service the Powerhouse Museum. The former railway from the Powerhouse Museum to Dulwich Hill, including Pyrmont and Glebe tunnels, has been converted to form part of the Dulwich Hill light rail line from Central station.
Also of interest is a tunnel connecting the Eveleigh rail yards, on the southern side of the main line, to the northern side of the main line, running beneath Redfern station. This tunnel remains in use for the transfer of empty trains from Central (terminal) station to the service centre.

</doc>
<doc id="28011" url="https://en.wikipedia.org/wiki?curid=28011" title="Subgroup">
Subgroup

In group theory, a branch of mathematics, given a group "G" under a binary operation ∗, a subset "H" of "G" is called a subgroup of "G" if "H" also forms a group under the operation ∗. More precisely, "H" is a subgroup of "G" if the restriction of ∗ to is a group operation on "H". This is usually denoted , read as ""H" is a subgroup of "G"".
The trivial subgroup of any group is the subgroup {"e"} consisting of just the identity element.
A proper subgroup of a group "G" is a subgroup "H" which is a proper subset of "G" (i.e. ). This is usually represented notationally by , read as ""H" is a proper subgroup of "G"". Some authors also exclude the trivial group from being proper (i.e. ).
If "H" is a subgroup of "G", then "G" is sometimes called an overgroup of "H".
The same definitions apply more generally when "G" is an arbitrary semigroup, but this article will only deal with subgroups of groups. The group "G" is sometimes denoted by the ordered pair , usually to emphasize the operation ∗ when "G" carries multiple algebraic or other structures.
This article will write "ab" for , as is usual.
Cosets and Lagrange's theorem.
Given a subgroup "H" and some "a" in G, we define the left coset "aH" = {"ah" : "h" in "H"}. Because "a" is invertible, the map φ : "H" → "aH" given by φ("h") = "ah" is a bijection. Furthermore, every element of "G" is contained in precisely one left coset of "H"; the left cosets are the equivalence classes corresponding to the equivalence relation "a"1 ~ "a"2 if and only if "a"1−1"a"2 is in "H". The number of left cosets of "H" is called the index of "H" in "G" and is denoted by ["G" : "H"].
Lagrange's theorem states that for a finite group "G" and a subgroup "H", 
where |"G"| and |"H"| denote the orders of "G" and "H", respectively. In particular, the order of every subgroup of "G" (and the order of every element of "G") must be a divisor of |"G"|.
Right cosets are defined analogously: "Ha" = {"ha" : "h" in "H"}. They are also the equivalence classes for a suitable equivalence relation and their number is equal to ["G" : "H"].
If "aH" = "Ha" for every "a" in "G", then "H" is said to be a normal subgroup. Every subgroup of index 2 is normal: the left cosets, and also the right cosets, are simply the subgroup and its complement. More generally, if "p" is the lowest prime dividing the order of a finite group "G," then any subgroup of index "p" (if such exists) is normal.
Example: Subgroups of Z8.
Let "G" be the cyclic group Z8 whose elements are
and whose group operation is addition modulo eight. Its Cayley table is
This group has two nontrivial subgroups: "J"={0,4} and "H"={0,2,4,6}, where "J" is also a subgroup of "H". The Cayley table for "H" is the top-left quadrant of the Cayley table for "G". The group "G" is cyclic, and so are its subgroups. In general, subgroups of cyclic groups are also cyclic.
Example: Subgroups of S4 (the symmetric group on 4 elements).
Every group has as many small subgroups as neutral elements on the main diagonal:
The trivial group and two-element groups Z2. These small subgroups are not counted in the following list.

</doc>
<doc id="28012" url="https://en.wikipedia.org/wiki?curid=28012" title="Series">
Series

Series (singular) may refer to anything of a serial form:

</doc>
<doc id="28013" url="https://en.wikipedia.org/wiki?curid=28013" title="Silicon Graphics">
Silicon Graphics

Silicon Graphics, Inc. (later rebranded SGI, historically known as Silicon Graphics Computer Systems or SGCS) was an American manufacturer of high-performance computing solutions, including computer hardware and software. Founded in Mountain View, California in November 1981 by Jim Clark, its initial market was 3D graphics computer workstations, but its products, strategies and market positions developed significantly over time.
Early systems were based on the Geometry Engine that Clark and Marc Hannah had developed at Stanford University, and were derived from Clark's broader background in computer graphics. The Geometry Engine was the first very-large-scale integration (VLSI) implementation of a "geometry pipeline", specialized hardware that accelerated the "inner-loop" geometric computations needed to display three-dimensional images. For much of its history, the company focused on 3D imaging and were a major supplier of both hardware and software in this market.
They reincorporated as a Delaware corporation in January 1990. Through the mid to late-1990s, the rapidly improving performance of commodity Wintel machines began to erode SGI's stronghold in the 3D market. The porting of Maya to other platforms is a major event in this process. SGI made several attempts to address this, including a disastrous move from their existing MIPS platforms to the Intel Itanium, as well as introducing their own Linux-based Intel IA-32 based workstations and servers that failed in the market. In the mid-2000s the company repositioned itself as a supercomputer vendor, a move that also failed.
On April 1, 2009, SGI filed for Chapter 11 bankruptcy protection and announced that it would sell substantially all of its assets to Rackable Systems, a deal finalized on May 11, 2009, with Rackable assuming the name "Silicon Graphics International". The remains of Silicon Graphics, Inc. became Graphics Properties Holdings, Inc.
History.
Early years.
James H. Clark left his position as an electrical engineering associate professor at Stanford University to found SGI in 1982 along with a group of seven graduate students and research staff from Stanford: Kurt Akeley, David J. Brown, Tom Davis, Rocky Rhodes, Marc Hannah, Herb Kuta, and Mark Grossman; along with Abbey Silverstone and a few others.
Massive growth.
Ed McCracken was CEO of Silicon Graphics from 1984 to 1997. During those years, SGI grew from annual revenues of $5.4 million to $3.7 billion.
Decline.
The addition of 3D graphic capabilities to PCs, and the ability of clusters of Linux- and BSD-based PCs to take on many of the tasks of larger SGI servers, ate into SGI's core markets. The porting of Maya to Linux, Mac OS X and Microsoft Windows further eroded the low end of SGI's product line.
In response to challenges faced in the marketplace and a falling share price Ed McCracken was fired and SGI brought in Richard Belluzzo to replace him. Under Belluzzo's leadership a number of initiatives were taken which are considered to have accelerated the corporate decline.
One such initiative was trying to sell workstations running Windows NT called Visual Workstations instead of just ones which ran IRIX, the company's version of UNIX. This put the company an even more direct competition with the likes of Dell, making it more difficult to justify a price premium. The product line was unsuccessful and abandoned a few years later.
SGI's premature announcement of its migration from MIPS to Itanium and its abortive ventures into IA-32 architecture systems (the Visual Workstation line, the ex-Intergraph Zx10 range and the SGI 1000-series Linux servers) damaged SGI's credibility in the market.
In 1999, in an attempt to clarify their current market position as more than a graphics company, Silicon Graphics Inc. changed its corporate identity to "SGI", although its legal name was unchanged.
At the same time, SGI announced a new logo consisting of only the letters "sgi" in a proprietary font called "SGI", created by branding and design consulting firm Landor Associates, in collaboration with designer Joe Stitzlein. The new logo drew criticism for wasting the professional goodwill associated with the previous cube logo (or "bug"). SGI continued to use the "Silicon Graphics" name for its workstation product line, and later re-adopted the cube logo for some workstation models.
In November 2005, SGI announced that it had been delisted from the New York Stock Exchange because its common stock had fallen below the minimum share price for listing on the exchange. SGI's market capitalization dwindled from a peak of over seven billion dollars in 1995 to just $120 million at the time of delisting. In February 2006, SGI noted that it could run out of cash by the end of the year.
Re-emergence.
In mid-2005, SGI hired Alix Partners to advise it on returning to profitability and received a new line of credit. SGI announced it was postponing its scheduled annual December stockholders meeting until March 2006. It proposed a reverse stock split to deal with the de-listing from the New York Stock Exchange.
In January 2006, SGI hired Dennis McKenna as its new CEO and chairman of the board of directors. Mr. McKenna succeeded Robert Bishop, who remained vice chairman of the board of directors.
On May 8, 2006, SGI announced that it had filed for Chapter 11 bankruptcy protection for itself and U.S. subsidiaries as part of a plan to reduce debt by $250 million. Two days later, the U.S. Bankruptcy Court approved its first day motions and its use of a $70 million financing facility provided by a group of its bondholders. Foreign subsidiaries were unaffected.
On September 6, 2006, SGI announced the end of development for the MIPS/IRIX line and the IRIX operating system. Production would end on 29 December and the last orders would be fulfilled by March 2007. Support for these products would end after December 2013.
SGI emerged from bankruptcy protection on October 17, 2006. Its stock symbol at that point, "SGID.pk", was canceled, and new stock was issued on the NASDAQ exchange under the symbol "SGIC". This new stock was distributed to the company's creditors, and the SGID common stockholders were left with worthless shares. At the end of that year, the company moved its headquarters from Mountain View to Sunnyvale. Its earlier North Shoreline headquarters is now occupied by the Computer History Museum; the newer Amphitheater Parkway headquarters was sold to Google. Both of these locations were award-winning designs by Studios Architecture.
In April 2008, SGI re-entered the visualization market with the SGI Virtu range of visualization servers and workstations, which were re-badged systems from BOXX Technologies based on Intel Xeon or AMD Opteron processors and Nvidia Quadro graphics chipsets, running Red Hat Enterprise Linux, SUSE Linux Enterprise Server or Windows Compute Cluster Server.
Graphics Properties Holdings, Inc. era.
During the Silicon Graphics Inc.'s second bankruptcy phase, it was renamed to Graphics Properties Holdings, Inc.(GPHI) in June 2009.
In 2010, GPHI announced it had won a significant favorable ruling in its litigation with ATI Technologies and AMD in June 2010, following the patent lawsuit originally filed during the Silicon Graphics, Inc. era. Following the 2008 appeal by ATI over the validity of ('327) and Silicon Graphics Inc's voluntary dismissal of the ('376) patent from the lawsuit, the Federal Circuit upheld the jury verdict on the validity of GPHI's U.S. Patent No. 6,650,327, and furthermore found that AMD had lost its right to challenge patent validity in future proceedings. On January 31, 2011, the District Court entered an order that permits AMD to pursue its invalidity affirmative defense at trial and does not permit SGI to accuse AMD's Radeon R700 series of graphics products of infringement in this case. On April 18, 2011, GPHI and AMD had entered into a confidential Settlement and License Agreement that resolved this litigation matter for an immaterial amount and that provides immunity under all GPHI patents for alleged infringement by AMD products, including components, software and designs. On April 26, 2011, the Court entered an order granting the parties' agreed motion for dismissal and final judgment.
In 2011-11, GPHI filed another patent infringement lawsuit against Apple Inc. in Delaware involving more patents than their original patent infringement case against Apple last November, for alleged violation of U.S. patents 6,650,327 ('327), ('145) and ('881).
In 2012, the GPHI filed lawsuit against Apple, Sony, HTC Corp, LG Electronics Inc. and Samsung Electronics Co., Research in Motion Ltd. for allegedly violating patent relating to a computer graphics process that turns text and images into pixels to be displayed on screens. Affected devices include Apple iPhone, HTC EVO4G, LG Thrill, Research in Motion Torch, Samsung Galaxy S and Galaxy S II, and Sony Xperia Play smartphones.
Final bankruptcy and acquisition by Rackable Systems.
In December 2008, SGI received a delisting notification from NASDAQ, as its market value had been below the minimum $35 million requirement for 10 consecutive trading days, and also did not meet NASDAQ's alternative requirements of a minimum stockholders' equity of $2.5 million or annual net income from continuing operations of $500,000 or more.
On April 1, 2009, SGI filed for Chapter 11 again, and announced that it would sell substantially all of its assets to Rackable Systems for $25 million. The sale, ultimately for $42.5 million, was finalized on May 11, 2009; at the same time, Rackable announced their adoption of "Silicon Graphics International" as their global name and brand. The Bankruptcy court scheduled continuing proceedings and hearings for June 3 and 24, 2009, and July 22, 2009.
After the Rackable acquisition, "Vizworld" magazine published a series of six articles that chronicle the downfall of SGI.
Technology.
Motorola 680x0-based systems.
SGI's first generation products, starting with the IRIS (Integrated Raster Imaging System) 1000 series of high-performance graphics terminals, were based on the Motorola 68000 family of microprocessors. The later IRIS 2000 and 3000 models developed into full UNIX workstations.
IRIS 1000 series.
The first entries in the 1000 series (models 1000 and 1200, introduced in 1984) were graphics terminals, peripherals to be connected to a general-purpose computer such as a Digital Equipment Corporation VAX, to provide graphical raster display abilities. They used 8 MHz Motorola 68000 CPUs with of RAM and had no disk drives. They booted over the network (via an Excelan EXOS/101 Ethernet card) from their controlling computer. They used the "PM1" CPU board, which was a variant of the board that was used in Stanford University's SUN workstation and later in the Sun-1 workstation from Sun Microsystems. The graphics system was composed of the GF1 frame buffer, the UC3 "Update Controller", DC3 "Display Controller", and the BP2 bitplane. The 1000-series machines were designed around the Multibus standard.
Later 1000-series machines, the 1400 and 1500, ran at 10 MHz and had 1.5 MB of RAM. The 1400 had a 73 MB ST-506 disk drive, while the 1500 had a 474 MB SMD-based disk drive with a Xylogics 450 disk controller. They may have used the PM2 CPU and PM2M1 RAM board from the 2000 series. The usual monitor for the 1000 series ran at 30 Hz interlaced. Six beta-test units of the 1400 workstation were produced, and the first production unit (SGI's first commercial computer) was shipped to Carnegie-Mellon University's Electronic Imaging Laboratory in 1984.
IRIS 2000 and 3000 series.
SGI rapidly developed its machines into workstations with its second product line — the IRIS 2000 series, first released in August, 1985. SGI began using the UNIX System V operating system. There were five models in two product ranges, the 2000/2200/2300/2400/2500 range which used 68010 CPUs (the PM2 CPU module), and the later "Turbo" systems, the 2300T, 2400T and 2500T, which had 68020s (the IP2 CPU module). All used the Excelan EXOS/201 Ethernet card, the same graphics hardware (GF2 Frame Buffer, UC4 Update Controller, DC4 Display Controller, BP3 Bitplane). Their main differences were the CPU, RAM, and Weitek Floating Point Accelerator boards, disk controllers and disk drives (both ST-506 and SMD were available). These could be upgraded, for example from a 2400 to a 2400T. The 2500 and 2500T had a larger chassis, a standard 6' 19" EIA rack with space at the bottom for two SMD disk drives weighing approximately each. The non-Turbo models used the Multibus for the CPU to communicate with the floating point accelerator, while the Turbos added a ribbon cable dedicated for this. 60 Hz monitors were used for the 2000 series.
The height of the machines using Motorola CPUs was reached with the IRIS 3000 series (models 3010/3020/3030 and 3110/3115/3120/3130, the 30s both being full-size rack machines). They used the same graphics subsystem and Ethernet as the 2000s, but could also use up to 12 "geometry engines", the first widespread use of hardware graphics accelerators. The standard monitor was a 19" 60 Hz non-interlaced unit with a tilt/swivel base; 19" 30 Hz interlaced and a 15" 60 Hz non-interlaced (with tilt/swivel base) were also available.
The IRIS 3130 and its smaller siblings were impressive for the time, being complete UNIX workstations. The 3130 was powerful enough to support a complete 3D animation and rendering package without mainframe support. With large capacity hard drives by standards of the day (two 300 MB drives), streaming tape and Ethernet, it could be the centerpiece of an animation operation.
The line was formally discontinued in November 1989, with about 3500 systems shipped of all 2000 and 3000 models combined.
RISC era.
With the introduction of the IRIS 4D series, SGI switched to MIPS microprocessors. These machines were more powerful and came with powerful on-board floating-point capability. As 3D graphics became more popular in television and film during this time, these systems were responsible for establishing much of SGI's reputation.
SGI produced a broad range of MIPS-based workstations and servers during the 1990s, running SGI's version of UNIX System V, now called IRIX. These included the massive Onyx visualization systems, the size of refrigerators and capable of supporting up to 64 processors while managing up to three streams of high resolution, fully realized 3D graphics.
In October 1991, MIPS announced the first commercially available 64-bit microprocessor, the R4000. SGI used the R4000 in its Crimson workstation. IRIX 6.2 was the first fully 64-bit IRIX release, including 64-bit pointers.
To secure the supply of future generations of MIPS microprocessors (the 64-bit R4000), SGI acquired the company in 1992 for $333 million and renamed it as MIPS Technologies Inc., a wholly owned subsidiary of SGI.
In 1996, Silicon Graphics (SGI) and MIPS Technologies were responsible for the CPU used in the Nintendo 64, a derivative of the R4300i microprocessor.
In 1998 relinquished MIPS Technologies, Inc in a Re-IPO.
In the late 1990s, when much of the industry expected the Itanium to replace both CISC and RISC architectures in non-embedded computers, SGI announced their intent to phase out MIPS in their systems. Development of new MIPS microprocessors stopped, and the existing R12000 design was extended multiple times until 2003 to provide existing customers more time to migrate to Itanium.
In August 2006, SGI announced the end of production for MIPS/IRIX systems, and by the end of the year MIPS/IRIX products were no longer generally available from SGI.
IRIS GL and OpenGL.
Until the second generation Onyx Reality Engine machines, SGI offered access to its high performance 3D graphics subsystems through a proprietary API known as "IRIS Graphics Language" (IRIS GL). As more features were added over the years, IRIS GL became harder to maintain and more cumbersome to use. In 1992, SGI decided to clean up and reform IRIS GL and made the bold move of allowing the resulting OpenGL API to be cheaply licensed by SGI's competitors, and set up an industry-wide consortium to maintain the OpenGL standard (the OpenGL Architecture Review Board).
This meant that for the first time, fast, efficient, cross-platform graphics programs could be written. To this day, OpenGL remains the only real-time 3D graphics standard to be portable across a variety of operating systems. OpenGL-ES even runs on many types of cell phones. Its main competitor (Direct3D from Microsoft) runs only on Microsoft Windows-based machines and some consoles.
ACE Consortium.
SGI was part of the Advanced Computing Environment initiative, formed in the early 1990s with 20 other companies, including Compaq, Digital Equipment Corporation, MIPS Computer Systems, Groupe Bull, Siemens, NEC, NeTpower, Microsoft and Santa Cruz Operation. Its intent was to introduce workstations based on the MIPS architecture and able to run Windows NT and SCO UNIX. The group produced the Advanced RISC Computing (ARC) specification, but began to unravel little more than a year after its formation.
Entertainment industry.
An SGI Crimson system with the fsn three-dimensional file system navigator appeared in the 1993 movie "Jurassic Park".
In the movie "Twister", protagonists can be seen using an SGI laptop computer, however the unit shown was not an actual working computer, but rather a fake laptop shell built around an SGI Corona LCD flat screen display.
The 1995 film "Congo" also features an SGI laptop computer being used by Dr. Ross (Laura Linney) to communicate via satellite to TraviCom HQ.
The purple, lowercased "sgi" logo can be seen at the beginning of the opening credits of the HBO series "Silicon Valley", before being taken down and replaced by the Google logo as the intro graphics progress.
Other on-screen credits include:
For eight consecutive years (1995–2002), all films nominated for an Academy Award for Distinguished Achievement in Visual Effects were created on Silicon Graphics computer systems.
Once inexpensive PCs began to have graphics performance close to the more expensive specialized graphical workstations which were SGI's core business, SGI shifted its focus to high performance servers for digital video and the Web. Many SGI graphics engineers left to work at other computer graphics companies such as ATI and Nvidia, contributing to the PC 3D graphics revolution.
Free software.
SGI was a promoter of free software, supporting several projects such as Linux and Samba, and opening some of its own previously proprietary code such as the XFS filesystem and the Open64 compiler.
Acquisition of Alias, Wavefront, Cray and Intergraph.
In 1995, SGI purchased Alias Research and Wavefront Technologies in a deal totaling approximately $500 million and merged the companies into Alias|Wavefront. In June 2004 SGI sold the business, later renamed to Alias Systems Corporation, to the private equity investment firm Accel-KKR for $57.1 million. In October 2005, Autodesk announced that it signed a definitive agreement to acquire Alias for $182 million in cash.
In February 1996, SGI purchased the well-known supercomputer manufacturer Cray Research for $740 million, and began to use marketing names such as "CrayLink" for (SGI-developed) technology integrated into the SGI server line. Three months later, it sold Cray's Business Systems Division, responsible for the CS6400 SPARC/Solaris server, to Sun Microsystems for an undisclosed amount (widely believed to be $50 million). Many of the Cray T3E engineers designed and developed the SGI Altix and NUMAlink technology. SGI sold the Cray brand and product lines to Tera Computer Company on March 31, 2000 for $35 million plus one million shares. SGI also distributed its remaining interest in MIPS Technologies through a spin-off effective June 20, 2000.
In September 2000, SGI acquired the Zx10 series of Windows workstations and servers from Intergraph Computer Systems (for a rumored $100 million). These models were rebadged as SGI systems, but discontinued in June 2001.
SGI Visual Workstations.
Another attempt by SGI in the late 1990s to introduce its own family of Intel-based workstations running Windows NT or Red Hat Linux (see also SGI Visual Workstation) proved to be a financial disaster, and shook customer confidence in SGI’s commitment to its own MIPS-based line.
Switch to Itanium.
In 1998, SGI announced that future generations of its machines would be based not on their own MIPS processors, but the upcoming "super-chip" from Intel, code-named "Merced" and later called Itanium. Funding for its own high-end processors was reduced, and it was planned that the R10000 would be the last MIPS mainstream processor. MIPS Technologies would focus entirely on the embedded market, where it was having some success, and SGI would no longer have to fund development of a CPU that, since the failure of ARC, found use only in their own machines. This plan quickly went awry. As early as 1999 it was clear the Itanium was going to be delivered very late, and then that it would have nowhere near the performance originally expected. As the production delays increased, MIPS' existing R10000-based machines grew increasingly uncompetitive. Eventually it was forced to introduce faster MIPS processors, the R12000, R14000 and R16000, which were used in a series of models from 2002 through 2006.
SGI's first Itanium-based system was the short-lived SGI 750 workstation, launched in 2001. SGI's MIPS-based systems were not to be superseded until the launch of the Itanium 2-based Altix servers and Prism workstations some time later. Unlike the MIPS systems, which ran IRIX, the Itanium systems used SuSE Linux Enterprise Server with SGI enhancements as their operating system. SGI used Transitive Corporation's QuickTransit software to allow their old MIPS/IRIX applications to run (in emulation) on the new Itanium/Linux platform.
In the server market the Itanium 2-based Altix eventually replaced the MIPS-based Origin product line. In the workstation market, the switch to Itanium was not completed before SGI exited the market.
The Altix was the most powerful computer in the world in 2006, assuming that a "computer" is defined as a collection of hardware running under a single instance of an operating system. The Altix had 512 Itanium processors running under a single instance of Linux. A cluster of 20 machines was then the eighth-fastest supercomputer. All faster supercomputers were clusters, but none have as many FLOPS per machine. However, more recent supercomputers are very large clusters of machines that are individually less capable. SGI acknowledged this and in 2007 moved away from the "massive NUMA" model to clusters.
Switch to Xeon.
Although SGI continued to market Itanium-based machines, its more recent machines were based on the Intel Xeon processor. The first Altix XE systems were relatively low-end machines, but by December 2006 the XE systems were more capable than the Itanium machines by some measures (e.g., power consumption in FLOPS/W, density in FLOPS/m3, cost/FLOPS). The XE1200 and XE1300 servers used a cluster architecture. This was a departure from the pure NUMA architectures of the earlier Itanium and MIPS servers.
In June 2007, SGI announced the Altix ICE 8200, a blade-based Xeon system with up to 512 Xeon cores per rack. An Altix ICE 8200 installed at New Mexico Computing Applications Center (with 14336 processors) ranked at number 3 on the TOP500 list of November 2007.
User base and core market.
Conventional wisdom holds that SGI's core market has traditionally been Hollywood visual effects studios. In fact, SGI's largest revenue has always been generated by government and defense applications, energy, and scientific and technical computing. The rise of cheap yet powerful commodity workstations running Linux, Windows and Mac OS X, and the availability of diverse professional software for them, effectively pushed SGI out of the visual effects industry in all but the most niche markets.
High-end server market.
SGI continued to enhance its line of servers (including some supercomputers) based on the SN architecture. SN, for Scalable Node, is a technology developed by SGI in the mid-1990s that uses cache-coherent non-uniform memory access (cc-NUMA). In an SN system, processors, memory, and a bus- and memory-controller are coupled together into an entity called a node, usually on a single circuit board. Nodes are connected by a high-speed interconnect called NUMAlink (originally marketed as CrayLink). There is no internal bus, and instead access between processors, memory, and I/O devices is done through a switched fabric of links and routers.
Thanks to the cache coherence of the distributed shared memory, SN systems scale along several axes at once: as CPU count increases, so does memory capacity, I/O capacity, and system bisection bandwidth. This allows the combined memory of all the nodes to be accessed under a single OS image using standard shared-memory synchronization methods. This makes an SN system far easier to program and able to achieve higher sustained-to-peak performance than non-cache-coherent systems like conventional clusters or massively parallel computers which require applications code to be written (or re-written) to do explicit message-passing communication between their nodes.
The first SN system, known as SN-0, was released in 1996 under the product name Origin 2000. Based on the MIPS R10000 processor, it scaled from 2 to 128 processors and a smaller version, the Origin 200 (SN-00), scaled from 1 to 4. Later enhancements enabled systems of as large as 512 processors.
The second generation system, originally called SN-1 but later SN-MIPS, was released in July 2000, as the Origin 3000. It scaled from 4 to 512 processors, and 1,024-processor configurations were delivered by special order to some customers. A smaller, less scalable implementation followed, called Origin 300.
In November 2002, SGI announced a repackaging of its SN system, under the name Origin 3900. It quadrupled the processor area density of the SN-MIPS system, from 32 up to 128 processors per rack while moving to a "fat tree" interconnect topology.
In January 2003, SGI announced a variant of the SN platform called the Altix 3000 (internally called SN-IA). It used Intel Itanium 2 processors and ran the Linux operating system kernel. At the time it was released, it was the world's most scalable Linux-based computer, supporting up to 64 processors in a single system node. Nodes could be connected using the same NUMAlink technology to form what SGI predictably termed "superclusters".
In February 2004, SGI announced general support for 128 processor nodes to be followed by 256 and 512 processor versions that year.
In April 2004, SGI announced the sale of its Alias software business for approximately $57 million.
In October 2004, SGI built the supercomputer Columbia, which broke the world record for computer speed, for the NASA Ames Research Center. It was a cluster of 20 Altix supercomputers each with 512 Intel Itanium 2 processors running Linux, and achieved sustained speed of 42.7 trillion floating-point operations per second (teraflops), easily topping Japan's famed Earth Simulator's record of 35.86 teraflops. (A week later, IBM's upgraded Blue Gene/L clocked in at 70.7 teraflops).
In July 2006, SGI announced an SGI Altix 4700 system with 1,024 processors and 4 TB of memory running a single Linux system image.
Hardware products.
Some 68k and MIPS-based models were also rebadged by other vendors, including CDC, Tandem Computers, Prime Computer and Siemens-Nixdorf.

</doc>
<doc id="28016" url="https://en.wikipedia.org/wiki?curid=28016" title="Steiner system">
Steiner system

In combinatorial mathematics, a Steiner system (named after Jakob Steiner) is a type of block design, specifically a with λ = 1 and "t" ≥ 2.
A Steiner system with parameters "t", "k", "n", written S("t","k","n"), is an "n"-element set "S" together with a set of "k"-element subsets of "S" (called blocks) with the property that each "t"-element subset of "S" is contained in exactly one block. In an alternate notation for block designs, an S("t","k","n") would be a "t"-("n","k",1) design.
This definition is relatively modern, generalizing the "classical" definition of Steiner systems which in addition required that "k" = "t" + 1. An S(2,3,"n") was (and still is) called a "Steiner triple" (or "triad") "system", while an S(3,4,"n") was called a "Steiner quadruple system", and so on. With the generalization of the definition, this naming system is no longer strictly adhered to.
A long-standing problem in design theory is if any nontrivial ("t" < "k" < "n") Steiner systems have "t" ≥ 6; also if infinitely many have "t" = 4 or 5. This was claimed to be solved in the affirmative by Peter Keevash.
Examples.
Finite projective planes.
A finite projective plane of order "q", with the lines as blocks, is an formula_1, since it has formula_2 points, each line passes through formula_3 points, and each pair of distinct points lies on exactly one line.
Finite affine planes.
A finite affine plane of order "q", with the lines as blocks, is an S(2, "q", "q"2). An affine plane of order "q" can be obtained from a projective plane of the same order by removing one block and all of the points in that block from the projective plane. Choosing different blocks to remove in this way can lead to non-isomorphic affine planes.
Classical Steiner systems.
Steiner triple systems.
An S(2,3,"n") is called a Steiner triple system, and its blocks are called triples. It is common to see the abbreviation STS("n") for a Steiner triple system of order "n". 
The number of triples through a point is "(n-1)/2" and hence the total number of triples is "n"("n"−1)/6. This shows that "n" must be of the form "6k+1" or "6k + 3" for some "k". The fact that this condition on "n" is sufficient for the existence of an S(2,3,"n") was proved by Raj Chandra Bose and T. Skolem. The projective plane of order 2 (the Fano plane) is an STS(7) and the affine plane of order 3 is an STS(9).
Up to isomorphism, the STS(7) and STS(9) are unique, there are two STS(13)s, 80 STS(15)s, and 11,084,874,829 STS(19)s.
We can define a multiplication on the set "S" using the Steiner triple system by setting "aa" = "a" for all "a" in "S", and "ab" = "c" if {"a","b","c"} is a triple. This makes "S" an idempotent, commutative quasigroup. It has the additional property that "ab" = "c" implies "bc" = "a" and "ca" = "b". Conversely, any (finite) quasigroup with these properties arises from a Steiner triple system. Commutative idempotent quasigroups satisfying this additional property are called "Steiner quasigroups".
Steiner quadruple systems.
An S(3,4,"n") is called a Steiner quadruple system. A necessary and sufficient condition for the existence of an S(3,4,"n") is that "n" formula_4 2 or 4 (mod 6). The abbreviation SQS("n") is often used for these systems.
Up to isomorphism, SQS(8) and SQS(10) are unique, there are 4 SQS(14)s and 1,054,163 SQS(16)s.
Steiner quintuple systems.
An S(4,5,"n") is called a "Steiner quintuple system". A necessary condition for the existence of such a system is that "n" formula_4 3 or 5 (mod 6) which comes from considerations that apply to all the classical Steiner systems. An additional necessary condition is that "n" formula_6 4 (mod 5), which comes from the fact that the number of blocks must be an integer. Sufficient conditions are not known.
There is a unique Steiner quintuple system of order 11, but none of order 15 or order 17. Systems are known for orders 23, 35, 47, 71, 83, 107, 131, 167 and 243. The smallest order for which the existence is not known (as of 2011) is 21.
Properties.
It is clear from the definition of S("t","k","n") that formula_7. (Equalities, while technically possible, lead to trivial systems.)
If S("t","k","n") exists, then taking all blocks containing a specific element and discarding that element gives a "derived system" S("t"−1,"k"−1,"n"−1). Therefore the existence of S("t"−1,"k"−1,"n"−1) is a necessary condition for the existence of S("t","k","n").
The number of "t"-element subsets in S is formula_8, while the number of "t"-element subsets in each block is formula_9. Since every "t"-element subset is contained in exactly one block, we have formula_10, or formula_11, where "b" is the number of blocks. Similar reasoning about "t"-element subsets containing a particular element gives us formula_12, or formula_13, where "r" is the number of blocks containing any given element. From these definitions follows the equation formula_14. It is a necessary condition for the existence of S("t","k","n") that "b" and "r" are integers. As with any block design, Fisher's inequality formula_15 is true in Steiner systems.
Given the parameters of a Steiner system S("t,k,n") and a subset of size formula_16, contained in at least one block, one can compute the number of blocks intersecting that subset in a fixed number of elements by constructing a Pascal triangle. In particular, the number of blocks intersecting a fixed block in any number of elements is independent of the chosen block.
It can be shown that if there is a Steiner system S(2,"k","n"), where "k" is a prime power greater than 1, then "n" formula_4 1 or "k" (mod "k"("k"−1)). In particular, a Steiner triple system S(2,3,"n") must have "n" = 6"m"+1 or 6"m"+3. And as we have already mentioned, this is the only restriction on Steiner triple systems, that is, for each natural number "m", systems S(2,3,6"m"+1) and S(2,3,6"m"+3) exist.
History.
Steiner triple systems were defined for the first time by W.S.B. Woolhouse in 1844 in the Prize question #1733 of Lady's and Gentlemen's Diary. The posed problem was solved by . In 1850 Kirkman posed a variation of the problem known as Kirkman's schoolgirl problem, which asks for triple systems having an additional property (resolvability). Unaware of Kirkman's work, reintroduced triple systems, and as this work was more widely known, the systems were named in his honor.
Mathieu groups.
Several examples of Steiner systems are closely related to group theory. In particular, the finite simple groups called Mathieu groups arise as automorphism groups of Steiner systems:
The Steiner system S(5, 6, 12).
There is a unique S(5,6,12) Steiner system; its automorphism group is the Mathieu group M12, and in that context it is denoted by W12.
Constructions.
There are different ways to construct an S(5,6,12) system.
Projective line method.
This construction is due to Carmichael (1937).
Add a new element, call it , to the 11 elements of the finite field 11 (that is, the integers mod 11). This set, , of 12 elements can be formally identified with the points of the projective line over 11. Call the following specific subset of size 6,
a "block". From this block, we obtain the other blocks of the S(5,6,12) system by repeatedly applying the linear fractional transformations:
With the usual conventions of defining and , these functions map the set onto itself. In geometric language, they are projectivities of the projective line. They form a group under composition which is the projective special linear group PSL(2,11) of order 660. There are exactly five elements of this group that leave the starting block fixed setwise, so there will be 132 images of that block. As a consequence of the multiply transitive property of this group acting on this set, any subset of five elements of will appear in exactly one of these 132 images of size six.
Kitten method.
An alternative construction of W12 is obtained by use of the 'kitten' of R.T. Curtis, which was intended as a "hand calculator" to write down blocks one at a time. The kitten method is based on completing patterns in a 3x3 grid of numbers, which represent an affine geometry on the vector space F3xF3, an S(2,3,9) system.
Construction from K6 graph factorization.
The relations between the graph factors of the complete graph K6 generate an S(5,6,12). A K6 graph has 6 different 1-factorizations (ways to partition the edges into disjoint perfect matchings), and also 6 vertices. The set of vertices and the set of factorizations provide one block each. For every distinct pair of factorizations, there exists exactly one perfect matching in common. Take the set of vertices and replace the two vertices corresponding to an edge of the common perfect matching with the labels corresponding to the factorizations; add that to the set of blocks. Repeat this with the other two edges of the common perfect matching. Similarly take the set of factorizations and replace the labels corresponding to the two factorizations with the end points of an edge in the common perfect matching. Repeat with the other two edges in the matching. There are thus 3+3 = 6 blocks per pair of factorizations, and there are 6C2 = 15 pairs among the 6 factorizations, resulting in 90 new blocks. Finally take the full set of 12C6 = 924 combinations of 6 objects out of 12, and discard any combination that has 5 or more objects in common with any of the 92 blocks generated so far. Exactly 40 blocks remain, resulting in 2+90+40 = 132 blocks of the S(5,6,12).
The Steiner system S(5, 8, 24).
The Steiner system S(5, 8, 24), also known as the Witt design or Witt geometry, was first described by and rediscovered by . This system is connected with many of the sporadic simple groups and with the exceptional 24-dimensional lattice known as the Leech lattice.
The automorphism group of S(5, 8, 24) is the Mathieu group M24, and in that context the design is denoted W24 ("W" for "Witt")
Constructions.
There are many ways to construct the S(5,8,24). Two methods are described here:
Method based on 8-combinations of 24 elements.
All 8-element subsets of a 24-element set are generated in lexicographic order, and any such subset which differs from some subset already found in fewer than four positions is discarded.
The list of octads for the elements 01, 02, 03, ..., 22, 23, 24 is then:
Each single element occurs 253 times somewhere in some octad. Each pair occurs 77 times. Each triple occurs 21 times. Each quadruple (tetrad) occurs 5 times. Each quintuple (pentad) occurs once. Not every hexad, heptad or octad occurs.
Method based on 24-bit binary strings.
All 24-bit binary strings are generated in lexicographic order, and any such string that differs from some earlier one in fewer than 8 positions is discarded. The result looks like this:
The list contains 4096 items, which are each code words of the extended binary Golay code. They form a group under the XOR operation. One of them has zero 1-bits, 759 of them have eight 1-bits, 2576 of them have twelve 1-bits, 759 of them have sixteen 1-bits, and one has twenty-four 1-bits. The 759 8-element blocks of the S(5,8,24) (called octads) are given by the patterns of 1's in the code words with eight 1-bits.

</doc>
<doc id="28017" url="https://en.wikipedia.org/wiki?curid=28017" title="Sirius">
Sirius

Sirius () is the brightest star (in fact, a star system) in the Earth's night sky. With a visual apparent magnitude of −1.46, it is almost twice as bright as Canopus, the next brightest star. The name "Sirius" is derived from the Ancient Greek Σείριος ("Seirios"), meaning "glowing" or "scorcher". The system has the Bayer designation Alpha Canis Majoris (α CMa). What the naked eye perceives as a single star is actually a binary star system, consisting of a white main-sequence star of spectral type A1V, termed Sirius A, and a faint white dwarf companion of spectral type DA2, called Sirius B. The distance separating Sirius A from its companion varies between 8.2 and 31.5 AU.
Sirius appears bright because of both its intrinsic luminosity and its proximity to Earth. At a distance of 2.6 parsecs (8.6 ly), as determined by the Hipparcos astrometry satellite, the Sirius system is one of Earth's near neighbors. Sirius is gradually moving closer to the Solar System, so it will slightly increase in brightness over the next 60,000 years. After that time its distance will begin to increase and it will become fainter, but it will continue to be the brightest star in the Earth's sky for the next 210,000 years.
Sirius A is about twice as massive as the Sun () and has an absolute visual magnitude of 1.42. It is 25 times more luminous than the Sun but has a significantly lower luminosity than other bright stars such as Canopus or Rigel. The system is between 200 and years old. It was originally composed of two bright bluish stars. The more massive of these, Sirius B, consumed its resources and became a red giant before shedding its outer layers and collapsing into its current state as a white dwarf around years ago.
Sirius is also known colloquially as the "Dog Star", reflecting its prominence in its constellation, Canis Major (Greater Dog). The heliacal rising of Sirius marked the flooding of the Nile in Ancient Egypt and the "dog days" of summer for the ancient Greeks, while to the Polynesians in the Southern Hemisphere the star marked winter and was an important reference for their navigation around the Pacific Ocean.
Observational history.
Sirius, known in ancient Egypt as "Sopdet" (Greek: Σῶθις "Sothis"), is recorded in the earliest astronomical records. During the era of the Middle Kingdom, Egyptians based their calendar on the heliacal rising of Sirius, namely the day it becomes visible just before sunrise after moving far enough away from the glare of the Sun. This occurred just before the annual flooding of the Nile and the summer solstice, after a 70-day absence from the skies. The hieroglyph for Sothis features a star and a triangle. Sothis was identified with the great goddess Isis, who formed a part of a triad with her husband Osiris and their son Horus, while the 70-day period symbolised the passing of Isis and Osiris through the "duat" (Egyptian underworld).
The ancient Greeks observed that the appearance of Sirius heralded the hot and dry summer, and feared that it caused plants to wilt, men to weaken, and women to become aroused. Due to its brightness, Sirius would have been noted to twinkle more in the unsettled weather conditions of early summer. To Greek observers, this signified certain emanations which caused its malignant influence. Anyone suffering its effects was said to be "astroboletos" (ἀστροβόλητος) or "star-struck". It was described as "burning" or "flaming" in literature. The season following the star's heliacal rising (i.e. rising with the Sun) came to be known as the Dog Days of summer. The inhabitants of the island of Ceos in the Aegean Sea would offer sacrifices to Sirius and Zeus to bring cooling breezes, and would await the reappearance of the star in summer. If it rose clear, it would portend good fortune; if it was misty or faint then it foretold (or emanated) pestilence. Coins retrieved from the island from the 3rd century BC feature dogs or stars with emanating rays, highlighting Sirius' importance. The Romans celebrated the heliacal setting of Sirius around April 25, sacrificing a dog, along with incense, wine, and a sheep, to the goddess Robigo so that the star's emanations would not cause wheat rust on wheat crops that year.
Ptolemy of Alexandria mapped the stars in Books VII and VIII of his "Almagest", in which he used Sirius as the location for the globe's central meridian. He curiously depicted it as one of six red-coloured stars (see the Red controversy section below). The other five are class M and K stars, such as Arcturus and Betelgeuse.
Bright stars were important to the ancient Polynesians for navigation between the many islands and atolls of the Pacific Ocean. Low on the horizon, they acted as stellar compasses to assist mariners in charting courses to particular destinations. They also served as latitude markers; the declination of Sirius matches the latitude of the archipelago of Fiji at 17°S and thus passes directly over the islands each night. Sirius served as the body of a "Great Bird" constellation called "Manu", with Canopus as the southern wingtip and Procyon the northern wingtip, which divided the Polynesian night sky into two hemispheres. Just as the appearance of Sirius in the morning sky marked summer in Greece, so it marked the chilly onset of winter for the Māori, whose name "Takurua" described both the star and the season. Its culmination at the winter solstice was marked by celebration in Hawaii, where it was known as "Ka'ulua", "Queen of Heaven". Many other Polynesian names have been recorded, including "Tau-ua" in the Marquesas Islands, "Rehua" in New Zealand, and "Ta'urua-fau-papa" "Festivity of original high chiefs" and "Ta'urua-e-hiti-i-te-tara-te-feiai" "Festivity who rises with prayers and religious ceremonies" in Tahiti. The Hawaiian people had many names for Sirius, including "Aa" ("glowing"), "Hoku-kauopae", "Kau-ano-meha" (also "Kaulanomeha"), "Standing-alone-and-sacred", "Hiki-kauelia" or "Hiki-kauilia" (the navigational name), "Hiki-kau-lono-meha" ("star of solitary Lono", the astrological name), "Kaulua" (also "Kaulua-ihai-mohai", "flower of the heavens"), "Hiki-kauelia", "Hoku-hoo-kele-waa" ("star which causes the canoe to sail", a marine navigation name), and "Kaulua-lena" ("yellow star"). The people of the Society Islands called Sirius variously "Taurua-fau-papa", "Taurua-nui-te-amo-aha", and "Taurua-e-hiti-i-tara-te-feiai". Other names for Sirius included "Palolo-mua" (Futuna), "Mere" (Mangaia), "Apura" (Manihiki), "Taku-ua" (Marquesas Islands), and "Tokiva" (Pukapuka). In the cosmology of the Tuamotus, Sirius had various names, including "Takurua-te-upuupu", "Te Kaha" ("coconut fiber"), "Te Upuupu", "Taranga", and "Vero-ma-torutoru" ("flaming and diminishing").
The indigenous Boorong people of northwestern Victoria named Sirius as "Warepil".
Kinematics.
In 1718, Edmond Halley discovered the proper motion of the hitherto presumed "fixed" stars after comparing contemporary astrometric measurements with those given in Ptolemy's "Almagest". The bright stars Aldebaran, Arcturus and Sirius were noted to have moved significantly, the last of which having progressed 30 arc minutes (about the diameter of the Moon) southwards in 1,800 years.
In 1868, Sirius became the first star to have its velocity measured. Sir William Huggins examined the spectrum of this star and observed a noticeable red shift. He concluded that Sirius was receding from the Solar System at about 40 km/s. Compared to the modern value of −5.5 km/s, this both was an overestimate and had the wrong sign; the minus means it is approaching the Sun. However, it is notable for introducing the study of celestial radial velocities.
Distance.
In his 1698 book "Cosmotheoros", Christiaan Huygens estimated the distance to Sirius at 27664 times the distance of the earth to the sun (about 0.437 light years).
The parallax of Sirius was measured by Thomas Henderson using his observations made in 1832-1833 and Maclear's observations made in 1836-1837, and was published in 1839. The value of the parallax was 0.23 arcseconds, and error of the parallax was estimated not to exceed a quarter of a second.
Also, there were earlier attempts to measure the parallax of Sirius: by the second Cassini (6 seconds); by some astronomers (including Nevil Maskelyne) using Lacaille's observations made at the Cape of Good Hope (4 seconds); by Piazzi (the same amount); using Lacaille's observations made at Paris, more numerous and certain than those made at the Cape (no sensible parallax); by Bessel (no sensible parallax).
Discovery of a companion.
In 1844 the German astronomer Friedrich Bessel deduced from changes in the proper motion of Sirius that it had an unseen companion. Nearly two decades later, on January 31, 1862, American telescope-maker and astronomer Alvan Graham Clark first observed the faint companion, which is now called Sirius B, or affectionately "the Pup". This happened during testing of an aperture great refractor telescope for Dearborn Observatory, which was the largest refracting telescope lens in existence at the time, and the largest telescope in the United States. Sirius B sighting was confirmed on March 8 with smaller telescopes as well.
The visible star is now sometimes known as Sirius A. Since 1894, some apparent orbital irregularities in the Sirius system have been observed, suggesting a third very small companion star, but this has never been definitely confirmed. The best fit to the data indicates a six-year orbit around Sirius A and a mass of only . This star would be five to ten magnitudes fainter than the white dwarf Sirius B, which would account for the difficulty of observing it. Observations published in 2008 were unable to detect either a third star or a planet. An apparent "third star" observed in the 1920s is now confirmed as a background object.
In 1915, Walter Sydney Adams, using a 60-inch (1.5 m) reflector at Mount Wilson Observatory, observed the spectrum of Sirius B and determined that it was a faint whitish star. This led astronomers to conclude that it was a white dwarf, the second to be discovered. The diameter of Sirius A was first measured by Robert Hanbury Brown and Richard Q. Twiss in 1959 at Jodrell Bank using their stellar intensity interferometer. In 2005, using the Hubble Space Telescope, astronomers determined that Sirius B has nearly the diameter of the Earth, , with a mass that is 98% of the Sun.
Red controversy.
Around 150 AD, the Greek astronomer of the Roman period Claudius Ptolemy described Sirius as reddish, along with five other stars, Betelgeuse, Antares, Aldebaran, Arcturus and Pollux, all of which are clearly of orange or red hue. The discrepancy was first noted by amateur astronomer Thomas Barker, squire of Lyndon Hall in Rutland, who prepared a paper and spoke at a meeting of the Royal Society in London in 1760. The existence of other stars changing in brightness gave credence to the idea that some may change in color too; Sir John Herschel noted this in 1839, possibly influenced by witnessing Eta Carinae two years earlier. Thomas Jefferson Jackson See resurrected discussion on red Sirius with the publication of several papers in 1892, and a final summary in 1926. He cited not only Ptolemy but also the poet Aratus, the orator Cicero, and general Germanicus as coloring the star red, though acknowledging that none of the latter three authors were astronomers, the last two merely translating Aratus' poem "Phaenomena". Seneca, too, had described Sirius as being of a deeper red color than Mars. However, not all ancient observers saw Sirius as red. The 1st century AD poet Marcus Manilius described it as "sea-blue", as did the 4th century Avienus. It is the standard star for the color white in ancient China, and multiple records from the 2nd century BC up to the 7th century AD all describe Sirius as white in hue.
In 1985, German astronomers Wolfhard Schlosser and Werner Bergmann published an account of an 8th-century Lombardic manuscript, which contains "De cursu stellarum ratio" by St. Gregory of Tours. The Latin text taught readers how to determine the times of nighttime prayers from positions of the stars, and Sirius is described within as "rubeola" — "reddish". The authors proposed this was further evidence Sirius B had been a red giant at the time. However, other scholars replied that it was likely St. Gregory had been referring to Arcturus instead.
The possibility that stellar evolution of either Sirius A or Sirius B could be responsible for this discrepancy has been rejected by astronomers on the grounds that the timescale of thousands of years is too short and that there is no sign of the nebulosity in the system that would be expected had such a change taken place. An interaction with a third star, to date undiscovered, has also been proposed as a possibility for a red appearance. Alternative explanations are either that the description as red is a poetic metaphor for ill fortune, or that the dramatic scintillations of the star when it was observed rising left the viewer with the impression that it was red. To the naked eye, it often appears to be flashing with red, white and blue hues when near the horizon.
Visibility.
With an apparent magnitude of −1.46, Sirius is the brightest star system in the night sky, almost twice the brightness of the second brightest star, Canopus. However, it is not as bright as the Moon, Venus, or Jupiter; at times, Mercury and Mars are also brighter than Sirius. Sirius can be seen from almost everywhere on the Earth's surface, with only observers north of 73 degrees latitude unable to see it, and it does not rise very high when viewed from some northern cities, reaching only 13° above the horizon from Saint Petersburg. Sirius, along with Procyon and Betelgeuse, forms one of the three vertices of the Winter Triangle to observers in the Northern Hemisphere. Due to its declination of roughly −17°, Sirius is a circumpolar star from latitudes south of 73° S. From the Southern Hemisphere in early July, Sirius can be seen in both the evening where it sets after the Sun, and in the morning where it rises before the Sun. Due to precession (and slight proper motion), Sirius will move further south in the future. Starting in the year 9000, Sirius will not be visible any more from northern and central Europe, and in 14000 its declination will be -67° and thus it will be circumpolar throughout South Africa and in most parts of Australia.
Sirius can even be observed in daylight with the naked eye under the right conditions. Ideally, the sky should be very clear, with the observer at a high altitude, the star passing overhead, and the Sun low down on the horizon. These observing conditions are more easily met in the southern hemisphere, due to the southerly declination of Sirius.
The orbital motion of the Sirius binary system brings the two stars to a minimum angular separation of 3 arcseconds and a maximum of 11 arcseconds. At the closest approach, it is an observational challenge to distinguish the white dwarf from its more luminous companion, requiring a telescope with at least 300 mm (12 in) aperture and excellent seeing conditions. A periastron occurred in 1994 and the pair have since been moving apart, making them easier to separate with a telescope.
At a distance of 2.6 parsecs (8.6 ly), the Sirius system contains two of the eight nearest stars to the Solar System (not including the Sun), and is the fifth closest stellar system to ours (again not including the Sun). This proximity is the main reason for its brightness, as with other near stars such as Alpha Centauri and in stark contrast to distant, highly luminous supergiants such as Canopus, Rigel or Betelgeuse. However, it is still around 25 times more luminous than the Sun. The closest large neighbouring star to Sirius is Procyon, 1.61 parsecs (5.24 ly) away. The "Voyager 2" spacecraft, launched in 1977 to study the four Jovian planets in the Solar System, is expected to pass within of Sirius in approximately 296,000 years.
System.
Sirius is a binary star system consisting of two white stars orbiting each other with a separation of about 20 AU (roughly the distance between the Sun and Uranus) and a period of 50.1 years. The brighter component, termed Sirius A, is a main-sequence star of spectral type A1V, with an estimated surface temperature of 9,940 K. Its companion, Sirius B, is a star that has already evolved off the main sequence and become a white dwarf. Currently 10,000 times less luminous in the visual spectrum, Sirius B was once the more massive of the two. The age of the system has been estimated at around 230 million years. Early in its lifespan it was thought to have been two bluish white stars orbiting each other in an elliptical orbit every 9.1 years. The system emits a higher than expected level of infrared radiation, as measured by IRAS space-based observatory. This may be an indication of dust in the system, and is considered somewhat unusual for a binary star. The Chandra X-ray Observatory image shows Sirius B outshining its bright partner as it is a brighter X-ray source.
Sirius A.
Sirius A has a mass of . The radius of this star has been measured by an astronomical interferometer, giving an estimated angular diameter of 5.936±0.016 mas. The projected rotational velocity is a relatively low 16 km/s, which does not produce any significant flattening of its disk. This is at marked variance with the similar-sized Vega, which rotates at a much faster 274 km/s and bulges prominently around its equator. A weak magnetic field has been detected on the surface of Sirius A.
Stellar models suggest that the star formed during the collapsing of a molecular cloud, and that after years, its internal energy generation was derived entirely from nuclear reactions. The core became convective and utilized the CNO cycle for energy generation. It is predicted that Sirius A will have completely exhausted the store of hydrogen at its core within a billion (109) years of its formation. At this point it will pass through a red giant stage, then settle down to become a white dwarf.
Sirius A is classed as an Am star because the spectrum shows deep metallic absorption lines, indicating an enhancement in elements heavier than helium, such as iron. When compared to the Sun, the proportion of iron in the atmosphere of Sirius A relative to hydrogen is given by formula_1, which is equivalent to 100.5, meaning it has 316% of the proportion of iron in the Sun's atmosphere. The high surface content of metallic elements is unlikely to be true of the entire star, rather the iron-peak and heavy metals are radiatively levitated towards the surface.
Sirius B.
With a mass nearly equal to the Sun's, Sirius B is one of the more massive white dwarfs known (); it is almost double the average. Yet that same mass is packed into a volume roughly equal to the Earth's. The current surface temperature is 25,200 K. However, because there is no internal heat source, Sirius B will steadily cool as the remaining heat is radiated into space over a period of more than two billion years.
A white dwarf forms only after the star has evolved from the main sequence and then passed through a red-giant stage. This occurred when Sirius B was less than half its current age, around 120 million years ago. The original star had an estimated and was a B-type star (roughly B4–5) when it still was on the main sequence. While it passed through the red giant stage, Sirius B may have enriched the metallicity of its companion.
This star is primarily composed of a carbon–oxygen mixture that was generated by helium fusion in the progenitor star. This is overlaid by an envelope of lighter elements, with the materials segregated by mass because of the high surface gravity. Hence the outer atmosphere of Sirius B is now almost pure hydrogen—the element with the lowest mass—and no other elements are seen in its spectrum.
Sirius star cluster.
In 1909, Ejnar Hertzsprung was the first to suggest that Sirius was a member of the Ursa Major Moving Group, based on his observations of the system's movements across the sky. The Ursa Major Group is a set of 220 stars that share a common motion through space and were once formed as members of an open cluster, which has since become gravitationally unbound. However, analyses in 2003 and 2005 found Sirius's membership in the group to be questionable: the Ursa Major Group has an estimated age of 500±100 million years, whereas Sirius, with metallicity similar to the Sun's, has an age that is only half this, making it too young to belong to the group. Sirius may instead be a member of the proposed Sirius Supercluster, along with other scattered stars such as Beta Aurigae, Alpha Coronae Borealis, Beta Crateris, Beta Eridani and Beta Serpentis. This is one of three large clusters located within of the Sun. The other two are the Hyades and the Pleiades, and each of these clusters consists of hundreds of stars.
Etymology and cultural significance.
The most commonly used proper name of this star comes from the Latin "Sīrius", from the Ancient Greek "Σείριος" ("Seirios", "glowing" or "scorcher"), although the Greek word itself may have been imported from elsewhere before the Archaic period, one authority suggesting a link with the Egyptian god Osiris. The name's earliest recorded use dates from the 7th century BC in Hesiod's poetic work "Works and Days". Sirius has over 50 other designations and names attached to it. In Geoffrey Chaucer's essay "Treatise on the Astrolabe", it bears the name Alhabor, and is depicted by a hound's head. This name is widely used on medieval astrolabes from Western Europe. In Sanskrit it is known as "Mrgavyadha" "deer hunter", or "Lubdhaka" "hunter". As Mrgavyadha, the star represents Rudra (Shiva). The star is referred as "Makarajyoti" in Malayalam and has religious significance to the pilgrim center Sabarimala. In Scandinavia, the star has been known as "Lokabrenna" ("burning done by Loki", or "Loki's torch"). In the astrology of the Middle Ages, Sirius was a Behenian fixed star, associated with beryl and juniper. Its astrological symbol was listed by Heinrich Cornelius Agrippa.
Many cultures have historically attached special significance to Sirius, particularly in relation to dogs. Indeed, it is often colloquially called the "Dog Star" as the brightest star of Canis Major, the "Great Dog" constellation.
It was classically depicted as Orion's dog. The Ancient Greeks thought that Sirius's emanations could affect dogs adversely, making them behave abnormally during the "dog days," the hottest days of the summer. The Romans knew these days as "dies caniculares", and the star Sirius was called Canicula, "little dog." The excessive panting of dogs in hot weather was thought to place them at risk of desiccation and disease. In extreme cases, a foaming dog might have rabies, which could infect and kill humans whom they had bitten. Homer, in the "Iliad", describes the approach of Achilles toward Troy in these words:
In Iranian mythology, especially in Persian mythology and in Zoroastrianism, the ancient religion of Persia, Sirius appears as "Tishtrya" and is revered as the rain-maker divinity (Tishtar of New Persian poetry). Beside passages in the sacred texts of the Avesta, the Avestan language "Tishtrya" followed by the version "Tir" in Middle and New Persian is also depicted in the Persian epic Shahnameh of Ferdowsi. Due to the concept of the yazatas, powers which are "worthy of worship", Tishtrya is a divinity of rain and fertility and an antagonist of apaosha, the demon of drought. In this struggle, Tishtrya is beautifully depicted as a white horse.
In Chinese astronomy the star is known as the star of the "celestial wolf" ( Chinese romanization: Tiānláng; Japanese romanization: Tenrō;) in the Mansion of Jǐng (井宿). Farther afield, many nations among the indigenous peoples of North America also associated Sirius with canines; the Seri and Tohono O'odham of the southwest note the star as a dog that follows mountain sheep, while the Blackfoot called it "Dog-face". The Cherokee paired Sirius with Antares as a dog-star guardian of either end of the "Path of Souls". The Pawnee of Nebraska had several associations; the Wolf (Skidi) tribe knew it as the "Wolf Star", while other branches knew it as the "Coyote Star". Further north, the Alaskan Inuit of the Bering Strait called it "Moon Dog".
Several cultures also associated the star with a bow and arrows. The ancient Chinese visualized a large bow and arrow across the southern sky, formed by the constellations of Puppis and Canis Major. In this, the arrow tip is pointed at the wolf Sirius. A similar association is depicted at the Temple of Hathor in Dendera, where the goddess Satet has drawn her arrow at Hathor (Sirius). Known as "Tir", the star was portrayed as the arrow itself in later Persian culture.
Sirius is mentioned in "Surah", "An-Najm" ("The Star"), of the Qur'an, where it is given the name الشِّعْرَى (transliteration: "aš-ši‘rā" or "ash-shira"; the leader). The verse is: "وأنَّهُ هُوَ رَبُّ الشِّعْرَى", "That He is the Lord of Sirius (the Mighty Star)." (An-Najm:49) Ibn Kathir said in his commentary "that it is the bright star, named Mirzam Al-Jawza' (Sirius), which a group of Arabs used to worship." The alternate name "Aschere", used by Johann Bayer, is derived from this.
In Theosophy, it is believed the "Seven Stars of the Pleiades" transmit the spiritual energy of the Seven Rays from the "Galactic Logos" to the "Seven Stars of the Great Bear", then to Sirius. From there is it sent via the Sun to the god of Earth (Sanat Kumara), and finally through the seven Masters of the Seven Rays to the human race.
Dogon.
The Dogon people are an ethnic group in Mali, West Africa, reported by some researchers to have traditional astronomical knowledge about Sirius that would normally be considered impossible without the use of telescopes. According to Marcel Griaule's books "Conversations with Ogotemmêli" and "The Pale Fox" they knew about the fifty-year orbital period of Sirius and its companion prior to western astronomers. They also refer to a third star accompanying Sirius A and B. Robert Temple's 1976 book "The Sirius Mystery", credits them with knowledge of the four Galilean moons of Jupiter and the rings of Saturn. This has been the subject of controversy and speculation.
Doubts have been raised about the validity of Griaule and Dieterlein's work. In a 1991 article in Current Anthropology anthropologist Walter van Beek concluded after his research among the Dogon that, "Though they do speak about codice_1 is what Griaule claimed the Dogon called Sirius they disagree completely with each other as to which star is meant; for some it is an invisible star that should rise to announce the codice_1 , for another it is Venus that, through a different position, appears as codice_1. All agree, however, that they learned about the star from Griaule."
Noah Brosch explained in his book "Sirius Matters" that the cultural transfer of relatively modern astronomical information could have taken place in 1893, when a French expedition arrived in Central West Africa to observe the total eclipse on April 16.
Serer religion.
In the religion of the Serer people of Senegal, The Gambia and Mauritania, Sirius is called "Yoonir" from the Serer language (and some of the Cangin language speakers, who are all ethnically Serers). The star Sirius is one of the most important and sacred stars in Serer religious cosmology and symbolism. The Serer high priests and priestesses, (Saltigues, the hereditary "rain priests") chart "Yoonir" in order to forecast rain fall and enable Serer farmers to start planting seeds. In Serer religious cosmology, it is the symbol of the universe.
Modern significance.
Sirius is a frequent subject of science fiction, and has been the subject of poetry. Dante and John Milton reference the star, while Tennyson's poem "The Princess" wonderfully describes the star's scintillation:
Several pop songs reference Sirius directly or using the 'Dog Star' name:
Other modern references:
Vehicles:

</doc>
<doc id="28018" url="https://en.wikipedia.org/wiki?curid=28018" title="Simon Magus">
Simon Magus

Simon the Sorcerer or Simon the Magician, in Latin Simon Magus (Greek Σίμων ὁ μάγος), was a Samaritan magus or religious figure and a convert to Christianity, baptised by Philip the Evangelist, whose later confrontation with Peter is recorded in . The sin of simony, or paying for position and influence in the church, is named after Simon. The "Apostolic Constitutions" also accuses him of lawlessness. According to "Recognitions", Simon's parents were named Antonius and Rachel.
Surviving traditions about Simon appear in orthodox texts, such as those of Irenaeus, Justin Martyr, Hippolytus, and Epiphanius, where he is often regarded as the source of all heresies. Justin wrote that nearly all the Samaritans in his time were adherents of a certain Simon of Gitta, a village not far from Flavia Neapolis. According to Josephus, Gitta (also spelled Getta) was settled by the tribe of Dan. Irenaeus held him as being one of the founders of Gnosticism and the sect of the Simonians. Hippolytus quotes from a work he attributes to Simon or his followers the Simonians, "Apophasis Megale", or "Great Declaration". According to the early church heresiologists, Simon is also supposed to have written several lost treatises, two of which bear the titles "The Four Quarters of the World" and "The Sermons of the Refuter".
In apocryphal works including the "Acts of Peter", Pseudo-Clementines, and the "Epistle of the Apostles", Simon also appears as a formidable sorcerer with the ability to levitate and fly at will.
History.
Acts of the Apostles.
The different sources for information on Simon contain quite different pictures of him, so much so that it has been questioned whether they all refer to the same person. Assuming all references are to the same person, as some (but by no means all) of the Church fathers did, the earliest reference to him is in the canonical Acts of the Apostles; this is his only appearance in the New Testament.
Josephus.
Josephus mentions a magician named Atomus (Simon in Latin manuscripts) as being involved with the procurator Felix, King Agrippa II and his sister Drusilla, where Felix has Simon convince Drusilla to marry him instead of the man she was engaged to. Some scholars have considered the two to be identical, although this is not generally accepted, as the Simon of Josephus is a Jew rather than a Samaritan.
Justin Martyr and Irenaeus.
Justin Martyr (in his "Apologies", and in a lost work against heresies, which Irenaeus used as his main source) and Irenaeus ("Adversus Haereses") record that after being cast out by the Apostles, Simon Magus came to Rome where, having joined to himself a profligate woman of the name of Helen, he gave out that it was he who appeared among the Jews as the Son, in Samaria as the Father and among other nations as the Holy Spirit. He performed such miracles by magic acts during the reign of Claudius that he was regarded as a god and honored with a statue on the island in the Tiber which the two bridges cross, with the inscription "Simoni Deo Sancto", "To Simon the Holy God" ("Apologia, XXVI").
Myth of Simon and Helen.
Justin and Irenaeus are the first to recount the myth of Simon and Helen, which became the center of Simonian doctrine. Epiphanius of Salamis also makes Simon speak in the first person in several places in his "Panarion", and the Implication is that he is quoting from a version of it, though perhaps not verbatim.
In this account of Simon there is a large portion common to almost all forms of Gnostic myths, together with something special to this form. They have in common the place in the work of creation assigned to the female principle, the conception of the Deity; the ignorance of the rulers of this lower world with regard to the Supreme Power; the descent of the female (Sophia) into the lower regions, and her inability to return. Special to the Simonian tale is the identification of Simon himself with the Supreme, and of his consort Helena with the female principle.
Hippolytus.
In Philosophumena Hippolytus provides an extensive quotation of the document called Apophasis Megale or Great Revelation which the author believed to be written by Simon himself. Apart from that he retells the narrative on Simon written by Irenaeus (who in his turn based it on the lost Syntagma of Justin).
Upon the story of "the lost sheep," Hippolytus (in his "Philosophumena") comments as follows.
Also, Hippolytus demonstrates acquaintance with the folk tradition on Simon which depicts him rather as a magician than gnostic and contains multiple stories on his confrontation with Peter (also present in the apocrypha and Pseudo-Clementine literature).
Reduced to despair by the curse laid upon him by Peter in the Acts, Simon soon abjured the faith and embarked on the career of a sorcerer:
Simonians.
Hippolytus gives a much more doctrinally detailed account of Simonianism, including a system of divine emanations and interpretations of the Old Testament, with extensive quotations from the "Apophasis Megale". Some believe that Hippolytus' account is of a later, more developed form of Simonianism, and that the original doctrines of the group were simpler, close to the account given by Justin Martyr and Irenaeus (this account however is also included in Hippolytus' work).
Hippolytus says the free love doctrine was held by them in its purest form, and speaks in language similar to that of Irenaeus about the variety of magic arts practiced by the Simonians, and also of their having images of Simon and Helen under the forms of Zeus and Athena. But he also adds, "if any one, on seeing the images either of Simon or Helen, shall call them by those names, he is cast out, as showing ignorance of the mysteries."
Epiphanius.
Epiphanius writes that there were some Simonians still in existence in his day (c. AD 367), but he speaks of them as almost extinct. Gitta, he says, had sunk from a town into a village. Epiphanius further charges Simon with having tried to wrest the words of St. Paul about the armour of God (Ephesians 6:14–16) into agreement with his own identification of the "Ennoia" with Athena. He tells us also that he gave barbaric names to the "principalities and powers," and that he was the beginning of the Gnostics. The Law, according to him, was not of God, but of "the sinister power." The same was the case with the prophets, and it was death to believe in the Old Testament.
Cyril of Jerusalem.
Cyril of Jerusalem (346 AD) in the sixth of his Catechetical Lectures prefaces his history of the Manichaeans by a brief account of earlier heresies: Simon Magus, he says, had given out that he was going to be translated to heaven, and was actually careening through the air in a chariot drawn by demons when Peter and Paul knelt down and prayed, and their prayers brought him to earth a mangled corpse.
Apocrypha.
"Acts of Peter".
The apocryphal "Acts of Peter" gives a more elaborate tale of Simon Magus' death. Simon is performing magic in the Forum, and in order to prove himself to be a god, he levitates up into the air above the Forum. The apostle Peter prays to God to stop his flying, and he stops mid-air and falls into a place called the "Sacra Via" (meaning, Holy Way), breaking his legs "in three parts". The previously non-hostile crowd then stones him. Now gravely injured, he had some people carry him on a bed at night from Rome to Ariccia, and was brought from there to Terracina to a person named Castor, who on accusations of sorcery was banished from Rome. The Acts then continue to say that he died "while being sorely cut by two physicians".
"Acts of Peter and Paul".
Another apocryphal document, the "Acts of Peter and Paul" gives a slightly different version of the above incident, which was shown in the context of a debate in front of the Emperor Nero. In this version, Paul the Apostle is present along with Peter, Simon levitates from a high wooden tower made upon his request, and dies "divided into four parts" due to the fall. Peter and Paul were then put in prison by Nero while ordering Simon's body be kept carefully for three days (thinking he would rise again).
Pseudo-Clementine literature.
The Pseudo-Clementine "Recognitions" and "Homilies" give an account of Simon Magus and some of his teachings in regards to the Simonians. They are of uncertain date and authorship, and seem to have been worked over by several hands in the interest of diverse forms of belief.
Simon was a Samaritan, and a native of Gitta. The name of his father was Antonius, that of his mother Rachel. He studied Greek literature in Alexandria, and, having in addition to this great power in magic, became so ambitious that he wished to be considered a highest power, higher even than the God who created the world. And sometimes he "darkly hinted" that he himself was Christ, calling himself the Standing One. Which name he used to indicate that he would stand for ever, and had no cause in him for bodily decay. He did not believe that the God who created the world was the highest, nor that the dead would rise. He denied Jerusalem, and introduced Mount Gerizim in its stead. In place of the Christ of the Christians he proclaimed himself; and the Law he allegorized in accordance with his own preconceptions. He did indeed preach righteousness and judgment to come.
There was one John the Baptist, who was the forerunner of Jesus in accordance with the law of parity; and as Jesus had twelve Apostles, bearing the number of the twelve solar months, so had he thirty leading men, making up the monthly tale of the moon. One of these thirty leading men was a woman called Helen, and the first and most esteemed by John was Simon. But on the death of John he was away in Egypt for the practice of magic, and one Dositheus, by spreading a false report of Simon's death, succeeded in installing himself as head of the sect. Simon on coming back thought it better to dissemble, and, pretending friendship for Dositheus, accepted the second place. Soon, however, he began to hint to the thirty that Dositheus was not as well acquainted as he might be with the doctrines of the school.
The encounter between both Dositheus and Simon Magus was the beginnings of the sect of Simonians. The narrative goes on to say that Simon, having fallen in love with Helen, took her about with him, saying that she had come down into the world from the highest heavens, and was his mistress, inasmuch as she was Sophia, the Mother of All. It was for her sake, he said, that the Greeks and Barbarians fought the Trojan War, deluding themselves with an image of truth, for the real being was then present with the First God. By such allegories Simon deceived many, while at the same time he astounded them by his magic. A description is given of how he made a familiar spirit for himself by conjuring the soul out of a boy and keeping his image in his bedroom, and many instances of his feats of magic are given.
"Simon Magus" as a cipher.
Anti-Paulinism.
The Pseudo-Clementine writings were used in the 4th century by members of the Ebionite sect, one characteristic of which was hostility to Paul, whom they refused to recognize as an apostle. Ferdinand Christian Baur (1792–1860), founder of the Tübingen School, drew attention to the anti-Pauline characteristic in the Pseudo-Clementines, and pointed out that in the disputations between Simon and Peter, some of the claims Simon is represented as making (e.g. that of having seen the Lord, though not in his lifetime, yet subsequently in vision) were really the claims of Paul; and urged that Peter's refutation of Simon was in some places intended as a polemic against Paul. The enmity between Peter and Simon is clearly shown. Simon's magical powers are juxtaposed with Peter's powers in order to express Peter's authority over Simon through the power of prayer, and in the the identification of Paul with Simon Magus is effected. Simon is there made to maintain that he has a better knowledge of the mind of Jesus than the disciples, who had seen and conversed with Jesus in person. His reason for this strange assertion is that visions are superior to waking reality, as divine is superior to human. Peter has much to say in reply to this, but the passage which mainly concerns us is as follows:
The anti-Pauline context of the Pseudo-Clementines is recognised, but the association with Simon Magus is surprising since they have little in common. However the majority of scholars accept Baur's identification, though others, including Lightfoot, argued extensively that the "Simon Magus" of the Pseudo-Clementines was not meant to stand for Paul. Recently, Berlin pastor Hermann Detering (1995) has made the case that the veiled anti-Pauline stance of the Pseudo-Clementines has historical roots, that the Acts 8 encounter between Simon the magician and Peter is itself based on the conflict between Peter and Paul. Detering's belief has not found general support among scholars, but Robert M. Price argues much the same case in "The Amazing Colossal Apostle:The Search for the Historical Paul" (2012).
Anti-Marcionism.
There are other features in the portrait which remind us strongly of Marcion. For the first thing which we learn from the "Homilies" about Simon's opinions is that he denied that God was just. By "God" he meant the creator god. But he undertakes to prove from the jewish scriptures that there is a higher god, who really possesses the perfections which are falsely ascribed to the lower god. On these grounds Peter complains that, when he was setting out for the gentiles to convert them from their worship of "many gods upon earth", Satan had sent Simon before him to make them believe that there were "many gods in heaven".
Medieval legends, later interpretations.
The church of Santa Francesca Romana, Rome, is claimed to have been built on the spot where Simon fell. Within the Church is a dented slab of marble that purports to bear the imprints of the knees of Peter and Paul during their prayer. The fantastic stories of Simon the Sorcerer persisted into the later Middle Ages, becoming a possible inspiration for the "Faustbuch" and Goethe's Faust.
The opening story in Danilo Kiš's 1983 collection "The Encyclopedia of the Dead", "Simon Magus", retells the confrontation between Simon and Peter agreeing with the account in the "Acts of Peter", and provides an additional alternative ending in which Simon asks to be buried alive in order to be resurrected three days later (after which his body is found putrefied).
Bibliography.
Attribution

</doc>
<doc id="28020" url="https://en.wikipedia.org/wiki?curid=28020" title="September 10">
September 10


</doc>
<doc id="28021" url="https://en.wikipedia.org/wiki?curid=28021" title="September 12">
September 12


</doc>
<doc id="28022" url="https://en.wikipedia.org/wiki?curid=28022" title="School">
School

A school is an institution designed to provide learning spaces and learning environments for the teaching of students (or "pupils") under the direction of teachers. Most countries have systems of formal education, which is commonly compulsory. In these systems, students progress through a series of schools. The names for these schools vary by country (discussed in the "Regional" section below) but generally include primary school for young children and secondary school for teenagers who have completed primary education. An institution where higher education is taught, is commonly called a university college or university.
In addition to these core schools, students in a given country may also attend schools before and after primary and secondary education. Kindergarten or pre-school provide some schooling to very young children (typically ages 3–5). University, vocational school, college or seminary may be available after secondary school. A school may also be dedicated to one particular field, such as a school of economics or a school of dance. Alternative schools may provide nontraditional curriculum and methods.
There are also non-government schools, called private schools. Private schools may be required when the government does not supply adequate, or special education. Other private schools can also be religious, such as Christian schools, hawzas, yeshivas, and others; or schools that have a higher standard of education or seek to foster other personal achievements. Schools for adults include institutions of corporate training, Military education and training and business schools.
In homeschooling and online schools, teaching and learning take place outside of a traditional school building. Schools are commonly organized in several different organizational models, including departmental, small learning communities, academies, integrated, and schools-within-a-school. 
Etymology.
The word "school" derives from Greek " ("), originally meaning "leisure" and also "that in which leisure is employed", but later "a group to whom lectures were given, school".
History and development.
The concept of grouping students together in a centralized location for learning has existed since Classical antiquity. Formal schools have existed at least since ancient Greece (see Academy), ancient Rome (see Education in Ancient Rome) ancient India (see Gurukul), and ancient China (see History of education in China). The Byzantine Empire had an established schooling system beginning at the primary level. According to "Traditions and Encounters", the founding of the primary education system began in 425 AD and "... military personnel usually had at least a primary education ...". The sometimes efficient and often large government of the Empire meant that educated citizens were a must. Although Byzantium lost much of the grandeur of Roman culture and extravagance in the process of surviving, the Empire emphasized efficiency in its war manuals. The Byzantine education system continued until the empire's collapse in 1453 AD.
Islam was another culture that developed a school system in the modern sense of the word. Emphasis was put on knowledge, which required a systematic way of teaching and spreading knowledge, and purpose-built structures. At first, mosques combined both religious performance and learning activities, but by the 9th century, the Madrassa was introduced, a proper school that was built independently from the mosque. They were also the first to make the "Madrassa" system a public domain under the control of the Caliph. The Nizamiyya madrasa is considered by consensus of scholars to be the earliest surviving school, built towards 1066 AD by Emir Nizam Al-Mulk.
Under the Ottomans, the towns of Bursa and Edirne became the main centers of learning. The Ottoman system of Külliye, a building complex containing a mosque, a hospital, madrassa, and public kitchen and dining areas, revolutionized the education system, making learning accessible to a wider public through its free meals, health care and sometimes free accommodation.
The 19th century historian, Scott holds that a remarkable correspondence exists between the procedure established by those institutions and the methods of the present day. They had their collegiate courses, their prizes for proficiency in scholarship, their oratorical and poetical contests, their commencements and their degrees. In the department of medicine, a severe and prolonged examination, conducted by the most eminent physicians of the capital, was exacted of all candidates desirous of practicing their profession, and such as were unable to stand the test were formally pronounced incompetent. 
In Europe, universities emerged during the 12th century; here, scholasticism was an important tool, and the academicians were called "schoolmen". During the Middle Ages and much of the Early Modern period, the main purpose of schools (as opposed to universities) was to teach the Latin language. This led to the term grammar school, which in the United States informally refers to a primary school, but in the United Kingdom means a school that selects entrants based on ability or aptitude. Following this, the school curriculum has gradually broadened to include literacy in the vernacular language as well as technical, artistic, scientific and practical subjects.
Obligatory school attendance became common in parts of Europe during the 18th century. In Denmark-Norway, this was introduced as early as in 1739-1741, the primary end being to increase the literacy of the "almue", i.e. the "regular people". Many of the earlier public schools in the United States and elsewhere were one-room schools where a single teacher taught seven grades of boys and girls in the same classroom. Beginning in the 1920s, one-room schools were consolidated into multiple classroom facilities with transportation increasingly provided by kid hacks and school buses.
Regional terms.
The use of the term "school" varies by country, as do the names of the various levels of education within the country.
United Kingdom and Commonwealth of Nations.
In the United Kingdom, the term "school" refers primarily to pre-university institutions, and these can, for the most part, be divided into pre-schools or nursery schools, primary schools (sometimes further divided into infant school and junior school), and secondary schools. Various types of secondary schools in England and Wales include grammar schools, comprehensives, secondary moderns, and city academies. In Scotland, while they may have different names, all Secondary schools are the same, except in that they may be funded by the state, or independently funded (see next paragraph). It is unclear if "Academies", which are a hybrid between state and independently funded/controlled schools and have been introduced to England in recent years, will ever be introduced to Scotland. School performance in Scotland is monitored by Her Majesty's Inspectorate of Education. Ofsted reports on performance in England and Estyn reports on performance in Wales.
In the United Kingdom, most schools are publicly funded and known as state schools or maintained schools in which tuition is provided free. There are also private schools or independent schools that charge fees. Some of the most selective and expensive private schools are known as public schools, a usage that can be confusing to speakers of North American English. In North American usage, a public school is one that is publicly funded or run.
In much of the Commonwealth of Nations, including Australia, New Zealand, India, Pakistan, Bangladesh, Sri Lanka, South Africa, Kenya, and Tanzania, the term "school" refers primarily to pre-university institutions.
India.
In ancient India, schools were in the form of Gurukuls. Gurukuls were traditional Hindu residential schools of learning; typically the teacher's house or a monastery. During the Mughal rule, Madrasahs were introduced in India to educate the children of Muslim parents. British records show that indigenous education was widespread in the 18th century, with a school for every temple, mosque or village in most regions of the country. The subjects taught included Reading, Writing, Arithmetic, Theology, Law, Astronomy, Metaphysics, Ethics, Medical Science and Religion.
Under the British rule in India, Christian missionaries from England, USA and other countries established missionary and boarding schools throughout the country. Later as these schools gained in popularity, more were started and some gained prestige. These schools marked the beginning of modern schooling in India and the syllabus and calendar they followed became the benchmark for schools in modern India. Today most of the schools follow the missionary school model in terms of tutoring, subject / syllabus, governance etc.with minor changes. Schools in India range from schools with large campuses with thousands of students and hefty fees to schools where children are taught under a tree with a small / no campus and are totally free of cost. There are various boards of schools in India, namely Central Board for Secondary Education (CBSE), Council for the Indian School Certificate Examinations (CISCE), Madrasa Boards of various states, Matriculation Boards of various states, State Boards of various boards, Anglo Indian Board, and so on. The typical syllabus today includes Language(s), Mathematics, Science — Physics, Chemistry, Biology, Geography, History, General Knowledge, Information Technology / Computer Science etc.. Extra curricular activities include physical education / sports and cultural activities like music, choreography, painting, theater / drama etc.
Europe.
In much of continental Europe, the term "school" usually applies to primary education, with primary schools that last between four and nine years, depending on the country. It also applies to secondary education, with secondary schools often divided between "Gymnasiums" and vocational schools, which again depending on country and type of school educate students for between three and six years. In Germany students graduating from Grundschule are not allowed to directly progress into a vocational school, but are supposed to proceed to one of Germany's general education schools such as Gesamtschule, Hauptschule, Realschule or Gymnasium. When they leave that school, which usually happens at age 15-19 they are allowed to proceed to a vocational school. The term school is rarely used for tertiary education, except for some "upper" or "high" schools (German: Hochschule), which describe colleges and universities.
In Eastern Europe modern schools (after World War II), of both primary and secondary educations, often are combined, while secondary education might be split into accomplished or not. The schools are classified as middle schools of general education and for the technical purposes include "degrees" of the education they provide out of three available: the first — primary, the second — unaccomplished secondary, and the third — accomplished secondary. Usually the first two degrees of education (eight years) are always included, while the last one (two years) gives option for the students to pursue vocational or specialized educations.
North America and the United States.
In North America, the term "school" can refer to any educational institution at any level, and covers all of the following: preschool (for toddlers), kindergarten, elementary school, middle school (also called intermediate school or junior high school, depending on specific age groups and geographic region), senior high school, college, university, and graduate school.
In the US, school performance through high school is monitored by each state's Department of Education. Charter schools are publicly funded elementary or secondary schools that have been freed from some of the rules, regulations, and statutes that apply to other public schools. The terms grammar school and "grade school" are sometimes used to refer to a primary school.
Ownership and operation.
Many schools are owned or funded by states. Private schools operate independently from the government. Private schools usually rely on fees from families whose children attend the school for funding; however, sometimes such schools also receive government support (for example, through School vouchers). Many private schools are affiliated with a particular religion; these are known as parochial schools.
Starting a school.
The Toronto District School Board is an example of a school board that allows parents to design and propose new schools.
When designing a school, factors that need to be decided include:
Components of most schools.
Schools are organized spaces purposed for teaching and learning. The classrooms, where teachers teach and students learn, are of central importance. Classrooms may be specialized for certain subjects, such as laboratory classrooms for science education and workshops for industrial arts education.
Typical schools have many other rooms and areas, which may include:
Security.
The safety of staff and students is increasingly becoming an issue for school communities, an issue most schools are addressing through improved security. Some have also taken measures such as installing metal detectors or video surveillance. Others have even taken measures such as having the children swipe identification cards as they board the school bus. For some schools, these plans have included the use of door numbering to aid public safety response.
Other security concerns faced by schools include bomb threats, gangs, vandalism, and bullying.
Health services.
School health services are services from medical, teaching and other professionals applied in or out of school to improve the health and well-being of children and in some cases whole families. These services have been developed in different ways around the globe but the fundamentals are constant: the early detection, correction, prevention or amelioration of disease, disability and abuse from which school aged children can suffer.
Online schools and classes.
Some schools offer remote access to their classes over the Internet. Online schools also can provide support to traditional schools, as in the case of the School Net Namibia.
Some online classes also provide experience in a class, so that when people take them, they have already been introduced to the subject and know what to expect, and even more classes provide High School/College credit allowing people to take the classes at their own pace. Many online classes cost money to take but some are offered free.
Internet-based distance learning programs are offered widely through many universities. Instructors teach through online activities and assignments. Online classes are taught the same as physically being in class with the same curriculum. The instructor offers the syllabus with their fixed requirements like any other class. Students can virtually turn their assignments in to their instructors according to deadlines. This being through via email or in the course webpage. This allowing students to work at their own pace, yet meeting the correct deadline. Students taking an online class have more flexibility in their schedules to take their classes at a time that works best for them. Conflicts with taking an online class may include not being face to face with the instructor when learning or being in an environment with other students. Online classes can also make understanding the content difficult, especially when not able to get in quick contact with the instructor. Online students do have the advantage of using other online sources with assignments or exams for that specific class. Online classes also have the advantage of students not needing to leave their house for a morning class or worrying about their attendance for that class. Students can work at their own pace to learn and achieve within that curriculum.
The convenience of learning at home has been a major attractive point for enrolling online. Students can attend class anywhere a computer can go—at home, a library or while traveling internationally. Online school classes are designed to fit your needs, while allowing you to continue working and tending to your other obligations. Online school education is divided into three subcategories: Online Elementary School, Online Middle School, Online High school.
Stress.
As a profession, teaching has levels of work-related stress (WRS) that are among the highest of any profession in some countries, such as the United Kingdom and the United States. The degree of this problem is becoming increasingly recognized and support systems are being put into place. Teacher education increasingly recognizes the need to train those new to the profession to be aware of and overcome mental health challenges they may face.
Stress sometimes affects students more severely than teachers, up to the point where the students are prescribed stress medication. This stress is claimed to be related to standardized testing, and the pressure on students to score above average. "See Cram school".
Discipline.
Schools and their teachers have always been under pressure — for instance, pressure to cover the curriculum, to perform well in comparison to other schools, and to avoid the stigma of being "soft" or "spoiling" toward students. Forms of discipline, such as control over when students may speak, and normalized behaviour, such as raising a hand to speak, are imposed in the name of greater efficiency. Practitioners of critical pedagogy maintain that such disciplinary measures have no positive effect on student learning. Indeed, some argue that disciplinary practices detract from learning, saying that they undermine students' individual dignity and sense of self-worth—the latter occupying a more primary role in students' hierarchy of needs.

</doc>
<doc id="28024" url="https://en.wikipedia.org/wiki?curid=28024" title="Sontaran">
Sontaran

The Sontarans are a fictional extraterrestrial agendered race of humanoids from the British science fiction television series "Doctor Who", and also seen in spin-off series "The Sarah Jane Adventures". A warrior race who live to kill, they are characterised by their ruthlessness and fearlessness of death.
They were created by writer Robert Holmes. During rehearsals for their first appearance, actor Kevin Lindsay, who portrayed the original Sontaran, Linx, pronounced the race's name as ""son-TAR-an"." Alan Bromly, the director, tried to correct him by saying it should be pronounced with the stress on the first syllable. Lindsay declared "Well, I think it's ""son-TAR-an"", and since I'm from the place, I should know." His preferred pronunciation was retained.
Culture.
The Sontarans are a race of humanoids with a stocky build, greenish brown skin, a distinctive dome-shaped head, and they have only three fingers on each hand, though some members of their species do have five fingers. Their musculature is designed for load-bearing rather than leverage, because of the significant amount of gravity on their home planet. Ross Jenkins in "The Sontaran Stratagem" describes a Sontaran as resembling "a talking baked potato". Sontarans come from a large, dense planet named Sontar in the "southern spiral arm of the galaxy" which has a very strong gravitational field, which explains their compact stocky form. They are far stronger than humans, and in the recent series are shorter than the average human male.
The Sontarans have an extremely militaristic culture which prizes discipline and honor as its highest virtues; every aspect of their draconian society is geared toward warfare, and every experience is viewed in terms of its martial relevance. In "The Sontaran Experiment", the Fourth Doctor comments that "Sontarans never do anything without a military reason." In fact, to die heroically in battle is their ultimate goal. Aside from a ritualistic chant in "The Sontaran Strategem"/"The Poison Sky", they are never seen to engage in any activity that would be considered recreation, though a few offhand comments by Commander Skorr in "The Poison Sky" suggest they do consider hunting a sport. According to their creator Robert Holmes, Sontarans do have a highly developed artistic culture, but have put it on hold for the duration of the war, while the opening chapter of the novelisation of "The Time Warrior", based on Holmes' incomplete draft, refers to Linx listening to the Sontaran anthem while his spaceship is in flight.
The Sontarans depicted in the series have detached, smug personalities, and a highly developed sense of honour; on multiple occasions, the Doctor has used his knowledge of their pride in their species to manipulate them. In "The Sontaran Stratagem", the Doctor nevertheless referred to them as "the finest soldiers in the galaxy".
Although physically formidable, the Sontarans' weak spot is the "probic vent" at the back of their neck, through which they draw nutrition. It is also part of their cloning process. It provides incentive to continue moving forward in battle since retreat would expose this area to their enemies. They have been killed by targeting that location with a knife ("The Invasion of Time"), a screwdriver (""), and an arrow ("The Time Warrior"). Even something as simple as a squash ball aimed at that point ("The Sontaran Stratagem") or contact by the heel of a shoe ("The Last Sontaran") is capable of incapacitating them temporarily. They are also vulnerable to "coronic acid" ("The Two Doctors"). While the Sontaran wear protective helmets in battle, to fight without their helmets, or to be "open-skinned," is an honour for the Sontaran.
In the episode "The Poison Sky", it is revealed that the Sontaran Empire have been at war with the Rutan Host for more than 50,000 years, and which, at a time around 2008, they are losing. The war is still raging at least 20,000 years later, in the serial "The Sontaran Experiment".
Most of the Sontarans depicted in the television series have had monosyllabic names, many beginning with an initial 'st' sound (e.g. Styre ("The Sontaran Experiment"), Stor ("The Invasion of Time"), Stike ("The Two Doctors"), Staal ("The Sontaran Stratagem"), Skorr ("The Sontaran Stratagem"), Stark ("The Pandorica Opens"), and Strax ("A Good Man Goes To War"); exceptions are Linx ("The Time Warrior"), Varl ("The Two Doctors"), Jask ("The End of Time"), and Kaagh ("The Sarah Jane Adventures")). Elements of the Sontaran military structure mentioned in the series include the "Sontaran G3 Military Assessment Survey" and the "Grand Strategic Council", the Ninth Sontaran Battle Group, the "Fifth Army Space Fleet of the Sontaran Army Space Corps", and the "Tenth Sontaran Battle Fleet". Military titles include Commander, Group Marshal, Field Major, and General. Agnomens include "the Undefeated", "the Bloodbringer", "the Avenger" and "the Slayer".
Reproduction and gender identity.
The Sontarans are a monogender-asexual (a "male gender-only" species); they reproduce by means of cloning rather than sexual reproduction, and thus for the most part are extremely similar in appearance. Human characters in both "The Sontaran Experiment" and "The Sontaran Stratagem" comment on how closely individual Sontarans resemble one another; however, it should be noted that their height, skin tone, facial features, vocal timbre and accent, hair, spacing of teeth and even number of fingers have varied from story to story, and sometimes within stories. When Luke Rattigan asks how they can tell each other apart in "The Sontaran Stratagem", General Staal remarks that they say the same of humans.
In "The Time Warrior", Linx states that "at the Sontaran Military Academy we have hatchings of a million cadets at each muster parade." The Doctor also comments in "The Invasion of Time" that Sontarans can mass-clone themselves at rates up to a million embryos every four minutes. Thereafter the clones take just ten minutes to grow to adulthood. When the Sontaran reach adulthood, under the charge of Sontaran High Command, each warrior is immediately given a rank and dispatched on a battle mission. From day one, the Sontarans are sent to battle.
Sontarans reproduce asexually and all the Sontarans depicted in the television series are of one gender; referred to with masculine pronouns, however it is not known if they possess distinctly male physiologies. General Staal comments that "words are the weapons of womenfolk" and that the clone of Martha Jones performed well "for a female" as commentary on the gender inequalities of other species. This typifies a Sontaran trait: interested only in the strongest fighters in any group or race. Despite this, Strax appeared perfectly comfortable with the prospect of wearing dresses in "The Battle of Demon's Run - Two Days Later"; he ultimately dressed in human gentleman's attire, nevertheless. In "The Time Warrior", when Linx examines Sarah Jane, he comments on how the human reproduction system is 'inefficient' and that humans 'should change it'. As multiple genders are foreign to them, Sontarans are known to confuse the human male and female sexes; Strax routinely addresses young women as "Boy" and vice versa. and claims not to have known that River Song was a woman.
In "The Sontaran Stratagem", the Sontarans are seen to create human clones by growing them in tubs of green fluid. "Enemy of the Bane" confirms that Sontarans are cloned in the same way. In a human clone, the umbilical corresponds to the probic vent on the back of a Sontaran's neck, suggesting that the vent is not unlike the human navel, albeit clearly more complex.
Technology.
The Tenth Sontaran Battle Fleet in the new series consists of a Command Ship and a number of capsules that can be moved into position when Battle Status is enjoined. Sontaran ships are impervious to nuclear missiles. In both the classic and new series, Sontarans are depicted using spherical or semi-spherical single-occupant spacecraft known as capsules. Each capsule is small enough to avoid detection by radar and is piloted by an individual Sontaran. "The Sontaran Stratagem" also saw the introduction of a large mothership from which the small Sontaran capsules could be seen to originate. The Doctor notes that the one ship by itself is enough to completely wipe out Earth.
The Sontarans have a variety of weapons. Their trade-mark weapon is a small rod with two handles and a plunger at one end, giving it a syringe style. This is so it can be held and fired using three fingers. This weapon fires a disabling beam that can temporarily render a person useless and emits an energy pulse that can repair systems like the teleport, and has appeared in every Sontaran story except "The Sontaran Experiment". When first used by Commander Linx in "The Time Warrior", it shows the ability to fire a beam which can disarm by knocking the weapon out of the wielder's hand, hypnotise, as well as cutting through wood, disabling limbs and killing.
In "The Sontaran Experiment", Field Major Styre instead used a small red laser pistol which only killed (although it did not kill the Doctor, because of a small metal plate the Doctor had been keeping in his inside pocket). "The Invasion of Time" saw Commander Stor using the small rod again, but also in episode six, a Sontaran trooper uses a short black rifle-like laser to try to burn through a lock on a door inside the TARDIS. "The Two Doctors" introduced a weapon called the Meson Gun (as named in the Jim'll Fix It Sketch, "A Fix with Sontarans"), a large silver rifle with a red fuel tank in the centre which was used by Group Marshal Stike and Varl in the third episode. It seemed to be some kind of flame-thrower as it fired a jet of flames very briefly. Group Marshal Stike was also seen carrying a baton.
It would not be until "The Sontaran Stratagem" that General Staal would show that the baton can fire an orange beam that could stun the target. In "The Poison Sky", Commander Skorr and his troops carry large laser rifles into battle. These rifles are the Sontaran gun of the Tenth Sontaran Battle Fleet. Each rifle has a laser beam that kills instantly and is designed for a three-fingered grip. In "The Invasion of Time", their armour is shown to be resistant to Time Lord stasers and K-9's blaster. However, their armour is vulnerable to standard human firearms in "The Poison Sky", but the Sontarans in that episode used a 'cordolane signal' which caused the copper-lined bullets to expand, jamming most firearms instantly. UNIT troops overcame this by switching to steel-lined bullets.
"The Sarah Jane Adventures" story "The Last Sontaran" showed further technological advancements of the modern Sontarans. Commander Kaagh, a surviving pilot from the tenth Sontaran battle fleet, had slightly different armour due to being from the special forces. His suit featured no gloves, so his bare hands were visible. And on his left arm was a control panel for his suit and ship. His helmet could fold up and retract and both his suit and ship featured cloaking devices, turning them both invisible. While the soldiers of the tenth fleet were armed with large laser rifles, Kaagh has a smaller laser carbine. Rather than hypnotising humans (as Sarah pointed out they usually do), instead, Kaagh fixed neural control devices to the back of the necks of his human agents. A red light flashes when it is operational, and Kaagh can activate and deactivate them when he wants with his control panel.
A pair of Sontarans that tried to invade Trenzalore in "Time of the Doctor" used a two-man craft with an invisibility field.
Appearances.
Television.
The Sontarans made their first appearance in 1973 in the serial "The Time Warrior" by Robert Holmes, where a Sontaran named Linx is stranded in the Middle Ages. Linx uses a projector to bring back human scientists from the future to fix his spacecraft.
Another Sontaran named Styre appears in "The Sontaran Experiment" (1975), experimenting on captured astronauts on a far future Earth. Their third appearance is in "The Invasion of Time", where they successfully invade Gallifrey, but are driven out again after less than a day. They appeared for the final time in the original series in "The Two Doctors". The Sontarans also appeared in a skistreat for the BBC children's programme "Jim'll Fix It" titled "A Fix with Sontarans", along with Colin Baker as the Sixth Doctor and Janet Fielding as Tegan Jovanka. References are made in Sontaran episodes to the Rutan Host, an equally militaristic race with whom the Sontarans have been at war for thousands of years though the Rutans were not shown until the 1977 serial "Horror of Fang Rock".
Sporting an updated design, Sontarans returned to the revived series in the series 4 (2008) episodes "The Sontaran Strategem" and "The Poison Sky". The Sontarans plan to terraform the Earth into a new clone world, but their plans are averted by the Tenth Doctor (David Tennant). It is also revealed that the race was excluded from the Time War of the revived series' backstory. In "Turn Left", the same events are depicted in a parallel universe, where through exposition describes their plan as foiled by Torchwood (characters from the spin-off show of that name), at the cost of their lives, with Torchwood leader Jack Harkness being captured by the Sontarans. In "The Stolen Earth", UNIT is revealed to have developed a teleportation device based on Sontaran technology. A lone survivor from the events of "The Poison Sky", Kaagh (Anthony O'Donnell), next appears in "The Last Sontaran", from spin-off series "The Sarah Jane Adventures". Kaagh appears again in "Enemy of the Bane". In "Doctor Who"s "The End of Time, Part Two" (2010), a Sontaran sniper (Dan Starkey) briefly appears pursuing the Doctor's former companions Mickey Smith (Noel Clarke) and his wife Martha Jones (Freema Agyeman), but is defeated by the Doctor before he can assassinate them. Alongside the Eleventh Doctor (Matt Smith), Sontarans battle fleets are seen in series five (2010) finale episode "The Pandorica Opens", as part of an alliance of the Doctor's enemies. Series 6 episode "A Good Man Goes to War" (2011) introduces Strax (Starkey), a Sontaran nurse who has been assigned this role as a means of making penance. He fights on the side of the Doctor and his allies, which include the Silurian warrior Vastra (Neve McIntosh) and her lover Jenny (Catrin Stewart). Strax then appears alongside Vastra and Jenny in "The Snowmen" (2012), "The Crimson Horror" and "The Name of the Doctor" (both 2013), and "Deep Breath" (2014). A troop of Sontarans is also shown among Trenzalore's invaders in the 2013 Christmas special "The Time of the Doctor".
Games.
The origins of the Sontarans have not been revealed in the television series. The "Doctor Who" role-playing game published by FASA claimed that they were all descended from the genetic stock of General Sontar (or Sontaris), who used newly developed bioengineering techniques to clone millions of duplicates of himself and annihilated the non-clone population. He renamed the race after himself and turned the Sontarans into an expansionist and warlike society set on universal conquest. However, this origin has no basis in anything seen in the television series.
The Sontarans have also appeared as a character in the PC game "Destiny of the Doctors" released on 5 December 1997, by BBC Multimedia. They can be defeated by firing the occupants of an angry beehive at them.
The Sontarans appear in the "" episode, "The Gunpowder Plot".
Other appearances.
Big Finish Productions first used the Sontarans for their audio drama "Heroes of Sontar" - a 2011 Fifth Doctor story. They next featured in The Five Companions and were stuck in an alternative version of the Death Zone with the Fifth Doctor and various companions. In 2012, "The First Sontarans" was released. A Sixth Doctor Lost Story from the mid-1980s, written by Andrew Smith, it features the Sontarans and the Rutans on nineteenth century Earth, tracking down a scientist named Jacob, who escaped through time and space. It is revealed that Jacob is from Sontar, and was responsible for genetically creating the Sontarans as a defence against a Rutan invasion. They were first developed on Sontar's gravity-heavy moon and quickly proved themselves to be at least on par with the unstoppable Rutan horde. However, believing themselves to be superior, the Sontarans turned on their creators, conquering the planet Sontar and changing it to suit their biology.
Other appearances by the Sontarans include the spin-off videos "Mindgame", "" and "Do You Have A License To Save This Planet?"; three audio plays by BBV: "Silent Warrior", "Old Soldiers" and "Conduct Unbecoming"; the Faction Paradox audio "The Shadow Play"; and a cameo appearance in "Infidel's Comet". "Shakedown" marks the only occasion in which the Sontarans and their Rutan foes appear on screen together, and was adapted into a Virgin New Adventures novel.
They have also appeared in several spin-off novels, including "Lords of the Storm" by David A. McIntee and "The Infinity Doctors" by Lance Parkin. In "The Infinity Doctors", the Doctor negotiated a peace between the Sontarans and the Rutan Host when two of them were left trapped in a TARDIS for several hours and got to talking due to their inability to kill each other. General Sontar also made an appearance in that novel. In "The Crystal Bucephalus" by Craig Hinton, the name of their planet was given as Sontara. The Sontarans also briefly appear in "The Eight Doctors", sent to the Eye of Orion by an agent of the Celestial Intervention Agency to kill the Fifth and Eighth Doctors.
In 1982, Jean Airey's novella "The Doctor and the Enterprise" featured a crossover between the universes of "Doctor Who" and "Star Trek", in which the Fourth Doctor finds himself on the USS "Enterprise". The "Enterprise" is attacked by a Sontaran fleet (which is unrecognizable to Captain Kirk and crew), prompting the Doctor to urgently warn the crew to flee the area.
They appear in 2009, in the novella "The Sontaran Games" by Jacqueline Rayner, featuring the Tenth Doctor and appeared in the New Series Adventures (Doctor Who) book "The Taking of Chelsea 426" by David Llewellyn, featuring the Tenth Doctor, fighting both times against the Rutan Host.
In 2008, as part of Character options first series 4 2008 wave of action figures, they released some Sontaran action figures. These include General Staal, Commander Skorr and several Sontaran soldiers.
The Sontarans are mentioned in the audio book Wraith World, when Clyde Langer remarks he cannot understand why Luke and Rani would want to read about made up adventures, when they have faced Sontarans.
Comic books.
The Sontarans have also appeared several times in the "Doctor Who Magazine" comic strip, both as adversaries of the Doctor and in strips not involving the Doctor. In "The Outsider" (DWM #25-26), by Steve Moore and David Lloyd, a Sontaran named Skrant invaded the world of Brahtilis with the unwitting help of Demimon, a local astrologer. The Fourth Doctor faced the Sontarans in "Dragon's Claw" (DWM #39-#45), by Steve Moore and Dave Gibbons, where a crew of Sontarans menaced China in 1522 AD.
In Steven Moffat's short story "What I Did on My Christmas Holidays by Sally Sparrow" (the basis for the Tenth Doctor episode "Blink"), the Ninth Doctor has a rooftop sword fight with two Sontarans in 21st century Istanbul, defeating them with the help of spy Sally Sparrow, apparently before the events of "Rose" in his personal timeline.
The Sontaran homeworld was destroyed in the future during the events of the Seventh Doctor strip "Pureblood" (DWM #193-196) but the Sontaran race pool survived, allowing for further cloning; the strip introduced the concept of "pureblood" Sontarans not born of cloning. The Sontarans also feature in the Kroton solo strip "Unnatural Born Killers" (DWM #277) and the Tenth Doctor's comic strip debut "The Betrothal of Sontar" (DWM #365-#368), by John Tomlinson and Nick Abadzis, where a Sontaran mining rig on the ice planet Serac comes under attack by a mysterious force.

</doc>
<doc id="28027" url="https://en.wikipedia.org/wiki?curid=28027" title="Skateboarding">
Skateboarding

Skateboarding is an action sport which involves riding and performing tricks using a skateboard. Skateboarding can also be considered a recreational activity, an art form, a job, or a method of transportation. Skateboarding has been shaped and influenced by many skateboarders throughout the years. A 2009 report found that the skateboarding market is worth an estimated $4.8 billion in annual revenue with 11.08 million active skateboarders in the world.
Since the 1970s, skateparks have been constructed specifically for use by skateboarders, Freestyle BMXers, aggressive skaters, and very recently, scooters.
History.
1940s–1960s.
The first skateboards started with wooden boxes, or boards, with roller skate wheels attached to the bottom. Crate scooters preceded skateboards, having a wooden crate attached to the nose (front of the board), which formed rudimentary handlebars. The boxes turned into planks, similar to the skateboard decks of today. An American WAC, Betty Magnuson, reported seeing French children in the Montmartre section of Paris riding on boards with roller skate wheels attached to them in late 1944. 
Skateboarding, as we know it, was probably born sometime in the late 1940s, or early 1950s, when surfers in California wanted something to do when the waves were flat. No one knows who made the first board; it seems that several people came up with similar ideas at around the same time. The first manufactured skateboards were ordered by a Los Angeles, California surf shop, meant to be used by surfers in their downtime. The shop owner, Bill Richard, made a deal with the Chicago Roller Skate Company to produce sets of skate wheels, which they attached to square wooden boards. Accordingly, skateboarding was originally denoted "sidewalk surfing" and early skaters emulated surfing style and maneuvers, and performed barefoot.
By the 1960s a small number of surfing manufacturers in Southern California such as Jack's, Kips', Hobie, Bing's and Makaha started building skateboards that resembled small surfboards, and assembled teams to promote their products. One of the earliest Skateboard exhibitions was sponsored by Makaha's founder, Larry Stevenson, in 1963 and held at the Pier Avenue Junior High School in Hermosa Beach, California. Some of these same teams of skateboarders were also featured on a television show called "Surf's Up" in 1964, hosted by Stan Richards, that helped promote skateboarding as something new and fun to do.
As the popularity of skateboarding began expanding, the first skateboarding magazine, "The Quarterly Skateboarder" was published in 1964. John Severson who published the magazine wrote in his first editorial:
The magazine only lasted four issues, but resumed publication as "Skateboarder" in 1975. The first broadcast of an actual skateboarding competition was the 1965 National Skateboarding Championships, which were held in Anaheim, California and aired on ABC’s “Wide World of Sports. Because skateboarding was a new sport during this time, there were only two original disciplines during competitions; flatland freestyle & slalom downhill racing.
One of the earliest sponsored skateboarders, Patti McGee, was paid by Hobie and Vita Pak to travel around the country to do skateboarding exhibitions and to demonstrate skateboarding safety tips. McGee made the cover of "Life" magazine in 1965 and was featured on several popular television programs "The Mike Douglas Show", "What's My Line?" and "The Tonight Show Starring Johnny Carson", which helped make skateboarding even more popular at the time. Some of the other well known surfer-style skateboarders of the time also included Danny Bearer, Torger Johnson, Bruce Logan, Bill and Mark Richards, Woody Woodward, & Jim Fitzpatrick.
The growth of the sport during this period can also be seen in sales figures for Makaha, which quoted $10 million worth of board sales between 1963 and 1965 (Weyland, 2002:28). By 1966 a variety of sources began to claim that skateboarding was dangerous, resulting in shops being reluctant to sell them, and parents being reluctant to buy them. In 1966 sales had dropped significantly (ibid) and Skateboarder Magazine had stopped publication. The popularity of skateboarding dropped and remained low until the early 1970s.
1970s.
In the early 1970s, Frank Nasworthy started to develop a skateboard wheel made of polyurethane, calling his company Cadillac Wheels. Prior to this new material, skateboards wheels were metal or "clay" wheels. The improvement in traction and performance was so immense that from the wheel's release in 1972 the popularity of skateboarding started to rise rapidly again, causing companies to invest more in product development. Nasworthy commissioned artist Jim Evans to do a series of paintings promoting Cadillac Wheels, they were featured as ads and posters in the resurrected Skateboarder magazine, and proved immensely popular in promoting the new style of skateboarding.
In the early 1970s skateparks hadn't been invented yet, so skateboarders would flock and skateboard in such urban places like The Escondido reservoir in San Diego, California. Skateboarding magazine would publish the location and Skateboarders made up nicknames for each location such as the Tea Bowl, the Fruit Bowl, Bellagio, the Rabbit Hole, Bird Bath, the Egg Bowl, Upland Pool and the Sewer Slide. Some of the development concepts in the terrain of skateparks were actually taken from the Escondido reservoir. Many companies started to manufacture trucks (axles) specially designed for skateboarding, reached in 1976 by Tracker Trucks. As the equipment became more maneuverable, the decks started to get wider, reaching widths of and over, thus giving the skateboarder even more control. A banana board is a skinny, flexible skateboard made of polypropylene with ribs on the underside for structural support. These were very popular during the mid-1970s and were available in myriad colors, bright yellow probably being the most memorable, hence the name.
In 1975 skateboarding had risen back in popularity enough to have one of the largest skateboarding competition's since the 1960s, the Del Mar National Championships, which is said to have had up to 500 competitors. The competition lasted two days and was sponsored by Bahne Skateboards & Cadillac Wheels. While the main event was won by freestyle spinning skate legend Russ Howell, a local skate team from Santa Monica, California, the Zephyr team, ushered in a new era of surfer style skateboarding during the competition that would have a lasting impact on skateboarding's history. With a team of 12, including skating legends such as Jay Adams, Tony Alva, Peggy Oki & Stacy Peralta, they brought a new progressive style of skateboarding to the event, based on the style of Hawaiian surfers Larry Bertlemann, Buttons Kaluhiokalani and Mark Liddell. Craig Stecyk, a photo journalist for Skateboarder Magazine wrote about and photographed the team, along with Glen E. Friedman, shortly afterwards and ran a series on the team called the Dogtown articles, which eventually immortalized the Zephyr skateboard team. The team became known as the Z-Boys and would go on to become one of the most influential teams in skateboarding's history.
It was soon after that skateboarding contest for cash and prizes using a professional tier system began to be held throughout California, like the The California Free Former World Professional Skateboard Championships, which featured Freestyle and Slalom competitions.
A precursor to the extreme sport of Street luge, that was sanctioned by the United States Skateboarding Association (USSA), also took place during the 1970s in Signal Hill, California. The competition was called "The Signal Hill Skateboarding Speed Run", with several competitors earning entries into the Guinness Book of World Records, at the time clocking speeds of over 50 mph on a skateboard. Due to technology and safety concerns at the time, when many competitors crashed during their runs, the sport did not gain popularity or support during this time.
In March 1976, Skateboard City skatepark in Port Orange, Florida and Carlsbad Skatepark in San Diego County, California, would be the first two skateparks to be opened to the public in just a week apart. They were the first of some 200 skateparks that would be built through 1982. This was due in part to articles that were running in the Investment Journals at the time, stating that skateparks were a good investment. Notable skateboarders from the 1970s also include Ty Page, Tom Inouye, Laura Thornhill, Ellen O'Neal, Kim Cespedes, Bob Biniak, Jana Payne, Waldo Autry, Robin Logan, Bobby Piercy, Russ Howell, Ellen Berryman, Shogo Kubo, Desiree Von Essen, Henry Hester, Robin Alaway, Paul Hackett, Michelle Matta, Bruce Logan, Steve Cathey, Edie Robertson, Mike Weed, David Hackett, Gregg Ayres, Darren Ho, and Tom Sims.
Manufacturers started to experiment with more exotic composites and metals, like fiberglass and aluminium, but the common skateboards were made of maple plywood. The skateboarders took advantage of the improved handling of their skateboards and started inventing new tricks. Skateboarders, most notably Ty Page, Bruce Logan, Bobby Piercy, Kevin Reed, and the Z-Boys started to skate the vertical walls of swimming pools that were left empty in the 1976 California drought. This started the "vert" trend in skateboarding. With increased control, vert skaters could skate faster and perform more dangerous tricks, such as slash grinds and frontside/backside airs. This caused liability concerns and increased insurance costs to skatepark owners, and the development (first by Norcon, then more successfully by Rector) of improved knee pads that had a hard sliding cap and strong strapping proved to be too-little-too-late. During this era, the "freestyle" movement in skateboarding began to splinter off and develop into a much more specialized discipline, characterized by the development of a wide assortment of flat-ground tricks.
As a result of the "vert" skating movement, skate parks had to contend with high-liability costs that led to many park closures. In response, vert skaters started making their own ramps, while freestyle skaters continued to evolve their flatland style. Thus by the beginning of the 1980s, skateboarding had once again declined in popularity.
1980s.
This period was fueled by skateboard companies that were run by skateboarders. The focus was initially on vert ramp skateboarding. The invention of the no-hands aerial (later known as the ollie) by Alan Gelfand in Florida in 1976, and the almost parallel development of the grabbed aerial by George Orton and Tony Alva in California, made it possible for skaters to perform airs on vertical ramps. While this wave of skateboarding was sparked by commercialized vert ramp skating, a majority of people who skateboarded during this period didn't ride vert ramps. As most people could not afford to build vert ramps, or did not have access to nearby ramps, street skating increased in popularity.
Freestyle skating remained healthy throughout this period, with pioneers such as Rodney Mullen inventing many of the basic tricks that would become the foundation of modern street skating, such as the "Impossible" and the "kickflip". The influence that freestyle exerted upon street skating became apparent during the mid-1980s; however, street skating was still performed on wide vert boards with short noses, slide rails, and large soft wheels. In response to the tensions created by this confluence of skateboarding "genres", an rapid evolution occurred in the late 1980s to accommodate the street skater. Since few skateparks were available to skaters at this time, street skating pushed skaters to seek out shopping centers and public and private property as their "spot" to skate (public opposition, in which businesses, governments, and property owners have banned skateboarding on properties under their jurisdiction or ownership, would progressively intensify over the following decades). By 1992, only a small fraction of skateboarders remained as a highly technical version of street skating, combined with the decline of vert skating, produced a sport that lacked the mainstream appeal to attract new skaters.
1990s.
Skateboarding during the 1990s became dominated by street skateboarding. Most boards are about wide and long. The wheels are made of an extremely hard polyurethane, with hardness (durometer) approximately 99A. The wheel sizes are relatively small so that the boards are lighter, and the wheels' inertia is overcome quicker, thus making tricks more manageable. Board styles have changed dramatically since the 1970s but have remained mostly alike since the mid-1990s. The contemporary shape of the skateboard is derived from the freestyle boards of the 1980s with a largely symmetrical shape and relatively narrow width. This form had become standard by the mid '90s.
2000–present.
By 2001 skateboarding had gained in such popularity, that more participants under the age of 18 rode skateboards (10.6 million) than played baseball (8.2 million), although traditional organized team sports still dominated youth programs overall. Skateboarding and skateparks began to be viewed and used in a variety of new ways to compliment academic lessons in schools, including new non-traditional physical education skateboarding programs, like Skatepass and Skateistan that are used to encourage youth to have better attendance, self-discipline and confidence. This was also based on the healthy physical opportunities skateboarding was understood to bring participants for muscle & bone strengthening, balance and the positive impacts it can have on youth in teaching them mutual respect, social networking, artistic expression and an appreciation of the environment.
In 2003 Go Skateboarding Day was founded in southern California by the International Association of Skateboard Companies to promote skateboarding throughout the world. It is celebrated annually on June 21 “to define skateboarding as the rebellious, creative celebration of independence it continues to be.”
According to market research firm American Sports Data the number of skateboarders worldwide increased by more than 60 percent between 1999 and 2002—from 7.8 million to 12.5 million.
Many cities also began implementing recreation plans and statutes, during this time period, as part of their vision for local parks and communities to make public lands more available in particular, for skateboarding, inviting skateboarders to come in off of the city streets and into organized skateboarding activity areas. By 2006 there were over 2,400 Skateparks worldwide and the design of skateparks themselves had made a transition, as skaters turned designers, began to emerge in the field adding features for all levels of skaters. Many new places to skateboard designed specifically for street skaters, such as the “Safe Spot Skate Spot” program, first initiated by professional skateboarder Rob Dyrdek throughout many cites, allowed for the creation of smaller alternative safe skate plazas to be built at a lower cost. One of the largest locations ever built to skateboard in the world, SMP Skatepark in China, at 12,000 square meters in size, was built complete with a 5,000-seat stadium.
In 2009 Skatelab opened the Skateboarding Hall of Fame & Skateboard Museum. Nominees are chosen by the International Association of Skateboard Companies (IASC).
Recently, barefoot skating has been experiencing a revival. Many skaters ride barefoot, particularly in summer and in warmer countries, such as South Africa, Australia, Spain and South America. The plastic penny board is intended to be ridden barefoot, as is the surfboard-inspired hamboard
Trick skating.
With the evolution of skateparks and ramp skating, the skateboard began to change. Early skate tricks had consisted mainly of two-dimensional freestyle manoeuvres like riding on only two wheels ("wheelie" or "manual"), spinning only on the back wheels (a "pivot"), high jumping over a bar and landing on the board again, also known as a "hippie jump", long jumping from one board to another, (often over small barrels or fearless teenagers), or slalom. Another popular trick was the Bertlemann slide, named after Larry Bertelemann's surfing manoeuvres.
In 1976, skateboarding was transformed by the invention of the ollie by Alan "Ollie" Gelfand. It remained largely a unique Florida trick until the summer of 1978, when Gelfand made his first visit to California. Gelfand and his revolutionary maneuvers caught the attention of the West Coast skaters and the media where it began to spread worldwide. The ollie was adapted to flat ground by Rodney Mullen in 1982. Mullen also invented the "Magic Flip," which was later renamed the kickflip, as well as many other tricks including, the 360 kickflip, which is a 360 pop shove-it and a kickflip in the same motion. The flat ground ollie allowed skateboarders to perform tricks in mid-air without any more equipment than the skateboard itself, it has formed the basis of many street skating tricks. A recent development in the world of trick skating is the 1080, which was first ever landed by Tom Schaar in 2012.
Culture.
Skateboarding was popularized by the 1986 skateboarding cult classic "Thrashin"'. Directed by David Winters and starring Josh Brolin, it features appearances from many famous skaters such as Tony Alva, Tony Hawk, Christian Hosoi and Steve Caballero. "Thrashin'" also had a direct impact on "Lords of Dogtown", as Catherine Hardwicke, who directed "Lords of Dogtown", was hired by Winters to work on "Thrashin'" as a production designer where she met, worked with and befriended many famous skaters including the real Tony Alva, Tony Hawk, Christian Hosoi and Steve Caballero.
Skateboarding was, at first, tied to the culture of surfing. As skateboarding spread across the United States to places unfamiliar with surfing or surfing culture, it developed an image of its own. For example, the classic film short "Video Days" (1991) portrayed skateboarders as reckless rebels.
California duo Jan and Dean recorded the song "Sidewalk Surfin'" in 1964, which is the Beach Boys song "Catch a Wave" with new lyrics associated with skateboarding.
The image of the skateboarder as a rebellious, non-conforming youth has faded in recent years. Certain cities still oppose the building of skateparks in their neighborhoods, for fear of increased crime and drugs in the area. The rift between the old image of skateboarding and a newer one is quite visible: magazines such as "Thrasher" portray skateboarding as dirty, rebellious, and still firmly tied to punk, while other publications, "Transworld Skateboarding" as an example, paint a more diverse and controlled picture of skateboarding. Furthermore, as more professional skaters use hip hop, reggae, or hard rock music accompaniment in their videos, many urban youths, hip-hop fans, reggae fans, and hard rock fans are also drawn to skateboarding, further diluting the sport's punk image.
Films such as the 1986 "Thrashin'", "Grind" and "Lords of Dogtown", have helped improve the reputation of skateboarding youth, depicting individuals of this subculture as having a positive outlook on life, prone to poking harmless fun at each other, and engaging in healthy sportsman's competition. According to the film, lack of respect, egotism and hostility towards fellow skateboarders is generally frowned upon, albeit each of the characters (and as such, proxies of the "stereotypical" skateboarder) have a firm disrespect for authority and for rules in general. Group spirit is supposed to heavily influence the members of this community. In presentations of this sort, showcasing of criminal tendencies is absent, and no attempt is made to tie extreme sports to any kind of illegal activity.
"Gleaming the Cube", a 1989 movie starring Christian Slater as a skateboarding teen investigating the death of his adopted Vietnamese brother, was somewhat of an iconic landmark to the skateboarding genre of the era. Many well-known skaters had cameos in the film, including Tony Hawk and Rodney Mullen, where Mullen served as Slater's stunt double.
The increasing availability of technology is apparent within the skateboarding community. Many skateboarders record and edit videos of themselves and friends skateboarding. However, part of this culture is to not merely replicate but to innovate; emphasis is placed on finding new places and landing new tricks.
Skateboarding video games have also become very popular in skateboarding culture. Some of the most popular are the "Tony Hawk" series and "Skate series" for various consoles (including hand-held) and personal computer.
Skate shoe.
Whilst early skateboarders generally rode barefoot, preferring direct foot-to-board contact, and some skaters continue to do so, one of the early leading trends associated with the sub-culture of skateboarding itself, was the sticky sole "Slip-On" Skate shoe, most popularized by Sean Penn's skateboarding character from the film Fast Times at Ridgemont High. Because early skateboarders were actually surfers trying to emulate the sport of surfing, at the time when skateboards first came out on the market, many skateboarded barefoot. But skaters often lacked traction, which led to foot injuries. This necessitated the need for a shoe that was specifically designed and marketed for skateboarding, such as the Randy "720", manufactured by the Randolph Rubber Company, and Vans sneakers, which eventually became cultural iconic signifiers for skateboarders during the 70s & 80's as skateboarding became more widespread.
While the skate shoes design afforded better connection & traction with the deck, skaterboarders themselves could often be identified when wearing the shoes, with Tony Hawk once saying, "If you were wearing Vans shoes in 86, you were a skateboarder" Because of its connection with skateboarding, Vans financed the legendary skateboarding documentary "Dogtown and Z-Boys" and was the first sneaker company to endorse a professional skateboarder Stacy Peralta. Vans has a long history of being a major sponsor of many of skateboarding's competitions and events throughout skateboarding's history as well, including the Vans Warped Tour and the Vans Triple Crown Series.
As it eventually became more apparent that skateboarding had a particular identity with a style of shoe, other brands of shoe companies began to specifically design skate shoes for functionality and style to further enhance the experience and culture of skateboarding including such brands as; Converse, Nike, DC Shoes, Globe, Adidas, Zoo York and World Industries. Many professional skateboarders are designed a pro-model skate shoe, with their name on it, once they have received a skateboarding sponsorship after becoming notable skateboarders. Some shoe companies involved with skateboarding, like Sole Technology, an American footwear company that makes the Etnies skate shoe brand, further distinguish themselves in the market by collaborating with local cities to open public Skateparks, such as the etnies skatepark in Lake Forest, California.
Skateboard deck.
Individuality and a self-expressed casual style, have always been, among two of the cultural values for skateboarders, as uniforms and jerseys are not typically worn. This type of personal style for skateboarders is often reflected in the graphical designs illustrated on the bottom of the deck of skateboards, since its initial conception in the mid seventies, when Wes Humpston and Jim Muri first began doing design work for Dogtown Skateboards out of their garage by hand, creating the very first iconic skateboard-deck art with the design of the "Dogtown Cross".
Prior to the mid-seventies many early skateboards were originally based upon the concept of “Sidewalk Surfing” and were tied to the surf culture, skateboards were surfboard like in appearance with little to no graphics located under the bottom of the skateboard-deck. Some of the early manufactured skateboards such as "Roller Derby", the "Duraflex Surfer" and the "Banana board" are characteristic. Some skateboards during that time were manufactured with company logo's or stickers across the top of the deck of the skateboard, as griptape was not initially used for construction. But as skateboarding progressed & evolved, and as artist began to design and add influence to the artwork of skateboards, designs and themes began to change.
There were several artistic skateboarding pioneer's that had an influence on the culture of skateboarding during the 1980s, that transformed skateboard-deck art like Jim Phillips, who's edgy comic-book style "Screaming Hand", not only became the main logo for Santa Cruz Skateboards, but eventually transcended into tattoos of the same image for thousands of people & vinyl collectable figurines over the years. Artist Vernon Courtlandt Johnson is said to have used his artwork of skeletons and skulls, for Powell Peralta, during the same time that the music genres of punk rock and new wave music were beginning to mesh with the culture of skateboarding. Some other notable skateboard artists that made contribrutions to the culture of skateboarding also include Andy Jenkins, Todd Bratrud, Neil Blender, Marc McKee, Tod Swank, Mark Gonzales, Lance Mountain, Natas Kaupas and Jim Evans.
Over the years skateboard-deck art has continued to influence and expand the culture of skateboarding, as many people began collecting skateboards based on their artistic value and nostalgia. Productions of limited editions with particular designs and types of collectible prints that can be hung on the wall, have been created by such famous artist as Andy Warhol and Keith Haring. Most professional skateboarders today have their own signature skateboard decks, with their favorite artistic designs printed on them using Computer graphics.
Safety.
Skateboards, along with other small-wheeled transportation such as in-line skates and scooters, suffer a safety problem: riders may easily be thrown from small cracks and outcroppings in pavement, especially where the cracks run across the direction of travel. Hitting such an irregularity is the major cause of falls and injuries. The risk may be reduced at higher travel speeds.
Severe injuries are relatively rare. Commonly, a skateboarder who falls suffers from scrapes, cuts, bruises, and sprains. Among injuries reported to a hospital, about half involve broken bones, usually the long bones in the leg or arm. One-third of skateboarders with reported injuries are very new to the sport, having started skating within one week of the injury. Although less common, involving 3.5–9 percent of reported injuries, traumatic head injuries and death are possible severe outcomes.
Skating as a form of transportation exposes the skateboarder to the dangers of other traffic. Skateboarders on the street may be hit by other vehicles or may fall into vehicular traffic.
Skateboarders also pose a risk to other pedestrians and traffic. If the skateboarder falls, the skateboard may roll or fly into another person. A skateboarder who collides with a person who is walking or biking may injure or, rarely, kill that person.
Many jurisdictions require skateboarders to wear bicycle helmets to reduce the risk of head injuries and death. Other protective gear, such as wrist guards, also reduce injury. Some medical researchers have proposed restricting skateboarding to designated, specially designed areas, to reduce the number and severity of injuries, and to eliminate injuries caused by motor vehicles or to other pedestrians.
The use, ownership and sale of skateboards were forbidden in Norway from 1978 to 1989 because of the high number of injuries caused by boards. The ban led skateboarders to construct ramps in the forest and other secluded areas to avoid the police. There was, however, one legal skatepark in the country in Frognerparken in Oslo.
Other uses and styles.
Transportation.
The use of skateboards solely as a form of transportation is often associated with the longboard. Depending on local laws, using skateboards as a form of transportation outside residential areas may or may not be legal. Backers cite portability, exercise, and environmental friendliness as some of the benefits of skateboarding as an alternative to automobiles.
Military.
The United States Marine Corps tested the usefulness of commercial off-the-shelf skateboards during urban combat military exercises in the late 1990s in a program called Urban Warrior '99. Their special purpose was "for maneuvering inside buildings in order to detect tripwires and sniper fire".
Trampboarding.
Trampboarding is a variant of skateboarding that uses a board without the trucks and the wheels on a trampoline. Using the bounce of the trampoline gives height to perform a tricks, whereas in skateboarding you need to make the height by performing an ollie. Trampboarding is seen on YouTube in numerous videos.
Swing boarding.
Swing boarding is the activity where a skateboard deck is suspended from a pivot point above the rider which allows the rider to swing about that pivot point. The board swings in an arc which is a similar movement to riding a half pipe. The incorporation of a harness and frame allows the rider to perform turns spins all while flying though the air.
Controversy.
Skateboarding is sometimes associated with property damage to urban terrain features such as curbs, benches, and ledges when skateboarders perform tricks known as grinds on these surfaces. Private industry has responded to this perceived damage with skate deterrent devices, such as the Skatestopper, in an effort to mitigate damage and discourage skateboarding on these surfaces.
The passing of ordinances and the use of posted signs stating "Skateboarding is not allowed" have also become common methods to mitigate skateboarding in public areas in many cities, to protect pedestrians and property. In the area of street skating, tickets and arrest from police for trespassing are not uncommon.

</doc>
<doc id="28028" url="https://en.wikipedia.org/wiki?curid=28028" title="Speed skating">
Speed skating

Speed skating is a competitive form of ice skating in which the competitors race each other in travelling a certain distance on skates. Types of speed skating are long track speed skating, short track speed skating, and marathon speed skating. In the Olympic Games, long-track speed skating is usually referred to as just "speed skating", while short-track speed skating is known as "short track". The ISU, the governing body of both ice sports, refers to long track as "speed skating" and short track as "short track skating".
The standard rink for long track is 400 meters long, but tracks of 200, 250 and 333⅓ meters are used occasionally. It is one of two Olympic forms of the sport and the one with the longer history. An international federation was founded in 1892, the first for any winter sport. The sport enjoys large popularity in the Netherlands and Norway. There are top international rinks in a number of other countries, including Canada, the United States, Germany, Italy, Japan, South Korea and Russia. A World Cup circuit is held with events in those countries and with two events in Thialf, the ice hall in Heerenveen, Netherlands.
International Skating Union rules allow some leeway in the size and radius of curves.
Short track speed skating takes place on a smaller rink, normally the size of an ice hockey rink, on a 111.12 m oval track. Distances are shorter than in long-track racing, with the longest Olympic individual race being 1500 meters (the women's relay is 3000 meters and the men's relay 5000 meters). Races are usually held as knockouts, with the best two in heats of four or five qualifying for the final race, where medals are awarded. Disqualifications and falls are not uncommon.
There are variations on the mass-start races. In the regulations of roller sports, eight different types of mass starts are described. Among them are elimination races, where one or more competitors are eliminated at fixed points during the course; simple distance races, which may include preliminary knockout races; endurance races with time limits instead of a fixed distance; points races; and individual pursuits.
Races usually have some rules about disqualification if an opponent is unfairly hindered; these rules vary between the disciplines. In long track speed skating, almost any infringement on the pairmate is punished, though skaters are permitted to change from the inner to the outer lane out of the final curve if they are not able to hold the inner curve, as long as they are not interfering with the other skater. In mass-start races, skaters will usually be allowed some physical contact.
Team races are also held; in long track speed skating, the only team race at the highest level of competition is the team pursuit, though athletics-style relay races are held at children's competitions. Relay races are also held in short track and inline competitions, but here, exchanges may take place at any time during the race, though exchanges may be banned during the last couple of laps.
Most races are held on an oval course, but there are exceptions. Oval sizes vary; in short track speed skating, the rink must be an oval of 111.12 metres, while long track speed skating uses a similarly standardized 400 m rink. Inline skating rinks are between 125 and 400 metres, though banked tracks can only be 250 metres long. Inline skating can also be held on closed road courses between 400 and 1,000 metres, as well as open-road competitions where starting and finishing lines do not coincide. This is also a feature of outdoor marathons.
In the Netherlands, marathon competitions may be held on natural ice on canals, and bodies of water such as lakes and rivers, but may also be held on artificially frozen 400 m tracks, with skaters circling the track 100 times, for example.
History.
The roots of speed skating date back over a millennium to Scandinavia, Northern Europe and the Netherlands, where the natives added bones to their shoes and used them to travel on frozen rivers, canals and lakes. It was much later, in the 16th century, that people started seeing skating as fun and perhaps even a sporting activity.
Later, in Norway, King Eystein Magnusson, later King Eystein I of Norway, boasts of his skills racing on ice legs. 
However, skating and speed skating was not limited to the Netherlands and Scandinavia; in 1592, a Scotsman designed a skate with an iron blade. It was iron-bladed skates that led to the spread of skating and, in particular, speed skating.
By 1642, the first official skating club, The Skating Club Of Edinburgh, was born, and, in 1763, the world saw its first official speed skating race, on the Fens in England organized by the National Ice Skating Association.
While in the Netherlands, people began touring the waterways connecting the 11 cities of Friesland, a challenge which eventually led to the Elfstedentocht.
By 1851, North Americans had discovered a love of the sport, and indeed the all-steel blade was later developed there.
The Netherlands came back to the fore in 1889 with the organization of the first world championships. The ISU (International Skating Union) was also born in the Netherlands in 1892.
By the start of the 20th century, skating and speed skating had come into its own as a major popular sporting activity.
ISU development.
Organized races on ice skates developed in the 19th century. Norwegian clubs hosted competitions from 1863, with races in Christiania drawing five-digit crowds. In 1884, the Norwegian Axel Paulsen was named Amateur Champion Skater of the World after winning competitions in the United States. Five years later, a sports club in Amsterdam held an ice-skating event they called a world championship, with participants from Russia, the United States and the United Kingdom, as well as the host country. The "Internationale Eislauf Vereinigung", now known as the International Skating Union, was founded at a meeting of 15 national representatives in Scheveningen in 1892, the first international winter sports federation. The Nederlandse Schaatsrijderbond was founded in 1882 and organised the world championships of 1890 and 1891. Competitions were held around tracks of varying lengths—the 1885 match between Axel Paulsen and Remke van der Zee was skated on a track of 6/7 mile (1400 metres)—but the 400 metre track was standardised by the ISU in 1892, along with the standard distances for world championships, 500 m, 1500 m, 5000 m and 10,000 m. Skaters started in pairs, each to their own lane, and changed lanes for every lap to ensure that each skater completed the same distance. This is what is now known as long track speed skating. Competitions were exclusively for amateur skaters, which was enforced. Peter Sinnerud was disqualified for professionalism in 1904 and lost his world title.
Long track world records were first registered in 1891 and improved rapidly, Jaap Eden lowering the world 5000-metre record by half a minute during the Hamar European Championships in 1894. The record stood for 17 years, and it took 50 years to lower it by further half a minute.
Elfstedentocht.
The Elfstedentocht was organized as a competition in 1909 and has been held at irregular intervals, whenever the ice on the course is deemed good enough. Other outdoor races developed later, with Friesland in the northern Netherlands hosting a race in 1917, but the Dutch natural ice conditions have rarely been conducive to skating. The Elfstedentocht has been held 15 times in the nearly 100 years since 1909, and, before artificial ice was available in 1962, national championships had been held in 25 of the years between 1887, when the first championship was held in Slikkerveer, and 1961. Since artificial ice became common in the Netherlands, Dutch speed skaters have been among the world top in long track ice skating and marathon skating. Another solution to still be able to skate marathons on natural ice became the Alternative Elfstedentocht. The Alternative Elfstedentocht races take part in other countries, such as Austria, Finland or Canada, and all top marathon skaters, as well as thousands of recreative skaters, travel from the Netherlands to the location where the race is held. According to the NRC Handelsblad journalist Jaap Bloembergen, the country "takes a carnival look" during international skating championships.
Olympic Games.
Speed Skating started in 1924.
At the 1914 Olympic Congress, the delegates agreed to include ice speed skating in the 1916 Olympics, after figure skating had featured in the 1908 Olympics. However, World War I put an end to the plans of Olympic competition, and it was not until the winter sports week in Chamonix in 1924—retroactively awarded Olympic status—that ice speed skating reached the Olympic programme. Charles Jewtraw from Lake Placid, New York, won the first Olympic gold medal, though several Norwegians in attendance claimed Oskar Olsen had clocked a better time. Timing issues on the 500 were a problem within the sport until electronic clocks arrived in the 1960s; during the 1936 Olympic 500–metre race, it was suggested that Ivar Ballangrud's 500-metre time was almost a second too good. Finland won the remaining four gold medals at the 1924 Games, with Clas Thunberg winning 1,500 metres, 5,000 metres, and allround. It was the first and only time an allround Olympic gold medal has been awarded in speed skating. Speed Skating is also a sport in today's Olympic's.
Norwegian and Finnish skaters won all the gold medals in world championships between the world wars, with Latvians and Austrians visiting the podium in the European Championships. However, North American races were usually conducted packstyle, similar to the marathon races in the Netherlands, but the Olympic races were to be held over the four ISU-approved distances. The ISU approved the suggestion that the speed skating at the 1932 Winter Olympics should be held as packstyle races, and Americans won all four gold medals. Canada won five medals, all silver and bronze, while defending World Champion Clas Thunberg stayed at home, protesting against this form of racing. At the World Championships held immediately after the games, without the American champions, Norwegian racers won all four distances and occupied the three top spots in the allround standings.
Norwegians, Swedes, Finns and Japanese skating leaders protested to the USOC, condemning the manner of competition and expressing the wish that mass-start races were never to be held again at the Olympics. However, the ISU adopted the short track speed skating branch, with mass-start races on shorter tracks, in 1967, arranged international competitions from 1976, and brought them back to the Olympics in 1992.
Technical developments.
Artificial ices entered the long track competitions with the 1960 Winter Olympics, and the competitions in 1956 on Lake Misurina were the last Olympic competitions on natural ice. 1960 also saw the first Winter Olympic competitions for women. Lidia Skoblikova won two gold medals in 1960 and four in 1964.
More aerodynamic skating suits were also developed, with Swiss skater Franz Krienbühl (who finished 8th on the Olympic 10,000 m at the age of 46) at the front of development. After a while, national teams took over development of body suits, which are also used in short track skating, though without headcover attached to the suit—short trackers wear helmets instead, as falls are more common in mass-start races. Suits and indoor skating, as well as the clap skate, has helped to lower long track world records considerably; from 1971 to 2009, the average speed on the men's 1500 metres has been raised from 45 to 52 km/h. Similar speed increases are shown in the other distances.
Professionalism.
After the 1972 season, European long track skaters founded a professional league, International Speedskating League, which included Ard Schenk, three-time Olympic gold medallist in 1972, as well as five Norwegians, four other Dutchmen, three Swedes, and a few other skaters. Jonny Nilsson, 1963 world champion and Olympic gold medallist, was the driving force behind the league, which folded in 1974 for economic reasons, and the ISU also excluded tracks hosting professional races from future international championships. The ISU later organised its own World Cup circuit with monetary prizes, and full-time professional teams developed in the Netherlands during the 1990s, which led them to a dominance on the men's side only challenged by Japanese 500 m racers and American inline skaters who changed to long tracks to win Olympic gold.
North American professionals.
During the 20th century, roller skating also developed as a competitive sport. Roller-skating races were professional from an early stage. Professional World Championships were arranged in North America between the competitors on that circuit. Later, roller derby leagues appeared, a professional contact sport that originally was a form of racing. FIRS World Championships of inline speed skating go back to the 1980s, but many world champions, such as Derek Parra and Chad Hedrick, have switched to ice in order to win Olympic medals.
Like roller skating, ice speed skating was also professional in North America. Oscar Mathisen, five-time ISU world champion and three-time European champion, renounced his amateur status in 1916 and travelled to America, where he won many races but was beaten by Bobby McLean of Chicago, four-time American champion, in one of the races. Chicago was a centre of ice speed skating in America; the "Chicago Tribune" sponsored a competition called the Silver Skates from 1912 to 2014.
Short track enters the Olympics.
In 1992, short track speed skating was accepted as an Olympic sport. Short track speed skating had little following in the long track speed skating countries of Europe, such as Norway, the Netherlands and the former Soviet Union, with none of these nations having won official medals (though the Netherlands won two gold medals when the sport was a demonstration event in 1988). The Norwegian publication "Sportsboken" spent ten pages detailing the long track speed skating events at the Albertville Games in 1993, but short track was not mentioned by word, though the results pages appeared in that section.
Although this form of speed skating is newer, it is growing faster than long-track speed skating, largely because short track can be done on an ice hockey rink rather than a long-track oval. South Korea has been the dominant nation in this sport, winning 17 Olympic gold medals.
Rules.
Short track.
Races are run counter-clockwise on a 111 meter track. Short track races are almost always run in a mass start format in which two to six skaters may race at once. Skaters may be disqualified for false starts, impeding, and cutting inside the track. False starts occur when a skater moves before the gun goes off at the start of a race. Skaters are disqualified for impeding when one skater cuts in front of another skater and causes the first skater to stand up to avoid collision or fall. Cutting inside the track occurs when a skater's skates goes inside the blocks which mark the track on the ice. If disqualified the skater will be given last place in their heat of final.
Long track.
Races are run counter-clockwise on a 400 meter oval. In all individual competition forms, only two skaters are allowed to race at once. Skaters must change lanes every lap. The skater changing from the outside lane to the inside has right-of-way. Skaters may be disqualified for false starts, impeding, and cutting inside the track. If a skater misses their race or falls they have the option to race their distance again. There are no heats or finals in long track, all rankings are by time.
The starting procedure in long-track speed-skating consists of three parts. First, the referee tells the athletes to ""Go to the start"". Second, the referee cues the athletes to get "Ready", and waits until the skaters have stopped moving. Finally, the referee waits for a random duration between 1 and 1.5 seconds, and then fires the starting shot. Some argue that this inherent timing variability could disadvantage athletes that start after longer pauses, due to the alerting effect.
In the only non-individual competition form, the team pursuit, two teams of each three to four skaters are allowed to race at once. Both teams remain in the inner lane for the duration of the race; they start on opposite sides of the rink. If four skaters are racing one skater is allowed to drop off and stop racing. The clock stops when the third skater crosses the finish line.
Equipment.
Speed skates Speed skates differ greatly from hockey skates and figure skates. Unlike hockey skates and figure skates, speed skates cut off at the ankle and are built more like a shoe than a boot to allow for more ankle compression. The blades range in length on average from 12" to 18" depending on the age and height of the skater. Short track blades are fixed to the boot in two places once at the heel and the other right behind the ball of the foot. Long track skates, also called klap skates, attach firmly to the boot only at the front. The heel of the boot actually detaches from the blade every stroke. This is accomplished through a spring mechanism located at the front connector. Speed skates cannot be sharpened at a shop like most skates. Instead each skater sharpens his or her own skates. This is accomplished using a jig which is a frame to hold the skates in place while they are manually sharpened.
Short track
All short track skaters must have speed skates, a spandex skin suit, protective helmet, specific cut proof skating gloves, knee pads and shin pads(in suit), neck guard (bib style) and ankle protection. Protective eye wear is mandatory. Many skater wear smooth ceramic or carbon fiber tips on the left hand grove to reduce friction when skaters put their hand on the ice in the corners. All skaters who race at a national level must wear a cutproof kevlar suit to protect against being cut from another skater's blade.
'"Long track
For long track skaters the same equipment should be worn as short track racers but with the exception of a helmet, shin pads, knee pads, and neck guard which are not required. Protective eye wear is optional. The suit also does not need to be kevlar. Long track skaters wear a hood that is built into the suit.

</doc>
<doc id="28030" url="https://en.wikipedia.org/wiki?curid=28030" title="September 13">
September 13


</doc>
<doc id="28032" url="https://en.wikipedia.org/wiki?curid=28032" title="Square (disambiguation)">
Square (disambiguation)

A square is a regular quadrilateral with four equal sides and four right angles.
Square may also refer to:

</doc>
<doc id="28034" url="https://en.wikipedia.org/wiki?curid=28034" title="Scanning electron microscope">
Scanning electron microscope

A scanning electron microscope (SEM) is a type of electron microscope that produces images of a sample by scanning it with a focused beam of electrons. The electrons interact with atoms in the sample, producing various signals that contain information about the sample's surface topography and composition. The electron beam is generally scanned in a raster scan pattern, and the beam's position is combined with the detected signal to produce an image. SEM can achieve resolution better than 1 nanometer. Specimens can be observed in high vacuum, in low vacuum, in wet conditions (in environmental SEM), and at a wide range of cryogenic or elevated temperatures.
The most common SEM mode is detection of secondary electrons emitted by atoms excited by the electron beam. The number of secondary electrons that can be detected depends, among other things, on the angle at which beam meets surface of specimen, i.e. on specimen topography. By scanning the sample and collecting the secondary electrons that are emitted using a special detector, an image displaying the topography of the surface is created.
History.
An account of the early history of SEM has been presented by McMullan. Although Max Knoll produced a photo with a 50 mm object-field-width showing channeling contrast by the use of an electron beam scanner, it was Manfred von Ardenne who in 1937 invented a true microscope with high magnification by scanning a very small raster with a demagnified and finely focused electron beam. Ardenne applied the scanning principle not only to achieve magnification but also to purposefully eliminate the chromatic aberration otherwise inherent in the electron microscope. He further discussed the various detection modes, possibilities and theory of SEM, together with the construction of the first high magnification SEM. Further work was reported by Zworykin's group, followed by the Cambridge groups in the 1950s and early 1960s headed by Charles Oatley, all of which finally led to the marketing of the first commercial instrument by Cambridge Scientific Instrument Company as the "Stereoscan" in 1965, which was delivered to DuPont.
Principles and capacities.
The types of signals produced by an SEM include secondary electrons (SE), reflected or back-scattered electrons (BSE), photons of characteristic X-rays and light (cathodoluminescence) (CL), absorbed current (specimen current) and transmitted electrons. Secondary electron detectors are standard equipment in all SEMs, but it is rare that a single machine would have detectors for all other possible signals.
The signals result from interactions of the electron beam with atoms at various depths within the sample. In the most common or standard detection mode, secondary electron imaging or SEI, the secondary electrons are emitted from very close to the specimen surface. Consequently, SEM can produce very high-resolution images of a sample surface, revealing details less than 1 nm in size. Back-scattered electrons (BSE) are beam electrons that are reflected from the sample by elastic scattering. They emerge from deeper locations within the specimen and consequently the resolution of BSE images is generally poorer than SE images. However, BSE are often used in analytical SEM along with the spectra made from the characteristic X-rays, because the intensity of the BSE signal is strongly related to the atomic number (Z) of the specimen. BSE images can provide information about the distribution of different elements in the sample. For the same reason, BSE imaging can image colloidal gold immuno-labels of 5 or 10 nm diameter, which would otherwise be difficult or impossible to detect in secondary electron images in biological specimens. Characteristic X-rays are emitted when the electron beam removes an inner shell electron from the sample, causing a higher-energy electron to fill the shell and release energy. These characteristic X-rays are used to identify the composition and measure the abundance of elements in the sample.
Due to the very narrow electron beam, SEM micrographs have a large depth of field yielding a characteristic three-dimensional appearance useful for understanding the surface structure of a sample. This is exemplified by the micrograph of pollen shown above. A wide range of magnifications is possible, from about 10 times (about equivalent to that of a powerful hand-lens) to more than 500,000 times, about 250 times the magnification limit of the best light microscopes.
Sample preparation.
All samples must be of an appropriate size to fit in the specimen chamber and are generally mounted rigidly on a specimen holder called a specimen stub. Several models of SEM can examine any part of a semiconductor wafer, and some can tilt an object of that size to 45°.
For conventional imaging in the SEM, specimens must be electrically conductive, at least at the surface, and electrically grounded to prevent the accumulation of electrostatic charge at the surface. Metal objects require little special preparation for SEM except for cleaning and mounting on a specimen stub. Nonconductive specimens tend to charge when scanned by the electron beam, and especially in secondary electron imaging mode, this causes scanning faults and other image artifacts. They are therefore usually coated with an ultrathin coating of electrically conducting material, deposited on the sample either by low-vacuum sputter coating or by high-vacuum evaporation. Conductive materials in current use for specimen coating include gold, gold/palladium alloy, platinum, osmium, iridium, tungsten, chromium, and graphite. Additionally, coating with heavy metals may increase signal/noise ratio for samples of low atomic number (Z). The improvement arises because secondary electron emission for high-Z materials is enhanced.
An alternative to coating for some biological samples is to increase the bulk conductivity of the material by impregnation with osmium using variants of the OTO staining method (O-osmium tetroxide, T-thiocarbohydrazide, O-osmium).
Nonconducting specimens may be imaged uncoated using environmental SEM (ESEM) or low-voltage mode of SEM operation. In ESEM instruments the specimen is placed in a relatively high-pressure chamber and the electron optical column is differentially pumped to keep vacuum adequately low at the electron gun. The high-pressure region around the sample in the ESEM neutralizes charge and provides an amplification of the secondary electron signal. Low-voltage SEM is typically conducted in an FEG-SEM because field emission guns (FEG) are capable of producing high primary electron brightness and small spot size even at low accelerating potentials. To prevent charging of non-conductive specimens, operating conditions must be adjusted such that the incoming beam current is equal to sum of outcoming secondary and backscattered electrons currents a condition that is more often met at accelerating voltages of 0.3–4 kV.
Synthetic replicas can be made to avoid the use of original samples when they are not suitable or available for SEM examination due to methodological obstacles or legal issues. This technique is achieved in two steps: (1) a mold of the original surface is made using a silicone-based dental elastomer, and (2) a replica of the original surface is obtained by pouring a synthetic resin into the mold.
Embedding in a resin with further polishing to a mirror-like finish can be used for both biological and materials specimens when imaging in backscattered electrons or when doing quantitative X-ray microanalysis.
The main preparation techniques are not required in the environmental SEM outlined below, but some biological specimens can benefit from fixation.
Biological samples.
For SEM, a specimen is normally required to be completely dry, since the specimen chamber is at high vacuum. Hard, dry materials such as wood, bone, feathers, dried insects, or shells can be examined with little further treatment, but living cells and tissues and whole, soft-bodied organisms usually require chemical fixation to preserve and stabilize their structure. Fixation is usually performed by incubation in a solution of a buffered chemical fixative, such as glutaraldehyde, sometimes in combination with formaldehyde and other fixatives, and optionally followed by postfixation with osmium tetroxide. The fixed tissue is then dehydrated. Because air-drying causes collapse and shrinkage, this is commonly achieved by replacement of water in the cells with organic solvents such as ethanol or acetone, and replacement of these solvents in turn with a transitional fluid such as liquid carbon dioxide by critical point drying. The carbon dioxide is finally removed while in a supercritical state, so that no gas–liquid interface is present within the sample during drying. The dry specimen is usually mounted on a specimen stub using an adhesive such as epoxy resin or electrically conductive double-sided adhesive tape, and sputter-coated with gold or gold/palladium alloy before examination in the microscope.
If the SEM is equipped with a cold stage for cryo microscopy, cryofixation may be used and low-temperature scanning electron microscopy performed on the cryogenically fixed specimens. Cryo-fixed specimens may be cryo-fractured under vacuum in a special apparatus to reveal internal structure, sputter-coated, and transferred onto the SEM cryo-stage while still frozen. Low-temperature scanning electron microscopy is also applicable to the imaging of temperature-sensitive materials such as ice (see e.g. illustration at left) and fats.
Freeze-fracturing, freeze-etch or freeze-and-break is a preparation method particularly useful for examining lipid membranes and their incorporated proteins in "face on" view. The preparation method reveals the proteins embedded in the lipid bilayer.
Materials.
Back scattered electron imaging, quantitative X-ray analysis, and X-ray mapping of specimens often requires that the surfaces be ground and polished to an ultra smooth surface. Specimens that undergo WDS or EDS analysis are often carbon coated. In general, metals are not coated prior to imaging in the SEM because they are conductive and provide their own pathway to ground.
Fractography is the study of fractured surfaces that can be done on a light microscope or commonly, on an SEM. The fractured surface is cut to a suitable size, cleaned of any organic residues, and mounted on a specimen holder for viewing in the SEM.
Integrated circuits may be cut with a focused ion beam (FIB) or other ion beam milling instrument for viewing in the SEM. The SEM in the first case may be incorporated into the FIB.
Metals, geological specimens, and integrated circuits all may also be chemically polished for viewing in the SEM.
Special high-resolution coating techniques are required for high-magnification imaging of inorganic thin films.
Scanning process and image formation.
In a typical SEM, an electron beam is thermionically emitted from an electron gun fitted with a tungsten filament cathode. Tungsten is normally used in thermionic electron guns because it has the highest melting point and lowest vapor pressure of all metals, thereby allowing it to be electrically heated for electron emission, and because of its low cost. Other types of electron emitters include lanthanum hexaboride () cathodes, which can be used in a standard tungsten filament SEM if the vacuum system is upgraded or field emission guns (FEG), which may be of the cold-cathode type using tungsten single crystal emitters or the thermally assisted Schottky type, that use emitters of zirconium oxide.
The electron beam, which typically has an energy ranging from 0.2 keV to 40 keV, is focused by one or two condenser lenses to a spot about 0.4 nm to 5 nm in diameter. The beam passes through pairs of scanning coils or pairs of deflector plates in the electron column, typically in the final lens, which deflect the beam in the "x" and "y" axes so that it scans in a raster fashion over a rectangular area of the sample surface.
When the primary electron beam interacts with the sample, the electrons lose energy by repeated random scattering and absorption within a teardrop-shaped volume of the specimen known as the interaction volume, which extends from less than 100 nm to approximately 5 µm into the surface. The size of the interaction volume depends on the electron's landing energy, the atomic number of the specimen and the specimen's density. The energy exchange between the electron beam and the sample results in the reflection of high-energy electrons by elastic scattering, emission of secondary electrons by inelastic scattering and the emission of electromagnetic radiation, each of which can be detected by specialized detectors. The beam current absorbed by the specimen can also be detected and used to create images of the distribution of specimen current. Electronic amplifiers of various types are used to amplify the signals, which are displayed as variations in brightness on a computer monitor (or, for vintage models, on a cathode ray tube). Each pixel of computer video memory is synchronized with the position of the beam on the specimen in the microscope, and the resulting image is therefore a distribution map of the intensity of the signal being emitted from the scanned area of the specimen. In older microscopes images may be captured by photography from a high-resolution cathode ray tube, but in modern machines they are digitised and saved as digital images.
Magnification.
Magnification in an SEM can be controlled over a range of about 6 orders of magnitude from about 10 to 500,000 times. Unlike optical and transmission electron microscopes, image magnification in an SEM is not a function of the power of the objective lens. SEMs may have condenser and objective lenses, but their function is to focus the beam to a spot, and not to image the specimen. Provided the electron gun can generate a beam with sufficiently small diameter, an SEM could in principle work entirely without condenser or objective lenses, although it might not be very versatile or achieve very high resolution. In an SEM, as in scanning probe microscopy, magnification results from the ratio of the dimensions of the raster on the specimen and the raster on the display device. Assuming that the display screen has a fixed size, higher magnification results from reducing the size of the raster on the specimen, and vice versa. Magnification is therefore controlled by the current supplied to the x, y scanning coils, or the voltage supplied to the x, y deflector plates, and not by objective lens power.
Detection of secondary electrons.
The most common imaging mode collects low-energy (<50 eV) secondary electrons that are ejected from the k-shell of the specimen atoms by inelastic scattering interactions with beam electrons. Due to their low energy, these electrons originate within a few nanometers from the sample surface. The electrons are detected by an Everhart-Thornley detector, which is a type of scintillator-photomultiplier system. The secondary electrons are first collected by attracting them towards an electrically biased grid at about +400 V, and then further accelerated towards a phosphor or scintillator positively biased to about +2,000 V. The accelerated secondary electrons are now sufficiently energetic to cause the scintillator to emit flashes of light (cathodoluminescence), which are conducted to a photomultiplier outside the SEM column via a light pipe and a window in the wall of the specimen chamber. The amplified electrical signal output by the photomultiplier is displayed as a two-dimensional intensity distribution that can be viewed and photographed on an analogue video display, or subjected to analog-to-digital conversion and displayed and saved as a digital image. This process relies on a raster-scanned primary beam. The brightness of the signal depends on the number of secondary electrons reaching the detector. If the beam enters the sample perpendicular to the surface, then the activated region is uniform about the axis of the beam and a certain number of electrons "escape" from within the sample. As the angle of incidence increases, the "escape" distance of one side of the beam will decrease, and more secondary electrons will be emitted. Thus steep surfaces and edges tend to be brighter than flat surfaces, which results in images with a well-defined, three-dimensional appearance. Using the signal of secondary electrons image resolution less than 0.5 nm is possible.
Detection of backscattered electrons.
Backscattered electrons (BSE) consist of high-energy electrons originating in the electron beam, that are reflected or back-scattered out of the specimen interaction volume by elastic scattering interactions with specimen atoms. Since heavy elements (high atomic number) backscatter electrons more strongly than light elements (low atomic number), and thus appear brighter in the image, BSE are used to detect contrast between areas with different chemical compositions. The Everhart-Thornley detector, which is normally positioned to one side of the specimen, is inefficient for the detection of backscattered electrons because few such electrons are emitted in the solid angle subtended by the detector, and because the positively biased detection grid has little ability to attract the higher energy BSE. Dedicated backscattered electron detectors are positioned above the sample in a "doughnut" type arrangement, concentric with the electron beam, maximizing the solid angle of collection. BSE detectors are usually either of scintillator or of semiconductor types. When all parts of the detector are used to collect electrons symmetrically about the beam, atomic number contrast is produced. However, strong topographic contrast is produced by collecting back-scattered electrons from one side above the specimen using an asymmetrical, directional BSE detector; the resulting contrast appears as illumination of the topography from that side. Semiconductor detectors can be made in radial segments that can be switched in or out to control the type of contrast produced and its directionality.
Backscattered electrons can also be used to form an electron backscatter diffraction (EBSD) image that can be used to determine the crystallographic structure of the specimen.
Beam-injection analysis of semiconductors.
The nature of the SEM's probe, energetic electrons, makes it uniquely suited to examining the optical and electronic properties of semiconductor materials. The high-energy electrons from the SEM beam will inject charge carriers into the semiconductor. Thus, beam electrons lose energy by promoting electrons from the valence band into the conduction band, leaving behind holes.
In a direct bandgap material, recombination of these electron-hole pairs will result in cathodoluminescence; if the sample contains an internal electric field, such as is present at a p-n junction, the SEM beam injection of carriers will cause electron beam induced current (EBIC) to flow. Cathodoluminescence and EBIC are referred to as "beam-injection" techniques, and are very powerful probes of the optoelectronic behavior of semiconductors, in particular for studying nanoscale features and defects.
Cathodoluminescence.
Cathodoluminescence, the emission of light when atoms excited by high-energy electrons return to their ground state, is analogous to UV-induced fluorescence, and some materials such as zinc sulfide and some fluorescent dyes, exhibit both phenomena. Over the last decades, cathodoluminescence was most commonly experienced as the light emission from the inner surface of the cathode ray tube in television sets and computer CRT monitors. In the SEM, CL detectors either collect all light emitted by the specimen or can analyse the wavelengths emitted by the specimen and display an emission spectrum or an image of the distribution of cathodoluminescence emitted by the specimen in real color.
X-ray microanalysis.
Characteristic X-rays that are produced by the interaction of electrons with the sample may also be detected in an SEM equipped for energy-dispersive X-ray spectroscopy or wavelength dispersive X-ray spectroscopy. Analysis of the x-ray signals may be used to map the distribution and estimate the abundance of elements in the sample.
Resolution of the SEM.
SEM is not a camera and the detector is not continuously image-forming like a CCD array or film. Unlike in an optical system, the resolution is not limited by the diffraction limit, fineness of lenses or mirrors or detector array resolution. The focusing optics can be large and coarse, and the SE detector is fist-sized and simply detects current. Instead, the spatial resolution of the SEM depends on the size of the electron spot, which in turn depends on both the wavelength of the electrons and the electron-optical system that produces the scanning beam. The resolution is also limited by the size of the interaction volume, the volume of specimen material that interacts with the electron beam. The spot size and the interaction volume are both large compared to the distances between atoms, so the resolution of the SEM is not high enough to image individual atoms, as is possible transmission electron microscope (TEM). The SEM has compensating advantages, though, including the ability to image a comparatively large area of the specimen; the ability to image bulk materials (not just thin films or foils); and the variety of analytical modes available for measuring the composition and properties of the specimen. Depending on the instrument, the resolution can fall somewhere between less than 1 nm and 20 nm. As of 2009, The world's highest resolution conventional (<30 kV) SEM can reach a point resolution of 0.4 nm using a secondary electron detector.
Environmental SEM.
Conventional SEM requires samples to be imaged under vacuum, because a gas atmosphere rapidly spreads and attenuates electron beams. As a consequence, samples that produce a significant amount of vapour, e.g. wet biological samples or oil-bearing rock, must be either dried or cryogenically frozen. Processes involving phase transitions, such as the drying of adhesives or melting of alloys, liquid transport, chemical reactions, and solid-air-gas systems, in general cannot be observed. Some observations of living insects have been possible, however.
The first commercial development of the ESEM in the late 1980s
The first commercial ESEMs were produced by the ElectroScan Corporation in USA in 1988. ElectroScan was taken over by Philips (who later sold their electron-optics division to FEI Company) in 1996.
ESEM is especially useful for non-metallic and biological materials because coating with carbon or gold is unnecessary. Uncoated Plastics and Elastomers can be routinely examined, as can uncoated biological samples. Coating can be difficult to reverse, may conceal small features on the surface of the sample and may reduce the value of the results obtained. X-ray analysis is difficult with a coating of a heavy metal, so carbon coatings are routinely used in conventional SEMs, but ESEM makes it possible to perform X-ray microanalysis on uncoated non-conductive specimens; however some specific for ESEM artifacts are introduced in X-ray analysis. ESEM may be the preferred for electron microscopy of unique samples from criminal or civil actions, where forensic analysis may need to be repeated by several different experts.
Transmission SEM.
The SEM can also be used in transmission mode by simply incorporating an appropriate detector below a thin specimen section
. Both bright and dark field imaging has been reported in the generally low accelerating beam voltage range used in SEM, which increases the contrast of unstained biological specimens at high magnifications with a field emission electron gun. This mode of operation has been abbreviated by the acronym STEM.
Color in SEM.
Electron microscopes do not naturally produce color images, as a SEM produces a single value per pixel; this value corresponds to the number of electrons received by the detector during a small period of time of the scanning when the beam is targeted to the (x,y) pixel position.
This single number is usually represented, for each pixel, by a grey level, forming a "black-and-white" image. However, several ways have been used to get color electron microscopy images.
False color using a single detector.
The easiest way to get color is to associate to this single number an arbitrary color, using a color look-up table (i.e. each grey level is replaced by a chosen color). This method is known as false color. On a BSE image, false color may be performed to better distinguish the various phases of the sample.
As an alternative to simply replacing each grey level by a color, a sample observed by an oblique beam allows to create an approximative topography image (see further section "Photometric 3D rendering from a single SEM image"). Such topography can then be processed by 3D-rendering algorithms for a more natural rendering of the surface texture
SEM image coloring.
Very often, published SEM images are artificially colored. This may be done for aesthetic effect, to clarify structure or to add a realistic appearance to the sample and generally does not add information about the specimen.
Coloring may be performed manually with photo-editing software, or semi-automatically with dedicated software using feature-detection or object-oriented segmentation.
Color built using multiple electron detectors.
In some configurations more information is gathered per pixel, often by the use of multiple detectors.
As a common example, secondary electron and backscattered electron detectors are superimposed and a color is assigned to each of the images captured by each detector, with an end result of a combined color image where colors are related to the density of the components. This method is known as density-dependent color SEM (DDC-SEM). Micrographs produced by DDC-SEM retain topographical information, which is better captured by the secondary electrons detector and combine it to the information about density, obtained by the backscattered electron detector.
Analytical signals based on generated photons.
Measurement of the energy of photons emitted from the specimen is a common method to get analytical capabilities. Examples are the Energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and Cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors it is common to color code these extra signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal data, which is not modified in any way.
3D in SEM.
SEMs do not naturally provide 3D images contrary to SPMs. However 3D data can be obtained using an SEM with different methods as follows.
Photometric 3D SEM reconstruction from a four-quadrant detector "shape from shading".
This method typically uses a four-quadrant BSE detector. The microscope produces four images of the same specimen at the same time, so no tilt is required. The method gives metrological 3D dimensions as far as the slope of the specimen remains reasonable. As it works by integration of the slope, vertical slopes and overhangs are ignored ; for instance, if an entire sphere lies on a flat, only the main part of the upper hemisphere is seen emerging above the flat, resulting in wrong altitude of the sphere apex.
Photometric 3D rendering from a single SEM image.
This method requires an SEM image obtained in oblique low angle lighting. The grey-level is then interpreted as the slope, and the slope integrated to restore the specimen topography. This method is interesting for visual enhancement and the detection of the shape and position of objects ; however the vertical heights cannot usually be calibrated, contrary to other methods such as photogrammetry.
Applications of 3D SEM.
Possible applications are roughness measurement, measurement of fractal dimension, examining fracture surface of metals, characterization of materials, corrosion measurement, and dimensional measurements at the nano scale (step height, volume, angle, flatness, bearing ratio, coplanarity, etc.).
Gallery of SEM images.
The following are examples of images taken using an SEM.

</doc>
<doc id="28044" url="https://en.wikipedia.org/wiki?curid=28044" title="Timeline of the September 11 attacks">
Timeline of the September 11 attacks

The September 11 attacks timeline is a chronological list of all the major events leading up to, during, and immediately following the terrorist attacks on New York and Washington that day. The timeline starts with the completion of the first World Trade Center tower in 1970 through the first anniversary of the attacks in 2002.
September 11, 2001.
All times are in local time (EDT or UTC − 4).

</doc>
<doc id="28045" url="https://en.wikipedia.org/wiki?curid=28045" title="Hijackers in the September 11 attacks">
Hijackers in the September 11 attacks

The hijackers in the September 11 attacks were 19 men affiliated with al-Qaeda. 15 of the 19 were citizens of Saudi Arabia. The others were from the United Arab Emirates (2), Egypt and Lebanon. The hijackers were organized into four teams, each led by a pilot-trained hijacker with three or four "muscle hijackers" who were trained to help subdue the pilots, passengers, and crew.
The first hijackers to arrive in the United States were Khalid al-Mihdhar and Nawaf al-Hazmi, who settled in the San Diego area in January 2000. They were followed by three hijacker-pilots, Mohamed Atta, Marwan al-Shehhi, and Ziad Jarrah in mid-2000 to undertake flight training in south Florida. The fourth hijacker-pilot, Hani Hanjour, arrived in San Diego in December 2000. The rest of the "muscle hijackers" arrived in early and mid-2001. They had taken classes to learn how to fly the planes properly.
Background.
The 2001 attacks were preceded by the less well known Bojinka plot which was planned in the Philippines by Ramzi Yousef (of the 1993 World Trade Center bombing) and Khalid Shaikh Mohammed. Its objective was to blow up twelve airliners and their approximately 4,000 passengers as they flew from Asia to the United States. The plan included crashing a plane into the CIA headquarters, lending credence to the theory that Khalid Shaikh Mohammed evolved this plot into the September 11 attacks. The plot was disrupted in January 1995 after a chemical fire drew the Filipino police and investigation authorities' attention, resulting in the arrest of one terrorist and seizure of a laptop containing the plans. One person was killed in the course of the plot — a Japanese passenger seated near a nitroglycerin bomb on Philippine Airlines Flight 434. The money handed down to the plotters originated from Al-Qaeda, the international Islamic jihadi organization then based in Sudan.
Selection.
Khalid al-Mihdhar and Nawaf al-Hazmi were both experienced and respected jihadists in the eyes of al-Qaeda leader, Osama bin Laden. Mihdhar and Hazmi both had prior experience fighting in Bosnia, and had trained during the 1990s at camps in Afghanistan. Both were so eager to participate in operations within the United States, that they obtained visas in April 1999. Once selected, Mihdhar and Hazmi were sent to the Mes Aynak training camp in Afghanistan. In late 1999, Hazmi, Attash, and Yemeni went to Karachi, Pakistan to see Mohammed, who instructed them on Western culture and travel; however, Mihdhar did not go to Karachi, instead returning to Yemen.
As for the pilots who would go on to participate in the attacks, three of them were original members of the Hamburg cell (Mohammed Atta, Marwan al-Shehhi and Ziad Jarrah). Following their training at Al-Qaeda training camps in Afghanistan, they were chosen by Bin Laden and Al-Qaeda's military wing due to their extensive knowledge of western culture and language skills, increasing the mission's operational security and its chances for success. The fourth intended pilot, Ramzi bin al-Shibh, a member of the Hamburg cell, was also chosen to participate in the attacks yet was unable to obtain a visa for entry into the United States. He was later replaced by Hani Hanjour, a Saudi national.
Mihdhar and Hazmi were also potential pilot hijackers, but did not do well in their initial pilot lessons in San Diego. Both were kept on as "muscle" hijackers, who would help overpower the passengers and crew, and allow the pilot hijackers to take control of the flights. In addition to Mihdhar and Hazmi, thirteen other muscle hijackers were selected in late 2000 or early 2001. All were from Saudi Arabia, with the exception of Fayez Banihammad, who was from the United Arab Emirates.
Hijacked aircraft.
American Airlines Flight 11 – World Trade Center - North Tower.
Bold letters note the hijackers who piloted the planes.
Hijackers: Mohamed Atta (Egyptian), Abdulaziz al-Omari (Saudi Arabian), Wail al-Shehri (Saudi Arabian), Waleed al-Shehri (Saudi Arabian), Satam al-Suqami (Saudi Arabian).
Two flight attendants called the American Airlines reservation desk during the hijacking. Betty Ong reported that "the five hijackers had come from first-class seats: 2A, 2B, 9A, 9C and 9B." Flight attendant Amy Sweeney called a flight services manager at Logan Airport in Boston and described them as Middle Eastern. She gave the staff the seat numbers and they pulled up the ticket and credit card information of the hijackers, identifying Mohamed Atta.
Mohamed Atta's voice was heard over the air traffic control system, broadcasting messages thought to be intended for the passengers.
United Airlines Flight 175 – Two World Trade Center.
Hijackers: Marwan al-Shehhi (United Arab Emirates), Fayez Banihammad (United Arab Emirates), Mohand al-Shehri (Saudi Arabian), Hamza al-Ghamdi (Saudi Arabian), Ahmed al-Ghamdi (Saudi Arabian).
A United Airlines mechanic was called by a flight attendant who stated the crew had been murdered and the plane hijacked.
American Airlines Flight 77 – Pentagon.
Hijackers: Hani Hanjour (Saudi Arabian), Khalid al-Mihdhar (Saudi Arabian), Majed Moqed (Saudi Arabian), Nawaf al-Hazmi (Saudi Arabian), Salem al-Hazmi (Saudi Arabian).
Two hijackers, Hani Hanjour and Majed Moqed were identified by clerks as having bought single, first-class tickets for Flight 77 from Advance Travel Service in Totowa, New Jersey with $1,842.25 in cash. Renee May, a flight attendant on Flight 77, used a cell phone to call her mother in Las Vegas. She said her flight was being hijacked by six individuals who had moved them to the rear of the plane. Unlike the other flights, there was no report of stabbings or bomb threats. According to the 9/11 Commission Report, it is possible that pilots were not stabbed to death and were sent to the rear of the plane. One of the hijackers, most likely Hanjour, announced on the intercom that the flight had been hijacked.
Passenger Barbara Olson called her husband, Theodore Olson, the Solicitor General of the United States, stating the flight had been hijacked and the hijackers had knives and box cutters. Two of the passengers had been on the FBI's terrorist-alert list: Khalid al-Mihdhar and Nawaf al-Hazmi.
United Airlines Flight 93.
Hijackers: Ziad Jarrah (Lebanese), Ahmed al-Haznawi (Saudi Arabian), Ahmed al-Nami (Saudi Arabian), Saeed al-Ghamdi (Saudi Arabian).
Passenger Jeremy Glick stated that the hijackers were Arabic-looking, wearing red headbands, and carrying knives.
Spoken messages from Ziad Jarrah intended for passengers, were also thought mistakenly broadcast over the air traffic control system:
Jarrah is also heard on the cockpit voice recorder. In addition, DNA samples submitted by his girlfriend were matched to remains recovered in Shanksville.
Investigation.
Before the attacks.
Before the attacks, FBI agent Robert Wright, Jr. had written vigorous criticisms of FBI's alleged incompetence in investigating terrorists residing within the United States. Wright was part of the Bureau's Chicago counter-terrorism task force and involved in project Vulgar Betrayal which was linked to Yasin al-Qadi.
According to James Bamford, the NSA had picked up communications of al-Mihdhar and al-Hazmi back in 1999, but had been hampered by internal bureaucratic conflicts between itself and the CIA, and did not do a full analysis of the information it passed on to the agency. For example; it only passed the first names on, Nawaf and Khalid.
Bamford also claims that the CIA's Alec Station (a unit assigned to bin Laden) knew that al-Mihdhar was planning to come to New York as far back as January 2000. Doug Miller, one of 3 FBI agents working inside the CIA station, tried to send a message (a CIR) to the FBI to alert them about this, so they could put al-Mihdhar on a watch list. His CIA boss, Tom Wilshire, deputy station chief, allegedly denied permission to Miller. Miller asked his associate Mark Rossini for advice; Rossini pressed Wilshire's deputy but was again rebuffed.
Bamford also claims that al-Mihdhar and Hazmi wound up living with Abdussattar Shaikh for a time to save money. Shaikh was, coincidentally, an FBI informant, but since they never acted suspiciously around him, he never reported them. The CIA Bangkok station told Alec Station that Hazmi had gone to Los Angeles. None of this information made it back to the FBI headquarters.
Attacks.
Within minutes of the attacks, the Federal Bureau of Investigation opened the largest FBI investigation in United States history, operation PENTTBOM. The suspects were identified within 72 hours because few made any attempt to disguise their names on flight and credit card records. They were also among the few non-U.S. citizens and nearly the only passengers with Arabic names on their flights, enabling the FBI to identify them using such details as dates of birth, known or possible residences, visa status, and specific identification of the suspected pilots. On September 27, 2001 the FBI released photos of the 19 hijackers, along with information about many of their possible nationalities and aliases. The suspected hijackers were from Saudi Arabia (fifteen hijackers), United Arab Emirates (two hijackers), Lebanon (one hijacker) and Egypt (one hijacker).
The passport of Satam al-Suqami was reportedly recovered "a few blocks from where the World Trade Center's twin towers once stood"; a passerby picked it up and gave it to a NYPD detective shortly before the towers collapsed. The passports of two other hijackers, Ziad Jarrah and Saeed al-Ghamdi, were recovered from the crash site of United Airlines Flight 93 in Pennsylvania, and a fourth passport, that of Abdulaziz al-Omari was recovered from luggage that did not make it onto American Airlines Flight 11.
According to the 9/11 Commission Report, 26 al-Qaeda terrorist conspirators sought to enter the United States to carry out a suicide mission. In the end, the FBI reported that there were 19 hijackers in all: five on three of the flights, and four on the fourth. On September 14, three days after the attacks, the FBI announced the names of 19 persons. After a controversy about an earlier remark, U.S. Homeland Secretary Janet Napolitano stated in May 2009 that the 9/11 Commission found that none of the hijackers entered the United States through Canada.
Nawaf al-Hazmi and Hani Hanjour, attended the Dar al-Hijrah Islamic Center in Falls Church, Virginia in early April 2001 where the Imam Anwar al-Awlaki preached. Through interviews with the FBI, it was discovered that Awlaki had previously met Nawaf al-Hazmi several times while the two lived in San Diego. At the time, Hazmi was living with Khalid al-Mihdhar, another 9/11 hijacker. The hijackers of the same plane often had very strong ties as many of them attended school together or lived together prior to the attacks
Cases of mistaken identity.
Soon after the attacks and before the FBI had released the pictures of all the hijackers, several reports claimed some of the men named as hijackers on 9/11 were alive. and had their identities stolen.

</doc>
<doc id="28046" url="https://en.wikipedia.org/wiki?curid=28046" title="Closings and cancellations following the September 11 attacks">
Closings and cancellations following the September 11 attacks

Many closings and cancellations followed the September 11 attacks, including major landmarks, buildings, restrictions on access to Lower Manhattan, and postponement or cancellation of major sporting and other events. Landmarks were closed primarily because of fears that they may be attacked. At some places, streets leading up to the institutions were also closed. When they reopened, there was heightened security. Many states declared a state of emergency.
Lower Manhattan.
Speaking at a press conference at 11:02am on the morning of the attacks, Mayor Giuliani told New Yorkers: "If you are south of Canal Street, get out. Walk slowly and carefully. If you can’t figure what else to do, just walk north." The neighborhood was covered in dust and debris, and electrical failures caused traffic light outages. Emergency vehicles were given priority to respond to ongoing fires, building collapses, and expected mass casualties. Over a million workers and residents south of Canal Street evacuated, and police stopped pedestrians from entering lower Manhattan. With subways shut down, vehicle traffic restricted, and tunnels closed, they mainly fled on foot, pouring over bridges and ferries to Brooklyn and New Jersey.
On September 12, vehicle traffic was banned south of 14th Street, subway stations south of Canal Street were bypassed, and pedestrians were not permitted below Chambers Street. Vehicle traffic below Canal Street was not allowed until October 13.
The New York Stock Exchange did not open on September 11 even as CNBC showed futures numbers early in the day. As Wall Street was covered in debris from the World Trade Center and suffered infrastructure damage, it remained closed until September 17.
Bridges and tunnels.
For at least a full day after the attacks, bridges and tunnels to Manhattan were closed to non-emergency traffic in both directions. Among other things, this interrupted scheduled deliveries of food and other perishables, leading to shortages in restaurants. From September 27, 2001, one-occupant cars were banned from crossing into Lower Manhattan from Midtown on weekday mornings in an effort to relieve some of the crush of traffic in the city (the morning rush hour lasts from 5:30 a.m. to 12:00 p.m.), caused largely by the increased security measures and closure of major vehicle and transit crossings.
Mass transit.
New York City Subway.
The tracks and station under the WTC were shut down within minutes of the first plane crash. All remaining New York City Subway service was suspended from 10:20am to 12:48pm. Immediately after the attacks and more so after the collapses of the Twin Towers, many trains running in Lower Manhattan lost power and had to be evacuated through the tunnels. Some trains had power but the signals did not, requiring special operating procedures to ensure safety.
The IRT Broadway – Seventh Avenue Line, which ran below the World Trade Center between Chambers Street and Rector Street, was the most crippled. Sections of the tunnel as well as Cortlandt Street were badly damaged and had to be rebuilt. Service was immediately suspended south of Chambers Street and then cut back to 14th Street. There was also subsequent flooding on the line south of 34th Street – Penn Station. After the flood was cleaned up, express service was able to resume on September 17 with trains running between Van Cortlandt Park – 242nd Street and 14th Street, making local stops north of and express stops south of 96th Street, while and trains made all stops in Manhattan (but bypassed all stations between Canal Street and Fulton Street until October 1). 1/9 skip-stop service was suspended.
After a few switching delays at 96th Street, service was changed on September 19. The train resumed local service in Manhattan, but was extended to New Lots Avenue in Brooklyn (switching onto the express tracks at Chambers Street) to replace the 3, which now terminated at 14th Street as an express. The train continued to make local stops in Manhattan and service between Chambers Street and South Ferry as well as skip-stop service remained suspended. Normal service on all four trains was restored September 15, 2002, but Cortlandt Street will remain closed while the World Trade Center site is redeveloped.
Service on the BMT Broadway Line was also disrupted because the tracks from the Montague Street Tunnel run adjacent to the World Trade Center and there were concerns that train movements could cause unsafe settling of the debris pile. Cortlandt Street station, which sits under Church Street, sustained significant damage in the collapse of the towers. It was closed until September 15, 2002 for removal of debris, structural repairs, and restoration of the track beds, which had suffered flood damage in the aftermath of the collapse. Starting September 17, 2001, and service was suspended and respectively replaced by the (which was extended to Coney Island – Stillwell Avenue via the BMT Montague Street Tunnel, BMT Fourth Avenue Line, and BMT Sea Beach Line) and the (also extended via Fourth Avenue to Bay Ridge – 95th Street). In Queens, the replaced the while the replaced the . All service on the BMT Broadway Line ran local north of Canal Street except for the <Q>, which ran normally from 57th Street to Brighton Beach via Broadway and Brighton Express. J/Z skip-stop service was suspended at this time. Normal service on all seven trains resumed on October 28.
The only subway line running between Midtown and Lower Manhattan was the IRT Lexington Avenue Line, which was overcrowded before the attacks and at crush density until the BMT Broadway Line reopened. Wall Street was closed until September 21.
The IND Eighth Avenue Line, which has a stub terminal serving the train under Five World Trade Center was not damaged, but covered in soot. E trains were extended to Euclid Avenue, Brooklyn, replacing the then suspended train (the and trains replaced it as the local north of 59th Street – Columbus Circle on nights and weekends, respectively. The train, which ran normally from 145th Street or Bedford Park Boulevard to 34th Street – Herald Square via Central Park West Local, also replaced C trains on weekdays). Service was cut back to Canal Street when C service resumed on September 21, but Chambers Street and Broadway – Nassau Street remained closed until October 1. World Trade Center remained closed until January 2002.
There were no reported casualties on the subway or loss of train cars, but an MCI coach bus was destroyed. Another bus was damaged, but repaired and is back in normal service with a special commemoration livery.
PATH.
PATH started evacuating passengers from its Manhattan trains and tracks within minutes of the first plane crash. The PATH station at World Trade Center was heavily damaged (a train parked in the station was crushed by debris and was removed during the excavation process in January 2002) and all service there was suspended. For several hours, PATH did not run any trains to Manhattan, but was able to restore service on the midtown line by the afternoon. Exchange Place was unusable since the switch configuration at the time required all trains to continue to World Trade Center. As a result, PATH ran a modified service: Hoboken-Journal Square, Hoboken-33rd Street, and Newark-33rd Street. Exchange Place reopened with modifications on June 29, 2003; a temporary station replacing World Trade Center opened on November 23.
Ferries.
Liberty Water Taxi and NY Waterway had a ferry terminal at the World Financial Center. As the area around the terminal was in the restricted zone, NY Waterway suspended service to the terminal with alternate service going to Midtown and Wall Street and Liberty Water Taxi service was suspended. Free ad-hoc ferry service to New Jersey, Brooklyn, and Queens began by evening, with about half a million evacuees transported by Circle Line Tours, NY Waterway, privately owned dining boats, tug boats, and at least one fire boat.
Buses.
MTA buses were temporarily suspended south of Canal Street, and MTA and NJ Transit buses were re-routed to serve passengers arriving in Brooklyn and New Jersey by walking and taking ferries out of Manhattan.
Intercity transit.
The Port Authority Bus Terminal was closed until September 13. Amtrak suspended all of its rail service nationwide until 6pm. Greyhound Bus Lines cancelled its bus service in the Northeast, but was running normally by September 13.
North American airspace.
The entire airspaces of the United States and Canada were closed ("ground stop") except for military, police, and medical flights. (The unprecedented implementation of Security Control of Air Traffic and Air Navigation Aids (SCATANA) was the first unplanned closure in the U.S.; military exercises known as Operation Skyshield had temporarily closed the airspace in the early 1960s.) Domestic planes were diverted to the nearest available airport. All non-military flights needed specific approval from President Bush and FAA. There were only a few dozen private aircraft which received the approval in that time period. United Airlines cancelled all flights worldwide temporarily. Grounded passengers and planes were searched for security threats. Amtrak was closed until 6pm on September 11, but by September 13 it had increased capacity 30% to deal with an influx of stranded plane passengers.
Many incoming international flights were diverted to Atlantic Canada to avoid proximity to potential targets in the U.S. and large cities in Canada. Some of the international flights that departed from South America were diverted to Mexico as well, however, its airspace was not shut down. On Thursday night, the New York area airports (JFK, LaGuardia, and Newark) were closed again and reopened the next morning. The only traffic from LaGuardia during the closure was a single C-9C government VIP jet, departing at approximately 5:15 p.m. on the 12th.
Civilian air traffic was allowed to resume on September 13, 2001, with stricter airport security checks, disallowing for example the box cutting knives that were used by the hijackers. (Reinforcement of cockpit doors began in October 2001, and was required for larger airlines by 2003.) First, the stranded planes were allowed to go to their intended destinations, then limited service resumed. The backlog of delayed passengers took several days to clear.
Due to a translation error controllers believed Korean Air Flight 85 might have been hijacked. Canadian Prime Minister Jean Chrétien and U.S. authorities ordered the United States Air Force to surround the plane and force it to land in Whitehorse, Canada and to shoot down the plane if the pilots did not cooperate. Alaska Governor Tony Knowles ordered the evacuation of large hotels and government buildings in Anchorage. At nearby Valdez, (also in Alaska), the U.S. Coast Guard ordered all tankers filling up with oil to head out to sea. Canadian officials evacuated all schools and large buildings in Whitehorse before the plane landed safely.
Precautionary building closings and evacuations.
Many businesses across the United States closed after the intentional nature of the events became clear, and many national landmarks and financial district skyscrapers were evacuated out of fear of further attacks.
Government and cultural cancellations and postponements.
In an atmosphere reminiscent of the assassination of John F. Kennedy in 1963, everyday life in the United States came to a standstill in the days after the September 11 attacks. There was a widespread perception immediately following the attacks that recreational events and sports were not appropriate out of respect for the dead and wounded. For this reason, as well as for reasons of perceived threat associated with large gatherings, many events were postponed or cancelled. Other events were also cancelled, postponed, or modified:

</doc>
<doc id="28047" url="https://en.wikipedia.org/wiki?curid=28047" title="Memorials and services for the September 11 attacks">
Memorials and services for the September 11 attacks

The first memorials to the victims of the September 11 attacks in 2001 began to take shape online, as hundreds of webmasters posted their own thoughts, links to the Red Cross and other rescue agencies, photos, and eyewitness accounts. Numerous online September 11 memorials began appearing a few hours after the attacks, although many of these memorials were only temporary. Around the world, U.S. embassies and consulates became makeshift memorials as people came out to pay their respects.
The Tribute in Light was the first major physical memorial at the World Trade Center site. A permanent memorial and museum, the National September 11 Memorial & Museum at the World Trade Center, were built as part of the design for overall site redevelopment. The Memorial consists of two massive pools set within the original footprints of the Twin Towers with waterfalls cascading down their sides. The names of the victims of the attacks are inscribed around the edges of the waterfalls. Other permanent memorials are being constructed around the world.
One of the places that saw many memorials and candlelight vigils was Pier A in Hoboken, New Jersey, directly across from the World Trade Center. There was also a memorial service on March 11, 2002, at dusk on Pier A when the Tribute in Light first turned on, marking the half-year anniversary of the terrorist attack. A permanent September 11 memorial for Hoboken, called Hoboken Island, was chosen in September 2004.
List.
Temporary memorials.
Soon after the attacks, temporary memorials were set up in New York and elsewhere.
Performances and benefits.
2001 events.
The Raoul Wallenberg Award was given to New York City in 2001 "For all of its citizens who searched for the missing, cared for the injured, gave comfort to loved ones of the missing or lost, and provided sustenance and encouragement to those who searched through the rubble at Ground Zero."
2002 and later events.
On February 3, 2002, during the Halftime Show of Super Bowl XXXVI, rock group U2 performed Where the Streets Have No Name, while the names of the victims were projected onto banners. Bono opened his jacket to reveal a U.S. flag pattern sewn in the inside lining.
On February 23, 2003, the 45th Annual Grammy Awards were held at Madison Square Garden and paid tribute to those who died during the 9/11 attacks, to whom the ceremony was dedicated. Ceremony host Bruce Springsteen performed "The Rising" at the Awards.
American country singer Darryl Worley paid tribute to the people with his 2003 single, "Have You Forgotten?" from the album of the same name.
Newark International Airport was renamed "Newark Liberty International Airport".
On September 11, 2002, representatives from over 90 countries came to Battery Park City as New York City Mayor Michael Bloomberg lit an eternal flame to mark the first anniversary of the attacks. Leading the dignitaries were Canadian Prime Minister Jean Chrétien, U.N. Secretary General Kofi Annan, Bloomberg, and Secretary of State Colin Powell. The same day, the Victims of Terrorist Attack on the Pentagon Memorial was dedicated at Arlington National Cemetery near the Pentagon. The memorial is dedicated to the five individuals at the Pentagon whose remains were never found, and the partial remains of another 25 victims are buried beneath the memorial. The names of the 184 victims of the Pentagon attack are inscribed on the memorial's side.
10th anniversary memorial services.
Many organizations held memorial services and events for the 10th anniversary of the attacks.
Annual commemorations.
Every year on September 11 a commemoration is held at the National September 11 Memorial. Family members read the names of victims of the attacks, as well as victims of the 1993 World Trade Center truck bombing. Elected officials and other dignitaries attend, but since the 2012 event they have not given speeches.
Memorial flags.
The National 9/11 Flag was made from a tattered remains of a American flag found by recovery workers in the early morning of September 12, 2001. It was hanging precariously from some scaffolding at a construction site next to Ground Zero. Because of safety reasons the flag could not be taken down until late October 2001. Charlie Vitchers, a construction superintendent for the Ground Zero cleanup effort, had a crew recover the flag. It was placed in storage for seven years.
The flag has made a number appearances across the country including a Boston Red Sox Game, a New York Giants Home Opener, and the USS New York Commissioning Ceremony. It also appeared on the CBS Evening News and on ABC World News Tonight "Persons of the Week."
The flag began a national tour on Flag day, which was on June 14, 2009. It will visit all 50 states where service heroes, veterans, and other honorees will each add stitching and material from other retired American flags in order to restore the original 13 stripes of the flag. The flag will have a permanent home at the National September 11 Memorial and Museum.
The 9-11 Remembrance Flag was created to be a permanent reminder of the thousands of people lost in the September 11 attacks. The purpose of keeping the memories of September 11 alive is not to be forever mourning, but for "learning from the circumstances and making every effort to prevent similar tragedies in our future." The flag is also meant to be a reminder of how the people of this country came together to help each other after the attacks. The red background of the flag represents the blood shed by Americans for their country. The stars represent the lost airplanes and their passengers. The blue rectangles stand for the twin towers and the white pentagon represents the Pentagon building. The blue circle symbolizes the unity of this country after the attacks.
The 9/11 National Remembrance Flag was designed by Stephan and Joanne Galvin soon after September 11, 2001. They wanted to do something to help and were inspired by a neighbor's POW/MIA flag. They wanted sell the flag so people would remember the September 11 attacks and in order to raise money for relief efforts. The blue represents the colors of the state flags that were involved in the attacks. The black represents sorrow for innocent lives lost. The four stars stand for the four planes that crashed and the lives lost, both in the crash and in the rescue efforts, as well as the survivors. The blue star is a representation of American Airlines Flight 77 and the Pentagon. The two white stars represent American Airlines Flight 11 and United Airlines flight 175, as well as the twin towers. The red star stands for United Flight 93 that crashed in Shanksville, Pennsylvania and all those who sacrifice their lives to protect the innocent. The colors of the stars represent the American flag. The four stars are touching each other and the blue parts of the flag in order to symbolize the unity of the people of the United States.
The National Flag of Honor and the National Flag of Heroes were created by John Michelotti for three main reasons: (1)"To immortalize the individual victims that were killed in the terrorist attacks of September 11, 2001." (2)"To give comfort to the families left behind knowing that their loved one will be forever honored and remembered." (2)"To create an enduring symbol, recognized by the world, of the human sacrifice that occurred on September 11, 2001."
The Flag of Honor and the Flag of Heroes are based on the American flag. They both have the names of all the innocent people who were killed in the September 11 attacks printed on the red and white stripes of the American Flag. Both flags have a white space across the bottom with the name of the flag and a description printed in black. The Flag of Honor reads: "This flag contains the names of those killed in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them" The Flag of Heroes reads: " This flag contains the names of the emergency service personnel who gave their lives to save others in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them."
The Flag of Honor and the Flag of Heroes were featured at the NYC 9/11 Memorial Field 5th Anniversary in Manhattan's Inwood Hill Park September 8–12, 2006. There 3,000 flags which represented those who died in the September 11 attacks. The flags were also featured on the msnbc Today Show and on ABC 13 News, Norfolk, VA.
The Remembrance Flag has a white background with large, black Roman numerals IX/XI in the center and four black stars across the top. The IX/XI are the Roman numerals for 9/11. The four stars represent World Trade Center North, World Trade Center South, the Pentagon, and Shanksville, PA.
The 10th Anniversary September 11 Memorial Flag was designed by Carrot-Top Industries, a privately owned company in Hillsborough, NC. The exclusive 9/11 memorial flag was designed with the two World Trade Towers set inside a pentagon decorated with a ribbon to commemorate all of the Americans that lost their lives on September 11, 2001.
Virtual memorials.
The growing popularity of virtual worlds such as Secondlife has led to the construction of permanent virtual memorials and exhibits. Examples include:

</doc>
<doc id="28051" url="https://en.wikipedia.org/wiki?curid=28051" title="Airport security repercussions due to the September 11 attacks">
Airport security repercussions due to the September 11 attacks

After the September 11 attacks, questions were raised regarding the effectiveness of airport security at the time, as all 19 hijackers involved in 9/11 managed to pass existing checkpoints and board the airplanes without incident. In the months and years following September 11, 2001, security at many airports worldwide was escalated to deter similar terrorist plots.
Changes in airport security.
Prior to September 11, 2001, airport screening was provided in the U.S. by private companies contracted by the airline or airport. In November 2001, the Transportation Security Administration (TSA) was introduced to takeover all of the security functions of the FAA, the airlines, and the airports. Among other changes introduced by TSA, bulletproof and locked cockpit doors became standard on commercial passenger aircraft.
In some countries, for example Sweden, Norway and Finland, there were no or only random security checks for domestic flights in year 2001 and before that. On or quickly after September 11, decisions were made to introduce full security checks there. It was immediately implemented where possible, but took 1-2 years to implement everywhere since terminals were often not prepared with room for it.
Improved security on aircraft.
Cockpit doors on many aircraft are now reinforced and bulletproof to prevent unauthorized access. Passengers are now prohibited from entering the cockpit during flight. Some aircraft are also equipped with CCTV cameras, so the pilots can monitor cabin activity. Pilots are now allowed to carry firearms, but they must be trained and licensed. In the U.S., more air marshals have been placed on flights to improve security.
Improved security screening.
On September 11, hijackers Khalid al-Mihdhar, Majed Moqed, Nawaf al-Hazmi and Salem al-Hazmi all set off the metal detector. Despite being "wanded" (scanned with a hand-held detector), the hijackers were passed through. Security camera footage later showed some hijackers had what appeared to be box cutters clipped to their back pockets. Box cutters and similar small knives were allowed onboard aircraft at the time.
Airport checkpoint screening has been significantly tightened since 2001, and security personnel are more thoroughly trained to detect weapons or explosives. In addition to standard metal detectors, many U.S. airports now employ full-body scanning machines, in which passengers are essentially X-rayed to check for potential hidden weapons or explosives on their persons. Initially, early body scanners provoked quite a bit of controversy because the images produced by the machines were deemed graphic and intrusive. Many considered this an invasion of personal privacy, as TSA screeners were essentially shown an image of each passenger's naked body. Newer body scanners have since been introduced which do not produce an image, but rather alert TSA screeners of areas on the body where an unknown item or substance may be hidden. A TSA security screener then inspects the indicated area(s) manually.
Identification checks.
On September 11, some hijackers lacked proper ID, yet they were allowed to board. After 9/11, all passengers 18 years or older must now have valid, government-issued identification in order to fly. Airports may check the ID of any passenger at any time to ensure the details on the ID match those on the printed boarding pass. Only under exceptional circumstances may an individual fly without a valid ID. If approved for flying without an ID, the individual will be subject to extra screening of their person and their carry-on items. TSA does not have the capability to conduct background checks on passengers at checkpoints. Sensitive areas in airports, including airport ramps and operational spaces, are restricted from the general public. Called a SIDA (Security Identification Display Area) in the U.S., these spaces require special qualifications to enter.
A European Union regulation demanded airlines to make sure the same person checking in luggage also boards the aircraft. The method of implementing this was demanding ID from every passenger having check-in luggage, both when checking in a bag and before boarding.
Criticism.
With regard to the 2015 Germanwings flight 9525 crash incident, some have stated that security features added to commercial airliners after 9/11 actually work against the safety of such planes.
Lawsuit.
In 2003 John Gilmore sued United Airlines, Southwest Airlines and U.S. Attorney General John Ashcroft, arguing that requiring passengers to show identification before boarding domestic flights is tantamount to an internal passport, and is unconstitutional. Gilmore lost the case, known as "Gilmore v. Gonzales", and an appeal to the U.S. Supreme Court was denied.

</doc>
<doc id="28061" url="https://en.wikipedia.org/wiki?curid=28061" title="U.S. government response to the September 11 attacks">
U.S. government response to the September 11 attacks

The response of the U.S. government to the September 11 attacks sparked investigations into the motivations and execution of the attacks, as well as the ongoing War on Terrorism in Afghanistan The response included funds for affected families, plans for the War on Terrorism, rebuilding of Lower-East Manhattan, and the invasion and investigation of Iraq and Afghanistan.
Rescue, recovery, and compensation.
Within hours of the attack, a massive search and rescue (SAR) operation was launched, which included over 350 search and rescue dogs. Initially, only a handful of wounded people were found at the site, and in the weeks that followed it became evident that there weren't any survivors to be found.
Rescue and recovery efforts took months to complete. It took several weeks to simply put out the fires burning in the rubble of the buildings, and the clean-up was not completed until May, 2002. Temporary wooden "viewing platforms" were set up for tourists to view construction crews clearing out the gaping holes where the towers once stood. All of these platforms were closed on May 30, 2002.
Many relief funds were immediately set up to assist victims of the attacks, with the task of providing financial assistance to the survivors and the families of victims. By the deadline for victim's compensation, September 11, 2003, 2,833 applications had been received from the families of those killed.
War on Terrorism.
In the aftermath of the attacks, many U.S. citizens held the view that the attacks had "changed the world forever." The Bush administration announced a war on terrorism, with the goal of bringing Osama bin Laden and al-Qaeda to justice and preventing the emergence of other terrorist networks. These goals would be accomplished by means including economic and military sanctions against states perceived as harboring terrorists and increasing global surveillance and intelligence sharing. Immediately after the September 11 attacks U.S. officials speculated on possible involvement by Saddam Hussein. Although unfounded, the association contributed to public support for the 2003 invasion of Iraq. On October 7, 2001, the War in Afghanistan began when U.S and British forces initiated aerial bombing campaigns in Afghanistan targeting Taliban and Al-Qaeda camps, then later invaded Afghanistan with ground troops of the Special Forces. This was the second-largest operation of the U.S. Global War on Terrorism outside of the United States, and the largest directly connected to terrorism, resulting in the overthrow of Taliban rule in Afghanistan, by a U.S.-led coalition. The U.S. was not the only nation to increase its military readiness, with other notable examples being the Philippines and Indonesia, countries that have their own internal conflicts with Islamist terrorism. 
Because the attacks on the United States were judged to be within the parameters of its charter, NATO declared that Article 5 of the NATO agreement was satisfied on September 12, 2001, making the US war on terrorism the first time since its inception that NATO would actually participate in a "hot" war.
Arrests.
Following the attacks, 762 suspects were taken into custody in the United States. On December 12, 2001, Fox News reported that some 60 Israelis were among them. Federal investigators were reported to have described them as part of a long-running effort to spy on American government officials. A "handful" of these Israelis were described as active Israeli military or intelligence operatives.
In a letter to the editor, Ira Glaser, former head of the ACLU, claimed that none of those 762 detainees were charged with terrorism. "The Justice Department inspector general's report implies more than the violation of the civil liberties of 762 non-citizens. It also implies a dysfunctional and ineffective approach to protecting the public after Sept. 11, 2001... No one can be made safer by arresting the wrong people".
Domestic response.
Immediately after opening the hunt on Osama bin Laden, President Bush also visited the Islamic Center of Washington and asked the public to view Arabs and Muslims living in the United States as American patriots.
Congress passed and President Bush signed the Homeland Security Act of 2002, creating the Department of Homeland Security, representing the largest restructuring of the U.S. government in contemporary history. Congress passed the USA PATRIOT Act, stating that it would help detect and prosecute terrorism and other crimes. Civil liberties groups have criticized the PATRIOT Act, saying that it allows law enforcement to invade the privacy of citizens and eliminates judicial oversight of law-enforcement and domestic intelligence gathering. The Bush Administration also invoked 9/11 as the reason to have the National Security Agency initiate a secret operation, "to eavesdrop on telephone and e-mail communications between the United States and people overseas without a warrant."
On June 6, 2002, Attorney General Ashcroft proposed regulations that would create a special registration program that required males aged 16 to 64 who were citizens of designated foreign nations resident in the U.S. to register with the Immigration and Naturalization Service (INS), have their identity verified, and be interviewed, photographed and fingerprinted. Called the National Security Entry-Exit Registration System (NSEERS), it comprised two programs, the tracking of arrivals and departures on the one hand, and voluntary registrations of those already in the U.S., known as the "call-in" program. The DOJ acted under the authority of the Immigration and Nationality Act of 1952, which had authorized a registration system but was allowed to lapse in the 1980s because of budget concerns. Ashcroft identified those required to register as "individuals of elevated national security concern who stay in the country for more than 30 days."
The processing of arrivals as part of their customs screening began in October 2002. It first focused on arrivals from Iran, Iraq, Libya, Sudan,and Syria. It handled 127,694 people before being phased out as universal screening processes were put in place.
The "call-in" registrations began in December. It initially applied to nationals of five countries, Iran, Iraq, Syria, Libya and Sudan, who were required to register by December 16. On November 6, the Department of Justice set a deadline of January 10 for those from another 13 countries: Afghanistan, Algeria, Bahrain, Eritrea, Lebanon, Morocco, North Korea, Oman, Qatar, Somalia, Tunisia, the United Arab Emirates, and Yemen. On December 16, it set a deadline of February 21 for those from Armenia, Pakistan and Saudi Arabia. It later included those from Egypt, Jordan, Kuwait, Indonesia, and Bangladesh. It eventually included citizens of 23 nations with majority Muslim populations, as well as Eritrea, which has a large Muslim population, and North Korea. Failure to register at an INS office resulted in deportation. Those found in violation of their visa were allowed to post bail while processed for deportation. The program registered 82,880 people, of whom 13,434 were found in violation of their visas. Because nationality and Muslim affiliation are only approximations for one another, the program extended to such non-Muslims as Iranian Jews. The program was phased out beginning in May 2003.
Government officials pronounced the program a success. They said in the course of the combined programs, registration upon entry and that of residents, they had arrested 11 suspected terrorists, found more than 800 criminal suspects or deportable convicts, and identified more than 9,000 illegal aliens. DOJ general counsel Kris Kobach said: "I regard this as a great success. Sept. 11th awakened the country to the fact that weak immigration enforcement presents a huge vulnerability that terrorists can exploit." DOJ officials said fewer than 5% of those who came in to INS offices to register were detained. James W. Ziglar, former head of INS who left the agency early in 2002, said his objections to the program were proven correct: "The people who could be identified as terrorists weren't going to show up. This project was a huge exercise and caused us to use resources in the field that could have been much better deployed."
Investigations.
Collapse of the World Trade Center.
A federal technical building and fire safety investigation of the collapses of the Twin Towers was conducted by the United States Department of Commerce's National Institute of Standards and Technology (NIST). The goals of this investigation, completed on April 6, 2005, were to investigate the building construction, the materials used, and the technical conditions that contributed to the outcome of the WTC disaster. The investigation was to serve as the basis for:
The report concludes that the fireproofing on the Twin Towers' steel infrastructures was blown off by the initial impact of the planes and that, if this had not occurred, the towers would likely have remained standing. The fires weakened the trusses supporting the floors, making the floors sag. The sagging floors pulled on the exterior steel columns to the point where exterior columns bowed inward. With the damage to the core columns, the buckling exterior columns could no longer support the buildings, causing them to collapse. In addition, the report asserts that the towers' stairwells were not adequately reinforced to provide emergency escape for people above the impact zones. NIST stated that the final report on the collapse of 7 WTC will appear in a separate report.
Internal review of the CIA.
The Inspector General of the CIA conducted an internal review of the CIA's performance prior to 9/11, and was harshly critical of senior CIA officials for not doing everything possible to confront terrorism, including failing to stop two of the 9/11 hijackers, Nawaf al-Hazmi and Khalid al-Mihdhar, as they entered the United States and failing to share information on the two men with the FBI.
9/11 Commission Report.
The "National Commission on Terrorist Attacks Upon the United States" (9/11 Commission), chaired by former New Jersey Governor Thomas Kean, was formed in late 2002 to prepare a full and complete account of the circumstances surrounding the attacks, including preparedness for, and the immediate response to, the attacks. On July 22, 2004, the report was released. The commission has been subject to criticism.
Civilian aircraft grounding.
For the first time in history, all nonemergency civilian aircraft in the United States and several other countries including Canada were immediately grounded, stranding tens of thousands of passengers across the world. The order was given at 9:42 by Federal Aviation Administration Command Center national operations manager Ben Sliney. According to the 9/11 Commission Report, "This was an unprecedented order. The air traffic control system handled it with great skill, as about 4,500 commercial and general aviation aircraft soon landed without incident.
Invocation of the continuity of government.
Contingency plans for the continuity of government and the evacuation of leaders were implemented almost immediately after the attacks. Congress, however, was not told that the US was under a continuity of government status until February 2002.

</doc>
<doc id="28064" url="https://en.wikipedia.org/wiki?curid=28064" title="Financial assistance following the September 11 attacks">
Financial assistance following the September 11 attacks

Charities and relief agencies raised over $657 million in the three weeks following the September 11, 2001 attacks, the vast bulk going to immediate survivors and victims' families.
Government assistance.
In the morning hours of September 21, 2001, the Congress approved a bill to aid the airline industry and establish a federal fund for victims. The cost of the mostly open-ended fund reached $7 billion (the average payout was $1.8 million per family). Victims of earlier terrorist attacks, including those linked to al-Qaida, were not included in the fund—nor were those who would not surrender the right to hold the airlines legally responsible.
American Red Cross.
From the donations to the Emergency Relief Fund, as of 19 November 2001, the American Red Cross granted 3,165 checks to 2,776 families totaling $54.3 million.
172,612 cases were referred to mental health contacts. The 866-GET INFO number received 29,820 calls. As of 3:10 p.m. November 20, 2001, there had been 1,592,295 blood donations since September 11.
"Fire Donations" took charitable contributions on behalf of firefighters, EMS, and rescue workers.
Emergency supplies.
On Thursday and Friday, September 14–15 September 2001, various relief supplies for the World Trade Center relief effort were collected from the New York City area, and dropped off at the Javits Convention Center or at a staging area at Union Square. By Saturday morning, enough supplies (and volunteers) were collected.
Memorial funds.
Many families and friends of victims have set up memorial funds and projects to give back to their communities and change the world in honor of their loved ones' lives. Examples include:

</doc>
<doc id="28066" url="https://en.wikipedia.org/wiki?curid=28066" title="Rescue and recovery effort after the September 11 attacks on the World Trade Center">
Rescue and recovery effort after the September 11 attacks on the World Trade Center

The local, state, federal and global reaction to the September 11 attacks on the World Trade Center was unprecedented. The equally unsurpassed events of that day elicited the largest response of local emergency and rescue personnel to assist in the evacuation of the two towers and also contributed to the largest loss of the same personnel when the towers collapsed. After the attacks, the media termed the World Trade Center site "Ground Zero", while rescue personnel referred to it as "the Pile".
In the ensuing recovery and cleanup efforts, personnel related to metalwork and construction professions would descend on the site to offer their services and remained until the site was cleared in May 2002. In the years since, investigations and studies have examined effects upon those who participated, noting a variety of afflictions attributed to the debris and stress.
Building evacuation.
After American Airlines Flight 11 crashed into the North Tower (1 WTC) of the World Trade Center, a standard announcement was given to tenants in the South Tower (2 WTC) to stay put and that the building was secure. However, many defied those instructions and proceeded to evacuate the South Tower (most notably, Rick Rescorla, Morgan Stanley Security Director, evacuated 2687 of the 2700 Morgan Stanley employees in the building). All people evacuating were ordered through a door on the mezzanine level that led to a bridge to another building, and everyone was evacuated through the neighboring building. The firefighters in charge did not want anyone going through the front doors at first due to falling debris, and then because of falling people who had jumped from the towers.
Standard evacuation procedures for fires in the World Trade Center called for evacuating only the floors immediately above and below the fire, as simultaneous evacuation of up to 50,000 workers would be chaotic.
Emergency response.
Firefighters.
Firefighters from the New York City Fire Department rushed to the World Trade Center minutes after the first plane struck the north tower. Chief Joseph Pfeifer and his crew with Battalion 1 were among the first on the scene. At 8:50 a.m., an Incident Command Post was established in the lobby of the North Tower. By 9:00 a.m., shortly before United Airlines Flight 175 hit the South Tower, the FDNY chief had arrived and took over command of the response operations. Due to falling debris and safety concerns, he moved the incident command center to a spot located across West Street, but numerous fire chiefs remained in the lobby which continued to serve as an operations post where alarms, elevators, communications systems, and other equipment were operated. The initial response by the FDNY was on rescue and evacuation of building occupants, which involved sending firefighters up to assist people that were trapped in elevators and elsewhere. Firefighters were also required to ensure all floors were completely evacuated.
Numerous staging areas were set up near the World Trade Center, where responding fire units could report and get deployment instructions. However, many firefighters arrived at the World Trade Center without stopping at the staging areas. As a result, many chiefs could not keep track of the whereabouts of their units. Numerous firefighters reported directly to the building lobbies, and were ordered by those commanding the operating post to proceed into the building.
Problems with radio communication caused commanders to lose contact with many of the firefighters who went into the buildings. The repeater system in the World Trade Center, which was required for portable radio signals to transmit reliably, was malfunctioning after the impact of the planes. As a result, firefighters were unable to report to commanders on their progress, and were unable to hear evacuation orders. Also, many off-duty firefighters arrived to help, without their radios. FDNY commanders lacked communication with the NYPD, who had helicopters at the scene, or with Emergency Medical Service (EMS) dispatchers. The firefighters on the scene also did not have access to television reports or other outside information, which could help in assessing the situation. When the South Tower collapsed at 9:59 a.m., firefighters in the North Tower were not aware of exactly what had happened. The battalion chief in the North Tower lobby immediately issued an order over the radio for firefighters in the tower to evacuate, but many did not hear the order, due to the faulty radios. Because of this, 343 firefighters died in the collapse of the towers.
The command post located across West Street was taken out when the South Tower collapsed, making command and control even more difficult and disorganized . When the North Tower collapsed, falling debris killed Peter Ganci, the FDNY chief. Following the collapse of the World Trade Center, a command post was set up at a firehouse in Greenwich Village.
The FDNY deployed 200 units (half of all units) to the site, with more than 400 firefighters on the scene when the buildings collapsed. This included a total of 121 engine companies, 62 ladder companies, and other special units. The FDNY also received assistance from fire departments in Nassau, Suffolk, Westchester County, and other neighboring jurisdictions, but with limited ability to manage and coordinate efforts.
Besides assisting with recovery operations at Ground Zero, volunteer firefighters from Long Island and Westchester manned numerous firehouses throughout the city to assist with other fire and emergency calls.
Doctors, EMTs, and other medical staff.
FDNY Emergency medical technicians (EMTs), along with 9-1-1 system ambulances operated by voluntary hospitals and volunteer ambulance corps, began arriving at 8:53 a.m., and quickly set up a staging area outside the North Tower, at West Street, which was quickly moved over to the corner of Vesey and West Streets. As more EMTs responded to the scene, five triage areas were set up around the World Trade Center site. EMS chiefs experienced difficulties communicating via their radios, due to the overwhelming volume of radio traffic. At 9:45, an additional dispatch channel was set aside for use by chiefs and supervisors only, but many did not know about this and continued to operate on the other channel. The communication difficulties meant that commanders lacked good situational awareness.
Dispatchers at the 9-1-1 call center, who coordinate EMS response and assign units, were overwhelmed with incoming calls, as well as communications over the radio system. Dispatchers were unable to process and make sense of all the incoming information, including information from people trapped in the towers, about conditions on the upper floors. Overwhelmed dispatchers were unable to effectively give instructions and manage the situation.
EMS personnel were in disarray after the collapse of the South Tower at 9:59 a.m. Following the collapse of the North Tower at 10:28 a.m., EMS commanders regrouped on the North End of Battery Park City, at the Embassy Suites Hotel. Around 11:00 a.m., EMS triage centers were relocated and consolidated at the Chelsea Piers and the Staten Island Ferry Terminal. Throughout the early afternoon, the soundstages at the pier were separated into two areas, one for the more seriously injured and one for the walking wounded. On the acute side, multiple makeshift tables, each with a physician, nurse, and other health care workers, and non-emergency service volunteers, were set up for the arrival of mass casualties.
Supplies, including equipment for airway and vascular control, were obtained from neighboring hospitals. Throughout the afternoon, local merchants arrived to donate food. Despite this, few patients arrived for treatment, the earliest at about 5 p.m., and were not seriously injured, being limited to smoke inhalation. An announcement was made around 6–7 p.m. that a second shift of providers would cover the evening shift, and that an area was being set up for the day personnel to sleep. Soon after, when it was realized that few would have survived the collapse and be brought to the piers, many decided to leave and the area was closed down.
Police.
The New York City Police Department quickly responded with the Emergency Service Units (ESU) and other responders after the crash of American Airlines Flight 11 into the North Tower. The NYPD set up its incident command center at Church Street and Vesey Street, on the opposite side of the World Trade Center from where the FDNY was commanding its operations. NYPD helicopters were soon at the scene, reporting on the status of the burning buildings. When the buildings collapsed, 23 NYPD officers were killed, along with 37 Port Authority Police Department officers. The NYPD helped facilitate the evacuation of civilians out of Lower Manhattan, including approximately 5,000 civilians evacuated by the Harbor Unit to Staten Island and to New Jersey. In ensuing days, the police department worked alternating 12-hour shifts to help in the rescue and recovery efforts.
Coast Guard, maritime industry, individual boat owners.
Immediately after the first attack, the captains and crews of a large number of local boats steamed into the attack zone to assist in evacuation.</ref> These ships had responded by a request from the U.S. Coast Guard to help evacuate those stranded on Manhattan Island. Others, such as the "John J. Harvey", provided supplies and water, which became urgently needed after the Towers' collapse severed downtown water mains.</ref> Estimates of the number of people evacuated by water from Lower Manhattan that day in the eight-hour period following the attacks range from 500,000 to 1,000,000.</ref> Norman Mineta, Secretary of Transportation during the attacks, called the efforts "the largest maritime evacuation conducted in the United States." The evacuation was the largest maritime evacuation in history by most estimates, passing the nine-day evacuation of Dunkirk during World War II. As many as 2,000 people injured in the attacks were evacuated by this means.</ref>
Amateur radio.
Amateur radio played a role in the rescue and clean-up efforts. Amateur radio operators established communications, maintained emergency networks, and formed bucket brigades with hundreds of other volunteer personnel. Approximately 500 amateur radio operators volunteered their services during the disaster and recovery.
The New Jersey Legislature honored the role of Amateur Radio operators in a proclamation on December 12, 2002.
Note: "Government exhibits are from the trial of Zacarias Moussaoui"
Search and rescue efforts.
On the day following the attacks, 11 people were rescued from the rubble, including six firefighters and three police officers. One woman was rescued from the rubble, near where a West Side Highway pedestrian bridge had been. Two PAPD officers, John McLoughlin and Will Jimeno, were also rescued. Discovered by former U.S. Marines Jason Thomas and Dave Karnes, McLoughlin and Jimeno were pulled out alive after spending nearly 24 hours beneath 30 feet of rubble. Their rescue was later portrayed in the Oliver Stone film, "World Trade Center".
Some firefighters and civilians who survived made cell phone calls from voids beneath the rubble, though the amount of debris made it difficult for rescue workers to get to them.
By Wednesday night, 82 deaths had been confirmed by officials in New York City.
Rescue efforts were paused numerous times in the days after the attack, due to concerns that nearby buildings, including One Liberty Plaza, were in danger of collapsing.
Recovery efforts.
The search and rescue effort in the immediate aftermath at the World Trade Center site involved ironworkers, structural engineers, heavy machinery operators, asbestos workers, boilermakers, carpenters, cement masons, construction managers, electricians, insulation workers, machinists, plumbers and pipefitters, riggers, sheet metal workers, steelworkers, truckers and teamsters, American Red Cross volunteers, and many others.
Lower Manhattan, south of 14th Street, was off-limits, except for rescue and recovery workers. There were also about 400 working dogs, the largest deployment of dogs in the nation's history.
Organization.
New York City Office of Emergency Management was the agency responsible for coordination of the City's response to the attacks. Headed by then-Director Richard Sheirer, the agency was forced to vacate its headquarters, located in 7 World Trade Center, within hours of the attack. The building later collapsed. OEM reestablished operations temporarily at the police academy, where Mayor Giuliani gave many press conferences throughout the afternoon and evening of September 11. By Friday, rescue and reliefs were organized and administered from Pier 92 on the Hudson River.
Volunteers quickly descended on Ground Zero to help in the rescue and recovery efforts. At Jacob Javits Convention Center, thousands showed up to offer help, where they registered with authorities. Construction projects around the city came to a halt, as workers walked off the jobs to help at Ground Zero. Ironworkers, welders, steel burners, and others with such skills were in high demand. By the end of the first week, over one thousand ironworkers from across North America had arrived to help, along with countless others.
The New York City Department of Design & Construction oversaw the recovery efforts. Beginning on September 12, the Structural Engineers Association of New York (SEAoNY) became involved in the recovery efforts, bringing in experts to review the stability of the rubble, evaluate safety of hundreds of buildings near the site, and designing support for the cranes brought in to clear the debris. The City of New York hired the engineering firm, LZA-Thornton Tomasetti, to oversee the structural engineering operations at the site.
To make the effort more manageable, the World Trade Center site was divided into four quadrants or zones. Each zone was assigned a lead contractor, and a team of three structural engineers, subcontractors, and rescue workers.
The Federal Emergency Management Agency (FEMA), the United States Army Corps of Engineers, the Occupational Safety and Health Administration (OSHA), and the New York City Office of Emergency Management (OEM) provided support. Forestry incident management teams (IMTs) also provided support beginning in the days after the attacks to help manage operations.
A nearby Burger King restaurant was used as a center for police operations. Given that workers worked at the site, or "The Pile", for shifts as long as twelve hours, a specific culture developed at the site, leading to workers developing their own argot.
Debris removal.
"The Pile" was the term coined by the rescue workers to describe the tons of wreckage left from the collapse of the World Trade Center. They avoided the use of "ground zero", which describes the epicenter of a bomb explosion.
Numerous volunteers organized to form "bucket brigades", which passed 5-gallon buckets full of debris down a line to investigators, who sifted through the debris in search of evidence and human remains. Ironworkers helped cut up steel beams into more manageable sizes for removal. Much of the debris was hauled off to the Fresh Kills Landfill on Staten Island where it was searched and sorted.
Reuse of steel.
Some of the steel was reused for memorials. New York City firefighters donated a cross made of steel from the World Trade Center to the Shanksville Volunteer Fire Company. The beam, mounted atop a platform shaped like the Pentagon, was erected outside the Shanksville's firehouse near the crash site of United Airlines Flight 93.
Twenty-four tons of the steel used in construction of USS "New York" (LPD-21) came from the small amount of rubble from the World Trade Center preserved for posterity.
Hazards.
Hazards at the World Trade Center site included a diesel fuel tank buried seven stories below. Approximately 2,000 automobiles that had been in the parking garage also presented a risk, with each containing, on average, at least five gallons of gasoline. Once recovery workers reached down to the parking garage level, they found some cars that had exploded and burned. The United States Customs Service, which was housed in 6 World Trade Center, had 1.2 million rounds of ammunition and weapons in storage in a third-floor vault, to support their firing range.
Morale.
In the hours immediately after the attacks on the World Trade Center, three firefighters raised an American flag over the rubble. The flag was taken from a yacht, and the moment, which was captured on a well-known photograph, evoked comparisons to the iconic Iwo Jima photograph. Morale of rescue workers was boosted on September 14, 2001 when President George W. Bush paid a visit to Ground Zero. Standing with retired firefighter Bob Beckwith, Bush addressed the firefighters and rescue workers with a bullhorn and thanked them. Bush remarked, "I'm shocked at the size of the devastation, It's hard to describe what it's like to see the gnarled steel and broken glass and twisted buildings silhouetted against the smoke. I said that this was the first act of war on America in the 21st century, and I was right, particularly having seen the scene." After some workers shouted that they could not hear the President, Bush famously responded by saying "I can hear you! The rest of the world hears you. And the people who knocked these buildings down will hear all of us soon!"
At some point, rescue workers realized that they were not going to find any more survivors. After a couple of weeks, the conditions at Ground Zero remained harsh, with lingering odors of decaying human remains and smoke. Morale among workers was boosted by letters they received from children around the United States and the world, as well as support from thousands of neighbors in TriBeCa and other Lower Manhattan neighborhoods.
This support continued to spread and eventually led to the founding of over 250 non-profit organizations of which raised almost $700 million within their first two years of operation. Jay Winukne, founder of the nonprofit MyGoodDeed explained what the founders experienced after establishment, "the amazing spirit of compassion and community service that grew out of 9/11 was so natural and evident immediately after the attacks." 
By 2012, many of the 250 plus organizations had disbanded due to lack of funding as the years progressed. Of the ones that remain, a handful remained functioning for those who remain in need. One of these organizations, Tuesday’s Children, was founded the day after September 11 in hopes of supporting the children immediately affected by the attacks. The founder of this non-profit, David Weild IV, now calls them one of the “last men standing” in that they are now one of the few remaining organizations who “provide direct services for what social-service groups and survivors of the attacks call the ‘9-11 Community.’”
Other notable non-profits who are “still standing” include:
Military support.
Civil Air Patrol.
Immediately following the attacks, members of the Civil Air Patrol (CAP) were called up to help respond. Northeast Region placed their region personnel and assets on alert mere moments after he learned of the attack. With the exception of CAP, civilian flights were grounded by the Federal Aviation Administration. CAP flew aerial reconnaissance missions over Ground Zero, to provide detailed analysis of the wreckage and to aid in recovery efforts, including transportation of blood donations.
National Guard.
Elements of the New York Army National Guard's 1-101st Cavalry (Staten Island), 258th Field Artillery, and 69th Infantry Regiment based in Manhattan were the first military force to secure Ground Zero on September 11th. The 69th Infantry's armory on Lexington Avenue became the Family Information Center to assist persons in locating missing family members.
The National Guard supplemented the NYPD and FDNY, with 2,250 guard members on the scene by the next morning. Eventually thousands of New York Army and Air National Guardsmen participated in the rescue/recovery efforts. They conducted site security at the WTC, and at other locations. They provided the NYPD with support for traffic control, and they participated directly in recovery operations providing manpower in the form of "bucket brigades" sorting through the debris by hand.
Additionally service members provided security at a variety of location throughout the city and New York State to deter further attacks and reassure the public.
Members of the Air National Guard's 109th Airlift Wing out of Scotia, and Syracuse's 174th Fighter Wing immediately responded to New York City, setting up camp at places such as Fort Hamilton. Mostly civil engineers, firefighters and military police, they greatly aided in the clean-up effort. F-16s from the 174th Fighter Wing also ramped up their flying sorties and patrolled the skies.
U.S. Marine Corps.
U.S. Marines were also present to assist in the rescue efforts. No official numbers of men who helped out was released but there was evidence that they were there.
Films such as 2006 docudrama "World Trade Center" talked of two Marines who rescued two trapped police officers in the rubble. US Marines were HQ at 340 Westside Hwy Bloomberg News Building. The Commanding Officer was Navy Commander Hardy, and Executive Officer was Maj Priester. These two oversaw 110 military personnel of various branches, various police departments and EMTs.
U.S. Navy.
U.S. Navy deployed a hospital ship USNS "Comfort" (T-AH-20) to Pier 92 in Manhattan. Crew members provided food and shelter for more than 10,000 relief workers. Comfort's 24-hour galley also provided an impressive 30,000 meals. Its medical resources were also used to provide first-aid and sick call services to nearly 600 people. The ship's psychological response team also saw more than 500 patients.
Handling of cleanup procedure.
A May 14, 2007, "New York Times" article, "Ground Zero Illness Clouding Giuliani's Legacy," gave the interpretation that thousands of workers at Ground Zero have become sick and that "many regard Mr. Giuliani's triumph of leadership as having come with a human cost." The article reported that the mayor seized control of the cleanup of Ground Zero, taking control away from established federal agencies, such as the Federal Emergency Management Agency, the U.S. Army Corps of Engineers and the Occupational Safety and Health Administration. He instead handed over responsibility to the "largely unknown" city Department of Design and Construction. Documents indicate that the Giuliani administration never enforced federal requirements requiring the wearing of respirators. Concurrently, the administration threatened companies with dismissal if cleanup work slowed.
Workers at the Ground Zero pit worked without proper respirators. They wore painters' masks or no facial covering. Specialists claim that the only effective protection against toxins, such as airborne asbestos, is a special respirator. New York Committee for Occupational Safety and Health industrial hygienist David Newman said, "I was down there watching people working without respirators." He continued, "Others took off their respirators to eat. It was a surreal, ridiculous, unacceptable situation."
The local EPA office sidelined the regional EPA office. Dr. Cate Jenkins, a whistle-blower EPA scientist, said that on September 12, 2001, a regional EPA office offered to dispatch 30 to 40 electron microscopes to the WTC pit to test bulk dust samples for the presence of asbestos fibers. Instead, the local office chose the less effective polarized light microscopy testing method. Dr. Jenkins alleged that the local office refused, and said, "We don't want you fucking cowboys here. The best thing they could do is reassign you to Alaska."
Health effects.
There were many health problems caused by the toxins. 99% of exposed firefighters reported at least one new respiratory problem while working at the World Trade Center site that they had not experienced before. Chronic airway disease is the main lung injury among firefighters who were exposed to toxins during 9/11. Six years after the attacks, among those who never smoked, approximately 13% of firefighters and 22% of EMS had lungs that did not function as well as others around the same age. Steep declines in pulmonary lung function has been a problem since first detected among firefighters and EMS within a year of 9/11 have persisted. 
Increasing numbers of Ground Zero workers are getting illnesses, such as cancer. Between September 11, 2001 through 2008, there were 263 new cases of cancer found in 8,927 male firefighters who responded to 9/11 attacks. This number is 25 more than what is expected from men from a similar age group and race. There is a 19% increase in cancer overall, between firefighters who responded to the attacks and those who were not exposed to toxins from responding to the attacks on September 11.
On January 30, 2007, Ground Zero workers and groups such as Sierra Club and Unsung Heroes Helping Heroes met at the Ground Zero site and urged President George Bush to spend more money on aid for sick Ground Zero workers. They said that the $25 million that Bush promised for the ill workers was inadequate. A Long Island iron-worker, John Sferazo, at the protest rally said, "Why has it taken you 5½ years to meet with us, Mr. President?"
Firefighters, police and their unions, have criticized Mayor Rudy Giuliani over the issue of protective equipment and illnesses after the attacks. An October study by the National Institute of Environmental Safety and Health said that cleanup workers lacked adequate protective gear. The Executive Director of the National Fraternal Order of Police reportedly said of Giuliani: "Everybody likes a Churchillian kind of leader who jumps up when the ashes are still falling and takes over. But two or three good days don't expunge an eight-year record." Sally Regenhard, said, "There's a large and growing number of both FDNY families, FDNY members, former and current, and civilian families who want to expose the true failures of the Giuliani administration when it comes to 9/11." She told the "New York Daily News" that she intends to "Swift Boat" Giuliani.
Various health programs arose after the attacks to provide treatment for 9/11-related illnesses among responders, recovery workers, and other survivors. When the James Zadroga 9/11 Health and Compensation Act became federal law in January 2011, these programs were replaced by the World Trade Center Health Program.
Investigations.
Soon after the attacks, New York City commissioned McKinsey & Company to investigate the response of both the New York City Fire Department and New York City Police Department and make recommendations on how to respond more effectively to such large-scale emergencies in the future.
Officials with the International Association of Fire Fighters have also criticized Rudy Giuliani for failing to support modernized radios that might have spared the lives of more firefighters. Some firefighters never heard the evacuation orders and died in the collapse of the towers.
Estimated costs.
Estimated total costs, as of October 3, 2001
Reconstruction.
Plans for the World Trade Center rebuilding started in July 2002 which was headed by the Lower Manhattan Development Corporation. There were many proposals on how to build the World Trade Center back however many lacked creativity. Several architects were chosen throughout this construction process all of them ran into many problems with the design. Then there was a financial crisis in 2008 which forced the construction process over to the Port Authority. The Port Authority construction is not going as smoothly as planned. However, the construction of the buildings is currently active round the clock with only days off for bad weather. City officials are looking for better ways to lower the problems and delays. The World Trade Center completion of construction has been scheduled for 2016.
References.
Notes
Bibliography
External links.
NY Times:
Other:

</doc>
<doc id="28070" url="https://en.wikipedia.org/wiki?curid=28070" title="Communication during the September 11 attacks">
Communication during the September 11 attacks

Communication problems and successes played an important role in the September 11, 2001 attacks and their aftermath. Systems were variously destroyed or overwhelmed by loads greater than they were designed to carry, or failed to operate as intended or desired.
Attackers.
The organizers of the September 11, 2001 attacks apparently planned and coordinated their mission in face to face meetings and used little or no electronic communication. This "radio silence" made their plan more difficult to detect.
Federal government.
According to 9/11 Commission staff statement No. 17 there were several communications failures at the federal government level during and after the 9/11 attacks. Perhaps the most serious occurred in an "Air Threat Conference Call" initiated by the National Military Command Center (NMCC) after two planes had crashed into the World Trade Center, but shortly before The Pentagon was hit. The participants were unable to include the Federal Aviation Administration (FAA) air traffic control command center, which had the most information about the hijackings, in the call.
According to the staff report: 
Operators worked feverishly to include the FAA in this teleconference, but they had equipment problems and difficulty finding secure phone numbers. NORAD asked three times before 10:03 to confirm the presence of FAA on the conference, to provide an update on hijackings. The FAA did not join the call until 10:17. The FAA representative who joined the call had no familiarity with or responsibility for a hijack situation, had no access to decision makers, and had none of the information available to senior FAA officials by that time.
We found no evidence that, at this critical time, during the morning of September 11, NORAD’s top commanders, in Florida or Cheyenne Mountain Complex, ever coordinated with their counterparts at FAA headquarters to improve situational awareness and organize a common response. Lower-level officials improvised—the FAA’s Boston Center bypassing the chain of command to contact NEADS. But the highest level Defense Department officials relied on the NMCC’s Air Threat Conference, in which FAA did not meaningfully participate.
First responders.
After the 1993 World Trade Center bombing, radio repeaters for New York City Fire Department communication were installed in the tower complex. Because they were unaware that several controls needed to be operated to fully activate the repeater system, fire chiefs at their command post in the lobby of the North Tower thought the repeater was not functioning and did not use it, though it did work and was used by some firefighters. When police officials concluded the twin towers were in danger of collapsing and ordered police to leave the complex, fire officials were not notified . Fire officials on the scene were not monitoring broadcast news reports and did not immediately understand what had happened when the first (South) tower did collapse .
There was little communication between New York City Police Department and fire department commands even though an Office of Emergency Management (OEM) had been created in 1996 in part to provide such coordination . A primary reason for OEM's inability to coordinate communications and information-sharing in the early hours of the WTC response was the loss of its emergency operations center, located on the twenty third floor of 7 World Trade Center which had been evacuated after debris from tower's collapse struck the building, igniting several fires .
Emergency relief efforts in both Lower Manhattan and at the Pentagon were augmented by volunteer amateur radio operators in the weeks after the attacks.
Victims.
Cell phones and in-plane credit card phones played a major role during and after the attack, starting with hijacked passengers who called family or notified the authorities about what was happening. Passengers and crew who made calls include: Sandra Bradshaw, Todd Beamer, Tom Burnett, Mark Bingham, Peter Hanson, Jeremy Glick, Barbara K. Olson, Renee May, Madeline Amy Sweeney, Betty Ong, Robert Fangman, Brian David Sweeney, and Ed Felt. Innocent occupants aboard United Airlines Flight 93 were able to assess their situation based on these conversations and plan a revolt that resulted in the aircraft crashing. According to the commission staff: "Their actions saved the lives of countless others, and may have saved either the U.S. Capitol or the White House from destruction."
According to the 9/11 Commission Report, 13 passengers from Flight 93 made a total of over 30 calls to both family and emergency personnel (twenty-two confirmed air phone calls, two confirmed cell phone and eight not specified in the report). Brenda Raney, Verizon Wireless spokesperson, said that Flight 93 was supported by several cell sites. There were reportedly three phone calls from Flight 11, five from Flight 175, and three calls from Flight 77. Two calls from these flights were recorded, placed by flight attendants: Betty Ong on Flight 11 and CeeCee Lyles on Flight 93 
Alexa Graf, an AT&T spokesperson, said it was almost a fluke
that the calls reached their destinations. Marvin Sirbu, professor of Engineering and Public Policy at Carnegie Mellon University said on September 14, 2001, that "The fact of the matter is that cell phones can work in almost all phases of a commercial flight." Other industry experts said that it is possible to use cell phones with varying degrees of success during the ascent and descent of commercial airline flights.
After each of the hijacked aircraft struck the World Trade Center, people inside the towers made calls to family and loved ones; for the victims, this was their last communication. Other callers directed their pleas for help to 9-1-1. Over nine hours of the 9-1-1 calls were eventually released after petitioning by The "New York Times" and families of the WTC victims. In 2001, cell phones did not yet have texting or photography capabilities that came by the mid-2000s.
General public.
After the attack, the cell phone network of New York City was rapidly overloaded (a mass call event) as traffic doubled over normal levels. Cell phone traffic also overloaded across the East Coast, leading to crashes of the cell phone network. Verizon's downtown wire phone service was interrupted for days and weeks due to cut subscriber cables, and to the 140 West Street exchange being shut for days. Capacity between Brooklyn and Manhattan was also diminished by cut trunk cables.
Following the attacks, the issues with the cell network weren't resolved until 36 cellular COWs (cell towers on wheels) were deployed by September 14, 2001, in Lower Manhattan to support the U.S. Federal Emergency Management Agency (FEMA) and provide critical phone service to rescue and recovery workers.
Since three of the major television broadcast network owned-and-operated stations had their transmission towers atop the North Tower (One World Trade Center), coverage was limited after the collapse of the tower. The FM transmitter of National Public Radio station WNYC was also destroyed in the collapse of the North Tower and its offices evacuated. For an interim period, it continued broadcasting on its AM frequency and used NPR's New York offices to produce its programming.
The satellite feed of one television station, WPIX, froze on the last image received from the WTC mast; the image (a remote-camera shot of the burning towers), viewable across North America (as WPIX is available on cable TV in many areas), remained on the screen for much of the day until WPIX was able to set up alternate transmission facilities. It shows the WTC at the moment power cut off to the WPIX transmitter, prior to the towers' collapse.
During the September 11 attacks, WCBS-TV channel 2 and WXTV-TV channel 41 stayed on the air. Unlike most other major New York television stations, WCBS-TV maintained a full-powered backup transmitter at the Empire State Building after moving its main transmitter to the North Tower of the World Trade Center. The station was also simulcasted nationally on Viacom (which at the time owned CBS) cable network VH1 that day. In the immediate aftermath of the attacks, the station lent transmission time to the other stations who had lost their transmitters, until they found suitable backup equipment and locations.
The Emergency Alert System was never activated in the terrorist attacks, as the extensive media coverage made it unnecessary.
AT&T eliminated any costs for domestic calls originating from the New York City area (phones using area codes 212/718/917/646/347) in the days following 9/11.
Radio communications.
Radio communications during the September 11 attacks served a vital role in coordinating rescue efforts by New York Police Department, New York Fire Department, Port Authority Police Department, and Emergency Medical Services.
While radio communications were modified to address problems discovered after the 1993 World Trade Center bombing, investigations into the radio communications during the September 11th attacks discovered that communication systems and protocols that distinguished each department was hampered by the lack of interoperability, damaged or failed network infrastructure during the attack, and overwhelmed by simultaneous communication between superiors and subordinates.
Background.
A rough time line of the incident could include:
The scale of the incident was described in the National Commission report on the attacks as "unprecedented". In roughly fifteen minutes from 8:46 to 9:03 am, over a thousand police, fire, and emergency medical services (EMS) staff arrived at the scene. At some point during a large incident, any agency will reach a point where they find their resources overrun by needs. For example, the Port Authority Police could not schedule staff as if a September 11 attack would occur every shift. There is always a balance struck between readiness and costs. There is conflicting data but some sources suggest there may have been 2,000 to 3,000 workers involved in the rescue operation. It would be rare for most agencies to see an event where there were that many people to be rescued.
There is some level of confusion present in any large incident. The National Institute for Standards and Technology (NIST) asserts commanders did not have adequate information and interagency information sharing was inadequate. For example, on September 11, persons in the New York City Police Department (NYPD) 9-1-1 center told callers from the World Trade Center to remain in place and wait for instruction from firefighters and police officers. This was the plan for managing a fire incident in the building and the 9-1-1 center staff were following the plan. This was partly countered by public safety workers going floor-by-floor and telling people to evacuate. The Commission report suggests people in the NYPD 9-1-1 center and New York City Fire Department (FDNY) dispatch would benefit from better situation awareness. The Commission described the call centers as not "fully integrated" with line personnel at the WTC. The report suggests the NYPD 9-1-1 center and FDNY dispatch were overrun by call volumes that had never been seen before. Adding to the confusion, radio coverage problems, radio traffic blocking, and building system problems occurred inside the burning towers. The facts show that much of the equipment worked as designed and users made the best of what was available to them.
Typical of any large fire, many 9-1-1 calls with conflicting information were received beginning at 8:46 am. In addition to reports that a plane had hit the World Trade Center, the EMS computer-aided dispatch (CAD) log shows reports of a helicopter crash, explosions, and a building fire. Throughout the incident, people at different locations had very different views of the situation. After the collapse of the first tower, many firefighters in the remaining tower had no idea the first tower had fallen.
A factor in radio communications problems included the fact that off-duty personnel self-dispatched to the incident scene. Some off-duty staff went into the towers without radios. According to the Commission report and news coverage, this was true of NYPD, Port Authority Police Department (PAPD), and FDNY personnel. Regardless of any radio coverage problems, these persons could not be commanded or informed by radio. In any incident of this scale, self-dispatched staff without radios would likely be a problem. Even if a cache of radios were brought to the scene to hand out, the scale of this incident would be likely to overrun the number of radios in the cache.
NIST concluded, at the beginning of the incident, there was an approximate factor of five (peak) increase in radio communications traffic over a normal level. After the initial peak, radio traffic through the incident followed an approximate factor of three steady increase. FDNY recordings suggest the dispatch personnel were overloaded: both fire and EMS dispatch were often delayed in responding to radio calls. Many 9-1-1 telephone calls to dispatch were disconnected or routed to "all circuits are busy now," intercept recordings.
Voice radio systems.
NIST calculated that about one third of radio messages transmitted during the surge of communications were incomplete or unintelligible. Documentary footage suggests the tactical channels were also overloaded; some footage captured audio of two or three conversations occurring simultaneously on a particular channel.
In this study of WTC incident communications, radio systems used at the site had problems but were generally effective in that users were able to communicate with one another. A 2002 video documentary "9/11" by Gedeon and Jules Naudet, (referred to as "the documentary") was reviewed. It captured audio from hand-held radios in use at the incident and showed users communicating over radios from the lobby command post in the North Tower. 26 Red Book audio CDs of New York City Fire Department radio transmissions, covering the incident's initial dispatch and the tower failures, were reviewed. These CDs were digitized versions of audio from the Fire Department's logging recorders. In addition, text on an oral history CD with transcripts of fire personnel debriefed on the incident were reviewed.
NYPD and PAPD systems in 2001.
In 2001, the NYPD used Ultra High Frequency (UHF) radios and divided the city into 35 radio zones. Most hand-held radios had at least 20 channels: while not all officers had all channels, all officers had the ability to communicate citywide. As a characteristic of physics, UHF signals penetrate buildings better than lower Very High Frequency (VHF) frequencies used by the FDNY fire units but generally cover shorter distances over open terrain. The Commission report did not cite any technical flaws with the NYPD radio system.
PAPD has systems described as "low-power UHF". The Commission report says the systems were specific to a single site with the exception of one channel which was Port-Authority-wide. It's unclear whether the PAPD systems were interstitial and limited to 2 watts output, used normal local-control channels but were limited in power output by the frequency coordinator, or used leaky cable systems which were solely intended to work inside the Port Authority buildings. The report says there were 7, site-specific Port Authority Police channels. In 2001, officers at one site could not, (in all cases), carry their radios to another site and use them. Not all radios had all channels.
Fire and EMS dispatch channels.
Recordings of Citywide, Brooklyn, and Manhattan channels for Fire and Citywide, Brooklyn, and Manhattan channels for Emergency medical services were reviewed. Systems generally performed well. The audio coupling point for the logging recorder on Manhattan Fire made the dispatcher's voice difficult to hear. An anonymous fire dispatcher who identifies as "Dispatcher 416" is noteworthy.
The Commission report says that, in 2001, FDNY used a system with 5 repeater channels: one for each of the boroughs of Manhattan, Brooklyn, Queens, with the Bronx and Staten Island sharing a single frequency using different Private Line (PL) tones, and a city-wide channel. There were also five simplex channels in FDNY radios.
Observation shows, back in 2001, that the citywide EMS channel was voting more frequently than normal, signals were noisy, interfering signals were present, and that some receiver sites had equalization differences. Some transmissions had choppy audio possibly representative of interference from FSK paging or intermittent microwave radio paths to one or more receiver sites. For example, if a microwave radio path fails for half-second intervals, the voting comparator may vote out that receiver site until silence is detected. This can cause dropped syllables in the voted audio. Some transmissions were noisy, although transactions show the dispatcher was understanding radio traffic in spite of audio drop-outs in almost every case.
Port Authority fire repeater system (Repeater 7).
The Port Authority repeater, intended to allow communications inside the towers, did not appear to work as intended on September 11. The system, also called "Port Authority Channel 30", was installed after the 1993 World Trade Center attack. News accounts said the system had been turned off for unspecified technical reasons. The Commission report said it was customary to turn the system off because it somehow caused interference to radios in use at fire operations in other parts of the city. The documentary film gives different information, with a Fire Department member from Engine 7/Ladder 1 claiming that the aircraft's impact caused the system to fail. Evidence suggests the remote control console in the lobby command was not working but the repeater was. The radio repeater was located in 5 World Trade Center. A remote control console was connected to the repeater allowing staff at the North Tower lobby command post to communicate without using a hand-held radio.
In a review of the logging recorder track of the Port Authority repeater, someone arrived early during the incident and began to establish a command post. From the command post in the lobby of the North Tower (1 World Trade Center), the user can be heard trying to transmit using a remote control unit. After several failed attempts to communicate with a user on the channel, the user steps through every channel selection on the remote, trying each one. The recording contains the tone remote control console stepping through all of its eight function tones. Someone says, "...the wireline isn't working," over the Port Authority channel. Something that looks like a Motorola T-1380-series remote is shown in the documentary. The fact that users pressing buttons on the remote control can clearly be heard on the logging recorder shows the transmit audio path was working. The recording does not reveal whether or not the console function tones were keying the transmitter.
Some users in the North Tower lobby interpreted the remote control unit not working as a failure of the entire channel. Other fire units, not knowing the channel had failed, arrived and began using it successfully. The recordings show at least some units were successfully using the repeater to communicate inside the South Tower until the moment it collapsed. The Commission report says the North Tower lobby command may not have worked because of a technical problem, the volume control turned all the way down, or because a button that must be pressed to enable it had not been pushed.
On the audio track, an outside agency, possibly in New Jersey and using a repeater, comes through the receive audio on the Port Authority Repeater 7 system. An ambulance being dispatched by the outside (non-FDNY) agency is heard. This may be what the FDNY had described as interference caused when the repeater was left enabled at all times. The distant user appears to be repeated through the system, (possibly on the same CTCSS tone as was configured in Repeater 7). This appears to be a distant co-channel user on the same input frequency as Repeater 7. It's possible that by the random button pressing, a user sent a function tone that temporarily put the base station in "monitor" and that's what caused the outside agency's traffic to be heard. This is unlikely because subsequent transmit function tones should have toggled the receiver from monitor back to CTCSS-enabled.
Fire Department system.
An oral history interview revealed the Port Authority UHF radios were normally used at incidents inside the World Trade Center. The interviewee said in normal, day-to-day calls, the WTC staff handed Port Authority UHF radios to firefighters on their arrival and that these radios, "worked all over." This implies, but does not prove, that it was common knowledge among department members that FDNY radios had coverage problems inside the buildings. The 9-11 Commission uses the phrase, "performed poorly" to describe FDNY radios during the incident.
Oral history files show that at least four channels were employed at WTC:
One officer said a channel named "Command 3" was used for the North Tower. To those unfamiliar with the details of the FDNY system, it is unclear whether the interviewee meant Tactical 3 or a fifth channel.
FDNY personnel are seen using radios during the documentary footage of the WTC lobby area. Analysis of these scenes showed the radios all appeared to be receiving properly. Oral history files confirm radio communications were at least partly functional.
A problem that shows up in these types of incidents is that receivers in hand-held radios are subjected to signal levels that are likely to overload the receiver. Several radios may be transmitting within feet of one another on different channels. If overloading occurs, only very strong signals can be received while weaker signals disappear and are not received. The hand held radio receivers shown in the documentary appeared to work properly even though several other hand-held radios were transmitting only feet away. This is a hostile environment and suggests the hand-held equipment used by FDNY had good quality receivers, though in this case, the suggestion is incorrect. Second-hand observation is hardly the proper way to 'test' radio receivers or to distinguish 'good quality' from 'bad' and this is likely a source of continued misunderstanding; particularly when these same radios were operating at higher floors, in closer proximity to, and in direct line-of-sight of digital cellular repeaters. Those repeaters were likely operating at unlicensed power levels, which was a common practice of cellular providers at the time, and continues to this day. Footage reveals intelligible recovered audio coming out of the radios and shows radio users communicating with others. This may not have been true of the entire WTC complex but was true of radio users in the crowded lobby.
Analysis of the 26 FDNY audio CDs showed the radios seemed to transmit into the radio systems okay. Radios calling dispatch got through. Calling units were intelligible. Users spoke with dispatchers. Dispatchers answered in ways that suggest they understood what was said. There were no noisy or truncated transmissions heard on any channel, (the equivalent of a dropped cellular call). This suggests the Fire Department's radio backbone is soundly designed and working properly. It is possible that system coverage problems are present; problems that could have been mitigated had the Command Post radio (with greater transmit power) been used. It is also likely that some transmissions did not reach any of the receivers in the system and therefore would not be a detectable problem when listening to the recordings. At the same time those recordings were made, the cellular system was operating at or near full-capacity, meaning every cellular repeater was transmitting. The dense RF interference environment created in NYC that day was essentially a 'perfect storm'; one in which a radio designed 25 years prior could not possibly contend with.
In some scenes, captured documentary audio showed the channels were busy. In some cases, two or more conversations were taking place over a single radio channel at the same time. Users on Tactical 1 may have been close enough to one another to communicate because signals in proximity to each other would overpower weaker signals. At any incident of this size, there is likely to be some overlapping radio traffic. In the same way that large incidents exhaust all the firefighting vehicles and staff, the radio channel resources may become taxed to their limits. NIST says about one third of the fire department radio transmissions were not complete or not understandable.
Some radio users had selected the wrong channels. For example, on the Repeater 7 channel, a unit was heard to call "Manhattan" dispatch and "Citywide". Although the circumstances that lead to the user selecting the wrong channel are not known, this can occur when the user is trapped in darkness or smoke and cannot see the radio. Users will typically try to count steps in a rotary switch channel selector starting from one end of the switch's travel.
A communications van operated by FDNY responded to the incident. Its radio identifier was, "Field Comm." A backup van was in use on the day of the incident because the primary van was out-of-service. The backup van was destroyed and audio recordings of tactical channels used at the incident site were lost.
FDNY radio programming.
One annoyance with the fire systems was the presence of "unit ID" data bursts. These constant squawks, heard at the end of transmissions, are decoded at dispatch to identify the calling radio. The annoyance of the data bursts is a trade-off that could help find a firefighter who has been injured or needs help. It also automatically displays the unit ID at the dispatch console. In most systems, it also saves dispatch personnel from typing the unit ID. They press one key and the calling unit's ID is inserted into the current CAD screen or command line.
Recordings show radios were programmed to send unit ID on tactical channels. Radios accept unit ID on a per-channel basis. When mobile or hand-held radios are programmed, the unit ID encoders should be disabled on all channels where the feature is not used. This saves air time for about two to three syllables of speech per push-to-talk press. For example, unless the communications van or chief's vehicles had push-to-talk unit ID decoders, or the channels were recorded for later analysis where unit IDs were decoded from the recordings, the encoders should be turned off for tactical channels to reduce air time used.
It also sounded like some vehicle radios may have had "status buttons" using the data bursts. If true, the operator presses a button on the vehicle radio which sends a short data burst to dispatch. Dispatch gets the unit identity and the new status from a data decoder. These can cause interruptions in voice traffic but cut down on total air time required to conduct business because they occupy the channel for less time than it takes to say, "Engine fifty on scene."
Tactical 1.
This channel was the primary method of communication in the North tower. It was a simplex channel. Users complained it would only reach from the lobby to floors in the thirties. Tactical 1 was a default channel for use at some fire scenes. Some users who realized Repeater 7 was functional switched to that channel and were afforded better coverage than simplex users on Tactical 1. Audio recordings on the documentary film and NIST analysis show Tactical 1 was overloaded with heavy radio traffic. In contrast, the audio CD of Repeater 7 shows the channel was mostly idle.
The 9-11 commission report said a new portable repeater system had been developed to address shortcomings of Tactical 1 at a large incident. The system, called, "the post," is carried to an area near the incident and set up for the duration to augment weak signals.
Command channel.
The command channel used by officers at the incident was either called "Channel 5" or "Command 5" in documentation. Documents suggest the channel had a repeater but it was not clear if the repeater was citywide, installed in the Field Comm van, or housed in a battalion chief's vehicle. Recordings of this channel were lost when the Field Comm van was destroyed. The documentary film and oral history records show the channel being used effectively.
Interoperability.
The federal 9/11 Commission Report included recommendations on communications systems used by police, fire, and emergency medical services (EMS) at the WTC incident. In the report and in appearances on television news programs, commissioners said the capabilities of communications systems lacked the ability to communicate across department lines. That is to say, police units could not communicate with fire units directly by radio. Ambulances could not talk with police units directly by radio. Commission member Lee Hamilton, in several television appearances related to a 2006 book on the topic of the WTC incident, reiterated this factually-correct view.
Aviation assets.
An example that was cited by Hamilton: during the incident the Police Department helicopter was unable to communicate with Fire Department units in order to warn them of the towers' imminent collapse. The NIST document suggests the helicopter may have been able to offer several minutes warning. "Several minutes" may have been enough to get some people from the lower floors outside. This warning of imminent collapse went out over at least one police radio channel but there is nothing showing it was relayed to other people or channels. FDNY operates at least two communications vans: one of which was brought to the scene at the WTC incident. The Commission report reveals the primary FDNY van was equipped to talk to NYPD helicopters but the backup van, (which had no NYPD helicopter capability,) was in use on September 11, 2001.
In practice, many US helicopters used in emergency services are equipped with radios that allow communications on nearly any conventional two-way radio system, so long as the aircrew know the frequency and associated signaling tones. The radios usually have presets, like a car's broadcast radio, that allow some channels to be configured ahead of need. There was no information in the Commission report suggesting NYPD helicopters had such a capability.
Cross-department.
While it is technically possible to implement communications across departments, doing so introduces a host of new training and incident command problems. These are problems that would need to be managed in addition to the existing set of issues present at any large incident. The ability to maintain command, and monitor the safety of, groups working at an incident is diminished if a group of firefighters cannot be reached because they've switched over to the EMS channel. This could cause people to be sent to rescue them when there was no need. Similarly, if the Manhattan EMS dispatcher can't reach an ambulance because they are on one of the fire channels, patient care is affected. New York City Police Commissioner Raymond Kelly, appearing on the "Charlie Rose" show, expressed his view that the existing radio systems performed satisfactorily during the WTC incident. In his view, the interoperability desired by the 9-11 Commission was not needed.
Need for NYPD/FDNY cross-department links?
These problems are not new to the World Trade Center incident; cross-department and cross-discipline communication has been a hotly contested and long-identified issue. For example, at the Oklahoma City federal building bombing incident, the inability to communicate among departments was also cited as a problem. Firefighters heard an evacuation order on their radio channel because of the reported presence of a second bomb. Police and EMS workers reportedly did not know of the order.
In Hurricane Katrina's wake, a sergeant in the Louisiana Department of Wildlife and Fisheries appeared on national television to describe not being able to reach persons from other agencies who were assisting with the recovery. She described seeing the people in a nearby boat but not being able to communicate with them.
Even if the technical problems are solved, the issue is more complicated than just adding radio channels or talk groups. It is also a cultural problem. In one local incident, a large number of officers from three police agencies were fielded to search for a violent criminal who had evaded officers from one of the agencies. The officers did not coordinate by switching to a shared radio channel. After the incident, one participant said the users thought their radios were incompatible and did not understand how the shared channel worked. This possibly reflects a training problem or a technology literacy problem. The problem seems to have been remedied since then.
In another instance, a fire agency had thoroughly trained for interoperability scenarios. During an incident where two agencies with different radio channels responded, a decision-maker said personnel from his agency would stay on their own channel. Decision-makers may occasionally act in unpredictable ways, even if technology literate and well-trained. It's the Rumsfeldian concept that democracy is sometimes messy. It is not solely a technical problem, but an operational problem as well. Changes to ICS command structure, or operational changes in how the command post for an incident is set up, may produce better results than buying equipment or adding channels. Sometimes there are interoperability problems even where a structure for interoperability exists.
ICS: part of the solution?
One view of the Incident Command System is that units across department lines would communicate with their own representative at the command post or division level. That representative would relay any needs to another department. For example, a fire unit requesting five paramedic ambulances would identify the magnitude of a medical problem to their fire officer at the command post. This request would add to their commander's operational picture of the division or incident command as she called EMS to request the ambulances. Situation awareness is an important part of effective command and is easy to lose at a large incident. Bypassing incident commanders can contribute to a decomposing of command.
Trunked systems, commercial services, and cross-department netting.
One approach to cross-department netting is the capability of some modern trunked systems to provide a function called "dynamic regrouping"; a feature that Motorola doesn't support in simplex (e.g. 'fireground') operations. It is therefore necessary for a disaster to be near enough the infrastructure to allow for repeater access/operation. Many agencies with Motorola trunked systems already have this capability but it's hardly ever used; even in a crisis. The difficulty of operating such a system is often too great for poorly educated dispatchers who often have no college - much less any particular training in computers or communications systems - other than the 'cursory' training they receive in a 3 or 5 day class the vendors offer. The feature allows the dispatch center personnel to send units from different agencies who are responding to the same incident to a common talk group or virtual channel. This assumes the agencies all share a capability to operate over the same trunked radio system, which is rare. In an informal survey of three agencies with trunked systems that included this feature, users at two sites reported they did not think their system included the feature. A representative from a third site said he "...thought they had the feature but never used it." Of the three agencies with the feature, no one knew how to use it. This would suggest, (in at least the three agencies contacted,) that "dynamic regrouping" was not valuable. Like other disaster readiness processes, users would have to practice using the feature in order for it to be useful during an incident.
Some agencies use commercial two-way radio as an adjunct to their own communications networks. One professional engineering evaluation of public safety radio systems explains that commercial systems such as Nextel's are not built to the same standards of coverage and non-blocking as public safety trunked systems. Like toy walkie talkies marketed to children, they are usable and helpful for non-urgent communications but should not be considered reliable enough for life safety uses. It is also true that most trunked radio system users are likely to hear busy signals, (error tones showing no channels are available,) for the first time during a large disaster. All systems have a finite capacity.
"We don't want or need trunking" is what Chief Charles Dowd (NYPD) was heard to say at an APCO convention in Orlando (2006). NYPD operates a large, conventional repeater network with many legacy channels in the UHF band; and a technology developed "so a large number of users can share a small number of channels" (e.g. trunking) is clearly unnecessary and a frivolous waste of money.
With sufficient channels, there is no need for trunking. There are no 'busy' tones in a conventional repeater system. In the event an individual needs to chime in, he simply waits his turn - just as he would do in a trunked system.
Mobile data terminals.
All 911 ambulances and other FDNY vehicles have data terminals, sometimes referred to by staff in recordings and transcripts as MDTs. These terminals are connected to the computer-aided dispatch (CAD) back end or server. They can display text, page through screens describing jobs, and display lists of units assigned to a job.
A thorough analysis of data communications is not possible. What recordings show is that data terminals in at least some field units did not work properly during at least a portion of the incident. At 09:11:14, "Division 3" told Manhattan Fire dispatch, referring to the "summary" screen, "Summary is only giving me a few units. You're going to have to give it to me over the radio. I'm ready to write." This means the terminal was not displaying the entire list of units assigned to Division 3, as it would under normal conditions. The work-around: the Chief had to hand-write the list of units responding. In this one instance, the dispatcher reading the list of about 29 units tied up the Manhattan Fire channel for 53 seconds. During the reading of the list of units responding, one can hear several FDNY units try to interrupt the dispatcher. Their radio traffic was delayed until the entire list was read. This need to read lists of units because of slow or inoperable terminals occurred in at least three or four cases.
It's unclear what caused data delays and incomplete screens on the mobile data terminals. Evidenced by the dispatcher reading the list of units assigned to Division 3, the CAD system was working properly at dispatch positions. At least some field units experienced problems. Possible causes of problems with data terminals in vehicles may have included:
Data terminals are partly purchased and installed to reduce load on dispatch staff and to reduce traffic on voice channels. When they work properly, they have a significant operational benefit. A data outage during an occurrence of high call traffic can quickly overrun dispatch and voice channel capacity in cases where a routine level of calls for service requires both data terminals and voice channels.
New York City Council investigation.
New York City Council member Eric Gioia introduced a measure to have the Council investigate the issue of FDNY radio problems.

</doc>
<doc id="28082" url="https://en.wikipedia.org/wiki?curid=28082" title="Timeline for October following the September 11 attacks">
Timeline for October following the September 11 attacks

This article summarizes the events in October 2001 that were related to the September 11 attacks. All times, except where otherwise noted, are in Eastern Daylight Time (EDT), or .

</doc>
