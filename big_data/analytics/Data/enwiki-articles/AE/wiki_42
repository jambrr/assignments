<doc id="26980" url="https://en.wikipedia.org/wiki?curid=26980" title="Sun Microsystems">
Sun Microsystems

Sun Microsystems, Inc. was a company that sold :computers, computer components, :computer software, and :information technology services and that created the Java programming language, Solaris Unix and the Network File System (NFS). Sun significantly evolved several key computing technologies, among them Unix, RISC processors, thin client computing, and virtualized computing. Sun was founded on February 24, 1982. At its height, Sun headquarters were in Santa Clara, California (part of Silicon Valley), on the former west campus of the Agnews Developmental Center.
On January 27, 2010, Sun was acquired by Oracle Corporation for US $7.4 billion, based on an agreement signed on April 20, 2009. The following month, Sun Microsystems, Inc. was merged with Oracle USA, Inc. to become Oracle America, Inc.
Sun products included computer servers and workstations built on its own RISC-based SPARC processor architecture as well as on x86-based AMD's Opteron and Intel's Xeon processors; storage systems; and a suite of software products including the Solaris operating system, developer tools, Web infrastructure software, and identity management applications. Other technologies include the Java platform, MySQL, and NFS. Sun was a proponent of open systems in general and Unix in particular, and a major contributor to open source software. Sun's main manufacturing facilities were located in Hillsboro, Oregon, and Linlithgow, Scotland.
History.
The initial design for what became Sun's first Unix workstation, the Sun-1, was conceived by Andy Bechtolsheim when he was a graduate student at Stanford University in Palo Alto, California. Bechtolsheim originally designed the SUN workstation for the Stanford University Network communications project as a personal CAD workstation. It was designed around the Motorola 68000 processor with an advanced memory management unit (MMU) to support the Unix operating system with virtual memory support. He built the first ones from spare parts obtained from Stanford's Department of Computer Science and Silicon Valley supply houses.
On February 24, 1982, Vinod Khosla, Andy Bechtolsheim, and Scott McNealy, all Stanford graduate students, founded "Sun Microsystems". Bill Joy of Berkeley, a primary developer of the Berkeley Software Distribution (BSD), joined soon after and is counted as one of the original founders. The Sun name is derived from the initials of the Stanford University Network. Sun was profitable from its first quarter in July 1982.
By 1983 Sun was known for producing 68000-based systems with high-quality graphics that were the only computers other than DEC's VAX to run 4.2BSD. It licensed the computer design to other manufacturers, which typically used it to build Multibus-based systems running Unix from UniSoft. Sun's initial public offering was in 1986 under the stock symbol "SUNW", for "Sun Workstations" (later "Sun Worldwide"). The symbol was changed in 2007 to "JAVA"; Sun stated that the brand awareness associated with its Java platform better represented the company's current strategy.
Sun's logo, which features four interleaved copies of the word "sun", was designed by professor Vaughan Pratt, also of Stanford. The initial version of the logo was orange and had the sides oriented horizontally and vertically, but it was subsequently rotated to stand on one corner and re-colored purple, and later blue.
The "bubble" and its aftermath.
In the dot-com bubble, Sun began making much more money, and its shares rose dramatically. It also began spending much more, hiring workers and building itself out. Some of this was because of genuine demand, but much was from web start-up companies anticipating business that would never happen. In 2000, the bubble burst. Sales in Sun's important hardware division went into free-fall as customers closed shop and auctioned off high-end servers.
Several quarters of steep losses led to executive departures, rounds of layoffs, and other cost cutting. In December 2001, the stock fell to the 1998, pre-bubble level of about $100. But it kept falling, faster than many other tech companies. A year later it had dipped below $10 (a tenth of what it was even in 1990) but bounced back to $20. In mid-2004, Sun closed their Newark, California factory and consolidated all manufacturing to Hillsboro, Oregon. In 2006, that factory also closed.
Post-crash focus.
In 2004, Sun canceled two major processor projects which emphasized high instruction level parallelism and operating frequency. Instead, the company chose to concentrate on processors optimized for multi-threading and multiprocessing, such as the UltraSPARC T1 processor (codenamed "Niagara"). The company also announced a collaboration with Fujitsu to use the Japanese company's processor chips in mid-range and high-end Sun servers. These servers were announced on April 17, 2007 as the M-Series, part of the SPARC Enterprise series.
In February 2005, Sun announced the Sun Grid, a grid computing deployment on which it offered utility computing services priced at US$1 per CPU/hour for processing and per GB/month for storage. This offering built upon an existing 3,000-CPU server farm used for internal R&D for over 10 years, which Sun marketed as being able to achieve 97% utilization. In August 2005, the first commercial use of this grid was announced for financial risk simulations which was later launched as its first software as a service product.
In January 2005, Sun reported a net profit of $19 million for fiscal 2005 second quarter, for the first time in three years. This was followed by net loss of $9 million on GAAP basis for the third quarter 2005, as reported on April 14, 2005. In January 2007, Sun reported a net GAAP profit of $126 million on revenue of $3.337 billion for its fiscal second quarter. Shortly following that news, it was announced that Kohlberg Kravis Roberts (KKR) would invest $700 million in the company.
Sun had engineering groups in Bangalore, Beijing, Dublin, Grenoble, Hamburg, Prague, St. Petersburg, Tel Aviv, Tokyo, and Trondheim.
In 2007–2008, Sun posted revenue of $13.8 billion and had $2 billion in cash. First-quarter 2008 losses were $1.68 billion; revenue fell 7% to $12.99 billion. Sun's stock lost 80% of its value November 2007 to November 2008, reducing the company's market value to $3 billion. With falling sales to large corporate clients, Sun announced plans to lay off 5,000 to 6,000 workers, or 15–18% of its work force. It expected to save $700 million to $800 million a year as a result of the moves, while also taking up to $600 million in charges.
Major stockholders.
As of May 11, 2009, the following shareholders held over 100,000 common shares of Sun and at $9.40 per share offered by Oracle, they received the amounts indicated when the acquisition closed.
Hardware.
For the first decade of Sun's history, the company positioned its products as technical workstations, competing successfully as a low-cost vendor during the Workstation Wars of the 1980s. It then shifted its hardware product line to emphasize servers and storage. High-level telecom control systems such as Operational Support Systems service predominantly used Sun equipment.
Motorola-based systems.
Sun originally used Motorola 68000 family central processing units for the Sun-1 through Sun-3 computer series. The Sun-1 employed a 68000 CPU, the Sun-2 series, a 68010. The Sun-3 series was based on the 68020, with the later Sun-3x using the 68030.
SPARC-based systems.
In 1987, the company began using "SPARC", a RISC processor architecture of its own design, in its computer systems, starting with the Sun-4 line. SPARC was initially a 32-bit architecture (SPARC V7) until the introduction of the SPARC V9 architecture in 1995, which added 64-bit extensions.
Sun has developed several generations of SPARC-based computer systems, including the SPARCstation, Ultra and Sun Blade series of workstations, and the SPARCserver, Netra, Enterprise and Sun Fire line of servers.
In the early 1990s the company began to extend its product line to include large-scale symmetric multiprocessing servers, starting with the four-processor SPARCserver 600MP. This was followed by the 8-processor SPARCserver 1000 and 20-processor SPARCcenter 2000, which were based on work done in conjunction with Xerox PARC. In 1995 the company introduced Sun Ultra series machines that were equipped with the first 64-bit implementation of SPARC processors (UltraSPARC). In the late 1990s the transformation of product line in favor of large 64-bit SMP systems was accelerated by the acquisition of Cray Business Systems Division from Silicon Graphics. Their 32-bit, 64-processor Cray Superserver 6400, related to the SPARCcenter, led to the 64-bit Sun Enterprise 10000 high-end server (otherwise known as "Starfire").
In September 2004 Sun made available systems with UltraSPARC IV which was the first multi-core SPARC processor. It was followed by UltraSPARC IV+ in September 2005 and its revisions with higher clock speeds in 2007. These CPUs were used in the most powerful, enterprise class high-end CC-NUMA servers developed by Sun, such as Sun Fire E25K.
In November 2005 Sun launched the UltraSPARC T1, notable for its ability to concurrently run 32 threads of execution on 8 processor cores. Its intent was to drive more efficient use of CPU resources, which is of particular importance in data centers, where there is an increasing need to reduce power and air conditioning demands, much of which comes from the heat generated by CPUs. The T1 was followed in 2007 by the UltraSPARC T2, which extended the number of threads per core from 4 to 8. Sun has open sourced the design specifications of both the T1 and T2 processors via the OpenSPARC project.
In 2006, Sun ventured into the "blade server" (high density rack-mounted systems) market with the Sun Blade (distinct from the Sun Blade workstation).
In April 2007 Sun released the SPARC Enterprise server products, jointly designed by Sun and Fujitsu and based on Fujitsu SPARC64 VI and later processors. The "M-class" SPARC Enterprise systems include high-end reliability and availability features. Later T-series servers have also been badged SPARC Enterprise rather than Sun Fire.
In April 2008 Sun released servers with UltraSPARC T2 Plus, which is an SMP capable version of UltraSPARC T2, available in 2 or 4 processor configurations. It was the first CoolThreads CPU with multi-processor capability and it made possible to build standard rack-mounted servers that could simultaneously process up to massive 256 CPU threads in hardware (Sun SPARC Enterprise T5440), which is considered a record in the industry.
Since 2010, all further development of Sun machines based on SPARC architecture (including new SPARC T-Series servers, SPARC T3 and T4 chips) is done as a part of Oracle Corporation hardware division.
x86-based systems.
In the late 1980s, Sun also marketed an Intel 80386-based machine, the Sun386i; this was designed to be a hybrid system, running SunOS but at the same time supporting DOS applications. This only remained on the market for a brief time. A follow-up "486i" upgrade was announced but only a few prototype units were ever manufactured.
Sun's brief first foray into x86 systems ended in the early 1990s, as it decided to concentrate on SPARC and retire the last Motorola systems and 386i products, a move dubbed by McNealy as "all the wood behind one arrowhead". Even so, Sun kept its hand in the x86 world, as a release of Solaris for PC compatibles began shipping in 1993.
In 1997 Sun acquired Diba, Inc., followed later by the acquisition of Cobalt Networks in 2000, with the aim of building "network appliances" (single function computers meant for consumers). Sun also marketed a "network computer" (a term popularized and eventually trademarked by Oracle); the JavaStation was a diskless system designed to run Java applications.
Although none of these business initiatives were particularly successful, the Cobalt purchase gave Sun a toehold for its return to the x86 hardware market. In 2002, Sun introduced its first general purpose x86 system, the LX50, based in part on previous Cobalt system expertise. This was also Sun's first system announced to support Linux as well as Solaris.
In 2003, Sun announced a strategic alliance with AMD to produce x86/x64 servers based on AMD's Opteron processor; this was followed shortly by Sun's acquisition of Kealia, a startup founded by original Sun founder Andy Bechtolsheim, which had been focusing on high-performance AMD-based servers.
The following year, Sun launched the Opteron-based Sun Fire V20z and V40z servers, and the Java Workstation W1100z and W2100z workstations.
On September 12, 2005, Sun unveiled a new range of Opteron-based servers: the Sun Fire X2100, X4100 and X4200 servers. These were designed from scratch by a team led by Bechtolsheim to address heat and power consumption issues commonly faced in data centers. In July 2006, the Sun Fire X4500 and X4600 systems were introduced, extending a line of x64 systems that support not only Solaris, but also Linux and Microsoft Windows.
On January 22, 2007, Sun announced a broad strategic alliance with Intel. Intel endorsed Solaris as a mainstream operating system and as its mission critical Unix for its Xeon processor-based systems, and contributed engineering resources to OpenSolaris. Sun began using the Intel Xeon processor in its x64 server line, starting with the Sun Blade X6250 server module introduced in June 2007.
On May 5, 2008, AMD announced its Operating System Research Center (OSRC) expanded its focus to include optimization to Sun's OpenSolaris and xVM virtualization products for AMD based processors.
Software.
Although Sun was initially known as a hardware company, its software history began with its founding in 1982; co-founder Bill Joy was one of the leading Unix developers of the time, having contributed the vi editor, the C shell, and significant work developing TCP/IP and the BSD Unix OS. Sun later developed software such as the Java programming language and acquired software such as StarOffice, VirtualBox and MySQL.
Sun used community-based and open-source licensing of its major technologies, and for its support of its products with other open source technologies. GNOME-based desktop software called Java Desktop System (originally code-named "Madhatter") was distributed for the Solaris operating system, and at one point for Linux. Sun supported its Java Enterprise System (a middleware stack) on Linux. It released the source code for Solaris under the open-source Common Development and Distribution License, via the OpenSolaris community. Sun's positioning includes a commitment to indemnify users of some software from intellectual property disputes concerning that software. It offers support services on a variety of pricing bases, including per-employee and per-socket.
A 2006 report prepared for the EU by UNU-MERIT stated that Sun was the largest corporate contributor to open source movements in the world. According to this report, Sun's open source contributions exceed the combined total of the next five largest commercial contributors.
Operating systems.
Sun is best known for its Unix systems, which have a reputation for system stability and a consistent design philosophy.
Sun's first workstation shipped with UniSoft V7 Unix. Later in 1982 Sun began providing SunOS, a customized 4.1BSD Unix, as the operating system for its workstations.
In the late 1980s, AT&T tapped Sun to help them develop the next release of their branded UNIX, and in 1988 announced they would purchase up to a 20% stake in Sun. UNIX System V Release 4 (SVR4) was jointly developed by AT&T and Sun; Sun used SVR4 as the foundation for Solaris 2.x, which became the successor to SunOS 4.1.x (later retrospectively named Solaris 1.x). By the mid-1990s, the ensuing Unix wars had largely subsided, AT&T had sold off their Unix interests, and the relationship between the two companies was significantly reduced.
From 1992 Sun also sold Interactive Unix, an operating system it acquired when it bought Interactive Systems Corporation from Eastman Kodak Company. This was a popular Unix variant for the PC platform and a major competitor to market leader SCO UNIX. Sun's focus on Interactive Unix diminished in favor of Solaris on both SPARC and x86 systems; it was dropped as a product in 2001.
Sun dropped the Solaris 2.x version numbering scheme after the Solaris 2.6 release (1997); the following version was branded Solaris 7. This was the first 64-bit release, intended for the new UltraSPARC CPUs based on the SPARC V9 architecture. Within the next four years, the successors Solaris 8 and Solaris 9 were released in 2000 and 2002 respectively.
Following several years of difficult competition and loss of server market share to competitors' Linux-based systems, Sun began to include Linux as part of its strategy in 2002. Sun supported both Red Hat Enterprise Linux and SUSE Linux Enterprise Server on its x64 systems; companies such as Canonical Ltd., Wind River Systems and MontaVista also supported their versions of Linux on Sun's SPARC-based systems.
In 2004, after having cultivated a reputation as one of Microsoft's most vocal antagonists, Sun entered into a joint relationship with them, resolving various legal entanglements between the two companies and receiving US$1.95 billion in settlement payments from them. Sun supported Microsoft Windows on its x64 systems, and announced other collaborative agreements with Microsoft, including plans to support each other's virtualization environments.
In 2005, the company released Solaris 10. The new version included a large number of enhancements to the operating system, as well as very novel features, previously unseen in the industry. Solaris 10 update releases continued through the next 8 years, the last release from Sun Microsystems being Solaris 10 10/09. The following updates were released by Oracle under the new license agreement; the final release is Solaris 10 1/13.
Previously, Sun offered a separate variant of Solaris called Trusted Solaris, which included augmented security features such as multilevel security and a least privilege access model. Solaris 10 included many of the same capabilities as Trusted Solaris at the time of its initial release; Solaris 10 11/06 included Solaris Trusted Extensions, which give it the remaining capabilities needed to make it the functional successor to Trusted Solaris.
After releasing Solaris 10, it's source code was opened under CDDL free software license and developed in open with contributing Opensolaris community through SXCE that used SVR4 .pkg packaging and supported Opensolaris releases that used IPS.
Following acquisition of Sun by Oracle , Opensolaris continued to develop in open under illumos with illumos distributions.
Oracle Corporation continued to develop OpenSolaris into next Solaris release, changing back the license to proprietary, and released it as Oracle Solaris 11 in November 2011.
Java platform.
The Java platform was developed at Sun in the early 1990s with the objective of allowing programs to function regardless of the device they were used on, sparking the slogan "Write once, run anywhere" (WORA). While this objective was not entirely achieved (prompting the riposte "Write once, debug everywhere"), Java is regarded as being largely hardware- and operating system-independent.
Java was initially promoted as a platform for client-side "applets" running inside web browsers. Early examples of Java applications were the HotJava web browser and the HotJava Views suite. However, since then Java has been more successful on the server side of the Internet.
The platform consists of three major parts: the Java programming language, the Java Virtual Machine (JVM), and several Java Application Programming Interfaces (APIs). The design of the Java platform is controlled by the vendor and user community through the Java Community Process (JCP).
Java is an object-oriented programming language. Since its introduction in late 1995, it became one of the world's most popular programming languages.
Java programs are compiled to byte code, which can be executed by any JVM, regardless of the environment.
The Java APIs provide an extensive set of library routines. These APIs evolved into the "Standard Edition" (Java SE), which provides basic infrastructure and GUI functionality; the "Enterprise Edition" (Java EE), aimed at large software companies implementing enterprise-class application servers; and the "Micro Edition" (Java ME), used to build software for devices with limited resources, such as mobile devices.
On November 13, 2006, Sun announced it would be licensing its Java implementation under the GNU General Public License; it released its Java compiler and JVM at that time.
In February 2009 Sun entered a battle with Microsoft and Adobe Systems, which promoted rival platforms to build software applications for the Internet. JavaFX was a development platform for music, video and other applications that builds on the Java programming language.
Office suite.
In 1999, Sun acquired the German software company StarDivision and with it the office suite StarOffice, which Sun later released as OpenOffice.org under both GNU LGPL and the SISSL (Sun Industry Standards Source License). OpenOffice.org supported Microsoft Office file formats (though not perfectly), was available on many platforms (primarily Linux, Microsoft Windows, Mac OS X, and Solaris) and was used in the open source community.
The principal differences between StarOffice and OpenOffice.org were that StarOffice was supported by Sun, was available as either a single-user retail box kit or as per-user blocks of licensing for the enterprise, and included a wider range of fonts and document templates and a commercial quality spellchecker. StarOffice also contained commercially licensed functions and add-ons; in OpenOffice.org these were either replaced by open-source or free variants, or are not present at all. Both packages had native support for the OpenDocument format.
Virtualization and datacenter automation software.
In 2007, Sun announced the Sun xVM virtualization and datacenter automation product suite for commodity hardware. Sun also acquired VirtualBox in 2008. Earlier virtualization technologies from Sun like "Dynamic System Domains" and "Dynamic Reconfiguration" were specifically designed for high-end SPARC servers, and Logical Domains only supports the UltraSPARC T1/T2/T2 Plus server platforms. Sun marketed "Sun Ops Center" provisioning software for datacenter automation.
On the client side, Sun offered virtual desktop solutions. Desktop environments and applications could be hosted in a datacenter, with users accessing these environments from a wide range of client devices, including Microsoft Windows PCs, Sun Ray virtual display clients, Apple Macintoshes, PDAs or any combination of supported devices. A variety of networks were supported, from LAN to WAN or the public Internet. Virtual desktop products included Sun Ray Server Software, Sun Secure Global Desktop and Sun Virtual Desktop Infrastructure.
Database management systems.
Sun acquired MySQL AB, the developer of the MySQL database in 2008 for US$1 billion. CEO Jonathan Schwartz mentioned in his blog that optimizing the performance of MySQL was one of the priorities of the acquisition. In February 2008, Sun began to publish results of the MySQL performance optimization work. Sun contributed to the PostgreSQL project. On the Java platform, Sun contributed to and supported Java DB.
Other software.
Sun offered other software products for software development and infrastructure services. Many were developed in house; others came from acquisitions, including Tarantella, Waveset Technologies, SeeBeyond, and Vaau. Sun acquired many of the Netscape non-browser software products as part a deal involving Netscape's merger with AOL. These software products were initially offered under the "iPlanet" brand; once the Sun-Netscape alliance ended, they were re-branded as "Sun ONE" (Sun Open Network Environment), and then the "Sun Java System".
Sun's middleware product was branded as the "Java Enterprise System" (or JES), and marketed for web and application serving, communication, calendaring, directory, identity management and service-oriented architecture. Sun's Open ESB and other software suites were available free of charge on systems running Solaris, Red Hat Enterprise Linux, HP-UX, and Windows, with support available optionally.
Sun developed data center management software products, which included the "Solaris Cluster" high availability software, and a grid management package called "Sun Grid Engine" and firewall software such as SunScreen.
For Network Equipment Providers and telecommunications customers, Sun developed the Sun Netra High-Availability Suite.
Sun produced compilers and development tools under the "Sun Studio" brand, for building and developing Solaris and Linux applications.
Sun entered the software as a service (SaaS) market with zembly, a social cloud-based computing platform and Project Kenai, an open-source project hosting service.
Storage.
Sun sold its own storage systems to complement its system offerings; it has also made several storage-related acquisitions.
On June 2, 2005, Sun announced it would purchase Storage Technology Corporation (StorageTek) for US$4.1 billion in cash, or $37.00 per share, a deal completed in August 2005.
In 2006, Sun introduced the Sun StorageTek 5800 System, the first application-aware programmable storage solution. In 2008, Sun contributed the source code of the StorageTek 5800 System under the BSD license.
Sun announced the Sun Open Storage platform in 2008 built with open source technologies.
In late 2008 Sun announced the Sun Storage 7000 Unified Storage systems (codenamed Amber Road). Transparent placement of data in the systems' solid-state drives (SSD) and conventional hard drives was managed by ZFS to take advantage of the speed of SSDs and the economy of conventional hard disks.
Other storage products included Sun Fire X4500 storage server and SAM-QFS filesystem and storage management software.
HPC solutions.
Sun marketed the Sun Constellation System for High-Performance Computing (HPC). Even before the introduction of the Sun Constellation System in 2007, Sun's products were in use in many of the TOP500 systems and supercomputing centers:
The "Sun HPC ClusterTools" product was a set of Message Passing Interface (MPI) libraries and tools for running parallel jobs on Solaris HPC clusters. Beginning with version 7.0, Sun switched from its own implementation of MPI to Open MPI, and donated engineering resources to the Open MPI project.
Sun was a participant in the OpenMP language committee. Sun Studio compilers and tools implemented the OpenMP specification for shared memory parallelization.
In 2006, Sun built the "TSUBAME supercomputer", which was until June 2008 the fastest supercomputer in Asia. Sun built "Ranger" at the Texas Advanced Computing Center (TACC) in 2007. Ranger had a peak performance of over 500 TFLOPS, and was the 6th most powerful supercomputer on the TOP500 list in November 2008.
Sun announced an OpenSolaris distribution that integrated many of Sun's HPC products and other 3rd-party solutions.
Staff.
Notable Sun employees included John Gilmore, Whitfield Diffie, Radia Perlman, and Marc Tremblay. Sun was an early advocate of Unix-based networked computing, promoting TCP/IP and especially NFS, as reflected in the company's motto "The Network Is The Computer", coined by John Gage. James Gosling led the team which developed the Java programming language. Jon Bosak led the creation of the XML specification at W3C.
Sun staff published articles on the company's blog site. Staff were encouraged to use the site to blog on any aspect of their work or personal life, with few restrictions placed on staff, other than commercially confidential material. Jonathan I. Schwartz was one of the first CEOs of large companies to regularly blog; his postings were frequently quoted and analyzed in the press. In 2005, Sun Microsystems was one of the first Fortune 500 companies that instituted a formal Social Media program.
Acquisition by Oracle.
Sun was sold to Oracle Corporation in 2009.
Sun's staff were asked to share anecdotes about their experiences at Sun. A web site containing videos, stories, and photographs from 27 years at Sun was made available on September 2, 2009.
In October, Sun announced a second round of thousands of employees to be laid off, blamed partially on delays in approval of the merger.
The transaction completed in early 2010.
In January 2011 Oracle agreed to pay $46 million to settle charges that it submitted false claims to US federal government agencies and paid "kickbacks" to systems integrators.
In February 2011 Sun's former Menlo Park, California campus of about was sold, and it was announced that it would become headquarters for Facebook.
The sprawling facility built around an enclosed courtyard had been nicknamed "Sun Quentin". On September 1, 2011, Sun India legally became part of Oracle. It had been delayed due to legal issues in Indian court.

</doc>
<doc id="26981" url="https://en.wikipedia.org/wiki?curid=26981" title="Solaris">
Solaris

Solaris may refer to:

</doc>
<doc id="26983" url="https://en.wikipedia.org/wiki?curid=26983" title="Saladin">
Saladin

An-Nasir Salah ad-Din Yusuf ibn Ayyub ( / ALA-LC: "Ṣalāḥ ad-Dīn Yūsuf ibn Ayyūb"; / ALC-LC: "Selahedînê Eyûbî"), known as Saladin (1137 – March 1193), was the first sultan of Egypt and Syria and the founder of the Ayyubid dynasty. A Sunni Muslim of Kurdish origin, Saladin led the Muslim military campaign against the Crusader states in the Levant. At the height of his power, his sultanate included Egypt, Syria, Upper Mesopotamia, the Hejaz, Yemen and other parts of North Africa.
Originally sent to Fatimid Egypt in 1163 by his Zengid lord, Nur ad-Din, Saladin climbed the ranks of the Fatimid government by virtue of his military successes against Crusader assaults against its territory and his personal closeness to Fatimid caliph al-Adid. When Saladin's uncle Shirkuh died in 1169, al-Adid appointed Saladin vizier, a rare nomination of a Sunni Muslim to such an important position in the Shia Muslim-led caliphate. During his term as vizier, Saladin began to undermine the Fatimid establishment, and following al-Adid's death in 1171 he assumed control over the government and realigned the country's allegiance with the Sunni Muslim, Baghdad-based Abbasid Caliphate. In the following years, he led forays against the Crusaders in Palestine, commissioned the successful conquest of Yemen and staved off pro-Fatimid rebellions in Upper Egypt.
Not long after Nur ad-Din's death in 1174, Saladin launched his conquest of Syria, peacefully entering Damascus at the request of its governor. By mid-1175, Saladin had conquered Hama and Homs, inviting the animosity of his former Zengid lords, who had been the official rulers of Syria. Soon after, he defeated the Zengid army at the Battle of the Horns of Hama and was thereafter proclaimed the "Sultan of Egypt and Syria" by Abbasid caliph al-Mustadi. Saladin made further conquests in northern Syria and Jazira, escaping two attempts on his life by the Assassins, before returning to Egypt in 1177 to address issues there. By 1182, Saladin completed the conquest of Muslim Syria after capturing Aleppo, but ultimately failed to take over the Zengid stronghold of Mosul.
Under Saladin's command, the Ayyubid army defeated the Crusaders at the decisive Battle of Hattin in 1187, and thereafter wrested control of Palestine from the Crusaders, who had conquered the area 88 years earlier. Although the Crusader Kingdom of Jerusalem continued to exist until the late 13th century, its defeat at Hattin marked a turning point in its conflict with the Muslim powers of the region. Saladin died in Damascus in 1193, having given away much of his personal wealth to his subjects. He is buried in a mausoleum adjacent to the Umayyad Mosque. Saladin has become a prominent figure in Muslim, Arab, Turkish and Kurdish culture, and he has often been described as being probably the most famous Kurd in history.
Early life.
Saladin was born in Tikrit in modern-day Iraq. His personal name was "Yusuf"; "Salah ad-Din" is a "laqab", a descriptive epithet, meaning "Righteousness of the Faith." His family was of Kurdish ancestry, and had originated from the city of Dvin in medieval Armenia. The Rawadid tribe he hailed from had been partially assimilated into the Arabic-speaking world by this time. In 1132 the defeated army of the Imad ad-Din Zengi, the Lord of Mosul, found their retreat blocked by the Tigris River opposite the Tikrit fortress where Saladin's father, Najm ad-Din Ayyub served as the warden. Ayyub provided ferries for the army and gave them refuge in Tikrit. Mujahed al-Din Bihruz, a former Greek slave who had been appointed the military governor of northern Mesopotamia for his service to the Seljuks had reprimanded Ayyub for giving Zengi refuge and in 1137, banished Ayyub from Tikrit after his brother Asad al-Din Shirkuh killed a friend of Bihruz in an honour killing. According to Baha ad-Din ibn Shaddad, Saladin was born the same night his family left Tikrit. In 1139, Ayyub and his family moved to Mosul where Imad ad-Din Zengi acknowledged his debt and appointed Ayyub commander of his fortress in Baalbek. After the death of Zengi in 1146, his son, Nur ad-Din, became the regent of Aleppo and the leader of the Zengids.
Saladin, who now lived in Damascus, was reported to have a particular fondness of the city, but information on his early childhood is scarce. About education, Saladin wrote "children are brought up in the way in which their elders were brought up." According to one of his biographers, al-Wahrani, Saladin was able to answer questions on Euclid, the Almagest, arithmetic, and law, but this was an academic ideal and it was study of the Qur'an and the "sciences of religion" that linked him to his contemporaries. Several sources claim that during his studies he was more interested in religion than joining the military. Another factor which may have affected his interest in religion was that during the First Crusade, Jerusalem was taken in a surprise attack by the Christians.
Early expeditions.
Saladin's military career began under the tutelage of his uncle Asad al-Din Shirkuh, a prominent military commander under Nur ad-Din, Emir of Damascus and Aleppo, member of the Turkic Zengid dynasty and the most influential teacher of Saladin. In 1163, the vizier to the Fatimid caliph al-Adid, Shawar, had been driven out of Egypt by rival Dirgham, a member of the powerful Banu Ruzzaik tribe. He asked for military backing from Nur ad-Din, who complied and in 1164, sent Shirkuh to aid Shawar in his expedition against Dirgham. Saladin, at age 26, went along with them. After Shawar was successfully reinstated as vizier, he demanded that Shirkuh withdraw his army from Egypt for a sum of 30,000 dinars, but he refused insisting it was Nur ad-Din's will that he remain. Saladin's role in this expedition was minor, and it is known that he was ordered by Shirkuh to collect stores from Bilbais prior to its siege by a combined force of Crusaders and Shawar's troops.
After the sacking of Bilbais, the Crusader-Egyptian force and Shirkuh's army were to engage in a battle on the desert border of the River Nile, just west of Giza. Saladin played a major role, commanding the right wing of the Zengid army (Muslim Dynasty of Oghuz Turk origin), while a force of Kurds commanded the left, and Shirkuh stationed in the center. Muslim sources at the time, however, put Saladin in the "baggage of the centre" with orders to lure the enemy into a trap by staging a false retreat. The Crusader force enjoyed early success against Shirkuh's troops, but the terrain was too steep and sandy for their horses, and commander Hugh of Caesarea was captured while attacking Saladin's unit. After scattered fighting in little valleys to the south of the main position, the Zengid central force returned to the offensive; Saladin joined in from the rear.
The battle ended in a Zengid victory, and Saladin is credited to have helped Shirkuh in one of the "most remarkable victories in recorded history", according to Ibn al-Athir, although more of Shirkuh's men were killed and the battle is considered by most sources as not a total victory. Saladin and Shirkuh moved towards Alexandria where they were welcomed, given money, arms and provided a base. Faced by a superior Crusader-Egyptian force who attempted to besiege the city, Shirkuh split his army. He and the bulk of his force withdrew from Alexandria, while Saladin was left with the task of guarding the city.
In Egypt.
Emir of Egypt.
Shirkuh was in a power struggle over Egypt with Shawar and Amalric I of the Kingdom of Jerusalem, in which Shawar requested Amalric's assistance. In 1169, Shawar was reportedly assassinated by Saladin, and Shirkuh died later that year. Nur ad-Din chose a successor for Shirkuh, but al-Adid appointed Saladin to replace Shawar as vizier.
The reasoning behind the Shia caliph al-Adid's selection of Saladin, a Sunni, varies. Ibn al-Athir claims that the caliph chose him after being told by his advisers that "there is no one weaker or younger" than Saladin, and "not one of the emirs obeyed him or served him." However, according to this version, after some bargaining, he was eventually accepted by the majority of "emirs". Al-Adid's advisers were also suspected of attempting to split the Syria-based Zengid ranks. Al-Wahrani wrote that Saladin was selected because of the reputation of his family in their "generosity and military prowess." Imad ad-Din wrote that after the brief mourning period of Shirkuh, during which "opinions differed", the Zengid "emirs" decided upon Saladin and forced the caliph to "invest him as vizier." Although positions were complicated by rival Muslim leaders, the bulk of the Syrian rulers supported Saladin because of his role in the Egyptian expedition, in which he gained a record of military qualifications.
Inaugurated as Emir on 26 March, Saladin repented "wine-drinking and turned from frivolity to assume the dress of religion", according to Arabic sources of the time. Having gained more power and independence than ever before in his career, he still faced the issue of ultimate loyalty between al-Adid and Nur ad-Din. Later in the year, a group of Egyptian soldiers and "emirs" attempted to assassinate Saladin, but having already known of their intentions, thanks to his intelligence chief Ali bin Safyan, he had the chief conspirator, Naji, Mu'tamin al-Khilafa—the civilian controller of the Fatimid Palace—arrested and killed. The day after, 50,000 black African soldiers from the regiments of the Fatimid army opposed to Saladin's rule along with a number of Egyptian "emirs" and commoners staged a revolt. By 23 August, Saladin had decisively quelled the uprising, and never again had to face a military challenge from Cairo.
Towards the end of 1169, Saladin, with reinforcements from Nur ad-Din defeated a massive Crusader-Byzantine force near Damietta. Afterward, in the spring of 1170, Nur ad-Din sent Saladin's father to Egypt in compliance with Saladin's request, as well as encouragement from the Baghdad-based Abbasid caliph, al-Mustanjid, who aimed to pressure Saladin in deposing his rival caliph, al-Adid. Saladin himself had been strengthening his hold on Egypt and widening his support base there. He began granting his family members high-ranking positions in the region; he ordered the construction of a college for the Maliki branch of Sunni Islam in the city, as well as one for the Shafi'i denomination to which he belonged in al-Fustat.
After establishing himself in Egypt, Saladin launched a campaign against the Crusaders, besieging Darum in 1170. Amalric withdrew his Templar garrison from Gaza to assist him in defending Darum, but Saladin evaded their force and fell on Gaza instead. He destroyed the town built outside the city's castle and killed most of its inhabitants after they were refused entry into the castle. It is unclear exactly when, but during that same year, he attacked and captured the Crusader castle of Eilat, built on an island off the head of the Gulf of Aqaba. It did not pose a threat to the passage of the Muslim navy, but could harass smaller parties of Muslim ships and Saladin decided to clear it from his path.
Sultan of Egypt.
According to Imad ad-Din, Nur ad-Din wrote to Saladin in June 1171, telling him to reestablish the Abbasid caliphate in Egypt, which Saladin coordinated two months later after additional encouragement by Najm ad-Din al-Khabushani, the Shafi'i "faqih", who vehemently opposed Shia rule in the country. Several Egyptian "emirs" were thus killed, but al-Adid was told that they were killed for rebelling against him. He then fell ill, or was poisoned according to one account. While ill, he asked Saladin to pay him a visit to request that he take care of his young children, but Saladin refused, fearing treachery against the Abbasids, and is said to have regretted his action after realizing what al-Adid had wanted. He died on September 13 and five days later, the Abbasid "khutba" was pronounced in Cairo and al-Fustat, proclaiming al-Mustadi as caliph.
On 25 September, Saladin left Cairo to take part in a joint attack on Kerak and Montreal, the desert castles of the Kingdom of Jerusalem, with Nur ad-Din who would attack from Syria. Prior to arriving at Montreal, Saladin however withdrew back to Cairo as he received the reports that in his absence the Crusader Leaders had increased their support to the traitors inside Egypt to attack Saladin from within and lessen his power especially the Fatimid who started plotting to restore their past glory. Because of this, Nur ad-Din went on alone.
During the summer of 1173, a Nubian army along with a contingent of Armenian refugees were reported on the Egyptian border, preparing for a siege against Aswan. The "emir" of the city had requested Saladin's assistance and was given reinforcements under Turan-Shah—Saladin's brother. Consequently, the Nubians departed, but returned in 1173 and were again driven off. This time Egyptian forces advanced from Aswan and captured the Nubian town of Ibrim. Saladin sent a gift to Nur ad-Din who had been his friend and teacher, 60,000 dinars, "wonderful manufactured goods", some jewels, and an elephant. While transporting these goods to Damascus, Saladin took the opportunity to ravage the Crusader countryside. He did not press an attack against the desert castles, but attempted to drive out the Muslim Bedouins who lived in Crusader territory with the aim of depriving the Franks of guides.
On 31 July 1173, Saladin's father Ayyub was wounded in a horse-riding accident, ultimately causing his death on 9 August. In 1174, Saladin sent Turan-Shah to conquer Yemen to allocate it and its port Aden to the territories of the Ayyubid Dynasty.
Conquest of Syria.
Conquest of Damascus.
In the early summer of 1174, Nur ad-Din was mustering an army, sending summons to Mosul, Diyarbakir, and al-Jazira in an apparent preparation of attack against Saladin's Egypt. The Ayyubid dynasty held a council upon the revelation of his preparations to discuss the possible threat and Saladin collected his own troops outside Cairo. On May 15, Nur ad-Din died after falling ill the previous week and his power was handed to his eleven-year-old son as-Salih Ismail al-Malik. His death left Saladin with political independence and in a letter to as-Salih, he promised to "act as a sword" against his enemies and referred to the death of his father as an "earthquake shock".
In the wake of Nur ad-Din's death, Saladin faced a difficult decision; he could move his army against the Crusaders from Egypt or wait until invited by as-Salih in Syria to come to his aid and launch a war from there. He could also take it upon himself to annex Syria before it could possibly fall into the hands of a rival, but he feared that attacking a land that formerly belonged to his master—which is forbidden in the Islamic principles he believed—could portray him as hypocritical and thus making it unsuitable for leading the war against the Crusaders. Saladin saw that in order to acquire Syria; he either needed an invitation from As-Salih or to warn him that potential anarchy could rise danger from the Crusaders.
When as-Salih was removed to Aleppo in August, Gumushtigin, the "emir" of the city and a captain of Nur ad-Din's veterans assumed guardianship over him. The "emir" prepared to unseat all of his rivals in Syria and al-Jazira, beginning with Damascus. In this emergency, the "emir" of Damascus appealed to Saif al-Din (a cousin of Gumushtigin) of Mosul for assistance against Aleppo, but he refused, forcing the Syrians to request the aid of Saladin who complied. Saladin rode across the desert with 700 picked horsemen, passing through al-Kerak then reaching Bosra and according to him, was joined by ""emirs", soldiers, and Bedouins—the emotions of their hearts to be seen on their faces." On 23 November, he arrived in Damascus amid general acclamation and rested at his father's old home there, until the gates of the Citadel of Damascus, whose commander Raihan initially refused to surrender, were opened to Saladin four days later after a brief siege by his brother Tughtakin ibn Ayyub. He installed himself in the castle and received the homage and salutations of the inhabitants.
Further conquests in Syria.
Leaving his brother Tughtigin as Governor of Damascus, Saladin proceeded to reduce other cities that had belonged to Nur al-Din, but were now practically independent. His army conquered Hamah with relative ease, but avoided attacking Homs because of the strength of its citadel. Saladin moved north towards Aleppo, besieging it on December 30 after Gumushtigin refused to abdicate his throne. As-Salih, fearing capture by Saladin, came out of his palace and appealed to the inhabitants not to surrender him and the city to the invading force. One of Saladin's chroniclers claimed "the people came under his spell."
Gumushtigin requested from Rashid ad-Din Sinan, grand-master of the Assassins of Syria, who were already at odds with Saladin since he replaced the Fatimids of Egypt, to assassinate Saladin in his camp. On 11 May 1175 a group of thirteen Assassins easily gained admission into Saladin's camp, but were detected immediately before they carried out their attack by Nasih al-Din Khumartekin of Abu Qubays. One was killed by a general of Saladin and the others were slain while trying to escape. To deter Saladin's progress, Raymond of Tripoli gathered his forces by Nahr al-Kabir where they were well placed for an attack on Muslim territory. Saladin later moved toward Homs instead, but retreated after being told a relief force was being sent to the city by Saif al-Din.
Meanwhile, Saladin's rivals in Syria and Jazira waged a propaganda war against him, claiming he had "forgotten his own condition of Nur ad-Din" and showed no gratitude for his old master by besieging his son, rising "in rebellion against his Lord." Saladin aimed to counter this propaganda by ending the siege, claiming he was defending Islam from the Crusaders; his army returned to Hama to engage a Crusader force there. The Crusaders withdrew beforehand and Saladin proclaimed it "a victory opening the gates of men's hearts." Soon after, Saladin entered Homs and captured its citadel in March 1175, after stubborn resistance from its defenders.
Saladin's successes alarmed Saif al-Din. As head of the Zengids, including Gumushtigin, he regarded Syria and Mesopotamia as his family estate and was angered when Saladin attempted to usurp his dynasty's holdings. Saif al-Din mustered a large army and dispatched it to Aleppo whose defenders anxiously had awaited them. The combined forces of Mosul and Aleppo marched against Saladin in Hama. Heavily outnumbered, Saladin initially attempted to make terms with the Zengids by abandoning all conquests north of the Damascus province, but they refused, insisting he return to Egypt. Seeing that confrontation was unavoidable, Saladin prepared for battle, taking up a superior position at the Horns of Hama, hills by the gorge of the Orontes River. On 13 April 1175, the Zengid troops marched to attack his forces, but soon found themselves surrounded by Saladin's Ayyubid veterans who crushed them. The battle ended in a decisive victory for Saladin who pursued the Zengid fugitives to the gates of Aleppo, forcing as-Salih's advisers to recognize Saladin's control of the provinces of Damascus, Homs and Hama, as well as a number of towns outside Aleppo such as Ma'arat al-Numan.
After his victory against the Zengids, Saladin proclaimed himself king and suppressed the name of as-Salih in Friday prayers and Islamic coinage. From then on, he ordered prayers in all the mosques of Syria and Egypt as the sovereign king and he issued at the Cairo mint gold coins bearing his official title—"al-Malik an-Nasir Yusuf Ayyub, ala ghaya" "the King Strong to Aid, Joseph son of Job; exalted be the standard." The Abbasid caliph in Baghdad graciously welcomed Saladin's assumption of power and declared him "Sultan of Egypt and Syria." The Battle of Hama did not end the contest for power between the Ayyubids and the Zengids, with the final confrontation occurring in the spring of 1176. Saladin had gathered massive reinforcements from Egypt while Saif al-Din was levying troops among the minor states of Diyarbakir and al-Jazira. When Saladin crossed the Orontes, leaving Hama, the sun was eclipsed. He viewed this as an omen, but he continued his march north. He reached the Sultan's Mound, c. 25 km from Aleppo, where his forces encountered Saif al-Din's army. A hand-to-hand fight ensued and the Zengids managed to plow Saladin's left wing, driving it before him, when Saladin himself charged at the head of the Zengid guard. The Zengid forces panicked and most of Saif al-Din's officers ended up being killed or captured—Saif al-Din narrowly escaped. The Zengid army's camp, horses, baggage, tents and stores were seized by the Ayyubids. The Zengid prisoners of war, however, were given gifts and freed. All of the booty from the Ayyubid victory was accorded to the army, Saladin not keeping anything himself.
He continued towards Aleppo which still closed its gates to him, halting before the city. On the way, his army took Buza'a, then captured Manbij. From there they headed west to besiege the fortress of A'zaz on 15 May. Several days later, while Saladin was resting in one of his captain's tents, an assassin rushed forward at him and struck at his head with a knife. The cap of his head armour was not penetrated and he managed to grip the assassin's hand—the dagger only slashing his gambeson—and the assailant was soon killed. Saladin was unnerved at the attempt on his life, which he accused Gumushtugin and the Assassins of plotting, and so increased his efforts in the siege.
A'zaz capitulated on 21 June, and Saladin then hurried his forces to Aleppo to punish Gumushtigin. His assaults were again resisted, but he managed to secure not only a truce, but a mutual alliance with Aleppo, in which Gumushtigin and as-Salih were allowed to continue their hold on the city and in return, they recognized Saladin as the sovereign over all of the dominions he conquered. The "emirs" of Mardin and Keyfa, the Muslim allies of Aleppo, also recognised Saladin as the King of Syria. When the treaty was concluded, the younger sister of as-Salih came to Saladin and requested the return of the Fortress of A'zaz; he complied and escorted her back to the gates of Aleppo with numerous presents.
Campaign against Assassins.
Saladin had by now agreed truces with his Zengid rivals and the Kingdom of Jerusalem (latter occurred in the summer of 1175), but faced a threat from the Ismaili sect known then as the "Assassins" led by Rashid ad-Din Sinan. Based in the an-Nusayriyah Mountains, they commanded nine fortresses, all built on high elevations. As soon as he dispatched the bulk of his troops to Egypt, Saladin led his army into the an-Nusayriyah range in August 1176. He retreated the same month, after laying waste to the countryside, but failing to conquer any of the forts. Most Muslim historians claim that Saladin's uncle, the governor of Hama, mediated a peace agreement between him and Sinan.
Saladin had his guards supplied with link lights and had chalk and cinders strewed around his tent outside Masyaf—which he was besieging—to detect any footsteps by the Assassins. According to this version, one night Saladin's guards noticed a spark glowing down the hill of Masyaf and then vanishing among the Ayyubid tents. Presently, Saladin awoke to find a figure leaving the tent. He saw that the lamps were displaced and beside his bed laid hot scones of the shape peculiar to the Assassins with a note at the top pinned by a poisoned dagger. The note threatened that he would be killed if he didn't withdraw from his assault. Saladin gave a loud cry, exclaiming that Sinan himself was the figure that had left the tent.
Another version claims that Saladin hastily withdrew his troops from Masyaf because they were urgently needed to fend off a Crusader force in the vicinity of Mount Lebanon. In reality, Saladin sought to form an alliance with Sinan and his Assassins, consequently depriving the Crusaders of a potent ally against him. Viewing the expulsion of the Crusaders as a mutual benefit and priority, Saladin and Sinan maintained cooperative relations afterwards, the latter dispatching contingents of his forces to bolster Saladin's army in a number of decisive subsequent battlefronts.
Return to Cairo and forays in Palestine.
After leaving the an-Nusayriyah Mountains, Saladin returned to Damascus and had his Syrian soldiers return home. He left Turan Shah in command of Syria and left for Egypt with only his personal followers, reaching Cairo on 22 September. Having been absent roughly two years, he had much to organize and supervise in Egypt, namely fortifying and reconstructing Cairo. The city walls were repaired and their extensions laid out, while the construction of the Cairo Citadel was commenced. The deep Bir Yusuf ("Joseph's Well") was built on Saladin's orders. The chief public work he commissioned outside of Cairo was the large bridge at Giza, which was intended to form an outwork of defense against a potential Moorish invasion.
Saladin remained in Cairo supervising its improvements, building colleges such as the Madrasa of the Sword Makers and ordering the internal administration of the country. In November 1177, he set out upon a raid into Palestine; the Crusaders had recently forayed into the territory of Damascus, so Saladin saw the truce as no longer worth preserving. The Christians sent a large portion of their army to besiege the fortress of Harim north of Aleppo, so southern Palestine bore few defenders. Saladin found the situation ripe and marched to Ascalon, which he referred to as the "Bride of Syria." William of Tyre recorded that the Ayyubid army consisted of soldiers, of which 8,000 were elite forces and were black soldiers from Sudan. This army proceeded to raid the countryside, sack Ramla and Lod, and dispersed themselves as far as the Gates of Jerusalem.
Battles and truce with Baldwin.
The Ayyubids allowed King Baldwin to enter Ascalon with his Gaza-based Templars without taking any precautions against a sudden attack. Although the Crusader force consisted of only 375 knights, Saladin hesitated to ambush them because of the presence of highly skilled generals. On 25 November, while the greater part of the Ayyubid army was absent, Saladin and his men were surprised near Ramla in the battle of Montgisard. Before they could form up, the Templar force hacked the Ayyubid army down. Initially, Saladin attempted to organize his men into battle order, but as his bodyguards were being killed, he saw that defeat was inevitable and so with a small remnant of his troops mounted a swift camel, riding all the way to the territories of Egypt.
Undiscouraged by his defeat at Tell Jezer, Saladin was prepared to fight the Crusaders once again. In the spring of 1178, he was encamped under the walls of Homs, and a few skirmishes occurred between his generals and the Crusader army. His forces in Hama won a victory over their enemy and brought the spoils, together with many prisoners of war, to Saladin who ordered the captives to be beheaded for "plundering and laying waste the lands of the Faithful." He spent the rest of the year in Syria without a confrontation with his enemies.
Saladin's intelligence services reported to him that the Crusaders were planning a raid into Syria. He ordered one of his generals, Farrukh-Shah, to guard the Damascus frontier with a thousand of his men to watch for an attack, then to retire, avoiding battle, and to light warning beacons on the hills, after which Saladin would march out. In April 1179, the Crusaders led by King Baldwin expected no resistance and waited to launch a surprise attack on Muslim herders grazing their herds and flocks east of the Golan Heights. Baldwin advanced too rashly in pursuit of Farrukh-Shah's force, which was concentrated southeast of Quneitra and was subsequently defeated by the Ayyubids. With this victory, Saladin decided to call in more troops from Egypt; he requested al-Adil to dispatch 1,500 horsemen.
In the summer of 1179, King Baldwin had set up an outpost on the road to Damascus and aimed to fortify a passage over the Jordan River, known as Jacob's Ford, that commanded the approach to the Banias plain (the plain was divided by the Muslims and the Christians). Saladin had offered 100,000 gold pieces for Baldwin to abandon the project, which was particularly offensive to the Muslims, but to no avail. He then resolved to destroy the fortress, called Chastellet and manned by the Templars, moving his headquarters to Banias. As the Crusaders hurried down to attack the Muslim forces, they fell into disorder, with the infantry falling behind. Despite early success, they pursued the Muslims far enough to become scattered, and Saladin took advantage by rallying his troops and charged at the Crusaders. The engagement ended in a decisive Ayyubid victory, and many high-ranking knights were captured. Saladin then moved to besiege the fortress, which fell on 30 August 1179.
In the spring of 1180, while Saladin was in the area of Safad, anxious to commence a vigorous campaign against the Kingdom of Jerusalem, King Baldwin sent messengers to him with proposals of peace. Because droughts and bad harvests hampered his commissariat, Saladin agreed to a truce. Raymond of Tripoli denounced the truce but was compelled to accept after an Ayyubid raid on his territory in May and upon the appearance of Saladin's naval fleet off the port of Tartus.
Domestic affairs.
In June 1180, Saladin hosted a reception for Nur al-Din Muhammad, the Artuqid "emir" of Keyfa, at Geuk Su, in which he presented him and his brother Abu Bakr presents, valued at over 100,000 dinars according to Imad al-Din. This was intended to cement an alliance with the Artuqids and to impress other "emirs" in Mesopotamia and Anatolia. Previously, Saladin offered to mediate relations between Nur al-Din and Kilij Arslan II—the Seljuk Sultan of Rum—after the two came into conflict. The latter demanded Nur al-Din return the lands given to him as a dowry for marrying his daughter when he received reports that she was being abused and used by him to gain Seljuk territory. Nur al-Din requested Saladin mediate the issue, but Arslan refused.
After Nur al-Din and Saladin met at Geuk Su, the top Seljuk "emir", Ikhtiyar al-Din al-Hasan, confirmed Arslan's submission, after which an agreement was drawn up. Saladin was later enraged when he received a message from Arslan accusing Nur al-Din of more abuses against his daughter. He threatened to attack the city of Malatya, saying, "it is two days march for me and I shall not dismount horse until I am in the city." Alarmed at the threat, the Seljuks pushed for negotiations. Saladin felt that Arslan was correct to care for his daughter, but Nur al-Din had taken refuge with him, and therefore he could not betray his trust. It was finally agreed that Arslan's daughter would be sent away for a year and if Nur al-Din failed to comply, Saladin would move to abandon his support for him.
Leaving Farrukh-Shah in charge of Syria, Saladin returned to Cairo at the beginning of 1181. According to Abu Shama, he intended to spend the fast of Ramadan in Egypt and then make the "hajj" pilgrimage to Mecca in the summer. For an unknown reason he apparently changed his plans regarding the pilgrimage and was seen inspecting the Nile River banks in June. He was again embroiled with the Bedouin; he removed two-thirds of their fiefs to use as compensation for the fief-holders at Fayyum. The Bedouin were also accused of trading with the Crusaders and consequently, their grain was confiscated and they were forced to migrate westward. Later, Ayyubid warships were waged against Bedouin river pirates who were plundering the shores of Lake Tanis.
In the summer of 1181, Saladin's former palace administrator Qara-Qush led a force to arrest Majd al-Din—a former deputy of Turan-Shah in the Yemeni town of Zabid—while he was entertaining Imad ad-Din at his estate in Cairo. Saladin's intimates accused Majd al-Din of misappropriating the revenues of Zabid, but Saladin himself believed there was no evidence to back the allegations. He had Majd al-Din released in return for a payment of 80,000 dinars. In addition, other sums were to be paid to Saladin's brothers al-Adil and Taj al-Muluk Buri. The controversial detainment of Majd al-Din was a part of the larger discontent associated with the aftermath of Turan-Shah's departure from Yemen. Although his deputies continued to send him revenues from the province, centralized authority was lacking and internal quarrel arose between Izz al-Din Uthman of Aden and Hittan of Zabid. Saladin wrote in a letter to al-Adil: "this Yemen is a treasure house ... We conquered it, but up to this day we have had no return and no advantage from it. There have been only innumerable expenses, the sending out of troops ... and expectations which did not produce what was hoped for in the end."
Imperial expansions.
Conquest of Mesopotamian hinterland.
Saif al-Din had died earlier in June 1181 and his brother Izz al-Din inherited leadership of Mosul. On December 4, the crown-prince of the Zengids, as-Salih, died in Aleppo. Prior to his death, he had his chief officers swear an oath of loyalty to Izz al-Din, as he was the only Zengid ruler strong enough to oppose Saladin. Izz al-Din was welcomed in Aleppo, but possessing it and Mosul put too great of a strain on his abilities. He thus, handed Aleppo to his brother Imad al-Din Zangi, in exchange for Sinjar. Saladin offered no opposition to these transactions in order to respect the treaty he previously made with the Zengids.
On May 11, 1182, Saladin along with half of the Egyptian Ayyubid army and numerous non-combatants left Cairo for Syria. On the evening before he departed, he sat with his companions and the tutor of one of his sons quoted a line of poetry: "enjoy the scent of the ox-eye plant of Najd, for after this evening it will come no more." Saladin took this as an evil omen and he never saw Egypt again. Knowing that Crusader forces were massed upon the frontier to intercept him, he took the desert route across the Sinai Peninsula to Ailah at the head of the Gulf of Aqaba. Meeting no opposition, Saladin ravaged the countryside of Montreal, whilst Baldwin's forces watched on, refusing to intervene. He arrived in Damascus in June to learn that Farrukh-Shah had attacked the Galilee, sacking Daburiyya and capturing Habis Jaldek, a fortress of great importance to the Crusaders. In July, Saladin dispatched Farrukh-Shah to attack Kawkab al-Hawa. Later, in August, the Ayyubids launched a naval and ground assault to capture Beirut; Saladin led his army in the Bekaa Valley. The assault was leaning towards failure and Saladin abandoned the operation to focus on issues in Mesopotamia.
Kukbary (Gökböri), the "emir" of Harran, invited Saladin to occupy the Jazira region, making up northern Mesopotamia. He complied and the truce between him and the Zengids officially ended in September 1182. Prior to his march to Jazira, tensions had grown between the Zengid rulers of the region, primarily concerning their unwillingness to pay deference to Mosul. Before he crossed the Euphrates, Saladin besieged Aleppo for three days, signaling that the truce was over.
Once he reached Bira, near the river, he was joined by Kukbary and Nur al-Din of Hisn Kayfa and the combined forces captured the cities of Jazira, one after the other. First, Edessa fell, followed by Saruj, then ar-Raqqah, Karkesiya and Nusaybin. Ar-Raqqah was an important crossing point and held by Qutb al-Din Inal, who had lost Manbij to Saladin in 1176. Upon seeing the large size of Saladin's army, he made little effort to resist and surrendered on the condition that he would retain his property. Saladin promptly impressed the inhabitants of the town by publishing a decree that ordered a number of taxes to be canceled and erased all mention of them from treasury records, stating "the most miserable rulers are those whose purses are fat and their people thin." From ar-Raqqah, he moved to conquer al-Fudain, al-Husain, Maksim, Durain, 'Araban, and Khabur—all of which swore allegiance to him.
Saladin proceeded to take Nusaybin which offered no resistance. A medium-sized town, Nusaybin was not of great importance, but it was located in a strategic position between Mardin and Mosul and within easy reach of Diyarbakir. In the midst of these victories, Saladin received word that the Crusaders were raiding the villages of Damascus. He replied "Let them... whilst they knock down villages, we are taking cities; when we come back, we shall have all the more strength to fight them." Meanwhile, in Aleppo, the "emir" of the city Zangi raided Saladin's cities to the north and east, such as Balis, Manbij, Saruj, Buza'a, al-Karzain. He also destroyed his own citadel at A'zaz to prevent it from being used by the Ayyubids if they were to conquer it.
Possession of Aleppo.
Saladin turned his attention from Mosul to Aleppo, sending his brother Taj al-Muluk Buri to capture Tell Khalid, 130 km northeast of the city. A siege was set, but the governor of Tell Khalid surrendered upon the arrival of Saladin himself on May 17 before a siege could take place. According to Imad ad-Din, after Tell Khalid, Saladin took a detour northwards to Ain Tab, but he gained possession of it when his army turned towards it, allowing to quickly move backward another c. 100 km towards Aleppo. On May 21, he camped outside the city, positioning himself east of the Citadel of Aleppo, while his forces encircles the suburb of Banaqusa to the northeast and Bab Janan to the west. He stationed his men dangerously close to the city, hoping for an early success.
Zangi did not offer long resistance. He was unpopular with his subjects and wished to return to his Sinjar, the city he governed previously. An exchange was negotiated where Zangi would hand over Aleppo to Saladin in return for the restoration of his control of Sinjar, Nusaybin, and ar-Raqqa. Zangi would hold these territories as Saladin's vassals on terms of military service. On June 12, Aleppo was formally placed in Ayyubid hands. The people of Aleppo had not known about these negotiations and were taken by surprise when Saladin's standard was hoisted over the citadel. Two "emir"s, including an old friend of Saladin, Izz al-Din Jurduk, welcomed and pledged their service to him. Saladin replaced the Hanafi courts with Shafi'i administration, despite a promise he would not interfere in the religious leadership of the city. Although he was short of money, Saladin also allowed the departing Zangi to take all the stores of the citadel that he could travel with and to sell the remainder—which Saladin purchased himself. In spite of his earlier hesitation to go through with the exchange, he had no doubts about his success, stating that Aleppo was "the key to the lands" and "this city is the eye of Syria and the citadel is its pupil." For Saladin, the capture of the city marked the end of over eight years of waiting since he told Farrukh-Shah that ""we have only to do the milking and Aleppo will be ours"".
After spending one night in Aleppo's citadel, Saladin marched to Harim, near the Crusader-held Antioch. The city was held by Surhak, a "minor "mamluk"." Saladin offered him the city of Busra and property in Damascus in exchange for Harim, but when Surhak asked for more, his own garrison in Harim forced him out. He was arrested by Saladin's deputy Taqi al-Din on allegations that he was planning to cede Harim to Bohemond III of Antioch. When Saladin received its surrender, he proceeded to arrange the defense of Harim from the Crusaders. He reported to the caliph and his own subordinates in Yemen and Baalbek that was going to attack the Armenians. Before he could move, however, there were a number of administrative details to be settled. Saladin agreed to a truce with Bohemond in return for Muslim prisoners being held by him and then he gave A'zaz to Alam ad-Din Suleiman and Aleppo to Saif al-Din al-Yazkuj—the former was an "emir" of Aleppo who joined Saladin and the latter was a former "mamluk" of Shirkuh who helped rescue him from the assassination attempt at A'zaz.
Fight for Mosul.
As Saladin approached Mosul, he faced the issue of taking over a large city and justifying the action. The Zengids of Mosul appealed to an-Nasir, the Abbasid caliph at Baghdad whose vizier favored them. An-Nasir sent Badr al-Badr (a high-ranking religious figure) to mediate between the two sides. Saladin arrived at the city on 10 November 1182. Izz al-Din would not accept his terms because he considered them disingenuous and extensive, and Saladin immediately laid siege to the heavily fortified city.
After several minor skirmishes and a stalemate in the siege that was initiated by the caliph, Saladin intended to find a way to withdraw without damage to his reputation while still keeping up some military pressure. He decided to attack Sinjar, which was held by Izz al-Din's brother Sharaf al-Din. It fell after a 15-day siege on December 30. Saladin's soldiers broke their discipline, plundering the city; Saladin only managed to protect the governor and his officers by sending them to Mosul. After establishing a garrison at Sinjar, he awaited a coalition assembled by Izz al-Din consisting of his forces, those from Aleppo, Mardin, and Armenia. Saladin and his army met the coalition at Harran in February 1183, but on hearing of his approach, the latter sent messengers to Saladin asking for peace. Each force returned to their cities and al-Fadil wrote: ""They al-Din's coalition advanced like men, like women they vanished."" 
On 2 March, al-Adil from Egypt wrote to Saladin that the Crusaders had struck the "heart of Islam." Raynald de Châtillon had sent ships to the Gulf of Aqaba to raid towns and villages off the coast of the Red Sea. It was not an attempt to extend the Crusader influence into that sea or to capture its trade routes, but merely a piratical move. Nonetheless, Imad al-Din writes the raid was alarming to the Muslims because they were not accustomed to attacks on that sea, and Ibn al-Athir adds that the inhabitants had no experience with the Crusaders either as fighters or traders.
Ibn Jubair was told that sixteen Muslim ships were burnt by the Crusaders who then captured a pilgrim ship and caravan at Aidab. He also reported they intended to attack Medina and remove Muhammad's body. Al-Maqrizi added to the rumor by claiming Muhammad's tomb was going to be relocated to Crusader territory so Muslims would make pilgrimages there. Fortunately for Saladin, al-Adil had his warships moved from Fustat and Alexandria to the Red Sea under the command of an Armenian mercenary Lu'lu. They broke the Crusader blockade, destroyed most of their ships, and pursued and captured those who anchored and fled into the desert. The surviving Crusaders, numbered at 170, were ordered to be killed by Saladin in various Muslim cities.
From the point of view of Saladin, in terms of territory, the war against Mosul was going well, but he still failed to achieve his objectives and his army was shrinking; Taqi al-Din took his men back to Hama, while Nasir al-Din Muhammad and his forces had left. This encouraged Izz al-Din and his allies to take the offensive. The previous coalition regrouped at Harzam some 140 km from Harran. In early April, without waiting for Nasir al-Din, Saladin and Taqi al-Din commenced their advance against the coalition, marching eastward to Ras al-Ein unhindered. By late April, after three days of "actual fighting", according to Saladin, the Ayyubids had captured Amid. He handed the city to Nur al-Din Muhammad together with its stores, which consisted of 80,000 candles, a tower full of arrowheads, and 1,040,000 books. In return for a diploma granting him the city, Nur al-Din swore allegiance to Saladin, promising to follow him in every expedition in the war against the Crusaders, and repairing damage done to the city. The fall of Amid, in addition to territory, convinced Il-Ghazi of Mardin to enter the service of Saladin, weakening Izz al-Din's coalition.
Saladin attempted to gain the Caliph an-Nasir's support against Izz al-Din by sending him a letter requesting a document that would give him legal justification for taking over Mosul and its territories. Saladin aimed to persuade the caliph claiming that while he conquered Egypt and Yemen under the flag of the Abbasids, the Zengids of Mosul openly supported the Seljuks (rivals of the caliphate) and only came to the caliph when in need. He also accused Izz al-Din's forces of disrupting the Muslim "Holy War" against the Crusaders, stating "they are not content not to fight, but they prevent those who can." Saladin defended his own conduct claiming that he had come to Syria to fight the Crusaders, end the heresy of the Assassins, and stop the wrong-doing of the Muslims. He also promised that if Mosul was given to him, it would lead to the capture of Jerusalem, Constantinople, Georgia, and the lands of the Almohads in the Maghreb, "until the word of God is supreme and the Abbasid caliphate has wiped the world clean, turning the churches into mosques." Saladin stressed that all this would happen by the will of God, and instead of asking for financial or military support from the caliph, he would capture and give the caliph the territories of Tikrit, Daquq, Khuzestan, Kish Island, and Oman.
Wars against Crusaders.
On 29 September 1182, Saladin crossed the Jordan River to attack Beisan, which was found to be empty. The next day his forces sacked and burned the town and moved westwards. They intercepted Crusader reinforcements from Karak and Shaubak along the Nablus road and took a number of prisoners. Meanwhile, the main Crusader force under Guy of Lusignan moved from Sepphoris to al-Fula. Saladin sent out 500 skirmishers to harass their forces, and he himself marched to Ain Jalut. When the Crusader force—reckoned to be the largest the kingdom ever produced from its own resources, but still outmatched by the Muslims—advanced, the Ayyubids unexpectedly moved down the stream of Ain Jalut. After a few Ayyubid raids—including attacks on Zir'in, Forbelet, and Mount Tabor—the Crusaders still were not tempted to attack their main force, and Saladin led his men back across the river once provisions and supplies ran low.
Crusader attacks provoked further responses by Saladin. Raynald of Châtillon, in particular, harassed Muslim trading and pilgrimage routes with a fleet on the Red Sea, a water route that Saladin needed to keep open. In response, Saladin built a fleet of 30 galleys to attack Beirut in 1182. Raynald threatened to attack the holy cities of Mecca and Medina. In retaliation, Saladin twice besieged Kerak, Raynald's fortress in Oultrejordain, in 1183 and 1184. Raynald responded by looting a caravan of pilgrims on the Hajj in 1185. According to the later 13th-century "Old French Continuation of William of Tyre", Raynald captured Saladin's sister in a raid on a caravan; this claim is not attested in contemporary sources, Muslim or Frankish, however, instead stating that Raynald had attacked a preceding caravan, and Saladin set guards to ensure the safety of his sister and her son, who came to no harm.
Following the failure of his Kerak sieges, Saladin temporarily turned his attention back to another long-term project and resumed attacks on the territory of ʻIzz ad-Dīn (Masʻūd ibn Mawdūd ibn Zangi), around Mosul, which he had begun with some success in 1182. However, since then, Masʻūd had allied himself with the powerful governor of Azerbaijan and Jibal, who in 1185 began moving his troops across the Zagros Mountains, causing Saladin to hesitate in his attacks. The defenders of Mosul, when they became aware that help was on the way, increased their efforts, and Saladin subsequently fell ill, so in March 1186 a peace treaty was signed.
In July 1187 Saladin captured most of the Kingdom of Jerusalem. On July 4, 1187, at the Battle of Hattin, he faced the combined forces of Guy of Lusignan, King Consort of Jerusalem, and Raymond III of Tripoli. In this battle alone the Crusader force was largely annihilated by Saladin's determined army. It was a major disaster for the Crusaders and a turning point in the history of the Crusades. Saladin captured Raynald and was personally responsible for his execution in retaliation for his attacks against Muslim caravans. The members of these caravans had, in vain, besought his mercy by reciting the truce between the Muslims and the Crusaders, but Raynald ignored this and insulted the Islamic prophet, Muhammad, before murdering and torturing a number of them. Upon hearing this, Saladin swore an oath to personally execute Raynald. Guy of Lusignan was also captured. Seeing the execution of Raynald, he feared he would be next. However, his life was spared by Saladin, who said of Raynald, "t is not the wont of kings, to kill kings; but that man had transgressed all bounds, and therefore did I treat him thus." 
Capture of Jerusalem.
Saladin had captured almost every Crusader city. Saladin preferred to take Jerusalem without bloodshed and offered generous terms, but those inside refused to leave their holy city, vowing to destroy it in a fight to the death rather than see it handed over peacefully. Jerusalem capitulated to his forces on Friday, 2 October 1187, after a siege. When the siege had started, Saladin was unwilling to promise terms of quarter to the Frankish inhabitants of Jerusalem. Balian of Ibelin threatened to kill every Muslim hostage, estimated at 5,000, and to destroy Islam's holy shrines of the Dome of the Rock and the al-Aqsa Mosque if such quarter were not provided. Saladin consulted his council and the terms were accepted. The agreement was read out through the streets of Jerusalem so that everyone might within forty days provide for himself and pay to Saladin the agreed tribute for his freedom. An unusually low ransom for the times (around $50 today) was to be paid for each Frank in the city, whether man, woman, or child, but Saladin, against the wishes of his treasurers, allowed many families who could not afford the ransom to leave. Patriarch Heraclius of Jerusalem organised and contributed to a collection that paid the ransoms for about 18,000 of the poorer citizens, leaving another 15,000 to be enslaved. Saladin's brother al-Adil "asked Saladin for a thousand of them for his own use and then released them on the spot." Most of the foot soldiers were sold into slavery. Upon the capture of Jerusalem, Saladin summoned the Jews and permitted them to resettle in the city. In particular, the residents of Ashkelon, a large Jewish settlement, responded to his request.
Tyre, on the coast of modern-day Lebanon, was the last major Crusader city that was not captured by Muslim forces. Strategically, it would have made more sense for Saladin to capture Tyre before Jerusalem; Saladin, however, chose to pursue Jerusalem first because of the importance of the city to Islam. Tyre was commanded by Conrad of Montferrat, who strengthened its defences and withstood two sieges by Saladin. In 1188, at Tortosa, Saladin released Guy of Lusignan and returned him to his wife, Queen Sibylla of Jerusalem. They went first to Tripoli, then to Antioch. In 1189, they sought to reclaim Tyre for their kingdom but were refused admission by Conrad, who did not recognize Guy as king. Guy then set about besieging Acre.
Saladin was on friendly terms with Queen Tamar of Georgia. Saladin's biographer Bahā' ad-Dīn ibn Šaddād reports that, after Saladin's conquest of Jerusalem, the Georgian Queen sent envoys to the sultan to request the return of confiscated possessions of the Georgian monasteries in Jerusalem. Saladin's response is not recorded, but the queen's efforts seem to have been successful as Jacques de Vitry, the Bishop of Acre, reports the Georgians were, in contrast to the other Christian pilgrims, allowed a free passage into the city with their banners unfurled. Ibn Šaddād furthermore claims that Queen Tamar outbid the Byzantine emperor in her efforts to obtain the relics of the True Cross, offering 200,000 gold pieces to Saladin who had taken the relics as booty at the battle of Hattin, but to no avail.
Third Crusade.
Hattin and the fall of Jerusalem prompted the Third Crusade (1189–1192), financed in England by a special "Saladin tithe". Richard the Lionheart, King of England led Guy's siege of Acre, conquered the city and executed 3,000 Muslim prisoners, including women and children. Bahā' ad-Dīn wrote:
The armies of Saladin engaged in combat with the army of King Richard at the Battle of Arsuf on 7 September 1191, at which Saladin's forces, less in number, suffered heavy losses and were forced to withdraw. After the battle of Arsuf, Richard moved his forces towards Ascalon. Anticipating Richard's next move, Saladin emptied the city and camped a few miles away. When Richard arrived at the city, he was stunned to see it abandoned and the towers demolished. The next day when Richard was preparing to retreat to Jaffa, Saladin attacked his army. After a furious battle, Richard managed to save some of his troops and retreated to Ascalon. This was the last major battle between the two forces. All military attempts and battles made by Richard the Lionheart to retake Jerusalem were defeated and failed. Richard only had 2,000 fit soldiers and 50 fit knights to use in battle. With such a small force, he could not expect or hope to take Jerusalem although he got near enough to see the Holy City. Saladin's relationship with Richard was complicated despite their military rivalry. At Arsuf, when Richard lost his horse, Saladin sent him two replacements. Richard proposed that his sister, Joan of England, Queen of Sicily, should marry Saladin's brother and that Jerusalem could be their wedding gift. However, the two men never met face to face and communication was either written or by messenger. As leaders of their respective factions, the two men came to an agreement in the Treaty of Ramla in 1192, whereby Jerusalem would remain in Muslim hands but would be open to Christian pilgrimages. The treaty reduced the Latin Kingdom to a strip along the coast from Tyre to Jaffa.
Death.
Saladin died of a fever on 4 March 1193, at Damascus, not long after King Richard's departure. In Saladin’s possession at the time of his death, there were one piece of gold and forty pieces of silver. He had given away his great wealth to his poor subjects, leaving nothing to pay for his funeral. He was buried in a mausoleum in the garden outside the Umayyad Mosque in Damascus, Syria. Seven centuries later, Emperor Wilhelm II of Germany donated a new marble sarcophagus to the mausoleum. The original sarcophagus was not replaced, however. Instead, the mausoleum, which is open to visitors, now has two sarcophagi: the marble one placed on the side and the original wooden one, which covers Saladin's tomb. (Muslims are buried in a simple shroud, so if there are any sarcophagi present, they are usually used for covering the top of the Islamic burials.) 
Family.
According to Imad al-Din, Saladin had fathered five sons before he left Egypt in 1174. Saladin's oldest son, al-Afdal, was born in 1170, and Uthman was born in 1172 to Shamsa who accompanied Saladin to Syria. Saladin had a third son named, Az-Zahir Ghazi, who later became Lord of Aleppo. Al-Afdal's mother bore Saladin another child in 1177. A letter preserved by Qalqashandi records that a twelfth son was born in May 1178, while on Imad al-Din's list, he appears as Saladin's seventh son. Mas'ud was born in 1175 and Yaq'ub in 1176, the latter to Shamsa.
Recognition and legacy.
Western world.
In the nineteenth century, Saladin achieved a great reputation in Europe as a chivalrous knight, due to his fierce struggle against the crusaders and his generosity. Although Saladin faded into history after the Middle Ages, he appears in a sympathetic light in Gotthold Ephraim Lessing's play "Nathan the Wise" (1779) and in Sir Walter Scott's novel "The Talisman" (1825). The contemporary view of Saladin originates mainly from these texts. According to Jonathan Riley-Smith, Scott's portrayal of Saladin was that of a "modern [19th-century] liberal European gentlemen, beside whom medieval Westerners would always have made a poor showing." Despite the Crusaders' slaughter when they originally conquered Jerusalem in 1099, Saladin granted amnesty and free passage to all common Catholics and even to the defeated Christian army, as long as they were able to pay the aforementioned ransom (the Greek Orthodox Christians were treated even better, because they often opposed the western Crusaders). An interesting view of Saladin and the world in which he lived is provided by Tariq Ali's novel "The Book of Saladin".
Notwithstanding the differences in beliefs, the Muslim Saladin was respected by Christian lords, Richard especially. Richard once praised Saladin as a great prince, saying that he was without doubt the greatest and most powerful leader in the Islamic world. Saladin in turn stated that there was not a more honorable Christian lord than Richard. After the treaty, Saladin and Richard sent each other many gifts as tokens of respect but never met face to face. In April 1191, a Frankish woman's three-month-old baby had been stolen from her camp and had been sold on the market. The Franks urged her to approach Saladin herself with her grievance. According to Bahā' al-Dīn, Saladin used his own money to buy the child back:
At the end of World War I British Commander General Edmund Allenby had succeeded in capturing Damascus from Turkish troops. According to some sources, after his triumphal entry into the city, Allenby raised his sword in salute to the famous statue of Saladin and proudly declared, "Today the wars of the Crusaders are completed." This quotation was incorrectly attributed to Allenby, and throughout his life he vehemently protested against his conquest of Palestine in 1917 having been called a "Crusade." In 1933 Allenby reiterated this stance by saying: "The importance of Jerusalem lay in its strategic importance, there was no religious impulse in this campaign." The British press continued to celebrate his victory over the Ottoman Empire by printing cartoons of Richard the Lionheart looking down on Jerusalem from the heavens with the caption reading "At last my dream has come true."
Muslim world.
In 1898 German Emperor Wilhelm II visited Saladin's tomb to pay his respects. The visit, coupled with anti-imperialist sentiments, led nationalist Arabs to reinvent the image of Saladin and portray him as a hero of the struggle against the West. The image of Saladin they used was the romantic one created by Walter Scott and other Europeans in the West at the time, conveniently ignoring Saladin's Kurdish ethnicity. It replaced Saladin's reputation as a figure who had been largely forgotten in the Muslim world, eclipsed by more successful figures such as Baybars of Egypt.
Modern Arab states have sought to commemorate Saladin through various measures, often based on the image created of him in the 19th-century west. A governorate centered around Tikrit and Samarra in modern-day Iraq, Salah ad Din Governorate, is named after him, as is Salahaddin University in Arbil, the largest city of Iraqi Kurdistan. A suburb community of Arbil, Masif Salahaddin, is also named after him.
Few structures associated with Saladin survive within modern cities. Saladin first fortified the Citadel of Cairo (1175–1183), which had been a domed pleasure pavilion with a fine view in more peaceful times. In Syria, even the smallest city is centred on a defensible citadel, and Saladin introduced this essential feature to Egypt.
Although the Ayyubid dynasty that he founded would only outlive him by 57 years, the legacy of Saladin within the Arab World continues to this day. With the rise of Arab nationalism in the 20th Century, particularly with regard to the Arab-Israeli conflict, Saladin's heroism and leadership gained a new significance. Saladin's recapture of Palestine from the European Crusaders is considered an inspiration for modern-day Arabs' opposition to Zionism. Moreover, the glory and comparative unity of the Arab World under Saladin was seen as the perfect symbol for the new unity sought by Arab nationalists, such as Gamal Abdel Nasser. For this reason, the Eagle of Saladin became the symbol of revolutionary Egypt, and was subsequently adopted by several other Arab states (the United Arab Republic, Iraq, Libya, the State of Palestine, and Yemen).

</doc>
<doc id="26984" url="https://en.wikipedia.org/wiki?curid=26984" title="Sophocles">
Sophocles

Sophocles (; , "Sophoklēs", ; 497/6 – winter 406/5 BC) is one of three ancient Greek tragedians whose plays have survived. His first plays were written later than those of Aeschylus, and earlier than or contemporary with those of Euripides. Sophocles wrote 120 plays during the course of his life, but only seven have survived in a complete form: "Ajax", "Antigone", "The Women of Trachis", "Oedipus the King", "Electra", "Philoctetes" and "Oedipus at Colonus". For almost 50 years, Sophocles was the most-fêted playwright in the dramatic competitions of the city-state of Athens that took place during the religious festivals of the Lenaea and the Dionysia. He competed in 30 competitions, won 18, and was never judged lower than second place. Aeschylus won 14 competitions, and was sometimes defeated by Sophocles, while Euripides won 5 competitions.
The most famous tragedies of Sophocles feature Oedipus and also Antigone: they are generally known as the Theban plays, although each play was actually a part of a different tetralogy, the other members of which are now lost. Sophocles influenced the development of the drama, most importantly by adding a third actor, thereby reducing the importance of the chorus in the presentation of the plot. He also developed his characters to a greater extent than earlier playwrights such as Aeschylus.
Life.
Sophocles, the son of Sophilus, was a wealthy member of the rural "deme" (small community) of Hippeios Colonus in Attica, which was to become a setting for one of his plays, and he was probably born there. Sophocles was born a few years before the Battle of Marathon in 490 BC: the exact year is unclear, although 497/6 is the most likely. Sophocles was born into a wealthy family (his father was an armour manufacturer) and was highly educated. Sophocles' first artistic triumph was in 468 BC, when he took first prize in the Dionysia theatre competition over the reigning master of Athenian drama, Aeschylus. According to Plutarch, the victory came under unusual circumstances. Instead of following the usual custom of choosing judges by lot, the archon asked Cimon and the other "strategoi" present to decide the victor of the contest. Plutarch further contends that following this loss Aeschylus soon left for Sicily. Although Plutarch says that this was Sophocles' first production, it is now thought that his first production was probably in 470 BC. "Triptolemus" was probably one of the plays that Sophocles presented at this festival.
In 480 BC Sophocles was chosen to lead the paean (a choral chant to a god), celebrating the Greek victory over the Persians at the Battle of Salamis. Early in his career, the politician Cimon might have been one of his patrons, although if he was, there was no ill will borne by Pericles, Cimon's rival, when Cimon was ostracized in 461 BC. In 443/2 he served as one of the "Hellenotamiai", or treasurers of Athena, helping to manage the finances of the city during the political ascendancy of Pericles. According to the "Vita Sophoclis", in 441 BC he was elected one of the ten generals, executive officials at Athens, as a junior colleague of Pericles, and he served in the Athenian campaign against Samos; he was supposed to have been elected to this position as the result of his production of "Antigone".
In 420 BC, he welcomed and set up an altar for the image of Asclepius at his house, when the deity was introduced to Athens. For this, he was given the posthumous epithet "Dexion" (receiver) by the Athenians. He was also elected, in 413 BC, one of the commissioners ("probouloi") who responded to the catastrophic destruction of the Athenian expeditionary force in Sicily during the Peloponnesian War.
Sophocles died at the age of ninety or ninety-one in the winter of 406/5 BC, having seen within his lifetime both the Greek triumph in the Persian Wars and the bloodletting of the Peloponnesian War. As with many famous men in classical antiquity, his death inspired a number of apocryphal stories. The most famous is the suggestion that he died from the strain of trying to recite a long sentence from his "Antigone" without pausing to take a breath. Another account suggests he choked while eating grapes at the Anthesteria festival in Athens. A third holds that he died of happiness after winning his final victory at the City Dionysia. A few months later, a comic poet, in a play titled "The Muses", wrote this eulogy: "Blessed is Sophocles, who had a long life, was a man both happy and talented, and the writer of many good tragedies; and he ended his life well without suffering any misfortune." According to some accounts, however, his own sons tried to have him declared incompetent near the end of his life; he is said to have refuted their charge in court by reading from his as yet unproduced "Oedipus at Colonus". One of his sons, Iophon, and a grandson, also called Sophocles, also became playwrights.
Several ancient sources mention Sophocles' homosexuality or bisexuality. Athenaios reported that Sophocles loved boys like Euripides loved women. The poet Ion of Chios relates an anecdote involving Sophocles seducing a serving boy at a symposium, and Athenaios one in which Sophocles is tricked by a hustler.
Works and legacy.
Among Sophocles' earliest innovations was the addition of a third actor, which further reduced the role of the chorus and created greater opportunity for character development and conflict between characters. Aeschylus, who dominated Athenian playwriting during Sophocles' early career, followed suit and adopted the third character into his own work towards the end of his life. Aristotle credits Sophocles with the introduction of "skenographia", or scenery-painting. It was not until after the death of the old master Aeschylus in 456 BC that Sophocles became the pre-eminent playwright in Athens.
Thereafter, Sophocles emerged victorious in dramatic competitions at 18 Dionysia and 6 Lenaia festivals. In addition to innovations in dramatic structure, Sophocles' work is also known for its deeper development of characters than earlier playwrights. His reputation was such that foreign rulers invited him to attend their courts, although unlike Aeschylus who died in Sicily, or Euripides who spent time in Macedon, Sophocles never accepted any of these invitations. Aristotle used Sophocles' "Oedipus the King" in his "Poetics" (c. 335 BC) as an example of the highest achievement in tragedy, which suggests the high esteem in which his work was held by later Greeks.
Only two of the seven surviving plays can be dated securely: "Philoctetes" (409 BC) and "Oedipus at Colonus" (401 BC, staged after Sophocles' death by his grandson). Of the others, "Electra" shows stylistic similarities to these two plays, which suggests that it was probably written in the latter part of his career. "Ajax", "Antigone" and "The Trachiniae" are generally thought to be among his early works, again based on stylistic elements, with "Oedipus the King" coming in Sophocles' middle period. Most of Sophocles' plays show an undercurrent of early fatalism and the beginnings of Socratic logic as a mainstay for the long tradition of Greek tragedy.
Theban plays.
The Theban plays consist of three plays: "Oedipus the King" (also called "Oedipus Tyrannus" or by its Latin title "Oedipus Rex"), "Oedipus at Colonus" and "Antigone". All three plays concern the fate of Thebes during and after the reign of King Oedipus. They have often been published under a single cover. Sophocles, however, wrote the three plays for separate festival competitions, many years apart. Not only are the Theban plays not a true trilogy (three plays presented as a continuous narrative) but they are not even an intentional series and contain some inconsistencies among them. He also wrote other plays having to do with Thebes, such as the "Epigoni", of which only fragments have survived.
Subjects.
Each of the plays relates to the tale of the mythological Oedipus, who killed his father and married his mother without knowledge that they were his parents. His family is fated to be doomed for three generations.
In "Oedipus the King", Oedipus is the protagonist. Oedipus' infanticide is planned by his parents, Laius and Jocasta, to avert him from fulfilling a prophecy; in truth, the servant entrusted with the infanticide passes the infant on through a series of intermediaries to a childless couple, who adopt him not knowing his history. Oedipus eventually learns of the Delphic Oracle's prophecy of him, that he would kill his father and marry his mother; Oedipus attempts to flee his fate without harming those he knows as his parents (at this point, he does not know that he is adopted). Oedipus meets a man at a crossroads accompanied by servants; Oedipus and the man fought, and Oedipus killed the man. (This man was his father, Laius, not that anyone apart from the gods knew this at the time). He becomes the ruler of Thebes after solving the riddle of the sphinx and in the process, marries the widowed queen, his mother Jocasta. Thus the stage is set for horror. When the truth comes out, following from another true but confusing prophecy from Delphi, Jocasta commits suicide, Oedipus blinds himself and leaves Thebes. At the end of the play, order is restored. This restoration is seen when Creon, brother of Jocasta, becomes king, and also when Oedipus, before going off to exile, asks Creon to take care of his children. Oedipus's children will always bear the weight of shame and humiliation because of their father's actions.
In "Oedipus at Colonus", the banished Oedipus and his daughter Antigone arrive at the town of Colonus where they encounter Theseus, King of Athens. Oedipus dies and strife begins between his sons Polyneices and Eteocles.
In "Antigone", the protagonist is Oedipus' daughter, Antigone. She is faced with the choice of allowing her brother Polyneices' body to remain unburied, outside the city walls, exposed to the ravages of wild animals, or to bury him and face death. The king of the land, Creon, has forbidden the burial of Polyneices for he was a traitor to the city. Antigone decides to bury his body and face the consequences of her actions. Creon sentences her to death. Eventually, Creon is convinced to free Antigone from her punishment, but his decision comes too late and Antigone commits suicide. Her suicide triggers the suicide of two others close to King Creon: his son, Haemon, who was to wed Antigone, and his wife, Eurydice, who commits suicide after losing her only surviving son.
Composition and inconsistencies.
The plays were written across thirty-six years of Sophocles' career and were not composed in chronological order, but instead were written in the order "Antigone", "Oedipus the King", and "Oedipus at Colonus". Nor were they composed as a "trilogy" – a group of plays to be performed together, but are the remaining parts of three different groups of plays. As a result, there are some inconsistencies: notably, Creon is the undisputed king at the end of "Oedipus the King" and, in consultation with Apollo, single-handedly makes the decision to expel Oedipus from Thebes. Creon is also instructed to look after Oedipus' daughters Antigone and Ismene at the end of "Oedipus the King". By contrast, in the other plays there is some struggle with Oedipus' sons Eteocles and Polynices in regard to the succession. In "Oedipus at Colonus", Sophocles attempts to work these inconsistencies into a coherent whole: Ismene explains that, in light of their tainted family lineage, her brothers were at first willing to cede the throne to Creon. Nevertheless, they eventually decided to take charge of the monarchy, with each brother disputing the other's right to succeed. In addition to being in a clearly more powerful position in "Oedipus at Colonus", Eteocles and Polynices are also culpable: they consent (l. 429, Theodoridis, tr.) to their father's going to exile, which is one of his bitterest charges against them.
Other plays.
In addition to the three Theban plays, there are four surviving plays by Sophocles: "Ajax", "The Women of Trachis", "Electra", and "Philoctetes", the last of which won first prize in 409 BC in which it competed.
"Ajax" focuses on the proud hero of the Trojan War, Telamonian Ajax, who is driven to treachery and eventually suicide. Ajax becomes gravely upset when Achilles’ armor is presented to Odysseus instead of himself. Despite their enmity toward him, Odysseus persuades the kings Menelaus and Agamemnon to grant Ajax a proper burial.
"The Women of Trachis" (named for the Trachinian women who make up the chorus) dramatizes Deianeira's accidentally killing Heracles after he had completed his famous twelve labors. Tricked into thinking it is a love charm, Deianeira applies poison to an article of Heracles' clothing; this poisoned robe causes Heracles to die an excruciating death. Upon learning the truth, Deianeira commits suicide.
"Electra" corresponds roughly to the plot of Aeschylus' "Libation Bearers". It details how Electra and Orestes' avenge their father Agamemnon's murder by Clytemnestra and Aegisthus.
"Philoctetes" retells the story of Philoctetes, an archer who had been abandoned on Lemnos by the rest of the Greek fleet while on the way to Troy. After learning that they cannot win the Trojan War without Philoctetes' bow, the Greeks send Odysseus and Neoptolemus to retrieve him; due to the Greeks' earlier treachery, however, Philoctetes refuses to rejoin the army. It is only Heracles' deus ex machina appearance that persuades Philoctetes to go to Troy.
Fragmentary plays.
Although the list of over 120 titles of plays associated with Sophocles are known and presented below, little is known of the precise dating of most of them. "Philoctetes" is known to have been written in 409 BC, and "Oedipus at Colonus" is known to have only been performed in 401 BC, posthumously, at the initiation of Sophocles' grandson. The convention on writing plays for the Greek festivals was to submit them in tetralogies of three tragedies along with one satyr play. Along with the unknown dating of the vast majority of over 120 play titles, it is also largely unknown how the plays were grouped. It is, however, known that the three plays referred to in the modern era as the "Theban plays" were never performed together in Sophocles' own lifetime, and are therefore not a trilogy (which they are sometimes erroneously seen as).
Fragments of "Ichneutae" ("Tracking Satyrs") were discovered in Egypt in 1907. These amount to about half of the play, making it the best preserved satyr play after Euripides' "Cyclops", which survives in its entirety. Fragments of the "Epigoni" were discovered in April 2005 by classicists at Oxford University with the help of infrared technology previously used for satellite imaging. The tragedy tells the story of the second siege of Thebes. A number of other Sophoclean works have survived only in fragments, including:
Sophocles' view of his own work.
There is a passage of Plutarch's tract "De Profectibus in Virtute 7 " in which Sophocles discusses his own growth as a writer. A likely source of this material for Plutarch was the "Epidemiae" of Ion of Chios, a book that recorded many conversations of Sophocles. This book is a likely candidate to have contained Sophocles' discourse on his own development because Ion was a friend of Sophocles, and the book is known to have been used by Plutarch. Though some interpretations of Plutarch's words suggest that Sophocles says that he imitated Aeschylus, the translation does not fit grammatically, nor does the interpretation that Sophocles said that he was making fun of Aeschylus' works. C. M. Bowra argues for the following translation of the line:
"After practising to the full the bigness of Aeschylus, then the painful ingenuity of my own invention, now in the third stage I am changing to the kind of diction which is most expressive of character and best."
Here Sophocles says that he has completed a stage of Aeschylus' work, meaning that he went through a phase of imitating Aeschylus' style but is finished with that. Sophocles' opinion of Aeschylus was mixed. He certainly respected him enough to imitate his work early on in his career, but he had reservations about Aeschylus' style, and thus did not keep his imitation up. Sophocles' first stage, in which he imitated Aeschylus, is marked by "Aeschylean pomp in the language". Sophocles' second stage was entirely his own. He introduced new ways of evoking feeling out of an audience, as in his "Ajax", when Ajax is mocked by Athene, then the stage is emptied so that he may commit suicide alone. Sophocles mentions a third stage, distinct from the other two, in his discussion of his development. The third stage pays more heed to diction. His characters spoke in a way that was more natural to them and more expressive of their individual character feelings.

</doc>
<doc id="26985" url="https://en.wikipedia.org/wiki?curid=26985" title="Salinity">
Salinity

Salinity is the saltiness or dissolved salt content of a body of water (see also soil salinity).
Salinity is an important factor in determining many aspects of the chemistry
of natural waters and of biological processes within it, and is a thermodynamic state variable that, along with temperature and pressure, governs physical characteristics like the density and heat capacity of the water.
A contour line of constant salinity is called an isohaline – or sometimes isohale.
Definitions.
Salinity in rivers, lakes, and the ocean is conceptually simple, but technically challenging to define and measure precisely. Conceptually the salinity is the quantity of dissolved salt content of the water. Salts are compounds like sodium chloride, magnesium sulfate, potassium nitrate, and sodium bicarbonate which dissolve into ions. The concentration of dissolved chloride ions is sometimes referred to as chlorinity. Operationally, dissolved matter is defined as that which can pass through a very fine filter (historically a filter with a pore size of 0.45 μm, but nowadays usually 0.2 μm). Salinity can be expressed in the form of a mass fraction, i.e. the mass of the dissolved material in a unit mass of solution.
Seawater typically has a salinity of around 35 g/kg, although lower values are typical near coasts where rivers enter the ocean. Rivers and lakes can have a wide range of salinities, from less than 0.01 g/kg to a few g/kg, although there are many places where higher salinities are found. The Dead Sea has a salinity of more than 200 g/kg.
Whatever pore size is used in the definition, the resulting salinity value of a given sample of natural water will not vary by more than a few percent (%). Physical oceanographers working in the abyssal ocean, however, are often concerned with precision and intercomparability of measurements by different researchers, at different times, to almost five significant digits. A bottled seawater product known as IAPSO Standard Seawater is used by oceanographers to standardize their measurements with enough precision to meet this requirement.
Composition.
Measurement and definition difficulties arise because natural waters contain a complex mixture of many different elements from different sources (not all from dissolved salts) in different molecular forms. The chemical properties of some of these forms depend on temperature and pressure. Many of these forms are difficult to measure with high accuracy, and in any case complete chemical analysis is not practical when analyzing multiple samples. Different practical definitions of salinity result from different attempts to account for these problems, to different levels of precision, while still remaining reasonably easy to use.
For practical reasons salinity is usually related to the sum of masses of a subset of these dissolved chemical constituents (so-called "solution salinity"), rather than to the unknown mass of salts that gave rise to this composition (an exception is when artificial seawater is created). For many purposes this sum can be limited to a set of eight major ions in natural waters, although for seawater at highest precision an additional seven minor ions are also included. The major ions dominate the inorganic composition of most (but by no means all) natural waters. Exceptions include some pit lakes and waters from some hydrothermal springs.
The concentrations of dissolved gases like oxygen and nitrogen are not usually included in descriptions of salinity. However, carbon dioxide gas, which when dissolved is partially converted into carbonates and bicarbonates, is often included. Silicon in the form of silicic acid, which usually appears as a neutral molecule in the pH range of most natural waters, may also be included for some purposes (e.g., when salinity/density relationships are being investigated).
Seawater.
[[File:Aquarius flat 2048x1024.ogv|thumb|
Full 3 minute NASA video Feb 27,2013
The NASA Aquarius instrument aboard Argentina's SAC-D satellite is designed to measure global sea surface salinity. This movie shows salinity patterns as measured by Aquarius from December 2011 through December 2012. Red colors represent areas of high salinity, while blue shades represent areas of low salinity. It is important to understand salinity, the amount of dissolved salts in water, because it will lead us to better understanding of the water cycle and can lead to improved climate models. High concentrations (over 37 practical salinity units) are usually in the center of the ocean basins away from the mouths of rivers, which input fresh water. High concentrations are also in sub-tropical regions due to high rates of evaporation (clear skies, little rain, and prevailing winds) and in landlocked seas in arid regions. At high latitudes, salinity is low. This can be attributed to lower evaporation rates and the melting of ice that dilutes seawater. To sum up, salinity is low where precipitation is greater than evaporation, mainly in coastal or equatorial regions. Credit: NASA/GSFC/JPL-Caltech
The term 'salinity' is, for oceanographers, usually associated with one of a set of specific measurement techniques. As the dominant techniques evolve, so do different descriptions of salinity. The distinctions between these different descriptions are important to physical oceanographers but are obscure and confusing to nonspecialists.
Salinities were largely measured using titration-based techniques before the 1980s. Titration with silver nitrate could be used to determine the concentration of halide ions (mainly chlorine and bromine) to give a chlorinity. The chlorinity was then multiplied by a factor to account for all other constituents. The resulting 'Knudsen salinities' are expressed in units of parts per thousand (ppt or ‰).
The use of electrical conductivity measurements to estimate the ionic content of seawater led to the development of the so-called "practical salinity scale 1978" (PSS-78). Salinities measured using PSS-78 do not have units. The suffix psu or PSU (denoting "practical salinity unit") is sometimes added to PSS-78 measurement values, however this practice is officially discouraged.
In 2010 a new standard for the properties of seawater was introduced, the so-called "thermodynamic equation of seawater 2010" (TEOS-10). This standard includes a new scale, the so-called "reference composition salinity scale". Absolute salinities on this scale are expressed as a mass fraction, in grams per kilogram of solution. Salinities on this scale are determined by combining electrical conductivity measurements with other information that can account for regional changes in the composition of seawater. They can also be determined by making direct density measurements.
A sample of seawater from most locations with a chlorinity of 19.37 ppt will have a Knudsen salinity of 35.00 ppt, a PSS-78 practical salinity of about 35.0, and a TEOS-10 absolute salinity of about 35.2 g/kg. The electrical conductivity of this water at a temperature of 15 °C is 42.9 mS/cm.
Lakes and rivers.
Limnologists and chemists often define salinity in terms of mass of salt per unit volume, expressed in units of mg per litre or g per litre. It is implied, although often not stated, that this value applies accurately only at some reference temperature. Values presented in this way are typically accurate to of the order of 1%. Limnologists also use electrical conductivity, or "reference conductivity", as a proxy for salinity. This measurement may be corrected for temperature effects, and is usually expressed in units of μS/cm.
A river or lake water with a salinity of around 70 mg/L will typically have a specific conductivity at 25 °C of between 80 and 130 μS/cm. The actual ratio depends on the ions present. The actual conductivity usually changes by about 2% per degree Celsius, so the measured conductivity at 5 °C might only be in the range of 50–80 μS/cm.
Direct density measurements are also used to estimate salinities, particularly in highly saline lakes. Sometimes density at a specific temperature is used as a proxy for salinity. At other times an empirical salinity/density relationship developed for a particular body of water is used to estimate the salinity of samples from a measured density.
Systems of classification of water bodies based upon salinity.
Marine waters are those of the ocean, another term for which is "euhaline seas". The salinity of euhaline seas is 30 to 35. "Brackish seas" or waters have salinity in the range of 0.5 to 29 and "metahaline seas" from 36 to 40. These waters are all regarded as "thalassic" because their salinity is derived from the ocean and defined as "homoiohaline" if salinity does not vary much over time (essentially constant). The table on the right, modified from Por (1972), follows the "Venice system" (1959).
In contrast to homoiohaline environments are certain "poikilohaline" environments (which may also be "thalassic") in which the salinity variation is biologically significant. "Poikilohaline" water salinities may range anywhere from 0.5 to greater than 300. The important characteristic is that these waters tend to vary in salinity over some biologically meaningful range seasonally or on some other roughly comparable time scale. Put simply, these are bodies of water with quite variable salinity.
Highly saline water, from which salts crystallize (or are about to), is referred to as brine.
Environmental considerations.
Salinity is an ecological factor of considerable importance, influencing the types of organisms that live in a body of water. As well, salinity influences the kinds of plants that will grow either in a water body, or on land fed by a water (or by a groundwater). A plant adapted to saline conditions is called a halophyte. A halophyte which is tolerant to residual sodium carbonate salinity are called glasswort or saltwort or barilla plants. Organisms (mostly bacteria) that can live in very salty conditions are classified as extremophiles, or halophiles specifically. An organism that can withstand a wide range of salinities is euryhaline.
Salt is expensive to remove from water, and salt content is an important factor in water use (such as potability).
The degree of salinity in oceans is a driver of the world's ocean circulation, where density changes due to both salinity changes and temperature changes at the surface of the ocean produce changes in buoyancy, which cause the sinking and rising of water masses. Changes in the salinity of the oceans are thought to contribute to global changes in carbon dioxide as more saline waters are less soluble to carbon dioxide. In addition, during glacial periods, the hydrography is such that a possible cause of reduced circulation is the production of stratified oceans. Hence it is difficult in this case to subduct water through the thermohaline circulation.

</doc>
<doc id="26987" url="https://en.wikipedia.org/wiki?curid=26987" title="Saxifragales">
Saxifragales

The Saxifragales are an order of flowering plants. Their closest relatives are a large eudicot group known as the rosids by the definition of rosids given in the APG II classification system. Some authors define the rosids more widely, including Saxifragales as their most basal group. Saxifragales is one of the eight groups that compose the core eudicots. The others are Gunnerales, Dilleniaceae, rosids, Santalales, Berberidopsidales, Caryophyllales, and asterids.
Saxifragales have an extensive fossil record. The extant members are apparently remnants of a formerly diverse and widespread order.
The Saxifragales order, as it is now understood, is based upon the results of molecular phylogenetic studies of DNA sequences. It is not part of any of the classification systems based on plant morphology. The group is much in need of comparative anatomical study, especially in light of the recent expansion of the family Peridiscaceae to include "Medusandra", a genus that before 2009 had usually not been placed in Saxifragales.
The order is divided into suprafamilial groups as shown on the phylogenetic tree below. These groups are informal and are not understood to have any particular taxonomic rank.
Families.
Saxifragales contain about 2470 species. These are distributed into 15 families, or into 12 families if Haloragaceae "sensu lato" is recognized as a family consisting of Haloragaceae "sensu stricto", "Penthorum", "Tetracarpaea", and "Aphanopetalum". About 95% of the species are in five families: Crassulaceae (1400), Saxifragaceae (500), Grossulariaceae (150 - 200), Haloragaceae (150), and Hamamelidaceae (100). Most of the families are monogeneric. The number of genera in each family is:
Some authors do not recognize "Choristylis" as a separate genus from "Itea". Similarly, some authors sink "Liquidambar" and "Semiliquidambar" into "Altingia". Thus Altingiaceae and Iteaceae are monogeneric in some classifications.
History.
Within the Saxifragales is a suprafamilial group known as the Saxifragaceae alliance. It comprises four families: Pterostemonaceae, Iteaceae, Grossulariaceae, and Saxifragaceae. These have long been known to be related to each other, but the circumscription of Saxifragaceae has changed dramatically. It is now a much smaller family than it had been. Crassulaceae and Tetracarpaeaceae have long been associated with Saxifragaceae. "Penthorum" has usually been associated with Crassulaceae, but sometimes with Saxifragaceae.
Two members of the core Saxifragales had sometimes been placed near Saxifragaceae, but usually elsewhere. "Aphanopetalum" was often placed in Cunoniaceae, a family in Oxalidales, even though there were good reasons to put it in Saxifragales. "Aphanopetalum" is now excluded from Cunoniaceae. Haloragaceae was often thought to be a family in Myrtales, but it is no longer included in that order.
Cercidiphyllaceae had for a long time been associated with Hamamelidaceae and Trochodendraceae and was often thought to be closer to the latter. Cercidiphyllaceae is now known to be a member of the woody clade of Saxifragales, along with Hamamelidaceae, Altingiaceae, and Daphniphyllaceae, but Trochodendraceae is in the basal eudicot order Trochodendrales. Altingiaceae was usually not separated from Hamamelidaceae until phylogenetic studies showed that its inclusion might make Hamamelidaceae paraphyletic. The recognition of Altingiaceae as a separate family received strong statistical support in 2008.
"Daphniphyllum" was always thought to have an anomalous combination of characters and it was placed in several different orders before molecular phylogenetic analysis showed it to belong to Saxifragales.
Paeoniaceae possesses many unique features and its taxonomic position was for a long time controversial. The idea has long persisted that "Paeonia" belongs in Ranunculales, close to "Glaucidium". Paeoniaceae has been shown unequivocally to belong in Saxifragales, while "Glaucidium" is in the family Ranunculaceae.
The family Peridiscaceae underwent radical shifting and recircumscription from 2003 to 2009. Originally, it consisted of two closely related genera, "Peridiscus" and "Whittonia". The APG II system placed the family in Malpighiales, based on a DNA sequence for the "rbcL" gene from "Whittonia". This sequence turned out to be not from "Whittonia", but from other plants whose DNA had contaminated the sample. After Peridiscaceae was finally placed in Saxifragales, it was expanded to include "Soyauxia" in 2007, and expanded again to include "Medusandra" in 2009.
Phylogeny.
The phylogeny shown below is based on the one published by Shuguang Jian and coauthors in 2008. All branches have 100% maximum likelihood bootstrap support except where labeled with bootstrap percentage. Monogeneric families are represented by genus names.

</doc>
<doc id="26988" url="https://en.wikipedia.org/wiki?curid=26988" title="CLIÉ">
CLIÉ

The Sony CLIÉ was a series of personal digital assistants running the Palm Operating System developed and marketed by Sony from 2000 to 2005. The devices introduced many new features to the PDA market, such as a jog-wheel interface, high-resolution displays, and Sony technologies like Memory Stick slots and ATRAC3 audio playback. Most models were designed and manufactured in Japan. The name is an acronym for "creativity, lifestyle, innovation, emotion" though formerly "communication, link, information and entertainment". It was initially an attempt at a new coinage term, though it means "tool" in the Jèrriais language.
The CLIÉ handhelds were distinguished from other Palm OS models by their emphasis on multimedia capabilities, including photo, video, and audio playback, long before any other Palm OS PDAs had such capabilities. Later models have been credited with spurring competition in the previously stagnant Palm market, closing many of the gaps that existed between Palm OS PDAs and those powered by Microsoft's Windows Mobile operating system, particularly on the multimedia front, but also with Sony's proprietary application launcher interface.
Closure of handheld line.
In the summer of 2004, Sony announced that new CLIÉs would, from then on, be manufactured and available only in Japan, and in the spring of 2005, Sony announced the total termination of its CLIÉ line of products. The last models to be released worldwide were the PEG-TJ27, PEG-TJ37, and PEG-TH55. The last model released in Japan was the PEG-VZ90. Soon after the closure of the CLIÉ line, Sony stopped providing original installation drivers, including Sony's version of Palm Desktop for the CLIÉ, which are necessary for Hotsyncing with the PC and otherwise taking advantage of the handhelds' many features for which a PC may be required. Several CLIÉ fans took it upon themselves to offer these drivers freely for download at www.sonyclie.org.
Models.
CLIÉ handhelds were released in series, usually with a few models released in each series. In later years, multiple series would be in production at the same time.
Macintosh support.
Officially, the CLIÉ line did not support the Mac OS, and Sony never provided any software with the handhelds for Mac OS. However, as a Palm OS device, every CLIÉ handheld was inherently capable of HotSync operations with a Mac OS computer. This allowed for the synchronization of the basic PIM functions, as well as for the installation of new software, though this inherent capability was unusable because the Mac HotSync software would not recognize the handheld. PalmSource, however, silently added the capability to recognize older CLIÉ devices when providing new versions of its Palm Desktop software for Mac. This was necessary for those who could synchronize only via USB.
The CLIÉ user community soon discovered that these "updates" were simply a matter of adding a few lines to the USB-detection property-list file. Since then, detailed instructions have been posted online for those who want to synchronize their CLIÉ handhelds. No modifications are required for Bluetooth synchronization, but Wi-Fi synchronization is impossible because the Mac OS HotSync software does not support network synchronization. Some workarounds for the multimedia features also exist. For those who desire stronger Mac OS/CLIÉ integration, the product Missing Sync made by the company Mark/Space is also available. This does make unencrypted Wi-Fi synchronization possible but a bug in the CLIÉ network stack reverses IP addresses which means that the Macintosh involved needs a palindromic IP address such as 10.0.0.10.

</doc>
<doc id="26989" url="https://en.wikipedia.org/wiki?curid=26989" title="Sony">
Sony

, commonly referred to as Sony , is a Japanese multinational conglomerate corporation headquartered in Kōnan Minato, Tokyo, Japan. Its diversified business includes consumer and professional electronics, gaming, entertainment and financial services. The company is one of the leading manufacturers of electronic products for the consumer and professional markets. Sony is ranked 116th on the 2015 list of Fortune Global 500.
Sony Corporation is the electronics business unit and the parent company of the Sony Group, which is engaged in business through its four operating segments – electronics (including video games, network services and medical business), motion pictures, music and financial services. These make Sony one of the most comprehensive entertainment companies in the world. Sony's principal business operations include Sony Corporation (Sony Electronics in the U.S.), Sony Pictures Entertainment, Sony Interactive Entertainment, Sony Music Entertainment, Sony Mobile Communications (formerly Sony Ericsson) and Sony Financial. Sony is among the Worldwide Top 20 Semiconductor Sales Leaders and as of 2013, the fourth-largest television manufacturer in the world, after Samsung Electronics, LG Electronics and TCL.
The is a Japan-based corporate group primarily focused on the Electronics (such as AV/IT products and components), Game (such as the PlayStation), Entertainment (such as motion pictures and music) and Financial Services (such as insurance and banking) sectors. The group consists of Sony Corporation (holding and electronics), Sony Interactive Entertainment (games), Sony Pictures Entertainment (motion pictures), Sony Music Entertainment (music), Sony/ATV Music Publishing (music publishing), Sony Financial Holdings (financial services) and others.
The company's current slogan is "BE MOVED". Their former slogans were "make.believe" (2009–2014) and "like.no.other" (2005–2014).
Sony is a member of the SMFG (Sumitomo Mitsui Financial Group) keiretsu, the successor to the Mitsui keiretsu to which it previously belonged.
History.
Tokyo Tsushin Kogyo.
Sony began in the wake of World War II. In 1946, Masaru Ibuka started an electronics shop in a department store building in Tokyo. The company had $530 in capital and a total of eight employees. In the following year he was joined by his colleague, Akio Morita, and they founded a company called "Tokyo Tsushin Kogyo" (Tokyo Telecommunications Engineering Corporation). The company built Japan's first tape recorder, called the Type-G. In 1958 the company changed its name to "Sony".
Name.
When Tokyo Tsushin Kogyo was looking for a romanized name to use to market themselves, they strongly considered using their initials, TTK. The primary reason they did not is that the railway company Tokyo Kyuko was known as TTK. The company occasionally used the acronym "Totsuko" in Japan, but during his visit to the United States, Morita discovered that Americans had trouble pronouncing that name. Another early name that was tried out for a while was "Tokyo Teletech" until Akio Morita discovered that there was an American company already using Teletech as a brand name.
The name "Sony" was chosen for the brand as a mix of two words. One was the Latin word ""Sonus"", which is the root of sonic and sound, and the other was ""Sonny"", a common slang term used in 1950s America to call a boy. In the 1950s Japan "sonny boys", was a loan word into Japanese which connoted smart and presentable young men, which Sony founders Akio Morita and Masaru Ibuka considered themselves to be.
The first Sony-branded product, the TR-55 transistor radio, appeared in 1955 but the company name did not change to Sony until January 1958.
At the time of the change, it was extremely unusual for a Japanese company to use Roman letters to spell its name instead of writing it in kanji. The move was not without opposition: TTK's principal bank at the time, Mitsui, had strong feelings about the name. They pushed for a name such as Sony Electronic Industries, or Sony Teletech. Akio Morita was firm, however, as he did not want the company name tied to any particular industry. Eventually, both Ibuka and Mitsui Bank's chairman gave their approval.
Globalization.
According to Schiffer, Sony's TR-63 radio "cracked open the U.S. market and launched the new industry of consumer microelectronics." By the mid-1950s, American teens had begun buying portable transistor radios in huge numbers, helping to propel the fledgling industry from an estimated 100,000 units in 1955 to 5 million units by the end of 1968.
Sony co-founder Akio Morita founded Sony Corporation of America in 1960. In the process, he was struck by the mobility of employees between American companies, which was unheard of in Japan at that time. When he returned to Japan, he encouraged experienced, middle-aged employees of other companies to reevaluate their careers and consider joining Sony. The company filled many positions in this manner, and inspired other Japanese companies to do the same. Moreover, Sony played a major role in the development of Japan as a powerful exporter during the 1960s, 1970s and 1980s. It also helped to significantly improve American perceptions of "made in Japan" products. Known for its production quality, Sony was able to charge above-market prices for its consumer electronics and resisted lowering prices.
In 1971, Masaru Ibuka handed the position of president over to his co-founder Akio Morita. Sony began a life insurance company in 1979, one of its many peripheral businesses. Amid a global recession in the early 1980s, electronics sales dropped and the company was forced to cut prices. Sony's profits fell sharply. "It's over for Sony," one analyst concluded. "The company's best days are behind it." Around that time, Norio Ohga took up the role of president. He encouraged the development of the Compact Disc in the 1970s and 1980s, and of the PlayStation in the early 1990s. Ohga went on to purchase CBS Records in 1988 and Columbia Pictures in 1989, greatly expanding Sony's media presence. Ohga would succeed Morita as chief executive officer in 1989.
Under the vision of co-founder Akio Morita and his successors, the company had aggressively expanded into new businesses. Part of its motivation for doing so was the pursuit of "convergence," linking film, music and digital electronics via the Internet. This expansion proved unrewarding and unprofitable, threatening Sony's ability to charge a premium on its products as well as its brand name. In 2005, Howard Stringer replaced Nobuyuki Idei as chief executive officer, marking the first time that a foreigner had run a major Japanese electronics firm. Stringer helped to reinvigorate the company's struggling media businesses, encouraging blockbusters such as "Spider-Man" while cutting 9,000 jobs. He hoped to sell off peripheral business and focus the company again on electronics. Furthermore, he aimed to increase cooperation between business units, which he described as "silos" operating in isolation from one another. In a bid to provide a unified brand for its global operations, Sony introduced a slogan known as "make.believe" in 2009.
Despite some successes, the company faced continued struggles in the mid- to late-2000s. In 2012, Kazuo Hirai was promoted to president and CEO, replacing Sir Howard Stringer. Shortly thereafter, Hirai outlined his company-wide initiative, named "One Sony" to revive Sony from years of financial losses and bureaucratic management structure, which proved difficult for former CEO Stringer to accomplish, partly due to differences in business culture and native languages between Stringer and some of Sony's Japanese divisions and subsidiaries. Hirai outlined three major areas of focus for Sony's electronics business, which include imaging technology, gaming and mobile technology, as well as a focus on reducing the major losses from the television business.
In February 2014, Sony announced the sale of its Vaio PC division to a new corporation owned by investment fund Japan Industrial Partners and spinning its TV division into its own corporation as to make it more nimble to turn the unit around from past losses totaling $7.8 billion over a decade. Later that month, they announced that they would be closing 20 stores. In April, the company announced that they would be selling 9.5 million shares in Square Enix (roughly 8.2 percent of the game company's total shares) in a deal worth approximately $48 million. In May 2014 the company announced it was forming two joint ventures with Shanghai Oriental Pearl Group to manufacture and market Sony's PlayStation games consoles and associated software in China.
Formats and technologies.
Sony has historically been notable for creating its own in-house standards for new recording and storage technologies, instead of adopting those of other manufacturers and standards bodies. Sony (either alone or with partners) has introduced several of the most popular recording formats, including the floppy disk, Compact Disc and Blu-ray Disc.
Video recording.
The company launched the Betamax videocassette recording format in 1975. Sony became embroiled in the infamous videotape format war of the early 1980s, when Sony was marketing the Betamax system for video cassette recorders against the VHS format developed by JVC. In the end, VHS gained critical mass in the marketbase and became the worldwide standard for consumer VCRs.
While Betamax is for all practical purposes an obsolete format, a professional-oriented component video format called Betacam that was derived from Betamax is still used today, especially in the television industry, although far less so in recent years with the introduction of digital and high definition.
In 1985, Sony launched their Handycam products and the Video8 format. Video8 and the follow-on hi-band Hi8 format became popular in the consumer camcorder market. In 1987 Sony launched the 4 mm DAT or Digital Audio Tape as a new digital audio tape standard.
Audio recording.
In 1979 the Walkman brand was introduced, in the form of the world's first portable music player using the compact cassette format. Sony introduced the MiniDisc format in 1992 as an alternative to Philips DCC or Digital Compact Cassette and as a successor to the compact cassette. Since the introduction of MiniDisc, Sony has attempted to promote its own audio compression technologies under the ATRAC brand, against the more widely used MP3. Until late 2004, Sony's Network Walkman line of digital portable music players did not support the MP3 standard natively.
In 2004, Sony built upon the MiniDisc format by releasing Hi-MD. Hi-MD allows the playback and recording of audio on newly introduced 1 GB Hi-MD discs in addition to playback and recording on regular MiniDiscs. In addition to saving audio on the discs, Hi-MD allows the storage of computer files such as documents, videos and photos.
Audio encoding.
In 1993, Sony challenged the industry standard Dolby Digital 5.1 surround sound format with a newer and more advanced proprietary motion picture digital audio format called SDDS (Sony Dynamic Digital Sound). This format employed eight channels (7.1) of audio opposed to just six used in Dolby Digital 5.1 at the time. Ultimately, SDDS has been vastly overshadowed by the preferred DTS (Digital Theatre System) and Dolby Digital standards in the motion picture industry. SDDS was solely developed for use in the theatre circuit; Sony never intended to develop a home theatre version of SDDS.
Sony and Philips jointly developed the Sony-Philips digital interface format (S/PDIF) and the high-fidelity audio system SACD. The latter has since been entrenched in a format war with DVD-Audio. At present, neither has gained a major foothold with the general public. CDs are preferred by consumers because of ubiquitous presence of CD drives in consumer devices.
Optical storage.
In 1983, Sony followed their counterpart Philips to the Compact Disc (CD). In addition to developing consumer-based recording media, after the launch of the CD Sony began development of commercially based recording media. In 1986 they launched Write-Once optical discs (WO) and in 1988 launched Magneto-optical discs which were around 125MB size for the specific use of archival data storage. In 1984, Sony launched the Discman series which extended their Walkman brand to portable CD products.
In the early 1990s, two high-density optical storage standards were being developed: one was the MultiMedia Compact Disc (MMCD), backed by Philips and Sony, and the other was the Super Density disc (SD), supported by Toshiba and many others. Philips and Sony abandoned their MMCD format and agreed upon Toshiba's SD format with only one modification. The unified disc format was called DVD and was introduced in 1997.
Sony was one of the leading developers of the Blu-ray Disc optical disc format, the newest standard for disc-based content delivery. The first Blu-ray players became commercially available in 2006. The format emerged as the standard for HD media over the competing format, Toshiba's HD DVD, after a two-year-long high definition optical disc format war.
Disk storage.
In 1983 Sony introduced 90 mm micro diskettes (better known as floppy disks), which it had developed at a time when there were 4" floppy disks, and a lot of variations from different companies, to replace the then on-going 5.25" floppy disks. Sony had great success and the format became dominant. 3.5" floppy disks gradually became obsolete as they were replaced by current media formats.
Flash memory.
Sony launched in 1998 their Memory Stick format, flash memory cards for use in Sony lines of digital cameras and portable music players. It has seen little support outside of Sony's own products, with Secure Digital cards (SD) commanding considerably greater popularity. Sony has made updates to the Memory Stick format with Memory Stick Duo and Memory Stick Micro.
Business units.
Sony offers products in a variety of product lines around the world. Sony has developed a music playing robot called Rolly, dog-shaped robots called AIBO and a humanoid robot called QRIO.
As of 1 April 2014, Sony is organized into the following business segments: Mobile Communications (MC), Game & Network Services (G&NS), Imaging Products & Solutions (IP&S), Home Entertainment & Sound (HE&S), Devices, Pictures, Music, Financial Services and All Other. The network and medical businesses are included in the G&NS and IP&S, respectively.
Electronics.
Sony Corporation.
Sony Corporation is the electronics business unit and the parent company of the Sony Group. It primarily conducts strategic business planning of the group, research and development (R&D), planning, designing and marketing for electronics products. Its subsidiaries such as Sony Global Manufacturing & Operations Corporation (SGMO; ４ plants in Japan), Sony Semiconductor Manufacturing Corporation (7 plants in Japan), Sony Storage Media and Devices Corporation, Sony Energy Devices Corporation and its subsidiaries outside Japan (Brazil, China, UK (Wales), India, Malaysia, Singapore, South Korea, Thailand, Ireland and United States) are responsible for manufacturing as well as product engineering (SGMO is also responsible for customer service operations). In 2012, Sony rolled most of its consumer content services (including video, music and gaming) into the Sony Entertainment Network.
Audio.
Sony produced the world's first portable music player, the Walkman in 1979. This line fostered a fundamental change in music listening habits by allowing people to carry music with them and listen to music through lightweight headphones. Walkman originally referred to portable audio cassette players. The company now uses the Walkman brand to market its portable audio and video players as well as a line of former Sony Ericsson mobile phones.
Sony utilized a related brand, Discman, to refer to its CD players. It dropped this name in the late 1990s.
Computing.
Sony produced computers (MSX home computers and NEWS workstations) during the 1980s, exclusively for sale in the Japanese market. The company withdrew from the computer business around 1990. Sony entered again into the global computer market under the new VAIO brand, began in 1996. Short for "Video Audio Integrated Operation", the line was the first computer brand to highlight visual-audio features.
Sony faced considerable controversy when some of its laptop batteries exploded and caught fire in 2006, resulting in the largest computer-related recall to that point in history.
In a bid to join the tablet computer market, the company launched its Sony Tablet line of Android tablets in 2011. Since 2012, Sony's Android products have been marketed under the Xperia brand used for its smartphones.
On 4 February 2014, Sony announced that it will sell its VAIO PC business due to poor sales and Japanese company Japan Industrial Partners (JIP) will purchase the VAIO brand, with the deal finalized by the end of March 2014. Sony maintains a minority stake in the new, independent company.
Photography & Videography.
Sony offers a wide range of digital cameras. Point-and-shoot models adopt the Cyber-shot name, while digital single-lens reflex models are branded using Alpha.
The first Cyber-shot was introduced in 1996. At the time, digital cameras were a relative novelty. Sony's market share of the digital camera market fell from a high of 20% to 9% by 2005.
Sony entered the market for digital single-lens reflex cameras in 2006 when it acquired the camera business of Konica Minolta. Sony rebranded the company's line of cameras as its Alpha line. Sony is the world's third largest manufacturer of the cameras, behind Canon and Nikon respectively.
There are also a variety of Camcorders which are manufactured by Sony.
Video.
In 1968 Sony introduced the Trinitron brand name for its lines of aperture grille cathode ray tube televisions and (later) computer monitors. Sony stopped production of Trinitron for most markets, but continued producing sets for markets such as Pakistan, Bangladesh and China. Sony discontinued its series of Trinitron computer monitors in 2005. The company discontinued the last Trinitron-based television set in the USA in early 2007. The end of Trinitron marked the end of Sony's analog television sets and monitors.
Sony used the LCD WEGA name for its LCD TVs until summer 2005. The company then introduced the BRAVIA name. BRAVIA is an in house brand owned by Sony which produces high-definition LCD televisions, projection TVs and front projectors, home cinemas and the BRAVIA home theatre range. All Sony high-definition flat-panel LCD televisions in North America have carried the logo for BRAVIA since 2005. Sony is the third-largest maker of televisions in the world. , Sony's television business has been unprofitable for eight years.
In December 2011, Sony agreed to sell all stake in an LCD joint venture with Samsung Electronics for about $940 million. On 28 March 2012, Sony Corporation and Sharp Corporation announced that they have agreed to further amend the joint venture agreement originally executed by the parties in July 2009, as amended in April 2011, for the establishment and operation of Sharp Display Products Corporation ("SDP"), a joint venture to produce and sell large-sized LCD panels and modules.
On November 9, 2015 Sony announced that they are going to stop producing Betamax Tapes in March 2016.
Sony also sells a range of DVD players. It has shifted its focus in recent years to promoting the Blu-ray format, including discs and players.
Semiconductor and components.
Sony produces a wide range of semiconductors and electronic components including image sensors, image processor (BIONZ), laser diodes, system LSIs, mixed-signal LSIs, OLED panels, etc. The company has a strong presence in the image sensor market. Sony-manufactured CCD and CMOS image sensors are widely used in digital cameras, tablet computers and smartphones.
Medical-related business.
Sony has targeted medical, healthcare and biotechnology business as a growth sector in the future. The company acquired iCyt Mission Technology, Inc. (renamed Sony Biotechnology Inc. in 2012), a manufacture of flow cytometers, in 2010 and Micronics, Inc., a developer of microfluidics-based diagnostic tools, in 2011.
In 2012, Sony announced that it will acquire all shares of So-net Entertainment Corporation, which is the majority shareholder of M3, Inc., an operator of portal sites (m3.com, MR-kun, MDLinx and MEDI:GATE) for healthcare professionals.
On 28 September 2012, Olympus and Sony announced that the two companies will establish a joint venture to develop new surgical endoscopes with 4K resolution (or higher) and 3D capability. Sony Olympus Medical Solutions Inc. (Sony 51%, Olympus 49%) was established on 16 April 2013.
On 28 February 2014, Sony, M3 and Illumina established a joint venture called P5, Inc. to provide a genome analysis service for research institutions and enterprises in Japan.
Sony Mobile Communications.
Sony Mobile Communications Inc. (formerly Sony Ericsson) is a multinational mobile phone manufacturing company headquartered in Tokyo, Japan and a wholly owned subsidiary of Sony Corporation.
In 2001, Sony entered into a joint venture with Swedish telecommunications company Ericsson, forming Sony Ericsson. Initial sales were rocky, and the company posted losses in 2001 and 2002. However, SMC reached a profit in 2003. Sony Ericsson distinguished itself with multimedia-capable mobile phones, which included features such as cameras. These were unusual for the time. Despite their innovations, SMC faced intense competition from Apple's iPhone, released in 2007. From 2008 to 2010, amid a global recession, SMC slashed its workforce by several thousand. Sony acquired Ericsson's share of the venture in 2012 for over US$1 billion. In 2009, SMC was the fourth-largest mobile phone manufacturer in the world (after Nokia, Samsung and LG). By 2010, its market share had fallen to sixth place. Sony Mobile Communications now focuses exclusively on the smartphone market under the Xperia name. In 2015, Sony released Xperia Z5 Premium in Canada following US and Europe.
In the year 2013, Sony contributed to two percent of the mobile phone market with 37 million mobile phones sold.
Sony Interactive Entertainment.
Sony Interactive Entertainment (formerly Sony Computer Entertainment) is best known for producing the popular line of PlayStation consoles. The line grew out of a failed partnership with Nintendo. Originally, Nintendo requested for Sony to develop an add-on for its console that would play Compact Discs. In 1991 Sony announced the add-on, as well as a dedicated console known as the "Play Station". However, a disagreement over software licensing for the console caused the partnership to fall through. Sony then continued the project independently.
Launched in 1994, the first PlayStation gained 61% of global console sales and broke Nintendo's long-standing lead in the market. Sony followed up with the PlayStation 2 in 2000, which was even more successful. The console has become the most successful of all time, selling over 150 million units . Sony released the PlayStation 3, a high-definition console, in 2006. It was the first console to use the Blu-ray format, although its expensive Cell processor made it considerably more expensive than competitors Xbox 360 and Wii. Early on, poor sales performance resulted in significant losses for the company, pushing it to sell the console at a loss. The PlayStation 3 sold generally more poorly than its competitors in the early years of its release but managed to overtake the Xbox 360 in global sales later on. It later introduced the PlayStation Move, an accessory that allows players to control video games using motion gestures.
Sony extended the brand to the portable games market in 2005 with the PlayStation Portable (PSP). The console has sold reasonably, but has taken a second place to a rival handheld, the Nintendo DS. Sony developed the Universal Media Disc (UMD) optical disc medium for use on the PlayStation Portable. Early on, the format was used for movies, but it has since lost major studio support. Sony released a disc-less version of its PlayStation Portable, the PSP Go. The company went on to release its second portable video game system, PlayStation Vita, in 2011 and 2012. Sony launched its fourth console, the PlayStation 4, on 15 November 2013, which as of 3 January 2016 has sold 35.9 million units.
On 18 March 2014, at GDC, President of Sony Computer Entertainment Worldwide Studios Shuhei Yoshida announced their new virtual reality technology dubbed Project Morpheus, and later named PlayStation VR, for PlayStation 4. The headset, still in prototype form, will bring VR gaming and non-gaming software to the company's new console. According to a report released by Houston-based patent consulting firm LexInnova in May 2015, Sony is leading the virtual reality patent race. According to the firm’s analysis of nearly 12,000 patents or patent applications, Sony has 366 virtual reality patents or patent applications.
Sony Creative Software.
Sony develops consumer software for digital video (Sony Vegas Pro and Movie Studio), audio (Sound Forge), music production (ACID Pro), DVD production and Blu-ray Disc authoring (DVD Architect Pro). Sony Sound Series is a collection of royalty-free loops and samples.
Electric vehicles and batteries.
In 2014, Sony participated within NRG Energy eVgo Ready for Electric Vehicle (REV) program, for EV charging parking lots.
Sony is in the business of electric vehicle lithium-ion batteries.
IT giants such as Google (driverless car) and Apple (iCar/Project Titan) are working on electric vehicles and self driving cars, competing with Tesla; Sony is entering into this field by investing $842,000 in the ZMP company.
Entertainment.
Sony Pictures Entertainment.
Sony Pictures Entertainment, Inc. (SPE) is the television and film production/distribution unit of Sony. With 12.5% box office market share in 2011, the company was ranked third among movie studios. Its group sales in 2010 were $7.2 billion USD. The company has produced many notable movie franchises, including "Spider-Man", "The Karate Kid" and "Men in Black 3". It has also produced the popular television game shows "Jeopardy!" and "Wheel of Fortune".
Sony entered the television and film production market when it acquired Columbia Pictures Entertainment in 1989 for $3.4 billion. Columbia lives on in the Columbia TriStar Motion Picture Group, a subsidiary of SPE which in turn owns Columbia Pictures and TriStar Pictures. SPE's television division is known as Sony Pictures Television.
For the first several years of its existence, Sony Pictures Entertainment performed poorly, leading many to suspect the company would sell off the division. Sony Pictures Entertainment encountered controversy in the early 2000s. In July 2000, a marketing executive working for Sony Corporation created a fictitious film critic, David Manning, who gave consistently good reviews for releases from Sony subsidiary Columbia Pictures that generally received poor reviews amongst real critics. Sony later pulled the ads, suspended Manning's creator and his supervisor and paid fines to the state of Connecticut and to fans who saw the reviewed films in the US. In 2006 Sony started using ARccOS Protection on some of their film DVDs, but later issued a recall.
Sony Music Entertainment.
Sony Music Entertainment (also known as SME or Sony Music) is the second-largest global recorded music company of the "big four" record companies and is controlled by Sony Corporation of America, the United States subsidiary of Japan's "Sony". The company owns full or partial rights to the catalogues of Michael Jackson, The Beatles, Usher, Eminem, Akon and others.
In one of its largest-ever acquisitions, Sony purchased CBS Record Group in 1987 for US$2 billion. In the process, Sony gained the rights to the catalogue of Michael Jackson, considered by the "Guinness Book of World Records" to be the most successful entertainer of all time. The acquisition of CBS Records provided the foundation for the formation of Sony Music Entertainment, which Sony established in 1991.
In 2004, Sony entered into a joint venture with Bertelsmann AG, merging Sony Music Entertainment with Bertelsmann Music Group to create Sony BMG. In 2005, Sony BMG faced a copy protection scandal, because its music CDs had installed malware on users' computers that was posing a security risk to affected customers. In 2007, the company acquired Famous Music for US$370 million, gaining the rights to the catalogues of Eminem and Akon, among others.
Sony bought out Bertelsmann's share in the company and formed a new Sony Music Entertainment in 2008. Since then, the company has undergone management changes. In January 1988, Sony acquired CBS Records and the 50% of CBS/Sony Group. In March 1988, four wholly owned subsidiaries were folded into CBS/Sony Group and the company was renamed as Sony Music Entertainment Japan
Sony/ATV Music Publishing.
Besides its record label, Sony operates other music businesses. In 1995, Sony purchased a 50% stake in ATV Music Publishing, forming Sony/ATV Music Publishing. At the time, the publishing company was the second-largest of its kind in the world. The company owns much of the publishing rights to the catalog of The Beatles. Sony purchased digital music recognition company Gracenote for $260 million USD in 2008.
Finance.
Sony Financial Services.
Sony Financial Holdings is a holding company for Sony's financial services business. It owns and oversees the operation of Sony Life (in Japan and the Philippines), Sony Assurance, Sony Bank and Sony Bank Securities. The company is headquartered in Tokyo, Japan.
Sony Financial accounts for half of Sony's global earnings. The unit proved the most profitable of Sony's businesses in fiscal year 2006, earning $1.7 billion in profit. Sony Financial's low fees have aided the unit's popularity while threatening Sony's premium brand name.
Mobile payments.
Sony wants to contend with Apple Inc. and Samsung Electronics Co. on mobile payments in Asia. Sony plans to use its contact-less payment technology to make ground in the public transportation industry across Asia. The system, known as FeliCa, relies on two forms of technologies to make it viable, either chips embedded in smartphones or plastic cards with chips embedded in them. Sony plans to implement this technology in train systems in Indonesia as early as Spring 2016.
Corporate information.
Finances.
Sony is one of Japan's largest corporations by revenue. It had revenues of ¥6.493 trillion in 2012. It also maintains large reserves of cash, with ¥895 billion on hand as of 2012. In May 2012, Sony shares were valued at about $15 billion.
The company was immensely profitable throughout the 1990s and early 2000s, in part because of the success of its new PlayStation line. The company encountered financial difficulty in the mid- to late-2000s due to a number of factors: the global financial crisis, increased competition for PlayStation, and the devastating Japanese earthquake of 2011. The company faced three consecutive years of losses leading up to 2011. While noting the negative effects of intervening circumstances such as natural disasters and fluctuating currency exchange rates, the "Financial Times" criticized the company for its "lack of resilience" and "inability to gauge the economy." The newspaper voiced skepticism about Sony's revitalization efforts, given a lack of tangible results.
In September 2000 Sony had a market capitalization of $100 billion; but by December 2011 it had plunged to $18 billion, reflecting falling prospects for Sony but also reflecting grossly inflated share prices of the 'dot.com' years. Net worth, as measured by stockholder equity, has steadily grown from $17.9 billion in March 2002 to $35.6 billion through December 2011. Earnings yield (inverse of the price to earnings ratio) has never been more than 5% and usually much less; thus Sony has always traded in over-priced ranges with the exception of the 2009 market bottom.
In April 2012, Sony announced that it would reduce its workforce by 10,000 (6% of its employee base) as part of CEO Hirai's effort to get the company back into the black. This came after a loss of 520 billion yen (roughly US$6.36 billion) for fiscal 2012, the worst since the company was founded. Accumulation loss for the past four years was 919.32 billion-yen. Sony planned to increase its marketing expenses by 30% in 2012.
1,000 of the jobs cut come from the company's mobile phone unit's workforce. 700 jobs will be cut in the 2012–2013 fiscal year and the remaining 300 in the following fiscal year.
On 9 December 2008, Sony Corporation announced that it would be cutting 8,000 jobs, dropping 8,000 contractors and reducing its global manufacturing sites by 10% to save $1.1 billion per year.
In January 2013, Sony announced it was selling its US headquarters building for $1.1 billion to a consortium led by real estate developer The Chetrit Group.
On 28 January 2014, Moody's Investors Services dropped Sony's credit rating to Ba1—"judged to have speculative elements and a significant credit risk"—saying that the company's "profitability is likely to remain weak and volatile."
On 6 February 2014, Sony announced it would trim as many as 5,000 jobs as it attempts to sell its PC business and focus on mobile and tablets.
Environmental record.
In November 2011, Sony was ranked 9th (jointly with Panasonic) in Greenpeace's Guide to Greener Electronics. This chart grades major electronics companies on their environmental work. The company scored 3.6/10, incurring a penalty point for comments it has made in opposition to energy efficiency standards in California. It also risks a further penalty point in future editions for being a member of trade associations that have commented against energy efficiency standards. Together with Philips, Sony receives the highest score for energy policy advocacy after calling on the EU to adopt an unconditional 30% reduction target for greenhouse gas emissions by 2020. Meanwhile, it receives full marks for the efficiency of its products. In 2007, Sony ranked 14th on the Greenpeace guide. Sony fell from its earlier 11th-place ranking due to Greenpeace's claims that Sony had double standards in their waste policies.
Since 1976, Sony has had an Environmental Conference. Sony's policies address their effects on global warming, the environment, and resources. They are taking steps to reduce the amount of greenhouse gases that they put out as well as regulating the products they get from their suppliers in a process that they call "green procurement". Sony has said that they have signed on to have about 75 percent of their Sony Building running on geothermal power. The "Sony Take Back Recycling Program" allow consumers to recycle the electronics products that they buy from Sony by taking them to eCycle (Recycling) drop-off points around the U.S. The company has also developed a biobattery that runs on sugars and carbohydrates that works similarly to the way living creatures work. This is the most powerful small biobattery to date.
In 2000, Sony faced criticism for a document entitled "NGO Strategy" that was leaked to the press. The document involved the company's surveillance of environmental activists in an attempt to plan how to counter their movements. It specifically mentioned environmental groups that were trying to pass laws that held electronics-producing companies responsible for the cleanup of the toxic chemicals contained in their merchandise.
Community engagement.
"EYE SEE project".
Sony Corporation is actively involved in the EYE SEE project conducted by UNICEF. EYE SEE digital photography workshops have been run for children in Argentina, Tunisia, Mali, South Africa, Ethiopia, Madagascar, Rwanda, Liberia and Pakistan.
"South Africa Mobile Library Project".
Sony assists The South Africa Primary Education Support Initiative (SAPESI) through financial donations and children book donations to the South Africa Mobile Library Project.
"The Sony Canada Charitable Foundation".
The Sony Canada Charitable Foundation (SCCF) is a non-profit organization which supports three key charities; the Make-A-Wish Canada, the United Way of Canada and the EarthDay and ECOKIDS program.
"Sony Foundation" and "You Can".
After the 2011 Queensland floods and Victorian bushfires, Sony Music released benefit albums with money raised going to the Sony Foundation.
You Can is the youth cancer program of Sony Foundation.
"Open Planet Ideas Crowdsourcing Project".
Sony launched its Open Planet Ideas Crowdsourcing Project, in partnership with the World Wildlife Fund and the design group, IDEO.
"Street Football Stadium Project".
On the occasion of the 2014 World Cup in Brazil, Sony partnered with streetfootballworld and launched the Street Football Stadium Project to support football-based educational programmes in local communities across Latin America and Brazil. More than 25 Street Stadiums were developed since the project's inception.

</doc>
<doc id="26990" url="https://en.wikipedia.org/wiki?curid=26990" title="Social psychology">
Social psychology

In psychology, social psychology is the scientific study of how people's thoughts, feelings, and behaviors are influenced by the actual, imagined, or implied presence of others. In this definition, "scientific" refers to the empirical method of investigation. The terms "thoughts", "feelings", and "behaviors" include all psychological variables that are measurable in a human being. The statement that others' presence may be "imagined" or "implied" suggests that we are prone to social influence even when no other people are present, such as when watching television, or following internalized cultural norms.
Social psychologists typically explain human behavior as a result of the interaction of mental states and immediate social situations.
Social psychologists therefore deal with the factors that lead us to behave in a given way in the presence of others, and look at the conditions under which certain behavior/actions and feelings occur. Social psychology is concerned with the way these feelings, thoughts, beliefs, intentions and goals are constructed and how such psychological factors, in turn, influence our interactions with others.
Social psychology is a discipline that had traditionally bridged the gap between psychology and sociology. During the years immediately following World War II there was frequent collaboration between psychologists and sociologists. However, the two disciplines have become increasingly specialized and isolated from each other in recent years, with sociologists focusing on "macro variables" (e.g., social structure) to a much greater extent. Nevertheless, sociological approaches to social psychology remain an important counterpart to psychological research in this area.
In addition to the split between psychology and sociology, there has been a somewhat less pronounced difference in emphasis between American social psychologists and European social psychologists. As a generalization, American researchers traditionally have focused more on the individual, whereas Europeans have paid more attention to group level phenomena (see group dynamics).
History.
Although there were some older treatises about social psychology, such as those by Islamic philosopher Al-Farabi (Alpharabius), the discipline of social psychology, as its modern-day definition, began in the United States at the dawn of the 20th century. However, the discipline had already developed a significant foundation. Following the 18th century, those in the emerging field of social psychology were concerned with developing concrete explanations for different aspects of human nature. They desired to discover concrete cause and effect relationships that explained the social interactions in the world around them. In order to do so, they believed that the scientific method, an empirically based scientific measure, could be applied to human behavior.
The first published study in this area was an experiment in 1898 by Norman Triplett, on the phenomenon of social facilitation. During the 1930s, many Gestalt psychologists, most notably Kurt Lewin, fled to the United States from Nazi Germany. They were instrumental in developing the field as something separate from the behavioral and psychoanalytic schools that were dominant during that time, and social psychology has always maintained the legacy of their interests in perception and cognition. Attitudes and small group phenomena were the most commonly studied topics in this era.
During World War II, social psychologists studied persuasion and propaganda for the U.S. military. After the war, researchers became interested in a variety of social problems, including gender issues and racial prejudice. Most notable, revealing, and contentious of these were the Stanley Milgram shock experiments on obedience to authority. In the sixties, there was growing interest in new topics, such as cognitive dissonance, bystander intervention, and aggression. By the 1970s, however, social psychology in America had reached a crisis. There was heated debate over the ethics of laboratory experimentation, whether or not attitudes really predicted behavior, and how much science could be done in a cultural context. This was also the time when a radical situationist approach challenged the relevance of self and personality in psychology.
Social psychology reached a more mature level in both theories and methods during the 1980s and 1990s. Careful ethical standards now regulate research. Pluralistic and multicultural perspectives have emerged. Modern researchers are interested in many phenomena, but attribution, social cognition, and the self-concept are perhaps the greatest areas of growth in recent years. Social psychologists have also maintained their applied interests with contributions in health, environmental, and legal psychology.
Intrapersonal phenomena.
Attitudes.
In social psychology, attitudes are defined as learned, global evaluations of a person, object, place, or issue that influence thought and action. Put more simply, attitudes are basic expressions of approval or disapproval, favorability or unfavorability, or as Bem put it, likes and dislikes. Examples would include liking chocolate ice cream, or endorsing the values of a particular political party.
Social psychologists have studied attitude formation, the structure of attitudes, attitude change, the function of attitudes, and the relationship between attitudes and behavior. Because people are influenced by the situation, general attitudes are not always good predictors of specific behavior. For example, for a variety of reasons, a person may value the environment but not recycle a can on a particular day.
In recent times, research on attitudes has examined the distinction between traditional, self-reported attitude measures and "implicit" or unconscious attitudes. For example, experiments using the Implicit Association Test have found that people often demonstrate implicit bias against other races, even when their explicit responses reveal equal mindedness. One study found that explicit attitudes correlate with verbal behavior in interracial interactions, whereas implicit attitudes correlate with nonverbal behavior.
One hypothesis on how attitudes are formed, first advanced by Abraham Tesser in 1983, is that strong likes and dislikes are rooted in our genetic make-up. Tesser speculates that individuals are disposed to hold certain strong attitudes as a result of inborn physical, sensory, and cognitive skills, temperament, and personality traits. Whatever disposition nature elects to give us, our most treasured attitudes are often formed as a result of exposure to attitude objects; our history of rewards and punishments; the attitude that our parents, friends, and enemies express; the social and cultural context in which we live; and other types of experiences we have. Obviously, attitudes are formed through the basic process of learning. Numerous studies have shown that people can form strong positive and negative attitudes toward neutral objects that are in some way linked to emotionally charged stimuli.
Attitudes are also involved in several other areas of the discipline, such as conformity, interpersonal attraction, social perception, and prejudice.
Persuasion.
The topic of persuasion has received a great deal of attention in recent years. Persuasion is an active method of influence that attempts to guide people toward the adoption of an attitude, idea, or behavior by rational or emotive means. Persuasion relies on "appeals" rather than strong pressure or coercion. Numerous variables have been found to influence the persuasion process; these are normally presented in five major categories: "who" said "what" to "whom" and "how".
Dual-process theories of persuasion (such as the elaboration likelihood model) maintain that the persuasive process is mediated by two separate routes; central and peripheral. The central route of persuasion is more fact-based and results in longer lasting change, but requires motivation to process. The peripheral route is more superficial and results in shorter lasting change, but does not require as much motivation to process. An example of a peripheral route of persuasion might be a politician using a flag lapel pin, smiling, and wearing a crisp, clean shirt. Notice that this does not require motivation to be persuasive, but should not last as long as persuasion based on the central route. If that politician were to outline exactly what they believed, and their previous voting record, this would be using the central route, and would result in longer lasting change, but would require a good deal of motivation to process.
Social cognition.
Social cognition is a growing area of social psychology that studies how people perceive, think about, and remember information about others. Much research rests on the assertion that people think about (other) people differently from non-social targets. This assertion is supported by the social cognitive deficits exhibited by people with Williams syndrome and autism. Person perception is the study of how people form impressions of others. The study of how people form beliefs about each other while interacting is known as interpersonal perception.
A major research topic in social cognition is attribution. Attributions are the explanations we make for people's behavior, either our own behavior or the behavior of others. We can ascribe the locus of a behavior to either internal or external factors. An "internal", or dispositional, attribution assigns behavior to causes related to inner traits such as personality, disposition, character or ability. An "external", or situational, attribution involves situational elements, such as the weather. A second element, attribution, ascribes the cause of behavior to either stable or unstable factors (whether the behavior will be repeated or changed under similar circumstances). Finally, we also attribute causes of behavior to either controllable or uncontrollable factors : how much control one has over the situation at hand.
Numerous biases in the attribution process have been discovered. For instance, the fundamental attribution error is the tendency to make dispositional attributions for behavior, overestimating the influence of personality and underestimating the influence of situations. The actor-observer difference is a refinement of this bias, the tendency to make dispositional attributions for other people's behavior and situational attributions for our own. The self-serving bias is the tendency to attribute dispositional causes for successes, and situational causes for failure, particularly when self-esteem is threatened. This leads to assuming one's successes are from innate traits, and one's failures are due to situations, including other people. Other ways people protect their self-esteem are by believing in a just world, blaming victims for their suffering, and making defensive attributions, which explain our behavior in ways which defend us from feelings of vulnerability and mortality. Researchers have found that mildly depressed individuals often lack this bias and actually have more realistic perceptions of reality (as measured by the opinions of others).
Heuristics are cognitive short cuts. Instead of weighing all the evidence when making a decision, people rely on heuristics to save time and energy. The availability heuristic occurs when people estimate the probability of an outcome based on how easy that outcome is to imagine. As such, vivid or highly memorable possibilities will be perceived as more likely than those that are harder to picture or are difficult to understand, resulting in a corresponding cognitive bias. The representativeness heuristic is a shortcut people use to categorize something based on how similar it is to a prototype they know of. Numerous other biases have been found by social cognition researchers. The hindsight bias is a false memory of having predicted events, or an exaggeration of actual predictions, after becoming aware of the outcome. The confirmation bias is a type of bias leading to the tendency to search for, or interpret information in a way that confirms one's preconceptions.
Another key concept in social cognition is the assumption that reality is too complex to easily discern. As a result, we tend to see the world according to simplified schemas or images of reality. Schemas are generalized mental representations that organize knowledge and guide information processing. Schemas often operate automatically and unintentionally, and can lead to biases in perception and memory. Expectations from schemas may lead us to see something that is not there. One experiment found that people are more likely to misperceive a weapon in the hands of a black man than a white man. This type of schema is actually a stereotype, a generalized set of beliefs about a particular group of people (when incorrect, an ultimate attribution error). Stereotypes are often related to negative or preferential attitudes (prejudice) and behavior (discrimination). Schemas for behaviors (e.g., going to a restaurant, doing laundry) are known as "scripts".
Self-concept.
Self-concept is a term referring to the whole sum of beliefs that people have about themselves. However, what specifically does self-concept consist of? According to Hazel Markus (1977), the self-concept is made up of cognitive molecules called self-schemas – beliefs that people have about themselves that guide the processing of self-reliant information. For example, an athlete at a university would have multiple selves that would process different information pertinent to each self: the student would be one "self," who would process information pertinent to a student (taking notes in class, completing a homework assignment, etc.); the athlete would be the "self" who processes information about things related to being an athlete (recognizing an incoming pass, aiming a shot, etc.). These "selves" are part of one's identity and the self-reliant information is the information that relies on the proper "self" to process and react on it. If a "self" is not part of one's identity, then it is much more difficult for one to react. For example, a civilian may not know how to handle a hostile threat as a trained Marine would. The Marine contains a "self" that would enable him/her to process the information about the hostile threat and react accordingly, whereas a civilian may not contain that self, disabling them from properly processing the information from the hostile threat and, furthermore, debilitating them from acting accordingly. Self-schemas are to an individual’s total self–concept as a hypothesis is to a theory, or a book is to a library. A good example is the body weight self-schema; people who regard themselves as over or underweight, or for those whom body image is a significant self-concept aspect, are considered "schematics" with respect to weight. For these people a range of otherwise mundane events – grocery shopping, new clothes, eating out, or going to the beach – can trigger thoughts about the self. In contrast, people who do not regard their weight as an important part of their lives are "a-schematic" on that attribute.
It is rather clear that the self is a special object of our attention. Whether one is mentally focused on a memory, a conversation, a foul smell, the song that is stuck in one's head, or this sentence, consciousness is like a spotlight. This spotlight can shine on only one object at a time, but it can switch rapidly from one object to another and process the information out of awareness. In this spotlight the self is front and center: things relating to the self have the spotlight more often.
The self's ABCs are affect, behavior, and cognition. An affective (or emotional) question: How do people evaluate themselves, enhance their self-image, and maintain a secure sense of identity? A behavioral question: How do people regulate their own actions and present themselves to others according to interpersonal demands? A cognitive question: How do individuals become themselves, build a self-concept, and uphold a stable sense of identity?
Affective forecasting is the process of predicting how one would feel in response to future emotional events. Studies done by Timothy Wilson and Daniel Gilbert in 2003 have shown that people overestimate the strength of reaction to anticipated positive and negative life events that they actually feel when the event does occur.
There are many theories on the perception of our own behavior. Daryl Bem's (1972) self-perception theory claims that when internal cues are difficult to interpret, people gain self-insight by observing their own behavior. Leon Festinger's 1954 social comparison theory is that people evaluate their own abilities and opinions by comparing themselves to others when they are uncertain of their own ability or opinions. There is also the facial feedback hypothesis: that changes in facial expression can lead to corresponding changes in emotion.
The fields of social psychology and personality have merged over the years, and social psychologists have developed an interest in self-related phenomena. In contrast with traditional personality theory, however, social psychologists place a greater emphasis on cognitions than on traits. Much research focuses on the self-concept, which is a person's understanding of his or her self. The self-concept is often divided into a cognitive component, known as the "self-schema", and an evaluative component, the "self-esteem". The need to maintain a healthy self-esteem is recognized as a central human motivation in the field of social psychology.
Self-efficacy beliefs are associated with the self-schema. These are expectations that performance on some task will be effective and successful. Social psychologists also study such self-related processes as self-control and self-presentation.
People develop their self-concepts by varied means, including introspection, feedback from others, self-perception, and social comparison. By comparison to relevant others, people gain information about themselves, and they make inferences that are relevant to self-esteem. Social comparisons can be either "upward" or "downward," that is, comparisons to people who are either higher in status or ability, or lower in status or ability. Downward comparisons are often made in order to elevate self-esteem.
Self-perception is a specialized form of attribution that involves making inferences about oneself after observing one's own behavior. Psychologists have found that too many extrinsic rewards (e.g. money) tend to reduce intrinsic motivation through the self-perception process, a phenomenon known as overjustification. People's attention is directed to the reward and they lose interest in the task when the reward is no longer offered. This is an important exception to reinforcement theory.
Interpersonal phenomena.
Social influence.
Social influence is an overarching term given to describe the persuasive effects people have on each other. It is seen as a fundamental value in social psychology and overlaps considerably with research on attitudes and persuasion. The three main areas of social influence include: conformity, compliance, and obedience. Social influence is also closely related to the study of group dynamics, as most principles of influence are strongest when they take place in social groups.
The first major area of social influence is conformity. Conformity is defined as the tendency to act or think like other members of a group. The identity of members within a group, i.e. status, similarity, expertise, as well as cohesion, prior commitment, and accountability to the group help to determine the level of conformity of an individual. Individual variation among group members plays a key role in the dynamic of how willing people will be to conform. Conformity is usually viewed as a negative tendency in American culture, but a certain amount of conformity is adaptive in some situations, as is nonconformity in other situations.
The second major area of social influence research is compliance. Compliance refers to any change in behavior that is due to a request or suggestion from another person. The Foot-in-the-door technique is a compliance method in which the persuader requests a small favor and then follows up with requesting a larger favor, e.g., asking for the time and then asking for ten dollars. A related trick is the Bait and switch.
The third major form of social influence is obedience; this is a change in behavior that is the result of a direct order or command from another person. Obedience as a form of compliance was dramatically highlighted by the Milgram study, wherein people were ready to administer shocks to a person in distress on a researcher's command.
An unusual kind of social influence is the self-fulfilling prophecy. This is a prediction that, in being made, actually causes itself to become true. For example, in the stock market, if it is widely believed that a crash is imminent, investors may lose confidence, sell most of their stock, and thus actually cause the crash. Similarly, people may expect hostility in others and actually induce this hostility by their own behavior.
Group dynamics.
A group can be defined as two or more individuals that are connected to each another by social relationships. Groups tend to interact, influence each other, and share a common identity. They have a number of emergent qualities that distinguish them from aggregates:
Temporary groups and aggregates share few or none of these features, and do not qualify as true social groups. People waiting in line to get on a bus, for example, do not constitute a group.
Groups are important not only because they offer social support, resources, and a feeling of belonging, but because they supplement an individual's self-concept. To a large extent, humans define themselves by the group memberships which form their social identity. The shared social identity of individuals within a group influences intergroup behavior, the way in which groups behave towards and perceive each other. These perceptions and behaviors in turn define the social identity of individuals within the interacting groups. The tendency to define oneself by membership in a group may lead to intergroup discrimination, which involves favorable perceptions and behaviors directed towards the in-group, but negative perceptions and behaviors directed towards the out-group. On the other hand, such discrimination and segregation may sometimes exist partly to facilitate a diversity which strengthens society. Intergroup discrimination leads to prejudice and stereotyping, while the processes of social facilitation and group polarization encourage extreme behaviors towards the out-group.
Groups often moderate and improve decision making, and are frequently relied upon for these benefits, such as in committees and juries. A number of group biases, however, can interfere with effective decision making. For example, group polarization, formerly known as the "risky shift," occurs when people polarize their views in a more extreme direction after group discussion. More problematic is the phenomenon of groupthink. This is a collective thinking defect that is characterized by a premature consensus or an incorrect assumption of consensus, caused by members of a group failing to promote views which are not consistent with the views of other members. Groupthink occurs in a variety of situations, including isolation of a group and the presence of a highly directive leader. Janis offered the 1961 Bay of Pigs Invasion as a historical case of groupthink.
Groups also affect performance and productivity. Social facilitation, for example, is a tendency to work harder and faster in the presence of others. Social facilitation increases the "dominant response"s likelihood, which tends to improve performance on simple tasks and reduce it on complex tasks. In contrast, social loafing is the tendency of individuals to slack off when working in a group. Social loafing is common when the task is considered unimportant and individual contributions are not easy to see.
Social psychologists study group-related (collective) phenomena such as the behavior of crowds. An important concept in this area is deindividuation, a reduced state of self-awareness that can be caused by feelings of anonymity. Deindividuation is associated with uninhibited and sometimes dangerous behavior. It is common in crowds and mobs, but it can also be caused by a disguise, a uniform, alcohol, dark environments, or online anonymity.
Interpersonal attraction.
A major area in the study of people's relations to each other is interpersonal attraction. This refers to all forces that lead people to like each other, establish relationships, and (in some cases) fall in love. Several general principles of attraction have been discovered by social psychologists, but many still continue to experiment and do research to find out more. One of the most important factors in interpersonal attraction is how similar two particular people are. The more similar two people are in general attitudes, backgrounds, environments, worldviews, and other traits, the more probable an attraction is possible.
Physical attractiveness is an important element of romantic relationships, particularly in the early stages characterized by high levels of passion. Later on, similarity and other compatibility factors become more important, and the type of love people experience shifts from "passionate" to "companionate". Robert Sternberg has suggested that there are actually three components of love: intimacy, passion, and commitment. When two (or more) people experience all three, they are said to be in a state of consummate love.
According to social exchange theory, relationships are based on rational choice and cost-benefit analysis. If one partner's costs begin to outweigh his or her benefits, that person may leave the relationship, especially if there are good alternatives available. This theory is similar to the minimax principle proposed by mathematicians and economists (despite the fact that human relationships are not zero-sum games). With time, long term relationships tend to become communal rather than simply based on exchange.
Research.
Methods.
Social psychology is an empirical science that attempts to answer questions about human behavior by testing hypotheses, both in the laboratory and in the field. Careful attention to sampling, research design, and statistical analysis is important; results are published in peer reviewed journals such as the "Journal of Experimental Social Psychology", "Personality and Social Psychology Bulletin" and the "Journal of Personality and Social Psychology". Social psychology studies also appear in general science journals such as "Psychological Science" and "Science".
Experimental methods involve the researcher altering a variable in the environment and measuring the effect on another variable. An example would be allowing two groups of children to play violent or nonviolent videogames, and then observing their subsequent level of aggression during free-play period. A valid experiment is controlled and uses random assignment.
Correlational methods examine the statistical association between two naturally occurring variables. For example, one could correlate the amount of violent television children watch at home with the number of violent incidents the children participate in at school. Note that this study would "not" prove that violent TV causes aggression in children: it is quite possible that aggressive children choose to watch more violent TV.
Observational methods are purely descriptive and include naturalistic observation, "contrived" observation, participant observation, and archival analysis. These are less common in social psychology but are sometimes used when first investigating a phenomenon. An example would be to unobtrusively observe children on a playground (with a videocamera, perhaps) and record the number and types of aggressive actions displayed.
Whenever possible, social psychologists rely on controlled experimentation. Controlled experiments require the manipulation of one or more independent variables in order to examine the effect on a dependent variable. Experiments are useful in social psychology because they are high in internal validity, meaning that they are free from the influence of confounding or extraneous variables, and so are more likely to accurately indicate a causal relationship. However, the small samples used in controlled experiments are typically low in external validity, or the degree to which the results can be generalized to the larger population. There is usually a trade-off between experimental control (internal validity) and being able to generalize to the population (external validity).
Because it is usually impossible to test everyone, research tends to be conducted on a sample of persons from the wider population. Social psychologists frequently use survey research when they are interested in results that are high in external validity. Surveys use various forms of random sampling to obtain a sample of respondents that are representative of a population. This type of research is usually descriptive or correlational because there is no experimental control over variables. However, new statistical methods like structural equation modeling are being used to test for potential causal relationships in this type of data.
Regardless of which method is used, it is important to evaluate the research hypothesis using the results, either confirming or rejecting the original prediction. Social psychologists use statistics and probability testing to judge their results; these define a significant finding as less than 5% likely to be due to chance. Replications are important, to ensure that the result is valid and not due to chance, or some feature of a particular sample. False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are a hazard in the field.
Ethics.
The goal of social psychology is to understand cognition and behavior as they naturally occur in a social context, but the very act of observing people can influence and alter their behavior. For this reason, many social psychology experiments utilize deception to conceal or distort certain aspects of the study. Deception may include false cover stories, false participants (known as confederates or stooges), false feedback given to the participants, and so on.
The practice of deception has been challenged by some psychologists who maintain that deception under any circumstances is unethical, and that other research strategies (e.g., role-playing) should be used instead. Unfortunately, research has shown that role-playing studies do not produce the same results as deception studies and this has cast doubt on their validity. In addition to deception, experimenters have at times put people into potentially uncomfortable or embarrassing situations (e.g., the Milgram experiment and Stanford prison experiment), and this has also been criticized for ethical reasons.
To protect the rights and well-being of research participants, and at the same time discover meaningful results and insights into human behavior, virtually all social psychology research must pass an ethical review process. At most colleges and universities, this is conducted by an ethics committee or Institutional Review Board. This group examines the proposed research to make sure that no harm is likely to be done to the participants, and that the study's benefits outweigh any possible risks or discomforts to people taking part in the study.
Furthermore, a process of informed consent is often used to make sure that volunteers know what will happen in the experiment and understand that they are allowed to quit the experiment at any time. A debriefing is typically done at the experiment's conclusion in order to reveal any deceptions used and generally make sure that the participants are unharmed by the procedures. Today, most research in social psychology involves no more risk of harm than can be expected from routine psychological testing or normal daily activities.
Replication crisis.
Social psychology has recently found itself at the center of a "replication crisis" due to some research findings proving difficult to replicate. Replication failures are not unique to social psychology and are found in all fields of science. However, several factors have combined to put social psychology at the center of the current controversy.
Firstly, questionable research practices (QRP) have been identified as common in the field. Such practices, while not intentionally fraudulent, involve converting undesired statistical outcomes into desired outcomes via the manipulation of statistical analyses, sample size or data management, typically to convert non-significant findings into significant ones. Some studies have suggested that at least mild versions of QRP are highly prevalent. One of the critics of Daryl Bem in the feeling the future controversy has suggested that the evidence for precognition in this study could (at least in part) be attributed to QRP.
Secondly, social psychology has found itself at the center of several recent scandals involving outright fraudulent research. Most notably the admitted data fabrication by Diederik Stapel as well as allegations against others. However, most scholars acknowledge that fraud is, perhaps, the lesser contribution to replication crises.
Third, several effects in social psychology have been found to be difficult to replicate even before the current replication crisis. For example, the scientific journal Judgment and Decision Making has published several studies over the years that fail to provide support for the unconscious thought theory. Replications appear particularly difficult when research trials are pre-registered and conducted by research groups not highly invested in the theory under questioning.
These three elements together have resulted in renewed attention for replication supported by Kahneman. Scrutiny of many effects have shown that several core beliefs are hard to replicate. A recent special edition of the journal Social Psychology focused on replication studies and a number of previously held beliefs were found to be difficult to replicate. A 2012 special edition of the journal Perspectives on Psychological Science also focused on issues ranging from publication bias to null-aversion that contribute to the replication crises in psychology
It is important to note that this replication crisis does not mean that social psychology is unscientific. Rather this process is a healthy if sometimes acrimonious part of the scientific process in which old ideas or those that cannot withstand careful scrutiny are pruned. The consequence is that some areas of social psychology once considered solid, such as social priming, have come under increased scrutiny due to failed replications
Famous experiments.
The Asch conformity experiments demonstrated the power of conformity in small groups with a line estimation task that was designed to be extremely easy. On over a third of the trials, participants conformed to the majority, even though the majority judgment was clearly wrong. Seventy-five percent of the participants conformed at least once during the experiment.
Muzafer Sherif's Robbers' Cave Experiment divided boys into two competing groups to explore how much hostility and aggression would emerge. Sherif's explanation of the results became known as realistic group conflict theory, because the intergroup conflict was induced through competition over resources. Inducing cooperation and superordinate goals later reversed this effect.
In Leon Festinger's cognitive dissonance experiment, participants were asked to perform a boring task. They were divided into 2 groups and given two different pay scales. At the study's end, some participants were paid $1 to say that they enjoyed the task and another group of participants was paid $20 to say the same lie. The first group ($1) later reported liking the task better than the second group ($20). Festinger's explanation was that for people in the first group being paid only $1 is not sufficient incentive for lying and those who were paid $1 experienced dissonance. They could only overcome that dissonance by justifying their lies by changing their previously unfavorable attitudes about the task. Being paid $20 provides a reason for the doing the boring task, therefore no dissonance.
One of the most notable experiments in social psychology was the Milgram experiment, which studied how far people would go to obey an authority figure. Following the events of The Holocaust in World War II, the experiment showed that (most) normal American citizens were capable of following orders from an authority even when they believed they were causing an innocent person to suffer.
Albert Bandura's Bobo doll experiment demonstrated how aggression is learned by imitation. This set of studies fueled debates regarding media violence which continue to be waged among scholars.
In the Stanford prison study, by Philip Zimbardo, a simulated exercise between student prisoners and guards showed how far people would follow an adopted role. In just a few days, the "guards" became brutal and cruel, and the prisoners became miserable and compliant. This was initially argued to be an important demonstration of the power of the immediate social situation and its capacity to overwhelm normal personality traits. However, to this day, it remains a matter of contention what conclusions may be drawn from this study. For example, it has been pointed out that participant self-selection may have affected the participants' behaviour, and that the participants' personality influenced their reactions in a variety of ways, including how long they chose to remain in the study. One of the most concerted empirical revisitations of the themes raised by Zimbardo came with the 2002 BBC prison study.

</doc>
<doc id="26992" url="https://en.wikipedia.org/wiki?curid=26992" title="Suleiman the Magnificent">
Suleiman the Magnificent

Suleiman I (; or simply Solomon as a Biblical name; 6 November 1494 – 7 September 1566), commonly known as Suleiman the Magnificent in the West and "Kanuni" (the Lawgiver) in the East, was the tenth and longest-reigning Great Sultan of the Ottoman Empire, from 1520 to his death in 1566. Under his administration, the Ottoman State ruled over 20 to 30 million people.
Suleiman became a prominent monarch of 16th-century Europe, presiding over the apex of the Ottoman Empire's economic, military and political power. Suleiman personally led Ottoman armies in conquering the Christian strongholds of Belgrade and Rhodes as well as most of Hungary before his conquests were checked at the Siege of Vienna in 1529. He annexed much of the Middle East in his conflict with the Persian Safavids and large areas of North Africa as far west as Algeria. Under his rule, the Ottoman fleet dominated the seas from the Mediterranean to the Red Sea and through the Persian Gulf.
At the helm of an expanding empire, Suleiman personally instituted major legislative changes relating to society, education, taxation and criminal law. His canonical law (or the "Kanuns") fixed the form of the empire for centuries after his death. He was a distinguished poet and goldsmith; he also became a great patron of culture, overseeing the "Golden" age of the Ottoman Empire in its artistic, literary and architectural development.
Breaking with Ottoman tradition, Suleiman married Roxelana, a former Christian girl converted to Islam from his harem, who became subsequently known and influential as Hürrem Sultan. Their son Selim II succeeded Suleiman following his death in 1566 after 46 years of rule, thus beginning a long state of stagnation and decline during Selim II's reign. Suleiman's previous heirs apparent Mehmed and Mustafa had died, the former from smallpox and the latter had been strangled to death 13 years previously at the sultan's order. His other son Bayezid had been killed by his support and Selim's order in 1561 with four of his sons.
Alternative names and titles.
Suleiman the Magnificent ( "Muḥteşem Süleymān"), as he was known in the West, was also called Suleiman the First ( "Sulṭān Süleymān-ı Evvel"), and Suleiman the Lawgiver ( "Ḳānūnī Sulṭān Süleymān") for his complete reconstruction of the Ottoman legal system.
Early life.
Suleiman was born in Trabzon along the east coast of the Black Sea, probably on 6 November 1494. His mother was Ayşe Hafsa Sultan (she was possibly the daughter of Meñli I Giray, a descendant of Genghis Khan, through Jochi); little is known of her other than that she died in 1534.
Education.
At the age of seven, Suleiman was sent to study science, history, literature, theology and military tactics in the schools of the imperial Topkapı Palace in Constantinople (modern Istanbul). As a young man, he befriended Pargalı Ibrahim, a slave who later became one of his most trusted advisers.
Viceroy in Anatolia.
From the age of seventeen, he was appointed as the governor of first Kaffa (Theodosia), then Sarukhan (Manisa) with a brief tenure at Adrianople (now Edirne).
Accession.
Upon the death of his father, Selim I (1465–1520), Suleiman entered Constantinople and ascended to the throne as the tenth Ottoman Sultan. An early description of Suleiman, a few weeks following his accession, was provided by the Venetian envoy Bartolomeo Contarini: "He is twenty-six years of age, tall, but wiry, and of a delicate complexion. His neck is a little too long, his face thin, and his nose aquiline. He has a shade of a mustache and a small beard; nevertheless he has a pleasant mien, though his skin tends to be a light pallor. He is said to be a wise Lord, fond of study, and all men hope for good from his rule." Some historians claim that in his youth Suleiman had an admiration for Alexander the Great. He was influenced by Alexander's vision of building a world empire that would encompass the east and the west, and this created a drive for his subsequent military campaigns in Asia and in Africa, as well as in Europe.
Military campaigns.
Conquests in Europe.
Upon succeeding his father, Suleiman began a series of military conquests, eventually suppressing a revolt led by the Ottoman-appointed governor of Damascus in 1521. Suleiman soon made preparations for the conquest of Belgrade from the Kingdom of Hungary—something his great-grandfather Mehmed II had failed to achieve because of John Hunyadi's strong defense in the region. Its capture was vital in removing the Hungarians and Croats who, following the defeats of the Serbs, Bulgarians, Albanians and the Byzantines, remained the only formidable force who could block further Ottoman gains in Europe. Suleiman encircled Belgrade and began a series of heavy bombardments from an island in the Danube. Belgrade, with a garrison of only 700 men, and receiving no aid from Hungary, fell in August 1521.
The fall of Christendom's major strongholds spread fear across Europe. As the ambassador of the Holy Roman Empire to Constantinople was to note, "The capture of Belgrade was at the origin of the dramatic events which engulfed Hungary. It led to the death of King Louis, the capture of Buda, the occupation of Transylvania, the ruin of a flourishing kingdom and the fear of neighboring nations that they would suffer the same fate ..."
The road to Hungary and Austria lay open, but Suleiman turned his attention instead to the Eastern Mediterranean island of Rhodes, the home base of the Knights Hospitaller. In the summer of 1522, taking advantage of the large navy he inherited from his father, Suleiman dispatched an armada of some 400 ships towards Rhodes, while personally leading an army of 100,000 across Asia Minor to a point opposite the island itself. Here Suleiman built a large fortification, Marmaris Castle, that served as a base for the Ottoman Navy. Following the brutal five-month Siege of Rhodes (1522), Rhodes capitulated and Suleiman allowed the Knights of Rhodes to depart. (The Knights of Rhodes eventually formed a new base in Malta, becoming known as Knights of Malta, even now.)
As relations between Hungary and the Ottoman Empire deteriorated, Suleiman resumed his campaign in Central Europe and on 29 August 1526, he defeated Louis II of Hungary (1506–26) at the Battle of Mohács. In its wake, Hungarian resistance collapsed and the Ottoman Empire became the preeminent power in Central Europe. Upon encountering the lifeless body of King Louis, Suleiman is said to have lamented: "I came indeed in arms against him; but it was not my wish that he should be thus cut off before he scarcely tasted the sweets of life and royalty." While Suleiman was campaigning in Hungary, Turkmen tribes in central Anatolia revolted under the leadership of Kalender Çelebi.
Some Hungarian nobles proposed that Ferdinand, who was ruler of neighboring Austria and tied to Louis II's family by marriage, be King of Hungary, citing previous agreements that the Habsburgs would take the Hungarian throne if Louis died without heirs. 
However, other nobles turned to the nobleman John Zápolya, who was being supported by Suleiman. Under Charles V and his brother Ferdinand I, the Habsburgs reoccupied Buda and took possession of Hungary. Reacting in 1529, Suleiman marched through the valley of the Danube and regained control of Buda; in the following autumn his forces laid siege to Vienna. This was to be the Ottoman Empire's most ambitious expedition and the apogee of its drive to the West. With a reinforced garrison of 16,000  men, the Austrians inflicted the first defeat on Suleiman, sowing the seeds of a bitter Ottoman-Habsburg rivalry, which lasted until the 20th century. His second attempt to conquer Vienna failed in 1532, with Ottoman forces delayed by the siege of Güns, failing to reach Vienna. In both cases, the Ottoman army was plagued by bad weather (forcing them to leave behind essential siege equipment) and was hobbled by overstretched supply lines.
By the 1540s a renewal of the conflict in Hungary presented Suleiman with the opportunity to avenge the defeat suffered at Vienna.
In 1541 the Habsburgs once again engaged in conflict with the Ottomans, by attempting to lay siege to Buda. With their efforts repulsed and more Habsburg fortresses captured by the Ottomans in two consecutive campaigns in 1541 and in 1544 as a result, Ferdinand and his brother Charles V were forced to conclude a humiliating five-year treaty with Suleiman. Ferdinand renounced his claim to the Kingdom of Hungary and was forced to pay a fixed yearly sum to the Sultan for the Hungarian lands he continued to control. Of more symbolic importance, the treaty referred to Charles V not as 'Emperor', but as the 'King of Spain', leading Suleiman to identify as the true 'Caesar'.
With his main European rivals subdued, Suleiman ensured that the Ottoman Empire had a powerful role in the political landscape of Europe for some years to come.
Ottoman–Safavid War.
As Suleiman stabilized his European frontiers, he now turned his attention to the ever present threat posed by the Shi'a Safavid dynasty of Persia. Two events in particular were to precipitate a recurrence of tensions. First, Shah Tahmasp had the Baghdad governor loyal to Suleiman killed and replaced with an adherent of the Shah, and second, the governor of Bitlis had defected and sworn allegiance to the Safavids. As a result, in 1533, Suleiman ordered his Grand Vizier Pargalı Ibrahim Pasha to lead an army into eastern Asia Minor where he retook Bitlis and occupied Tabriz without resistance. Having joined Ibrahim in 1534, Suleiman made a push towards Persia, only to find the Shah sacrificing territory instead of facing a pitched battle, resorting to harassment of the Ottoman army as it proceeded along the harsh interior. When in the following year Suleiman and Ibrahim made a grand entrance into Baghdad, its commander surrendered the city, thereby confirming Suleiman as the leader of the Sunni Islamic world and the legitimate successor to the Sunni Abbasid Caliphs. Moreover, the fact Suleiman restored the grave of Sunni imam Abu Hanifa also strengthened his credentials and claim to the caliphate.
Attempting to defeat the Shah once and for all, Suleiman embarked upon a second campaign in 1548–1549. As in the previous attempt, Tahmasp avoided confrontation with the Ottoman army and instead chose to retreat, using scorched earth tactics in the process and exposing the Ottoman army to the harsh winter of the Caucasus. Suleiman abandoned the campaign with temporary Ottoman gains in Tabriz and the Urmia region, a lasting presence in the province of Van, control of the western half of Azerbaijan and some forts in Georgia.
In 1553 Suleiman began his third and final campaign against the Shah. Having initially lost territories in Erzurum to the Shah's son, Suleiman retaliated by recapturing Erzurum, crossing the Upper Euphrates and laying waste to parts of Persia. The Shah's army continued its strategy of avoiding the Ottomans, leading to a stalemate from which neither army made any significant gain. In 1554, a settlement was signed which was to conclude Suleiman's Asian campaigns. Part of the treaty included and confirmed the return of Tabriz, but secured Baghdad, lower Mesopotamia, the mouths of the river Euphrates and Tigris, as well as part of the Persian Gulf. The Shah also promised to cease all raids into Ottoman territory.
Campaigns in the Indian Ocean.
Ottoman ships had been sailing in the Indian Ocean since the year 1518. Ottoman Admirals such as Hadim Suleiman Pasha, Seydi Ali Reis and Kurtoğlu Hızır Reis are known to have voyaged to the Mughal imperial ports of Thatta, Surat and Janjira. The Mughal Emperor Akbar himself is known to have exchanged six documents with Suleiman the Magnificent.
In the Indian Ocean, Suleiman led several naval campaigns against the Portuguese in an attempt to remove them and reestablish trade with India. Aden in Yemen was captured by the Ottomans in 1538, in order to provide an Ottoman base for raids against Portuguese possessions on the western coast of modern India and Pakistan. Sailing on to India, the Ottomans failed against the Portuguese at the Siege of Diu in September 1538, but then returned to Aden, where they fortified the city with 100 pieces of artillery. From this base, Sulayman Pasha managed to take control of the whole country of Yemen, also taking Sana'a. Aden rose against the Ottomans however and invited the Portuguese instead, so that the Portuguese were in control of the city until its seizure by Piri Reis in the Capture of Aden (1548).
With its strong control of the Red Sea, Suleiman successfully managed to dispute control of the Indian trade routes to the Portuguese and maintained a significant level of trade with the Mughal Empire of South Asia throughout the 16th century. His admiral Piri Reis led an Ottoman fleet in the Indian Ocean, achieving the Capture of Muscat in 1552.
From 1526 till 1543, Suileman stationed over 900 Turkish soldiers to fight alongside the Somali Adal Sultanate led by Ahmad ibn Ibrahim al-Ghazi during the Conquest of Abyssinia. After the first Ajuran-Portuguese war, the Ottoman Empire would in 1559 absorb the weakened Adal Sultanate into its domain. This expansion fathered Ottoman rule in Somalia and the Horn of Africa. This also increased its influence in the Indian Ocean to compete with the Portuguese Empire with its close ally the Ajuran Empire.
In 1564, Suleiman received an embassy from Aceh (a sultanate on Sumatra, in modern Indonesia), requesting Ottoman support against the Portuguese. As a result, an Ottoman expedition to Aceh was launched, which was able to provide extensive military support to the Acehnese.
The discovery of new maritime trade routes by Western European states allowed them to avoid the Ottoman trade monopoly. The Portuguese discovery of the Cape of Good Hope in 1488 initiated a series of Ottoman-Portuguese naval wars in the Indian Ocean throughout the 16th century. The Ajuran Sultanate allied with the Ottomans defied the Portuguese economic monopoly in the Indian Ocean by employing a new coinage which followed the Ottoman pattern, thus proclaiming an attitude of economic independence in regard to the Portuguese.
Mediterranean and North Africa.
Having consolidated his conquests on land, Suleiman was greeted with the news that the fortress of Koroni in Morea (the modern Peloponnese, peninsular Greece) had been lost to Charles V's admiral, Andrea Doria. The presence of the Spanish in the Eastern Mediterranean concerned Suleiman, who saw it as an early indication of Charles V's intention to rival Ottoman dominance in the region. Recognizing the need to reassert naval preeminence in the Mediterranean, Suleiman appointed an exceptional naval commander in the form of Khair ad Din, known to Europeans as Barbarossa. Once appointed admiral-in-chief, Barbarossa was charged with rebuilding the Ottoman fleet, to such an extent that the Ottoman navy equaled in number those of all other Mediterranean countries put together.
In 1535, Charles V won an important victory against the Ottomans at Tunis, which together with the war against Venice the following year, led Suleiman to accept proposals from Francis I of France to form an alliance against Charles. In 1538, the Spanish fleet was defeated by Barbarossa at the Battle of Preveza, securing the eastern Mediterranean for the Turks for 33 years, until the defeat at the Battle of Lepanto in 1571.
East of Morocco, huge Muslim territories in North Africa were annexed. The Barbary States of Tripolitania, Tunisia and Algeria became autonomous provinces of the Empire, serving as the leading edge of Suleiman's conflict with Charles V, whose attempt to drive out the Turks failed in 1541. The piracy carried on thereafter by the Barbary pirates of North Africa can be seen in the context of the wars against Spain. For a short period Ottoman expansion secured naval dominance in the Mediterranean. 
In 1542, facing a common Habsburg enemy, Francis I sought to renew the Franco-Ottoman alliance. As a result, Suleiman dispatched 100 galleys under Barbarossa to assist the French in the western Mediterranean. Barbarossa pillaged the coast of Naples and Sicily before reaching France, where Francis made Toulon the Ottoman admiral's naval headquarters. The same campaign saw Barbarossa attack and capture Nice in 1543. By 1544, a peace between Francis I and Charles V had put a temporary end to the alliance between France and the Ottoman Empire.
Elsewhere in the Mediterranean, when the Knights Hospitallers were re-established as the Knights of Malta in 1530, their actions against Muslim navies quickly drew the ire of the Ottomans, who assembled another massive army in order to dislodge the Knights from Malta. The Ottomans invaded Malta in 1565, undertaking the Great Siege of Malta, which began on 18 May and lasted until 8 September, and is portrayed vividly in the frescoes of Matteo Perez d'Aleccio in the Hall of St. Michael and St. George. At first it seemed that this would be a repeat of the battle on Rhodes, with most of Malta's cities destroyed and half the Knights killed in battle; but a relief force from Spain entered the battle, resulting in the loss of 10,000 Ottoman troops and the victory of the local Maltese citizenry.
Administrative reforms.
While Sultan Suleiman was known as "the Magnificent" in the West, he was always "Kanuni" Suleiman or "The Lawgiver" () to his own Ottoman subjects. As the historian Lord Kinross notes, "Not only was he a great military campaigner, a man of the sword, as his father and great-grandfather had been before him. He differed from them in the extent to which he was also a man of the pen. He was a great legislator, standing out in the eyes of his people as a high-minded sovereign and a magnanimous exponent of justice". The overriding law of the empire was the Shari'ah, or Sacred Law, which as the divine law of Islam was outside of the Sultan's powers to change. Yet an area of distinct law known as the "Kanuns" (, canonical legislation) was dependent on Suleiman's will alone, covering areas such as criminal law, land tenure and taxation. He collected all the judgments that had been issued by the nine Ottoman Sultans who preceded him. After eliminating duplications and choosing between contradictory statements, he issued a single legal code, all the while being careful not to violate the basic laws of Islam. It was within this framework that Suleiman, supported by his Grand Mufti Ebussuud, sought to reform the legislation to adapt to a rapidly changing empire. When the Kanun laws attained their final form, the code of laws became known as the "kanun‐i Osmani" (), or the "Ottoman laws". Suleiman's legal code was to last more than three hundred years.
Suleiman gave particular attention to the plight of the rayas, Christian subjects who worked the land of the Sipahis. His Kanune Raya, or "Code of the Rayas", reformed the law governing levies and taxes to be paid by the rayas, raising their status above serfdom to the extent that Christian serfs would migrate to Turkish territories to benefit from the reforms. The Sultan also played a role in protecting the Jewish subjects of his empire for centuries to come. In late 1553 or 1554, on the suggestion of his favorite doctor and dentist, the Spanish Jew Moses Hamon, the Sultan issued a "firman" () formally denouncing blood libels against the Jews. Furthermore, Suleiman enacted new criminal and police legislation, prescribing a set of fines for specific offenses, as well as reducing the instances requiring death or mutilation. In the area of taxation, taxes were levied on various goods and produce, including animals, mines, profits of trade, and import-export duties. In addition to taxes, officials who had fallen into disrepute were likely to have their land and property confiscated by the Sultan.
Education was another important area for the Sultan. Schools attached to mosques and funded by religious foundations provided a largely free education to Muslim boys in advance of the Christian countries of the time. In his capital, Suleiman increased the number of "mektebs" (, primary schools) to fourteen, teaching boys to read and write as well as the principles of Islam. Young men wishing further education could proceed to one of eight "medreses" (, colleges), whose studies included grammar, metaphysics, philosophy, astronomy and astrology. Higher "medreses" provided education of university status, whose graduates became "imams" () or teachers. Educational centers were often one of many buildings surrounding the courtyards of mosques, others included libraries, refectories, fountains, soup kitchens and hospitals for the benefit of the public.
Cultural achievements.
Under Suleiman's patronage, the Ottoman Empire entered the golden age of its cultural development. Hundreds of imperial artistic societies (called the "Ehl-i Hiref", "Community of the Talented") were administered at the Imperial seat, the Topkapı Palace. After an apprenticeship, artists and craftsmen could advance in rank within their field and were paid commensurate wages in quarterly annual installments. Payroll registers that survive testify to the breadth of Suleiman's patronage of the arts, the earliest of documents dating from 1526 list 40 societies with over 600 members. The "Ehl-i Hiref" attracted the empire's most talented artisans to the Sultan's court, both from the Islamic world and from the recently conquered territories in Europe, resulting in a blend of Arabic, Turkish and European cultures. Artisans in service of the court included painters, book binders, furriers, jewellers and goldsmiths. Whereas previous rulers had been influenced by Persian culture (Suleiman's father, Selim I, wrote poetry in Persian), Suleiman's patronage of the arts saw the Ottoman Empire assert its own artistic legacy.
Suleiman himself was an accomplished poet, writing in Persian and Turkish under the takhallus (nom de plume) "Muhibbi" (, "Lover"). Some of Suleiman's verses have become Turkish proverbs, such as the well-known "Everyone aims at the same meaning, but many are the versions of the story". When his young son Mehmed died in 1543, he composed a moving chronogram to commemorate the year: "Peerless among princes, my Sultan Mehmed". In addition to Suleiman's own work, many great talents enlivened the literary world during Suleiman's rule, including Fuzuli and Baki. The literary historian E. J. W. Gibb observed that "at no time, even in Turkey, was greater encouragement given to poetry than during the reign of this Sultan". Suleiman's most famous verse is:
Suleiman also became renowned for sponsoring a series of monumental architectural developments within his empire. The Sultan sought to turn Constantinople into the center of Islamic civilization by a series of projects, including bridges, mosques, palaces and various charitable and social establishments. The greatest of these were built by the Sultan's chief architect, Mimar Sinan, under whom Ottoman architecture reached its zenith. Sinan became responsible for over three hundred monuments throughout the empire, including his two masterpieces, the Süleymaniye and Selimiye mosques—the latter built in Adrianople (now Edirne) in the reign of Suleiman's son Selim II. Suleiman also restored the Dome of the Rock in Jerusalem and the Jerusalem city walls (which are the current walls of the Old City of Jerusalem), renovated the Kaaba in Mecca, and constructed a complex in Damascus.
Personal life.
Consorts and progeny.
Suleiman had three known consorts:
Suleiman had some children with his consorts:
Relationship with Hürrem Sultan.
Suleiman was infatuated with Hürrem Sultan, a harem girl from Ruthenia, then part of Poland. Western diplomats, taking notice of the palace gossip about her, called her "Russelazie" or "Roxelana", referring to her Ruthenian (Ukrainian) origins. The daughter of an Orthodox priest, she was captured by Tatars from Crimea, sold as a slave in Constantinople, and eventually rose through the ranks of the Harem to become Suleiman's favorite. Breaking with two centuries of Ottoman tradition, a former concubine had thus become the legal wife of the Sultan, much to the astonishment of the observers in the palace and the city. He also allowed Hürrem Sultan to remain with him at court for the rest of her life, breaking another tradition—that when imperial heirs came of age, they would be sent along with the imperial concubine who bore them to govern remote provinces of the Empire, never to return unless their progeny succeeded to the throne.
Under his pen name, Muhibbi, Sultan Suleiman composed this poem for Hürrem Sultan:
"Throne of my lonely niche, my wealth, my love, my moonlight.
My most sincere friend, my confidant, my very existence, my Sultan, my one and only love.
The most beautiful among the beautiful ...
My springtime, my merry faced love, my daytime, my sweetheart, laughing leaf ...
My plants, my sweet, my rose, the one only who does not distress me in this room ...
My Istanbul, my Caraman, the earth of my Anatolia
My Badakhshan, my Baghdad and Khorasan
My woman of the beautiful hair, my love of the slanted brow, my love of eyes full of misery ...
I'll sing your praises always
I, lover of the tormented heart, Muhibbi of the eyes full of tears, I am happy."
Grand Vizier Pargalı Ibrahim Pasha.
Pargalı Ibrahim Pasha was the boyhood friend of Suleiman. Ibrahim was originally a Christian from Parga (in Epirus), and when he was young was educated at the Palace School under the devshirme system. Suleiman made him the royal falconer, then promoted him to first officer of the Royal Bedchamber. Ibrahim Pasha rose to Grand Vizier in 1523 and commander-in-chief of all the armies. Suleiman also conferred upon Ibrahim Pasha the honor of "beylerbey" of Rumelia (first-ranking military governor-general), granting Ibrahim authority over all Turkish territories in Europe, as well as command of troops residing within them in times of war. According to a 17th-century chronicler, Ibrahim had asked Suleiman not to promote him to such high positions, fearing for his safety; to which Suleiman replied that under his reign, no matter what the circumstance, Ibrahim would never be put to death.
Yet Ibrahim eventually fell from grace with the Sultan. During his thirteen years as Grand Vizier, his rapid rise to power and vast accumulation of wealth had made Ibrahim many enemies at the Sultan's court. Reports had reached the Sultan of Ibrahim's impudence during a campaign against the Persian Safavid empire: in particular his adoption of the title "serasker sultan" () was seen as a grave affront to Suleiman.
Suleiman's suspicion of Ibrahim was worsened by a quarrel between the latter and the finance secretary ("defterdar") Iskender Çelebi. The dispute ended in the disgrace of Çelebi on charges of intrigue, with Ibrahim convincing Suleiman to sentence the "defterdar" to death. Before his death however, Çelebi's last words were to accuse Ibrahim of conspiracy against the Sultan. These dying words convinced Suleiman of Ibrahim's disloyalty, and on 15 March 1536 Ibrahim was executed.
Succession.
Sultan Suleiman's two wives (Hürrem and Mahidevran) had borne him six sons, four of whom survived past the 1550s. They were Mustafa, Selim, Bayezid, and Cihangir. Of these, only Mustafa, the eldest, was not Hürrem Sultan's son, but rather Mahidevran Sultan's, and therefore preceded Hürrem's children in the order of succession. Hürrem was aware that should Mustafa become Sultan her own children would be strangled. Yet Mustafa was recognized as the most talented of all the brothers and was supported by Pargalı İbrahim Pasha, who was by this time Suleiman's Grand Vizier. The Austrian ambassador Busbecq would note "Suleiman has among his children a son called Mustafa, marvelously well educated and prudent and of an age to rule, since he is 24 or 25 years old; may God never allow a Barbary of such strength to come near us", going on to talk of Mustafa's "remarkable natural gifts".
Hürrem is usually held at least partly responsible for the intrigues in nominating a successor. Although she was Suleiman's wife, she exercised no official public role. This did not, however, prevent Hürrem from wielding powerful political influence. Since the Empire lacked, until the reign of Ahmed I, any formal means of nominating a successor, successions usually involved the death of competing princes in order to avert civil unrest and rebellions. In attempting to avoid the execution of her sons, Hürrem used her influence to eliminate those who supported Mustafa's accession to the throne.
Thus in power struggles apparently instigated by Hürrem, Suleiman had Ibrahim murdered and replaced with her sympathetic son-in-law, Rüstem Pasha. By 1552, when the campaign against Persia had begun with Rüstem appointed commander-in-chief of the expedition, intrigues against Mustafa began. Rüstem sent one of Suleiman's most trusted men to report that since Suleiman was not at the head of the army, the soldiers thought the time had come to put a younger prince on the throne; at the same time he spread rumors that Mustafa had proved receptive to the idea. Angered by what he came to believe were Mustafa's plans to claim the throne, the following summer upon return from his campaign in Persia, Suleiman summoned him to his tent in the Ereğli valley, stating he would "be able to clear himself of the crimes he was accused of and would have nothing to fear if he came".
Mustafa was confronted with a choice: either he appeared before his father at the risk of being killed; or, if he refused to attend, he would be accused of betrayal. In the end, Mustafa chose to enter his father's tent, confident that the support of the army would protect him. Busbecq, who claims to have received an account from an eyewitness, describes Mustafa's final moments. As Mustafa entered his father's tent, Suleiman's eunuchs attacked Mustafa, with the young prince putting up a brave defence. Suleiman, separated from the struggle only by the linen hangings of the tent, peered through the chamber of his tent and "directed fierce and threatening glances upon the mutes, and by menacing gestures sternly rebuked their hesitation. Thereupon, the mutes in their alarm, redoubling their efforts, hurled Mustafa to the ground and, throwing the bowstring round his neck, strangled him."
Cihangir is said to have died of grief a few months after the news of his half-brother's murder. The two surviving brothers, Selim and Bayezid, were given command in different parts of the empire. Within a few years, however, civil war broke out between the brothers, each supported by his loyal forces. With the aid of his father's army, Selim defeated Bayezid in Konya in 1559, leading the latter to seek refuge with the Safavids along with his four sons. Following diplomatic exchanges, the Sultan demanded from the Safavid Shah that Bayezid be either extradited or executed. In return for large amounts of gold, the Shah allowed a Turkish executioner to strangle Bayezid and his four sons in 1561, clearing the path for Selim's succession to the throne seven years later.
Death.
His mausoleum is adjacent to Hurrem’s, a separate and more sombre domed structure, at the courtyard of the Süleymaniye Mosque. On 5 September 1566, Suleiman, who had set out from Constantinople to command an expedition to Hungary, died before an Ottoman victory at the Battle of Szigetvár in Hungary and the Grand Vizier kept his death secret during the retreat for the enthronement of Selim II. Just the night before the sickly sultan died in his tent, two months before he would have turned 72. The sultan’s body was taken home to be buried in Istanbul, leaving behind the legend along with his heart and internal organs in Hungary, buried under the military tent where he died, in a golden casket.
Legacy.
At the time of Suleiman's death, the Ottoman Empire was one of the world's foremost powers. Suleiman's conquests had brought under the control of the Empire the major Muslim cities (Mecca, Medina, Jerusalem, Damascus, Cairo and Baghdad), many Balkan provinces (reaching present day Croatia and Austria), and most of North Africa. His expansion into Europe had given the Ottoman Turks a powerful presence in the European balance of power. Indeed, such was the perceived threat of the Ottoman Empire under the reign of Suleiman that Austria's ambassador Busbecq warned of Europe's imminent conquest: "On Turks' side are the resources of a mighty empire, strength unimpaired, habituation to victory, endurance of toil, unity, discipline, frugality and watchfulness ... Can we doubt what the result will be? ... When the Turks have settled with Persia, they will fly at our throats supported by the might of the whole East; how unprepared we are I dare not say."
Even thirty years after his death, "Sultan Solyman" was quoted by the English playwright William Shakespeare as a military prodigy in "The Merchant of Venice", where the Prince of Morocco boasts about his prowess by saying that he defeated Suleiman in three battles (Act 2, Scene 1).
Suleiman's legacy was not, however, merely in the military field. The French traveler Jean de Thévenot bears witness a century later to the "strong agricultural base of the country, the well being of the peasantry, the abundance of staple foods and the pre-eminence of organization in Suleiman's government". The administrative and legal reforms which earned him the name Law Giver ensured the Empire's survival long after his death, an achievement which "took many generations of decadent heirs to undo".
Through his personal patronage, Suleiman also presided over the Golden Age of the Ottoman Empire, representing the pinnacle of the Ottoman Turks' cultural achievement in the realm of architecture, literature, art, theology and philosophy. Today the skyline of the Bosphorus and of many cities in modern Turkey and the former Ottoman provinces, are still adorned with the architectural works of Mimar Sinan. One of these, the Süleymaniye Mosque, is the final resting place of Suleiman and Hürrem Sultan: they are buried in separate domed mausoleums attached to the mosque.
However, after his death, the Ottoman Empire entered into a state of decline and stagnation during the reign of Sultan Selim II and later (not so great) sultans. The Ottoman conquests of Europe were ended permanently by major defeats such as the Battle of Lepanto and the Battle of Vienna. As the years passed, the Ottoman Empire slowly turned into a shadow of its former glory, becoming known as the "sick man of Europe", where the Christian powers gradually regained their might, gaining new technologies and weapons for their armies until the Empire's dissolution by the reign of Mehmed VI, the last Sultan of the Ottoman Empire, who was removed after World War I, which allowed the empire's total dismemberment as even the Muslim provinces became independent or part of colonial empires and Atatürk opted for a republican Turkish nation state.

</doc>
<doc id="26994" url="https://en.wikipedia.org/wiki?curid=26994" title="Scotland">
Scotland

Scotland (; Scots: ; ) is a country that is part of the United Kingdom and covers the northern third of the island of Great Britain. It shares a border with England to the south, and is otherwise surrounded by the Atlantic Ocean, with the North Sea to the east and the North Channel and Irish Sea to the south-west. In addition to the mainland, the country is made up of more than 790 islands, including the Northern Isles and the Hebrides.
Edinburgh, the country's capital and second-largest city, was the hub of the Scottish Enlightenment of the 18th century, which transformed Scotland into one of the commercial, intellectual, and industrial powerhouses of Europe. Glasgow, Scotland's largest city, was once one of the world's leading industrial cities and now lies at the centre of the Greater Glasgow conurbation. Scottish waters consist of a large sector of the North Atlantic and the North Sea, containing the largest oil reserves in the European Union. This has given Aberdeen, the third-largest city in Scotland, the title of Europe's oil capital.
The Kingdom of Scotland emerged as an independent sovereign state in the Early Middle Ages and continued to exist until 1707. By inheritance in 1603, King James VI of Scotland became King of England and King of Ireland, thus forming a personal union of the three kingdoms. Scotland subsequently entered into a political union with England on 1 May 1707 to create the new Kingdom of Great Britain. The union also created a new Parliament of Great Britain, which succeeded both the Parliament of Scotland and the Parliament of England. The Treaty of Union was agreed in 1706 and enacted by the twin Acts of Union 1707 passed by the Parliaments of both countries, despite popular opposition and anti-union riots in Edinburgh, Glasgow, and elsewhere. Great Britain itself subsequently entered into a political union with Ireland on 1 January 1801 to create the United Kingdom of Great Britain and Ireland.
Scotland's legal system has remained separate from those of England and Wales and Northern Ireland, and Scotland constitutes a distinct jurisdiction in public and private law. The continued existence of legal, educational and religious institutions distinct from those in the remainder of the UK have all contributed to the continuation of Scottish culture and national identity since the 1707 union. Following a referendum in 1997, a Scottish Parliament was re-established, this time as a devolved legislature with authority over many areas of home affairs. The Scottish National Party, which supports Scottish independence, won an overall majority in the 2011 general election. An independence referendum held on 18 September 2014 rejected independence by a majority of 55% to 45% on an 85% voter turnout.
Scotland is a member nation of the British–Irish Council, and the British–Irish Parliamentary Assembly. Scotland is represented in the European Union and the European Parliament with six MEPs.
History.
Etymology.
"Scotland" comes from "Scoti", the Latin name for the Gaels. The Late Latin word "Scotia" ("land of the Gaels") was initially used to refer to Ireland. By the 11th century at the latest, "Scotia" was being used to refer to (Gaelic-speaking) Scotland north of the river Forth, alongside "Albania" or "Albany", both derived from the Gaelic "Alba". The use of the words "Scots" and "Scotland" to encompass all of what is now Scotland became common in the Late Middle Ages.
Early history.
Repeated glaciations, which covered the entire land mass of modern Scotland, destroyed any traces of human habitation that may have existed before the Mesolithic period. It is believed the first post-glacial groups of hunter-gatherers arrived in Scotland around 12,800 years ago, as the ice sheet retreated after the last glaciation.
Groups of settlers began building the first known permanent houses on Scottish soil around 9,500 years ago, and the first villages around 6,000 years ago. The well-preserved village of Skara Brae on the mainland of Orkney dates from this period. Neolithic habitation, burial and ritual sites are particularly common and well preserved in the Northern Isles and Western Isles, where a lack of trees led to most structures being built of local stone.
The 2009 discovery in Scotland of a 4000-year-old tomb with burial treasures at Forteviot, near Perth, the capital of a Pictish Kingdom in the 8th and 9th centuries AD, is unrivalled anywhere in Britain. It contains the remains of an early Bronze Age ruler laid out on white quartz pebbles and birch bark. It was also discovered for the first time that early Bronze Age people placed flowers in their graves.
Scotland may have been part of a Late Bronze Age maritime trading culture called the Atlantic Bronze Age, which included other Celtic nations, and the areas that became England, France, Spain, and Portugal.
In the winter of 1850, a severe storm hit Scotland, causing widespread damage and over 200 deaths. In the Bay of Skaill, the storm stripped the earth from a large irregular knoll, known as "Skerrabra". When the storm cleared, local villagers found the outline of a village, consisting of a number of small houses without roofs. William Watt of Skaill, the local laird, began an amateur excavation of the site, but after uncovering four houses, the work was abandoned in 1868. The site remained undisturbed until 1913, when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artefacts. In 1924, another storm swept away part of one of the houses and it was determined the site should be made secure and more seriously investigated. The job was given to University of Edinburgh's Professor Vere Gordon Childe who travelled to Skara Brae for the first time in mid-1927.
Roman influence.
The written protohistory of Scotland began with the arrival of the Roman Empire in southern and central Great Britain, when the Romans occupied what is now England and Wales, administering it as a province called "Britannia". Roman invasions and occupations of southern Scotland were a series of brief interludes.
According to the Roman historian Tacitus, the Caledonians "turned to armed resistance on a large scale", attacking Roman forts and skirmishing with their legions. In a surprise night-attack, the Caledonians very nearly wiped out the whole 9th Legion until it was saved by Agricola's cavalry.
In AD 83–84, the General Gnaeus Julius Agricola defeated the Caledonians at the Battle of Mons Graupius. Tacitus wrote that, before the battle, the Caledonian leader, Calgacus, gave a rousing speech in which he called his people the "last of the free" and accused the Romans of "making the world a desert and calling it peace" (freely translated). After the Roman victory, Roman forts were briefly set along the Gask Ridge close to the Highland line (only Cawdor near Inverness is known to have been constructed beyond that line). Three years after the battle, the Roman armies had withdrawn to the Southern Uplands.
The Romans erected Hadrian's Wall to control tribes on both sides of the wall so the "Limes Britannicus" became the northern border of the Roman Empire; although the army held the Antonine Wall in the Central Lowlands for two short periods – the last of these during the time of Emperor Septimius Severus from 208 until 210.
The Roman military occupation of a significant part of what is now northern Scotland lasted only about 40 years; although their influence on the southern section of the country, occupied by Brythonic tribes such as the Votadini and Damnonii, would still have been considerable between the first and fifth centuries. The Welsh term Hen Ogledd ("Old North") is used by scholars to describe what is now the North of England and the South of Scotland during its habitation by Brittonic-speaking people around AD 500 to 800. According to writings from the 9th and 10th centuries, the Gaelic kingdom of Dál Riata was founded in the 6th century in western Scotland. The 'traditional' view is that settlers from Ireland founded the kingdom, bringing Gaelic language and culture with them. However, recently some archaeologists have argued against this view, saying there is no archaeological or placename evidence for a migration or a takeover by a small group of elites.
Middle Ages.
The Kingdom of the Picts (based in Fortriu by the 6th century) was the state that eventually became known as "Alba" or "Scotland". The development of "Pictland", according to the historical model developed by Peter Heather, was a natural response to Roman imperialism. Another view places emphasis on the Battle of Dun Nechtain, and the reign of Bridei m. Beli (671–693), with another period of consolidation in the reign of Óengus mac Fergusa (732–761).
The Kingdom of the Picts as it was in the early 8th century, when Bede was writing, was largely the same as the kingdom of the Scots in the reign of Alexander I (1107–1124). However, by the tenth century, the Pictish kingdom was dominated by what we can recognise as Gaelic culture, and had developed a traditional story of an Irish conquest around the ancestor of the contemporary royal dynasty, Cináed mac Ailpín (Kenneth MacAlpin).
From a base of territory in eastern Scotland north of the River Forth and south of the River Oykel, the kingdom acquired control of the lands lying to the north and south. By the 12th century, the kings of Alba had added to their territories the English-speaking land in the south-east and attained overlordship of Gaelic-speaking Galloway and Norse-speaking Caithness; by the end of the 13th century, the kingdom had assumed approximately its modern borders. However, processes of cultural and economic change beginning in the 12th century ensured Scotland looked very different in the later Middle Ages.
The push for this change was the reign of David I and the Davidian Revolution. Feudalism, government reorganisation and the first legally recognised towns (called burghs) began in this period. These institutions and the immigration of French and Anglo-French knights and churchmen facilitated cultural osmosis, whereby the culture and language of the low-lying and coastal parts of the kingdom's original territory in the east became, like the newly acquired south-east, English-speaking, while the rest of the country retained the Gaelic language, apart from the Northern Isles of Orkney and Shetland, which remained under Norse rule until 1468. The Scottish state entered a largely successful and stable period between the 12th and 14th centuries, there was relative peace with England, trade and educational links were well developed with the Continent and at the height of this cultural flowering John Duns Scotus was one of Europe's most important and influential philosophers.
The death of Alexander III in March 1286, followed by that of his granddaughter Margaret, Maid of Norway, broke the centuries-old succession line of Scotland's kings and shattered the 200-year golden age that began with David I. Edward I of England was asked to arbitrate between claimants for the Scottish crown, and he organised a process known as the Great Cause to identify the most legitimate claimant. John Balliol was pronounced king in the Great Hall of Berwick Castle on 17 November 1292 and inaugurated at Scone on 30 November, St. Andrew's Day. Edward I, who had coerced recognition as Lord Paramount of Scotland, the feudal superior of the realm, steadily undermined John's authority. In 1294, Balliol and other Scottish lords refused Edward's demands to serve in his army against the French. Instead the Scottish parliament sent envoys to France to negotiate an alliance. Scotland and France sealed a treaty on 23 October 1295, known as the Auld Alliance (1295–1560). War ensued and King John was deposed by Edward who took personal control of Scotland. Andrew Moray and William Wallace initially emerged as the principal leaders of the resistance to English rule in what became known as the Wars of Scottish Independence (1296–1328).
The nature of the struggle changed significantly when Robert the Bruce, Earl of Carrick, killed his rival John Comyn on 10 February 1306 at Greyfriars Kirk in Dumfries. He was crowned king (as Robert I) less than seven weeks later. Robert I battled to restore Scottish Independence as King for over 20 years, beginning by winning Scotland back from the Norman English invaders piece by piece. Victory at the Battle of Bannockburn in 1314 proved the Scots had regained control of their kingdom. In 1315, Edward Bruce, brother of the King, was briefly appointed High King of Ireland during an ultimately unsuccessful Scottish invasion of Ireland aimed at strengthening Scotland's position in its wars against England. In 1320 the world's first documented declaration of independence, the Declaration of Arbroath, won the support of Pope John XXII, leading to the legal recognition of Scottish sovereignty by the English Crown.
However, war with England continued for several decades after the death of Bruce. A civil war between the Bruce dynasty and their long-term Comyn-Balliol rivals lasted until the middle of the 14th century. Although the Bruce dynasty was successful, David II's lack of an heir allowed his half-nephew Robert II to come to the throne and establish the Stewart Dynasty. The Stewarts ruled Scotland for the remainder of the Middle Ages. The country they ruled experienced greater prosperity from the end of the 14th century through the Scottish Renaissance to the Reformation. This was despite continual warfare with England, the increasing division between Highlands and Lowlands, and a large number of royal minorities.
This period was the height of the Franco-Scottish alliance. The Scots Guard – la Garde Écossaise – was founded in 1418 by Charles VII of France. The Scots soldiers of the Garde Écossaise fought alongside Joan of Arc against England during the Hundred Years War. In March 1421, a Franco-Scots force under John Stewart, 2nd Earl of Buchan, and Gilbert de Lafayette, defeated a larger English army at the Battle of Baugé. Three years later, at the Battle of Verneuil, the French and Scots lost around 7000 men. The Scottish intervention contributed to France's victory in the war.
Early modern era.
In 1502, James IV of Scotland signed the Treaty of Perpetual Peace with Henry VII of England. He also married Henry's daughter, Margaret Tudor, setting the stage for the Union of the Crowns. For Henry, the marriage into one of Europe's most established monarchies gave legitimacy to the new Tudor royal line. A decade later, James made the fateful decision to invade England in support of France under the terms of the Auld Alliance. He was the last British monarch to die in battle, at the Battle of Flodden. Within a generation the Auld Alliance was ended by the Treaty of Edinburgh. France agreed to withdraw all land and naval forces. In the same year, 1560, John Knox realised his goal of seeing Scotland become a Protestant nation and the Scottish parliament revoke papal authority in Scotland. Mary, Queen of Scots, a Catholic and former queen of France, was forced to abdicate in 1567.
In 1603, James VI, King of Scots inherited the thrones of the Kingdom of England and the Kingdom of Ireland, and became King James I of England and Ireland, and left Edinburgh for London. With the exception of a short period under the Protectorate, Scotland remained a separate state, but there was considerable conflict between the crown and the Covenanters over the form of church government. The Glorious Revolution of 1688–89 saw the overthrow of the King James VII of Scotland and II of England by the English Parliament in favour of William and Mary. As late as the 1690s, Scotland experienced famine, which reduced the population of parts of the country by at least 20 per cent.
In 1698, the Scots attempted an ambitious project to secure a trading colony on the Isthmus of Panama. Almost every Scottish landowner who had money to spare is said to have invested in the Darien scheme. Its failure bankrupted these landowners, but not the burghs. Nevertheless, the nobles' bankruptcy, along with the threat of an English invasion, played a leading role in convincing the Scots elite to back a union with England.
On 22 July 1706, the Treaty of Union was agreed between representatives of the Scots Parliament and the Parliament of England and the following year twin Acts of Union were passed by both parliaments to create the united Kingdom of Great Britain with effect from 1 May 1707.
18th century.
With trade tariffs with England now abolished, trade blossomed, especially with Colonial America. The clippers belonging to the Glasgow Tobacco Lords were the fastest ships on the route to Virginia. Until the American War of Independence in 1776, Glasgow was the world's premier tobacco port, dominating world trade. The disparity between the wealth of the merchant classes of the Scottish Lowlands and the ancient clans of the Scottish Highlands grew, amplifying centuries of division.
The deposed Jacobite Stuart claimants had remained popular in the Highlands and north-east, particularly amongst non-Presbyterians, including Roman Catholics and Episcopalian Protestants. However, two major Jacobite Risings launched in 1715 and 1745 failed to remove the House of Hanover from the British throne. The threat of the Jacobite movement to the United Kingdom and its monarchs effectively ended at the Battle of Culloden, Great Britain's last pitched battle. This defeat paved the way for large-scale removals of the indigenous populations of the Highlands and Islands, known as the Highland Clearances.
The Scottish Enlightenment and the Industrial Revolution made Scotland into an intellectual, commercial and industrial powerhouse–so much so Voltaire said "We look to Scotland for all our ideas of civilisation." With the demise of Jacobitism and the advent of the Union, thousands of Scots, mainly Lowlanders, took up numerous positions of power in politics, civil service, the army and navy, trade, economics, colonial enterprises and other areas across the nascent British Empire. Historian Neil Davidson notes "after 1746 there was an entirely new level of participation by Scots in political life, particularly outside Scotland." Davidson also states "far from being 'peripheral' to the British economy, Scotland – or more precisely, the Lowlands – lay at its core."
19th century.
The Scottish Reform Act 1832 increased the number of Scottish MPs and widened the franchise to include more of the middle classes. From the mid-century there were increasing calls for Home Rule for Scotland and the post of Secretary of State for Scotland was revived. Towards the end of the century Prime Ministers of Scottish descent included William E. Gladstone, and the Earl of Rosebery. In the later 19th century the growing importance of the working classes was marked by Keir Hardie's success in the Mid Lanarkshire by-election, 1888, leading to the foundation of the Scottish Labour Party, which was absorbed into the Independent Labour Party in 1895, with Hardie as its first leader.
Glasgow became one of the largest cities in the world, and known as "the Second City of the Empire" after London. After 1860 the Clydeside shipyards specialised in steamships made of iron (after 1870, made of steel), which rapidly replaced the wooden sailing vessels of both the merchant fleets and the battle fleets of the world. It became the world's pre-eminent shipbuilding centre. The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis.
While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the 18th century, disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as the physicists James Clerk Maxwell and Lord Kelvin, and the engineers and inventors James Watt and William Murdoch, whose work was critical to the technological developments of the Industrial Revolution throughout Britain. In literature the most successful figure of the mid-19th century was Walter Scott. His first prose work, Waverley in 1814, is often called the first historical novel. It launched a highly successful career that probably more than any other helped define and popularise Scottish cultural identity. In the late 19th century, a number of Scottish-born authors achieved international reputations, such as Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald. Scotland also played a major part in the development of art and architecture. The Glasgow School, which developed in the late 19th century, and flourished in the early 20th century, produced a distinctive blend of influences including the Celtic Revival the Arts and Crafts Movement, and Japonisme, which found favour throughout the modern art world of continental Europe and helped define the Art Nouveau style. Proponents included architect and artist Charles Rennie Mackintosh.
This period saw a process of rehabilitation for Highland culture. In the 1820s, as part of the Romantic revival, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe, prompted by the popularity of Macpherson's Ossian cycle and then Walter Scott's Waverley novels. However, the Highlands remained very poor and traditional. The desire to improve agriculture and profits led to the Highland Clearances, in which much of the population of the Highlands suffered forced displacement as lands were enclosed, principally so that they could be used for sheep farming. The clearances followed patterns of agricultural change throughout Britain, but were particularly notorious as a result of the late timing, the lack of legal protection for year-by-year tenants under Scots law, the abruptness of the change from the traditional clan system, and the brutality of many evictions. One result was a continuous exodus from the land—to the cities, or further afield to England, Canada, America or Australia. The population of Scotland grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. Even with the development of industry there were not enough good jobs. As a result, during the period 1841–1931, about 2 million Scots migrated to North America and Australia, and another 750,000 Scots relocated to England.
After prolonged years of struggle in the Kirk, in 1834 the Evangelicals gained control of the General Assembly and passed the Veto Act, which allowed congregations to reject unwanted "intrusive" presentations to livings by patrons. The following "Ten Years' Conflict" of legal and political wrangling ended in defeat for the non-intrusionists in the civil courts. The result was a schism from the church by some of the non-intrusionists led by Dr Thomas Chalmers, known as the Great Disruption of 1843. Roughly a third of the clergy, mainly from the North and Highlands, formed the separate Free Church of Scotland. In the late 19th century growing divisions between fundamentalist Calvinists and theological liberals resulted in a further split in the Free Church as the rigid Calvinists broke away to form the Free Presbyterian Church in 1893. Catholic Emancipation in 1829 and the influx of large numbers of Irish immigrants, particularly after the famine years of the late 1840s, mainly to the growing lowland centres like Glasgow, led to a transformation in the fortunes of Catholicism. In 1878, despite opposition, a Roman Catholic ecclesiastical hierarchy was restored to the country, and Catholicism became a significant denomination within Scotland.
Industrialisation, urbanisation and the Disruption of 1843 all undermined the tradition of parish schools. From 1830 the state began to fund buildings with grants; then from 1846 it was funding schools by direct sponsorship; and in 1872 Scotland moved to a system like that in England of state-sponsored largely free schools, run by local school boards. The historic University of Glasgow became a leader in British higher education by providing the educational needs of youth from the urban and commercial classes, as opposed to the upper class. The University of St Andrews pioneered the admission of women to Scottish universities. From 1892 Scottish universities could admit and graduate women and the numbers of women at Scottish universities steadily increased until the early 20th century.
Early 20th century.
Scotland played a major role in the British effort in the First World War. It especially provided manpower, ships, machinery, fish and money. With a population of 4.8 million in 1911, Scotland sent over half a million men to the war, of whom over a quarter died in combat or from disease, and 150,000 were seriously wounded. Field Marshal Sir Douglas Haig was Britain's commander on the Western Front.
The war saw the emergence of a radical movement called "Red Clydeside" led by militant trades unionists. Formerly a Liberal stronghold, the industrial districts switched to Labour by 1922, with a base among the Irish Catholic working class districts. Women were especially active in building neighbourhood solidarity on housing issues. However, the "Reds" operated within the Labour Party and had little influence in Parliament and the mood changed to passive despair by the late 1920s.
The shipbuilding industry expanded by a third and expected renewed prosperity, but instead a serious depression hit the economy by 1922 and it did not fully recover until 1939. The interwar years were marked by economic stagnation in rural and urban areas, and high unemployment. Indeed, the war brought with it deep social, cultural, economic, and political dislocations. Thoughtful Scots pondered their declension, as the main social indicators such as poor health, bad housing, and long-term mass unemployment, pointed to terminal social and economic stagnation at best, or even a downward spiral. Service abroad on behalf of the Empire lost its allure to ambitious young people, who left Scotland permanently. The heavy dependence on obsolescent heavy industry and mining was a central problem, and no one offered workable solutions. The despair reflected what Finlay (1994) describes as a widespread sense of hopelessness that prepared local business and political leaders to accept a new orthodoxy of centralised government economic planning when it arrived during the Second World War.
The Second World War brought renewed prosperity, despite extensive bombing of cities by the Luftwaffe. It saw the invention of radar by Robert Watson-Watt, which was invaluable in the Battle of Britain as was the leadership at RAF Fighter Command of Air Chief Marshal Sir Hugh Dowding.
Since 1945.
After 1945, Scotland's economic situation became progressively worse due to overseas competition, inefficient industry, and industrial disputes. Only in recent decades has the country enjoyed something of a cultural and economic renaissance. Economic factors contributing to this recovery include a resurgent financial services industry, electronics manufacturing, (see Silicon Glen), and the North Sea oil and gas industry. The introduction in 1989 by Margaret Thatcher's government of the Community Charge (widely known as the Poll Tax) one year before the rest of the United Kingdom, contributed to a growing movement for a return to direct Scottish control over domestic affairs. Following a referendum on devolution proposals in 1997, the Scotland Act 1998 was passed by the United Kingdom Parliament to establish a devolved Scottish Parliament and Scottish Government with responsibility for most laws specific to Scotland.
Education.
The Scottish education system has always remained distinct from the rest of the United Kingdom, with a characteristic emphasis on a broad education. In the 15th century, the Humanist emphasis on education cumulated with the passing of the Education Act 1496, which decreed that all sons of barons and freeholders of substance should attend grammar schools to learn "perfyct Latyne", resulting in an increase in literacy among a male and wealthy elite. In the Reformation the 1560 "First Book of Discipline" set out a plan for a school in every parish, but this proved financially impossible. In 1616 an act in Privy council commanded every parish to establish a school. By the late seventeenth century there was a largely complete network of parish schools in the lowlands, but in the Highlands basic education was still lacking in many areas. Education remained a matter for the church rather than the state until the Education Act (1872).
Geography and natural history.
The mainland of Scotland comprises the northern third of the land mass of the island of Great Britain, which lies off the north-west coast of Continental Europe. The total area is , comparable to the size of the Czech Republic. Scotland's only land border is with England, and runs for between the basin of the River Tweed on the east coast and the Solway Firth in the west. The Atlantic Ocean borders the west coast and the North Sea is to the east. The island of Ireland lies only from the south-western peninsula of Kintyre; Norway is to the east and the Faroes, to the north.
The territorial extent of Scotland is generally that established by the 1237 Treaty of York between Scotland and the Kingdom of England and the 1266 Treaty of Perth between Scotland and Norway. Important exceptions include the Isle of Man, which having been lost to England in the 14th century is now a crown dependency outside of the United Kingdom; the island groups Orkney and Shetland, which were acquired from Norway in 1472; and Berwick-upon-Tweed, lost to England in 1482.
The geographical centre of Scotland lies a few miles from the village of Newtonmore in Badenoch. Rising to above sea level, Scotland's highest point is the summit of Ben Nevis, in Lochaber, while Scotland's longest river, the River Tay, flows for a distance of .
Geology and geomorphology.
The whole of Scotland was covered by ice sheets during the Pleistocene ice ages and the landscape is much affected by glaciation. From a geological perspective, the country has three main sub-divisions.
The Highlands and Islands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland largely comprises ancient rocks from the Cambrian and Precambrian, which were uplifted during the later Caledonian Orogeny. It is interspersed with igneous intrusions of a more recent age, remnants of which formed mountain massifs such as the Cairngorms and Skye Cuillins.
A significant exception to the above are the fossil-bearing beds of Old Red Sandstones found principally along the Moray Firth coast. The Highlands are generally mountainous and the highest elevations in the British Isles are found here. Scotland has over 790 islands divided into four main groups: Shetland, Orkney, and the Inner Hebrides and Outer Hebrides. There are numerous bodies of freshwater including Loch Lomond and Loch Ness. Some parts of the coastline consist of machair, a low lying dune pasture land.
The Central Lowlands is a rift valley mainly comprising Paleozoic formations. Many of these sediments have economic significance for it is here that the coal and iron bearing rocks that fuelled Scotland's industrial revolution are found. This area has also experienced intense volcanism, Arthur's Seat in Edinburgh being the remnant of a once much larger volcano. This area is relatively low-lying, although even here hills such as the Ochils and Campsie Fells are rarely far from view.
The Southern Uplands are a range of hills almost long, interspersed with broad valleys. They lie south of a second fault line (the Southern Uplands fault) that runs from Girvan to Dunbar. The geological foundations largely comprise Silurian deposits laid down some 4–500 million years ago. The high point of the Southern Uplands is Merrick with an elevation of . The Southern Uplands is home to the UK's highest village, Wanlockhead ( above sea level).
Climate.
The climate of Scotland is temperate and oceanic, and tends to be very changeable. As it is warmed by the Gulf Stream from the Atlantic, it has much milder winters (but cooler, wetter summers) than areas on similar latitudes, such as Labrador, southern Scandinavia, the Moscow region in Russia, and the Kamchatka Peninsula on the opposite side of Eurasia. However, temperatures are generally lower than in the rest of the UK, with the coldest ever UK temperature of recorded at Braemar in the Grampian Mountains, on 11 February 1895. Winter maxima average in the Lowlands, with summer maxima averaging . The highest temperature recorded was at Greycrook, Scottish Borders on 9 August 2003.
The west of Scotland is usually warmer than the east, owing to the influence of Atlantic ocean currents and the colder surface temperatures of the North Sea. Tiree, in the Inner Hebrides, is one of the sunniest places in the country: it had more than 300 hours of sunshine in May 1975. Rainfall varies widely across Scotland. The western highlands of Scotland are the wettest, with annual rainfall in a few places exceeding . In comparison, much of lowland Scotland receives less than annually. Heavy snowfall is not common in the lowlands, but becomes more common with altitude. Braemar has an average of 59 snow days per year, while many coastal areas average fewer than 10 days of lying snow per year.
Flora and fauna.
Scotland's wildlife is typical of the north west of Europe, although several of the larger mammals such as the lynx, brown bear, wolf, elk and walrus were hunted to extinction in historic times. There are important populations of seals and internationally significant nesting grounds for a variety of seabirds such as gannets. The golden eagle is something of a national icon.
On the high mountain tops species including ptarmigan, mountain hare and stoat can be seen in their white colour phase during winter months. Remnants of the native Scots pine forest exist and within these areas the Scottish crossbill, the UK's only endemic bird species and vertebrate, can be found alongside capercaillie, wildcat, red squirrel and pine marten. In recent years various animals have been re-introduced, including the white-tailed sea eagle in 1975, the red kite in the 1980s, and more recently there have been experimental projects involving the beaver and wild boar. Today, much of the remaining native Caledonian Forest lies within the Cairngorms National Park and remnants of the forest remain at 84 locations across Scotland. On the west coast, remnants of ancient Celtic Rainforest still remain, particularly on the Taynish peninsula in Argyll, these forests are particularly rare due to high rates of deforestation throughout Scottish history.
The flora of the country is varied incorporating both deciduous and coniferous woodland and moorland and tundra species. However, large scale commercial tree planting and the management of upland moorland habitat for the grazing of sheep and commercial field sport activities impacts upon the distribution of indigenous plants and animals. The UK's tallest tree is a grand fir planted beside Loch Fyne, Argyll in the 1870s, and the Fortingall Yew may be 5,000 years old and is probably the oldest living thing in Europe. Although the number of native vascular plants is low by world standards, Scotland's substantial bryophyte flora is of global importance.
Demographics.
The population of Scotland at the 2001 Census was 5,062,011. This rose to 5,295,400, the highest ever, at the 2011 Census.
In the 2011 Census, 62% of Scotland's population stated their national identity as 'Scottish only', 18% as 'Scottish and British', 8% as 'British only', and 4% chose other national identities.
Although Edinburgh is the capital of Scotland, the largest city is Glasgow, which has just over 584,000 inhabitants. The Greater Glasgow conurbation, with a population of almost 1.2 million, is home to nearly a quarter of Scotland's population. The Central Belt is where most of the main towns and cities are located, including Glasgow, Edinburgh, Dundee and Perth. Scotland's only major city outside the Central Belt is Aberdeen.
In general, only the more accessible and larger islands retain inhabited. Currently, fewer than 90 remain inhabited. The Southern Uplands are essentially rural in nature and dominated by agriculture and forestry. Because of housing problems in Glasgow and Edinburgh, five new towns were created between 1947 and 1966. They are East Kilbride, Glenrothes, Livingston, Cumbernauld, and Irvine.
Immigration since World War II has given Glasgow, Edinburgh and Dundee small South Asian communities. In 2011, there were an estimated 49,000 ethnically Pakistani people living in Scotland, making them the largest non-White ethnic group. Since the Enlargement of the European Union more people from Central and Eastern Europe have moved to Scotland, and the 2011 census indicated that 61,000 Poles live there.
Scotland has three officially recognised languages: English, Scots, and Scottish Gaelic. Scottish Standard English, a variety of English as spoken in Scotland, is at one end of a bipolar linguistic continuum, with broad Scots at the other. Scottish Standard English may have been influenced to varying degrees by Scots. The 2011 census indicated that 63% of the population had "no skills in Scots". Others speak Highland English. Gaelic is mostly spoken in the Western Isles, where a large proportion of people still speak it; however, nationally its use is confined to just 1% of the population. The number of Gaelic speakers in Scotland dropped from 250,000 in 1881 to 60,000 in 2008.
There are many more people with Scottish ancestry living abroad than the total population of Scotland. In the 2000 Census, 9.2 million Americans self-reported some degree of Scottish descent. Ulster's Protestant population is mainly of lowland Scottish descent, and it is estimated that there are more than 27 million descendants of the Scots-Irish migration now living in the US. In Canada, the Scottish-Canadian community accounts for 4.7 million people. About 20% of the original European settler population of New Zealand came from Scotland.
In August 2012, the Scottish population reached an all-time high of 5.25 million people. The reasons given were that, in Scotland, births were outnumbering the number of deaths, and immigrants were moving to Scotland from overseas. In 2011, 43,700 people moved from Wales, Northern Ireland or England to live in Scotland.
The total fertility rate (TFR) in Scotland is below the replacement rate of 2.1 (the TFR was 1.73 in 2011). The majority of births today are to unmarried women (51.3% of births were outside of marriage in 2012).
Life expectancy for those born in Scotland between 2010 and 2012 is 76.5 years for males and 80.7 years for females. This is the lowest of any of the four countries of the UK.
Religion.
Just over half (54%) of the Scottish population reported being a Christian while nearly 37% reported not having a religion in a 2011 census.
Since the Scottish Reformation of 1560, the national church (the Church of Scotland, also known as The Kirk) has been Protestant and Reformed in theology. Since 1689 it has had a Presbyterian system of church government, and enjoys independence from the state. About 12% of the population are currently members of the Church of Scotland, with 40% claiming affinity. The Church operates a territorial parish structure, with every community in Scotland having a local congregation.
Scotland also has a significant Roman Catholic population, 19% claiming that faith, particularly in the west. After the Reformation, Roman Catholicism in Scotland continued in the Highlands and some western islands like Uist and Barra, and it was strengthened during the 19th century by immigration from Ireland. Other Christian denominations in Scotland include the Free Church of Scotland, various other Presbyterian offshoots, and the Scottish Episcopal Church.
Islam is the largest non-Christian religion (estimated at around 75,000, which is about 1.4% of the population), and there are also significant Jewish, Hindu and Sikh communities, especially in Glasgow. The Samyé Ling monastery near Eskdalemuir, which celebrated its 40th anniversary in 2007, is the first Buddhist monastery in western Europe.
Government.
Scotland's head of state is the monarch of the United Kingdom, currently Queen Elizabeth II (since 1952). The regnal numbering "Elizabeth II" caused controversy around the time of the Queen's coronation because there had never been an Elizabeth I in Scotland. A legal action, MacCormick v. Lord Advocate (1953 SC 396), was brought to contest the right of the Queen to entitle herself "Elizabeth II" within Scotland, arguing that this was a breach of Article 1 of the Treaty of Union. The Crown won the case. It was decided that future British monarchs would be numbered according to either their English or their Scottish predecessors, whichever number is higher. For instance any future King James would be styled James VIIIsince the last Scottish King James was James VII (also James II of England, etc.)while the next King Henry would be King Henry IX throughout the UK even though there have been no Scottish kings of that name.
Scotland has limited self-government within the United Kingdom, as well as representation in the UK Parliament. Executive and legislative powers respectively have been devolved to the Scottish Government and the Scottish Parliament at Holyrood in Edinburgh since 1999. The UK Parliament retains control over reserved matters specified in the Scotland Act 1998, including UK taxes, social security, defence, international relations and broadcasting. The Scottish Parliament has legislative authority for all other areas relating to Scotland, as well as a limited power to vary income tax.
The Scottish Parliament can give legislative consent over devolved matters back to the UK Parliament by passing a Legislative Consent Motion if United Kingdom-wide legislation is considered more appropriate for a certain issue. The programmes of legislation enacted by the Scottish Parliament have seen a divergence in the provision of public services compared to the rest of the UK. For instance, university education and care services for the elderly are free at point of use in Scotland, while fees are paid in the rest of the UK. Scotland was the first country in the UK to ban smoking in enclosed public places.
The Scottish Parliament is a unicameral legislature with 129 members (MSPs): 73 of them represent individual constituencies and are elected on a first past the post system; the other 56 are elected in eight different electoral regions by the additional member system. MSPs serve for a four-year period (exceptionally five years from 2011–16). The Parliament nominates one of its Members, who is then appointed by the Monarch to serve as First Minister. Other ministers are appointed by the First Minister and serve at his/her discretion. Together they make up the Scottish Government, the executive arm of the devolved government.
In the 2011 election, the Scottish National Party (SNP) formed a majority government after winning 69 seats out of 129. This was the first majority government since the modern post-devolution Scottish Parliament was established in 1999. The leader of the SNP, Alex Salmond, continued as First Minister until 2014. The Labour Party continued as the largest opposition party, with the Conservative Party, the Liberal Democrats, and the Green Party also represented in the Parliament. As of 29 September 2014, there are also three independent MSPs sitting in parliament. On 19 November 2014, Nicola Sturgeon became First Minister of Scotland, the first woman to hold the office. The next Scottish Parliament general election is due to be held on 5 May 2016.
Scotland is represented in the British House of Commons by 59 MPs elected from territory-based Scottish constituencies. In the most recent general election, held on 7 May 2015, the Scottish National Party won 56 of the 59 seats and saw elected the youngest current member of the House of Commons, Mhairi Black. The next United Kingdom general election is due to be held in May 2020. The Scotland Office represents the UK government in Scotland on reserved matters and represents Scottish interests within the UK government. The Scotland Office is led by the Secretary of State for Scotland, who sits in the Cabinet of the United Kingdom; the current incumbent is David Mundell.
Constitutional changes.
A policy of devolution had been advocated by the three main UK parties with varying enthusiasm during recent history. The late Labour leader John Smith described the revival of a Scottish parliament as the "settled will of the Scottish people". The devolved Scottish Parliament was created after a referendum in 1997 found majority support for both creating the Parliament and granting it limited powers to vary income tax. The constitutional status of Scotland is nonetheless subject to ongoing debate.
The Scottish National Party (SNP), which supports Scottish independence, was first elected to form the Scottish Government in 2007. The new government established a "National Conversation" on constitutional issues, proposing a number of options such as increasing the powers of the Scottish Parliament, federalism, or a referendum on Scottish independence from the United Kingdom. In rejecting the last option, the three main opposition parties in the Scottish Parliament created a commission to investigate the distribution of powers between devolved Scottish and UK-wide bodies. The Scotland Act 2012, based on proposals by the commission, is currently in the process of devolving additional powers to the Scottish Parliament.
In August 2009 the SNP proposed a bill to hold a referendum on independence in November 2010. Opposition from all other major parties led to an expected defeat. After the 2011 elections gave the SNP an overall majority in the Scottish Parliament, a referendum on independence for Scotland was held on 18 September 2014. The referendum rejected independence by a majority of 55% to 45%. During the campaign, the three main parties in the UK Parliament pledged to extend the powers of the Scottish Parliament; an all-party commission chaired by Lord Smith of Kelvin has been formed.
Administrative subdivisions.
Historical subdivisions of Scotland included the mormaerdom, stewartry, earldom, burgh, parish, county and regions and districts. Some of these names are still sometimes used as geographical descriptors.
Modern Scotland is subdivided in various ways depending on the purpose. In local government, there have been 32 single-tier council areas since 1996, whose councils are responsible for the provision of all local government services. Community councils are informal organisations that represent specific sub-divisions of a council area.
In the Scottish Parliament, there are 73 constituencies and eight regions. For the Parliament of the United Kingdom, there are 59 constituencies. Until 2013 the Scottish fire brigades and police forces were based on a system of regions introduced in 1975. For healthcare and postal districts, and a number of other governmental and non-governmental organisations such as the churches, there are other long-standing methods of subdividing Scotland for the purposes of administration.
City status in the United Kingdom is conferred by letters patent. There are seven cities in Scotland: Aberdeen, Dundee, Edinburgh, Glasgow, Inverness, Stirling and Perth.
Law and criminal justice.
Scots law has a basis derived from Roman law, combining features of both uncodified civil law, dating back to the "Corpus Juris Civilis", and common law with medieval sources. The terms of the Treaty of Union with England in 1707 guaranteed the continued existence of a separate legal system in Scotland from that of England and Wales. Prior to 1611, there were several regional law systems in Scotland, most notably Udal law in Orkney and Shetland, based on old Norse law. Various other systems derived from common Celtic or Brehon laws survived in the Highlands until the 1800s.
Scots law provides for three types of courts responsible for the administration of justice: civil, criminal and heraldic. The supreme civil court is the Court of Session, although civil appeals can be taken to the Supreme Court of the United Kingdom (or before 1 October 2009, the House of Lords). The High Court of Justiciary is the supreme criminal court in Scotland. The Court of Session is housed at Parliament House, in Edinburgh, which was the home of the pre-Union Parliament of Scotland with the High Court of Justiciary and the Supreme Court of Appeal currently located at the Lawnmarket. The sheriff court is the main criminal and civil court, hearing most cases. There are 49 sheriff courts throughout the country. District courts were introduced in 1975 for minor offences and small claims. These were gradually replaced by Justice of the Peace Courts from 2008 to 2010. The Court of the Lord Lyon regulates heraldry.
For many decades the Scots legal system was unique for being the only legal system without a parliament. This ended with the advent of the Scottish Parliament, which legislates for Scotland. Many features within the system have been preserved. Within criminal law, the Scots legal system is unique in having three possible verdicts: "guilty", "not guilty" and ""not proven"". Both "not guilty" and "not proven" result in an acquittal, typically with no possibility of retrial in accordance with the rule of double jeopardy. There is however the possibility of a retrial where new evidence emerges at a later date that might have proven conclusive in the earlier trial at first instance, where the person acquitted subsequently admits the offence or where it can be proved that the acquittal was tainted by an attempt to pervert the course of justice – see the provisions of the Double Jeopardy (Scotland) Act 2011. Many laws differ between Scotland and the other parts of the United Kingdom, and many terms differ for certain legal concepts. Manslaughter, in England and Wales, is broadly similar to culpable homicide in Scotland, and arson is called wilful fire raising. Indeed, some acts considered crimes in England and Wales, such as forgery, are not so in Scotland. Procedure also differs. Scots juries, sitting in criminal cases, consist of fifteen, rather than twelve jurors, as is more common in English-speaking countries.
The Scottish Prison Service (SPS) manages the prisons in Scotland, which collectively house over 8,500 prisoners. The Cabinet Secretary for Justice is responsible for the Scottish Prison Service within the Scottish Government.
Health care.
Healthcare in Scotland is mainly provided by NHS Scotland, Scotland's public health care system. This was founded by the National Health Service (Scotland) Act 1947 (later repealed by the National Health Service (Scotland) Act 1978) that took effect on 5 July 1948 to coincide with the launch of the NHS in England and Wales. However, even prior to 1948, half of Scotland's landmass was already covered by state funded health care, provided by the Highlands and Islands Medical Service. Healthcare policy and funding is the responsibility of the Scottish Government's Health Directorates. The current Cabinet Secretary for Health and Wellbeing is Alex Neil and the Director-General (DG) Health and chief executive, NHS Scotland is Paul Gray.
In 2008, the NHS in Scotland had around 158,000 staff including more than 47,500 nurses, midwives and health visitors and over 3,800 consultants. In addition, there are also more than 12,000 doctors, family practitioners and allied health professionals, including dentists, opticians and community pharmacists, who operate as independent contractors providing a range of services within the NHS in return for fees and allowances. These fees and allowances were removed in May 2010, and prescriptions are entirely free, although dentists and opticians may charge if the patient's household earns over a certain amount, about £30,000 per annum.
Economy.
Scotland has a western style open mixed economy closely linked with the rest of Europe and the wider world. Traditionally, the Scottish economy has been dominated by heavy industry underpinned by shipbuilding in Glasgow, coal mining and steel industries. Petroleum related industries associated with the extraction of North Sea oil have also been important employers from the 1970s, especially in the north east of Scotland.
In February 2012, the Centre for Economics and Business Research concluded that "Scotland receives no net subsidy" from the UK, as greater per capita tax generation in Scotland balanced out greater per capita public spending. More recent data, from 2012–13, show that Scotland generated 9.1% (£53.1bn; this included a geographical share of North Sea oil revenue – without it, the figures were 8.2% and £47.6bn) of the UK's tax revenues and received 9.3% (£65.2bn) of spending. Scotland's public spending deficit in 2012–13 was £12bn, a £3.5bn increase on the previous year; over the same period, the UK's deficit decreased by £2.6bn. Over the past thirty years, Scotland contributed a relative budget surplus of almost £20billion to the UK economy.
In the third quarter of 2015, the Scottish economy grew by 0.1%, below the 0.4% recorded for the UK. As of September 2015, the Scottish unemployment rate of 5.9% was above the UK rate of 5.5%, while the Scottish employment rate of 74.0% was higher than the UK figure of 73.5%.
De-industrialisation during the 1970s and 1980s saw a shift from a manufacturing focus towards a more service-oriented economy. 
Finance.
Edinburgh is the financial services centre of Scotland, with many large finance firms based there, including: Lloyds Banking Group (owners of HBOS); the Government owned Royal Bank of Scotland and Standard Life. Edinburgh was ranked 15th in the list of world financial centres in 2007, but fell to 37th in 2012, following damage to its reputation, and in 2015 was ranked 71st out of 84.
Exports.
In 2012, total Scottish exports (excluding intra-UK trade) were estimated to be £26 billion, of which 59% (£15.4 billion) were attributable to manufacturing. Scotland's primary exports include whisky, electronics and financial services. The United States, Netherlands, Germany, France and Norway constitute the country's major export markets. Scotland's Gross Domestic Product (GDP), including oil and gas produced in Scottish waters, was estimated at £150 billion for the calendar year 2012. If Scotland became independent, it would hold 95% of the UK's current oil and gas reserves if they were split geographically using a median line from the English-Scottish border. If the reserves were split by population, that figure would be reduced to 9%. 
Whisky.
Whisky is probably the best known of Scotland's manufactured products. Exports increased by 87% in the decade to 2012 and were valued at £4.3 billion in 2013, which was 85% of Scotland's food and drink exports. It supports around 10,000 jobs directly and 25,000 indirectly. It may contribute £400–682 million to Scotland, rather than several billion pounds, as more than 80% of whisky produced is owned by non-Scottish companies. 
Tourism.
A briefing published in 2002 by the Scottish Parliament Information Centre (SPICe) for the Scottish Parliament's Enterprise and Life Long Learning Committee stated that tourism accounted for up to 5% of GDP and 7.5% of employment.
Currency.
Although the Bank of England is the central bank for the UK, three Scottish clearing banks still issue their own Sterling banknotes: the Bank of Scotland; the Royal Bank of Scotland; and the Clydesdale Bank. The value of the Scottish banknotes in circulation in 2013 was £3.8 billion, underwritten by the Bank of England.
Military.
Of the money spent on UK defence, about £3.3 billion can be attributed to Scotland as of 2013. Although Scotland has a long military tradition predating the Treaty of Union with England, its armed forces now form part of the British Armed Forces, with the notable exception of the Atholl Highlanders, Europe's only legal private army. In 2006, the infantry regiments of the Scottish Division were amalgamated to form the Royal Regiment of Scotland. Other distinctively Scottish regiments in the British Army include the Scots Guards, the Royal Scots Dragoon Guards and the Scottish Transport Regiment, a Territorial Army Regiment of the Royal Logistic Corps.
Because of their topography and perceived remoteness, parts of Scotland have housed many sensitive defence establishments. Between 1960 and 1991, the Holy Loch was a base for the US fleet of Polaris ballistic missile submarines. Today, Her Majesty's Naval Base Clyde, north west of Glasgow, is the base for the four Trident-armed "Vanguard" class ballistic missile submarines that comprise the UK's nuclear deterrent. Scapa Flow was the major Fleet base for the Royal Navy until 1956.
A single front-line Royal Air Force base is located in Scotland. RAF Lossiemouth, located in Moray, is the most northerly air defence fighter base in the United Kingdom and is home to Typhoon and Tornado fast-jet squadrons 
The only open-air live depleted uranium weapons test range in the British Isles is located near Dundrennan. As a result, over 7000 potentially toxic munitions lie on the seabed of the Solway Firth.
Land ownership.
432 people own half of the land area of Scotland. Sixteen of them own 10% of the land.
Education.
The Scottish education system is distinct from the rest of the United Kingdom.
The "Curriculum for Excellence" provides the curricular framework for children and young people from age 3 to 18. All 3- and 4-year-old children in Scotland are entitled to a free nursery place. Formal primary education begins at approximately 5 years old and lasts for 7 years (P1–P7); today, children in Scotland study Standard Grades, or Intermediate qualifications between the ages of 14 and 16. These are being phased out and replaced by the National Qualifications of the Curriculum for Excellence. The school leaving age is 16, after which students may choose to remain at school and study for Access, Intermediate or Higher Grade and Advanced Higher qualifications. A small number of students at certain private, independent schools may follow the English system and study towards GCSEs and A and AS-Levels instead.
There are fifteen Scottish universities, some of which are amongst the oldest in the world. These include the University of St Andrews, the University of Glasgow, the University of Aberdeen, the University of Edinburgh and the University of Dundee —many of which are ranked amongst the best in the UK. Proportionally, Scotland has more universities in QS' World University Rankings' top 100 than any other nation in the world. The country produces 1% of the world's published research with less than 0.1% of the world's population, and higher education institutions account for 9% of Scotland's service sector exports. Scotland's University Courts are the only bodies in Scotland authorised to award degrees.
Scotland's Universities are complemented in the provision of Further and Higher Education by 43 Colleges. Colleges offer National Certificates, Higher National Certificates and Higher National Diplomas. These Group Awards, alongside Scottish Vocational Qualifications, aim to ensure Scotland's population has the appropriate skills and knowledge to meet workplace needs.
In 2014, research reported by the Office for National Statistics found that Scotland was the most highly educated country in Europe and among the most well-educated in the world in terms of tertiary education attainment, with roughly 40% of people in Scotland aged 16–64 educated to NVQ level 4 and above. Based on the original data for EU statistical regions, all four Scottish regions ranked significantly above the European average for completion of tertiary-level education by 25- to 64-year-olds.
Culture.
Scottish music is a significant aspect of the nation's culture, with both traditional and modern influences. A famous traditional Scottish instrument is the Great Highland Bagpipe, a wind instrument consisting of three drones and a melody pipe (called the chanter), which are fed continuously by a reservoir of air in a bag. Bagpipe bands, featuring bagpipes and various types of drums, and showcasing Scottish music styles while creating new ones, have spread throughout the world. The clàrsach (harp), fiddle and accordion are also traditional Scottish instruments, the latter two heavily featured in Scottish country dance bands. Today, there are many successful Scottish bands and individual artists in varying styles including Annie Lennox, Amy Macdonald, Runrig, Boards of Canada, Cocteau Twins, Deacon Blue, Franz Ferdinand, Susan Boyle, Emeli Sandé, Texas, The View, The Fratellis, Twin Atlantic and Biffy Clyro. Other Scottish musicians include Shirley Manson, Paolo Nutini and Calvin Harris.
Scotland has a literary heritage dating back to the early Middle Ages. The earliest extant literature composed in what is now Scotland was in Brythonic speech in the 6th century, but is preserved as part of Welsh literature. Later medieval literature included works in Latin, Gaelic, Old English and French. The first surviving major text in Early Scots is the 14th-century poet John Barbour's epic "Brus", focusing on the life of Robert I, and was soon followed by a series of vernacular romances and prose works. In the 16th century the crown's patronage helped the development of Scots drama and poetry, but the accession of James VI to the English throne removed a major centre of literary patronage and Scots was sidelined as a literary language. Interest in Scots literature was revived in the 18th century by figures including James Macpherson, whose Ossian Cycle made him the first Scottish poet to gain an international reputation and was a major influence on the European Enlightenment. It was also a major influence on Robert Burns, whom many consider the national poet, and Walter Scott, whose Waverley Novels did much to define Scottish identity in the 19th century. Towards the end of the Victorian era a number of Scottish-born authors achieved international reputations as writers in English, including Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald. In the 20th century the Scottish Renaissance saw a surge of literary activity and attempts to reclaim the Scots language as a medium for serious literature. Members of the movement were followed by a new generation of post-war poets including Edwin Morgan, who would be appointed the first Scots Makar by the inaugural Scottish government in 2004. From the 1980s Scottish literature enjoyed another major revival, particularly associated with a group of writers including Irvine Welsh. Scottish poets who emerged in the same period included Carol Ann Duffy, who, in May 2009, was the first Scot named UK Poet Laureate.
As one of the Celtic nations, Scotland and Scottish culture is represented at interceltic events at home and over the world. Scotland hosts several music festivals including Celtic Connections (Glasgow), and the Hebridean Celtic Festival (Stornoway). Festivals celebrating Celtic culture, such as Festival Interceltique de Lorient (Brittany), the Pan Celtic Festival (Ireland), and the National Celtic Festival (Portarlington, Australia), feature elements of Scottish culture such as language, music and dance.
National symbols.
The image of St. Andrew, martyred while bound to an X-shaped cross, first appeared in the Kingdom of Scotland during the reign of William I. Following the death of King Alexander III in 1286 an image of Andrew was used on the seal of the Guardians of Scotland who assumed control of the kingdom during the subsequent interregnum. Use of a simplified symbol associated with Saint Andrew, the saltire, has its origins in the late 14th century; the Parliament of Scotland decreeing in 1385 that Scottish soldiers should wear a white Saint Andrew's Cross on the front and back of their tunics. Use of a blue background for the Saint Andrew's Cross is said to date from at least the 15th century. Since 1606 the saltire has also formed part of the design of the Union Flag. There are numerous other symbols and symbolic artefacts, both official and unofficial, including the thistle, the nation's floral emblem (celebrated in the song, The Thistle o' Scotland), the Declaration of Arbroath, incorporating a statement of political independence made on 6 April 1320, the textile pattern tartan that often signifies a particular Scottish clan and the royal Lion Rampant flag. Highlanders can thank James Graham, 3rd Duke of Montrose, for the repeal in 1782 of the Act of 1747 prohibiting the wearing of tartans.
Although there is no official national anthem of Scotland, "Flower of Scotland" is played on special occasions and sporting events such as football and rugby matches involving the Scotland national teams and since 2010 is also played at the Commonwealth Games after it was voted the overwhelming favourite by participating Scottish athletes. Other currently less popular candidates for the National Anthem of Scotland include "Scotland the Brave", "Highland Cathedral", "Scots Wha Hae" and "A Man's A Man for A' That".
St Andrew's Day, 30 November, is the national day, although Burns' Night tends to be more widely observed, particularly outside Scotland. In 2006, the Scottish Parliament passed the St. Andrew's Day Bank Holiday (Scotland) Act 2007, designating the day an official bank holiday.Tartan Day is a recent innovation from Canada.
The national animal of Scotland is the unicorn, which has been a Scottish heraldic symbol since the 12th century.
Media.
Newspapers.
National newspapers such as the "Daily Record", "The Herald", and "The Scotsman" are all produced in Scotland. Important regional dailies include the Evening News in Edinburgh "The Courier" in Dundee in the east, and "The Press and Journal" serving Aberdeen and the north. Scotland is represented at the Celtic Media Festival, which showcases film and television from the Celtic countries. Scottish entrants have won many awards since the festival began in 1980.
Television.
Television in Scotland is largely the same as UK-wide broadcasts, however the national broadcaster is BBC Scotland, a constituent part of the British Broadcasting Corporation, the publicly funded broadcaster of the United Kingdom. It runs three national television stations, and the national radio stations, "BBC Radio Scotland" and "BBC Radio nan Gaidheal", amongst others. Scotland also has some programming in the Gaelic language. BBC Alba is the national Gaelic-language channel. The main Scottish commercial television station is STV.
Sport.
Scotland hosts its own national sporting competitions and has independent representation at several international sporting events, including the FIFA World Cup, the Rugby Union World Cup, the Rugby League World Cup, the Cricket World Cup and the Commonwealth Games. Scotland has its own national governing bodies, such as the Scottish Football Association (the second oldest national football association in the world) and the Scottish Rugby Union. Variations of football have been played in Scotland for centuries, with the earliest reference dating back to 1424. Association football is the most popular sport and the Scottish Cup is the world's oldest national trophy.
Scotland contested the first ever international football game in 1872 against England. The match took place at Hamilton Crescent, Glasgow, home of the West of Scotland Cricket Club. Scottish clubs have been successful in European competitions with Celtic winning the European Cup in 1967, Rangers and Aberdeen winning the UEFA Cup Winners' Cup in 1972 and 1983 respectively, and Aberdeen also winning the UEFA Super Cup in 1983. Dundee United have also made it to a European final, reaching the UEFA Cup Final in 1987, but losing on aggregate 2-1 to IFK Göteborg.
With the modern game of golf originating in 15th century Scotland, the country is promoted as the home of golf. To many golfers the Old Course in the Fife town of St. Andrews, an ancient links course dating to before 1574, is considered a site of pilgrimage. In 1764, the standard 18-hole golf course was created at St Andrews when members modified the course from 22 to 18 holes. The world's oldest golf tournament, and golf's first major, is The Open Championship, which was first played on 17 October 1860 at Prestwick Golf Club, in Ayrshire, Scotland, with Scottish golfers winning the earliest majors. There are many other famous golf courses in Scotland, including Carnoustie, Gleneagles, Muirfield, and Royal Troon. Other distinctive features of the national sporting culture include the Highland games, curling and shinty. In boxing, Scotland has had 13 world champions, including Ken Buchanan, Benny Lynch and Jim Watt.
Scotland has competed at every Commonwealth Games since 1930 and has won 356 medals in total—91 Gold, 104 Silver and 161 Bronze. Edinburgh played host to the Commonwealth Games in 1970 and 1986, and most recently Glasgow in 2014.
Infrastructure.
Transport.
Scotland has five main international airports (Glasgow, Edinburgh, Aberdeen, Prestwick and Inverness), which together serve 150 international destinations with a wide variety of scheduled and chartered flights. GIP operates Edinburgh airport and BAA operates (Aberdeen and Glasgow International), while Highland and Islands Airports operates 11 regional airports, including Inverness, which serve the more remote locations. Infratil operates Prestwick.
The Scottish motorways and major trunk roads are managed by Transport Scotland. The remainder of the road network is managed by the Scottish local authorities in each of their areas. 
Water transport.
Regular ferry services operate between the Scottish mainland and many islands. These ferries are mostly run by Caledonian MacBrayne, but some are operated by local councils. Other ferry routes, served by multiple companies, connect to Northern Ireland, Belgium, Norway, the Faroe Islands and also Iceland. 
Rail.
Network Rail Infrastructure Limited owns and operates the fixed infrastructure assets of the railway system in Scotland, while the Scottish Government retains overall responsibility for rail strategy and funding in Scotland. Scotland's rail network has around 340 railway stations and of track. Over 62 million passenger journeys are made each year.
The East Coast and West Coast main railway lines connect the major cities and towns of Scotland with each other and with the rail network in England. Domestic rail services within Scotland are operated by ScotRail. During the time of British Rail the West Coast Main Line from London Euston to Glasgow Central was electrified in the early 1970s, followed by the East Coast Main Line in the late 1980s. British Rail created the ScotRail brand. When British Rail existed, many railway lines in Strathclyde were electrified. Strathclyde Passenger Transport Executive was at the forefront with the acclaimed "largest electrified rail network outside London". Some parts of the network are electrified, but there are no electrified lines in the Highlands, Angus, Aberdeenshire, the cities of Dundee or Aberdeen, or Perth & Kinross, and none of the islands has a rail link (although the railheads at Kyle of Lochalsh and Mallaig principally serve the islands).
The East Coast Main Line crosses the Firth of Forth by the Forth Bridge. Completed in 1890, this cantilever bridge has been described as "the one internationally recognised Scottish landmark".
Airports.
Scotland's rail network is managed by Transport Scotland. 
Subway.
In addition, Glasgow has had a small integrated subway system since 1896. Completely gutted and modernised between 1977 and 1980, its 15 stations serve just under 40,000 passengers per day. There are plans to extensively refurbish the system in time for the 2014 Commonwealth Games.

</doc>
<doc id="26995" url="https://en.wikipedia.org/wiki?curid=26995" title="Shire">
Shire

A shire is a traditional term for a division of land, found in the United Kingdom and Australia. The word derives from the Old English "scir", itself a derivative of the Proto-Germanic "skizo" (cf. Old High German "scira"), meaning care or official charge.
In Britain, "shire" is the original term for what is usually known now as a "county"; the word "county" having been introduced at the Norman Conquest of England. The two are nearly synonymous. Although in modern British usage counties are referred to as "shires" mainly in poetic contexts, terms such as Shire Hall remain common. Shire also remains a common part of many county names.
In some rural parts of Australia, a shire is a local government area; however, in Australia it is not synonymous with a "county", which is a lands administrative division. Individually, or as a suffix in Scotland and in the far northeast of England, the word is pronounced . As a suffix in an English or Welsh place name, it is in most regions pronounced , or sometimes . (In south-east England the is dropped in accordance with normal regional phonology, so that (for instance) "Berkshire" becomes .)
Origins.
The system was first used in Wessex from the beginning of Anglo-Saxon settlement, and spread to most of the rest of England in the tenth century, along with West Saxon political control. In Domesday (1086) the city of York was divided into shires. The first shires of Scotland were created in English-settled areas such as Lothian and the Borders, in the ninth century. King David I more consistently created shires and appointed sheriffs across lowland "shores" of Scotland.
The shire in early days was governed by an "Ealdorman" and in the later Anglo-Saxon period by royal official known as a "shire reeve" or sheriff. The shires were divided into hundreds or wapentakes, although other less common sub-divisions existed. An alternative name for a shire was a "sheriffdom" until sheriff court reforms separated the two concepts. In Scotland the word "county" was not adopted for the shires. Although "county" appears in some texts, "shire" was the normal name until counties for statutory purposes were created in the nineteenth century.
Shire county.
The phrase "shire county" applies, unofficially, to non-metropolitan counties in England, specifically those that are not unitary local authority areas.
Shire names in the United Kingdom.
"Shire" also refers, in a narrower sense, to ancient counties with names that ended in "shire". These counties are typically (though not always) named after their county town.
The suffix "-shire" is attached to most of the names of English, Scottish and Welsh counties. It tends not to be found in the names of shires that were pre-existing divisions. Essex, Kent, and Sussex, for example, have never borne a "-shire", as each represents a former Anglo-Saxon kingdom. Similarly Cornwall was a British kingdom before it became an English county. The term 'shire' is not used in the names of the six traditional counties of Northern Ireland.
Shire names in England.
Shires in England bearing the "-shire" suffix include:
Bedfordshire, Berkshire, Buckinghamshire, Cambridgeshire, Cheshire, Derbyshire, Gloucestershire, Hampshire, Herefordshire, Hertfordshire, Huntingdonshire, Lancashire, Lincolnshire, Leicestershire, Northamptonshire, Nottinghamshire, Oxfordshire, Shropshire, Staffordshire, Warwickshire, Wiltshire, Worcestershire, and Yorkshire. These counties, on their historical boundaries, cover a little more than half the area of England. The counties that do not use "-shire" are mainly in three areas, in the south-east, south-west and far north of England.
Yorkshire no longer exists as a unit of local government in England. Some of the successor modern counties formed by its breakup (North Yorkshire, South Yorkshire and West Yorkshire) however retain the old county name as a component. (East Yorkshire has a more complicated signification.)
The county of Devon is also known as Devonshire, although this is not an official name and is not often used outside the county. The counties of Dorset, Rutland and Somerset were occasionally Dorsetshire, Rutlandshire and Somersetshire, but these usages are now considered archaic.
Hexhamshire was a county in the north-east of England from the early 12th century until 1572, when it was incorporated into Northumberland.
Shire names in Scotland.
In Scotland, barely affected by the Norman conquest of England, the word "shire" prevailed over "county" until the 19th century. Earliest sources have the same usage of the "-shire" suffix as in England (though in Scots this was oftenmost "schyr"). Later the "Shire" appears as a separate word.
"Shire" names in Scotland include Aberdeenshire, Ayrshire, Banffshire, Berwickshire, Clackmannanshire, Cromartyshire, Dumfriesshire, Dunbartonshire, Inverness-shire, Kincardineshire, Kinross-shire, Kirkcudbrightshire, Lanarkshire, Morayshire, Nairnshire, Peeblesshire, Perthshire, Renfrewshire, Ross-shire, Roxburghshire, Selkirkshire, Stirlingshire, and Wigtownshire. 
In Scotland four shires have alternative names with the "-shire" suffix: Angus (Forfarshire), East Lothian (Haddingtonshire), Midlothian (Edinburghshire) and West Lothian (Linlithgowshire).
Sutherland is occasionally still referred to as Sutherlandshire. Similarly, Argyllshire, Buteshire, Caithness-shire and Fifeshire are sometimes found. Also, Morayshire was previously called Elginshire. There is currently much debate about whether Argyllshire was ever really used.
Shire names in Wales.
Shires in Wales bearing the "-shire" suffix include:
Brecknockshire (or Breconshire), Caernarfonshire, Cardiganshire, Carmarthenshire, Denbighshire, Flintshire, Monmouthshire, Montgomeryshire, Pembrokeshire, and Radnorshire.
In Wales, the counties of Merioneth and Glamorgan are occasionally referred to with the "shire" suffix. The only traditional Welsh county that never takes "shire" is Anglesey—in English: in Welsh it is referred to as 'Sir Fon'.
Non-county "shires".
The suffix –"shire" could be a generalised term referring to a district. It did not acquire the strong association with county until later.
Other than these, the term was used for several other districts. Bedlingtonshire, Craikshire, Norhamshire and Islandshire were exclaves of County Durham, which were incorporated into Northumberland or Yorkshire in 1844. The suffix was also used for many hundreds, wapentakes and liberties such as Allertonshire, Blackburnshire, Halfshire, Howdenshire, Leylandshire, Powdershire, Pydarshire, Richmondshire, Riponshire, Salfordshire, Triggshire, Tynemouthshire, West Derbyshire and Wivelshire, counties corporate such as Hullshire, and other districts such as Applebyshire, Bamburghshire, Bunkleshire, Carlisleshire, Coldinghamshire, Coxwoldshire, Cravenshire, Hallamshire, Mashamshire and Yetholmshire.
Non-county shires were very common in Scotland. Kinross-shire and Clackmannanshire are arguably survivals from such districts. Non-county "shires" in Scotland include Bunkleshire, Coldinghamshire and Yetholmshire.
Richmondshire is today the name of a local government district of North Yorkshire.
Shires in Australia.
"Shire" is the most common word in Australia for rural local government areas (LGAs). The states of New South Wales, Victoria, Queensland and Western Australia use the term "Shire" for this unit.
In contrast, South Australia uses district and region for its rural LGA units, while Tasmania uses municipality.
Shires are generally functionally indistinguishable from towns, borough, municipalities, or cities.
Three LGAs in outer metropolitan Sydney and four in outer metropolitan Melbourne have populations exceeding that of towns or municipalities, but retain significant bushlands and/or semi-rural areas, and most have continued to use "Shire" in their titles—possibly due to community demand or popularity; or for financial and socio-political gain—whilst others have dropped "Shire" from their titles. These "city-shires" are:
Sydney:
Melbourne:
Shires in the United States.
In 1634, eight "shires" were created in the Virginia Colony by order of Charles I, King of England. They were renamed as counties only a few years later. They were:
As of 2013 six of the original eight Shires of Virginia are considered to be still extant whilst two have consolidated with a neighbouring city. Most of their boundaries have changed in the intervening centuries.
Before the Province of New York was granted county subdivisions and a greater royal presence in 1683, the early ducal colony consisted of York Shire, as well as Albany and Ulster, after the three titles held by Prince James: Duke of York, Duke of Albany, Earl of Ulster. While these were basically renamed Dutch core settlements, they were quickly converted to English purposes, while the Dutch remained within the colony, as opposed to later practice of the Acadian Expulsion. Further Anglo-Dutch synthesis occurred when James enacted the Dominion of New England and later when William III of England took over through the Glorious Revolution.
A few New England states and commonwealths (namely Vermont, Massachusetts, and Maine), still use the term "shire town" for their county seats, although they use the term county, rather than shire.

</doc>
<doc id="26997" url="https://en.wikipedia.org/wiki?curid=26997" title="Scientist">
Scientist

A scientist is a person engaging in a systematic activity to acquire knowledge that describes and predicts the natural world. In a more restricted sense, a scientist may refer to an individual who uses the scientific method. The person may be an expert in one or more areas of science. This article focuses on the more restricted use of the word. Scientists perform research toward a more comprehensive understanding of nature, including physical, mathematical and social realms.
Philosophy is a distinct activity that is not generally considered science. Philosophers aim to provide a comprehensive understanding of intangible aspects of reality and experience that cannot be physically measured.
Scientists are also distinct from engineers, those who design, build, and maintain devices for particular situations; however, no engineer attains that title without significant study of science and the scientific method. When science is done with a goal toward practical utility, it is called applied science. An applied scientist may not be designing something in particular, but rather is conducting research with the aim of developing new technologies and practical methods. When science is done with an inclusion of intangible aspects of reality it is called "natural philosophy".
Description.
Science and technology have continually modified human existence through the engineering process. As a profession the scientist of today is widely recognized. Scientists include theoreticians who mainly develop new models to explain existing data and predict new results, and experimentalists who mainly test models by making measurements — though in practice the division between these activities is not clear-cut, and many scientists perform both tasks.
Jurisprudence and mathematics are often grouped with the sciences. Some of the greatest physicists have also been creative mathematicians and lawyers. There is a continuum from the most theoretical to the most empirical scientists with no distinct boundaries. In terms of personality, interests, training and professional activity, there is little difference between applied mathematicians and theoretical physicists.
Scientists can be motivated in several ways. Many have a desire to understand why the world is as we see it and how it came to be. They exhibit a strong curiosity about reality. Other motivations are recognition by their peers and prestige, or the desire to apply scientific knowledge for the benefit of people's health, the nations, the world, nature or industries (academic scientist and industrial scientist). Scientists tend to be less motivated by direct financial reward for their work than other careers. As a result, scientific researchers often accept lower average salaries when compared with many other professions which require a similar amount of training and qualification.
Demography.
The number of scientists is vastly different from country to country. For instance, there are only 4 full-time scientists per 10,000 workers in India while this number is 79 for the United Kingdom and the United States. 
Historical development and etymology of the term.
Until the late 19th or early 20th century, scientists were called "natural philosophers" or "men of science".
English philosopher and historian of science William Whewell coined the term "scientist" in 1833, and it was first published in Whewell's anonymous 1834 review of Mary Somerville's "On the Connexion of the Physical Sciences" published in the "Quarterly Review". Whewell's suggestion of the term was partly satirical, a response to changing conceptions of science itself in which natural knowledge was increasingly seen as distinct from other forms of knowledge. Whewell wrote of "an increasing proclivity of separation and dismemberment" in the sciences; while highly specific terms proliferated—chemist, mathematician, naturalist—the broad term "philosopher" was no longer satisfactory to group together those who pursued science, without the caveats of "natural" or "experimental" philosopher. Members of the British Association for the Advancement of Science had been complaining about the lack of a good term at recent meetings, Whewell reported in his review; alluding to himself, he noted that "some ingenious gentleman proposed that, by analogy with "artist", they might form word "scientist", and added that there could be no scruple in making free with this term since we already have such words as "economist", and "atheist"—but this was not generally palatable". Scientists are the people who ask a question about a phenomenon and proceed to systematically go about answering the question themselves. They are by nature curious, creative and well organized. They need to have the ability to observe something and see in it some of the properties other people overlook.
Whewell proposed the word again more seriously (and not anonymously) in his 1840 "The Philosophy of the Inductive Sciences":
He also proposed the term "physicist" at the same time, as a counterpart to the French word "physicien". Neither term gained wide acceptance until decades later; "scientist" became a common term in the late 19th century in the United States and around the turn of the 20th century in Great Britain. By the twentieth century, the modern notion of science as a special brand of information about the world, practiced by a distinct group and pursued through a unique method, was essentially in place.
The social roles of "scientists", and their predecessors before the emergence of modern scientific disciplines, have evolved considerably over time. Scientists of different eras (and before them, natural philosophers, mathematicians, natural historians, natural theologians, engineers, and other who contributed to the development of science) have had widely different places in society, and the social norms, ethical values, and epistemic virtues associated with scientists—and expected of them—have changed over time as well. Accordingly, many different historical figures can be identified as early scientists, depending on which elements of modern science are taken to be essential.
Some historians point to the 17th century as the period when science in a recognizably modern form developed (what is popularly called the Scientific Revolution). It wasn't until the 19th century that sufficient socioeconomic changes occurred for scientists to emerge as a major profession.
Ancient and medieval science.
Knowledge about nature in Classical Antiquity was pursued by many kinds of scholars. Greek contributions to science—including works of geometry and mathematical astronomy, early accounts of biological processes and catalogs of plants and animals, and theories of knowledge and learning—were produced by philosophers and physicians, as well as practitioners of various trades. These roles, and their associations with scientific knowledge, spread with the Roman Empire and, with the spread of Christianity, became closely linked to religious institutions in most of European countries. Astrology and astronomy became an important area of knowledge, and the role of astronomer/astrologer developed with the support of political and religious patronage. By the time of the medieval university system, knowledge was divided into the "trivium"—philosophy, including natural philosophy—and the "quadrivium"—mathematics, including astronomy. Hence, the medieval analogs of scientists were often either philosophers or mathematicians. Knowledge of plants and animals was broadly the province of physicians.
Science in medieval Islam generated some new modes of developing natural knowledge, although still within the bounds of existing social roles such as philosopher and mathematician. Many proto-scientists from the Islamic Golden Age are considered polymaths, in part because of the lack of anything corresponding to modern scientific disciplines. Many of these early polymaths were also religious priests and theologians: for example, Alhazen and al-Biruni were mutakallimiin; the physician Avicenna was a hafiz; the physician Ibn al-Nafis was a hafiz, muhaddith and ulema; the botanist Otto Brunfels was a theologian and historian of Protestantism; the astronomer and physician Nicolaus Copernicus was a priest. During the Italian Renaissance scientists like Leonardo Da Vinci, Michelangelo, Galileo Galilei and Gerolamo Cardano have been considered as the most recognizable polymaths.
Historical scientists.
During the Renaissance, Italians made substantial contributions in science. Leonardo Da Vinci made significant discoveries in paleontology and anatomy. The Father of modern Science,
Galileo Galilei, made key improvements on the thermometer and telescope which allowed him to observe and clearly describe the solar system. Descartes was not only a pioneer of analytic geometry but formulated a theory of mechanics and advanced ideas about the origins of animal movement and perception. Vision interested the physicists Young and Helmholtz, who also studied optics, hearing and music. Newton extended Descartes' mathematics by inventing calculus (contemporaneously with Leibniz). He provided a comprehensive formulation of classical mechanics and investigated light and optics. Fourier founded a new branch of mathematics — infinite, periodic series — studied heat flow and infrared radiation, and discovered the greenhouse effect. Girolamo Cardano, Blaise Pascal Pierre de Fermat, Von Neumann, Turing, Khinchin, Markov and Wiener, all mathematicians, made major contributions to science and probability theory, including the ideas behind computers, and some of the foundations of statistical mechanics and quantum mechanics. Many mathematically inclined scientists, including Galileo, were also musicians.
Luigi Galvani, the pioneer of the bioelectromagnetics, discovered the animal electricity. He discovered that a charge applied to the spinal cord of a frog could generate muscular spasms throughout its body. Charges could make frog legs jump even if the legs were no longer attached to a frog. While cutting a frog leg, Galvani's steel scalpel touched a brass hook that was holding the leg in place. The leg twitched. Further experiments confirmed this effect, and Galvani was convinced that he was seeing the effects of what he called animal electricity, the life force within the muscles of the frog. At the University of Pavia, Galvani's colleague Alessandro Volta was able to reproduce the results, but was sceptical of Galvani's explanation.
During the age of Enlightenment, Francesco Redi, discovered that microorganisms can cause disease. This was later explained by Louis Pasteur. There are many compelling stories in medicine and biology, such as the development of ideas about the circulation of blood from Galen to Harvey. The flowering of genetics and molecular biology in the 20th century is replete with famous names. Ramón y Cajal won the Nobel Prize in 1906 for his remarkable observations in neuroanatomy.
Lazzaro Spallanzani is one of the most influential figures in experimental physiology and the natural sciences. His investigations have exerted a lasting influence on the medical sciences. He made important contributions to the experimental study of bodily functions and animal reproduction.
Some see a dichotomy between experimental sciences and purely "observational" sciences such as astronomy, meteorology, oceanography and seismology. But astronomers have done basic research in optics, developed charge-coupled devices, and in recent decades have sent space probes to study other planets in addition to using the Hubble Telescope to probe the origins of the Universe some 14 billion years ago. Microwave spectroscopy has now identified dozens of organic molecules in interstellar space, requiring laboratory experimentation and computer simulation to confirm the observational data and starting a new branch of chemistry. Computer modeling and numerical methods are techniques required of students in every field of quantitative science.
Women in science.
The percent of women entering into science are usually intertwined with engineering stats but the combination of the percentages shows the low numbers that are involved. The number of science and engineering doctorates awarded to women rose from a mere 7 percent in 1970 to 34 percent in 1985 and in engineering alone the numbers of bachelor's degrees awarded to women rose from only 385 in 1975 to more than 11000 in 1985.
The inequality prevails into the professional setting in ways such as starting position inequality and income inequality. According to Eisenhart and Finkel, women's experiences, even when they have equal qualifications, are that they start in lower positions while men are granted tenure track positions. This later predicts a gender inequality of tenured positions as scientists in universities, "as of 1989, 65 percent of men and only 40 percent of women held tenured positions." Income conflicts occur when median annual salaries for full-time employed civilian scientists are compared, "salary for men is $48,000, and that for women is $42,000."
Types of scientists.
Those considering science as a career often look to the frontiers. These include cosmology and biology, especially molecular biology and the human genome project. Other areas of active research include the exploration of matter at the scale of elementary particles as described by high-energy physics, and materials science, which seeks to discover and design new materials. Although there have been remarkable discoveries with regard to brain function and neurotransmitters, the nature of the mind and human thought still remains unknown.

</doc>
<doc id="27000" url="https://en.wikipedia.org/wiki?curid=27000" title="Smog">
Smog

Smog is a type of air pollutant. The word "smog" was coined in the early 20th century as a portmanteau of the words smoke and fog to refer to smoky fog. The word was then intended to refer to what was sometimes known as pea soup fog, a familiar and serious problem in London from the 19th century to the mid 20th century. This kind of visible air pollution is composed of nitrogen oxides, sulfur oxides, ozone, smoke or particulates among others (less visible pollutants include carbon monoxide, CFCs and radioactive sources). Man-made smog is derived from coal emissions, vehicular emissions, industrial emissions, forest and agricultural fires and photochemical reactions of these emissions.
Modern smog, as found for example in Los Angeles, is a type of air pollution derived from vehicular emission from internal combustion engines and industrial fumes that react in the atmosphere with sunlight to form secondary pollutants that also combine with the primary emissions to form photochemical smog. In certain other cities, such as Delhi, smog severity is often aggravated by stubble burning in neighboring agricultural areas. The atmospheric pollution levels of Los Angeles, Beijing, Delhi, Mexico City, Tehran and other cities are increased by inversion that traps pollution close to the ground. It is usually highly toxic to humans and can cause severe sickness, shortened life or death.
Etymology.
Coinage of the term "smog" is generally attributed to Dr. Henry Antoine Des Voeux in his 1905 paper, "Fog and Smoke" for a meeting of the Public Health Congress. The July 26, 1905 edition of the London newspaper "Daily Graphic" quoted Des Voeux, "He said it required no science to see that there was something produced in great cities which was not found in the country, and that was smoky fog, or what was known as 'smog.'" The following day the newspaper stated that "Dr. Des Voeux did a public service in coining a new word for the London fog." However, this is predated by a "Los Angeles Times" article of January 19, 1893, in which the word is attributed to "a witty English writer."
Causes.
Coal.
Coal fires, used to heat individual buildings or in a power-producing plant, can emit significant clouds of smoke that contributes to smog. Air pollution from this source has been reported in England since the Middle Ages. London, in particular, was notorious up through the mid-20th century for its coal-caused smogs, which were nicknamed 'pea-soupers.' Air pollution of this type is still a problem in areas that generate significant smoke from burning coal, as witnessed by the 2013 autumnal smog in Harbin, China, which closed roads, schools, and the airport.
Transportation emissions.
Traffic emissions – such as from trucks, buses, and automobiles – also contribute. Airborne by-products from vehicle exhaust systems cause air pollution and are a major ingredient in the creation of smog in some large cities.
The major culprits from transportation sources are carbon monoxide (CO), nitrogen oxides (NO and NOx), volatile organic compounds, sulfur dioxide, and hydrocarbons. (Hydrocarbons are the main components of petroleum fuels such as gasoline and diesel fuel.) These molecules react with sunlight, heat, ammonia, moisture, and other compounds to form the noxious vapors, ground level ozone, and particles that comprise smog.
Photochemical smog.
Photochemical smog is the chemical reaction of sunlight, nitrogen oxides and volatile organic compounds in the atmosphere, which leaves airborne particles and ground-level ozone. This noxious mixture of air pollutants may include the following:
A primary pollutant is an air pollutant emitted directly from a source.
A secondary pollutant is not directly emitted as such, but forms when other pollutants (primary pollutants) react in the atmosphere.
Examples of a secondary pollutant include ozone, which is formed when hydrocarbons (HC) and nitrogen oxides (NOx) combine in the presence of sunlight; nitrogen dioxide (NO2), which is formed as nitric oxide (NO) combines with oxygen in the air; and acid rain, which is formed when sulfur dioxide or nitrogen oxides react with water.
All of these harsh chemicals are usually highly reactive and oxidizing. Photochemical smog is therefore considered to be a problem of modern industrialization. It is present in all modern cities, but it is more common in cities with sunny, warm, dry climates and a large number of motor vehicles. Because it travels with the wind, it can affect sparsely populated areas as well.
The link between automotive exhaust and photochemical smog was discovered in the 1950s by Arie Haagen-Smit.
Natural causes.
An erupting volcano can also emit high levels of sulphur dioxide along with a large quantity of particulate matter; two key components to the creation of smog. However, the smog created as a result of a volcanic eruption is often known as vog to distinguish it as a natural occurrence.
The radiocarbon content of some plant life has been linked to the distribution of smog in some areas. For example, the creosote bush in the Los Angeles area has been shown to have an effect on smog distribution that is more than fossil fuel combustion alone.
Health effects.
Smog is a serious problem in many cities and continues to harm human health. Ground-level ozone, sulfur dioxide, nitrogen dioxide and carbon monoxide are especially harmful for senior citizens, children, and people with heart and lung conditions such as emphysema, bronchitis, and asthma. It can inflame breathing passages, decrease the lungs' working capacity, cause shortness of breath, pain when inhaling deeply, wheezing, and coughing. It can cause eye and nose irritation and it dries out the protective membranes of the nose and throat and interferes with the body's ability to fight infection, increasing susceptibility to illness. Hospital admissions and respiratory deaths often increase during periods when ozone levels are high.
Levels of unhealthy exposure.
The U.S. EPA has developed an Air Quality Index to help explain air pollution levels to the general public. 8 hour average ozone concentrations of 85 to 104 ppbv are described as "Unhealthy for Sensitive Groups", 105 ppbv to 124 ppbv as "unhealthy" and 125 ppb to 404 ppb as "very unhealthy". The "very unhealthy" range for some other pollutants are: 355 μg m−3 - 424 μg m−3 for PM10; 15.5 ppm - 30.4ppm for CO and 0.65 ppm - 1.24 ppm for NO2.
Premature deaths due to cancer and respiratory disease.
The Ontario Medical Association announced that smog is responsible for an estimated 9,500 premature deaths in the province each year.
A 20-year American Cancer Society study found that cumulative exposure also increases the likelihood of premature death from a respiratory disease, implying the 8-hour standard may be insufficient.
Smog and the risk of certain birth defects.
A study examining 806 women who had babies with birth defects between 1997 and 2006, and 849 women who had healthy babies, found that smog in the San Joaquin Valley area of California was linked to two types of neural tube defects: spina bifida (a condition involving, among other manifestations, certain malformations of the spinal column), and anencephaly (the underdevelopment or absence of part or all of the brain, which if not fatal usually results in profound impairment).
Smog and low birth weight.
According to a study published in The Lancet, even a very small (5 μg) change in PM2.5 exposure was associated with an increase (18%) in risk of a low birth weight at delivery, and this relationship held even below the current accepted safe levels.
Areas affected.
Smog can form in almost any climate where industries or cities release large amounts of air pollution, such as smoke or gases. However, it is worse during periods of warmer, sunnier weather when the upper air is warm enough to inhibit vertical circulation. It is especially prevalent in geologic basins encircled by hills or mountains. It often stays for an extended period of time over densely populated cities or urban areas, and can build up to dangerous levels.
Delhi, India.
Delhi is the most polluted city in the world and according to one estimate, air pollution causes the death of about 10,500 people in Delhi every year. During 2013-14, peak levels of fine particulate matter (PM) in Delhi increased by about 44%, primarily due to high vehicular and industrial emissions, construction work and crop burning in adjoining states. Delhi has the highest level of the airborne particulate matter, PM2.5 considered most harmful to health, with 153 micrograms. Rising air pollution level has significantly increased lung-related ailments (especially asthma and lung cancer) among Delhi's children and women. The dense smog in Delhi during winter season results in major air and rail traffic disruptions every year. According to Indian meteorologists, the average maximum temperature in Delhi during winters has declined notably since 1998 due to rising air pollution.
Environmentalists have criticised the Delhi government for not doing enough to curb air pollution and to inform people about air quality issues. Most of Delhi's residents are unaware of alarming levels of air pollution in the city and the health risks associated with it. Since the mid-1990s, Delhi has undertaken some measures to curb air pollution – Delhi has the third highest quantity of trees among Indian cities and the Delhi Transport Corporation operates the world's largest fleet of environmentally friendly compressed natural gas (CNG) buses. In 1996, the Centre for Science and Environment (CSE) started a public interest litigation in the Supreme Court of India that ordered the conversion of Delhi's fleet of buses and taxis to run on CNG and banned the use of leaded petrol in 1998. In 2003, Delhi won the United States Department of Energy's first 'Clean Cities International Partner of the Year' award for its "bold efforts to curb air pollution and support alternative fuel initiatives". The Delhi Metro has also been credited for significantly reducing air pollutants in the city.
However, according to several authors, most of these gains have been lost, especially due to stubble burning, rise in market share of diesel cars and a considerable decline in bus ridership. According to CSE and System of Air Quality Weather Forecasting and Research (SAFAR), burning of agricultural waste in nearby Punjab, Haryana and Uttar Pradesh regions results in severe intensification of smog over Delhi. The state government of adjoining Uttar Pradesh is considering imposing a ban on crop burning to reduce pollution in Delhi NCR and an environmental panel has appealed to India's Supreme Court to impose a 30% cess on diesel cars.
United Kingdom.
London.
In 1306, concerns over air pollution were sufficient for Edward I to (briefly) ban coal fires in London. In 1661, John Evelyn's "Fumifugium" suggested burning fragrant wood instead of mineral coal, which he believed would reduce coughing. The "" the same year describes how the smoke "does our lungs and spirits choke, Our hanging spoil, and rust our iron."
Severe episodes of smog continued in the 19th and 20th centuries, mainly in the winter, and were nicknamed "pea-soupers," from the phrase "as thick as pea soup." The Great Smog of 1952 darkened the streets of London and killed approximately 4,000 people in the short time of 4 days (a further 8,000 died from its effects in the following weeks and months). Initially a flu epidemic was blamed for the loss of life.
In 1956 the Clean Air Act started legally enforcing smokeless zones in the capital. There were areas where no soft coal was allowed to be burned in homes or in businesses, only coke, which produces no smoke. Because of the smokeless zones, reduced levels of sooty particulates eliminated the intense and persistent London smog.
It was after this that the great clean-up of London began. One by one, historical buildings which, during the previous two centuries had gradually completely blackened externally, had their stone facades cleaned and restored to their original appearance. Victorian buildings whose appearance changed dramatically after cleaning included the British Museum of Natural History. A more recent example was the Palace of Westminster, which was cleaned in the 1980s. A notable exception to the restoration trend was 10 Downing Street, whose bricks upon cleaning in the late 1950s proved to be naturally "yellow"; the smog-derived black colour of the façade was considered so iconic that the bricks were painted black to preserve the image. Smog caused by traffic pollution, however, does still occur in modern London.
Other areas.
Other areas of the United Kingdom were affected by smog, especially heavily industrialised areas.
The cities of Glasgow and Edinburgh, in Scotland, suffered smoke-laden fogs in 1909. Des Voeux, commonly credited with creating the "smog" moniker, presented a paper in 1911 to the Manchester Conference of the Smoke Abatement League of Great Britain about the fogs and resulting deaths.
One Birmingham resident described near black-out conditions in the 1900s before the Clean Air Act, with visibility so poor that cyclists had to dismount and walk in order to stay on the road.
Mexico City, Mexico.
Due to its location in a highland "bowl", cold air sinks down onto the urban area of Mexico City, trapping industrial and vehicle pollution underneath, and turning it into the most infamously smog-plagued city of Latin America. Within one generation, the city has changed from being known for some of the cleanest air of the world into one with some of the worst pollution, with pollutants like nitrogen dioxide being double or even triple international standards.
Santiago, Chile.
Similar to Mexico City, the air pollution of Santiago valley, located between the Andes and the Chilean Coast Range, turn it into the most infamously smog-plagued city of South America. Other aggravates of the situation reside in its high latitude (31 degrees South) and dry weather during most of the year.
Tehran, Iran.
In December 2005, schools and public offices had to close in Tehran, Iran and 1600 people were taken to hospital, in a severe smog blamed largely on unfiltered car exhaust.
United States.
Smog was brought to the attention of the general US public in 1933 with the publication of the book "Stop That Smoke", by Henry Obermeyer, a New York public utility official, in which he pointed out the effect on human life and even the destruction of of a farmer's spinach crop. Since then, the United States Environmental Protection Agency has designated over 300 U.S. counties to be non-attainment areas for one or more pollutants tracked as part of the National Ambient Air Quality Standards. These areas are largely clustered around large metropolitan areas, with the largest contiguous non-attainment zones in California and the Northeast. Various U.S. and Canadian government agencies collaborate to produce real-time air quality maps and forecasts.
Los Angeles and the San Joaquin Valley.
Because of their locations in low basins surrounded by mountains, Los Angeles and the San Joaquin Valley are notorious for their smog. The millions of vehicles in these regions combined with the additional effects of the San Francisco Bay and Los Angeles/Long Beach port complexes frequently contribute to further air pollution.
Los Angeles in particular is strongly predisposed to accumulation of smog, because of peculiarities of its geography and weather patterns. Los Angeles is situated in a flat basin with ocean on one side and mountain ranges on three sides. A nearby cold ocean current depresses surface air temperatures in the area, resulting in an inversion layer: a phenomenon where air temperature increases, instead of decreasing, with altitude, suppressing thermals and restricting vertical convection. All taken together, this results in a relatively thin, enclosed layer of air above the city that can't easily escape out of the basin and tends to accumulate pollution.
Though Los Angeles was one of the best known cities suffering from transportation smog for much of the 20th century, so much so that it was sometimes said that "Los Angeles" was a synonym for "smog", strict regulations by government agencies overseeing this problem, including tight restrictions on allowed emissions levels for all new cars sold in California and mandatory regular emission tests of older vehicles, resulted in significant improvements in air quality. For example, air concentrations of volatile organic compounds declined by a factor of 50 between 1962 and 2012. Concentrations of air pollutants such as nitrous oxides and ozone declined by 70% to 80% over the same period of time.
Ulaanbaatar, Mongolia.
In the late 1990s, massive immigration to Ulaanbaatar from the countryside began. An estimated 150,000 households, mainly living in traditional Mongolian gers on the outskirts of Ulaanbaatar, burn wood and coal (some poor families burn even car tires and trash) to heat themselves during the harsh winter, which lasts from October to April, since these outskirts are not connected to the city's central heating system. A temporary solution to decrease smog was proposed in the form of stoves with improved efficiency, although with no visible results. 
Coal-fired ger stoves release high levels of ash and other particulate matter (PM). When inhaled, these particles can settle in the lungs and respiratory tract and cause health problems. At two to 10 times above Mongolian and international air quality standards, Ulaanbaatar's PM rates are among the worst in the world, according to a December 2009 World Bank report. The Asian Development Bank (ADB) estimates that health costs related to this air pollution account for as much as 4 percent of Mongolia's GDP.
Southeast Asia.
Smog is a regular problem in Southeast Asia caused by land and forest fires in Indonesia, especially Sumatra and Kalimantan, although the term haze is preferred in describing the problem. Farmers and plantation owners are usually responsible for the fires, which they use to clear tracts of land for further plantings. Those fires mainly affect Brunei, Indonesia, Philippines, Malaysia, Singapore and Thailand, and occasionally Guam and Saipan. The economic losses of the fires in 1997 have been estimated at more than US$9 billion. This includes damages in agriculture production, destruction of forest lands, health, transportation, tourism, and other economic endeavours. Not included are social, environmental, and psychological problems and long-term health effects. The second-latest bout of haze to occur in Malaysia, Singapore and the Malacca Straits is in October 2006, and was caused by smoke from fires in Indonesia being blown across the Straits of Malacca by south-westerly winds. A similar haze has occurred in June 2013, with the PSI setting a new record in Singapore on June 21 at 12pm with a reading of 401, which is in the "Hazardous" range.
The Association of Southeast Asian Nations (ASEAN) reacted. In 2002, the Agreement on Transboundary Haze Pollution was signed between all ASEAN nations. ASEAN formed a Regional Haze Action Plan (RHAP) and established a co-ordination and support unit (CSU). RHAP, with the help of Canada, established a monitoring and warning system for forest/vegetation fires and implemented a Fire Danger Rating System (FDRS). The Malaysian Meteorological Department (MMD) has issued a daily rating of fire danger since September 2003. Indonesia has been ineffective at enforcing legal policies on errant farmers.
Pollution index.
The severity of smog is often measured using automated optical instruments such as Nephelometers, as haze is associated with visibility and traffic control in ports. Haze however can also be an indication of poor air quality though this is often better reflected using accurate purpose built air indexes such as the American Air Quality Index, the Malaysian API (Air Pollution Index) and the Singaporean Pollutant Standards Index.
In hazy conditions, it is likely that the index will report the suspended particulate level. The disclosure of the responsible pollutant is mandated in some jurisdictions.
The Malaysian API does not have a capped value; hence its most hazardous readings can go above 500. Above 500, a state of emergency is declared in the affected area. Usually, this means that non-essential government services are suspended, and all ports in the affected area are closed. There may also be prohibitions on private sector commercial and industrial activities in the affected area excluding the food sector. So far, state of emergency rulings due to hazardous API levels were applied to the Malaysian towns of Port Klang, Kuala Selangor and the state of Sarawak during the 2005 Malaysian haze and the 1997 Southeast Asian haze.

</doc>
<doc id="27001" url="https://en.wikipedia.org/wiki?curid=27001" title="Smoke">
Smoke

Smoke is a collection of airborne solid and liquid particulates and gases emitted when a material undergoes combustion or pyrolysis, together with the quantity of air that is entrained or otherwise mixed into the mass. It is commonly an unwanted by-product of fires (including stoves, candles, oil lamps, and fireplaces), but may also be used for pest control (fumigation), communication (smoke signals), defensive and offensive capabilities in the military (smoke-screen), cooking, or smoking (tobacco, cannabis, etc.). Smoke is used in rituals where incense, sage, or resin is burned to produce a smell for spiritual purposes. Smoke is sometimes used as a flavoring agent, and preservative for various foodstuffs. Smoke is also a component of internal combustion engine exhaust gas, particularly diesel exhaust.
Smoke inhalation is the primary cause of death in victims of indoor fires. The smoke kills by a combination of thermal damage, poisoning and pulmonary irritation caused by carbon monoxide, hydrogen cyanide and other combustion products.
Smoke is an aerosol (or mist) of solid particles and liquid droplets that are close to the ideal range of sizes for Mie scattering of visible light. This effect has been likened to three-dimensional textured privacy glass — a smoke cloud does not obstruct an image, but thoroughly scrambles it.
Chemical composition.
The composition of smoke depends on the nature of the burning fuel and the conditions of combustion.
Fires with high availability of oxygen burn at a high temperature and with small amount of smoke produced; the particles are mostly composed of ash, or with large temperature differences, of condensed aerosol of water. High temperature also leads to production of nitrogen oxides. Sulfur content yields sulfur dioxide, or in case of incomplete combustion, hydrogen sulfide. Carbon and hydrogen are almost completely oxidized to carbon dioxide and water. Fires burning with lack of oxygen produce a significantly wider palette of compounds, many of them toxic. Partial oxidation of carbon produces carbon monoxide, nitrogen-containing materials can yield hydrogen cyanide, ammonia, and nitrogen oxides. Hydrogen gas can be produced instead of water. Content of halogens such as chlorine (e.g. in polyvinyl chloride or brominated flame retardants) may lead to production of e.g. hydrogen chloride, phosgene, dioxin, and chloromethane, bromomethane and other halocarbons. Hydrogen fluoride can be formed from fluorocarbons, whether fluoropolymers subjected to fire or halocarbon fire suppression agents. Phosphorus and antimony oxides and their reaction products can be formed from some fire retardant additives, increasing smoke toxicity and corrosivity. Pyrolysis of polychlorinated biphenyls (PCB), e.g. from burning older transformer oil, and to lower degree also of other chlorine-containing materials, can produce 2,3,7,8-tetrachlorodibenzodioxin, a potent carcinogen, and other polychlorinated dibenzodioxins. Pyrolysis of fluoropolymers, e.g. teflon, in presence of oxygen yields carbonyl fluoride (which hydrolyzes readily to HF and CO2); other compounds may be formed as well, e.g. carbon tetrafluoride, hexafluoropropylene, and highly toxic perfluoroisobutene (PFIB).
Pyrolysis of burning material, especially incomplete combustion or smoldering without adequate oxygen supply, also results in production of a large amount of hydrocarbons, both aliphatic (methane, ethane, ethylene, acetylene) and aromatic (benzene and its derivates, polycyclic aromatic hydrocarbons; e.g. , studied as a carcinogen, or retene), terpenes. Heterocyclic compounds may be also present. Heavier hydrocarbons may condense as tar; smoke with significant tar content is yellow to brown. Presence of such smoke, soot, and/or brown oily deposits during a fire indicates a possible hazardous situation, as the atmosphere may be saturated with combustible pyrolysis products with concentration above the upper flammability limit, and sudden inrush of air can cause flashover or backdraft.
Presence of sulfur can lead to formation of e.g. hydrogen sulfide, carbonyl sulfide, sulfur dioxide, carbon disulfide, and thiols; especially thiols tend to get adsorbed on surfaces and produce a lingering odor even long after the fire. Partial oxidation of the released hydrocarbons yields in a wide palette of other compounds: aldehydes (e.g. formaldehyde, acrolein, and furfural), ketones, alcohols (often aromatic, e.g. phenol, guaiacol, syringol, catechol, and cresols), carboxylic acids (formic acid, acetic acid, etc.).
The visible particulate matter in such smokes is most commonly composed of carbon (soot). Other particulates may be composed of drops of condensed tar, or solid particles of ash. The presence of metals in the fuel yields particles of metal oxides. Particles of inorganic salts may also be formed, e.g. ammonium sulfate, ammonium nitrate, or sodium chloride. Inorganic salts present on the surface of the soot particles may make them hydrophilic. Many organic compounds, typically the aromatic hydrocarbons, may be also adsorbed on the surface of the solid particles. Metal oxides can be present when metal-containing fuels are burned, e.g. solid rocket fuels containing aluminium. Depleted uranium projectiles after impacting the target ignite, producing particles of uranium oxides. Magnetic particles, spherules of magnetite-like ferrous ferric oxide, are present in coal smoke; their increase in deposits after 1860 marks the beginning of the Industrial Revolution. (Magnetic iron oxide nanoparticles can be also produced in the smoke from meteorites burning in the atmosphere.) Magnetic remanence, recorded in the iron oxide particles, indicates the strength of Earth's magnetic field when they were cooled beyond their Curie temperature; this can be used to distinguish magnetic particles of terrestrial and meteoric origin. Fly ash is composed mainly of silica and calcium oxide. Cenospheres are present in smoke from liquid hydrocarbon fuels. Minute metal particles produced by abrasion can be present in engine smokes. Amorphous silica particles are present in smokes from burning silicones; small proportion of silicon nitride particles can be formed in fires with insufficient oxygen. The silica particles have about 10 nm size, clumped to 70-100 nm aggregates and further agglomerated to chains. Radioactive particles may be present due to traces of uranium, thorium, or other radionuclides in the fuel; hot particles can be present in case of fires during nuclear accidents (e.g. Chernobyl disaster) or nuclear war.
Smoke particulates, like other aerosols, are categorized into three modes based on particle size:
Most of the smoke material is primarily in coarse particles. Those undergo rapid dry precipitation, and the smoke damage in more distant areas outside of the room where the fire occurs is therefore primarily mediated by the smaller particles.
Aerosol of particles beyond visible size is an early indicator of materials in a preignition stage of a fire.
Burning of hydrogen-rich fuel produces water; this results in smoke containing droplets of water vapor. In absence of other color sources (nitrogen oxides, particulates...), such smoke is white and cloud-like.
Smoke emissions may contain characteristic trace elements. Vanadium is present in emissions from oil fired power plants and refineries; oil plants also emit some nickel. Coal combustion produces emissions containing aluminium, arsenic, chromium, cobalt, copper, iron, mercury, selenium, and uranium.
Traces of vanadium in high-temperature combustion products form droplets of molten vanadates. These attack the passivation layers on metals and cause high temperature corrosion, which is a concern especially for internal combustion engines. Molten sulfate and lead particulates also have such effect.
Some components of smoke are characteristic of the combustion source. Guaiacol and its derivatives are products of pyrolysis of lignin and are characteristic of wood smoke; other markers are syringol and derivates, and other methoxy phenols. Retene, a product of pyrolysis of conifer trees, is an indicator of forest fires. Levoglucosan is a pyrolysis product of cellulose. Hardwood vs softwood smokes differ in the ratio of guaiacols/syringols. Markers for vehicle exhaust include polycyclic aromatic hydrocarbons, hopanes, steranes, and specific nitroarenes (e.g. 1-nitropyrene). The ratio of hopanes and steranes to elemental carbon can be used to distinguish between emissions of gasoline and diesel engines.
Many compounds can be associated with particulates; whether by being adsorbed on their surfaces, or by being dissolved in liquid droplets. Hydrogen chloride is well absorbed in the soot particles.
Inert particulate matter can be disturbed and entrained into the smoke. Of particular concern are particles of asbestos.
Deposited hot particles of radioactive fallout and bioaccumulated radioisotopes can be reintroduced into the atmosphere by wildfires and forest fires; this is a concern in e.g. the Zone of alienation containing contaminants from the Chernobyl disaster.
Polymers are a significant source of smoke. Aromatic side groups, e.g. in polystyrene, enhance generation of smoke. Aromatic groups integrated in the polymer backbone produce less smoke, likely due to significant charring. Aliphatic polymers tend to generate the least smoke, and are non-self-extinguishing. However presence of additives can significantly increase smoke formation. Phosphorus-based and halogen-based flame retardants decrease production of smoke. Higher degree of cross-linking between the polymer chains has such effect too.
Visible and invisible particles of combustion.
The naked eye detects particle sizes greater than 7 µm (micrometres). Visible particles emitted from a fire are referred to as smoke. Invisible particles are generally referred to as gas or fumes. This is best illustrated when toasting bread in a toaster. As the bread heats up, the products of combustion increase in size. The fumes initially produced are invisible but become visible if the toast is burnt.
An ionization chamber type smoke detector is technically a product of combustion detector, not a smoke detector. Ionization chamber type smoke detectors detect particles of combustion that are invisible to the naked eye. This explains why they may frequently false alarm from the fumes emitted from the red-hot heating elements of a toaster, before the presence of visible smoke, yet they may fail to activate in the early, low-heat smoldering stage of a fire.
Smoke from a typical house fire contains hundreds of different chemicals and fumes. As a result, the damage caused by the smoke can often exceed that caused by the actual heat of the fire. In addition to the physical damage caused by the smoke of a fire – which manifests itself in the form of stains – is the often even harder to eliminate problem of a smoky odor. Just as there are contractors that specialize in rebuilding/repairing homes that have been damaged by fire and smoke, fabric restoration companies specialize in restoring fabrics that have been damaged in a fire.
Dangers of smoke.
Smoke from oxygen-deprived fires contains a significant concentration of compounds that are flammable. A cloud of smoke, in contact with atmospheric oxygen, therefore has the potential of being ignited – either by another open flame in the area, or by its own temperature. This leads to effects like backdraft and flashover. Smoke inhalation is also a danger of smoke that can cause serious injury and death.
Many compounds of smoke from fires are highly toxic and/or irritating. The most dangerous is carbon monoxide leading to carbon monoxide poisoning, sometimes with the additive effects of hydrogen cyanide and phosgene. Smoke inhalation can therefore quickly lead to incapacitation and loss of consciousness. Sulfur oxides, hydrogen chloride and hydrogen fluoride in contact with moisture form sulfuric, hydrochloric and hydrofluoric acid, which are corrosive to both lungs and materials. When asleep the nose does not sense smoke nor does the brain, but the body will wake up if the lungs become enveloped in smoke and the brain will be stimulated and the person will be awoken. This does not work if the person is incapacitated or under the influence of drugs and/or alcohol.
Cigarette smoke is a major modifiable risk factor for lung disease, heart disease, and many cancers. Smoke can also be a component of ambient air pollution due to the burning of coal in power plants, forest fires or other sources, although the concentration of pollutants in ambient air is typically much less than that in cigarette smoke. One day of exposure to PM2.5 at a concentration of 880 μg/m3, such as occurs in Beijing, China, is the equivalent of smoking one or two cigarettes in terms of particulate inhalation by weight. The analysis is complicated, however, by the fact that the organic compounds present in various ambient particulates may have a higher carcinogenicity than the compounds in cigarette smoke particulates.
Smoke can obscure visibility, impeding occupant exiting from fire areas. In fact, the poor visibility due to the smoke that was in the Worcester Cold Storage Warehouse fire in Worcester, Massachusetts was the exact reason why the trapped rescue firefighters couldn't evacuate the building in time. Because of the striking similarity that each floor shared, the dense smoke caused the firefighters to become disoriented.
The effect of smoke burning off the end of a tobacco product or the smoke exhaled by a smoker is known as secondhand smoke. This smoke is contained with harmful substances that can harm the body of the smoker. A person could have serious health problems from the result of breathing in secondhand smoke such as diseases or cancers. A child’s undeveloped body can face respiratory problems that could affect their lives forever. Secondhand smoke also harms the environment by being an indoor air pollutant that people breathe in.
Smoke corrosion.
Smoke contains a wide variety of chemicals, many of them aggressive in nature. Examples are hydrochloric acid and hydrobromic acid, produced from halogen-containing plastics and fire retardants, hydrofluoric acid released by pyrolysis of fluorocarbon fire suppression agents, sulfuric acid from burning of sulfur-containing materials, nitric acid from high-temperature fires where nitrous oxide gets formed, phosphoric acid and antimony compounds from P and Sb based fire retardants, and many others. Such corrosion is not significant for structural materials, but delicate structures, especially microelectronics, are strongly affected. Corrosion of circuit board traces, penetration of aggressive chemicals through the casings of parts, and other effects can cause an immediate or gradual deterioration of parameters or even premature (and often delayed, as the corrosion can progress over long time) failure of equipment subjected to smoke. Many smoke components are also electrically conductive; deposition of a conductive layer on the circuits can cause crosstalks and other deteriorations of the operating parameters or even cause short circuits and total failures. Electrical contacts can be affected by corrosion of surfaces, and by deposition of soot and other conductive particles or nonconductive layers on or across the contacts. Deposited particles may adversely affect the performance of optoelectronics by absorbing or scattering the light beams.
Corrosivity of smoke produced by materials is characterized by the corrosion index (CI), defined as material loss rate (angstrom/minute) per amount of material gasified products (grams) per volume of air (m3). It is measured by exposing strips of metal to flow of combustion products in a test tunnel. Polymers containing halogen and hydrogen (polyvinyl chloride, polyolefins with halogenated additives, etc.) have the highest CI as the corrosive acids are formed directly with water produced by the combustion, polymers containing halogen only (e.g. polytetrafluoroethylene) have lower CI as the formation of acid is limited to reactions with airborne humidity, and halogen-free materials (polyolefins, wood) have the lowest CI. However, some halogen-free materials can also release significant amount of corrosive products.
Smoke damage to electronic equipment can be significantly more extensive than the fire itself. Cable fires are of special concern; low smoke zero halogen materials are preferable for cable insulation.
When smoke comes into contact with the surface of any substance or structure, the chemicals contained in it are transferred to it. The corrosive properties of the chemicals cause the substance or structure to decompose at a rapid rate. Certain materials or structures absorb these chemicals, which is why clothing, unsealed surfaces, potable water, piping, wood, etc., are replaced in most cases of structural fires.
Secondhand tobacco smoke inhalation.
Secondhand tobacco smoke is the combination of both sidestream and mainstream smoke emissions. These emissions contain more than 50 carcinogenic chemicals. According to the Surgeon General's latest report on the subject, "Short exposures to secondhand smoke can cause blood platelets to become stickier, damage the lining of blood vessels, decrease coronary flow velocity reserves, and reduce heart variability, potentially increasing the risk of a heart attack". The American Cancer Society lists "heart disease, lung infections, increased asthma attacks, middle ear infections, and low birth weight" as ramifications of smoker's emission.
Measurement.
As early as the 15th century Leonardo da Vinci commented at length on the difficulty of assessing smoke, and distinguished between black smoke (carbonized particles) and white 'smoke' which is not a smoke at all but merely a suspension of harmless water particulates.
Smoke from heating appliances is commonly measured in one of the following ways:
In-line capture. A smoke sample is simply sucked through a filter which is weighed before and after the test and the mass of smoke found. This is the simplest and probably the most accurate method, but can only be used where the smoke concentration is slight, as the filter can quickly become blocked.
The ASTM smoke pump is a simple and widely used method of in-line capture where a measured volume of smoke is pulled through a filter paper and the dark spot so formed is compared with a standard.
Filter/dilution tunnel. A smoke sample is drawn through a tube where it is diluted with air, the resulting smoke/air mixture is then pulled through a filter and weighed. This is the internationally recognized method of measuring smoke from combustion.
Electrostatic precipitation. The smoke is passed through an array of metal tubes which contain suspended wires. A (huge) electrical potential is applied across the tubes and wires so that the smoke particles become charged and are attracted to the sides of the tubes. This method can over-read by capturing harmless condensates, or under-read due to the insulating effect of the smoke. However, it is the necessary method for assessing volumes of smoke too great to be forced through a filter, i.e., from bituminous coal.
Ringelmann scale. A measure of smoke color. Invented by Professor Maximilian Ringelmann in Paris in 1888, it is essentially a card with squares of black, white and shades of gray which is held up and the comparative grayness of the smoke judged. Highly dependent on light conditions and the skill of the observer it allocates a grayness number from 0 (white) to 5 (black) which has only a passing relationship to the actual quantity of smoke. Nonetheless, the simplicity of the Ringelmann scale means that it has been adopted as a standard in many countries.
Optical scattering. A light beam is passed through the smoke. A light detector is situated at an angle to the light source, typically at 90°, so that it receives only light reflected from passing particles. A measurement is made of the light received which will be higher as the concentration of smoke particles becomes higher.
Optical obscuration. A light beam is passed through the smoke and a detector opposite measures the light. The more smoke particles are present between the two, the less light will be measured.
Combined optical methods. There are various proprietary optical smoke measurement devices such as the 'nephelometer' or the 'aethalometer' which use several different optical methods, including more than one wavelength of light, inside a single instrument and apply an algorithm to give a good estimate of smoke. It has been claimed that these devices can differentiate types of smoke and so their probable source can be inferred, though this is disputed.
Inference from carbon monoxide. Smoke is incompletely burned fuel, carbon monoxide is incompletely burned carbon, therefore it has long been assumed that measurement of CO in flue gas (a cheap, simple and very accurate procedure) will provide a good indication of the levels of smoke. Indeed, several jurisdictions use CO measurement as the basis of smoke control. However it is far from clear how accurate the correspondence is.
Medicinal smoke.
Throughout recorded history, humans have used the smoke of medicinal plants to cure illness. A sculpture from Persepolis shows Darius the Great (522–486 BC), the king of Persia, with two censers in front of him for burning Peganum harmala and/or sandalwood Santalum album, which was believed to protect the king from evil and disease. More than 300 plant species in 5 continents are used in smoke form for different diseases. As a method of drug administration, smoking is important as it is a simple, inexpensive, but very effective method of extracting particles containing active agents. More importantly, generating smoke reduces the particle size to a microscopic scale thereby increasing the absorption of its active chemical principles.

</doc>
<doc id="27002" url="https://en.wikipedia.org/wiki?curid=27002" title="Tobacco pipe">
Tobacco pipe

A tobacco pipe, often called simply a pipe, is a device specifically made to smoke tobacco. It comprises a chamber (the bowl) for the tobacco from which a thin hollow stem (shank) emerges, ending in a mouthpiece (the bit). Pipes can range from very simple machine-made briar models to highly prized hand-made artisanal implements made by renowned pipemakers, which are often very expensive collector's items. Pipe smoking is the oldest known traditional form of tobacco smoking.
History.
Smoking pipes of various types have been used since ancient times. Herodotus described Scythians inhaling the fumes of burning leaves in 500 B.C.
Some Native American cultures smoke tobacco in ceremonial pipes, and have done so since long before the arrival of Europeans. Other American Indian cultures smoke tobacco socially. The tobacco plant is native to South America but spread into North America long before Europeans arrived. Tobacco was introduced to Europe from the Americas in the 16th century and spread around the world rapidly.
As tobacco was not introduced to the Old World until the 16th century, the older pipes outside of the Americas were usually used to smoke hashish, a rare and expensive substance outside areas of the Middle East, Central Asia and India, where it was then produced.
Workings of a tobacco pipe.
Materials.
The bowls of tobacco pipes are commonly made of briar wood, meerschaum, corncob or clay. Less common are other dense-grained woods such as cherry, olive, maple, mesquite, oak, and bog-wood. Minerals such as catlinite and soapstone have also been used. Pipe bowls are sometimes decorated by carving, and moulded clay pipes often had simple decoration in the mould.
Unusual, but still noteworthy pipe materials include gourds, as in the famous calabash pipe, and pyrolytic graphite. Metal and glass are uncommon materials for tobacco pipes, but are common for pipes intended for other substances, such as cannabis (due to the desire for the smoke to enter the lungs).
The stem needs a long channel of constant position and diameter running through it for a proper draw, although filter pipes have varying diameters and can be successfully smoked even without filters or adapters. Because it is molded rather than carved, clay may make up the entire pipe or just the bowl, but most other materials have stems made separately and detachable. Stems and bits of tobacco pipes are usually made of moldable materials like vulcanite, lucite, Bakelite, and soft plastic. Less common are stems made of reeds, bamboo, or hollowed out pieces of wood. Expensive pipes once had stems made of amber, though this is rare now.
Types.
Briar.
The majority of pipes sold today, whether handmade or machine-made, are fashioned from briar (). Briar is a particularly well suited wood for pipe making for a number of reasons. The first and most important characteristic is its natural resistance to fire. The second is its inherent ability to absorb moisture. The burl absorbs water in nature to supply the tree in the dry times and likewise will absorb the moisture that is a byproduct of combustion. Briar is cut from the root burl of the tree heath ("Erica arborea"), which is native to the rocky and sandy soils of the Mediterranean region. Briar burls are cut into two types of blocks; ebauchon and plateaux. Ebauchon is taken from the heart of the burl while plateaux is taken from the outer part of the burl. While both types of blocks can produce pipes of the highest quality, most artisan pipemakers prefer to use plateaux because of its superior graining.
Meerschaum.
Meerschaum (hydrated magnesium silicate), a mineral found in small shallow deposits mainly around the city of Eskişehir in central Turkey, is prized for the properties which allow it to be carved into finely detailed decorative and figural shapes. It has been used since the 17th century and, with clay pipes, represented the most common medium for pipes before the introduction of briar as the material of choice in the 19th century. The word "meerschaum" means "sea foam" in German, alluding to its natural white color and its surprisingly low weight. Meerschaum is a very porous mineral that absorbs elements of the tobacco during the smoking process, and gradually changes color to a golden brown. Old, well-smoked meerschaum pipes are valued by collectors for their distinctive coloring. In selecting a meerschaum pipe it is advisable to verify that the product is indeed carved from a block of meerschaum, and is not made from meerschaum dust collected after carving and mixed with a binder then pressed into a pipe shape. These products are not absorbent, do not color, and lack the smoking quality of the block carved pipe.
Clay.
Ceramic pipes, made of moulded and then fired clay, were used almost universally by Europeans before the 19th century. The material is not very strong and the early varieties had long thin stems, so they frequently broke, but were cheap to replace. They were made in moulds with the bore created by pushing an oiled wire inside the stem. The preferred material was pipeclay or "tobacco pipe clay", which fires to a white colour and is only found in certain locations. In North America many clay pipes were historically made from more typical terracotta-coloured clays. According to one British writer in 1869, the French preferred old pipes and the English new, and the working class preferred short stems but the middle class long ones.
Later low-quality clay pipes were made by slip casting in a mould. Higher quality pipes are made in a labour-intensive hand shaping process. Traditionally, clay pipes are un-glazed. Clays burn "hot" in comparison to other types of pipes, so they are often difficult for most pipe-smokers to use. Their proponents claim that, unlike other materials, a well-made clay pipe gives a "pure" smoke with no flavour addition from the pipe bowl. In addition to aficionados, reproductions of historical clay styles are used by some historical re-enactors. Clay pipes were once considered disposable items and the rapidly changing designs in the past are often used as an aid in dating by archaeologists. They were once very popular in Ireland, where they were called a "dúidín"s.
Calabash.
Calabash gourds (usually with meerschaum or porcelain bowls set inside them) have long made prized pipes, but they are labour-intensive and, today, quite expensive. Because of this expense, pipes with bodies made of wood (usually mahogany) instead of gourd, but with the same classic shape, are sold as calabashes. Both wood and gourd pipes are functionally the same (with the important exception that the dried gourd, usually being noticeably lighter, sits more comfortably in the mouth). They consist of a downward curve that ends with an upcurve where the bowl sits. Beneath the bowl is an air chamber which serves to cool, dry, and mellow the smoke. There are also briar pipes being sold as calabashes. These typically do not have an air chamber and are so named only because of their external shape.
A calabash pipe is rather large and easy to recognize as a pipe when used on a stage in dramatic productions. Early portrayers of the character Sherlock Holmes, particularly William Gillette and Basil Rathbone, took advantage of this fact when it was required to portray Holmes smoking. This is why Holmes is stereotypically depicted as favouring a calabash. In fact, most stories, particularly "The Adventure of the Copper Beeches", described him as preferring a long-stemmed cherry-wood churchwarden pipe or a clay pipe.
Corncob.
On the other end of the scale, the specifically American style of pipes made from corncobs are cheap and effective, even if some regard them as inelegant. The cobs are first dried for two years. Then they are hollowed out to make a bowl shape. The bowls are dipped in a plaster-based mixture and varnished or lacquered on the outside. Shanks made from pine wood are then inserted into the bowls. The first and largest manufacturer of corncob pipes is Missouri Meerschaum, located in Washington, Missouri, in the USA. Missouri Meerschaum has produced the pipes since 1869. General Douglas MacArthur and Mark Twain were perhaps the most famous smokers of this type of pipe, along with the cartoon characters Popeye and Frosty the Snowman.
Corncob pipes remain popular today because they are inexpensive and require no "break-in" period like briar pipes. For these two reasons, corncob pipes are often recommended as a "Beginner's pipe." But their enjoyment is by no means limited to beginners. Corncob pipes are equally valued by both learners and experienced smokers who simply desire a cool, clean smoke. Pipesmokers who wish to sample a wide variety of different tobaccos and blends also might keep a stock of corncobs on hand to permit them to try new flavors without "carryover" from an already-used pipe, or to keep a potentially bad-tasting tobacco from adding its flavor to a more expensive or favored pipe.
Churchwarden.
A churchwarden pipe is a tobacco pipe with a long stem.
Synthetics.
A variety of other materials may also be used for pipes. The Redmanol corporation manufactured pipes with translucent stems in the 1920s and a series of pipes were manufactured and distributed by the Tar Gard (later Venturi) Corporation of San Francisco from 1965-1975. Marketed under names such as "the pipe," "THE SMOKE" and "Venturi," they used materials such as pyrolytic graphite, phenolic resin, nylon, Bakelite and other synthetics, allowing for higher temperatures in the bowl, reduced tar, and aesthetic variations of color and style.
After Venturi stopped making pipes, several companies continue to make pipes from Brylon, a composite of nylon and wood flour, as a cheaper substitute for briar.
Metal.
Metal is an uncommon material for making tobacco pipes, but they are not unknown. The most common form of this is a pipe with a shank made of aluminum, which serves as a heat sink. Mouthpieces are made of vulcanite or lucite. The bowls are removable, though not interchangeable between manufacturers. They are made of varying materials to allow the smoker to try different characteristics or to dedicate particular bowls for particular tobaccos.
Other metal tobacco pipes include the very small Japanese kiseru and Arabian midwakh. Hookahs also may have metal stems, but fall into the general category of water pipes.
Hookahs.
A "hookah", "ghelyan", or "narghile", is a Middle Eastern water pipe that cools the smoke by filtering it through a water chamber. Often ice, cough-drops, milk, or fruit juice is added to the water. Traditionally, the tobacco is mixed with a sweetener, such as honey or molasses. Fruit flavors have also become popular. Modern hookah smokers, especially in the US, smoke "me'assel" "moassel" "molasses" or "shisha" all names for the same wet mixture of tobacco, molasses/honey, glycerine, and often, flavoring. This style of tobacco is smoked in a bowl with foil or a screen (metal or glass) on top of the bowl. More traditional tobaccos are "tombiek" (a dry unflavored tobacco, which the user moistens in water, squeezes out the extra liquid, and places coals directly on top) or "jarak" (more of a paste of tobacco with fruit to flavor the smoke).
Use.
Smoking a pipe requires more apparatus and technique than cigarette or even cigar smoking. In addition to the pipe itself and matches or a pipe lighter, smokers usually require a pipe tool for packing, adjusting, and emptying the tobacco in the bowl, and a regular supply of pipe cleaners.
Tobacco.
Tobaccos for smoking in pipes are often carefully treated and blended to achieve flavour nuances not available in other tobacco products. Many of these are blends using staple ingredients of variously cured Burley and Virginia tobaccos which are enhanced by spice tobaccos, among them many Oriental or Balkan varietals, Latakia (a fire-cured spice tobacco of Syrian origin), Perique (uniquely grown in St. James Parish, Louisiana) which is also an old method of fermentation, or blends of Virginia and Burley tobaccos of African, Indian, or South American origins. Traditionally, many U.S. blends are made of American Burley with sweeteners and flavorings added to create an "aromatic" flavor, whereas "English" blends are based on natural Virginia tobaccos enhanced with Oriental and other natural tobaccos. There is a growing tendency towards "natural" tobaccos which derive their aromas from artful blending with selected spice tobaccos only and careful, often historically-based, curing processes.
Pipe tobacco can be purchased in several forms, which vary both in flavour (leading to many blends and opportunities for smokers to blend their own tobaccos) and in the physical shape and size to which the tobacco has been reduced. Most pipe tobaccos are less mild than cigarette tobacco, substantially more moist and cut much more coarsely. Too finely cut tobacco does not allow enough air to flow through the pipe, and overly dry tobacco burns too quickly with little flavour. Pipe tobacco must be kept in an airtight container, such as a canning jar or sealed tin, to keep from drying out.
Some pipe tobaccos are cut into long narrow ribbons. Some are pressed into flat cakes which are sliced. Others are tightly wound into long ropes, then sliced into discs. Plug tobacco is maintained in its pressed cake form and sold in small blocks. The plug will be sliced into thin flakes by the smoker and then prepared in a similar fashion to flake tobacco. It is considered that plug tobacco holds its flavor better than rubbed or flake tobacco. Flake tobacco (sliced cakes or ropes) may be prepared in several ways. Generally it is rubbed out with the fingers and palms until it is loose enough to pack. It can also be crumbled or simply folded and stuffed into a pipe. Some people also prefer to dice up very coarse tobaccos before using them, making them easier to pack.
Packing.
In the most common method of packing, tobacco is added to the bowl of the pipe in several batches, each one pressed down until the mixture has a uniform density that optimizes airflow (something that it is difficult to gauge without practice). This can be done with a finger or thumb, but if the tobacco needs to be repacked later, while it is burning, the tamper on a pipe tool is sometimes used. If it needs to be loosened, the reamer, or any similar long pin can be used. A traditional way of packing the pipe is to fill the bowl and then pack gently to about ⅓ full, fill again and pack slightly more firmly to about ⅔ full, and then pack more firmly still to the top.
An alternative packing technique called the Frank method involves lightly dropping tobacco in the pipe, after which a large plug is gingerly pushed into the bowl all at once.
Lighting.
Matches, or separately lit slivers of wood are often considered preferable to lighters because of lower burning temperature. Butane lighters made specifically for pipes 
emit flame sideways or at an angle to make it easier to direct flame into the bowl. Torch-style lighters should never be used to light a pipe because their flames are too hot and can char the rim of the pipe bowl. Matches should be allowed to burn for several seconds to allow the sulfur from the tip to burn away and the match to produce a full flame. A naphtha fueled lighter should also be allowed to burn a few seconds to get rid of stray naphtha vapors that could give a foul taste to the smoke. When a flame has been produced, it is then moved in circles above the rim of the bowl while the smoker puffs to draw the flame down and light the tobacco. Packing method and humidity can affect how often a pipe must be relit.
Burning prevention.
With care, a briar pipe can last a very long time without burning out. However, due to aggressive (hot) smoking, imperfections in the wood, or just bad luck, a hole can be burned in the tobacco chamber of the pipe. There are several methods used to help prevent a wood pipe from burning out. These generally involve coating the chamber with any of a variety of substances, or by gently smoking a new pipe to build up a cake (a mixture of ash, unburned tobacco, oils, sugars, and other residue) on the walls.
These coatings may include honey and water; powdered sugar and water; cigar ash and water; and sour cream, buttermilk, and activated charcoal among many others.
Many modern briar pipes are pre-treated by the manufacturer to resist burning. If smoked correctly, the cake will build up properly on its own. Another technique is to alternate a half-bowl and a full-bowl the first several times the pipe is used to build an even cake. Burley is often recommended to help a new pipe build cake.
The effectiveness of these methods is by no means universally agreed upon.
The caked layer that helps prevent burning through the bottom or sides of a briar wood pipe may damage other pipes, such as meerschaum or clay. As the cake layer heats up, it expands and may cause cracks or breaks in non-briar pipes.
Smoking.
Pipe smoke, like cigar smoke, is usually not inhaled. It is merely brought into the mouth, pumped around oral and nasal cavities to permit absorption of nicotine toward the brain through the mucous membranes, and released. It is normal to have to relight a pipe periodically. If it is smoked too slowly, this will happen more often. If it is smoked too quickly, it can produce excess moisture causing a gurgling sound in the pipe and an uncomfortable sensation on the tongue (referred to as "pipe tongue", or more commonly, "tongue bite").
A pipe cleaner can be used to dry out the bowl and, wetted with alcohol, the inner channel. The bowl of the pipe can also become uncomfortably hot, depending on the material and the rate of smoking. For this reason clay pipes in particular are often held by the stem. Meerschaum pipes are held in a square of chamois leather, with gloves, or else by the stem in order to prevent uneven coloring of the material.
Cleaning.
The ash and the last bits of unburned tobacco, known as dottle, should be cleaned out with a suitable pipe tool. A soft or bristle pipe cleaner, which may be moistened with strong spirits is then run through the airways of the stem and shank to remove any moisture, ash, and other residue before the pipe is allowed to dry. A pipe should be allowed to cool before removing the stem to avoid the possibility of warping it.
A cake of ash eventually develops inside the bowl. This is generally considered desirable for controlling overall heat. However, if it becomes too thick, it may expand faster than the bowl of the pipe itself when heated, cracking the bowl. Before reaching this point, it needs to be scraped down with a reamer. It is generally recommended to keep the cake at approximately the thickness of a U.S. dime (about 1/20th of an inch or 1.5 mm), though sometimes the cake is removed entirely as part of efforts to eliminate flavors or aromas.
Cake is considered undesirable in meerschaum pipes because it can easily crack the bowl and/or interfere with the mineral's natural porosity. Meerschaum also softens when heated so it is recommended to allow meerschaum pipes to cool before cleaning as people have been known to push pipe cleaners through the walls of heated pipes.
Regardless if a pipe is cleaned after every smoke, over time there is a buildup of cake in the bowl and tars in the internals of a smoking pipe. The cake can be controlled by gentle reaming, but a buildup of tars in the shank and airway of a pipe is more difficult to deal with. This may require the services of a professional pipe restorer to properly clean and sanitize the pipe.
Sweetening.
When tobacco is burned, oils from adjoining not yet ignited particles vaporize and condense into the existing cake on the walls of the bowl and shank. Over time, these oils can oxidize and turn rancid, causing the pipe to give a sour or bitter smoke. A purported countermeasure involves filling the bowl with kosher salt and carefully wetting it with strong spirits. It is important to not use iodized salt, as the iodine and other additives may impart an unpleasant flavor. Regularly wiping out the bowl with spirits such as vodka or rum is helpful in preventing souring. Commercial pipe-sweetening products are also available.

</doc>
<doc id="27003" url="https://en.wikipedia.org/wiki?curid=27003" title="Swiss cheese">
Swiss cheese

Swiss cheese is a generic name in North America for several related varieties of cheese, mainly of North American manufacture, which resemble Emmental cheese, a yellow, medium-hard cheese that originated in the area around Emmental, in Switzerland. Some types of Swiss cheese have a distinctive appearance, as the blocks of the cheese are riddled with holes known as "eyes". Swiss cheese without eyes is known as "blind". (The term is applied to cheeses of this style made outside Switzerland, such as Jarlsberg cheese, which originates in Norway).
Production.
Three types of bacteria are used in the production of Emmental cheese: "Streptococcus salivarius" subspecies "thermophilus", "Lactobacillus" ("Lactobacillus helveticus" or "Lactobacillus delbrueckii" subspecies "bulgaricus"), and "Propionibacterium" ("Propionibacterium freudenreichii" subspecies "shermani"). In a late stage of cheese production, the propionibacteria consume the lactic acid excreted by the other bacteria and release acetate, propionic acid, and carbon dioxide gas. The carbon dioxide slowly forms the bubbles that develop the "eyes". The acetate and propionic acid give Swiss its nutty and sweet flavor. A hypothesis proposed by Swiss researchers in 2015 notes that particulate matter may also play a role in the holes' development and that modern sanitation eliminated debris such as hay dust in the milk played a role in reduced hole size in Swiss cheeses, or even "blind cheese". Historically, the holes were seen as a sign of imperfection and cheese makers originally tried to avoid them by pressing during production. In modern times, the holes have become an identifier of the cheese. 
In general, the larger the eyes in a Swiss cheese, the more pronounced its flavor because a longer fermentation period gives the bacteria more time to act. This poses a problem, however, because cheese with large eyes does not slice well and comes apart in mechanical slicers. As a result, industry regulators have limited the eye size by which Swiss cheese receives the Grade A stamp.
In 2014, 297.8 million pounds of Swiss cheese was reportedly produced in the United States.
Varieties.
Baby Swiss and Lacy Swiss are two varieties of American Swiss cheeses. Both have small holes and a mild flavor. Baby Swiss is made from whole milk, and Lacy Swiss is made from low fat milk. Baby Swiss was developed in the mid-1960s outside of Charm, Ohio, by the Guggisberg Cheese Company, owned by Alfred Guggisberg.

</doc>
<doc id="27004" url="https://en.wikipedia.org/wiki?curid=27004" title="Spontaneous combustion (disambiguation)">
Spontaneous combustion (disambiguation)

Spontaneous combustion is the self-ignition of a mass, for example, a pile of oily rags. Allegedly, humans can also ignite and burn without an obvious cause; this phenomenon is known as spontaneous human combustion.
Spontaneous Combustion is also the name of:

</doc>
<doc id="27005" url="https://en.wikipedia.org/wiki?curid=27005" title="Smoke signal">
Smoke signal

The smoke signal is one of the oldest forms of long-distance communication. It is a form of visual communication used over long distance. In general smoke signals are used to transmit news, signal danger, or gather people to a common area.
History and usage.
In Ancient China, soldiers stationed along the Great Wall would alert each other of impending enemy attack by signaling from tower to tower. In this way, they were able to transmit a message as far away as in just a few hours.
Polybius, a Greek historian, devised a more complex system of alphabetical smoke signals around 150 BC, which converted Greek alphabetic characters into numeric characters. It enabled messages to be easily signaled by holding sets of torches in pairs. This idea, known as the "Polybius square", also lends itself to cryptography and steganography. This cryptographic concept has been used with Japanese Hiragana and the Germans in the later years of the First World War.
The North American indigenous peoples also communicated via smoke signal. Each tribe had its own signaling system and understanding. A signaler started a fire on an elevation typically using damp grass, which would cause a column of smoke to rise. The grass would be taken off as it dried and another bundle would be placed on the fire. Reputedly the location of the smoke along the incline conveyed a meaning. If it came from halfway up the hill, this would signify all was well, but from the top of the hill it would signify danger.
Smoke signals remain in use today. In Rome, the College of Cardinals uses smoke signals to indicate the selection of a new Pope during a papal conclave. Eligible cardinals conduct a secret ballot until someone receives a vote of two-thirds plus one. The ballots are burned after each vote. Black smoke indicates a failed ballot, while white smoke means a new Pope has been elected.
Examples.
Native Americans.
Lewis and Clark's journals cite several occasions when they adopted the Native American method of setting the plains on fire to communicate the presence of their party or their desire to meet with local tribes.
Yámana.
Yámanas used fire to send messages by smoke signals, for instance if a whale drifted ashore. The large amount of meat required notification of many people, so that it would not decay. They might also have used smoke signals on other occasions, thus it is possible that Magellan saw such fires (which inspired him to name the landscape Tierra del Fuego) but he may have seen the smoke or lights of natural phenomena.
Noon Gun.
Noon Gun time signalling was used to set marine chronometers in Table Bay.
Aboriginal Australians.
Aboriginal Australians in the Western Desert would send up smoke to notify others of their presence, particularly when entering lands which were not their own. 'Putting up a smoke' would often result in nearby individuals or groups replying with their own signals.

</doc>
<doc id="27006" url="https://en.wikipedia.org/wiki?curid=27006" title="Serendipity">
Serendipity

Serendipity means a "fortunate happenstance" or "pleasant surprise". It was coined by Horace Walpole in 1754. In a letter he wrote to a friend, Walpole explained an unexpected discovery he had made by reference to a Persian fairy tale, "The Three Princes of Serendip". The princes, he told his correspondent, were “always making discoveries, by accidents and sagacity, of things which they were not in quest of”.
The notion of serendipity is a common occurrence throughout the history of scientific innovation such as Alexander Fleming's accidental discovery of penicillin in 1928, the invention of the microwave oven by Percy Spencer in 1945, and the invention of the Post-it note by Spencer Silver in 1968.
The word has been voted one of the ten English words hardest to translate in June 2004 by a British translation company. However, due to its sociological use, the word has since been exported into many other languages.
Etymology.
The first noted use of "serendipity" (meaning pleasant surprise) in the English language was by Horace Walpole (1717–1797). In a letter to Horace Mann (dated 28 January 1754) he said he formed it from the Persian fairy tale "The Three Princes of Serendip", whose heroes "were always making discoveries, by accidents and sagacity, of things they were not in quest of". The name stems from "Serendip", an old name for Sri Lanka (aka Ceylon), from Tamil "Ceralamdivu", Sanskrit "Simhaladvipa" and Persian "Sarandīp" (). Parts of Sri Lanka were under the rule of Tamil kings for extended periods of time in history. Kings of Kerala, India (Cheranadu), were called Ceran Kings and "divu", "tivu" or "dheep", which means island. The island belonging to the Chera King was called "Cherandeep", hence "Sarandib" by Arab traders.
The structure of serendipity.
Serendipity is not just a matter of a random event, nor can it be taken simply as a synonym for "a happy accident" (Ferguson, 1999; Khan, 1999), "finding out things without being searching for them" (Austin, 2003), or "a pleasant surprise" (Tolson, 2004).
The "New Oxford Dictionary of English" defines serendipity as the occurrence and development of events by chance in a satisfactory or beneficial way, understanding the chance as any event that takes place in the absence of any obvious project (randomly or accidentally), which is not relevant to any present need, or in which the cause is unknown.
Innovations presented as examples of serendipity have an important characteristic: they were made by individuals able to "see bridges where others saw holes" and connect events creatively, based on the perception of a significant link.
The chance is an event, serendipity a capacity. The Nobel Prize laureate Paul Flory suggests that significant inventions are not mere accidents.
Serendipity and scientific discoveries.
The serendipitous can play an important role in the search for truth, but is often ignored in the scientific literature because of traditional scientific behavior and scientific thinking based on logic and predictability.
Successful researchers can observe scientific results with careful attention to analyzing a phenomenon under the most diverse and different perspectives. They can question themselves on assumptions that do not fit with empirical observations. Realizing that serendipitous events can generate important research ideas, these researchers recognize and appreciate the unexpected, encouraging their assistants to observe and discuss unexpected events.
Serendipity can be achieved in groups where a 'critical mass' of multidisciplinary scientists work together in an environment that fosters communication, establishing the idea that the work and the interest of a researcher can be shared with others who may find a new application for new knowledge.
Various thinkers discuss the role that luck can play in science. One aspect of Walpole's original definition of serendipity, often missed in modern discussions of the word, is the need for an individual to be "sagacious" enough to link together apparently innocuous facts in order to come to a valuable conclusion. Indeed, the scientific method, and the scientists themselves, can be prepared in many other ways to harness luck and make discoveries.
Business and strategy.
M. E. Graebner describes serendipitous value in the context of the acquisition of a business as "windfalls that were not anticipated by the buyer prior to the deal": i.e., unexpected advantages or benefits incurred due to positive synergy effects of the merger.
Ikujiro Nonaka points out that the serendipitous quality of innovation is highly recognized by managers and links the success of Japanese enterprises to their ability to create knowledge not by processing information but rather by "tapping the tacit and often highly subjective insights, intuitions, and hunches of individual employees and making those insights available for testing and use by the company as a whole".
Serendipity is postulated by Napier and Vuong (2013) as a 'strategic advantage' with which a firm can tap its potential creativity.
Serendipity is a key concept in competitive intelligence because it is one of the tools for avoiding blind spots (see Blindspots analysis).
Uses.
Serendipity is used as a sociological method in Anselm L. Strauss' and Barney G. Glaser's Grounded Theory, building on ideas by sociologist Robert K. Merton, who in "Social Theory and Social Structure" (1949) referred to the "serendipity pattern" as the fairly common experience of observing an unanticipated, anomalous and strategic datum which becomes the occasion for developing a new theory or for extending an existing theory. Robert K. Merton also coauthored (with Elinor Barber) "The Travels and Adventures of Serendipity" which traces the origins and uses of the word "serendipity" since it was coined. The book is "a study in sociological semantics and the sociology of science", as the subtitle of the book declares. It further develops the idea of serendipity as scientific "method" (as juxtaposed with purposeful discovery by experiment or retrospective prophecy).
Related terms.
William Boyd coined the term zemblanity to mean somewhat the opposite of serendipity: "making unhappy, unlucky and expected discoveries occurring by design". A zemblanity is, effectively, an "unpleasant unsurprise". It derives from Novaya Zemlya (or Nova Zembla), a cold, barren land with many features opposite to the lush Sri Lanka (Serendip). On this island Willem Barents and his crew were stranded while searching for a new route to the east.
Bahramdipity is derived directly from Bahram Gur as characterized in the "The Three Princes of Serendip". It describes the "suppression" of serendipitous discoveries or research results by powerful individuals.(b) Sommer, Toby J. "Bahramdipity and Nulltiple Scientific Discoveries," "Science and Engineering Ethics", 2001, "7"(1), 77–104.</ref>

</doc>
<doc id="27007" url="https://en.wikipedia.org/wiki?curid=27007" title="Samuel Morse">
Samuel Morse

Samuel Finley Breese Morse (April 27, 1791 – April 2, 1872) was an American painter and inventor. After having established his reputation as a portrait painter, in his middle age Morse contributed to the invention of a single-wire telegraph system based on European telegraphs. He was a co-developer of the Morse code, and helped to develop the commercial use of telegraphy.
Birth and education.
Samuel F. B. Morse was born in Charlestown, Massachusetts, the first child of the pastor Jedidiah Morse (1761–1826), who was also a geographer, and his wife Elizabeth Ann Finley Breese (1766–1828). His father was a great preacher of the Calvinist faith and supporter of the American Federalist party. He thought it helped preserve Puritan traditions (strict observance of Sabbath, among other things), and believed in the Federalist support of an alliance with Britain and a strong central government. Morse strongly believed in education within a Federalist framework, alongside the instillation of Calvinist virtues, morals and prayers for his first son.
After attending Phillips Academy in Andover, Massachusetts, Samuel Morse went on to Yale College to receive instruction in the subjects of religious philosophy, mathematics and science of horses. While at Yale, he attended lectures on electricity from Benjamin Silliman and Jeremiah Day, and was a member of the Society of Brothers in Unity. He supported himself by painting. In 1810, he graduated from Yale with Phi Beta Kappa honors.
Painting.
Morse expressed some of his Calvinist beliefs in his painting, "Landing of the Pilgrims", through the depiction of simple clothing as well as the people's austere facial features. His image captured the psychology of the Federalists; Calvinists from England brought to North America ideas of religion and government, thus linking the two countries. This work attracted the attention of the notable artist Washington Allston. Allston wanted Morse to accompany him to England to meet the artist Benjamin West. Allston arranged — with Morse's father — a three-year stay for painting study in England. The two men set sail aboard the "Lybia" on July 15, 1811.
In England, Morse perfected his painting techniques under Allston's watchful eye; by the end of 1811, he gained admittance to the Royal Academy. At the Academy, he was moved by the art of the Renaissance and paid close attention to the works of Michelangelo and Raphael. After observing and practicing life drawing and absorbing its anatomical demands, the young artist produced his masterpiece, the "Dying Hercules". (He first made a sculpture as a study for the painting.)
To some, the "Dying Hercules" seemed to represent a political statement against the British and also the American Federalists. The muscles symbolized the strength of the young and vibrant United States versus the British and British-American supporters. During Morse's time in Britain, the Americans and British were engaged in the War of 1812. Both societies were conflicted over loyalties. Anti-Federalist Americans aligned themselves with the French, abhorred the British, and believed a strong central government to be inherently dangerous to democracy.
As the war raged on, Morse's letters to his parents became more anti-Federalist in tone. In one such letter, Morse wrote:
"I assert that the Federalists in the Northern States have done more injury to their country by their violent opposition measures than a French alliance could. Their proceedings are copied into the English papers, read before Parliament, and circulated through their country, and what do they say of them... they call them cowards, a base set, say they are traitors to their country and ought to be hanged like traitors."
Although Jedidiah Morse did not change Samuel's political views, he continued as an influence. Critics believe that the elder Morse's Calvinist ideas are integral to Morse's "Judgment of Jupiter," another significant work completed in England. Jupiter is shown in a cloud, accompanied by his eagle, with his hand spread above the parties and he is pronouncing judgment. Marpessa, with an expression of compunction and shame, is throwing herself into the arms of her husband. Idas, who tenderly loved Marpessa, is eagerly rushing forward to receive her, while Apollo stares with surprise.
Critics have suggested that Jupiter represents God's omnipotence — watching every move that is made. Some call the portrait a moral teaching by Morse on infidelity. Although Marpessa fell victim, she realized that her eternal salvation was important and desisted from her wicked ways. Apollo shows no remorse for what he did, but stands with a puzzled look. Many American paintings throughout the early nineteenth century had religious themes, and Morse was an early exemplar of this. "Judgment of Jupiter" allowed Morse to express his support of Anti-Federalism while maintaining his strong spiritual convictions. Benjamin West sought to present the "Jupiter" at another Royal Academy exhibition, but Morse's time had run out. He left England on August 21, 1815, to return to the United States and begin his full-time career as a painter.
The decade 1815–1825 marked significant growth in Morse's work, as he sought to capture the essence of America's culture and life. He painted the Federalist former President John Adams (1816). The Federalists and Anti-Federalists clashed over Dartmouth College. Morse painted portraits of Francis Brown — the college's president — and Judge Woodward (1817), who was involved in bringing the Dartmouth case before the U.S. Supreme Court.
Morse also sought commissions among the elite of Charleston, South Carolina. Morse's 1818 painting of Mrs. Emma Quash symbolized the opulence of Charleston. The young artist was doing well for himself. Between 1819 and 1821, Morse went through great changes in his life, including a decline in commissions due to the Panic of 1819. Unable to stop the rift within Calvinism, his father was forced to resign from his ministerial position, which he had held for three decades. The new branch that formed was the Congregational Unitarians, Morse considered them to be anti-Federalists, as their beliefs were related to religious salvation.
Although Samuel Morse respected his father's religious opinions, he sympathized with the Unitarians. Among the converts to Unitarianism were the prominent Pickerings of Portsmouth, New Hampshire, whom Morse had painted. Some critics thought his sympathies represented his own anti-Federalism. Morse was commissioned to paint President James Monroe in 1820. He embodied Jeffersonian democracy by favoring the common man over the aristocrat.
Morse had moved to New Haven. His commissions for the "Hall of Congress" (1821) and a portrait of the Marquis de Lafayette (1825) engaged his sense of democratic nationalism. The "Hall of Congress" was designed to capitalize on the success of François-Marius Granet's "The Capuchin Chapel in Rome," which toured the United States extensively throughout the 1820s, attracting audiences willing to pay the 25-cent admission fee.
The artist chose to paint the House of Representatives, in a similar way, with careful attention to architecture and dramatic lighting. He also wished to select a uniquely American topic that would bring glory to the young nation. His subject did just that, showing American democracy in action. He traveled to Washington D.C. to draw the architecture of the new Capitol, and placed eighty individuals within the painting. He chose to portray a night scene, balancing the architecture of the Rotunda with the figures, and using lamplight to highlight the work. Pairs of people, those who stood alone, individuals bent over their desks working, were each painted simply but with faces of character. Morse chose nighttime to convey that Congress' dedication to the principles of democracy transcended day.
"The Hall of Congress" failed to draw a crowd when exhibited in New York City in 1821. By contrast, John Trumbull's "Declaration of Independence" had won popular acclaim the previous year. Viewers may have felt that the architecture of "The Hall of Congress" overshadows the individuals, making it hard to appreciate the drama of what was happening.
Morse was honored to paint the Marquis de Lafayette, the leading French supporter of the American Revolution. He felt compelled to paint a grand portrait of the man who helped to establish a free and independent America. He features Lafayette against a magnificent sunset. He has positioned Lafayette to the right of three pedestals: one has a bust of Benjamin Franklin, another of George Washington, and the third seems reserved for Lafayette. A peaceful woodland landscape below him symbolized American tranquility and prosperity as it approached the age of fifty. The developing friendship between Morse and Lafayette, and their discussions of the Revolutionary War, affected the artist after his return to New York City.
In 1826 he helped found the National Academy of Design in New York City. He served as the Academy's President from 1826 to 1845 and again from 1861 to 1862.
From 1830 to 1832, Morse traveled and studied in Europe to improve his painting skills, visiting Italy, Switzerland and France. During his time in Paris, he developed a friendship with the writer James Fennimore Cooper. As a project, he painted miniature copies of 38 of the Louvre's famous paintings on a single canvas (6 ft. x 9 ft), which he entitled "The Gallery of the Louvre." He completed the work upon his return to the United States.
On a subsequent visit to Paris in 1839, Morse met Louis Daguerre. He became interested in the latter's daguerreotype—the first practical means of photography. Morse wrote a letter to the "New York Observer" describing the invention, which was published widely in the American press and provided a broad awareness of the new technology.
Some of Morse's paintings and sculptures are on display at his Locust Grove estate in Poughkeepsie, New York.
Telegraph.
As noted, in 1825 New York City had commissioned Morse to paint a portrait of Lafayette in Washington, DC. While Morse was painting, a horse messenger delivered a letter from his father that read, "Your dear wife is convalescent". The next day he received a letter from his father detailing his wife's sudden death. Morse immediately left Washington for his home at New Haven, leaving the portrait of Lafayette unfinished. By the time he arrived, his wife had already been buried. Heartbroken that for days he was unaware of his wife's failing health and her death, he decided to explore a means of rapid long distance communication.
While returning by ship from Europe in 1832, Morse encountered Charles Thomas Jackson of Boston, a man who was well schooled in electromagnetism. Witnessing various experiments with Jackson's electromagnet, Morse developed the concept of a single-wire telegraph. He set aside his painting, "The Gallery of the Louvre". The original Morse telegraph, submitted with his patent application, is part of the collections of the National Museum of American History at the Smithsonian Institution. In time the Morse code, which he developed, would become the primary language of telegraphy in the world. It is still the standard for rhythmic transmission of data.
Meanwhile, William Cooke and Professor Charles Wheatstone had learned of the Wilhelm Weber and Carl Gauss electromagnetic telegraph in 1833. They had reached the stage of launching a commercial telegraph prior to Morse, despite starting later. In England, Cooke became fascinated by electrical telegraphy in 1836, four years after Morse. Aided by his greater financial resources, Cooke abandoned his primary subject of anatomy and built a small electrical telegraph within three weeks. Wheatstone also was experimenting with telegraphy and (most importantly) understood that a single large battery would not carry a telegraphic signal over long distances. He theorized that numerous small batteries were far more successful and efficient in this task. (Wheatstone was building on the primary research of Joseph Henry, an American physicist). Cooke and Wheatstone formed a partnership and patented the electrical telegraph in May 1837, and within a short time had provided the Great Western Railway with a stretch of telegraph. However, within a few years, Cooke and Wheatstone's multiple-wire signaling method would be overtaken by Morse's cheaper method.
In an 1848 letter to a friend, Morse describes how vigorously he fought to be called the sole inventor of the electromagnetic telegraph despite the previous inventions.
Relays.
Morse encountered the problem of getting a telegraphic signal to carry over more than a few hundred yards of wire. His breakthrough came from the insights of Professor Leonard Gale, who taught chemistry at New York University (he was a personal friend of Joseph Henry). With Gale's help, Morse introduced extra circuits or relays at frequent intervals, and was soon able to send a message through ten miles (16 km) of wire. This was the great breakthrough he had been seeking. Morse and Gale were soon joined by Alfred Vail, an enthusiastic young man with excellent skills, insights and money.
At the Speedwell Ironworks in Morristown, New Jersey on January 11, 1838, Morse and Vail made the first public demonstration of the electric telegraph. Although Morse and Alfred Vail had done most of the research and development in the ironworks facilities, they chose a nearby factory house as the demonstration site. Without the repeater, the range of the telegraph was limited to two miles (3 km), and the inventors had pulled two miles (3 km) of wires inside the factory house through an elaborate scheme. The first public transmission, with the message, "A patient waiter is no loser", was witnessed by a mostly local crowd.
Morse traveled to Washington, D.C. in 1838 seeking federal sponsorship for a telegraph line but was not successful. He went to Europe, seeking both sponsorship and patents, but in London discovered that Cooke and Wheatstone had already established priority. After his return to the US, Morse finally gained financial backing by Maine congressman Francis Ormand Jonathan Smith.
Federal support.
Morse made his last trip to Washington, D.C., in December 1842, stringing "wires between two committee rooms in the Capitol, and sent messages back and forth" to demonstrate his telegraph system. Congress appropriated $30,000 in 1843 for construction of an experimental telegraph line between Washington, D.C., and Baltimore along the right-of-way of the Baltimore and Ohio Railroad. An impressive demonstration occurred on May 1, 1844, when news of the Whig Party's nomination of Henry Clay for U.S. President was telegraphed from the party's convention in Baltimore to the Capitol Building in Washington.
On May 24, 1844, the line was officially opened as Morse sent the now-famous words, "What hath God wrought," from the Supreme Court chamber in the basement of the U.S. Capitol building in Washington, D.C., to the B&O's Mount Clare Station in Baltimore. Annie Ellsworth chose these words from the Bible (Numbers 23:23); her father, U.S. Patent Commissioner Henry Leavitt Ellsworth, had championed Morse's invention and secured early funding for it. His telegraph could transmit thirty characters per minute.
In May 1845 the Magnetic Telegraph Company was formed in order to build telegraph lines from New York City toward Philadelphia, Boston, Buffalo, New York and the Mississippi.
Morse at one time adopted Wheatstone and Carl August von Steinheil's idea of broadcasting an electrical telegraph signal through a body of water or down steel railroad tracks or anything conductive. He went to great lengths to win a lawsuit for the right to be called "inventor of the telegraph", and promoted himself as being an inventor. But, Alfred Vail also played an important role in the development of the Morse code, which was based on earlier codes for the electromagnetic telegraph.
Patent.
Morse received a patent for the telegraph in 1847, at the old Beylerbeyi Palace (the present Beylerbeyi Palace was built in 1861–1865 on the same location) in Istanbul, which was issued by Sultan Abdülmecid, who personally tested the new invention. He was elected an Associate Fellow of the American Academy of Arts and Sciences in 1849. The original patent went to the Breese side of the family after the death of Samuel Morse.
In the 1850s, Morse went to Copenhagen and visited the Thorvaldsens Museum, where the sculptor's grave is in the inner courtyard. He was received by King Frederick VII, who decorated him with the Order of the Dannebrog. Morse expressed his wish to donate his portrait from 1830 to the king. The Thorvaldsen portrait today belongs to Margrethe II of Denmark.
The Morse telegraphic apparatus was officially adopted as the standard for European telegraphy in 1851. Only the United Kingdom (with its extensive overseas empire) kept the needle telegraph of Cooke and Wheatstone.
In 1858, Morse introduced wired communication to Latin America when he established a telegraph system in Puerto Rico, then a Spanish Colony. Morse's oldest daughter, Susan Walker Morse (1819–1885), would often visit her uncle Charles Pickering Walker, who owned the Hacienda Concordia in the town of Guayama. During one of her visits, she met Edward Lind, a Danish merchant who worked in his brother-in-law's Hacienda La Henriqueta in the town of Arroyo. They later married. Lind purchased the Hacienda from his sister when she became a widow. Morse, who often spent his winters at the Hacienda with his daughter and son-in-law, set a two-mile telegraph line connecting his son-in-law's Hacienda to their house in Arroyo. The line was inaugurated on March 1, 1859, in a ceremony flanked by the Spanish and American flags. The first words transmitted by Samuel Morse that day in Puerto Rico were:
"Puerto Rico, beautiful jewel! When you are linked with the other jewels of the Antilles in the necklace of the world's telegraph, yours will not shine less brilliantly in the crown of your Queen!"
There is an argument amongst historians that Morse may have received the idea of a plausible telegraph from Harrison Gray Dyar some eighteen years earlier than his patent.
Political views.
Morse was a leader in the anti-Catholic and anti-immigration movement of the mid-19th century. In 1836, he ran unsuccessfully for mayor of New York under the anti-immigrant Nativist Party's banner, receiving only 1496 votes. When Morse visited Rome, he allegedly refused to take his hat off in the presence of the Pope.
Morse worked to unite Protestants against Catholic institutions (including schools), wanted to forbid Catholics from holding public office, and promoted changing immigration laws to limit immigration from Catholic countries. On this topic, he wrote, "We must first stop the leak in the ship through which muddy waters from without threaten to sink us."
He wrote numerous letters to the New York "Observer" (his brother Sidney was the editor at the time) urging people to fight the perceived Catholic menace. These were widely reprinted in other newspapers. Among other claims, he believed that the Austrian government and Catholic aid organizations were subsidizing Catholic immigration to the United States in order to gain control of the country.
In his "Foreign Conspiracy Against the Liberties of the United States", Morse wrote: 
Surely American Protestants, freemen, have discernment enough to discover beneath them the cloven foot of this subtle foreign heresy. They will see that Popery is now, what it has ever been, a system of the darkest political intrigue and despotism, cloaking itself to avoid attack under the sacred name of religion. They will be deeply impressed with the truth, that Popery is a political as well as a religious system; that in this respect it differs totally from all other sects, from all other forms of religion in the country.
In the 1850s, Morse became well known as a defender of slavery, considering it to be sanctioned by God. This was a position held by many Southerners and others. In his treatise "An Argument on the Ethical Position of Slavery," he wrote:
Marriages.
Morse married Lucretia Pickering Walker on September 29, 1818, in Concord, New Hampshire. She died on February 7, 1825, shortly after the birth of their third child (Susan b. 1819, Charles b. 1823, James b. 1825). He married his second wife, Sarah Elizabeth Griswold on August 10, 1848 in Utica, New York and had four children (Samuel b. 1849, Cornelia b. 1851, William b. 1853, Edward b. 1857).
Later years.
Litigation over telegraph patent.
In the United States, Morse held his telegraph patent for many years, but it was both ignored and contested. In 1853 "The Telegraph Patent case--O'Reilly v. Morse--" came before the U.S. Supreme Court where, after very lengthy investigation, Chief Justice Roger B. Taney ruled that Morse had been the first to combine the battery, electromagnetism, the electromagnet and the correct battery configuration into a workable practical telegraph. However, in spite of this clear ruling, Morse still received no official recognition from the United States government.
The Supreme Court did not accept all of Morse's claims. The "O'Reilly v. Morse" case has become widely known among patent lawyers because the Supreme Court explicitly denied Morse's claim 8 for any and all use of the electromagnetic force for purposes of transmitting intelligible signals to any distance.
The Supreme Court sustained, however, Morse's claim to such telecommunication when effectuated by means of Morse's inventive "repeater" apparatus. This was an electrical circuit in which a cascade of many sets comprising a relay and a battery were connected in series, so that when each relay closed, it closed a circuit to cause the next battery to power the succeeding relay, as suggested in the accompanying figure. This caused Morse's signal to pass along the cascade without degrading into noise as its amplitude decreased with the distance traveled. (Each time the amplitude of the signal approaches the noise level, the repeater effect, a nonlinear amplifier boosts the signal amplitude well above the noise level.)
The effect of Morse's use of "repeaters" is shown in the figure at the left, a graph of signal amplitude (with noise) vs. distance. The signal decays between repeaters, but Morse restores the signal to a predetermined level before it falls into the noise. That permits the message to be sent to great distances, which was previously not feasible.
The Supreme Court thus held that Morse could properly claim a patent monopoly on the system or process of transmitting signals at any distance by means of the repeater circuitry indicated above, but he could not properly claim a monopoly over any and all uses of electromagnetic force to transmit signals. The apparatus limitation in the former type of claim limited the patent monopoly to what Morse taught and gave the world. The lack of that limitation in the latter type of claim (i.e., claim 8) both gave Morse more than was commensurate with what he had contributed to society and discouraged the inventive efforts of others who might come up with different and/or better ways to send signals at a distance using the electromagnetic force.
The problem that Morse faced and how he solved it is discussed in more detail in the article O'Reilly v. Morse. In summary, the solution, as the Supreme Court stated, was the repeater apparatus described in the preceding paragraphs.
The importance of this legal precedent in patent law cannot be overstated, as it became the foundation of the law governing the eligibility of computer program implemented inventions (as well as inventions implementing natural laws) to be granted patents.
Foreign recognition.
Assisted by the American ambassador in Paris, the governments of Europe were approached about their long neglect of Morse while their countries were using his invention. There was a widespread recognition that something must be done, and in 1858 Morse was awarded the sum of 400,000 French francs (equivalent to about $80,000 at the time) by the governments of France, Austria, Belgium, the Netherlands, Piedmont, Russia, Sweden, Tuscany and Turkey, each of which contributed a share according to the number of Morse instruments in use in each country. In 1858, he was also elected a foreign member of the Royal Swedish Academy of Sciences.
Transatlantic cable.
Morse lent his support to Cyrus West Field's ambitious plan to construct the first transoceanic telegraph line. Morse had experimented with underwater telegraph circuits since 1842. He invested $10,000 in Field's Atlantic Telegraph Company, took a seat on its board of directors, and was appointed honorary "Electrician". In 1856, Morse traveled to London to help Charles Tilston Bright and Edward Whitehouse test a 2,000-mile-length of spooled cable.
After the first two cable-laying attempts failed, Field reorganized the project, removing Morse from direct involvement. Though the cable broke three times during the third attempt, it was successfully repaired, and the first transatlantic telegraph messages were sent in 1858. The cable failed after just three months of use. Though Field had to wait out the Civil War, the cable laid in 1866 proved more durable, and the era of reliable transatlantic telegraph service had begun.
In addition to the telegraph, Morse invented a marble-cutting machine that could carve three-dimensional sculptures in marble or stone. He could not patent it, however, because of an existing 1820 Thomas Blanchard design.
Last years and death.
Samuel Morse gave large sums to charity. He also became interested in the relationship of science and religion and provided the funds to establish a lectureship on "the relation of the Bible to the Sciences". Though he was rarely awarded any royalties for the later uses and implementations of his inventions, he was able to live comfortably.
He died in New York City on April 2, 1872, and was interred at Green-Wood Cemetery in Brooklyn, New York. By the time of his death, his estate was valued at some $500,000 ($ today).
Honors and awards.
Morse was elected a member of the American Antiquarian Society in 1815.
Despite honors and financial awards received from foreign countries there was no such recognition in the U.S. until he neared the end of his life, when on June 10, 1871 a bronze statue of Samuel Morse was unveiled in Central Park, New York City. An engraved portrait of Morse appeared on the reverse side of the United States two-dollar bill silver certificate series of 1896. He was depicted along with Robert Fulton. An example can be seen on the website of the Federal Reserve Bank of San Francisco's website in their "American Currency Exhibit":
A blue plaque was erected to commemorate him at 141 Cleveland Street, London, where he lived from 1812 to 1815.
According to his "The New York Times" obituary published on April 3, 1872, Morse received respectively the decoration of the Atiq Nishan-i-Iftikhar (English: Order of Glory) medal on wearer's right depicted in photo of Morse with medals, set in diamonds, from Sultan Abdülmecid of Turkey (c.1847), a "golden snuff box containing the Prussian gold medal for scientific merit" from the King of Prussia (1851); the "Great Gold Medal of Arts and Sciences" from the King of Württemberg (1852); and the "Great Golden Medal of Science and Arts" from Emperor of Austria (1855); a cross of Chevalier in the Légion d'honneur from the Emperor of France; the "Cross of a Knight" of the Order of the Dannebrog from the King of Denmark (1856); the Cross of Knight Commander of the Order of Isabella the Catholic, from the Queen of Spain, besides being elected member of innumerable scientific and art societies in this States and other countries. Other awards include Order of the Tower and Sword from the kingdom of Portugal (1860); and Italy conferred on him the insignia of chevalier of the Order of Saints Maurice and Lazarus in 1864. Morse's telegraph was recognized as an IEEE Milestone in 1988.
On April 1, 2012, Google announced the release of "Gmail Tap", an April Fools' Day joke that allowed users to use Morse Code to send text from their mobile phones. Morse's great-great-grandnephew Reed Morse—a Google engineer—was instrumental in the prank, which became a real product.

</doc>
<doc id="27008" url="https://en.wikipedia.org/wiki?curid=27008" title="Ship">
Ship

A ship is a large buoyant watercraft. Ships are generally distinguished from boats based on size, shape and cargo or passenger capacity. Ships are used on lakes, seas, rivers, and oceans for a variety of activities, such as the transport of people or goods, fishing, entertainment, public safety, and warfare. Historically, a "ship" was a sailing vessel with at least three square-rigged masts and a full bowsprit. 
In armed conflict and in daily life, ships have become an integral part of modern commercial and military systems. Fishing boats are used by millions of fishermen throughout the world. Military forces operate vessels for naval warfare and to transport and support forces ashore. Commercial vessels, nearly 35,000 in number, carried 7.4 billion tons of cargo in 2007. As of 2011, there are about 104,304 ships with IMO numbers in the world.
Ships were always a key in history's great explorations and scientific and technological development. Navigators such as Zheng He spread such inventions as the compass and gunpowder. Ships have been used for such purposes as colonization and the slave trade, and have served scientific, cultural, and humanitarian needs. After the 16th century, new crops that had come from and to the Americas via the European seafarers significantly contributed to the world population growth. Ship transport has shaped the world's economy into today's energy-intensive pattern.
Nomenclature.
Ships can usually be distinguished from boats based on size and the ship's ability to operate independently for extended periods. A commonly used rule of thumb is that if one vessel can carry another, the larger of the two is a ship. Dinghies are carried on sailing yachts as small as , clearly not ships; this rule of thumb is not foolproof.
In the age of sail, a "ship" was a sailing vessel with at least three square-rigged masts and a full bowsprit; other types of vessel were also defined by their sailplan, e.g. barque, brigantine, etc.
A number of large vessels are usually referred to as boats. Submarines are a prime example. Other types of large vessel which are traditionally called boats are Great Lakes freighters, riverboats, and ferryboats. Though large enough to carry their own boats and heavy cargoes, these vessels are designed for operation on inland or protected coastal waters.
In most maritime traditions ships have individual names, and modern ships may belong to a ship class often named after its first ship. In English, a ship is traditionally referred to as "she", even if named after a man, but this is not universal usage; some journalistic style guides advise using "it" as referring to ships with female pronouns can be seen as offensive and outdated.
History.
Prehistory and antiquity.
The first known vessels date back about 10,000 years ago, but could not be described as ships. The first navigators began to use animal skins or woven fabrics as sails. Affixed to the top of a pole set upright in a boat, these sails gave early ships range. This allowed men to explore widely, allowing for the settlement of Oceania for example (about 3,000 years ago).
By around 3000 BC, Ancient Egyptians knew how to assemble wooden planks into a hull. They used woven straps to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. The Greek historian and geographer Agatharchides had documented ship-faring among the early Egyptians: ""During the prosperous period of the Old Kingdom, between the 30th and 25th centuries B. C., the river-routes were kept in order, and Egyptian ships sailed the Red Sea as far as the myrrh-country."" Sneferu's ancient cedar wood ship Praise of the Two Lands is the first reference recorded (2613 BC) to a ship being referred to by name.
The ancient Egyptians were perfectly at ease building sailboats. A remarkable example of their shipbuilding skills was the Khufu ship, a vessel in length entombed at the foot of the Great Pyramid of Giza around 2500 BC and found intact in 1954.
It is known that ancient Nubia/Axum traded with India, and there is evidence that ships from Northeast Africa may have sailed back and forth between India/Sri Lanka and Nubia trading goods and even to Persia, Himyar and Rome. Aksum was known by the Greeks for having seaports for ships from Greece and Yemen.
Elsewhere in Northeast Africa, the Periplus of the Red Sea reports that Somalis, through their northern ports such as Zeila and Berbera, were trading frankincense and other items with the inhabitants of the Arabian Peninsula well before the arrival of Islam as well as with then Roman-controlled Egypt.
A panel found at Mohenjodaro depicted a sailing craft. Vessels were of many types; their construction is vividly described in the Yukti Kalpa Taru, an ancient Indian text on shipbuilding. This treatise gives a technical exposition on the techniques of shipbuilding. It sets forth minute details about the various types of ships, their sizes, and the materials from which they were built. The Yukti Kalpa Taru sums up in a condensed form all the available information. The Yukti Kalpa Taru gives sufficient information and dates to prove that, in ancient times, Indian shipbuilders had a good knowledge of the materials which were used in building ships. In addition to describing the qualities of the different types of wood and their suitability for shipbuilding, the Yukti Kalpa Taru gives an elaborate classification of ships based on their size.
The oldest discovered sea faring hulled boat is the Egyptian Uluburun shipwreck off the coast of Turkey, dating back to 1300 BC.
The Phoenicians, the first to sail completely around Africa, and Greeks gradually mastered navigation at sea aboard triremes, exploring and colonizing the Mediterranean via ship. Around 340 BC, the Greek navigator Pytheas of Massalia ventured from Greece to Western Europe and Great Britain. In the course of the 2nd century BC, Rome went on to destroy Carthage and subdue the Hellenistic kingdoms of the eastern Mediterranean, achieving complete mastery of the inland sea, that they called "Mare Nostrum". The monsoon wind system of the Indian Ocean was first sailed by Greek navigator Eudoxus of Cyzicus in 118 BC.
In China, by the time of the Zhou Dynasty ship technologies such as stern mounted rudders were developed, and by the Han Dynasty, a well kept naval fleet was an integral part of the military. Ship technology advanced to the point where by the medieval period, water tight compartments were developed.
The Swahili people had various extensive trading ports dotting the coast of medieval East Africa and Great Zimbabwe had extensive trading contacts with Central Africa, and likely also imported goods brought to Africa through the Southeast African shore trade of Kilwa in modern-day Tanzania.
It is known by historians that at its height the Mali Empire built a large naval fleet under Emperor Mansa Musa in the late 13th and early 14th century. Arabic sources describe what some consider to be visits to the New World by a Mali fleet in 1311.
Before the introduction of the compass, celestial navigation was the main method for navigation at sea. In China, early versions of the magnetic compass were being developed and used in navigation between 1040 and 1117. The true mariner's compass, using a pivoting needle in a dry box, was developed in Europe no later than 1300.
Renaissance.
Until the Renaissance, navigational technology remained comparatively primitive. This absence of technology did not prevent some civilizations from becoming sea powers. Examples include the maritime republics of Genoa and Venice, Hanseatic League, and the Byzantine navy. The Vikings used their knarrs to explore North America, trade in the Baltic Sea and plunder many of the coastal regions of Western Europe.
Towards the end of the 14th century, ships like the carrack began to develop towers on the bow and stern. These towers decreased the vessel's stability, and in the 15th century, the caravel, designed by the Portuguese, based on the Arabic "qarib" which could sail closer to the wind, became more widely used. The towers were gradually replaced by the forecastle and sterncastle, as in the carrack "Santa María" of Christopher Columbus. This increased freeboard allowed another innovation: the freeing port, and the artillery associated with it.
In the 16th century, the use of freeboard and freeing ports became widespread on galleons. The English modified their vessels to maximize their firepower and demonstrated the effectiveness of their doctrine, in 1588, by defeating the Spanish Armada.
At this time, ships were developing in Asia in much the same way as Europe. Japan used defensive naval techniques in the Mongol invasions of Japan in 1281. It is likely that the Mongols of the time took advantage of both European and Asian shipbuilding techniques. During the 15th century, China's Ming Dynasty assembled one of the largest and most powerful naval fleets in the world for the diplomatic and power projection voyages of Zheng He. Elsewhere in Japan in the 15th century, one of the world's first iron-clads, "Tekkōsen" (鉄甲船), literally meaning "iron ships", was also developed. In Japan, during the Sengoku era from the fifteenth to 17th century, the great struggle for feudal supremacy was fought, in part, by coastal fleets of several hundred boats, including the atakebune. In Korea, in the early 15th century during the Joseon era, "Geobukseon"(거북선), was developed. The "turtle ship", as it was called is recognized as the first armored ship in the world.
During the Age of the Ajuran, the Somali sultanates and republics of Merca, Mogadishu, Barawa, Hobyo and their respective ports flourished, enjoying a lucrative foreign commerce with ships sailing to and coming from Arabia, India, Venetia, Persia, Egypt, Portugal and as far away as China. In the 16th century, Duarte Barbosa noted that many ships from the Kingdom of Cambaya in what is modern-day India sailed to Mogadishu with cloth and spices, for which they in return received gold, wax and ivory. Barbosa also highlighted the abundance of meat, wheat, barley, horses, and fruit on the coastal markets, which generated enormous wealth for the merchants.
Middle Age Swahili Kingdoms are known to have had trade port bullship and trade routes with the Islamic world and Asia and were described by Greek historians as "metropolises". Famous African trade ports such as Mombasa, Zanzibar, and Kilwa were known to Chinese sailors such as Zheng He and medieval Islamic historians such as the Berber Islamic voyager Abu Abdullah ibn Battua. In the 14th century AD, King Abubakari I, the brother of King Mansa Musa of the Mali Empire, is thought to have had a great armada of ships sitting on the coast of West Africa. This is corroborated by ibn Battuta himself who recalls several hundred Malian ships off the coast. This has led to great speculation, with historical evidence, that it is possible that Malian sailors may have reached the coast of Pre-Columbian America under the rule of Abubakari II, nearly two hundred years before Christopher Columbus and that black traders may have been in the Americas before Columbus.
Fifty years before Christopher Columbus, Chinese navigator Zheng He traveled the world at the head of what was for the time a huge armada. The largest of his ships had nine masts, were long and had a beam of . His fleet carried 30,000 men aboard 70 vessels, with the goal of bringing glory to the Chinese emperor.
At the same time Zheng He made his expedition, Portuguese explorer Gil Eanes sailed on a square-rigged caravel beyond Cape Bojador the end of what was then considered the known world opening the route to deep sea exploration, continental sea communication technology and the spherical earth principle.
The carrack and then the caravel were developed in Portugal. After Columbus, European exploration rapidly accelerated, and many new trade routes were established. In 1498, by reaching India, Vasco da Gama proved that the access to the Indian Ocean from the Atlantic was possible. These explorations in the Atlantic and Indian Oceans were soon followed by France, England and the Netherlands, who explored the Portuguese and Spanish trade routes into the Pacific Ocean, reaching Australia in 1606 and New Zealand in 1642. In the 17th century Dutch explorers such as Abel Tasman explored the coasts of Australia, while in the 18th century it was British explorer James Cook who mapped much of Polynesia.
Specialization and modernization.
Parallel to the development of warships, ships in service of marine fishery and trade also developed in the period between antiquity and the Renaissance. Still primarily a coastal endeavor, fishing is largely practiced by individuals with little other money using small boats.
Maritime trade was driven by the development of shipping companies with significant financial resources. Canal barges, towed by draft animals on an adjacent towpath, contended with the railway up to and past the early days of the industrial revolution. Flat-bottomed and flexible scow boats also became widely used for transporting small cargoes. Mercantile trade went hand-in-hand with exploration, self-financed by the commercial benefits of exploration.
During the first half of the 18th century, the French Navy began to develop a new type of vessel known as a ship of the line, featuring seventy-four guns. This type of ship became the backbone of all European fighting fleets. These ships were long and their construction required 2,800 oak trees and of rope; they carried a crew of about 800 sailors and soldiers.
During the 19th century the Royal Navy enforced a ban on the slave trade, acted to suppress piracy, and continued to map the world. A clipper was a very fast sailing ship of the 19th century. The clipper routes fell into commercial disuse with the introduction of steam ships with better fuel efficiency, and the opening of the Suez and Panama Canals.
Ship designs stayed fairly unchanged until the late 19th century. The industrial revolution, new mechanical methods of propulsion, and the ability to construct ships from metal triggered an explosion in ship design. Factors including the quest for more efficient ships, the end of long running and wasteful maritime conflicts, and the increased financial capacity of industrial powers created an avalanche of more specialized boats and ships. Ships built for entirely new functions, such as firefighting, rescue, and research, also began to appear.
In light of this, classification of vessels by type or function can be difficult. Even using very broad functional classifications such as fishery, trade, military, and exploration fails to classify most of the old ships. This difficulty is increased by the fact that the terms such as sloop and frigate are used by old and new ships alike, and often the modern vessels sometimes have little in common with their predecessors.
Today.
In 2007, the world's fleet included 34,882 commercial vessels with gross tonnage of more than 1,000 tons, totaling 1.04 billion tons. These ships carried 7.4 billion tons of cargo in 2006, a sum that grew by 8% over the previous year. In terms of tonnage, 39% of these ships are tankers, 26% are bulk carriers, 17% container ships and 15% were other types.
In 2002, there were 1,240 warships operating in the world, not counting small vessels such as patrol boats. The United States accounted for 3 million tons worth of these vessels, Russia 1.35 million tons, the United Kingdom 504,660 tons and China 402,830 tons. The 20th century saw many naval engagements during the two world wars, the Cold War, and the rise to power of naval forces of the two blocs. The world's major powers have recently used their naval power in cases such as the United Kingdom in the Falkland Islands and the United States in Iraq.
The size of the world's fishing fleet is more difficult to estimate. The largest of these are counted as commercial vessels, but the smallest are legion. Fishing vessels can be found in most seaside villages in the world. As of 2004, the United Nations Food and Agriculture Organization estimated 4 million fishing vessels were operating worldwide. The same study estimated that the world's 29 million fishermen caught of fish and shellfish that year.
Types of ships.
Because ships are constructed using the principles of naval architecture that require same structural components, their classification is based on their function such as suggested by Paulet and Presles., which requires modification of the components. The categories accepted in general by naval architects are:
Some of these are discussed in the following sections.
Inland and coastal boats.
Many types of boats are designed for inland and coastal waterways. These are the vessels that trade upon the lakes, rivers and canals.
Barges are a prime example of inland vessels. Flat-bottomed boats built to transport heavy goods, most barges are not self-propelled and need to be moved by tugboats towing or towboats pushing them. Barges towed along canals by draft animals on an adjacent towpath contended with the railway in the early industrial revolution but were out competed in the carriage of high value items because of the higher speed, falling costs, and route flexibility of rail transport.
Lake freighters, also called lakers, are cargo vessels that ply the Great Lakes. The most well-known is the , the latest major vessel to be wrecked on the Lakes. These vessels are traditionally called boats, not ships. Visiting ocean-going vessels are called "salties." Because of their additional beam, very large salties are never seen inland of the Saint Lawrence Seaway. Because the smallest of the Soo Locks is larger than any Seaway lock, salties that can pass through the Seaway may travel anywhere in the Great Lakes. Because of their deeper draft, salties may accept partial loads on the Great Lakes, "topping off" when they have exited the Seaway. Similarly, the largest lakers are confined to the Upper Lakes (Superior, Michigan, Huron, Erie) because they are too large to use the Seaway locks, beginning at the Welland Canal that bypasses the Niagara River.
Since the freshwater lakes are less corrosive to ships than the salt water of the oceans, lakers tend to last much longer than ocean freighters. Lakers older than 50 years are not unusual, and as of 2005, all were over 20 years of age.
The "SS St. Marys Challenger", built in 1906 as the "William P Snyder", was the oldest laker still working on the Lakes until its conversion into a barge starting in 2013. Similarly, the "E.M. Ford", built in 1898 as the "Presque Isle", was sailing the lakes 98 years later in 1996. As of 2007 the "Ford" was still afloat as a stationary transfer vessel at a riverside cement silo in Saginaw, Michigan.
Seagoing commercial vessels.
Commercial vessels or merchant ships can be divided into four broad categories: fishing, cargo ships, passenger ships, and special-purpose ships. Modern commercial vessels are typically powered by a single propeller driven by a diesel or, less usually, gas turbine engine., but until the mid-19th century they were predominantly square sail rigged. The fastest vessels may use pump-jet engines. Most commercial vessels have full hull-forms to maximize cargo capacity. Hulls are usually made of steel, although aluminum can be used on faster craft, and fiberglass on the smallest service vessels. Commercial vessels generally have a crew headed by a captain, with deck officers and marine engineers on larger vessels. Special-purpose vessels often have specialized crew if necessary, for example scientists aboard research vessels.
Fishing boats are generally small, often little more than but up to for a large tuna or whaling ship. Aboard a fish processing vessel, the catch can be made ready for market and sold more quickly once the ship makes port. Special purpose vessels have special gear. For example, trawlers have winches and arms, stern-trawlers have a rear ramp, and tuna seiners have skiffs. In 2004, of fish were caught in the marine capture fishery. Anchoveta represented the largest single catch at . That year, the top ten marine capture species also included Alaska pollock, Blue whiting, Skipjack tuna, Atlantic herring, Chub mackerel, Japanese anchovy, Chilean jack mackerel, Largehead hairtail, and Yellowfin tuna. Other species including salmon, shrimp, lobster, clams, squid and crab, are also commercially fished. Modern commercial fishermen use many methods. One is fishing by nets, such as purse seine, beach seine, lift nets, gillnets, or entangling nets. Another is trawling, including bottom trawl. Hooks and lines are used in methods like long-line fishing and hand-line fishing. Another method is the use of fishing trap.
Cargo ships transport dry and liquid cargo. Dry cargo can be transported in bulk by bulk carriers, packed directly onto a general cargo ship in break-bulk, packed in intermodal containers as aboard a container ship, or driven aboard as in roll-on roll-off ships. Liquid cargo is generally carried in bulk aboard tankers, such as oil tankers which may include both crude and finished products of oil, chemical tankers which may also carry vegetable oils other than chemicals and LPG/LNG tankers, although smaller shipments may be carried on container ships in tank containers.
Passenger ships range in size from small river ferries to very large cruise ships. This type of vessel includes ferries, which move passengers and vehicles on short trips; ocean liners, which carry passengers from one place to another; and cruise ships, which carry passengers on voyages undertaken for pleasure, visiting several places and with leisure activities on board, often returning them to the port of embarkation. Riverboats and inland ferries are specially designed to carry passengers, cargo, or both in the challenging river environment. Rivers present special hazards to vessels. They usually have varying water flows that alternately lead to high speed water flows or protruding rock hazards. Changing siltation patterns may cause the sudden appearance of shoal waters, and often floating or sunken logs and trees (called snags) can endanger the hulls and propulsion of riverboats. Riverboats are generally of shallow draft, being broad of beam and rather square in plan, with a low freeboard and high topsides. Riverboats can survive with this type of configuration as they do not have to withstand the high winds or large waves that are seen on large lakes, seas, or oceans.
Fishing vessels are a subset of commercial vessels, but generally small in size and often subject to different regulations and classification. They can be categorized by several criteria: architecture, the type of fish they catch, the fishing method used, geographical origin, and technical features such as rigging. As of 2004, the world's fishing fleet consisted of some 4 million vessels. Of these, 1.3 million were decked vessels with enclosed areas and the rest were open vessels. Most decked vessels were mechanized, but two-thirds of the open vessels were traditional craft propelled by sails and oars. More than 60% of all existing large fishing vessels were built in Japan, Peru, the Russian Federation, Spain or the United States of America.
Special purpose vessels.
A weather ship was a ship stationed in the ocean as a platform for surface and upper air meteorological observations for use in marine weather forecasting. Surface weather observations were taken hourly, and four radiosonde releases occurred daily. It was also meant to aid in search and rescue operations and to support transatlantic flights. Proposed as early as 1927 by the aviation community, the establishment of weather ships proved to be so useful during World War II that the International Civil Aviation Organization (ICAO) established a global network of weather ships in 1948, with 13 to be supplied by the United States. This number was eventually negotiated down to nine.
The weather ship crews were normally at sea for three weeks at a time, returning to port for 10 day stretches. Weather ship observations proved to be helpful in wind and wave studies, as they did not avoid weather systems like other ships tended to for safety reasons. They were also helpful in monitoring storms at sea, such as tropical cyclones. The removal of a weather ship became a negative factor in forecasts leading up to the Great Storm of 1987. Beginning in the 1970s, their role became largely superseded by weather buoys due to the ships' significant cost. The agreement of the use of weather ships by the international community ended in 1990. The last weather ship was "Polarfront", known as weather station M ("Mike"), which was put out of operation on 1 January 2010. Weather observations from ships continue from a fleet of voluntary merchant vessels in routine commercial operation.
Naval vessels.
Naval vessels are those used by a navy for military purposes. 
There have been many types of naval vessel. Modern naval vessels can be broken down into three categories: surface warships, submarines, and support and auxiliary vessels.
Modern warships are generally divided into seven main categories: aircraft carriers, cruisers, destroyers, frigates, corvettes, submarines and amphibious assault ships. The distinction between cruisers, destroyers, frigates, and corvettes is not rigorous; the same vessel may be described differently in different navies. Battleships were used during the Second World War and occasionally since then (the last battleships were removed from the U.S. Naval Vessel Register in March 2006), but were made obsolete by the use of carrier-borne aircraft and guided missiles.
Most military submarines are either attack submarines or ballistic missile submarines. Until the end of World War II the primary role of the diesel/electric submarine was anti-ship warfare, inserting and removing covert agents and military forces, and intelligence-gathering. With the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to effectively hunt each other. The development of submarine-launched nuclear and cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster munitions to nuclear weapons.
Most navies also include many types of support and auxiliary vessel, such as minesweepers, patrol boats, offshore patrol vessels, replenishment ships, and hospital ships which are designated medical treatment facilities.
Fast combat vessels such as cruisers and destroyers usually have fine hulls to maximize speed and maneuverability. They also usually have advanced marine electronics and communication systems, as well as weapons.
Architecture.
Some components exist in vessels of any size and purpose. Every vessel has a hull of sorts. Every vessel has some sort of propulsion, whether it's a pole, an ox, or a nuclear reactor. Most vessels have some sort of steering system. Other characteristics are common, but not as universal, such as compartments, holds, a superstructure, and equipment such as anchors and winches.
Hull.
For a ship to float, its weight must be less than that of the water displaced by the ship's hull. There are many types of hulls, from logs lashed together to form a raft to the advanced hulls of America's Cup sailboats. A vessel may have a single hull (called a monohull design), two in the case of catamarans, or three in the case of trimarans. Vessels with more than three hulls are rare, but some experiments have been conducted with designs such as pentamarans. Multiple hulls are generally parallel to each other and connected by rigid arms.
Hulls have several elements. The bow is the foremost part of the hull. Many ships feature a bulbous bow. The keel is at the very bottom of the hull, extending the entire length of the ship. The rear part of the hull is known as the stern, and many hulls have a flat back known as a transom. Common hull appendages include propellers for propulsion, rudders for steering, and stabilizers to quell a ship's rolling motion. Other hull features can be related to the vessel's work, such as fishing gear and sonar domes.
Hulls are subject to various hydrostatic and hydrodynamic constraints. The key hydrostatic constraint is that it must be able to support the entire weight of the boat, and maintain stability even with often unevenly distributed weight. Hydrodynamic constraints include the ability to withstand shock waves, weather collisions and groundings.
Older ships and pleasure craft often have or had wooden hulls. Steel is used for most commercial vessels. Aluminium is frequently used for fast vessels, and composite materials are often found in sailboats and pleasure craft. Some ships have been made with concrete hulls.
Propulsion systems.
Propulsion systems for ships fall into three categories: human propulsion, sailing, and mechanical propulsion. Human propulsion includes rowing, which was used even on large galleys. Propulsion by sail generally consists of a sail hoisted on an erect mast, supported by stays and spars and controlled by ropes. Sail systems were the dominant form of propulsion until the 19th century. They are now generally used for recreation and competition, although experimental sail systems, such as the turbosails, rotorsails, and wingsails have been used on larger modern vessels for fuel savings.
Mechanical propulsion systems generally consist of a motor or engine turning a propeller, or less frequently, an impeller or wave propulsion fins. Steam engines were first used for this purpose, but have mostly been replaced by two-stroke or four-stroke diesel engines, outboard motors, and gas turbine engines on faster ships. Nuclear reactors producing steam are used to propel warships and icebreakers, and there have been attempts to utilize them to power commercial vessels (see NS "Savannah").
In addition to traditional fixed and controllable pitch propellers there are many specialized variations, such as contra-rotating and nozzle-style propellers. Most vessels have a single propeller, but some large vessels may have up to four propellers supplemented with transverse thrusters for maneuvring at ports. The propeller is connected to the main engine via a propeller shaft and, in case of medium- and high-speed engines, a reduction gearbox. Some modern vessels have a diesel-electric powertrain in which the propeller is turned by an electric motor powered by the ship's generators.
Steering systems.
For ships with independent propulsion systems for each side, such as manual oars or some paddles, steering systems may not be necessary. In most designs, such as boats propelled by engines or sails, a steering system becomes necessary. The most common is a rudder, a submerged plane located at the rear of the hull. Rudders are rotated to generate a lateral force which turns the boat. Rudders can be rotated by a tiller, manual wheels, or electro-hydraulic systems. Autopilot systems combine mechanical rudders with navigation systems. Ducted propellers are sometimes used for steering.
Some propulsion systems are inherently steering systems. Examples include the outboard motor, the bow thruster, and the Z-drive.
Holds, compartments, and the superstructure.
Larger boats and ships generally have multiple decks and compartments. Separate berthings and heads are found on sailboats over about . Fishing boats and cargo ships typically have one or more cargo holds. Most larger vessels have an engine room, a galley, and various compartments for work. Tanks are used to store fuel, engine oil, and fresh water. Ballast tanks are equipped to change a ship's trim and modify its stability.
Superstructures are found above the main deck. On sailboats, these are usually very low. On modern cargo ships, they are almost always located near the ship's stern. On passenger ships and warships, the superstructure generally extends far forward.
Equipment.
Shipboard equipment varies from ship to ship depending on such factors as the ship's era, design, area of operation, and purpose. Some types of equipment that are widely found include:
Design considerations.
Hydrostatics.
Boats and ships are kept on (or slightly above) the water in three ways:
A vessel is in equilibrium when the upwards and downwards forces are of equal magnitude. As a vessel is lowered into the water its weight remains constant but the corresponding weight of water displaced by its hull increases. When the two forces are equal, the boat floats. If weight is evenly distributed throughout the vessel, it floats without trim or heel.
A vessel's stability is considered in both this hydrostatic sense as well as a hydrodynamic sense, when subjected to movement, rolling and pitching, and the action of waves and wind. Stability problems can lead to excessive pitching and rolling, and eventually capsizing and sinking.
Hydrodynamics.
The advance of a vessel through water is resisted by the water. This resistance can be broken down into several components, the main ones being the friction of the water on the hull and wave making resistance. To reduce resistance and therefore increase the speed for a given power, it is necessary to reduce the wetted surface and use submerged hull shapes that produce low amplitude waves. To do so, high-speed vessels are often more slender, with fewer or smaller appendages. The friction of the water is also reduced by regular maintenance of the hull to remove the sea creatures and algae that accumulate there. Antifouling paint is commonly used to assist in this. Advanced designs such as the bulbous bow assist in decreasing wave resistance.
A simple way of considering wave-making resistance is to look at the hull in relation to its wake. At speeds lower than the wave propagation speed, the wave rapidly dissipates to the sides. As the hull approaches the wave propagation speed, however, the wake at the bow begins to build up faster than it can dissipate, and so it grows in amplitude. Since the water is not able to "get out of the way of the hull fast enough", the hull, in essence, has to climb over or push through the bow wave. This results in an exponential increase in resistance with increasing speed.
This hull speed is found by the formula:
formula_1
or, in metric units:
formula_2
where "L" is the length of the waterline in feet or meters.
When the vessel exceeds a speed/length ratio of 0.94, it starts to outrun most of its bow wave, and the hull actually settles slightly in the water as it is now only supported by two wave peaks. As the vessel exceeds a speed/length ratio of 1.34, the hull speed, the wavelength is now longer than the hull, and the stern is no longer supported by the wake, causing the stern to squat, and the bow rise. The hull is now starting to climb its own bow wave, and resistance begins to increase at a very high rate. While it is possible to drive a displacement hull faster than a speed/length ratio of 1.34, it is prohibitively expensive to do so. Most large vessels operate at speed/length ratios well below that level, at speed/length ratios of under 1.0.
For large projects with adequate funding, hydrodynamic resistance can be tested experimentally in a hull testing pool or using tools of computational fluid dynamics.
Vessels are also subject to ocean surface waves and sea swell as well as effects of wind and weather. These movements can be stressful for passengers and equipment, and must be controlled if possible. The rolling movement can be controlled, to an extent, by ballasting or by devices such as fin stabilizers. Pitching movement is more difficult to limit and can be dangerous if the bow submerges in the waves, a phenomenon called pounding. Sometimes, ships must change course or speed to stop violent rolling or pitching.
How it has been convincingly shown in scientific studies of the 21st century, controllability of some vessels decreases dramatically in some cases that are conditioned by effects of the bifurcation memory. This class of vessels includes ships with high manoeuvring capabilities, aircraft and controlled underwater vehicles designed to be unstable in steady-state motion that are interesting in terms of applications. These features must be considered in designing ships and in their control in critical situations.
Lifecycle.
A ship will pass through several stages during its career. The first is usually an initial contract to build the ship, the details of which can vary widely based on relationships between the shipowners, operators, designers and the shipyard. Then, the design phase carried out by a naval architect. Then the ship is constructed in a shipyard. After construction, the vessel is launched and goes into service. Ships end their careers in a number of ways, ranging from shipwrecks to service as a museum ship to the scrapyard.
Design.
A vessel's design starts with a specification, which a naval architect uses to create a project outline, assess required dimensions, and create a basic layout of spaces and a rough displacement. After this initial rough draft, the architect can create an initial hull design, a general profile and an initial overview of the ship's propulsion. At this stage, the designer can iterate on the ship's design, adding detail and refining the design at each stage.
The designer will typically produce an overall plan, a general specification describing the peculiarities of the vessel, and construction blueprints to be used at the building site. Designs for larger or more complex vessels may also include sail plans, electrical schematics, and plumbing and ventilation plans.
As environmental laws are becoming more strict, ship designers need to create their design in such a way that the ship, when it nears its end-of-term, can be disassembled or disposed easily and that waste is reduced to a minimum.
Construction.
Ship construction takes place in a shipyard, and can last from a few months for a unit produced in series, to several years to reconstruct a wooden boat like the frigate "Hermione", to more than 10 years for an aircraft carrier. During World War II, the need for cargo ships was so urgent that construction time for Liberty Ships went from initially eight months or longer, down to weeks or even days. Builders employed production line and prefabrication techniques such as those used in shipyards today.
Hull materials and vessel size play a large part in determining the method of construction. The hull of a mass-produced fiberglass sailboat is constructed from a mold, while the steel hull of a cargo ship is made from large sections welded together as they are built.
Generally, construction starts with the hull, and on vessels over about , by the laying of the keel. This is done in a drydock or on land. Once the hull is assembled and painted, it is launched. The last stages, such as raising the superstructure and adding equipment and accommodation, can be done after the vessel is afloat.
Once completed, the vessel is delivered to the customer. Ship launching is often a ceremony of some significance, and is usually when the vessel is formally named. A typical small rowboat can cost under US$100, $1,000 for a small speedboat, tens of thousands of dollars for a cruising sailboat, and about $2,000,000 for a Vendée Globe class sailboat. A trawler may cost $2.5 million, and a 1,000-person-capacity high-speed passenger ferry can cost in the neighborhood of $50 million. A ship's cost partly depends on its complexity: a small, general cargo ship will cost $20 million, a Panamax-sized bulk carrier around $35 million, a supertanker around $105 million and a large LNG carrier nearly $200 million. The most expensive ships generally are so because of the cost of embedded electronics: a costs around $2 billion, and an aircraft carrier goes for about $3.5 billion.
Repair and conversion.
Ships undergo nearly constant maintenance during their career, whether they be underway, pierside, or in some cases, in periods of reduced operating status between charters or shipping seasons.
Most ships, however, require trips to special facilities such as a drydock at regular intervals. Tasks often done at drydock include removing biological growths on the hull, sandblasting and repainting the hull, and replacing sacrificial anodes used to protect submerged equipment from corrosion. Major repairs to the propulsion and steering systems as well as major electrical systems are also often performed at dry dock.
Vessels that sustain major damage at sea may be repaired at a facility equipped for major repairs, such as a shipyard. Ships may also be converted for a new purpose: oil tankers are often converted into floating production storage and offloading units.
End of service.
Most ocean-going cargo ships have a life expectancy of between 20 and 30 years. A sailboat made of plywood or fiberglass can last between 30 and 40 years. Solid wooden ships can last much longer but require regular maintenance. Carefully maintained steel-hulled yachts can have a lifespan of over 100 years.
As ships age, forces such as corrosion, osmosis, and rotting compromise hull strength, and a vessel becomes too dangerous to sail. At this point, it can be scuttled at sea or scrapped by shipbreakers. Ships can also be used as museum ships, or expended to construct breakwaters or artificial reefs.
Many ships do not make it to the scrapyard, and are lost in fires, collisions, grounding, or sinking at sea. The Allies lost some 5,150 ships during World War II.
Measuring ships.
One can measure ships in terms of overall length, length of the ship at the waterline, beam (breadth), depth (distance between the crown of the weather deck and the top of the keelson), draft (distance between the highest waterline and the bottom of the ship) and tonnage. A number of different tonnage definitions exist and are used when describing merchant ships for the purpose of tolls, taxation, etc.
In Britain until Samuel Plimsoll's Merchant Shipping Act of 1876, ship-owners could load their vessels until their decks were almost awash, resulting in a dangerously unstable condition. Anyone who signed on to such a ship for a voyage and, upon realizing the danger, chose to leave the ship, could end up in jail. Plimsoll, a Member of Parliament, realised the problem and engaged some engineers to derive a fairly simple formula to determine the position of a line on the side of any specific ship's hull which, when it reached the surface of the water during loading of cargo, meant the ship had reached its maximum safe loading level. To this day, that mark, called the "Plimsoll Line", exists on ships' sides, and consists of a circle with a horizontal line through the centre. On the Great Lakes of North America the circle is replaced with a diamond. Because different types of water (summer, fresh, tropical fresh, winter north Atlantic) have different densities, subsequent regulations required painting a group of lines forward of the Plimsoll mark to indicate the safe depth (or freeboard above the surface) to which a specific ship could load in water of various densities. Hence the "ladder" of lines seen forward of the Plimsoll mark to this day. This is called the "freeboard mark" or "load line mark" in the marine industry.
Ship pollution.
Ship pollution is the pollution of air and water by shipping. It is a problem that has been accelerating as trade has become increasingly globalized, posing an increasing threat to the world’s oceans and waterways as globalization continues. It is expected that, “...shipping traffic to and from the USA is projected to double by 2020." Because of increased traffic in ocean ports, pollution from ships also directly affects coastal areas. The pollution produced affects biodiversity, climate, food, and human health. However, the degree to which humans are polluting and how it affects the world is highly debated and has been a hot international topic for the past 30 years.
Oil spills.
Oil spills have devastating effects on the environment. Crude oil contains polycyclic aromatic hydrocarbons (PAHs) which are very difficult to clean up, and last for years in the sediment and marine environment. Marine species constantly exposed to PAHs can exhibit developmental problems, susceptibility to disease, and abnormal reproductive cycles.
By the sheer amount of oil carried, modern oil tankers must be considered something of a threat to the environment. An oil tanker can carry of crude oil, or . This is more than six times the amount spilled in the widely known "Exxon Valdez" incident. In this spill, the ship ran aground and dumped of oil into the ocean in March 1989. Despite efforts of scientists, managers, and volunteers, over 400,000 seabirds, about 1,000 sea otters, and immense numbers of fish were killed.
The International Tanker Owners Pollution Federation has researched 9,351 accidental spills since 1974. According to this study, most spills result from routine operations such as loading cargo, discharging cargo, and taking on fuel oil. 91% of the operational oil spills were small, resulting in less than 7 tons per spill. Spills resulting from accidents like collisions, groundings, hull failures, and explosions are much larger, with 84% of these involving losses of over 700 tons.
Following the "Exxon Valdez" spill, the United States passed the Oil Pollution Act of 1990 (OPA-90), which included a stipulation that all tankers entering its waters be double-hulled by 2015. Following the sinkings of the "Erika" (1999) and "Prestige" (2002), the European Union passed its own stringent anti-pollution packages (known as Erika I, II, and III), which require all tankers entering its waters to be double-hulled by 2010. The Erika packages are controversial because they introduced the new legal concept of "serious negligence".
Ballast water.
When a large vessel such as a container ship or an oil tanker unloads cargo, seawater is pumped into other compartments in the hull to help stabilize and balance the ship. During loading, this ballast water is pumped out from these compartments.
One of the problems with ballast water transfer is the transport of harmful organisms. Meinesz believes that one of the worst cases of a single invasive species causing harm to an ecosystem can be attributed to a seemingly harmless jellyfish. "Mnemiopsis leidyi", a species of comb jellyfish that inhabits estuaries from the United States to the Valdés peninsula in Argentina along the Atlantic coast, has caused notable damage in the Black Sea. It was first introduced in 1982, and thought to have been transported to the Black Sea in a ship’s ballast water. The population of the jellyfish shot up exponentially and, by 1988, it was wreaking havoc upon the local fishing industry. "The anchovy catch fell from in 1984 to in 1993; sprat from in 1984 to in 1993; horse mackerel from in 1984 to zero in 1993." Now that the jellyfish have exhausted the zooplankton, including fish larvae, their numbers have fallen dramatically, yet they continue to maintain a stranglehold on the ecosystem. Recently the jellyfish have been discovered in the Caspian Sea. Invasive species can take over once occupied areas, facilitate the spread of new diseases, introduce new genetic material, alter landscapes and jeopardize the ability of native species to obtain food. "On land and in the sea, invasive species are responsible for about 137 billion dollars in lost revenue and management costs in the U.S. each year."
Ballast and bilge discharge from ships can also spread human pathogens and other harmful diseases and toxins potentially causing health issues for humans and marine life alike. Discharges into coastal waters, along with other sources of marine pollution, have the potential to be toxic to marine plants, animals, and microorganisms, causing alterations such as changes in growth, disruption of hormone cycles, birth defects, suppression of the immune system, and disorders resulting in cancer, tumors, and genetic abnormalities or even death.
Exhaust emissions.
Exhaust emissions from ships are considered to be a significant source of air pollution. “Seagoing vessels are responsible for an estimated 14 percent of emissions of nitrogen from fossil fuels and 16 percent of the emissions of sulfur from petroleum uses into the atmosphere.” In Europe ships make up a large percentage of the sulfur introduced to the air, “...as much sulfur as all the cars, lorries and factories in Europe put together.” “By 2010, up to 40% of air pollution over land could come from ships.” Sulfur in the air creates acid rain which damages crops and buildings. When inhaled sulfur is known to cause respiratory problems and increase the risk of a heart attack.
Ship breaking.
Ship breaking or ship demolition is a type of ship disposal involving the breaking up of ships for scrap recycling, with the hulls being discarded in ship graveyards. Most ships have a lifespan of a few decades before there is so much wear that refitting and repair becomes uneconomical. Ship breaking allows materials from the ship, especially steel, to be reused.
In addition to steel and other useful materials, however, ships (particularly older vessels) can contain many substances that are banned or considered dangerous in developed countries. Asbestos and polychlorinated biphenyls (PCBs) are typical examples. Asbestos was used heavily in ship construction until it was finally banned in most of the developed world in the mid 1980s. Currently, the costs associated with removing asbestos, along with the potentially expensive insurance and health risks, have meant that ship-breaking in most developed countries is no longer economically viable. Removing the metal for scrap can potentially cost more than the scrap value of the metal itself. In most of the developing world, however, shipyards can operate without the risk of personal injury lawsuits or workers' health claims, meaning many of these shipyards may operate with high health risks. Furthermore, workers are paid very low rates with no overtime or other allowances. Protective equipment is sometimes absent or inadequate. Dangerous vapors and fumes from burning materials can be inhaled, and dusty asbestos-laden areas around such breakdown locations are commonplace.
Aside from the health of the yard workers, in recent years, ship breaking has also become an issue of major environmental concern. Many developing nations, in which ship breaking yards are located, have lax or no environmental law, enabling large quantities of highly toxic materials to escape into the environment and causing serious health problems among ship breakers, the local population and wildlife. Environmental campaign groups such as Greenpeace have made the issue a high priority for their campaigns.
Buoyancy.
A floating boat displaces its weight in water. The material of the boat hull may be denser than water, but if this is the case then it forms only the outer layer. If the boat floats, the mass of the boat (plus contents) "as a whole" divided by the volume "below the waterline" is equal to the density of water (1 kg/l). If weight is added to the boat, the volume below the waterline will increase to keep the weight balance equal, and so the boat sinks a little to compensate.
See also.
Model ships
Lists
Ship sizes

</doc>
