<doc id="29452" url="https://en.wikipedia.org/wiki?curid=29452" title="Stasi">
Stasi

The Ministry for State Security (), commonly known as the Stasi () (abbreviation German: Staatssicherheit, literally State Security), also State Security Service (German Staatssicherheitsdienst, SSD), was the official state security service of the German Democratic Republic (GDR) ("Deutsche Demokratische Republik", "DDR"), colloquially known as East Germany. It has been described as one of the most effective and repressive intelligence and secret police agencies to have ever existed. The Stasi was headquartered in East Berlin, with an extensive complex in Berlin-Lichtenberg and several smaller facilities throughout the city. The Stasi motto was ""Schild und Schwert der Partei"" (Shield and Sword of the Party), referring to the ruling Socialist Unity Party of Germany (German: "Sozialistische Einheitspartei Deutschlands", SED). Erich Mielke was its longest-serving chief, in power for thirty-two of the GDR's forty years of existence.
One of its main tasks was spying on the population, mainly through a vast network of citizens turned informants, and fighting any opposition by overt and covert measures, including hidden psychological destruction of dissidents ("Zersetzung", literally meaning decomposition). Its "Main Directorate for Reconnaissance" (German: "Hauptverwaltung Aufklärung") was responsible for both espionage and for conducting covert operations in foreign countries. Under its long-time head Markus Wolf, this directorate gained a reputation as one of the most effective intelligence agencies of the Cold War. 
Numerous Stasi officials were prosecuted for their crimes after 1990. After German reunification, the surveillance files that the Stasi had maintained on millions of East Germans were laid open, so that any citizen could inspect their personal file on request; these files are now maintained by the Federal Commissioner for the Stasi Records.
Creation.
The Stasi was founded on 8 February 1950. Wilhelm Zaisser was the first Minister of State Security of the GDR, and Erich Mielke was his deputy. Zaisser tried to depose SED General Secretary Walter Ulbricht after the June 1953 uprising, but was instead removed by Ulbricht and replaced with Ernst Wollweber thereafter. Wollweber resigned in 1957 after clashes with Ulbricht and Erich Honecker, and was succeeded by his deputy, Erich Mielke.
In 1957, Markus Wolf became head of the Hauptverwaltung Aufklärung (HVA) (Main Reconnaissance Administration), the foreign intelligence section of the Stasi. As intelligence chief, Wolf achieved great success in penetrating the government, political and business circles of West Germany with spies. The most influential case was that of Günter Guillaume, which led to the downfall of West German Chancellor Willy Brandt in May 1974. In 1986, Wolf retired and was succeeded by Werner Grossmann.
Relationship with the KGB.
Although Mielke's Stasi was superficially granted independence in 1957, until 1990 the KGB continued to maintain liaison officers in all eight main Stasi directorates, each with his own office inside the Stasi's Berlin compound, and in each of the fifteen Stasi district headquarters around East Germany. Collaboration was so close that the KGB invited the Stasi to establish operational bases in Moscow and Leningrad to monitor visiting East German tourists and Mielke referred to the Stasi officers as "Chekists of the Soviet Union". In 1978, Mielke formally granted KGB officers in East Germany the same rights and powers that they enjoyed in the Soviet Union.
Organization.
The Ministry for State Security also included the following entities:
Operations.
Personnel and recruitment.
Between 1950 and 1989, the Stasi employed a total of 274,000 people in an effort to root out the class enemy. In 1989, the Stasi employed 91,015 people full-time, including 2,000 fully employed unofficial collaborators, 13,073 soldiers and 2,232 officers of GDR army, along with 173,081 unofficial informants inside GDR and 1,553 informants in West Germany.
Regular commissioned Stasi officers were recruited from conscripts who had been honourably discharged from their 28 months' compulsory military service, had been members of the SED, had had a high level of participation in the Party's youth wing's activities and had been Stasi informers during their service in the Military. The candidates would then have to be recommended by their military unit political officers and Stasi agents, the local chiefs of the District (Bezirk) Stasi and Volkspolizei office, of the district in which they were permanently resident, and the District Secretary of the SED. These candidates were then made to sit through several tests and exams, which identified their intellectual capacity to be an officer, and their political reliability. University graduates who had completed their military service did not need to take these tests and exams. They then attended a two-year officer training programme at the Stasi college ("Hochschule") in Potsdam. Less mentally and academically endowed candidates were made ordinary technicians and attended a one-year technology-intensive course for non-commissioned officers.
By 1995, some 174,000 "inoffizielle Mitarbeiter" (IMs) Stasi informants had been identified, almost 2.5% of East Germany's population between the ages of 18 and 60. 10,000 IMs were under 18 years of age. From the volume of material destroyed in the final days of the regime, the office of the Federal Commissioner for the Stasi Records (BStU) believes that there could have been as many as 500,000 informers. A former Stasi colonel who served in the counterintelligence directorate estimated that the figure could be as high as 2 million if occasional informants were included.
Infiltration.
Full-time officers were posted to all major industrial plants (the extensiveness of any surveillance largely depended on how valuable a product was to the economy) and one tenant in every apartment building was designated as a watchdog reporting to an area representative of the Volkspolizei (Vopo). Spies reported every relative or friend who stayed the night at another's apartment. Tiny holes were drilled in apartment and hotel room walls through which Stasi agents filmed citizens with special video cameras. Schools, universities, and hospitals were extensively infiltrated.
The Stasi had formal categorizations of each type of informant, and had official guidelines on how to extract information from, and control, those who they came into contact with. The roles of informants ranged from those already in some way involved in state security (such as the police and the armed services) to those in the dissident movements (such as in the arts and the Protestant Church). Information gathered about the latter groups was frequently used to divide or discredit members. Informants were made to feel important, given material or social incentives, and were imbued with a sense of adventure, and only around 7.7%, according to official figures, were coerced into cooperating. A significant proportion of those informing were members of the SED; to employ some form of blackmail, however, was not uncommon. A large number of Stasi informants were trolley conductors, janitors, doctors, nurses and teachers; Mielke believed that the best informants were those whose jobs entailed frequent contact with the public.
The Stasi's ranks swelled considerably after Eastern Bloc countries signed the 1975 Helsinki accords, which GDR leader Erich Honecker viewed as a grave threat to his regime because they contained language binding signatories to respect "human and basic rights, including freedom of thought, conscience, religion, and conviction." The number of IMs peaked at around 180,000 in that year, having slowly risen from 20,000–30,000 in the early 1950s, and reaching 100,000 for the first time in 1968, in response to "Ostpolitik" and protests worldwide. The Stasi also acted as a proxy for KGB to conduct activities in other Eastern Bloc countries, such as Poland, where the Soviets were despised.
The Stasi infiltrated almost every aspect of GDR life. In the mid-1980s, a network of IMs began growing in both German states; by the time that East Germany collapsed in 1989, the Stasi employed 91,015 employees and 173,081 informants. About one out of every 63 East Germans collaborated with the Stasi. By at least one estimate, the Stasi maintained greater surveillance over its own people than any secret police force in history. The Stasi employed one full-time agent for every 166 East Germans. The ratios swelled when informers were factored in: counting part-time informers, the Stasi had one informer per 6.5 people. By comparison, the Gestapo employed one secret policeman per 2,000 people. This comparison led Nazi hunter Simon Wiesenthal to call the Stasi even more oppressive than the Gestapo. Additionally, Stasi agents infiltrated and undermined West Germany's government and spy agencies.
In some cases, spouses even spied on each other. A high-profile example of this was peace activist Vera Lengsfeld, whose husband, Knud Wollenberger, was a Stasi informant.
Zersetzung.
The Stasi perfected the technique of psychological harassment of perceived enemies known as "Zersetzung" () – a term borrowed from chemistry which literally means "decomposition".
By the 1970s, the Stasi had decided that the methods of overt persecution that had been employed up to that time, such as arrest and torture, were too crude and obvious. It was realised that psychological harassment was far less likely to be recognised for what it was, so its victims, and their supporters, were less likely to be provoked into active resistance, given that they would often not be aware of the source of their problems, or even its exact nature. "Zersetzung" was designed to side-track and "switch off" perceived enemies so that they would lose the will to continue any "inappropriate" activities.
Tactics employed under "Zersetzung" generally involved the disruption of the victim's private or family life. This often included psychological attacks, such as breaking into homes and subtly manipulating the contents, in a form of gaslighting – moving furniture, altering the timing of an alarm, removing pictures from walls or replacing one variety of tea with another. Other practices included property damage, sabotage of cars, purposely incorrect medical treatment, smear campaigns including sending falsified compromising photos or documents to the victim's family, denunciation, provocation, psychological warfare, psychological subversion, wiretapping, bugging, mysterious phone calls or unnecessary deliveries, even including sending a vibrator to a target's wife. Usually, victims had no idea that the Stasi were responsible. Many thought that they were losing their minds, and mental breakdowns and suicide could result.
One great advantage of the harassment perpetrated under "Zersetzung" was that its subtle nature meant that it was able to be plausibly denied. This was important given that the GDR was trying to improve its international standing during the 1970s and 80s, especially in conjunction with the "Ostpolitik" of West-German chancellor Willy Brandt massively improving relations between the two German states.
"Zersetzung" techniques were used by other East Bloc intelligence and security agencies. This includes extensive use by the hierarchically superior agency in the USSR intelligence framework, the Soviet KGB. Techniques that would be described as "Zersetzung" techniques, would otherwise be described and fall under those of "active measures" as termed by the KGB. Techniques that would be classified as active measures or "Zersetzung" continue to be employed by selected security and intelligence organizations worldwide to this date. The Russian FSB, which has the present organizational responsibilities and congruent authorizations to the internal security, CI, investigatory directorates of the former KGB, has been widely implicated in continued use of active measures techniques in numerous operations.
International operations.
Other files (the Rosenholz Files), which contained the names of East German spies abroad, led American spy agencies to capture them. After German reunification, revelations of Stasi's international activities were publicized, such as its military training to the West German Red Army Faction.
Directorate X was responsible for disinformation. Rolf Wagenbreth, director of disinformation operations, stated "Our friends in Moscow call it 'dezinformatsiya'. Our enemies in America call it 'active measures', and I, dear friends, call it ‘my favorite pastime'".
Fall of the Soviet Union.
Recruitment of informants became increasingly difficult towards the end of the GDR's existence, and, after 1986, there was a negative turnover rate of IMs. This had a significant impact on the Stasi's ability to survey the population, in a period of growing unrest, and knowledge of the Stasi's activities became more widespread. Stasi had been tasked during this period with preventing the country's economic difficulties becoming a political problem, through suppression of the very worst problems the state faced, but it failed to do so.
Stasi officers reportedly had discussed re-branding East Germany as a democratic capitalist country to the West, but which would be in practice taken over by Stasi officers. The plan specified 2,587 OibE officers who would take over power ("Offiziere im besonderen Einsatz", "officers on special assignment") and it was registered as Top Secret Document 0008-6/86 of 17 March 1986. According to Ion Mihai Pacepa, the chief intelligence officer in communist Romania, other communist intelligence services had similar plans. On 12 March 1990, "Der Spiegel" reported that the Stasi was indeed attempting to implement 0008-6/86. Pacepa has noted that what happened in Russia and how KGB Colonel Vladimir Putin took over Russia resembles these plans. See Putinism.
On 7 November 1989, in response to the rapidly changing political and social situation in the GDR in late 1989, Erich Mielke resigned. On 17 November 1989, the Council of Ministers "(Ministerrat der DDR)" renamed the Stasi as the "Office for National Security" "(Amt für Nationale Sicherheit" – AfNS), which was headed by "Generalleutnant" Wolfgang Schwanitz. On 8 December 1989, GDR Prime Minister Hans Modrow directed the dissolution of the AfNS, which was confirmed by a decision of the "Ministerrat" on 14 December 1989.
As part of this decision, the "Ministerrat" originally called for the evolution of the AfNS into two separate organizations: a new foreign intelligence service "(Nachrichtendienst der DDR)" and an "Office for the Protection of the Constitution of the GDR" "(Verfassungsschutz der DDR)", along the lines of the West German "Bundesamt für Verfassungsschutz", however, the public reaction was extremely negative, and under pressure from the "Round Table" "(Runder Tisch)", the government dropped the creation of the "Verfassungsschutz der DDR" and directed the immediate dissolution of the AfNS on 13 January 1990. Certain functions of the AfNS reasonably related to law enforcement were handed over to the GDR Ministry of Internal Affairs. The same ministry also took guardianship of remaining AfNS facilities.
When the parliament of Germany investigated public funds that disappeared after the Fall of the Berlin Wall, it found out that East Germany had transferred large amounts of money to Martin Schlaff through accounts in Vaduz, the capital of Liechtenstein, in return for goods "under Western embargo".
Moreover, high-ranking Stasi officers continued their post-GDR careers in management positions in Schlaff's group of companies. For example, in 1990, Herbert Kohler, Stasi commander in Dresden, transferred 170 million marks to Schlaff for "harddisks" and months later went to work for him.
The investigations concluded that "Schlaff's empire of companies played a crucial role" in the Stasi attempts to secure the financial future of Stasi agents and keep the intelligence network alive.
The "Stern" magazine noted that KGB officer Vladimir Putin worked with his Stasi colleagues in Dresden in 1989.
A counter-mainstream narrative on the GDR and its demise is offered by the book Stasi State or Socialist Paradise? (Artery Publications 2015).
Recovery of the Stasi files.
During the Peaceful Revolution of 1989, Stasi offices were overrun by enraged citizens, but not before the Stasi destroyed a number of documents (approximately 5%) consisting of, by one calculation, 1 billion sheets of paper.
Storming the Stasi headquarters.
With the fall of the German Democratic Republic the Stasi was dissolved. Stasi employees began to destroy the extensive files and documents they held, by hand, fire and with the use of shredders. When these activities became known, a protest began in front of the Stasi headquarters, The evening of 15 January 1990 saw a large crowd form outside the gates calling for a stop to the destruction of sensitive files. The building contained vast records of personal files, many of which would form important evidence in convicting those who had committed atrocities for the Stasi. The protesters continued to grow in number until they were able to overcome the police and gain entry into the complex. Once inside, specific targets of the protesters' anger were portraits of Erich Honecker which were trampled on or burnt. Among the protesters were former Stasi collaborators seeking to destroy incriminating documents.
Controversy of the Stasi files.
With the German Reunification on 3 October 1990, a new government agency was founded called the "Federal Commissioner for the Records of the State Security Service of the former German Democratic Republic" (, officially abbreviated "BStU". There was a debate about what should happen to the files, whether they should be opened to the people or kept closed.
Those who opposed opening the files cited privacy as a reason. They felt that the information in the files would lead to negative feelings about former Stasi members, and, in turn, cause violence. Pastor Rainer Eppelmann, who became Minister of Defense and Disarmament after March 1990, felt that new political freedoms for former Stasi members would be jeopardized by acts of revenge. Prime Minister Lothar de Maizière even went so far as to predict murder. They also argued against the use of the files to capture former Stasi members and prosecute them, arguing that not all former members were criminals and should not be punished solely for being a member. There were also some who believed that everyone was guilty of something. Peter Michael Diestel, the Minister of Interior, opined that these files could not be used to determine innocence and guilt, claiming that "there were only two types of individuals who were truly innocent in this system, the newborn and the alcoholic". Other opinions, such as the one of West German Interior Minister Wolfgang Schäuble, believed in putting the Stasi behind them and working on German reunification.
Others argued that everyone should have the right to see their own file, and that the files should be opened to investigate former Stasi members and prosecute them, as well as not allow them to hold office. Opening the files would also help clear up some of the rumors that were floating around. Some also believed that politicians involved with the Stasi should be investigated.
The fate of the files was finally decided under the Unification Treaty between the GDR and Federal Republic of Germany (FRG). This treaty took the Volkskammer law further and allowed more access and use of the files. Along with the decision to keep the files in a central location in the East, they also decided who could see and use the files, allowing people to see their own files.
In 1992, following a declassification ruling by the German government, the Stasi files were opened, leading people to look for their files. Timothy Garton Ash, an English historian, after reading his file, wrote "The File: A Personal History".
Between 1991 and 2011, around 2.75 million individuals, mostly GDR citizens, requested to see their own files. The ruling also gave people the ability to make duplicates of their documents. Another big issue was how the media could use and benefit from the documents. It was decided that the media could obtain files as long as they were depersonalized and not regarding an individual under the age of 18 or a former Stasi member. This ruling not only gave the media access to the files, but also gave schools access.
Tracking down former Stasi informers with the files.
Even though groups of this sort were active in the community, those who were tracking down ex-members were, as well. Many of these hunters succeeded in catching ex-Stasi; however, charges could not be made for merely being a member. The person in question would have to have participated in an illegal act, not just be a registered Stasi member. Among the high-profile individuals who were arrested and tried were Erich Mielke, Third Minister of State Security of the GDR, and Erich Honecker, head of state for the GDR. Mielke was sentenced to six years prison for the murder of two policemen in 1931. Honecker was charged with authorizing the killing of would-be escapees on the East-West frontier and the Berlin Wall. During his trial, he went through cancer treatment. Because he was nearing death, Honecker was allowed to spend his final time in freedom. He died in Chile in May 1994.
Reassembling the destroyed files.
Some of it is very easy due to the number of archives and the failure of shredding machines (in some cases "shredding" meant tearing paper in two by hand and documents could be recovered easily). In 1995, the BStU began reassembling the shredded documents; 13 years later, the three dozen archivists commissioned to the projects had only reassembled 327 bags; they are now using computer-assisted data recovery to reassemble the remaining 16,000 bagsestimated at 45 million pages. It is estimated that this task may be completed at a cost of 30 million dollars.
The CIA acquired some Stasi records during the looting of the Stasi's archives. The Federal Republic of Germany has asked for their return and received some in April 2000. See also Rosenholz files.
Museum in the old headquarters.
The Anti-Stalinist Action Normannenstraße (ASTAK), an association founded by former GDR Citizens' Committees, has transformed the former headquarters of the Stasi into a museum. It is divided into three floors:
The ground floor has been kept as it used to be. The decor is original, with many statues and flags.
Photo gallery:
Stasi officers after the reunification.
Recruitment by Russian state-owned companies.
Former Stasi agent Matthias Warnig (codename "Arthur") is currently the CEO of Nord Stream.
German investigations have revealed that some of the key Gazprom Germania managers are former Stasi agents.
Lobbying.
Former Stasi officers continue to be politically active via the "Gesellschaft zur Rechtlichen und Humanitären Unterstützung e. V." (GRH, Society for Legal and Humanitarian Support). Former high-ranking officers and employees of the Stasi, including the last Stasi director, Wolfgang Schwanitz, make up the majority of the organization's members, and it receives support from the German Communist Party, among others.
Impetus for the establishment of the GRH was provided by the criminal charges filed against the Stasi in the early 1990s. The GRH, decrying the charges as "victor's justice", called for them to be dropped. Today the group provides an alternative if somewhat utopian voice in the public debate on the GDR legacy. It calls for the closure of the museum in Hohenschönhausen and can be a vocal presence at memorial services and public events. In March 2006 in Berlin, GRH members disrupted a museum event; a political scandal ensued when the Berlin Senator (Minister) of Culture refused to confront them.
Behind the scenes, the GRH also lobbies people and institutions promoting opposing viewpoints. For example, in March 2006, the Berlin Senator for Education received a letter from a GRH member and former Stasi officer attacking the Museum for promoting "falsehoods, anticommunist agitation and psychological terror against minors".

</doc>
<doc id="29455" url="https://en.wikipedia.org/wiki?curid=29455" title="Sandra Bullock">
Sandra Bullock

Sandra Annette Bullock (; born July 26, 1964) is an American actress and producer. She is one of Hollywood's highest-paid actresses, and is an Academy Award and Golden Globe Award winner. She was named "Most Beautiful Woman" by "People" magazine in 2015.
Bullock made her acting debut with a minor role in the 1987 thriller "Hangmen". She made her television debut in the television film "Bionic Showdown: The Six Million Dollar Man and the Bionic Woman" (1989), and played the lead role in the short-lived NBC sitcom "Working Girl." Her breakthrough role was in the film "Demolition Man" (1993), after which she starred in several successful films including "Speed" (1994), "While You Were Sleeping" (1995), "The Net" (1995), "A Time to Kill" (1996), "Hope Floats" (1998), and "Practical Magic" (1998). 
Bullock achieved further success in the 2000s and 2010s with starring roles in "Miss Congeniality" (2000), "Two Weeks Notice" (2002), "Crash" (2004), "The Proposal" (2009) and "The Heat" (2013). She was awarded the Academy Award for Best Actress and the Golden Globe Award for Best Actress in a Drama for playing Leigh Anne Tuohy in "The Blind Side" (2009), and also nominated in the same categories for her performance in "Gravity" (2013). Bullock's greatest commercial success is the animated comedy film "Minions" (2015), which grossed over at the box office.
In addition to acting, Bullock is the founder of the production company Fortis Films. She has produced some of the films in which she starred, including "Two Weeks Notice", "", and "All About Steve". She was an executive producer of the popular ABC sitcom, "George Lopez", and made several appearances during its run.
Early life.
Bullock was born in Arlington, Virginia, a suburb of Washington, D.C. Her father, John W. Bullock (born 1925), was a United States Army employee and part-time voice coach; her mother, Helga Mathilde Meyer (1942–2000), was an opera singer and voice teacher. Bullock's father is from Birmingham, Alabama, and has English, Irish, German, and French ancestry, while Bullock's mother was German. Bullock's maternal grandfather was a rocket scientist from Nuremberg, Germany. Bullock's father, who was in charge of the Army's Military Postal Service in Europe, was stationed in Nuremberg when he met his wife. They married in Germany and moved to Arlington, where John worked with the Army Materiel Command, before becoming a contractor for The Pentagon. She has a younger sister, Gesine Bullock-Prado, who was formerly the vice-president of Bullock's production company Fortis Films.
Bullock was raised in Nuremberg for twelve years and grew up speaking German. She attended the humanistic Waldorf School. As a child, Bullock frequently accompanied her mother on European opera tours. Bullock studied ballet and vocal arts as a child, taking small parts in her mother's opera productions. She sang in the opera's children's choir at the Staatstheater Nürnberg. The scar above her left eye was caused when she fell into a creek as a child. Bullock attended Washington-Lee High School, where she was a cheerleader and performed in high school theater productions. After graduating in 1982, she attended East Carolina University in Greenville, North Carolina, where she received a Bachelor of Fine Arts degree in Drama in 1987. While at ECU, she performed in multiple theater productions, including "Peter Pan" and "Three Sisters." She then moved to Manhattan and supported herself as a bartender, cocktail waitress, and coat checker while auditioning for roles.
Until the age of eighteen, Bullock held American/German dual citizenship. In 2009, she reapplied for German citizenship.
Career.
1987–99: Early career.
While in New York, Bullock took acting classes with Sanford Meisner. She appeared in several student films, and later landed a role in an Off-Broadway play "No Time Flat". Director Alan J. Levi was impressed by Bullock's performance and offered her a part in the TV movie "Bionic Showdown: The Six Million Dollar Man and the Bionic Woman" (1989). This led to her being cast in a series of small roles in several independent films as well as in the lead role of the short-lived NBC television version of the film "Working Girl" (1990). She went on to appear in several films, such as "Love Potion No. 9" (1992), "The Thing Called Love" (1993) and "Fire on the Amazon" (1993).
Bullock had a prominent supporting role in the science-fiction/action film "Demolition Man" (1993), followed by a leading role in "Speed" the following year. "Speed" took in $350 million at the box office worldwide.
A string of successes during the mid-1990s included "While You Were Sleeping" (1995), for which she received her first Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy, "The Net" (1995) and "A Time to Kill" (1996). Bullock received $11 million for "" (1997), which she agreed to star in for financial backing for her own project, "Hope Floats" (1998). She has stated that she regrets making the sequel.
She was selected as one of "People" magazine's 50 Most Beautiful People in the World in 1996 and 1999, and was also ranked No. 58 in "Empire" magazine's Top 100 Movie Stars of All Time list.
2000–08: Producing.
In 2000, Bullock starred in "Miss Congeniality", a financial success that took in $212 million at the box office worldwide, and received another Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy. She was presented with the 2002 Raúl Juliá Award for Excellence for her efforts, as the executive producer of the sitcom "George Lopez", in helping expand career openings for Hispanic talent in the media and entertainment industry. She also made several appearances on the show as Accident Amy, an accident-prone employee at the factory Lopez's character manages. The same year, she starred opposite Hugh Grant in "Two Weeks Notice" (2002).
In 2004, Bullock had a supporting role in the film "Crash", which won the Academy Award for Best Picture. She received positive reviews for her performance, with some critics suggesting that it was the best performance of her career. She later received a $17.5-million-salary for "" (2005). The same year, she was a co-recipient of the Women in Film Crystal Award.
Although Bullock was reunited with her "Speed" co-star Keanu Reeves in the romantic drama "The Lake House", their film characters are separated throughout the film, so Bullock and Reeves were only on set together for two weeks during filming. The same year, Bullock appeared in "Infamous", playing author Harper Lee. Bullock also starred in "Premonition" with Julian McMahon, which was released in March 2007. In 2008, Bullock was announced as "the face" of the cosmetic brand Artistry.
2009–12: Recognition.
The year 2009 proved to be especially good for Bullock, giving the actress two record highs in her career, as earlier in the year she released "The Proposal", with co-star Ryan Reynolds, grossed $317 million at the box office worldwide, making it her fourth most successful picture to date. She received her third Golden Globe Award nomination for Best Actress role for Motion Picture Musical or Comedy.
In November 2009, Bullock starred in "The Blind Side", which opened at No. 2 behind "New Moon" with $34.2 million, making it her second highest opening weekend ever. "The Blind Side" is unique in that it had a 17.6% increase at the box office its second weekend, and it took the top spot of the box office in its third weekend. The film cost $29 million to make according to the Box Office Mojo. It grossed over $309 million, making it her domestic highest grossing film, her fourth highest grossing film worldwide, and the first one in history to pass the $200 million mark with only one top-billed female star.
Bullock had initially turned down the role of Leigh Anne Tuohy three times due to a discomfort with portraying a devout Christian. She was awarded the Academy Award for Best Actress, Golden Globe Award for Best Actress – Motion Picture Drama, Screen Actors Guild Award for Outstanding Performance by a Female Actor in a Leading Role and Broadcast Film Critics Association Award for Best Actress. "The Blind Side" also received an Academy Award for Best Picture nomination.
Winning the Oscar also gave Bullock another unique distinction—since she won two "Razzies" the day before, for her performance in "All About Steve" (2009), she is the only performer ever to have been named both "Best" and "Worst" for the same year.
In 2011, Bullock starred in the drama "Extremely Loud and Incredibly Close" alongside Tom Hanks, a film adaptation based on the novel of the same name. Despite mixed reviews, the film was nominated for numerous awards, including an Academy Award for Best Picture nomination. Bullock was nominated for Best Actress Drama by Teen Choice Awards.
2013–present: Continued success.
In 2013, Bullock starred in the comedy film "The Heat", alongside Melissa McCarthy. It received positive reviews from critics, and took in $230 million at the box office worldwide. Bullock also starred in the science fiction film "Gravity", opposite George Clooney. The film premiered at the 70th Venice Film Festival, and was released on October 4, 2013 to coincide with the beginning of World Space Week. "Gravity" received universal acclaim among critics and a standing ovation in Venice. The film was called "the most realistic and beautifully choreographed film ever set in space". Bullock's performance was praised, with some critics calling "Gravity" the best work of her career. "Variety" wrote, "Gravity" took in $716 million at the box office worldwide, making it Bullock's second most successful picture. For her role as Dr. Ryan Stone, Bullock was nominated for the Academy Award, Golden Globe Award, BAFTA Award, Screen Actors Guild Award, and Broadcast Film Critics Association Award for Best Actress. By August 2014, Bullock was the highest earning actress in Hollywood. In 2015, she voiced the character of Scarlet Overkill in the animated film "Minions", which became her highest grossing film to date, with over $1.1 billion worldwide. That same year, Bullock also starred in the comedy-drama film "Our Brand Is Crisis".
By 2015, Bullock's films had grossed over $5.3 billion worldwide, which makes her 14th most profitable movie star. According to "The Numbers", her total domestic gross stands at over $2.5 billion, placing her among the Top 100 Stars at the Box Office.
Public image.
Since her acting debut, Bullock has been dubbed "America’s sweetheart" in the media due to her "friendly and direct and so unpretentious" nature.
On March 24, 2005, Bullock received a star on the Hollywood Walk of Fame at 6801 Hollywood Boulevard in Hollywood.
While critics have praised her screen persona, some have been less receptive towards her films. At the 2009 release of "The Proposal", Mark Kermode said Bullock had made only three "good" films—"Speed", "While You Were Sleeping", and "Crash", and added that "she's funny, she's gorgeous, it's impossible not to love her, and yet she makes rotten film after rotten film..."
Bullock was selected by "People" magazine as its 2010 Woman of the Year 
and ranked No. 12 on "People"s Most Beautiful 2011 list.
In 2010, "Time" magazine included Bullock in its annual "TIME" 100 as one of the "Most Influential People in the World."
In 2013, "The Hollywood Reporter" named Bullock among the most powerful women in entertainment. In September 2013, Bullock joined other Hollywood legends at the TCL Chinese Theatre on Hollywood Boulevard by making imprints of her hand- and footprints in cement in the theater's forecourt.
In November 2013 it was announced that Bullock was named "Entertainment Weekly"s Entertainer of the Year due to her success with "The Heat" and "Gravity", which "Entertainment Weekly" believed would earn her an Oscar nomination. Bullock shared the title with other distinguished people in the industry such as the creators of the television show "Breaking Bad", Matthew McConaughey, Jennifer Lawrence, "Grumpy Cat" and others.
In 2014, Bullock ranked No. 2 on "Forbes"s list of most powerful actresses and was honored with the Decade of Hotness Award by Spike Guys' Choice Awards.
In 2015, she was named "The Most Beautiful Woman" by "People."
Business ventures.
Bullock runs her own production company, Fortis Films. She was an executive producer of the "George Lopez" sitcom (co-produced with Robert Borden), which garnered a syndication deal that banked her some $10 million . Bullock tried to produce a film based on F.X. Toole's short story "Million-Dollar Baby" but could not interest the studios in a female boxing drama. The story was eventually adapted and directed by Clint Eastwood as the Oscar-winning film, "Million Dollar Baby" (2004). Fortis Films also produced "All About Steve" which was released in September 2009. Her father, John Bullock, is the company's CEO, and her sister, Gesine Bullock-Prado, is the former president.
In November 2006 Bullock founded an Austin, Texas, restaurant, Bess Bistro, located on West 6th Street. She later opened another business, Walton's Fancy and Staple, across the street in a building she extensively renovated. Walton's is a bakery, upscale restaurant and floral shop that also offers services including event planning. After almost nine years in business, Bess Bistro closed on September 20, 2015.
Personal life.
Relationships.
Bullock was once engaged to actor Tate Donovan, whom she met while filming "Love Potion No. 9"; their relationship lasted three years. She previously dated football player Troy Aikman, and actors Matthew McConaughey and Ryan Gosling.
Bullock married motorcycle builder and "Monster Garage" host Jesse James on July 16, 2005. They first met when Bullock arranged for her ten-year-old godson to meet James as a Christmas present. In November 2009, Bullock and James entered into a custody battle with James' second ex-wife, former pornographic actress Janine Lindemulder, with whom James had a child. Bullock and James subsequently won full legal custody of James' five-year-old daughter.
In March 2010, a scandal arose when several women claimed to have had affairs with James during his marriage to Bullock. Bullock canceled European promotional appearances for "The Blind Side" citing "unforeseen personal reasons." On March 18, 2010, James responded to the rumors of infidelity by issuing a public apology to Bullock. He stated, "The vast majority of the allegations reported are untrue and unfounded" and "Beyond that, I will not dignify these private matters with any further public comment." James declared that "There is only one person to blame for this whole situation, and that is me", and asked that his wife and children one day "find it in their hearts to forgive me" for their current "pain and embarrassment." James' publicist subsequently announced on March 30, 2010, that James had checked into a rehab facility "to deal with personal issues" and "save his marriage" to Bullock. However, on April 28, 2010, it was reported that Bullock had filed for divorce on April 23 in Austin. Their divorce was finalized on June 28, 2010, with "conflict of personalities" cited as the reason.
As of late 2015, Bullock is dating photographer Bryan Randall.
Children.
Bullock announced on April 28, 2010, that she had proceeded with plans to adopt a son born in January 2010 in New Orleans. Bullock and James had begun an initial adoption process four months earlier. Bullock's son began living with them in January 2010, but they chose to keep the news private until after the Oscars in March 2010. However, given the couple's separation and then divorce, Bullock continued the adoption of her son Louis Bardo Bullock, as a single parent.
In December 2015, Bullock announced that she had adopted a second child, and appeared on the cover of "People" magazine with her then year-old new daughter.
Accidents.
On December 20, 2000, Bullock, the only other passenger, and the two crew all escaped the crash of a chartered business jet uninjured. The pilots were unable to activate the runway lights during a night landing at Jackson Hole Airport due to not using up-to-date approach plates, but continued the landing anyway. The aircraft landed in the airport's graded safety area between the runway and parallel taxiway and hit a snowbank. The accident caused a separation of the nose cone and landing gear, partial separation of the right wing, and a bend in the left wing.
On April 18, 2008, while Bullock was in Massachusetts shooting the film "The Proposal", she and her husband were in an SUV that was hit head-on (driver's side offset) at moderate speed by a drunken driver. Vehicle damage was not major and there were no injuries.
Legal matters.
In October 2004, Bullock won a multimillion-dollar judgment against Benny Daneshjou, the builder of her Lake Austin, Texas home; the jury ruled the house was uninhabitable. It has since been torn down and rebuilt. Daneshjou and his insurer later settled with Bullock for roughly half the awarded verdict.
On April 22, 2007, Marcia Diana Valentine was found lying outside James and Bullock's Southern California home in Orange County. When James confronted the woman, she ran to her car, got behind the wheel, and tried to run over him. The woman is said to be an obsessed fan of Sandra Bullock. The woman was charged with one felony count each of aggravated assault and stalking. Bullock obtained a restraining order to bar Valentine from "contacting or coming near her home, family or work for three years". Valentine pleaded not guilty to charges of aggravated assault and stalking. She was subsequently convicted of stalking and sentenced to three years' probation.
Beginning in 2002, Bullock was also stalked across several states by Thomas James Weldon. In 2003 Bullock obtained a restraining order against him, which was renewed in 2006. After the restraining order expired and Weldon was released from a mental institution, he again traveled across several states to find Bullock; she then obtained another restraining order.
Philanthropy.
Bullock has been a public supporter of the American Red Cross, having donated $1 million to the organization at least four times. Her first public donation of that amount was to the Red Cross's Liberty Disaster Relief Fund. Three years later, she sent money in response to the 2004 Indian Ocean earthquake and tsunamis. In 2010, she donated $1 million to relief efforts in Haiti following the Haiti earthquake, and again donated the same amount following the 2011 Tōhoku earthquake and tsunami.
Along with other stars, Bullock did a public service announcement urging people to sign a petition for clean-up efforts of the oil spill in the Gulf of Mexico. Bullock backs the Texas non-profit organization The Kindred Life Foundation, Inc., and in late 2008 joined other top celebrities in supporting the work of KLF's founder and CEO Amos Ramirez. At a fundraising gala for the organization, Bullock said, "Amos has led many efforts across our nation that have helped families that are in need. Our country needs more organizations that are committed to the service that Kindred Life is."
In 2012, Bullock was inducted into the Warren Easton Hall of Fame for her donations to charities, and in 2013 was honored with the Favorite Humanitarian Award at the 2013 People's Choice Awards for her contributions to New Orleans' Warren Easton Charter High School, which was severely damaged by 2005's Hurricane Katrina.

</doc>
<doc id="29458" url="https://en.wikipedia.org/wiki?curid=29458" title="Smallfilms">
Smallfilms

Smallfilms is a British company that made animated television programmes for children, from 1959 to the 1980s. In 2014 the company was operating again and producing a remake of the Clangers. It was originally a partnership between Oliver Postgate (writer, animator and narrator) and Peter Firmin (modelmaker and illustrator). Several very popular series of short films were made using stop-motion animation, including "Clangers", "Noggin the Nog", and "Ivor the Engine". Another Smallfilms production, "Bagpuss", came top of a BBC poll to find the favourite children's programme.
Background.
In 1957 Postgate was appointed a stage manager with Associated-Rediffusion, which then held the commercial weekday television franchise for London. Attached to the children's programming section, he thought he could do better with the relatively low budgets of the then black and white television productions.
Postgate wrote "Alexander the Mouse," a story about a mouse born to be king. Using an Irish-produced magnetic system – on which animated characters were attached to a painted background, and then photographed through a 45 degree mirror – he persuaded Peter Firmin, who was then teaching at the Central School of Art, to create the background scenes. Postgate later recalled they undertook around 26 of these programmes live-to-air, which were made harder by the production problems encountered by the use and restrictions of using magnets.
After the relative success of "Alexander the Mouse", Postgate agreed a deal to make the next series on film, for a budget of £175 per programme. Making a stop motion animation table in his bedroom, he wrote the Chinese story "The Journey of Master Ho". This was intended for deaf children, a distinct advantage in that the production required no soundtrack which reduced the production costs. He engaged a painter to produce the backgrounds, but as the painter was classical Chinese-trained he produced them in three-quarters view, rather than in the conventional Egyptian full-view manner used for flat animation under a camera. This resulted in the Firmin-produced characters looking like they were short in one leg, but the success of the production provided the foundation for Postgate and Firmin to start up their own company solely producing animated children's programmes.
History.
Setting up their business in a disused cowshed at Firmin's home in Blean near Canterbury, Kent, Postgate and Firmin worked on children's animation programmes. Based on concepts which mostly originated with Postgate, Firmin did the artwork and built the models, while Postgate wrote the scripts, did the stop motion filming and many of the voices. Smallfilms was resultantly able to produce two minutes of film per day, ten times as much as a conventional animation studio, with Postgate moving the cardboard pieces himself, and working his 16mm camera frame-by-frame with a home-made clicker. As Postgate wholly voiced many of the productions, including the WereBear story tapes, his distinctive voice became familiar to generations of children.
They started in 1959 with "Ivor the Engine", a series for ITV about a Welsh steam locomotive who wanted to sing in a choir. Based on Postgate's wartime encounter with Welshman Denzyl Ellis, who used to be the fireman on the Royal Scot, it was remade in colour for the BBC in the 1970s. This was followed by "Noggin the Nog" for the BBC, which established Smallfilms as a safe and reliable pair of hands to produce children's entertainment, in the days when the number of UK television channels was restricted.
In 2000 Postgate and his friend Loaf set up a small publishing company called the Dragons Friendly Society to look after Noggin the Nog, Pogles Wood, Pingwings.
After Postgate's death in December 2008 Smallfilms was inherited by his son Daniel Postgate. Also, Universal took the distribution rights to the works of Smallfilms. Any such agreement does not include the materials Oliver published with The Dragons Friendly Society.
In 2014 Postgate's son Daniel was working with Peter Firmin on a remake of the Clangers. 
Series development and philosophy.
Postgate and Firmin recognised that their product was not to be sold to or bought by children, but by the commissioning television executives. Postgate described in a later interview the then "gentlemanly and rather innocent" business of programme commissioning thus: "We would go to the BBC once a year, show them the films we'd made, and they would say: 'Yes, lovely, now what are you going to do next?' We would tell them, and they would say: 'That sounds fine, we'll mark it in for eighteen months from now,' and we would be given praise and encouragement and some money in advance, and we'd just go away and do it."
Postgate had strict views on storyline development, which perhaps resultantly restricted the length of each particular series development. When asked if the "Clangers" adventures were quite surreal sometimes, Postgate replied "They're surreal but logical. I have a strong prejudice against fantasy for its own sake. Once one gets to a point beyond where cause and effect mean anything at all, then science fiction becomes science nonsense. Everything that happened was strictly logical according to the laws of physics which happened to apply in that part of the world."
In June 2015, the BBC’s Mark Savage reported: "Firmin said the "Clangers"' surrealism had led to accusations that Postgate was taking hallucinogenic drugs". Firmin told Savage: "People used to say, 'ooh, what's Oliver on, with all of these weird ideas?' And we used to say, 'he's on cups of tea and biscuits.'"
The Smallfilms system was reliant on the company's only two employees – Postgate and Firmin – and was devoid of the modern considerations and essentials, as Postgate pointed out: "excused the interference of educationalists, sociologists and other pseudo-scientists, which produces eventually a confection of formulae which have no integrity. No, the mainspring of what we did was because it was fun."
Recognising their commissioning audience, Smallfilms purposefully developed storylines which were engaging for both adults and children. While the storylines and production were remembered by children, the adult jokes like those about the Welsh in "Ivor the Engine", or the fact that the Clangers swore occasionally; gave them both an instant parent engagement as well as a later revival with children who had grown up and were re-watching their favourite programmes.
Coolabi.
In October 2008, production company Coolabi acquired the merchandising and distribution rights until 2013 to a number of the Smallfilms productions. Coolabi planned to introduce Bagpuss to a new generation; the company said there was "significant potential to build on the affection in which this classic brand is held".

</doc>
<doc id="29460" url="https://en.wikipedia.org/wiki?curid=29460" title="List of mayors of Sacramento, California">
List of mayors of Sacramento, California

This is a list of mayors of Sacramento, California. The Sacramento City Council met for the first time on August 1, 1849 and the citizens approved the city charter on October 13, 1849. The City Charter was recognized by the State of California on February 27, 1850 and Sacramento was incorporated on March 18, 1850.
"See also :" Lists of incumbents
References.
http://ohp.parks.ca.gov/?page_id=21454

</doc>
<doc id="29462" url="https://en.wikipedia.org/wiki?curid=29462" title="Sabotage">
Sabotage

Sabotage is a deliberate action aimed at weakening a polity or corporation through subversion, obstruction, disruption or destruction. In a workplace setting, sabotage is the conscious withdrawal of efficiency generally directed at causing some change in workplace conditions. One who engages in sabotage is a saboteur. Saboteurs typically try to conceal their identities because of the consequences of their actions.
Any unexplained adverse condition might be sabotage. Sabotage is sometimes called tampering, meddling, tinkering, malicious pranks, malicious hacking, a practical joke or the like to avoid needing to invoke legal and organizational requirements for addressing sabotage.
Etymology.
The word "sabotage" appears in the beginning of the 19th century from the French word "sabotage". It is sometimes said that some workers (from Netherlands for some, canuts from Lyon for others, luddites in England, etc.) used to throw their "sabots" (clogs) in the machines to break them, but this is not supported by the etymology.
One of its first appearances in French literature is in the "Dictionnaire du Bas-Langage ou manières de parler usitées parmi le peuple" of D'Hautel, edited en 1808
The verb "saboter" is also found in 1873-1874 in the "Dictionnaire de la langue française" of Émile Littré. But it is at the end of the 19th century that it really began to be used with the meaning of "deliberately and maliciously destroying property" or "working slower". In 1897, Émile Pouget, a famous syndicalist and anarchist wrote "action de saboter un travail" (action of sabotaging a work) in "Le Père Peinard" and in 1911 he also wrote a book entitled "Le Sabotage".
As industrial action.
At the inception of the Industrial Revolution, skilled workers such as the Luddites (1811–1812) used sabotage as a means of negotiation in labor disputes.
Labor unions such as the Industrial Workers of the World (IWW) have advocated sabotage as a means of self-defense and direct action against unfair working conditions.
The IWW was shaped in part by the industrial unionism philosophy of Big Bill Haywood, and in 1910 Haywood was exposed to sabotage while touring Europe:
The experience that had the most lasting impact on Haywood was witnessing a general strike on the French railroads. Tired of waiting for parliament to act on their demands, railroad workers walked off their jobs all across the country. The French government responded by drafting the strikers into the army and then ordering them back to work. Undaunted, the workers carried their strike to the job. Suddenly, they could not seem to do anything right. Perishables sat for weeks, sidetracked and forgotten. Freight bound for Paris was misdirected to Lyon or Marseille instead. This tactic — the French called it "sabotage" — won the strikers their demands and impressed Bill Haywood.
For the IWW, sabotage came to mean any withdrawal of efficiency, including the slowdown, the strike, working to rule, or creative bungling of job assignments.
One of the most severe examples was at the construction site of the Robert-Bourassa Generating Station in 1974, in Québec, Canada, when workers used bulldozers to topple electric generators, damaged fuel tanks, and set buildings on fire. The project was delayed a year, and the direct cost of the damage estimated at $2 million CAD. The causes were not clear, but three possible factors have been cited: inter-union rivalry, poor working conditions, and the perceived arrogance of American executives of the contractor, Bechtel Corporation.
As environmental action.
Certain groups turn to destruction of property to stop environmental destruction or to make visible arguments against forms of modern technology they consider detrimental to the environment. The U.S. Federal Bureau of Investigation (FBI) and other law enforcement agencies use the term eco-terrorist when applied to damage of property. Proponents argue that since property cannot feel terror, damage to property is more accurately described as sabotage. Opponents, by contrast, point out that property owners and operators can indeed feel terror. The image of the monkey wrench thrown into the moving parts of a machine to stop it from working was popularized by Edward Abbey in the novel "The Monkey Wrench Gang" and has been adopted by eco-activists to describe destruction of earth damaging machinery.
As war tactic.
In war, the word is used to describe the activity of an individual or group not associated with the military of the parties at war, such as a foreign agent or an indigenous supporter, in particular when actions result in the destruction or damaging of a productive or vital facility, such as equipment, factories, dams, public services, storage plants or logistic routes. Prime examples of such sabotage are the events of Black Tom and the Kingsland Explosion. Like spies, saboteurs who conduct a military operation in civilian clothes or enemy uniforms behind enemy lines are subject to prosecution and criminal penalties instead of detention as prisoners of war. It is common for a government in power during war or supporters of the war policy to use the term loosely against opponents of the war. Similarly, German nationalists spoke of a stab in the back having cost them the loss of World War I.
A modern form of sabotage is the distribution of software intended to damage specific industrial systems. For example, the U.S. Central Intelligence Agency (CIA) is alleged to have sabotaged a Siberian pipeline during the Cold War, using information from the Farewell Dossier. A more recent case may be the Stuxnet computer worm, which was designed to subtly infect and damage specific types of industrial equipment. Based on the equipment targeted and the location of infected machines, security experts believe it was an attack on the Iranian nuclear program by the United States, Israel or, according to the latest news, even Russia.
Sabotage, done well, is inherently difficult to detect and difficult to trace to its origin. During World War II, the U.S. Federal Bureau of Investigation (FBI) investigated 19,649 cases of sabotage and concluded the enemy had not caused any of them.
Sabotage in warfare, according to the Office of Strategic Services (OSS) manual, varies from highly technical "coup de main" acts that require detailed planning and specially trained operatives, to innumerable simple acts that ordinary citizen-saboteurs can perform. Simple sabotage is carried out in such a way as to involve a minimum danger of injury, detection, and reprisal. There are two main methods of sabotage; physical destruction and the "human element." While physical destruction as a method is self-explanatory, its targets are nuanced, reflecting objects to which the saboteur has normal and inconspicuous access in everyday life. The "human element" is based on universal opportunities to make faulty decisions, to adopt a non-cooperative attitude, and to induce others to follow suit.
There are many examples of physical sabotage in wartime. However, one of the most effective uses of sabotage is against organizations. The OSS manual provides numerous techniques under the title "General Interference with Organizations and Production":
From the section entitled, "General Devices for Lowering Morale and Creating Confusion" comes the following quintessential simple sabotage advice: "Act stupid."
Value of simple sabotage in wartime.
The United States Office of Strategic Services, later renamed the CIA, noted specific value in committing simple sabotage against the enemy during wartime: "... slashing tires, draining fuel tanks, starting fires, starting arguments, acting stupidly, short-circuiting electric systems, abrading machine parts will waste materials, manpower, and time." To underline the importance of simple sabotage on a widespread scale, they wrote, "Widespread practice of simple sabotage will harass and demoralize enemy administrators and police." The OSS was also focused on the battle for hearts and minds during wartime; "the very practice of simple sabotage by natives in enemy or occupied territory may make these individuals identify themselves actively with the United Nations War effort, and encourage them to assist openly in periods of Allied invasion and occupation."
In World War I.
On 30 July 1916, the Black Tom explosion occurred when German agents set fire to a complex of warehouses and ships in Jersey City, New Jersey that held munitions, fuel, and explosives bound to aid the Allies in their fight.
On 11 January 1917, Fiodore Wozniak, using a rag saturated with phosphorus or an incendiary pencil supplied by German sabotage agents, set fire to his workbench at an ammunition assembly plant near Lyndhurst, New Jersey, causing a four-hour fire that destroyed half a million 3-inch explosive shells and destroyed the plant for an estimated at $17 million in damages. Wozniak's involvement was not discovered until 1927.
On 12 February 1917, Bedouins allied with the British destroyed a Turkish railroad near the port of Wajh, derailing a Turkish locomotive. The Bedouins traveled by camel and used explosives to demolish a portion of track.
Post World War I.
In Ireland, the Irish Republican Army (IRA) used sabotage against the British following the Easter 1916 uprising. The IRA compromised communication lines and lines of transportation and fuel supplies. The IRA also employed passive sabotage, refusing dock and train workers to work on ships and rail cars used by the government. In 1920, agents of the IRA committed arson against at least fifteen British warehouses in Liverpool. The following year, the IRA set fire to numerous British targets again, including the Dublin Customs House, this time sabotaging most of Liverpool's firetrucks in the firehouses before lighting the matches.
In World War II.
Sabotage training for the Allies consisted of teaching would-be saboteurs key components of working machinery to destroy.
"Saboteurs learned hundreds of small tricks to cause the Germans big trouble. The cables in a telephone junction box ... could be jumbled to make the wrong connections when numbers were dialed. A few ounces of plastique, properly placed, could bring down a bridge, cave in a mine shaft, or collapse the roof of a railroad tunnel."
The French Resistance ran an extremely effective sabotage campaign against the Germans during World War II. Receiving their sabotage orders through messages over the BBC radio or by aircraft, the French used both passive and active forms of sabotage. Passive forms included losing German shipments and allowing poor quality material to pass factory inspections. Many active sabotage attempts were against critical rail lines of transportation. German records count 1,429 instances of sabotage from French Resistance forces between January 1942 and February 1943. From January through March 1944, sabotage accounted for three times the number of locomotives damaged by Allied air power. See also Normandy Landings for more information about sabotage on D-Day.
During World War II, the Allies committed sabotage against the Peugeot truck factory. After repeated failures in Allied bombing attempts to hit the factory, a team of French Resistance fighters and Special Operations Executive (SOE) agents distracted the German guards with a game of soccer while part of their team entered the plant and destroyed machinery.
In December 1944, the Germans ran a false flag sabotage infiltration, Operation Greif, which was commanded by Waffen-SS commando Otto Skorzeny during the Battle of the Bulge. German commandos, wearing US Army uniforms, carrying US Army weapons, and using US Army vehicles, penetrated US lines to spread panic and confusion among US troops and to blow up bridges, ammunition dumps, and fuel stores and to disrupt the lines of communication. Many of the commandos were captured by the Americans. Because they were wearing US uniforms, a number of the Germans were executed as spies, either summarily or after military commissions.
After World War II.
From 1948 to 1960, the Malayan Communists committed numerous effective acts of sabotage against the Malaysian Government, first targeting railway bridges, then hitting larger targets such as military camps. Most of their efforts were centered around crippling Malaysia's economy and involved sabotage against trains, rubber trees, water pipes, and electric lines. The Communist's sabotage efforts were so successful that they caused backlash amongst the Malaysian population, who gradually withdrew support for the Communist movement as their livelihoods became threatened.
In Mandatory Palestine from 1945 to 1948, Jewish groups opposed British control. Though that control was to end according to the Balfour Declaration in 1948, the groups used sabotage as an opposition tactic. The Haganah focused their efforts on camps used by the British to hold refugees and radar installations that could be used to detect illegal immigrant ships. The Stern Gang and the Irgun used terrorism and sabotage against the British government and against lines of communications. In November 1946, the Irgun and Stern Gang attacked a railroad twenty-one times in a three-week period, eventually causing shell-shocked Arab railway workers to strike. The 6th Airborne Division was called in to provide security as a means of ending the strike.
In Vietnam.
The Viet Cong used swimmer saboteurs often and effectively during the Vietnam War. Between 1969 and 1970, swimmer saboteurs sunk, destroyed, or damaged 77 assets of the U.S. and its allies. Viet Cong swimmers were poorly equipped but well-trained and resourceful. The swimmers provided a low-cost/low-risk option with high payoff; possible loss to the country for failure compared to the possible gains from a successful mission led to the obvious conclusion the swimmer saboteurs were a good idea.
During the Cold War.
On 1 January 1984, the Cuscatlan bridge over Lempa river in El Salvador, critical to flow of commercial and military traffic, was destroyed by guerrilla forces using explosives after using mortar fire to "scatter" the bridge's guards, causing an estimated $3.7 million in required repairs, and considerably impacting on El Salvadoran business and security.
In 1982 in Honduras, a group of nine Salvadorans and Nicaraguans destroyed a main electrical power station, leaving the capital city Tegucigalpa without power for three days.
As crime.
Some criminals have engaged in acts of sabotage for reasons of extortion. For example, Klaus-Peter Sabotta sabotaged German railway lines in the late 1990s in an attempt to extort DM10 million from the German railway operator Deutsche Bahn. He is now serving a sentence of life imprisonment. In 1989, ex-Scotland Yard detective Rodney Whitchelo was sentenced to 17 years in prison for spiking Heinz baby food products in supermarkets, in an extortion attempt on the food manufacturer.
As political action.
The term political sabotage is sometimes used to define the acts of one political camp to disrupt, harass or damage the reputation of a political opponent, usually during an electoral campaign, such as during Watergate. Smear campaigns are a commonly used tactic. The term could also describe the actions and expenditures of private entities, corporations and organizations against democratically approved or enacted laws, policies and programs.
From 1992 to late 2007 an radical environmental activist movement known as ELF or Earth Liberation Front engaged in a near constant campaign of decentralized sabotage of any construction projects near wild lands and extractive industries such as logging and even the burning down of a ski resort of Vail Colorado. ELF used sabotage tactics often in loose coordination with other environmental activist movements to physically delay or destroy threats to wild lands as the political will developed to protect the targeted wild areas that ELF engaged.
In a coup d'etat.
Sabotage is a crucial tool of the successful coup d'etat, which requires control of communications before, during, and after the coup is staged. Simple sabotage against physical communications platforms using semi-skilled technicians, or even those trained only for this task, could effectively silence the target government of the coup, leaving the information battle space open to the dominance of the coup's leaders. To underscore the effectiveness of sabotage, "A single cooperative technician will be able temporarily to put out of action a radio station which would otherwise require a full-scale assault."
Railroads, where strategically important to the regime the coup is against, are prime targets for sabotage- if a section of the track is damaged entire portions of the transportation network can be stopped until it is fixed.
Derivative usages.
Sabotage radio.
A sabotage radio was a small two-way radio designed for use by resistance movements in World War II, and after the war often used by expeditions and similar parties.
Cybotage.
Arquilla and Rondfeldt, in their work entitled "Networks and Netwars", differentiate their definition of "netwar" from a list of "trendy synonyms," including "cybotage," a portmanteau from the words "sabotage" and "cyber." They dub the practitioners of cybotage "cyboteurs" and note while all cybotage is not netwar, some netwar is cybotage.
Counter-sabotage.
Counter-sabotage, defined by "Webster's Dictionary", is "counterintelligence designed to detect and counteract sabotage." The United States Department of Defense definition, found in the "Dictionary of Military and Associated Terms", is "action designed to detect and counteract sabotage. See also counterintelligence".
In World War II.
During World War II, British subject Eddie Chapman, trained by the Germans in sabotage, became a double agent for the British. The German Abwehr entrusted Chapman to destroy the British de Havilland Company's main plant which manufactured the outstanding Mosquito light bomber, but required photographic proof from their agent to verify the mission's completion. A special unit of the Royal Engineers known as the Magic Gang covered the de Havilland plant with canvas panels and scattered papier-mâché furniture and chunks of masonry around three broken and burnt giant generators. Photos of the plant taken from the air reflected devastation for the factory and a successful sabotage mission, and Chapman, as a British sabotage double-agent, fooled the Germans for the duration of the war.
Borrowed into Japanese.
In Japanese, the verb saboru (サボる) means to skip school or loaf on the job.

</doc>
<doc id="29463" url="https://en.wikipedia.org/wiki?curid=29463" title="Scabbard">
Scabbard

A scabbard is a sheath for holding a sword, knife, or other large blade. Scabbards have been made of many materials over the millennia, including leather, wood, and metals such as brass or steel.
Types of scabbards.
Most commonly, scabbards were worn suspended from a sword belt or shoulder belt called a (baldric).
Ancient scabbards.
Wooden scabbards were usually covered in fabric or leather, and leather versions also usually bore metal fittings for added protection and carrying ease. Japanese blades, however, typically have their sharp cutting edge protected by a wooden scabbard called a "saya". Many scabbards like the ones the Greeks and Romans used were small and light. They were designed for holding the sword rather than protecting it. All-metal scabbards were popular items for a display of wealth among elites in the European Iron Age, and often intricately decorated. Little is known about the scabbards of the early Iron Age, due to their wooden construction. However during the Middle and late Iron Ages, the scabbard became important especially as a vehicle for decorative elaboration. After 200 BC fully decorated scabbards became rare. A number of ancient scabbards have been recovered from weapons sacrifices, a few of which had a lining of fur on the inside. The fur was probably kept oily, keeping the blade free from rust. The fur would also allow a smoother, quicker draw.
Modern scabbards.
Entirely metal scabbards became popular in Europe early in the 19th century and eventually superseded most other types. Metal was more durable than leather and could better withstand the rigors of field use, particularly among troops mounted on horseback. In addition, metal offered the ability to present a more military appearance, as well as the opportunity to display increased ornamentation. Nevertheless, leather scabbards never entirely lost favor among military users and were widely used as late as the American Civil War (1861–65).
Some military police forces, naval shore patrols, law enforcement and other groups used leather scabbards as a kind of truncheon.
Scabbards were never worn across one's back in European, Near East, or Indian military cultures and depictions of such are a modern invention and have enjoyed great popularity in fiction and fantasy, to the point that they are widely believed to be a Medieval invention. Some well-known examples of this include the back scabbard depicted in the movie "Braveheart" and the back scabbard seen in the video game series "The Legend of Zelda". There is some limited data from woodcuts and textual fragments that Mongol light horse archers, Chinese soldiers, and Japanese Samurai wore a slung baldric over the shoulder, allowing longer blades to be strapped across the back.
However in "The Ancient Celts" by Barry Cunliffe, on page 94 of that book, Professor Cunliffe writes,"All these pieces of equipment spears, swords, mail armour, mentioned in the texts, are reflected in the archaeological record and in the surviving iconography, though it is sometimes possible to detect regional variations. Among the Parisii of Yorkshire, for example, "the sword was sometimes worn across the back and therefore had to be drawn over the shoulder from behind the head".""
Common terms.
The metal fitting where the blade enters the leather or metal scabbard is called the throat, which is often part of a larger scabbard mount, or locket, that bears a carrying ring or stud to facilitate wearing the sword. The blade's point in leather scabbards is usually protected by a metal tip, or chape, which on both leather and metal scabbards is often given further protection from wear by an extension called a drag, or shoe.

</doc>
<doc id="29467" url="https://en.wikipedia.org/wiki?curid=29467" title="Spinel">
Spinel

Spinel is the magnesium aluminium member of the larger spinel group of minerals. It has the formula MgAl2O4 in the cubic crystal system. Its name comes from Latin "spina" (arrow). Balas ruby is an old name for a rose-tinted variety.
Properties of true spinel.
Spinel crystallizes in the isometric system; common crystal forms are octahedra, usually twinned. It has an imperfect octahedral cleavage and a conchoidal fracture. Its hardness is 8, its specific gravity is 3.5–4.1, and it is transparent to opaque with a vitreous to dull luster. It may be colorless, but is usually various shades of red, blue, green, yellow, brown, or black. There is a unique natural white spinel, now lost, that surfaced briefly in what is now Sri Lanka. Some spinels are among the most famous gemstones: among them are the Black Prince's Ruby and the "Timur ruby" in the British Crown Jewels, and the "Côte de Bretagne", formerly from the French Crown jewels. The Samarian Spinel is the largest known spinel in the world, weighing .
The transparent red spinels were called spinel-rubies or balas rubies. In the past, before the arrival of modern science, spinels and rubies were equally known as rubies. After the 18th century the word ruby was only used for the red gem variety of the mineral corundum and the word spinel came to be used. "Balas" is derived from Balascia, the ancient name for Badakhshan, a region in central Asia situated in the upper valley of the Kokcha River, one of the principal tributaries of the Oxus River. The Badakshan Province was for centuries the main source for red and pink spinels.
Occurrence.
Spinel has long been found in the gemstone-bearing gravel of Sri Lanka and in limestones of the Badakshan Province in modern-day Afghanistan and of Mogok in Burma. Recently gem quality spinels were also found in the marbles of Luc Yen (Vietnam), Mahenge and Matombo (Tanzania), Tsavo (Kenya) and in the gravels of Tunduru (Tanzania) and Ilakaka (Madagascar). Spinel is found as a metamorphic mineral, and also as a primary mineral in rare mafic igneous rocks; in these igneous rocks, the magmas are relatively deficient in alkalis relative to aluminium, and aluminium oxide may form as the mineral corundum or may combine with magnesia to form spinel. This is why spinel and ruby are often found together.
Spinel, (Mg,Fe)(Al,Cr)2O4, is common in peridotite in the uppermost Earth's mantle, between approximately 20 km to approximately 120 km, possibly to lower depths depending on the chromium content. At significantly shallower depths, above the Moho, calcic plagioclase is the more stable aluminous mineral in peridotite while garnet is the stable phase deeper in the mantle below the spinel stability region.
Spinel, (Mg,Fe)Al2O4, is a common mineral in the Ca-Al-rich inclusions (CAIs) in some chondritic meteorites.
Synthetic spinel.
Synthetic spinel, accidentally produced in the middle of the 18th century, has been described more recently in scientific publications in 2000 and 2004. By 2015, transparent spinel was being made in sheets and other shapes through sintering. Synthetic spinel which looks like glass but has notably higher strength against pressure, can also have applications in military and commercial use.

</doc>
<doc id="29468" url="https://en.wikipedia.org/wiki?curid=29468" title="Speech recognition">
Speech recognition

Speech recognition (SR) is the inter-disciplinary sub-field of computational linguistics which incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields to develop methodologies and technologies that enables the recognition and translation of spoken language into text by computers and computerized devices such as those categorized as smart technologies and robotics. It is also known as "automatic speech recognition" (ASR), "computer speech recognition", or just "speech to text" (STT).
Some SR systems use "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent".
Speech recognition applications include voice user interfaces such as voice dialing (e.g. "Call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed Direct Voice Input).
The term "voice recognition" or "speaker identification" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the world-wide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Google, Microsoft, IBM, Baidu (China), Apple, Amazon, Nuance, IflyTek (China), many of which have publicized the core technology in their speech recognition systems as being based on deep learning.
History.
As early as 1932, Bell Labs researchers like Harvey Fletcher were investigating the science of speech perception. In 1952 three Bell Labs researchers built a system for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.
Unfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce's letter compared speech recognition to "schemes for turning water into gasoline, extracting gold from the sea, curing cancer, or going to the moon." Pierce defunded speech recognition research at Bell Labs.
Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess. Also around this time Soviet researchers invented the dynamic time warping algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. Achieving speaker independence was a major unsolved goal of researchers during this time period.
In 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. BBN. IBM., Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter. Despite the fact that CMU's Harpy system met the goals established at the outset of the program, many of the predictions turned out to be nothing more than hype disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.
During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. At CMU, Raj Reddy's students James Baker and Janet Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.
Under Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominate speech recognition algorithm in the 1980s.
IBM had a few competitors including Dragon Systems founded by James and Janet Baker in 1982. The 1980s also saw the introduction of the n-gram language model. Katz introduced the back-off model in 1987, which allowed language models to use multiple length n-grams. During the same time, also CSELT was using HMM (especially, the diphonies) to recognize language like Italian. At the same time, CSELT led a series of European projects (Esprit I, II), and summarized the state-of-the-art in a book, later (2013) reprinted.
Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. Using these computers it could take up to 100 minutes to decode just 30 seconds of speech. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models.
Another one of Raj Reddy's former students, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Huang went on to found the speech recognition group at Microsoft in 1993.
The 1990s saw the first introduction of commercially successful speech recognition technologies. By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. In 2000, Lernout & Hauspie acquired Dragon Systems and was an industry leader until an accounting scandal brought an end to the company in 2001. The L&H speech technology was bought by ScanSoft which became Nuance in 2005. In 2011 Nuance acquired Loquendo, the company spin-off from the former CSELT speech technology group in 2001. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.
21st Century.
In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, 
Cambridge University, and a team composed of ISCI, SRI and University of Washington. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages.
In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.
In the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks.
Today, however, many aspects of speech recognition have been taken over by a deep learning method called Long short term memory (LSTM), 
a recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech.
Around 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.
The use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979." In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.
In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 80's, 90's and a few years into 2000.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.
A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models.
All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009-2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.
Models, methods, and algorithms.
Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.
Hidden Markov models.
Modern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.
Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of "n"-dimensional real-valued vectors (with "n" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.
Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).
Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).
A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.
Dynamic time warping (DTW)-based speech recognition.
Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.
Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.
A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are "warped" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.
Neural networks.
Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, and speaker adaptation.
In contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. Few assumptions on the statistics of input features are made with neural networks. However, in spite of their effectiveness in classifying short-time units such as individual phones and isolated words, neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.
However, recently LSTM Recurrent Neural Networks (RNNs) and Time Delay Neural Networks(TDNN's) have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition.
Deep Neural Networks and Denoising Autoencoders were also being experimented with to tackle this problem in an effective manner.
Due to the inability of feedforward Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.
Deep Feedforward and Recurrent Neural Networks.
A deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.
A success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.
recent overview articles.
One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.
Many aspects of speech recognition have been taken over by a deep learning method called Long short term memory (LSTM), 
a recurrent neural network published by Hochreiter & Schmidhuber in 1997.
LSTM is normally augmented by recurrent gates called forget gates. 
LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks that require memories of events that happened thousands of discrete time steps ago, which is very important for speech recognition. 
Many applications use stacks of LSTM RNNs and train them by Connectionist Temporal Classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition. Around 2007, CTC-trained LSTM outperformed for the first time traditional speech recognition in certain applications. In 2014, the Chinese search giant Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without using any traditional speech processing methods.
LSTM also improved large-vocabulary speech recognition, text-to-speech synthesis, also for Google Android, and photo-real talking heads. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users. Speech conferences such as INTERSPEECH and ICASSP have published numerous LSTM applications.
Since the initial successful debut of DNNs for speech recognition around 2009-2011 and of LSTM around 2007, there have been huge new progresses made. This progress (as well as future directions) has been summarized into the following eight major areas: 
Large-scale automatic speech recognition is the first and the most convincing successful case of deep learning in the recent history, embraced by both industry and academic across the board. Between 2010 and 2014, the two major conferences on signal processing and speech recognition, IEEE-ICASSP and Interspeech, have seen near exponential growth in the numbers of accepted papers in their respective annual conference papers on the topic of deep learning for speech recognition. More importantly, all major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) nowadays are based on deep learning methods. See also the recent media interview with the CTO of Nuance Communications.
Applications of Speech Recognition.
In-car systems.
Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.
Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands. allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.
Health care.
Medical documentation.
In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.
One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.
A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice "macros", where the use of certain phrases - e.g., "normal report", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam - e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.
As an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.
Therapeutic use.
Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.
Military.
High-performance fighter aircraft.
Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note is the U.S. program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), and a program in France installing speech recognition systems on Mirage aircraft, and also programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.
Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing G-loads. It was also concluded that adaptation greatly improved the results in all cases and introducing models for breathing was shown to improve recognition scores significantly. Contrary to what might be expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as could be expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.
The Eurofighter Typhoon currently in service with the UK RAF employs a speaker-dependent system, i.e. it requires each pilot to create a template. The system is not used for any safety critical or weapon critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to himself with two simple voice commands or to any of his wingmen with only five commands.
Speaker-independent systems are also being developed and are in testing for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy in excess of 98%.
Helicopters.
The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.
As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.
Training air traffic controllers.
Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation.
Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.
The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.
Telephony and other domain.
ASR in the field of telephony is now commonplace and in the field of computer gaming and simulation is becoming more widespread. Despite the high level of integration with word processing in general personal computing. However, ASR in the field of document production has not seen the expected increases in use.
The improvement of mobile processor speeds made feasible the speech-enabled Symbian and Windows Mobile smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands. Leading software vendors in this field are: Google, Microsoft Corporation (Microsoft Voice Command), Digital Syphon (Sonic Extractor), LumenVox, Nuance Communications (Nuance Voice Control), VoiceBox Technology, Speech Technology Center, Vito Technologies (VITO Voice2Go), Speereo Software (Speereo Voice Translator), Verbyx VRX and SVOX.
Usage in education and daily life.
For language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.
Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.
Students who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.
Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.
Voice Recognition Software's use, in conjunction with a digital audio recorder, a personal computer and Microsoft Word has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.
People with disabilities.
People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.
Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.
This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.
Performance.
The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).
However, speech recognition (by a machine) is a very complex problem. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition vary with the following:
Accuracy.
As mentioned earlier in this article, accuracy of speech recognition varies in the following:
e.g. The 10 digits "zero" to "nine" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.
e.g. The 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: "B, C, D, E, G, P, T, V, Z");
an 8% error rate is considered good for this vocabulary.
A speaker-dependent system is intended for use by a single speaker.
A speaker-independent system is intended for use by any speaker, more difficult.
With isolated speech single words are used, therefore it becomes easier to recognize the speech.
With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. 
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.
e.g. Querying application may dismiss the hypothesis "The apple is red." 
e.g. Constraints may be semantic; rejecting "The apple is angry." 
e.g. Syntactic; rejecting "Red is apple the." 
Constraints are often represented by a grammar. 
When a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like "uh" and "um", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary. 
Environmental noise (e.g. Noise in a car or a factory) 
Acoustical distortions (e.g. echoes, room acoustics)
Speech recognition is a multi-levelled pattern recognition task.
e.g. Phonemes, Words, Phrases, and Sentences;
e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;
By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level;
Speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches: 
For telephone speech the sampling rate is 8000 samples per second; 
computed every 10 ms, with one 10 ms section called a frame;
Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has 2 descriptions; Amplitude (how strong is it), and frequency (how often it vibrates per second).
The sound waves can be digitized: Sample a strength at short intervals like in picture above to get bunch of numbers that approximate at each time step the strength of a wave. Collection of these numbers represent analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. Like the waves would. This way they create odd-looking waves. For example, if there are two waves that interact with each other we can add them which creates new odd-looking wave.
Given basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. This analysis depends on programmer's instructions. At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. Last level of nodes should be output nodes that tell us with high probability what original sound really was.
Further information.
Conferences and Journals.
Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing --- after merging with an ACM publication), Computer Speech and Language, and Speech Communication.
Books.
Books like "Fundamentals of Speech Recognition" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be "Statistical Methods for Speech Recognition" by Frederick Jelinek and "Spoken Language Processing (2001)" by Xuedong Huang etc. More up to date are "Computer Speech", by Manfred R. Schroeder, second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. The recently updated textbook of "Speech and Language Processing (2008)" by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniuqes as is done in speech recognition. A most recent comprehensive textbook, "Fundamentals of Speaker Recognition" by Homayoon Beigi, is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).
A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. Building Computers That Understand Speech" by Roberto Pieraccini (2012).
The most recent book on speech recognition is "Automatic Speech Recognition: A Deep Learning Approach" (Publisher: Springer) written by D. Yu and L. Deng published near the end of 2014, with highly mathematically-oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, "Deep Learning: Methods and Applications" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009-2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.
Software.
In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). The AT&T libraries GRM and DCD are also general software libraries for large-vocabulary speech recognition. For more recent and state-of-the-art techniques, Kaldi toolkit can be used.
For more software resources, see List of speech recognition software.

</doc>
<doc id="29469" url="https://en.wikipedia.org/wiki?curid=29469" title="Sapphire">
Sapphire

Sapphire (; "sappheiros", 'blue stone', itself derived from Sanskrit "ShaniPriya" which probably referred instead at the time to lapis lazuli) is a typically blue gemstone variety of the mineral corundum, an aluminium oxide (). Trace amounts of elements such as iron, titanium, chromium, copper, or magnesium can give corundum respectively blue, yellow, purple, orange, or green color. Chromium impurities in corundum yield pink or red tint, the latter being called ruby.
Commonly, sapphires are worn in jewelry. Sapphires may be found naturally, by searching through certain sediments (due to their resistance to being eroded compared to softer stones) or rock formations. They also may be manufactured for industrial or decorative purposes in large crystal boules. Because of the remarkable hardness of sapphires – 9 on the Mohs scale (the third hardest mineral, after diamond at 10 and moissanite at 9.5) – and of aluminium oxide in general, sapphires are used in some non-ornamental applications, including infrared optical components, such as in scientific instruments; high-durability windows; wristwatch crystals and movement bearings; and very thin electronic wafers, which are used as the insulating substrates of very special-purpose solid-state electronics (especially integrated circuits and GaN-based LEDs).
Natural sapphires.
The sapphire is one of the three gem-varieties of corundum, the other two being ruby (defined as corundum in a shade of red) and padparadscha (a pinkish orange variety). Although blue is their most well-known color, sapphires may also be colorless and they are found in many colors including shades of gray and black.
The cost of natural sapphires varies depending on their color, clarity, size, cut, and overall quality – as well as their geographic origin. Significant sapphire deposits are found in Eastern Australia, Thailand, Sri Lanka, China (Shandong), Madagascar, East Africa, and in North America in a few locations, mostly in Montana. Sapphire and rubies are often found in the same geographic environment, but one of the gems is usually more abundant in any of the sites.
Blue sapphire.
Color in gemstones breaks down into three components: hue, saturation, and tone. Hue is most commonly understood as the "color" of the gemstone. Saturation refers to the vividness or brightness of the hue, and tone is the lightness to darkness of the hue. Blue sapphire exists in various mixtures of its primary (blue) and secondary hues, various tonal levels (shades) and at various levels of saturation (vividness).
Blue sapphires are evaluated based upon the purity of their primary hue. Purple, violet, and green are the most common secondary hues found in blue sapphires. Violet and purple can contribute to the overall beauty of the color, while green is considered to be distinctly negative. Blue sapphires with up to 15% violet or purple are generally said to be of fine quality. Blue sapphires with any amount of green as a secondary hue are not considered to be fine quality. Gray is the normal saturation modifier or mask found in blue sapphires. Gray reduces the saturation or brightness of the hue, and therefore has a distinctly negative effect.
The color of fine blue sapphires may be described as a vivid medium dark violet to purplish blue where the primary blue hue is at least 85% and the secondary hue no more than 15%, without the least admixture of a green secondary hue or a gray mask.
The Logan sapphire in the National Museum of Natural History, in Washington, D.C., is one of the largest faceted gem-quality blue sapphires in existence.
Sapphires of other colors.
Yellow and green sapphires are also commonly found. Pink sapphires deepen in color as the quantity of chromium increases. The deeper the pink color the higher their monetary value, as long as the color is tending toward the red of rubies. In the United States, a minimum color saturation must be met to be called a ruby, otherwise the stone is referred to as a "pink sapphire".
Sapphires also occur in shades of orange and brown. Colorless sapphires are sometimes used as diamond substitutes in jewelry. Natural padparadscha (pinkish orange) sapphires often draw higher prices than many of even the finest blue sapphires. Recently, more sapphires of this color have appeared on the market as a result of a new artificial treatment method called "lattice diffusion".
Padparadscha.
"Padparadscha" is a delicate light to medium toned pink-orange to orange-pink hue corundum, originally found in Sri Lanka, but also found in deposits in Vietnam and parts of East Africa. Padparadscha sapphires are rare; the rarest of all is the totally natural variety, with no sign of artificial treatment.
The name is derived from the Sanskrit "padma ranga" (padma = lotus; ranga = color), a color akin to the lotus flower (Nelumbo nucifera).
Star sapphire.
A "star sapphire" is a type of sapphire that exhibits a star-like phenomenon known as asterism; red stones are known as "star rubies". Star sapphires contain intersecting needle-like inclusions following the underlying crystal structure that causes the appearance of a six-rayed "star"-shaped pattern when viewed with a single overhead light source. The inclusion is often the mineral rutile, a mineral composed primarily of titanium dioxide. The stones are cut "en cabochon", typically with the center of the star near the top of the dome. Occasionally, twelve-rayed stars are found, typically because two different sets of inclusions are found within the same stone, such as a combination of fine needles of rutile with small platelets of hematite; the first results in a whitish star and the second results in a golden-colored star. During crystallisation, the two types of inclusions become preferentially oriented in different directions within the crystal, thereby forming two six-rayed stars that are superimposed upon each other to form a twelve-rayed star. Misshapen stars or 12-rayed stars may also form as a result of twinning.
The inclusions can alternatively produce a "cat's eye" effect if the 'face-up' direction of the cabochon's dome is oriented perpendicular to the crystal's c-axis rather than parallel to it. If the dome is oriented in between these two directions, an 'off-center' star will be visible, offset away from the high point of the dome.
The Star of Adam is the largest blue star sapphire which weighs 1404.49 carats. The gem was mined in the city of Ratnapura, southern Sri Lanka. The Black Star of Queensland, the largest gem-quality star sapphire in the world, weighs 733 carats. The Star of India (mined in Sri Lanka) (weighing 563.4 carats) is thought to be the second-largest star sapphire (the largest blue), and is currently on display at the American Museum of Natural History in New York City. The 182-carat Star of Bombay, (mined in Sri Lanka), located in the National Museum of Natural History, in Washington, D.C., is another example of a large blue star sapphire. The value of a star sapphire depends not only on the weight of the stone, but also the body color, visibility, and intensity of the asterism.
Color change sapphire.
A rare variety of natural sapphire, known as color-change sapphire, exhibits different colors in different light. Color change sapphires are blue in outdoor light and purple under incandescent indoor light, or green to gray-green in daylight and pink to reddish-violet in incandescent light. Color change sapphires come from a variety of locations, including Thailand and Tanzania. The color-change effect is caused by the interaction of the sapphire, which absorbs specific wavelengths of light, and the light-source, whose spectral output varies depending upon the illuminant. Transition-metal impurities in the sapphire, such as chromium and vanadium, are responsible for the color change.
Certain synthetic color-change sapphires have a similar color change to the natural gemstone alexandrite and they are sometimes marketed as "alexandrium" or "synthetic alexandrite". However, the latter term is a misnomer: synthetic color-change sapphires are, technically, not synthetic alexandrites but rather alexandrite "simulants". This is because genuine alexandrite is a variety of chrysoberyl: not sapphire, but an entirely different mineral.
Source of color.
Rubies are corundum which contain chromium impurities that absorb yellow-green light and result in deeper ruby red color with increasing content. Purple sapphires contain trace amounts of vanadium and come in a variety of shades. Corundum that contains ~0.01% of titanium is colorless. If trace amounts of iron are present, a very pale yellow to green color may be seen. However, if both titanium and iron impurities are present together, and in the correct valence states, the result is a deep-blue color.
Unlike localized ("intra-atomic") absorption of light which causes color for chromium and vanadium impurities, blue color in sapphires comes from intervalence charge transfer, which is the transfer of an electron from one transition-metal ion to another via the conduction or valence band. The iron can take the form Fe2+ or Fe3+, while titanium generally takes the form Ti4+. If Fe2+ and Ti4+ ions are substituted for Al3+, localized areas of charge imbalance are created. An electron transfer from Fe2+ and Ti4+ can cause a change in the valence state of both. Because of the valence change there is a specific change in energy for the electron, and electromagnetic energy is absorbed. The wavelength of the energy absorbed corresponds to yellow light. When this light is subtracted from incident white light, the complementary color blue results. Sometimes when atomic spacing is different in different directions there is resulting blue-green dichroism.
Intervalence charge transfer is a process that produces a strong colored appearance at a low percentage of impurity. While at least 1% chromium must be present in corundum before the deep red ruby color is seen, sapphire blue is apparent with the presence of only 0.01% of titanium and iron.
Treatments.
Sapphires may be treated by several methods to enhance and improve their clarity and color. It is common practice to heat natural sapphires to improve or enhance color. This is done by heating the sapphires in furnaces to temperatures between for several hours, or by heating in a nitrogen-deficient atmosphere oven for seven days or more. Upon heating, the stone becomes more blue in color, but loses some of the rutile inclusions (silk). When high temperatures are used, the stone loses all silk (inclusions) and it becomes clear under magnification. The inclusions in natural stones are easily seen with a jeweler's loupe. Evidence of sapphire and other gemstones being subjected to heating goes back at least to Roman times. Un-heated natural stones are somewhat rare and will often be sold accompanied by a certificate from an independent gemological laboratory attesting to "no evidence of heat treatment".
Yogo sapphires sometimes do not need heat treating because their cornflower blue coloring is uniform and deep, they are generally free of the characteristic inclusions, and they have high uniform clarity. When Intergem Limited began marketing the Yogo in the 1980s as the world's only guaranteed untreated sapphire, heat treatment was not commonly disclosed; by 1982 the heat treatment became a major issue. At that time, 95% of all the world's sapphires were being heated to enhance their natural color. Intergem's marketing of guaranteed untreated Yogos set them against many in the gem industry. This issue appeared as a front page story in the "Wall Street Journal" on 29 August 1984 in an article by Bill Richards, "Carats and Schticks: Sapphire Marketer Upsets The Gem Industry".
Diffusion treatments are used to add impurities to the sapphire to enhance color. Typically beryllium is diffused into a sapphire under very high heat, just below the melting point of the sapphire. Initially ("c." 2000) orange sapphires were created, although now the process has been advanced and many colors of sapphire are often treated with beryllium. The colored layer can be removed when stones chip or are repolished or refaceted, depending on the depth of the impurity layer. Treated padparadschas may be very difficult to detect, and many stones are certified by gemological labs ("e.g.", Gubelin, SSEF, AGTA).
According to United States Federal Trade Commission guidelines, disclosure is required of any mode of enhancement that has a significant effect on the gem's value.
There are several ways of treating sapphire. Heat-treatment in a reducing or oxidising atmosphere (but without the use of any other added impurities) is commonly used to improve the color of sapphires, and this process is sometimes known as "heating only" in the gem trade. In contrast, however, heat treatment combined with the deliberate addition of certain specific impurities (e.g. beryllium, titanium, iron, chromium or nickel, which are absorbed into the crystal structure of the sapphire) is also commonly performed, and this process can be known as "diffusion" in the gem trade. However, despite what the terms "heating only" and "diffusion" might suggest, both of these categories of treatment actually involve diffusion processes.
Mining.
Sapphires are mined from alluvial deposits or from primary underground workings. Commercial mining locations for sapphire and ruby include (but are not limited to) the following countries: Afghanistan, Australia, Myanmar/Burma, Cambodia, China, Colombia, India, Kenya, Laos, Madagascar, Malawi, Nepal, Nigeria, Pakistan, Sri Lanka, Tajikistan, Tanzania, Thailand, USA, and Vietnam. Sapphires from different geographic locations may have different appearances or chemical-impurity concentrations, and tend to contain different types of microscopic inclusions. Because of this, sapphires can be divided into three broad categories: classic metamorphic, non-classic metamorphic or magmatic, and classic magmatic.
Sapphires from certain locations, or of certain categories, may be more commercially appealing than others, particularly classic metamorphic sapphires from Kashmir (Pakistan), Burma, or Sri Lanka that have not been subjected to heat-treatment.
The Logan sapphire, the Star of India, and the Star of Bombay originate from Sri Lankan mines. Madagascar is the world leader in sapphire production (as of 2007) specifically its deposits in and around the town of Ilakaka. Prior to the opening of the Ilakaka mines, Australia was the largest producer of sapphires (such as in 1987). In 1991 a new source of sapphires was discovered in Andranondambo, southern Madagascar. That area has been exploited for its sapphires started in 1993, but it was practically abandoned just a few years later—because of the difficulties in recovering sapphires in their bedrock.
In North America, sapphires have been mined mostly from deposits in Montana: fancies along the Missouri River near Helena, Montana, Dry Cottonwood Creek near Missoula, Montana, and Rock Creek near Philipsburg, Montana. Fine blue Yogo sapphires are found at Yogo Gulch west of Lewistown, Montana. A few gem-grade sapphires and rubies have also been found in the area of Franklin, North Carolina.
The sapphire deposits of Kashmir are still well known in the gem industry, despite the fact that the peak production from this area mostly took place in a relatively short period at the end of the nineteenth and early twentieth centuries. Kashmir-origin contributes meaningfully to the value of a sapphire, and most corundum of Kashmir origin can be readily identified by its characteristic silky appearance and exceptional hue. At present, the world record price-per-carat for sapphire at auction was achieved by a sapphire from Kashmir in a ring, which sold for approximately $242,000 per carat (more than $6.74 million in total, including buyer's premium) in October 2015.
Synthetic sapphire.
In 1902 the French chemist Auguste Verneuil developed a process for producing synthetic sapphire crystals. In the Verneuil process, named after him, fine alumina powder is added to an oxyhydrogen flame, and this is directed downward against a mantle. The alumina in the flame is slowly deposited, creating a teardrop shaped "boule" of sapphire material. Chemical dopants can be added to create artificial versions of the ruby, and all the other natural colors of sapphire, and in addition, other colors never seen in geological samples. Artificial sapphire material is identical to natural sapphire, except it can be made without the flaws that are found in natural stones. The disadvantage of Verneuil process is that the grown crystals have high internal strains. Many methods of manufacturing sapphire today are variations of the Czochralski process, which was invented in 1916 by Polish chemist Jan Czochralski. In this process, a tiny sapphire seed crystal is dipped into a crucible made of the precious metal iridium or molybdenum, containing molten alumina, and then slowly withdrawn upward at a rate of 1 to 100 mm per hour. The alumina crystallizes on the end, creating long carrot-shaped boules of large size up to 200 kg in mass.
Synthetic sapphire is also produced industrially from agglomerated aluminium oxide, sintered and fused (such as by hot isostatic pressing) in an inert atmosphere, yielding a transparent but slightly porous polycrystalline product.
In 2003 the world's production of synthetic sapphire was 250 tons (1.25 × 109 carats), mostly by the United States and Russia. The availability of cheap synthetic sapphire unlocked many industrial uses for this unique material:
The first laser was made with a rod of synthetic ruby. Titanium-sapphire lasers are popular due to their relatively rare capacity to be tuned to various wavelengths in the red and near-infrared region of the electromagnetic spectrum. They can also be easily mode-locked. In these lasers a synthetically produced sapphire crystal with chromium or titanium impurities is irradiated with intense light from a special lamp, or another laser, to create stimulated emission.
High quality synthetic sapphire substrates use in nanotechnology is often called Blue Glass, due to its blue color.
Common applications.
Along with zirconia and aluminium oxynitride, synthetic sapphire is used for shatter resistant windows in armored vehicles and various military body armor suits, in association with composites.
One type of xenon arc lamp (originally called the "Cermax"), which is now known generically as the "ceramic body xenon lamp", uses sapphire crystal output windows that tolerate higher thermal loads – and thus higher output powers when compared with conventional Xe lamps with pure silica window.
Sapphire glass.
One application of synthetic sapphire is "sapphire glass" often called "blue glass" as sapphires are blue in color. Here "glass" is a layman term which refers not to the amorphous state, but to the transparency. Sapphire is not only highly transparent to wavelengths of light between 150 nm (UV) and 5500 nm (IR) (the human eye can discern wavelengths from about 380 nm to 750 nm), but is also extraordinarily scratch-resistant. Sapphire has a value of 9 on the Mohs scale of mineral hardness.
The key benefits of sapphire windows are:
So-called "sapphire glass" refers to crystalline sapphire used as an optical window or cover. Some windows are made from pure sapphire boules that have been grown in a specific crystal orientation, typically along the optical axis, the c-axis, for minimum birefringence for the application. The boules are sliced up into the desired window thickness and finally polished to the desired surface finish. Sapphire optical windows can be polished to a wide range of surface finishes due to its crystal structure and its hardness. The surface finishes of optical windows are normally called out by the scratch-dig specifications in accordance with the globally adopted MIL-O-13830 specification.
Sapphire windows were to be used in Apple's iPhone 5s, iPhone 6, and iPad mini 3 but those products were released with Gorilla Glass screens instead. Two of the Apple Watch models' screens are made from sapphire.
The sapphire windows are used in both high pressure and vacuum chambers for spectroscopy, crystals in various watches, and windows in grocery store barcode scanners since the material's exceptional hardness and toughness makes it very resistant to scratching.
It is used for end windows on some high-powered laser tubes as its wide-band transparency and thermal conductivity allow it to handle very high power densities in the infra-red or UV spectrum without degrading due to heating.
Use as substrate for semiconducting circuits.
Thin sapphire wafers were the first successful use of an insulating substrate upon which to deposit silicon to make the integrated circuits known as silicon on sapphire or "SOS"; now other substrates can also be used for the class of circuits known more generally as silicon on insulator. Besides its excellent electrical insulating properties, sapphire has high thermal conductivity. CMOS chips on sapphire are especially useful for high-power radio-frequency (RF) applications such as those found in cellular telephones, public-safety band radios, and satellite communication systems. "SOS" also allows for the monolithic integration of both digital and analog circuitry all on one IC chip, and the construction of extremely low power circuits.
In one process, after single crystal sapphire boules are grown, they are core-drilled into cylindrical rods, and wafers are then sliced from these cores.
Wafers of single-crystal sapphire are also used in the semiconductor industry as a substrate for the growth of devices based on gallium nitride (GaN). The use of sapphire significantly reduces the cost, because it has about one-seventh the cost of germanium. Gallium nitride on sapphire is commonly used in blue light-emitting diodes (LEDs).
Use for endoprostheses.
Monocrystalline sapphire is fairly biocompatible and the exceptionally low wear of sapphire–metal pairs have led to the introduction (in Ukraine) of sapphire monocrystals for hip 
joint endoprostheses.

</doc>
<doc id="29471" url="https://en.wikipedia.org/wiki?curid=29471" title="Slack voice">
Slack voice

Slack voice (or lax voice) is the pronunciation of consonant or vowels with a glottal opening slightly wider than that occurring in modal voice. Such sounds are often referred to informally as lenis or half-voiced in the case of consonants. In some Chinese varieties, such as Wu, and in many Austronesian languages, the 'intermediate' phonation of slack stops confuses listeners of languages without these distinctions, so that different transcription systems may use or for the same consonant. In Xhosa, slack-voiced consonants have usually been transcribed as breathy voice. Although the IPA has no dedicated diacritic for slack voice, the voiceless diacritic (the under-ring) may be used with a voiced consonant letter, though this convention is also used for partially voiced consonants in languages such as English.
Wu Chinese "muddy" consonants are slack voice word-initially, the primary effect of which is a slightly breathy quality of the following vowel.
Javanese contrasts slack and stiff voiced bilabial, dental, retroflex, and velar stops.
Parauk contrasts slack voicing in its vowels. The contrast is between "slightly stiff" and "slightly breathy" vowels; the first are between modal and stiff voice, while the latter are captured by slack voice.

</doc>
<doc id="29472" url="https://en.wikipedia.org/wiki?curid=29472" title="SADC">
SADC

SADC may stand for:

</doc>
<doc id="29473" url="https://en.wikipedia.org/wiki?curid=29473" title="Salvation">
Salvation

Salvation (Latin "salvatio"; Greek "sōtēria"; Hebrew "yasha") is being saved or protected from harm or being saved or delivered from some dire situation. In religion, salvation is stated as the saving of the soul from sin and its consequences.
The academic study of salvation is called soteriology.
Meaning.
In religion, salvation is the saving of the soul from sin and its consequences. It may also be called "deliverance" or "redemption" from sin and its effects. Salvation is considered to be caused either by the grace of a deity, by free will and personal efforts through prayer and asceticism, or by some combination of the two. Religions often emphasize the necessity of both personal effort—for example, repentance and asceticism—and divine action (e.g. grace).
Abrahamic religions.
Judaism.
In contemporary Judaism, redemption (Hebrew "ge'ulah"), refers to God redeeming the people of Israel from their various exiles. This includes the final redemption from the present exile.
Judaism holds that adherents do not need personal salvation as Christians believe. Jews do not subscribe to the doctrine of Original sin. Instead, they place a high value on individual morality as defined in the law of God — embodied in what Jews know as the Torah or The Law, given to Moses by God on Mount Sinai, the summary of which is comprised in the Ten Commandments. The Jewish sage Hillel the Elder states that The Law can be further compressed in just one line, popularly known as the Golden Rule: "That which is hateful to you, do not do unto your fellow".
In Judaism, salvation is closely related to the idea of redemption, a saving from the states or circumstances that destroy the value of human existence. God as the universal spirit and Creator of the World, is the source of all salvation for humanity, provided an individual honours God by observing his precepts. So redemption or salvation depends on the individual. Judaism stresses that salvation cannot be obtained through anyone else or by just invoking a deity or believing in any outside power or influence.
When examining Jewish intellectual sources throughout history, there is clearly a spectrum of opinions regarding death versus the Afterlife. Possibly an over-simplification, one source says salvation can be achieved in the following manner: Live a holy and righteous life dedicated to Yahweh, the God of Creation. Fast, worship, and celebrate during the appropriate holidays.
By origin and nature, Judaism is an ethnic religion. Therefore, salvation has been primarily conceived in terms of the destiny of Israel as the elect people of Yahweh (often referred to as “the Lord”), the God of Israel. In the biblical text of Psalms, there is a description of death, when people go into the earth or the "realm of the dead" and cannot praise God. The first reference to resurrection is collective in Ezekiel's vision of the dry bones, when all the Israelites in exile will be resurrected. There is a reference to individual resurrection in the Book of Daniel (165 BCE), the last book of the Hebrew Bible. It was not until the 2nd century BCE that there arose a belief in an afterlife, in which the dead would be resurrected and undergo divine judgment. Before that time, the individual had to be content that his posterity continued within the holy nation.
The salvation of the individual Jew was connected to the salvation of the entire people. This belief stemmed directly from the teachings of the Torah. In the Torah, God taught his people sanctification of the individual. However, he also expected them to function together (spiritually) and be accountable to one another. The concept of salvation was tied to that of restoration for Israel.
Christianity.
Christianity’s primary premise is that the incarnation and death of Jesus Christ formed the climax of a divine plan for humanity’s salvation. This plan was conceived by God consequent on the Fall of Adam, the progenitor of the human race, and it would be completed at the Last Judgment, when the Second Coming of Christ would mark the catastrophic end of the world.
For Christianity, salvation is only possible through Jesus Christ. Christians believe that Jesus' death on the cross was the once-for-all sacrifice that atoned for the sin of humanity.
The Christian religion, though not the exclusive possessor of the idea of redemption, has given to it a special definiteness and a dominant position. Taken in its widest sense, as deliverance from dangers and ills in general, most religions teach some form of it. It assumes an important position, however, only when the ills in question form part of a great system against which human power is helpless.
According to Christian belief, sin as the human predicament is considered to be universal. For example, in the Apostle Paul declared everyone to be under sin—Jew and Gentile alike. Similarly, the Apostle John was explicit: "If we say that we have no sin, we deceive ourselves and the truth is not in us". Again, he said, "Should we say that we have not sinned, we make him a liar and his word is not in us". Salvation is made possible by the life, death, and resurrection of Jesus, which in the context of salvation is referred to as the "atonement". Christian soteriology ranges from exclusive salvation to universal reconciliation concepts. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.
Variant views on salvation are among the main fault lines dividing the various Christian denominations, both between Roman Catholicism and Protestantism and within Protestantism, notably in the Calvinist–Arminian debate, and the fault lines include conflicting definitions of depravity, predestination, atonement, but most pointedly justification.
Salvation is believed to be a process that begins when a person first becomes a Christian, continues through that person's life, and is completed when they stand before Christ in judgment. Therefore, according to Catholic apologist James Akin, the faithful Christian can say in faith and hope, "I "have been" saved; I "am being" saved; and I "will be" saved."
Christian salvation concepts are varied and complicated by certain theological concepts, traditional beliefs, and dogmas. Scripture is subject to individual and ecclesiastical interpretations. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.
The purpose of salvation is debated, but in general most Christian theologians agree that God devised and implemented his plan of salvation because he loves them and regards human beings as his children. Since human existence on Earth is said to be "", salvation also has connotations that deal with the liberation of human beings from sin, and the suffering associated with the punishment of sin—i.e., "the wages of sin is death."
Christians believe that salvation depends on the grace of God. Stagg writes that a fact assumed throughout the Bible is that humanity is in "serious trouble from which we need deliverance…. The fact of sin as the human predicament is implied in the mission of Jesus, and it is explicitly affirmed in that connection". By its nature, salvation must answer to the plight of humankind as it actually is. Each individual's plight as sinner is the result of a fatal choice involving the whole person in bondage, guilt, estrangement, and death. Therefore, salvation must be concerned with the total person. "It must offer redemption from bondage, forgiveness for guilt, reconciliation for estrangement, renewal for the marred image of God".
Islam.
In Islam, salvation refers to the eventual entrance to heaven. Islam teaches that people who die disbelieving in the God do not receive salvation. It also teaches that non-Muslims who die believing in the God but disbelieving in his message (Islam), are left to his will. Those who die believing in the One God and his message (Islam) receive salvation.
Narrated Anas Radeyallāhu ′Anhu that Muhammad said,
Islam teaches that all who enter into Islam must remain so in order to receive salvation.
For those who have not been granted Islam or to whom the message has not been brought;
Tawhid.
Belief in the “One God”, also known as the "Tawhid" (التَوْحيدْ) in Arabic, consists of two parts (or principles):
Sin and repentance.
Islam also stresses that in order to gain salvation, one must also avoid sinning along with performing good deeds. Islam acknowledges the inclination of humanity towards sin. Therefore, Muslims are constantly commanded to seek God's forgiveness and repent. Islam teaches that no one can gain salvation simply by virtue of their belief or deeds, instead it is the Mercy of God, which merits them salvation. However, this repentance must not be used to sin any further. Islam teaches that God is Merciful, but it also teaches that He is Omnipresent. The Quran states:
Islam describes a true believer to have Love of God and Fear of God. Islam also teaches that every person is responsible for their own sins. The Quran states;
Al-Agharr al-Muzani, a companion of Mohammad, reported that Ibn 'Umar stated to him that Mohammad said,
Sin in Islam is not a state, but an action (a bad deed); Islam teaches that a child is born sinless, regardless of the belief of his parents, dies a Muslim; he enters heaven, and does not enter hell. 
Five Pillars.
There are acts of worship that Islam teaches to be mandatory. Islam is built on five principles. Narrated Ibn 'Umar that Muhammad said,
Not performing the mandatory acts of worship may deprive Muslims of the chance of salvation.
Indian religions.
Hinduism, Buddhism, Jainism and Sikhism share certain key concepts, which are interpreted differently by different groups and individuals. In those religions one is not liberated from sin and its consequences, but from the saṃsāra (cycle of rebirth) perpetuated by passions and delusions and its resulting karma. They differ however on the exact nature of this liberation. Salvation is called "moksha" or "mukti" which mean liberation and release respectively. This state and the conditions considered necessary for its realization is described in early texts of Indian religion such as the Upanishads and the Pāli Canon, and later texts such the Yoga Sutras of Patanjali and the Vedanta tradition. "Moksha" can be attained by sādhanā, literally "means of accomplishing something". It includes a variety of disciplines, such as yoga and meditation.
Nirvana is the profound peace of mind that is acquired with moksha (liberation). In Buddhism and Jainism, it is the state of being free from suffering. In Hindu philosophy, it is union with the Brahman (Supreme Being). The word literally means "blown out" (as in a candle) and refers, in the Buddhist context, to the blowing out of the fires of desire, aversion, and delusion, and the imperturbable stillness of mind acquired there-after.
In Theravada Buddhism the emphasis is on one's own liberation from samsara. The Mahayana traditions emphasize the bodhisattva path, in which "each Buddha and Bodhisattva is a redeemer", assisting the Buddhist in seeking to achieve the redemptive state. The assistance rendered is a form of self-sacrifice on the part of the teachers, who would presumably be able to achieve total detachment from worldly concerns, but have instead chosen to remain engaged in the material world to the degree that this is necessary to assist others in achieving such detachment.
Jainism.
In Jainism, "salvation", "moksa" and "nirvana" are one and the same. When a soul ("atman") achieves moksa, it is released from the cycle of births and deaths, and achieves its pure self. It then becomes a "siddha" (literally means one who has accomplished his ultimate objective). Attaining Moksa requires annihilation of all "karmas", good and bad, because if karma is left, it must bear fruit.

</doc>
<doc id="29475" url="https://en.wikipedia.org/wiki?curid=29475" title="Lockheed S-3 Viking">
Lockheed S-3 Viking

The Lockheed S-3 Viking is a four-seat, twin-engine turbofan-powered jet aircraft that was used by the U.S. Navy to identify and track enemy submarines. In the late 1990s, the S-3B's mission focus shifted to surface warfare and aerial refueling. The Viking also provided electronic warfare and surface surveillance capabilities to the carrier battle group. A carrier-based, subsonic, all-weather, multi-mission aircraft with long range; it carried automated weapon systems, and was capable of extended missions with in-flight refueling. Because of the Viking's engines’ low-pitched sound, it was nicknamed the "Hoover" after the vacuum cleaner brand.
The S-3 was retired from front-line fleet service aboard aircraft carriers by the US Navy in January 2009, with its missions being assumed by other platforms such as the P-3C Orion, Sikorsky SH-60 Seahawk, and Boeing F/A-18E/F Super Hornet. Several aircraft were flown by Air Test and Evaluation Squadron Thirty (VX-30) at Naval Base Ventura County / NAS Point Mugu, California for range clearance and surveillance operations on the NAVAIR Point Mugu Range until 2016, and one S-3 is operated by the National Aeronautics and Space Administration (NASA) at the NASA Glenn Research Center.
Development.
In the mid-1960s, the U.S. Navy developed the VSX (Heavier-than-air, Anti-submarine, Experimental) requirement for a replacement for the piston-engined Grumman S-2 Tracker as an anti-submarine aircraft to fly off the Navy's aircraft carriers. In August 1968, a team led by Lockheed and a Convair/Grumman team were asked to further develop their proposals to meet this requirement. Lockheed recognised that it had little recent experience in designing carrier based aircraft, so Ling-Temco-Vought (LTV) was brought into the team, being responsible for the folding wings and tail, the engine nacelles, and the landing gear, which was derived from LTV A-7 Corsair II (nose) and Vought F-8 Crusader (main). Sperry Univac Federal Systems was assigned the task of developing the aircraft's onboard computers which integrated input from sensors and sonobuoys.
On 4 August 1969, Lockheed's design was selected as the winner of the contest, and eight prototypes, designated YS-3A were ordered. The first prototype flew on 21 January 1972 and the S-3 entered service in 1974. During the production run from 1974 to 1978, a total of 186 S-3As were built. The majority of the surviving S-3As were later upgraded to the S-3B variant, with sixteen aircraft converted into ES-3A Shadow electronic intelligence (ELINT) collection aircraft.
Design.
The S-3 is a conventional monoplane with a high-mounted cantilever wing, swept at an angle of 15°. The two GE TF-34 high-bypass turbofan engines mounted in nacelles under the wings provide excellent fuel efficiency, giving the Viking the required long range and endurance, while maintaining docile engine-out characteristics.
The aircraft can seat four crew members, three officers and one enlisted aircrewman, with the pilot and the copilot/tactical coordinator (COTAC) in the front of the cockpit and the tactical coordinator (TACCO) and sensor operator (SENSO) in the back. Entry is by an entry door / ladder which folds out of the side of the fuselage. When the aircraft's anti-submarine warfare (ASW) role ended in the late 1990s, the enlisted SENSOs were removed from the crew. In the tanking crew configuration, the S-3B typically flew with only a crew of two (pilot and co-pilot/COTAC). The wing is fitted with leading edge and Fowler flaps. Spoilers are fitted to both the upper and the lower surfaces of the wings. All control surfaces are actuated by dual hydraulically boosted irreversible systems. In the event of dual hydraulic failures, an Emergency Flight Control System (EFCS) permits manual control with greatly increased stick forces and reduced control authority.
Unlike many tactical jets which required ground service equipment, the S-3 was equipped with an auxiliary power unit (APU) and capable of unassisted starts. The aircraft's original APU could provide only minimal electric power and pressurized air for both aircraft cooling and for the engines' pneumatic starters. A newer, more powerful APU could provide full electrical service to the aircraft. The APU itself was started from a hydraulic accumulator by pulling a mechanical handle in the cockpit. The APU accumulator was fed from the primary hydraulic system, but could also be pumped up manually (with much effort) from the cockpit.
All crew members sit on forward-facing, upward-firing Douglas Escapac zero-zero ejection seats. In "group eject" mode, initiating ejection from either front seat ejects the entire crew in sequence, with the back seats ejecting 0.5 seconds before the front in order to provide safe separation. The rear seats are capable of self ejection, and the ejection sequence includes a pyrotechnic charge that stows the rear keyboard trays out of the occupants' way immediately before ejection. Safe ejection requires the seats to be weighted in pairs, and when flying with a single crewman in the back the unoccupied seat is fitted with ballast blocks.
At the time it entered the fleet, the S-3 introduced an unprecedented level of systems integration. Previous ASW aircraft like the Lockheed P-3 Orion and S-3's predecessor, the Grumman S-2 Tracker, featured separate instrumentation and controls for each sensor system. Sensor operators often monitored paper traces, using mechanical calipers to make precise measurements and annotating data by writing on the scrolling paper. Beginning with the S-3, all sensor systems were integrated through a single General Purpose Digital Computer (GPDC). Each crew station had its own display, the co-pilot/COTAC, TACCO and SENSO displays were Multi-Purpose Displays (MPD) capable of displaying data from any of a number of systems. This new level of integration allowed the crew to consult with each other by examining the same data at multiple stations simultaneously, to manage workload by assigning responsibility for a given sensor from one station to another, and to easily combine clues from each sensor to classify faint targets. Because of this, the four-man S-3 was considered roughly equivalent in capability to the much larger P-3 with a crew of 12.
The aircraft has two underwing hardpoints that can be used to carry fuel tanks, general purpose and cluster bombs, missiles, rockets, and storage pods. It also has four internal bomb bay stations that can be used to carry general purpose bombs, aerial torpedoes, and special stores (B57 and B61 nuclear weapons). Fifty-nine sonobuoy chutes are fitted, as well as a dedicated Search and Rescue (SAR) chute. The S-3 is fitted with the ALE-39 countermeasure system and can carry up to 90 rounds of chaff, flares, and expendable jammers (or a combination of all) in three dispensers. A retractable magnetic anomaly detector (MAD) Boom is fitted in the tail.
In the late 1990s, the S-3B's role was changed from anti-submarine warfare (ASW) to anti-surface warfare (ASuW). At that time, the MAD Boom was removed, along with several hundred pounds of submarine detection electronics. With no remaining sonobuoy processing capability, most of the sonobuoy chutes were faired over with a blanking plate.
Operational history.
On 20 February 1974, the S-3A officially became operational with the Air Antisubmarine Squadron FORTY-ONE (VS-41), the "Shamrocks," at NAS North Island, California, which served as the initial S-3 Fleet Replacement Squadron (FRS) for both the Atlantic and Pacific Fleets until a separate Atlantic Fleet FRS, VS-27, was established in the 1980s. The first operational cruise of the S-3A took place in 1975 with the VS-21 "Fighting Redtails" aboard .
Starting in 1987, some S-3As were upgraded to S-3B standard with the addition of a number of new sensors, avionics, and weapons systems, including the capability to launch the AGM-84 Harpoon anti-ship missile. The S-3B could also be fitted with "buddy stores", external fuel tanks that allowed the Viking to refuel other aircraft. In July 1988, VS-30 became the first fleet squadron to receive the enhanced capability Harpoon/ISAR equipped S-3B, based at NAS Cecil Field in Jacksonville, Florida. 16 S-3As were converted to ES-3A Shadows for carrier-based electronic intelligence (ELINT) duties. Six aircraft, designated US-3A, were converted for a specialized utility and limited cargo COD requirement. Plans were also made to develop the KS-3A carrier-based tanker aircraft, but this program was ultimately cancelled after the conversion of just one early development S-3A.
With the collapse of the Soviet Union and the breakup of the Warsaw Pact, the Soviet-Russian submarine threat was perceived as much reduced, and the Vikings had the majority of their antisubmarine warfare equipment removed. The aircraft's mission subsequently changed to sea surface search, sea and ground attack, over-the-horizon targeting, and aircraft refueling. As a result, the S-3B after 1997 was typically crewed by one pilot and one copilot ; the additional seats in the S-3B could still support additional crew members for certain missions. To reflect these new missions the Viking squadrons were redesignated from "Air Antisubmarine Warfare Squadrons" to "Sea Control Squadrons."
Prior to the aircraft's retirement from front-line fleet use aboard US aircraft carriers, a number of upgrade programs were implemented. These include the Carrier Airborne Inertial Navigation System II (CAINS II) upgrade, which replaced older inertial navigation hardware with ring laser gyroscopes with a Honeywell EGI (Enhanced GPS Inertial Navigation System) and added digital electronic flight instruments (EFI). The Maverick Plus System (MPS) added the capability to employ the AGM-65E laser-guided or AGM-65F infrared-guided air-to-surface missile, and the AGM-84H/K Stand-off Land Attack Missile Expanded Response (SLAM/ER). The SLAM/ER is a GPS/inertial/infrared guided cruise missile derived from the AGM-84 Harpoon that can be controlled by the aircrew in the terminal phase of flight if an AWW-13 data link pod is carried by the aircraft.
The S-3B saw extensive service during the 1991 Gulf War, performing attack, tanker, and ELINT duties, and launching ADM-141 TALD decoys. This was the first time an S-3B was employed overland during an offensive air strike. The first mission occurred when an aircraft from VS-24, from the , attacked an Iraqi Silkworm missile site. The aircraft also participated in the Yugoslav wars in the 1990s and in Operation Enduring Freedom in 2001.
The first ES-3A was delivered in 1991, entering service after two years of testing. The Navy established two squadrons of eight ES-3A aircraft each in both the Atlantic and Pacific Fleets to provide detachments of typically two aircraft, ten officers, and 55 enlisted aircrew, maintenance and support personnel (which comprised/supported four complete aircrews) to deploying carrier air wings. The Pacific Fleet squadron, Fleet Air Reconnaissance Squadron FIVE (VQ-5), the "Sea Shadows," was originally based at the former NAS Agana, Guam but later relocated to NAS North Island in San Diego, California with the Pacific Fleet S-3 Viking squadrons when NAS Agana closed in 1995 as a result of a 1993 Base Realignment and Closure (BRAC) decision. The Atlantic Fleet squadron, the VQ-6 "Black Ravens," were originally based with all Atlantic Fleet S-3 Vikings at the former NAS Cecil Field in Jacksonville, Florida, but later moved to NAS Jacksonville, approximately to the east, when NAS Cecil Field was closed in 1999 as a result of the same 1993 BRAC decision that closed NAS Agana.
The ES-3A operated primarily with carrier battle groups, providing organic ‘Indications and Warning’ support to the group and joint theater commanders. In addition to their warning and reconnaissance roles, and their extraordinarily stable handling characteristics and range, Shadows were a preferred recovery tanker (aircraft that provide refueling for returning aircraft). They averaged over 100 flight hours per month while deployed. Excessive utilization caused earlier than expected equipment replacement when Naval aviation funds were limited, making them an easy target for budget-driven decision makers. In 1999, both ES-3A squadrons and all 16 aircraft were decommissioned and the ES-3A inventory placed in Aerospace Maintenance and Regeneration Group (AMARG) storage at Davis-Monthan AFB, Arizona.
Iraq War.
In March 2003, during Operation Iraqi Freedom, an S-3B Viking from Sea Control Squadron 38 (The "Red Griffins") piloted by Richard McGrath Jr. launched from . The crew successfully executed a time sensitive strike and fired a laser-guided Maverick missile to neutralize a significant Iraqi naval and leadership target in the port city of Basra, Iraq. This was one of the few times in its operational history that the S-3B Viking had been employed overland on an offensive combat air strike and the first time it launched a laser-guided Maverick missile in combat.
On 1 May 2003, US President George W. Bush flew in the co-pilot seat of a VS-35 Viking from NAS North Island, California to off the California coast. There, he delivered his "Mission Accomplished" speech announcing the end of major combat in the 2003 invasion of Iraq. During the flight, the aircraft used the customary presidential callsign of "Navy One". The aircraft that President Bush flew in was retired shortly thereafter and on 15 July 2003 was accepted as an exhibit at the National Museum of Naval Aviation at NAS Pensacola, Florida.
Between July and December 2008 the VS-22 Checkmates, the last sea control squadron, operated a detachment of four S-3Bs from the Al Asad Airbase in Al Anbar Province, west of Baghdad. The planes were fitted with LANTIRN pods and they performed non-traditional intelligence, surveillance, and reconnaissance (NTISR). After more than 350 missions, the Checkmates returned to NAS Jacksonville, Florida on 15 December 2008, prior to disestablishing on 29 January 2009.
Retirement.
Though a proposed airframe known as the Common Support Aircraft was once advanced as a successor to the S-3, E-2 and C-2, this plan failed to materialize. As the surviving S-3 airframes were forced into sundown retirement, a Lockheed Martin full scale fatigue test was performed and extended the service life of the aircraft by approximately 11,000 flight-hours. This supported Navy plans to retire all Vikings from front-line fleet service by 2009 so new strike fighter and multi-mission aircraft could be introduced to recapitalize the aging fleet inventory, with former Viking missions assumed by other fixed-wing and rotary-wing aircraft.
The final carrier based S-3B Squadron, VS-22 was decommissioned at NAS Jacksonville on 29 January 2009. Sea Control Wing Atlantic was decommissioned the following day on 30 January 2009, concurrent with the U.S. Navy retiring the last S-3B Viking from front-line Fleet service.
In June 2010 the first of three aircraft to patrol the Pacific Missile Test Center's range areas off of California was reactivated and delivered. The jet aircraft's higher speed, 10-hour endurance, modern radar, and a LANTIRN targeting pod allowed it to quickly confirm the test range being clear of wayward ships and aircraft before tests commence. These S-3Bs are flown by Air Test and Evaluation Squadron Thirty(VX-30) based out of NAS Point Mugu, California. Also, the NASA Glenn Research Center acquired four S-3Bs in 2005. Since 2009, one of these aircraft (USN BuNo 160607) has also carried the civil registration N601NA and is used for various tests.
By late 2015, the U.S. Navy had three Vikings remaining operational in support roles. One was moved to The Boneyard in November 2015, and the final two were retired, one stored and the other transferred to NASA, on 11 January 2016, officially retiring the S-3 from Navy service.
Naval analysts have suggested returning the stored S-3s to service with the U.S. Navy to fill gaps it left in the carrier air wing when it was retired. This is in response to the realization that the Chinese navy is producing new weapons that can threaten carriers beyond the range their aircraft can strike them. Against the DF-21D anti-ship ballistic missile, carrier-based F/A-18 Super Hornets and F-35C Lightning IIs have about half the unrefueled strike range, so bringing the S-3 back to aerial tanking duties would extend their range against it, as well as free up more Super Hornets that were forced to fill the role. Against submarines armed with anti-ship cruise missiles like the Klub and YJ-18, the S-3 would restore area coverage for ASW duties. Bringing the S-3 out of retirement could at least be a stop-gap measure to increase the survivability and capabilities of aircraft carriers until new aircraft can be developed for such purposes.
Potential interest.
In October 2013, the South Korean navy expressed an interest in acquiring up to 18 ex-USN S-3s to augment their current fleet of 16 Lockheed P-3 Orion aircraft. In August 2015, a military program review group approved a proposal to incorporate 12 mothballed S-3s to perform ASW duties; the Viking plan will be sent to the Defense Acquisition Program Administration for further assessment before final approval by the national defense system committee. Although the planes are old, being in storage kept them serviceable and using them is a cheaper way to fulfill short-range airborne ASW capabilities left after the retirement of the S-2 Tracker than buying newer aircraft. Refurbished S-3s could be returned to use by 2019.
In April 2014, Lockheed Martin announced that they would offer refurbished and remanufactured S-3s, dubbed the C-3, as a replacement for the Northrop Grumman C-2A Greyhound for Carrier onboard delivery. The requirement for 35 aircraft would be met from the 91 S-3s currently in storage. In February 2015, the Navy announced that the Bell Boeing V-22 Osprey had been selected to replace the C-2 for the COD mission.
References.
Notes
Bibliography

</doc>
<doc id="29476" url="https://en.wikipedia.org/wiki?curid=29476" title="Kaman SH-2 Seasprite">
Kaman SH-2 Seasprite

The Kaman SH-2 Seasprite is a ship-based helicopter originally developed in the late 1950s as a fast utility helicopter for the United States Navy. In the 1970s, anti-submarine, anti-surface threat capabilities were added to the design, including over-the-horizon targeting, resulting in modifying most existing UH-2 models to the "SH-2 Seasprite".
This aircraft extends and increases shipboard sensor and weapon capabilities against several types of enemy threats, including submarines of all types, surface ships and patrol craft that may be armed with anti-ship missiles. It served with the U.S. Navy from the 1960s until the last SH-2G helicopters were retired in 2001.
Design and development.
Origins.
In 1956, the U.S. Navy launched a competition to meet its requirement for a compact, all-weather multipurpose naval helicopter. Kaman's K-20 model was selected as the winner. Kaman was awarded a contract for four prototype and 12 production "HU2K-1" helicopters in late 1957. Kaman's design was for a conventional helicopter powered by a single General Electric T58-8F turboshaft engine, driving a 44-foot four-bladed main rotor and a four-bladed tail rotor.
In 1960, the Royal Canadian Navy announced that the HU2K was the frontrunner for a large anti-submarine warfare contract; the Canadian Treasury Board had approved an initial procurement of 12 units for $14.5 million. Abruptly, Kaman raised the estimated price to $23 million, and there was concern that the manufacturer's weight and performance projections were overly optimistic. The Naval Board decided to wait until after the US Navy had conducted sea trials before approving the purchase. These trials revealed the HU2K to be overweight and underpowered, and thus incapable of meeting Canadian requirements. Hence, in late 1961, the Sikorsky Sea King was selected.
With no follow-on orders, Kaman ended production in the late 1960s after delivering 184 SH-2s to the U.S. Navy; production was later restarted in 1971 to manufacture an improved variant of the helicopter, the "SH-2F". A significant factor in the reopening of the production line was that the Navy's Sikorsky SH-60 Sea Hawk, which was newer and more capable in anti-submarine operations, was too large to be operated from the small flight decks of older frigates.
Further development.
Upon enactment of the 1962 United States Tri-Service aircraft designation system, the HU2K-1 was redesignated "UH-2A" and the "HU2K-1U" was redesignated "UH-2B". In service, the UH-2 Seasprite would see several modifications and improvements, such as the addition of fixtures for mounting external stores. Beginning in 1968, the Navy's remaining UH-2s were extensively remanufactured, their single engines being replaced by a twin-engine arrangement.
The UH-2 was selected to be the airframe for the interim Light Airborne Multi-Purpose System (LAMPS) helicopter in October 1970. LAMPS evolved in the late 1960s from an urgent requirement to develop a manned helicopter that would support a non-aviation ship and serve as its tactical Anti-Submarine Warfare arm. Known as LAMPS Mark I, the advanced sensors, processors, and display capabilities aboard the helicopter enabled ships to extend their situational awareness beyond the line-of-sight limitations that hamper shipboard radars and the short distances for acoustic detection and prosecution of underwater threats associated with hull-mounted sonars. H-2s reconfigured for the LAMPS mission were redesignated "SH-2D". On 16 March 1971, the first SH-2D LAMPS prototype first flew.
The full LAMPS I system was equipped on the "SH-2F". The SH-2F was delivered to the Navy beginning in 1973. This variant had upgraded engines, longer life rotor, and higher take-off weight. In 1981, the Navy ordered 60 production SH-2Fs. Beginning in 1987, 16 SH-2Fs were upgraded with chin mounted Forward Looking Infrared Sensors (FLIR), Chaff (AIRBOC)/Flares, dual rear mounted IR scramblers, and Missile/Mine detecting equipment.
Eventually all but two H-2s then in Navy inventory were remanufactured into SH-2Fs. The final production procurement of the SH-2F was in Fiscal Year 1986. The last six orders for production SH-2Fs were switched to the SH-2G Super Seasprite variant.
Operational history.
United States.
The UH-2 began entering operational service in 1962. The Navy soon found the helicopter's capabilities to be restricted by its single engine, and ordered Kaman to retrofit all of its Seasprites with a twin-engine arrangement instead; with two engines the Seasprite was capable of reaching an airspeed of 130 knots and operating at a range of up to 411 nautical miles. The Navy would operate a total fleet of nearly 200 Seasprites for various duties, such as anti-submarine warfare (ASW), search and rescue (SAR) and transportation. Typically, several UH-2s would be deployed upon an aircraft carrier to perform plane guard and SAR missions.
The UH-2 was introduced in time to see action in the Tonkin Gulf incident in August 1964; the Seasprite's principal contribution to what would become the Vietnam War was the retrieval of downed aircrews, both from the sea and from inside enemy territory, and was increasingly relied upon in this mission as the war intensified, such as during Operation Rolling Thunder in 1965. In October 1966 alone, out of 269 downed pilots, helicopter-based SAR teams were able to recover 103 men.
In the 1970s, the conversion of UH-2s to the SH-2 anti-submarine configuration provided the US Navy with its first ASW helicopter capable of operating from vessels other than its aircraft carriers. The small size of the SH-2 allowed it to be operated from flight decks that were too small for most helicopters, this being a factor in the Navy's decision to acquire the improved SH-2F in the early 1980s.
SH-2Fs were utilized to enforce and support Operation Earnest Will in July 1987, Operation Praying Mantis in April 1988, and Operation Desert Storm during January 1991 in the Persian Gulf region. The countermeasures and additional equipment on the SH-2F allowed it to conduct combat support and surface warfare missions in these hostile environments, which had an often-minimal submarine threat. The SH-2F was retired from active service April 1994, corresponding with the retirement of the last the Vietnam-era Knox Class Frigates that were unable to accommodate the new and larger SH-60 Sea Hawk that were replacing the aging Seasprites.
In 1991, the US Navy began to receive deliveries of the new SH-2G Super Seasprite; a total of 18 converted SH-2Fs and 6 new-built SH-2Gs were produced. These were assigned to Naval Reserve squadrons, the SH-2G entered service with HSL-84 in 1993. The SH-2 served in some 600 deployments and flew 1.5 million flight hours before the last of the type were finally retired in mid-2001.
New Zealand.
The Royal New Zealand Navy (RNZN) replaced its Westland Wasps with four interim SH-2F Seasprites (ex-US Navy), operated and maintained by a mix of Navy and Air Force personnel known as No. 3 Squadron RNZAF Naval Support Flight, to operate with ANZAC class frigates until the fleet of five new SH-2G Super Seasprites were delivered. The Navy air element was transferred to No. 6 Squadron RNZAF at RNZAF Base Auckland in Whenuapai in October 2005. RNZN Seasprites have seen service in East Timor. Six additional SH-2Fs rejected by the Royal Australian Navy were purchased and are now stationed at the RNZAF Ground Training Wing (GTW) at Woodbourne near Blenheim as training helicopters. An SH-2F (ex-RNZN, NZ3442) is preserved in the Royal New Zealand Air Force Museum, donated to the museum by Kaman Aircraft Corporation after an accident while in service with the RNZN.
Exports.
In the late 1990s the United States offered surplus U.S. Navy SH-2Fs as foreign aid to a number of countries, Greece which had been offered six and Turkey which had been offered 14 rejected the offer. Egypt acquired four SH-2F aircraft under the aid program mainly for spares to support a fleet of ten SH-2Gs. Poland acquired the later SH-2G.

</doc>
<doc id="29480" url="https://en.wikipedia.org/wiki?curid=29480" title="Stop consonant">
Stop consonant

In phonetics, a stop, also known as a plosive or oral occlusive, is a consonant in which the vocal tract is blocked so that all airflow ceases.
The occlusion may be made with the tongue blade (, ) or body (, ), lips (, ), or glottis (). Stops contrast with nasals, where the vocal tract is blocked but airflow continues through the nose, as in and , and with fricatives, where partial occlusion impedes but does not block airflow in the vocal tract.
Terminology.
The terms "stop, occlusive," and "plosive" are often and inaccurately used interchangeably. Linguists who distinguish them may not agree on the distinction being made. The terms refer to different features of the consonant. "Stop" refers to the airflow that is stopped. "Occlusive" refers to the articulation, which occludes (blocks) the vocal tract. "Plosive" refers to the release burst (plosion) of the consonant. Therefore, a plosive is a stop that is released, typically into a more open speech sound such as a vowel. It is inaccurate to call an unreleased stop a plosive. Rather, such stops are "applosives".
Either "occlusive" or "stop" may be used as a general term covering the other together with nasals. That is, 'occlusive' may be defined as oral occlusives (stops/plosives) plus nasal occlusives (nasals such as , ), or 'stop' may be defined as oral stops (plosives) plus nasal stops (nasals). Ladefoged and Maddieson (1996) prefer to restrict 'stop' to oral occlusives. They say,
In addition, they use "plosive" for a pulmonic stop; "stops" in their usage include ejective and implosive consonants. 
If a term such as 'plosive' is used for oral obstruents, and nasals are not called nasal stops, then a "stop" may mean the glottal stop; 'plosive' may even mean non-glottal stop. In other cases, however, it may be the word 'plosive' that is restricted to the glottal stop. Note that, generally speaking, stops do not have plosion (a release burst). In English, for example, there are stops with no audible release, such as the in "apt". However, pulmonic stops do have plosion in other environments.
In Ancient Greek, the term for stop was ("áphōnon"), which means "unpronounceable", "voiceless", or "silent", because stops could not be pronounced without a vowel. This term was calqued into Latin as , and from there borrowed into English as "mute". "Mute" was sometimes used instead for voiceless consonants, whether stops or fricatives, a usage that was later replaced with "surd", from Latin "" "deaf" or "silent", a term still occasionally seen in the literature. For more information on the Ancient Greek terms, see .
Common stops.
All languages in the world have stops, and most have at least the voiceless stops , , and . However, there are exceptions: Colloquial Samoan lacks the coronal , and several North American languages, such as the northern Iroquoian and southern Iroquoian languages (i.e., Cherokee), lack the labial . In fact, the labial is the least stable of the voiceless stops in the languages of the world, as the unconditioned sound change → (→ → Ø) is quite common in unrelated languages, having occurred in the history of Classical Japanese, Classical Arabic, and Proto-Celtic, for instance. Formal Samoan has only one word with velar ; colloquial Samoan conflates and to . Ni‘ihau Hawaiian has for to a greater extent than Standard Hawaiian, but neither distinguish a from a . It may be more accurate to say that Hawaiian and colloquial Samoan do not distinguish velar and coronal stops than to say they lack one or the other.
See Common occlusives for the distribution of both stops and nasals.
Articulation.
In the articulation of the stop, three phases can be distinguished:
In many languages, such as Malay and Vietnamese, word-final stops lack a release burst, even when followed by a vowel, or have a nasal release. See no audible release.
Nasal occlusives are somewhat similar. In the catch and hold, airflow continues through the nose; in the release, there is no burst, and final nasals are typically unreleased across most languages. 
In affricates, the catch and hold are those of a stop, but the release is that of a fricative. That is, affricates are stop–fricative contours.
Classification.
Voice.
Voiced stops are pronounced with vibration of the vocal cords, voiceless stops without. Stops are commonly voiceless, and many languages, such as Mandarin Chinese and Hawaiian, have only voiceless stops. Others, such as most Australian languages, are indeterminate: stops may vary between voiced and voiceless without distinction.
Aspiration.
In aspirated stops, the vocal cords (vocal folds) are abducted at the time of release. In a prevocalic aspirated stop (a stop followed by a vowel or sonorant), the time when the vocal cords begin to vibrate will be delayed until the vocal folds come together enough for voicing to begin, and will usually start with breathy voicing. The duration between the release of the stop and the voice onset is called the "voice onset time" (VOT) or the "aspiration interval". Highly aspirated stops have a long period of aspiration, so that there is a long period of voiceless airflow (a phonetic ) before the onset of the vowel. In tenuis stops, the vocal cords come together for voicing immediately following the release, and there is little or no aspiration (a voice onset time close to zero). In English, there may be a brief segment of breathy voice that identifies the stop as voiceless and not voiced. In voiced stops, the vocal folds are set for voice before the release, and often vibrate during the entire hold, and in English, the voicing after release is not breathy. A stop is called "fully voiced" if it is voiced during the entire occlusion. In English, however, initial voiced stops like or may have no voicing during the period of occlusion, or the voicing may start shortly before the release and continue after release, though word-final stops tend to be fully voiced: In most dialects of English, the final "g" in the "bag" is likely to be fully voiced, whereas the initial "b" will only be voiced during part of its occlusion. Initial voiceless stops, like the "p" in "pie", are aspirated, with a palpable puff of air upon release, whereas a stop after an "s", as in "spy", is tenuis (unaspirated). When spoken near a candle flame, the flame will flicker more after the words "par, tar," and "car" are articulated, compared with "spar, star," and "scar". In the common pronunciation of "papa", the initial "p" is aspirated whereas the medial "p" is not.
Length.
In a geminate or long consonant, the occlusion lasts longer than in simple consonants. In languages where stops are only distinguished by length (e.g., Arabic, Ilwana, Icelandic), the long stops may be held up to three times as long as the short stops. Italian is well known for its geminate stops, as the double "t" in the name "Vittoria" takes just as long to say as the "ct" does in English "Victoria". Japanese also prominently features geminate consonants, such as in the minimal pair 来た "kita" 'came' and 切った "kitta" 'cut'.
Note that there are many languages where the features voice, aspiration, and length reinforce each other, and in such cases it may be hard to determine which of these features predominates. In such cases, the terms fortis is sometimes used for aspiration or gemination, whereas lenis is used for single, tenuous, or voiced stops. Be aware, however, that the terms "fortis" and "lenis" are poorly defined, and their meanings vary from source to source.
Nasalization.
Simple nasals are differentiated from stops only by a lowered velum that allows the air to escape through the nose during the occlusion. Nasals are acoustically sonorants, as they have a non-turbulent airflow and are nearly always voiced, but they are articulatorily obstruents, as there is complete blockage of the oral cavity. The term occlusive may be used as a cover term for both nasals and stops.
A prenasalized stop starts out with a lowered velum that raises during the occlusion. The closest examples in English are consonant clusters such as the in "candy", but many languages have prenasalized stops that function phonologically as single consonants. Swahili is well known for having words beginning with prenasalized stops, as in "ndege" 'bird', and in many languages of the South Pacific, such as Fijian, these are even spelled with single letters: "b" [mb, "d" .
A postnasalized stop begins with a raised velum that lowers during the occlusion. This causes an audible nasal "release", as in English "sudden". This could also be compared to the /dn/ cluster found in Russian and other Slavic languages, which can be seen in the name of the Dnieper River.
Note that the terms "prenasalization" and "postnasalization" are normally used only in languages where these sounds are phonemic: that is, not analyzed into sequences of stop plus nasal.
Airstream mechanism.
Stops may be made with more than one airstream mechanism. The normal mechanism is pulmonic egressive, that is, with air flowing outward from the lungs. All languages have pulmonic stops. Some languages have stops made with other mechanisms as well: ejective stops (glottalic egressive), implosive stops (glottalic ingressive), or click consonants (lingual ingressive).
Tenseness.
A fortis stop (in the narrow sense) is produced with more muscular tension than a lenis stop (in the narrow sense). However, this is difficult to measure, and there is usually debate over the actual mechanism of alleged fortis or lenis consonants. 
There are a series of stops in the Korean language, sometimes written with the IPA symbol for ejectives, which are produced using "stiff voice", meaning there is increased contraction of the glottis than for normal production of voiceless stops. The indirect evidence for stiff voice is in the following vowels, which have a higher fundamental frequency than those following other stops. The higher frequency is explained as a result of the glottis being tense. Other such phonation types include breathy voice, or murmur; slack voice; and creaky voice.
Transcription.
The following stops have been given dedicated symbols in the IPA. 
Variations.
Many subclassifications of stops are transcribed by adding a diacritic or modifier letter to the IPA symbols above.

</doc>
<doc id="29482" url="https://en.wikipedia.org/wiki?curid=29482" title="Stayman convention">
Stayman convention

Stayman is a bidding convention in the card game contract bridge. It is used by a partnership to find a 4-4 or 5-3 trump fit in a suit after making a one (1NT) opening bid and it has been adapted for use after a 2NT opening, a 1NT overcall, and many other natural notrump bids.
The convention is named for Sam Stayman, who wrote the first published description in 1945, but its inventors were two other players: the British expert Jack Marx in 1939, who published it only in 1946, and Stayman's regular partner George Rapée in 1944.
Rationale.
A bid and made in a major suit (i.e. 4 or 4 ) scores better than a game contract bid and made in a minor suit (i.e. 5 or 5 ) or in notrump (i.e. 3NT). Also, the success rate for a game contract in a major suit when a partnership has a combined holding of 26 points and eight cards in the major is about 80%, whereas a game contract in 3NT with 26 (HCP) has a success rate of only 60%, or 50% with 25 HCP; the success rate for a minor suit game contract when holding 26 points is about 30%.
Accordingly, partnership priority is to find an eight card or better major suit fit when jointly holding sufficient values for a game contract. 5-3 and 6-2 fits are easy to find in basic methods as responder can bid 3 or 3 over 1NT, and opener will not normally have a 5 card major to bid 1NT. However, finding 4-4 fits presents a problem. The 2 and 2 bids cannot be used for this as they are weak takeouts, a sign-off bid.
Standard Stayman.
After an opening bid or an overcall of 1NT (2NT), or bids an artificial 2 (3) to ask opener or overcaller if he holds a four or five card major suit; some partnership agreements may require the major to be headed by an honor of at least a specified rank, such as the queen. The artificial club bid typically promises four cards in at least one of the major suits (promissory Stayman) and, "in standard form", enough strength to continue bidding after partner's response (8 HCP for an invitational bid opposite a standard strong 1NT opening or overcall showing 15-17 HCP, 11 HCP opposite a weak notrump of 12-14 HCP, or 5 HCP to go to game opposite a standard 2NT showing 20-21 points). It also promises distribution that is not 4333. By invoking the Stayman convention, the responder takes control of the bidding since strength and distribution of the opener's hand is already known within a limited range. The opener responds with the following rebids.
A notrump opener should have neither a suit longer than five cards nor more than one 5-card suit since an opening notrump bid shows a balanced hand. A notrump bidder who has at least four cards in each major suit normally responds in hearts, as this can still allow a spade fit to be found. Variant methods are to bid the longer or stronger major, with a preference given to spades, or to use 2NT to show both majors.
In the standard form of Stayman over 1NT, the responder has a number of options depending on his partner's answer:
Over these bids, the notrump bidder (1) with a maximum hand (17 HCP), goes to game over an invitational bid and (2) with four (or more) cards in each major suit, corrects to the previously unbid major suit.
In the standard form of Stayman over 2NT, the responder has only two normal rebids.
In either case, a responder who rebids notrump over a response in a major suit promises four cards of the other major suit. Thus, a notrump opener who holds at least four cards in each major suit should "correct" by bidding the other major suit at the lowest level.
Of course, once a fit is found, responder who has sufficient strength also may bid 4 (Gerber) or 4NT (Blackwood), or cue bid aces, depending upon partnership agreement, to explore slam in any of the above sequences. Some partnerships also admit responder's rebids of a major suit that the notrump bidder did not name.
A bid of 4 over an opening bid of 3NT may be either Stayman or Gerber, depending upon the partnership agreement.
If an adverse suit bid is inserted immediately after a 1NT opening, Stayman may be employed via a double (by partnership agreement) or a cue bid, depending on the strength of his hand. The cue bid, which is conventional, is completely artificial and means nothing other than invoking Stayman. For example, if South opens 1NT, and West overcalls 2, North, if he has adequate values, may call 3, invoking Stayman. South would then show his major or bid game in notrump. Alternatively, North, if his hand lacks the values for game in notrump, may double, which by partnership agreement employs Stayman. This keeps the Stayman bidding at second level.
Partnerships who have not yet learned Stayman but choose to adopt Stayman (without having yet learned or having chosen not to use Jacoby Transfers) will need to adjust their use of normal level 2 responses after a 1NT opening, because the availability of this convention changes the nature of what had been normal 1 NT responses. When the notrump bidder's partner does not invoke Stayman but instead calls 2 or 2, it is a sign of relative weakness (since if responder held 8 HCP or more, he would have invoked Stayman). These bids are commonly referred to as "drop dead bids", as the opening notrump bidder is requested to withdraw from the auction. If opener has maximum values, a fit, and strong support, he may raise to the 3-level, but under no circumstances may he take any other action. This provides the partnership with an advantage that the non-Stayman partnership doesn't enjoy. For example, a responder may have no honors at all; that is, a total of zero HCP. His partner is likely to be set if he passes. A non-Stayman responder would have to pass, because to bid would provoke a rebid. But a Stayman responder can respond to his partner's 1NT opening at level 2 if he has a 6-card non-club suit. The responder with 3 HCP and a singleton can make a similar call with a 5-card non-club suit. This gives the partnership a better than even chance of success in making the contract, whereas without a response (and without Stayman), the contract would likely be set.
Similarly, a response of 2 Diamonds indicates less than 8 HCP and should usually be passed. In rare cases, when the opener has maximum values and a fit in Diamonds with at least two of the top three honors, he may raise Diamonds, and responder may see a chance for game in notrump.
There are many variations on this basic theme, and partnership agreement may alter the details of its use. It is one of the most widely used conventions in bridge.
Non promissory Stayman and 2 checkback by responder.
Some partnerships play that 2 Stayman does not absolutely promise a four-card major (non promissory Stayman). For example, if responder has a short suit and wishes to know if opener has four-card cover in it, so as to play in notrumps. If opener shows hearts initially, 2 can be used to find a fit in spades when the 2 does not promise a four-card major.
1NT - 2, 2 -
Alternatively 2 can be used for all hands with four spades and not four hearts, either invitational or game values, while 3NT denies four spades.
Using Jacoby transfers with Stayman.
Today, most players use Stayman in conjunction with Jacoby transfers. With Stayman in effect, the responder practically denies having a five-card major, as otherwise he would transfer to the major immediately. The only exception is when responder has 5-4 in the majors; in that case, he could use Stayman, and in the case of a 2 response, bid the five-card major at the two level (weakness take-out / Garbage Stayman) or at the three level (forcing to game). However, the latter hand can also be bid by first using a transfer and then showing the second suit naturally. The Smolen convention provides an alternative method to show a five-card major and game-going values. A minor drawback of Jacoby transfers is that a 2 contract is not possible.
Smolen convention.
The Smolen convention is an adjunct to Stayman for situations in which the notrump opener has denied holding a four-card major and responder has a five-card major and a four-card major with game-going values.
If the notrump opener responds to the Stayman 2 asking bid with 2, denying a four-card major, responder initiates the Smolen Transfer with a jump shift to three of his four-card major. The jump shift shows which is the four-card major and promises five in the other major. The notrump opener then bids four of the other major with three cards in the suit or 3NT with fewer than three.
Smolen may also be used when responder has a six-card major and a four-card major with game-going values; after the 2 negative response by opener, responder double jump shifts to four in the suit just below his six-card major and the notrump opener transfers to four of his partner's six-card major.
This convention allows a partnership to find either a 5-3 fit, 6-3 and 6-2 fit while ensuring that the notrump opener, who has the stronger hand, will be declarer.
Garbage Stayman and Crawling Stayman.
"Garbage" Stayman (or "Weak Stayman") and "Crawling" Stayman are adaptations of Stayman frequently used for damage control when holding a weak hand opposite a 1NT opening bid. Suppose you hold the following hand.
Your partner opens 1NT (15-17), and your right hand opponent passes. Now, what?
In this scenario, opener has about 16 HCP and the opponents have about 24 HCP. Thus, 1NT is virtually certain to go down by at least three or four tricks. Indeed, in Notrump, this dummy will be completely worthless.
But consider what happens if you bid 2 Stayman rather than passing on the first round, and then "pass opener's response". If opener rebids a major suit you have found a 4-4 fit and ability to trump club losers (or, alternately, to sluff the other major on club winners and then to trump losers in the other major). Likewise, a response of 2 guarantees no worse than a 5-2 fit in diamonds and, with a fifth trump, a potential additional ruff. The ability to reach dummy with a couple ruffs also may allow the declarer to take a couple finesses or execute a squeeze that otherwise would not be possible, and which might yield another trick or two. The result is a contract that will go down fewer tricks or that might even make, especially with a somewhat better hand than the example, rather than a contract that is virtually certain to go down at least three or four tricks. The practice of bidding Stayman with a relatively weak hand of this (or similar) shape and then passing the Notrump bidder's reply is often called "Garbage Stayman" because it is bidding Stayman with a "garbage" hand.
"Crawling Stayman" is an optional extension of "Garbage Stayman" for situations in which the responder's diamond suit is short. In "Crawling Stayman", the responder rebids 2 over the Notrump bidder's 2 reply. This conventional bid shows a weak hand with at least four cards in each major suit, asking the Notrump bidder to choose between the major suits at the cheapest level by either passing the 2 bid or correcting to 2. The name "Crawling Stayman" comes from the fact that the bidding "crawls" at the slowest possible pace: (pass) – 1NT – (pass) – 2; (pass) – 2 – (pass) – 2; (pass) – 2; (pass) – pass – (pass).
Alternatively, responder's 2 and 2 bids after the 2 rebid can be weak sign-offs. This allows responder to effectively bid hands which are 5-4 in the majors, by looking first for a 4-4 fit and, if none is found, signing off in his 5 card suit.
"Garbage Stayman" and "Crawling Stayman" bids over a 2NT bid work the same way, but occur at the "three" level.
Forcing and Non-Forcing Stayman.
If Jacoby transfers are not played, there are two approaches to resolve the situation when responder has a 5-card major but only invitational values. In one, more common, referred to as "non-forcing Stayman", in the sequence:
responder's simple rebid of a major suit is only invitational, showing 8-9 points and a 5-card spade suit. In the "forcing Stayman" variant, the bid is one-round forcing.
In the original Precision Club system, forcing and non-forcing Stayman are differentiated in the start: 2 by responder shows only invitational values (and the continuation is the same as in basic Stayman), while 2 is forcing to game (responder bids 2NT without majors).
Non Promissory Game Forcing Stayman.
This allows responder to find exact shape of 1NT opener. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2, 2NT, 3 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. Disadvantage is 2 can't be used as a damage control bid.
1NT – 2♣
Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 range finder/transfer to minors (opener's rebids: 2NT 12-13 HCP, 3 14 HCP. Responder passes or corrects to 3 or 3 sign off if weak. After opener's 3 rebid responder bids 3 to show 4 hearts or 3 to show 4 spades both game forcing. Responder's rebid of 3NT denies 4 card major); 2NT invitational hand with both 4 card majors (opener's rebids: no bid no 4 card major 12-13 HCP, 3 4 hearts 12-13 HCP, 3 4 spades 12-13 HCP, 3 4 hearts 14 HCP, 3 4 spades 14 HCP, 3NT 14 HCP no 4 card major)
Non Promissory Relay Stayman.
This allows responder to find exact shape of 1NT opener. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. May be also used as a damage control bid.
1NT – 2♣
1NT – 3♣ weak sign off.
Opener's rebids of 2, 2, 2 may all be passed if responder is weak.
Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 5 spades 4 hearts 10-11 HCP; 2NT invitational hand with 5,5 minors 10-11 HCP.
Five Card Major Stayman.
This allows responder to check for 5-3 major fits where it is possible that opener's 1NT or 2NT might include a five card major. As described by Australian Ron Klinger, it can be played with a weak or strong 1NT.
1NT - 2
1NT - 2, 2 OR 2NT
After a transfer, accept it with any 4333, bid 3NT with only two trumps, otherwise bid 4M.
1NT - 2, 2 OR 2NT - 3 = Stayman
1NT - 2, (2 OR 2NT) - 3, 3
An alternative, simpler version of 5 card Stayman is:
1NT - 2
This structure permits use by weak hands with 5+ diamonds and 2+ cards in each major.
After 1NT - 2, 2
If responder has a five-card major, he begins with a transfer. After completion of the transfer, bidding the other major at the three level shows four cards in it and a game forcing hand, in line with the 1NT - 2, 2 structure above (1NT - 2, 2 - 2 = invitational 5-4).
Similarly after 2NT - 3, 3
A drawback of Five Card Major Stayman (particularly the simpler version) is that the weaker hand may become declarer in a 4-4 major fit.
Puppet Stayman.
Puppet Stayman is similar to Five Card Stayman. It is more complex but has the major advantage that the strong hand virtually always becomes declarer.
Initially developed by Neil Silverman and refined by Kit Woolsey and Steve Robinson in 1977-78, is a variation of the Stayman convention designed to find a 5-3 fit in a major, augmenting the search for a 4-4 major fit by standard Stayman. In 1977, Woolsey wrote that Puppet Stayman has several advantages over standard Stayman:
Responder's rebids.
As in standard Stayman, Puppet Stayman begins with a 2 response to a 1NT opening and is at least game invitational; this asks opener to bid a 5-card major if he has one and otherwise to bid 2. Over a 2 response, rebids by responder are intended to dislose his distributional features in the majors as well as his strength. The original 1977 and 1978 revised rebids described by Woolsey are tabulated below: 
Opener and responder continue the bidding having a clearer understanding of each other's distributional features and are better positioned to select the ultimate and level of the contract.
Modern applications.
Many variations to the Puppet Stayman bidding structure have been devised since Woolsey's 1978 summary; partnership review and agreement on the preferred modern treatment is required.
Some no longer advocate use of Puppet Stayman over a 1NT opening preferring to use the concept exclusively over a 2NT opening and reserving other Stayman variations and conventions such Jacoby Transfers and Smolen Transfers in search of major-suit fits after a 1NT opening.
Responses to a 2NT opening or rebid.
Puppet Stayman is more commonly used after a 2NT opening than after a 1NT opening. Responses to a 2NT opening or very strong 2NT rebid (20-22 or 23-24):
Responder bids 3 seeking information about opener's major suit holding. Opener replies:
By this means all 5-3 and 4-4 major suit fits can be found.
An alternative pattern frees up the 2NT-3 sequence as a slam try in the minors. To allow 3-5 spade fits to be found when responder holds 5 spades and 4 hearts, some of the responses change:
Checkback Stayman.
2 Checkback Stayman (or simply Checkback) is used after a 1NT rebid by opener rather than a 1NT opening. It is used to "check back" if opener has major suit support, saying nothing additional about the club suit. It can find 3-5 fits, 4-4 fits (in Standard American) and 5-3 fits (in Acol), and also shows whether opener was maximum or minimum strength for his notrump bid. In five-card major systems, bidding Checkback implies that the responder has five cards in his major, and may have four in the other.
1m – 1M; 1NT – 2
The 2 is "Checkback Stayman". Responses by opener shows the following:
Partnership agreement is required on how to handle the case of holding four of the other major and three of partner's suit. One could agree to bid up the line, or support partner's suit first. If partner cannot support your first suit, he will invite with 2NT or bid game with 3NT and you will then correct to your other suit.
In Acol, if the opening bid was a major, opener can rebid his major after a Checkback inquiry to show that it has five cards rather than four and find 5-3 fits. Moreover, 1M – 2m; 2NT – 3 can also be used as Checkback Stayman. It is useful also to include an indication of range, particularly if opener's 2NT rebid is forcing to game and shows a wide points range (15-19). This is achieved by using 3 for minimum hands and 3/3/3NT for maximum hands, or vice versa. After 3, responder can still bid 3/3 to look for a 5-3 fit.
New Minor Forcing is an alternative to Checkback Stayman where either 2 or 2 can be used as the checkback bid. It can be used by responder with invitational values or better to find three-card support for his major or to find a 4-4 heart fit if holding five spades and four hearts); it also allows a return to the minor to play.

</doc>
<doc id="29483" url="https://en.wikipedia.org/wiki?curid=29483" title="Saks Fifth Avenue">
Saks Fifth Avenue

Saks Fifth Avenue is an American department store owned by the oldest commercial corporation in North America, the Hudson's Bay Company. Its main flagship store is located on Fifth Avenue in Midtown Manhattan, New York City. Competitors in the US have historically included Bergdorf Goodman, Neiman Marcus, Bloomingdale's, and its sister Lord & Taylor.
History.
Saks Fifth Avenue is the successor of a business founded by Andrew Saks in 1867 and incorporated in New York in 1902 as Saks & Company. Saks died in 1912, and in 1923 Saks & Co. merged with Gimbel Brothers, Inc., which was owned by a cousin of Horace Saks, Bernard Gimbel, operating as a separate autonomous subsidiary. On September 15, 1924, Horace Saks and Bernard Gimbel opened Saks Fifth Avenue in New York City, with a full-block avenue frontage south of St Patrick's Cathedral, facing Rockefeller Center. The architects were Starrett & van Vleck, who developed a reticent, genteel Anglophile classicizing facade similar to their Gimbels Department Store in Pittsburgh (1914).
When Bernard's brother, Adam Gimbel, became president of Saks Fifth Avenue in 1926 after Bernard's sudden passing, the company expanded, opening seasonal resort branches in Palm Beach, Florida and Southampton, New York, in 1928. The first full-line year-round Saks store opened in Chicago, in 1929, followed by another resort store in Miami Beach, Florida. In 1938, Saks expanded to the West Coast, opening in Beverly Hills, California. By the end of the 1930s, Saks Fifth Avenue had a total of 10 stores, including resort locations such as Sun Valley, Idaho, Mount Stowe, and Newport, Rhode Island. More full-line stores followed with Detroit, Michigan, in 1940 and Pittsburgh, Pennsylvania, in 1949. In Downtown Pittsburgh, the company moved to its own freestanding location approximately one block from its former home on the fourth floor in the downtown Gimbel's flagship. The San Francisco location opened in 1952. BATUS Inc. acquired Gimbel Bros., Inc. and its Saks Fifth Avenue subsidiary in 1973 as part of its diversification strategy. More expansion followed from the 1960s through the 1990s including the Midwest, and the South, particularly in Texas. In 1990, BATUS sold Saks to Investcorp S.A., which took Saks public in 1996 as Saks Holdings, Inc.
In 1998, Proffitt's, Inc. the parent company of Proffitt's and other department stores, acquired Saks Holdings Inc. Upon completing the acquisition, Proffitt's, Inc. changed its name to Saks, Inc.
Since 2000 Saks has opened international locations in Saudi Arabia, United Arab Emirates, Bahrain, Kazakhstan, and Mexico City.
In August 2007, the United States Postal Service began an experimental program selling the plus zip code extension to businesses. The first company to do so was Saks Fifth Avenue, which received the zip code of 10022-SHOE (7463) for the eighth-floor shoe department in its flagship Fifth Avenue store.
As of 2013, the New York flagship store generated around 20% of annual sales at $1 billion.
On July 29, 2013, the Hudson's Bay Company (HBC), owner of the competing chain Lord & Taylor, announced it would acquire Saks Fifth Avenue's parent company for US$2.9 billion. Plans called for up to seven Saks Fifth Avenues to open in major Canadian markets. Expansion into Canada is expected to compete with Canadian Holt Renfrew chain and challenge Nordstrom's expansion into Canada, which began in summer 2014 with the opening of a Nordstrom store in Calgary. In January 2014, HBC announced the first Saks store in Canada would occupy in its flagship Queen Street building in downtown Toronto, connected to the Toronto Eaton Centre via sky bridge. The store opened in February 2016 with a second Toronto area location in the Sherway Gardens shopping center opening in spring 2016.
On January 6, 2014, Marigay McKee, previously Chief Merchant at Harrods, became president of Saks Fifth Avenue. She stepped down 15 months later on April 2, 2015, and was replaced by Marc Metrick, a former executive at Saks’s parent company, Hudson’s Bay.
Starting in 2015 Saks began a $250 million, three-year restoration of its Fifth Avenue flagship store.
In the summer of 2015, it was announced that Saks will debut a new location in Greenwich, Connecticut. In the fall of 2015, Saks plans to replace its existing store at the Houston Galleria with a new store.
In February 2016, it opened its first Canadian store, at the corner of Queen Street West and Yonge Street in Toronto, in the Hudson's Bay flagship store.
Legal controversies.
In 2005, vendors filed against Saks alleging unlawful chargebacks. The U.S. Securities and Exchange Commission (SEC) investigated the complaint for years and, according to the "New York Times", "exposed a tangle of illicit tactics that let Saks... keep money it owed to clothing makers", inflating Saks' yearly earnings up to 43% and abusively collecting around $30 million from suppliers over seven years. Saks settled with the SEC in 2007, after firing three or more executives involved in the fraudulent activities.
In 2014 Saks fired transgender employee Leyth Jamal after she was allegedly "belittled by coworkers, forced to use the men's room and repeatedly referred to by male pronouns (he and him)". After Jamal submitted a lawsuit for unfair dismissal, the company stated in a motion to dismiss that "it is well settled that transsexuals are not protected by Title VII." In a court filing, the United States Department of Justice rebuked Saks' argument, stating that "discrimination against an individual based on gender identity is discrimination because of sex." The company was removed from the Human Rights Campaign's list of "allies" during the controversy. The lawsuit was later settled amicably, without disclosing the terms of the settlement.

</doc>
<doc id="29484" url="https://en.wikipedia.org/wiki?curid=29484" title="Seabee">
Seabee

A Seabee is a member of the United States Naval Construction Forces (NCF). The word "Seabee" comes from initials "CB", which in turn comes from the term "Construction Battalion". The Seabees have a history of building bases, bulldozing and paving thousands of miles of roadway and airstrips, and accomplishing myriad other construction projects in a wide variety of military theaters dating back to World War II.
History.
World War II.
In December 1941, with U.S. involvement in war soon expected on both the Pacific and Atlantic Oceans, Rear Admiral Ben Moreell, Chief of the Navy's Bureau of Yards and Docks, recommended establishing Naval Construction Battalions at a newly constructed base at Davisville, Rhode Island (part of North Kingstown). With the attack on Pearl Harbor and the United States' entry into the war, he was given the go-ahead. The Davisville Advanced Base Depot became operational in June 1942. It eventually contained 500 Quonset huts for personnel. On August 11, 1942, the Naval Construction Training Center, known as Camp Endicott, was commissioned at Davisville. The Camp trained over 100,000 Seabees during World War II. Camp Thomas, a personnel-receiving station on the base, was established in October of that year.
In California in May 1942, a base for supporting the Naval Construction Force was established at Port Hueneme in Ventura County, California. This base became responsible for shipping large amounts of equipment and matériel to the efforts in the Pacific.
The earliest Seabees were recruited from the civilian construction trades and were placed under the leadership of the Navy's Civil Engineer Corps. Because of the emphasis on experience and skill rather than physical standards, the average age of Seabees during World War II was 37.
More than 325,000 men served with the Seabees in World War II, fighting and building on six continents and more than 300 islands. In the Pacific, where most of the construction work was needed, the Seabees landed soon after the Marines and built airstrips, bridges, roads, gasoline storage tanks, and Quonset huts for warehouses, hospitals, and housing. They often operated under fire and frequently were forced to take part in the fighting to defend themselves and their construction projects. In the Pacific Theater they built 111 major airstrips and 441 piers, tanks for the storage of 100 m gallons of fuel, housing for 1.5 million men and hospitals for 70,000 patients.
The Seabees were officially organized in the Naval Reserve on December 31, 1947.
With the general demobilization following the war, the Naval Construction Battalions (NCBs) were reduced to 3,300 men on active duty by 1950. Between 1949 and 1953, Naval Construction Battalions were organized into two types of units: Amphibious Construction Battalions (ACBs) and Mobile Construction Battalions MCBs), which were later designated Naval Mobile Construction Battalions (NMCBs) in the early- to mid-1960s to eliminate confusion with Marine Corps Base (MCB) in Vietnam.
Korean War.
The Korean War saw a call-up of more than 10,000 men. The expansion of the Seabees came from the Naval Reserve Seabee program where individuals volunteered for active duty. The Seabees landed at Inchon with the assault troops. They fought enormous tides as well as enemy fire and provided causeways within hours of the initial landings. Their action here and at other landings emphasized the role of the Seabees, and there was no Seabee demobilization when the truce was declared.
During the Korean War, the Navy realized they needed a naval air station in this region. Cubi Point in the Philippines was selected, and civilian contractors were initially selected for the project. After seeing the forbidding Zambales Mountains and the maze of jungle, they claimed it could not be done.
The Navy then turned to the Seabees. The first Seabees to arrive were MCB-3 on October 2, 1951; followed by MCB-5 on November 5, 1951. Over the next five years, MCB-2, -7, -9, -11 and -13 were also deployed to Cubi Point.
Seabees cut a mountain in half to make way for a nearly two-mile-long runway. Cubi Point turned out to be one of the largest earth-moving projects in the world, equivalent to the construction of the Panama Canal. The $100 million facility was commissioned on July 25, 1956, and comprised an air station and an adjacent pier that was capable of docking the Navy's largest carriers.
Following Korea, the Seabees embarked on a new mission. From providing much needed assistance in the wake of a devastating earthquake in Greece in 1953 to providing construction work and training to underdeveloped countries, the Seabees became "The Navy's Goodwill Ambassadors". Seabees built or improved many roads, orphanages and public utilities in many remote parts of the world.
Antarctica.
In 1955, Seabees began deploying yearly to the continent of Antarctica. As participants in Operation Deep Freeze, their mission was to build and expand scientific bases located on the frozen continent. The first "wintering over" party included 200 Seabees who distinguished themselves by constructing a ice runway on McMurdo Sound. Despite a blizzard that once destroyed the entire project, the airstrip was completed in time for the advance party of Deep Freeze II to become the first to arrive at the South Pole by plane.
Over the following years and under adverse conditions, Seabees added to their list of accomplishments such things as snow-compacted roads, underground storage, laboratories, and living areas. One of the most notable achievements took place in 1962, when the Navy's builders constructed Antarctica's first nuclear power plant, at McMurdo Station.
During the Cold War, the Seabees undertook a number of other missions, including constructing the Distant Early Warning Line in the Arctic. Again operating often under extreme conditions, the Seabees successfully completed every mission assigned to them.
Vietnam War.
Seabees were deployed to Vietnam throughout the conflict beginning in small numbers in June 1954 and extending to November 1972. By 1962, they began building camps for Special Forces. In June 1965, Construction Mechanic 3rd Class Marvin G. Shields, part of Seabee Team 1104, was actively engaged at the Battle of Dong Xoai and was posthumously awarded the Medal of Honor for his actions there. Shields remains the only Seabee ever to be awarded the Medal of Honor. These "Civic Action Teams" continued into the Vietnam War where Seabees, often fending off enemy forces alongside their Marine and Army counterparts, also built schools and infrastructure and provided health care service. Beginning in 1965, full Seabee battalions (NMCBs) and Naval Construction Regiments (NCRs), along with other unit types, were deployed throughout Vietnam. Seabees from the Naval Reserve provided individual personnel early on to augment regular units and two battalions, RNMCB- 12 and RNMCB- 22.
In Vietnam, the Seabees supported the Marines and built a staggering number of aircraft-support facilities, roads, and bridges; they also paved roads that provided access to farms and markets, supplied fresh water to countless numbers of Vietnamese through hundreds of Seabee-dug wells, provided medical treatment to thousands of villagers, and built schools, hospitals, utilities systems, roads and other community facilities. Seabees also worked with and taught construction skills to the Vietnamese people.
After Vietnam, the Seabees built and repaired Navy bases in Puerto Rico, Japan, Guam, Greece, Sicily, and Spain. Their civic action projects focused on the Trust Territories of the Pacific.
In 1971, the Seabees began their largest peacetime construction on Diego Garcia, a small atoll in the Indian Ocean. This project took 11 years and cost $200 million. The complex accommodates the Navy's largest ships and the biggest military cargo jets. This base proved invaluable when Iraq invaded Kuwait in August 1990 and Operations Desert Shield and Desert Storm were launched.
From the Cold War to terrorism.
As the Cold War cooled off, new challenges were presented by the increased incidence of terrorism. Also there were ongoing support missions to Diego Garcia, Guam, Okinawa, Navy and Marine Bases in Japan, the Philippines, Puerto Rico, Guantanamo Bay, Guatemala, the Naval Support Facility for Polaris and Poseidon Submarines in Holy Loch Scotland, Rota Spain, Naples Italy and Suda Bay Crete.
Seabee construction efforts led to the expansion and improvement of Naval Air Facility, Sigonella Sicily, turning this into a major base for the Navy’s Sixth Fleet aviation activities.
There were combat roles as well. In 1983, a truck bomb demolished the barracks the Marines had secured in Beirut, Lebanon. After moving to the Beirut International Airport and setting up quarters there, Druse militia artillery began harassing the Marines. After consultations with the theater commander and Marine amphibious command and combat engineers, the forward deployed battalion, NMCB-1 in Rota Spain sent in a 70-man AirDet working party with heavy equipment. Construction of artillery-resistant quarters went on from December 1983 until the Marines’ withdrawal in February 1984. Only one casualty occurred when an Equipment Operator using a bulldozer to clear fields of fire was wounded by an RPG attack. Seabee EO2 Kirt May received the first Purple Heart awarded to a Seabee since Vietnam. The Seabees were proud that the Marines had greatly improved protection from ongoing artillery harassment.
Robert Stethem was murdered by the Lebanese Shia militia Hezbollah when they hijacked TWA Flight 847 in 1985. Stethem was a Steelworker Second Class (SW2), a Seabee diver and member of Underwater Construction Team ONE. The USS "Stethem" (DDG-63) is named in his honor. On August 24, 2010, onboard USS "Stethem" in Yokosuka, Japan, Stethem was posthumously made an honorary Master Chief Constructionman (CUCM) by the Master Chief Petty Officer of the Navy.
Persian Gulf War.
During the Persian Gulf War, more than 5,000 Seabees (4,000 active and 1,000 reservists) served in the Middle East. In Saudi Arabia, Seabees built 10 camps for more than 42,000 personnel; 14 galleys capable of feeding 75,000 people; and 6 million ft² (600,000 m²) of aircraft parking apron and runways as well as 200+ Helo landing zones. They built and maintained two 500-bed Fleet Hospitals near the port city of Al-Jubayl.
Iraq War and the War in Afghanistan.
Seabees continue to provide critical construction skills in connection with the effort to rebuild the infrastructure of Afghanistan. All active and reserve Naval Mobile Construction Battalions (NMCBs) and Naval Construction Regiments (NCRs) have been deployed to both Iraq and Afghanistan. The Seabees have been deployed since the beginning of the invasion of Afghanistan in 2001 and Iraq in 2003. One of their most high profile tasks in Iraq has been the removal of statues of Saddam Hussein in Baghdad. In Afghanistan, the Seabees' main task has been the construction of multiple forward operating bases for U.S. and coalition forces.
Operation Enduring Freedom Southern Philippines.
Since 2002, Seabees have provided critical and tactical construction skills in an effort to win the hearts and minds of locals. Their efforts have begun to deter the rising influence of radical terrorists in the southern Philippines, most notably the Abu Sayyaf's jungle training area. Seabees work along with Army, Marines, and Air Force under Joint Special Operations Task Force-Philippines.
Disaster relief and recovery.
In 1969 when Hurricane Camille hit the gulf coast, the MCB-121 battalion stationed at Gulfport were called upon for cleanup, rescue, and community outreach for months to come. They fed displaced families and supported the community.
Seabees supported disaster recovery efforts for victims of the 1994 Northridge earthquake.
In summer 1992, Seabees were called on to provide recovery assistance for Homestead, Florida following Hurricane Andrew. Seabees were also vital to the humanitarian efforts in Somalia during Operation Restore Hope from 1992 to 1993. In 1994, they were again called on to provide assistance to the Haitian Relief effort at Guantanamo Bay Naval Base, Cuba. On Christmas Day 1995, Seabees arrived in Croatia to support the Army by building camps as part of Operation Joint Endeavor, the peacekeeping effort in Sarajevo, Bosnia and Herzegovina. NMCB 40 played a pivotal role serving with the U.S. Army 1st Infantry Division "The Big Red One", in assisting with the dismantling of FOB's during the IFOR/SFOR phase.
On September 23, 1998, Hurricane Georges plowed through the Caribbean Islands causing millions of dollars in damage and generating thousands of DRT (disaster recovery team) man hours for the Seabees. The Navy provided generators and water trucks that were taken to nearby cities and damage assessment teams were sent to the local islands.
Shortly after Hurricane Georges ravaged Puerto Rico and most of the Caribbean, the Seabees immediately turned their focus towards Hurricane Mitch, which was the most powerful hurricane of the 1998 season. Mitch left more than 17,000 people dead due to the high winds and heavy rains, which led to mudflows that buried thousands in Central America. The Seabees deployed to Honduras, participating in operations with Joint Task Force Bravo, providing capabilities to conduct engineer reconnaissance, repair roads and bridges, clear debris, remove bridges, and build base camps. Naval Mobile Construction Battalion Seven was the first Navy element to arrive in Central America, taking part in their second humanitarian mission on the deployment.
Seabees deployed in September 2004 in response to Hurricane Ivan’s destruction to the Naval Air Station Pensacola in Florida. The Seabees cleared hurricane debris, repaired roads, erected tents, and otherwise assisted fellow service members.
The Naval Construction Battalion Center in Gulfport, Mississippi, suffered damage during Hurricane Katrina in 2005. Seabees were tasked to rebuild the base and the Gulf Coast of the United States.
Seabees of Naval Mobile Construction Battalion Seven deployed to provide construction support and disaster relief to Haiti following the earthquake in 2010. Seabee divers from Underwater Construction Team One along with ACB-2 and the Army Engineer divers made repairs to the heavily damaged port facilities in Port-au-Prince. This resulted in the re-opening of the port to allow humanitarian supplies into the country.
Seabees from NMCB-133 and Underwater Construction Team Two deployed to Japan as part of the relief effort after the 2011 earthquake and tsunami.
Seabees of Naval Mobile Construction Battalion Eleven Air Detachment deployed for roughly two weeks to support federal, state, and local authorities in disaster recovery operations in the New Jersey and New York areas affected by Hurricane Sandy. The Air Detachment mounted out 90 personnel and 94 pieces of civil engineering support equipment including front-end loaders, backhoes, pumps, electric generators, storage containers, and other equipment which was convoyed to the disaster area.
Organization.
Unit nomenclature.
Battalion.
The battalion is the fundamental unit of the Naval Construction Force (NCF). Seabee battalions are constituted in such a way as to be self-sustaining in the field. The nomenclature for NCF battalions has evolved over the years. 
From the early 1960s through 1991, reserve battalions were referred to as Reserve Naval Mobile Construction Battalions (RNMCB). After 1991, all reserve battalions were renamed to NMCB, signifying the integration of the reserve units with the active units of the NCF.
Regiment.
During the rapid build-up of the Seabees during World War II, the number of battalions in a given area increased and larger construction programs were undertaken. This necessitated a higher command echelon to plan, coordinate, and assign the work of several battalions in one area. As a result, Naval Construction Regiments (NCR) were established in December 1942.
Brigade.
In April 1943, Naval Construction Brigades (NCB) were organized to coordinate the work of regiments. Brigades were the highest NCF command echelon until early in the 21st Century. At that time, the last two brigades were the SECOND Naval Construction Brigade (2nd NCB) and the THIRD Naval Construction Brigade (3rd NCB). The 2nd NCB commanded Atlantic Fleet Seabee units and the 3rd NCB commanded Pacific Fleet Seabee units. Both brigades were decommissioned in August 2002 and are no longer part of the NCF structure.
Division.
Shortly after the commencement of the Global War on Terror, it was realized that a single command interface for global Seabee operations would be required. On August 9, 2002, the FIRST Naval Construction Division (1 NCD) was stood-up and commissioned at NAB Little Creek in Virginia. Since January 2006, 1NCD has been a subordinate unit of Navy Expeditionary Combat Command (NECC). First Naval Construction Division (1NCD) was decommissioned May 31, 2013. The 1NCD staff will be integrated into NECC. Some 1NCD functions have been transferred to the newly created Naval Construction Groups (NCGs) in Gulfport, Mississippi, and Port Hueneme, California, which are now the East and West Coast continuity for the NCF.
Specialty units.
Construction Battalion Maintenance Unit (CBMU).
When first organized during World War II, these units consisted of approximately one-fourth the personnel of an NCB and were intended to take over the maintenance of bases on which major construction had been completed. Today, CBMU's provide public works support at Naval Support Activities, Forward Operating Bases, and Fleet Hospital/Expeditionary Medical Facilities during wartime or contingency operations. They also provide disaster recovery support to Naval Regional Commanders in CONUS.
Underwater Construction Team (UCT).
UCT's deploy worldwide to conduct underwater construction, inspection, repair, and demolition operations of ocean facilities, to include repair of battle damage. They maintain a capability to support a Fleet Marine Force amphibious assault, subsequent combat service support ashore, and self-defense for their camp and facilities under construction.
Naval Construction Groups.
In 2013, the Seabee Readiness Groups (SRG) were decommissioned and re-formed into Naval Construction Groups ONE and TWO. They are regimental-level command groups tasked with administrative and tactical control of Seabee Battalions, as well as conducting pre-deployment training of NCF units in the NCG's respective homeport locations. Currently, Naval Construction Group TWO (NCG-2) is based at CBC Gulfport, and Naval Construction Group ONE (NCG-ONE) is based at CBC Port Hueneme.
Amphibious Construction Battalion (ACB).
ACB's (also abbreviated as PHIBCB) evolved out of pontoon assembly battalions formed as part of the Seabees during World War II. After the war, these battalions (originally MCBs 104 and 105) were renamed ACB's and assigned to Naval Beach Groups.
Today, while the ACBs are part of the NCF, they do not report to 1 NCD, instead reporting to surface TYCOMs. Additionally, the ACBs have a different personnel mix than an NMCB with half the enlisted personnel being traditional Seabee rates and the other half being fleet rates.
Obsolete units.
NCF unit types that are no longer in use include:
Construction Battalion Hospital Unit (CBHU)
Training.
The newcomers begin "A" School (preliminary training) fresh out of boot camp, or they come from the fleet after their service term is met, spending about 75% of the twelve weeks immersed in hands-on training. The remaining 25% is spent in classroom instruction. From "A" School, new Seabees most often report to an NMCB command for their first tour of duty. For training, the new Seabees attend a four-week course known as Expeditionary Combat Skills (ECS) at the Naval Construction Battalion Center in Gulfport, Mississippi, and Port Hueneme, California. ECS is also being taught to all personnel who report to a unit in the Navy Expeditionary Combat Command. ECS is a basic combat-skills course where the students spend time in a classroom environment learning map reading and land navigation, battlefield first aid, how to lay out defensive plans, how to conduct patrols, vehicle egress, and many other combat-related skills. Half of each course is spent at a shooting range where students learn basic rifle marksmanship and then qualify with the M16A2 and M16A3 service rifles. ECS students also learn fundamentals of the M9 service pistol and qualify. At the end of training, new Seabees are ready to perform with their new battalion. During their tenure with an NMCB, personnel may be assigned to a crew-served weapon, such as the MK 19 40 mm machine gun, the M2HB .50-caliber machine gun, or the M240 machine gun. Many reserve units still field variants of the M60 machine gun. Until 2012, Seabees wore the U.S. Woodland camouflage uniform or the legacy tri-color Desert Camouflage Uniform, the last members of the entire U.S. military to do so, but are now transitioning to the NWU Type III. Seabees use ALICE field gear as well as some units working with Marines use USMC issue Improved Load Bearing Equipment (ILBE) gear.
About one-third of new Seabees are assigned to Public Works Departments (PWD) at naval installations both within the United States and overseas. While stationed at a Public Works Department, a Seabee has the opportunity to get specialized training and extensive experience in one or more facets of their rating.
Ratings.
There are seven source ratings for the Seabee community:
Badge.
The military qualification badge for the Seabees is known as the Seabee Combat Warfare Specialist insignia (SCW). It is issued to both officers and enlisted personnel and recognizes those who have been fully trained and qualified as a member of the various Naval Construction Force (NCF) units. Only members attached to a qualifying NCF unit are eligible for the SCWs pin. The qualifying units include: Naval Mobile Construction Battalions (NMCB), Amphibious Construction Battalions (ACB), Naval Construction Force Support Units (NCFSU), Underwater Construction Teams (UCT), and, since the end of 2008, Naval Construction Regiments (NCR).
The SCWs insignia has been in existence since it was officially approved for use in 1993.
Ranks.
The ranks of E-1 through E-3 in the Navy include Seaman (white stripes), Airman (green stripes), and Fireman (red stripes). E-1 through E-3 Seabees use the designation "Constructionman" and wear sky-blue stripes on their dress and service uniforms.
Logo.
Frank J. Iafrate, a civilian plan file clerk at Quonset Point Air National Guard Station, Rhode Island, was the artist who designed the original Seabee logo ("Fighting 'Bee") in early 1942. The logo has remained in use, unchanged. In late 1942, after designing the logo, he enlisted in the Seabees.
During World War II, artists working for Walt Disney designed logos for about ten Naval Construction units, including the 60th Naval (Canal) Construction Battalion and the 133rd Naval Construction Battalion, in 1945.
Battalions.
During World War II, there were more than 140 battalions commissioned. In the years between then and the present, battalions have been activated and deactivated as required by shifting national defense priorities. At present, there are five active-duty naval mobile construction battalions (NMCBs) — known as Seabees — in the United States Navy, split between the east and west coasts. The remaining battalions are Navy Reserve battalions:
Museums.
The U.S. Navy Seabee Museum is located at Naval Base Ventura County, Port Hueneme, California near the entrance, but outside the main gate. Due to the location, visitors are able to visit the museum without having to enter the base proper. The museum re-opened on July 22, 2011 in a new building built by Carlsbad-based RQ Construction. The design of the single-story, 38,833 square foot structure was inspired by the Seabee Quonset hut. Inside are galleries for exhibition space, a grand hall, a theater for 45 people, collections storage, and research areas.
On February 7, 2011, the museum was certified as LEED Silver for utilizing a number of sustainable design and construction strategies. Features include the use of low-maintenance landscaping; a “cool” roofing system with high solar reflectance and thermal emittance; use of photocell-controlled light fixtures and energy-efficient lighting fixtures; 30% use of regional materials and 80% construction debris was recycled and diverted from landfills; low-volatility organic compounds (VOCs); and, use of dual-flush toilets and low-flow aerator faucets.
The Seabee Heritage Center is located in Building 446 at the Naval Construction Battalion Center (Gulfport, Mississippi). The Heritage Center is the Atlantic Coast Annex of the Seabee Museum in Port Hueneme. Opened in 1995, the Museum Annex commemorates the history and achievements of the Atlantic Coast Naval Construction Force (Seabees) and the Navy's Civil Engineer Corps. Exhibits at the Gulfport Annex are provided by the Seabee Museum in Port Hueneme.
The Seabee Museum and Memorial Park in Davisville, Rhode Island was opened in the late 1990s by a group of former Seabees. The Fighting Seabee Statue is located here.

</doc>
<doc id="29485" url="https://en.wikipedia.org/wiki?curid=29485" title="Skyscraper">
Skyscraper

A skyscraper is a tall, continuously habitable building of over 40 floors, mostly designed for office, commercial and residential uses. A skyscraper can also be called a high-rise, but the term skyscraper is often used for buildings higher than . For buildings above a height of , the term "Supertall" can be used, while skyscrapers reaching beyond are classified as "Megatall".
One common feature of skyscrapers is having a steel framework that supports curtain walls. These curtain walls either bear on the framework below or are suspended from the framework above, rather than load-bearing walls of conventional construction. Some early skyscrapers have a steel frame that enables the construction of load-bearing walls taller than of those made of reinforced concrete. Modern skyscrapers' walls are not load-bearing and most skyscrapers are characterized by large surface areas of windows made possible by the concept of steel frame and curtain walls. However, skyscrapers can have curtain walls that mimic conventional walls and a small surface area of windows. Modern skyscrapers often have a tubular structure, and are designed to act like a hollow cylinder to resist lateral loads (wind, seismic, etc.). To appear more slender, allow less wind exposure and to transmit more daylight to the ground, many skyscrapers have a design with setbacks; sometimes they are also structurally required.
Definition.
A relatively big building may be considered a skyscraper if it protrudes well above its built environment and changes the overall skyline. The maximum height of structures has progressed historically with building methods and technologies and thus what is today considered a skyscraper is taller than before. The Burj Khalifa is currently the tallest building in the world.
High-rise buildings are considered shorter than skyscrapers. There is no clear definition of any difference between a tower block and a skyscraper though a building lower than about thirty stories is not likely to be a skyscraper and a building with fifty or more stories is certainly a skyscraper.
The term "skyscraper" was first applied to buildings of steel framed construction of at least 10 stories in the late 19th century, a result of public amazement at the tall buildings being built in major cities like Chicago, New York City, Philadelphia, Detroit, and St. Louis. The first steel-frame skyscraper was the Home Insurance Building (originally 10 stories with a height of ) in Chicago, Illinois in 1885. Some point to Philadelphia's 10-story Jayne Building (1849–50) as a proto-skyscraper, or to New York's seven-floor Equitable Life Building (New York City), built in 1870, for its innovative use of a kind of skeletal frame, but such designation depends largely on what factors are chosen. Even the scholars making the argument find it to be purely academic.
The structural definition of the word "skyscraper" was refined later by architectural historians, based on engineering developments of the 1880s that had enabled construction of tall multi-story buildings. This definition was based on the steel skeleton—as opposed to constructions of load-bearing masonry, which passed their practical limit in 1891 with Chicago's Monadnock Building.
The Emporis Standards Committee defines a high-rise building as "a multi-story structure between 35–100 meters tall, or a building of unknown height from 12–39 floors" and a skyscraper as "a multi-story building whose architectural height is at least ." Some structural engineers define a highrise as any vertical construction for which wind is a more significant load factor than earthquake or weight. Note that this criterion fits not only high-rises but some other tall structures, such as towers.
The word "skyscraper" often carries a connotation of pride and achievement. The skyscraper, in name and social function, is a modern expression of the age-old symbol of the world center or "axis mundi": a pillar that connects earth to heaven and the four compass directions to one another.
A loose convention of some in the United States and Europe draws the lower limit of a skyscraper at .
The tallest building in ancient times was the Great Pyramid of Giza in ancient Egypt, built in the 26th century BC. It was not surpassed in height for thousands of years, the 14th century AD Lincoln Cathedral being conjectured by many to have exceeded it. The latter in turn was not surpassed until the Washington Monument in 1884. However, being uninhabited, none of these structures actually comply with the modern definition of a skyscraper.
High-rise apartments flourished in classical antiquity. Ancient Roman insulae in imperial cities reached 10 and more stories. Beginning with Augustus (r. 30 BC-14 AD), several emperors attempted to establish limits of 20–25 m for multi-story buildings, but met with only limited success. Lower floors were typically occupied by shops or wealthy families, the upper rented to the lower classes. Surviving Oxyrhynchus Papyri indicate that seven-story buildings existed in provincial towns such as in 3rd century AD Hermopolis in Roman Egypt.
The skylines of many important medieval cities had large numbers of high-rise urban towers, built by the wealthy for defense and status. The residential Towers of 12th century Bologna numbered between 80 and 100 at a time, the tallest of which is the high Asinelli Tower. A Florentine law of 1251 decreed that all urban buildings be immediately reduced to less than 26 m. Even medium-sized towns of the era are known to have proliferations of towers, such as the 72 up to 51 m height in San Gimignano.
The medieval Egyptian city of Fustat housed many high-rise residential buildings, which Al-Muqaddasi in the 10th century described as resembling minarets. Nasir Khusraw in the early 11th century described some of them rising up to 14 stories, with roof gardens on the top floor complete with ox-drawn water wheels for irrigating them. Cairo in the 16th century had high-rise apartment buildings where the two lower floors were for commercial and storage purposes and the multiple stories above them were rented out to tenants. An early example of a city consisting entirely of high-rise housing is the 16th-century city of Shibam in Yemen. Shibam was made up of over 500 tower houses, each one rising 5 to 11 stories high, with each floor being an apartment occupied by a single family. The city was built in this way in order to protect it from Bedouin attacks. Shibam still has the tallest mudbrick buildings in the world, with many of them over high.
An early modern example of high-rise housing was in 17th-century Edinburgh, Scotland, where a defensive city wall defined the boundaries of the city. Due to the restricted land area available for development, the houses increased in height instead. Buildings of 11 stories were common, and there are records of buildings as high as 14 stories. Many of the stone-built structures can still be seen today in the old town of Edinburgh. The oldest iron framed building in the world, although only partially iron framed, is The Flaxmill (also locally known as the "Maltings"), in Shrewsbury, England. Built in 1797, it is seen as the "grandfather of skyscrapers”, since its fireproof combination of cast iron columns and cast iron beams developed into the modern steel frame that made modern skyscrapers possible. In 2013 funding was confirmed to convert the derelict building into offices.
Early skyscrapers.
In 1857 Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. An early development in this area was Oriel Chambers in Liverpool. Designed by local architect Peter Ellis in 1864, the building was the world's first iron-framed, glass curtain-walled office building. It was only 5 floors high. Further developments led to the world's first skyscraper, the ten-story Home Insurance Building in Chicago, built in 1884–1885. While its height is not considered very impressive today, it was at that time. The architect, Major William Le Baron Jenney, created a load-bearing structural frame. In this building, a steel frame supported the entire weight of the walls, instead of load-bearing walls carrying the weight of the building. This development led to the "Chicago skeleton" form of construction. In addition to the steel frame, the Home Insurance Building also utilized fireproofing, elevators, and electrical wiring, key elements in most skyscrapers today.
Burnham and Root's 1889 Rand McNally Building in Chicago, 1889, was the first all-steel framed skyscraper, while Louis Sullivan's Wainwright Building in St. Louis, Missouri, 1891, was the first steel-framed building with soaring vertical bands to emphasize the height of the building and is therefore considered by some to be the first true skyscraper.
Most early skyscrapers emerged in the land-strapped areas of Chicago and New York City toward the end of the 19th century. A land boom in Melbourne, Australia between 1888–1891 spurred the creation of a significant number of early skyscrapers, though none of these were steel reinforced and few remain today. Height limits and fire restrictions were later introduced. London builders soon found building heights limited due to a complaint from Queen Victoria, rules that continued to exist with few exceptions until the 1950s.
Concerns about aesthetics and fire safety had likewise hampered the development of skyscrapers across continental Europe for the first half of the twentieth century. With some notable exceptions, like the 1898 Witte Huis "(White House)" in Rotterdam; the Royal Liver Building in Liverpool, completed in 1911 and high; the 1924 Marx House in Düsseldorf, Germany; the 17-story Kungstornen "(Kings' Towers)" in Stockholm, Sweden, which were built 1924–25, the 15-story Edificio Telefónica in Madrid, Spain, built in 1929; the 26-story Boerentoren in Antwerp, Belgium, built in 1932; the 16-story Prudential Office Building in Warsaw, Poland, built in 1934; and the 31-story Torre Piacentini in Genoa, Italy, built in 1940).
After an early competition between Chicago and New York City for the world's tallest building, New York took the lead by 1895 with the completion of the American Surety Building, leaving New York with the title of the world's tallest building for many years. New York City developers competed among themselves, with successively taller buildings claiming the title of "world's tallest" in the 1920s and early 1930s, culminating with the completion of the Chrysler Building in 1930 and the Empire State Building in 1931, the world's tallest building for forty years. The first completed World Trade Center tower became the world's tallest building in 1972. However, it was overtaken by the Sears Tower (now Willis Tower) in Chicago within two years. The Sears Tower stood as the world's tallest building for 24 years, from 1974 until 1998, until it was edged out by Petronas Twin Towers in Kuala Lumpur, which held the title for six years.
Modern skyscrapers.
Modern skyscrapers are built with steel or reinforced concrete frameworks and curtain walls of glass or polished stone. They use mechanical equipment such as water pumps and elevators.
From the 1930s onwards, skyscrapers began to appear around the world - also in Latin America (such as São Paulo, Rio de Janeiro, Buenos Aires, Santiago, Lima, Caracas, Bogotá, Mexico City) and in Asia (Tokyo, Shanghai, Hong Kong, Manila, Singapore, Mumbai, Seoul, Kuala Lumpur, Taipei, Bangkok).
Immediately after World War II, the Soviet Union planned eight massive skyscrapers, seven of which were actually built by 1953, dubbed the "Seven Sisters of Moscow". Other skyscrapers in the style of Socialist Classicism were erected in East Germany (Frankfurter Tor), Poland (PKiN), Ukraine (Hotel Ukrayina), Latvia (Academy of Sciences) and other countries. The western countries of Europe also began to permit taller skyscrapers than before WW2, such as Madrid during the 1950s (Gran Vía). Finally, skyscrapers also began to be constructed in cities of Africa, the Middle East and Oceania (mainly Australia) from the late 1950s on.
Skyscraper projects after World War II typically rejected the classical designs of the early skyscrapers, instead embracing the uniform international style; many older skyscrapers were redesigned to suit contemporary tastes or even demolished - such as New York's Singer Building, once the world's tallest skyscraper.
German architect Ludwig Mies van der Rohe became one of the world's most renowned architects in the second half of the 20th century. He conceived of the glass façade skyscraper and, along with Norwegian Fred Severud, he designed the Seagram Building in 1958, a skyscraper that is often regarded as the pinnacle of the modernist high-rise architecture.
After the Great Depression skyscrapers construction suffered a hiatus for over thirty years due to economic problems. A revival occurred with structural innovations that transformed the industry, making it possible for people to live and work in "cities in the sky".
In the early 1960s structural engineer Fazlur Khan realized that the dominating rigid steel frame structure was not the only system apt for tall buildings, marking a new era of skyscraper construction in terms of multiple structural systems. His central innovation in skyscraper design and construction was the concept of the "tube" structural system, including the "framed tube", "trussed tube", and "bundled tube". These systems allow greater economic efficiency, and also allow skyscrapers to take on various shapes, no longer needing to be rectangular and box-shaped. The first building to employ the tube structure was the Chestnut De-Witt apartment building. Over the next fifteen years, many towers were built by Khan and the "Second Chicago School", including the massive Willis Tower. Other pioneers of this field include Hal Iyengar and William LeMessurier.
Chicago, Hong Kong, and New York City, otherwise known as "the big three," are recognized in architectural circles as having especially compelling skylines. A landmark skyscraper can inspire a boom of new high-rise projects in its city, as Taipei 101 has done in Taipei since its opening in 2004.
Modern building practices regarding supertall structures have led to the study of "vanity height". Vanity height, according to the CTBUH, is the distance between the highest floor and its architectural top (excluding antennae, flagpole or other functional extensions). Vanity height first appeared in New York City skyscrapers as early as the 1920s and 1930s but supertall buildings have relied on such uninhabitable extensions for on average 30% of their height, raising potential definitional and sustainability issues.
The current era of skyscrapers focuses on sustainability, its built and natural environments, including the performance of structures, types of materials, construction practices, absolute minimal use of materials and natural resources, energy within the structure, and a holistically integrated building systems approach. LEED is a current green building standard.
Architecturally, with the movements of Postmodernism, New Urbanism and New Classical Architecture, that established since the 1980s, a more classical approach came back to global skyscraper design, that remains popular today. Examples are the Wells Fargo Center, NBC Tower, Parkview Square, 30 Park Place, the Messeturm, the iconic Petronas Towers and Jin Mao Tower.
Other contemporary styles and movements in skyscraper design include organic, sustainable, neo-futurist, structuralist, high-tech, deconstructivist, blob, digital, streamline, novelty, critical regionalist, vernacular, Neo Art Deco and neo-historist, also known as revivalist.
3 September is the global commemorative day for skyscrapers, called "Skyscraper Day".
Design and construction.
The design and construction of skyscrapers involves creating safe, habitable spaces in very tall buildings. The buildings must support their weight, resist wind and earthquakes, and protect occupants from fire. Yet they must also be conveniently accessible, even on the upper floors, and provide utilities and a comfortable climate for the occupants. The problems posed in skyscraper design are considered among the most complex encountered given the balances required between economics, engineering, and construction management.
One common feature of skyscrapers is a steel framework from which curtain walls are suspended, rather than load-bearing walls of conventional construction. Most skyscrapers have a steel frame that enables them to be built taller than typical load-bearing walls of reinforced concrete. Skyscrapers usually have a particularly small surface area of what are conventionally thought of as walls. Because the walls are not load-bearing most skyscrapers are characterized by surface areas of windows made possible by the concept of steel frame and curtain wall. However, skyscrapers can also have curtain walls that mimick conventional walls and have a small surface area of windows.
The concept of a skyscraper is a product of the industrialized age, made possible by cheap fossil fuel derived energy and industrially refined raw materials such as steel and concrete. The construction of skyscrapers was enabled by steel frame construction that surpassed brick and mortar construction starting at the end of the 19th century and finally surpassing it in the 20th century together with reinforced concrete construction as the price of steel decreased and labour costs increased.
The steel frames become inefficient and uneconomic for supertall buildings as usable floor space is reduced for progressively larger supporting columns. Since about 1960, tubular designs have been used for high rises. This reduces the usage of material (more efficient in economic terms - Willis Tower uses a third less steel than the Empire State Building) yet allows greater height. It allows fewer interior columns, and so creates more usable floor space. It further enables buildings to take on various shapes.
Elevators are characteristic to skyscrapers. In 1852 Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. Today major manufacturers of elevators include Otis, ThyssenKrupp, Schindler, and KONE.
Advances in construction techniques have allowed skyscrapers to narrow in width, while increasing in height. Some of these new techniques include mass dampers to reduce vibrations and swaying, and gaps to allow air to pass through, reducing wind shear.
Basic design considerations.
Good structural design is important in most building design, but particularly for skyscrapers since even a small chance of catastrophic failure is unacceptable given the high price. This presents a paradox to civil engineers: the only way to assure a lack of failure is to test for all modes of failure, in both the laboratory and the real world. But the only way to know of all modes of failure is to learn from previous failures. Thus, no engineer can be absolutely sure that a given structure will resist all loadings that could cause failure, but can only have large enough margins of safety such that a failure is acceptably unlikely. When buildings do fail, engineers question whether the failure was due to some lack of foresight or due to some unknowable factor.
Loading and vibration.
The load a skyscraper experiences is largely from the force of the building material itself. In most building designs, the weight of the structure is much larger than the weight of the material that it will support beyond its own weight. In technical terms, the dead load, the load of the structure, is larger than the live load, the weight of things in the structure (people, furniture, vehicles, etc.). As such, the amount of structural material required within the lower levels of a skyscraper will be much larger than the material required within higher levels. This is not always visually apparent. The Empire State Building's setbacks are actually a result of the building code at the time (1916 Zoning Resolution), and were not structurally required. On the other hand, John Hancock Center's shape is uniquely the result of how it supports loads. Vertical supports can come in several types, among which the most common for skyscrapers can be categorized as steel frames, concrete cores, tube within tube design, and shear walls.
The wind loading on a skyscraper is also considerable. In fact, the lateral wind load imposed on super-tall structures is generally the governing factor in the structural design. Wind pressure increases with height, so for very tall buildings, the loads associated with wind are larger than dead or live loads.
Other vertical and horizontal loading factors come from varied, unpredictable sources, such as earthquakes.
Shear walls.
A shear wall, in its simplest definition, is a wall where the entire material of the wall is employed in the resistance of both horizontal and vertical loads. A typical example is a brick or cinderblock wall. Since the wall material is used to hold the weight, as the wall expands in size, it must hold considerably more weight. Due to the features of a shear wall, it is acceptable for small constructions, such as suburban housing or an urban brownstone, to require low material costs and little maintenance. In this way, shear walls, typically in the form of plywood and framing, brick, or cinderblock, are used for these structures. For skyscrapers, though, as the size of the structure increases, so does the size of the supporting wall. Large structures such as castles and cathedrals inherently addressed these issues due to a large wall being advantageous (castles), or ingeniously designed around (cathedrals). Since skyscrapers seek to maximize the floor-space by consolidating structural support, shear walls tend to be used only in conjunction with other support systems.
Steel frame.
By 1895, steel had replaced cast iron as skyscrapers' structural material. Its malleability allowed it to be formed into a variety of shapes, and it could be riveted, ensuring strong connections. The simplicity of a steel frame eliminated the inefficient part of a shear wall, the central portion, and consolidated support members in a much stronger fashion by allowing both horizontal and vertical supports throughout. Among steel's drawbacks is that as more material must be supported as height increases, the distance between supporting members must decrease, which in turn increases the amount of material that must be supported. This becomes inefficient and uneconomic for buildings above 40 stories tall as usable floor spaces are reduced for supporting column and due to more usage of steel.
Tube structural systems.
A new structural system of framed tubes was developed in 1963. Fazlur Khan and J. Rankine defined the framed tube structure as "a three dimensional space structure composed of three, four, or possibly more frames, braced frames, or shear walls, joined at or near their edges to form a vertical tube-like structural system capable of resisting lateral forces in any direction by cantilevering from the foundation." Closely spaced interconnected exterior columns form the tube. Horizontal loads (primarily wind) are supported by the structure as a whole. Framed tubes allow fewer interior columns, and so create more usable floor space, and about half the exterior surface is available for windows. Where larger openings like garage doors are required, the tube frame must be interrupted, with transfer girders used to maintain structural integrity. Tube structures cut down costs, at the same time allowing buildings to reach greater heights. Concrete tube-frame construction was first used in the DeWitt-Chestnut Apartment Building, completed in Chicago in 1963, and soon after in the John Hancock Center and World Trade Center.
The tubular systems are fundamental to tall building design. Most buildings over 40-stories constructed since the 1960s now use a tube design derived from Khan’s structural engineering principles, examples including the construction of the World Trade Center, Aon Center, Petronas Towers, Jin Mao Building, and most other supertall skyscrapers since the 1960s. The strong influence of tube structure design is also evident in the construction of the current tallest skyscraper, the Burj Khalifa.
Trussed tube and X-bracing.
Khan pioneered several other variations of the tube structure design. One of these was the concept of X-bracing, or the "trussed tube", first employed for the John Hancock Center. This concept reduced the lateral load on the building by transferring the load into the exterior columns. This allows for a reduced need for interior columns thus creating more floor space. This concept can be seen in the John Hancock Center, designed in 1965 and completed in 1969. One of the most famous buildings of the structural expressionist style, the skyscraper's distinctive X-bracing exterior is actually a hint that the structure's skin is indeed part of its 'tubular system'. This idea is one of the architectural techniques the building used to climb to record heights (the tubular system is essentially the spine that helps the building stand upright during wind and earthquake loads). This X-bracing allows for both higher performance from tall structures and the ability to open up the inside floorplan (and usable floor space) if the architect desires.
The John Hancock Center was far more efficient than earlier steel-frame structures. Where the Empire State Building (1931), required about 206 kilograms of steel per square metre and Chase Manhattan Bank Building (1961) required 275, the John Hancock Center required only 145. The trussed tube concept was applied to many later skyscrapers, including the Onterie Center, Citigroup Center and Bank of China Tower.
Bundled tube.
An important variation on the tube frame is the "bundled tube", which uses several interconnected tube frames. The Willis Tower in Chicago used this design, employing nine tubes of varying height to achieve its distinct appearance. The bundled tube structure meant that "buildings no longer need be boxlike in appearance: they could become sculpture."
The elevator conundrum.
The invention of the elevator was a precondition for the invention of skyscrapers, given that most people would not (or could not) climb more than a few flights of stairs at a time. The elevators in a skyscraper are not simply a necessary utility, like running water and electricity, but are in fact closely related to the design of the whole structure: a taller building requires more elevators to service the additional floors, but the elevator shafts consume valuable floor space. If the service core, which contains the elevator shafts, becomes too big, it can reduce the profitability of the building. Architects must therefore balance the value gained by adding height against the value lost to the expanding service core. Many tall buildings use elevators in a non-standard configuration to reduce their footprint. Buildings such as the former World Trade Center Towers and Chicago's John Hancock Center use sky lobbies, where express elevators take passengers to upper floors which serve as the base for local elevators. This allows architects and engineers to place elevator shafts on top of each other, saving space. Sky lobbies and express elevators take up a significant amount of space, however, and add to the amount of time spent commuting between floors. Other buildings, such as the Petronas Towers, use double-deck elevators, allowing more people to fit in a single elevator, and reaching two floors at every stop. It is possible to use even more than two levels on an elevator, although this has never been done. The main problem with double-deck elevators is that they cause everyone in the elevator to stop when only people on one level need to get off at a given floor.
Buildings with sky lobbies include the World Trade Center, Petronas Twin Towers and Taipei 101. The 44th-floor sky lobby of the John Hancock Center also featured the first high-rise indoor swimming pool, which remains the highest in America.
Economic rationale.
Skyscrapers are usually situated in city centers where the price of land is high. Constructing a skyscraper becomes justified if the price of land is so high that it makes economic sense to build upwards as to minimize the cost of the land per the total floor area of a building. Thus the construction of skyscrapers is dictated by economics and results in skyscrapers in a certain part of a large city unless a building code restricts the height of buildings. Skyscrapers are rarely seen in small cities and they are characteristic of large cities, because of the critical importance of high land prices for the construction of skyscrapers. Usually only office, commercial and hotel users can afford the rents in the city center and thus most tenants of skyscrapers are of these classes. Some skyscrapers have been built in areas where the bedrock is near surface, because this makes constructing the foundation cheaper, for example this is the case in Midtown Manhattan and Lower Manhattan, in New York City, but not in-between these two parts of the city.
Today, skyscrapers are an increasingly common sight where land is expensive, as in the centers of big cities, because they provide such a high ratio of rentable floor space per unit area of land.
formula_1
One problem with skyscrapers is car parking. In the largest cities most people commute via public transport, but for smaller cities a lot of parking spaces are needed. Multi-storey car parks are impractical to build very tall, so a lot of land area is needed.
There may be a correlation between skyscraper construction and great income inequality but this has not been conclusively proved.
Environmental impact.
The environmental impact of skyscrapers and whether instead of skyscrapers multiple smaller, lighter buildings would be more environmentally friendly or sustainable is under debate. The concept of a skyscraper is a product of the industrialized age, made possible by cheap fossil fuel derived energy and industrially refined raw materials such as steel and concrete. The construction of skyscrapers was enabled by steel frame construction that surpassed brick and mortar construction starting at the end of the 19th century and finally surpassing it in the 20th century together with reinforced concrete construction as the price of steel decreased and labour costs increased.
The amount of steel, concrete and glass needed to construct a single skyscraper is large, and these materials represent a great deal of embodied energy. Skyscrapers are thus energy intensive buildings, but skyscrapers have a long lifespan, for example the Empire State Building in New York City, United States completed in 1931 and is still in active use. Skyscrapers have considerable mass, which means that they must be built on a sturdier foundation than would be required for shorter, lighter buildings. Building materials must also be lifted to the top of a skyscraper during construction, requiring more energy than would be necessary at lower heights. Furthermore, a skyscraper consumes a lot of electricity because potable and non-potable water have to be pumped to the highest occupied floors, skyscrapers are usually designed to be mechanically ventilated, elevators are generally used instead of stairs, and natural lighting cannot be utilized in rooms far from the windows and the windowless spaces such as elevators, bathrooms and stairwells.
Skyscrapers can be artificially lighted and the energy requirements can be covered by renewable energy or other electricity generation of low greenhouse gas emissions. Heating and cooling of skyscrapers can be efficient, because of centralized HVAC systems, heat radiation blocking windows and small surface area of the building. There is Leadership in Energy and Environmental Design (LEED) certification for skyscrapers. For example, the Empire State Building received a gold Leadership in Energy and Environmental Design rating in September 2011 and the Empire State Building is the tallest LEED certified building in the United States, proving that skyscrapers can be environmentally friendly. Also the 30 St Mary Axe in London, the United Kingdom is an environmentally friendly skyscraper.
In the lower levels of a skyscraper a larger percentage of the building cross section must be devoted to the building structure and services than is required for lower buildings:
In low-rise structures, the support rooms (chillers, transformers, boilers, pumps and air handling units) can be put in basements or roof space—areas which have low rental value. There is, however, a limit to how far this plant can be located from the area it serves. The farther away it is the larger the risers for ducts and pipes from this plant to the floors they serve and the more floor area these risers take. In practice this means that in highrise buildings this plant is located on 'plant levels' at intervals up the building.
History of the tallest skyscrapers.
At the beginning of the 20th century, New York City was a center for the Beaux-Arts architectural movement, attracting the talents of such great architects as Stanford White and Carrere and Hastings. As better construction and engineering technology became available as the century progressed, New York City and Chicago became the focal point of the competition for the tallest building in the world. Each city's striking skyline has been composed of numerous and varied skyscrapers, many of which are icons of 20th-century architecture:
Momentum in setting records passed from the United States to other nations with the opening of the Petronas Twin Towers in Kuala Lumpur, Malaysia, in 1998. The record for the world's tallest building has remained in Asia since the opening of Taipei 101 in Taipei, Taiwan, in 2004. A number of architectural records, including those of the world's tallest building and tallest free-standing structure, moved to the Middle East with the opening of the Burj Khalifa in Dubai, United Arab Emirates.
This geographical transition is accompanied by a change in approach to skyscraper design. For much of the twentieth century large buildings took the form of simple geometrical shapes. This reflected the "international style" or modernist philosophy shaped by Bauhaus architects early in the century. The last of these, the Willis Tower and World Trade Center towers in New York, erected in the 1970s, reflect the philosophy. Tastes shifted in the decade which followed, and new skyscrapers began to exhibit postmodernist influences. This approach to design avails itself of historical elements, often adapted and re-interpreted, in creating technologically modern structures. The Petronas Twin Towers recall Asian pagoda architecture and Islamic geometric principles. Taipei 101 likewise reflects the pagoda tradition as it incorporates ancient motifs such as the ruyi symbol. The Burj Khalifa draws inspiration from traditional Islamic art. Architects in recent years have sought to create structures that would not appear equally at home if set in any part of the world, but that reflect the culture thriving in the spot where they stand.
The following list measures height of the roof. The more common gauge is the "highest architectural detail"; such ranking would have included Petronas Towers, built in 1998.
Cancellation.
Many skyscrapers were never built due to financial problems, politics and culture. The Chicago Spire was to be the tallest building in the Western Hemisphere, but it was on hold due to the global financial crisis of 2008. One year later, the project was cancelled.
Future developments.
At the time Taipei 101 broke the half-km mark in height, it was already technically possible to build structures towering over a km above the ground. Proposals for such structures have been put forward, including the Burj Mubarak Al Kabir in Kuwait and Azerbaijan Tower in Baku. Kilometer-plus structures present architectural challenges that may eventually place them in a new architectural category.
The following skyscrapers, all contenders for being among the tallest in their city or region, are under construction and due to be completed in the next few years:

</doc>
<doc id="29486" url="https://en.wikipedia.org/wiki?curid=29486" title="Sagas of Icelanders">
Sagas of Icelanders

The Sagas of Icelanders (), also known as family sagas, are prose narratives mostly based on historical events that mostly took place in Iceland in the 9th, 10th, and early 11th centuries, during the so-called Saga Age. They are the best-known specimens of Icelandic literature.
They are focused on history, especially genealogical and family history. They reflect the struggle and conflict that arose within the societies of the early generations of Icelandic settlers.
Eventually many of Icelandic sagas were recorded, mostly in the thirteenth and fourteenth centuries. The 'authors', or rather recorders of these sagas are unknown. One, "Egils saga", is believed by some scholars to have been written by Snorri Sturluson, a descendant of the saga's hero, but this remains uncertain. The standard modern edition of Icelandic sagas is known as Íslenzk fornrit.
Gaukur's Saga – A Lost Saga.
The Saga of "Gaukur á Stöng" is believed to have existed but is now considered lost. The saga set in the anthology of sagas known as Möðruvallabók between "Njáls saga" and "Egils saga Skalla-Grímssonar" tells of a man named Gaukur Trandilsson who lived in the 10th century. Gaukur is mentioned in chapter 26 of Njáls saga. Icelandic professor and poet Jón Helgason managed to decipher a line that read "Let Trandilsson's story be written here. I am told that [Mr.] Grim knows it." However, the story was never put to paper. The Grim mentioned in the manuscript is believed to have been Grímur Þorsteinsson, knight and governor ().
Gaukur is reported to have been an exceptionally brave and gentle man. He was the foster brother of Ásgrimur. However, it is said that he had a falling out with his foster brother, who ultimately killed him.
Gaukur must have been a well known figure in Icelandic folklore as he is mentioned in not only Njáls Saga but also the Íslendigadrápa, a poem about the Icelandic heroes. He is also mentioned on a tomb in the Orkney Islands, where a runic inscription translates to "These runes were carved by the man who was the most knowledgable of runes in the west of the sea, using the axe that belonged to Gaukur Trandilsson in the south of the land". The south of the land refers to Iceland.
Explanations for saga writing.
Icelanders produced a high volume of literature relative to the size of the population. Historians have proposed various theories for the high volume of saga writing:

</doc>
<doc id="29489" url="https://en.wikipedia.org/wiki?curid=29489" title="Staind">
Staind

Staind ( ) is an American rock band, formed in 1995 in Springfield, Massachusetts. For years, the band consisted of lead vocalist and rhythm guitarist Aaron Lewis, lead guitarist Mike Mushok, bassist and backing vocalist Johnny April, and drummer Jon Wysocki (who left in May 2011). To date, the band has recorded seven studio albums: "Tormented" (1996), "Dysfunction" (1999), "Break the Cycle" (2001), "14 Shades of Grey" (2003), "Chapter V" (2005), "The Illusion of Progress" (2008), and their self-titled album (2011). The band has had five chart-topping singles and sold over 15 million records worldwide.
History.
Early years and "Tormented" (1995–1998).
Staind formed in Springfield, Massachusetts. The band met through friends and started covering Korn, Rage Against the Machine, Pearl Jam, Tool, and Alice in Chains, among others, and played at local clubs (most commonly playing at Club Infinity) for a year and a half. Staind self-released their debut album, "Tormented", in November 1996, citing influences Pantera and Machine Head. Until recently, the album was difficult to obtain, as only four thousand copies were originally sold.
During this time, Staind acquired a concert slot through Aaron's cousin Justin Cantor with Limp Bizkit. (Justin had managed them and was a rising star in the entertainment industry)..Just prior to the performance, Limp Bizkit frontman Fred Durst was appalled by Staind's grotesque album cover and unsuccessfully attempted to remove them from the bill. His perception changed, however, after being impressed with their performance.
"Dysfunction" (1999–2000).
On April 13, 1999, Staind released its major label debut "Dysfunction" on Flip Records. The album, which was co-produced by Fred Durst and Terry Date (who also produced acts like Soundgarden, Deftones, and Pantera), received comparisons to metal giants Tool and Korn. In particular, Aaron Lewis was lauded for his vocals, which were likened to those of Pearl Jam's Eddie Vedder.
The album achieved slow success, with the album reaching the No. 1 spot on Billboard's Heatseeker Charts almost six months after its debut. In the same week, the album jumped to No. 74 on Billboard's Top 200 Album Charts. The nine-track LP (with one hidden track, "Excess Baggage") produced three singles, "Just Go", "Mudshovel", and "Home". "Mudshovel" and "Home" both received radio play, cracking the Top 20 of Billboard's Modern Rock and Mainstream Rock charts. In promotion of "Dysfunction", Staind went on several tours, including the Family Values Tour with acts like Limp Bizkit and The Crystal Method, as well as opening for Sevendust's headlining tour.
"Break the Cycle" (2001–2002).
Staind toured with Limp Bizkit for the Family Values Tour during the fall of 1999, where Aaron Lewis performed their first mainstream hit "Outside" with Fred Durst at the Mississippi Coast Coliseum. Staind released their third studio album "Break the Cycle" on May 22, 2001. Propelled by the success of their first single "It's Been Awhile", the album debuted at No. 1 on Billboard's Top 200 Album charts, selling 716,000 albums in its first week. The album's first week sales were the second highest of any album that year, making more sales than Destiny's Child's Survivor.
The album saw the band retaining their previous nu metal sound from their previous album. Despite the album being a mostly nu metal record, the album saw the band going further into their post-grunge sound which is evident in the smash hit song called "It's Been Awhile" and the song led critics to compare the band to several of other post-grunge bands at the time. "It's Been Awhile" (which hit the Billboard Top 10), "Fade" (which has been featured on a number of movie soundtracks and television shows), "Outside", "For You", and the acoustic ballad "Epiphany". It also included a track called "Waste", devoted to two teenage fans who committed suicide shortly before the album was released. "It's Been Awhile" spent a total of 16 and 14 weeks on top of the modern and mainstream rock charts, respectively, making it one of the highest joint numbers of all time. In 2001, "Break the Cycle" sold four million copies worldwide, making it one of the best selling albums that year. "Break the Cycle" would go on to sell seven million copies worldwide, making this Staind's best selling album.
"14 Shades of Grey" (2003–2004).
In early 2003, Staind embarked on a worldwide tour to promote the release of the follow-up to "Break the Cycle", "14 Shades of Grey", which sold two million albums and debuted at number 1 on the Billboard 200. The album saw a departure from their previous nu metal sound and the album mostly contain a lighter and more melodic post-grunge sound something that was used in Staind's last two previous albums but wasn't the most dominant sound in the band. The fourteen-track collection was the band's most mainstream yet, and showed Aaron Lewis writing songs about his daughter, as well as moving on with his life and forgetting his past, hence the title representing uncertainty for the future and forgiveness. The album provided two mainstream hits: the lead single "Price to Play" and "So Far Away" (which spent 14 weeks on top of the rock chart and was featured on an episode of "Smallville"); in addition, two other singles failed to crack the Hot 100—"How About You" and "Zoe Jane"— but "How About You" was a fairly popular song on modern rock radio. Their song "Price to Play" was the official theme song of WWE's Vengeance pay-per-view event in July 2003. In addition, their song "So Far Away" was featured on an episode of "WWE Raw" as part of a video tribute to hardcore wrestling legend Mick Foley. The band's appearance at Reading Festival during the 2003 tour had another impromptu acoustic set, this time due to equipment failure. The singles "So Far Away" and "Price to Play" came with two unreleased tracks, "Novocaine" and "Let It Out", which were released for the special edition of the group's "Chapter V", which came out in late 2005. In 2003, Staind unsuccessfully sued their logo designer Jon Stainbrook in New York Federal Court for attempting to re-use the logo he had sold to the band. They re-opened the case in mid-2005.
"Chapter V" (2005–2007).
After extensive promotions, including an appearance on Fuse TV's 7th Avenue Drop, Staind's fifth album, titled "Chapter V", was released on August 9, 2005, and became their third consecutive number one. The album opened to sales of 185,000 and has since been certified platinum in the U.S. The first single "Right Here" was the biggest success from the album, garnering much mainstream radio play and peaking at number 1 on the mainstream rock chart. "Falling" (the video of which does not feature the band members at all) was released as the second single, followed by "Everything Changes" and "King of All Excuses". Staind went on the road when the album came out, doing live shows and promoting it for a full year, including participating in the Fall Brawl tour with P.O.D., Taproot and Flyleaf, a solo tour across Europe and a mini-promotional tour in Australia for the first time. Other live shows included a cover of Pantera's "This Love", a tribute to Dimebag Darrell. Staind appeared on "The Howard Stern Show" on August 10, 2005, to promote "Chapter V". They performed acoustic renditions of the single "Right Here" and Beetlejuice's song "This is Beetle". Their rendition of "Beetle" is immensely popular with fans and listeners alike and became a staple of the show. Staind also performed a version of "Comfortably Numb" by Pink Floyd. Kevin Lofton, who does all the animation for the Howard Stern website, created a black-and-white animated video for the song. During a January 2006 episode of WWE RAW, a tribute video to then WWE Champion Edge featured the band's song "Right Here". In early November 2005, Staind released the limited edition 2-CD/DVD set of "Chapter V". The set included several rarities and fan favorites— music videos; a complete, 36-page booklet with exclusive artwork; an audio disc with an acoustic rendition of "This is Beetle"; the original, melodic rendition of "Reply"; the previously released B-side singles "Novocaine" and "Let It Out"; and live versions of "It's Been Awhile" and "Falling", among many others.
In 2006, Staind performed an acoustic show in the Hiro Ballroom, New York City on September 6 and the songs played were recorded for their "Greatest Hits" album. They played sixteen songs including three covers: Tool's "Sober", Pink Floyd's "Comfortably Numb" and Alice in Chains's "Nutshell".
The title was later renamed to "The Singles" and finally "". The album had most of Staind's singles (including "Everything Changes" which was recorded at the New York show), the three covers performed at the New York show and a remastered version of "Come Again", from Staind's first independent release "Tormented". It was released on November 14, 2006.
"The Illusion of Progress" (2008–2009).
On August 19, 2008, Staind released their sixth album, "The Illusion of Progress". A limited edition of the album was also made available to fans, which included three bonus tracks and a year membership in the Staind Fan Club, along with other items (the first 200 fans who pre-ordered the album through Atlantic Records received a signed copy of the album by the band). Prior to the album's release, the track "This Is It" was available for download on the iTunes Store, as well as for "Rock Band". The album debuted at No. 3 on US Billboard 200, No. 1 on the Top Modern Rock/Alternative Albums Chart, No. 1 on the Top Digital Albums Chart, and also No. 1 on the Top Internet Albums Chart, with first week sales of 91,800 units. The first single on the album, "Believe", topped Billboard's Top 10 Modern Rock Tracks on September 5, 2008. The band also supported Nickelback on their 2008 European tour. The second single was "All I Want", and came out on November 24. The video continues the story of first single and was available on Staind's MySpace on December 12. The single also became Staind's 13th top 20 hit on the rock charts. In Europe the second single was "The Way I Am", released on January 26, 2009. The final single released from the album, "This Is It", was sent to radio stations across the country on May 4, 2009. The single was also included on the successful "" released in late June 2009. Aaron Lewis stated in an interview that he began writing for his upcoming solo album, which was set to be released later that year. Staind embarked on a fall tour with the newly reunited Creed.
"Staind" and departure of Jon Wysocki (2010–2012).
In December 2010, Staind posted three webisodes from the studio, which featured the band members discussing the writing and recording process of their new album. They announced that as of April 20, the band had completed the recording of their untitled seventh album and would release it later that year.
On May 20, 2011, Staind announced that the band and long-time drummer Jon Wysocki (currently in the band Soil) had parted ways. Drummer Will Hunt filled in for select dates, but was replaced by Sal Giancarelli. Three days later, it was reported that Staind's new album was originally called "Seven", but was renamed "Staind". It was released on September 13, 2011. The first single "Not Again" was released to active radio stations on July 18.
A new track titled "The Bottom" appeared on the "" soundtrack. On June 30, Staind released a song called "Eyes Wide Open" from their new record. "Eyes Wide Open" would later be released on November 29 as the album's second single. A tracklist for the album was also released. On July 12, Staind released the first single "Not Again" through YouTube and was officially released/available on July 26.
In November 2011, the band officially announced through their YouTube page that Sal Giancarelli had become their new drummer. The band was on tour that spring with Godsmack and Halestorm. The tour was called the MASSCHAOS tour. Staind hit the road with Shinedown, Godsmack and more as part of the Uproar Festival summer tour across North America.
Hiatus and return to touring (2012–2014).
It was announced in July 2012 that the band was to be taking a hiatus. In an interview with Billboard Aaron Lewis stated that ""We're not breaking up. We're not gonna stop making music. We're just going to take a little hiatus that really hasn't ever been taken in our career. We put out seven records in 14 years. We've been pretty busy."". Lewis also had plans to release his first solo album The Road.
During such time, Mike Mushok auditioned, and was selected, to play guitar for former Metallica bassist Jason Newsted's new band Newsted. He featured on their debut album "Heavy Metal Music".
Staind played their first show in two years at the Welcome To Rockville Festival on April 27, 2014 alongside Korn, Rob Zombie, Five Finger Death Punch, Seether, Theory of a Deadman, and others. They also played the Carolina Rebellion and Rock on the Range festivals in May 2014.
Return to hiatus and other projects (2015–present).
As of 2015, the band returned to its indefinite hiatus as Aaron Lewis continues his solo tour and writing his next solo album due out in 2016. Mike Mushok has teamed up with former Three Days Grace singer Adam Gontier, former Finger Eleven drummer Rich Beddoe and Eye Empire bassist Corey Lowery to form supergroup Saint Asonia, which released their first single "Better Place" in May 2015 with their self-titled debut album released on July 31, 2015. 
Trademark infringement.
In November 2003 Staind attempted to sue Ohio musician Jon Stainbrook in New York Federal Court over his 1980 trademark of Joe "The Stain" Stainbrook. Staind was unsuccessful in the suit, prompting Stainbrook to pursue litigation against the band for including false statements in their application with the U.S. Patent and Trademark Office. If Stainbrook's suit was successful, Staind would have had to change the band name and forfeit all trademark claims.
In 2005, guitarist Mike Mushok claimed during a deposition in Toledo, Ohio, that Staind avoided any contact with Stainbrook because they were not made aware of promises a Geffen Records executive made to Stainbrook on their behalf during a 1999 licensing negotiation. After several legal confrontations, the parties settled their claims in 2006, partially re-negotiating the 1998 licensing agreement, which allows Stainbrook and Staind the right to use the trademark in certain areas.
Music and lyrics.
Topics of their lyrics cover issues of depression, relationships, death, substance abuse, finding oneself, betrayal and even Lewis' thoughts about becoming a father in the song "Zoe Jane" from "14 Shades of Grey", as well as reflecting on his upbringing in the song "The Corner" from "The Illusion of Progress". Also from "14 Shades of Grey", the track titled "Layne" was written about Alice in Chains frontman Layne Staley in response to his death in 2002. The song is also about Staley's legacy and the effect his music had on the members of Staind, especially Aaron Lewis. Staind has been categorized as nu metal, alternative metal, heavy metal, hard rock and post-grunge. Musically, the band is influenced by Alice in Chains, Mr. Bungle, Pantera, Korn, Machine Head, Pearl Jam, Stone Temple Pilots, Pink Floyd, Helmet, Tool, Nirvana, Led Zeppelin, Rage Against The Machine, Metallica, Queensrÿche, Slayer, Iron Maiden, and AC/DC.

</doc>
<doc id="29490" url="https://en.wikipedia.org/wiki?curid=29490" title="Saddam Hussein">
Saddam Hussein

Saddam Hussein Abd al-Majid al-Tikriti (Arabic: ""; 28 April 1937 – 30 December 2006) was the fifth President of Iraq, serving in this capacity from 16 July 1979 until 9 April 2003. A leading member of the revolutionary Arab Socialist Ba'ath Party, and later, the Baghdad-based Ba'ath Party and its regional organization Ba'ath Party – Iraq Region—which espoused Ba'athism, a mix of Arab nationalism and socialism—Saddam played a key role in the 1968 coup (later referred to as the 17 July Revolution) that brought the party to power in Iraq.
As vice president under the ailing General Ahmed Hassan al-Bakr, and at a time when many groups were considered capable of overthrowing the government, Saddam created security forces through which he tightly controlled conflict between the government and the armed forces. In the early 1970s, Saddam nationalized oil and other industries. The state-owned banks were put under his control, leaving the system eventually insolvent mostly due to the Iran–Iraq War, the Gulf War, and UN sanctions. Through the 1970s, Saddam cemented his authority over the apparatuses of government as oil money helped Iraq's economy to grow at a rapid pace. Positions of power in the country were mostly filled with Sunni Arabs, a minority that made up only a fifth of the population.
Saddam formally rose to power in 1979, although he had been the "de facto" head of Iraq for several years prior. He suppressed several movements, particularly Shi'a and Kurdish movements, seeking to overthrow the government or gain independence, and maintained power during the Iran–Iraq War and the Gulf War. Whereas some venerated Saddam for his opposition to the United States and for attacking Israel—he was widely condemned in the West for the brutality of his dictatorship. The total number of Iraqis killed by the security services of Saddam's government in various purges and genocides is unknown, but the lowest estimate is 250,000.
In 2003, a coalition led by the U.S. invaded Iraq to depose Saddam, in which U.S. President George W. Bush and British Prime Minister Tony Blair accused him of possessing weapons of mass destruction and having ties to al-Qaeda. Saddam's Ba'ath party was disbanded and elections were held. Following his capture on 13 December 2003, the trial of Saddam took place under the Iraqi Interim Government. On 5 November 2006, Saddam was convicted of charges related to the 1982 killing of 148 Iraqi Shi'ites, and was sentenced to death by hanging. His execution was carried out on 30 December 2006.
Youth.
Saddam Hussein Abd al-Majid al-Tikriti was born in the town of Al-Awja, 13 km (8 mi) from the Iraqi town of Tikrit, to a family of shepherds from the al-Begat tribal group, a sub-group of the Al-Bu Nasir (البو ناصر) tribe. His mother, Subha Tulfah al-Mussallat, named her newborn son "Saddam", which in Arabic means "One who confronts". He is always referred to by this personal name, which may be followed by the patronymic and other elements. He never knew his father, Hussein 'Abid al-Majid, who disappeared six months before Saddam was born. Shortly afterward, Saddam's 13-year-old brother died of cancer. The infant Saddam was sent to the family of his maternal uncle Khairallah Talfah until he was three.
His mother remarried, and Saddam gained three half-brothers through this marriage. His stepfather, Ibrahim al-Hassan, treated Saddam harshly after his return. At about age 10, Saddam fled the family and returned to live in Baghdad with his uncle Kharaillah Tulfah. Tulfah, the father of Saddam's future wife, was a devout Sunni Muslim and a veteran of the 1941 Anglo-Iraqi War between Iraqi nationalists and the United Kingdom, which remained a major colonial power in the region.
Later in his life relatives from his native Tikrit became some of his closest advisors and supporters. Under the guidance of his uncle he attended a nationalistic high school in Baghdad. After secondary school Saddam studied at an Iraqi law school for three years, dropping out in 1957 at the age of 20 to join the revolutionary pan-Arab Ba'ath Party, of which his uncle was a supporter. During this time, Saddam apparently supported himself as a secondary school teacher.
Revolutionary sentiment was characteristic of the era in Iraq and throughout the Middle East. In Iraq progressives and socialists assailed traditional political elites (colonial era bureaucrats and landowners, wealthy merchants and tribal chiefs, and monarchists). Moreover, the pan-Arab nationalism of Gamal Abdel Nasser in Egypt profoundly influenced young Ba'athists like Saddam. The rise of Nasser foreshadowed a wave of revolutions throughout the Middle East in the 1950s and 1960s, with the collapse of the monarchies of Iraq, Egypt, and Libya. Nasser inspired nationalists throughout the Middle East by fighting the British and the French during the Suez Crisis of 1956, modernizing Egypt, and uniting the Arab world politically.
In 1958, a year after Saddam had joined the Ba'ath party, army officers led by General Abd al-Karim Qasim overthrew Faisal II of Iraq in the 14 July Revolution.
Rise to power.
Of the 16 members of Qasim's cabinet, 12 were Ba'ath Party members; however, the party turned against Qasim due to his refusal to join Gamal Abdel Nasser's United Arab Republic. To strengthen his own position within the government, Qasim created an alliance with the Iraqi Communist Party, which was opposed to any notion of pan-Arabism. Later that year, the Ba'ath Party leadership was planning to assassinate Qasim. Saddam was a leading member of the operation. At the time, the Ba'ath Party was more of an ideological experiment than a strong anti-government fighting machine. The majority of its members were either educated professionals or students, and Saddam fit the bill. The choice of Saddam was, according to historian Con Coughlin, "hardly surprising". The idea of assassinating Qasim may have been Nasser's, and there is speculation that some of those who participated in the operation received training in Damascus, which was then part of the UAR.
The assassins planned to ambush Qasim at Al-Rashid Street on 7 October 1959: one man was to kill those sitting at the back of the car, the rest killing those in front. During the ambush it is claimed that Saddam began shooting prematurely, which disorganised the whole operation. Qasim's chauffeur was killed, and Qasim was hit in the arm and shoulder. The assassins believed they had killed him and quickly retreated to their headquarters, but Qasim survived. At the time of the attack the Ba'ath Party had less than 1,000 members.
Some of the plotters quickly managed to leave the country for Syria, the spiritual home of Ba'athist ideology. There Saddam was given full-membership in the party by Michel Aflaq. Some members of the operation were arrested and taken into custody by the Iraqi government. At the show trial, six of the defendants were given the death sentence; for unknown reasons the sentences were not carried out. Aflaq, the leader of the Ba'athist movement, organised the expulsion of leading Iraqi Ba'athist members, such as Fuad al-Rikabi, on the grounds that the party should not have initiated the attempt on Qasim's life. At the same time, Aflaq managed to secure seats in the Iraqi Ba'ath leadership for his supporters, one them being Saddam. Saddam fled to Egypt in 1959, and he continued to live there until 1963.
Army officers with ties to the Ba'ath Party overthrew Qasim in the Ramadan Revolution coup of 1963. Ba'athist leaders were appointed to the cabinet and Abdul Salam Arif became president. Arif dismissed and arrested the Ba'athist leaders later that year in the November 1963 Iraqi coup d'état.
Arif died in a plane crash in 1966, in what may have been an act of sabotage by Ba'athist elements in the Iraqi military. Abd ar-Rahman al-Bazzaz became acting president for three days, and a power struggle for the presidency occurred. In the first meeting of the Defense Council and cabinet to elect a president, Al-Bazzaz needed a two-thirds majority to win the presidency. Al-Bazzaz was unsuccessful, and Abdul Rahman Arif was elected president. He was viewed by army officers as weaker and easier to manipulate than his brother.
Saddam returned to Iraq, but was imprisoned in 1964. In 1966, Ahmed Hassan al-Bakr appointed him Deputy Secretary of the Regional Command. Saddam escaped from prison in 1967. Saddam, who would prove to be a skilled organiser, revitalised the party. He was elected to the Regional Command, as the story goes, with help from Michel Aflaq—the founder of Ba'athist thought.
In 1968, Saddam participated in a bloodless coup led by Ahmed Hassan al-Bakr that overthrew Abdul Rahman Arif. Saddam and Salah Omar al-Ali contacted Ba'athists in the military and helped lead them on the ground. Arif was given refuge in London and then Istanbul. Al-Bakr was named president and Saddam was named his deputy, and deputy chairman of the Ba'athist Revolutionary Command Council. According to biographers, Saddam never forgot the tensions within the first Ba'athist government, which formed the basis for his measures to promote Ba'ath party unity as well as his resolve to maintain power and programs to ensure social stability. Although Saddam was al-Bakr's deputy, he was a strong behind-the-scenes party politician. Al-Bakr was the older and more prestigious of the two, but by 1969 Saddam clearly had become the moving force behind the party.
Political program.
In the late 1960s and early 1970s, as vice chairman of the Revolutionary Command Council, formally al-Bakr's second-in-command, Saddam built a reputation as a progressive, effective politician. At this time, Saddam moved up the ranks in the new government by aiding attempts to strengthen and unify the Ba'ath party and taking a leading role in addressing the country's major domestic problems and expanding the party's following.
After the Ba'athists took power in 1968, Saddam focused on attaining stability in a nation riddled with profound tensions. Long before Saddam, Iraq had been split along social, ethnic, religious, and economic fault lines: Sunni versus Shi'ite, Arab versus Kurd, tribal chief versus urban merchant, nomad versus peasant. The desire for stable rule in a country rife with factionalism led Saddam to pursue both massive repression and the improvement of living standards.
Saddam actively fostered the modernization of the Iraqi economy along with the creation of a strong security apparatus to prevent coups within the power structure and insurrections apart from it. Ever concerned with broadening his base of support among the diverse elements of Iraqi society and mobilizing mass support, he closely followed the administration of state welfare and development programs.
At the center of this strategy was Iraq's oil. On 1 June 1972, Saddam oversaw the seizure of international oil interests, which, at the time, dominated the country's oil sector. A year later, world oil prices rose dramatically as a result of the 1973 energy crisis, and skyrocketing revenues enabled Saddam to expand his agenda.
Within just a few years, Iraq was providing social services that were unprecedented among Middle Eastern countries. Saddam established and controlled the "National Campaign for the Eradication of Illiteracy" and the campaign for "Compulsory Free Education in Iraq," and largely under his auspices, the government established universal free schooling up to the highest education levels; hundreds of thousands learned to read in the years following the initiation of the program. The government also supported families of soldiers, granted free hospitalization to everyone, and gave subsidies to farmers. Iraq created one of the most modernized public-health systems in the Middle East, earning Saddam an award from the United Nations Educational, Scientific and Cultural Organization (UNESCO).
With the help of increasing oil revenues, Saddam diversified the largely oil-based Iraqi economy. Saddam implemented a national infrastructure campaign that made great progress in building roads, promoting mining, and developing other industries. The campaign helped Iraq's energy industries. Electricity was brought to nearly every city in Iraq, and many outlying areas. Before the 1970s, most of Iraq's people lived in the countryside and roughly two-thirds were peasants. This number would decrease quickly during the 1970s as global oil prices helped revenues to rise from less than a half billion dollars to tens of billions of dollars and the country invested into industrial expansion.
The oil revenue benefitted Saddam politically. According to "The Economist", "Much as Adolf Hitler won early praise for galvanising German industry, ending mass unemployment and building autobahns, Saddam earned admiration abroad for his deeds. He had a good instinct for what the "Arab street" demanded, following the decline in Egyptian leadership brought about by the trauma of Israel's six-day victory in the 1967 war, the death of the pan-Arabist hero, Gamal Abdul Nasser, in 1970, and the "traitorous" drive by his successor, Anwar Sadat, to sue for peace with the Jewish state. Saddam's self-aggrandising propaganda, with himself posing as the defender of Arabism against Jewish or Persian intruders, was heavy-handed, but consistent as a drumbeat. It helped, of course, that his mukhabarat (secret police) put dozens of Arab news editors, writers and artists on the payroll."
In 1972, Saddam signed a 15-year Treaty of Friendship and Cooperation with the Soviet Union. According to historian Charles R. H. Tripp, the Ba'athist coup of 1968 upset "the US-sponsored security system established as part of the Cold War in the Middle East. It appeared that any enemy of the Baghdad regime was a potential ally of the United States." From 1973-5, the CIA colluded with Shah Mohammad Reza Pahlavi of Iran to finance and arm Kurdish rebels in the Second Kurdish–Iraqi War in an attempt to weaken al-Bakr. When Iran and Iraq signed the Algiers Agreement in 1975, the support ceased.
Saddam focused on fostering loyalty to the Ba'athists in the rural areas. After nationalizing foreign oil interests, Saddam supervised the modernization of the countryside, mechanizing agriculture on a large scale, and distributing land to peasant farmers. The Ba'athists established farm cooperatives and the government also doubled expenditures for agricultural development in 1974–1975. Saddam's welfare programs were part of a combination of "carrot and stick" tactics to enhance support for Saddam. The state-owned banks were put under his thumb. Lending was based on cronyism. Development went forward at such a fevered pitch that two million people from other Arab countries and even Yugoslavia worked in Iraq to meet the growing demand for labor.
Succession.
In 1976, Saddam rose to the position of general in the Iraqi armed forces, and rapidly became the strongman of the government. As the ailing, elderly al-Bakr became unable to execute his duties, Saddam took on an increasingly prominent role as the face of the government both internally and externally. He soon became the architect of Iraq's foreign policy and represented the nation in all diplomatic situations. He was the "de facto" leader of Iraq some years before he formally came to power in 1979. He slowly began to consolidate his power over Iraq's government and the Ba'ath party. Relationships with fellow party members were carefully cultivated, and Saddam soon accumulated a powerful circle of support within the party.
In 1979 al-Bakr started to make treaties with Syria, also under Ba'athist leadership, that would lead to unification between the two countries. Syrian President Hafez al-Assad would become deputy leader in a union, and this would drive Saddam to obscurity. Saddam acted to secure his grip on power. He forced the ailing al-Bakr to resign on 16 July 1979, and formally assumed the presidency.
Shortly afterwards, he convened an assembly of Ba'ath party leaders on 22 July 1979. During the assembly, which he ordered videotaped, Saddam claimed to have found a fifth column within the Ba'ath Party and directed Muhyi Abdel-Hussein to read out a confession and the names of 68 alleged co-conspirators. These members were labelled "disloyal" and were removed from the room one by one and taken into custody. After the list was read, Saddam congratulated those still seated in the room for their past and future loyalty. The 68 people arrested at the meeting were subsequently tried together and found guilty of treason. 22 were sentenced to execution. Other high-ranking members of the party formed the firing squad. By 1 August 1979, hundreds of high-ranking Ba'ath party members had been executed.
Paramilitary and police organizations.
Iraqi society fissures along lines of language, religion and ethnicity. The Ba'ath Party, secular by nature, adopted Pan-Arab ideologies which in turn were problematic for significant parts of the population. Following the Iranian Revolution of 1979, Iraq faced the prospect of régime change from two Shi'ite factions (Dawa and SCIRI) which aspired to model Iraq on its neighbour Iran as a Shia theocracy. A separate threat to Iraq came from parts of the ethnic Kurdish population of northern Iraq which opposed being part of an Iraqi state and favoured independence (an ongoing ideology which had preceded Ba'ath Party rule). To alleviate the threat of revolution, Saddam afforded certain benefits to the potentially hostile population. Membership in the Ba'ath Party remained open to all Iraqi citizens regardless of background. However, repressive measures were taken against its opponents.
The major instruments for accomplishing this control were the paramilitary and police organizations. Beginning in 1974, Taha Yassin Ramadan (himself a Kurdish Ba'athist), a close associate of Saddam, commanded the People's Army, which had responsibility for internal security. As the Ba'ath Party's paramilitary, the People's Army acted as a counterweight against any coup attempts by the regular armed forces. In addition to the People's Army, the Department of General Intelligence was the most notorious arm of the state-security system, feared for its use of torture and assassination. Barzan Ibrahim al-Tikriti, Saddam's younger half-brother, commanded Mukhabarat. Foreign observers believed that from 1982 this department operated both at home and abroad in its mission to seek out and eliminate Saddam's perceived opponents.
Saddam was notable for using terror against his own people. "The Economist" described Saddam as "one of the last of the 20th century's great dictators, but not the least in terms of egotism, or cruelty, or morbid will to power". Saddam's regime brought about the deaths of at least 250,000 Iraqis
and committed war crimes in Iran, Kuwait, and Saudi Arabia. Human Rights Watch and Amnesty International issued regular reports of widespread imprisonment and torture.
Political and cultural image.
As a sign of his consolidation of power, Saddam's personality cult pervaded Iraqi society. He had thousands of portraits, posters, statues and murals erected in his honor all over Iraq. His face could be seen on the sides of office buildings, schools, airports, and shops, as well as on Iraqi currency. Saddam's personality cult reflected his efforts to appeal to the various elements in Iraqi society. This was seen in his variety of apparel: he appeared in the costumes of the Bedouin, the traditional clothes of the Iraqi peasant (which he essentially wore during his childhood), and even Kurdish clothing, but also appeared in Western suits fitted by his favorite tailor, projecting the image of an urbane and modern leader. Sometimes he would also be portrayed as a devout Muslim, wearing full headdress and robe, praying toward Mecca.
He also conducted two show elections, in 1995 and 2002. In the 1995 referendum, conducted on 15 October, he reportedly received 99.96% of the votes in a 99.47% turnout, getting only 3052 negative votes among an electorate of 8.4 million. In the October 15, 2002 referendum he officially achieved 100% of approval votes and 100% turnout, as the electoral commission reported the next day that every one of the 11,445,638 eligible voters cast a "Yes" vote for the president.
He erected statues around the country, which Iraqis toppled after his fall.
Foreign affairs.
Iraq's relations with the Arab world have been extremely varied. Relations between Iraq and Egypt violently ruptured in 1977, when the two nations broke relations with each other following Iraq's criticism of Egyptian President Anwar Sadat's peace initiatives with Israel. In 1978, Baghdad hosted an Arab League summit that condemned and ostracized Egypt for accepting the Camp David Accords. However, Egypt's strong material and diplomatic support for Iraq in the war with Iran led to warmer relations and numerous contacts between senior officials, despite the continued absence of ambassadorial-level representation. Since 1983, Iraq has repeatedly called for restoration of Egypt's "natural role" among Arab countries.
Saddam developed a reputation for liking expensive goods, such as his diamond-coated Rolex wristwatch, and sent copies of them to his friends around the world. To his ally Kenneth Kaunda Saddam once sent a Boeing 747 full of presents – rugs, televisions, ornaments. Kaunda sent back his own personal magician.
Saddam enjoyed a close relationship with Russian intelligence agent Yevgeny Primakov that dated back to the 1960s; Primakov may have helped Saddam to stay in power in 1991.
Saddam's only visit to a Western country took place in September 1975 when he met with his friend, Prime Minister Jacques Chirac in Paris, France.
Several Iraqi leaders, Lebanese arms merchant Sarkis Soghanalian and others have told that Saddam financed Chirac's party. In 1991 Saddam threatened to expose those who had taken largesse from him: "From Mr. Chirac to Mr. Chevènement, politicians and economic leaders were in open competition to spend time with us and flatter us. We have now grasped the reality of the situation. If the trickery continues, we will be forced to unmask them, all of them, before the French public." France armed Saddam and it was Iraq's largest trade partner throughout Saddam's rule. Seized documents show how French officials and businessmen close to Chirac, including Charles Pasqua, his former interior minister, personally benefitted from the deals with Saddam.
Because Saddam Hussein rarely left Iraq, Tariq Aziz, one of Saddam's aides, traveled abroad extensively and represented Iraq at many diplomatic meetings. In foreign affairs, Saddam sought to have Iraq play a leading role in the Middle East. Iraq signed an aid pact with the Soviet Union in 1972, and arms were sent along with several thousand advisers. However, the 1978 crackdown on Iraqi Communists and a shift of trade toward the West strained Iraqi relations with the Soviet Union; Iraq then took on a more Western orientation until the Gulf War in 1991.
After the oil crisis of 1973, France had changed to a more pro-Arab policy and was accordingly rewarded by Saddam with closer ties. He made a state visit to France in 1975, cementing close ties with some French business and ruling political circles. In 1975 Saddam negotiated an accord with Iran that contained Iraqi concessions on border disputes. In return, Iran agreed to stop supporting opposition Kurds in Iraq. Saddam led Arab opposition to the Camp David Accords between Egypt and Israel (1979).
Saddam initiated Iraq's nuclear enrichment project in the 1980s, with French assistance. The first Iraqi nuclear reactor was named by the French "Osirak". Osirak was destroyed on 7 June 1981 by an Israeli air strike (Operation Opera).
Nearly from its founding as a modern state in 1920, Iraq has had to deal with Kurdish separatists in the northern part of the country. Saddam did negotiate an agreement in 1970 with separatist Kurdish leaders, giving them autonomy, but the agreement broke down. The result was brutal fighting between the government and Kurdish groups and even Iraqi bombing of Kurdish villages in Iran, which caused Iraqi relations with Iran to deteriorate. However, after Saddam had negotiated the 1975 treaty with Iran, the Shah withdrew support for the Kurds, who suffered a total defeat.
Iran–Iraq War.
In early 1979, Iran's Shah Mohammad Reza Pahlavi was overthrown by the Islamic Revolution, thus giving way to an Islamic republic led by the Ayatollah Ruhollah Khomeini. The influence of revolutionary Shi'ite Islam grew apace in the region, particularly in countries with large Shi'ite populations, especially Iraq. Saddam feared that radical Islamic ideas—hostile to his secular rule—were rapidly spreading inside his country among the majority Shi'ite population.
There had also been bitter enmity between Saddam and Khomeini since the 1970s. Khomeini, having been exiled from Iran in 1964, took up residence in Iraq, at the Shi'ite holy city of An Najaf. There he involved himself with Iraqi Shi'ites and developed a strong, worldwide religious and political following against the Iranian Government, whom Saddam tolerated. However, when Khomeini began to urge the Shi'ites there to overthrow Saddam and under pressure from the Shah, who had agreed to a rapprochement between Iraq and Iran in 1975, Saddam agreed to expel Khomeini in 1978 to France. However this turned out to be an imminent failure and a political catalyst, for Khomeini had access to more media connections and also collaborated with a much larger Iranian community under his support whom he used to his advantage.
After Khomeini gained power, skirmishes between Iraq and revolutionary Iran occurred for ten months over the sovereignty of the disputed Shatt al-Arab waterway, which divides the two countries. During this period, Saddam Hussein publicly maintained that it was in Iraq's interest not to engage with Iran, and that it was in the interests of both nations to maintain peaceful relations. However, in a private meeting with Salah Omar al-Ali, Iraq's permanent ambassador to the United Nations, he revealed that he intended to invade and occupy a large part of Iran within months. Later (probably to appeal for support from the United States and most Western nations), he would make toppling the Islamic government one of his intentions as well.
Iraq invaded Iran, first attacking Mehrabad Airport of Tehran and then entering the oil-rich Iranian land of Khuzestan, which also has a sizable Arab minority, on 22 September 1980 and declared it a new province of Iraq. With the support of the Arab states, the United States, and Europe, and heavily financed by the Arab states of the Persian Gulf, Saddam Hussein had become "the defender of the Arab world" against a revolutionary Iran. The only exception was the Soviet Union, who initially refused to supply Iraq on the basis of neutrality in the conflict, although in his memoirs, Mikhail Gorbachev claimed that Leonid Brezhnev refused to aid Saddam over infuriation of Saddam's treatment of Iraqi communists. Consequently, many viewed Iraq as "an agent of the civilized world". The blatant disregard of international law and violations of international borders were ignored. Instead Iraq received economic and military support from its allies, who conveniently overlooked Saddam's use of chemical warfare against the Kurds and the Iranians and Iraq's efforts to develop nuclear weapons.
In the first days of the war, there was heavy ground fighting around strategic ports as Iraq launched an attack on Khuzestan. After making some initial gains, Iraq's troops began to suffer losses from human wave attacks by Iran. By 1982, Iraq was on the defensive and looking for ways to end the war.
At this point, Saddam asked his ministers for candid advice. Health Minister Dr. Riyadh Ibrahim suggested that Saddam temporarily step down to promote peace negotiations. Initially, Saddam Hussein appeared to take in this opinion as part of his cabinet democracy. A few weeks later, Dr. Ibrahim was sacked when held responsible for a fatal incident in an Iraqi hospital where a patient died from intravenous administration of the wrong concentration of potassium supplement.
Dr. Ibrahim was arrested a few days after he started his new life as a sacked Minister. He was known to have publicly declared before that arrest that he was "glad that he got away alive." Pieces of Ibrahim's dismembered body were delivered to his wife the next day.
Iraq quickly found itself bogged down in one of the longest and most destructive wars of attrition of the 20th century. During the war, Iraq used chemical weapons against Iranian forces fighting on the southern front and Kurdish separatists who were attempting to open up a northern front in Iraq with the help of Iran. These chemical weapons were developed by Iraq from materials and technology supplied primarily by West German companies as well as as using dual-use technology imported following the Reagan administration's lifting of export restrictions. The United States also supplied Iraq with "satellite photos showing Iranian deployments". In a US bid to open full diplomatic relations with Iraq, the country was removed from the US list of State Sponsors of Terrorism. Ostensibly, this was because of improvement in the regime's record, although former United States Assistant Secretary of Defense Noel Koch later stated, "No one had any doubts about Iraqis' continued involvement in terrorism... The real reason was to help them succeed in the war against Iran." The Soviet Union, France, and China together accounted for over 90% of the value of Iraq's arms imports between 1980 and 1988.
Saddam reached out to other Arab governments for cash and political support during the war, particularly after Iraq's oil industry severely suffered at the hands of the Iranian navy in the Persian Gulf. Iraq successfully gained some military and financial aid, as well as diplomatic and moral support, from the Soviet Union, China, France, and the United States, which together feared the prospects of the expansion of revolutionary Iran's influence in the region. The Iranians, demanding that the international community should force Iraq to pay war reparations to Iran, refused any suggestions for a cease-fire. Despite several calls for a ceasefire by the United Nations Security Council, hostilities continued until 20 August 1988.
On 16 March 1988, the Kurdish town of Halabja was attacked with a mix of mustard gas and nerve agents, killing 5,000 civilians, and maiming, disfiguring, or seriously debilitating 10,000 more. ("see Halabja poison gas attack") The attack occurred in conjunction with the 1988 al-Anfal Campaign designed to reassert central control of the mostly Kurdish population of areas of northern Iraq and defeat the Kurdish peshmerga rebel forces. The United States now maintains that Saddam ordered the attack to terrorize the Kurdish population in northern Iraq, but Saddam's regime claimed at the time that Iran was responsible for the attack which some including the U.S. supported until several years later.
The bloody eight-year war ended in a stalemate. There were hundreds of thousands of casualties with estimates of up to one million dead. Neither side had achieved what they had originally desired and at the borders were left nearly unchanged. The southern, oil rich and prosperous Khuzestan and Basra area (the main focus of the war, and the primary source of their economies) were almost completely destroyed and were left at the pre 1979 border, while Iran managed to make some small gains on its borders in the Northern Kurdish area. Both economies, previously healthy and expanding, were left in ruins.
Saddam borrowed tens of billions of dollars from other Arab states and a few billions from elsewhere during the 1980s to fight Iran, mainly to prevent the expansion of Shiite radicalism. However, this had proven to completely backfire both on Iraq and on the part of the Arab states, for Khomeini was widely perceived as a hero for managing to defend Iran and maintain the war with little foreign support against the heavily backed Iraq and only managed to boost Islamic radicalism not only within the Arab states, but within Iraq itself, creating new tensions between the Sunni Ba'ath Party and the majority Shiite population. Faced with rebuilding Iraq's infrastructure and internal resistance, Saddam desperately re-sought cash, this time for postwar reconstruction.
Al-Anfal Campaign.
The Al-Anfal Campaign was a campaign against the Kurdish people (and many others) in Kurdish regions of Iraq led by the government of Saddam Hussein and headed by Ali Hassan al-Majid. The campaign takes its name from Surat al-Anfal in the Qur'an, which was used as a code name by the former Iraqi Ba'athist administration for a series of attacks against the "peshmerga" rebels and the mostly Kurdish civilian population of rural Northern Iraq, conducted between 1986 and 1989 culminating in 1988. This campaign also targeted Shabaks and Yazidis, Assyrians, Turkoman people and Mandeans and many villages belonging to these ethnic groups were also destroyed. Human Rights Watch estimates that between 50,000 and 100,000 people were killed. Some Kurdish sources put the number higher, estimating that 182,000 Kurds were killed.
Tensions with Kuwait.
The end of the war with Iran served to deepen latent tensions between Iraq and its wealthy neighbor Kuwait. Saddam urged the Kuwaitis to forgive the Iraqi debt accumulated in the war, some $30 billion, but they refused.
Saddam pushed oil-exporting countries to raise oil prices by cutting back production; Kuwait refused, however. In addition to refusing the request, Kuwait spearheaded the opposition in OPEC to the cuts that Saddam had requested. Kuwait was pumping large amounts of oil, and thus keeping prices low, when Iraq needed to sell high-priced oil from its wells to pay off a huge debt.
Saddam had always argued that Kuwait was historically an integral part of Iraq, and that Kuwait had only come into being through the maneuverings of British imperialism; this echoed a belief that Iraqi nationalists had voiced for the past 50 years. This belief was one of the few articles of faith uniting the political scene in a nation rife with sharp social, ethnic, religious, and ideological divides.
The extent of Kuwaiti oil reserves also intensified tensions in the region. The oil reserves of Kuwait (with a population of 2 million next to Iraq's 25) were roughly equal to those of Iraq. Taken together, Iraq and Kuwait sat on top of some 20 percent of the world's known oil reserves; as an article of comparison, Saudi Arabia holds 25 percent.
Saddam complained to the U.S. State Department that Kuwait had slant drilled oil out of wells that Iraq considered to be within its disputed border with Kuwait. Saddam still had an experienced and well-equipped army, which he used to influence regional affairs. He later ordered troops to the Iraq–Kuwait border.
As Iraq-Kuwait relations rapidly deteriorated, Saddam was receiving conflicting information about how the U.S. would respond to the prospects of an invasion. For one, Washington had been taking measures to cultivate a constructive relationship with Iraq for roughly a decade. The Reagan administration gave Iraq roughly $4 billion in agricultural credits to bolster it against Iran. Saddam's Iraq became "the third-largest recipient of U.S. assistance".
Reacting to western criticism in April 1990 Saddam threatened to destroy half of Israel with chemical weapons if it moved against Iraq. In May 1990 he criticized U.S. support for Israel warning that "the United States cannot maintain such a policy while professing friendship towards the Arabs." In July 1990 he threatened force against Kuwait and the UAE saying "The policies of some Arab rulers are American... They are inspired by America to undermine Arab interests and security." The U.S. sent aerial planes and combat ships to the Persian Gulf in response to these threats.
U.S. ambassador to Iraq April Glaspie met with Saddam in an emergency meeting on 25 July 1990, where the Iraqi leader attacked American policy with regards to Kuwait and the United Arab Emirates:
Glaspie replied:
Saddam stated that he would attempt last-ditch negotiations with the Kuwaitis but Iraq "would not accept death".
U.S. officials attempted to maintain a conciliatory line with Iraq, indicating that while George H. W. Bush and James Baker did not want force used, they would not take any position on the Iraq–Kuwait boundary dispute and did not want to become involved.
Later, Iraq and Kuwait met for a final negotiation session, which failed. Saddam then sent his troops into Kuwait. As tensions between Washington and Saddam began to escalate, the Soviet Union, under Mikhail Gorbachev, strengthened its military relationship with the Iraqi leader, providing him military advisers, arms and aid.
Gulf War.
On 2 August 1990, Saddam invaded Kuwait, initially claiming assistance to "Kuwaiti revolutionaries," thus sparking an international crisis. On 4 August an Iraqi-backed "Provisional Government of Free Kuwait" was proclaimed, but a total lack of legitimacy and support for it led to an 8 August announcement of a "merger" of the two countries. On 28 August Kuwait formally became the 19th Governorate of Iraq. Just two years after the 1988 Iraq and Iran truce, "Saddam Hussein did what his Gulf patrons had earlier paid him to prevent." Having removed the threat of Iranian fundamentalism he "overran Kuwait and confronted his Gulf neighbors in the name of Arab nationalism and Islam."
When later asked why he invaded Kuwait, Saddam first claimed that it was because Kuwait was rightfully Iraq's 19th province and then said "When I get something into my head I act. That's just the way I am." After Saddam's seizure of Kuwait in August 1990, a UN coalition led by the United States drove Iraq's troops from Kuwait in February 1991. The ability for Saddam Hussein to pursue such military aggression was from a "military machine paid for in large part by the tens of billions of dollars Kuwait and the Gulf states had poured into Iraq and the weapons and technology provided by the Soviet Union, Germany, and France."
Shortly before he invaded Kuwait, he shipped 100 new Mercedes 200 Series cars to top editors in Egypt and Jordan. Two days before the first attacks, Saddam reportedly offered Egypt's Hosni Mubarak 50 million dollars in cash, "ostensibly for grain".
U.S. President George H. W. Bush responded cautiously for the first several days. On one hand, Kuwait, prior to this point, had been a virulent enemy of Israel and was the Persian Gulf monarchy that had had the most friendly relations with the Soviets. On the other hand, Washington foreign policymakers, along with Middle East experts, military critics, and firms heavily invested in the region, were extremely concerned with stability in this region. The invasion immediately triggered fears that the world's price of oil, and therefore control of the world economy, was at stake. Britain profited heavily from billions of dollars of Kuwaiti investments and bank deposits. Bush was perhaps swayed while meeting with British prime minister Margaret Thatcher, who happened to be in the U.S. at the time.
Cooperation between the United States and the Soviet Union made possible the passage of resolutions in the United Nations Security Council giving Iraq a deadline to leave Kuwait and approving the use of force if Saddam did not comply with the timetable. U.S. officials feared Iraqi retaliation against oil-rich Saudi Arabia, since the 1940s a close ally of Washington, for the Saudis' opposition to the invasion of Kuwait. Accordingly, the U.S. and a group of allies, including countries as diverse as Egypt, Syria and Czechoslovakia, deployed a massive amount of troops along the Saudi border with Kuwait and Iraq in order to encircle the Iraqi army, the largest in the Middle East.
Saddam's officers looted Kuwait, stripping even the marble from its palaces to move it to Saddam's own palace.
During the period of negotiations and threats following the invasion, Saddam focused renewed attention on the Palestinian problem by promising to withdraw his forces from Kuwait if Israel would relinquish the occupied territories in the West Bank, the Golan Heights, and the Gaza Strip. Saddam's proposal further split the Arab world, pitting U.S.- and Western-supported Arab states against the Palestinians. The allies ultimately rejected any linkage between the Kuwait crisis and Palestinian issues.
Saddam ignored the Security Council deadline. Backed by the Security Council, a U.S.-led coalition launched round-the-clock missile and aerial attacks on Iraq, beginning 16 January 1991. Israel, though subjected to attack by Iraqi missiles, refrained from retaliating in order not to provoke Arab states into leaving the coalition. A ground force consisting largely of U.S. and British armoured and infantry divisions ejected Saddam's army from Kuwait in February 1991 and occupied the southern portion of Iraq as far as the Euphrates.
On 6 March 1991, Bush announced "What is at stake is more than one small country, it is a big idea—a new world order, where diverse nations are drawn together in common cause to achieve the universal aspirations of mankind: peace and security, freedom, and the rule of law."
In the end, the out-numbered and under-equipped Iraqi army proved unable to compete on the battlefield with the highly mobile coalition land forces and their overpowering air support. Some 175,000 Iraqis were taken prisoner and casualties were estimated at over 85,000. As part of the cease-fire agreement, Iraq agreed to scrap all poison gas and germ weapons and allow UN observers to inspect the sites. UN trade sanctions would remain in effect until Iraq complied with all terms. Saddam publicly claimed victory at the end of the war.
Postwar period.
Iraq's ethnic and religious divisions, together with the brutality of the conflict that this had engendered, laid the groundwork for postwar rebellions. In the aftermath of the fighting, social and ethnic unrest among Shi'ite Muslims, Kurds, and dissident military units threatened the stability of Saddam's government. Uprisings erupted in the Kurdish north and Shi'a southern and central parts of Iraq, but were ruthlessly repressed.
The United States, which had urged Iraqis to rise up against Saddam, did nothing to assist the rebellions. The Iranians, despite the widespread Shi'ite rebellions, had no interest in provoking another war, while Turkey opposed any prospect of Kurdish independence, and the Saudis and other conservative Arab states feared an Iran-style Shi'ite revolution. Saddam, having survived the immediate crisis in the wake of defeat, was left firmly in control of Iraq, although the country never recovered either economically or militarily from the Gulf War.
Saddam routinely cited his survival as "proof" that Iraq had in fact won the war against the U.S. This message earned Saddam a great deal of popularity in many sectors of the Arab world. John Esposito, however, claims that "Arabs and Muslims were pulled in two directions. That they rallied not so much to Saddam Hussein as to the bipolar nature of the confrontation (the West versus the Arab Muslim world) and the issues that Saddam proclaimed: Arab unity, self-sufficiency, and social justice." As a result, Saddam Hussein appealed to many people for the same reasons that attracted more and more followers to Islamic revivalism and also for the same reasons that fueled anti-Western feelings.
As one U.S. Muslim observer noted: "People forgot about Saddam's record and concentrated on America ... Saddam Hussein might be wrong, but it is not America who should correct him." A shift was, therefore, clearly visible among many Islamic movements in the post war period "from an initial Islamic ideological rejection of Saddam Hussein, the secular persecutor of Islamic movements, and his invasion of Kuwait to a more populist Arab nationalist, anti-imperialist support for Saddam (or more precisely those issues he represented or championed) and the condemnation of foreign intervention and occupation."
Saddam, therefore, increasingly portrayed himself as a devout Muslim, in an effort to co-opt the conservative religious segments of society. Some elements of Sharia law were re-introduced, and the ritual phrase "Allahu Akbar" ("God is great"), in Saddam's handwriting, was added to the national flag. Saddam also commissioned the production of a "Blood Qur'an", written using 27 litres of his own blood, to thank God for saving him from various dangers and conspiracies.
Relations between the United States and Iraq remained tense following the Gulf War. The U.S. launched a missile attack aimed at Iraq's intelligence headquarters in Baghdad 26 June 1993, citing evidence of repeated Iraqi violations of the "no fly zones" imposed after the Gulf War and for incursions into Kuwait.
Former CIA case officer Robert Baer reports that he "tried to assassinate" Saddam in 1995 amid "a decade-long effort to encourage a military coup in Iraq."
The United Nations sanctions placed upon Iraq when it invaded Kuwait were not lifted, blocking Iraqi oil exports. During the late 1990s, the UN considered relaxing the sanctions imposed because of the hardships suffered by ordinary Iraqis. Studies dispute the number of people who died in south and central Iraq during the years of the sanctions. On 9 December 1996, Saddam's government accepted the Oil-for-Food Programme that the UN had first offered in 1992.
U.S. officials continued to accuse Saddam of violating the terms of the Gulf War's cease fire, by developing weapons of mass destruction and other banned weaponry, and violating the UN-imposed sanctions. Also during the 1990s, President Bill Clinton maintained sanctions and ordered air strikes in the "Iraqi no-fly zones" (Operation Desert Fox), in the hope that Saddam would be overthrown by political enemies inside Iraq. Western charges of Iraqi resistance to UN access to suspected weapons were the pretext for crises between 1997 and 1998, culminating in intensive U.S. and British missile strikes on Iraq, 16–19 December 1998. After two years of intermittent activity, U.S. and British warplanes struck harder at sites near Baghdad in February 2001.
Saddam's support base of Tikriti tribesmen, family members, and other supporters was divided after the war. Domestic repression inside Iraq increased, and Saddam's sons, Uday and Qusay Hussein, became increasingly powerful.
Iraqi co-operation with UN weapons inspection teams was intermittent throughout the 1990s.
Saddam continued involvement in politics abroad. Video tapes retrieved after show his intelligence chiefs meeting with Arab journalists, including a meeting with the former managing director of Al-Jazeera, Mohammed Jassem al-Ali, in 2000. In the video Saddam's son Uday advised al-Ali about hires in Al-Jazeera: "During your last visit here along with your colleagues we talked about a number of issues, and it does appear that you indeed were listening to what I was saying since changes took place and new faces came on board such as that lad, Mansour." He was later sacked by Al-Jazeera.
In 2002, Austrian prosecutors investigated Saddam government's transactions with Fritz Edlinger that possibly violated Austrian money laundering and embargo regulations. Fritz Edlinger, president of the "General Secretary of the Society for Austro-Arab relations" (GÖAB) and a former member of Socialist International's Middle East Committee, was an outspoken supporter of Saddam Hussein. In 2005, an Austrian journalist revealed that Fritz Edlinger's GÖAB had received $100,000 from an Iraqi front company as well as donations from Austrian companies soliciting business in Iraq.
In 2002, a resolution sponsored by the European Union was adopted by the Commission for Human Rights, which stated that there had been no improvement in the human rights crisis in Iraq. The statement condemned President Saddam Hussein's government for its "systematic, widespread and extremely grave violations of human rights and international humanitarian law". The resolution demanded that Iraq immediately put an end to its "summary and arbitrary executions ... the use of rape as a political tool and all enforced and involuntary disappearances".
Oil vouchers.
In the United Nations Oil-for-Food Programme, Saddam was supposed to trade oil for food. In practice, the program benefitted political parties, politicians, journalists, companies, and individuals around the world.
The Russian state was the largest beneficiary.
Invasion of Iraq in 2003.
The international community, especially the U.S., continued to view Saddam as a bellicose tyrant who was a threat to the stability of the region. After the September 11 attacks, Vladimir Putin began to tell the United States that Iraq was preparing terrorist attacks against the United States. In his January 2002 state of the union address to Congress, President George W. Bush spoke of an "axis of evil" consisting of Iran, North Korea, and Iraq. Moreover, Bush announced that he would possibly take action to topple the Iraqi government, because of the threat of its weapons of mass destruction. Bush stated that "The Iraqi regime has plotted to develop anthrax, and nerve gas, and nuclear weapons for over a decade ... Iraq continues to flaunt its hostility toward America and to support terror."
After the passing of United Nations Security Council Resolution 1441, which demanded that Iraq give "immediate, unconditional and active cooperation" with UN and IAEA inspections, Saddam allowed U.N. weapons inspectors led by Hans Blix to return to Iraq. During the renewed inspections beginning in November 2002, Blix found no stockpiles of WMD and noted the "proactive" but not always "immediate" Iraqi cooperation as called for by UN Security Council Resolution 1441.
With war still looming on 24 February 2003, Saddam Hussein took part in an interview with CBS News reporter Dan Rather. Talking for more than three hours, he denied possessing any weapons of mass destruction, or any other weapons prohibited by UN guidelines. He also expressed a wish to have a live televised debate with George W. Bush, which was declined. It was his first interview with a U.S. reporter in over a decade. CBS aired the taped interview later that week. Saddam Hussein later told an FBI interviewer that he once left open the possibility that Iraq possessed weapons of mass destruction in order to appear strong against Iran.
The Iraqi government and military collapsed within three weeks of the beginning of the U.S.-led 2003 invasion of Iraq on 20 March. By the beginning of April, U.S.-led forces occupied much of Iraq. The resistance of the much-weakened Iraqi Army either crumbled or shifted to guerrilla tactics, and it appeared that Saddam had lost control of Iraq. He was last seen in a video which purported to show him in the Baghdad suburbs surrounded by supporters. When Baghdad fell to U.S.-led forces on 9 April, marked symbolically by the toppling of his statue by iconoclasts, Saddam was nowhere to be found.
Incarceration and trial.
Capture and incarceration.
In April 2003, Saddam's whereabouts remained in question during the weeks following the fall of Baghdad and the conclusion of the major fighting of the war. Various sightings of Saddam were reported in the weeks following the war, but none was authenticated. At various times Saddam released audio tapes promoting popular resistance to his ousting.
Saddam was placed at the top of the "U.S. list of most-wanted Iraqis". In July 2003, his sons Uday and Qusay and 14-year-old grandson Mustapha were killed in a three-hour gunfight with U.S. forces.
On 13 December 2003, Saddam Hussein was captured by American forces at a farmhouse in ad-Dawr near Tikrit in a hole in Operation Red Dawn. Following his capture on 13 December Saddam was transported to a U.S. base near Tikrit, and later taken to the American base near Baghdad. On 14 December 2003, U.S. administrator in Iraq L. Paul Bremer confirmed that Saddam Hussein had indeed been captured at a farmhouse in ad-Dawr near Tikrit. Bremer presented video footage of Saddam in custody.
Saddam was shown with a full beard and hair longer than his familiar appearance. He was described by U.S. officials as being in good health. Bremer reported plans to put Saddam on trial, but claimed that the details of such a trial had not yet been determined. Iraqis and Americans who spoke with Saddam after his capture generally reported that he remained self-assured, describing himself as a "firm, but just leader."
British tabloid newspaper "The Sun" posted a picture of Saddam wearing white briefs on the front cover of a newspaper. Other photographs inside the paper show Saddam washing his trousers, shuffling, and sleeping. The United States government stated that it considers the release of the pictures a violation of the Geneva Convention, and that it would investigate the photographs. During this period Saddam was interrogated by FBI agent George Piro.
The guards at the Baghdad detention facility called their prisoner "Vic," and let him plant a small garden near his cell. The nickname and the garden are among the details about the former Iraqi leader that emerged during a 27 March 2008 tour of prison of the Baghdad cell where Saddam slept, bathed, and kept a journal in the final days before his execution.
Trial.
On 30 June 2004, Saddam Hussein, held in custody by U.S. forces at the U.S. base "Camp Cropper", along with 11 other senior Ba'athist leaders, were handed over legally (though not physically) to the interim Iraqi government to stand trial for crimes against humanity and other offences.
A few weeks later, he was charged by the Iraqi Special Tribunal with crimes committed against residents of Dujail in 1982, following a failed assassination attempt against him. Specific charges included the murder of 148 people, torture of women and children and the illegal arrest of 399 others.393 members of the pro Iranian Dawa Party (a banned organisation) were arrested as suspects of which 148, including ten children, confessed to taking part in the plot. It is believed more than 40 suspects died during interrogation or while in detention. Those arrested who were found not guilty were either exiled if relatives of the convicted or released and returned to Dujail. Only 96 of the 148 condemned were actually executed, two of the condemned were accidentally released while a third was mistakenly transferred to another prison and survived. The 96 executed included four men mistakenly executed after having been found not guilty and ordered released. The ten children were originally believed to have been among the 96 executed, but they had in fact been imprisoned near the city of Samawah.</ref>
Among the many challenges of the trial were:
On 5 November 2006, Saddam Hussein was found guilty of crimes against humanity and sentenced to death by hanging. Saddam's half brother, Barzan Ibrahim, and Awad Hamed al-Bandar, head of Iraq's Revolutionary Court in 1982, were convicted of similar charges. The verdict and sentencing were both appealed, but subsequently affirmed by Iraq's Supreme Court of Appeals. On 30 December 2006, Saddam was hanged.
Execution.
Saddam was hanged on the first day of Eid ul-Adha, 30 December 2006, despite his wish to be shot (which he felt would be more dignified). The execution was carried out at Camp Justice, an Iraqi army base in Kadhimiya, a neighborhood of northeast Baghdad.
Video of the execution was recorded on a mobile phone and his captors could be heard insulting Saddam. The video was leaked to electronic media and posted on the Internet within hours, becoming the subject of global controversy. It was later claimed by the head guard at the tomb where his remains lay that Saddam's body had been stabbed six times after the execution.
Not long before the execution, Saddam's lawyers released his last letter. The following includes several excerpts:
To the great nation, to the people of our country, and humanity,
Many of you have known the writer of this letter to be faithful, honest, caring for others, wise, of sound judgment, just, decisive, careful with the wealth of the people and the state ... and that his heart is big enough to embrace all without discrimination.
You have known your brother and leader very well and he never bowed to the despots and, in accordance with the wishes of those who loved him, remained a sword and a banner.
This is how you want your brother, son or leader to be ... and those who will lead you (in the future) should have the same qualifications.
Here, I offer my soul to God as a sacrifice, and if He wants, He will send it to heaven with the martyrs, or, He will postpone that ... so let us be patient and depend on Him against the unjust nations.
Remember that God has enabled you to become an example of love, forgiveness and brotherly coexistence ... I call on you not to hate, because hate does not leave a space for a person to be fair and it makes you blind and closes all doors of thinking and keeps away one from balanced thinking and making the right choice.
I also call on you not to hate the peoples of the other countries that attacked us and differentiate between the decision-makers and peoples. Anyone who repents – whether in Iraq or abroad – you must forgive him.
You should know that among the aggressors, there are people who support your struggle against the invaders, and some of them volunteered for the legal defence of prisoners, including Saddam
Hussein ... some of these people wept profusely when they said goodbye to me.
Dear faithful people, I say goodbye to you, but I will be with the merciful God who helps those who take refuge in him and who will never disappoint any faithful, honest believer ... God is Great ... God is great ... Long live our nation ... Long live our great struggling people ... Long live Iraq, long live Iraq ... Long live Palestine ... Long live jihad and the mujahedeen.
Saddam Hussein
President and Commander in Chief of the Iraqi Mujahed Armed Forces
Additional clarification note:
I have written this letter, because the lawyers told me that the so-called criminal court — established and named by the invaders — will allow the so-called defendants the chance for a last word. But that court and its chief judge did not give us the chance to say a word, and issued its verdict without explanation and read out the sentence — dictated by the invaders — without presenting the evidence. I wanted the people to know this.
A second unofficial video, apparently showing Saddam's body on a trolley, emerged several days later. It sparked speculation that the execution was carried out incorrectly as Saddam Hussein had a gaping hole in his neck.
Saddam was buried at his birthplace of Al-Awja in Tikrit, Iraq, 3 km (2 mi) from his sons Uday and Qusay Hussein, on 31 December 2006.
Marriage and family relationships.
In August 1995, Raghad and her husband Hussein Kamel al-Majid and Rana and her husband, Saddam Kamel al-Majid, defected to Jordan, taking their children with them. They returned to Iraq when they received assurances that Saddam would pardon them. Within three days of their return in February 1996, both of the Kamel brothers were attacked and killed in a gunfight with other clan members who considered them traitors.
In August 2003, Saddam's daughters Raghad and Rana received sanctuary in Amman, Jordan, where they are currently staying with their nine children. That month, they spoke with CNN and the Arab satellite station Al-Arabiya in Amman. When asked about her father, Raghad told CNN, "He was a very good father, loving, has a big heart." Asked if she wanted to give a message to her father, she said: "I love you and I miss you." Her sister Rana also remarked, "He had so many feelings and he was very tender with all of us."
Philanthropic Connection to the City of Detroit, Michigan.
In 1979, Rev. Jacob Yasso of Chaldean Sacred Heart Church congratulated Saddam Hussein on his presidency. In return, Rev. Yasso said that Saddam Hussein donated US$250,000 to his church, which is made up of at least 1,200 families of Middle Eastern descent. In 1980, Detroit Mayor Coleman Young allowed Rev. Yasso to present the key to the city of Detroit to Saddam Hussein. At the time, Saddam then asked Rev. Yasso, "I heard there was a debt on your church. How much is it?" After the inquiry, Saddam then donated another $200,000 to Chaldean Sacred Heart Church. Rev. Yasso said that Saddam made donations to Chaldean churches all over the world, and even went on record as saying "He's very kind to Christians."

</doc>
<doc id="29491" url="https://en.wikipedia.org/wiki?curid=29491" title="Sonja Henie">
Sonja Henie

Sonja Henie (8 April 1912 – 12 October 1969) was a Norwegian figure skater and film star. She was a three-time Olympic Champion (1928, 1932, 1936) in Ladies' Singles, a ten-time World Champion (1927–1936) and a six-time European Champion (1931–1936). Henie won more Olympic and World titles than any other ladies' figure skater. At the height of her acting career she was one of the highest paid stars in Hollywood.
Biography.
Early life.
Henie was born in Kristiania, current Oslo, the only daughter of Wilhelm Henie (1872–1937), a prosperous Norwegian furrier, and his wife Selma Lochmann-Nielsen (1888–1961). In addition to the income from the fur business, both of Henie's parents had inherited wealth. Wilhelm Henie had been a one-time World Cycling Champion and the Henie children were encouraged to take up a variety of sports at a young age. Henie initially showed talent at skiing, and then followed her older brother Leif to take up figure skating. As a girl, Henie was also a nationally ranked tennis player and a skilled swimmer and equestrienne. Once Henie began serious training as a figure skater, her formal schooling ended. She was educated by tutors, and her father hired the best experts in the world, including the famous Russian ballerina Tamara Karsavina, to transform his daughter into a sporting celebrity.
Competitive career.
Henie won her first major competition, the senior Norwegian championships, at the age of 10. She then placed eighth in a field of eight at the 1924 Winter Olympics, at the age of eleven. During the 1924 program, she skated over to the side of the rink several times to ask her coach for directions, but by the next Olympiad, she needed no such assistance.
Henie won the first of an unprecedented ten consecutive World Figure Skating Championships in 1927 at the age of fourteen. The results of 1927 World Championships, where Henie won in 3–2 decision (or 7 vs. 8 ordinal points) over the defending Olympic and World Champion Herma Szabo of Austria, was controversial, as three of the five judges that gave Henie first-place ordinals were Norwegian (1 + 1 + 1 + 2 + 2 = 7 points) while Szabo received first-place ordinals from an Austrian and a German Judge (1 + 1 + 2 + 2 + 2 = 8 points). Henie went on to win first of her three Olympic gold medals the following year. She defended her Olympic titles in 1932 and in 1936, and her world titles annually until 1936. She also won six consecutive European championships from 1931 to 1936. Henie's unprecedented three Olympic gold medals haven't been matched by any ladies' single skater since; neither are her achievements as ten-time consecutive World Champion. While Irina Slutskaya of Russia won her seventh European Championship in 2006 to become the most successful ladies' skater in European Championships, Henie retains record of most consecutive titles, sharing it with Katarina Witt of Eastern Germany/Germany (1983–1988).
Towards the end of her career, she began to be strongly challenged by younger skaters including Cecilia Colledge, Megan Taylor and Hedy Stenuf. However, she held off these competitors and went on to win her third Olympic title at the 1936 Winter Olympics, albeit in very controversial circumstances with Cecilia Colledge finishing a very close second. Indeed, after the school figures section at the 1936 Olympic competition, Colledge and Henie were virtually neck and neck with Colledge trailing by just a few points. As Sandra Stevenson recounted in her article in "The Independent" of 21 April 2008, "the closeness the competition infuriated Henie, who, when the result for that section was posted on a wall in the competitors' lounge, swiped the piece of paper and tore it into little pieces. The draw for the free skating came under suspicion after Henie landed the plum position of skating last, while Colledge had to perform second of the 26 competitors. The early start was seen as a disadvantage, with the audience not yet whipped into a clapping frenzy and the judges known to become freer with their higher marks as the event proceeded. Years later, a fairer, staggered draw was adopted to counteract this situation".
During her competitive career, Henie traveled widely and worked with a variety of foreign coaches. At home in Oslo, she trained at Frogner Stadium, where her coaches included Hjørdis Olsen and Oscar Holte. During the latter part of her competitive career she was coached primarily by the American Howard Nicholson in London. In addition to traveling to train and compete, she was much in demand as a performer at figure skating exhibitions in both Europe and North America. Henie became so popular with the public that police had to be called out for crowd control on her appearances in various disparate cities such as Prague and New York City. It was an open secret that, in spite of the strict amateurism requirements of the time, Wilhelm Henie demanded "expense money" for his daughter's skating appearances. Both of Henie's parents had given up their own pursuits in Norway—leaving Leif to run the fur business—in order to accompany Sonja on her travels and act as her managers.
Henie is credited with being the first figure skater to adopt the short skirt costume in figure skating, wear white boots, and make use of dance choreography. Her innovative skating techniques and glamorous demeanor transformed the sport permanently and confirmed its acceptance as a legitimate sport in the Winter Olympics.
Professional and film career.
After the 1936 World Figure Skating Championships, Henie gave up her amateur status and took up a career as a professional performer in acting and live shows. While still a girl, Henie had decided that she wanted to move to Hollywood and become a movie star when her competitive days were over, without considering that her thick accent might hinder her acting ambitions.
In 1936, following a successful ice show in Los Angeles orchestrated by her father to launch her film career, Hollywood studio chief Darryl Zanuck signed her to a long term contract at Twentieth Century Fox, which made her one of the highest-paid actresses of the time. After the success of her first film, "One in a Million", Henie's position was assured and she became increasingly demanding in her business dealings with Zanuck. Henie also insisted on having total control of the skating numbers in her films such as "Second Fiddle" (1939).
In addition to her film career at Fox, Henie formed a business arrangement with Arthur Wirtz, who produced her touring ice shows under the name of "Hollywood Ice Revue". Wirtz also acted as Henie's financial advisor. At the time, figure skating and ice shows were not yet an established form of entertainment in the United States. Henie's popularity as a film actress attracted many new fans and instituted skating shows as a popular new entertainment. Throughout the 1940s, Henie and Wirtz produced lavish musical ice skating extravaganzas at Rockefeller Center's Center Theatre attracting millions of ticket buyers.
At the height of her fame, her shows and touring activities brought Henie as much as $2 million per year. She also had numerous lucrative endorsement contracts, and deals to market skates, clothing, jewelry, dolls, and other merchandise branded with her name. These activities made her one of the wealthiest women in the world in her time.
Henie broke off her arrangement with Wirtz in 1950 and for the next three seasons produced her own tours under the name "Sonja Henie Ice Revue". It was an ill-advised decision to set herself up in competition with Wirtz, whose shows now featured the new Olympic champion Barbara Ann Scott. Since Wirtz controlled the best arenas and dates, Henie was left playing smaller venues and markets already saturated by other touring ice shows such as Ice Capades. The collapse of a section of bleachers during a show in Baltimore, Maryland in 1952 compounded the tour's legal and financial woes.
In 1953 Henie formed a new partnership with Morris Chalfen to appear in his European "Holiday On Ice" tour. This was a great success. She produced her own show at New York's Roxy Theatre in January 1956. However, a subsequent South American tour in 1956 was a disaster. Henie was drinking heavily at that time and could no longer keep up with the demands of touring, and this marked her retirement from skating.
In 1938, she published her autobiography "Mitt livs eventyr", which was translated and released as "Wings on My Feet" in 1940, which was republished in a revised edition in 1954. At the time of her death, Henie was planning a comeback for a television special that would have aired in January 1970.
Nazi controversy.
Henie's connections with Adolf Hitler and other high-ranking Nazi officials made her the subject of controversy before, during, and after World War II. During her amateur skating career, she performed often in Germany and was a favorite of German audiences and of Hitler personally. As a wealthy celebrity, she moved in the same social circles as royalty and heads of state and made Hitler's acquaintance as a matter of course.
Controversy appeared first when Henie greeted Hitler with a Nazi salute during an exhibition in Berlin some time before the 1936 Winter Olympics; she was strongly denounced by the Norwegian press. She did not repeat the salute at the Olympics in Garmisch-Partenkirchen, but after the Games she accepted an invitation to lunch with Hitler at his resort home in nearby Berchtesgaden, where Hitler presented Henie with an autographed photo with a lengthy inscription. After beginning her film career, Henie kept up her Nazi connections, for example personally arranging with Joseph Goebbels for the release of her first film, "One in a Million", in Germany.
During the occupation of Norway by Nazi Germany, German troops saw Hitler's autographed photo prominently displayed at the piano in the Henie family home in Landøya, Asker. As a result, none of Henie's properties in Norway were confiscated or damaged by the Germans. Henie became a naturalized citizen of the United States in 1941. Like many Hollywood stars, she supported the U.S. war effort through USO and similar activities, but she was careful to avoid supporting the Norwegian resistance movement, or making public statements against the Nazis. For this, she was condemned by many Norwegians and Norwegian-Americans. After the war, Henie was mindful that many of her countrymen considered her to be a quisling. However, she made a triumphant return to Norway with the Holiday on Ice tour in 1953 and 1955.
Personal life.
Henie was married three times, to Dan Topping (1940-1946), Winthrop Gardiner Jr. (1949-1956), and the Norwegian shipping magnate and art patron Niels Onstad (1956-1969) (her death). After her retirement in 1956, Henie and Onstad settled in Oslo and accumulated a large collection of modern art that formed the basis for the Henie Onstad Kunstsenter at Høvikodden in Bærum near Oslo.
In addition to her marriages, Henie had a variety of love interests, including her skating partners Jack Dunn and Stewart Reburn, celebrated boxing legend Joe Louis, a much-publicized affair with Tyrone Power, and a later romance with actor Van Johnson and a very high profile relationship with Liberace.
Death.
Henie was diagnosed with leukemia in the mid-1960s. She died of the disease at age 57 in 1969 during a flight from Paris to Oslo. Generally regarded as one of the greatest figure skaters in history, she is buried with Onstad in Oslo on the hilltop overlooking the Henie-Onstad Art Centre.

</doc>
<doc id="29493" url="https://en.wikipedia.org/wiki?curid=29493" title="Science &amp; Environmental Policy Project">
Science &amp; Environmental Policy Project

The Science & Environmental Policy Project (SEPP) is a research and advocacy group financed by private contributions based in Arlington, Virginia in the United States. It was founded in 1990 by atmospheric physicist S. Fred Singer. SEPP disputes the prevailing scientific views of climate change and ozone depletion. SEPP also questioned the science used to establish the dangers of secondhand smoke, arguing the risks are overstated.
SEPP's former Chairman of the Board of Directors is listed as Rockefeller University president emeritus Frederick Seitz, a former president of the National Academy of Sciences, now deceased.
SEPP's views.
SEPP lists the following key issues: [http://www.sepp.org/key%20issues/keyissue.html]
On September 2, 1997, Singer said that "The possibility that global temperatures could rise because of an increase in carbon dioxide in the atmosphere is a concern that needs to be monitored...But there has been no indication in the last century that we've seen anything other than natural climate fluctuations. Both greenhouse theory and computer models predict that global warming should be more rapid in the polar regions than anywhere else," he says, "but in July the Antarctic experienced the coldest weather on record."[http://www.sepp.org/pressrel/goreglac.html]
SEPP was the author of the Leipzig Declaration, which was based on the conclusions drawn from a November 1995 conference in Leipzig, Germany, which SEPP organized with the European Academy for Environmental Affairs.
Rebuttals.
SEPP's critics offer the following rebuttals to its claims:
NIPCC.
In 2008, The Science and Environmental Policy Project completed the organization of the Nongovernmental International Panel on Climate Change (NIPCC) as the culmination of a process that began in 2003. The NIPCC calls itself "an international coalition of scientists convened to provide an independent examination of the evidence available on the causes and consequences of climate change in the published, peer-reviewed literature – examined without bias and selectivity."
The 2008 NIPCC document titled "Nature, Not Human Activity Rules the Climate: Summary for Policymakers of the Report of the Nongovernmental International Panel of Climate Change", published by The Heartland Institute, was released in February–March 2008. Singer served as General Editor and also holds the copyright.
Unnamed climate scientists from NASA, Stanford University and Princeton who were contacted by ABC News dismissed the same report as "fabricated nonsense.". In response, Singer objected to the ABC News piece, calling it "an appalling display of bias, unfairness, journalistic misbehavior, and a breakdown of ethical standards" which used "prejudicial language, distorted facts, libelous insinuations, and anonymous smears."
Further reading.
In 2004 Singer was coauthor of two papers published in Geophysical Research Letters:
Scientific criticism of SEPP's views:

</doc>
<doc id="29494" url="https://en.wikipedia.org/wiki?curid=29494" title="Abbey of Saint Gall">
Abbey of Saint Gall

The Abbey of Saint Gall () is a Roman Catholic religious complex in the city of St. Gallen in Switzerland of a dissolved abbey (747-1805). The Carolingian-era monastery has existed since 719 and became an independent principality between 9th and 13th centuries, and was for many centuries one of the chief Benedictine abbeys in Europe. It was founded by Saint Othmar on the spot where Saint Gall had erected his hermitage. The library at the Abbey is one of the richest medieval libraries in the world. The city of St. Gallen originated as an adjoining settlement of the abbey. Following the secularization of the abbey around 1800 the former Abbey church became a Cathedral in 1848. Since 1983 the whole remaining abbey precinct has been a UNESCO World Heritage Site.
History.
Foundation.
Around 613 Gallus, according to tradition an Irish monk and disciple and companion of Saint Columbanus, established a hermitage on the site that would become the monastery. He lived in his cell until his death in 646. in Arbon. The people kept looking for protection at Gallus' cell in time of danger.
Following Gallus' death, Charles Martel appointed Otmar as custodian of St Gall's relics. Several different dates are given for the foundation of the monastery, including 719, 720, 747 and the middle of the 8th century. During the reign of Pepin the Short, in the 8th century, Othmar founded the Carolingian style Abbey of St Gall, where arts, letters and sciences flourished. The abbey grew fast and many Alemannic noblemen became monks. At the end of abbot Otmar's reign, the "Professbuch" mentions 53 names. Two monks of the Abbey of St Gall, Magnus von Füssen and Theodor, founded the monasteries in Kempten and Füssen in the Allgäu. With the increase in the number of monks the abbey grew stronger also economically. Much land in Thurgau, Zürichgau and in the rest of Alemannia as far as the Neckar was transferred to the abbey due to "Stiftungen". Under abbot Waldo of Reichenau (740–814) copying of manuscripts was undertaken and a famous library was gathered. Numerous Anglo-Saxon and Irish monks came to copy manuscripts. At Charlemagne's request Pope Adrian I sent distinguished chanters from Rome, who propagated the use of the Gregorian chant. In 744, the Alemannic nobleman Beata sells several properties to the abbey in order to finance his journey to Rome.
The Golden Age.
In the subsequent century, St Gall came into conflict with the nearby Bishopric of Constance which had recently acquired jurisdiction over the Abbey of Reichenau on Lake Constance. It was not until Emperor Louis the Pious (ruled 814–840) confirmed in 813 the imperial immediacy ("Reichsunmittelbarkeit") of the abbey, that this conflict ceased. The abbey became an Imperial Abbey ("Reichsabtei"). King Louis the German confirmed in 833 the immunity of the abbey and allowed the monks the free choice of their abbot. In 854 finally, the Abbey of St Gall reached its full autonomy by King Louis the German releasing the abbey from the obligation to pay tithes to the Bishop of Constance.
From this time until the 10th century, the abbey flourished. It was home to several famous scholars, including Notker of Liège, Notker the Stammerer, Notker Labeo and Hartker (who developed the antiphonal liturgical books for the abbey). During the 9th century a new, larger church was built and the library was expanded. Manuscripts on a wide variety of topics were purchased by the abbey and copies were made. Over 400 manuscripts from this time have survived and are still in the library today.
The Silver Age.
Between 924 and 933 the Magyars threatened the abbey and the books had to be removed to Reichenau for safety. Not all the books were returned.
On 26 April 937 a scholar kindled a fire and the abbey and the adjoining settlement were almost completely destroyed; the library was undamaged, however. About 954 they started to protect the monastery and buildings by a sourrounding wall. Around 971/974 abbot Notker finalized the walling and the adjoining settlements started to become the town of St Gall.
The death of abbot Ulrich on 9 December 1076 terminates the cultural silver age of the monastery.
Under the Prince-Abbots.
In 1207 abbot Ulrich von Sax becomes an Prince ("Reichsfürst", or simply "Fürst") of the Holy Roman Empire by King Philip of Swabia. The abbey became a Princely Abbey ("Reichsabtei"). As the abbey became more involved in local politics, it entered a period of decline. 
The city of St. Gallen proper progressively freed itself from the rule of the abbot, acquiring Imperial immediacy, and by the late 15th century was recognized as a Free imperial city.
By about 1353 the guilds, headed by the cloth-weavers guild, gained control of the civic government. In 1415 the city bought its liberty from the German king King Sigismund.
During the 14th century Humanists were allowed to carry off some of the rare texts from the abbey library.
In the late 14th and early 15th centuries, the farmers of the abbot's personal estates (known as Appenzell, from meaning "cell (i.e. estate) of the abbot) began seeking independence. In 1401, the first of the Appenzell Wars broke out, and following the Appenzell victory at Stoss in 1405 they became allies of the Swiss Confederation in 1411. During the Appenzell Wars, the town of St. Gallen often sided with Appenzell against the abbey. So when Appenzell allied with the Swiss, the town of St. Gallen followed just a few months later. The abbot became an ally of several members of the Swiss Confederation (Zürich, Lucerne, Schwyz and Glarus) in 1451. While Appenzell and St. Gallen became full members of the Swiss Confederation in 1454. Then, in 1457 the town of St. Gallen became officially free from the abbot.
In 1468 the abbot, Ulrich Rösch, bought the county of Toggenburg from the representatives of its counts, after the family died out in 1436. In 1487 he built a monastery at Rorschach on Lake Constance, to which he planned to move. However, he encountered stiff resistance from the St. Gallen citizenry, other clerics, and the Appenzell nobility in the Rhine Valley who were concerned about their holdings. The town of St. Gallen wanted to restrict the increase of power in the abbey and simultaneously increase the power of the town. The mayor of St. Gallen, Ulrich Varnbüler, established contact with farmers and Appenzell residents (led by the fanatical Hermann Schwendiner) who were seeking an opportunity to weaken the abbot. Initially, he protested to the abbot and the representatives of the four sponsoring Confederate cantons (Zürich, Lucerne, Schwyz, and Glarus) against the construction of the new abbey in Rorschach. Then on July 28, 1489 he had armed troops from St. Gallen and Appenzell destroy the buildings already under construction. When the abbot complained to the Confederates about the damages and demanded full compensation, Varnbüler responded with a counter suit and in cooperation with Schwendiner rejected the arbitration efforts of the non-partisan Confederates. He motivated the clerics from Wil to Rorschach to discard their loyalty to the abbey and spoke against the abbey at the town meeting at Waldkirch, where the popular league was formed. He was confident that the four sponsoring cantons would not intervene with force, due to the prevailing tensions between the Confederation and the Swabian League. He was strengthened in his resolve by the fact that the people of St. Gallen elected him again to the highest magistrate in 1490.
An associate of the Swiss Confederation.
However, in early 1490 the four cantons decided to carry out their duty to the abbey and to invade the St. Gallen canton with an armed force. The people of Appenzell and the local clerics submitted to this force without noteworthy resistance, while the city of St. Gallen braced for a fight to the finish. However, when they learned that their compatriots had given up the fight, they lost confidence; the end result was that they concluded a peace pact that greatly restricted the city's powers and burdened the city with serious penalties and reparations payments. Varnbüler and Schwendiner fled to the court of King Maximilian and lost all their property in St. Gallen and Appenzell. However, the abbot's reliance on the Swiss to support him reduced his position almost to that of a "subject district"
The town adopted the Reformation in 1524, while the abbey remained Catholic, which damaged relations between the town and abbey. Both the abbot and a representative of the town were admitted to the Swiss Tagsatzung or Diet as the closest associates of the Confederation.
In the 16th century the abbey was raided by Calvinist groups, which scattered many of the old books. In 1530, abbot Diethelm began a restoration that stopped the decline and led to an expansion of the schools and library.
Under abbot Pius (1630–74) a printing press was started. In 1712 during the Toggenburg war, also called the second war of Villmergen, the Abbey of St. Gall was pillaged by the Swiss. They took most of the books and manuscripts to Zürich and Bern. For security, the abbey was forced to request the protection of the townspeople of St. Gallen. Until 1457 the townspeople had been serfs of the abbey, but they had grown in power until they were protecting the abbey.
End of the Prince-Abbots.
Following the disturbances, the abbey was still the largest religious city-state in Switzerland, with over 77,000 inhabitants. A final attempt to expand the abbey resulted in the demolition of most of the medieval monastery. The new structures, including the cathedral by architect Peter Thumb (1681-1766), were designed in the late Baroque style and constructed between 1755 and 1768. The large and ornate new abbey did not remain a monastery for very long. In 1798 the Prince-Abbot's secular power was suppressed, and the abbey was secularized. The monks were driven out and moved into other abbeys. The abbey became a separate See in 1846, with the abbey church as its cathedral and a portion of the monastic buildings for the bishop.
Cultural treasures.
The Abbey library of Saint Gall is recognized as one of the richest medieval libraries in the world. It is home to one of the most comprehensive collections of early medieval books in the German-speaking part of Europe. , the library consists of over 160,000 books, of which 2100 are handwritten. Nearly half of the handwritten books are from the Middle Ages and 400 are over 1000 years old. Lately the "Stiftsbibliothek" has launched a project for the digitisation of the priceless manuscript collection, which currently (December 2009) contains 355 documents that are available on the "Codices Electronici Sangallenses" webpage.
The library interior is exquisitely realised in the Rococo style with carved polished wood, stucco and paint used to achieve its overall effect. It was designed by the architect Peter Thumb and is open to the public. In addition it holds exhibitions as well as concerts and other events.
One of the more interesting documents in the Stiftsbibliothek is a copy of Priscian's "Institutiones grammaticae" which contains the poem "Is acher in gaíth in-nocht..." written in Old Irish.
The library also preserves a unique 9th-century document, known as the Plan of St. Gall, the only surviving major architectural drawing from the roughly 700-year period between the fall of the Western Roman Empire and the 13th century. The Plan drawn was never actually built, and was so named because it was kept at the famous medieval monastery library, where it remains to this day. The plan was an ideal of what a well-designed and well-supplied monastery should have, as envisioned by one of the synods held at Aachen for the reform of monasticism in the Frankish empire during the early years of emperor Louis the Pious (between 814 and 817).
A late 9th-century drawing of St. Paul lecturing an agitated crowd of Jews and gentiles, part of a copy of a Pauline epistles produced at and still held by the monastery, was included in a medieval-drawing show at the Metropolitan Museum of Art in New York the summer of 2009. A reviewer noted that the artist had "a special talent for depicting hair, ... with the saint's beard ending in curling droplets of ink."
St. Gall is noted its early use of the neume, the basic element of Western and Eastern systems of musical notation prior to the invention of five-line staff notation. The earliest extant manuscripts are from the 9th or 10th century.
In 1983, the Convent of St. Gall was inscribed on the UNESCO World Heritage List as "a perfect example of a great Carolingian monastery".
List of the abbots.
There were a total of 73 ruling abbots (including six anti-abbots) during 719 and 1805.
A complete collection of abbots' biographies was published 
by Henggeler (1929). A table of abbots' names complete with their coats of arms was printed by Beat Jakob Anton Hiltensperger in 1778.

</doc>
<doc id="29498" url="https://en.wikipedia.org/wiki?curid=29498" title="Secondary education">
Secondary education

Secondary education normally takes place in secondary schools, taking place after primary education and may be followed by higher education or vocational training. In some countries, only primary or basic education is compulsory, but secondary education is included in compulsory education in most countries. In post-Soviet countries it is also known as general education or general middle education.
Terminology.
Secondary schools may be called "high schools", "gymnasia", "lyceums", "middle schools", "sixth-form", "sixth-form colleges", "vocational schools", or "preparatory schools", and the exact meaning of any of these varies between the countries.
By country.
Argentina.
The school system is free and mandatory.
Australia.
School is compulsory in Australia between the ages of five/six to fifteen/sixteen/seventeen, depending on the state, with, in recent years, over three-quarters of people staying on until their thirteenth year in school. Government schools educate about two-thirds of Australian students, with the other third in independent schools. Government schools are free although most schools charge what are known as "voluntary contributions" or "tax levies", while independent schools, both religious and secular, charge fees as well as levies. Regardless of what whether a school is government or independent, it is required to adhere to the same curriculum frameworks. Most school students, whether in government or independent school, usually wear uniforms, although there are varying expectations and a few school exceptions.
Each state and territory has its own format of Year 12 matriculation:
Belgium.
For more details see "Education in Belgium - Secondary education".
The Belgian school has a three-tier education system, with each stage divided into various levels:
Brazil.
In Brazil, since 1996 high school is officially called "Ensino Médio" (formerly "Segundo Grau"). Until the year 1971, "ensino médio" had three different names: "curso científico", "curso normal" and "curso clássico" ("classic"). As a result, the course was changed after and called "colegial", also divided, with the first three years were the same for everyone and anyone who would subsequently make the old "normal" and "clássico", had to do another year.
Historically, in Brazil, is called secondary what is now the second part of primary school (from the sixth year to the ninth year), plus high school.
It is the last phase to basic education. Brazilian high school lasts three years, attempting to deepen what students have learned in the "Ensino Fundamental". Brazilian high school students are referenced by their year – 1st, 2nd and 3rd years.
Unlike other countries, Brazilian students don't have a final test to conclude studies. Their approval depends only on their final grade on each subject. Each university elaborates its own test to select new students – this test, the "vestibular", generally happens once a year. Enem, a non-mandatory national exam, evaluates high school students in Brazil and is used to rank both private and public schools.
Best scores are usually achieved by students on public universities. Despite lack of funds and historical and social problems contribute to poor attendance from the students, especially those in public schools, those Universities usually are recognized as academically excellent.
Private establishments, on the other hand, may be recognized as academically excellent or merely as investments in social networking. Schedules vary from school to school. The subjects taught, however, are conceived by the "Ministério da Educação" (Ministry of Education) which emphasises the hard sciences.
The educational year begins in February and finishes in December, often having July as a break; institutions are permitted to define their own actual start and end dates. They must, however, provide at least 200 days of classes per year.
Universities are also divided into public and private. At this level, public ones are considered excellent and their vestibular exam is highly competitive (the exam for med school in UNICAMP may hit 300 candidates per place). For better preparation, therefore, many students take a "curso pré-vestibular" (university preparation course), which is offered by large private high schools.
Colombia.
Secondary education in Colombia is divided into two; basic secondary that goes from years 6 to 9, and mid secondary that are grades 10 and 11. In Colombia, education has always been mandatory but it wasn't until 2012 that all education for kids and teens was made free of charge at any public institution.
Croatia.
Secondary education is currently optional, although most political parties now advocate the stance that it should also become compulsory.
Secondary schools in Croatia are subdivided into:
Gymnasiums, schools of economics and schools of engineering take four years. There are also some vocational schools that last only three years.
Secondary schools supply students with primary subjects needed for the necessary work environment in Croatia. People who completed secondary school are classified as "medium expertise" ("srednja stručna sprema" or SSS).
There are currently around 90 gymnasiums and some 300 vocational schools in Croatia. The public secondary schools are under the jurisdiction of regional government, the counties.
Cyprus.
1.1 General overview of education stages
Cyprus has a three-tier educational system, each stage being divided into specific levels:
Czech Republic.
Due to historic reasons, the Czech school system is almost the same as the German school system. The school system is free and mandatory until age 15. After the "Základní škola" (elementary school) at age 15, students are directed to three different optional secondary education schools:
The "maturita" is required for study in university. The Abitur from gymnasium is better for a humanistic pointed university and SOŠ Abitur is better for a technical pointed university.
Denmark.
In Denmark it is mandatory to receive education answering to the basic school syllabus until the 10th year of school education, which likewise extends to compulsory pre-schooling since 2009. Pupils can choose an 11th year of school. After the basic school the majority of pupils between ages 15–19 usually choose to go through the three-year "Gymnasium", which is university-preparatory or high school. Adolescents not attending the Gymnasium most commonly attend vocational training. There are over 100 different vocational courses in Denmark.
Egypt.
The secondary school, known as Thanawya Amma (ثانوية عامة), is a three-year program after which the student, according to his score in the final year, can join a higher level of education in a university or, when the score is lower, an institution of education that issues a degree not equal with the university one.
The main defect of such a system that it depends on the final written exam to determine the student's higher education regardless of any activities.
Finland.
The Finnish education system is a comparatively egalitarian Nordic system. This means for example no tuition fees for full-time students, and free meals are served to pupils.
The second level education is not compulsory, but an overwhelming majority attends. There is a choice between upper secondary school ("lukio", "gymnasium") and vocational school ("ammatillinen oppilaitos", "yrkesinstitut"). Graduates of both upper secondary school and vocational school can apply to study in further education (university and polytechnics).
Upper secondary school, unlike vocational school, concludes with a nationally graded matriculation examination ("ylioppilastutkinto", "studentexamen"). Passing the test is a "de facto" prerequisite for further education. The system is designed so that approximately the lowest scoring 5% fails and also 5% get the best grade. The exam allows for a limited degree of specialization in either natural sciences or social sciences. The graduation is an important and formal family event, like christenings, weddings, and funerals.
In the OECD's international assessment of student performance, PISA, Finland has consistently been among the highest scorers worldwide; in 2003, Finnish 15-year-olds came first in reading literacy, science, and mathematics; and second in problem solving, worldwide. The World Economic Forum ranks Finland's tertiary education #1 in the world.
Germany.
The German school system is free and compulsory until 9th grade. After the "Grundschule" (primary/elementary school lasting four to six years), teachers recommend each pupil for one of three different types of secondary education. Parents have the final say about which school their child will attend.
Students with special needs are assigned to Förderschule.
Hong Kong.
"secondary school" (中學, Cantonese: "jung1 hok6"), "college" (書院)
Secondary education in Hong Kong is largely based on the British education system. Secondary school starts in the seventh year, or Form One, of formal education, after Primary Six. Students normally spend five years in secondary schools, of which the first three years (Forms One to Three) are compulsory like primary education. Forms Four and Five students prepare for the Hong Kong Certificate of Education Examination (HKCEE), which takes place after Form Five. Students obtaining a satisfactory grade will be promoted to Form Six. They then prepare for the Hong Kong Advanced Level Examination (HKALE) (colloquially "the A-levels"), which is to be taken after Form Seven. The HKALE and HKCEE results will be considered by universities for admission. Some secondary schools in Hong Kong are called 'colleges'. In some schools, Form Six and Form Seven are also called Lower Six and Upper Six respectively.
The HKCEE is equivalent to the British GCSE and HKALE is equivalent to the British A-level.
As of October 2004, there has been heated discussion on proposed changes in the education system, which includes (amongst others) reduction of the duration of secondary education from seven years to six years, and merging the two exams HKCEE and HKALE into one exam, Hong Kong Diploma of Secondary Education (HKDSE). The proposed changes will take effect in 2009.
The secondary education system of Hong Kong, just as other East Asian countries, is examination-oriented. This does the strong but controversial post-school tutorial education industry a favor.
India.
There are three popular Indian school boards -
CBSE - Central board of secondary education , std 1 to 12, has competitive exams at 10th and 12th grade . Uniformity of school curriculum with more emphasis on maths and science.
ICSE - Indian council for secondary education, std 1 to 12, has more indepth study materials, considered to be the toughest board in India, English level is on par with UK, this board offers more choices of subjects.
State board - available in each state of India, std 1 to 12, the curriculum varies from state to state, generally easier board for scoring, has more of local appeal to the curriculum.
In India, before the Indian Constitutional Amendment in 2002, Article 45 (Articles 36 - 51 are on Directive-Principles of State Policy) of the Constitution was "Art. 45. Provision for free and compulsory education for children. - The State shall endeavour to provide, within a period of ten years from the commencement of this Constitution, for free and compulsory education for all children until they complete the age of fourteen years." But that Constitutional obligation was time and again deferred - first to 1970 and then to 1980, 1990 and 2000. The 10th Five-Year Plan visualized that India will achieve universal elementary education by 2007. However, the Union Human Resource Development Minister announced in 2001 that India will achieve this target only by 2010.
The Ninety-third Amendment Bill, 2002, renumbered as the Constitution (86th Amendment) Act, 2002, passed on 12 December 2002, stated:
An Act further to amend the Constitution of India. .
BE it enacted by Parliament in the Fifty-third Year of the Republic of India as follows:-
1. Short title and commencement.
(1) This Act may be called the Constitution (Eighty-sixth
Amendment) Act, 2002.
(2) It shall come into force on such date as the Central Government may, by notification in the
Official Gazette, appoint.
2. Insertion of new article 21A.- After article 21 of the Constitution, the following article shall be inserted, namely
Right to education.-
"Art.21A. The State shall provide free and compulsory education to all children of the age of six to
fourteen years in such manner as the State may, by law, determine.".
3. Substitution of new article for article 45.- For article 45 of the Constitution, the following article
shall be substituted, namely:-
Provision for early childhood care and education to children below the age of six years.
"Art.45. The State shall endeavour to provide early childhood care and education for all children until
they complete the age of six years.".
4. Amendment of article 51A.
Indonesia.
Indonesia follows the historical Dutch education system, where the secondary education consists of junior high school ("Sekolah Menengah Pertama" or SMP) and senior high school ("Sekolah Menengah Atas" or SMA); each takes three years. Usually a student continues to SMP at age 12 and starts SMA at age 15.
In the second year (grade 11) of high school (SMA), students can choose one of three majors, namely Natural Science, Social Science and Literature. At the end of the third year (grade 12), students are required to follow the National Examination (formerly "EBTANAS") that affect students' graduation. High school graduates can continue their education to college or straight to work.
Senior High education is not included in the compulsory government program, only the 6-years primary education and junior high education are, even though since 2005 there is en effort to make high school education compulsory in some areas, for example in the Bantul Regency of Yogyakarta.
Ireland.
In Ireland secondary school starts at the age of 12, and lasts three or optionally five or six years. The main types of secondary school are: community schools, comprehensive schools, "colleges" (though this term is more usually applied to third-level institutions like universities), vocational schools, voluntary secondary schools and meánscoileanna (secondary schools that teach all subjects through Irish). After three years (age 14-16), every student takes a compulsory state exam known as the Junior Certificate. Typically a student will sit exams in 9 to 11 subjects; English (L1), Irish (L2) and Mathematics are compulsory.
After completing the Junior Certificate, a student may continue for two years to take a second state exam, the Leaving Certificate, around age 17-18. Students typically take 6-8 subjects. Except in exceptional circumstances, subjects taken must include Irish (L1), English (L2) and Mathematics. Leaving Certificate results directly determine admission to university via a ranking system managed by the CAO. More than 80% of students who complete the Junior Certificate continue to the Leaving Certificate.
There is an optional year in many secondary schools in Ireland known as Transition Year, which some students choose to take after completing the Junior Certificate, and before starting the Leaving Certificate. Focusing on broadening horizons, the year is often structured around student projects such as producing a magazine, charity work, or running a small business. Regular classes may be mixed with classes on music, drama, public speaking, etc. Transition year is not formally examined but student progress is monitored by teachers on a continuous basis. Programs vary from school to school. This year also focuses on giving the children an insight into the working world through work experience placements.
In addition to the main school system, Ireland has a parallel system of vocational schools, which place less focus on academic subjects and more on vocational and technical skills - around 25% of students attend these. Many vocational schools also offer night classes to adults. There is also a prominent movement known as Gaelscoileanna where every subject is taught through the Irish language, and these are growing fast in number.
Italy.
Secondary school ("Scuola secondaria") starts at age 11, after 5 years of primary school, and lasts 8 years. Secondary school is divided into 3 + 5 years, according to the following scheme:
All kinds of second-grade secondary schools end with an examination ("Esame di Stato", "state exam", but usually still called by its traditional name "Esame di Maturità", "maturity exam") whose contents are defined nationwide and score is on a 100-point scale.
Republic of Macedonia.
High school in Republic of Macedonia is called "средно училиште" or "middle school", and its structure is left from the socialist period. Reforms are being instituted with the goal of bringing the education system in line with the global community. In general, there is high school for preparing for every faculty on the university. There are: electro technical high school, mechanical high school, economics high school, pharmaceutical, medical, and natural sciences and linguistics gymnasium. The high school is attended between the years of 14 and 18 and is compulsory.
Malaysia.
The national secondary education in Malaysia, modelled after the (historical) English system, consists of five school years referred to as "forms" ("tingkatan" in Malay). Students begin attending secondary schools in the year they turn 13, after sitting for the UPSR (Ujian Pencapaian Sekolah Rendah or Primary School Assessment Examination) at the end of primary school. Students failing the academic requirement in UPSR are required to read an additional year called the Remove ("Peralihan") year before they are allowed to proceed to Form 1. Automatic promotion up to Form 5 has been in place since 1996. Some secondary schools offer an additional two years known as "sixth form", divided into "lower sixth" and "upper sixth".
Forms 1 to 3 are known as Lower Secondary ("Menengah Rendah"), while Forms 4 and 5 are known as Upper Secondary ("Menengah Tinggi"). Streaming into Art, Science or Commerce streams is done at the beginning of the Upper Secondary stage. Students sit for a standardised test at the end of both stages; Penilaian Menengah Rendah (PMR) for Lower Secondary, and Sijil Pelajaran Malaysia (SPM, equivalent to the O-Level examination) for Upper Secondary. At the end of the sixth form, students sit for the Sijil Tinggi Pelajaran Malaysia or the Malaysian Higher School Certificate (equivalent to the A levels). The language of instruction in national secondary schools is Malay except for language, science and mathematics subjects. Science and mathematics subjects are taught in English since 2003, but Malay will be reintroduced in stages from 2012.
Mexico.
Lower-secondary education (three years) is considered part of basic education in Mexico and is compulsory. For entry, students are required to have successfully completed six years of primary education. The next stage (three years), upper-secondary education or preparation school ("preparatoria"), has been compulsory since 2012. It has three pathways: general upper-secondary, technical professional education, and technological upper-secondary. As it has been called "bachillerato" it has been frequently confused with the US' "bachelor's level", which is called "Licenciatura o Ingeniería" in Latin American countries (though not all, as in Venezuela, the US' bachelor's Level is referred to as "doctor").
Nepal.
Nepal ranks 11th in quality education in the world.
Tribhuwan International University is a worldwide known institution.
Secondary education Nepal was 7 years in duration . Its highest value over the past 42 years was 7 years in 2012, while its lowest value was 5 years in 1970.
Netherlands.
In the Netherlands, high school is called "middelbare school" (literally "middle-level school") and starts right after the 6th grade of primary school (group 8). Pupils who start at a high school are around age 12. Because education in the Netherlands is compulsory between the ages of 4 and 16 (and partially compulsory between the ages of 16 and 18), all pupils must attend high school.
The high schools are part of the "voortgezet onderwijs" (literally: "continued education"). The "voortgezet onderwijs" consists of three main streams: VMBO, which has 4 grades and is subdivided over several levels; HAVO, which has 5 grades, and VWO, which has six grades. The choice for a particular stream is made based on the scores of an aptitude test (most commonly the CITO test), the advice of the grade 6 teacher, and the opinion of the pupil's parents or caretakers. It is possible to switch between streams. After completing a particular stream, a pupil can continue in the penultimate year of the next stream, from VMBO to HAVO, and from HAVO to VWO.
Successfully completing a particular stream grants access to different levels of tertiary education. After VMBO, a pupil can continue training at the MBO ("middle-level applied education"). A HAVO diploma allows for admission to the HBO ("higher professional education"), which are universities of professional education. Only with a VWO graduation, a pupil can enter a research university.
New Zealand.
In New Zealand students attend secondary school from the ages from about 13 to 18 (though it is possible to be 12). Formerly known as Forms 3 to 7, these grades are now known as Years 9 to 13. Schooling is compulsory until the student's 16th birthday. Historically secondary schools are named as either a high school or a college with no differentiation between the two types. NCEA is the Government-supported school qualification. New Zealand also has intermediate schools, but these cover the last two years of primary education (years 7 and 8) and are not secondary schools.
Pakistan.
Secondary education in Pakistan begins from grade 9 and lasts for four years. Upon completion of grade 10, students are expected to take a standardised test administered by a regional Board of Intermediate and Secondary Education (BISE). Upon successful completion of this examination, they are awarded a Secondary School Certificate (SSC). This is locally called the "matriculation certificate" or "matric". Students then enter a college and complete grades 11 and 12. Upon completion of grade 12, they again take a standardised test which is also administered by the regional boards. Upon successful completion of this test, students are awarded the Higher Secondary (School) Certificate (HSC). This level of education is also called the F.Sc./F.A/ICS or "intermediate". There are many streams students can choose for their 11 and 12 grades, such as pre-medical, pre-engineering, humanities (or social sciences), computer science and commerce. Some technical streams have recently been introduced for grades 11 and 12.
Alternative qualifications in Pakistan are also available but not maintained by the BISE but by other examination boards. Most common alternative is the General Certificate of Education (GCE), where SSC and HSC are replaced by Ordinary Level (O Level) and Advanced Level (A Level) respectively. Other qualifications include IGCSE which replaces SSC. GCE O Level, IGCSE and GCE AS/A Levels are managed by British examination boards of CIE of the Cambridge Assessment and Edexcel of the Pearson PLC. Advanced Placement (AP) is an alternative option but much less common than GCE or IGCSE. This replaces the secondary school education as "high school education" instead. AP exams are monitored by a North American examination board, the College Board, and can only be given under supervision of centers which are registered with the College Board, unlike GCE O/AS/A Level and IGCSE which can also be given privately.
Paraguay.
In Paraguay, secondary education is called "educación media". After nine years of "educación escolar básica" (primary school), a student can choose to go to either a "bachillerato técnico" (vocational school) or a "bachillerato científico" (high school); both are part of the "educación media" system. These two forms of secondary education last three years, and are usually located in the same campus called "colegio".
The "bachillerato técnico" combines general education with some specific subjects, referred to as pre-vocational education and career orientation. Fields include mechanical, electricity, commerce, construction, and business administration.
After completing secondary education, a student can enter university. It is also possible for a student to choose both técnico and científico schooling.
Portugal.
See High School in Portugal
Russia.
There were around 60,000 general education schools in 2007–2008 school year; this number includes ca. 5,000 advanced learning schools specializing in foreign languages, mathematics etc., 2,300 advanced general-purpose schools. Those identified as , "gymnasiums" and lycaeums, and 1,800 schools for all categories of disabled children; it does not include vocational technical schools and technicums. Private schools accounted for 0.3% of elementary school enrolment in 2005 and 0.5% in 2005.
According to a 2005 UNESCO report, 96% of the adult population has completed lower secondary schooling and most of them also have an upper secondary education.
Singapore.
Children attend primary school for the first 6 levels, then secondary schools for the next 4/5 levels. This is followed by either junior college for two-year courses or centralised institutes for three-year courses.
Based on results of the Primary School Leaving Examination (PSLE), Singapore's students undergo secondary education in either the Special (abolished in 2008), Express, Normal streams or the Integrated Programme (implemented in 2004). Both the Special and Express are four-year courses leading up to a Singapore-Cambridge General Certificate of Education (GCE) "Ordinary" or "O level" examination. The difference between Special and Express is that the former takes higher mother tongue, which can be used as a first language in exams instead of the subject "mother tongue" that Express students take. However, if some Express students can cope with higher mother tongue, they are allowed to used it as a first language in exams too.
The Normal stream is a four-year course leading up to a Singapore-Cambridge GCE "Normal" - "N" level examination, with the possibility of a 5th year followed by a Singapore-Cambridge GCE "Ordinary" - "O" level examination. It is split into "Normal (Academic)" and "Normal (Technical)" where in the latter students take subjects that are technical in nature, such as design and technology.
The Integrated Programme (IP) is a six-year programme offered to the top 10 percent of the cohort to pass through the O level exams, and go straight to the affiliated JC.
After the second year of a secondary school course, students are typically streamed into a wide range of course combinations, making the total number of subject they have to sit for in "O" level six to ten subjects. This includes science (Physics, Biology and Chemistry), humanities (Elective Geography/History, Pure Geography/History, Social Studies, Literature) and additional mathematics subject at a higher level, or "combined" subject modules.
Some schools have done away with the O level examination, and pupils only sit for the A level examination or the International Baccalaureate at the end of their sixth year (known as Year 6 or Junior College 2).
Co-curricular activities have become compulsory at the Secondary level, where all pupils must participate in at least one core CCA, and participation is graded together with other things like Leadership throughout the four years of Secondary education, in a scoring system. Competitions are organised so that students can have an objective towards to work, and in the case of musical groups, showcase talents.
Slovenia.
In Slovenia, a variety of high-school institutions for secondary education exists one can choose in accordance with his or her interests, abilities and beliefs. The majority of them are public and government-funded, although there are some diocesan upper secondary schools and a Waldorf upper secondary school, which are private and require tuition to be paid.
Upper secondary schools ("gimnazije") are the most elite and the most difficult high-school programmes, intended for the best students who wish to pursue university education. They are further divided into general upper secondary schools, classical upper secondary schools, technical upper secondary schools, upper secondary schools for arts, and upper secondary schools for business. They all last for four years and conclude with a compulsory leaving examination ("matura") that is a prerequisite for studying at universities. Their curricula include a wide range of subjects that should deliver a broad general knowledge.
Technical high schools last for four years and cover a wide range of disciplines. They end with a vocational leaving examination and allow pupils to study at vocational or professional colleges.
Vocational high schools come in two varieties: the dual and in school-based programme. For the former, the apprenticeship is provided by employers, while the practical training for the latter is offered in school. Both of them complete with a final examination. Students may continue their education in the two-year vocational-technical programme (colloquially known as 3+2 programme), which prepares them for vocational leaving exam if they want to pursue higher education.
The leaving exam course is a one-year programme, intended for vocational leaving exam graduates. After completing leaving exam course, they take the leaving examination, which makes the eligible for university education.
The vocational course is a one-year programme provided to upper secondary school students who, for various reasons, do not want to continue their education. It concludes with a final examinations, qualifying the applicants for a selected occupation.
Spain.
Secondary education in Spain is called "educación secundaria obligatoria" ("compulsory secondary education"), usually known as ESO, and lasts for four years (age 12 to 16). As its name indicates, every Spanish citizen must, by law, attend secondary education when they arrive at the defined age. The state is also committed to guaranteeing every student the possibility of attending it, and also at a state-run school (hence no tuition fees) if so demanded.
Turkey.
Secondary education includes all of the general, vocational and technical education institutions that provide at least four years of education after primary school. The system for being accepted to a high school changes almost every year. Sometimes private schools have different exams; sometimes there are three exams for three years; sometimes there's only one exam but it is calculated differently; sometimes they only look at a student's school grades. Secondary education aims to give students a good level of common knowledge, and to prepare them for higher education, for a vocation, for life and for business in line with their interests, skills and abilities. In the academic year 2001-2002 2.3 million students were enrolled and 134,800 teachers were employed in 6,000 education institutions.
General secondary education covers the education of children between 15-18 for at least four years after primary education. General secondary education includes high schools, foreign language teaching high schools, Anatolian high schools, high schools of science, Anatolian teacher training high schools, and Anatolian fine arts high schools.
Vocational and technical secondary education involves the institutions that both raise students as manpower in business and other professional areas, prepare them for higher education and meet the objectives of general secondary education. Vocational and technical secondary education includes technical education schools for boys, technical education schools for girls, trade and tourism schools, religious education schools, multi-program high schools, special education schools, private education schools and health education schools.
Secondary education is often referred to as high school education, since the schools are called lyceum ("lise").
United Kingdom.
In the United Kingdom secondary schools offer secondary education covering the later years of schooling. State secondary schools in England and Wales are classed as either (selective) grammar schools, (non-selective) comprehensive schools, city technology colleges or academies. Within Scotland, there are only two types of state-run schools, Roman Catholic or non-denominational. Most secondary schools in England and Wales are comprehensive schools. Grammar schools have been retained in some counties in England. Academies (previously known as city academies) are a new type of school introduced in 2000 by the New Labour government of Tony Blair. Independent secondary schools generally take pupils at age 13.
The table below lists the equivalent secondary school year systems used in the United Kingdom:
Private schools in England and Wales generally still refer to years 7-11 as 1st-5th Form, or alternatively privates schools refer to Year 7 as IIIrds (Thirds), Y8 as LIV (Lower Four), Y9 as UIV (Upper Four), Y10 as LV (Lower Fifth), Y11 as UV (Upper Fifth) and then Sixth-Form.
England, Wales and Northern Ireland.
Education in England, Wales, Northern Ireland"
In England, Wales and Northern Ireland, students usually transfer from primary school straight to secondary school at age 11. In a few parts of the UK there are middle schools for ages 9 to 13 (similar to American middle schools), and upper schools for ages 13–18. A handful of 8-12 middle schools, and 12-16 or 18 secondary schools still exist. These schools were first introduced in September 1968, and the number rose dramatically during the 1970s, but the number of such schools has declined since the mid-1980s.
It is uncommon, but sometimes secondary schools (particularly in South West Wales) can also be split into 'Upper' (ages 13–16) and 'Lower' secondary schools (ages 11–13).
Education is compulsory up until the end of year 13 (the last Friday in June in the academic year a person turns 18). Traditionally the five years of compulsory secondary schooling from ages 11 to 16 were known as "first year" through to "fifth year," (and still are in the private sector) but from September 1990 these years were renumbered Year 7 through to Year 11 (Year 8 to Year 12 in Northern Ireland) with the coming of the National Curriculum.
After Year 11 a student can opt to remain at school, transfer to a college, or to start an apprenticeship. Those who stay at school enter Years 12 and 13 (Years 13 and 14 in Northern Ireland). These years are traditionally known as the Sixth Form ("Lower Sixth" and "Upper Sixth"), and require students to specialise in three to five subjects for their A Levels. In ever-increasing numbers since the 1990s some students also undertake more vocational courses at college such as a BTEC or other such qualification.
This is an unusually specialised curriculum for this age group by international standards, and recently some moves have been made to increase the number of subjects studied. After attaining the relevant A Level qualifications the student can enter university.
Scotland.
In Scotland, students usually transfer from primary to secondary education at 12 years old, one year later than in the rest of the UK. The first and second years of secondary school (abbreviated to S1 and S2) continues the "Curriculum for Excellence" started in primary school. At age 14, students choose which subjects they wish to study with certain compulsory subjects such as English and Mathematics for S3 and S4. These are called Standard Grades, but some schools use Intermediates which take two years to complete with an exam at the end of S4. At age 16, after Standard Grades or Intermediates, some students leave to gain employment or attend further education colleges, but most students study for Highers, of which five are usually chosen. These take a year to complete. At age 17 some students decide to apply for university or stay on for 6th year, where other Highers are gained, or Advanced Highers are studied. Due to the nature of schooling in Scotland, undergraduate honours degree programmes are four years long as matriculation is normally at the completion of Highers in S5 (age 17), which compares with three years for the rest of the UK from age 18. As well as instruction through the English language education Gaelic medium education is also available throughout Scotland.
United States.
As part of education in the United States, the definition of secondary education varies among school districts but generally comprises grades 6, 7, 8, and 9 through 12; grade 5 is sometimes also included. Grades 9 through 12 is the most common grade structure for high school.
Vietnam.
Secondary education in Vietnam is optional under the law, however most children choose to receive secondary education, since the school fee is affordable for most working families. It is divided into two levels, secondary (grades 6-9) and tertiary (grades 10-12). Students have 12 compulsory subjects to learn, including but not limited to, Literature, Mathematics, Chemistry, Physics, Biology, History, Geography, and Foreign language. Starting from tertiary school (grade 10), each of the above-mentioned subject has two levels of study: Basic and Advanced. Students are divided into five groups:
The division into groups is deemed necessary, as until 2014, students who wishes to go to college had to take a University Entrance exam covering three subjects according to those listed groups. Since 2015, the Ministry of Education has started an experimental program to merge the Graduation Exam and University Entrance exam into one.
To continue tertiary level education, students must pass all end-of-year exams at the end of Grade 9. Students will graduate from high school if they pass the Graduation Test (used to cover six subjects). If not, they must wait for a year to retake the test.
An alternative for tertiary education is institutes of vocational training ("trung cấp nghề"). Students receive specialized training for a specific trade. After 2.5–3 years students are able to apply for jobs.

</doc>
<doc id="29500" url="https://en.wikipedia.org/wiki?curid=29500" title="Serotonin syndrome">
Serotonin syndrome

Serotonin syndrome is any of a group of symptoms which may be an indication of any number of potentially life-threatening drug interactions that may occur following therapeutic drug use, combination, overdose of particular drugs, or the recreational use of certain drugs. Serotonin syndrome is not an idiopathic drug reaction; it is a predictable consequence of excess serotonin on the CNS and/or peripheral nervous system. For this reason, some experts strongly prefer the terms serotonin toxicity or serotonin toxidrome which more accurately reflect that it is a form of poisoning.
Excessive levels of serotonin produce a spectrum of specific symptoms including cognitive, autonomic, and somatic effects. Symptoms may range from barely perceptible to fatal. Numerous drugs and drug combinations have been reported to produce serotonin syndrome, though the exact mechanism is not well understood in many instances.
Diagnosis includes observing symptoms and investigating patient history for causal factors (interacting drugs). The syndrome has a characteristic picture but can be mistaken for other illnesses in some people, particularly those with neuroleptic malignant syndrome, a condition characterized by excessive blockade of the dopamine receptors (usually the result of anti-nausea/vomiting or antipsychotic drugs), leading to movement disorders, changes in temperature, and other problems. No laboratory tests can currently confirm the diagnosis. Hence it is diagnosed based on symptoms, disease course (that is, the progression of the disease) and the exclusion of other possible causes of the presenting symptoms.
Treatment consists of discontinuing medications which may contribute and in moderate to severe cases administering a serotonin antagonist. An important adjunct treatment includes controlling agitation with benzodiazepine sedation. The high-profile case of Libby Zion, who is generally accepted to have died from serotonin syndrome, resulted in changes to graduate medical education in New York State.
Signs and symptoms.
Symptom onset is usually rapid, often occurring within minutes of elevated serotonin levels. Serotonin syndrome encompasses a wide range of clinical findings. Mild symptoms may consist of increased heart rate, shivering, sweating, dilated pupils, myoclonus (intermittent jerking or twitching), as well as overresponsive reflexes. However, many of these symptoms may be side effects of the drug or drug interaction causing excessive levels of serotonin; not an effect of elevated serotonin itself. Tremor is a common side effect of MDMA's action at dopamine, whereas hyperreflexia is symptomatic of exposure to serotonin agonists. Moderate intoxication includes additional abnormalities such as hyperactive bowel sounds, high blood pressure and hyperthermia; a temperature as high as . The overactive reflexes and clonus in moderate cases may be greater in the lower limbs than in the upper limbs. Mental changes include hypervigilance or insomnia and agitation. Severe symptoms include severe increases in heart rate and blood pressure that may lead to shock. Temperature may rise to above in life-threatening cases. Other abnormalities include metabolic acidosis, rhabdomyolysis, seizures, renal failure, and disseminated intravascular coagulation; these effects usually arising as a consequence of hyperthermia.
The symptoms are often described as a clinical triad of abnormalities:
Cause.
A large number of medications either alone in high dose or in combination can produce serotonin syndrome.
Many cases of serotonin toxicity occur in patients who have ingested drug combinations that synergistically increase synaptic serotonin. It may also occur as a symptom of overdose of a single serotonergic agent. The combination of MAOIs with precursors such as l-tryptophan or 5-htp pose a particularly acute risk of life-threatening serotonin syndrome. The case of combination of MAOIs with tryptamine agonists (commonly known as ayahuasca) can present similar dangers as their combination with precursors, but this phenomenon has been described in general terms as the "cheese effect". Many MAOIs irreversibly inhibit monoamine oxidase. It can take at least four weeks for this enzyme to be replaced by the body in the instance of irreversible inhibitors.
Many medications may have been incorrectly thought to cause serotonin syndrome. For example, some case reports have implicated atypical antipsychotics in serotonin syndrome, but it appears based on their pharmacology that they are unlikely to cause the syndrome. It has also been suggested that mirtazapine has no significant serotonergic effects, and is therefore not a dual action drug. Bupropion has also been suggested to cause serotonin syndrome, although as there is no evidence that it has any significant serotonergic activity, it is thought unlikely to produce the syndrome. In 2006 the United States Food and Drug Administration issued an alert suggesting that the combined use of SSRIs or SNRIs and triptan medications or sibutramine could potentially lead to severe cases of serotonin syndrome. This has been disputed by other researchers as none of the cases reported by the FDA met the Hunter criteria for serotonin syndrome. The condition has however occurred in surprising clinical situations, and because of phenotypic variations among individuals, it has been associated with unexpected drugs, including mirtazapine.
The relative risk and severity of serotonergic side effects and serotonin toxicity, with individual drugs and combinations, is complex. Serotonin syndrome has been reported in patients of all ages, including the elderly, children, and even newborn infants due to in utero exposure. The serotonergic toxicity of SSRIs increases with dose, but even in over-dose it is insufficient to cause fatalities from serotonin syndrome in healthy adults. Elevations of central nervous system serotonin will typically only reach potentially fatal levels when drugs with different mechanisms of action are mixed together. Various drugs, other than SSRIs, also have clinically significant potency as serotonin reuptake inhibitors, (e.g. tramadol, amphetamine, and MDMA) and are associated with severe cases of the syndrome.
Pathophysiology.
Serotonin is a neurotransmitter involved in multiple states including aggression, pain, sleep, appetite, anxiety, depression, migraine, and vomiting. In humans the effects of excess serotonin were first noted in 1960 in patients receiving a monoamine oxidase inhibitor (MAOI) and tryptophan. The syndrome is caused by increased serotonin in the central nervous system. It was originally suspected that agonism of 5-HT1A receptors in central grey nuclei and the medulla was responsible for the development of the syndrome. Further study has determined that overstimulation of primarily the 5-HT2A receptors appears to contribute substantially to the condition. The 5-HT1A receptor may still contribute through a pharmacodynamic interaction in which increased synaptic concentrations of a serotonin agonist saturate all receptor subtypes. Additionally, noradrenergic CNS hyperactivity may play a role as CNS norepinephrine concentrations are increased in serotonin syndrome and levels appear to correlate with the clinical outcome. Other neurotransmitters may also play a role; NMDA receptor antagonists and GABA have been suggested as affecting the development of the syndrome. Serotonin toxicity is more pronounced following supra-therapeutic doses and overdoses, and they merge in a continuum with the toxic effects of overdose.
Spectrum concept.
A postulated "spectrum concept" of serotonin toxicity emphasises the role that progressively increasing serotonin levels play in mediating the clinical picture as side effects merge into toxicity. The dose-effect relationship is the effects of progressive elevation of serotonin, either by raising the dose of one drug, or combining it with another serotonergic drug which may produce large elevations in serotonin levels.
Diagnosis.
There is no laboratory test for serotonin syndrome. Therefore, diagnosis is by symptom observation and investigation of the patient's history. Several diagnostic criteria have been proposed. The first rigorously evaluated criteria were introduced in 1991 by Harvey Sternbach, a professor of psychiatry at UCLA. Researchers in Australia later developed the Hunter Toxicity Criteria Decision Rules, which have better sensitivity and specificity, 84% and 97%, respectively, when compared with the gold standard of diagnosis by a medical toxicologist. As of 2007, Sternbach's criteria were still the most commonly used.
The most important symptoms for diagnosing serotonin syndrome are tremor, extreme aggressiveness, akathisia, or clonus (spontaneous, inducible and ocular). Physical examination of the patient should include assessment of deep-tendon reflexes and muscle rigidity, the dryness of the oral mucosa, the size and reactivity of the pupils, the intensity of bowel sounds, skin color, and the presence or absence of sweating. The patient's history also plays an important role in diagnosis, investigations should include inquries about the use of prescription and over-the-counter drugs, illicit substances, and dietary supplements, as all these agents have been implicated in the development of serotonin syndrome. To fulfill the Hunter Criteria, a patient must have taken a serotonergic agent and meet one of the following conditions:
Differential diagnosis.
Serotonin toxicity has a characteristic picture which is generally hard to confuse with other medical conditions, but in some situations it may go unrecognized because it may be mistaken for a viral illness, anxiety, neurological disorder, anticholinergic
poisoning, sympathomimetic toxicity, or worsening psychiatric condition. The condition most often confused with serotonin syndrome is neuroleptic malignant syndrome (NMS). The clinical features of neuroleptic malignant syndrome and serotonin syndrome share some features which can make differentiating them difficult. In both conditions, autonomic dysfunction and altered mental status develop. However, they are actually very different conditions with different underlying dysfunction (serotonin excess vs dopamine blockade). Both the time course and the clinical features of NMS differ significantly from those of serotonin toxicity. Serotonin toxicity has a rapid onset after the administration of a serotonergic drug and responds to serotonin blockade such as drugs like chlorpromazine and cyproheptadine. Dopamine receptor blockade (NMS) has a slow onset and typically evolves over several days after administration of a neuroleptic drug and responds to dopamine agonists such as bromocriptine.
Differential diagnosis may become difficult in patients recently exposed to both serotonergic drugs and neuroleptic drugs. Features that are classically present in NMS, that are useful for differentiating the two, are bradykinesia and extrapyramidal "lead pipe" rigidity, whereas serotonin syndrome causes hyperkinesia and clonus.
Management.
Management is based primarily on stopping the usage of the precipitating drugs, the administration of serotonin antagonists such as cyproheptadine, and supportive care including the control of agitation, the control of autonomic instability, and the control of hyperthermia. Additionally, those who ingest large doses of serotonergic agents may benefit from gastrointestinal decontamination with activated charcoal if it can be administered within an hour of overdose. The intensity of therapy depends on the severity of symptoms. If the symptoms are mild, treatment may only consist of discontinuation of the offending medication or medications, offering supportive measures, giving benzodiazepines for myoclonus, and waiting for the symptoms to resolve. Moderate cases should have all thermal and cardiorespiratory abnormalities corrected and can benefit from serotonin antagonists. The serotonin antagonist cyproheptadine is the recommended initial therapy, although there have been no controlled trials demonstrating its efficacy for serotonin syndrome. Despite the absence of controlled trials, there are a number of case reports detailing apparent improvement after people have been administered cyproheptadine. Animal experiments also suggest a benefit from serotonin antagonists. Cyproheptadine is only available as tablets and therefore can only be administered orally or via a nasogastric tube; it is unlikely to be effective in people administered activated charcoal and has limited use in severe cases. Additional pharmacological treatment for severe case includes administering atypical antipsychotic drugs with serotonin antagonist activity such as olanzapine. Critically ill people should receive the above therapies as well as sedation or neuromuscular paralysis. People who have autonomic instability such as low blood pressure require treatment with direct-acting sympathomimetics such as epinephrine, norepinephrine, or phenylephrine. Conversely, hypertension or tachycardia can be treated with short-acting antihypertensive drugs such as nitroprusside or esmolol; longer acting drugs such as propranolol should be avoided as they may lead to hypotension and shock. The cause of serotonin toxicity or accumulation is an important factor in determining the course of treatment. Serotonin is catabolized by monoamine oxidase in the presence of oxygen, so if care is taken to prevent an unsafe spike in body temperature or metabolic acidosis, oxygenation will assist in dispatching the excess serotonin. The same principle applies to alcohol intoxication. In cases of serotonin syndrome caused by monoamine oxidase inhibitors oxygenation will not help to dispatch serotonin. In such instances hydration is the main concern until the enzyme is regenerated.
Agitation.
Specific treatment for some symptoms may be required. One of the most important treatments is the control of agitation due to the extreme possibility of injury to the person themselves or caregivers, benzodiazepines should be administered at first sign of this. Physical restraints are not recommended for agitation or delirium as they may contribute to mortality by enforcing isometric muscle contractions that are associated with severe lactic acidosis and hyperthermia. If physical restraints are necessary for severe agitation they must be rapidly replaced with pharmacological sedation. The agitation can cause a large amount of muscle breakdown. This breakdown can cause severe damage to the kidneys through a condition called rhabdomyolysis.
Hyperthermia.
Treatment for hyperthermia includes reducing muscle overactivity via sedation with a benzodiazepine. More severe cases may require muscular paralysis with vecuronium, intubation, and artificial ventilation. Succinylcholine is not recommended for muscular paralysis as it may increase the risk of cardiac dysrhythmia from hyperkalemia associated with rhabdomyolysis. Antipyretic agents are not recommended as the increase in body temperature is due to muscular activity, not a hypothalamic temperature set point abnormality.
Prognosis.
Upon the discontinuation of serotonergic drugs, most cases of serotonin syndrome resolve within 24 hours, although in some cases delirium may persist for a number of days. Symptoms typically persist for a longer time frame in patients taking drugs which have a long elimination half-life, active metabolites, or a protracted duration of action.
Cases have reported muscle pain and weakness persisting for months, and antidepressant discontinuation may contribute to ongoing features. Following appropriate medical management, serotonin syndrome is generally associated with a favorable prognosis.
Epidemiology.
Epidemiological studies of serotonin syndrome are difficult as many physicians are unaware of the diagnosis or they may miss the syndrome due to its variable manifestations. In 1998 a survey conducted in England found that 85% of the general practitioners that had prescribed the antidepressant nefazodone were unaware of serotonin syndrome. The incidence may be increasing as a larger number of pro-serotonergic drugs (drugs which increase serotonin levels) are now being used in clinical practice. One postmarketing surveillance study identified an incidence of 0.4 cases per 1000 patient-months for patients who were taking nefazodone. Additionally, around 14 to 16 percent of persons who overdose on SSRIs are thought to develop serotonin syndrome.
Notable cases.
The most widely recognized example of serotonin syndrome was the death of Libby Zion in 1984. Zion was a freshman at Bennington College at her death on March 5, 1984, at age 18. She died within 8 hours of her emergency admission to the New York Hospital Cornell Medical Center. She had an ongoing history of depression, and came to the Manhattan hospital on the evening of March 4, 1984, with a fever, agitation and "strange jerking motions" of her body. She also seemed disoriented at times. The emergency room physicians were unable to diagnose her condition definitively, but admitted her for hydration and observation. Her death was caused by a combination of pethidine and phenelzine. A medical intern prescribed the pethidine. The case had an impact on graduate medical education and residency work hours. Limits were set on working hours for medical postgraduates, commonly referred to as interns or residents, in hospital training programs, and they also now require closer senior physician supervision.

</doc>
<doc id="29501" url="https://en.wikipedia.org/wiki?curid=29501" title="Sustainable development">
Sustainable development

Sustainable development (SD) is defined in the Brundtland Report as “development that meets the needs and aspirations of the present without compromising the ability of future generations to meet their own needs”. Thus, sustainable development is the organizing principle for sustaining finite resources necessary to provide for the needs of future generations of life on the planet. It is a process that envisions a desirable future state for human societies in which living conditions and resource-use continue to meet human needs without undermining the "integrity, stability and beauty" of natural biotic systems.
Definition.
Sustainability can be defined as the practice of reserving resources for future generation without any harm to the nature and other components of it . Sustainable development ties together concern for the carrying capacity of natural systems with the social, political, and economic challenges faced by humanity. Sustainability science is the study of the concepts of sustainable development and environmental science. There is an additional focus on the present generations' responsibility to regenerate, maintain and improve planetary resources for use by future generations.
Dimensions.
Sustainable development has been described in terms of three dimensions, domains or pillars. In the three-dimension model, these are seen as "economic, environmental and social" or "ecology, economy and equity"; this has been expanded by some authors to include a fourth pillar of culture, institutions or governance.
Ecology.
The ecological sustainability of human settlements is part of the relationship between humans and their natural, social and built environments. Also termed human ecology, this broadens the focus of sustainable development to include the domain of human health. Fundamental human needs such as the availability and quality of air, water, food and shelter are also the ecological foundations for sustainable development; addressing public health risk through investments in ecosystem services can be a powerful and transformative force for sustainable development which, in this sense, extends to all species.
Environment.
Environmental Economics.
Our total environment includes not just the biosphere of earth, air, and water, but also our interaction with these things, our relationship with nature, and what all humans have created as their surroundings.
Economics, Ecology and Ethics.
Environmental sustainability concerns the natural environment and how it endures and remains diverse and productive. Since natural resources are derived from the environment, the state of air, water, and the climate are of particular concern. The IPCC Fifth Assessment Report outlines current knowledge about scientific, technical and socio-economic information concerning climate change, and lists options for adaptation and mitigation.
Environmental sustainability requires society to design activities to meet human needs while preserving the life support systems of the planet. This, for example, entails using water sustainably, utilizing renewable energy, and sustainable material supplies (e.g. harvesting wood from forests at a rate that maintains the biomass and biodiversity).
An unsustainable situation occurs when natural capital (the sum total of nature's resources) is used up faster than it can be replenished. Sustainability requires that human activity only uses nature's resources at a rate at which they can be replenished naturally. Inherently the concept of sustainable development is intertwined with the concept of carrying capacity. Theoretically, the long-term result of environmental degradation is the inability to sustain human life. Such degradation on a global scale should imply an increase in human death rate until population falls to what the degraded environment can support. If the degradation continues beyond a certain tipping point or critical threshold it would lead to eventual extinction for humanity.
Integral elements for a sustainable development are research and innovation activities. A telling example is the European environmental research and innovation policy, which aims at defining and implementing a transformative agenda to greening the economy and the society as a whole so to achieve a truly sustainable development. Research and innovation in Europe is financially supported by the programme Horizon 2020, which is also open to participation worldwide. A promising direction towards sustainable development is to design systems that are flexible and reversible 
Agriculture.
Sustainable agriculture consists of environmentally-friendly methods of farming that allow the production of crops or livestock without damage to human or natural systems. It involves preventing adverse effects to soil, water, biodiversity, surrounding or downstream resources—as well as to those working or living on the farm or in neighboring areas. The concept of sustainable agriculture extends intergenerationally, passing on a conserved or improved natural resource, biotic, and economic base rather than one which has been depleted or polluted. Elements of sustainable agriculture include permaculture, agroforestry, mixed farming, multiple cropping, and crop rotation.
Permaculture.
This is a system of agricultural and social design principles centered around simulating or directly utilizing the patterns and features observed in natural ecosystems.
Agroforestry.
This is a land use management system in which trees or shrubs are grown around or among crops or pastureland.
Mixed Farming.
This is an agrarian system that mixes arable farming with the raising of livestock at the same time.
Multiple Cropping.
This is the practice of growing two or more crops on the same piece of land during a single growing season
Crop Rotation.
This is the practice of growing a series of dissimilar or different types of crops in the same area in sequenced seasons
Standards.
Numerous sustainability standards and certification systems have been established in recent years, offering consumer choices for sustainable agriculture practices. These include Organic certification, Rainforest Alliance, Fair Trade, UTZ Certified, Bird Friendly, and the Common Code for the Coffee Community (4C).
Energy.
Sustainable energy is clean and can be used over a long period of time. Unlike fossil fuels that most countries are using, renewable energy only produces little or even no pollution.
The most common types of renewable energy in US are hydroelectric, solar and wind energy. Solar energy is commonly used on public parking meters, street lights and the roof of buildings. Wind power has expanded quickly, it's share of worldwide electricity usage at the end of 2014 was 3.1%. Most of California’s fossil fuel infrastructures are sited in or near low-income communities, and have traditionally suffered the most from California’s fossil fuel energy system. These communities are historically left out during the decision-making process, and often end up with dirty power plants and other dirty energy projects that poison the air and harm the area. These toxicants are major contributors to health problems in the communities. As renewable energy becomes more common, fossil fuel infrastructures are replaced by renewables, providing better social equity to these communities.
Overall, and in the long run, sustainable development in the field of energy is also deemed to contribute to economic sustainability and national security of communities, thus being increasingly encouraged through investment policies.
Technology.
One of the core concepts in sustainable development is that technology can be used to assist people meet their developmental needs. Technology to meet these sustainable development needs is often referred to as appropriate technology, which is an ideological movement (and its manifestations) originally articulated as intermediate technology by the economist Dr. Ernst Friedrich "Fritz" Schumacher in his influential work, "Small is Beautiful." and now covers a wide range of technologies. Both Schumacher and many modern-day proponents of appropriate technology also emphasize the technology as people-centered. Today appropriate technology is often developed using open source principles, which have led to open-source appropriate technology (OSAT) and thus many of the plans of the technology can be freely found on the Internet. OSAT has been proposed as a new model of enabling innovation for sustainable development.
Transportation.
Transportation is a large contributor to greenhouse gas emissions. It is said that one-third of all gasses produced are due to transportation. Some western countries are making transportation more sustainable in both long-term and short-term implementations. An example is the modifications in available transportation in Freiburg, Germany. The city has implemented extensive methods of public transportation, cycling, and walking, along with large areas where cars are not allowed.
Since many western countries are highly automobile-orientated areas, the main transit that people use is personal vehicles. About 80% of their travel involves cars. Therefore, California, deep in the automobile-oriented west, is one of the highest greenhouse gases emitters in the country. The federal government has to come up with some plans to reduce the total number of vehicle trips in order to lower greenhouse gases emission. Such as:
Other states and nations have built efforts to translate knowledge in behavioral economics into evidence-based sustainable transportation policies.
Economics.
It has been suggested that because of rural poverty and overexploitation, environmental resources should be treated as important economic assets, called natural capital. Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption.
According to ecological economist Malte Faber, ecological economics is defined by its focus on nature, justice, and time. Issues of intergenerational equity, irreversibility of environmental change, uncertainty of long-term outcomes, and sustainable development guide ecological economic analysis and valuation.
As early as the 1970s, the concept of sustainability was used to describe an economy "in equilibrium with basic ecological support systems." Scientists in many fields have highlighted "The Limits to Growth", and economists have presented alternatives, for example a 'steady-state economy'; to address concerns over the impacts of expanding human development on the planet. In 1987 the economist Edward Barbier published the study "The Concept of Sustainable Economic Development", where he recognized that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other.
A World Bank study from 1999 concluded that based on the theory of genuine savings, policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental.
A study from 2001 noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule steady state. The study, "Interpreting Sustainability in Economic Terms", found three pillars of sustainable development, interlinkage, intergenerational equity, and dynamic efficiency.
A meta review in 2002 looked at environmental and economic valuations and found a lack of "sustainability policies". A study in 2004 asked if we consume too much. A study concluded in 2007 that knowledge, manufactured and human capital(health and education) has not compensated for the degradation of natural capital in many parts of the world. It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics. A meta review in 2009 identified conditions for a strong case to act on climate change, and called for more work to fully account of the relevant economics and how it affects human welfare.
According to John Baden "the improvement of environment quality depends on the market economy and the existence of legitimate and protected property rights." They enable the effective practice of personal responsibility and the development of mechanisms to protect the environment. The State can in this context "create conditions which encourage the people to save the environment."
Business.
The most broadly accepted criterion for corporate sustainability constitutes a firm’s efficient use of natural capital. This eco-efficiency is usually calculated as the economic value added by a firm in relation to its aggregated ecological impact. This idea has been popularised by the World Business Council for Sustainable Development (WBCSD) under the following definition:
"Eco-efficiency is achieved by the delivery of competitively priced goods and services that satisfy human needs and bring quality of life, while progressively reducing ecological impacts and resource intensity throughout the life-cycle to a level at least in line with the earth’s carrying capacity." (DeSimone and Popoff, 1997: 47)
Similar to the eco-efficiency concept but so far less explored is the second criterion for corporate sustainability. Socio-efficiency describes the relation between a firm's value added and its social impact. Whereas, it can be assumed that most corporate impacts on the environment are negative (apart from rare exceptions such as the planting of trees) this is not true for social impacts. These can be either positive (e.g. corporate giving, creation of employment) or negative (e.g. work accidents, mobbing of employees, human rights abuses). Depending on the type of impact socio-efficiency thus either tries to minimize negative social impacts (i.e. accidents per value added) or maximise positive social impacts (i.e. donations per value added) in relation to the value added.
Both eco-efficiency and socio-efficiency are concerned primarily with increasing economic sustainability. In this process they instrumentalize both natural and social capital aiming to benefit from win-win situations. However, as Dyllick and Hockerts point out the business case alone will not be sufficient to realise sustainable development. They point towards eco-effectiveness, socio-effectiveness, sufficiency, and eco-equity as four criteria that need to be met if sustainable development is to be reached.
Income.
At the present time, sustainable development as well as solidarity or Catholic social teaching can impact reduce the poverty. Because over many thousands of years the ‘stronger’ (economically or physically) used to defeat/eliminate the weaker, nowadays, no matter what we call the reason for this decision – within Catholic social teaching, social solidarity, and sustainable development – the stronger helps the weaker. This aid may take the form of in-kind or material, refer to the present or the future. ‘The Stronger’, should offer real help and not, as demonstrated by the frequent experience – strive for the elimination or annihilation of another entity. Sustainable development reduce poverty through economic (among other things, a balanced budget), environmental (living conditions) and also social (including equality of income) dimensions.
Architecture.
In sustainable architecture the recent movements of New Urbanism and New Classical architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and International Style architecture, as well as opposing to solitary housing estates and suburban sprawl, with long commuting distances and large ecological footprints. Both trends started in the 1980s. (It should be noted that sustainable architecture is predominantly relevant to the economics domain while architectural landscaping pertains more to the ecological domain.)
Politics.
A study concluded that social indicators and, therefore, sustainable development indicators, are scientific constructs whose principal objective is to inform public policy-making. The International Institute for Sustainable Development has similarly developed a political policy framework, linked to a sustainability index for establishing measurable entities and metrics. The framework consists of six core areas, international trade and investment, economic policy, climate change and energy, measurement and assessment, natural resource management, and the role of communication technologies in sustainable development.
The United Nations Global Compact Cities Programme has defined sustainable political development is a way that broadens the usual definition beyond states and governance. The political is defined as the domain of practices and meanings associated with basic issues of social power as they pertain to the organisation, authorisation, legitimation and regulation of a social life held in common. This definition is in accord with the view that political change is important for responding to economic, ecological and cultural challenges. It also means that the politics of economic change can be addressed. They have listed seven subdomains of the domain of politics:
This accords with the Brundtland Commission emphasis on development that is guided by human rights principles (see above).
Culture.
Working with a different emphasis, some researchers and institutions have pointed out that a fourth dimension should be added to the dimensions of sustainable development, since the triple-bottom-line dimensions of economic, environmental and social do not seem to be enough to reflect the complexity of contemporary society. In this context, the Agenda 21 for culture and the United Cities and Local Governments (UCLG) Executive Bureau lead the preparation of the policy statement "Culture: Fourth Pillar of Sustainable Development", passed on 17 November 2010, in the framework of the World Summit of Local and Regional Leaders – 3rd World Congress of UCLG, held in Mexico City. although some which still argue that economics is primary, and culture and politics should be included in 'the social'. This document inaugurates a new perspective and points to the relation between culture and sustainable development through a dual approach: developing a solid cultural policy and advocating a cultural dimension in all public policies. The Circles of Sustainability approach distinguishes the four domains of economic, ecological, political and cultural sustainability.
Other organizations have also supported the idea of a fourth domain of sustainable development. The Network of Excellence "Sustainable Development in a Diverse World", sponsored by the European Union, integrates multidisciplinary capacities and interprets cultural diversity as a key element of a new strategy for sustainable development. The Fourth Pillar of Sustainable Development Theory has been referenced by executive director of IMI Institute at UNESCO Vito Di Bari in his manifesto of art and architectural movement Neo-Futurism, whose name was inspired by the 1987 United Nations’ report Our Common Future. The Circles of Sustainability approach used by Metropolis defines the (fourth) cultural domain as practices, discourses, and material expressions, which, over time, express continuities and discontinuities of social meaning.
Themes.
Progress.
The United Nations Conference on Sustainable Development (UNCSD), also known as Rio 2012, Rio+20, or Earth Summit 2012, was the third international conference on sustainable development, which aimed at reconciling the economic and environmental goals of the global community. An outcome of this conference was the development of the Sustainable Development Goals that aim to promote sustainable progress and eliminate inequalities around the world. However, few nations met the World Wide Fund for Nature's definition of sustainable development criteria established in 2006. Although some nations are more developed than others, all nations are constantly developing because each nation struggles with perpetuating disparities, inequalities and unequal access to fundamental rights and freedoms.
Measurement.
In 2007 a report for the U.S. Environmental Protection Agency stated: "While much discussion and effort has gone into sustainability indicators, none of the resulting systems clearly tells us whether our society is sustainable. At best, they can tell us that we are heading in the wrong direction, or that our current activities are not sustainable. More often, they simply draw our attention to the existence of problems, doing little to tell us the origin of those problems and nothing to tell us how to solve them."
Nevertheless, a majority of authors assume that a set of well defined and harmonised indicators is the only way to make sustainability tangible. Those indicators are expected to be identified and adjusted through empirical observations (trial and error).
The most common critiques are related to issues like data quality, comparability, objective function and the necessary resources.
However a more general criticism is coming from the project management community: How can a sustainable development be achieved at global level if we cannot monitor it in any single project?
The Cuban-born researcher and entrepreneur Sonia Bueno suggests an alternative approach that is based upon the integral, long-term cost-benefit relationship as a measure and monitoring tool for the sustainability of every project, activity or enterprise. Furthermore, this concept aims to be a practical guideline towards sustainable development following the principle of conservation and increment of value rather than restricting the consumption of resources.
Reasonable qualifications of sustainability are seen U.S. Green Building Council’s (USGBC) Leadership in Energy and Environmental Design (LEED). This design incorporates some ecological, economic, and social elements. The goals presented by LEED design goals are sustainable sites, water efficiency, energy and atmospheric emission reduction, material and resources efficiency, and indoor environmental quality. Although amount of structures for sustainability development is many, these qualification has become a standard for sustainable building.
Recent research efforts created also the SDEWES Index to benchmark the performance of cities across aspects that are related to energy, water and environment systems. The SDEWES Index consists of 7 dimensions, 35 indicators, and close to 20 sub-indicators. It is currently applied to 58 cities.
Natural capital.
The sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible. Daly (1992), for example, points to the fact that natural capital can not necessarily be substituted by economic capital. While it is possible that we can find ways to replace some natural resources, it is much more unlikely that they will ever be able to replace eco-system services, such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest. In fact natural capital, social capital and economic capital are often complementarities. A further obstacle to substitutability lies also in the multi-functionality of many natural resources. Forests, for example, not only provide the raw material for paper (which can be substituted quite easily), but they also maintain biodiversity, regulate water flow, and absorb CO2.
Another problem of natural and social capital deterioration lies in their partial irreversibility. The loss in biodiversity, for example, is often definite. The same can be true for cultural diversity. For example, with globalisation advancing quickly the number of indigenous languages is dropping at alarming rates. Moreover, the depletion of natural and social capital may have non-linear consequences. Consumption of natural and social capital may have no observable impact until a certain threshold is reached. A lake can, for example, absorb nutrients for a long time while actually increasing its productivity. However, once a certain level of algae is reached lack of oxygen causes the lake’s ecosystem to break down suddenly.
Business-as-usual.
If the degradation of natural and social capital has such important consequence the question arises why action is not taken more systematically to alleviate it. Cohen and Winn point to four types of market failure as possible explanations: First, while the benefits of natural or social capital depletion can usually be privatized, the costs are often externalized (i.e. they are borne not by the party responsible but by society in general). Second, natural capital is often undervalued by society since we are not fully aware of the real cost of the depletion of natural capital. Information asymmetry is a third reason—often the link between cause and effect is obscured, making it difficult for actors to make informed choices. Cohen and Winn close with the realization that contrary to economic theory many firms are not perfect optimizers. They postulate that firms often do not optimize resource allocation because they are caught in a "business as usual" mentality.
Historical Development.
Sustainable development has its roots in ideas about sustainable forest management which were developed in Europe during the seventeenth and eighteenth centuries. In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued that "sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over-exploitation of natural resources" in his 1662 essay "Sylva". In 1713 Hans Carl von Carlowitz, a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published "Sylvicultura oeconomica", a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert, von Carlowitz developed the concept of managing forests for sustained yield. His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig, leading in turn to the development of a science of forestry. This in term influenced people like Gifford Pinchot, first head of the US Forest Service, whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s.
Following the publication of Rachel Carson's "Silent Spring" in 1962, the developing environmental movement drew attention to the relationship between economic growth and development and environmental degradation. Kenneth E. Boulding in his influential 1966 essay "The Economics of the Coming Spaceship Earth" identified the need for the economic system to fit itself to the ecological system with its limited pools of resources. One of the first uses of the term sustainable in the contemporary sense was by the Club of Rome in 1972 in its classic report on the "Limits to Growth", written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology. Describing the desirable "state of global equilibrium", the authors wrote: "We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people."
In 1980 the International Union for the Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority. Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged. In 1987 the United Nations World Commission on Environment and Development released the report "Our Common Future", commonly called the Brundtland Report. The report included what is now one of the most widely recognized definitions of sustainable development.
In 1992, the UN Conference on Environment and Development published in 1992 the Earth Charter, which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognizes these interdependent pillars. It emphasises that in sustainable development everyone is a user and provider of information. It stresses the need to change from old sector-centered ways of doing business to new approaches that involve cross-sectoral co-ordination and the integration of environmental and social concerns into all development processes. Furthermore, Agenda 21 emphasises that broad public participation in decision making is a fundamental prerequisite for achieving sustainable development. Under the principles of the United Nations Charter the Millennium Declaration identified principles and treaties on sustainable development, including economic development, social development and environmental protection. Broadly defined, sustainable development is a systems approach to growth and development and to manage natural, produced, and social capital for the welfare of their own and future generations. The term sustainable development as used by the United Nations incorporates both issues associated with land development and broader issues of human development such as education, public health, and standard of living.
A 2013 study concluded that sustainability reporting should be reframed through the lens of four interconnected domains: ecology, economics, politics and culture.
Criticism of the concept.
It has been argued that since the 1960s, the concept of sustainable development has changed from 'conservation management' to 'economic development', whereby the original meaning of the concept has been stretched somewhat.
In the 1960s, the international community realised that many African countries needed national plans to safeguard wildlife habitats, and that rural areas had to confront the limits imposed by soil, climate and water availability. This was a strategy of conservation management. In the 70s, however, the focus shifted to the broader issues of the provisioning of basic human needs, community participation as well as appropriate technology use throughout the developing countries (and not just in Africa). This was a strategy of economic development, and the strategy was carried even further by the Brundtland Report when the issues went from regional to international in scope and application. In effect, the conservationists were crowded out and superseded by the developers.
But shifting the focus of sustainable development from conservation to development has had the imperceptible effect of stretching the original forest management term of sustainable yield from the use of renewable resources only (like forestry), to now also accounting for the use of non-renewable resources (like minerals). This stretching of the term has been questioned. Thus, environmental economist Kerry Turner has argued that literally, there can be no such thing as overall 'sustainable development' in an industrialised world economy that remains heavily dependent on the extraction of Earth's finite stock of exhaustible mineral resources:
In effect, it has been argued that the Industrial Revolution as a whole is unsustainable. Even worse, the entire universe appears to be
unsustainable. 

</doc>
<doc id="29507" url="https://en.wikipedia.org/wiki?curid=29507" title="Scientific American">
Scientific American

Scientific American (informally abbreviated SciAm) is an American popular science magazine. Many famous scientists, including Albert Einstein, have contributed articles in the past 170 years. It is the oldest continuously published monthly magazine in the United States.
History.
"Scientific American" was founded by inventor and publisher Rufus M. Porter in 1845 as a four-page weekly newspaper. Throughout its early years, much emphasis was placed on reports of what was going on at the U.S. Patent Office. It also reported on a broad range of inventions including perpetual motion machines, an 1860 device for buoying vessels by Abraham Lincoln, and the universal joint which now can be found in nearly every automobile manufactured. Current issues include a "this date in history" section, featuring excerpts from articles originally published 50, 100, and 150 years earlier. Topics include humorous incidents, wrong-headed theories, and noteworthy advances in the history of science and technology.
Porter sold the publication to Alfred Ely Beach and Orson Desaix Munn I a mere ten months after founding it. Until 1948, it remained owned by Munn & Company. Under Orson Desaix Munn III, grandson of Orson I, it had evolved into something of a "workbench" publication, similar to the twentieth century incarnation of "Popular Science".
In the years after World War II, the magazine fell into decline. In 1948, three partners who were planning on starting a new popular science magazine, to be called "The Sciences", purchased the assets of the old "Scientific American" instead and put its name on the designs they had created for their new magazine. Thus the partnerspublisher Gerard Piel, editor Dennis Flanagan, and general manager Donald H. Miller, Jr.essentially created a new magazine. Miller retired in 1979, Flanagan and Piel in 1984, when Gerard Piel's son Jonathan became president and editor; circulation had grown fifteen-fold since 1948. In 1986, it was sold to the Holtzbrinck group of Germany, which has owned it since.
In the fall of 2008, "Scientific American" was put under the control of Nature Publishing Group, a division of Holtzbrinck.
Donald Miller died in December 1998, Gerard Piel in September 2004 and Dennis Flanagan in January 2005. Mariette DiChristina is the current editor-in-chief, after John Rennie stepped down in June 2009.
International editions.
"Scientific American" published its first foreign edition in 1890, the Spanish-language "La America Cientifica". Publication was suspended in 1905, and another 63 years would pass before another foreign-language edition appeared: In 1968, an Italian edition, "Le Scienze", was launched, and a Japanese edition, "Nikkei Science" (日経サイエンス), followed three years later. A new Spanish edition, "Investigación y Ciencia" was launched in Spain in 1976, followed by a French edition, "Pour la Science", in France in 1977, and a German edition, "Spektrum der Wissenschaft", in Germany in 1978. A Russian edition "V Mire Nauki" was launched in the Soviet Union in 1983, and continues in the present-day Russian Federation. "Kexue" (科学, "Science" in Chinese), a simplified Chinese edition launched in 1979, was the first Western magazine published in the People's Republic of China. Founded in Chongqing, the simplified Chinese magazine was transferred to Beijing in 2001. Later in 2005, a newer edition, "Global Science" (环球科学), was published instead of "Kexue", which shut down due to financial problems. A traditional Chinese edition, known as 科學人 ("Scientist" in Chinese), was introduced to Taiwan in 2002, and has been developed to the best popular science magazine in Taiwan. The Hungarian edition "Tudomány" existed between 1984 and 1992. In 1986, an Arabic-edition, "Oloom magazine" (مجلة العلوم), was published. In 2002, a Portuguese edition was launched in Brazil.
Today, "Scientific American" publishes 18 foreign-language editions around the globe: Arabic, Brazilian Portuguese, Simplified Chinese, Traditional Chinese, Czech, Dutch, French, German, Greek, Hebrew, Italian, Japanese, Korean, Lithuanian (discontinued after 15 issues), Polish, Romanian, Russian, and Spanish.
From 1902 to 1911, "Scientific American" supervised the publication of the "Encyclopedia Americana", which during some of that period was known as "The Americana".
First issue.
It originally styled itself "The Advocate of Industry and Enterprise" and "Journal of Mechanical and other Improvements". On the front page of the first issue was the engraving of "Improved Rail-Road Cars". The masthead had a commentary as follows:
The commentary under the illustration gives the flavor of its style at the time:
Also in the first issue is commentary on Signor Muzio Muzzi's proposed device for aerial navigation.
Scientific American 50 award.
The Scientific American 50 award was started in 2002 to recognize contributions to science and technology during the magazine's previous year. The magazine's 50 awards cover many categories including agriculture, communications, defence, environment, and medical diagnostics. The complete list of each year's winners appear in the December issue of the magazine, as well as on the magazine's web site.
Website.
In March 1996, "Scientific American" launched its own website that includes articles from current and past issues, online-only features, daily news, weird science, special reports, trivia, "Scidoku" and more.
Columns.
Notable features have included:
Television.
From 1990 to 2005 "Scientific American" also produced a television program on PBS called "Scientific American Frontiers".
Books.
From 1983 to 1997, "Scientific American" has produced an encyclopedia set of volumes from their publishing division, the Scientific American Library. These books were not sold in retail stores, but as a Book of the Month club selection priced from $24.95 to $32.95. Topics covered dozens of areas of scientific knowledge and included in-depth essays on: The Animal Mind; Atmosphere, Climate, and Change; Beyond the Third Dimension; Cosmic Clouds; Cycles of Life • Civilization and the Biosphere; The Discovery Of Subatomic Particles; Diversity and the Tropical Rain Forest; Earthquakes and Geological Discovery; Exploring Planetary Worlds; Gravity’s Fatal Attraction; Fire; Fossils And The History Of Life; From Quarks to the Cosmos; A Guided Tour Of The Living Cell; Human Diversity; Perception; The Solar System; Sun and Earth; The Science of Words (Linguistics); The Science Of Musical Sound; The Second Law (of Thermodynamics); Stars; Supercomputing and the Transformation of Science.
Scientific and political debate.
In April 1950, the U.S. Atomic Energy Commission ordered "Scientific American" to cease publication of an issue containing an article by Hans Bethe that appeared to reveal classified information about the thermonuclear hydrogen bomb. Subsequent review of the material determined that the AEC had overreacted. The incident was important for the "new" "Scientific American"'s history, as the AEC's decision to burn 3000 copies of an early press-run of the magazine containing the offending material appeared to be "book burning in a free society" when publisher Gerard Piel leaked the incident to the press.
In its January 2002 issue, "Scientific American" published a series of criticisms of the Bjørn Lomborg book "The Skeptical Environmentalist". Cato Institute fellow Patrick J. Michaels said the attacks came because the book "threatens billions of taxpayer dollars that go into the global change kitty every year." Journalist Ronald Bailey called the criticism "disturbing" and "dishonest", writing, "The subhead of the review section, 'Science defends itself against "The Skeptical Environmentalist",' gives the show away: Religious and political views need to defend themselves against criticism, but science is supposed to be a process for determining the facts."
The May 2007 issue featured a column by Michael Shermer calling for a United States pullout from the Iraq War. In response, "Wall Street Journal" online columnist James Taranto jokingly called "Scientific American" "a liberal political magazine".
The publisher was criticized in 2009 when it notified collegiate libraries that subscribe to the journal that yearly subscription prices would increase by nearly 500% for print and 50% for online access to $1500 yearly.
Controversy.
In 2013, Dr. Danielle Lee, a female scientist who blogs at "Scientific American", was called a "whore" in an email by an editor at the science website "Biology Online" after refusing to write professional content without compensation. When Lee, outraged about the email, wrote a rebuttal on her "Scientific American" blog, the editor-in-chief of "Scientific American", Mariette DiChristina, removed the post, sparking an outrage by supporters of Lee. While DiChristina cited legal reasons for removing the blog, others criticized her for censoring Lee. The editor at Biology Online was fired after the incident.
The controversy widened in the ensuing days. The magazine's blog editor, Bora Zivkovic, was the subject of allegations of sexual harassment by another blogger, Monica Byrne. Although the alleged incident had occurred about a year earlier, editor Mariette DiChristina informed readers that the incident had been investigated and resolved to Ms. Byrne's satisfaction. However, the incident involving Dr. Lee had prompted Ms. Byrne to reveal the identity of Zivkovic, following the latter's support of Dr. Lee. Zivkovic responded on Twitter and his own blog, admitting the incident with Ms. Byrne had taken place. His blog post apologized to Ms. Byrne, and referred to the incident as "singular", stating that his behavior was not "engaged in before or since."
Due to the allegations, Zivkovic resigned from the board of Science Online, the popular science blogging conference that he helped establish. Following Zivkovic's admission, several prominent female bloggers, including other bloggers for the magazine, wrote their own accounts that contradicted Zivkovic's assertions, alleging additional incidents of sexual harassment. A day after these new revelations, Zivkovic resigned his position at "Scientific American", according to a press release from the magazine.

</doc>
