<doc id="26750" url="https://en.wikipedia.org/wiki?curid=26750" title="Sri Lanka">
Sri Lanka

Sri Lanka ( or ; Sinhalese: ශ්‍රී ලංකා ', Tamil: இலங்கை "Ilaṅkai"), officially the Democratic Socialist Republic of Sri Lanka and known from the beginning of British colonial rule until 1972 as Ceylon'" (), is an island country in South Asia near south-east India.
Sri Lanka has maritime borders with India to the northwest and the Maldives to the southwest. Its documented history spans 3,000 years, with evidence of pre-historic human settlements dating back to at least 125,000 years. Its geographic location and deep harbours made it of great strategic importance from the time of the ancient Silk Road through to World War II.
A diverse and multicultural country, Sri Lanka is home to many religions, ethnic groups, and languages. In addition to the majority Sinhalese, it is home to large groups of Sri Lankan and Indian Tamils, Moors, Burghers, Malays, Kaffirs and the aboriginal Vedda. Sri Lanka has a rich Buddhist heritage, and the first known Buddhist writings of Sri Lanka, the Pāli Canon, dates back to the Fourth Buddhist council in 29 BC. Sri Lanka's recent history has been marred by a thirty-year civil war which decisively ended when Sri Lankan military defeated Liberation Tigers of Tamil Eelam in 2009.
Sri Lanka is a republic and a unitary state governed by a Semi-presidential system. The legislative capital, Sri Jayawardenepura Kotte, is a suburb of the commercial capital and largest city, Colombo.
Sri Lanka has had a long history of international engagement, as a founding member of the South Asian Association for Regional Cooperation (SAARC), and a member of the United Nations, the Commonwealth of Nations, the G77, and the Non-Aligned Movement. Along with Maldives it is one of the two countries in South Asia that are currently rated "high" on the Human Development Index.
Etymology.
In antiquity, Sri Lanka was known to travellers by a variety of names. According to the "Mahavamsa", the legendary Prince Vijaya named the land Tambapanni ("copper-red hands" or "copper-red earth"), because his followers' hands were reddened by the red soil of the area. In Hindu mythology, such as the Mahabharata, the island was referred to as "Lankā" ("Island"). In Tamil, the island is referred to as Eelam.
Ancient Greek geographers called it "Taprobanā" () or "Taprobanē" () from the word "Tambapanni". The Persians and Arabs referred to it as "Sarandīb" (the origin of the word "serendipity") from the word "Cerentivu"."", the name given to Sri Lanka by the Portuguese Empire when it arrived in 1505, was transliterated into English as "Ceylon". As a British crown colony, the island was known as Ceylon; it achieved independence as the Dominion of Ceylon in 1948.
The country is known in Sinhalese as ' () and in Tamil as ' (, ). In 1972, its formal name was changed to "Free, Sovereign and Independent Republic of Sri Lanka". Later in 1978 it was changed to the "Democratic Socialist Republic of Sri Lanka". As the name Ceylon still appears in the names of a number of organisations, the Sri Lankan government announced in 2011 a plan to rename all those over which it has authority.
History.
Prehistory.
The pre-history of Sri Lanka goes back 125,000 years and possibly even as far back as 500,000 years. The era spans the Palaeolithic, Mesolithic and early Iron Ages. Among the Paleolithic human settlements discovered in Sri Lanka, Pahiyangala (named after the Chinese traveller monk Faxian), which dates back to 37,000 BP, Batadombalena (28,500 BP) and Belilena (12,000 BP) are the most important. In these caves, archaeologists have found the remains of anatomically modern humans which they have named Balangoda Man, and other evidence suggesting that they may have engaged in agriculture and kept domestic dogs for driving game.
One of the first written references to the island is found in the Indian epic Ramayana, which provides details of a kingdom named "Lanka" that was created by the divine sculptor Vishwakarma for Kubera, the Lord of Wealth. It is said that Kubera was overthrown by his demon stepbrother Ravana, the powerful emperor who built a mythical flying machine named Dandu Monara. The modern city of Wariyapola is described as Ravana's airport.
Early inhabitants of Sri Lanka were probably ancestors of the Vedda people, an indigenous people numbering approximately 2,500 living in modern-day Sri Lanka. The 19th-century Irish historian James Emerson Tennent theorized that Galle, a city in southern Sri Lanka, was the ancient seaport of Tarshish from which King Solomon is said to have drawn ivory, peacocks, and other valuables.
Pre-Anuradhapura period.
According to the "Mahāvamsa", a chronicle written in Pāḷi, the original inhabitants of Sri Lanka are the Yakshas and Nagas. Ancient cemeteries that were used before 600BC and other signs of advanced civilization has also been discovered in Sri Lanka. Sinhalese history traditionally starts in 543 BCE with the arrival of Prince Vijaya , a semi-legendary prince who sailed with 700 followers to Sri Lanka, after being expelled from Vanga Kingdom (present-day Bengal). He established the Kingdom of Tambapanni, near modern-day Mannar. Vijaya (Singha) is the first of the approximately 189 native monarchs of Sri Lanka described in chronicles such as the "Dipavamsa", "Mahāvaṃsa", "Cūḷavaṃsa", and "Rājāvaliya" ("see list of Sinhalese monarchs"). Sri Lankan dynastic history ended in 1815 CE, when the land became part of the British Empire.
Anuradhapura period.
The Anuradhapura Kingdom was established in 380 BCE during the reign of Pandukabhaya of Anuradhapura. Thereafter, Anuradhapura served as the capital city of the country for nearly 1,400 years. Ancient Sri Lankans excelled at building certain types of structures (constructions) such as tanks, dagobas and palaces. Society underwent a major transformation during the reign of Devanampiya Tissa of Anuradhapura, with the arrival of Buddhism from India. In 250 BC, Mahinda, the son of the Mauryan Emperor Ashoka and a bhikkhu (Buddhist monk) arrived in Mihintale carrying the message of Buddhism. His mission won over the monarch, who embraced the faith and propagated it throughout the Sinhalese population.
Succeeding kingdoms of Sri Lanka would maintain a large number of Buddhist schools and monasteries and support the propagation of Buddhism into other countries in Southeast Asia. Sri Lankan Bhikkhus studied in India's famous ancient Buddhist University of Nalanda, which was destroyed by Bakhtiyar Khilji. It is probable that many of the scriptures from Nalanda are preserved in Sri Lanka's many monasteries and that the written form of the Tipitaka, including Sinhalese Buddhist literature, were part of the University of Nalanda. In 245 BC, bhikkhuni Sangamitta arrived with the Jaya Sri Maha Bodhi tree, which is considered to be a sapling from the historical Bodhi tree under which Gautama Buddha became enlightened. It is considered the oldest human-planted tree (with a continuous historical record) in the world. (Bodhivamsa)
Sri Lanka first experienced a foreign invasion during the reign of Suratissa, who was defeated by two horse traders named Sena and Guttika from South India. The next invasion came immediately in 205 BC by a Chola king named Elara, who overthrew Asela and ruled the country for 44 years. Dutugemunu, the eldest son of the southern regional sub-king, Kavan Tissa, defeated Elara in the Battle of Vijithapura. He built Ruwanwelisaya, the second stupa in ancient Sri Lanka, and the Lovamahapaya.
During its two and a half millennia of existence, the Kingdom of Sri Lanka was invaded at least eight times by neighbouring South Asian dynasties such as the Chola, Pandya, Chera, and Pallava. These invaders were all subsequently driven back. There also were incursions by the kingdoms of Kalinga (modern Odisha) and from the Malay Peninsula as well. "Kala Wewa" and the Avukana Buddha statue were built during the reign of Dhatusena.
The Fourth Buddhist council of Theravada Buddhism was held at the Anuradhapura Maha Viharaya in Sri Lanka under the patronage of Valagamba of Anuradhapura in 25 BCE. The council was held in response to a year in which the harvests in Sri Lanka were particularly poor and many Buddhist monks subsequently died of starvation. Because the Pāli Canon was at that time oral literature maintained in several recensions by "dhammabhāṇaka"s (dharma reciters), the surviving monks recognized the danger of not writing it down so that even if some of the monks whose duty it was to study and remember parts of the Canon for later generations died, the teachings would not be lost.
After the Council, palm-leaf manuscripts containing the completed Canon were taken to other countries such as Burma, Thailand, Cambodia and Laos.
Sri Lanka was the first Asian country known to have a female ruler: Anula of Anuradhapura (r. 47–42 BCE). Sri Lankan monarchs undertook some remarkable construction projects such as Sigiriya, the so-called "Fortress in the Sky", built during the reign of Kashyapa I of Anuradhapura, who ruled between 477 and 495. The Sigiriya rock fortress is surrounded by an extensive network of ramparts and moats. Inside this protective enclosure were gardens, ponds, pavilions, palaces and other structures.
The 1,600-year-old Sigiriya frescoes are an example of ancient Sri Lankan art at its finest. It is one of the best preserved examples of ancient urban planning in the world. It has been declared by UNESCO as one of the seven World Heritage Sites in Sri Lanka. Among other structures, large reservoirs, important for conserving water in a climate with rainy and dry seasons, and elaborate aqueducts, some with a slope as finely calibrated as one inch to the mile, are most notable. Biso Kotuwa, a peculiar construction inside a dam, is a technological marvel based on precise mathematics that allows water to flow outside the dam, keeping pressure on the dam to a minimum.
Ancient Sri Lanka was the first country in the world to establish a dedicated hospital, in Mihintale in the 4th century. It was also the leading exporter of cinnamon in the ancient world. It maintained close ties with European civilisations including the Roman Empire. For example, Bhatikabhaya (22 BCE – 7 CE) sent an envoy to Rome who brought back red coral, which was used to make an elaborate netlike adornment for the Ruwanwelisaya. In addition, Sri Lankan male dancers witnessed the assassination of Caligula. When Queen Cleopatra sent her son Caesarion into hiding, he was headed to Sri Lanka.
The upasampada for bhikkhunis (Buddhist nuns) first arrived in China when Devasāra and ten other bhikkhunis came from Sri Lanka at the request of Chinese women and established the order there in 429.
Polonnaruwa and Transitional periods.
The medieval period of Sri Lanka begins with the fall of Anuradhapura Kingdom. In AD 993, the invasion of Chola emperor Rajaraja I forced the then Sri Lankan ruler Mahinda V to flee to the southern part of Sri Lanka. Taking advantage of this situation, Rajendra I, son of Rajaraja I, launched a large invasion in AD 1017. Mahinda V was captured and taken to India, and the Cholas sacked the city of Anuradhapura. Subsequently, they moved the capital to Polonnaruwa.
This marked the end of the two great houses of dynasties of ancient Sri Lanka, the Moriya and the Lambakanna. Following a seventeen-year-long campaign, Vijayabahu I successfully drove the Chola out of Sri Lanka in 1070, reuniting the country for the first time in over a century. Upon his request, ordained monks were sent from Burma to Sri Lanka to re-establish Buddhism, which had almost disappeared from the country during the Chola reign. During the medieval period, Sri Lanka was divided into three sub-territories, namely Ruhunu, Pihiti and Maya.
Sri Lanka's irrigation system was extensively expanded during the reign of Parākramabāhu the Great (AD 1153–1186). This period is considered as a time when Sri Lanka was at the height of its power. He built 1470 reservoirs – the highest number by any ruler in the history – repaired 165 dams, 3910 canals, 163 major reservoirs, and 2376 mini reservoirs. His most famous construction is the Parakrama Samudra, the largest irrigation project of medieval Sri Lanka. Parākramabāhu's reign is memorable for two major campaigns – in the south of India as part of a Pandyan war of succession, and a punitive strike against the kings of Ramanna (Myanmar) for various perceived insults to Sri Lanka.
After his demise, Sri Lanka gradually decayed in power. In AD 1215, Kalinga Magha, a South Indian with uncertain origins, identified as the founder of the Jaffna kingdom, invaded and captured the Kingdom of Polonnaruwa with a 24,000 strong army sailed 690 nautical miles on 100 large ships from Kalinga. Unlike the previous invaders, he looted, ransacked, and destroyed everything in the ancient Anuradhapura and Polonnaruwa Kingdoms beyond recovery. His priorities in ruling were to extract as much as possible from the land and overturn as many of the traditions of Rajarata as possible. His reign saw the massive migration of native Sinhalese people to the south and west of Sri Lanka, and into the mountainous interior, in a bid to escape his power.
Sri Lanka never really recovered from the impact of Kalinga Magha's invasion. King Vijayabâhu III, who led the resistance, brought the kingdom to Dambadeniya. The north, in the meanwhile, eventually evolved into the Jaffna kingdom. The Jaffna kingdom never came under the rule of any kingdom of the south except on one occasion; in 1450, following the conquest led by king Parâkramabâhu VI's adopted son, Prince Sapumal. He ruled the North from AD 1450 to 1467.
The next three centuries stating from 1215 were marked by kaleidoscopically shifting collections of kingdoms in south and central Sri Lanka, including Dambadeniya, Yapahuwa, Gampola, Raigama, Kotte, Sitawaka, and finally, Kandy. Chinese admiral Zheng He and his naval expeditionary force landed at Galle, Sri-Lanka in 1409 and got into battle with the local king as the local king tried to capture him. Zheng He captured the local king and later released him. Zheng He erected a stone tablet inscription at Galle in three languages, Chinese, Tamil and Persian which is known as Galle Trilingual Inscription to commemorate his visit. The stele was discovered by S. H. Thomlin at Galle in 1911 and is now preserved in the Colombo National Museum.
Kandyan period.
The early modern period of Sri Lanka begins with the arrival of Portuguese soldier and explorer Lourenço de Almeida, the son of Francisco de Almeida, in 1505. In 1517, the Portuguese built a fort at the port city of Colombo and gradually extended their control over the coastal areas. In 1592, after decades of intermittent warfare with the Portuguese, Vimaladharmasuriya I moved his kingdom to the inland city of Kandy, a location he thought more secure from attack. In 1619, succumbing to attacks by the Portuguese, the independent existence of Jaffna kingdom came to an end.
During the reign of the Rajasinghe II, Dutch explorers arrived on the island. In 1638, the king signed a treaty with the Dutch East India Company to get rid of the Portuguese who ruled most of the coastal areas. The following Dutch–Portuguese War resulted in a Dutch victory, with Colombo falling into Dutch hands by 1656. The Dutch remained in the areas they had captured, thereby violating the treaty they had signed in 1638. An ethnic group named Burgher people emerged in Sri Lankan society as a result of Dutch rule.
The Kingdom of Kandy was the last independent monarchy of Sri Lanka. In 1595, Vimaladharmasurya brought the sacred Tooth Relic – the traditional symbol of royal and religious authority amongst the Sinhalese – to Kandy, and built the Temple of the Tooth. In spite of on-going intermittent warfare with Europeans, the kingdom survived. Later, a crisis of succession emerged in Kandy upon king Vira Narendrasinha's death in 1739. He was married to a Telugu-speaking Nayakkar princess from South India and was childless by her.
Eventually, with the support of bhikku Weliwita Sarankara, the crown passed to the brother of one of Narendrasinha's princesses, overlooking the right of ""Unambuwe Bandara"", Narendrasinha's own son by a Sinhalese concubine. The new king was crowned Sri Vijaya Rajasinha later that year. Kings of the Nayakkar dynasty launched several attacks on Dutch controlled areas, which proved to be unsuccessful.
British rule.
During the Napoleonic Wars, fearing that French control of the Netherlands might deliver Sri Lanka to the French, Great Britain occupied the coastal areas of the island (which they called Ceylon) with little difficulty in 1796. Two years later, in 1798, Sri Rajadhi Rajasinha, third of the four Nayakkar kings of Sri Lanka, died of a fever. Following his death, a nephew of Rajadhi Rajasinha, eighteen-year-old Kannasamy, was crowned. The young king, now named Sri Vikrama Rajasinha, faced a British invasion in 1803 but successfully retaliated.
By then the entire coastal area was under the British East India Company as a result of the Treaty of Amiens. On 14 February 1815, Kandy was occupied by the British in the second Kandyan War, ending Sri Lanka's independence. Sri Vikrama Rajasinha, the last native monarch of Sri Lanka, was exiled to India. The Kandyan Convention formally ceded the entire country to the British Empire. Attempts by Sri Lankan noblemen to undermine British power in 1818 during the Uva Rebellion were thwarted by Governor Robert Brownrigg.
The beginning of the modern period of Sri Lanka is marked by the Colebrooke-Cameron reforms of 1833. They introduced a utilitarian and liberal political culture to the country based on the rule of law and amalgamated the Kandyan and maritime provinces as a single unit of government. An Executive Council and a Legislative Council were established, later becoming the foundation of a representative legislature. By this time, experiments with coffee plantations were largely successful.
Soon coffee became the primary commodity export of Sri Lanka. Falling coffee prices as a result of the depression of 1847 stalled economic development and prompted the governor to introduce a series of taxes on firearms, dogs, shops, boats, etc., and to reintroduce a form of "rajakariya", requiring six days free labour on roads or payment of a cash equivalent. These harsh measures antagonised the locals, and another rebellion broke out in 1848. A devastating leaf disease, "Hemileia vastatrix", struck the coffee plantations in 1869, destroying the entire industry within fifteen years. The British quickly found a replacement: abandoning coffee, they began cultivating tea instead. Tea production in Sri Lanka thrived in the following decades. Large-scale rubber plantations began in the early 20th century.
By the end of the 19th century, a new educated social class transcending race and caste arose through British attempts to staff the Ceylon Civil Service and the legal, educational, and medical professions. New leaders represented the various ethnic groups of the population in the Ceylon Legislative Council on a communal basis. Buddhist and Hindu revivalism reacted against Christian missionary activities. The first two decades in the 20th century are noted by the unique harmony among Sinhalese and Tamil political leadership, which has since been lost.
In 1919, major Sinhalese and Tamil political organisations united to form the Ceylon National Congress, under the leadership of Ponnambalam Arunachalam, pressing colonial masters for more constitutional reforms. But without massive popular support, and with the governor's encouragement for "communal representation" by creating a "Colombo seat" that dangled between Sinhalese and Tamils, the Congress lost momentum towards the mid-1920s.
The Donoughmore reforms of 1931 repudiated the communal representation and introduced universal adult franchise (the franchise stood at 4% before the reforms). This step was strongly criticised by the Tamil political leadership, who realised that they would be reduced to a minority in the newly created State Council of Ceylon, which succeeded the legislative council. In 1937, Tamil leader G. G. Ponnambalam demanded a 50–50 representation (50% for the Sinhalese and 50% for other ethnic groups) in the State Council. However, this demand was not met by the Soulbury reforms of 1944–45.
Modern Sri Lanka.
The Soulbury constitution ushered in Dominion status, with independence proclaimed on 4 February 1948. D. S. Senanayake became the first Prime Minister of Ceylon. Prominent Tamil leaders like Ponnambalam and Arunachalam Mahadeva joined his cabinet. The British Royal Navy remained stationed at Trincomalee until 1956. A countrywide popular demonstration against withdrawal of the rice ration, known as Hartal 1953, resulted in the resignation of prime minister Dudley Senanayake.
S. W. R. D. Bandaranaike was elected prime minister in 1956. His three-year rule had a profound impact through his self-proclaimed role of "defender of the besieged Sinhalese culture". He introduced the controversial Sinhala Only Act, recognising Sinhala as the only official language of the government. Although partially reversed in 1958, the bill posed a grave concern for the Tamil community, which perceived in it a threat to their language and culture.
The Federal Party (FP) launched a movement of non-violent resistance (satyagraha) against the bill, which prompted Bandaranaike to reach an agreement (Bandaranaike–Chelvanayakam Pact) with S. J. V. Chelvanayakam, leader of the FP, to resolve the looming ethnic conflict. The pact proved ineffective in the face of ongoing protests by opposition and the Buddhist clergy. The bill, together with various government colonisation schemes, contributed much towards the political rancour between Sinhalese and Tamil political leaders. Bandaranaike was assassinated by an extremist Buddhist monk in 1959.
Sirimavo Bandaranaike, the widow of Bandaranaike, took office as prime minister in 1960, and withstood an attempted coup d'état in 1962. During her second term as prime minister, the government instituted socialist economic polices, strengthening ties with the Soviet Union and China, while promoting a policy of non-alignment. In 1971, Ceylon experienced a Marxist insurrection, which was quickly suppressed. In 1972, the country became a republic named Sri Lanka, repudiating its dominion status. Prolonged minority grievances and the use of communal emotionalism as an election campaign weapon by both Sinhalese and Tamil leaders abetted a fledgling Tamil militancy in the north during the 1970s. The policy of standardisation by the Sirimavo government to rectify disparities created in university enrolment, which was in essence an affirmative action to assist geographically disadvantaged students to obtain tertiary education, resulted in reducing the proportion of Tamil students at university level and acted as the immediate catalyst for the rise of militancy. The assassination of Jaffna Mayor Alfred Duraiyappah in 1975 by LTTE marked a crisis point.
The Government of J. R. Jayawardene swept to power in 1977, defeating the largely unpopular United Front government. Jayawardene introduced a new constitution, together with a free-market economy and a powerful executive presidency modelled after that of France. It made Sri Lanka the first South Asian country to liberalise its economy. Beginning in 1983, ethnic tensions were manifested in an on-and-off insurgency against the government by the Liberation Tigers of Tamil Eelam (LTTE). An LTTE attack on 13 soldiers resulted in the anti-Tamil race riots in July 1983 allegedly backed by Sinhalese hard-line ministers, which resulted in more than 150,000 Tamil civilians fleeing the island, seeking asylum in other countries.
Lapses in foreign policy resulted in India strengthening the Tigers by providing arms and training. In 1987, the Indo-Sri Lanka Accord was signed and the Indian Peace Keeping Force (IPKF) was deployed in northern Sri Lanka to stabilise the region by neutralising the LTTE. The same year, the JVP launched its second insurrection in Southern Sri Lanka, necessitating redeployment of the IPKF in 1990. In 1990 October, the LTTE expelled Sri Lankan Moors (Muslims by religion) from northern Sri Lanka. In 2002, the Sri Lankan government and LTTE signed a Norwegian-mediated ceasefire agreement.
The 2004 Asian tsunami killed over 35,000 in Sri Lanka. From 1985 to 2006, Sri Lankan government and Tamil insurgents held four rounds of peace talks without success. Both LTTE and the government resumed fighting in 2006, and the government officially backed out of the ceasefire in 2008. In 2009, under the Presidency of Mahinda Rajapaksa the Sri Lanka Armed Forces defeated the LTTE, and re-established control of the entire country by the Sri Lankan Government. Overall, between 60,000 and 100,000 people were killed during the 26 years of conflict.
40,000 Tamil civilians may have been killed in the final phases of the Sri Lankan civil war, according to an Expert Panel convened by U.N. Secretary General Ban Ki-moon. The exact number of Tamils killed is still a speculation that needs further study. Following the LTTE's defeat, the Tamil National Alliance, the largest Tamil political party in Sri Lanka, dropped its demand for a separate state in favour of a federal solution. The final stages of the war left some 294,000 people displaced.
According to the Ministry of Resettlement, most of the displaced persons had been released or returned to their places of origin, leaving only 6,651 in the camps as of December 2011. In May 2010, President Rajapaksa appointed the Lessons Learnt and Reconciliation Commission (LLRC) to assess the conflict between the time of the ceasefire agreement in 2002 and the defeat of the LTTE in 2009. Sri Lanka has emerged from its 26-year war to become one of the fastest growing economies of the world.
Geography.
Sri Lanka lies on the Indian Plate, a major tectonic plate that was formerly part of the Indo-Australian Plate. It is in the Indian Ocean southwest of the Bay of Bengal, between latitudes 5° and 10°N, and longitudes 79° and 82°E. Sri Lanka is separated from the Indian subcontinent by the Gulf of Mannar and Palk Strait. According to Hindu mythology, a land bridge existed between the Indian mainland and Sri Lanka. It now amounts to only a chain of limestone shoals remaining above sea level. Legends claim that it was passable on foot up to 1480 AD, until cyclones deepened the channel. Portions are still as shallow as , hindering navigation.
The island consists mostly of flat to rolling coastal plains, with mountains rising only in the south-central part. The highest point is Pidurutalagala, reaching above sea level. The climate is tropical and warm, due to the moderating effects of ocean winds. Mean temperatures range from in the central highlands, where frost may occur for several days in the winter, to a maximum of in other low-altitude areas. Average yearly temperatures range from to nearly . Day and night temperatures may vary by to .
Rainfall pattern is influenced by monsoon winds from the Indian Ocean and Bay of Bengal. The "wet zone" and some of the windward slopes of the central highlands receive up to of rain each month, but the leeward slopes in the east and northeast receive little rain. Most of the east, southeast, and northern parts of Sri Lanka comprise the "dry zone", which receives between of rain annually.
The arid northwest and southeast coasts receive the least amount of rain at per year. Periodic squalls occur and sometimes tropical cyclones bring overcast skies and rains to the southwest, northeast, and eastern parts of the island. Humidity is typically higher in the southwest and mountainous areas and depends on the seasonal patterns of rainfall.
An increase in average rainfall coupled with heavier rainfall events has resulted in recurrent flooding and related damages to infrastructure, utility supply and the urban economy.
Sri Lanka has 103 rivers. The longest of these is the Mahaweli River, extending . These waterways give rise to 51 natural waterfalls of 10 meters or more. The highest is Bambarakanda Falls, with a height of . Sri Lanka's coastline is 1,585 km long. Sri Lanka claims an Exclusive Economic Zone (EEZ) extending 200 nautical miles, which is approximately 6.7 times Sri Lanka's land area. The coastline and adjacent waters support highly productive marine ecosystems such as fringing coral reefs and shallow beds of coastal and estuarine seagrasses.
Sri Lanka has 45 estuaries and 40 lagoons. Sri Lanka's mangrove ecosystem spans over 7,000 hectares and played a vital role in buffering the force of the waves in the 2004 Indian Ocean tsunami. The island is rich in minerals such as ilmenite, feldspar, graphite, silica, kaolin, mica and thorium. Existence of petroleum and gas in the Gulf of Mannar has also been confirmed and the extraction of recoverable quantities is underway.
Flora and fauna.
Lying within the Indomalaya ecozone, Sri Lanka is one of 25 biodiversity hotspots in the world. Although the country is relatively small in size, it has the highest biodiversity density in Asia. A remarkably high proportion of the species among its flora and fauna, 27% of the 3,210 flowering plants and 22% of the mammals ("see List"), are endemic. Sri Lanka has declared 24 wildlife reserves, which are home to a wide range of native species such as Asian elephants, leopards, sloth bears, the unique small loris, a variety of deer, the purple-faced langur, the endangered wild boar, porcupines and Indian pangolins.
Flowering acacias flourish on the arid Jaffna Peninsula. Among the trees of the dry-land forests are valuable species such as satinwood, ebony, ironwood, mahogany and teak. The wet zone is a tropical evergreen forest with tall trees, broad foliage, and a dense undergrowth of vines and creepers. Subtropical evergreen forests resembling those of temperate climates flourish in the higher altitudes.
Yala National Park in the southeast protects herds of elephant, deer, and peacocks. The Wilpattu National Park in the northwest, the largest national park, preserves the habitats of many water birds such as storks, pelicans, ibis, and spoonbills. The island has four biosphere reserves: Bundala, Hurulu Forest Reserve, the Kanneliya-Dediyagala-Nakiyadeniya, and Sinharaja. Of these, Sinharaja forest reserve is home to 26 endemic birds and 20 rainforest species, including the elusive red-faced malkoha, the green-billed coucal and the Sri Lanka blue magpie.
The untapped genetic potential of Sinharaja flora is enormous. Of the 211 woody trees and lianas within the reserve, 139 (66%) are endemic. The total vegetation density, including trees, shrubs, herbs and seedlings, has been estimated at 240,000 individuals per hectare. The Minneriya National Park borders the Minneriya tank, which is an important source of water for numerous elephants (Elephus maximus) inhabiting the surrounding forests. Dubbed "The Gathering", the congregation of elephants can be seen on the tank-bed in the late dry season (August to October) as the surrounding water sources steadily disappear. The park also encompasses a range of micro-habitats which include classic dry zone tropical monsoonal evergreen forest, thick stands of giant bamboo, hilly pastures (patanas). and grasslands (talawas).
Sri Lanka is home to over 250 types of resident birds ("see List"). It has declared several bird sanctuaries including Kumana. During the Mahaweli Program of the 1970s and 1980s in northern Sri Lanka, the government set aside four areas of land totalling as national parks. Sri Lanka's forest cover, which was around 49% in 1920, had fallen to approximately 24% by 2009.
Politics.
Sri Lanka is the oldest democracy in Asia. The Donoughmore Constitution, drafted by the Donoughmore Commission in 1931, enabled general elections with adult universal suffrage (universal adult voting) in the country. It was the first time a non-white country within the empires of Western Europe was given one man, one vote and the power to control domestic affairs. The first election under the universal adult franchise, held in June 1931, was for the Ceylon State Council. Sir Don Baron Jayatilaka was elected as Leader of the House.
In 1944, the Soulbury Commission was appointed to draft a new constitution. During this time, struggle for independence was fought on "constitutionalist" lines under the leadership of D. S. Senanayake. The draft constitution was enacted in the same year, and Senanayake was appointed Prime Minister following the parliamentary election in 1947. The Soulbury constitution ushered in Dominion status and granted independence to Sri Lanka in 1948.
Political culture.
The current political culture in Sri Lanka is a contest between two rival coalitions led by the centre-leftist and progressivist United People's Freedom Alliance (UPFA), an offspring of Sri Lanka Freedom Party (SLFP), and the comparatively right-wing and pro-capitalist United National Party (UNP). Sri Lanka is essentially a multi-party democracy with many smaller Buddhist, socialist and Tamil nationalist political parties. As of July 2011, the number of registered political parties in the country is 67. Of these, the Lanka Sama Samaja Party (LSSP), established in 1935, is the oldest.
The UNP, established by D. S. Senanayake in 1946, was until recently the largest single political party. It is the only political group which had representation in all parliaments since independence. SLFP was founded by S. W. R. D. Bandaranaike, who was the Cabinet minister of Local Administration before he left the UNP in July 1951. SLFP registered its first victory in 1956, defeating the ruling UNP in 1956 Parliamentary election. Following the parliamentary election in July 1960, Sirimavo Bandaranaike became the prime minister and the world's first elected female head of government.
G. G. Ponnambalam, the Tamil nationalist counterpart of S. W. R. D. Bandaranaike, founded the All Ceylon Tamil Congress (ACTC) in 1944. Objecting to Ponnambalam's cooperation with D. S. Senanayake, a dissident group led by S.J.V. Chelvanayakam broke away in 1949 and formed the Illankai Tamil Arasu Kachchi (ITAK), also known as the Federal Party, becoming the main Tamil political party in Sri Lanka for next two decades. The Federal Party advocated a more aggressive stance toward the Sinhalese.
With the constitutional reforms of 1972, the All Ceylon Tamil Congress (ACTC) and Illankai Tamil Arasu Kachchi (ITAK) created a common front called the Tamil United Front (later Tamil United Liberation Front). Following a period of turbulence as Tamil militants rose to power in the late 1970s, these Tamil political parties were succeeded in October 2001 by the Tamil National Alliance. Janatha Vimukthi Peramuna, a Marxist-Leninist political party founded by Rohana Wijeweera in 1965, serves as a third force in the current political context. It endorses leftist policies which are more radical than the traditionalist leftist politics of the LSSP and the Communist Party. Founded in 1981, the Sri Lanka Muslim Congress is the largest Muslim political party in Sri Lanka.
Government.
Sri Lanka is a democratic republic and a unitary state which is governed by a semi-presidential system, with a mixture of a presidential system and a parliamentary system. Most provisions of the constitution can be amended by a two-thirds majority in parliament. The amendment of certain basic features such as the clauses on language, religion, and reference to Sri Lanka as a unitary state require both a two-thirds majority and approval in a nationwide referendum.
In common with many democracies, the Sri Lankan government has three branches:
Administrative divisions.
For administrative purposes, Sri Lanka is divided into nine provinces and twenty-five districts.
Provinces
There have been provinces ( ) in Sri Lanka since the 19th century, but they had no legal status until 1987 when the 13th Amendment to the 1978 constitution established provincial councils after several decades of increasing demand for a decentralisation of the Government of Sri Lanka. Each provincial council is an autonomous body not under the authority of any Ministry. Some of its functions had been undertaken by central government ministries, departments, corporations, and statutory authorities, but authority over land and police is not as a rule given to provincial councils. Between 1989 and 2006, the Northern and Eastern provinces were temporarily merged to form the North-East Province. Prior to 1987, all administrative tasks for the provinces were handled by a district-based civil service which had been in place since colonial times. Now each province is administered by a directly elected provincial council:
Districts and local authorities
Sri Lanka is also divided into 25 districts ( ). Each district is administered under a District Secretariat. The districts are further subdivided into 256 divisional secretariats, and these, in turn, to approximately 14,008 Grama Niladhari divisions. The Districts are known in Sinhala as "Disa" and in Tamil as "Māwaddam". Originally, a Disa (usually rendered into English as Dissavony) was a duchy, notably Matale and Uva. A government agent, who is known as "District Secretary", administers a district.
There are three other types of local authorities: Municipal Councils (18), Urban councils (13) and Pradeshiya Sabha, also called Pradesha Sabhai (256). Local authorities were originally based on feudal counties named "korale" and "rata", and were formerly known as 'D.R.O. divisions' after the 'Divisional Revenue Officer'. Later the D.R.O.s became 'Assistant Government Agents' and the divisions were known as 'A.G.A. divisions'. These Divisional Secretariats are currently administered by a 'Divisional Secretary'.
Foreign relations.
Sri Lanka is a founding member of the Non-Aligned Movement (NAM). While ensuring that it maintains its independence, Sri Lanka has cultivated relations with India. Sri Lanka became a member of the United Nations in 1955. Today, it is also a member of the Commonwealth, the SAARC, the World Bank, the International Monetary Fund, the Asian Development Bank, and the Colombo Plan.
One of the two parties that have governed Sri Lanka since its independence, the United National Party, has traditionally favoured links with the West while its left-leaning counterpart, the Sri Lanka Freedom Party, has favoured links with the East. Sri Lankan Finance Minister J. R. Jayewardene, together with then Australian Foreign Minister Sir Percy Spencer, proposed the Colombo Plan at the Commonwealth Foreign Minister's Conference held in Colombo in 1950. At the San Francisco Peace Conference in 1951, while many countries were reluctant, Sri Lanka argued for a free Japan and refused to accept payment of reparations for World War II damage because it believed it would harm Japan's economy. Sri Lanka-China relations started as soon as the PRC was formed in 1949. The two countries signed an important Rice-Rubber Pact in 1952. Sri Lanka played a vital role at the Asian–African Conference in 1955, which was an important step in the crystallisation of the NAM.
The Bandaranaike government of 1956 significantly changed the pro-western policies set by the previous UNP government. It recognised Cuba under Fidel Castro in 1959. Shortly afterward, Cuba's revolutionary Ernesto Che Guevara paid a visit to Sri Lanka. The "Sirima-Shastri Pact" of 1964 and "Sirima-Gandhi Pact" of 1974 were signed between Sri Lankan and Indian leaders in an attempt to solve the long-standing dispute over the status of plantation workers of Indian origin. In 1974, Kachchatheevu, a small island in Palk Strait, was formally ceded to Sri Lanka. By this time, Sri Lanka was strongly involved in the NAM and Colombo held the fifth NAM summit in 1976. The relationship between Sri Lanka and India became tense under the government of J. R. Jayawardene. As a result, India intervened in the Sri Lankan Civil War and subsequently deployed an Indian Peace Keeping Force in 1987. In the present, Sri Lanka enjoys extensive relations with China, Russia and Pakistan.
Military.
The Sri Lanka Armed Forces, comprising the Sri Lanka Army, the Sri Lanka Navy, and the Sri Lanka Air Force, come under the purview of the Ministry of Defence (MoD). The total strength of the three services is around 259,000 personnel, with nearly 36,000 reserves. Sri Lanka has not enforced military conscription. Paramilitary units include the Special Task Force, the Civil Security Force, and the Sri Lanka Coast Guard
Since independence in 1948, the primary focus of the armed forces has been internal security, crushing three major insurgencies, two by Marxist militants of the JVP and a 30-year-long conflict with the LTTE which has been proscribed as a terrorist organisation by 32 countries. The armed forces have been in a continuous mobilised state for the last 30 years. Marking a rare occurrence in modern military history, the Sri Lankan military was able to bring a decisive end to the Sri Lankan Civil War in May 2009. Sri Lanka has claimed to be the first country in the modern world to eradicate terrorism on its own soil. The Sri Lankan Armed Forces have engaged in United Nations peacekeeping operations since the early 1960s, contributing forces to permanent contingents deployed in several UN peacekeeping missions in Chad, Lebanon, and Haiti.
Economy.
According to the International Monetary Fund, Sri Lanka's GDP in terms of purchasing power parity is second only to the Maldives in the South Asian region in terms of per capita income. 
In the 19th and 20th centuries, Sri Lanka became a plantation economy, famous for its production and export of cinnamon, rubber and Ceylon tea, which remains a trademark national export. The development of modern ports under British rule raised the strategic importance of the island as a centre of trade. From 1948 to 1977 socialism strongly influenced the government's economic policies. Colonial plantations were dismantled, industries were nationalised and a welfare state established. In 1977 the Free market economy was introduced to the country, incorporating privatisation, deregulation and the promotion of private enterprise.
While the production and export of tea, rubber, coffee, sugar and other commodities remain important, industrialisation has increased the importance of food processing, textiles, telecommunications and finance. The country's main economic sectors are tourism, tea export, clothing, rice production and other agricultural products. In addition to these economic sectors, overseas employment, especially in the Middle East, contributes substantially in foreign exchange.
, the service sector makes up 60% of GDP, the industrial sector 28%, and the agriculture sector 12%. The private sector accounts for 85% of the economy. India is Sri Lanka's largest trading partner. Economic disparities exist between the provinces, with the Western province contributing 45.1% of the GDP and the Southern province and the Central province contributing 10.7% and 10%, respectively. With the end of the war, the Northern province reported a record 22.9% GDP growth in 2010.
The per capita income of Sri Lanka has doubled since 2005. During the same period, poverty has dropped from 15.2% to 7.6%, unemployment rate has dropped from 7.2% to 4.9%, market capitalisation of Colombo Stock Exchange has quadrupled and budget deficit has doubled. Over 90% of the households in Sri Lanka are electrified. 87.3% of the population have access to safe drinking water and 39% have access to pipe-borne water. Income inequality has also dropped in recent years, indicated by a gini coefficient of 0.36 in 2010. Sri Lanka's cellular subscriber base has shown a staggering 550% growth, from 2005 to 2010. Sri Lanka was the first country in the South Asian region to introduce 3G, 3.5G HSDPA, 3.75G HSUPA and 4G LTE mobile broadband Internet technologies.
The Global Competitiveness Report, published by the World Economic Forum, has described Sri Lanka's economy as transitioning from the factor-driven stage to the efficiency-driven stage, and that it ranks 52nd in global competitiveness. Also, out of the 142 countries surveyed, Sri Lanka ranked 45th in health and primary education, 32nd in business sophistication, 42nd in innovation, and 41st in goods market efficiency. Sri Lanka ranks 8th in the World Giving Index, registering high levels of contentment and charitable behaviour in its society. In 2010, "The New York Times" placed Sri Lanka at the top of its list of 31 places to visit. The Dow Jones classified Sri Lanka as an emerging market in 2010, and Citigroup classified it as a 3G country in February 2011. Sri Lanka ranks well above other South Asian countries in the Human Development Index (HDI) with 0.750 points.
Sri Lanka's road network consists of 35 A grade highways and two Controlled-access highways (E01)and(E03). The railway network, operated by the state-run national railway operator, Sri Lanka Railways, spans . Sri Lanka also has three deep-water ports, at Colombo, Galle, and Trincomalee, in addition to the newest port being built at Hambantota. The port at Trincomalee is the fifth largest natural harbour in the world: during World War II the British stated that they could place their entire navy in the harbour with room to spare. Sri Lanka's flag carrier airline is SriLankan Airlines. Fitch Ratings has affirmed Sri Lanka's Foreign- and Local-Currency Issuer Default Ratings (IDRs) at 'BB-' with a "stable" outlook. With a grant of 20 million dollars from the US and help from China, a space academy has been set up for the purpose of developing an indigenous space sector to launch satellites of other nations as well as of Sri Lanka. This dual use of launching technology will also serve to develop missile technology. On 26 September 2012 China launched Sri Lanka's first satellite, with plans for more launches in the coming years.
Demographics.
Sri Lanka is the 57th most populated nation in the world, with 20,277,597 people, and an annual population growth rate of 0.73%. Sri Lanka has a birth rate of 17.6 births per 1,000 people and a death rate of 6.2 deaths per 1,000 people. Population density is highest in western Sri Lanka, especially in and around the capital. Sinhalese constitute the largest ethnic group in the country, with 74.8% of the total population.
Sri Lankan Tamils are the second major ethnic group in the island, with a percentage of 11.2. Sri Lankan Moors comprise 9.2%. Tamils of Indian origin were brought into the country as indentured labourers by British colonists to work on estate plantations. Nearly 50% of them were repatriated following independence in 1948. They are distinguished from the native Tamil population that has resided in Sri Lanka since ancient times. There are also small ethnic groups such as the Burghers (of mixed European descent) and Malays from Southeast Asia. Moreover, there is a small population of Vedda people who are believed to be the original indigenous group to inhabit the island.
Languages.
Sinhalese and Tamil are the two official languages of Sri Lanka. The Constitution defines English as the link language. English is widely used for education, scientific and commercial purposes. Members of the Burgher community speak variant forms of Portuguese Creole and Dutch with varying proficiency, while members of the Malay community speak a form of Creole Malay that is unique to the island.
Religion.
Sri Lanka is a multi-religious country. Buddhists comprise 70 percent of the population, with the Theravada school being predominant. Most Buddhists are of the Sinhalese ethnic group. Buddhism was introduced to Sri Lanka in the 2nd century BC by Venerable Mahinda. A sapling of the Bodhi Tree under which the Buddha attained enlightenment was brought to Sri Lanka during the same time. The Pali Canon ("Thripitakaya"), having previously been preserved as an oral tradition, was first committed to writing in Sri Lanka around 30 BC.
Sri Lanka has the longest continuous history of Buddhism of any predominately Buddhist nation, with the Sangha having existed in a largely unbroken lineage since its introduction in the 2nd century BC. During periods of decline, the Sri Lankan monastic lineage was revived through contact with Thailand and Burma. Buddhism is given special recognition in the Constitution which requires Sri Lankan to "protect and foster the Buddha Sasana".
Hinduism is the second most prevalent religion in Sri Lanka and predates Buddhism. Today, Hinduism is dominant in Northern, Eastern and Central Sri Lanka. Hindus are mainly Tamils.
Islam is the third most dominant religion in the country, having first been brought to the island by Arab traders over the course of many centuries, starting around the 7th century AD. Most Muslims are Sunni who follow the Shafi'i school. Most followers of Islam in Sri Lanka today are believed to be descendants of these Arab traders and the local women they married.
Christianity reached the country through Western colonists in the early 16th century. Around 7.4% of the Sri Lankan population are Christians, of which 82% are Roman Catholics who trace their religious heritage directly to the Portuguese. The remaining Christians are evenly split between the Anglican Church of Ceylon and other Protestant denominations.
There is also a small population of Zoroastrian immigrants from India (Parsis) who settled in Ceylon during the period of British rule, but this community has steadily dwindled in recent years. Religion plays a prominent role in the life and culture of Sri Lankans. The Buddhist majority observe Poya Days each month according to the Lunar calendar, and Hindus and Muslims also observe their own holidays. In a 2008 Gallup poll, Sri Lanka was ranked the third most religious country in the world, with 99% of Sri Lankans saying religion was an important part of their daily life.
Health.
Sri Lankans have a life expectancy of 77.9 years at birth, which is 10% higher than the world average. The infant mortality rate stands at 8.5 per 1,000 births and the maternal mortality rate at 0.39 per 1,000 births, which is on par with figures from the developed countries. The universal "pro-poor" health care system adopted by the country has contributed much towards these figures.
Education.
With a literacy rate of 92.5%, Sri Lanka has one of the most literate populations amongst developing nations. Its youth literacy rate stands at 98%, computer literacy rate at 35%, and primary school enrolment rate at over 99%. An education system which dictates 9 years of compulsory schooling for every child is in place. The free education system established in 1945, is a result of the initiative of C. W. W. Kannangara and A. Ratnayake. It is one of the few countries in the world that provide universal free education from primary to tertiary stage.
Kannangara led the establishment of the Madhya Maha Vidyalayas (Central Schools) in different parts of the country in order to provide education to Sri Lanka's rural children. In 1942 a special education committee proposed extensive reforms to establish an efficient and quality education system for the people. However, in the 1980s changes to this system saw the separation of the administration of schools between the central government and the provincial government. Thus the elite National Schools are controlled directly by the Ministry of Education and the provincial schools by the provincial government. Sri Lanka has approximately 9675 government schools, 817 private schools and Pirivenas.
The number of public universities in Sri Lanka is 15. A lack of responsiveness of the education system to labour market requirements, disparities in access to quality education, lack of an effective linkage between secondary and tertiary education remain major challenges for the education sector. A number of private, degree awarding institutions have emerged in recent times to fill in these gaps, yet the participation at tertiary level education remains at 5.1%. The proposed private university bill has been withdrawn by the Higher Education Ministry after university students' heavy demonstrations and resistance.
The British science fiction author Arthur C. Clarke served as Chancellor of Moratuwa University in Sri Lanka from 1979 to 2002.
Transport.
Sri Lanka has an extensive road network for inland transportation. With more than 100,000 km of paved roads, it has one of the highest road densities in the world (1.5 km of paved roads per every 1sq.km. of land). E-grade highways are the latest addition to Sri Lanka's road network. These are access-controlled, high-mobility roads with permitted speeds up to 120 km/h. These highways connect local communities together, by-passing busy and congested town centers.
A and B grade roads are national (arterial) highways administered by Road Development Authority. C & D grade roads are provincial roads coming under the purview of the Provincial Road Development Authority of the respective province. The other roads are local roads falling under local government authorities.
Rail network of Sri Lanka consists of main line, coastal line, and up-country line. In addition, there are small contributions from air and water based media, to the inland transport of the country.
Human rights and media.
The Sri Lanka Broadcasting Corporation (formerly Radio Ceylon) is the oldest-running radio station in Asia, established in 1923 by Edward Harper just three years after broadcasting began in Europe. The station broadcasts services in Sinhalese, Tamil, English and Hindi. Since the 1980s, a large number of private radio stations have also been introduced. Broadcast television was introduced to the country in 1979 when the Independent Television Network was launched. Initially all Television stations were state controlled, but private television networks began broadcasts in 1992.
, 51 newspapers (30 Sinhala, 10 Tamil, 11 English) are published and 34 TV stations and 52 radio stations are in operation. In recent years, freedom of the press in Sri Lanka has been alleged by media freedom groups to be amongst the poorest in democratic countries. Alleged abuse of a newspaper editor by a senior government minister achieved international notoriety because of the unsolved murder of the editor's predecessor Lasantha Wickrematunge who had also been a critic of the government and had presaged his own death in a posthumously published article.
Officially, the constitution of Sri Lanka guarantees human rights as ratified by the United Nations. However human rights in Sri Lanka have come under criticism by Amnesty International, Freedom from Torture and Human Rights Watch, as well as the United States Department of State. British colonial rulers, the separatist Liberation Tigers of Tamil Eelam (LTTE) and the government of Sri Lanka are accused of violating human rights. A report by an advisory panel to the UN secretary-general has accused both the LTTE and the Sri Lankan government of alleged war crimes during final stages of the civil war. Corruption remains a problem in Sri Lanka, and there is currently very little protection for those who stand up against corruption.
The UN Human Rights Council has documented over 12,000 named individuals who have undergone disappearance after detention by security forces in Sri Lanka, the second highest figure in the world since the Working Group came into being in 1980. The Sri Lankan government has confirmed that 6,445 of these are dead. Allegations of human rights abuses have not ended with the close of the ethnic conflict.
UN Human Rights Commissioner Navanethem Pillay visited Sri Lanka in May 2013. After her visit she said, "The war may have ended Sri Lanka, but in the meantime democracy has been undermined and the rule of law eroded." Pillay spoke about the military's increasing involvement in civilian life and reports of military land grabbing. She also said that while in Sri Lanka she had been allowed to go wherever she wanted but that Sri Lankans who came to meet her were harassed and intimidated by security forces.
In 2012, the UK charity Freedom from Torture reported that it had received 233 referrals of torture survivors from Sri Lanka for clinical treatment or other services provided by the charity. In the same year, Freedom from Torture published "Out of the Silence" which documents evidence of torture in Sri Lanka and demonstrates that the practice has continued long after the end of the civil war in May 2009.
Culture.
The culture of Sri Lanka dates back over 2500 years. It is influenced primarily by Buddhism and Hinduism. Sri Lanka is the home to two main traditional cultures: the Sinhalese (centred in the ancient cities of Kandy and Anuradhapura) and the Tamil (centred in the city of Jaffna). In more recent times, the British colonial culture has also influenced the locals. Sri Lanka claims a democratic tradition matched by few other developing countries.
The first Tamil immigration was probably around the 3rd century BC. Tamils co-existed with the Sinhalese people since then, and the early mixing rendered the two ethnic groups almost physically indistinct. Ancient Sri Lanka is marked for its genius in hydraulic engineering and architecture. The rich cultural traditions shared by all Sri Lankan cultures is the basis of the country's long life expectancy, advanced health standards and high literacy rate.
Food and festivals.
Dishes include rice and curry, pittu, Kiribath, wholemeal Roti, String hoppers, wattalapam (a rich pudding of Malay origin made of coconut milk, jaggery, cashew nuts, eggs, and spices including cinnamon and nutmeg), kottu, and hoppers. Jackfruit may sometimes replace rice. Traditionally food is served on a plantain leaf or lotus leaf.
Middle Eastern influences and practices are found in traditional Moor dishes, while Dutch and Portuguese influences are found with the island's Burgher community preserving their culture through traditional dishes such as Lamprais (rice cooked in stock and baked in a banana leaf), Breudher (Dutch Holiday Biscuit), and Bolo Fiado (Portuguese-style layer cake).
In April, Sri Lankans celebrate the Buddhist and Hindu new year festival. Esala Perahera is a symbolic Buddhist festival consisting of dances and decorated elephants held in Kandy in August. Fire-dances, whip-dances, Kandian dances and various other cultural dances are integral parts of the festival. Christians celebrate Christmas on 25 December to celebrate the birth of Jesus Christ and Easter to celebrate the resurrection of Jesus. Tamils celebrate Thai Pongal, Maha Shivaratri and Muslims celebrate Hajj, Ramadan in their respective days of the year.
Visual, literary and performing arts.
The movie "Kadawunu Poronduwa" (The broken promise), produced by S. M. Nayagam of Chitra Kala Movietone, heralded the coming of Sri Lankan cinema in 1947. "Ranmuthu Duwa" (Island of treasures, 1962) marked the transition cinema from black-and-white to colour. It in the recent years has featured subjects such as family melodrama, social transformation and the years of conflict between the military and the LTTE. The Sri Lankan cinematic style is similar to Bollywood movies. In 1979, movie attendance rose to an all-time high, but has been in steady decline since then.
An influential filmmaker is Lester James Peiris, who has directed a number of movies which led to global acclaim, including "Rekava" (Line of destiny, 1956), "Gamperaliya" (The changing village, 1964), "Nidhanaya" (The treasure, 1970) and "Golu Hadawatha" (Cold heart, 1968). Sri Lankan-Canadian poet Rienzi Crusz, is the subject of a documentary on his life in Sri Lanka. His work is published in Sinhalese and English. Similarly, naturalized-Canadian Michael Ondaatje, is well known for his English-language novels and three films.
The earliest music in Sri Lanka came from theatrical performances such as "Kolam", "Sokari" and "Nadagam". Traditional music instruments such as "Béra", "Thammátama", "Daŭla" and "Răbān" were performed at these dramas. The first music album, "Nurthi", recorded in 1903, was released through Radio Ceylon (founded in 1925). Songwriters like Mahagama Sekara and Ananda Samarakoon and musicians such as W. D. Amaradeva, H. R. Jothipala and Clarence Wijewardene have contributed much towards the upliftment of Sri Lankan music. Baila is another popular music genre in the country, originated among Kaffirs or the Afro-Sinhalese community.
There are three main styles of Sri Lankan classical dance. They are, the Kandyan dances, low country dances and Sabaragamuwa dances. Of these, the Kandyan style, which flourished under kings of the Kingdom of Kandy, is more prominent. It is a sophisticated form of dance, that consists of five sub-categories: "Ves dance", "Naiyandi dance", "Udekki dance", "Pantheru dance" and "18 Vannam". An elaborate headdress is worn by the male dancers and a drum called "Geta Béraya" is used to assist the dancer to keep on rhythm. In addition, four folk drama variants named "Sokri", Kolam "Nadagam", "Pasu", and several devil dance variants such as Sanni Yakuma and "Kohomba Kankariya" can be also observed.
The history of Sri Lankan painting and sculpture can be traced as far back as to the 2nd or 3rd century BC. The earliest mention about the art of painting on Mahavamsa, is to the drawing of a palace on cloth using cinnabar in the 2nd century BC. The chronicles have description of various paintings in relic-chambers of Buddhist stupas, and in monastic residence.
Theatre moved into the country when a Parsi theatre company from Mumbai introduced "Nurti", a blend of European and Indian theatrical conventions to the Colombo audience in the 19th century. The golden age of Sri Lankan drama and theatre began with the staging of "Maname", a play written by Ediriweera Sarachchandra in 1956. It was followed by a series of popular dramas like "Sinhabāhu", "Pabāvatī", "Mahāsāra", "Muudu Puththu" and "Subha saha Yasa".
Sri Lankan literature spans at least two millennia, and is heir to the Aryan literary tradition as embodied in the hymns of the Rigveda. The Pāli Canon, the standard collection of scriptures in the Theravada Buddhist tradition, was written down in Sri Lanka during the Fourth Buddhist council, at the Alulena cave temple, Kegalle, as early as 29 BC. Ancient chronicles such as the Mahāvamsa, written in the 6th century, provide vivid descriptions of Sri Lankan dynasties. According to the German philosopher Wilhelm Geiger, the chronicles are based on Sinhala Atthakatha (commentary), that dates few more centuries back. The oldest surviving prose work is the "Dhampiya-Atuva-Getapadaya", compiled in the 9th century.
The greatest literary feats of medieval Sri Lanka include "Sandesha Kāvya" (poetic messages) such as "Girā Sandeshaya" (Parrot message), "Hansa Sandeshaya" (Swan message) and "Salalihini Sandeshaya" (Myna message). Poetry including "Kavsilumina", "Kavya-Sekharaya" (diadem of poetry) and proses such as "Saddharma-Ratnāvaliya", "Amāvatura" (Flood of nectar) and "Pujāvaliya" are also notable works of this period, which is considered to be the golden age of Sri Lankan literature. The first modern-day novel, "Meena", a work of Simon de Silva appeared in 1905, and was followed by a number of revolutionary literary works. Martin Wickramasinghe, the author of "Madol Doova" is considered the iconic figure of Sri Lankan literature.
Sports.
While the national sport in Sri Lanka is volleyball, by far the most popular sport in the country is cricket. Rugby union also enjoys extensive popularity, as do athletics, football (soccer) and tennis. Sri Lanka's schools and colleges regularly organise sports and athletics teams, competing on provincial and national levels.
The Sri Lanka national cricket team achieved considerable success beginning in the 1990s, rising from underdog status to winning the 1996 Cricket World Cup. They also won 2014 ICC World Twenty20 played in Bangladesh, beating India in the final. In addition, Sri Lanka became the runners up of the Cricket World Cup in 2007, 2011. and of the ICC World Twenty20 in 2009 and 2012.
Former Sri Lankan off-spinner, Muttiah Muralitharan has been rated as the greatest Test match bowler ever by "Wisden Cricketers' Almanack". Sri Lanka has won the Asia Cup in 1986, 1997, 2004, 2008 and 2014. Current world records for highest team score in all three formats of the game are also held by Sri Lanka. The country co-hosted the Cricket World Cup in 1996, 2011 and have hosted the 2012 ICC World Twenty20.
Sri Lankans have won two medals at Olympic Games, one silver, by Duncan White at 1948 London Olympics for men's 400 metres hurdles and one silver by Susanthika Jayasinghe at 2000 Sydney Olympics for women's 200 metres. In 1973, Mohammed Lafir won the World Billiards Championship, highest feat of a Sri Lankan in a Cue sport. Aquatic sports such as boating, surfing, swimming, kitesurfing and scuba diving on the coast, the beaches and backwaters attract a large number of Sri Lankans and foreign tourists. There are two styles of martial arts native to Sri Lanka, Cheena di and Angampora.

</doc>
<doc id="26751" url="https://en.wikipedia.org/wiki?curid=26751" title="Sun">
Sun

The Sun (in Greek: "Helios", in Latin: "Sol") is the star at the center of the Solar System and is by far the most important source of energy for life on Earth. It is a nearly perfect spherical ball of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. Its diameter is about 109 times that of Earth, and it has a mass about 330,000 times that of Earth, accounting for about 99.86% of the total mass of the Solar System.
About three quarters of the Sun's mass consists of hydrogen; the rest is mostly helium, with much smaller quantities of heavier elements, including oxygen, carbon, neon and iron.
The Sun is a G-type main-sequence star (G2V) based on spectral class and it is informally referred to as a yellow dwarf. It formed approximately 4.6 billion years ago from the gravitational collapse of matter within a region of a large molecular cloud. Most of this matter gathered in the center, whereas the rest flattened into an orbiting disk that became the Solar System. The central mass became increasingly hot and dense, eventually initiating nuclear fusion in its core. It is thought that almost all stars form by this process.
The Sun is roughly middle aged and has not changed dramatically for over four billion years, and will remain fairly stable for more than another five billion years. However, after hydrogen fusion in its core has stopped, the Sun will undergo severe changes and become a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of Mercury, Venus, and possibly Earth.
The enormous effect of the Sun on Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity. Earth's movement around the Sun is the basis of the solar calendar, which is the predominant calendar in use today.
Name and etymology.
The English proper noun "Sun" developed from Old English "sunne" and may be related to "south". Cognates to English "sun" appear in other Germanic languages, including Old Frisian "sunne", "sonne", Old Saxon "sunna", Middle Dutch "sonne", modern Dutch "zon", Old High German "sunna", modern German "Sonne", Old Norse "sunna", and Gothic "sunnō". All Germanic terms for the Sun stem from Proto-Germanic *"sunnōn".
The English weekday name "Sunday" stems from Old English ("Sunnandæg"; "Sun's day", from before 700) and is ultimately a result of a Germanic interpretation of Latin "dies solis", itself a translation of the Greek ἡμέρα ἡλίου ("hēméra hēlíou"). The Latin name for the Sun, "Sol", is widely known but is not common in general English language use; the adjectival form is the related word "solar". The term "sol" is also used by planetary astronomers to refer to the duration of a solar day on another planet, such as Mars. A mean Earth solar day is approximately 24 hours, whereas a mean Martian 'sol' is 24 hours, 39 minutes, and 35.244 seconds.
Religious aspects.
Solar deities and Sun worship can be found throughout most of recorded history in various forms, including the Egyptian Ra, the Hindu Surya, the Japanese Amaterasu, the Germanic Sól, and the Aztec Tonatiuh, among others.
From at least the 4th Dynasty of Ancient Egypt, the Sun was worshipped as the god Ra, portrayed as a falcon-headed divinity surmounted by the solar disk, and surrounded by a serpent. In the New Empire period, the Sun became identified with the dung beetle, whose spherical ball of dung was identified with the Sun. In the form of the Sun disc Aten, the Sun had a brief resurgence during the Amarna Period when it again became the preeminent, if not only, divinity for the Pharaoh Akhenaton.
The Sun is viewed as a goddess in Germanic paganism, Sól/Sunna. Scholars theorize that the Sun, as a Germanic goddess, may represent an extension of an earlier Proto-Indo-European Sun deity because of Indo-European linguistic connections between Old Norse "Sól", Sanskrit "Surya", Gaulish "Sulis", Lithuanian "Saulė", and Slavic "Solntse".
In ancient Roman culture, Sunday was the day of the Sun god. It was adopted as the Sabbath day by Christians who did not have a Jewish background. The symbol of light was a pagan device adopted by Christians, and perhaps the most important one that did not come from Jewish traditions. In paganism, the Sun was a source of life, giving warmth and illumination to mankind. It was the center of a popular cult among Romans, who would stand at dawn to catch the first rays of sunshine as they prayed. The celebration of the winter solstice (which influenced Christmas) was part of the Roman cult of the unconquered Sun (Sol Invictus). Christian churches were built with an orientation so that the congregation faced toward the sunrise in the East.
Characteristics.
The Sun is a G-type main-sequence star that comprises about 99.86% of the mass of the Solar System. The Sun has an absolute magnitude of +4.83, estimated to be brighter than about 85% of the stars in the Milky Way, most of which are red dwarfs.
The Sun is a Population I, or heavy-element-rich, star. The formation of the Sun may have been triggered by shockwaves from one or more nearby supernovae. This is suggested by a high abundance of heavy elements in the Solar System, such as gold and uranium, relative to the abundances of these elements in so-called Population II, heavy-element-poor, stars. These elements could most plausibly have been produced by endothermic nuclear reactions during a supernova, or by transmutation through neutron absorption within a massive second-generation star.
The Sun is by far the brightest object in the sky, with an apparent magnitude of −26.74. This is about 13 billion times brighter than the next brightest star, Sirius, which has an apparent magnitude of −1.46. The mean distance of the Sun's center to Earth's center is approximately , though the distance varies as Earth moves from perihelion in January to aphelion in July. At this average distance, light travels from the Sun's horizon to Earth's horizon in about 8 minutes and 19 seconds, while light from the closest points of the Sun and Earth takes about two seconds less. The energy of this sunlight supports almost all life on Earth by photosynthesis, and drives Earth's climate and weather.
The Sun does not have a definite boundary, and in its outer parts its density decreases exponentially with increasing distance from its center. For the purpose of measurement, however, the Sun's radius is considered to be the distance from its center to the edge of the photosphere, the apparent visible surface of the Sun. By this measure, the Sun is a near-perfect sphere with an oblateness estimated at about 9 millionths, which means that its polar diameter differs from its equatorial diameter by only .
The tidal effect of the planets is weak and does not significantly affect the shape of the Sun. The Sun rotates faster at its equator than at its poles. This differential rotation is caused by convective motion due to heat transport and the Coriolis force due to the Sun's rotation. In a frame of reference defined by the stars, the rotational period is approximately 25.6 days at the equator and 33.5 days at the poles. Viewed from Earth as it orbits the Sun, the "apparent rotational period" of the Sun at its equator is about 28 days.
Sunlight.
The solar constant is the amount of power that the Sun deposits per unit area that is directly exposed to sunlight. The solar constant is equal to approximately (watts per square meter) at a distance of one astronomical unit (AU) from the Sun (that is, on or near Earth). Sunlight on the surface of Earth is attenuated by Earth's atmosphere, so that less power arrives at the surface (closer to ) in clear conditions when the Sun is near the zenith. Sunlight at the top of Earth's atmosphere is composed (by total energy) of about 50% infrared light, 40% visible light, and 10% ultraviolet light. The atmosphere in particular filters out over 70% of solar ultraviolet, especially at the shorter wavelengths. Solar ultraviolet radiation ionizes Earth's dayside upper atmosphere, creating the electrically conducting ionosphere.
The Sun's color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when high in the sky; when low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta. Despite its typical whiteness, most people mentally picture the Sun as yellow; the reasons for this are the subject of debate.
The Sun is a G2V star, with "G2" indicating its surface temperature of approximately 5,778 K (5,505 °C, 9,941 °F), and "V" that it, like most stars, is a main-sequence star. The average luminance of the Sun is about 1.88 giga candela per square metre, but as viewed through Earth's atmosphere, this is lowered to about 1.44 Gcd/m2. However, the luminance is not constant across the disk of the Sun (limb darkening).
Composition.
The Sun is composed primarily of the chemical elements hydrogen and helium; they account for 74.9% and 23.8% of the mass of the Sun in the photosphere, respectively. All heavier elements, called "metals" in astronomy, account for less than 2% of the mass, with oxygen (roughly 1% of the Sun's mass), carbon (0.3%), neon (0.2%), and iron (0.2%) being the most abundant.
The Sun inherited its chemical composition from the interstellar medium out of which it formed. The hydrogen and helium in the Sun were produced by Big Bang nucleosynthesis, and the heavier elements were produced by stellar nucleosynthesis in generations of stars that completed their stellar evolution and returned their material to the interstellar medium before the formation of the Sun. The chemical composition of the photosphere is normally considered representative of the composition of the primordial Solar System. However, since the Sun formed, some of the helium and heavy elements have gravitationally settled from the photosphere. Therefore, in today's photosphere the helium fraction is reduced, and the metallicity is only 84% of what it was in the protostellar phase (before nuclear fusion in the core started). The protostellar Sun's composition is believed to have been 71.1% hydrogen, 27.4% helium, and 1.5% heavier elements.
Today, nuclear fusion in the Sun's core has modified the composition by converting hydrogen into helium, so the innermost portion of the Sun is now roughly 60% helium, with the abundance of heavier elements unchanged. Because heat is transferred from the Sun's core by radiation rather than by convection (see Radiative zone below), none of the fusion products from the core have risen to the photosphere.
The reactive core zone of "hydrogen burning", where hydrogen is converted into helium, is starting to surround an inner core of "helium ash". This development will continue and will eventually cause the Sun to leave the main sequence, to become a red giant.
The solar heavy-element abundances described above are typically measured both using spectroscopy of the Sun's photosphere and by measuring abundances in meteorites that have never been heated to melting temperatures. These meteorites are thought to retain the composition of the protostellar Sun and are thus not affected by settling of heavy elements. The two methods generally agree well.
Singly ionized iron-group elements.
In the 1970s, much research focused on the abundances of iron-group elements in the Sun. Although significant research was done, until 1978 it was difficult to determine the abundances of some iron-group elements (e.g. cobalt and manganese) via spectrography because of their hyperfine structures.
The first largely complete set of oscillator strengths of singly ionized iron-group elements were made available in the 1960s, and these were subsequently improved. In 1978, the abundances of singly ionized elements of the iron group were derived.
Isotopic composition.
Various authors have considered the existence of a gradient in the isotopic compositions of solar and planetary noble gases, e.g. correlations between isotopic compositions of neon and xenon in the Sun and on the planets.
Prior to 1983, it was thought that the whole Sun has the same composition as the solar atmosphere. In 1983, it was claimed that it was fractionation in the Sun itself that caused the isotopic-composition relationship between the planetary and solar-wind-implanted noble gases.
Structure.
Core.
The core of the Sun extends from the center to about 20–25% of the solar radius. It has a density of up to (about 150 times the density of water) and a temperature of close to 15.7 million kelvin (K). By contrast, the Sun's surface temperature is approximately 5,800 K. Recent analysis of SOHO mission data favors a faster rotation rate in the core than in the radiative zone above. Through most of the Sun's life, energy is produced by nuclear fusion in the core region through a series of steps called the p–p (proton–proton) chain; this process converts hydrogen into helium. Only 0.8% of the energy generated in the Sun comes from the CNO cycle, though this proportion is expected to increase as the Sun becomes older.
The core is the only region in the Sun that produces an appreciable amount of thermal energy through fusion; 99% of the power is generated within 24% of the Sun's radius, and by 30% of the radius, fusion has stopped nearly entirely. The remainder of the Sun is heated by this energy as is transferred outwards through many successive layers, finally to the solar photosphere where it escapes into space as sunlight or the kinetic energy of particles.
The proton–proton chain occurs around times each second in the core, converting about 3.7 protons into alpha particles (helium nuclei) every second (out of a total of ~8.9 free protons in the Sun), or about 6.2 kg/s. Fusing four free protons (hydrogen nuclei) into a single alpha particle (helium nuclei) releases around 0.7% of the fused mass as energy, so the Sun releases energy at the mass–energy conversion rate of 4.26 million metric tons per second, for 384.6 yotta watts (), or 9.192 megatons of TNT per second. Theoretical models of the Sun's interior indicate a power density of approximately 276.5 W/m3, a value that more nearly approximates reptile metabolism than a thermonuclear bomb.
The fusion rate in the core is in a self-correcting equilibrium: a slightly higher rate of fusion would cause the core to heat up more and expand slightly against the weight of the outer layers, reducing the density and hence the fusion rate and correcting the perturbation; and a slightly lower rate would cause the core to cool and shrink slightly, increasing the density and increasing the fusion rate and again reverting it to its present rate.
Radiative zone.
From the core out to about 0.7 solar radii, thermal radiation is the primary means of energy transfer. The transfer of energy through this zone is by radiation not by thermal convection. The temperature drops from approximately 7 million to 2 million kelvin with increasing distance from the core. This temperature gradient is less than the value of the adiabatic lapse rate and hence cannot drive convection, hence, energy is transferred by radiation. Ions of hydrogen and helium emit photons, which travel only a brief distance before being reabsorbed by other ions. The density drops a hundredfold (from 20 g/cm3 to only 0.2 g/cm3) from 0.25 solar radii to the 0.7 radii, the top of the radiative zone.
Tachocline.
The radiative zone and the convective zone are separated by a transition layer, the tachocline. This is a region where the sharp regime change between the uniform rotation of the radiative zone and the differential rotation of the convection zone results in a large shear between the two—a condition where successive horizontal layers slide past one another. The fluid motion of the convection zone above, slowly disappears from the top of this layer to its bottom where it matches that of the radiative zone. Presently, it is hypothesized (see Solar dynamo) that a magnetic dynamo within this layer generates the Sun's magnetic field.
Convective zone.
The Sun's convection zone extends from 0.7 solar radii (200,000 km) to near the surface. In this layer, the temperature is lower than in the radiative zone and heavier atoms are not fully ionized. As a result, radiative heat transport is less effective and convection moves the Sun's energy outward through this layer. The density of the plasma is low enough to allow convective currents to develop. Material heated at the tachocline picks up heat and expands, thereby reducing its density and allowing it to rise. As a result, an orderly motion of the mass develops into thermal cells that carry the majority of the heat outward to the Sun's photosphere above. Once the material diffusively and radiatively cools just beneath the photospheric surface, its density increases, and it sinks to the base of the convection zone, where it again picks up heat from the top of the radiative zone and the convective cycle continues. At the photosphere, the temperature has dropped to 5,700 K and the density to only 0.2 g/m3 (about 1/6,000th the density of air at sea level).
The thermal columns of the convection zone form an imprint on the surface of the Sun giving it a granular appearance called the solar granulation at the smallest scale and supergranulation at larger scales. Turbulent convection in this outer part of the solar interior sustains "small-scale" dynamo action over the near-surface volume of the Sun. The Sun's thermal columns are Bénard cells and take the shape of hexagonal prisms.
Photosphere.
The visible surface of the Sun, the photosphere, is the layer below which the Sun becomes opaque to visible light. Above the photosphere visible sunlight is free to propagate into space, and its energy escapes the Sun entirely. The change in opacity is due to the decreasing amount of H− ions, which absorb visible light easily. Conversely, the visible light we see is produced as electrons react with hydrogen atoms to produce H− ions.
The photosphere is tens to hundreds of kilometers thick, and is slightly less opaque than air on Earth. Because the upper part of the photosphere is cooler than the lower part, an image of the Sun appears brighter in the center than on the edge or "limb" of the solar disk, in a phenomenon known as limb darkening. The spectrum of sunlight has approximately the spectrum of a black-body radiating at about 6,000 K, interspersed with atomic absorption lines from the tenuous layers above the photosphere. The photosphere has a particle density of ~1023 m−3 (about 0.37% of the particle number per volume of Earth's atmosphere at sea level). The photosphere is not fully ionized—the extent of ionization is about 3%, leaving almost all of the hydrogen in atomic form.
During early studies of the optical spectrum of the photosphere, some absorption lines were found that did not correspond to any chemical elements then known on Earth. In 1868, Norman Lockyer hypothesized that these absorption lines were caused by a new element that he dubbed "helium", after the Greek Sun god Helios. Twenty-five years later, helium was isolated on Earth.
Atmosphere.
During a total solar eclipse, when the disk of the Sun is covered by that of the Moon, parts of the Sun's surrounding atmosphere can be seen. It is composed of four distinct parts: the chromosphere, the transition region, the corona and the heliosphere.
The coolest layer of the Sun is a temperature minimum region extending to about above the photosphere, and has a temperature of about . This part of the Sun is cool enough to allow the existence of simple molecules such as carbon monoxide and water, which can be detected via their absorption spectra.
The chromosphere, transition region, and corona are much hotter than the surface of the Sun. The reason is not well understood, but evidence suggests that Alfvén waves may have enough energy to heat the corona.
Above the temperature minimum layer is a layer about thick, dominated by a spectrum of emission and absorption lines. It is called the "chromosphere" from the Greek root "chroma", meaning color, because the chromosphere is visible as a colored flash at the beginning and end of total solar eclipses. The temperature of the chromosphere increases gradually with altitude, ranging up to around near the top. In the upper part of the chromosphere helium becomes partially ionized.
Above the chromosphere, in a thin (about 200 km) transition region, the temperature rises rapidly from around 20,000 K in the upper chromosphere to coronal temperatures closer to 1,000,000 K. The temperature increase is facilitated by the full ionization of helium in the transition region, which significantly reduces radiative cooling of the plasma. The transition region does not occur at a well-defined altitude. Rather, it forms a kind of nimbus around chromospheric features such as spicules and filaments, and is in constant, chaotic motion. The transition region is not easily visible from Earth's surface, but is readily observable from space by instruments sensitive to the extreme ultraviolet portion of the spectrum.
The corona is the next layer of the Sun. The low corona, near the surface of the Sun, has a particle density around 1015 m−3 to 1016 m−3. The average temperature of the corona and solar wind is about 1,000,000–2,000,000 K; however, in the hottest regions it is 8,000,000–20,000,000 K. Although no complete theory yet exists to account for the temperature of the corona, at least some of its heat is known to be from magnetic reconnection.
The corona is the extended atmosphere of the Sun, which has a volume much larger than the volume enclosed by the Sun's photosphere. A flow of plasma outward from the Sun into interplanetary space is the solar wind.
The heliosphere, the tenuous outermost atmosphere of the Sun, is filled with the solar wind plasma. This outermost layer of the Sun is defined to begin at the distance where the flow of the solar wind becomes "superalfvénic"—that is, where the flow becomes faster than the speed of Alfvén waves, at approximately 20 solar radii (0.1 AU).
Turbulence and dynamic forces in the heliosphere cannot affect the shape of the solar corona within, because the information can only travel at the speed of Alfvén waves. The solar wind travels outward continuously through the heliosphere, forming the solar magnetic field into a spiral shape, until it impacts the heliopause more than 50 AU from the Sun. In December 2004, the Voyager 1 probe passed through a shock front that is thought to be part of the heliopause. In late 2012 Voyager 1 recorded a marked increase in cosmic ray collisions and a sharp drop in lower energy particles from the solar wind, which suggested that the probe had passed through the heliopause and entered the interstellar medium.
Photons and neutrinos.
High-energy gamma-ray photons initially released with fusion reactions in the core are almost immediately absorbed by the solar plasma of the radiative zone, usually after traveling only a few millimeters. Re-emission happens in a random direction and usually at a slightly lower energy. With this sequence of emissions and absorptions, it takes a long time for radiation to reach the Sun's surface. Estimates of the photon travel time range between 10,000 and 170,000 years. In contrast, it takes only 2.3 seconds for the neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface. Because energy transport in the Sun is a process that involves photons in thermodynamic equilibrium with matter, the time scale of energy transport in the Sun is longer, on the order of 30,000,000 years. This is the time it would take the Sun to return to a stable state, if the rate of energy generation in its core were suddenly changed.
Neutrinos are also released by the fusion reactions in the core, but, unlike photons, they rarely interact with matter, so almost all are able to escape the Sun immediately. For many years measurements of the number of neutrinos produced in the Sun were lower than theories predicted by a factor of 3. This discrepancy was resolved in 2001 through the discovery of the effects of neutrino oscillation: the Sun emits the number of neutrinos predicted by the theory, but neutrino detectors were missing of them because the neutrinos had changed flavor by the time they were detected.
Magnetism and activity.
Magnetic field.
[[File:Heliospheric-current-sheet.gif|thumb|right|The heliospheric current sheet extends to the outer reaches of the Solar System, and results from the influence of the Sun's rotating magnetic field on the plasma in the interplanetary medium.]]
The Sun has a magnetic field that varies across the surface of the Sun. Its polar field is , whereas the field is typically in features on the Sun called sunspots and in solar prominences.
The magnetic field also varies in time and location. The quasi-periodic 11-year solar cycle is the most prominent variation in which the number and size of sunspots waxes and wanes.
Sunspots are visible as dark patches on the Sun's photosphere, and correspond to concentrations of magnetic field where the convective transport of heat is inhibited from the solar interior to the surface. As a result, sunspots are slightly cooler than the surrounding photosphere, and, so, they appear dark. At a typical solar minimum, few sunspots are visible, and occasionally none can be seen at all. Those that do appear are at high solar latitudes. As the solar cycle progresses towards its maximum, sunspots tend form closer to the solar equator, a phenomenon known as Spörer's law. The largest sunspots can be tens of thousands of kilometers across.
An 11-year sunspot cycle is half of a 22-year Babcock–Leighton dynamo cycle, which corresponds to an oscillatory exchange of energy between toroidal and poloidal solar magnetic fields. At solar-cycle maximum, the external poloidal dipolar magnetic field is near its dynamo-cycle minimum strength, but an internal toroidal quadrupolar field, generated through differential rotation within the tachocline, is near its maximum strength. At this point in the dynamo cycle, buoyant upwelling within the convective zone forces emergence of toroidal magnetic field through the photosphere, giving rise to pairs of sunspots, roughly aligned east–west and having footprints with opposite magnetic polarities. The magnetic polarity of sunspot pairs alternates every solar cycle, a phenomenon known as the Hale cycle.
During the solar cycle’s declining phase, energy shifts from the internal toroidal magnetic field to the external poloidal field, and sunspots diminish in number. At solar-cycle minimum, the toroidal field is, correspondingly, at minimum strength, sunspots are relatively rare, and the poloidal field is at its maximum strength. With the rise of the next 11-year sunspot cycle, differential rotation shifts magnetic energy back from the poloidal to the toroidal field, but with a polarity that is opposite to the previous cycle. The process carries on continuously, and in an idealized, simplified scenario, each 11-year sunspot cycle corresponds to a change, then, in the overall polarity of the Sun's large-scale magnetic field.
The solar magnetic field extends well beyond the Sun itself. The electrically conducting solar wind plasma carries the Sun's magnetic field into space, forming what is called the interplanetary magnetic field. In an approximation known as ideal magnetohydrodynamics, plasma particles only move along the magnetic field lines. As a result, the outward-flowing solar wind stretches the interplanetary magnetic field outward, forcing it into a roughly radial structure. For a simple dipolar solar magnetic field, with opposite hemispherical polarities on either side of the solar magnetic equator, a thin current sheet is formed in the solar wind. At great distances, the rotation of the Sun twists the dipolar magnetic field and corresponding current sheet into an Archimedean spiral structure called the Parker spiral. The interplanetary magnetic field is much stronger than the dipole component of the solar magnetic field. The Sun's dipole magnetic field of 50–400 μT (at the photosphere) reduces with the inverse-cube of the distance to about 0.1 nT at the distance of Earth. However, according to spacecraft observations the interplanetary field at Earth's location is around 5 nT, about a hundred times greater. The difference is due to magnetic fields generated by electrical currents in the plasma surrounding the Sun.
Variation in activity.
The Sun's magnetic field leads to many effects that are collectively called solar activity. Solar flares and coronal-mass ejections tend to occur at sunspot groups. Slowly changing high-speed streams of solar wind are emitted from coronal holes at the photospheric surface. Both coronal-mass ejections and high-speed streams of solar wind carry plasma and interplanetary magnetic field outward into the Solar System. The effects of solar activity on Earth include auroras at moderate to high latitudes and the disruption of radio communications and electric power. Solar activity is thought to have played a large role in the formation and evolution of the Solar System.
With solar-cycle modulation of sunspot number comes a corresponding modulation of space weather conditions, including those surrounding Earth where technological systems can be affected.
Long-term change.
Long-term secular change in sunspot number is thought, by some scientists, to be correlated with long-term change in solar irradiance, which, in turn, might influence Earth's long-term climate.
For example, in the 17th century, the solar cycle appeared to have stopped entirely for several decades; few sunspots were observed during a period known as the Maunder minimum. This coincided in time with the era of the Little Ice Age, when Europe experienced unusually cold temperatures. Earlier extended minima have been discovered through analysis of tree rings and appear to have coincided with lower-than-average global temperatures.
A recent theory claims that there are magnetic instabilities in the core of the Sun that cause fluctuations with periods of either 41,000 or 100,000 years. These could provide a better explanation of the ice ages than the Milankovitch cycles.
Life phases.
The Sun today is roughly halfway through the most stable part of its life. It has not changed dramatically for over four billion years, and will remain fairly stable for more than five billion more. However, after hydrogen fusion in its core has stopped, the Sun will undergo severe changes, both internally and externally.
Formation.
The Sun formed about 4.6 billion years ago from the collapse of part of a giant molecular cloud that consisted mostly of hydrogen and helium and that probably gave birth to many other stars. This age is estimated using computer models of stellar evolution and through nucleocosmochronology. The result is consistent with the radiometric date of the oldest Solar System material, at 4.567 billion years ago. Studies of ancient meteorites reveal traces of stable daughter nuclei of short-lived isotopes, such as iron-60, that form only in exploding, short-lived stars. This indicates that one or more supernovae must have occurred near the location where the Sun formed. A shock wave from a nearby supernova would have triggered the formation of the Sun by compressing the matter within the molecular cloud and causing certain regions to collapse under their own gravity. As one fragment of the cloud collapsed it also began to rotate because of conservation of angular momentum and heat up with the increasing pressure. Much of the mass became concentrated in the center, whereas the rest flattened out into a disk that would become the planets and other Solar System bodies. Gravity and pressure within the core of the cloud generated a lot of heat as it accreted more matter from the surrounding disk, eventually triggering nuclear fusion. Thus, the Sun was born.
Main sequence.
The Sun is about halfway through its main-sequence stage, during which nuclear fusion reactions in its core fuse hydrogen into helium. Each second, more than four million tonnes of matter are converted into energy within the Sun's core, producing neutrinos and solar radiation. At this rate, the Sun has so far converted around 100 times the mass of Earth into energy, about 0.03% of the total mass of the Sun. The Sun will spend a total of approximately 10 billion years as a main-sequence star. The Sun is gradually becoming hotter during its time on the main sequence, because the helium atoms in the core occupy less volume than the hydrogen atoms that were fused. The core is therefore shrinking, allowing the outer layers of the Sun to move closer to the centre and experience a stronger gravitational force, according to the inverse-square law. This stronger force increases the pressure on the core, which is resisted by a gradual increase in the rate at which fusion occurs. This process speeds up as the core gradually becomes denser. It is estimated that the Sun has become 30% brighter in the last 4.5 billion years. At present, it is increasing in brightness by about 1% every 100 million years.
After core hydrogen exhaustion.
The Sun does not have enough mass to explode as a supernova. Instead it will exit the main sequence in approximately 5 billion years and start to turn into a red giant. As a red giant, the Sun will grow so large that it will engulf Mercury, Venus, and probably Earth.
Even before it becomes a red giant, the luminosity of the Sun will have nearly doubled, and Earth will be hotter than Venus is today. Once the core hydrogen is exhausted in 5.4 billion years, the Sun will expand into a subgiant phase and slowly double in size over about half a billion years. It will then expand more rapidly over about half a billion years until it is over two hundred times larger than today and a couple of thousand times more luminous. This then starts the red-giant-branch phase where the Sun will spend around a billion years and lose around a third of its mass.
After the red-giant branch the Sun has approximately 120 million years of active life left, but much happens. First, the core, full of degenerate helium ignites violently in the helium flash, where it is estimated that 6% of the core, itself 40% of the Sun's mass, will be converted into carbon within a matter of minutes through the triple-alpha process. The Sun then shrinks to around 10 times its current size and 50 times the luminosity, with a temperature a little lower than today. It will then have reached the red clump or horizontal branch, but a star of the Sun's mass does not evolve blueward along the horizontal branch. Instead, it just becomes moderately larger and more luminous over about 100 million years as it continues to burn helium in the core.
When the helium is exhausted, the Sun will repeat the expansion it followed when the hydrogen in the core was exhausted, except that this time it all happens faster, and the Sun becomes larger and more luminous. This is the asymptotic-giant-branch phase, and the Sun is alternately burning hydrogen in a shell or helium in a deeper shell. After about 20 million years on the early asymptotic giant branch, the Sun becomes increasingly unstable, with rapid mass loss and thermal pulses that increase the size and luminosity for a few hundred years every 100,000 years or so. The thermal pulses become larger each time, with the later pulses pushing the luminosity to as much as 5,000 times the current level and the radius to over 1 AU. According to a 2008 model, Earth's orbit is shrinking due to tidal forces (and, eventually, drag from the lower chromosphere), so that it is engulfed by the Sun near the end of the asymptotic-giant-branch phase. Models vary depending on the rate and timing of mass loss. Models that have higher mass loss on the red-giant branch produce smaller, less luminous stars at the tip of the asymptotic giant branch, perhaps only 2,000 times the luminosity and less than 200 times the radius. For the Sun, four thermal pulses are predicted before it completely loses its outer envelope and starts to make a planetary nebula. By the end of that phase – lasting approximately 500,000 years – the Sun will only have about half of its current mass.
The post-asymptotic-giant-branch evolution is even faster. The luminosity stays approximately constant as the temperature increases, with the ejected half of the Sun's mass becoming ionised into a planetary nebula as the exposed core reaches 30,000 K. The final naked core temperature will be over 100,000 K, after which the remnant will cool towards a white dwarf that contains an estimated 54.05% of the Sun's present day mass. The planetary nebula will disperse in about 10,000 years, but the white dwarf will survive for trillions of years before fading to black.
Motion and location.
Orbit in Milky Way.
The Sun lies close to the inner rim of the Milky Way's Orion Arm, in the Local Interstellar Cloud or the Gould Belt, at a distance of 7.5–8.5 kpc (25,000–28,000 light-years) from the Galactic Center.
The Sun is contained within the Local Bubble, a space of rarefied hot gas, possibly produced by the supernova remnant Geminga. The distance between the local arm and the next arm out, the Perseus Arm, is about 6,500 light-years. The Sun, and thus the Solar System, is found in what scientists call the galactic habitable zone.
The "Apex of the Sun's Way", or the solar apex, is the direction that the Sun travels relative to other nearby stars. This motion is towards a point in the constellation Hercules, near the star Vega. Of the 50 nearest stellar systems within 17 light-years from Earth (the closest being the red dwarf Proxima Centauri at approximately 4.2 light-years), the Sun ranks fourth in mass.
The sun is orbiting the center of the Milky Way, going in the direction of Cygnus. The Sun's orbit around the Milky Way is expected to be roughly elliptical with the addition of perturbations due to the galactic spiral arms and non-uniform mass distributions. In addition the Sun oscillates up and down relative to the galactic plane approximately 2.7 times per orbit. It has been argued that the Sun's passage through the higher density spiral arms often coincides with mass extinctions on Earth, perhaps due to increased impact events. It takes the Solar System about 225–250 million years to complete one orbit through the Milky Way (a "galactic year"), so it is thought to have completed 20–25 orbits during the lifetime of the Sun. The orbital speed of the Solar System about the center of the Milky Way is approximately 251 km/s (156 mi/s). At this speed, it takes around 1,190 years for the Solar System to travel a distance of 1 light-year, or 7 days to travel 1 AU.
The Milky Way is moving with respect to the cosmic microwave background radiation (CMB) in the direction of the constellation Hydra with a speed of 550 km/s, and the Sun's resultant velocity with respect to the CMB is about 370 km/s in the direction of Crater or Leo.
Theoretical problems.
Coronal heating problem.
The temperature of the photosphere is approximately 6,000 K, whereas the temperature of the corona reaches 1,000,000–2,000,000 K. The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.
It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient matter in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events—nanoflares.
Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvén waves have been found to dissipate or refract before reaching the corona. In addition, Alfvén waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.
Faint young Sun problem.
Theoretical models of the Sun's development suggest that 3.8 to 2.5 billion years ago, during the Archean period, the Sun was only about 75% as bright as it is today. Such a weak star would not have been able to sustain liquid water on Earth's surface, and thus life should not have been able to develop. However, the geological record demonstrates that Earth has remained at a fairly constant temperature throughout its history, and that the young Earth was somewhat warmer than it is today. The consensus among scientists is that the atmosphere of the young Earth contained much larger quantities of greenhouse gases (such as carbon dioxide, methane and/or ammonia) than are present today, which trapped enough heat to compensate for the smaller amount of solar energy reaching it.
History of observation.
The enormous effect of the Sun on Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity.
Early understanding.
The Sun has been an object of veneration in many cultures throughout human history. Humanity's most fundamental understanding of the Sun is as the luminous disk in the sky, whose presence above the horizon creates day and whose absence causes night. In many prehistoric and ancient cultures, the Sun was thought to be a solar deity or other supernatural entity. Worship of the Sun was central to civilizations such as the ancient Egyptians, the Inca of South America and the Aztecs of what is now Mexico. In religions such as Hinduism, the Sun is still considered a god. Many ancient monuments were constructed with solar phenomena in mind; for example, stone megaliths accurately mark the summer or winter solstice (some of the most prominent megaliths are located in Nabta Playa, Egypt; Mnajdra, Malta and at Stonehenge, England); Newgrange, a prehistoric human-built mount in Ireland, was designed to detect the winter solstice; the pyramid of El Castillo at Chichén Itzá in Mexico is designed to cast shadows in the shape of serpents climbing the pyramid at the vernal and autumn equinoxes.
The Egyptians portrayed the god Ra as being carried across the sky in a solar barque, accompanied by lesser gods, and to the Greeks, he was Helios, carried by a chariot drawn by fiery horses. From the reign of Elagabalus in the late Roman Empire the Sun's birthday was a holiday celebrated as Sol Invictus (literally "Unconquered Sun") soon after the winter solstice, which may have been an antecedent to Christmas. Regarding the fixed stars, the Sun appears from Earth to revolve once a year along the ecliptic through the zodiac, and so Greek astronomers considered it to be one of the seven planets (Greek "planetes", “wanderer”), after which the seven days of the week are named in some languages.
Development of scientific understanding.
In the early first millennium BC, Babylonian astronomers observed that the Sun's motion along the ecliptic is not uniform, though they did not know why; it is today known that this is due to the movement of Earth in an elliptic orbit around the Sun, with Earth moving faster when it is nearer to the Sun at perihelion and moving slower when it is farther away at aphelion.
One of the first people to offer a scientific or philosophical explanation for the Sun was the Greek philosopher Anaxagoras, who reasoned that it is a giant flaming ball of metal even larger than the Peloponnesus rather than the chariot of Helios, and that the Moon reflected the light of the Sun. For teaching this heresy, he was imprisoned by the authorities and sentenced to death, though he was later released through the intervention of Pericles. Eratosthenes estimated the distance between Earth and the Sun in the 3rd century BC as "of stadia myriads 400 and 80000", the translation of which is ambiguous, implying either 4,080,000 stadia (755,000 km) or 804,000,000 stadia (148 to 153 million kilometers or 0.99 to 1.02 AU); the latter value is correct to within a few percent. In the 1st century AD, Ptolemy estimated the distance as 1,210 times the radius of Earth, approximately .
The theory that the Sun is the center around which the planets orbit was first proposed by the ancient Greek Aristarchus of Samos in the 3rd century BC, and later adopted by Seleucus of Seleucia (see Heliocentrism). This view was developed in a more detailed mathematical model of a heliocentric system in the 16th century by Nicolaus Copernicus.
Observations of sunspots were recorded during the Han Dynasty (206 BC–AD 220) by Chinese astronomers, who maintained records of these observations for centuries. Averroes also provided a description of sunspots in the 12th century. The invention of the telescope in the early 17th century permitted detailed observations of sunspots by Thomas Harriot, Galileo Galilei and other astronomers. Galileo posited that sunspots were on the surface of the Sun rather than small objects passing between Earth and the Sun.
Arabic astronomical contributions include Albatenius' discovery that the direction of the Sun's apogee (the place in the Sun's orbit against the fixed stars where it seems to be moving slowest) is changing. (In modern heliocentric terms, this is caused by a gradual motion of the aphelion of the "Earth's" orbit). Ibn Yunus observed more than 10,000 entries for the Sun's position for many years using a large astrolabe.
From an observation of a transit of Venus in 1032, the Persian astronomer and polymath Avicenna concluded that Venus is closer to Earth than the Sun. In 1672 Giovanni Cassini and Jean Richer determined the distance to Mars and were thereby able to calculate the distance to the Sun.
In 1666, Isaac Newton observed the Sun's light using a prism, and showed that it is made up of light of many colors. In 1800, William Herschel discovered infrared radiation beyond the red part of the solar spectrum. The 19th century saw advancement in spectroscopic studies of the Sun; Joseph von Fraunhofer recorded more than 600 absorption lines in the spectrum, the strongest of which are still often referred to as Fraunhofer lines. In the early years of the modern scientific era, the source of the Sun's energy was a significant puzzle. Lord Kelvin suggested that the Sun is a gradually cooling liquid body that is radiating an internal store of heat. Kelvin and Hermann von Helmholtz then proposed a gravitational contraction mechanism to explain the energy output, but the resulting age estimate was only 20 million years, well short of the time span of at least 300 million years suggested by some geological discoveries of that time. In 1890 Joseph Lockyer, who discovered helium in the solar spectrum, proposed a meteoritic hypothesis for the formation and evolution of the Sun.
Not until 1904 was a documented solution offered. Ernest Rutherford suggested that the Sun's output could be maintained by an internal source of heat, and suggested radioactive decay as the source. However, it would be Albert Einstein who would provide the essential clue to the source of the Sun's energy output with his mass-energy equivalence relation . In 1920, Sir Arthur Eddington proposed that the pressures and temperatures at the core of the Sun could produce a nuclear fusion reaction that merged hydrogen (protons) into helium nuclei, resulting in a production of energy from the net change in mass. The preponderance of hydrogen in the Sun was confirmed in 1925 by Cecilia Payne using the ionization theory developed by Meghnad Saha, an Indian physicist. The theoretical concept of fusion was developed in the 1930s by the astrophysicists Subrahmanyan Chandrasekhar and Hans Bethe. Hans Bethe calculated the details of the two main energy-producing nuclear reactions that power the Sun. In 1957, Margaret Burbidge, Geoffrey Burbidge, William Fowler and Fred Hoyle showed that most of the elements in the universe have been synthesized by nuclear reactions inside stars, some like the Sun.
Solar space missions.
The first satellites designed to observe the Sun were NASA's Pioneers 5, 6, 7, 8 and 9, which were launched between 1959 and 1968. These probes orbited the Sun at a distance similar to that of Earth, and made the first detailed measurements of the solar wind and the solar magnetic field. Pioneer 9 operated for a particularly long time, transmitting data until May 1983.
In the 1970s, two Helios spacecraft and the Skylab Apollo Telescope Mount provided scientists with significant new data on solar wind and the solar corona. The Helios 1 and 2 probes were U.S.–German collaborations that studied the solar wind from an orbit carrying the spacecraft inside Mercury's orbit at perihelion. The Skylab space station, launched by NASA in 1973, included a solar observatory module called the Apollo Telescope Mount that was operated by astronauts resident on the station. Skylab made the first time-resolved observations of the solar transition region and of ultraviolet emissions from the solar corona. Discoveries included the first observations of coronal mass ejections, then called "coronal transients", and of coronal holes, now known to be intimately associated with the solar wind.
In 1980, the Solar Maximum Mission was launched by NASA. This spacecraft was designed to observe gamma rays, X-rays and UV radiation from solar flares during a time of high solar activity and solar luminosity. Just a few months after launch, however, an electronics failure caused the probe to go into standby mode, and it spent the next three years in this inactive state. In 1984 Space Shuttle "Challenger" mission STS-41C retrieved the satellite and repaired its electronics before re-releasing it into orbit. The Solar Maximum Mission subsequently acquired thousands of images of the solar corona before re-entering Earth's atmosphere in June 1989.
Launched in 1991, Japan's Yohkoh ("Sunbeam") satellite observed solar flares at X-ray wavelengths. Mission data allowed scientists to identify several different types of flares, and demonstrated that the corona away from regions of peak activity was much more dynamic and active than had previously been supposed. Yohkoh observed an entire solar cycle but went into standby mode when an annular eclipse in 2001 caused it to lose its lock on the Sun. It was destroyed by atmospheric re-entry in 2005.
One of the most important solar missions to date has been the Solar and Heliospheric Observatory, jointly built by the European Space Agency and NASA and launched on 2 December 1995. Originally intended to serve a two-year mission, a mission extension through 2012 was approved in October 2009. It has proven so useful that a follow-on mission, the Solar Dynamics Observatory (SDO), was launched in February 2010. Situated at the Lagrangian point between Earth and the Sun (at which the gravitational pull from both is equal), SOHO has provided a constant view of the Sun at many wavelengths since its launch. Besides its direct solar observation, SOHO has enabled the discovery of a large number of comets, mostly tiny sungrazing comets that incinerate as they pass the Sun.
All these satellites have observed the Sun from the plane of the ecliptic, and so have only observed its equatorial regions in detail. The Ulysses probe was launched in 1990 to study the Sun's polar regions. It first travelled to Jupiter, to "slingshot" into an orbit that would take it far above the plane of the ecliptic. Once Ulysses was in its scheduled orbit, it began observing the solar wind and magnetic field strength at high solar latitudes, finding that the solar wind from high latitudes was moving at about 750 km/s, which was slower than expected, and that there were large magnetic waves emerging from high latitudes that scattered galactic cosmic rays.
Elemental abundances in the photosphere are well known from spectroscopic studies, but the composition of the interior of the Sun is more poorly understood. A solar wind sample return mission, Genesis, was designed to allow astronomers to directly measure the composition of solar material.
The Solar Terrestrial Relations Observatory (STEREO) mission was launched in October 2006. Two identical spacecraft were launched into orbits that cause them to (respectively) pull further ahead of and fall gradually behind Earth. This enables stereoscopic imaging of the Sun and solar phenomena, such as coronal mass ejections.
The Indian Space Research Organisation has scheduled the launch of a 100 kg satellite named Aditya for 2017–18. Its main instrument will be a coronagraph for studying the dynamics of the Solar corona.
Observation and effects.
The brightness of the Sun can cause pain from looking at it with the naked eye; however, doing so for brief periods is not hazardous for normal non-dilated eyes. Looking directly at the Sun causes phosphene visual artifacts and temporary partial blindness. It also delivers about 4 milliwatts of sunlight to the retina, slightly heating it and potentially causing damage in eyes that cannot respond properly to the brightness. UV exposure gradually yellows the lens of the eye over a period of years, and is thought to contribute to the formation of cataracts, but this depends on general exposure to solar UV, and not whether one looks directly at the Sun. Long-duration viewing of the direct Sun with the naked eye can begin to cause UV-induced, sunburn-like lesions on the retina after about 100 seconds, particularly under conditions where the UV light from the Sun is intense and well focused; conditions are worsened by young eyes or new lens implants (which admit more UV than aging natural eyes), Sun angles near the zenith, and observing locations at high altitude.
Viewing the Sun through light-concentrating optics such as binoculars may result in permanent damage to the retina without an appropriate filter that blocks UV and substantially dims the sunlight. When using an attenuating filter to view the Sun, the viewer is cautioned to use a filter specifically designed for that use. Some improvised filters that pass UV or IR rays, can actually harm the eye at high brightness levels.
Herschel wedges, also called Solar Diagonals, are effective and inexpensive for small telescopes. The sunlight that is destined for the eyepiece is reflected from an unsilvered surface of a piece of glass. Only a very small fraction of the incident light is reflected. The rest passes through the glass and leaves the instrument. If the glass breaks because of the heat, no light at all is reflected, making the device fail-safe. Simple filters made of darkened glass allow the full intensity of sunlight to pass through if they break, endangering the observer's eyesight. Unfiltered binoculars can deliver hundreds of times as much energy as using the naked eye, possibly causing immediate damage. It is claimed that even brief glances at the midday Sun through an unfiltered telescope can cause permanent damage.
Partial solar eclipses are hazardous to view because the eye's pupil is not adapted to the unusually high visual contrast: the pupil dilates according to the total amount of light in the field of view, "not" by the brightest object in the field. During partial eclipses most sunlight is blocked by the Moon passing in front of the Sun, but the uncovered parts of the photosphere have the same surface brightness as during a normal day. In the overall gloom, the pupil expands from ~2 mm to ~6 mm, and each retinal cell exposed to the solar image receives up to ten times more light than it would looking at the non-eclipsed Sun. This can damage or kill those cells, resulting in small permanent blind spots for the viewer. The hazard is insidious for inexperienced observers and for children, because there is no perception of pain: it is not immediately obvious that one's vision is being destroyed.
During sunrise and sunset, sunlight is attenuated because of Rayleigh scattering and Mie scattering from a particularly long passage through Earth's atmosphere, and the Sun is sometimes faint enough to be viewed comfortably with the naked eye or safely with optics (provided there is no risk of bright sunlight suddenly appearing through a break between clouds). Hazy conditions, atmospheric dust, and high humidity contribute to this atmospheric attenuation.
An optical phenomenon, known as a green flash, can sometimes be seen shortly after sunset or before sunrise. The flash is caused by light from the Sun just below the horizon being bent (usually through a temperature inversion) towards the observer. Light of shorter wavelengths (violet, blue, green) is bent more than that of longer wavelengths (yellow, orange, red) but the violet and blue light is scattered more, leaving light that is perceived as green.
Ultraviolet light from the Sun has antiseptic properties and can be used to sanitize tools and water. It also causes sunburn, and has other medical effects such as the production of vitamin D. Ultraviolet light is strongly attenuated by Earth's ozone layer, so that the amount of UV varies greatly with latitude and has been partially responsible for many biological adaptations, including variations in human skin color in different regions of the globe.

</doc>
<doc id="26752" url="https://en.wikipedia.org/wiki?curid=26752" title="Smiley">
Smiley

A smiley (sometimes simply called a happy or smiling face) is a stylized representation of a smiling humanoid face, an important part of popular culture. The classic form designed in 1963 comprises a yellow circle with two black dots representing eyes and a black arc representing the mouth (codice_1). On the Internet and in other plain text communication channels, the emoticon form (sometimes also called the smiley-face emoticon) has traditionally been most popular, typically employing a colon and a right parenthesis to form sequences like codice_2, codice_3, or codice_4 that resemble a smiling face when viewed after rotation through 90 degrees. "Smiley" is also sometimes used as a generic term for any emoticon. The smiley has been referenced in nearly all areas of Western culture including music, movies, and art.
The plural form "smilies" is commonly used, but the variant spelling "smilie" is not as common as the "y" spelling.
History.
The poet and author Johannes V. Jensen was amongst other things famous for experimenting with the form of his writing. In a letter sent to publisher Ernst Bojesen in December 1900 he includes both a happy face and a sad face, resembling the modern smiley.
A commercial version of a smiley face with the word "THANKS" above it was available in 1919 and applied as a sticker on receipts issued by the Buffalo Steam Roller Company in Buffalo New York. The round face was much more detailed than the one depicted above, having eyebrows, nose, teeth, chin, facial creases and shading, and is reminiscent of "man-in-the-moon" style characterizations.
Ingmar Bergman's 1948 film "Port of Call" includes a scene where the unhappy Berit draws a "sad" face closely resembling the modern "frowny", but including a dot for the nose in lipstick on her mirror, before being interrupted. In 1953 and 1958, similar happy faces were used in promotional campaigns for the films "Lili" and "Gigi".
The smiley was first introduced to popular culture as part of a promotion by New York radio station WMCA beginning in 1962. Listeners who answered their phone "WMCA Good Guys!" were rewarded with a "WMCA good guys" sweatshirt that incorporated a happy face into its design. Thousands of these sweatshirts were given away. The WMCA smiley was yellow with black dots as eyes, but it had a slightly crooked smile instead of a full smile, and no creases in the mouth.
As per Smithsonian, the smiley face as we know it today was created by Harvey Ross Ball, an American graphic artist. In 1963, Harvey Ball was employed by State Mutual Life Assurance Company of Worcester, Massachusetts (now known as Hanover Insurance) to create a happy face to raise the morale of the employees. Ball created the design in ten minutes and was paid $45 (equivalent to $330 USD in 2012 currency). His rendition, with bright yellow background, dark oval eyes, full smile and creases at the sides of the mouth, was imprinted on more than fifty million buttons and was familiar around the world. The design is so simple that it is certain that similar versions were produced before 1963, including those cited above. However, Ball’s rendition, as described here, has become the most iconic version. In 1967, Seattle graphic artist George Tenagi drew his own version at the request of advertising agent, David Stern. Tenagi's design was used in an advertising campaign for Seattle-based University Federal Savings & Loan. The ad campaign was inspired by Charles Strouse' lyrics in "Put on a Happy Face" from the musical "Bye Bye Birdie". Stern, the man behind this campaign, incorporated the Happy Face in his run for Seattle Mayor in 1993.
The graphic was further popularized in the early 1970s by Philadelphia brothers Bernard and Murray Spain, who seized upon it in September 1970 in a campaign to sell novelty items. The two produced buttons as well as coffee mugs, t-shirts, bumper stickers and many other items emblazoned with the symbol and the phrase "Have a happy day" (devised by Gyula Bogar), which mutated into "Have a nice day". Working with New York button manufacturer NG Slater, some 50 million happy face badges were produced by 1972.
In 1972, Frenchman Franklin Loufrani became the first person to legally trademark the smiley face. He used it to highlight the good news parts of the newspaper "France Soir". He simply called the design "Smiley" and launched the Smiley Company. In 1996 Loufrani's son Nicolas took over the family business and transformed it into a huge multinational corporation. Nicolas Loufrani was outwardly skeptical of Harvey Ball's claim to creating the first smiley face. After all, the design that his father came up with and Ball's design were nearly identical. Loufrani argued that the design is so simple that no one person can lay claim to having created it. As evidence for this, Loufrani's website points to early cave paintings found in France (2500 BC) that he claims are the first depictions of a smiley face. Loufrani also points to a 1960 radio ad campaign that reportedly made use of a similar design.
In the UK, the happy face has been associated with psychedelic culture since Ubi Dwyer and the Windsor Free Festival in the 1970s and the electronic dance music culture, particularly with acid house, that emerged during the Second Summer of Love in the late 1980s. The association was cemented when the band Bomb the Bass used an extracted smiley from "Watchmen" on the centre of its "Beat Dis" hit single.
In text.
One of the first uses of the smiley in text may have been in Robert Herrick's poem "To Fortune" (1648), which contains the line "Upon my ruines (smiling yet :)". Journalist Levi Stahl has suggested that this may have been an intentional "orthographic joke", but this interpretation of the punctuation is disputed, and there are citations of similar punctuation in a non-humorous context, even within Herrick's own work. It is likely that the parenthesis was added later by modern editors.
On the Internet, the smiley has become a visual means of conveyance that uses images. The first known mention on the Internet was on September 19, 1982, when Scott Fahlman from Carnegie Mellon University wrote: 
As the digital age evolved the need for smileys that were easily understood across all cultures gave birth to the emoji.
The smiley is the printable version of characters 1 and 2 of (black-and-white versions of) codepage 437 (1981) of the first IBM PC and all subsequent PC compatible computers. For modern computers, all versions of Microsoft Windows after Windows 95 can use the smiley as part of Windows Glyph List 4, although some computer fonts miss some characters, and some characters cannot be reproduced by programs not compatible with Unicode. It also appears in Unicode's Basic Multilingual Plane.
Licensing and legal issues.
The rights to the Smiley trademark in one hundred countries are owned by the Smiley Company. Its subsidiary SmileyWorld Ltd, in London, headed by Nicolas Loufrani, creates or approves all the Smiley products sold throughout the world. The Smiley brand and logo have significant exposure through licensees in sectors such as clothing, home decoration, perfumery, plush, stationery, publishing, and through promotional campaigns. The Smiley Company is one of the 100 biggest licensing companies in the world, with a turnover of US$167 million in 2012. The first Smiley shop opened in London in the Boxpark shopping centre in December 2011.
In 1997, Franklin Loufrani and Smiley World attempted to acquire trademark rights to the symbol (and even to the word "smiley" itself) in the United States. This brought Loufrani into conflict with Wal-Mart, which had begun prominently featuring a happy face in its "Rolling Back Prices" campaign over a year earlier. Wal-Mart responded first by trying to block Loufrani's application, then later by trying to register the smiley face itself; Loufrani in turn sued to stop Wal-Mart's application, and in 2002 the issue went to court, where it would languish for seven years before a decision.
Wal-Mart began phasing out the smiley face on its vests and its website in 2006. Despite that, Wal-Mart sued an online parodist for alleged "trademark infringement" after he used the symbol (as well as various portmanteaus of "Wal-", such as "Walocaust"). The District Court found in favor of the parodist when in March 2008, the judge concluded that smiley face [logo was not shown to be "inherently distinctive" and that it "has failed to establish that the smiley face has acquired secondary meaning or that it is otherwise a protectible trademark" under U.S. law.
In June 2010, Wal-Mart and the Smiley Company founded by Loufrani settled their 10-year-old dispute in front of the Chicago federal court. The terms remain confidential.

</doc>
<doc id="26753" url="https://en.wikipedia.org/wiki?curid=26753" title="Signature">
Signature

A signature (; from , "to sign") is a handwritten (and often stylized) depiction of someone's name, nickname, or even a simple "X" or other mark that a person writes on documents as a proof of identity and intent. The writer of a signature is a signatory or signer. Similar to a handwritten signature, a signature work describes the work as readily identifying its creator. A signature may be confused with an autograph, which is chiefly an artistic signature. This can lead to confusion when people have both an autograph and signature and as such some people in the public eye keep their signatures private whilst fully publishing their autograph.
Function and types.
The traditional function of a signature is evidential: it is to give evidence of:
For example, the role of a signature in many consumer contracts is not solely to provide evidence of the identity of the contracting party, but also to provide evidence of deliberation and informed consent.
In many countries, signatures may be witnessed and recorded in the presence of a notary public to carry additional legal force. On legal documents, an illiterate signatory can make a "mark" (often an "X" but occasionally a personalized symbol), so long as the document is countersigned by a literate witness. In some countries, illiterate people place a thumbprint on legal documents in lieu of a written signature.
In the United States, signatures encompass marks and actions of all sorts that are indicative of identity and intent. The legal rule is that unless a statute specifically prescribes a particular method of making a signature it may be made in any number of ways. These include by a mechanical or rubber stamp facsimile. A signature may be made by the purported signatory; alternatively someone else duly authorized by the signatory, acting in the signer's presence and at the signatory's direction, may make the signature.
Many individuals have much more fanciful signatures than their normal cursive writing, including elaborate ascenders, descenders and exotic flourishes, much as one would find in calligraphic writing. As an example, the final "k" in John Hancock's famous signature on the US Declaration of Independence loops back to underline his name. This kind of flourish is also known as a "paraph".
Several cultures whose languages use writing systems other than alphabets do not share the Western notion of signatures per se: the "signing" of one's name results in a written product no different from the result of "writing" one's name in the standard way. For these languages, to write or to sign involves the same written characters. Also see Calligraphy.
Mechanically produced signatures.
Special signature machines, called autopens, are capable of automatically reproducing an individual's signature. These are typically used by people required to 
sign a lot of printed matter, such as celebrities, heads of state or CEOs.
More recently, Members of Congress in the United States have begun having their signature made into a TrueType font file. This allows staff members in the Congressman's office to easily reproduce it on correspondence, legislation, and official documents. 
In the East Asian languages of Chinese, Japanese, and Korean, people typically use stamp-like objects known as "name-seals" with the name carved in "tensho" script ("seal script") in lieu of a handwritten signature. 
Wet signatures.
Some government agencies require that professional persons or official reviewers sign originals and all copies of originals to authenticate that they personally viewed the content. In the United States this is prevalent with architectural and construction plans. Its intent is to prevent mistakes or fraud but the practice is not known to be effective.
Online usage.
In e-mail and newsgroup usage, another type of signature exists which is independent of one's language. Users can set one or more lines of custom text known as a signature block to be automatically appended to their messages. This text usually includes a name, contact information, and sometimes quotations and ASCII art. A shortened form of a signature block, only including one's name, often with some distinguishing prefix, can be used to simply indicate the end of a post or response. Some web sites also allow graphics to be used. Note, however, that this type of signature is not related to electronic signatures or digital signatures, which are more technical in nature and not directly understandable by humans.
Other uses.
The signature on a painting or other work of art has always been an important item in the assessment of art. Fake signatures are sometimes added to enhance the value of a painting, or are added to a fake painting to support its authenticity. A notorious case was the signature of Johannes Vermeer on the fake "Supper at Emmaus" made by the art-forger Han van Meegeren.
However, the fact that painters' signatures often vary over time (particularly in the modern and contemporary periods) might complicate the issue. The signatures of some painters take on an artistic form that may be of less value in determining forgeries. For example, Daniel C. Boyer's gouaches are known for their often large, elaborate to the point of near-illegibility, and multicoloured signatures.
The term "signature" is also used to mean the characteristics that give an object, or a piece of information, its identity—for example, the shape of a Coca-Cola bottle.
By analogy, the word "signature" may be used to refer to the characteristic expression of a process or thing. For example, the climate phenomenon known as ENSO or El Niño has characteristic modes in different ocean basins which are often referred to as the "signature" of ENSO.
Copyright.
Under British law, the appearance of signatures (not the names themselves) may be protected under copyright law.
Under United States Copyright Law, "titles, names [...]; mere variations of typographic ornamentation, lettering, or coloring" are not eligible for copyright; however, the appearance of signatures (not the names themselves) may be protected under copyright law.
Uniform Commercial Code.
Uniform Commercial Code §1-201(37) of the United States generally defines signed as "using any symbol executed or adopted with present intention to adopt or accept a writing."
Uniform Commercial Code §3-401(b) for negotiable instruments states "A signature may be made (i) manually or by means of a device or machine, and (ii) by the use of any name, including a trade or assumed name, or by a word, mark, or symbol executed or adopted by a person with present intention to authenticate a writing."

</doc>
<doc id="26754" url="https://en.wikipedia.org/wiki?curid=26754" title="Seal">
Seal

Seal commonly refers to:
Seal may also refer to:

</doc>
<doc id="26756" url="https://en.wikipedia.org/wiki?curid=26756" title="Sino-Tibetan languages">
Sino-Tibetan languages

The Sino-Tibetan languages, in a few sources also known as Tibeto-Burman or Trans-Himalayan, are a family of more than 400 languages spoken in East Asia, Southeast Asia and South Asia. The family is second only to the Indo-European languages in terms of the number of native speakers. The Sino-Tibetan languages with the most native speakers are the varieties of Chinese (1.2 billion speakers), Burmese (33 million) and the Tibetic languages (8 million). Many Sino-Tibetan languages are spoken by small communities in remote mountain areas and are poorly documented.
Several low-level groupings are well established, but the higher-level structure of the family remains unclear. Although the family is often presented as divided into Sinitic and Tibeto-Burman branches, a common origin of the non-Sinitic languages has never been demonstrated, and is rejected by an increasing number of researchers. 
History.
A genetic relationship between Chinese, Tibetan, Burmese and other languages was first proposed in the early 19th century, and is now broadly accepted. The initial focus on languages of civilizations with long literary traditions has been broadened to include less widely spoken languages, some of which have only recently, or never, been written. However, the reconstruction of the family is much less developed than for families such as Indo-European or Austroasiatic. Difficulties have included the great diversity of the languages, the lack of inflection in many of them, and the effects of language contact. In addition, many of the smaller languages are spoken in mountainous areas that are difficult to access, and are often also sensitive border zones.
Early work.
During the 18th century, several scholars had noticed parallels between Tibetan and Burmese, both languages with extensive literary traditions.
Early in the following century, Brian Houghton Hodgson and others noted that many non-literary languages of the highlands of northeast India and Southeast Asia were also related to these.
The name "Tibeto-Burman" was first applied to this group in 1856 by James Richardson Logan, who added Karen in 1858.
The third volume of the "Linguistic Survey of India", edited by Sten Konow, was devoted to the Tibeto-Burman languages of British India.
Studies of the "Indo-Chinese" languages of Southeast Asia from the mid-19th century by Logan and others revealed that they comprised four families: Tibeto-Burman, Tai, Mon–Khmer and Malayo-Polynesian.
Julius Klaproth had noted in 1823 that Burmese, Tibetan and Chinese all shared common basic vocabulary but that Thai, Mon, and Vietnamese were quite different.
Ernst Kuhn envisaged a group with two branches, Chinese-Siamese and Tibeto-Burman.
August Conrady called this group Indo-Chinese in his influential 1896 classification, though he had doubts about Karen. Conrady's terminology was widely used, but there was uncertainty regarding his exclusion of Vietnamese. Franz Nikolaus Finck in 1909 placed Karen as a third branch of Chinese-Siamese.
Jean Przyluski introduced the term "sino-tibétain" as the title of his chapter on the group in Meillet and Cohen's "Les langues du monde" in 1924.
He retained Conrady's two branches of Tibeto-Burman and "Sino-Daic", with Miao–Yao included within Daic (Tai–Kadai).
The English translation "Sino-Tibetan" first appeared in a short note by Przyluski and Luce in 1931.
Shafer and Benedict.
In 1935, the anthropologist Alfred Kroeber started the Sino-Tibetan Philology Project, funded by the Works Project Administration and based at the University of California, Berkeley.
The project was supervised by Robert Shafer until late 1938, and then by Paul K. Benedict.
Under their direction, the staff of 30 non-linguists collated all the available documentation of Sino-Tibetan languages.
The result was 8 copies of a 15-volume typescript entitled "Sino-Tibetan Linguistics".
This work was never published, but furnished the data for a series of papers by Shafer, as well as Shafer's five-volume "Introduction to Sino-Tibetan" and Benedict's "Sino-Tibetan, a Conspectus".
Benedict completed the manuscript of his work in 1941, but it was not published until 1972. Instead of building the entire family tree, he set out to reconstruct a Proto-Tibeto-Burman language by comparing five major languages, with occasional comparisons with other languages. He reconstructed a two-way distinction on initial consonants based on voicing, with aspiration conditioned by pre-initial consonants that had been retained in Tibetic but lost in many other languages. Thus, Benedict reconstructed the following initials:
Although the initial consonants of cognates tend to have the same place and manner of articulation, voicing and aspiration is often unpredictable.
This irregularity was attacked by Roy Andrew Miller, though Benedict's supporters attribute it to the effects of prefixes that have been lost and are often unrecoverable.
The issue remains unsolved today.
It was cited together with the lack of reconstructable shared morphology, and evidence that much shared lexical material has been borrowed from Chinese into Tibeto-Burman, by Christopher Beckwith, one of the few scholars still arguing that Chinese is not related to Tibeto-Burman.
Study of literary languages.
Old Chinese is by far the oldest recorded Sino-Tibetan language, with inscriptions dating from 1200 BC and a huge body of literature from the first millennium BC, but the Chinese script is not alphabetic. Scholars have sought to reconstruct the phonology of Old Chinese by comparing the obscure descriptions of the sounds of Middle Chinese in medieval dictionaries with phonetic elements in Chinese characters and the rhyming patterns of early poetry. The first complete reconstruction, the "Grammata Serica Recensa" of Bernard Karlgren, was used by Benedict and Shafer.
It was somewhat unwieldy, with many sounds with a highly non-uniform distribution. Later scholars have refined Karlgren's work by drawing on a range of other sources. Some proposals were based on cognates in other Sino-Tibetan languages, though workers have also found solely Chinese evidence for them. For example, recent reconstructions of Old Chinese have reduced Karlgren's 15 vowels to a six-vowel system originally suggested by Nicholas Bodman on the basis of comparisons with Tibetic. Similarly, Karlgren's *l has been recast as *r, with a different initial interpreted as *l, matching Tibeto-Burman cognates, but also supported by Chinese transcriptions of foreign names. A growing number of scholars believe that Old Chinese was an atonal language, and that the tones of Middle Chinese developed from final consonants. One of these, *-s, is believed to be a suffix, with cognates in other Sino-Tibetan languages.
Tibetic has extensive written records from the adoption of writing by the Tibetan Empire in the mid-7th century. The earliest records of Burmese (such as the 12th-century Myazedi inscription) are more limited, but later an extensive literature developed. Both languages are recorded in alphabetic scripts ultimately derived from the Brahmi script of Ancient India. Most comparative work has used the conservative written forms of these languages, following the dictionaries of Jäschke (Tibetan) and Judson (Burmese), though both contain entries from a wide range of periods.
There are also extensive records in Tangut, the language of the Western Xia (1038–1227). Tangut is recorded in a Chinese-inspired logographic script, whose interpretation presents many difficulties, even though multilingual dictionaries have been found.
Gong Hwang-cherng has compared Old Chinese, Tibetic, Burmese and Tangut in an effort to establish sound correspondences between those languages. He found that Tibetic and Burmese correspond to two vowels, *a and *ə, in Old Chinese. While this has been considered evidence for a separate Tibeto-Burman subgroup, Hill (2014) finds that Burmese still distinguishes the specific rhymes *-aj (> "-ay") and *-əj (> "-i"), and hence the development *ə > *a should be considered to have occurred independently in Tibetan and Burmese. 
Fieldwork.
The descriptions of non-literary languages used by Shafer and Benedict were often produced by missionaries and colonial administrators of varying linguistic skill.
Most of the smaller Sino-Tibetan languages are spoken in inaccessible mountainous areas, many of which are politically or militarily sensitive and thus closed to investigators.
Until the 1980s, the best-studied areas were Nepal and northern Thailand.
In the 1980s and 1990s, new surveys were published from the Himalayas and southwestern China.
Of particular interest was the discovery of a new branch of the family, the Qiangic languages of western Sichuan and adjacent areas.
Classification.
Several low-level branches of the family, particularly Lolo-Burmese, have been securely reconstructed, but in the absence of a secure reconstruction of proto-Sino-Tibetan, the higher-level structure of the family remains unclear.
Thus, a conservative classification of Sino-Tibetan/Tibeto-Burman would posit several dozen small coordinate families and isolates; attempts at subgrouping are either geographic conveniences or hypotheses for further research.
Li (1937).
In a survey in the 1937 "Chinese Yearbook", Li Fang-Kuei described the family as consisting of four branches:
Tai and Miao–Yao were included because they shared isolating typology, tone systems and some vocabulary with Chinese. At the time, tone was considered so fundamental to language that tonal typology could be used as the basis for classification. In the Western scholarly community, these languages are no longer included in Sino-Tibetan, with the similarities attributed to diffusion across the Mainland Southeast Asia linguistic area, especially since .
The exclusions of Vietnamese by Kuhn and of Tai and Miao–Yao by Benedict were vindicated in 1954 when André-Georges Haudricourt demonstrated that the tones of Vietnamese were reflexes of final consonants from Proto-Mon–Khmer.
Many Chinese linguists continue to follow Li's classification. However, this arrangement remains problematic. For example, there is disagreement over whether to include the entire Tai–Kadai family or just Kam–Tai (Zhuang–Dong excludes the Kra languages), because the Chinese cognates that form the basis of the putative relationship are not found in all branches of the family and have not been reconstructed for the family as a whole. In addition, Kam–Tai itself no longer appears to be a valid node within Tai–Kadai.
Benedict (1942).
Benedict overtly excluded Vietnamese (placing it in Mon–Khmer) as well as Hmong–Mien and Tai–Kadai (placing them in Austro-Tai).
He otherwise retained the outlines of Conrady's Indo-Chinese classification, though putting Karen in an intermediate position:
Shafer (1955).
Shafer criticized the division of the family into Tibeto-Burman and Sino-Daic branches, which he attributed to the different groups of languages studied by Konow and other scholars in British India on the one hand and by Henri Maspero and other French linguists on the other.
He proposed a detailed classification, with six top-level divisions:
Shafer was skeptical of the inclusion of Daic, but after meeting Maspero in Paris decided to retain it pending a definitive resolution of the question.
Matisoff (1978).
James Matisoff abandoned Benedict's Tibeto-Karen hypothesis:
Some more-recent Western scholars, such as Bradley (1997) and La Polla (2003), have retained Matisoff's two primary branches, though differing in the details of Tibeto-Burman. However, Jacques (2006) notes, "comparative work has never been able to put forth evidence for common innovations to all the Tibeto-Burman languages (the Sino-Tibetan languages to the exclusion of Chinese)" and that "it no longer seems justified to treat Chinese as the first branching of the Sino-Tibetan family," because the morphological divide between Chinese and Tibeto-Burman has been bridged by recent reconstructions of Old Chinese.
Starostin (1996).
Sergei Starostin proposed that both the Kiranti languages and Chinese are divergent from a "core" Tibeto-Burman of at least Bodish, Lolo-Burmese, Tamangic, Jinghpaw, Kukish, and Karen (other families were not analysed) in a hypothesis called "Sino-Kiranti". The proposal takes two forms: that Sinitic and Kiranti are themselves a valid node or that the two are not demonstrably close, so that Sino-Tibetan has three primary branches:
Van Driem (1997, 2001).
Van Driem, like Shafer, rejects a primary split between Chinese and the rest, suggesting that Chinese owes its traditional privileged place in Sino-Tibetan to historical, typological, and cultural, rather than linguistic, criteria. He calls the entire family "Tibeto-Burman", a name he says has historical primacy, but other linguists who reject a privileged position for Chinese continue to call the resulting family "Sino-Tibetan".
Like Matisoff, van Driem acknowledges that the relationships of the "Kuki–Naga" languages (Kuki, Mizo, Meithei, etc.), both amongst each other and to the other languages of the family, remain unclear. However, rather than placing them in a geographic grouping, as Matisoff does, van Driem leaves them unclassified.
He has proposed several hypotheses, including the reclassification of Chinese to a Sino-Bodic subgroup:
Van Driem points to two main pieces of evidence establishing a special relationship between Sinitic and Bodic and thus placing Chinese within the Tibeto-Burman family. First, there are a number of parallels between the morphology of Old Chinese and the modern Bodic languages. Second, there is an impressive body of lexical cognates between the Chinese and Bodic languages, represented by the Kirantic language Limbu.
In response, Matisoff notes that the existence of shared lexical material only serves to establish an absolute relationship between two language families, not their relative relationship to one another. Although some cognate sets presented by van Driem are confined to Chinese and Bodic, many others are found in Sino-Tibetan languages generally and thus do not serve as evidence for a special relationship between Chinese and Bodic.
Van Driem (2001).
Van Driem has also proposed a "fallen leaves" model that lists dozens of well-established low-level groups while remaining agnostic about intermediate groupings of these.
In the most recent version, 42 groups are identified:
Van Driem also suggested that the Sino-Tibetan language family be renamed "Trans-Himalayan", which he considers to be more neutral.
Blench and Post (2013).
Roger Blench and Mark W. Post have criticized the applicability of conventional Sino-Tibetan classification schemes to minor languages lacking an extensive written history (unlike Chinese, Tibetic, and Burmese). They find that the evidence for the subclassification or even ST affiliation at all of several minor languages of northeastern India, in particular, is either poor or absent altogether.
In their view, many such languages would for now be best considered unclassified, or "internal isolates" within the family. They propose a provisional classification of the remaining languages:
Because they propose that the three best-known branches may actually be much closer related to each other than they are to "minor" Sino-Tibetan languages, Blench and Post argue that "Sino-Tibetan" or "Tibeto-Burman" would be inappropriate names for a family whose earliest divergences led to different languages altogether. They support the proposed name "Trans-Himalayan".
Typology.
Word order.
Except for the Chinese, Karen, and Bai languages, the usual word order in Sino-Tibetan languages is object–verb. Most scholars believe this to be the original order, with Chinese, Karen and Bai having acquired subject–verb–object order due to the influence of neighbouring languages in the Mainland Southeast Asia linguistic area.
However, Chinese and Bai differ from almost all other VO languages in the world in placing relative clauses before the nouns they modify.
Morphology.
Hodgson had in 1849 noted a dichotomy between "pronominalized" (inflecting) languages, stretching across the Himalayas from Himachal Pradesh to eastern Nepal, and "non-pronominalized" (isolating) languages. Konow (1909) explained the pronominalized languages as due to a Munda substratum, with the idea that Indo-Chinese languages were essentially isolating as well as tonal. Maspero later attributed the putative substratum to Indo-Aryan. It was not until Benedict that the inflectional systems of these languages were recognized as (partially) native to the family.
Scholars disagree over the extent to which the agreement system in the various languages can be reconstructed for the proto-language.
In morphosyntactic alignment, many Tibeto-Burman languages have ergative and/or anti-ergative (an argument that is not an actor) case marking. However, the anti-ergative case markings can not be reconstructed at higher levels in the family and are thought to be innovations.
External classification.
Beyond the traditionally recognized families of Southeast Asia, a number of possible broader relationships have been suggested. One of these is the "Sino-Caucasian" hypothesis of Sergei Starostin, which posits that the Yeniseian languages and North Caucasian languages form a clade with Sino-Tibetan. The Sino-Caucasian hypothesis has been expanded by others to "Dené–Caucasian" to include the Na-Dené languages of North America, Burushaski, Basque and, occasionally, Etruscan. Edward Sapir had commented on a connection between Na-Dené and Sino-Tibetan. A narrower binary Dené–Yeniseian family has recently been well-received, though not conclusively demonstrated. In contrast, Laurent Sagart proposes a Sino-Austronesian family relating Sino-Tibetan to the Austronesian and Tai–Kadai languages.
Peoples and languages.
There is no ethnic unity among the many peoples who speak Sino-Tibetan languages. 
The most numerous are the Han Chinese, numbering 1.3 billion. The Hui (10 million) also speak Chinese but are officially classified as ethnically distinct by the Chinese government. The more numerous peoples speaking other Sino-Tibetan languages are the Burmese (42 million), Yi (Lolo) (7 million), Tibetans (6 million), Karen (5 million), Tripuri (1.3 million), Meitheis (1.5 million), Naga (1.2 million), Tamang (1.1 million), Chin (1.1 million), Newar (1 million), Bodo (2.2 million), and Kachin (1 million). The Burmese live in Burma (Myanmar). Kachin, Karen, Red Karen, and Chin peoples live in the Rakhine, Kachin, Kayin, Kayah, and Chin states of Burma. Tibetans live in the Tibet Autonomous Region, Qinghai, western Sichuan, Gansu, and northern Yunnan provinces in China and in Ladakh in the Kashmir region of Pakistan and India, whereas Manipuris, Mizo, Naga, Tripuri, Idu Mishmis, and Garo live in Manipur, Mizoram, Nagaland, Tripura, and Meghalaya states of India. Bodo and Karbi live in Assam, India, whereas Adi, Nishi, Apa Tani, and Galo live in Arunachal Pradesh, India. The Newar and Tamang live in Nepal and Sikkim, India.
J. A. Matisoff proposed that the urheimat of the Sino-Tibetan languages was around the upper reaches of the Yangtze, Brahmaputra, Salween, and Mekong. This view is in accordance with the hypothesis that bubonic plague, cholera, and other diseases made the easternmost foothills of the Himalayas between China and India difficult for people outside to migrate in but relatively easily for the indigenous people, who had been adapted to the environment, to migrate out.

</doc>
<doc id="26757" url="https://en.wikipedia.org/wiki?curid=26757" title="Slavic languages">
Slavic languages

The Slavic languages (also called Slavonic languages) are the Indo-European languages native to the Slavic peoples, originally from Eastern Europe. They are thought to descend from a proto-language called Proto-Slavic spoken during the Early Middle Ages, which in turn is thought to have descended from the earlier Proto-Balto-Slavic language, linking the Slavic languages to the Baltic languages in a Balto-Slavic group within the Indo-European family.
The Slavic languages are divided intro three subgroups: East, West, and South, which together constitute more than twenty languages. Of these, ten have at least one million speakers and official status as the national languages of the countries in which they are predominantly spoken: Russian, Belarusian and Ukrainian (of the East group), Polish, Czech and Slovak (of the West group) and Slovene, Serbo-Croatian, Macedonian and Bulgarian (of the South group).
The current geographic distribution of natively spoken Slavic languages covers Eastern Europe, the Balkans, the eastern parts of Central Europe and all of the territory of Russia, which includes northern and north-central Asia. Furthermore, the diasporas of many Slavic peoples have established isolated minorities of speakers of their languages all over the world. The number of speakers of all Slavic languages together is estimated to be 315 million.
Branches.
Scholars traditionally divide Slavic languages on the basis of geographical and genealogical principle into three main branches, some of which feature subbranches:
Some linguists speculate that a North Slavic branch has existed as well. The Old Novgorod dialect may have reflected some idiosyncrasies of this group. On the other hand, the term "North Slavic" is also used sometimes to combine the West and East Slavic languages into one group, in opposition to the South Slavic languages, due to traits the West and East Slavic branches share with each other that they do not with the South Slavic languages. Mutual intelligibility also plays a role in determining the West, East, and South branches. Speakers of languages within the same branch will in most cases be able to understand each other at least partially, but they are generally unable to across branches (for which it would be comparable to a native English speaker trying to understand any other Germanic language).
The most obvious differences between the West and East Slavic branches are in the orthography of the standard languages: West Slavic languages are written in the Latin script, and have had more western European influence due to their speakers being historically Roman Catholic, whereas the East Slavic languages are written in Cyrillic and, with Eastern Orthodox or Uniate faithful, have had more Greek influence. East Slavic languages such as Russian have, however, during and after Peter the Great's Europeanization campaign, absorbed many words of Latin, French, German, and Italian origin, somewhat reducing this difference in influence. Although the South Slavic group has traits that distinguish it from the West or East Slavic branches, within itself it displays much the same variations: Bulgarian, for example, has some East Slavic traits (Cyrillic alphabet, Russian loanwords, and Greek influence) and Croatian many West Slavic ones (Latin alphabet, overall central European influence like Czech), despite both being South Slavic.
The tripartite division of the Slavic languages does not take into account the spoken dialects of each language. Of these, certain so-called transitional dialects and hybrid dialects often bridge the gaps between different languages, showing similarities that do not stand out when comparing Slavic literary (i.e. standard) languages. For example, Slovak (West Slavic) and Ukrainian (East Slavic) are bridged by the Rusyn of Eastern Slovakia and western Ukraine. Similarly, Polish shares transitional features with both western Ukrainian and Belarusian dialects. The Croatian Kajkavian dialect is more similar to Slovene than to the standard Croatian language.
Although the Slavic languages diverged from a common proto-language later than any other group of the Indo-European language family, enough differences exist between the various Slavic dialects and languages to make communication between speakers of different Slavic languages difficult. Within the individual Slavic languages, dialects may vary to a lesser degree, as those of Russian, or to a much greater degree, as those of Slovene.
History.
Common roots and ancestry.
Slavic languages descend from Proto-Slavic, their immediate parent language, ultimately deriving from Proto-Indo-European, the ancestor language of all Indo-European languages, via a Proto-Balto-Slavic stage. During the Proto-Balto-Slavic period a number of exclusive isoglosses in phonology, morphology, lexis, and syntax developed, which makes Slavic and Baltic the closest related of all the Indo-European branches. The secession of the Balto-Slavic dialect ancestral to Proto-Slavic is estimated on archaeological and glottochronological criteria to have occurred sometime in the period 1500–1000 BCE.
A minority of Baltists maintain the view that the Slavic group of languages differs so radically from the neighboring Baltic group (Lithuanian, Latvian, and the now-extinct Old Prussian), that they could not have shared a parent language after the breakup of the Proto-Indo-European continuum about five millennia ago. Substantial advances in Balto-Slavic accentology that occurred in the last three decades, however, make this view very hard to maintain nowadays, especially when one considers that there was most likely no "Proto-Baltic" language and that West Baltic and East Baltic differ from each other as much as each of them does from Proto-Slavic.
Evolution.
The imposition of Church Slavonic on Orthodox Slavs was often at the expense of the vernacular. Says WB Lockwood, a prominent Indo-European linguist, "It [O.C.S] remained in use to modern times but was more and more influenced by the living, evolving languages, so that one distinguishes Bulgarian, Serbian, and Russian varieties. The use of such media hampered the development of the local languages for literary purposes, and when they do appear the first attempts are usually in an artificially mixed style." (148)
Lockwood also notes that these languages have "enriched" themselves by drawing on Church Slavonic for the vocabulary of abstract concepts. The situation in the Catholic countries, where Latin was more important, was different. The Polish Renaissance poet Jan Kochanowski and the Croatian Baroque writers of the 16th century all wrote in their respective vernaculars (though Polish itself had drawn amply on Latin in the same way Russian would eventually draw on Church Slavonic).
Although Church Slavonic hampered vernacular literatures, it fostered Slavonic literary activity and abetted linguistic independence from external influences. Only the Croatian vernacular literary tradition nearly matches Church Slavonic in age. It began with the Vinodol Codex and continued through the Renaissance until the codifications of Croatian in 1830, though much of the literature between 1300 and 1500 was written in much the same mixture of the vernacular and Church Slavonic as prevailed in Russia and elsewhere.
The most important early monument of Croatian literacy is the Baška tablet from the late 11th century. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk, containing text written mostly in Čakavian dialect in angular Croatian Glagolitic script. The independence of Dubrovnik facilitated the continuity of the tradition.
More recent foreign influences follow the same general pattern in Slavic languages as elsewhere and are governed by the political relationships of the Slavs. In the 17th century, bourgeois Russian ("delovoi jazyk") absorbed German words through direct contacts between Russians and communities of German settlers in Russia. In the era of Peter the Great, close contacts with France invited countless loan words and calques from French, a significant fraction of which not only survived but also replaced older Slavonic loans. In the 19th century, Russian influenced most literary Slavic languages by one means or another.
Differentiation.
The Proto-Slavic language existed until around 500 AD. By the 7th century, it had broken apart into large dialectal zones.
There are no reliable hypotheses about the nature of the subsequent breakups of West and South Slavic. East Slavic is generally thought to converge to one Old Russian or Old East Slavonic language, which existed until at least the 12th century.
Linguistic differentiation was accelerated by the dispersion of the Slavic peoples over a large territory, which in Central Europe exceeded the current extent of Slavic-speaking majorities. Written documents of the 9th, 10th, and 11th centuries already display some local linguistic features. For example, the Freising manuscripts show a language that contains some phonetic and lexical elements peculiar to Slovene dialects (e.g. rhotacism, the word "krilatec"). The Freising manuscripts are the first Latin-script continuous text in a Slavic language.
The migration of Slavic speakers into the Balkans in the declining centuries of the Byzantine Empire expanded the area of Slavic speech, but the pre-existing writing (notably Greek) survived in this area. The arrival of the Hungarians in Pannonia in the 9th century interposed non-Slavic speakers between South and West Slavs. Frankish conquests completed the geographical separation between these two groups, also severing the connection between Slavs in Moravia and Lower Austria (Moravians) and those in present-day Styria, Carinthia, East Tyrol in Austria, and in the provinces of modern Slovenia, where the ancestors of the Slovenes settled during first colonisation.
Linguistic history.
The following is a summary of the main changes from Proto-Indo-European (PIE) leading up to the Common Slavic (CS) period immediately following the Proto-Slavic language (PS).
Common features.
The Slavic languages are a relatively homogeneous family, compared with other families of Indo-European languages (e.g. Germanic, Romance, and Indo-Iranian). As late as the 10th century AD, the entire Slavic-speaking area still functioned as a single, dialectally differentiated language, termed "Common Slavic". Compared with most other Indo-European languages, the Slavic languages are quite conservative, particularly in terms of morphology (the means of inflecting nouns and verbs to indicate grammatical differences). Most Slavic languages have a rich, fusional morphology that conserves much of the inflectional morphology of Proto-Indo-European.
Consonants.
The following table shows the inventory of consonants of Late Common Slavic:
1The sound did not occur in West Slavic, where it had developed to .
This inventory of sounds is quite similar to what is found in most modern Slavic languages. The extensive series of palatal consonants, along with the affricates *ts and *dz, developed through a series of palatalizations that happened during the Proto-Slavic period, from earlier sequences either of velar consonants followed by front vowels (e.g. *ke, *ki, *ge, *gi, *xe, and *xi), or of various consonants followed by *j (e.g. *tj, *dj, *sj, *zj, *rj, *lj, *kj, and *gj, where *j is the palatal approximant (, the sound of the English letter "y" in "yes" or "you").
The biggest change in this inventory results from a further general palatalization occurring near the end of the Common Slavic period, where "all" consonants became palatalized before front vowels. This produced a large number of new palatalized (or "soft") sounds, which formed pairs with the corresponding non-palatalized (or "hard") consonants and absorbed the existing palatalized sounds . These sounds were best preserved in Russian but were lost to varying degrees in other languages (particularly Czech and Slovak). The following table shows the inventory of modern Russian:
This general process of palatalization did not occur in Serbo-Croatian and Slovenian. As a result, the modern consonant inventory of these languages is nearly identical to the Late Common Slavic inventory.
Late Common Slavic tolerated relatively few consonant clusters. However, as a result of the loss of certain formerly present vowels (the weak yers), the modern Slavic languages allow quite complex clusters, as in the Russian word взблеск ("flash"). Also present in many Slavic languages are clusters rarely found cross-linguistically, as in Russian ртуть ("mercury") or Polish mchu ("moss", gen. sg.). The word for "mercury" with the initial "rt-" cluster, for example, is also found in the other East and West Slavic languages, although Slovak retains an epenthetic vowel ("ortuť").
Vowels.
A typical vowel inventory is as follows:
The sound occurs only in some languages (Russian, Belarusian, Polish), and even in these languages, it is unclear whether it is its own phoneme or an allophone of /i/. Nonetheless, it is a quite prominent and noticeable characteristic of the languages in which it is present.
Common Slavic also had two nasal vowels: *ę and *ǫ . However, these are preserved only in modern Polish (along with a few lesser-known dialects and microlanguages; see Yus for more details).
Other phonemic vowels are found in certain languages (e.g. the schwa in Bulgarian and Slovenian, distinct high-mid and low-mid vowels in Slovenian, and the lax front vowel in Ukrainian).
Length, accent, and tone.
An area of great difference among Slavic languages is that of prosody (i.e. syllabic distinctions such as vowel length, accent, and tone). Common Slavic had a complex system of prosody, inherited with little change from Proto-Indo-European. This consisted of phonemic vowel length and a free, mobile pitch accent:
The modern languages vary greatly in the extent to which they preserve this system. On one extreme, Serbo-Croatian preserves the system nearly unchanged (even more so in the conservative Chakavian dialect); on the other, Macedonian has basically lost the system in its entirety. Between them are found numerous variations:
Grammar.
Similarly, Slavic languages have extensive morphophonemic alternations in their derivational and inflectional morphology, including between velar and postalveolar consonants, front and back vowels, and between a vowel and no vowel.
Selected cognates.
The following is a very brief selection of cognates in basic vocabulary across the Slavic language family, which may serve to give an idea of the sound changes involved. This is not a list of translations: cognates have a common origin, but their meaning may be shifted and loanwords may have replaced them.
Influence on neighboring languages.
Most languages of the former Soviet Union and of some neighbouring countries (for example, Mongolian) are significantly influenced by Russian, especially in vocabulary. In the south, the Romanian, Albanian, and Hungarian languages show the influence of the neighboring Slavic nations, especially in vocabulary pertaining to urban life, agriculture, and crafts and trade—the major cultural innovations at times of limited long-range cultural contact. In each one of these languages, Slavic lexical borrowings represent at least 20% of the total vocabulary. However, Romanian has much lower influence from Slavic than Albanian or Hungarian . This is because Slavic tribes crossed and partially settled the territories inhabited by ancient Illyrians and Vlachs on their way to the Balkans.
Although also spoken in neighbouring lands, the Germanic languages show less significant Slavic influence, partly because Slavic migrations were mostly headed south rather than west. Slavic tribes did push westwards into Germanic territory, but borrowing for the most part seems to have been from Germanic to Slavic rather than the other way: for instance, the now-extinct Polabian language was heavily influenced by German, far more than any living Slavic language today. The Slavic contributions to Germanic languages remains a moot question, though Max Vasmer, a specialist in Slavic etymology, has claimed that there were no Slavic loans into Proto-Germanic. The only Germanic languages that shows significant Slavic influence are Yiddish and the historical colonial dialects of German that were spoken East of the Oder–Neisse line, such as Silesian German (formerly spoken in Silesia and South of East Prussia) and the Eastern varieties of East Low German, with the exception of Low Prussian, which had a strong Baltic substratum. Modern Dutch slang, especially the Amsterdam dialect, borrowed much from Yiddish in turn. However, there are isolated Slavic loans (mostly recent) into other Germanic languages. For example, the word for "border" (in modern German "Grenze", Dutch "grens") was borrowed from the Common Slavic "granica". English derives "quark" (a kind of cheese, not the subatomic particle) from the German "Quark", which in turn is derived from the Slavic "tvarog", which means "curd". Many German surnames, particularly in Eastern Germany and Austria, are Slavic in origin. Swedish also has "torg" (market place) from Old Russian "tъrgъ" or Polish "targ", "tolk" (interpreter) from Old Slavic "tlŭkŭ", and "pråm" (barge) from West Slavonic "pramŭ".
The Czech word is now found in most languages worldwide, and the word , probably also from Czech, is found in many Indo-European languages, including Greek (, pistóli).
A well-known Slavic word in almost all European languages is vodka, a borrowing from Russian "водка" ("vodka") – which itself was borrowed from Polish "wódka" (lit. "little water"), from common Slavic "voda" ("water", cognate to the English word) with the diminutive ending "-ka". Owing to the medieval fur trade with Northern Russia, Pan-European loans from Russian include such familiar words as "sable". The English word "vampire" was borrowed (perhaps via French "vampire") from German "Vampir", in turn derived from Serbian "vampir", continuing Proto-Slavic "*ǫpyrь", although Polish scholar K. Stachowski has argued that the origin of the word is early Slavic "*vąpěrь", going back to Turkic "oobyr". Several European languages, including English, have borrowed the word "polje" (meaning "large, flat plain") directly from the former Yugoslav languages (i.e. Slovene, Croatian, and Serbian). During the heyday of the USSR in the 20th century, many more Russian words became known worldwide: "da", "Soviet", "sputnik", "perestroika", "glasnost", "kolkhoz", etc. Also in the English language borrowed from Russian is "samovar" (lit. "self-boiling") to refer to the specific Russian tea urn.
Detailed list.
The following tree for the Slavic languages derives from the Ethnologue report for Slavic languages. It includes the ISO 639-1 and ISO 639-3 codes where available.
East Slavic languages:
West Slavic languages:
South Slavic languages:
Para- and supranational languages

</doc>
<doc id="26758" url="https://en.wikipedia.org/wiki?curid=26758" title="SGI">
SGI

SGI may refer to:

</doc>
<doc id="26764" url="https://en.wikipedia.org/wiki?curid=26764" title="International System of Units">
International System of Units

The International System of Units (, SI) is the modern form of the metric system, and is the most widely used system of measurement. It comprises a coherent system of units of measurement built on seven base units. It defines twenty-two named units, and includes many more unnamed coherent derived units. The system also establishes a set of twenty prefixes to the unit names and unit symbols that may be used when specifying multiples and fractions of the units.
The system was published in 1960 as the result of an initiative that began in 1948. It is based on the metre-kilogram-second system of units (MKS) rather than any variant of the centimetre-gram-second system (CGS). SI is intended to be an evolving system, so prefixes and units are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves. The 24th and 25th General Conferences on Weights and Measures (CGPM) in 2011 and 2014, for example, discussed a proposal to change the definition of the kilogram, linking it to an invariant of nature rather than to the mass of a material artefact, thereby ensuring long-term stability.
The motivation for the development of the SI was the diversity of units that had sprung up within the CGS systems and the lack of coordination between the various disciplines that used them. The CGPM, which was established by the Metre Convention of 1875, brought together many international organisations to not only agree on the definitions and standards of the new system but also agree on the rules for writing and presenting measurements in a standardised manner around the world.
The International System of Units has been adopted by most developed countries; however, the adoption has not been universal in all English-speaking countries. While metrication in the United States is consistent in science, medicine, government, and various fields of technology and engineering, common measurements are mostly performed in United States customary units, although these have officially been defined in terms of SI units. The United Kingdom has officially adopted a policy of partial metrication. Canada has adopted the SI for most governmental, medical and scientific purposes and for such varied uses as grocery weights, weather reports, traffic signs and gasoline sales, but imperial units are still legally permitted and remain in common use throughout many sectors of Canadian society, particularly in the building trade and the railway sector.
History.
The metric system was first implemented during the French Revolution (1790s) with just the metre and kilogram as standards of length and mass respectively. In the 1830s Carl Friedrich Gauss laid the foundations for a coherent system based on length, mass, and time. In the 1860s a group working under the auspices of the British Association for the Advancement of Science formulated the requirement for a coherent system of units with base units and derived units. The inclusion of electrical units into the system was hampered by the customary use of more than one set of units, until 1900 when Giovanni Giorgi identified the need to define one single electrical quantity as a fourth base quantity alongside the original three base quantities.
Meanwhile, in 1875, the Treaty of the Metre passed responsibility for verification of the kilogram and metre against agreed prototypes from French to international control. In 1921, the Treaty was extended to include all physical quantities including electrical units originally defined in 1893.
In 1948, an overhaul of the metric system was set in motion which resulted in the development of the "Practical system of units" which, on its publication in 1960, was given the name "The International System of Units". In 1954, the 10th General Conference on Weights and Measures (CGPM) identified electric current as the fourth base quantity in the practical system of units and added two more base quantities—temperature and luminous intensity—making six base quantities in all. The units associated with these quantities were the metre, kilogram, second, ampere, kelvin and candela. In 1971, a seventh base quantity, amount of substance represented by the mole, was added to the definition of SI.
Early development.
The metric system was developed from 1791 onwards by a committee of the French Academy of Sciences, commissioned by the National Assembly and Louis XVI to create a unified and rational system of measures. The group, which included Antoine Lavoisier (the "father of modern chemistry") and the mathematicians Pierre-Simon Laplace and Adrien-Marie Legendre, used the same principles for relating length, volume, and mass that had been proposed by the English clergyman John Wilkins in 1668; </ref> and the concept of using the Earth's meridian as the basis of the definition of length, originally proposed in 1670 by the French abbot Mouton.
On 30 March 1791, the Assembly adopted the committee's proposed principles for the new decimal system of measure and authorised a survey between Dunkirk and Barcelona to establish the length of the meridian. On 11 July 1792, the committee proposed the names "metre", "are", "litre" and "grave" for the units of length, area, capacity, and mass, respectively. The committee also proposed that multiples and submultiples of these units were to be denoted by decimal-based prefixes such as "centi" for a hundredth and "kilo" for a thousand.
The law of 7 April 1795 () defined the terms "gramme" and "kilogramme", which replaced the former terms "gravet" (correctly "milligrave") and "grave", and on 22 June 1799 (after Pierre Méchain and Jean-Baptiste Delambre had completed the meridian survey) the definitive standard "mètre des Archives" and "kilogramme des Archives" were deposited in the "Archives nationales". On 10 December 1799 (a month after Napoleon's coup d'état), the law by which the metric system was to be definitively adopted in France () 
was passed.
During the first half of the nineteenth century there was little consistency in the choice of preferred multiples of the base units – typically the myriametre ( metres) was in widespread use in both France and parts of Germany, while the kilogram ( grams) rather than the myriagram was used for mass.
In 1832, the German mathematician Carl Friedrich Gauss, assisted by Wilhelm Weber, implicitly defined the second as a base unit when he quoted the earth's magnetic field in terms of millimetres, grams, and seconds. Prior to this, the strength of the earth’s magnetic field had only been described in relative terms. The technique used by Gauss was to equate the torque induced on a suspended magnet of known mass by the earth’s magnetic field with the torque induced on an equivalent system under gravity. The resultant calculations enabled him to assign dimensions based on mass, length and time to the magnetic field.
In the 1860s, James Clerk Maxwell, William Thomson (later Lord Kelvin) and others working under the auspices of the British Association for the Advancement of Science, built on Gauss' work and formalised the concept of a coherent system of units with base units and derived units. The principle of coherence was successfully used to define a number of units of measure based on the centimetre–gram–second (CGS) system of units (CGS), including the erg for energy, the dyne for force, the barye for pressure, the poise for dynamic viscosity and the stokes for kinematic viscosity.
Metre Convention.
A French-inspired initiative for international cooperation in metrology led to the signing in 1875 of the Metre Convention. Initially the convention only covered standards for the metre and the kilogram. A set of 30 prototypes of the metre and 40 prototypes of the kilogram, in each case made of a 90% platinum-10% iridium alloy, were manufactured by the British firm Johnson, Matthey & Co and accepted by the CGPM in 1889. One of each was selected at random to become the International prototype metre and International prototype kilogram that replaced the "mètre des Archives" and "kilogramme des Archives" respectively. Each member state was entitled to one of each of the remaining prototypes to serve as the national prototype for that country.
The treaty established three international organisations to oversee the keeping of international standards of measurement:
In 1921, the Metre Convention was extended to include all physical units, including the ampere and others defined by the Fourth International Conference of Electricians in Chicago in 1893, thereby enabling the CGPM to address inconsistencies in the way that the metric system had been used.
The official language of the Metre Convention is French and the definitive version of all official documents published by or on behalf of the CGPM is the French-language version.
Towards the SI.
At the close of the 19th century three different systems of units of measure existed for electrical measurements: a CGS-based system for electrostatic units, also known as the Gaussian or ESU system, a CGS-based system for electromechanical units (EMU) and an MKS-based system ("international system") for electrical distribution systems. 
Attempts to resolve the electrical units in terms of length, mass, and time using dimensional analysis was beset with difficulties—the dimensions depended on whether one used the ESU or EMU systems. This anomaly was resolved in 1900 when Giovanni Giorgi published a paper in which he advocated using a fourth base unit alongside the existing three base units. The fourth unit could be chosen to be electric current , voltage, or electrical resistance.
In the late 19th and early 20th centuries, a number of non-coherent units of measure based on the gram/kilogram, the centimetre/metre, and the second, such as the "Pferdestärke" (metric horsepower) for power, the darcy for permeability and the use of "millimetres of mercury" for the measurement of both barometric and blood pressure were developed or propagated, some of which incorporated standard gravity in their definitions.
At the end of the Second World War, a number of different systems of measurement were in use throughout the world. Some of these systems were metric system variations, whereas others were based on customary systems of measure. In 1948, after representations by the International Union of Pure and Applied Physics (IUPAP) and by the French Government, the 9th General Conference on Weights and Measures (CGPM) asked the International Committee for Weights and Measures (CIPM) to conduct an international study of the measurement needs of the scientific, technical, and educational communities and "to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention".
On the basis of the findings of this study, the 10th CGPM in 1954 decided that an international system should be derived from six base units to provide for the measurement of temperature and optical radiation in addition to mechanical and electromagnetic quantities. Six base units were recommended: the metre, kilogram, second, ampere, degree Kelvin (later renamed kelvin), and candela. In 1960, the 11th CGPM named the system the "International System of Units", abbreviated SI from the French name, . The BIPM has also described SI as "the modern metric system". The seventh base unit, the mole, was added in 1971 by the 14th CGPM.
International System of Quantities.
The International System of Quantities (ISQ) is a system based on seven base quantities: length, mass, time, electric current, thermodynamic temperature, amount of substance, and luminous intensity. Other quantities such as area, pressure, and electrical resistance are derived from these base quantities by clear non-contradictory equations. The ISQ defines the quantities that are measured with the SI units. The ISQ is defined in the international standard ISO/IEC 80000, and was finalised in 2009 with the publication of ISO 80000-1.
SI Brochure and conversion factors.
The CGPM publishes a brochure which defines and presents SI. Its official version is in French, in line with the Metre Convention. It leaves some scope for local interpretation, particularly regarding names and terms in different languages, so for example the United States' National Institute of Standards and Technology (NIST) has produced a version of the CGPM document (NIST SP 330) which clarifies local interpretation for English-language publications that use American English and another document (NIST SP 811) that gives general guidance for the use of SI in the United States and conversion factors between SI and customary units.
The writing and maintenance of the CGPM brochure is carried out by one of the committees of the International Committee for Weights and Measures (CIPM), the Consultative Committee for Units (CCU). The CIPM nominates the chairman of this committee, but the committee includes representatives of various other international bodies rather than CIPM or CGPM nominees. This committee thus provides a forum for the bodies concerned to provide input to the CIPM in respect of ongoing enhancements to SI.
The definitions of the terms "quantity", "unit", "dimension" etc. that are used in the "SI Brochure" are those given in the International vocabulary of metrology, a publication produced by the Joint Committee for Guides in Metrology (JCGM), a working group consisting of eight international standards organisations under the chairmanship of the director of the BIPM. The quantities and equations that define the SI units are now referred to as the "International System of Quantities" (ISQ), and are set out in the International Standard "ISO/IEC 80000 Quantities and Units".
Units and prefixes.
The International System of Units consists of a set of base units, a set of derived units with special names, and a set of decimal-based multipliers that are used as prefixes. The term "SI Units" covers all three categories, but the term "coherent SI units" includes only base units and coherent derived units.
Base units.
The SI base units are the building blocks of the system and all other units are derived from them. When Maxwell first introduced the concept of a coherent system, he identified three quantities that could be used as base units: mass, length and time. Giorgi later identified the need for an electrical base unit. Theoretically any one of electric current, potential difference, electrical resistance, electrical charge or a number of other quantities could have provided the base unit, with the remaining units then being defined by the laws of physics. In the event, the unit of electric current was chosen for SI. Another three base units (for temperature, substance and luminous intensity) were added later.
Derived units.
The derived units in the SI are formed by powers, products or quotients of the base units and are unlimited in number. Derived units are associated with derived quantities, for example velocity is a quantity that is derived from the base quantities of time and length, so in SI the derived unit is metres per second (symbol m/s). The dimensions of derived units can be expressed in terms of the dimensions of the base units.
Coherent units are derived units that contain no numerical factor other than 1—quantities such as standard gravity and density of water are absent from their definitions. In the example above, "one" newton is the force required to accelerate a mass of "one" kilogram by "one" metre per second squared. Since the SI units of mass and acceleration are kg and m·s−2 respectively and , the units of force (and hence of newtons) is formed by multiplication to give kg·m·s−2. Since the newton is part of a coherent set of units, the constant of proportionality is 1.
For the sake of convenience, some derived units have special names and symbols. Such units may themselves be used in combination with the names and symbols for base units and for other derived units to express the units of other derived quantities. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as "newtons per square metre" (N/m2).
Prefixes.
Prefixes are added to unit names to produce multiple and sub-multiples of the original unit. All multiples are integer powers of ten, and above a hundred or below a hundredth all are integer powers of a thousand. For example, "kilo-" denotes a multiple of a thousand and "milli-" denotes a multiple of a thousandth, so there are one thousand millimetres to the metre and one thousand metres to the kilometre. The prefixes are never combined, so for example a millionth of a metre is a "micrometre", not a millimillimetre. Multiples of the kilogram are named as if the gram were the base unit, so a millionth of a kilogram is a "milligram", not a microkilogram.
Non-SI units accepted for use with SI.
Although, in theory, SI can be used for any physical measurement, the CIPM has recognised that some non-SI units still appear in the scientific, technical, and commercial literature, and will continue to be used for many years to come. In addition, certain other units are so deeply embedded in the history and culture of the human race that they will continue to be used for the foreseeable future. The CIPM has catalogued several such units and published them in the SI Brochure so that their use may be consistent around the world. These units have been grouped as follows: 
Writing unit symbols and the values of quantities.
Before 1948, the writing of metric quantities was haphazard. In 1879, the CIPM published recommendations for writing the symbols for length, area, volume and mass, but it was outside its domain to publish recommendations for other quantities. Beginning in about 1900, physicists who had been using the symbol "μ" for "micrometre" (or "micron"), "λ" for "microlitre", and "γ" for "microgram" started to use the symbols "μm", "μL" and "μg", but it was only in 1935, a decade after the revision of the Metre Convention that the CIPM formally adopted this proposal and recommended that the symbol "μ" be used universally as a prefix for .
In 1948, the ninth CGPM approved the first formal recommendation for the writing of symbols in the metric system when the basis of the rules as they are now known was laid down. These rules were subsequently extended by International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) and now cover unit symbols and names, prefix symbols and names, how quantity symbols should be written and used and how the values of quantities should be expressed. Both ISO and the IEC have published rules for the presentation of SI units that are generally compatible with those published in the SI Brochure. ISO and IEC were in the process of merging their standards for quantities and units into a single set of compatible documents identified as the ISO/IEC 80000 Standard. The rules covering printing of quantities and units are part of ISO 80000-1:2009.
Unit names.
Names of units follow the grammatical rules associated with common nouns: in English and in French they start with a lowercase letter (e.g., newton, hertz, pascal), even when the symbol for the unit begins with a capital letter. This also applies to "degrees Celsius", since "degree" is the unit. In German, however, the names of units, as with all German nouns, start with capital letters. The spelling of unit names is a matter for the guardians of the language concerned – the official British and American spellings for certain SI units differ – British English, as well as Australian, Canadian and New Zealand English, uses the spelling "deca-", "metre", and "litre" whereas American English uses the spelling "deka-", "meter", and "liter", respectively.
Likewise, the plural forms of units follow the grammar of the language concerned: in English, the normal rules of English grammar are used, e.g. "henries" is the plural of "henry". However, the units lux, hertz, and siemens have irregular plurals in that they remain the same in both their singular and plural form.
In English, when unit names are combined to denote multiplication of the units concerned, they are separated with a hyphen or a space (e.g. newton-metre or newton metre). The plural is formed by converting the last unit name to the plural form (e.g. ten newton-metres).
Unit names as adjectives.
In English, a space is recommended between the number and the unit symbol when used as an adjective, e.g. "a 25 kg sphere".
The normal rules of English apply to unit names, where a hyphen is incorporated into the adjectival sense, e.g. "a 25-kilogram sphere".
Chinese and Japanese.
Chinese uses traditional logograms for writing the unit names, while in Japanese unit names are written in the phonetic katakana script; in both cases symbols are written using the internationally recognised Latin and Greek characters.
A set of characters representing various metric units was created in Japan in the late 19th century. Characters exist for three base units: the metre (), litre () and gram (). These were combined with a set of six prefix characters – "kilo-" (), "hecto-" (), "deca-" (), "deci-" (), "centi-" () and "milli-" () – to form an additional 18 single-character units. The seven length units (kilometre to millimetre), for example, are and . These characters, however, are not in common use today; instead, units are written out in katakana, the Japanese syllabary used for foreign borrowings, such as "" ("kiromētoru") for "kilometre". A few Sino-Japanese words for these units remain in use in Japanese, most significantly "" ("heibei") for "square metre", but otherwise borrowed pronunciations are used.
These characters are examples of the rare phenomenon of single-character loan words – a foreign word represented by a single Japanese character – and form the plurality of such words. Similar characters were also coined for other units, such as British units, though these also have fallen out of use; see Single character gairaigo: Metric units and Single character gairaigo: Other units for a full list.
The basic units are metre ( "mǐ"), litre ( "shēng"), gram ( "kè"), and second ( "miǎo"), while others include watt ( "wǎ"). Prefixes include "deci-" ( "fēn"), "centi-" ( "lí"), "milli-" ( "háo"), "micro-" ( "wēi"), "kilo-" ( "qiān"), and "mega-" ( "zhào"). These are combined to form disyllabic characters, such as "límǐ" "centimetre" or "qiānwǎ" "kilowatt". In the 19th century various compound characters were also used, similar to Japanese, either imported or formed on the same principles, such as for "qiānwǎ" (kilowatt) or for . These are generally not used today – for example centimetres is usually written "límǐ" – but are occasionally found in older or technical writing.
Unit symbols and the values of quantities.
Although the writing of unit names is language-specific, the writing of unit symbols and the values of quantities is consistent across all languages and therefore the SI Brochure has specific rules in respect of writing them. The guideline produced by the National Institute of Standards and Technology (NIST) clarifies language-specific areas in respect of American English that were left open by the SI Brochure, but is otherwise identical to the SI Brochure.
General rules.
General rules for writing SI units and quantities apply to text that is either handwritten or produced using an automated process:
Printing SI symbols.
Further rules are specified in respect of production of text using printing presses, word processors, typewriters and the like.
Realisation of units.
Metrologists carefully distinguish between the definition of a unit and its realisation. The definition of each base unit of the SI is drawn up so that it is unique and provides a sound theoretical basis on which the most accurate and reproducible measurements can be made. The realisation of the definition of a unit is the procedure by which the definition may be used to establish the value and associated uncertainty of a quantity of the same kind as the unit. A description of the "mise en pratique" of the base units is given in an electronic appendix to the SI Brochure.
The published "mise en pratique" is not the only way in which a base unit can be determined: the SI Brochure states that "any method consistent with the laws of physics could be used to realise any SI unit." In the current (2016
) exercise to overhaul the definitions of the base units, various consultative committees of the CIPM have required that more than one "mise en pratique" shall be developed for determining the value of each unit. In particular:
Post-1960 changes.
The preamble to the Metre Convention read ""Desiring the international uniformity and precision in standards of weight and measure, have resolved to conclude a convention ..."". Changing technology has led to an evolution of the definitions and standards that has followed two principal strands – changes to SI itself and clarification of how to use units of measure that are not part of SI, but are still nevertheless used on a worldwide basis.
Changes to the SI.
Since 1960 the CGPM has made a number of changes to SI. These include:
In addition, advantage was taken of developments in technology to redefine many of the base units enabling the use of higher precision techniques.
Retention of non-SI units.
Although, in theory, SI can be used for any physical measurement, it is recognised that some non-SI units still appear in the scientific, technical and commercial literature, and will continue to be used for many years to come. In addition, certain other units are so deeply embedded in the history and culture of the human race that they will continue to be used for the foreseeable future. The CIPM has catalogued such units and included them in the SI Brochure so that they can be used consistently.
The first such group comprises the units of time and of angles and certain legacy non-SI metric units. Most of mankind has used the day and its subdivisions as a basis of time with the result that the second, minute, hour and day, unlike the foot or the pound, were the same regardless of where it was being measured. The second has been catalogued as an SI unit, its multiples as units of measure that may be used alongside the SI. The measurement of angles has likewise had a long history of consistent use – the radian, being of a revolution, has mathematical niceties, but it is cumbersome for navigation, hence the retention of the degree, minute and second of arc. The tonne, litre and hectare were adopted by the CGPM in 1879 and have been retained as units that may be used alongside SI units, having been given unique symbols.
Physicists often use units of measure that are based on natural phenomena such as the speed of light, the mass of a proton (approximately one dalton), the charge of an electron and the like. These too have been catalogued in the SI Brochure with consistent symbols, but with the caveat that their physical values need to be measured.
In the interests of standardising health-related units of measure used in the nuclear industry, the 12th CGPM (1964) accepted the continued use of the curie (symbol Ci) as a non-SI unit of activity for radionuclides; the becquerel, sievert and gray were adopted in later years. Similarly, the millimetre of mercury (symbol mmHg) was retained for measuring blood pressure.
Global adoption.
SI has become the world's most widely used system of measurement, used in both everyday commerce and science. The change to SI had little effect on everyday life in countries that used the metric system – the metre, kilogram, litre and second remained unchanged as did the way in which they were used – most of the changes only affected measurements in the workplace. The CGPM has a role of recommending changes, but no formal role in the enforcement of such changes—another inter-governmental organisation, the International Organization of Legal Metrology (OIML) provides a forum for harmonisation of national standards and legislation in respect of metrology.
Both the degree and rate of adoption of SI varied from country to country—countries that had not adopted the metric system by 1960 and subsequently adopted SI did so directly as part of their metrication programs while others migrated from the CGS system of units to SI. In 1960, the world's largest economy was that of the United States, followed by the United Kingdom, West Germany, France, Japan, China and India. The United States and the United Kingdom were non-metric, France and Germany had been using the metric system for about a century, and China had been using the metric system for 35 years, while India and Japan had adopted the metric system within the preceding five years. Other non-metric countries were those where the United Kingdom or the United States had considerable influence. These differences are brought out in the examples below:
United Kingdom and the former British Empire.
Even though the use of metric units was legalised for trade in the UK in 1864, the UK had signed the Metre Convention in 1884 and the UK Parliament had defined the yard and the pound in terms of the metre and the kilogram in 1897, the UK continued to use the imperial system of measure 
and to export the imperial system of units to the Empire. In 1932, the system of Imperial Preference was set up at the Ottawa Conference. Although Ireland left the Commonwealth in 1948 and South Africa in 1961, both continued their close economic ties with the Commonwealth.
When the SI standard was published in 1960, the only major Commonwealth country to have adopted the metric system was India. In 1863, the first reading of a bill that would have made the metric system compulsory passed its first reading in the House of Commons by 110 votes to 75. The bill, however, failed to make the statute book because of lack of parliamentary time. In 1965, after this and similar false starts the then Federation of British Industry informed the British Government that its members favoured the adoption of the metric system. The rationale behind the request was that 80% of British exports were to countries that used the metric system or that were considering changing to the metric system. The Board of Trade, on behalf of the Government, agreed to support a ten-year metrication programme. The government agreed to a voluntary policy requiring minimal legislation and costs to be borne where they fell. SI would be used from the outset. The rest of the Commonwealth, South Africa and Ireland followed within a few years; in some countries such as South Africa and Australia metrication was mandatory rather than voluntary.
By 1980 all apart from the United Kingdom, Canada and Ireland had effectively completed their programs. In the United Kingdom the breakdown of voluntary metrication in the mid-1970s coincided with the United Kingdom's obligations as part of the EEC to adopt the metric system, resulting in legislation to force metrication in certain areas and the Eurosceptic movement adopting an anti-metrication stance and the United Kingdom seeking a number of derogations from the relevant EEC directives. Once the metrication of most consumer goods was completed in 2000, aspects of British life, especially in government, commerce and industry used SI. Although SI or units approved for use alongside SI are used in most areas where units of measure are regulated imperial units are widely encountered in unregulated areas such as the press and everyday speech. Canada has adopted it for most purposes, but imperial units are still legally permitted and remain in common use throughout a few sectors of Canadian society, particularly in the buildings, trades and railways sectors. The situation in Ireland, apart from road signs which were metricated in the early 2000s, is similar to that in the United Kingdom.
European Union.
In 1960, all the largest industrialised nations that had an established history of using the metric system were members of the European Economic Community (EEC).
In 1972, in order to harmonise units of measure as part of a programme to facilitate trade between member states, the EEC issued directive 71/354/EEC. This directive catalogued units of measure that could be used for "economic, public health, public safety and administrative purposes" and also provided instructions for a transition from the existing units of measure that were in use. The directive replicated the CGPM SI recommendations and in addition pre-empted some of the additions whose use had been recommended by the CIPM in 1969, but had not been ratified by the CGPM. The directive also catalogued units of measure whose status would be reviewed by the end of 1977 (mainly coherent CGS units of measure) and also catalogued units of measure that were to be phased out by the end of 1977, including the use of obsolete names for the sale of timber such as the stere, the use of units of force and pressure that made use of the acceleration due to gravity, the use of non-coherent units of power such as the Pferdestärke (PS), the use of the calorie as a measure of energy and the stilb as a measure of luminance. The directive was silent in respect of units that were specific to one or two countries including the "pond", "pfund", "livre" (Dutch, German and French synonyms for 500 g), thereby effectively prohibiting their use as well.
When the directive was revisited during 1977, some of the older units that were being reviewed (such as millimetre of mercury for blood pressure) were retained but others were phased out, thereby broadly aligning the allowable units with SI. The directive was however overhauled to accommodate British and Irish interests in retaining the imperial system in certain circumstances. It was reissued as directive 80/181/EEC. During subsequent revisions, the directive has reflected changes in the definition of SI. The directive also formalised the use of "supplementary units", which in 1979 were permitted for a period of ten years. The cut-off date for the use of supplementary units was extended a number of times and in 2009 was extended indefinitely.
India.
India was one of the last countries to start a metrication programme before the advent of SI. When it became independent in 1947, both imperial and native units of measure were in use. Its metrication programme started in 1956 with the passing of the Standards of Weights and Measures Act. Part of the act fixed the value of the seer (a legacy unit of mass) to 0.9331 kg exactly; elsewhere the Act declared that from 1960 all non-metric units of measure were to be illegal.
Four years after the Indian Government announced its metrication programme, SI was published. The result was that the initial metrication programme was a conversion to the CGS system of units and the subsequent adoption of SI has been haphazard. Fifty years later, many of the country's schoolbooks still use CGS or imperial units. Originally the Indian Government had planned to replace all units of measure with metric units by 1960. In 1976 a new Weights and Measures Act replaced the 1956 Act which, amongst other things, required that all weighing devices be approved before being released onto the market place. However, in 2012, it was reported that traditional units were still encountered in small manufacturing establishments and in the marketplace alongside CGS, SI and imperial measures, particularly in the poorer areas.
The use of the Indian numbering system of crores () and lakhs (), which do not map onto the SI system of prefixes, is widespread and is often found alongside or in place of the western numbering system.
United States.
Even though Congress set up a framework for the use of the metric system in the nineteenth century, the United States continues to use US customary units, based on English measure passed by parliament under the reign of Queen Anne in 1706, for most purposes apart from science and medicine. In Puerto Rico, metric units are widely used due to the vast majority of the population having Spanish heritage.
On 10 February 1964, the National Bureau of Standards (now the National Institute of Standards and Technology) issued a statement that it was to use SI except where this would have an obvious detrimental effect. In 1968 Congress authorised the U.S. Metric Study the emphasis of which was to examine the feasibility of adopting SI. The first volume was delivered in 1970. The study recommended that the United States adopt the International System of units, and in 1975 Congress passed the Metric Conversion Act of 1975 which established a national policy of coordinating and planning for the increased use of the metric measurement system in the United States. Metrication was voluntary and to be coordinated by the United States Metric Board (USMB).
Efforts during the Ford and Carter administrations to force metrication were seized on by many newspaper editorialists as being dictatorial. Public response included resistance, apathy, and sometimes ridicule. The underlying reasons for this response include a relative uniformity of weights and measures (though, notably, US liquid measure differed by about 20% from British Imperial measure, which was adopted throughout the British Empire in 1824) inherited from the United Kingdom in 1776, a homogeneous economy and the influence of business groups and populists in Congress caused the country to look at the short-term costs associated with the change-over, particularly those that would be borne by the consumer rather than long-term benefits of efficiency and international trade. The Metrication Board was disbanded under President Ronald Reagan's direction in 1982.
The 1988 Omnibus Foreign Trade and Competitiveness Act removed international trade barriers and amended the Metric Conversion Act of 1975, designating the metric system as "the Preferred system of weights and measures for United States trade and commerce". The legislation stated that the federal government has a responsibility to assist industry, especially small business, as it voluntarily converts to the metric system of measurement. Exceptions were made for the highway and construction industries; the Department of Transportation planned to require metric units by 2000, but this plan was cancelled by the 1998 highway bill TEA21. However, the US military uses the metric system widely, partly because of the need to work with armed services from other nations.
Although overall responsibility for labelling requirements of consumer goods lies with Congress and is therefore covered by federal law, details of labelling requirements for certain commodities are controlled by state law or by other authorities such as the Food and Drug Administration, Environmental Protection Agency and Alcohol and Tobacco Tax and Trade Bureau.
The federal Fair Packaging and Labeling Act (FPLA), originally passed in 1964, was amended in 1992 to require consumer goods directly under its jurisdiction to be labelled in both customary and metric units. Some industries are engaged in efforts to amend this law to allow manufacturers to use only metric labelling. The National Conference on Weights and Measures has developed the Uniform Packaging and Labeling Regulations (UPLR) which provides a standard approach to those sections of packaging law that are under state control. Acceptance of the UPLR varies from state to state – fourteen states accept it by merely citing it in their legislation. 
During the first decade of the 21st century, the EU directive 80/181/EEC had required that dual unit labelling of goods sold within the EU cease by the end of 2009. This was backed up by requests from other nations including Japan and New Zealand to permit metric-only labelling as an aid to trade with those countries. Opinion in the United States was split – a bill to permit metric-only labelling at the federal level was to have been introduced in 2005 but significant opposition from the Food Marketing Institute, representing US grocers, has delayed the introduction of the bill. During a routine decennial review of the directive in 2008, the EU postponed the sunset clause for dual units indefinitely.
Meanwhile, in 1999 the UPLR was amended to permit metric-only labelling and automatically became law in those states that accept UPLR "as is". By 1 January 2009, 48 out of 50 states permit metric-only labelling, either through UPLR or through their own legislation. the use of metric (and therefore SI) units in the United States does not follow any pattern. Dual-unit labelling on consumer goods is mandatory. Some consumer goods such as soft drinks are sold in metric quantities, but milk is sold in customary units. The engineering industry is equally split. The automotive industry is largely metric, but aircraft such as the Boeing 787 Dreamliner were designed using customary units.
Redefinition of units.
After the metre was redefined in 1960, the kilogram remained the only SI base unit that relied on a specific physical artifact, the international prototype of the kilogram (IPK), for its definition and thus the only unit that was still subject to periodic comparisons of national standard kilograms with the IPK. After the 1996–1998 recalibration, a clear divergence between the various prototype kilograms was observed.
At its 23rd meeting, held in 2007, the CGPM recommended that the CIPM should continue to investigate methods to provide exact fixed values for physical constants of nature that could then be used in the definitions of units of measure in place of the IPK, thus enabling the transition from explicit unit definitions to explicit constant definitions.
At a meeting of the CCU held in Reading, United Kingdom, in September 2010, a resolution and draft changes to the SI Brochure that were to be presented to the next meeting of the CIPM in October 2010 were agreed to in principle. The proposals that the CCU put forward were:
The CIPM meeting of October 2010 reviewed progress towards establishing fixed values for the constants but found that "the conditions set by the General Conference at its 23rd meeting have not yet been fully met. For this reason the CIPM does not propose a revision of the SI at the present time".
At the 24th CGPM meeting, held in October 2011, the CIPM sponsored a resolution in which the requisite definition changes were agreed to in principle and in which the conditions required to be met before the redefinitions could be implemented were restated.
By November 2014 the conditions set out at the 23rd meeting of the CGPM for the unit redefinitions had still not been met, and the 25th meeting of the CGPM, held in November 2014, adopted a similar resolution encouraging further work towards establishing fixed values for the fundamental constants.
The redefinitions are expected to be adopted at the 26th CGPM in the fall of 2018. The CODATA task group on fundamental constants has announced special submission deadlines for data to compute the values that will be announced at this event.

</doc>
<doc id="26766" url="https://en.wikipedia.org/wiki?curid=26766" title="Sapiens">
Sapiens

Sapiens, a Latin word meaning wise, may refer to :

</doc>
<doc id="26768" url="https://en.wikipedia.org/wiki?curid=26768" title="Sirenia">
Sirenia

Sirenia (commonly referred to as sea cows) are an order of fully aquatic, herbivorous mammals that inhabit swamps, rivers, estuaries, marine wetlands, and coastal marine waters. Four species are living, in two families and genera. These are the dugong (one species) and manatees (three species). Sirenia also includes Steller's sea cow, extinct since the 18th century, and a number of taxa known only from fossils. The order evolved during the Eocene, more than 50 million years ago.
Sirenia, commonly sirenians, are also referred to by the common name sirens, deriving from the sirens of Greek mythology. This comes from a legend about their discovery, involving lonely sailors mistaking them for mermaids.
"Sea cow" ("seekoei") is also the name for a hippopotamus in Afrikaans. In some Germanic languages, the word "Sea" can mean either a body of fresh or salt water, so this follows from the species inhabiting lakes in southern Africa rather than the sea itself.
Description.
Sirenians have major aquatic adaptations: the forelimbs have modified into arms used for steering, the tail has modified into a paddle used for propulsion, and the hindlimbs (legs) are but two small remnant bones floating deep in the muscle. They appear fat, but are fusiform, hydrodynamic, and highly muscular. Their skulls are highly modified for taking breaths of air at the water's surface, and dentition is greatly reduced. The skeletal bones of both the manatees and dugong are very dense, which helps to neutralize the buoyancy of their blubber. The manatee appears to have an almost unlimited ability to produce new teeth as the anterior teeth wear down. They have only two teats, located under their forelimbs, similar to elephants. The elephants are thought to be the closest living relatives of the sirenians.
The lungs of sirenians are unlobed. In sirenians, the lungs and diaphragm extend the entire length of the vertebral column. These adaptations help sirenians control their buoyancy and maintain their horizontal position in the water.
Living sirenians grow between 2.5 and 4.0 meters long and can weigh up to 1,500 kg. "Hydrodamalis gigas", Steller's sea cow, could reach lengths of 8 m.
The three manatee species (family Trichechidae) and the dugong (family Dugongidae) are endangered species. All four are vulnerable to extinction from habitat loss and other negative impacts related to human population growth and coastal development. Steller's sea cow, extinct since 1786, was hunted to extinction by humans. Manatees and dugongs are the only marine mammals classified as herbivores. Unlike the other marine mammals (dolphins, whales, seals, sea lions, sea otters, and walruses), sirenians eat primarily sea grasses and other aquatic vegetation, and have an extremely low metabolism and poor tolerance for especially cold water (the Steller's sea cow, which inhabited the cold waters of the northern Pacific, was an exception). Sirenians have been observed eating dead animals (sea gulls), but their diets are made up primarily of vegetation. Like dolphins and whales, manatees and dugongs are completely aquatic mammals that never leave the water—not even to give birth. These animals have been observed eating grass clippings from homes adjacent to waterways, but in this rare occurrence, only the top portion of the sirenian is lifted out of the water. The combination of these factors means sirenians are restricted to warm, shallow, coastal waters, estuaries, and rivers with healthy ecosystems that support large amounts of seagrass or other vegetation.
The Trichechidae species differ from the Dugongidae in the shape of their skull and tails.
Classification.
The order Sirenia has been placed in the clade Paenungulata, within Afrotheria, grouping it with two other orders of living mammals: Proboscidea, the elephant families, and Hyracoidea, the hyraxes, and two extinct orders, Embrithopoda and Desmostylia.
Subdivision.
After Voss, 2014.
† extinct

</doc>
<doc id="26769" url="https://en.wikipedia.org/wiki?curid=26769" title="South America">
South America

South America is a continent situated in the Western Hemisphere, mostly in the Southern Hemisphere, with a relatively small portion in the Northern Hemisphere. It is also considered as a subcontinent of the Americas, which is the model used in Spanish-speaking nations and most of South America.
It is bordered on the west by the Pacific Ocean and on the north and east by the Atlantic Ocean; North America and the Caribbean Sea lie to the northwest. It includes twelve sovereign states – Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Guyana, Paraguay, Peru, Suriname, Uruguay, and Venezuela – and two non-sovereign areas – French Guiana, an overseas department of France, and the Falkland Islands, a British Overseas Territory (though disputed by Argentina). In addition to this, the ABC islands of the Netherlands and Trinidad and Tobago may also be considered part of South America.
South America has an area of 17,840,000 square kilometers (6,890,000 sq mi). Its population has been estimated at more than 371,090,000. South America ranks fourth in area (after Asia, Africa, and North America) and fifth in population (after Asia, Africa, Europe, and North America). Brazil is by far the most populous South American country, with more than half of the continent's population, followed by Colombia, Argentina, Venezuela and Peru.
Most of the population lives near the continent's western or eastern coasts while the interior and the far south are sparsely populated. The geography of western South America is dominated by the Andes mountains; in contrast, the eastern part contains both highland regions and large lowlands where rivers such as the Amazon, Orinoco, and Paraná flow. Most of the continent lies in the tropics.
The continent's cultural and ethnic outlook has its origin with the interaction of indigenous peoples with European conquerors and immigrants and, more locally, with African slaves. Given a long history of colonialism, the overwhelming majority of South Americans speak Portuguese or Spanish, and societies and states commonly reflect Western traditions.
Geography.
South America occupies the southern portion of the Americas. The continent is generally delimited on the northwest by the Darién watershed along the Colombia–Panama border, although some may consider the border instead to be the Panama Canal. Geopolitically and geographically all of Panama – including the segment east of the Panama Canal in the isthmus – is typically included in North America alone and among the countries of Central America. Almost all of mainland South America sits on the South American Plate.
South America is home to the world's highest uninterrupted waterfall, Angel Falls in Venezuela; the highest single drop waterfall Kaieteur Falls in Guyana; the largest river (by volume), the Amazon River; the longest mountain range, the Andes (whose highest mountain is Aconcagua at ); the driest non-polar place on earth, the Atacama Desert; the largest rainforest, the Amazon Rainforest; the highest capital city, La Paz, Bolivia; the highest commercially navigable lake in the world, Lake Titicaca; and, excluding research stations in Antarctica, the world's southernmost permanently inhabited community, Puerto Toro, Chile.
South America's major mineral resources are gold, silver, copper, iron ore, tin, and petroleum. These resources found in South America have brought high income to its countries especially in times of war or of rapid economic growth by industrialized countries elsewhere. However, the concentration in producing one major export commodity often has hindered the development of diversified economies. The fluctuation in the price of commodities in the international markets has led historically to major highs and lows in the economies of South American states, often causing extreme political instability. This is leading to efforts to diversify production to drive away from staying as economies dedicated to one major export.
South America is one of the most biodiverse continents on earth. South America is home to many interesting and unique species of animals including the llama, anaconda, piranha, jaguar, vicuña, and tapir. The Amazon rainforests possess high biodiversity, containing a major proportion of the Earth's species.
Brazil is the largest country in South America, encompassing around half of the continent's land area and population. The remaining countries and territories are divided among three regions: The Andean States, the Guianas and the Southern Cone.
Outlying islands.
Traditionally, South America also includes some of the nearby islands. Aruba, Bonaire, Curaçao, Trinidad, Tobago, and the federal dependencies of Venezuela sit on the northerly South American continental shelf and are often considered part of the continent. Geo-politically, the island states and overseas territories of the Caribbean are generally grouped as a part or subregion of North America, since they are more distant on the Caribbean Plate, even though San Andres and Providencia are politically part of Colombia and Aves Island is controlled by Venezuela.
Other islands that are included with South America are the Galápagos Islands that belong to Ecuador and Easter Island (in Oceania but belonging to Chile), Robinson Crusoe Island, Chiloé (both Chilean) and Tierra del Fuego (split between Chile and Argentina). In the Atlantic, Brazil owns Fernando de Noronha, Trindade and Martim Vaz, and the Saint Peter and Saint Paul Archipelago, while the Falkland Islands are governed by the United Kingdom, whose sovereignty over the islands is disputed by Argentina. South Georgia and the South Sandwich Islands may be associated with either South America or Antarctica.
History.
Prehistory.
South America is believed to have been joined with Africa from the late Paleozoic Era to the early Mesozoic Era, until the supercontinent Pangaea began to rift and break apart about 225 million years ago. Therefore, South America and Africa share similar fossils and rock layers.
South America is thought to have been first inhabited by humans when people were crossing the Bering Land Bridge (now the Bering Strait) at least 15,000 years ago from the territory that is present-day Russia. They migrated south through North America, and eventually reached South America through the Isthmus of Panama.
The first evidence for the existence of the human race in South America dates back to about 9000 BC, when squashes, chili peppers and beans began to be cultivated for food in the highlands of the Amazon Basin. Pottery evidence further suggests that manioc, which remains a staple food today, was being cultivated as early as 2000 BC.
By 2000 BC, many agrarian communities had been settled throughout the Andes and the surrounding regions. Fishing became a widespread practice along the coast, helping establish fish as a primary source of food. Irrigation systems were also developed at this time, which aided in the rise of an agrarian society.
South American cultures began domesticating llamas, vicuñas, guanacos, and alpacas in the highlands of the Andes circa 3500 BC. Besides their use as sources of meat and wool, these animals were used for transportation of goods.
Pre-Columbian civilizations.
The rise of plant growing and the subsequent appearance of permanent human settlements allowed for the multiple and overlapping beginnings of civilizations in South America.
One of the earliest known South American civilizations was at Norte Chico, on the central Peruvian coast. Though a pre-ceramic culture, the monumental architecture of Norte Chico is contemporaneous with the pyramids of Ancient Egypt. Norte Chico governing class established a trade network and developed agriculture then followed by Chavín by 900 BC, according to some estimates and archaeological finds. Artifacts were found at a site called Chavín de Huantar in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned 900 BC to 300 BC.
In the central coast of Peru, around the beginning of the 1st millennium AD, Moche (100 BC – 700 AD, at the northern coast of Peru), Paracas and Nazca (400 BC – 800 AD, Peru) cultures flourished with centralized states with permanent militia improving agriculture through irrigation and new styles of ceramic art. At the Altiplano, Tiahuanaco or Tiwanaku (100 BC – 1200 AD, Bolivia) managed a large commercial network based on religion.
Around 7th century, both Tiahuanaco and Wari or Huari Empire (600–1200, Central and northern Peru) expanded its influence to all the Andean region, imposing the Huari urbanism and tiahuanaco religious iconography.
The Muisca were the main indigenous civilization in what is now Colombia. They established the Musica Confederation of many clans, or "cacicazgos", that had a free trade network among themselves. They were goldsmiths and farmers.
Other important Pre-Columbian cultures include: the Cañaris (in south central Ecuador), Chimú Empire (1300–1470, Peruvian northern coast), Chachapoyas, and the Aymaran kingdoms (1000–1450, Bolivia and southern Peru).
Holding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533. Known as "Tawantin suyu", and "the land of the four regions," in Quechua, the Inca civilization was highly distinct and developed. Inca rule extended to nearly a hundred linguistic or ethnic communities, some 9 to 14 million people connected by a 25,000 kilometer road system. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture.
The Mapuche in Central and Southern Chile resisted the European and Chilean settlers, waging the Arauco War for more than 300 years.
European colonization.
In 1494, Portugal and Spain, the two great maritime European powers of that time, on the expectation of new lands being discovered in the west, signed the Treaty of Tordesillas, by which they agreed, with the support of the Pope, that all the land outside Europe should be an exclusive duopoly between the two countries.
The treaty established an imaginary line along a north-south meridian 370 leagues west of the Cape Verde Islands, roughly 46° 37' W. In terms of the treaty, all land to the west of the line (known to comprise most of the South American soil) would belong to Spain, and all land to the east, to Portugal. As accurate measurements of longitude were impossible at that time, the line was not strictly enforced, resulting in a Portuguese expansion of Brazil across the meridian.
Beginning in the 1530s, the people and natural resources of South America were repeatedly exploited by foreign conquistadors, first from Spain and later from Portugal. These competing colonial nations claimed the land and resources as their own and divided it in colonies.
European infectious diseases (smallpox, influenza, measles, and typhus) – to which the native populations had no immune resistance – and systems of forced labor, such as the haciendas and mining industry's mit'a, decimated the native population under Spanish control. After this, African slaves, who had developed immunities to these diseases, were quickly brought in to replace them.
The Spaniards were committed to convert their native subjects to Christianity and were quick to purge any native cultural practices that hindered this end; however, many initial attempts at this were only partially successful, as native groups simply blended Catholicism with their established beliefs and practices. Furthermore, the Spaniards brought their language to the degree they did with their religion, although the Roman Catholic Church's evangelization in Quechua, Aymara, and Guaraní actually contributed to the continuous use of these native languages albeit only in the oral form.
Eventually, the natives and the Spaniards interbred, forming a mestizo class. At the beginning, many mestizos of the Andean region were offspring of Amerindian mothers and Spanish fathers. After independence, most mestizos had native fathers and European or mestizo mothers.
Many native artworks were considered pagan idols and destroyed by Spanish explorers; this included many gold and silver sculptures and other artifacts found in South America, which were melted down before their transport to Spain or Portugal. Spaniards and Portuguese brought the western European architectural style to the continent, and helped to improve infrastructures like bridges, roads, and the sewer system of the cities they discovered or conquered. They also significantly increased economic and trade relations, not just between the old and new world but between the different South American regions and peoples. Finally, with the expansion of the Portuguese and Spanish languages, many cultures that were previously separated became united through that of Latin American.
Guyana was first a Dutch, and then a British colony, though there was a brief period during the Napoleonic Wars when it was colonized by the French. The country was once partitioned into three parts, each being controlled by one of the colonial powers until the country was finally taken over fully by the British.
Slavery in Pre-Modern Latin America.
Indigenous peoples of the Americas in various European colonies were forced to work in European plantations and mines; along with African slaves who were also introduced in the proceeding centuries.
Independence from Spain and Portugal.
The European Peninsular War (1807–1814), a theater of the Napoleonic Wars, changed the political situation of both the Spanish and Portuguese colonies. First, Napoleon invaded Portugal, but the House of Braganza avoided capture by escaping to Brazil. Napoleon also captured King Ferdinand VII of Spain, and appointed his own brother instead. This appointment provoked severe popular resistance, which created Juntas to rule in the name of the captured king.
Many cities in the Spanish colonies, however, considered themselves equally authorized to appoint local Juntas like those of Spain. This began the Spanish American wars of independence between the patriots, who promoted such autonomy, and the royalists, who supported Spanish authority over the Americas. The Juntas, in both Spain and the Americas, promoted the ideas of the Enlightenment. Five years after the beginning of the war, Ferdinand VII returned to the throne and began the Absolutist Restoration as the royalists got the upper hand in the conflict.
The independence of South America was secured by Simón Bolívar (Venezuela) and José de San Martín (Argentina), the two most important "Libertadores". Bolívar led a great uprising in the north, then led his army southward towards Lima, the capital of the Viceroyalty of Peru. Meanwhile, San Martín led an army across the Andes Mountains, along with Chilean expatriates, and liberated Chile. He organized a fleet to reach Peru by sea, and sought the military support of various rebels from the Viceroyalty of Peru. The two armies finally met in Guayaquil, Ecuador, where they cornered the Royal Army of the Spanish Crown and forced its surrender.
In the Portuguese kingdom of Brazil and Algarve, Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese King Dom João VI, proclaimed the independent Kingdom of Brazil in 1822, which later became the Empire of Brazil. Despite the Portuguese loyalties of garrisons in Bahia and Pará, independence was diplomatically accepted by the crown in Portugal, on condition of a high compensation paid by Brazil.
Nation-building and balkanization.
The newly independent nations began a process of balkanization, with several civil and international wars. However, it was not as strong as in Central America. Some countries created from provinces of larger countries stayed as such up to modern day (such as Paraguay or Uruguay), while others were reconquered and reincorporated into their former countries (such as the Republic of Entre Ríos and the Riograndense Republic).
The Peru–Bolivian Confederation, a short-lived union of Peru and Bolivia, was blocked by Chile in the War of the Confederation (1836–1839) and again during the War of the Pacific (1879–1883). Paraguay was virtually destroyed by Argentina and Brazil in the Paraguayan War.
Rise and fall of military dictatorships.
Wars became less frequent in the 20th century, with Bolivia-Paraguay and Peru-Ecuador fighting the last inter-state wars.
Early in the 20th century, the three wealthiest South American countries engaged in a vastly expensive naval arms race which was catalyzed by the introduction of a new warship type, the "dreadnought". At one point, the Argentine government was spending a fifth of its entire yearly budget for just two dreadnoughts, a price that did not include later in-service costs, which for the Brazilian dreadnoughts was sixty percent of the initial purchase.
The continent became a battlefield of the Cold War in the late 20th century. Some democratically elected governments of Argentina, Brazil, Chile, Uruguay and Paraguay were overthrown or displaced by military dictatorships in the 1960s and 1970s. To curtail opposition, their governments detained tens of thousands of political prisoners, many of whom were tortured and/or killed on inter-state collaboration. Economically, they began a transition to neoliberal economic policies. They placed their own actions within the US Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict.
Argentina and Britain fought the Falklands War in 1982.
Colombia has had an ongoing, though diminished internal conflict, which started in 1964 with the creation of Marxist guerrillas (FARC-EP) and then involved several illegal armed groups of leftist-leaning ideology as well as the private armies of powerful drug lords. Many of these are now defunct, and only a small portion of the ELN remains, along with the stronger, though also greatly reduced FARC. These leftist groups smuggle narcotics out of Colombia to fund their operations, while also using kidnapping, bombings, land mines and assassinations as weapons against both elected and non-elected citizens.
Revolutionary movements and right-wing military dictatorships became common after World War II, but since the 1980s, a wave of democratization came through the continent, and democratic rule is widespread now. Nonetheless, allegations of corruption are still very common, and several countries have developed crises which have forced the resignation of their governments, although, in most occasions, regular civilian succession has continued.
International indebtedness turned into a severe problem in late 1980s, and some countries, despite having strong democracies, have not yet developed political institutions capable of handling such crises without resorting to unorthodox economic policies, as most recently illustrated by Argentina's default in the early 21st century. The last twenty years have seen an increased push towards regional integration, with the creation of uniquely South American institutions such as the Andean Community, Mercosur and Unasur. Notably, starting with the election of Hugo Chávez in Venezuela in 1998, the region experienced what has been termed a pink tide – the election of several leftist and center-left administrations to most countries of the area, except for the Guianas and Colombia.
Politics.
During the first decade of the 21st century, South American governments have drifted to the political left, with leftist leaders being elected in Chile, Uruguay, Brazil, Argentina, Ecuador, Bolivia, Paraguay, Peru and Venezuela. Most South American countries are making an increasing use of protectionist policies, helping local development.
Recently, an intergovernmental entity has been formed which aims to merge the two existing customs unions: Mercosur and the Andean Community, thus forming the third-largest trade bloc in the world.
This new political organization known as Union of South American Nations seeks to establish free movement of people, economic development, a common defense policy and the elimination of tariffs.
Ethnic demographics.
Descendants of indigenous peoples, such as the Quechua and Aymara, or the Urarina of Amazonia make up the majority of the population in Bolivia (56%) and, per some sources, in Peru (44%). In Ecuador, Amerindians are a large minority that comprises two-fifths of the population. The native European population is also a significant element in most other former Portuguese colonies.
South America is also home to one of the largest populations of Africans. This group is also significantly present in Guyana, Brazil, Colombia, Suriname, French Guiana, and Ecuador. Mestizos (mixed European and Amerindian) are the largest ethnic group in Paraguay, Venezuela, Colombia (49%) and Ecuador and the second group in Peru. East Indians form the largest ethnic group in Guyana and Suriname. Brazil followed by Peru also have the largest Japanese, Korean and Chinese communities in South America.
The demographics of Colombia include approximately 37% native European descendants, while in Peru, European descendants are the third group in importance (15%). Compared to other South American countries, the people who identify as of primarily or totally European descent, or identify their phenotype as corresponding to such group, are more of a majority in Argentina, Chile and Uruguay, and are about half of the population of Brazil. In Venezuela, according to the national census 42% of the population is primarily native Spanish, Italian and Portuguese descendants.
Indigenous people.
In many places indigenous people still practice a traditional lifestyle based on subsistence agriculture or as hunter-gatherers. There are still some uncontacted tribes residing in the Amazon Rainforest.
Economy.
South America relies less on the export of both manufactured goods and natural resources than the world average; merchandise exports from the continent were 16% of GDP on an exchange rate basis, compared to 25% for the world as a whole. Brazil (the seventh largest economy in the world and the largest in South America) leads in terms of merchandise exports at $251 billion, followed by Venezuela at $93 billion, Chile at $86 billion, and Argentina at $84 billion.
The economic gap between the rich and poor in most South American nations is larger than in most other continents. The richest 10% receive over 40% of the nation's income in Bolivia, Brazil, Chile, Colombia, and Paraguay, while the poorest 20% receive 3% or less in Bolivia, Brazil, and Colombia. This wide gap can be seen in many large South American cities where makeshift shacks and slums lie in the vicinity of skyscrapers and upper-class luxury apartments; nearly one in nine in South America live on less than $2 per day (on a purchasing power parity basis).
Tourism.
Tourism has increasingly become a significant source of income for many South American countries. Historical relics, architectural and natural wonders, a diverse range of foods and culture, vibrant and colorful cities, and stunning landscapes attract millions of tourists every year to South America. Some of the most visited places in the region are Iguazu Falls, Recife, Olinda, Machu Picchu, the Amazon rainforest, Rio de Janeiro, Salvador, Fortaleza, Maceió, Buenos Aires, Florianópolis, San Ignacio Miní, Isla Margarita, Natal, Lima, São Paulo, Angel Falls, Brasília, Nazca Lines, Perito Moreno Glacier, Cuzco, Belo Horizonte, Lake Titicaca, Los Roques archipelago, Bogotá, Patagonia, Gran Sabana, Cartagena and the Galápagos Islands.
In 2016 Brazil will play host to the 2016 Summer Olympics. With 600,000 overseas visitors expected to come for the World Cup alone.
Culture.
South Americans are culturally influenced by their indigenous peoples, the historic connection with the Iberian Peninsula and Africa, and waves of immigrants from around the globe.
South American nations have a rich variety of music. Some of the most famous genres include vallenato and cumbia from Colombia, pasillo from Ecuador, samba, bossa nova and música sertaneja from Brazil, and tango from Argentina and Uruguay. Also well known is the non-commercial folk genre Nueva Canción movement which was founded in Argentina and Chile and quickly spread to the rest of the Latin America. People on the Peruvian coast created the fine guitar and cajon duos or trios in the most mestizo (mixed) of South American rhythms such as the Marinera (from Lima), the Tondero (from Piura), the 19th century popular Creole Valse or Peruvian Valse, the soulful Arequipan Yaravi, and the early 20th century Paraguayan Guarania. In the late 20th century, Spanish rock emerged by young hipsters influenced by British pop and American rock. Brazil has a Portuguese-language pop rock industry as well a great variety of other music genres.
The literature of South America has attracted considerable critical and popular acclaim, especially with the Latin American Boom of the 1960s and 1970s, and the rise of authors such as Mario Vargas Llosa, Gabriel García Márquez in novels and Jorge Luis Borges and Pablo Neruda in other genres. The Brazilians Machado de Assis and João Guimarães Rosa are widely regarded as the greatest Brazilian writers.
Because of South America's broad ethnic mix, South American cuisine has African, South American Indian, Asian, and European influences. Bahia, Brazil, is especially well known for its West African–influenced cuisine. Argentines, Chileans, Uruguayans, Brazilians, Bolivians, and Venezuelans regularly consume wine. Argentina, Paraguay, Uruguay, and people in southern Chile, Bolivia and Brazil drink mate, a herb which is brewed. The Paraguayan version, terere, differs from other forms of mate in that it is served cold. Pisco is a liquor distilled from grapes in Peru and Chile. Peruvian cuisine mixes elements from Chinese, Japanese, Spanish, African, Andean, and Amazonic food.
Language.
Spanish and Portuguese are the most spoken languages in South America, with approximately 200 million speakers each. Spanish is the official language of most countries, along with other native languages in some countries. Portuguese is the official language of Brazil. Dutch is the official language of Suriname; English is the official language of Guyana, although there are at least twelve other languages spoken in the country, including Portuguese, Chinese, Hindustani and several native languages. English is also spoken in the Falkland Islands. French is the official language of French Guiana and the second language in Amapá, Brazil.
Indigenous languages of South America include Quechua in Ecuador, Peru, Chile, Colombia, and Bolivia; Wayuunaiki in northern Colombia (La Guajira) and northwestern Venezuela (Zulia); Guaraní in Paraguay and, to a much lesser extent, in Bolivia; Aymara in Bolivia, Peru, and less often in Chile; and Mapudungun is spoken in certain pockets of southern Chile and, more rarely, Argentina. At least three South American indigenous languages (Quechua, Aymara, and Guarani) are recognized along with Spanish as national languages.
Other languages found in South America include, Hindi and Javanese in Suriname; Italian in Argentina, Brazil, Uruguay, Venezuela and Chile; and German in certain pockets of Argentina, Brazil, and Chile. German is also spoken in many regions of the southern states of Brazil, Riograndenser Hunsrückisch being the most widely spoken German dialect in the country; among other Germanic dialects, a Brazilian form of Pomeranian is also well represented and is experiencing a revival. Welsh remains spoken and written in the historic towns of Trelew and Rawson in the Argentine Patagonia. There are also small clusters of Japanese-speakers in Brazil, Colombia and Peru. Arabic speakers, often of Lebanese, Syrian, or Palestinian descent, can be found in Arab communities in Argentina, Colombia, Brazil, Venezuela and in Paraguay.
Sport.
A wide range of sports are played in the continent of South America, with football being the most popular overall, while baseball is the most popular in Venezuela and northern Colombia.
Other sports include basketball, cycling, polo, volleyball, futsal, motorsports, rugby (mostly in Argentina and Uruguay), handball, tennis, golf, field hockey and boxing.
South America will hold its first Olympic Games in Rio de Janeiro, Brazil in 2016 and the Youth Olympic Games in Buenos Aires, Argentina in 2018.
South America shares with Europe the supremacy over the sport of football as all winners in FIFA World Cup history and all winning teams in the FIFA Club World Cup have come from these two continents. Brazil holds the record at the FIFA World Cup with five titles in total. Argentina and Uruguay have two titles each. So far four South American nations have hosted the tournament including the first edition in Uruguay (1930). The other three were Brazil (1950, 2014), Chile (1962), and Argentina (1978).
South America is home to the longest running international football tournament; the Copa América, which has been regularly contested since 1916. Uruguay have won the Copa America a record 15 times, surpassing hosts Argentina in 2011 to reach 15 titles (they were previously equal on 14 titles each during the 2011 Copa America). The continent has produced many of the most famous and most talented players including Diego Maradona, Pelé, Alfredo Di Stéfano, Lionel Messi, Ronaldo, Ronaldinho, Rivaldo, Teófilo Cubillas, Mario Kempes, Gabriel Batistuta, César Cueto, Enzo Francescoli, Arsenio Erico, Alberto Spencer, Carlos Valderrama, Elias Figueroa, Marcelo Salas, Juan Arango, Neymar, and Luis Suárez.
Also, in South America, a multi-sport event, the South American Games, are held every four years. The first edition was held in La Paz in 1978 and the most recent took place in Santiago in 2014.

</doc>
<doc id="26771" url="https://en.wikipedia.org/wiki?curid=26771" title="Spindletop">
Spindletop

Spindletop is a salt dome oil field located in the southern portion of Beaumont, Texas in the United States. The Spindletop dome was derived from the Louann Salt evaporite layer of the Jurassic geologic period. On January 10, 1901, a well at Spindletop struck oil ("came in"). The Spindletop gusher blew for nine days at a rate estimated at of oil per day. Gulf Oil and Texaco, now part of Chevron Corporation, were formed to develop production at Spindletop.
The frenzy of oil exploration and the economic development it generated in the state became known as the Texas Oil Boom. The United States soon became the world's leading oil producer.
__TOC__
History.
There has been oil around Texas for a long time.Native Americans thought drinking oil would cure digestive problems. In 1543, Spanish explorers used oil to water proof their boots.
There had long been suspicions that oil might be under "Spindletop Hill." The area was known for its vast sulfur springs and bubbling gas seepages that would ignite if lit. In August 1892, George W. O'Brien, George W. Carroll, Pattillo Higgins and others formed the Gladys City Oil, Gas, and Manufacturing Company to do exploratory drilling on Spindletop Hill. The company drilled many dry holes and ran into trouble, as investors began to balk at pouring more money into drilling with no oil to show for it.
Pattillo Higgins left the company and teamed with Captain Anthony F. Lucas, the leading expert in the U.S. on salt dome formations. Lucas made a lease agreement in 1899 with the Gladys City Company and a subsequent agreement with Higgins. Lucas drilled to before running out of money. He secured additional funding from John H. Galey and James M. Guffey of Pittsburgh, but the deal left Lucas with only an eighth share of the lease and Higgins with nothing.
Lucas continued drilling and on January 10, 1901, at a depth of 1,139 ft (347 m), what is known as the Lucas Gusher or the Lucas Geyser blew oil over in the air at a rate of (4,200,000 gallons). It took nine days before the well was brought under control. Spindletop was the largest gusher the world had seen and catapulted Beaumont into an oil-fueled boomtown. Beaumont's population of 10,000 tripled in three months and eventually rose to 50,000. Speculation led land prices to increase rapidly. By the end of 1902, more than 500 companies had been formed and 285 wells were in operation.
Spindletop was the first oil field found on the US Gulf Coast, and prompted further drilling, and further oil field discoveries. Oil drillers looking for another Spindletop particularly sought out other salt domes, and were often successful. The Gulf Coast turned into a major oil region.
Standard Oil, which then had a monopoly or near-monopoly on the petroleum industry in the eastern states, was prevented from moving aggressively into the new oil field by state antitrust laws. Populist sentiment against Standard Oil was particularly strong at the time of the Spindletop discovery. In 1900, an oil products marketing company affiliated with Standard Oil had been banned from the state for its cutthroat business practices. Although Standard built refineries in the area, Standard was unable to dominate the new Gulf Coast oil fields the way it had in the eastern states. As a result, a number of startup oil companies at Spindletop, such as Texaco and Gulf Oil, grew into formidable competitors to Standard Oil.
Among those drilling at Spindletop was W. Scott Heywood, a native of Cleveland, Ohio, who in 1901 made the first oil discovery in nearby Jeff Davis Parish in southwestern Louisiana. In 1932, Heywood was elected to a single term in the Louisiana State Senate.
Production at Spindletop began to decline rapidly after 1902, and the wells produced only by 1904. On November 14, 1925, the Yount-Lee Oil Company brought in its McFaddin No. 2 at a depth of about , sparking a second boom, which culminated in the field's peak production year of 1927, during which 21 million barrels (3.3 GL) were produced. Over the ten years following the McFaddin discovery, more than 72 million barrels (11.4 GL) of oil were produced, mostly from the newer areas of the field. Spindletop continued as a productive source of oil until about 1936. It was then mined for sulfur from the 1950s to about 1975.
Spindletop-Gladys City Boomtown Museum.
In 1976 Lamar University dedicated the Spindletop-Gladys City Boomtown Museum to preserve the history of the Spindletop oil gusher era in Beaumont. The museum features an oil derrick and many reconstructed Gladys City building interiors furnished with authentic artifacts from the Spindletop boomtown period.
The Lucas Gusher Monument is located at the museum. The Monument, erected at the wellhead in July, 1941, was moved to the Spindletop-Gladys City Museum after it became unstable due to ground subsidence. According to an article by Nedra Foster, LS in the July/August, 2000 issue of the "Professional Surveyor Magazine," the Monument was originally located within four feet of the site of the Spindletop well.
Today the wellhead is marked at Spindletop Park by a flagpole flying the Texas flag. It is located about 1.5 miles southwest of the museum, off West Port Arthur Road/Spur 93. The site includes a viewing platform with information placards, about a quarter mile from the flagpole. The wellhead site is in the middle of swampland on private land and is not accessible. Directions to the park and viewing platform are available at the museum.
On December 4, 1955, the Spindletop story was dramatized in "Spindletop - The First Great Texas Oil Strike (January 10, 1901)" on the CBS history series, "You Are There". Robert Bray was cast as Pattillo Higgins; Mike Ragan as Marion Fletcher; Parley Baer as Captain Lucas, Jean Byron as Caroline Lucas, DeForest Kelley as Al Hammill, Tyler McVey as Mayor Wheat, and William Fawcett as a farmer.

</doc>
<doc id="26773" url="https://en.wikipedia.org/wiki?curid=26773" title="Stendhal">
Stendhal

Marie-Henri Beyle (; 23 January 1783 – 23 March 1842), better known by his pen name Stendhal ( or ; in English , , or ), was a 19th-century French writer. Best known for the novels "Le Rouge et le Noir" ("The Red and the Black", 1830) and "La Chartreuse de Parme" ("The Charterhouse of Parma", 1839), he is highly regarded for the acute analysis of his characters' psychology and considered one of the earliest and foremost practitioners of realism.
Life.
Born in Grenoble, Isère, he was an unhappy child, disliking his "unimaginative" father and mourning his mother, whom he passionately loved, and who died when he was seven. He spent "the happiest years of his life" at the Beyle country house in Claix near Grenoble. His closest friend was his younger sister, Pauline, with whom he maintained a steady correspondence throughout the first decade of the 19th century.
The military and theatrical worlds of the First French Empire were a revelation to Beyle. He was named an auditor with the Conseil d'État on 3 August 1810, and thereafter took part in the French administration and in the Napoleonic wars in Italy. He travelled extensively in Germany and was part of Napoleon's army in the 1812 invasion of Russia.
Stendhal witnessed the burning of Moscow from just outside the city. Stendhal was appointed Commissioner of War Supplies and sent to Smolensk to prepare provisions for the returning army. He crossed the Berezina River by finding a usable ford rather than the overwhelmed pontoon bridge, which probably saved his life and those of his companions. Stendhal arrived in Paris in 1813, largely unaware of the general fiasco that the retreat had become. Stendhal became known, during the Russian campaign, for keeping his wits about him, and maintaining his "sang-froid and clear-headedness." He also maintained his daily routine, shaving each day during the retreat from Moscow.
After the 1814 Treaty of Fontainebleau, he left for Italy, where he settled in Milan. He formed a particular attachment to Italy, where he spent much of the remainder of his career, serving as French consul at Trieste and Civitavecchia. His novel "The Charterhouse of Parma", written in 52 days, is set in Italy, which he considered a more sincere and passionate country than Restoration France. An aside in that novel, referring to a character who contemplates suicide after being jilted, speaks about his attitude towards his home country: "To make this course of action clear to my French readers, I must explain that in Italy, a country very far away from us, people are still driven to despair by love."
Stendhal was a dandy and wit about town in Paris, as well as an inveterate womaniser who was obsessed with his sexual conquests. His genuine empathy towards women is evident in his books; Simone de Beauvoir spoke highly of him in "The Second Sex". One of his early works is "On Love," a rational analysis of romantic passion that was based on his unrequited love for Mathilde, Countess Dembowska, whom he met while living at Milan. This fusion of, and tension between, clear-headed analysis and romantic feeling is typical of Stendhal's great novels; he could be considered a Romantic realist.
Stendhal suffered miserable physical disabilities in his final years as he continued to produce some of his most famous work. As he noted in his journal, he was taking iodide of potassium and quicksilver to treat his syphilis, resulting in swollen armpits, difficulty swallowing, pains in his shrunken testicles, sleeplessness, giddiness, roaring in the ears, racing pulse and "tremors so bad he could scarcely hold a fork or a pen". Modern medicine has shown that his health problems were more attributable to his treatment than to his syphilis.
Stendhal died on 23 March 1842, a few hours after collapsing with a seizure on the streets of Paris. He is interred in the Cimetière de Montmartre.
Pseudonyms.
Before settling on the pen name Stendhal, he published under many pen names, including "Louis Alexandre Bombet" and "Anastasius Serpière". The only book that Stendhal published under his own name was "The History of Painting" (1817). From the publication of "Rome, Naples, Florence" (September 1817) onwards, he published his works under the pseudonym "M. de Stendhal, officier de cavalerie". He borrowed this "nom de plume" from the German city of Stendal, birthplace of Johann Joachim Winckelmann, an art historian and archaeologist famous at the time.
In 1807 Stendhal stayed near Stendal, where he fell in love with a woman named Wilhelmine, whom he called Minette, and for whose sake he remained in the city. "I have no inclination, now, except for Minette, for this blonde and charming Minette, this soul of the north, such as I have never seen in France or Italy." Stendhal added an additional "H" to make more clear the Germanic pronunciation.
Stendhal used many aliases in his autobiographical writings and correspondence, and often assigned pseudonyms to friends, some of whom adopted the names for themselves. Stendhal used more than a hundred pseudonyms, which were astonishingly diverse. Some he used no more than once, while others he returned to throughout his life. "Dominique" and "Salviati" served as intimate pet names. He coins comic names "that make him even more bourgeois than he really is: Cotonnet, Bombet, Chamier." He uses many ridiculous names: "Don phlegm", "Giorgio Vasari", "William Crocodile", "Poverino", "Baron de Cutendre". One of his correspondents, Prosper Mérimée, said: "He never wrote a letter without signing a false name."
Stendhal's "Journal" and autobiographical writings include many comments on masks and the pleasures of "feeling alive in many versions." "Look upon life as a masked ball," is the advice that Stendhal gives himself in his diary for 1814. In "Memoirs of an Egotist" he writes: "Will I be believed if I say I'd wear a mask with pleasure and be delighted to change my name?...for me the supreme happiness would be to change into a lanky, blonde German and to walk about like that in Paris."
Works.
Contemporary readers did not fully appreciate Stendhal's realistic style during the Romantic period in which he lived. He was not fully appreciated until the beginning of the 20th century. He dedicated his writing to "the Happy Few" (in English in the original). This can be interpreted as a reference to Canto 11 of Lord Byron's "Don Juan", which refers to "the thousand happy few" who enjoy high society, or to the "we few, we happy few, we band of brothers" line of William Shakespeare's "Henry V", but Stendhal's use more likely refers to "The Vicar of Wakefield" by Oliver Goldsmith, parts of which he had memorized in the course of teaching himself English.
In "The Vicar of Wakefield", "the happy few" refers ironically to the small number of people who read the title character's obscure and pedantic treatise on monogamy. As a literary critic, such as in "Racine and Shakespeare", Stendhal championed the Romantic aesthetic by unfavorably comparing the rules and strictures of Jean Racine's classicism to the freer verse and settings of Shakespeare, and supporting the writing of plays in prose.
Today, Stendhal's works attract attention for their irony and psychological and historical dimensions. Stendhal was an avid fan of music, particularly the works of the composers Domenico Cimarosa, Wolfgang Amadeus Mozart and Gioacchino Rossini. He wrote a biography of Rossini, "Vie de Rossini" (1824), now more valued for its wide-ranging musical criticism than for its historical content.
In his works, Stendhal reprised excerpts appropriated from Giuseppe Carpani, Théophile Frédéric Winckler, Sismondi and others.
Autobiography.
Stendhal's brief memoir, "Souvenirs d'Égotisme" ("Memoirs of an Egotist") was published posthumously in 1892. Also published was a more extended autobiographical work, thinly disguised as the "Life of Henry Brulard".
Non-fiction.
His other works include short stories, journalism, travel books ("Promenades dans Rome"), a famous collection of essays on Italian painting, and biographies of several prominent figures of his time, including Napoleon, Haydn, Mozart, Rossini and Metastasio.
Crystallization.
In Stendhal's 1822 classic "On Love" he describes or compares the "birth of love", in which the love object is 'crystallized' in the mind, as being a process similar or analogous to a trip to Rome. In the analogy, the city of Bologna represents "indifference" and Rome represents "perfect love":
When we are in Bologna, we are entirely indifferent; we are not concerned to admire in any particular way the person with whom we shall perhaps one day be madly in love; even less is our imagination inclined to overrate their worth. In a word, in Bologna "crystallization" has not yet begun. When the journey begins, love departs. One leaves Bologna, climbs the Apennines, and takes the road to Rome. The departure, according to Stendhal, has nothing to do with one’s will; it is an instinctive moment. This transformative process actuates in terms of four steps along a journey:
This journey or crystallization process (shown above) was detailed by Stendhal on the back of a playing card while speaking to Madame Gherardi, during his trip to the Salzburg salt mine.
Critical appraisal.
Hippolyte Taine considered the psychological portraits of Stendhal's characters to be "real, because they are complex, many-sided, particular and original, like living human beings." Emile Zola concurred with Taine's assessment of Stendhal's skills as a "psychologist", and although emphatic in his praise of Stendhal's psychological accuracy and rejection of convention, he deplored the various implausibilities of the novels and Stendhal's clear authorial intervention.
The German philosopher Friedrich Nietzsche refers to Stendhal as "France's last great psychologist" in "Beyond Good and Evil" (1886). He also mentions Stendhal in the "Twilight of the Idols" (1889) during a discussion of Dostoevsky as a psychologist, saying that encountering Dostoevsky was "the most beautiful accident of my life, more so than even my discovery of Stendhal".
Ford Maddox Ford, in "The English Novel", asserts that to Diderot and Stendhal "the Novel owes its next great step forward...At that point it became suddenly evident that the Novel as such was capable of being regarded as a means of profoundly serious and many-sided discussion and therefore as a medium of profoundly serious investigation into the human case."
Erich Auerbach considers modern "serious realism" to have begun with Stendhal and Balzac. In "", he remarks of a scene in "The Red and the Black" that "it would be almost incomprehensible without a most accurate and detailed knowledge of the political situation, the social stratification, and the economic circumstances of a perfectly definite historical moment, namely, that in which France found itself just before the July Revolution."
In Auerbach's view, in Stendhal's novels "characters, attitudes, and relationships of the "dramatis personæ", then, are very closely connected with contemporary historical circumstances; contemporary political and social conditions are woven into the action in a manner more detailed and more real than had been exhibited in any earlier novel, and indeed in any works of literary art except those expressly purporting to be politico-satirical tracts."
Even Stendhal's autobiographical works, such as "The Life of Henry Brulard" or "Memoirs of an Egotist", are "far more closely, essentially, and concretely connected with the politics, sociology, and economics of the period than are, for example, the corresponding works of Rousseau or Goethe; one feels that the great events of contemporary history affected Stendhal much more directly than they did the other two; Rousseau did not live to see them, and Goethe had managed to keep aloof from them." Auerbach goes on to say: 
Vladimir Nabokov was dismissive of Stendhal, in "Strong Opinions" calling him "that pet of all those who like their French plain". In the notes to his translation of "Eugene Onegin", he asserts that "Le Rouge et le Noir" is "much overrated," and that Stendhal has a "paltry style". In "Pnin" Nabokov wrote satirically, "Literary departments still labored under the impression that Stendhal, Galsworthy, Dreiser, and Mann were great writers."
Michael Dirda considers Stendhal "the greatest all round French writer--author of two of the top 20 French novels, author of a highly original autobiography ("Vie de Henry Brulard"), a superb travel writer, and as inimitable a presence on the page as any writer you'll ever meet."
Stendhal syndrome.
In 1817 Stendhal reportedly was overcome by the cultural richness of Florence he encountered when he first visited the Tuscan city. As he described in his book "Naples and Florence: A Journey from Milan to Reggio":
As I emerged from the porch of Santa Croce, I was seized with a fierce palpitation of the heart (that same symptom which, in Berlin, is referred to as an attack of the nerves); the well-spring of life was dried up within me, and I walked in constant fear of falling to the ground.
The condition was diagnosed and named in 1979 by Italian psychiatrist Dr. Graziella Magherini, who had noticed similar psychosomatic conditions (racing heart beat, nausea and dizziness) amongst first-time visitors to the city.
In homage to Stendhal, Trenitalia named their overnight train service from Paris to Venice the Stendhal Express.

</doc>
<doc id="26775" url="https://en.wikipedia.org/wiki?curid=26775" title="Syndicalism">
Syndicalism

Syndicalism is a proposed type of economic system, a form of socialism, considered a replacement for capitalism. It suggests that industries be organized into confederations or syndicates. It is "a system of economic organization in which industries are owned and managed by the workers".
Its theory and practice is the advocation of multiple cooperative productive units composed of specialists and representatives of workers in each respective field to negotiate and manage the economy. Syndicalism also refers to the political movement (praxis) and tactics used to bring about this type of system.
For adherents, labour unions and labour training (see below) are the potential means of both overcoming economic aristocracy and running society fairly and in the interest of informed and skilled majorities, through union democracy. Industry in a syndicalist system would be run through co-operative confederations and mutual aid. Local syndicates would communicate with other syndicates through the Bourse du Travail (labour exchange) which would cooperatively determine distributions of commodities.
"Syndicalism" is also used to refer to the tactic of bringing about this social arrangement, typically expounded by anarcho-syndicalism and De Leonism. It aims to achieve a general strike, a workers' outward refusal of their current modes of production, followed by organisation into federations of trade unions, such as the CNT. Throughout its history, the reformist section of syndicalism has been overshadowed by its revolutionary section, typified by the Federación Anarquista Ibérica section of the CNT.
Syndicalism and anarcho-syndicalism.
Syndicalism can be accurately divided into the purely economic-focused camp, exemplified by the Italian USI (Unione Sindacale Italiana) the largest Italian syndicalist union in 1920, taking part in the Biennio Rosso and the anarcho-syndicalism of the CNT (national confederation of labour), taking both political and economic action, wishing to take control of both workplace and political life, while syndicalism has traditionally focused on the economic sector alone.
Although the terms anarcho-syndicalism and revolutionary syndicalism are often used interchangeably, the anarcho-syndicalist label was not widely used until the early 1920s (some credit Sam Mainwaring with coining the term). "The term 'anarcho-syndicalist' only came into wide use in 1921-1922 when it was applied polemically as a pejorative term by communists to any syndicalists [...] who opposed increased control of syndicalism by the communist parties".
Traditionally the revolutionary political syndicalism of figures such as Rudolph Rocker (widely credited as the father of anarcho-syndicalism) has overshadowed the more reformist or economically-focused syndicalism.
Related theories include anarchism, socialism, Marxism, Leninism, and communism.
History.
"Syndicalisme"/"Sindicalismo" is a French/Spanish word meaning "trade unionism". More moderate versions of syndicalism were overshadowed in the early 20th century by revolutionary anarcho-syndicalism, which advocated, in addition to the abolition of capitalism, the abolition of the state, which was expected to be made obsolete by syndicalist economics. Anarcho-syndicalism was most powerful in Spain in and around the time of the Spanish Civil War, but also appeared in other parts of the world, such as in the US-based Industrial Workers of the World and the Unione Sindacale Italiana - the Italian Syndicalist Union.
The earliest expressions of syndicalist structure and methods were formulated in the International Workingmen's Association or First International, particularly in the Jura federation. In 1895, the Confédération Générale du Travail (CGT) in France expressed fully the organisational structure and methods of revolutionary syndicalism influencing labour movements the world over. The CGT was modelled on the development of the Bourse de Travail (labour exchange), a workers' central organisation which would encourage self-education and mutual aid, and facilitate communication with local workers' syndicates. Through a general strike, workers would take control of industry and services and self-manage society and facilitate production and consumption through the labour exchanges. The Charter of Amiens, adopted by the CGT in 1906, represents a key text in the development of revolutionary syndicalism rejecting parliamentarianism and political action in favour of revolutionary class struggle. The Central Organisation of the Workers of Sweden (SAC) (in Swedish the Sveriges Arbetares Centralorganisation), formed in 1910, are a notable example of an anarcho-syndicalist union influenced by the CGT. Today, the SAC is one of the largest anarcho-syndicalist unions in the world in proportion to the population, with some strongholds in the public sector.
The International Workers Association, formed in 1922, is an international syndicalist federation of various labour unions from different countries. At its peak, the International Workers Association represented millions of workers and competed directly for the hearts and minds of the working class with social democratic unions and parties. The Spanish Confederación Nacional del Trabajo played a major role in the Spanish labour movement. It was also a decisive force in the Spanish Civil War, organising worker militias and facilitating the collectivisation of vast sections of the industrial, logistical, and communications infrastructure, principally in Catalonia. Another Spanish anarcho-syndicalist union, the Confederacion General del Trabajo de España, is now the fourth largest union in Spain and the largest anarchist union with tens of thousands of members.
The Industrial Workers of the World (IWW), although explicitly "not" syndicalist, were informed by developments in the broader revolutionary syndicalist milieu at the turn of the twentieth-century. At its founding congress in 1905, influential members with strong anarchist or anarcho-syndicalist sympathies like Thomas J. Hagerty, William Trautmann, and Lucy Parsons contributed to the union's overall revolutionary syndicalist orientation. Lucy Parsons, in particular, was a veteran anarchist union organiser in Chicago from a previous generation, having participated in the struggle for the 8-hour day in Chicago and subsequent series of events which came to be known as the Haymarket Affair in 1886.
An emphasis on industrial organisation was a distinguishing feature of syndicalism when it began to be identified as a distinct current at the beginning of the 20th century. Due to a still-tangible faith in the viability of the state socialist system, most socialist groups of that period emphasised the importance of political action through party organisations as a means of bringing about socialism; in syndicalism, trade unions are thus seen as simply a stepping stone to common ownership. Although all syndicalists emphasise industrial organisation, not all reject political action altogether. For example, De Leonists and some other Industrial Unionists advocate parallel organisation both politically and industrially, while recognising that trade unions are at a comparable disadvantage due to the lobby of business groups on political leaders. Syndicalism would historically gain most of its support in Italy, France and particularly Spain, where the anarcho-syndicalist revolution during the Spanish civil war resulted in the widespread implementation of anarchist and more broadly socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia, and parts of the Levante. Much of Spain's economy was put under worker control; in anarchist strongholds like Catalonia, the figure was as high as 75%. Their eventual defeat and World War II led to the formerly prominent theory being repressed, as the three nations where it had the most power were now under fascist control. Support for Syndicalism never fully recovered to the height it enjoyed in the early 20th century.

</doc>
<doc id="26779" url="https://en.wikipedia.org/wiki?curid=26779" title="Soviet Union">
Soviet Union

The Union of Soviet Socialist Republics () abbreviated to USSR () or shortened to the Soviet Union (), was a socialist state on the Eurasian continent that existed between 1922 and 1991. A union of multiple subnational Soviet republics, its government and economy were highly centralized. The Soviet Union was a one-party state, governed by the Communist Party with Moscow as its capital.
The Soviet Union had its roots in the October Revolution of 1917, when the Bolsheviks, headed by Vladimir Lenin, overthrew the provisional government that had replaced the Tsar. They established the Russian Socialist Federative Soviet Republic (renamed Russian Soviet Federative Socialist Republic in 1936), beginning a civil war between the revolutionary "Reds" and the counter-revolutionary "Whites." The Red Army entered several territories of the former Russian Empire and helped local Communists take power through soviets, which nominally acted on behalf of workers and peasants. In 1922, the Communists were victorious, forming the Soviet Union with the unification of the Russian, Transcaucasian, Ukrainian, and Byelorussian republics. Following Lenin's death in 1924, a troika and a brief power struggle, Joseph Stalin came to power in the mid-1920s. Stalin suppressed all political opposition to his rule, committed the state ideology to Marxism–Leninism (which he created) and initiated a centrally planned economy. As a result, the country underwent a period of rapid industrialization and collectivization which laid the foundation for its victory in World War II and post-war dominance of Eastern Europe. Stalin also fomented political paranoia, and conducted the Great Purge to remove opponents of his from the Communist Party through the mass arbitrary arrest of many people (military leaders, Communist Party members, and ordinary citizens alike) who were then sent to correctional labor camps or sentenced to death.
At the beginning of World War II, Stalin signed a non-aggression pact with Hitler's Germany; the treaty delayed confrontation between the two countries. In June 1941 the Germans invaded, opening the largest and bloodiest theater of war in history. Soviet war casualties accounted for the highest proportion of the conflict in the cost of acquiring the upper hand over Axis forces at intense battles such as Stalingrad. Soviet forces eventually captured Berlin in 1945. The territory overtaken by the Red Army became satellite states of the Eastern Bloc. The Cold War emerged in 1947 as the Soviet bloc confronted the Western states that united in the North Atlantic Treaty Organization in 1949.
Following Stalin's death in 1953, a period of political and economic liberalization, known as "de-Stalinization" and "Khrushchev's Thaw", occurred under the leadership of Nikita Khrushchev. The country developed rapidly, as millions of peasants were moved into industrialized cities. The USSR took an early lead in the Space Race with the first ever satellite and the first human spaceflight. In the 1970s, there was a brief "détente" of relations with the United States, but tensions resumed when the Soviet Union deployed troops in Afghanistan in 1979. The war drained economic resources and was matched by an escalation of American military aid to Mujahideen fighters.
In the mid-1980s, the last Soviet leader, Mikhail Gorbachev, sought to further reform and liberalize the economy through his policies of "glasnost" and "perestroika". The goal was to preserve the Communist Party while reversing economic stagnation. The Cold War ended during his tenure, and in 1989 Soviet satellite countries in Eastern Europe overthrew their respective communist regimes. This led to the rise of strong nationalist and separatist movements inside the USSR as well. Central authorities initiated a referendum—boycotted by the Baltic republics, Armenia, Georgia, and Moldova—which resulted in the majority of participating citizens voting in favor of preserving the Union as a renewed federation. In August 1991, a coup d'état was attempted by Communist Party hardliners. It failed, with Russian President Boris Yeltsin playing a high-profile role in facing down the coup, resulting in the banning of the Communist Party. On 25 December 1991, Gorbachev resigned and the remaining twelve constituent republics emerged from the dissolution of the Soviet Union as independent post-Soviet states. The Russian Federation (formerly the Russian SFSR) assumed the Soviet Union's rights and obligations and is recognized as its continued legal personality.
Geography, climate and environment.
With an area of , the Soviet Union was the world's largest country, a status that is retained by the Russian Federation. Covering a sixth of Earth's land surface, its size was comparable to that of North America. The European portion accounted for a quarter of the country's area, and was the cultural and economic center. The eastern part in Asia extended to the Pacific Ocean to the east and Afghanistan to the south, and, except some areas in Central Asia, was much less populous. It spanned over east to west across 11 time zones, and over north to south. It had five climate zones: tundra, taiga, steppes, desert, and mountains.
The Soviet Union had the world's longest border, like Russia, measuring over , or circumferences of Earth. Two-thirds of it was a coastline. Across the Bering Strait was the United States. The Soviet Union bordered Afghanistan, China, Czechoslovakia, Finland, Hungary, Iran, Mongolia, North Korea, Norway, Poland, Romania, and Turkey from 1945 to 1991.
The Soviet Union's highest mountain was Communism Peak (now Ismoil Somoni Peak) in Tajikistan, at . The Soviet Union also included most of the world's largest lake, the Caspian Sea (shared with Iran), and Lake Baikal, the world's largest freshwater and deepest lake, an internal body of water in Russia.
History.
The last Russian Tsar, Nicholas II, ruled the Russian Empire until his abdication in March 1917 in the aftermath of the February Revolution, due in part to the strain of fighting in World War I, which lacked public support. A short-lived Russian Provisional Government took power, to be overthrown in the October Revolution (N.S. 7 November 1917) by revolutionaries led by the Bolshevik leader Vladimir Lenin.
The Soviet Union was officially established in December 1922 with the union of the Russian, Ukrainian, Byelorussian, and Transcaucasian Soviet republics, each ruled by local Bolshevik parties. Despite the foundation of the Soviet state as a federative entity of many constituent republics, each with its own political and administrative entities, the term "Soviet Russia"strictly applicable only to the Russian Federative Socialist Republicwas often applied to the entire country by non-Soviet writers and politicians.
Revolution and foundation.
Modern revolutionary activity in the Russian Empire began with the Decembrist revolt of 1825. Although serfdom was abolished in 1861, it was done on terms unfavorable to the peasants and served to encourage revolutionaries. A parliament—the State Duma—was established in 1906 after the Russian Revolution of 1905, but Tsar Nicholas II resisted attempts to move from absolute to constitutional monarchy. Social unrest continued and was aggravated during World War I by military defeat and food shortages in major Soviet cities.
A spontaneous popular uprising in Petrograd, in response to the wartime decay of Russia's economy and morale, culminated in the February Revolution and the toppling of the imperial government in March 1917. The tsarist autocracy was replaced by the Russian Provisional Government, which intended to conduct elections to the Russian Constituent Assembly and to continue fighting on the side of the Entente in World War I.
At the same time, workers' councils, known in Russian as "Soviets", sprang up across the country. The Bolsheviks, led by Vladimir Lenin, pushed for socialist revolution in the Soviets and on the streets. On 7 November 1917, the Red Guards stormed the Winter Palace in Petrograd, ending the rule of the Provisional Government and leaving all political power to the Soviets. This event would later be known as the Great October Socialist Revolution. In December, the Bolsheviks signed an armistice with the Central Powers, though by February 1918, fighting had resumed. In March, the Soviets ended involvement in the war for good and signed the Treaty of Brest-Litovsk.
A long and bloody Civil War ensued between the Reds and the Whites, starting in 1917 and ending in 1923 with the Reds' victory. It included foreign intervention, the execution of the former tsar and his family, and the famine of 1921, which killed about five million people. In March 1921, during a related conflict with Poland, the Peace of Riga was signed, splitting disputed territories in Belarus and Ukraine between the Republic of Poland and Soviet Russia. Soviet Russia had to resolve similar conflicts with the newly established Republic of Finland, the Republic of Estonia, the Republic of Latvia, and the Republic of Lithuania.
Unification of republics.
On 28 December 1922, a conference of plenipotentiary delegations from the Russian SFSR, the Transcaucasian SFSR, the Ukrainian SSR and the Byelorussian SSR approved the Treaty on the Creation of the USSR and the Declaration of the Creation of the USSR, forming the Union of Soviet Socialist Republics. These two documents were confirmed by the 1st Congress of Soviets of the USSR and signed by the heads of the delegations, Mikhail Kalinin, Mikhail Tskhakaya, Mikhail Frunze, Grigory Petrovsky, and Alexander Chervyakov, on 30 December 1922. The formal proclamation was made from the stage of the Bolshoi Theatre.
On 1 February 1924, the USSR was recognized by the British Empire. The same year, a Soviet Constitution was approved, legitimizing the December 1922 union.
An intensive restructuring of the economy, industry and politics of the country began in the early days of Soviet power in 1917. A large part of this was done according to the Bolshevik Initial Decrees, government documents signed by Vladimir Lenin. One of the most prominent breakthroughs was the GOELRO plan, which envisioned a major restructuring of the Soviet economy based on total electrification of the country. The plan was developed in 1920 and covered a 10 to 15-year period. It included construction of a network of 30 regional power stations, including ten large hydroelectric power plants, and numerous electric-powered large industrial enterprises. The plan became the prototype for subsequent Five-Year Plans and was fulfilled by 1931.
Stalin era.
From its creation, the government in the Soviet Union was based on the one-party rule of the Communist Party (Bolsheviks). After the economic policy of "War communism" during the Russian Civil War, as a prelude to fully developing socialism in the country, the Soviet government permitted some private enterprise to coexist alongside nationalized industry in the 1920s and total food requisition in the countryside was replaced by a food tax.
The stated purpose of the one-party state was to ensure that capitalist exploitation would not return to the Soviet Union and that the principles of democratic centralism would be most effective in representing the people's will in a practical manner. Debate over the future of the economy provided the background for a power struggle in the years after Lenin's death in 1924. Initially, Lenin was to be replaced by a "troika" consisting of Grigory Zinoviev of the Ukrainian SSR, Lev Kamenev of the Russian SFSR, and Joseph Stalin of the Transcaucasian SFSR.
On 3 April 1922, Stalin was named the General Secretary of the Communist Party of the Soviet Union. Lenin had appointed Stalin the head of the Workers' and Peasants' Inspectorate, which gave Stalin considerable power. By gradually consolidating his influence and isolating and outmaneuvering his rivals within the party, Stalin became the undisputed leader of the Soviet Union and, by the end of the 1920s, established totalitarian rule. In October 1927, Grigory Zinoviev and Leon Trotsky were expelled from the Central Committee and forced into exile.
In 1928, Stalin introduced the First Five-Year Plan for building a socialist economy. In place of the internationalism expressed by Lenin throughout the Revolution, it aimed to build Socialism in One Country. In industry, the state assumed control over all existing enterprises and undertook an intensive program of industrialization. In agriculture, rather than adhering to the "lead by example" policy advocated by Lenin, forced collectivization of farms was implemented all over the country.
Famines ensued, causing millions of deaths; surviving kulaks were persecuted and many sent to Gulags to do forced labour. Social upheaval continued in the mid-1930s. Stalin's Great Purge resulted in the execution or detainment of many "Old Bolsheviks" who had participated in the October Revolution with Lenin. According to declassified Soviet archives, in 1937 and 1938, the NKVD arrested more than one and a half million people, of whom 681,692 were shot. Over those two years that averages to over one thousand executions a day. According to historian Geoffrey Hosking, "...excess deaths during the 1930s as a whole were in the range of 10–11 million." Yet despite the turmoil of the mid-to-late 1930s, the Soviet Union developed a powerful industrial economy in the years before World War II.
1930s.
The early 1930s saw closer cooperation between the West and the USSR. From 1932 to 1934, the Soviet Union participated in the World Disarmament Conference. In 1933, diplomatic relations between the United States and the USSR were established when in November, the newly elected President of the United States, Franklin D. Roosevelt chose to formally recognize Stalin's Communist government and negotiated a new trade agreement between the two nations. In September 1934, the Soviet Union joined the League of Nations. After the Spanish Civil War broke out in 1936, the USSR actively supported the Republican forces against the Nationalists, who were supported by Fascist Italy and Nazi Germany.
In December 1936, Stalin unveiled a new Soviet Constitution. The constitution was seen as a personal triumph for Stalin, who By contrast, Western historians and historians from former Soviet occupied countries have viewed the constitution as a meaningless propaganda document.
The late 1930s saw a shift towards the Axis powers. In 1939, almost a year after the United Kingdom and France had concluded the Munich Agreement with Germany, the USSR dealt with the Nazis as well, both militarily and economically during extensive talks. The two countries concluded the German–Soviet Nonaggression Pact and the German–Soviet Commercial Agreement in August 1939. The nonaggression pact made possible Soviet occupation of Lithuania, Latvia, Estonia, Bessarabia, northern Bukovina, and eastern Poland. In late November of the same year, unable to coerce the Republic of Finland by diplomatic means into moving its border back from Leningrad, Joseph Stalin ordered the invasion of Finland.
In the east, the Soviet military won several decisive victories during border clashes with the Empire of Japan in 1938 and 1939. However, in April 1941, USSR signed the Soviet–Japanese Neutrality Pact with the Empire of Japan, recognizing the territorial integrity of Manchukuo, a Japanese puppet state.
World War II.
Although it has been debated whether the Soviet Union intended to invade Germany once it was strong enough, Germany itself broke the treaty and invaded the Soviet Union on 22 June 1941, starting what was known in the USSR as the "Great Patriotic War". The Red Army stopped the seemingly invincible German Army at the Battle of Moscow, aided by an unusually harsh winter. The Battle of Stalingrad, which lasted from late 1942 to early 1943, dealt a severe blow to the Germans from which they never fully recovered and became a turning point in the war. After Stalingrad, Soviet forces drove through Eastern Europe to Berlin before Germany surrendered in 1945. The German Army suffered 80% of its military deaths in the Eastern Front.
The same year, the USSR, in fulfillment of its agreement with the Allies at the Yalta Conference, denounced the Soviet–Japanese Neutrality Pact in April 1945 and invaded Manchukuo and other Japan-controlled territories on 9 August 1945. This conflict ended with a decisive Soviet victory, contributing to the unconditional surrender of Japan and the end of World War II.
The Soviet Union suffered greatly in the war, losing around 27 million people. During the war, the Soviet Union together with the United States, the United Kingdom and China were considered as the Big Four of Allied powers in World War II and later became the Four Policemen which was the foundation of the United Nations Security Council. It emerged as a superpower in the post-war period. Once denied diplomatic recognition by the Western world, the Soviet Union had official relations with practically every nation by the late 1940s. A member of the United Nations at its foundation in 1945, the Soviet Union became one of the five permanent members of the United Nations Security Council, which gave it the right to veto any of its resolutions.
The Soviet Union maintained its status as one of the world's two superpowers for four decades through its hegemony in Eastern Europe, military strength, economic strength, aid to developing countries, and scientific research, especially in space technology and weaponry.
Cold War.
During the immediate postwar period, the Soviet Union rebuilt and expanded its economy, while maintaining its strictly centralized control. It aided post-war reconstruction in the countries of Eastern Europe, while turning them into satellite states, binding them in a military alliance (the Warsaw Pact) in 1955, and an economic organization (The Council for Mutual Economic Assistance or Comecon) from 1949 to 1991, the latter a counterpart to the European Economic Community. Later, the Comecon supplied aid to the eventually victorious Communist Party of China, and saw its influence grow elsewhere in the world. Fearing its ambitions, the Soviet Union's wartime allies, the United Kingdom and the United States, became its enemies. In the ensuing Cold War, the two sides clashed indirectly using mostly proxies.
Khrushchev era.
Stalin died on 5 March 1953. Without a mutually agreeable successor, the highest Communist Party officials opted to rule the Soviet Union jointly. Nikita Khrushchev, who had won the power struggle by the mid-1950s, denounced Stalin's use of repression in 1956 and eased repressive controls over party and society. This was known as de-Stalinization.
Moscow considered Eastern Europe to be a buffer zone for the forward defense of its western borders, and ensured its control of the region by transforming the Eastern European countries into satellite states. Soviet military force was used to suppress anti-Stalinist uprisings in Hungary and Poland in 1956.
In the late 1950s, a confrontation with China regarding the USSR's rapprochement with the West and what Mao Zedong perceived as Khrushchev's revisionism led to the Sino–Soviet split. This resulted in a break throughout the global Marxist–Leninist movement, with the governments in Albania, Cambodia and Somalia choosing to ally with China in place of the USSR.
During this period of the late 1950s and early 1960s, the Soviet Union continued to realize scientific and technological exploits in the Space Race, rivaling the United States: launching the first artificial satellite, Sputnik 1 in 1957; a living dog named Laika in 1957; the first human being, Yuri Gagarin in 1961; the first woman in space, Valentina Tereshkova in 1963; Alexey Leonov, the first person to walk in space in 1965; the first soft landing on the moon by spacecraft Luna 9 in 1966 and the first moon rovers, Lunokhod 1 and Lunokhod 2.
Khrushchev initiated "The Thaw", a complex shift in political, cultural and economic life in the Soviet Union. This included some openness and contact with other nations and new social and economic policies with more emphasis on commodity goods, allowing living standards to rise dramatically while maintaining high levels of economic growth. Censorship was relaxed as well.
Khrushchev's reforms in agriculture and administration, however, were generally unproductive. In 1962, he precipitated a crisis with the United States over the Soviet deployment of nuclear missiles in Cuba. An agreement was made between the Soviet Union and the United States to remove enemy nuclear missiles from both Cuba and Turkey, concluding the crisis. This event caused Khrushchev much embarrassment and loss of prestige, resulting in his removal from power in 1964.
Brezhnev era.
Following the ousting of Khrushchev, another period of collective leadership ensued, consisting of Leonid Brezhnev as General Secretary, Alexei Kosygin as Premier and Nikolai Podgorny as Chairman of the Presidium, lasting until Brezhnev established himself in the early 1970s as the preeminent Soviet leader.
In 1968, the Soviet Union and Warsaw Pact allies invaded Czechoslovakia to halt the Prague Spring reforms. In the aftermath, Brezhnev justified the invasion along with the earlier invasions of Eastern European states by introducing the Brezhnev Doctrine, which claimed the right of the Soviet Union to violate the sovereignty of any country that attempted to replace Marxism–Leninism with capitalism.
Brezhnev presided over a period of "détente" with the West that resulted in treaties on armament control (SALT I, SALT II, Anti-Ballistic Missile Treaty) while at the same time building up Soviet military might.
In October 1977, the third Soviet Constitution was unanimously adopted. The prevailing mood of the Soviet leadership at the time of Brezhnev's death in 1982 was one of aversion to change. The long period of Brezhnev's rule had come to be dubbed one of "standstill", with an aging and ossified top political leadership.
Gorbachev era.
Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. Kenneth S. Deffeyes argued in "Beyond Oil" that the Reagan administration encouraged Saudi Arabia to lower the price of oil to the point where the Soviets could not make a profit selling their oil, so that the USSR's hard currency reserves became depleted.
Brezhnev's next two successors, transitional figures with deep roots in his tradition, did not last long. Yuri Andropov was 68 years old and Konstantin Chernenko 72 when they assumed power; both died in less than two years. In an attempt to avoid a third short-lived leader, in 1985, the Soviets turned to the next generation and selected Mikhail Gorbachev.
Gorbachev made significant changes in the economy and party leadership, called "perestroika". His policy of "glasnost" freed public access to information after decades of heavy government censorship.
Gorbachev also moved to end the Cold War. In 1988, the Soviet Union abandoned its nine-year war in Afghanistan and began to withdraw its forces. In the late 1980s, , which paved the way for Revolutions of 1989. With the tearing down of the Berlin Wall and with East Germany and West Germany pursuing unification, the Iron Curtain came down.
In the late 1980s, the constituent republics of the Soviet Union started legal moves towards potentially declaring sovereignty over their territories, citing Article 72 of the USSR constitution, which stated that any constituent republic was free to secede. On 7 April 1990, a law was passed allowing a republic to secede if more than two-thirds of its residents voted for it in a referendum. Many held their first free elections in the Soviet era for their own national legislatures in 1990. Many of these legislatures proceeded to produce legislation contradicting the Union laws in what was known as the "War of Laws".
In 1989, the Russian SFSR, which was then the largest constituent republic (with about half of the population) convened a newly elected Congress of People's Deputies. Boris Yeltsin was elected its chairman. On 12 June 1990, the Congress declared Russia's sovereignty over its territory and proceeded to pass laws that attempted to supersede some of the USSR's laws. After a landslide victory of Sąjūdis in Lithuania, that country declared its independence restored on 11 March 1990.
A referendum for the preservation of the USSR was held on 17 March 1991 in nine republics (the remainder having boycotted the vote), with the majority of the population in those nine republics voting for preservation of the Union. The referendum gave Gorbachev a minor boost. In the summer of 1991, the New Union Treaty, which would have turned the Soviet Union into a much looser Union, was agreed upon by eight republics.
The signing of the treaty, however, was interrupted by the August Coup—an attempted coup d'état by hardline members of the government and the KGB who sought to reverse Gorbachev's reforms and reassert the central government's control over the republics. After the coup collapsed, Yeltsin was seen as a hero for his decisive actions, while Gorbachev's power was effectively ended. The balance of power tipped significantly towards the republics. In August 1991, Latvia and Estonia immediately declared the restoration of their full independence (following Lithuania's 1990 example). Gorbachev resigned as general secretary in late August, and soon afterward the Party's activities were indefinitely suspended—effectively ending its rule. By the fall, Gorbachev could no longer influence events outside of Moscow, and he was being challenged even there by Yeltsin, who had been elected President of Russia in July 1991.
Dissolution.
The remaining 12 republics continued discussing new, increasingly looser, models of the Union. However, by December, all except Russia and Kazakhstan had formally declared independence. During this time, Yeltsin took over what remained of the Soviet government, including the Moscow Kremlin. The final blow was struck on 1 December, when Ukraine, the second most powerful republic, voted overwhelmingly for independence. Ukraine's secession ended any realistic chance of the Soviet Union staying together even on a limited scale.
On 8 December 1991, the presidents of Russia, Ukraine and Belarus (formerly Byelorussia), signed the Belavezha Accords, which declared the Soviet Union dissolved and established the Commonwealth of Independent States (CIS) in its place. While doubts remained over the authority of the accords to do this, on 21 December 1991, the representatives of all Soviet republics except Georgia signed the Alma-Ata Protocol, which confirmed the accords. On 25 December 1991, Gorbachev resigned as the President of the USSR, declaring the office extinct. He turned the powers that had been vested in the presidency over to Yeltsin. That night, the Soviet flag was lowered for the last time, and the Russian tricolor was raised in its place.
The following day, the Supreme Soviet, the highest governmental body of the Soviet Union, voted both itself and the Soviet Union out of existence. This is generally recognized as marking the official, final dissolution of the Soviet Union as a functioning state. The Soviet Army originally remained under overall CIS command, but was soon absorbed into the different military forces of the newly independent states. The few remaining Soviet institutions that had not been taken over by Russia ceased to function by the end of 1991.
Following the dissolution of the Soviet Union on 26 December 1991, Russia was internationally recognized as its legal successor on the international stage. To that end, Russia voluntarily accepted all Soviet foreign debt and claimed overseas Soviet properties as its own. Under the 1992 Lisbon Protocol, Russia also agreed to receive all nuclear weapons remaining in the territory of other former Soviet republics. Since then, the Russian Federation has assumed the Soviet Union's rights and obligations.
Post-Soviet states.
The analysis of the succession of states with respect to the 15 post-Soviet states is complex. The Russian Federation is seen as the legal "continuator" state and is for most purposes the heir to the Soviet Union. It retained ownership of all former Soviet embassy properties, as well as the old Soviet UN membership and permanent membership on the Security Council. The Baltic states are not successor states to the Soviet Union; they are instead considered to have "de jure" continuity with their pre-World War II governments through the non-recognition of the original Soviet incorporation in 1940. The other 11 post-Soviet states are considered newly-independent successor states to the Soviet Union.
There are additionally four states that claim independence from the other internationally recognized post-Soviet states, but possess limited international recognition: Abkhazia, Nagorno-Karabakh, South Ossetia, and Transnistria. The Chechen separatist movement of the Chechen Republic of Ichkeria lacks any international recognition.
The economic shocks that accompanied wholesale privatization were associated with sharp increases in mortality. Data shows Russia, Kazakhstan, Latvia, Lithuania and Estonia saw a tripling of unemployment and a 42% increase in male death rates between 1991 and 1994.
Politics.
There were three power hierarchies in the Soviet Union: the legislature represented by the Supreme Soviet of the Soviet Union, the government represented by the Council of Ministers, and the Communist Party of the Soviet Union (CPSU), the only legal party and the ultimate policymaker in the country.
Communist Party.
At the top of the Communist Party was the Central Committee, elected at Party Congresses and Conferences. The Central Committee in turn voted for a Politburo (called the Presidium between 1952–1966), Secretariat and the General Secretary (First Secretary from 1953 to 1966), the de facto highest office in the USSR. Depending on the degree of power consolidation, it was either the Politburo as a collective body or the General Secretary, who always was one of the Politburo members, that effectively led the party and the country (except for the period of the highly personalized authority of Stalin, exercised directly through his position in the Council of Ministers rather than the Politburo after 1941). They were not controlled by the general party membership, as the key principle of the party organization was democratic centralism, demanding strict subordination to higher bodies, and elections went uncontested, endorsing the candidates proposed from above.
The Communist Party maintained its dominance over the state largely through its control over the system of appointments. All senior government officials and most deputies of the Supreme Soviet were members of the CPSU. Of the party heads themselves, Stalin in 1941–1953 and Khrushchev in 1958–1964 were Premiers. Upon the forced retirement of Khrushchev, the party leader was prohibited from this kind of double membership, but the later General Secretaries for at least some part of their tenure occupied the largely ceremonial position of Chairman of the Presidium of the Supreme Soviet, the nominal head of state. The institutions at lower levels were overseen and at times supplanted by primary party organizations.
In practice, however, the degree of control the party was able to exercise over the state bureaucracy, particularly after the death of Stalin, was far from total, with the bureaucracy pursuing different interests that were at times in conflict with the party. Nor was the party itself monolithic from top to bottom, although factions were officially banned.
Government.
The Supreme Soviet (successor of the Congress of Soviets and Central Executive Committee) was nominally the highest state body for most of the Soviet history, at first acting as a rubber stamp institution, approving and implementing all decisions made by the party. However, the powers and functions of the Supreme Soviet were extended in the late 1950s, 1960s and 1970s, including the creation of new state commissions and committees. It gained additional powers relating to the approval of the Five-Year Plans and the Soviet government budget. The Supreme Soviet elected a Presidium to wield its power between plenary sessions, ordinarily held twice a year, and appointed the Supreme Court, the Procurator General and the Council of Ministers (known before 1946 as the Council of People's Commissars), headed by the Chairman (Premier) and managing an enormous bureaucracy responsible for the administration of the economy and society. State and party structures of the constituent republics largely emulated the structure of the central institutions, although the Russian SFSR, unlike the other constituent republics, for most of its history had no republican branch of the CPSU, being ruled directly by the union-wide party until 1990. Local authorities were organized likewise into party committees, local Soviets and executive committees. While the state system was nominally federal, the party was unitary.
The state security police (the KGB and its predecessor agencies) played an important role in Soviet politics. It was instrumental in the Stalinist terror, but after the death of Stalin, the state security police was brought under strict party control. Under Yuri Andropov, KGB chairman in 1967–1982 and General Secretary from 1982 to 1983, the KGB engaged in the suppression of political dissent and maintained an extensive network of informers, reasserting itself as a political actor to some extent independent of the party-state structure, culminating in the anti-corruption campaign targeting high party officials in the late 1970s and early 1980s.
Separation of power and reform.
The Union constitutions, which were promulgated in 1918, 1924, 1936 and 1977, did not limit state power. No formal separation of powers existed between the Party, Supreme Soviet and Council of Ministers that represented executive and legislative branches of the government. The system was governed less by statute than by informal conventions, and no settled mechanism of leadership succession existed. Bitter and at times deadly power struggles took place in the Politburo after the deaths of Lenin and Joseph Stalin, as well as after Khrushchev's dismissal, itself due to a decision by both the Politburo and the Central Committee. All leaders of the Communist Party before Gorbachev died in office, except Georgy Malenkov and Khrushchev, both dismissed from the party leadership amid internal struggle within the party.
Between 1988 and 1990, facing considerable opposition, Mikhail Gorbachev enacted reforms shifting power away from the highest bodies of the party and making the Supreme Soviet less dependent on them. The Congress of People's Deputies was established, the majority of whose members were directly elected in competitive elections held in March 1989. The Congress now elected the Supreme Soviet, which became a full-time parliament, much stronger than before. For the first time since the 1920s, it refused to rubber stamp proposals from the party and Council of Ministers. In 1990, Gorbachev introduced and assumed the position of the President of the Soviet Union, concentrated power in his executive office, independent of the party, and subordinated the government, now renamed the Cabinet of Ministers of the USSR, to himself.
Tensions grew between the union-wide authorities under Gorbachev, reformists led in Russia by Boris Yeltsin and controlling the newly elected Supreme Soviet of the Russian SFSR, and Communist Party hardliners. On 19–21 August 1991, a group of hardliners staged an abortive coup attempt. Following the failed coup, the State Council of the Soviet Union became the highest organ of state power "in the period of transition". Gorbachev resigned as General Secretary, only remaining President for the final months of the existence of the USSR.
Judicial system.
The judiciary was not independent of the other branches of government. The Supreme Court supervised the lower courts (People's Court) and applied the law as established by the Constitution or as interpreted by the Supreme Soviet. The Constitutional Oversight Committee reviewed the constitutionality of laws and acts. The Soviet Union used the inquisitorial system of Roman law, where the judge, procurator, and defense attorney collaborate to establish the truth.
Administrative divisions.
Constitutionally, the USSR was a federation of constituent Union Republics, which were either unitary states, such as Ukraine or Belarus (SSRs), or federal states, such as Russia or Transcaucasia (SFSRs), all four being the founding republics who signed the Treaty on the Creation of the USSR in December 1922. In 1924, during the national delimitation in Central Asia, the Uzbek and Turkmen SSRs were formed from parts of the Russia's Turkestan ASSR and two Soviet dependencies, the Khorezm and Bukharan SSRs. In 1929, the Tajik SSR was split off from the Uzbek SSR. With the constitution of 1936, the Transcaucasian SFSR was dissolved, resulting in its constituent Armenian, Georgian and Azerbaijan SSRs being elevated to Union Republics, while the Kazakh and Kirghiz SSRs were split off from Russian SFSR, resulting in the same status. In August 1940, the Moldavian SSR was formed from parts of the Ukrainian SSR and Bessarabia and Northern Bukovina. The Estonian, Latvian and Lithuanian SSRs were also admitted into the union. The Karelo-Finnish SSR was split off from Russia as a Union Republic in March 1940 and was reabsorbed in 1956. Between July 1956 and September 1991, there were 15 union republics (see map below).
Economy.
The Soviet Union became the first country to adopt a planned economy, whereby production and distribution of goods were centralized and directed by the government. The first Bolshevik experience with a command economy was the policy of War communism, which involved the nationalization of industry, centralized distribution of output, coercive requisition of agricultural production, and attempts to eliminate the circulation of money, as well as private enterprises and free trade. After the severe economic collapse caused by the war, Lenin replaced War Communism with the New Economic Policy (NEP) in 1921, legalising free trade and private ownership of smaller businesses. The economy quickly recovered.
Following a lengthy debate among the members of Politburo over the course of economic development, by 1928–1929, upon gaining control of the country, Joseph Stalin abandoned the NEP and pushed for full central planning, starting forced collectivization of agriculture and enacting draconian labor legislation. Resources were mobilized for rapid industrialization, which greatly expanded Soviet capacity in heavy industry and capital goods during the 1930s. Preparation for war was one of the main driving forces behind industrialization, mostly due to distrust of the outside capitalistic world. As a result, the USSR was transformed from a largely agrarian economy into a great industrial power, leading the way for its emergence as a superpower after World War II. During the war, the Soviet economy and infrastructure suffered massive devastation and required extensive reconstruction.
By the early 1940s, the Soviet economy had become relatively self-sufficient; for most of the period until the creation of Comecon, only a very small share of domestic products was traded internationally. After the creation of the Eastern Bloc, external trade rose rapidly. Still the influence of the world economy on the USSR was limited by fixed domestic prices and a state monopoly on foreign trade. Grain and sophisticated consumer manufactures became major import articles from around the 1960s. During the arms race of the Cold War, the Soviet economy was burdened by military expenditures, heavily lobbied for by a powerful bureaucracy dependent on the arms industry. At the same time, the Soviet Union became the largest arms exporter to the Third World. Significant amounts of Soviet resources during the Cold War were allocated in aid to the other socialist states.
From the 1930s until its collapse in late 1991, the way the Soviet economy operated remained essentially unchanged. The economy was formally directed by central planning, carried out by Gosplan and organized in five-year plans. In practice, however, the plans were highly aggregated and provisional, subject to "ad hoc" intervention by superiors. All key economic decisions were taken by the political leadership. Allocated resources and plan targets were normally denominated in rubles rather than in physical goods. Credit was discouraged, but widespread. Final allocation of output was achieved through relatively decentralized, unplanned contracting. Although in theory prices were legally set from above, in practice the actual prices were often negotiated, and informal horizontal links (between producer factories etc.) were widespread.
A number of basic services were state-funded, such as education and healthcare. In the manufacturing sector, heavy industry and defense were assigned higher priority than the production of consumer goods. Consumer goods, particularly outside large cities, were often scarce, of poor quality and limited choice. Under command economy, consumers had almost no influence over production, so the changing demands of a population with growing incomes could not be satisfied by supplies at rigidly fixed prices. A massive unplanned second economy grew up alongside the planned one at low levels, providing some of the goods and services that the planners could not. Legalization of some elements of the decentralized economy was attempted with the reform of 1965.
Although statistics of the Soviet economy are notoriously unreliable and its economic growth difficult to estimate precisely, by most accounts, the economy continued to expand until the mid-1980s. During the 1950s and 1960s, the Soviet economy experienced comparatively high growth and was catching up to the West. However, after 1970, the growth, while still positive, steadily declined much more quickly and consistently than in other countries despite a rapid increase in the capital stock (the rate of increase in capital was only surpassed by Japan).
Overall, between 1960 and 1989, the growth rate of per capita income in the Soviet Union was slightly above the world average (based on 102 countries). According to Stanley Fischer and William Easterly, growth could have been faster. By their calculation, per capita income of Soviet Union in 1989 should have been twice as high as it was considering the amount of investment, education and population. The authors attribute this poor performance to low productivity of capital in the Soviet Union. Steven Rosenfielde states that the standard of living actually declined as a result of Stalin's despotism, and while there was a brief improvement following his death, lapsed into stagnation.
In 1987, Mikhail Gorbachev tried to reform and revitalize the economy with his program of "perestroika". His policies relaxed state control over enterprises, but did not yet allow it to be replaced by market incentives, ultimately resulting in a sharp decline in production output. The economy, already suffering from reduced petroleum export revenues, started to collapse. Prices were still fixed, and property was still largely state-owned until after the dissolution of the Soviet Union. For most of the period after World War II up to its collapse, the Soviet economy was the second largest in the world by GDP (PPP), and was 3rd in the world during the middle of the 1980s to 1989, though in per capita terms the Soviet GDP was behind that of the First World countries.
Energy.
The need for fuel declined in the Soviet Union from the 1970s to the 1980s, both per ruble of gross social product and per ruble of industrial product. At the start, this decline grew very rapidly but gradually slowed down between 1970 and 1975. From 1975 and 1980, it grew even slower, only 2.6 percent. David Wilson, a historian, believed that the gas industry would account for 40 percent of Soviet fuel production by the end of the century. His theory did not come to fruition because of the USSR's collapse. The USSR, in theory, would have continued to have an economic growth rate of 2–2.5 percent during the 1990s because of Soviet energy fields. However, the energy sector faced many difficulties, among them the country's high military expenditure and hostile relations with the First World (pre-Gorbachev era).
In 1991, the Soviet Union had a pipeline network of for crude oil and another for natural gas. Petroleum and petroleum-based products, natural gas, metals, wood, agricultural products, and a variety of manufactured goods, primarily machinery, arms and military equipment, were exported. In the 1970s and 1980s, the Soviet Union heavily relied on fossil fuel exports to earn hard currency. At its peak in 1988, it was the largest producer and second largest exporter of crude oil, surpassed only by Saudi Arabia.
Science and technology.
The Soviet Union placed great emphasis on science and technology within its economy, however, the most remarkable Soviet successes in technology, such as producing the world's first space satellite, typically were the responsibility of the military. Lenin believed that the USSR would never overtake the developed world if it remained as technologically backward as it was upon its founding. Soviet authorities proved their commitment to Lenin's belief by developing massive networks, research and development organizations. In the early 1960s, the Soviets awarded 40% of chemistry PhDs to women, compared to only 5% who received such a degree in the United States. By 1989, Soviet scientists were among the world's best-trained specialists in several areas, such as energy physics, selected areas of medicine, mathematics, welding and military technologies. Due to rigid state planning and bureaucracy, the Soviets remained far behind technologically in chemistry, biology, and computers when compared to the First World.
Project Socrates, under the Reagan administration, determined that the Soviet Union addressed the acquisition of science and technology in a manner that was radically different from what the US was using. In the case of the US, economic prioritization was being used for indigenous research and development as the means to acquire science and technology in both the private and public sectors. In contrast, the Soviet Union was offensively and defensively maneuvering in the acquisition and utilization of the worldwide technology, to increase the competitive advantage that they acquired from the technology, while preventing the US from acquiring a competitive advantage. However, in addition, the Soviet Union's technology-based planning was executed in a centralized, government-centric manner that greatly hindered its flexibility. It was this significant lack of flexibility that was exploited by the US to undermine the strength of the Soviet Union and thus foster its reform.
Transport.
Transport was a key component of the nation's economy. The economic centralization of the late 1920s and 1930s led to the development of infrastructure on a massive scale, most notably the establishment of Aeroflot, an aviation enterprise. The country had a wide variety of modes of transport by land, water and air. However, due to bad maintenance, much of the road, water and Soviet civil aviation transport were outdated and technologically backward compared to the First World.
Soviet rail transport was the largest and most intensively used in the world; it was also better developed than most of its Western counterparts. By the late 1970s and early 1980s, Soviet economists were calling for the construction of more roads to alleviate some of the burden from the railways and to improve the Soviet government budget. The street network and automotive industry remained underdeveloped, and dirt roads were common outside major cities. Soviet maintenance projects proved unable to take care of even the few roads the country had. By the early-to-mid-1980s, the Soviet authorities tried to solve the road problem by ordering the construction of new ones. Meanwhile, the automobile industry was growing at a faster rate than road construction. The underdeveloped road network led to a growing demand for public transport.
Despite improvements, several aspects of the transport sector were still riddled with problems due to outdated infrastructure, lack of investment, corruption and bad decision-making. Soviet authorities were unable to meet the growing demand for transport infrastructure and services.
The Soviet merchant navy was one of the largest in the world.
Demographics.
Excess deaths over the course of World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million, some 10 million in the 1930s, and more than 26 million in 1941–5. The postwar Soviet population was 45 to 50 million smaller than it would have been if pre-war demographic growth had continued. According to Catherine Merridale, "... reasonable estimate would place the total number of excess deaths for the whole period somewhere around 60 million."
The birth rate of the USSR decreased from 44.0 per thousand in 1926 to 18.0 in 1974, largely due to increasing urbanization and the rising average age of marriages. The mortality rate demonstrated a gradual decrease as well – from 23.7 per thousand in 1926 to 8.7 in 1974. In general, the birth rates of the southern republics in Transcaucasia and Central Asia were considerably higher than those in the northern parts of the Soviet Union, and in some cases even increased in the post–World War II period, a phenomenon partly attributed to slower rates of urbanization and traditionally earlier marriages in the southern republics. Soviet Europe moved towards sub-replacement fertility, while Soviet Central Asia continued to exhibit population growth well above replacement-level fertility.
The late 1960s and the 1970s witnessed a reversal of the declining trajectory of the rate of mortality in the USSR, and was especially notable among men of working age, but was also prevalent in Russia and other predominantly Slavic areas of the country. An analysis of the official data from the late 1980s showed that after worsening in the late-1970s and the early 1980s, adult mortality began to improve again. The infant mortality rate increased from 24.7 in 1970 to 27.9 in 1974. Some researchers regarded the rise as largely real, a consequence of worsening health conditions and services. The rises in both adult and infant mortality were not explained or defended by Soviet officials, and the Soviet government simply stopped publishing all mortality statistics for ten years. Soviet demographers and health specialists remained silent about the mortality increases until the late-1980s, when the publication of mortality data resumed and researchers could delve into the real causes.
Education.
Before 1917, education was not free in the Russian Empire and was therefore either inaccessible or barely accessible for many children from lower-class working and peasant families. Estimates from 1917 recorded that 75–85 percent of the Russian population was illiterate.
Anatoly Lunacharsky became the first People's Commissar for Education of Soviet Russia. At the beginning, the Soviet authorities placed great emphasis on the elimination of illiteracy. People who were literate were automatically hired as teachers. For a short period, quality was sacrificed for quantity. By 1940, Joseph Stalin could announce that illiteracy had been eliminated. Throughout the 1930s social mobility rose sharply, which has been attributed to Soviet reforms in education. In the aftermath of the Great Patriotic War, the country's educational system expanded dramatically. This expansion had a tremendous effect. In the 1960s, nearly all Soviet children had access to education, the only exception being those living in remote areas. Nikita Khrushchev tried to make education more accessible, making it clear to children that education was closely linked to the needs of society. Education also became important in giving rise to the New Man. Citizens directly entering the work force had the constitutional right to a job and to free vocational training.
The country's system of education was highly centralized and universally accessible to all citizens, with affirmative action for applicants from nations associated with cultural backwardness. However, as part of the general antisemitic policy, an unofficial Jewish quota was applied in the leading institutions of higher education by subjecting Jewish applicants to harsher entrance examinations. The Brezhnev era also introduced a rule that required all university applicants to present a reference from the local Komsomol party secretary. According to statistics from 1986, the number of higher education students per the population of 10,000 was 181 for the USSR, compared to 517 for the U.S.
Ethnic groups.
The Soviet Union was a very ethnically diverse country, with more than 100 distinct ethnic groups. The total population was estimated at 293 million in 1991. According to a 1990 estimate, the majority were Russians (50.78%), followed by Ukrainians (15.45%) and Uzbeks (5.84%).
All citizens of the USSR had their own ethnic affiliation. The ethnicity of a person was chosen at the age of sixteen by the child's parents. If the parents did not agree, the child was automatically assigned the ethnicity of the father. Partly due to Soviet policies, some of the smaller minority ethnic groups were considered part of larger ones, such as the Mingrelians of Georgia, who were classified with the linguistically related Georgians. Some ethnic groups voluntarily assimilated, while others were brought in by force. Russians, Belarusians, and Ukrainians shared close cultural ties, while other groups did not. With multiple nationalities living in the same territory, ethnic antagonisms developed over the years.
Health.
In 1917, before the revolution, health conditions were significantly behind those of developed countries. As Lenin later noted, "Either the lice will defeat socialism, or socialism will defeat the lice". The Soviet principle of health care was conceived by the People's Commissariat for Health in 1918. Health care was to be controlled by the state and would be provided to its citizens free of charge, this at the time being a revolutionary concept. Article 42 of the 1977 Soviet Constitution gave all citizens the right to health protection and free access to any health institutions in the USSR. Before Leonid Brezhnev became General Secretary, the healthcare system of the Soviet Union was held in high esteem by many foreign specialists. This changed however, from Brezhnev's accession and Mikhail Gorbachev's tenure as leader, the Soviet health care system was heavily criticized for many basic faults, such as the quality of service and the unevenness in its provision. Minister of Health Yevgeniy Chazov, during the 19th Congress of the Communist Party of the Soviet Union, while highlighting such Soviet successes as having the most doctors and hospitals in the world, recognized the system's areas for improvement and felt that billions of Soviet rubles were squandered. 
After the socialist revolution, the life expectancy for all age groups went up. This statistic in itself was seen by some that the socialist system was superior to the capitalist system. These improvements continued into the 1960s, when the life expectancy in the Soviet Union surpassed that of the United States. It remained stable during most years, although in the 1970s, it went down slightly, possibly because of alcohol abuse. At the same time, infant mortality began to rise. After 1974, the government stopped publishing statistics on this. This trend can be partly explained by the number of pregnancies rising drastically in the Asian part of the country where infant mortality was highest, while declining markedly in the more developed European part of the Soviet Union. The USSR had several centers of excellence, such as the Fyodorov Eye Microsurgery Complex, founded in 1988 by Russian eye surgeon Svyatoslav Fyodorov.
Language.
The Soviet government headed by Vladimir Lenin gave small language groups their own writing systems. The development of these writing systems was very successful, even though some flaws were detected. During the later days of the USSR, countries with the same multilingual situation implemented similar policies. A serious problem when creating these writing systems was that the languages differed dialectally greatly from each other. When a language had been given a writing system and appeared in a notable publication, that language would attain "official language" status. There were many minority languages which never received their own writing system; therefore their speakers were forced to have a second language. There are examples where the Soviet government retreated from this policy, most notable under Stalin's regime, where education was discontinued in languages which were not widespread enough. These languages were then assimilated into another language, mostly Russian. During the Great Patriotic War, some minority languages were banned, and their speakers accused of collaborating with the enemy.
As the most widely spoken of the Soviet Union's many languages, Russian "de facto" functioned as an official language, as the "language of interethnic communication" (), but only assumed the "de jure" status as the official national language in 1990.
Religion.
The religious made up a significant minority of the Soviet Union prior to break up. In 1990, the religious makeup was 20% Russian Orthodox, 10% Muslim, 7% Protestant, Armenian Apostolic, Georgian Orthodox, and Roman Catholic, less than 1% Jewish and 60% atheist.
Christianity and Islam had the greatest number of adherents among the Soviet state's religious citizens. Eastern Christianity predominated among Christians, with Russia's traditional Russian Orthodox Church being the Soviet Union's largest Christian denomination. About 90 percent of the Soviet Union's Muslims were Sunnis, with Shias concentrated in Azerbaijan. Smaller groups included Roman Catholics, Jews, Buddhists, and a variety of Protestant sects.
Religious influence had been strong in the Russian Empire. The Russian Orthodox Church enjoyed a privileged status as the church of the monarchy and took part in carrying out official state functions. The immediate period following the establishment of the Soviet state included a struggle against the Orthodox Church, which the revolutionaries considered an ally of the former ruling classes.
In Soviet law, the "freedom to hold religious services" was constitutionally guaranteed, although the ruling Communist Party regarded religion as incompatible with the Marxist spirit of scientific materialism. In practice, the Soviet system subscribed to a narrow interpretation of this right, and in fact utilized a range of official measures to discourage religion and curb the activities of religious groups.
The 1918 Council of People's Commissars decree establishing the Russian Soviet Federative Socialist Republic (RSFSR) as a secular state also decreed that "the teaching of religion in all where subjects of general instruction are taught, is forbidden. Citizens may teach and may be taught religion privately." Among further restrictions, those adopted in 1929, a half-decade into Stalin's rule, included express prohibitions on a range of church activities, including meetings for organized Bible study. Both Christian and non-Christian establishments were shut down by the thousands in the 1920s and 1930s. By 1940, as many as 90 percent of the churches, synagogues, and mosques that had been operating in 1917 were closed.
Convinced that religious anti-Sovietism had become a thing of the past with most Soviet Christians, and with the looming threat of war, the Stalin regime began shifting to a more moderate religion policy in the late 1930s. Soviet religious establishments overwhelmingly rallied to support the war effort during the Soviet war with Nazi Germany. Amid other accommodations to religious faith after Hitler attacked the Soviet Union, churches were reopened, Radio Moscow began broadcasting a religious hour, and a historic meeting between Stalin and Orthodox Church leader Patriarch Sergius of Moscow was held in 1943. Stalin had the support of the majority of the religious people in the Soviet Union even through the late 1980s. The general tendency of this period was an increase in religious activity among believers of all faiths.
The Soviet establishment under General Secretary Nikita Khrushchev's leadership clashed with the churches in 1958–1964, a period when atheism was emphasized in the educational curriculum, and numerous state publications promoted atheistic views. During this period, the number of churches fell from 20,000 to 10,000 from 1959 to 1965, and the number of synagogues dropped from 500 to 97. The number of working mosques also declined, falling from 1,500 to 500 within a decade.
Religious institutions remained monitored by the Soviet government, but churches, synagogues, temples, and mosques were all given more leeway in the Brezhnev era. Official relations between the Orthodox Church and the Soviet government again warmed to the point that the Brezhnev government twice honored Orthodox Patriarch Alexy I with the Order of the Red Banner of Labour. A poll conducted by Soviet authorities in 1982 recorded 20 percent of the Soviet population as "active religious believers."
Women.
Soviet efforts to expand social, political and economic opportunities for women constitute "the earliest and perhaps most far-reaching attempt ever undertaken to transform the status and role of women."
Culture.
The culture of the Soviet Union passed through several stages during the USSR's 69-year existence. During the first eleven years following the Revolution (1918–1929), there was relative freedom and artists experimented with several different styles to find a distinctive Soviet style of art. Lenin wanted art to be accessible to the Russian people. On the other hand, hundreds of intellectuals, writers, and artists were exiled or executed, and their work banned, for example Nikolay Gumilyov (shot for alleged conspiring against the Bolshevik regime) and Yevgeny Zamyatin (banned).
The government encouraged a variety of trends. In art and literature, numerous schools, some traditional and others radically experimental, proliferated. Communist writers Maxim Gorky and Vladimir Mayakovsky were active during this time. Film, as a means of influencing a largely illiterate society, received encouragement from the state; much of director Sergei Eisenstein's best work dates from this period.
Later, during Stalin's rule, Soviet culture was characterized by the rise and domination of the government-imposed style of socialist realism, with all other trends being severely repressed, with rare exceptions, for example Mikhail Bulgakov's works. Many writers were imprisoned and killed.
Following the Khrushchev Thaw of the late 1950s and early 1960s, censorship was diminished. During this time, a distinctive period of Soviet culture developed characterized by conformist public life and intense focus on personal life. Greater experimentation in art forms were again permissible, with the result that more sophisticated and subtly critical work began to be produced. The regime loosened its emphasis on socialist realism; thus, for instance, many protagonists of the novels of author Yury Trifonov concerned themselves with problems of daily life rather than with building socialism. An underground dissident literature, known as "samizdat", developed during this late period. In architecture the Khrushchev era mostly focused on functional design as opposed to the highly decorated style of Stalin's epoch.
In the second half of the 1980s, Gorbachev's policies of "perestroika" and "glasnost" significantly expanded freedom of expression in the media and press.

</doc>
<doc id="26781" url="https://en.wikipedia.org/wiki?curid=26781" title="Social science">
Social science

Social science is a major category of academic disciplines, concerned with society and the relationships among individuals within a society. It in turn has many branches, each of which is considered a "social science". The main social sciences include economics, law, political science, human geography, demography and sociology. In a wider sense, social science also includes some fields in the humanities such as anthropology, archaeology, jurisprudence, psychology, history, and linguistics. The term is also sometimes used to refer specifically to the field of sociology, the original 'science of society', established in the 19th century.
Positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining the quantitative and qualitative techniques). The term social research has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.
History.
The history of the social sciences begins in the Age of Enlightenment after 1650, which saw a revolution within natural philosophy, changing the basic framework by which individuals understood what was "scientific". Social sciences came forth from the moral philosophy of the time and were influenced by the Age of Revolutions, such as the Industrial Revolution and the French Revolution. The "social sciences" developed from the sciences (experimental and applied), or the systematic knowledge-bases or prescriptive practices, relating to the social improvement of a group of interacting entities.
The beginnings of the social sciences in the 18th century are reflected in the grand encyclopedia of Diderot, with articles from Rousseau and other pioneers. The growth of the social sciences is also reflected in other specialized encyclopedias. The modern period saw ""social science"" first used as a distinct conceptual field. Social science was influenced by positivism, focusing on knowledge based on actual positive sense experience and avoiding the negative; metaphysical speculation was avoided. Auguste Comte used the term ""science sociale"" to describe the field, taken from the ideas of Charles Fourier; Comte also referred to the field as "social physics".
Following this period, there were five paths of development that sprang forth in the social sciences, influenced by Comte on other fields. One route that was taken was the rise of social research. Large statistical surveys were undertaken in various parts of the United States and Europe. Another route undertaken was initiated by Émile Durkheim, studying "social facts", and Vilfredo Pareto, opening metatheoretical ideas and individual theories. A third means developed, arising from the methodological dichotomy present, in which social phenomena were identified with and understood; this was championed by figures such as Max Weber. The fourth route taken, based in economics, was developed and furthered economic knowledge as a hard science. The last path was the correlation of knowledge and social values; the antipositivism and verstehen sociology of Max Weber firmly demanded this distinction. In this route, theory (description) and prescription were non-overlapping formal discussions of a subject.
Around the start of the 20th century, Enlightenment philosophy was challenged in various quarters. After the use of classical theories since the end of the scientific revolution, various fields substituted mathematics studies for experimental studies and examining equations to build a theoretical structure. The development of social science subfields became very quantitative in methodology. The interdisciplinary and cross-disciplinary nature of scientific inquiry into human behavior, social and environmental factors affecting it, made many of the natural sciences interested in some aspects of social science methodology. Examples of boundary blurring include emerging disciplines like social research of medicine, sociobiology, neuropsychology, bioeconomics and the history and sociology of science. Increasingly, quantitative research and qualitative methods are being integrated in the study of human action and its implications and consequences. In the first half of the 20th century, statistics became a free-standing discipline of applied mathematics. Statistical methods were used confidently.
In the contemporary period, Karl Popper and Talcott Parsons influenced the furtherance of the social sciences. Researchers continue to search for a unified consensus on what methodology might have the power and refinement to connect a proposed "grand theory" with the various midrange theories that, with considerable success, continue to provide usable frameworks for massive, growing data banks; for more, see consilience. The social sciences will for the foreseeable future be composed of different zones in the research of, and sometime distinct in approach toward, the field.
The term "social science" may refer either to the specific "sciences of society" established by thinkers such as Comte, Durkheim, Marx, and Weber, or more generally to all disciplines outside of "noble science" and arts. By the late 19th century, the academic social sciences were constituted of five fields: jurisprudence and amendment of the law, education, health, economy and trade, and art.
Around the start of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.
Branches.
The social science disciplines are branches of knowledge taught and researched at the college or university level. Social science disciplines are defined and recognized by the academic journals in which research is published, and the learned social science societies and academic departments or faculties to which their practitioners belong. Social science fields of study usually have several sub-disciplines or branches, and the distinguishing lines between these are often both arbitrary and ambiguous.
Anthropology.
Anthropology is the holistic "science of man", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities, and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The natural "sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.
The anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word "anthropos" (άνθρωπος) is from the Greek for "human being" or "person." Eric Wolf described sociocultural anthropology as "the most scientific of the humanities, and the most humanistic of the sciences."
The goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called "primitive" in anthropological literature, but without any connotation of "inferior." Today, anthropologists use terms such as "less complex" societies or refer to specific modes of subsistence or production, such as "pastoralist" or "forager" or "horticulturalist" to refer to humans living in non-industrial, non-Western cultures, such people or folk ("ethnos") remaining of great interest within anthropology.
The quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.
Communication studies.
Communication studies deals with processes of human communication, commonly defined as the sharing of symbols to create meaning. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, and social dimensions of their contexts. Communication is institutionalized under many different names at different universities, including "communication", "communication studies", "speech communication", "rhetorical studies", "communication science", "media studies", "communication arts", "mass communication", "media ecology," and "communication and media science."
Communication studies integrates aspects of both social sciences and the humanities. As a social science, the discipline often overlaps with sociology, psychology, anthropology, biology, political science, economics, and public policy, among others. From a humanities perspective, communication is concerned with rhetoric and persuasion (traditional graduate programs in communication studies trace their history to the rhetoricians of Ancient Greece). The field applies to outside disciplines as well, including engineering, architecture, mathematics, and information science.
Economics.
Economics is a social science that seeks to analyze and describe the production, distribution, and consumption of wealth. The word "economics" is from the Greek ["oikos"], "family, household, estate," and νόμος ["nomos"], "custom, law," and hence means "household management" or "management of the state." An economist is a person using economic concepts and data in the course of employment, or someone who has earned a degree in the subject. The classic brief definition of economics, set out by Lionel Robbins in 1932, is "the science which studies human behavior as a relation between scarce means having alternative uses." Without scarcity and alternative uses, there is no economic problem. Briefer yet is "the study of how people seek to satisfy needs and wants" and "the study of the financial aspects of human behavior."
Economics has two broad branches: microeconomics, where the unit of analysis is the individual agent, such as a household or firm, and macroeconomics, where the unit of analysis is an economy as a whole. Another division of the subject distinguishes positive economics, which seeks to predict and explain economic phenomena, from normative economics, which orders choices and actions by some criterion; such orderings necessarily involve subjective value judgments. Since the early part of the 20th century, economics has focused largely on measurable quantities, employing both theoretical models and empirical analysis. Quantitative models, however, can be traced as far back as the physiocratic school. Economic reasoning has been increasingly applied in recent decades to other social situations such as politics, law, psychology, history, religion, marriage and family life, and other social interactions.
This paradigm crucially assumes (1) that resources are scarce because they are not sufficient to satisfy all wants, and (2) that "economic value" is willingness to pay as revealed for instance by market (arms' length) transactions. Rival heterodox schools of thought, such as institutional economics, green economics, Marxist economics, and economic sociology, make other grounding assumptions. For example, Marxist economics assumes that economics primarily deals with the investigation of exchange value, of which human labor is the source.
The expanding domain of economics in the social sciences has been described as economic imperialism.
Education.
Education encompasses teaching and learning specific skills, and also something less tangible but more profound: the imparting of knowledge, positive judgement and well-developed wisdom. Education has as one of its fundamental aspects the imparting of culture from generation to generation (see socialization). To educate means 'to draw out', from the Latin "educare", or to facilitate the realization of an individual's potential and talents. It is an application of pedagogy, a body of theoretical and applied research relating to teaching and learning and draws on many disciplines such as psychology, philosophy, computer science, linguistics, neuroscience, sociology and anthropology.
The education of an individual human begins at birth and continues throughout life. (Some believe that education begins even before birth, as evidenced by some parents' playing music or reading to the baby in the womb in the hope it will influence the child's development.) For some, the struggles and triumphs of daily life provide far more instruction than does formal schooling (thus Mark Twain's admonition to "never let school interfere with your education"). Family members may have a profound educational effect — often more profound than they realize — though family teaching may function very informally
Geography.
Geography as a discipline can be split broadly into two main sub fields: human geography and physical geography. The former focuses largely on the built environment and how space is created, viewed and managed by humans as well as the influence humans have on the space they occupy. This may involve cultural geography, transportation, health, military operations, and cities. The latter examines the natural environment and how the climate, vegetation and life, soil, oceans, water and landforms are produced and interact. Physical geography examines phenomena related to the measurement of earth. As a result of the two subfields using different approaches a third field has emerged, which is environmental geography. Environmental geography combines physical and human geography and looks at the interactions between the environment and humans. Other branches of geography include social geography, regional geography, and geomatics.
Geographers attempt to understand the earth in terms of physical and spatial relationships. The first geographers focused on the science of mapmaking and finding ways to precisely project the surface of the earth. In this sense, geography bridges some gaps between the natural sciences and social sciences. Historical geography is often taught in a college in a unified Department of Geography.
Modern geography is an all-encompassing discipline, closely related to GISc, that seeks to understand humanity and its natural environment. The fields of urban planning, regional science, and planetology are closely related to geography. Practitioners of geography use many technologies and methods to collect data such as GIS, remote sensing, aerial photography, statistics, and global positioning systems (GPS).
History.
History is the continuous, systematic narrative and research into past human events as interpreted through historiographical paradigms or theories.
History has a base in both the social sciences and the humanities. In the United States the National Endowment for the Humanities includes history in its definition of humanities (as it does for applied linguistics). However, the National Research Council classifies history as a social science. The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. The Social Science History Association, formed in 1976, brings together scholars from numerous disciplines interested in social history.
Law.
The social science of law, jurisprudence, in common parlance, means a rule that (unlike a rule of ethics) is capable of enforcement through institutions. However, many laws are based on norms accepted by a community and thus have an ethical foundation. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a "system of rules", as an "interpretive concept" to achieve justice, as an "authority" to mediate people's interests, and even as "the command of a sovereign, backed by the threat of a sanction". However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on the distribution of wealth. The noun "law" derives from the late Old English "lagu", meaning something laid down or fixed and the adjective "legal" comes from the Latin word "lex".
Linguistics.
Linguistics investigates the cognitive and social aspects of human language. The field is divided into areas that focus on aspects of the linguistic signal, such as syntax (the study of the rules that govern the structure of sentences), semantics (the study of meaning), morphology (the study of the structure of words), phonetics (the study of speech sounds) and phonology (the study of the abstract sound system of a particular language); however, work in areas like evolutionary linguistics (the study of the origins and evolution of language) and psycholinguistics (the study of psychological factors in human language) cut across these divisions.
The overwhelming majority of modern research in linguistics takes a predominantly synchronic perspective (focusing on language at a particular point in time), and a great deal of it—partly owing to the influence of Noam Chomsky—aims at formulating theories of the cognitive processing of language. However, language does not exist in a vacuum, or only in the brain, and approaches like contact linguistics, creole studies, discourse analysis, social interactional linguistics, and sociolinguistics explore language in its social context. Sociolinguistics often makes use of traditional quantitative analysis and statistics in investigating the frequency of features, while some disciplines, like contact linguistics, focus on qualitative analysis. While certain areas of linguistics can thus be understood as clearly falling within the social sciences, other areas, like acoustic phonetics and neurolinguistics, draw on the natural sciences. Linguistics draws only secondarily on the humanities, which played a rather greater role in linguistic inquiry in the 19th and early 20th centuries. Ferdinand Saussure is considered the father of modern linguistics.
Political science.
Political science is an academic and research discipline that deals with the theory and practice of politics and the description and analysis of political systems and political behavior. Fields and subfields of political science include political economy, political theory and philosophy, civics and comparative politics, theory of direct democracy, apolitical governance, participatory direct democracy, national systems, cross-national political analysis, political development, international relations, foreign policy, international law, politics, public administration, administrative behavior, public law, judicial behavior, and public policy. Political science also studies power in international relations and the theory of great powers and superpowers.
Political science is methodologically diverse, although recent years have witnessed an upsurge in the use of the scientific method, that is, the proliferation of formal-deductive model building and quantitative hypothesis testing. Approaches to the discipline include rational choice, classical political philosophy, interpretivism, structuralism, and behavioralism, realism, pluralism, and institutionalism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents, interviews, and official records, as well as secondary sources such as scholarly articles are used in building and testing theories. Empirical methods include survey research, statistical analysis or econometrics, case studies, experiments, and model building. Herbert Baxter Adams is credited with coining the phrase "political science" while teaching history at Johns Hopkins University.
Psychology.
Psychology is an academic and applied field involving the study of behavior and mental processes. Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental illness. The word "psychology" comes from the ancient Greek ψυχή, "psyche" ("soul", "mind") and "logy" ("study").
Psychology differs from anthropology, economics, political science, and sociology in seeking to capture explanatory generalizations about the mental function and overt behavior of individuals, while the other disciplines focus on creating descriptive generalizations about the functioning of social groups or situation-specific human behavior. In practice, however, there is quite a lot of cross-fertilization that takes place among the various fields. Psychology differs from biology and neuroscience in that it is primarily concerned with the interaction of mental processes and behavior, and of the overall processes of a system, and not simply the biological or neural processes themselves, though the subfield of neuropsychology combines the study of the actual neural processes with the study of the mental effects they have subjectively produced.
Many people associate psychology with clinical psychology, which focuses on assessment and treatment of problems in living and psychopathology. In reality, psychology has myriad specialties including social psychology, developmental psychology, cognitive psychology, educational psychology, industrial-organizational psychology, mathematical psychology, neuropsychology, and quantitative analysis of behavior.
Psychology is a very broad science that is rarely tackled as a whole, major block. Although some subfields encompass a natural science base and a social science application, others can be clearly distinguished as having little to do with the social sciences or having a lot to do with the social sciences. For example, biological psychology is considered a natural science with a social scientific application (as is clinical medicine), social and occupational psychology are, generally speaking, purely social sciences, whereas neuropsychology is a natural science that lacks application out of the scientific tradition entirely. In British universities, emphasis on what tenet of psychology a student has studied and/or concentrated is communicated through the degree conferred: B.Psy. indicates a balance between natural and social sciences, B.Sc. indicates a strong (or entire) scientific concentration, whereas a B.A. underlines a majority of social science credits. This is not always necessarily the case however, and in many UK institutions students studying the B.Psy, B.Sc, and B.A. follow the same curriculum as outlined by The British Psychological Society and have the same options of specialism open to them regardless of whether they choose a balance, a heavy science basis, or heavy social science basis to their degree. If they applied to read the B.A. for example, but specialised in heavily science-based modules, then they will still generally be awarded the B.A.
Sociology.
Sociology is the systematic study of society and human social action. The meaning of the word comes from the suffix "-ology", which means "study of", derived from Greek, and the stem "soci-", which is from the Latin word socius, meaning "companion", or society in general.
Sociology was originally established by Auguste Comte (1798–1857) in 1838. Comte endeavoured to unify history, psychology and economics through the descriptive understanding of the social realm. He proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in "The Course in Positive Philosophy" [1830–1842] and "A General View of Positivism" (1844). Though Comte is generally regarded as the "Father of Sociology", the discipline was formally established by another French thinker, Émile Durkheim (1858–1917), who developed positivism as a foundation to practical social research. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his "Rules of the Sociological Method". In 1896, he established the journal "L'Année Sociologique". Durkheim's seminal monograph, "Suicide" (1897), a case study of suicide rates among Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy.
Karl Marx rejected Comte's positivism but nevertheless aimed to establish a "science of society" based on historical materialism, becoming recognised as a founding figure of sociology posthumously as the term gained broader meaning. Around the start of the 20th century, the first wave of German sociologists, including Max Weber and Georg Simmel, developed sociological antipositivism. The field may be broadly recognised as an amalgam of three modes of social thought in particular: Durkheimian positivism and structural functionalism; Marxist historical materialism and conflict theory; and Weberian antipositivism and verstehen analysis. American sociology broadly arose on a separate trajectory, with little Marxist influence, an emphasis on rigorous experimental methodology, and a closer association with pragmatism and social psychology. In the 1920s, the Chicago school developed symbolic interactionism. Meanwhile, in the 1930s, the Frankfurt School pioneered the idea of critical theory, an interdisciplinary form of Marxist sociology drawing upon thinkers as diverse as Sigmund Freud and Friedrich Nietzsche. Critical theory would take on something of a life of its own after World War II, influencing literary criticism and the Birmingham School establishment of cultural studies.
Sociology evolved as an academic response to the challenges of modernity, such as industrialization, urbanization, secularization, and a perceived process of enveloping rationalization. Because sociology is such a broad discipline, it can be difficult to define, even for professional sociologists. The field generally concerns the social rules and processes that bind and separate people not only as individuals, but as members of associations, groups, communities and institutions, and includes the examination of the organization and development of human social life. The sociological field of interest ranges from the analysis of short contacts between anonymous individuals on the street to the study of global social processes. In the terms of sociologists Peter L. Berger and Thomas Luckmann, social scientists seek an understanding of the "Social Construction of Reality". Most sociologists work in one or more subfields. One useful way to describe the discipline is as a cluster of sub-fields that examine different dimensions of society. For example, social stratification studies inequality and class structure; demography studies changes in a population size or type; criminology examines criminal behavior and deviance; and political sociology studies the interaction between society and state.
Since its inception, sociological epistemologies, methods, and frames of enquiry, have significantly expanded and diverged. Sociologists use a diversity of research methods, drawing upon either empirical techniques or critical theory. Common modern methods include case studies, historical research, interviewing, participant observation, social network analysis, survey research, statistical analysis, and model building, among other approaches. Since the late 1970s, many sociologists have tried to make the discipline useful for non-academic purposes. The results of sociological research aid educators, lawmakers, administrators, developers, and others interested in resolving social problems and formulating public policy, through subdisciplinary areas such as evaluation research, methodological assessment, and public sociology.
New sociological sub-fields continue to appear — such as community studies, computational sociology, environmental sociology, network analysis, actor-network theory and a growing list, many of which are cross-disciplinary in nature.
Additional fields of study.
Additional applied or interdisciplinary fields related to the social sciences include:
Methodology.
Social research.
The origin of the survey can be traced back at least early as the Domesday Book in 1086, while some scholars pinpoint the origin of demography to 1663 with the publication of John Graunt's "Natural and Political Observations upon the Bills of Mortality". Social research began most intentionally, however, with the positivist philosophy of science in the 19th century.
In contemporary usage, "social research" is a relatively autonomous term, encompassing the work of practitioners from various disciplines that share in its aims and methods. Social scientists employ a range of methods in order to analyse a vast breadth of social phenomena; from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. The methods originally rooted in classical sociology and statistical mathematics have formed the basis for research in other disciplines, such as political science, media studies, and marketing and market research.
Social research methods may be divided into two broad schools:
Social scientists will commonly combine quantitative and qualitative approaches as part of a multi-strategy design. Questionnaires, field-based data collection, archival database information and laboratory-based data collections are some of the measurement techniques used. It is noted the importance of measurement and analysis, focusing on the (difficult to achieve) goal of objective research or statistical hypothesis testing. A mathematical model uses mathematical language to describe a system. The process of developing a mathematical model is termed 'mathematical modelling' (also modeling). Eykhoff (1974) defined a "mathematical model" as 'a representation of the essential aspects of an existing system (or a system to be constructed) that presents knowledge of that system in usable form'. Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models.
These and other types of models can overlap, with a given model involving a variety of abstract structures. The system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. The concept of an "integrated whole" can also be stated in terms of a system embodying a set of relationships that are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. A dynamical system modeled as a mathematical formalization has a fixed "rule" that describes the time dependence of a point's position in its ambient space. Small changes in the state of the system correspond to small changes in the numbers. The "evolution rule" of the dynamical system is a fixed rule that describes what future states follow from the current state. The rule is deterministic: for a given time interval only one future state follows from the current state.
Theory.
Other social scientists emphasize the subjective nature of research. These writers share social theory perspectives that include various types of the following:
Other fringe social scientists delve in alternative nature of research. These writers share social theory perspectives that include various types of the following:
Education and degrees.
Most universities offer degrees in social science fields. The Bachelor of Social Science is a degree targeted at the social sciences in particular. It is often more flexible and in-depth than other degrees that include social science subjects.
In the United States, a university may offer a student who studies a social sciences field a Bachelor of Arts degree, particularly if the field is within one of the traditional liberal arts such as history, or a BSc: Bachelor of Science degree such as those given by the London School of Economics, as the social sciences constitute one of the two main branches of science (the other being the natural sciences). In addition, some institutions have degrees for a particular social science, such as the Bachelor of Economics degree, though such specialized degrees are relatively rare in the United States.

</doc>
<doc id="26783" url="https://en.wikipedia.org/wiki?curid=26783" title="Statute">
Statute

A statute is a formal written enactment of a legislative authority that governs a state, city or country. Typically, statutes command or prohibit something, or declare policy. Statutes are laws made by legislative bodies and distinguished from common law, which is decided by courts, and regulations issued by government agencies. As a source of law, statutes are considered primary authority (as opposed to secondary authority). A statute begins as a bill proposed or sponsored by a legislator. If the bill survives the legislative committee process and isapproved by both houses of the legislature, the bill becomes law when it is signed by the executive officer (the president on the federal level or the governor on the state level).
Ideally all statutes must be in harmony with constitutional law or the fundamental law of the land.
This word is used in contradistinction to the common law. Statutes acquire their force from the time of their passage, unless otherwise provided. Statutes are of several kinds, namely: public or private, declaratory or remedial, and temporary or perpetual. A temporary statute is one which is limited in its duration at the time of its enactment. It continues in force until the time of its limitation has expired, unless sooner repealed. A perpetual statute is one for the continuance of which there is no limited time, although it may not be expressly declared to be so. If, however, a statute which did not itself contain any limitation is to be governed by another which is temporary only, the former will also be temporary and dependent upon the existence of the latter.
Before a statute becomes law in some countries, it must be agreed upon by the highest executive in the government. In virtually all countries, newly enacted statutes are published in some kind of journal, gazette, or chronological compilation, which is then distributed so that everyone can look up the statutory law. A universal problem encountered by lawmakers throughout human history is that such chronological publications have a habit of starting small and growing rapidly over time as new statutes are enacted in response to the exigencies of the moment. Eventually persons trying to find the law are forced to sort through an enormous number of statutes that were enacted at vastly different points in time to determine which portions are still in effect.
The solution to this adopted in many countries was to organize existing statutory law in topical arrangements (or "codified") within publications called codes, such as the United States Code, then ensure that new statutes are consistently drafted so that they add, amend, repeal or move various code sections. In turn, in theory, the code will thenceforth reflect the current cumulative state of the statutory law in that jurisdiction. In many nations statutory law is distinguished from and subordinate to constitutional law.
Alternative meanings.
International law.
The term statute is also used to refer to an International treaty that establishes an institution, such as the Statute of the European Central Bank, a protocol to the international courts as well, such as the Statute of the International Court of Justice and the Rome Statute of the International Criminal Court. Statute is also another word for law. The term was adapted from England in about the 18th century.
Autonomy Statute.
In the Autonomous Communities of Spain, the autonomy statute is a legal document similar to a state constitution in a federated state. The autonomies statutes in Spain have the rank of "Ley Organica", a category of special laws reserved only for the main institutions and issues and mentioned in the Constitution (the highest ranking legal instrument in Spain). Leyes Organicas rank between the Constitution and ordinary laws. The name was chosen, among others, to avoid confusion with the term Constitution (i.e. the Spanish Constitution of 1978).
Religious statutes.
Biblical terminology.
In biblical terminology, statute (Hebrew "chok") refers to a law given without any reason or justification. The classic example is the statute regarding the Red Heifer.
The opposite of a chok is a "mishpat", a law given for a specified reason, e.g. the Sabbath laws, which were given because "God created the world in six days, but on the seventh day He rested" (Genesis 2:2-3).
Dharma.
"That which upholds, supports or maintains the regulatory order of the universe" meaning the "Law" or "Natural Law". This is a concept of central importance in Indian philosophy and religion.

</doc>
<doc id="26784" url="https://en.wikipedia.org/wiki?curid=26784" title="Statutory law">
Statutory law

Statutory law or statute law is written law set down by a body of legislature or by a singular legislator (in the case of an absolute monarchy). This is as opposed to oral or customary law; or regulatory law promulgated by the executive or common law of the judiciary. 
Statutes may originate with national, state legislatures or local municipalities. 
Codified law.
The term codified law refers to statutes that have been organized ("codified") by subject matter; in this narrower sense, some but not all statutes are considered "codified." The entire body of codified statute is referred to as a "code," such as the United States Code, the Ohio Revised Code or the Code of Canon Law. The substantive provisions of the Act could be codified (arranged by subject matter) in one or more titles of the United States Code while the provisions of the law that have not reached their "effective date" (remaining uncodified) would be available by reference to the United States Statutes at Large. Another meaning of "codified law" is a statute that takes the common law in a certain area of the law and puts it in statute or code form.
Private law (particular law).
Another example of statutes that are not typically codified is a "private law" that may originate as a private bill, a law affecting only one person or a small group of persons. An example was divorce in Canada prior to the passage of the Divorce Act of 1968. It was possible to obtain a legislative divorce in Canada by application to the Canadian Senate, which reviewed and investigated petitions for divorce, which would then be voted upon by the Senate and subsequently made into law. In the United Kingdom Parliament, private bills were used in the nineteenth century to create corporations, grant monopolies and give individuals attention to be more fully considered by the parliament. The government may also seek to have a bill introduced "unofficially" by a backbencher so as not to create a public scandal; such bills may also be introduced by the loyal opposition — members of the opposition party or parties. Sometimes a private member's bill may also have private bill aspects, in such case the proposed legislation is called a hybrid bill.
In Canon Law, private law is called "particular law."

</doc>
<doc id="26785" url="https://en.wikipedia.org/wiki?curid=26785" title="Sanction">
Sanction

A sanction may be either a permission or a restriction, depending on context, as the word is an auto-antonym.
Examples of sanctions include:
Involving countries:
In other uses:

</doc>
<doc id="26786" url="https://en.wikipedia.org/wiki?curid=26786" title="Sarajevo">
Sarajevo

Sarajevo (Cyrillic: Сарајево, ; ) is the capital and largest city of Bosnia and Herzegovina, with an estimated population of 369,534. The Sarajevo metropolitan area, including Sarajevo, East Sarajevo and surrounding municipalities, is home to 688,354 inhabitants. Moreover, it is also the capital of the Federation of Bosnia and Herzegovina entity, and the center of the Sarajevo Canton. Nestled within the greater Sarajevo valley of Bosnia, it is surrounded by the Dinaric Alps and situated along the Miljacka River in the heart of Southeastern Europe and the Balkans.
Sarajevo is the leading political, social and cultural center of Bosnia and Herzegovina, a prominent center of culture in the Balkans, with its region-wide influence in entertainment, media, fashion, and the arts.
Due to its long and rich history of religious and cultural variety, Sarajevo is sometimes called the "Jerusalem of Europe" or "Jerusalem of the Balkans". It was, until late in the 20th century, the only major European city to have a mosque, Catholic church, Orthodox church and synagogue within the same neighborhood. A regional center in education, the city is also home to the Balkans' first institution of tertiary education in the form of an Islamic polytechnic called the Saraybosna Osmanlı Medrese, today part of the University of Sarajevo.
Although settlement in the area stretches back to prehistoric times, the modern city arose as an Ottoman stronghold in the 15th century. Sarajevo has attracted international attention several times throughout its history. In 1885, Sarajevo was the first city in Europe and the second city in the world to have a full-time electric tram network running through the city, following San Francisco. In 1914, it was the site of the assassination of the Archduke of Austria that sparked World War I. In WWII, Sarajevo was part of Ustashe's Independent State of Croatia, where many Serbs and Jews were killed. Forty years later, it hosted the 1984 Winter Olympics. For nearly four years, from 1992 to 1996, the city suffered the longest siege of a city in the history of modern warfare (1,425 days long) during the Bosnian War.
Sarajevo has been undergoing post-war reconstruction, and is the fastest growing city in Bosnia and Herzegovina. The travel guide series, "Lonely Planet", has named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010. In 2011, Sarajevo was nominated to be the European Capital of Culture in 2014 and will be hosting the European Youth Olympic Festival in 2019.
Sarajevo is also a metropolis due to being the most important and influential city in the whole country.
Etymology.
The earliest known name for the large central Bosnian region of today's Sarajevo is Vrhbosna.
Sarajevo is a slavicized word based on "saray", the Turkish word for "palace" (the letter J is the equivalent in the Bosnian language of the English Y). The "evo" portion may come from the term "saray ovası" first recorded in 1455, meaning "the plains around the palace" or simply "palace plains".
However, in his Dictionary of Turkish loanwords, Abdulah Škaljić maintains that the ""evo"" ending is more likely to have come from the widespread Slavic suffix ""evo"" used to indicate place names, than from the Turkish ending ""ov"a", as proposed by some. The first mention of name Sarajevo was in 1507 letter written by Feriz Beg.
Sarajevo has had many nicknames. The earliest is "Šeher", which is the term Isa-Beg Ishaković used to describe the town he was going to build. It is a Turkish word meaning an advanced city of key importance ("şehir") which in turn comes from "shahr" (city). As Sarajevo developed, numerous nicknames came from comparisons to other cities in the Islamic world, i.e. "Damascus of the North". The most popular of these was "European Jerusalem".
Some argue that a more correct translation of "saray" is government office or house. "Saray" is a common word in Turkish for a "palace" or "mansion" (from Persian word "sarāy", means "house, palace").
Geography.
Sarajevo is located near the geometric center of the triangular-shaped Bosnia-Herzegovina and within the historical region of Bosnia proper. It is situated above sea level and lies in the Sarajevo valley, in the middle of the Dinaric Alps. The valley itself once formed a vast expanse of greenery, but gave way to urban expansion and development in the post-World War II era. The city is surrounded by heavily forested hills and five major mountains. The highest of the surrounding peaks is Treskavica at , then Bjelašnica mountain at , Jahorina at , Trebević at , with Igman being the shortest. The last four are also known as the Olympic Mountains of Sarajevo (see also 1984 Winter Olympics). The city itself has its fair share of hilly terrain, as evidenced by the many steeply inclined streets and residences seemingly perched on the hillsides.
The Miljacka river is one of the city's chief geographic features. It flows through the city from east through the center of Sarajevo to west part of city where eventually meets up with the Bosna river. Miljacka river is "The Sarajevo River", with its source ("Vrelo Miljacke") south of the town of Pale at the foothills of Mount Jahorina, several kilometers to the east of Sarajevo center. The Bosna's source, Vrelo Bosne near Ilidža (west Sarajevo), is another notable natural landmark and a popular destination for Sarajevans and other tourists. Several smaller rivers and streams such as Koševski Potok also run through the city and its vicinity.
Cityscape.
Sarajevo is located close to the center of the triangular shape of Bosnia and Herzegovina in southeastern Europe. Sarajevo city proper consists of four municipalities (or "in Bosnian and Serbian: opština, in Croatian: općina"): Centar (Center), Novi Grad (New City), Novo Sarajevo (New Sarajevo), and Stari Grad (Old City), while Metropolitan area of Sarajevo (Greater Sarajevo area) includes these and the neighbouring municipalities of Ilidža, Hadžići and Vogošća "(before the war and new (Deyton) administrative division, Metro of Sarajevo consisted also, beside above mentioned, three municipalities today's divided between Federacija Bosne i Hercegovine and Republika Srpska – Trnovo, Federacija Bosne i Hercegovine / Trnovo, Republika Srpska, Lukavica and Pale)". The city has an urban area of . Veliki Park (Great park) is the largest green area in the center of Sarajevo. It’s nestled between Titova, Koševo, Džidžikovac, Tina Ujevića and Trampina Streets and in the lower part there is a monument dedicated to the Children of Sarajevo.
Climate.
Sarajevo has a humid continental climate. Sarajevo's climate exhibits influences of oceanic zones, with four seasons and uniformly spread precipitation. The proximity of the Adriatic Sea moderates Sarajevo's climate somewhat, although the mountains to the south of the city greatly reduce this maritime influence. The average yearly temperature is , with January ( avg.) being the coldest month of the year and July ( avg.) the warmest.
The highest recorded temperature was on 19 August 1946, and on 23 August 2008 (41.0) while the lowest recorded temperature was on 25 January 1942. On average, Sarajevo has 6 days where the temperature exceeds and 4 days where the temperature drops below per year. The city typically experiences mildly cloudy skies, with an average yearly cloud cover of 45%.
The cloudiest month is December (75% average cloud cover) while the clearest is August (37%). Moderate precipitation occurs fairly consistently throughout the year, with an average 75 days of rainfall. Suitable climatic conditions have allowed winter sports to flourish in the region, as exemplified by the Winter Olympics in 1984 that were celebrated in Sarajevo. Average winds are and the city has 1,769 hours of sunshine.
History.
Ancient times and Middle Ages.
One of the earliest findings of settlement in the Sarajevo area is that of the Neolithic Butmir culture. The discoveries at Butmir were made on the grounds of the modern-day Sarajevo suburb Ilidža in 1893 by Austro-Hungarian authorities during the construction of an agricultural school. The area's richness in flint was no doubt attractive to Neolithic man, and the settlement appears to have flourished. The settlement developed unique ceramics and pottery designs, which characterize the Butmir people as a unique culture. This was largely responsible for the International congress of archaeologists and anthropologists meeting in Sarajevo in 1894.
The next prominent culture in Sarajevo were the Illyrians. The ancient people, who considered most of the West Balkans as their homeland, had several key settlements in the region, mostly around the river Miljacka and Sarajevo valley. The Illyrians in the Sarajevo region belonged to the "Daesitiates", a war-like people who were probably the last Illyrian people in Bosnia and Herzegovina to resist Roman occupation. Their defeat by the Roman emperor Tiberius in 9 A.D. marks the start of Roman rule in the region. The Romans never built up the region of modern-day Bosnia very much, but the Roman colony of Aquae Sulphurae was located near the top of present-day Ilidža, and was the most important settlement of the time. After the Romans, the Goths settled the area, followed by the Slavs in the 7th century.
Middle Ages
During the Middle Ages Sarajevo was part of the Bosnian province of Vrhbosna near the traditional center of the Kingdom of Bosnia. Though a city called "Vrhbosna" existed, the exact settlement of Sarajevo at this time is debated. Various documents of the high Middle Ages note a place called "Tornik" in the region. By all indications, Tornik was a very small marketplace surrounded by a proportionally small village, and was not considered very important by Ragusan merchants.
Other scholars say that "Vrhbosna" was a major city located at the site of modern-day Sarajevo. Today, that place still exists, but its name for small part of Sarajevo, at the north-east. Papal documents say that in 1238, a cathedral dedicated to Saint Paul was built in the city. Disciples of the notable saints Cyril and Methodius stopped by the region, founding a church at "Vrelobosna". Whether or not the city was located at modern-day Sarajevo, the documents attest to its and the region's importance. Vrhbosna was a Slavic citadel from 1263 until it was occupied by the Ottoman Empire in 1429.
Ottoman era.
Sarajevo was founded by the Ottoman Empire in the 1450s upon its conquest of the region, with 1461 used as the city's founding date. The first Ottoman governor of Bosnia, Isa-Beg Ishaković, transformed the cluster of villages into a city and state capitol by building a number of key structures, including a mosque, a closed marketplace, a public bath, a hostel, and of course the governor's castle ("Saray") which gave the city its present name. The mosque was named "Careva Džamija" (the Tsar's Mosque) in honor of the Sultan Mehmed II. With the improvements Sarajevo quickly grew into the largest city in the region. Many Christians converted to Islam at this time. To accommodate the new pilgrims on the road to Mecca, in 1541 Gazi Husrev-Bey’s quartermaster Vekil-Harrach built a Pilgrim’s mosque for which it is still known to this day Hadžijska mosque. By the 15th Century the settlement was established as a city, named "Bosna-Saraj", around the citadel in 1461. The name Sarajevo is derived from Turkish "saray ovası", meaning "the field around saray".
Under leaders such as the second governor Gazi Husrev-beg, Sarajevo grew at a rapid rate. Husrev-beg greatly shaped the physical city, as most of what is now the Old Town was built during his reign. Sarajevo became known for its large marketplace and numerous mosques, which by the middle of the 16th century numbered more than 100. At the peak of the empire, Sarajevo was the biggest and most important Ottoman city in the Balkans after Istanbul. By 1660, the population of Sarajevo was estimated to be over 80,000. By contrast, Belgrade in 1838 had 12,963 inhabitants, and Zagreb as late as 1851 had 14,000 people. As political conditions changed, Sarajevo became the site of warfare.
In 1697, during the Great Turkish War, a raid was led by Prince Eugene of Savoy of the Habsburg Monarchy against the Ottoman Empire, which conquered Sarajevo and left it plague-infected and burned to the ground. 
After his men had looted thoroughly, they set the city on fire and destroyed nearly all of it in one day. Only a handful of neighborhoods, some mosques, and an Orthodox church, were left standing.
Numerous other fires weakened the city, as well. The city was later rebuilt, but never fully recovered from the destruction. By 1807, it had only some 60,000 residents.
In the 1830s, several battles of the Bosnian uprising had taken place around the city. These had been led by Husein Gradaščević. Today, a major city street is named "Zmaj od Bosne" (Dragon of Bosnia) in his honor. The rebellion failed and, for several more decades, the crumbling Ottoman state remained in control of Bosnia.
The Ottoman Empire made Sarajevo an important administrative centre by 1850. Baščaršija was built becoming an old bazaar and a historical and cultural center of the city in the 15th century when Isa-Beg Isaković founded the town . The word Baščaršija derives from the Turkish language.
Austria-Hungary.
Austria-Hungary's occupation of Bosnia and Herzegovina came in 1878 as part of the Treaty of Berlin, and complete annexation followed in 1908, angering the Serbs. Sarajevo was industrialized by Austria-Hungary, who used the city as a testing area for new inventions, such as tramways, established in 1885, before installing them in Vienna. Architects and engineers wanting to help rebuild Sarajevo as a modern European capital rushed to the city. A fire that burned down a large part of the central city area ("čaršija") left more room for redevelopment. The city has a unique blend of the remaining Ottoman city market and contemporary western architecture. Sarajevo has some examples of Secession- and Pseudo-Moorish styles that date from this period.
The Austro-Hungarian period was one of great development for the city, as the Western power brought its new acquisition up to the standards of the Victorian age. Various factories and other buildings were built at this time, and a large number of institutions were both Westernized and modernized. For the first time in history, Sarajevo's population began writing in Latin script.
For the first time in centuries, the city significantly expanded outside its traditional borders. Much of the city's contemporary central municipality (Centar) was constructed during this period.
Architecture in Sarajevo quickly developed into a wide range of styles and buildings. The Cathedral of Sacred Heart, for example, was constructed using elements of neo-gothic and Romanesque architecture. The National Museum, Sarajevo brewery, and City Hall were also constructed during this period. Additionally, Austrian officials made Sarajevo the first city in this part of Europe to have a tramway.
Although the Bosnia Vilayet "de jure" remained part of the Ottoman Empire, it was "de facto" governed as an integral part of Austria-Hungary with the Ottomans having no say in its day-to-day governance. This lasted until 1908 when the territory was formally annexed and turned into a condominium, jointly controlled by both Austrian Cisleithania and Hungarian Transleithania.
In the event that triggered World War I, Archduke Franz Ferdinand of Austria was assassinated, along with his wife Sophie, Duchess of Hohenberg in Sarajevo on 28 June 1914 by a self-declared Yugoslav, Gavrilo Princip, a member of Young Bosnia. In response, many residents of Sarajevo organized riots against the Serbs, killing two and destroying their properties. In the ensuing war, however, most of the Balkan offensives occurred near Belgrade, and Sarajevo largely escaped damage and destruction.
Following the war, after the Balkans were unified under the Kingdom of Yugoslavia, Sarajevo became the capital of Drina Province.
Yugoslavia.
After World War I and contributions from the Serbian army alongside rebelling Slavic nations in Austria-Hungary, Sarajevo became part of the Kingdom of Yugoslavia. Though it held some political importance, as the center of first the Bosnian region and then the Drinska Banovina, it was not treated with the same attention or considered as significant as it was in the past. Outside of today's national bank of Bosnia and Herzegovina, virtually no significant contributions to the city were made during this period.
During World War II the Kingdom of Yugoslavia's army was overrun by superior German and Italian forces. Following a German bombing campaign, Sarajevo was captured on 15 April 1941 by the 16th Motorized infantry Division. The Axis powers created the Independent State of Croatia and included Sarajevo in its territory. On 12 October, a group of 108 notable Bosniak citizens of Sarajevo signed the Resolution of Sarajevo Muslims by which they condemned the persecution of Serbs organized by the Ustaše, made a distinction between the Bosniaks who participated in such persecutions and the rest of the Bosniak population, presented information about the persecutions of Bosniaks by Serbs, and requested security for all citizens of the country, regardless of their identity. By mid-summer 1942, around 20,000 Serbs found refuge in Sarajevo from Ustaše terror.
The city was bombed by the Allies from 1943 to 1944. The Yugoslav Partisan movement was represented in the city. Resistance was led by a NLA Partisan named Vladimir "Walter" Perić. He died while leading the final liberation of the city on 6 April 1945. Many of the WWII shell casings that were used during the attacks have been carved and polished in Sarajevo tradition and are sold as art.
Following the liberation, Sarajevo was the capital of the Socialist Republic of Bosnia and Herzegovina within the Socialist Federal Republic of Yugoslavia. The Republic Government invested heavily in Sarajevo, building many new residential blocks in Novi Grad Municipality and Novo Sarajevo Municipality, while simultaneously developing the city's industry and transforming Sarajevo into one of the modern cities, in SFRYugoslavia and SR Bosnia. From a post-war population of 115,000, by the end of Yugoslavia, Sarajevo had 600,000 people. Sarajevo grew rapidly as it became an important regional industrial center in Yugoslavia. The Vraca Memorial Park, a monument for victims of World War II, was dedicated on 25 November, the "Day of Statehood of Bosnia and Herzegovina" when the ZAVNOBIH held their first meeting in 1943.
The crowning moment of Sarajevo's time in Socialist Yugoslavia was the 1984 Winter Olympics. Sarajevo beat Sapporo, Japan; and Falun/Göteborg, Sweden for the privilege of hosting the games. They were followed by an immense boom in tourism, making the 1980s one of the city's best decades in a long time.
Siege of Sarajevo during Bosnian War.
The Bosnian War for independence resulted in large-scale destruction and dramatic population shifts during the Siege of Sarajevo between 1992 and 1995. Thousands of Sarajevans lost their lives under the constant bombardment and sniper shooting at civilians by the Serb forces during the siege. It is the longest siege of a capital city in the history of modern warfare. Serb forces of the Republika Srpska and the Yugoslav People's Army besieged Sarajevo, the largest city of Bosnia and Herzegovina, from 5 April 1992 to 29 February 1996 during the Bosnian War.
When Bosnia and Herzegovina declared independence from Yugoslavia and achieved United Nations recognition, the Serbian leaders and army whose goal was to create a "greater Serbia", declared a new Serbian national state Republika Srpska (RS) which was carved from the territory of Bosnia and Herzegovina, encircled Sarajevo with a siege force of 18,000 stationed in the surrounding hills, from which they assaulted the city with weapons that included artillery, mortars, tanks, anti-aircraft guns, heavy machine-guns, multiple rocket launchers, rocket-launched aircraft bombs, and sniper rifles. From 2 May 1992, the Serbs blockaded the city. The Bosnian government defence forces inside the besieged city were poorly equipped and unable to break the siege.
During the siege, 11,541 people lost their lives, including over 1,500 children. An additional 56,000 people were wounded, including nearly 15,000 children. The 1991 census indicates that before the siege the city and its surrounding areas had a population of 525,980.
When the siege ended, the concrete scars caused by mortar shell explosions left a mark that was filled with red resin. After the red resin was placed, it left a floral pattern which led to it being dubbed a Sarajevo Rose.
Present.
Various new modern buildings have been built, most significantly the Bosmal City Center, BBI Centar, Sarajevo City Center and the Avaz Twist Tower, which is the tallest skyscraper in the Balkans. A new highway was completed in the late 2000s between Sarajevo and the city of Kakanj. Due to growth in population, tourism and airport traffic the service sector in the city is developing fast and welcoming new investors from various businesses.
The business enclave Sarajevo City Center is one of the largest and most modern shopping and business centers in the region. It was completed in early 2014. Airport Center Sarajevo which will be connected directly to the new airport terminal will offer a great variety of brands, products and services.
If current growth trends continue, the Sarajevo metropolitan area should return to its pre-war population by 2020, with the city following soon after. At its current pace, Sarajevo won’t surpass the million resident mark until the second half of the 21st century. The most widely accepted and pursued goal was for the city to hold the Winter Olympics in 2014; that bid failed, so they will try again perhaps in 2022 or 2026.
Most recently, in 2014 the city saw anti-government protests and riots and record rainfall that caused historic flooding.
Trebević ropeway (cable car) transportation system has been announced to be rebuilt following the use of the same during 1984 Winter Olympic Games. Trebević cable car was one of Sarajevo’s key landmarks. The cost involved will be 12,109,000 euro and it is planned to be competed by late 2016. Cable cars and equipment have been donated by the Graechen ski centre in Wallis Canton, Switzerland. The selected cable cars are ideally suited to the project and meet the highest quality standards. The new Trebević cable car will have 6 sitting cabins and between 11 and 13 pillars, with a capacity to transport 1,200 passengers an hour. Further monetary donations (approx 3,000,000 euro) have been made by Dutch national Edmond Offermann.
Administration.
Largest city of Bosnia and Herzegovina.
Sarajevo is the capital of the country of Bosnia and Herzegovina and its sub-entity, the Federation of Bosnia and Herzegovina, as well as of the Sarajevo Canton. It is also the "de jure" capital of another entity, Republika Srpska. Each of these levels of government has their parliament or council, as well as judicial courts, in the city. In addition many foreign embassies are located in Sarajevo.
Sarajevo is home to the Council of Ministers of Bosnia and Herzegovina, Parliamentary Assembly of Bosnia and Herzegovina, Presidency of Bosnia and Herzegovina, the Constitutional Court of Bosnia and Herzegovina and the operational command of the Armed Forces of Bosnia and Herzegovina.
Bosnia and Herzegovina's Parliament office in Sarajevo was damaged heavily in the Bosnian War. Due to damage the staff and documents were moved to a nearby ground level office to resume the work. In late 2006 reconstruction work started on the Parliament and was finished in 2007. The cost of reconstruction is supported 80% by the Greek Government
through the Hellenic Program of Balkans Reconstruction (ESOAV) and 20% by Bosnia-Herzegovina.
Municipalities and city government.
The city comprises four municipalities Centar, Novi Grad, Novo Sarajevo, and Stari Grad. Each operate their own municipal government, united they form one city government with its own constitution. The executive branch () consists of a mayor, with two deputies and a cabinet. The legislative branch consists of the City Council, or "Gradsko Vijeće". The council has 28 members, including a council speaker, two deputies, and a secretary. Councilors are elected by the municipality in numbers roughly proportional to their population. The city government also has a judicial branch based on the post-transitional judicial system as outlined by the High Representative's "High Judicial and Prosecutorial Councils".
Sarajevo's Municipalities are further split into "local communities" (Bosnian, "Mjesne zajednice"). Local communities have a small role in city government and are intended as a way for ordinary citizens to get involved in city government. They are based on key neighborhoods in the city.
Metropolis.
Due to being political, economic, cultural, social, university, scientific infrastructure and cultural center of Bosnia and Herzegovina, Sarajevo is with over 688,354 inhabitants on its metropolitan area the only metropolis of the country. 
Not only the importance, size and opulence but also the panorama of Sarajevo, its skyscrapers like Avaz Twist Tower, Sarajevo City Center, Bosmal City Center, or Sarajevo Tower (which is under construction), and worldwide famous festivals like Sarajevo Film Festival give Sarajevo the flair of a metropolis.
With possible construction of in 2015 announced urban district Nova Ilidža (€2.2 billion), Tourist district Trnovo (€2.4 billion), other neighbourhoods like Sarajevo Waves and buildings which are under construction (only in 2014 over 700 million KM were invested in real estates),
Also, the proposed tram network extension to neighborhoods Hrasnica and Dobrinja as well as metro from Bascarsija to Otoka (1. phase) will improve the city´s infrastructure and attract new investments.
Between 2015 and 2017 Sarajevo will reconstruct the whole tram line from Marijin Dvor to Ilidža with estimated costs of 25 million KM. Due to this, Sarajevo will become the only city in the Balkans with completely reconstructed tram network. In 2015 the city will also get fully renovated trams from Konya and new trolleybuses from Geneva.
Also, the direct connection from Stup Interchange to the Sarajevo Bypass and thus to A1 (Bosnia and Herzegovina) will be finished by the end of 2015. Therefore, Sarajevo will have three direct connections to the motorway. Sarajevo Beltway was already finished in 2005.
Economy.
Sarajevo is Bosnia and Herzegovina's economic focal point, with Sarajevo Canton (which doesn't encompass the whole urban territory) generating almost 25% of the country's GDP. After the years of war, Sarajevo's economy was subject to reconstruction and rehabilitation programs. Amongst economic landmarks, the Central Bank of Bosnia and Herzegovina opened in Sarajevo in 1997 and the Sarajevo Stock Exchange began trading in 2002. The city's large manufacturing, administration, tourism sector, combined with a large informal market, makes it the strongest economic regions of Bosnia and Herzegovina.
While Sarajevo had a large industrial base during its communist period, only a few pre-existing businesses have successfully adapted to the market economy. Sarajevo industries now include tobacco products, furniture, hosiery, automobiles, and communication equipment. Companies based in Sarajevo include B&H Airlines, BH Telecom, Bosnalijek, Energopetrol, Sarajevo Tobacco Factory, and Sarajevska Pivara (Sarajevo Brewery).
In 2002 the total export for the greater Sarajevo region was worth about 259,569,000KM. This was an increase of 21.9% from the previous year. Most of Sarajevo's exports (28.2%) head to Germany, with Great Britain following behind at 16.8% and Serbia and Montenegro thirds with 12.8%. The largest amount of imported goods come from Germany, at 15.8%. With a worth of total import at about 1,322,585,000KM, the total import is almost 5.1 times the total export.
Foreign companies with a foothold in the Sarajevo region include Harris Communications, Brown & Root, and, most notably, Coca Cola. The Bosnian-Malaysian firm Bosmal is also situated in the city. Their main exports are clothing, electrical goods.
In 1981 Sarajevo's GDP per capita was 133% of the Yugoslav average.
Tourism and recreation.
Sarajevo has a wide tourist industry and a fast expanding service sector thanks to the strong annual growth in tourist arrivals. Sarajevo also benefits from being both a summer and winter destination with continuity in its tourism throughout the year. The travel guide series, "Lonely Planet", has named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010.
In 2013 302.570 tourists visited Sarajevo, up 17.9% compared to 2012, giving 595.637 overnight stays, which is 18% more than in 2012.
Sports-related tourism uses the legacy facilities of the 1984 Winter Olympics, especially the skiing facilities on the nearby mountains of Bjelašnica, Igman, Jahorina, Trebević, and Treskavica. Sarajevo's 600 years of history, influenced by both Western and Eastern empires, makes it a tourist attraction with splendid variations. 
Sarajevo has hosted travellers for centuries, because it was an important trading center during the Ottoman and Austria-Hungarian empires. Examples of popular destinations in Sarajevo include the Vrelo Bosne park, the Sarajevo cathedral, and the Gazi Husrev-beg's Mosque. Tourism in Sarajevo is chiefly focused on historical, religious, cultural aspects and winter sports.
Gross pay in Sarajevo in February 2015 was 1,578 km or 790 €, while net salary was 1,020 km or 521 €.
Sarajevo is after Ljubljana and Zagreb the richest city in former Yugoslavia and one of the richest cities in the Balkans.
Sarajevo is host to many parks throughout the city and on the outskirts of city. A popular activity among Sarajevo citizens is street chess, usually played at Trg oslobođenja Alija Izetbegović. Veliki Park is the largest green area in the center of Sarajevo. It’s nestled between Titova, Koševo, Džidžikovac, Tina Ujevića and Trampina Streets and in the lower part there is a monument dedicated to the Children of Sarajevo. Hastahana skate park is a popular place to relax in the Austro-Hungarian neighborhood of Marijin Dvor. Goat's Bridge, locally known as "Kozija Ćuprija", in the Miljacka Canyon is also a popular park destination along the Dariva walkway and river Miljacka.
Sarajevo is also famous for its beautiful city lookouts; best of which is an observation deck on Avaz Twist Tower, Park Prinčeva restaurant, Vidikovac lookout (Mt. Trebević), Zmajevac lookout and Yellow/White fortresses lookouts (in Vratnik) as well as numerous other rooftops throughout the city (i.e. Alta Shopping Center, BBI Center, Hotel Hecco Deluxe).
Demographics.
The last official census in Bosnia and Herzegovina took place 1991 and recorded 527,049 people living in city of Sarajevo (ten municipalities). In the settlement of Sarajevo proper, there were 416,497 inhabitants. The war displaced hundreds of thousands of people, a large majority of whom have not returned.
Today, Sarajevo's population is not known clearly and is based on estimates contributed by the United Nations Statistics Division and the Federal Office of Statistics of the Federation of Bosnia and Herzegovina, among other national and international non-profit organizations. , the population of the city's four municipalities is estimated to be 411,161, whereas the Sarajevo Canton population is estimated at 578,757. With an area of , Sarajevo has a population density of about . The Novo Sarajevo municipality is the most densely populated part of Sarajevo with about , while the least densely populated is the Stari Grad, with .
The war changed the ethnic and religious profile of the city. It had long been a multicultural city, and often went by the nickname of "Europe's Jerusalem". At the time of the 1991 census, 49.2 per cent of the city's population of 527,049 were Bosniaks, 29.8 percent Serbs, 10.7 percent Yugoslavs, 6.6 percent Croats and 3.6 percent other ethnicities (Jews, Romas, etc.). By 2002, 79.6 per cent of the canton's population of 401,118 were Bosniak, 11.2 percent Serb, 6.7 percent Croat and 2.5 percent others (Jews, Romas, etc.). Since its formalisation following the Dayton Agreement, the Bosniak category has been endorsed by the Federation of Bosnia and Herzegovina as the ethnic or national designation of choice to replace the former "Muslim by nationality" employed by the Yugoslav government. According to academic Fran Markowitz, it is not clear, however, whether the shift in identification from Muslim to Bosniak by such large numbers of Sarajevo's residents can be primarily attributed to a successful endorsement by the state among Muslims (and maybe also those who previously identified as Yugoslavs or "Ostali" (others)), or if its citizens have made that switch intrinsically. Her analysis of marriage registration data shows that 67 per cent of people marrying in 2003 identified as Muslim or Bosniak, which is significantly lower than the 79.6 per cent census figure (unlike the census, where people respond to an interviewer, applicants to the marriage registry fill in the form themselves). To explain this discrepancy, Markowitz points to a number of "administrative apparatuses and public pressures that push people who might prefer to identify as flexible, multiply constituted hybrids or with one of the now unnamed minority groups into one of the three Bosniac-Croat-Serb constituent nations". These include respondents being encouraged by census interviewers to identity as belonging to one of the three constituent peoples.
Many Serbs left urban areas including Sarajevo during the conflict, but the falling number of Serbs is also partly due to the redrawing of municipal boundaries as part of the Dayton Agreement.
According to Census 2013, Sarajevo has 291,422 inhabitants while 438,443 live in Sarajevo Canton. Sarajevo Metropolitan Area is home to 608,354. In comparison to Census 1991, the population decreased by 63,745 inhabitants.
Transportation.
Roads and highways.
Sarajevo's location in a valley between mountains makes it a compact city. Narrow city streets and a lack of parking areas restrict automobile traffic but allow better pedestrian and cyclist mobility. The two main roads are Titova Ulica (Street of Marshal Tito) and the east-west Zmaj od Bosne (Dragon of Bosnia) highway (E761).
Sarajevo is Bosnia's main intersection and the most passable city in Bosnia and Herzegovina and the third in region. The city is connected to all the other major cities by highway or national road like Zenica, Banja Luka, Tuzla, Mostar, Goražde and Foča.
Tourists from Central Europe and elsewhere visiting Dalmatia driving via Budapest through Sarajevo also contribute to the traffic congestion in and around Sarajevo.
The trans-European highway, Corridor 5C, runs through Sarajevo connecting it to Budapest in the north, and Ploče at the Adriatic sea in the south. The highway is built by the government and should cost 3.5 billion Euro. Up until March 2012, the Federation of Bosnia and Herzegovina invested around 600 million Euro in the A1. In 2014 the sections Sarajevo-Zenica and Sarajevo-Tarcin were completed including the Sarajevo Beltway ring road.
Tram, bus and trolleybus.
Sarajevo's electric tramways, in operation since 1885, are the oldest form of public transportation in the city. 
Sarajevo had the first full-time (dawn to dusk) tram line in Europe, and the second in the world. Opened on New Year's Day in 1885, it was the testing line for the tram in Vienna and the Austro-Hungarian Empire, and operated by horses. Originally built to , the present system in 1960 was upgraded to . The trams played a pivotal role in the growth of the city in the 20th century.
There are seven tramway lines supplemented by five trolleybus lines and numerous bus routes. The main railroad station in Sarajevo is located in the north-central area of the city. From there, the tracks head west before branching off in different directions, including to industrial zones in the city. Sarajevo is currently undergoing a major infrastructure renewal; many highways and streets are being repaved, the tram system is undergoing modernization, and new bridges and roads are under construction.
Future metro plans.
Sarajevo-based architect, Muzafer Osmanagić, in order to solve traffic congestion in Sarajevo, has proposed a study called "Eco Energy 2010–2015", idealizing a subway system underneath the bed of the river Miljacka. The first line of Metro Sarajevo should connect Basčarsija with Otoka. This line should cost some 150 million KM and be financed by the European Bank for Reconstruction and Development.
Cable car (Mt. Trebević).
Trebević ropeway (cable car), Sarajevo’s key landmark during 1984 Winter Olympic Games, has been announced to be rebuilt by JKP GRAS Sarajevo and Sarajevo Canton as one of the new transportation systems due out in last quarter of 2016. The cable car would run from Sarajevo at Bistrik station to Trebević at Vidikovac station.
Airport.
Sarajevo International Airport , also called Butmir, is located just a few kilometers southwest of the city and was voted Best European Airport With Under 1,000,000 Passengers at the 15th Annual ACI-Europe in Munich in 2005. During the war the airport was used for UN flights and humanitarian relief. Since the Dayton Accord in 1996, the airport has welcomed a thriving commercial flight business.
In 2011 Sarajevo International Airport had 599,996 passengers which is more than all of the airports in Bosnia-Herzegovina had together and 6,5% more than in 2010. The growth rate in 2012 is expected to be around 10%.
Plans for extension of the passenger terminal, together with upgrading and expanding the taxiway and apron, are planned to start in Fall 2012. The existing terminal will be expanded by approximately . The upgraded airport will also be directly linked to the commercial retail center Sarajevo Airport Center, making it easier for tourists and travellers to spend their time before flight boarding shopping and enjoying the many amenities that will be offered.
Between 2015 and 2018 the airport will be upgraded for more than 25 Million €.
Railway.
Sarajevo has only two daily international connections to Zagreb and Ploče. There are also connections between Sarajevo and all major cities within Bosnia and Herzegovina. Once, the East Bosnian railway connected Sarajevo to Beograd.
The Sarajevo Main Railway Station is among the biggest in Europe.
International relations.
Twin towns – Sister cities.
Sarajevo is twinned with:
Fraternity cities.
Sarajevo's fraternity cities include:
Communications and media.
As the largest city of Bosnia and Herzegovina, Sarajevo is the main center of the country's media. Most of the communications and media infrastructure was destroyed during the war but reconstruction monitored by the Office of the High Representative has helped to modernize the industry as a whole. For example, internet was first made available to the city in 1995.
"Oslobođenje" (Liberation), founded in 1943, is Sarajevo's longest running continuously circulating newspaper and the only one to survive the war. However, this long running and trusted newspaper has fallen behind "Dnevni Avaz" (Daily Voice), founded in 1995, and "Jutarnje Novine" (Morning News) in circulation in Sarajevo. Other local periodicals include the Croatian newspaper Hrvatska riječ and the Bosnian magazine Start, as well as weekly newspapers "Slobodna Bosna" ("Free Bosnia") and "BH Dani" ("BH Days"). "Novi Plamen", a monthly magazine, is the most left-wing publication currently.
The Radiotelevision of Bosnia-Herzegovina is Sarajevo's public television station, one of three in Bosnia and Herzegovina. Other stations based in the city include NRTV "Studio 99", NTV Hayat, TV 1, Open Broadcast Network, TV Kantona Sarajevo and Televizija Alfa.
The headquarters of Al Jazeera Balkans are also located in Sarajevo, with a broadcasting studio at the top of the BBI Center. The news channel covers Bosnia and Herzegovina, Serbia, Croatia and Montenegro and the surrounding Balkan states.
Many small independent radio stations exist, including established stations such as Radio M, Radio Stari Grad (Radio Old Town), Studentski eFM Radio, Radio 202, Radio BIR, and RSG. Radio Free Europe, as well as several American and Western European stations are available.
Education.
Higher education has a long and rich tradition in Sarajevo. The first institution that can be classified as a tertiary educational institution was a school of Sufi philosophy established by Gazi Husrev-beg in 1531; numerous other religious schools have been established over time. In 1887, under the Austro-Hungarian Empire, a Sharia Law School began a five-year program. In the 1940s the University of Sarajevo became the city's first secular higher education institute, effectively building upon the foundations established by the Saraybosna Hanıka in 1531. In the 1950s, post-bachelor graduate degrees became available. Severely damaged during the war, it was recently rebuilt in partnership with more than 40 other universities.
There are also several international and private universities located in Sarajevo:
The University of Sarajevo is the most important institution of higher education in Bosnia-Herzegovina, having been established originally in 1531 as an Ottoman Law School, and in its modern incarnation in 1949. With 23 faculties and around 55,000 enrolled students, it ranks among the largest universities in Europe in terms of enrollment. Since the university opened its doors, 122,000 students received bachelor's degrees, 3,891 received master's degrees and 2,284 doctorate degrees in 43 different fields.
, in Sarajevo there are 46 elementary schools (Grades 1–9) and 33 high schools (Grades 10–13), including three schools for children with special needs,
'Druga gimnazija' provides the MYP and International Baccalaureate diploma. 'Prva bošnjačka gimnazija' provides the IGCSE and GCE Advanced Level.
There are also several international schools in Sarajevo, catering to the expatriate community; some of which are Sarajevo International School and the French International School of Sarajevo, established in 1998.
Culture.
Sarajevo has been home to many different religions for centuries, giving the city a range of diverse cultures. In the time of Ottoman occupation of Bosnia, Muslims, Serbian Orthodox, Roman Catholics, and Sephardi Jews all shared the city while maintaining distinctive identities. They were joined during the brief occupation by Austria-Hungary by a smaller number of Germans, Hungarians, Slovaks, Czechs and Ashkenazi Jews. By 1909, about 50% of the city's inhabitants were Muslim, 25% were Catholic, 15% were Orthodox, and 10% were Jewish.
Historically, Sarajevo has been home to several famous Bosnian poets, scholars, philosophers, and writers during the Ottoman Empire. To list only a very few; Nobel Prize-winner Vladimir Prelog is from the city, as is Academy Award-winning director Danis Tanović, multiple award-winning writer Aleksander Hemon and prominent multiple award-winning writer and screenwriter Zlatko Topčić. One of the region's most prolific and prominent poets, writers and screenwriters, Abdulah Sidran is also a Sarajevo native. Nobel Prize-winner Ivo Andrić attended high school in Sarajevo for two years. Sarajevo is also the home of the East West Theatre Company, the only independent theater company in Bosnia and Herzegovina.
The Sarajevo National Theatre is the oldest professional theater in Bosnia and Herzegovina, having been established in 1921.
Museums.
The city is rich in museums, including the Museum of Sarajevo, the Ars Aevi Museum of Contemporary Art, Historical Museum of Bosnia and Herzegovina, The Museum of Literature and Theatre Arts of Bosnia and Herzegovina, and the National Museum of Bosnia and Herzegovina (established in 1888) home to the Sarajevo Haggadah, an illuminated manuscript and the oldest Sephardic Jewish document in the world issued in Barcelona around 1350, containing the traditional Jewish Haggadah, is on permanent display at the museum. It is the only remaining illustrated Sephardic Haggadah in the world. The National Museum also hosts year-round exhibitions pertaining to local, regional and international culture and history, and exhibits over 5,000 artefacts from Bosnia's history.
The Alija Izetbegović Museum was opened on 19 October 2007 and is located in the old town fort, more specifically in the Vratnik Kapija towers Ploča and Širokac. The museum is a commemoration to the influence and body of work of Alija Izetbegović, the first president of the Republic of Bosnia and Herzegovina.
The city also hosts the Sarajevo National Theater, established in 1919, as well as East West Theatre Company and the Sarajevo Youth Theatre. Some other cultural institutions include the Center for Sarajevo Culture, Sarajevo City Library, Art Gallery of Bosnia and Herzegovina, and the Bosniak Institute, a privately owned library and art collection focusing on Bosniak history.
Demolitions associated with the war, as well as reconstruction, destroyed several institutions and cultural or religious symbols including the Gazi Husrev-beg library, the national library, the Sarajevo Oriental Institute, and a museum dedicated to the 1984 Olympic games. Consequently, the different levels of government established strong cultural protection laws and institutions. Bodies charged with cultural preservation in Sarajevo include the Institute for the Protection of the Cultural, Historical and Natural Heritage of Bosnia and Herzegovina (and their Sarajevo Canton counterpart), and the Bosnia and Herzegovina Commission to Preserve National Monuments.
Music.
Sarajevo is and has historically been one of the most important musical enclaves in the region. The Sarajevo school of pop rock developed in the city between 1961 and 1991. This type of music began with bands like Indexi, and singer/songwriter Kemal Monteno. It continued into the 1980s, with bands such as Plavi Orkestar, Crvena Jabuka, and Divlje Jagode, by most accounts, pioneering the regional rock and roll movement. Sarajevo was also the home and birthplace of arguably the most popular and influential Yugoslav rock band of all time, Bijelo Dugme, somewhat of a Bosnian parallel to the Rolling Stones, in both popularity and influence. Sarajevo was also the home of a very notable post-punk urban subculture known as the New Primitives, which began during the early 1980s with the Baglama Band which was banned shortly after first LP and was brought into the mainstream through bands such as Zabranjeno Pušenje and Elvis J. Kurtović & His Meteors, as well as the Top Lista Nadrealista radio, and later television show. Other notable bands considered to be part of this subculture are Bombaj Štampa. Besides and separately from the "New Primitives", Sarajevo is the hometown to one of the most significant ex-Yugoslavian alternative industrial-noise bands, SCH (1983–current).
Perhaps more importantly, Sarajevo in the late 19th and throughout the 20th century was home to a burgeoning and large center of Sevdalinka record-making and contributed greatly to bringing this historical genre of music to the mainstream, which had for many centuries been a staple of Bosnian culture. Songwriters and musicians such as Himzo Polovina, Safet Isović, Zaim Imamović, Zehra Deović, Halid Bešlić, Hanka Paldum, Nada Mamula, Meho Puzić and many more composed and wrote some of their most important pieces in the city.
Sarajevo also greatly influenced the pop scene of Yugoslavia with musicians like Dino Merlin, Hari Mata Hari, Tifa, Kemal Monteno, Željko Bebek, and many more.
Many newer Sarajevo-based bands have also found a name and established themselves in Sarajevo, such as Regina who also had two albums out in Yugoslavia and Letu Štuke, who actually formed their band in Yugoslavia with the famous Bosnian-American writer Aleksandar Hemon and got their real breakthrough later in the 2000s. Sarajevo is now home to an important and eclectic mix of new bands and independent musicians, which continue to thrive with the ever-increasing number of festivals, creative showcases and concerts around the country. The city is also home to the region's largest jazz festival, the Sarajevo Jazz Festival (see "Festival" section below this).
Festivals.
Sarajevo is internationally renowned for its eclectic and diverse selection of festivals. The Sarajevo Film Festival was established in 1995 during the Bosnian War and has become the premier and largest film festival in the Balkans and South-East Europe. The Sarajevo Winter Festival, Sarajevo Jazz Festival and Sarajevo International Music Festival are well-known, as is the Baščaršija Nights festival, a month-long showcase of local culture, music, and dance.
The Sarajevo Film Festival has been hosted at the National Theater, with screenings at the Open-air theater Metalac and the Bosnian Cultural Center, all located in downtown Sarajevo and has been attended by celebrities such as Angelina Jolie, Brad Pitt, Emile Hirsch, Orlando Bloom, Daniel Craig, Danny Glover, John Malkovich, Morgan Freeman, Steve Buscemi, Bono Vox (Bono holds dual Bosnian and Irish citizenship and is an honorary citizen of Sarajevo), Nick Cave, Coolio, Stephen Frears, Mickey Rourke, Michael Moore, Gérard Depardieu, Darren Aronofsky, Sophie Okonedo, Gillian Anderson, Kevin Spacey, Willem Dafoe, Eric Cantona and many more.
In the past sixteen years, the festival has entertained people and celebrities alike, elevating it to a recognized international level. The first incarnation of the Sarajevo Film Festival was hosted in still-warring Sarajevo in 1995, and has now progressed into being the biggest and most significant festival in south-eastern Europe. A talent campus is also held during the duration of the festival, with numerous world-renowned lecturers speaking on behalf of world cinematography and holding workshops for film students from across South-Eastern Europe.
The Sarajevo Jazz Festival is the region's largest and most diverse of its kind and has been entertaining jazz connoisseurs for over ten years and has hosted such artists as Richard Bona, Biréli Lagrène, Cristina Branco, Dhafer Youssef, Bugge Wesseltoft, Dennis Chambers, Joseph Tawadros and many more. The festival takes place at the Bosnian Cultural Center (aka "Main Stage"), just down the street from the SFF, at the Sarajevo Youth Stage Theater (aka "Strange Fruits Stage"), at the Dom Vojske Federacije (aka "Solo Stage"), and at the CDA (aka "Groove Stage").
Sports.
The city was the location of the 1984 Winter Olympics. Yugoslavia won one medal, a silver in men's giant slalom awarded to Jure Franko. Many of the Olympic facilities survived the war or were reconstructed, including Olympic Hall Zetra and Asim Ferhatović Stadion. After co-hosting the Southeast Europe Friendship games, Sarajevo was awarded the 2009 Special Olympic winter games, but cancelled these plans. The ice arena for the 1984 Olympics, Zetra Stadium, was used during the war as a temporary hospital and, later, for housing NATO troops of the IFOR.
In 2011 Sarajevo was the host city of the 51st World Military Skiing Championship with over 350 participants from 23 different nations. This was the first international event of such standing since the 1984 Olympics.
Football (soccer) is popular in Sarajevo; the city hosts "FK Sarajevo" and "FK Željezničar", which both compete in European and international cups and tournaments and are have a very large trophy cabinet in the former Yugoslavia as well as independent Bosnia and Herzegovina. Other notable soccer clubs are "FK Olimpik" and "SAŠK". Another popular sport is basketball; the basketball club KK Bosna Sarajevo won the European Championship in 1979 as well as many Yugoslav and Bosnian national championships making it one of the greatest basketball clubs in the former Yugoslavia. The chess club, "Bosna" Sarajevo, has been a championship team since the 1980s and is the third ranked chess club in Europe, having won four consecutive European championships in the nineties. RK Bosna also competes in the European Champions League and is considered one of the most well organised handball clubs in South-Eastern Europe with a very large fan base and excellent national, as well as international results.
Sarajevo often holds international events and competitions in sports such as tennis and kickboxing.
The popularity of tennis has been picking up in recent years. Since 2003, BH Telecom Indoors is an annual tennis tournament in Sarajevo.
Since 2007, the Sarajevo Marathon is being organized in late September. Giro di Sarajevo is also run in the city with over 2,200 cyclists taking part in 2015.
In 2019, Sarajevo and East Sarajevo will host the European Youth Olympic Winter Festival (EYOWF).

</doc>
