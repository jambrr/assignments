<doc id="28175" url="https://en.wikipedia.org/wiki?curid=28175" title="Sinn Féin">
Sinn Féin

Sinn Féin ( ) is an Irish republican political party active in the Republic of Ireland and Northern Ireland. The phrase "Sinn Féin" is Irish for "ourselves" or "we ourselves", although it is frequently mistranslated as "ourselves alone". The Sinn Féin organisation was founded in 1905 by Arthur Griffith. It took its current form in 1970 after a split within the party (the other party became the Workers' Party of Ireland), and has been historically associated with the Provisional Irish Republican Army (IRA). Gerry Adams has been party president since 1983.
Sinn Féin is the second-largest party behind the Democratic Unionist Party (DUP) in the Northern Ireland Assembly, where it has four ministerial posts in the power-sharing Northern Ireland Executive, and the third-largest party in the Oireachtas, the parliament of the Republic. Sinn Féin received the second highest number of Northern Ireland votes and seats in the 2015 Westminster elections, behind the DUP.
Name.
The name of the party "Sinn Féin" is from the Irish language and means "We Ourselves" (often mistranslated as "Ourselves Alone") . The meaning of the name itself is an assertion of Irish national sovereignty and self-determination, i.e. – the Irish people governing themselves, rather than being part of a political union with Great Britain (England, Scotland and Wales) under the Westminster Parliament. 
Around the time of 1969–1970, due to the split in the Republican movement there were two groups calling themselves Sinn Féin; one under Tomás Mac Giolla, the other under Ruairí Ó Brádaigh. The latter became known as Sinn Féin (Kevin Street) or Provisional Sinn Féin and the former became known as Sinn Féin (Gardiner Place) or Official Sinn Féin. As the "Officials" dropped all mention of Sinn Féin from their name in 1982, instead calling itself the "Workers' Party", the Provisionals were now generally known as Sinn Féin. Supporters of Republican Sinn Féin from the 1986 split still use the term "Provisional Sinn Féin" to refer to the party led by Gerry Adams. 
History.
1905–1970.
Sinn Féin was founded on 28 November 1905, when, at the first annual Convention of the National Council, Arthur Griffith outlined the Sinn Féin policy, "to establish in Ireland's capital a national legislature endowed with the moral authority of the Irish nation". Sinn Féin contested the North Leitrim by-election, 1908, and secured 27% of the vote. Thereafter, both support and membership fell. At the 1910 Ard Fheis (party conference) the attendance was poor and there was difficulty finding members willing to take seats on the executive.
In 1914, Sinn Féin members, including Griffith, joined the anti-Redmond Irish Volunteers, which was referred to by Redmondites and others as the "Sinn Féin Volunteers". Although Griffith himself did not take part in the Easter Rising of 1916, many Sinn Féin members, who were also members of both the Volunteers and the Irish Republican Brotherhood, did. Government and newspapers dubbed the Rising "the Sinn Féin Rising". After the Rising, republicans came together under the banner of Sinn Féin, and at the 1917 Ard Fheis the party committed itself for the first time to the establishment of an Irish Republic. In the 1918 general election, Sinn Féin won 73 of Ireland's 105 seats, and in January 1919, its MPs assembled in Dublin and proclaimed themselves Dáil Éireann, the parliament of Ireland. The party supported the Irish Republican Army during the War of Independence, and members of the Dáil government negotiated the Anglo-Irish Treaty with the British Government in 1921. In the Dáil debates that followed, the party divided on the Treaty. Anti-Treaty members led by Éamon de Valera walked out, and pro- and anti-Treaty members took opposite sides in the ensuing Civil War.
Pro-Treaty Dáil deputies and other Treaty supporters formed a new party, Cumann na nGaedheal, on 27 April 1923 at a meeting in Dublin where delegates agreed on a constitution and political programme. Cumann na nGaedheal governed the new Irish Free State for ten years. It merged with two other organisations to form Fine Gael in 1933. Anti-Treaty Sinn Féin members continued to boycott the Dáil. At a special "Ard Fheis" in March 1926 de Valera proposed that elected members be allowed to take their seats in the Dáil if and when the controversial Oath of Allegiance was removed. When his motion was defeated, de Valera resigned from Sinn Féin and on 16 May 1926 founded his own party, Fianna Fáil, which was dedicated to republicanising the Free State from within its political structures. He took most Sinn Féin TDs with him. De Valera's resignation meant also the loss of financial support from America. The rump Sinn Féin party could field no more than fifteen candidates, and won only six seats in the June election, a level of support not seen since before 1916. Vice-President and de facto leader Mary MacSwiney announced that the party simply did not have the funds to contest the second election called that year, declaring "no true Irish citizen can vote for any of the other parties".
An attempt in the 1940s to access funds which had been put in the care of the High Court led to the Sinn Féin Funds case, which the party lost and in which the judge ruled that it was not the direct successor of the Sinn Féin of 1917. At the Westminister 1959 general election, the Sinn Féin vote dropped almost 60% from the 1955 number 152,000 to 63,000. In the 1960s, Sinn Féin moved to the left and became involved in campaigns over the provision of housing and social services. It also adopted a "National Liberation Strategy" which was the brainchild of Roy Johnston. In 1967 the Garland Commission was set up to investigate the possibility of ending abstentionism. Its report angered many traditionalists within the party, notably Seán Mac Stíofáin and Ruairí Ó Brádaigh.
1970–1975.
The Sinn Féin party split in two at the beginning of 1970. At the party's "Ard Fheis" on 11 January the proposal to end abstentionism and take seats, if elected, in the Dáil, the Parliament of Northern Ireland and the Parliament of the United Kingdom was put before the members. A similar motion had been adopted at an IRA convention the previous month, leading to the formation of a Provisional Army Council by Mac Stíofáin and other members opposed to the leadership. When the motion was put to the "Ard Fheis", it failed to achieve the necessary two-thirds majority. The Executive attempted to circumvent this by introducing a motion in support of IRA policy, at which point the dissenting delegates walked out of the meeting. These members reconvened at another place, appointed a Caretaker Executive and pledged allegiance to the Provisional Army Council. The Caretaker Executive declared itself opposed to the ending of abstentionism, the drift towards "extreme forms of socialism", the failure of the leadership to defend the nationalist people of Belfast during the 1969 Northern Ireland riots, and the expulsion of traditional republicans by the leadership during the 1960s. At its October 1970 "Ard Fheis", delegates were informed that an IRA convention had been held and had regularised its structure, bringing to an end the 'provisional' period. By then, however, the label "Provisional" or "Provo" was already being applied to them by the media. The opposing, anti-abstentionist party became known as "Official Sinn Féin". It changed its name in 1977 to "Sinn Féin, the Workers' Party", and in 1982 to "The Workers' Party".
Because the "Provisionals" were committed to military rather than political action, Sinn Féin's initial membership was largely confined, in Danny Morrison's words, to people "over military age or women". A Belfast Sinn Féin organiser of the time described the party's role as "agitation and publicity". New "cumainn" (branches) were established in Belfast, and a new newspaper, "Republican News", was published. Sinn Féin took off as a protest movement after the introduction of internment in August 1971, organising marches and pickets. The party launched its platform, "Éire Nua" (a New Ireland) at the 1971 "Ard Fheis". In general, however, the party lacked a distinct political philosophy. In the words of Brian Feeney, "Ó Brádaigh would use Sinn Féin "ard fheiseanna" to announce republican policy, which was, in effect, IRA policy, namely that Britain should leave the North or the 'war' would continue". Sinn Féin was given a concrete presence in the community when the IRA declared a ceasefire in 1975. 'Incident centres' were set up to communicate potential confrontations to the British authorities. They were manned by Sinn Féin, which had been legalised the year before by Secretary of State, Merlyn Rees.
1976–1983.
Political status for prisoners became an issue after the ending of the truce. Rees released the last of the internees but introduced the Diplock courts, and ended 'Special Category Status' for all prisoners convicted after 1 March 1976. This led first to the blanket protest, and then to the dirty protest . Around the same time, Gerry Adams began writing for "Republican News", calling for Sinn Féin to become more involved politically . Over the next few years, Adams and those aligned with him would extend their influence throughout the republican movement and slowly marginalise Ó Brádaigh, part of a general trend of power in both Sinn Féin and the IRA shifting north. In particular, Ó'Brádaigh's part in the 1975 IRA ceasefire had damaged his reputation in the eyes of Ulster republicans.
The prisoners' protest climaxed with the 1981 hunger strike, during which striker Bobby Sands was elected Member of Parliament for Fermanagh and South Tyrone as an Anti H-Block candidate. After his death on hunger strike, his seat was held, with an increased vote, by his election agent, Owen Carron. These successes convinced republicans that they should contest every election. Danny Morrison expressed the mood at the 1981 "Ard Fheis" when he said:
This was the origin of what became known as the Armalite and ballot box strategy. "Éire Nua" was dropped in 1982, and the following year Ó Brádaigh stepped down as leader, and was replaced by Adams.
1983–1998.
Under Adams' leadership electoral politics became increasingly important. In 1983 Alex Maskey was elected to Belfast City Council, the first Sinn Féin member to sit on that body. Sinn Féin polled over 100,000 votes in the Westminster elections that year, and Adams won the West Belfast seat that had been held by the Social Democratic and Labour Party (SDLP). By 1985 it had fifty-nine seats on seventeen of the twenty-six Northern Ireland councils, including seven on Belfast City Council.
The party began a reappraisal of the policy of abstention from the Dáil. At the 1983 "Ard Fheis" the constitution was amended to remove the ban on the discussion of abstentionism to allow Sinn Féin to run a candidate in the forthcoming European elections. However, in his address Adams said, "We are an abstentionist party. It is not my intention to advocate change in this situation." A motion to permit entry into the Dáil was allowed at the 1985 "Ard Fheis", but without the active support of the leadership, and Adams did not speak. The motion failed narrowly. By October of the following year an IRA Convention had indicated its support for elected Sinn Féin Teachtaí Dála (TDs) taking their seats. Thus, when the motion to end abstention was put to the "Ard Fheis" on 1 November 1986, it was clear that there would not be a split in the IRA as there had been in 1970. The motion was passed with a two-thirds majority. Ó Brádaigh and about twenty other delegates walked out, and met in a Dublin hotel with hundreds of supporters to re-organise as Republican Sinn Féin.
Tentative negotiations between Sinn Féin and the British government led to more substantive discussions with the SDLP in the 1990s. Multi-party negotiations began in 1994 in Northern Ireland, without Sinn Féin. The Provisional IRA declared a ceasefire in the autumn of 1994. Sinn Féin then joined the talks but the John Major-led Conservative government soon came to depend on unionist votes to remain in power. It suspended Sinn Féin from the talks and began to insist that the IRA decommission all of their weapons before Sinn Féin be re-admitted to the talks, leading to the IRA calling off its ceasefire. The new Labour government of Tony Blair wasn't reliant on unionist votes and re-admitted Sinn Féin, leading to another, permanent, ceasefire.
The talks led to the Good Friday Agreement of 10 April 1998 (officially known as the "Belfast Agreement"), which set up an inclusive devolved government in the North, and altered the Dublin government's constitutional claim to the whole island in Articles 2 and 3 of the Constitution of Ireland. Republicans opposed to the direction taken by Sinn Féin in the peace process formed the 32 County Sovereignty Movement in the late 1990s.
1998–present.
The party expelled Denis Donaldson, a party official, in December 2005, with him stating publicly that he had been in the employ of the British government as an agent since the 1980s. Donaldson told reporters that the British security agencies who employed him were behind the collapse of the Assembly and set up Sinn Féin to take the blame for it, a claim disputed by the British Government. Donaldson was found fatally shot in his home in County Donegal on 4 April 2006, and a murder inquiry was launched. In April 2009, the Real IRA released a statement taking responsibility for the killing.
When Sinn Féin and the DUP became the largest parties, by the terms of the Belfast Agreement no deal could be made without the support of both parties. They nearly reached a deal in November 2004, but the DUP insisted on photographic and/or video evidence that decommissioning had been carried out, which was unacceptable to Sinn Féin.
On 2 September 2006 Martin McGuinness publicly stated that Sinn Féin would refuse to participate in a shadow assembly at Stormont, asserting that his party would only take part in negotiations that were aimed at restoring a power-sharing government. This development followed a decision on the part of members of Sinn Féin to refrain from participating in debates since the Assembly's recall the previous May. The relevant parties to these talks were given a deadline of 24 November 2006 to decide upon whether or not they would ultimately form the executive.
The 86-year Sinn Féin boycott of policing in Northern Ireland ended on 28 January 2007 when the Ard Fheis voted overwhelmingly to support the Police Service of Northern Ireland (PSNI). Sinn Féin members began to sit on Policing Boards and join District Policing Partnerships. There was opposition to this decision within Sinn Féin, and some members left, including elected representatives. The most well-known opponent was former IRA prisoner Gerry McGeough, who stood in the 2007 Assembly Elections against Sinn Féin in the Assembly constituency of Fermanagh and South Tyrone as an Independent Republican. Others who opposed this development splintered off to found the Republican Network for Unity.
Links with the IRA.
Sinn Féin is the largest Irish republican political party and was closely associated with the Provisional IRA. The Irish Government alleged that senior members of Sinn Féin have held posts on the IRA Army Council. However, the SF leadership has denied these claims. The US Government has made similar allegations.
A republican document of the early 1980s stated: "Both Sinn Féin and the IRA play different but converging roles in the war of national liberation. The Irish Republican Army wages an armed campaign... Sinn Féin maintains the propaganda war and is the public and political voice of the movement".
The British Government stated in 2005 that "we had always said all the way through we believed that Sinn Féin and the IRA were inextricably linked and that had obvious implications at leadership level".
The Northern Bank robbery of £26.5 million in Belfast in December 2004 further delayed a political deal in Northern Ireland. The IRA were widely blamed for the robbery although Sinn Féin denied this and stated that party officials had not known of the robbery nor sanctioned it. Because of the timing of the robbery, it is considered that the plans for the robbery must have been laid whilst Sinn Féin was engaged in talks about a possible peace settlement. This undermined confidence among unionists about the sincerity of republicans towards reaching agreement. In the aftermath of the row over the robbery, a further controversy erupted when, on RTÉ's "Questions and Answers" programme, the chairman of Sinn Féin, Mitchel McLaughlin, insisted that the IRA's controversial killing of a mother of ten young children, Jean McConville, in the early 1970s though "wrong", was not a crime, as it had taken place in the context of the political conflict. Politicians from the Republic, along with the Irish media strongly attacked McLaughlin's comments.
On 10 February 2005, the government-appointed Independent Monitoring Commission reported that it firmly supported the PSNI and Garda Síochána assessments that the IRA was responsible for the Northern Bank robbery and that certain senior members of Sinn Féin were also senior members of the IRA and would have had knowledge of and given approval to the carrying out of the robbery. Sinn Féin have argued that the IMC is not independent and the inclusion of former Alliance Party Leader John Alderdice and a British security head was proof of this. The IMC recommended further financial sanctions against Sinn Féin members of the Northern Ireland Assembly. The British government responded by saying it would ask MPs to vote to withdraw the parliamentary allowances of the four Sinn Féin MPs elected in 2001.
Gerry Adams responded to the IMC report by challenging the Irish Government to have him arrested for IRA membership, a crime in both jurisdictions, and conspiracy.
On 20 February 2005, Irish Minister for Justice, Equality and Law Reform Michael McDowell publicly accused three of the Sinn Féin leadership, Gerry Adams, Martin McGuinness and Martin Ferris (TD for Kerry North) of being on the seven-man IRA Army Council which they later denied.
On 27 February 2005, a demonstration against the murder of Robert McCartney on 30 January 2005 was held in East Belfast. Alex Maskey, a former Sinn Féin Lord Mayor of Belfast, was told by relatives of McCartney to "hand over the 12" IRA members involved. The McCartney family, although formerly Sinn Féin voters themselves, urged witnesses to the crime to contact the PSNI. Three IRA men were expelled from the organisation, and a man was charged with McCartney's murder.
Irish Taoiseach Bertie Ahern subsequently called Sinn Féin and the IRA "both sides of the same coin". The official ostracism of Sinn Féin was shown in February 2005 when Dáil Éireann passed a motion condemning the party's alleged involvement in illegal activity. US President George W. Bush and Senator Edward Kennedy refused to meet Gerry Adams while meeting the family of Robert McCartney.
On 10 March 2005, the British House of Commons in London passed a motion placed by the British Government to withdraw the allowances of the four Sinn Féin MPs for one year in response to the Northern Bank Robbery without significant opposition. This measure cost the party approximately £400,000. However, the debate prior to the vote mainly surrounded the more recent events connected with the murder of Robert McCartney. Conservatives and unionists put down amendments to have the Sinn Féin MPs evicted from their offices at the House of Commons but these were defeated.
In March 2005, Mitchell Reiss, the United States special envoy to Northern Ireland, condemned the party's links to the IRA, saying "it is hard to understand how a European country in the year 2005 can have a private army associated with a political party".
The October 2015 Assessment on Paramilitary Groups in Northern Ireland concluded that the Provisional IRA still exists "in a much reduced form" and that some IRA members believe its Army Council oversees both the PIRA and Sinn Féin.
Policy and ideology.
Most of the party's policies are intended to be implemented on an 'all-Ireland' basis which further emphasises their central aim of creating a united Ireland.
Sinn Féin currently is considered a democratic socialist or left-wing party. In the European parliament, the party aligns itself with the European United Left–Nordic Green Left (GUE/NGL) parliamentary group. The party pledges support for minority rights, migrants' rights, and eradicating poverty. Although it is not in favour of the extension of legalised abortion (British 1967 Act) to Northern Ireland, Sinn Féin state they are opposed to the attitudes in society which "pressurise women" to have abortions and "criminalise" women who make this decision. The party does state that in cases of incest, rape, sexual abuse, "fatal foetal abnormalities", or when a woman's life and health are at risk or in danger that the final decision must rest with the woman.
Sinn Féin has been considered to be Eurosceptic. The party campaigned for a "No" vote in the referendum on joining the European Economic Community in 1972. The party was critical for the need of an EU constitution as proposed in 2002, and urged a "No" vote in the 2008 referendum on the Lisbon Treaty, although Mary Lou McDonald said that there was "no contradiction in being pro-Europe but anti-treaty." In its manifesto for the 2015 UK general election, Sinn Féin pledged that the party would campaign for the UK to stay within the European Union (EU), Martin McGuinness saying that an exit "would be absolutely economically disastrous". Gerry Adams said that if there were to be a referendum on the question, there ought to be a separate and binding referendum for Northern Ireland. Its policy of a "Europe of Equals", and its critical engagement after 2001, together with as its engagement with the European Parliament, marks a change from the party's previous opposition to the EU. The party expresses, on one hand, "support for Europe-wide measures that promote and enhance human rights, equality and the all-Ireland agenda", and on the other a "principled opposition" to a European superstate.
Social and cultural.
Sinn Féin's main political goal is a united Ireland. Other key policies from their most recent election manifesto are listed below:
International relations.
Sinn Féin supports the creation of a 'Minister for Europe', the independence of the Basque Country from Spain and France, and the Palestinians in the Israeli–Palestinian conflict.
Organisational structure.
Sinn Féin is organised throughout Ireland, and membership is open to all Irish residents over the age of 16. The party is organised hierarchically into cumainn (branches), comhairle ceantair (district executives), cúigí (regional executives). At national level, the Coiste Seasta (Standing Committee) oversees the day-to-day running of Sinn Féin. It is an eight-member body nominated by the Sinn Féin Ard Chomhairle (National Executive) and also includes the chairperson of each cúige. The Sinn Féin Ard Chomhairle meets at least once a month. It directs the overall implementation of Sinn Féin policy and activities of the party.
The Ard Chomhairle also oversees the operation of various departments of Sinn Féin, viz Administration, Finance, National Organiser, Campaigns, Sinn Féin Republican Youth, Women's Forum, Culture, Publicity and International Affairs. It is made up of the following: Officer Board and nine other members, all of whom are elected by delegates to the Ard Fheis, fifteen representing the five Cúige regions (three delegates each). The Ard Chomhairle can co-opt eight members for specific posts and additional members can be co-opted, if necessary, to ensure that at least thirty per cent of Ard Chomhairle members are women.
The Ardfheis (national delegate conference) is the ultimate policy-making body of the party where delegates – directly elected by members of cumainn – can decide on and implement policy. It is held at least once a year but a special Ard Fheis can be called by the Ard Chomhairle or the membership under special circumstances.
Ard Chomhairle Officer Board.
2010–2011:
Leadership Members elected at the Ard Fhéis 2012.
Six Men
Six Women
General election results.
Northern Ireland.
Trends.
Sinn Féin returned to Northern Ireland elections at the 1982 Assembly elections, winning five seats with 64,191 votes (10.1%). The party narrowly missed winning additional seats in Belfast North and Fermanagh and South Tyrone. In the 1983 Westminster elections eight months later Sinn Féin increased its support, breaking the hundred thousand vote barrier in Northern Ireland for the first time by polling 102,701 votes (13.4%). Gerry Adams won the Belfast West constituency with Danny Morrison only 78 votes short of victory in Mid Ulster.
The 1984 European elections proved to be a disappointment, with Sinn Féin's candidate Danny Morrison polling 91,476 (13.3%) and falling well behind the SDLP candidate John Hume.
By the beginning of 1985, Sinn Féin had won their first representation on local councils due to three by-election wins in Omagh (Seamus Kerr, May 1983) and Belfast (Alex Maskey in June 1983 and Sean McKnight in March 1984). Three sitting councillors also defected to Sinn Féin in Dungannon, Fermanagh and Derry (the last defecting from the SDLP). Sinn Féin succeeded in winning 59 seats in the 1985 local government elections, after it had predicted winning only 40 seats. However, the results continued to show a decline from the peak of 1983 as the party won 75,686 votes (11.8%). The party failed to gain any seats in the 1986 by-elections caused by the resignation of unionist MPs in protest at the Anglo-Irish Agreement. While this was partly due to an electoral pact between unionist candidates, the SF vote fell in the four constituencies they contested.
In the 1987 election Gerry Adams held his Belfast West seat but the party failed to make breakthroughs elsewhere and overall polled 83,389 votes (11.4%). The same year saw the party contest the Dáil election in the Republic of Ireland, however they failed to win any seats and polled less than 2%.
The 1989 local government elections saw a drop in support for Sinn Féin. Defending 58 seats (the 59 won in 1985 plus two 1987 by-election gains in West Belfast minus three councillors who had defected to Republican Sinn Féin in 1986) the party lost 15 seats. In the aftermath of the election Mitchell McLaughlin admitted that recent IRA activity had affected the Sinn Féin vote.
In the 1989 European elections, candidate Danny Morrison again failed to win a seat, polling at 48,914 votes (9%).
The nadir for SF in this period came in 1992, with Gerry Adams losing his Belfast West seat to the SDLP and the SF vote falling in the other constituencies that they had contested relative to 1987.
In the 1997 British General Election, Gerry Adams regained his Belfast West seat. Martin McGuinness also won a seat in Mid Ulster. In Irish elections the same year the party won its first seat since the 1957 elections with Caoimhghín Ó Caoláin gaining a seat in the Cavan-Monaghan constituency. In the Irish local elections in 1999 the party increased its number of councillors from 7 to 23.
The party overtook its nationalist rival, the Social Democratic and Labour Party as the largest nationalist party in the 2001 Westminster general election and local elections, winning four Westminster seats to the SDLP's three. The party continues to subscribe, however, to an abstentionist policy towards the Westminster British parliament, on account of opposing that parliament's jurisdiction in Northern Ireland, as well as its oath to the Queen.
Sinn Féin increased its share of the nationalist vote in the 2003, 2007, and 2011 Assembly elections, with Martin McGuinness, former Minister for Education, taking the post of deputy First Minister in the Northern Ireland power-sharing Executive Committee. The party has three ministers in the Executive Committee.
In the 2010 General Election, the party retained its five seats, and for the first time topped the poll at a Westminster Election in Northern Ireland, winning 25.5% of the vote. All Sinn Féin MPs increased their share of the vote and with the exception of Fermanagh and South Tyrone, increased their majorities. In Fermanagh and South Tyrone, Unionist parties agreed a joint candidate, this resulted in the closest contest of the election, with Sinn Féin MP Michelle Gildernew holding her seat by 4 votes after 3 recounts and an election petition challenging the result.
Republic of Ireland.
Dáil Éireann elections.
The party had five TDs elected in the 2002 Republic general election, an increase of four from the previous election. At the general election in 2007 the party had expectations of substantial gains, with poll predictions that they would gain five to ten seats. However, the party lost one of its seats to Fine Gael. Seán Crowe, who had topped the poll in Dublin South–West fell to fifth place, with his first preference vote reduced from 20.28% to 12.16%.
On 26 November 2010, Pearse Doherty won a seat in the Donegal South–West by-election. It was the party's first by-election victory in the Republic of Ireland since 1925. After negotiations with the left wing Independent TDs Finian McGrath and Maureen O'Sullivan, a Technical Group was formed in the Dáil to give its members more speaking time.
In the 2011 Irish General Election the party made gains. All its sitting TDs were returned with Seán Crowe regaining the seat in Dublin South–West he lost in 2007. In addition to winning long time targeted seats such as Dublin Central and Dublin North–West the party gained unexpected seats in Cork East and Sligo–North Leitrim. It ultimately won 14 seats, the best performance for the party's current incarnation. The party went on to win three seats in the Seanad election which followed their success at the General Election.
Local Government elections.
Sinn Féin is represented on most county and city councils. It made large gains in the local elections of 2004, increasing its number of councillors from 21 to 54, and replacing the Progressive Democrats as the fourth-largest party in local government. At the local elections of June 2009, the party's vote fell by 0.95% to 7.34%, with no change in the number of seats. Losses in Dublin and urban areas were balanced by gains in areas such as Limerick, Wicklow, Cork, Tipperary and Kilkenny and the border counties . However, three of Sinn Féin's seven representatives on Dublin City Council resigned within six months of the June 2009 elections, one of them defecting to the Labour Party.
European elections.
In the 2004 European Parliament election, Bairbre de Brún won Sinn Féin's first seat in the European Parliament, at the expense of the Social Democratic and Labour Party (SDLP). She came in second behind Jim Allister, then of the Democratic Unionist Party (DUP). In the 2009 election, de Brún was re-elected with 126,184 first preference votes, the only candidate to reach the quota on the first count. This was the first time since elections began in 1979 that the DUP failed to take the first seat, and was the first occasion Sinn Féin topped a poll in any Northern Ireland election.
Sinn Féin made a breakthrough in the Dublin constituency in 2004. The party's candidate, Mary Lou McDonald, was elected on the sixth count as one of four MEPs for Dublin, effectively taking the seat of Patricia McKenna of the Green Party. In the 2009 election, when Dublin's representation was reduced to three MEPs, she failed to hold her seat. In the South constituency their candidate, Councillor Toireasa Ferris, managed to nearly double the number of first preference votes, lying third after the first count, but failed to get enough transfers to win a seat.
In the 2014 election, Martina Anderson topped the poll in Northern Ireland, as did Lynn Boylan in Dublin. Liadh Ní Riada was elected in the South constituency, and Matt Carthy in Midlands–North-West.

</doc>
<doc id="28176" url="https://en.wikipedia.org/wiki?curid=28176" title="Willis Tower">
Willis Tower

The Willis Tower, built and still commonly referred to as Sears Tower, is a 108-story, skyscraper in Chicago, Illinois, United States. At completion in 1973, it surpassed the World Trade Center towers in New York to become the tallest building in the world, a title it held for nearly 25 years. The Willis Tower is the second-tallest building in the United States and the 14th-tallest in the world. More than one million people visit its observation deck each year, making it one of Chicago's most popular tourist destinations. The structure was renamed in 2009 by the Willis Group as part of its lease on a portion of the tower's space.
, the building's largest tenant is United Airlines, which moved its corporate headquarters from the United Building at 77 West Wacker Drive in 2012 and today occupies around 20 floors with its headquarters and operations center.
The building's official address is 233 South Wacker Drive, Chicago, Illinois 60606.
History.
Planning and construction.
In 1969, Sears, Roebuck & Co. was the largest retailer in the world, with about 350,000 employees. Sears executives decided to consolidate the thousands of employees in offices distributed throughout the Chicago area into one building on the western edge of Chicago's Loop. Sears asked its outside counsel, Arnstein, Gluck, Weitzenfeld & Minow (now known as Arnstein & Lehr, LLP) to suggest a location. The firm consulted with local and federal authorities and the applicable law, then offered Sears two options: an area known as Goose Island and a two-block area bounded by Franklin Street on the east, Jackson Boulevard on the south, Wacker Drive on the west and Adams Street on the north, with Quincy Street running through the middle from east to west.
This latter site was decided upon, and preliminary inquiries determined that the necessary permits could be obtained and Quincy Street vacated. The next step was to acquire the property; a team of attorneys from the Arnstein law firm, headed by Andrew Adsit, began buying the property parcel by parcel. Sears purchased 15 old buildings from 100 owners and paid $2.7 million to the City of Chicago for the portion of Quincy Street that divided the property.
Sears, which needed of office space for its planned consolidation and predicted that growth would require yet more, commissioned architects Skidmore, Owings & Merrill (SOM) to produce a structure to be one of the largest office buildings in the world. Their team of architect Bruce Graham and structural engineer Fazlur Rahman Khan designed the building as nine square "tubes" (each essentially a separate building), clustered in a 3×3 matrix forming a square base with sides. All nine tubes would rise up to the 50th floor of the building. At the 50th floor, the northwest and southeast tubes end, and the remaining seven continue up. At the 66th floor, the northeast and the southwest tubes end. At the 90th floor, the north, east, and south tubes end. The remaining west and center tubes continue up to the 108th floor.
The Willis Tower was the first building to use Khan's bundled tube structure. This innovative design was structurally efficient and economic: at 1,450 feet, it provided more space and rose higher than the Empire State Building, yet cost much less per unit area. This structural system would prove highly influential in skyscraper construction. It has been used in most supertall buildings since then, including the world's tallest building, the Burj Khalifa. To honor Khan's contributions, the Structural Engineers Association of Illinois commissioned a sculpture of him for the lobby of the Willis Tower.
Sears executives decided that the space they would immediately occupy should be efficiently designed to house their Merchandise Group, and that floor space for future growth would be rented out to smaller firms and businesses until Sears could retake it. The latter floor areas had to be designed to a smaller plate, with a high window-space to floor-space ratio, to be attractive and marketable to prospective lessees. Smaller floorplates required a taller structure to yield sufficient square footage. Skidmore architects proposed a tower with large floors in the lower part of the building, and gradually tapered areas of floorplates in a series of setbacks, which would give the Sears Tower its distinctive look.
As Sears continued to offer optimistic projections for growth, the tower's proposed floor count rapidly increased into the low hundreds, surpassing the height of New York's unfinished World Trade Center to become the world's tallest building. The height was restricted by a limit imposed by the Federal Aviation Administration (FAA) to protect air traffic. The financing of the tower was provided by the Sears company. It was topped with two antennas to permit local television and radio broadcasts. Sears and the City of Chicago approved the design, and the first steel was put in place in April 1971. The structure was completed in May 1973. The construction cost about US$150 million at the time, equivalent to $ million in . By comparison, Taipei 101, built in 2004 in Taiwan, cost around the equivalent of US$2.14 billion in 2014 dollars.
Black bands appear on the tower around the 29th–32nd, 64th–65th, 88th–89th, and 104th–108th floors. These are louvres that allow ventilation for service equipment and obscure the structure's belt trusses. Even though regulations did not require a fire sprinkler system, the building was equipped with one from the beginning. There are around 40,000 sprinkler heads in the building, installed at a cost of $4 million.
In February 1982, two television antennas were added to the structure, increasing its total height to . The western antenna was later extended, bringing the overall height to on June 5, 2000 to improve reception of local NBC station WMAQ-TV.
Suits filed to halt construction.
As the construction of the building neared the 50th floor, lawsuits for an injunction were filed seeking to stop the building from exceeding 67 floors. The suits alleged that above that point television reception would deteriorate and cause property values to plummet. The first suit was filed by the State's Attorney in neighboring Lake County on March 17, 1972. A second suit was filed on March 28 in the Cook County Circuit Court by the Villages of Skokie, Northbrook and Deerfield, Illinois.
Sears filed motions to dismiss the Lake County and the Cook County lawsuits and on May 17, 1972, Judge LaVerne Dickson, Chief of the Lake County Circuit Court dismissed the suit, saying, "I find nothing that gives television viewers the right to reception without interference. They will have to find some other means of ensuring reception such as taller antennas." The Lake County State's Attorney filed a Notice of Appeal and the Supreme Court agreed to permit bypassing the appellate court and to hear the matter on an expedited basis. The State's Attorney then asked the Illinois Supreme Court for a temporary injunction to stop the construction and his request was denied. On June 12, Judge Charles R. Barrett granted Sears' motion to dismiss the suit filed by three Chicago suburbs on the ground that interference with television reception caused by construction of the Sears building did not violate constitutional rights and that the suburbs involved in the suit do not have any right to undistorted television reception. This decision, too, also was appealed and consolidated with the Lake County appeal with the Supreme Court of Illinois.
Meanwhile, an Illinois Citizens Committee for Broadcasting requested the Federal Communications Commission to halt construction on the grounds so that it would not interfere with area television reception. On May 26, 1972, the Commission declined to take action on the ground that it did not have jurisdiction to do so.
On June 30, 1972, the Illinois Supreme Court affirmed the previous rulings by Lake and Cook County Circuit Courts, by a letter order with a written opinion to follow. On September 8, 1972, the United States Court of Appeals for the Seventh Circuit upheld the decision by the Federal Communications Commission to dismiss the complaint brought by the Illinois Citizens Committee for Broadcasting charging that the building would drastically affect reception in the Chicago market and requesting the FCC to halt construction. The Supreme Court of Illinois written opinion was filed on September 20, 1972. In affirming the judgments of lower courts the Court held, "Considering the foregoing, it is clear to us that absent legislation to the contrary defendant has a propriety right to construct a building to its desired height and that completion of the project would not constitute a nuisance under the circumstances of this case." 
Post-opening.
Sears' optimistic growth projections were not met. Competition from its traditional rivals (like Montgomery Ward) continued, with new competition by retailing giants such as Kmart, Kohl's, and Walmart. The fortunes of Sears & Roebuck declined in the 1970s as the company lost market share; its management grew more cautious. Nor did the Sears Tower draw as many tenants as Sears had hoped. The tower stood half-vacant for a decade as a surplus of office space was erected in Chicago in the 1980s.
In 1990, the law firm of Keck, Mahin & Cate decided to move out of its space in the Sears Tower and into a development that would become 77 West Wacker Drive, rebuffing Sears' attempts to entice the firm to stay. Two years later, Sears began moving its own offices out of the Sears Tower.
In 1994, Sears sold the building to Boston-based AEW Capital Management, with financing from MetLife. At the time, it was one-third vacant. By 1995, Sears had completely left the building, moving to a new office campus in Hoffman Estates, Illinois.
In 1997, Toronto-based TrizecHahn Corporation (the owner of the CN Tower at the time) purchased the building for $110 million, and assumption of $4 million in liabilities, and a $734 million mortgage. In 2003, Trizec surrendered the building to lender MetLife.
In 2004, MetLife sold the building to a group of investors, including New York-based Joseph Chetrit, Joseph Moinian, Lloyd Goldman, Joseph Cayre and Jeffrey Feil, and Skokie, Illinois-based American Landmark Properties. The quoted price was $840 million, with $825 million held in a mortgage.
In June 2006, seven men were arrested by the FBI and charged with plotting to destroy the tower. Deputy FBI Director John Pistole described their plot as "more aspirational than operational". The case went to court in October 2007; after three trials, five of the suspects were convicted and two were acquitted. The alleged leader of the group, Narseal Batiste, was sentenced to 13½ years in prison in November 2009.
Plans.
In February 2009, the owners announced they were considering a plan to paint the structure silver; this plan was later dropped. The paint would have "rebranded" the building and highlighted its advances in energy efficiency. The estimated cost was $50 million.
Since 2007, the building owners have been considering building a hotel on the north side of Jackson, between Wacker and Franklin, at the plaza that is the entrance to the tower's observation deck. The tower's parking garage is beneath the plaza. Building owners say the second building was considered in the original design. The plan was eventually cancelled as city zoning does not permit construction of such a tall tower there.
Although Sears' naming rights expired in 2003, the building continued to be called the Sears Tower for several years. In March 2009, London-based insurance broker Willis Group Holdings agreed to lease a portion of the building, and obtained the building's naming rights. On July 16, 2009, the building was officially renamed Willis Tower. On August 13, 2012, United Airlines announced it would move its corporate headquarters from 77 West Wacker Drive to Willis Tower.
In 2015, the Blackstone Group completed purchase of the tower for a reported $1.3 billion, the highest price ever paid for a U.S. property outside New York City. The new owners are considering several plans for further site developments.
Skydeck.
The Willis Tower observation deck, called the Skydeck, opened on June 22, 1974. Located on the 103rd floor of the tower, it is high and is one of the most famous tourist attractions in Chicago. Tourists can experience how the building sways on a windy day. They can see far over the plains of Illinois and across Lake Michigan to Indiana, Michigan and Wisconsin on a clear day. Elevators take tourists to the top in about 60 seconds, and allow tourists to feel the pressure change as they rise up. The Skydeck competes with the John Hancock Center's observation floor a mile and a half away, which is lower. Some 1.3 million tourists visit the Skydeck annually. A second Skydeck on the 99th floor is also used if the 103rd floor is closed. The tourist entrance can be found on the south side of the building along Jackson Boulevard.
In January 2009, Willis Tower's owners began a major renovation of the Skydeck, including the installation of retractable glass balconies, which can be extended approximately from the facade of the 103rd floor, overlooking South Wacker Drive. The all-glass boxes, informally dubbed "The Ledge", allow visitors to look through the glass floor to the street below. The boxes, which can bear of weight, opened to the public on July 2, 2009. However, on May 29, 2014, the laminated glass covering the floor of one of the glass boxes shattered while visitors were sitting on it, but caused no injuries. The broken glass was replaced within days, and tourist operations resumed as before.
Height.
Willis Tower remains the second tallest building in the Americas (after One World Trade Center) and the Western Hemisphere. With a pinnacle height of , it is the third tallest freestanding structure in the Americas, as it is shorter than Toronto's CN Tower. Willis Tower is the eighth-tallest freestanding structure in the world by pinnacle height.
At tall, including decorative spires, the Petronas Twin Towers in Kuala Lumpur, Malaysia, laid claim to replacing the Sears Tower as the tallest building in the world in 1998. Not everyone agreed, and in the ensuing controversy, four different categories of "tallest building" were created. Of these, Petronas was the tallest in the first category (height to top of architectural elements, meaning spires but not antennas) giving it the title of world's tallest building.
Taipei 101 in Taiwan claimed the record in three of the four categories in 2004 to become recognized as the tallest building in the world. Taipei 101 surpassed the Petronas Twin Towers in spire height and the Sears Tower in roof height and highest occupied floor. The Sears Tower retained one record: its antenna exceeded Taipei 101's spire in height. In 2008, the Shanghai World Financial Center claimed the records of tallest building by roof and highest occupied floor.
On August 12, 2007, the Burj Khalifa in Dubai, United Arab Emirates was reported by its developers to have surpassed the Sears Tower in all height categories.
Upon completion, One World Trade Center in New York City surpassed Willis Tower through its structural and pinnacle heights, but not by roof, observation deck elevation or highest occupied floor.
Until 2000, the Sears Tower did not hold the record for the tallest building by pinnacle height. From 1969 to 1978, this record was held by the John Hancock Center, whose antenna reached a height of , or taller than the Sears Tower's original height of . In 1978, One World Trade Center became taller by pinnacle height due to the addition of a antenna, which brought its total height to . In 1982, two antennas were installed on top of the Sears Tower which brought its total height to , making it taller than the John Hancock Center but not One World Trade Center. However, the extension of the Sears Tower's western antenna in June 2000 to allowed it to just barely claim the title of tallest building by pinnacle height.
Climbing.
On May 25, 1981, Dan Goodwin, wearing a homemade Spider-Man suit while using suction cups, camming devices, and sky hooks, and despite several attempts by the Chicago Fire Department to stop him, made the first successful outside ascent of the Sears Tower. Goodwin was arrested at the top after the seven-hour climb and charged with trespassing. Goodwin stated that the reason he made the climb was to call attention to shortcomings in high-rise rescue and firefighting techniques. After a lengthy interrogation by Chicago's District Attorney and Fire Commissioner, Goodwin was released.
In August 1999, French urban climber Alain "Spiderman" Robert, using only his bare hands and bare feet, scaled the building's exterior glass and steel wall all the way to the top. A thick fog settled in near the end of his climb, making the last 20 stories of the building's glass and steel exterior slippery.
Naming rights.
Although Sears sold the Tower in 1994 and had completely vacated it by 1995, the company retained the naming rights to the building through 2003. The new owners were rebuffed in renaming deals with CDW Corp in 2005 and the U.S. Olympic Committee in 2008. London-based insurance broker Willis Group Holdings, Ltd. leased more than of space on three floors in 2009. A Willis spokesman said the naming rights were obtained as part of the negotiations at no cost to Willis, and the building was renamed Willis Tower on July 16, 2009.
The naming rights are valid for 15 years, so it is possible that the building's name could change again in 2024 or later. The "Chicago Tribune" joked that the building's new name reminded them of the oft-repeated "What you talkin' 'bout, Willis?" catchphrase from the 1980s American television sitcom "Diff'rent Strokes" and considered the name-change ill-advised in "a city with a deep appreciation of tradition and a healthy ego, where some Chicagoans still mourn the switch from Marshall Field's to Macy's". This feeling was confirmed in a July 16, 2009 CNN article in which some Chicago area residents expressed reluctance to accept the Willis Tower name, and in an article that appeared in the October 2010 issue of "Chicago" magazine that ranked the building among Chicago's 40 most important, the author pointedly refused to acknowledge the name change and referred to the building as the "Sears Tower". "Time" magazine called the name change one of the top 10 worst corporate name changes and pointed to negative press coverage by local news outlets and online petitions from angry residents. The naming rights issue continued into 2013, when Eric Zorn noted in the "Chicago Tribune" that "We're stubborn about such things. This month marked four years since the former Sears Tower was re-christened Willis Tower, and the new name has yet to stick."
Broadcasting.
Many broadcast station transmitters are located at the top of Willis Tower. Each list is ranked by height from the top down. Stations at the same height on the same mast indicate the use of a diplexer into the same shared antenna. Due to its extreme height, FM stations (all class B) are very limited in power output.
Radio stations.
Also, NOAA Weather Radio station KWO39 transmits off the top of Willis Tower, at 162.550 MHz. KWO39, programmed by the National Weather Service Weather Forecast Office in Chicago, is equipped with Specific Area Message Encoding (SAME), which sets off a siren on specially-programmed weather radios to alert of an impending hazard, such as a tornado or civil emergency.
Cultural depictions.
Film and television.
The building has appeared in numerous films and television shows set in Chicago such as "Ferris Bueller's Day Off", where Ferris and company watch the streets of Chicago from the observation deck. The television show "Late Night with Conan O'Brien" introduced a character called The Sears Tower Dressed In Sears Clothing when the show visited Chicago in 2006. The building is also featured in History Channel's "Life After People", in which it and other human-made landmarks suffer from neglect without humans around, and it collapses two hundred years after people are gone. In an episode of the television series "Monk", Adrian Monk tries to conquer his fear of heights by imagining that he is on top of the Sears Tower. Also, in an episode of "Kenan and Kel", Kenan Rockmore and Kel Kimble decide to climb to the top of the Sears Tower, so that Kenan can declare his love for a girl.
In the movie "", the tower is damaged by a tornado.
In "1969", a Season 2 episode of the science-fiction series "Stargate SG-1", the SG-1 team accidentally travels back in time to the titular year. At one point, the team travels though Chicago and the Sears Tower is shown (erroneously, since construction did not begin on the tower until two years later in 1971).
In the 2004 film "I, Robot", the tower is shown updated in the year 2035 with new triangular antennas. The tower is shown surpassed in height by the USR (United States Robotics) Building.
In the 2011 film "", the tower is featured in a number of scenes. The most notable one is when the N.E.S.T team tries to enter the city using V-22 Osprey helicopters. They use Willis Tower for cover before using wing suits to descend into the city streets. In the movie, the tower is shown to be severely damaged by the Decepticon invasion of the city.
In the 2013 film "Man of Steel", the tower's interior and parts of its exterior portrayed the offices of the "Daily Planet".
In the 2015 film "Jupiter Ascending", the tower is featured prominently as the place where Caine and Jupiter await a spaceship to lift them off the planet.
In the film "Divergent", the tower is shown abandoned and decayed in a future Chicago.
Other.
Older versions of "Microsoft Flight Simulator" would begin with the player on the runway of Meigs Field, facing a virtual version of the tower.
In Sufjan Stevens' 2005 album "Illinois", the tower is referenced in the track "Seer's Tower", whose title is a play on the tower's now-former name, Sears Tower.

</doc>
<doc id="28177" url="https://en.wikipedia.org/wiki?curid=28177" title="Simony">
Simony

Simony ( or ) is the act of selling church offices and roles. The practice is named after Simon Magus, who is described in the Acts of the Apostles 8:9–24 as having offered two disciples of Jesus, Peter and John, payment in exchange for their empowering him to impart the power of the Holy Spirit to anyone on whom he would place his hands. The term also extends to other forms of trafficking for money in "spiritual things". Simony was also one of the important issues during the Investiture Controversy.
History.
Catholic church.
Although an offense against canon law, Simony became widespread in the Catholic Church in the 9th and 10th centuries. In the canon law, the word bears a more extended meaning than in English law. "Simony according to the canonists", says John Ayliffe in his "Parergon",
In the "Corpus Juris Canonici" the Decretum and the Decretals deal with the subject. The offender whether "simoniacus" (one who had bought his orders) or "simoniace promotus" (one who had bought his promotion), was liable to deprivation of his benefice and deposition from orders if a secular priest, or to confinement in a stricter monastery if a regular. No distinction seems to have been drawn between the sale of an immediate and of a reversionary interest. The innocent "simoniace promotus" was, apart from dispensation, liable to the same penalties as though he were guilty.
Certain matters were simoniacal by the canon law which would not be so regarded in English law. So grave was the crime of simony considered that even infamous persons (deprived of citizens' rights due to conviction) could accuse another of it. English provincial and legatine constitutions continually assailed simony.
Church of England.
The Church of England also struggled with the practice after its separation from Rome. For the purposes of English law, simony is defined by William Blackstone as the corrupt presentation of any person to an ecclesiastical benefice for money, gift or reward. While English law recognized simony as an offence, it treated it as merely an ecclesiastical matter, rather than a crime, for which the punishment was forfeiture of the office or any advantage from the offence and severance of any patronage relationship with the person who bestowed the office. Both Edward VI of England and Elizabeth I promulgated statutes against simony. The cases of Bishop of St. David's Thomas Watson in 1699 and of Dean of York William Cockburn in 1841 were particularly notable.
By the Benefices Act 1892, a person guilty of simony is guilty of an offence for which he may be proceeded against under the Clergy Discipline Act 1892. An innocent clerk is under no disability, as he might be by the canon law. Simony may be committed in three ways – in promotion to orders, in presentation to a benefice, and in resignation of a benefice. The common law (with which the canon law is incorporated, as far as it is not contrary to the common or statute law or the prerogative of the Crown) has been considerably modified by statute. Where no statute applies to the case, the doctrines of the canon law may still be of authority.
, simony remains an offence. An unlawfully bestowed office can be declared void by the Crown, and the offender can be disabled from making future appointments and fined up to £1000. Clergy are no longer required to make a declaration as to simony on ordination, but offences are now likely to be dealt with under the Clergy Discipline Measure 2003, r.8.

</doc>
<doc id="28178" url="https://en.wikipedia.org/wiki?curid=28178" title="September 26">
September 26


</doc>
<doc id="28179" url="https://en.wikipedia.org/wiki?curid=28179" title="Samaritans">
Samaritans

The Samaritans (Samaritan Hebrew: שוֹמְרִים "Samerim" "Guardians/Keepers/Watchers the Law/Torah", Jewish "Shomronim", "Sameriyyun") are an ethnoreligious group of the Levant, originating from the Israelites or Hebrews of the Ancient Near East.
The Samaritans are adherents of Samaritanism, an Abrahamic religion closely related to Judaism. Samaritans believe that their worship, which is based on the Samaritan Pentateuch, is the true religion of the ancient Israelites from before the Babylonian Exile, preserved by those who remained in the Land of Israel, as opposed to Judaism, which they see as a related but altered and amended religion, brought back by those returning from the Babylonian exile.
Ancestrally, Samaritans claim descent from the Israelite tribes of Ephraim and Manasseh (two sons of Joseph) as well as from the priestly tribe of Levi, who have links to ancient Samaria from the period of their entry into the land of Canaan, while some suggest that it was from the beginning of the Babylonian Exile up to the Samaritan polity of Baba Rabba. Samaritans used to include a line of Benjamin tribe, but it became extinct during the decline period of the Samaritan demographics. The split between them and the Judeans began during the time of Eli the priest when, according to Samaritan tradition, Judeans split off from the central Israelite tradition.
The Samaritans believe that Mount Gerizim was the original Holy Place of Israel from the time that Joshua conquered Israel. The major issue between Rabbinical Jews (Jews who follow post-exile rabbinical interpretations of Judaism, who are the vast majority of Jews today) and Samaritans has always been the location of the chosen place to worship God; Jerusalem according to the Jewish faith or Mount Gerizim according to the Samaritan faith.
In the Talmud, a central post-exilic religious text of Judaism, the Samaritans are called "Cutheans" (, "Kutim"), referring to the ancient city of Kutha, geographically located in what is today Iraq. In the biblical account, however, Cuthah was one of several cities from which people were brought to Samaria, and they worshiped Nergal. Modern genetics partially supports both the claims of the Samaritans and the account in the Talmud, suggesting that the genealogy of the Samaritans lies in some combination of these two accounts.
Once a large community of over a million in late Roman times, the Samaritans shrank to several tens of thousands in the wake of the bloody suppression of the Third Samaritan Revolt (529 CE) against the Byzantine Christian rulers and mass conversion to Christianity under Byzantine rulers and to Islam under hundreds of years of Arab and Turkish rulers.
As of January 1, 2015, the population was 777, divided between Kiryat Luza on Mount Gerizim and the city of Holon, just outside Tel Aviv. Most Samaritans in Israel and the West Bank today speak Hebrew and Arabic. For liturgical purposes, Samaritan Hebrew, Samaritan Aramaic, and Samaritan Arabic are used, all written in the Samaritan alphabet, a variant of the Old Hebrew alphabet, which is distinct from the Hebrew alphabet. Hebrew and later Aramaic were languages in use by the Jewish and Samaritan inhabitants of Judea prior to the Roman exile.
Although they are drafted into the Israel Defense Forces and considered by Rabbinical Judaism to be a branch of Jews, the Chief Rabbinate of Israel requires Samaritans to officially go through formal Orthodox conversion in order to be recognized as Halakhic Jews in Israel. One example is Israeli TV personality Sofi Tsedaka, who formally converted to Judaism at the age of 18.
Etymology.
There is conflict over the etymology of the name for the Samaritans in Hebrew, stemming from the fact that they are referred to differently in different dialects of Hebrew. This has accompanied controversy over whether the Samaritans are named after the geographic area of Samaria (in the northern West Bank), or whether the area received its name from the group. This distinction is controversial in part because different interpretations can be used to justify or deny claims of ancestry over this region, which has been deeply contested in modern times.
In Samaritan Hebrew the Samaritans call themselves "Samerim", which according to the Anchor Bible Dictionary, is derived from the Ancient Hebrew term "Šamerim/Samerim" שַמֶרִים, meaning "Guardians/Keepers/Watchers the Law/Torah".
The Ancient Hebrew "Šomerim" ("Samerin" سامرين in Arabic which have the same meaning), which in the Bible means "Guardians" (singular Šomer) comes from the Hebrew root verb S-M-R שמר which means: "to watch", or "to guard".
Historically, Samaria was the key geographical concentration of the Samaritan community. Thus, it may suggest the region of Samaria is named after the Samaritans, rather than the Samaritans being named after the region. In Jewish tradition, however, it is sometimes claimed that Mount Samaria, meaning "Watch Mountain", is actually named so because watchers used to watch from those mountains for approaching armies from Egypt in ancient times.. In modern Jewish Hebrew, the Samaritans are called "Shomronim", which would appear to simply mean ("inhabitants of Samaria", Samaria in Jewish Hebrew being Shomron). This is a politically sensitive distinction.
That the etymology of the Samaritans' ethnonym in Samaritan Hebrew is derived from "Guardians/Keepers/Watchers the Law/Torah", as opposed to Samaritans being named after the region of Samaria, has in history been supported by a number of Christian Church fathers, including Epiphanius of Salamis in Panarion, Jerome and Eusebius in Chronicon and Origen in The Commentary of Origen on S. John's Gospel, and in some Ancient Jewish Talmudic Bible Interpretations of Midrash Tanhuma on Genesis chapter 31, and Pirke De-Rabbi Eliezer chapter 38 Page 21.
History and origin.
Samaritan sources.
According to Samaritan tradition, Mount Gerizim was the original Holy Place of the Israelites from the time that Joshua conquered Canaan and the tribes of Israel settled the land. The reference to Mount Gerizim derives from the biblical story of Moses ordering Joshua to take the Twelve Tribes of Israel, to the mountains by Nablus and place half of the tribes, six in number, on the top of Mount Gerizim, the Mount of the Blessing, and the other half in Mount Ebal, the Mount of the Curse. The two mountains were used to symbolize the significance of the commandments and serve as a warning to whoever disobeyed them (Deut. 11:29; 27:12; Josh. 8:33).
Samaritans claim they are Israelite descendants of the Northern Israelite tribes of Ephraim and Manasseh, who survived the destruction of the Kingdom of Israel (Samaria) by the Assyrians in 722 BCE.
Samaritan historiography places the basic schism from the remaining part of Israel after the tribes of Israel conquered and returned to the land of Canaan, led by Joshua. After Joshua's death, Eli the priest left the tabernacle which Moses erected in the desert and established on Mount Gerizim, and built another one under his own rule in the hills of Shiloh.
Abu l-Fath, who in the 14th century wrote a major work of Samaritan history, comments on Samaritan origins as follows:
Further, the "Samaritan Chronicle Adler", or New Chronicle, believed to have been composed in the 18th century using earlier chronicles as sources states:
Jewish sources.
The emergence of the Samaritans as an ethnic and religious community distinct from other Levant peoples appears to have occurred at some point after the Assyrian conquest of the Israelite Kingdom of Israel in approximately 721 BCE. The records of Sargon II of Assyria indicate that he deported 27,290 inhabitants of the former kingdom.
Jewish tradition affirms the Assyrian deportations and replacement of the previous inhabitants by forced resettlement by other peoples, but claims a different ethnic origin for the Samaritans. The Talmud accounts for a people called "Cuthim" on a number of occasions, mentioning their arrival by the hands of the Assyrians. According to 2 Kings and Josephus the people of Israel were removed by the king of the Assyrians (Sargon II) to Halah, to Gozan on the Khabur River and to the towns of the Medes. The king of the Assyrians then brought people from Babylon, Cuthah, Avah, Emath, and Sepharvaim to place in Samaria. Because God sent lions among them to kill them, the king of the Assyrians sent one of the priests from Bethel to teach the new settlers about God's ordinances. The eventual result was that the new settlers worshipped both the God of the land and their own gods from the countries from which they came.
This account is contradicted by the version in Chronicles, where, following Samaria's destruction, King Hezekiah is depicted as endeavouring to draw the Ephraimites and Manassites closer to Judah. Temple repairs at the time of Josiah were financed by money from all "the remnant of Israel" in Samaria, including from Manasseh, Ephraim and Benjamin. Jeremiah likewise speaks of people from Shechem, Shiloh and Samaria who brought offerings of frankincense and grain to the house of the Lord. Chronicles makes no mention of an Assyrian resettlement. Yitzakh Magen argues that the version of Chronicles is perhaps closer to the historical truth, and that the Assyrian settlement was unsuccessful, a notable Israelite population remained in Samaria, part of which, following the conquest of Judah, fled south and settled there as refugees.
A Midrash (Genesis Rabbah Sect. 94) relates about an encounter between Rabbi Meir and a Samaritan. The story that developed includes the following dialogue:
Zertal dates the Assyrian onslaught at 721 BCE to 647 BCE and discusses three waves of imported settlers. He shows that Mesopotamian pottery in Samaritan territory cluster around the lands of Menasheh and that the type of pottery found was produced around 689 BCE. Some date their split with the Jews to the time of Nehemiah, Ezra, and the building of the Second Temple in Jerusalem after the Babylonian exile. Returning exiles considered the Samaritans to be non-Israelites and, thus, not fit for this religious work.
The "Encyclopaedia Judaica" (under "Samaritans") summarizes both past and the present views on the Samaritans' origins. It says:
Furthermore, to this day the Samaritans claim descent from the tribe of Joseph:
Dead Sea scrolls.
The Dead Sea scroll 4Q372 hopes that the northern tribes will return to the land of Joseph. The current dwellers in the north are referred to as fools, an enemy people. However they are not referred to as foreigners. It goes on to say that the Samaritans mocked Jerusalem and built a temple on a high place to provoke Israel.
Tensions between the Samaritans and the Judeans.
The narratives in Genesis about the rivalries among the twelve sons of Jacob describe tensions between north and south. They were temporarily united in the United Monarchy, but after the death of Solomon the kingdom split in two, the Kingdom of Israel with its capital Samaria and the Kingdom of Judea with its capital Jerusalem.
The Deuteronomistic history, written in Judah, portrayed Israel as a sinful kingdom, divinely punished for its idolatry and iniquity by being destroyed by the Assyrians in 720 BCE.
The tensions continued in the postexilic period. Chronicles is more inclusive than Ezra–Nehemiah since for the Chronicler the ideal is of one Israel with twelve tribes; the Chronicler concentrates on Judah and ignores northern Israel.
The Samaritans claimed that they were the true Israel who were descendants of the "Ten Lost Tribes" taken into Assyrian captivity. They had their own temple on Mount Gerizim and claimed that it was the original sanctuary. Moreover, they claimed that their version of the Pentateuch was the original and that the Jews had a falsified text produced by Ezra during the Babylonian exile.
Both Jewish and Samaritan religious leaders taught that it was wrong to have any contact with the opposite group, and neither was to enter each other's territories or even to speak to one another. During the New Testament period, although the tensions went unrecognized by Roman authorities, Josephus reports numerous violent confrontations between Jews and Samaritans throughout the first half of the first century.
Rejection by Judeans.
According to the Jewish version of events, when the Judean exile ended in 538 BCE and the exiles began returning home from Babylon, they found their former homeland populated by other people who claimed the land as their own and Jerusalem, their former glorious capital, in ruins. The inhabitants worshiped the Pagan gods, but when the then-sparsely populated areas became infested with dangerous wild beasts, they appealed to the king of Assyria for Israelite priests to instruct them on how to worship the "God of that country." The result was a syncretistic religion, in which national groups worshiped the Hebrew God, but they also served their own gods in accordance with the customs of the nations from which they had been brought.
According to , the Persian emperor, Cyrus the Great (reigned 559–530 BCE), permitted the return of the exiles to their homeland and ordered the rebuilding of the Temple in Jerusalem (Zion). The prophet Isaiah identified Cyrus as "the Lord's Messiah" (Mashiach; see ). The word "Messiah" refers to an anointed one, such as a king or priest.
The text is not clear on this matter, but one possibility is that these "people of the land" were thought of as Samaritans. We do know that Samaritan and Jewish alienation increased, and that the Samaritans eventually built their own temple on Mount Gerizim, near Shechem.
The rebuilding of the Jewish Temple in Jerusalem took several decades. The project was first led by Sheshbazzar (ca. 538 BCE), later by Zerubbabel and Jeshua, and later still by Haggai and Zechariah (520–515 BCE). The work was completed in 515 BCE.
The term "Cuthim" applied by Jews to the Samaritans had clear pejorative connotations, implying that they were interlopers brought in from Kutha in Mesopotamia and rejecting their claim of descent from the ancient Tribes of Israel.
Assyrian account of the conquest and settlement of Samaria.
However, the following account of the Assyrian kings, which was among the archaeological discoveries in Babylon, differs from the Samaritan account, and confirms much of the Jewish biblical account but may differ in regard to the ethnicity of the foreigners settled in Samaria by Assyria. At one point it is simply said that they were from Arabia, while at another, that they were brought from a number of countries conquered by Sargon II:
Also,
Further history.
Temple on Mount Gerizim.
Archaeological excavations at Mount Gerizim indicate that a Samaritan temple was built there in the first half of the 5th century BC. The date of the schism between Samaritans and Jews is unknown, but by the early 4th century BCE the communities seem to have had distinctive practices and communal separation.
According to Samaritans, it was on Mount Gerizim that Abraham was commanded by God to offer Isaac, his son, as a sacrifice . In both narratives, God then causes the sacrifice to be interrupted, explaining that this was the ultimate test of Abraham's obedience, as a result of which all the world would receive blessing.
The Torah mentions the place where God shall choose to establish His name (Deut 12:5), and Judaism takes this to refer to Jerusalem. However, the Samaritan text speaks of the place where God "has chosen" to establish His name, and Samaritans identify it as Mount Gerizim, making it the focus of their spiritual values.
The legitimacy of the Samaritan temple was attacked by Jewish scholars including Andronicus ben Meshullam.
In the Christian Bible, the Gospel of John relates an encounter between a Samaritan woman and Jesus in which she says that the mountain was the center of their worship .
Antiochus IV Epiphanes and Hellenization.
In the 2nd century BCE a series of events led to a revolution of some Judeans against Antiochus IV.
Antiochus IV Epiphanes was on the throne of the Seleucid Empire from 175 to 163 BCE. His policy was to Hellenize his entire kingdom and standardize religious observance. According to 1 Maccabees 1:41-50 he proclaimed himself the incarnation of the Greek god Zeus and mandated death to anyone who refused to worship him.
The universal peril led the Samaritans, eager for safety, to repudiate all connection and kinship with the Jews. The request was granted. This was put forth as the final breach between the two groups, being alleged at a much later date in the Christian Bible (John 4:9), "For Jews have no dealings with Samaritans"—or not "alleged" if the Greek sunchrasthai merely refers to not sharing utensils (NABRE).
Anderson notes that during the reign of Antiochus IV (175–164 BCE):
Josephus Book 12, Chapter 5 quotes the Samaritans as saying:
This Samaritan Temple at Mount Gerizim was destroyed by John Hyrcanus in about 128 BC, having existed about 200 years. Only a few stone remnants of it exist today.
164 BCE and after.
During the Hellenistic period, Samaria was largely divided between a Hellenizing faction based in Samaria (Sebastaea) and a pious faction, led by the High Priest and based largely around Shechem and the rural areas. Samaria was a largely autonomous state nominally dependent on the Seleucid Empire until around 129 BCE, when the Jewish Hasmonean king Yohanan Girhan (John Hyrcanus) destroyed the Samaritan temple and devastated Samaria.
Roman period.
Under the Roman Empire, Samaria was a part of the Roman-ruled province of Judaea.
Samaritans appear briefly in the Christian gospels, most notably in the account of the Samaritan woman at the well and the parable of the Good Samaritan. In the latter it is only the Samaritan who helped the man stripped of clothing, beaten, and left on the road half dead, his Abrahamic covenantal circumcision implicitly evident. The priest and Levite walked past. But the Samaritan helped the naked man regardless of his nakedness (itself religiously offensive to the priest and Levite), his self-evident poverty, or to which Hebrew sect he belonged (which was unclear to any, due to his nakedness). 
This period is considered as something of a golden age for the Samaritan community, the population thought to number up to a million. The Temple of Gerizim was rebuilt after the Bar Kochba revolt against the Romans, around 135 CE. Much of Samaritan liturgy was set by the high priest Baba Rabba in the 4th century.
A building excavated on Delos, dating to the 2nd century BCE, is commonly identified as a Samaritan synagogue, which would make it the oldest known Jewish or Samaritan synagogue. On the other hand, Matassa argues that, although there is evidence of Samaritans on Delos, there is no evidence the building was a synagogue.
There were some Samaritans in the Persian Empire, where they served in the Sassanid army.
Byzantine times.
According to Samaritan sources, Eastern Roman Emperor Zeno (who ruled 474-491 and whom the sources call "Zait the King of Edom") persecuted the Samaritans. The Emperor went to Sichem ("Neapolis"), gathered the elders and asked them to convert; when they refused, Zeno had many Samaritans killed, and re-built the synagogue to a church. Zeno then took for himself Mount Gerizim, where the Samaritans worshipped God, and built several edifices, among whom a tomb for his recently deceased son, on which he put a cross, so that the Samaritans, worshipping God, would prostrate in front of the tomb. Later, in 484, the Samaritans revolted. The rebels attacked Sichem, burnt five churches built on Samaritan holy places and cut the finger of bishop Terebinthus, who was officiating the ceremony of Pentecost. They elected a Justa (or Justasa/Justasus) as their king and moved to Caesarea, where a noteworthy Samaritan community lived. Here several Christians were killed and the church of St. Sebastian was destroyed. Justa celebrated the victory with games in the circus. According to John Malalas, the "dux Palaestinae" Asclepiades, whose troops were reinforced by the Caesarea-based Arcadiani of Rheges, defeated Justa, killed him and sent his head to Zeno. According to Procopius, Terebinthus went to Zeno to ask for revenge; the Emperor personally went to Samaria to quell the rebellion.
Modern historians believe that the order of the facts preserved by Samaritan sources should be inverted, as the persecution of Zeno was a consequence of the rebellion rather than its cause, and should have happened after 484, around 489. Zeno rebuilt the church of St. Procopius in Neapolis (Sichem) and the Samaritans were banned from Mount Gerizim, on whose top a signalling tower was built to alert in case of civil unrest.
Under a charismatic, messianic figure named Julianus ben Sabar (or ben Sahir), the Samaritans launched a war to create their own independent state in 529. With the help of the Ghassanid Arabs, Emperor Justinian I crushed the revolt; tens of thousands of Samaritans died or were enslaved. The Samaritan faith was virtually outlawed thereafter by the Christian Byzantine Empire; from a population once at least in the hundreds of thousands, the Samaritan community dwindled to near extinction.
After the Muslim Conquests.
By the time of the Muslim Conquests, Samaritans were living in an area stretching between Egypt, Syria, and Iran. Like other non-Muslims in the empire, such as Jews, Samaritans were considered to be People of the Book.
Their minority status was protected by the Muslim rulers, and they had the right to practice their religion, but, as dhimmi, adult males had to pay the jizya or "protection tax".
It has been suggested that they were forced to wear red colored turbans as a result of the terms of a document known as the Pact of Umar II, but this stipulation is not explicitly mentioned in the document, the authenticity has been questioned by contemporary scholars, and the tradition cannot be independently verified.
During the Crusades, Samaritans, like the non-Latin Christian inhabitants of the Kingdom of Jerusalem, were second-class citizens, but they were tolerated and perhaps favoured because they were docile and had been mentioned positively in the Christian New Testament.
Over the centuries of Byzantine, Arab and Turkish rule, the Samaritans suffered many hardships which included forced conversion to Christianity, forced conversion to Islam, harsh religious decrees, massacre and persecution.
While the majority of the Samaritan population in Damascus was killed or converted during the reign of the Ottoman Pasha Mardam Beq in the early 17th century, the remainder of the Samaritan communities from Damascus and the other cities where they had a presence moved to Shechem, due to its close proximity to Mount Gerizim.
The Shechem community endured because most of the surviving diaspora returned, and they have maintained a tiny presence there to this day. In 1624, the last Samaritan High Priest of the line of Eleazar son of Aaron died without issue, but descendants of Aaron's other son, Ithamar, remained and took over the office.
The situation of the Samaritan community improved significantly during the British Mandate of Palestine. At that time, they began to work in the public sector, like many other groups. The censuses of 1922 and 1931 recorded 163 and 182 Samaritans in Palestine, respectively. The majority of them lived in Nablus.
Samaritan origins of Palestinian Muslims in Nablus.
Much of the local Palestinian population of Nablus is believed to be descended from Samaritans who converted to Islam. According to the historian Fayyad Altif, large numbers of Samaritans converted due to persecution under various Muslim rulers, and because the monotheistic nature of Islam made it easy for them to accept it. The Samaritans themselves describe the Ottoman period as the worst period in their modern history, as many Samaritan families were forced to convert their religion to Islam during that time. Even today, certain Nabulsi family names such as Maslamani, Yaish, and Shaksheer among others, are associated with Samaritan ancestry.
For the Samaritans in particular, the passing of the al-Hakem Edict by the Fatimids in 1021, under which all Jews and Christians in the Fatimid ruled southern Levant were ordered to either convert to Islam or leave, along with another notable forced conversion to Islam imposed at the hands of the rebel Ibn Firāsa, would contribute to their rapid unprecedented decrease, and ultimately almost complete extinction as a separate religious community. As a result, they have decreased from more than a million in late Roman (Byzantine) times to 150 people by the end of the Ottoman Era.
In 1940, the future Israeli president and historian Yitzhak Ben-Zvi wrote an article in which he stated that two thirds of the residents of Nablus and the surrounding neighboring villages are of Samaritan origin. He mentioned the name of several Palestinian Muslim families as having Samaritan origins, including the Buwarda and Kasem families, who protected Samaritans from Muslim persecution in the 1850s. He further claimed that these families had written records testifying to their Samaritan ancestry, which were maintained by their priests and elders.
Genetic studies.
Demographic investigation.
Demographic investigations of the Samaritan community were carried out in the 1960s. Detailed pedigrees of the last 13 generations show that the Samaritans comprise four lineages:
Y-DNA and mtDNA comparisons.
Recently several genetic studies on the Samaritan population were made using haplogroup comparisons as well as wide-genome genetic studies. Of the 12 Samaritan males used in the analysis, 10 (83%) had Y chromosomes belonging to haplogroup J, which includes three of the four Samaritan families. The Joshua-Marhiv family belongs to haplogroup J1, while the Danfi and Tsedakah families belong to haplogroup J2, and can be further distinguished by M67, the derived allele of which has been found in the Danfi family. The only Samaritan family not found in haplogroup J was the Cohen family (Tradition: Tribe of Levi) which was found in haplogroup E3b1a M78. This article predated the change of the classification of haplogroup E3b1-M78 to E3b1a-M78 and the further subdivision of E3b1a-M78 into 6 subclades based on the research of Cruciani, et al.
The 2004 article on the genetic ancestry of the Samaritans by Shen "et al." concluded from a sample comparing Samaritans to several Jewish populations, all currently living in Israel—representing Ethiopian Jews, Ashkenazi Jews, Iraqi Jews, Libyan Jews, Moroccan Jews, and Yemenite Jews, as well as Israeli Druze and Israeli Arabs—that "the principal components analysis suggested a common ancestry of Samaritan and Jewish patrilineages. Most of the former may be traced back to a common ancestor in what is today identified as the paternally inherited Israelite high priesthood (Cohanim) with a common ancestor projected to the time of the Assyrian conquest of the kingdom of Israel."
Archaeologists Aharoni, et al., estimated that this "exile of peoples to and from Israel under the Assyrians" took place during ca. 734–712 BCE. The authors speculated that when the Assyrians conquered the northern kingdom of Israel, resulting in the exile of many of the Israelites, a subgroup of the Israelites that remained in the Land of Israel "married Assyrian and female exiles relocated from other conquered lands, which was a typical Assyrian policy to obliterate national identities." The study goes on to say that "Such a scenario could explain why Samaritan Y chromosome lineages cluster tightly with Jewish Y lineages, while their mitochondrial lineages are closest to Iraqi Jewish and Israeli Arab mtDNA sequences." Non-Jewish Iraqis were not sampled in this study; however, mitochondrial lineages of Jewish communities tend to correlate with their non-Jewish host populations, unlike paternal lineages which almost always correspond to Israelite lineages.
Modern times.
As of January 1, 2012, there were 751 Samaritans, half of whom reside in their modern homes at Kiryat Luza on Mount Gerizim, which is sacred to them, and the rest in the city of Holon, just outside Tel Aviv. There are also four Samaritan families residing in Binyamina-Giv'at Ada, Matan and Ashdod.
After the end of the British Mandate of Palestine and the subsequent establishment of the State of Israel, some of the Samaritans who were living in Jaffa emigrated to the West Bank and lived in Nablus. But by the late 1950s, around 100 Samaritans left the West Bank for Israel under an agreement with the Jordanian authorities in the West Bank.
Until the 1980s, most of the Samaritans resided in the Samarian town of Nablus below Mount Gerizim. They relocated to the mountain itself near the Israeli settlement neighborhood of Har Brakha as a result of violence during the First Intifada (1987–1990). Consequently, all that is left of the Samaritan community in Nablus/Shechem itself is an abandoned synagogue. The Israeli army maintains a presence in the area.
As a small community physically divided between neighbors in a hostile region, Samaritans have been hesitant overtly to take sides in the Arab–Israeli conflict, fearing that doing so could lead to negative repercussions. While the Samaritan communities in both the West Bank's Nablus and Israeli Holon have assimilated to the surrounding culture, Hebrew has become the primary domestic language for Samaritans. Samaritans who are Israeli citizens are drafted into the military, along with the Jewish citizens of Israel.
Relations of Samaritans with Jewish Israelis and Muslim and Christian Palestinians in neighboring areas have been mixed. In 1954, Israeli President Yitzhak Ben-Zvi fostered a Samaritan enclave in Holon, Israel. Samaritans living in both Israel and in the West Bank enjoy Israeli citizenship. Samaritans in the Palestinian Authority-ruled territories are a minority in the midst of a Muslim majority. They had a reserved seat in the Palestinian Legislative Council in the election of 1996, but they no longer have one. Samaritans living in the West Bank have been granted passports by both Israel and the Palestinian Authority.
Survival.
One of the biggest problems facing the community today is the issue of continuity. With such a small population, divided into only four families (Cohen, Tsedakah, Danfi and Marhib, a fifth family died out in the twentieth century) and a general refusal to accept converts, there has been a history of genetic disease within the group due to the small gene pool. To counter this, the Samaritan community has recently agreed that men from the community may marry non-Samaritan (primarily, Israeli Jewish) women, provided that the women agree to follow Samaritan religious practices. There is a six-month trial period prior to officially joining the Samaritan community to see whether this is a commitment that the woman would like to take. This often poses a problem for the women, who are typically less than eager to adopt the strict interpretation of biblical (Levitical) laws regarding menstruation, by which they must live in a separate dwelling during their periods and after childbirth. There have been a few instances of intermarriage. In addition, all marriages within the Samaritan community are first approved by a geneticist at Tel HaShomer Hospital, in order to prevent the spread of genetic disease. In meetings arranged by "international marriage agencies", a small number of Ukrainian women have recently been allowed to marry into the community in an effort to expand the gene pool.
The head of the community is the Samaritan High Priest, who is selected by age from the priestly family, and resides on Mount Gerizim. The current high priest is Aabed-El ben Asher ben Matzliach who assumed the office in 2013.
Samaritanism.
The Samaritan religion is based on some of the same books used as the basis of rabbinic Judaism, but differs from the latter. Samaritan scriptures include the Samaritan version of the Torah, the Memar Markah, the Samaritan liturgy, and Samaritan law codes and biblical commentaries. Samaritans appear to have texts of the Torah as old as the Masoretic Text and the Septuagint; scholars have various theories concerning the actual relationships between these three texts.
Religious beliefs.
The Samaritans retained the Ancient Hebrew script, the high priesthood, animal sacrifices, the eating of lambs at Passover, and the celebration of Aviv in spring as the New Year. Yom Teruah (the biblical name for Rosh Hashanah), at the beginning of Tishrei, is not considered a new year as it is in Judaism. Their main Torah text differs from the Masoretic Text, as well. Some differences are doctrinal: for example, the Samaritan Torah explicitly states that Mount Gerizim is "the place that God "has chosen"" for the Temple, as opposed to the Jewish Torah that refers to "the place that God "will" choose". Other differences are minor and seem more or less accidental.
Relationship to rabbinic Judaism.
Samaritans refer to themselves as "Bene Yisrael" ("Children of Israel") which is a term used by all Jewish denominations as a name for the Jewish people as a whole. They however do not refer to themselves as "Yehudim" (Judeans), the standard Hebrew name for Jews.
The Talmudic attitude expressed in tractate Kutim is that they are to be treated as Jews in matters where their practice coincides with rabbinic Judaism but as non-Jews where their practice differs. Since the 19th century, rabbinic Judaism has regarded the Samaritans as a Jewish sect and the term "Samaritan Jews" has been used for them.
Religious texts.
Samaritan law is not the same as halakha (Rabbinical Jewish law). The Samaritans have several groups of religious texts, which correspond to Jewish halakhah. A few examples of such texts are:
Christian sources: New Testament.
Samaria or Samaritans are mentioned in the New Testament books of Matthew, Luke, John and Acts. The Gospel of Mark contains no mention of Samaritans or Samaria. The best known reference to the Samaritans is the Parable of the Good Samaritan, found in the Gospel of Luke. The following references are found:
The rest of the New Testament makes no specific mention of Samaria or Samaritans.
Media.
"The Samaritan News", a monthly magazine started in 1969, is written in Samaritan, Hebrew, Arabic, and English and deals with current and historical issues with which the Samaritan community is concerned. The "Samaritan Update" is a bi-monthly e-newsletter for Samaritan Studies.
External links.
Samaritan view
Jewish view
Independent views
Books and other information
Photographic links

</doc>
<doc id="28180" url="https://en.wikipedia.org/wiki?curid=28180" title="Seneca Lake (New York)">
Seneca Lake (New York)

Seneca Lake is the largest of the glacial Finger Lakes of the U.S. state of New York, and the deepest lake entirely within the state. It is promoted as being the lake trout capital of the world, and is host of the National Lake Trout Derby. Because of its depth and relative ease of access, the US Navy uses Seneca Lake to perform test and evaluation of equipment ranging from single element transducers to complex sonar arrays and systems.
The lake takes its name from the Seneca nation of Native Americans. At the north end of Seneca Lake is the city of Geneva, New York, home of Hobart and William Smith Colleges and the New York State Agricultural Experiment Station, a division of Cornell University. At the south end of the lake is the village of Watkins Glen, New York, famed for auto racing and waterfalls.
Due to Seneca Lake's unique macroclimate it is home to over 50 wineries, many of them farm wineries and is the location of the Seneca Lake AVA. (See Seneca Lake wine trail).
Description.
At long, it is the second longest of the Finger Lakes and has the largest volume, estimated at , roughly half of the water in all the Finger Lakes. It has an average depth of , a maximum depth of , and a surface area of .
For comparison, Scotland's famous Loch Ness is long, wide, has a surface area of , an average depth of , a maximum depth of , and total volume of of water.
Seneca's two main inlets are Catharine Creek at the southern end and the Keuka Lake Outlet. Seneca Lake lets out into the Cayuga-Seneca Canal, which joins Seneca and Cayuga Lakes at their northern ends.
It is fed by underground springs and replenished at a rate of 328,000 gallons (1240 m³) per minute. These springs keep the water moving in a circular motion, giving it little chance to freeze over. Because of Seneca Lake's great depth its temperature remains a near-constant . In summer the top warms to .
Ecology.
Seneca lake has a typical aquatic population for large deep lakes in the northeast, with coldwater fish such as lake trout and Atlantic salmon inhabiting the deeper waters, and warmwater fish such as smallmouth bass and yellow perch inhabiting the shallower areas. The lake is also home to a robust population of "sawbellies", the local term for gizzard shad.
History.
Over 200 years ago, there were Iroquois villages on Seneca Lake's surrounding hillsides. During the American Revolutionary War, their villages, including Kanadaseaga ("Seneca Castle"), were wiped out during the 1779 Sullivan Expedition by Continental troops under order by General George Washington to invade their homeland, destroy their dwellings and crops, and end their threat to the patriots. They destroyed nearly 50 Seneca and Cayuga villages. Today roadside signs trace Sullivan and Clinton's route along the east side of Seneca Lake where the burning of villages and crops occurred.
After the war, the Iroquois were forced to cede their land when Britain was defeated. Their millions of acres were sold and some lands in this area were granted to veterans of the army in payment for their military service. A slow stream of European-American settlers began to arrive circa 1790. Initially the settlers were without a market nearby or a way to get their crops to market. The settlers' isolation ended in 1825 with the opening of the Erie Canal.
The canal linked the Finger Lakes Region to the outside world. Steamships, barges and ferries quickly became Seneca Lake's ambassadors of commerce and trade. The former, short Crooked Lake Canal linked Seneca Lake to Keuka Lake.
Numerous canal barges sank during operations and rest on the bottom of the lake. A collection of barges at the southwest end of the lake, near the village of Watkins Glen, is being preserved and made accessible for scuba diving by the Finger Lakes Underwater Preserve Association.
Painted rocks.
The painted rocks located at the southern end of the lake on the eastern cliff face depict an American flag, Tee-pee, and several Native Americans. The older paintings, located on the bottom of the cliff, were said to have been drawn in 1779 after the Senecas escaped men from John Sullivan's campaign. However, this account is questioned by historian Barbara Bell, arguing that it is unlikely that the Senecas would have returned to paint the paintings having just escaped from Sullivan's men. She suggests instead that these paintings may have been made much later, for tourists on Seneca Lake boat tours.
It is known that the more visible and prominent paintings of the Native Americans, American flag, and Tee-pee were added in 1929 during the Sullivan Sesquicentennial. There are two mistakes in these 1929 additions: firstly the Native Americans in the Seneca Region used longhouses and not Tee-pees, and secondly the flag is displayed pointing to the left which is never to be done on a horizontal surface.
Seneca Guns.
Seneca Lake is also the site of strange and currently unexplained cannon-like booms and shakes that are heard and felt in the surrounding area. They are known locally as the Seneca Guns, Lake Drums, or Lake Guns, and these types of phenomena are known elsewhere as skyquakes. The term Lake Guns originated in the short story "The Lake Gun" by James Fenimore Cooper in 1851. There is no explanation that takes into account sounds the Iroquois heard before Cooper's time; it is possible sonic booms have been mistaken for natural sounds in modern days.
Sampson Navy and Air Force bases.
The east side of Seneca Lake was once home to a military training ground called Sampson Naval Base, primarily used during World War II. It became Sampson Air Force Base during the Korean War and was used for basic training. After Sampson AFB closed, the airfield remained as Seneca Army Airfield but was closed in 2000. The training grounds of Sampson have since been converted to a civilian picnic area called Sampson State Park. 
There is still a Naval facility at Seneca Lake, the Naval Undersea Warfare Center (NUWC) Sonar test facility. A scale model of the sonar section of the nuclear submarine USS Seawolf (SSN 21) was tested during the development of this ship, which was launched in June, 1995.
Water quality buoy.
There is a YSI EMM-2500 Buoy Platform located in the north end of Seneca Lake roughly in the center. Its coordinates are: latitude: 42°41'49.99"N, longitude: 76°55'29.93"W. The buoy has cellular modem communications and measures wind speed and direction, relative humidity, air temperature, barometric pressure, light intensity, and the water's depth and temperature, conductivity, turbidity, and chlorophyll-a levels.
The buoy was initially deployed in June 2006. The water depth where it is located is about .
Wine.
Viticulture and winemaking in the area date back to the 19th century, with the foundation of the Seneca Lake Wine Company in 1866 marking the first major winery in the area. The modern era of wine production began in the 1970s with the establishment of several wineries and the passage of the New York Farm Winery Act of 1976. The region was established as an American Viticultural Area in 1988.
Seneca Lake Wine Trail hosts many events on and around the lake, annually. With more than 30 wineries currently located on the shores of Seneca Lake, the winter 'Deck the Halls' event is a great time at the lake with participating wineries showcasing their vintages and pairing these wines with distinctive, tasty treats. Wineries also provide participants with an ornament at each stop to commemorate the event. 2011 marks the 20th anniversary of this event.

</doc>
<doc id="28181" url="https://en.wikipedia.org/wiki?curid=28181" title="Strait of Gibraltar">
Strait of Gibraltar

The Strait of Gibraltar (, ) is a narrow strait that connects the Atlantic Ocean to the Mediterranean Sea and separates Gibraltar and Peninsular Spain in Europe from Morocco and Ceuta (Spain) in Africa. The name comes from the Rock of Gibraltar, which in turn originates from the Arabic "Jebel Tariq" (meaning "Tariq's mountain") named after Tariq ibn Ziyad. It is also known as the Straits of Gibraltar, the Gut of Gibraltar (although this is mostly archaic), STROG (Strait Of Gibraltar) in naval use, and in the ancient world as the "Pillars of Hercules" (Ancient Greek: αἱ Ἡράκλειοι στῆλαι).
Europe and Africa are separated by of ocean at the strait's narrowest point. The Strait's depth ranges between which possibly interacted with the lower mean sea level of the last major glaciation 20,000 years ago when the level of the sea is believed to have been lower by . Ferries cross between the two continents every day in as little as 35 minutes. The Spanish side of the Strait is protected under El Estrecho Natural Park.
Location.
On the northern side of the Strait are Spain and Gibraltar (a British overseas territory in the Iberian Peninsula), while on the southern side are Morocco and Ceuta (a Spanish exclave in North Africa). Its boundaries were known in antiquity as the Pillars of Hercules. There are several islets, such as the disputed Isla Perejil, that are claimed by both Morocco and Spain.
Due to its location, the Strait is commonly used for illegal immigration from Africa to Europe.
Extent.
The International Hydrographic Organization defines the limits of the Strait of Gibraltar as follows:
Geology.
Around 5.9 million years ago, the connection between the Mediterranean Sea and the Atlantic Ocean along the Betic and Rifan Corridor was progressively restricted until its total closure, effectively causing the salinity of the Mediterranean to periodically rise within the gypsum and salt deposition range, during what is known as the Messinian salinity crisis. In this water chemistry environment, dissolved mineral concentrations, temperature and stilled water currents combined and occurred regularly to precipitate many mineral salts in layers on the seabed. The resultant accumulation of various huge salt and mineral deposits about the Mediterranean basin are directly linked to this era. It is believed that this process took a short time, by geological standards, lasting between 500,000 and 600,000 years.
It is estimated that, were the straits closed even at today's higher sea level, most water in the Mediterranean basin would evaporate within only a thousand years, as it is believed to have done then, and such an event would lay down mineral deposits like the salt deposits now found under the sea floor all over the Mediterranean.
After a lengthy period of restricted intermittent or no water exchange between the Atlantic Ocean and Mediterranean basin, approximately 5.33 million years ago, the Atlantic-Mediterranean connection was completely reestablished through the Strait of Gibraltar by the Zanclean flood, and has remained open ever since. The erosion produced by the incoming waters seems to be the main cause for the present depth of the strait (900 m at the narrows, 280 m at the Camarinal Sill). The strait is expected to close again as the African Plate moves northward relative to the Eurasian Plate, but on geological rather than human timescales.
Important Bird Area.
The Strait has been identified as an Important Bird Area by BirdLife International because hundreds of thousands of seabirds use it every year to pass between the Mediterranean and the Atlantic, including large numbers of Cory's and Balearic shearwaters, Audouin's, yellow-legged and lesser black-backed gulls, razorbills and Atlantic puffins.
History.
Evidence of the first human habitation of the area by Neanderthals dates back to 125,000 years ago. In fact, it is believed that the Rock of Gibraltar may have been one of the last outposts of Neanderthal habitation in the world, with evidence of their presence there dating to as recently as only 24,000 years ago. Archaeological evidence of Homo sapiens habitation of the area dates back c. 40,000 years.
The relatively short distance between the two shores has served as a quick hop-over point for various groups and civilizations throughout history, including Carthaginian campaigns against Rome, Roman travel between the provinces of Hispania and Mauritania, Vandals raiding down from Germania through Western Rome and into North Africa in the 5th century, Moors/Berbers in the 8th - 11th centuries, and Spain and Portugal in the 16th century.
Beginning in 1492, the straits began to play a certain cultural role in acting as a barrier against cross-strait conquest and the flow of culture and language that would naturally follow such a conquest. In that year, the last Muslim government north of the straits was overthrown by a Spanish force. Since that time, the straits have come to foster the development of two very distinct and varied cultures on either side of the straits after sharing much the same culture and greater degrees of tolerance for over 300+ years from the 8th century to the early 13th century.
On the northern side, Christian/European culture has remained dominant since the expulsion of the last Muslim kingdom in 1492, along with the Romance Spanish language, while on the southern side, Muslim-Arabic/Mediterranean has been dominant since the spread of Islam into North Africa in the 700's, along with the Arabic language. For the last 500 years, religious and cultural intolerance, more than the small travel barrier that the straits present, has come to act as a powerful enforcing agent of the cultural separation that exists between these two groups.
The small British enclave of the city of Gibraltar presents a third cultural group found in the straits. This enclave was first established in 1704 and has since been used by Britain to act as a surety for control of the sea lanes into and out of the Mediterranean.
Following the Spanish coup of July 1936 the Spanish Republican Navy tried to blockade the Strait of Gibraltar to hamper the transport of Army of Africa troops from Spanish Morocco to Peninsular Spain. But on 5 August 1936 the so-called Convoy de la victoria was able to bring at least 2,500 men across the strait breaking the republican blockade.
Communications.
The Strait is an important shipping route from the Mediterranean to the Atlantic. There are ferries that operate between Spain and Morocco across the strait, as well as between Spain and Ceuta and Gibraltar to Tangier.
Tunnel across the strait.
In December 2003, Spain and Morocco agreed to explore the construction of an undersea rail tunnel to connect their rail systems across the Strait. The gauge of the rail would be to match the proposed construction and conversion of significant parts of the existing broad gauge system to standard gauge. While the project remains in a planning phase, Spanish and Moroccan officials have met to discuss it as recently as 2012, and proposals predict it could be completed by 2025.
Special flow and wave patterns.
The Strait of Gibraltar links the Atlantic Ocean directly to the Mediterranean Sea. This direct linkage creates certain unique flow and wave patterns. These unique patterns are created due to the interaction of various regional and global evaporative forces, tidal forces, and wind forces.
Inflow and outflow.
Through the strait, water generally flows more or less continually in both an eastward and a westward direction. A smaller amount of deeper saltier and therefore denser waters continually work their way westwards (the Mediterranean outflow), while a larger amount of surface waters with lower salinity and density continually work their way eastwards (the Mediterranean inflow). These general flow tendencies may be occasionally interrupted for brief periods to accommodate temporary tidal flow requirements, depending on various lunar and solar alignments. Still, on the whole and over time, the balance of the water flow is eastwards, due to an evaporation rate within the Mediterranean basin higher than the combined inflow of all the rivers that empty into it. The shallow Camarinal Sill of the Strait of Gibraltar, which forms the shallowest point within the strait, acts to limit mixing between the cold, less saline Atlantic water and the warm Mediterranean waters. The Camarinal Sill is located at the far western end of the strait.
The Mediterranean waters are so much saltier than the Atlantic waters that they sink below the constantly incoming water and form a highly saline ("thermohaline", both warm and salty) layer of bottom water. This layer of bottom-water constantly works its way out into the Atlantic as the Mediterranean outflow. On the Atlantic side of the strait, a density boundary separates the Mediterranean outflow waters from the rest at about depth. These waters flow out and down the continental slope, losing salinity, until they begin to mix and equilibrate more rapidly, much further out at a depth of about . The Mediterranean outflow water layer can be traced for thousands of kilometres west of the strait, before completely losing its identity.
During the Second World War, German U-boats used the currents to pass into the Mediterranean Sea without detection, by maintaining silence with engines off. From September 1941 to May 1944 Germany managed to send 62 U-boats into the Mediterranean. All these boats had to navigate the British-controlled Strait of Gibraltar where nine U-boats were sunk while attempting passage and 10 more had to break off their run due to damage. No U-boats ever made it back into the Atlantic and all were either sunk in battle or scuttled by their own crews.
Internal waves.
Internal waves (waves at the density boundary layer) are often produced by the strait. Like traffic merging on a highway, the water flow is constricted in both directions because it must pass over the Camarinal Sill. When large tidal flows enter the Strait and the high tide relaxes, internal waves are generated at the Camarinal Sill and proceed eastwards. Even though the waves may occur down to great depths, occasionally the waves are almost imperceptible at the surface, at other times they can be seen clearly in satellite imagery. These "internal waves" continue to flow eastward and to refract around coastal features. They can sometimes be traced for as much as , and sometimes create interference patterns with refracted waves.
Territorial waters.
The Strait lies mostly within the territorial waters of Spain and Morocco, except for at the far eastern end. The United Kingdom (through Gibraltar) claims 3 nautical miles around Gibraltar putting part of the Strait inside British territorial waters, and the smaller-than-maximal claim also means that part of the Strait therefore lies in international waters according to the British claim. However, the ownership of Gibraltar and its territorial waters is disputed by Spain.
Power generation.
Some studies have proposed the possibility of erecting tidal power generating stations within the strait, to be powered from the predictable current at the strait.
In the 1920s and 1930s, the Atlantropa project proposed damming the strait to generate large amounts of electricity and lower the sea level of the Mediterranean by several hundreds of meters to create large new lands for settlement.

</doc>
<doc id="28182" url="https://en.wikipedia.org/wiki?curid=28182" title="Social epistemology">
Social epistemology

Social epistemology refers to a broad set of approaches to the study of knowledge that construes human knowledge as a collective achievement. Another way of characterizing social epistemology is as the study of the social dimensions of knowledge. One of the enduring difficulties with defining "social epistemology" is that of determining what the word "knowledge" means in this context. There is also a challenge in arriving at a definition of "social" which satisfies academics from different disciplines. Social epistemologists may be found working in many of the disciplines of the humanities and social sciences, most commonly in philosophy and sociology. In addition to marking a distinct movement in traditional, analytic epistemology, social epistemology is associated with the interdisciplinary field of Science and Technology Studies (STS).
The emergence of social epistemology.
The term "social epistemology" was first used by the library scientists Margaret Egan and Jesse Shera in the 1950s. Steven Shapin also used it in 1979. But its current sense began to emerge in the late 1980s. In 1987, the philosophical journal "Synthese" published a special issue on "social epistemology" which included two authors that have since taken "social epistemology" in two divergent directions: Alvin Goldman and Steve Fuller3. Fuller founded a journal called "Social Epistemology: a journal of knowledge, culture, and policy" in 1987 and published his first book, "Social Epistemology", in 1988. Goldman's "Knowledge in a Social World" came out in 1999. Goldman advocates for a type of epistemology which is sometimes called "veritistic epistemology" because of its large emphasis on truth. This type of epistemology is sometimes seen to side with "essentialism" as opposed to "multiculturalism". But Goldman has argued that this association between veritistic epistemology and essentialism is not necessary.
In 2012, on the occasion of the 25th anniversary of "Social Epistemology", Fuller reflected on the history and the prospects of the field, including the need for social epistemology to re-connect with the larger issues of knowledge production first identified by Charles Sanders Peirce as "cognitive economy" and nowadays often pursued by library and information science. As for the "analytic social epistemology", to which Goldman has been a significant contributor, Fuller concludes that it has "failed to make significant progress owing, in part, to a minimal understanding of actual knowledge practices, a minimised role for philosophers in ongoing inquiry, and a focus on maintaining the status quo of epistemology as a field."[http://www.tandfonline.com/doi/pdf/10.1080/02691728.2012.714415]
The basic view of knowledge that motivated the emergence of social epistemology can be traced to the work of Thomas Kuhn and Michel Foucault, which gained in prominence at the end of the 1960s. Both brought historical concerns directly to bear on problems long associated with the philosophy of science. Perhaps the most notable issue here was the nature of truth, which both Kuhn and Foucault described as a relative and contingent notion. On this background, ongoing work in the sociology of scientific knowledge (SSK) and the history and philosophy of science (HPS) was able to assert its epistemological consequences, leading most notably to the establishment of the "Strong Programme" at the University of Edinburgh. In terms of the two strands of social epistemology, Fuller is more sensitive and receptive to this historical trajectory (if not always in agreement) than Goldman, whose "veritistic" social epistemology can be reasonably read as a systematic rejection of the more extreme claims associated with Kuhn and Foucault.
Social Epistemology as a field within analytic philosophy.
As a field within analytic philosophy, Social Epistemology foregrounds the social aspects of knowledge creation and dissemination. What precisely these social aspects are, and whether they have beneficial or detrimental effects upon the possibilities to create, acquire and spread knowledge is a subject of continuous debate.
Within the field, `the social' is approached in two complementary and not mutually exclusive ways: `the social' character of knowledge can either be approached through inquiries in "inter-individual" epistemic relations or through inquiries focusing on epistemic "communities". The inter-individual approach typically focuses on issues such as testimony, epistemic trust as a form of trust placed by one individual in another, epistemic dependence, epistemic authority etc. The community approach typically focuses on issues such as community standards of justification, community procedures of critique, diversity, epistemic justice, and collective knowledge.
Social Epistemology as a field within analytic philosophy has close ties to, and often overlaps with Feminist Epistemology and Philosophy of Science. While parts of the field engage in abstract, normative considerations of knowledge creation and dissemination, other parts of the field are `naturalized epistemology' in the sense that they draw on empirically gained insights---be that natural science research from, e.g., cognitive psychology, be that qualitative or quantitative social science research. (For the notion of `naturalized epistemology see Willard Van Orman Quine.) And while parts of the field are concerned with analytic considerations of rather general character, case-based and domain-specific inquiries in, e.g., knowledge creation in collaborative scientific practice or knowledge exchange on online platforms, play an increasing role.
Important academic journals for Social Epistemology as a field within analytic philosophy are, e.g., Episteme, Hypatia, Social Epistemology, and Synthese. However, major work within this field is also published in journals that predominantly address philosophers of science or in interdisciplinary journals which focus on particular domains of inquiry (such as, e.g., Ethics and Information Technology).
Present and future concerns.
At this stage, both varieties of "social epistemology" remain largely "academic" or "theoretical" projects. But both emphasize the social significance of knowledge and therefore the cultural value of social epistemology itself. A range of journals publishing Social Epistemology welcome papers that include a policy dimension. More practical applications of social epistemology can be found in the areas of library science, academic publishing, guidelines for scientific authorship and collaboration, knowledge policy and debates over the role over the Internet in knowledge transmission and creation.
Notes.
1. "What Is Social Epistemology? A Smorgasbord of projects", in "Pathways to Knowledge: Private and Public", Oxford University Press, Pg:182-204, ISBN 0-19-517367-8
2. "Relativism, Rationality and Sociality of Knowledge", Barry Barnes and David Bloor, in "Rationality and Relativism", Pg:22 ISBN 0-262-58061-6
3. A comparison of Goldman and Fuller can be found in "Legitimizing Scientific Knowledge: An Introduction to Steve Fuller's Social Epistemology", Francis Remedios, Lexington Books, 2003. pp. 106 –112.<br>
http://social-epistemology.com/2013/07/12/orienting-social-epistemology-francis-remedios/
4. "Social Epistemology", Steve Fuller, Indiana University Press, p. 3.

</doc>
<doc id="28184" url="https://en.wikipedia.org/wiki?curid=28184" title="Sound card">
Sound card

A sound card (also known as an audio card) is an internal computer expansion card that facilitates economical input and output of audio signals to and from a computer under control of computer programs. The term "sound card" is also applied to external audio interfaces that use software to generate sound, as opposed to using hardware inside the PC. Typical uses of sound cards include providing the audio component for multimedia applications such as music composition, editing video or audio, presentation, education and entertainment (games) and video projection.
Sound functionality can also be integrated onto the motherboard, using components similar to plug-in cards. The best plug-in cards, which use better and more expensive components, can achieve higher quality than integrated sound. The integrated sound system is often still referred to as a "sound card". Sound processing hardware is also present on modern video cards with HDMI to output sound along with the video using that connector; previously they used a SPDIF connection to the motherboard or sound card.
General characteristics.
Most sound cards use a digital-to-analog converter (DAC), which converts recorded or generated digital data into an analog format. The output signal is connected to an amplifier, headphones, or external device using standard interconnects, such as a TRS phone connector or an RCA connector. If the number and size of connectors is too large for the space on the backplate the connectors will be off-board, typically using a breakout box, an auxiliary backplate, or a panel mounted at the front. More advanced cards usually include more than one sound chip to support higher data rates and multiple simultaneous functionality, for example digital production of synthesized sounds, usually for real-time generation of music and sound effects using minimal data and CPU time.
Digital sound reproduction is usually done with multichannel DACs, which are capable of simultaneous and digital samples at different pitches and volumes, and application of real-time effects such as filtering or deliberate distortion. Multichannel digital sound playback can also be used for music synthesis, when used with a compliance, and even multiple-channel emulation. This approach has become common as manufacturers seek simpler and lower-cost sound cards.
Most sound cards have a line in connector for an input signal
from a cassette tape or other sound source that has higher voltage levels than a microphone. The sound card digitizes this signal. The DMAC transfers the samples to the main memory, from where a recording software may write it to the hard disk for storage, editing, or further processing. Another common external connector is the "microphone" connector, for signals from a microphone or other low-level input device. Input through a microphone jack can be used, for example, by speech recognition or voice over IP applications.
Sound channels and polyphony.
An important sound card characteristic is polyphony, which refers to its ability to process and output multiple "independent" voices or sounds "simultaneously". These distinct "channels" are seen as the number of audio outputs, which may correspond to a speaker configuration such as 2.0 (stereo), 2.1 (stereo and sub woofer), 5.1 (surround), or other configuration. Sometimes, the terms "voice" and "channel" are used interchangeably to indicate the degree of polyphony, not the output speaker configuration.
For example, many older sound chips could accommodate three voices, but only one audio channel (i.e., a single mono output) for output, requiring all voices to be mixed together. Later cards, such as the AdLib sound card, had a 9-voice polyphony combined in 1 mono output channel.
For some years, most PC sound cards have had multiple FM synthesis voices (typically 9 or 16) which were usually used for MIDI music. The full capabilities of advanced cards are often not fully used; only one (mono) or two (stereo) voice(s) and channel(s) are usually dedicated to playback of digital sound samples, and playing back more than one digital sound sample usually requires a software downmix at a fixed sampling rate. Modern low-cost integrated soundcards (i.e., those built into motherboards) such as audio codecs like those meeting the AC'97 standard and even some lower-cost expansion sound cards still work this way. These devices may provide more than two sound output channels (typically 5.1 or 7.1 surround sound), but they usually have no actual hardware polyphony for either sound effects or MIDI reproduction – these tasks are performed entirely in software. This is similar to the way inexpensive softmodems perform modem tasks in software rather than in hardware.
Also, in the early days of 'wavetable' sample-based synthesis, some sound card manufacturers advertised polyphony solely on the MIDI capabilities alone. In this case, the card's output channel is irrelevant; typically, the card is only capable of two channels of digital sound. Instead, the polyphony measurement solely applies to the number of MIDI instruments the sound card is capable of producing at one given time.
Today, a sound card providing actual hardware polyphony, regardless of the number of output channels, is typically referred to as a "hardware audio accelerator", although actual voice polyphony is not the sole (or even a necessary) prerequisite, with other aspects such as hardware acceleration of 3D sound, positional audio and real-time DSP effects being more important.
Since digital sound playback has become available and single and provided better performance than synthesis, modern soundcards with hardware polyphony do not actually use DACs with as many channels as voices; instead, they perform voice mixing and effects processing in hardware, eventually performing digital filtering and conversions to and from the frequency domain for applying certain effects, inside a dedicated DSP. The final playback stage is performed by an external (in reference to the DSP chip(s)) DAC with significantly fewer channels than voices (e.g., 8 channels for 7.1 audio, which can be divided among 32, 64 or even 128 voices).
Color codes.
Connectors on the sound cards are color-coded as per the PC System Design Guide. They will also have symbols with arrows, holes and soundwaves that are associated with each jack position, the meaning of each is given below:
History of sound cards for the IBM PC architecture.
Sound cards for computers compatible with the IBM PC were very uncommon until 1988, which left the single internal PC speaker as the only way early PC software could produce sound and music. The speaker hardware was typically limited to square waves, which fit the common nickname of "beeper". The resulting sound was generally described as "beeps and boops". Several companies, most notably Access Software, developed techniques for digital sound reproduction over the PC speaker (see RealSound); the resulting audio, while barely functional, suffered from distorted output and low volume, and usually required all other processing to be stopped while sounds were played. Other home computer models of the 1980s included hardware support for digital sound playback, or music synthesis (or both), leaving the IBM PC at a disadvantage to them when it came to multimedia applications such as music composition or gaming. The initial design and marketing focuses of sound cards for the IBM PC platform were not based on gaming, but rather on specific audio applications such as music composition (AdLib Personal Music System, IBM Music Feature Card, Creative Music System), or on speech synthesis (Digispeech "DS201", Covox Speech Thing, Street Electronics "Echo").
In 1988 a panel of computer-game CEOs stated at the Consumer Electronics Show that the PC's limited sound capability prevented it from becoming the leading home computer, that it needed a $49–79 sound card with better capability than current products, and that once such hardware was widely installed their companies would support it. Sierra On-Line, which had pioneered supporting EGA and VGA video, and 3 1/2" disks, that year promised to support AdLib, IBM Music Feature, and Roland MT-32 in its games; the cards cost $195 to $600. A 1989 "Computer Gaming World" survey found that 18 of 25 game companies planned to support AdLib, six Roland and Covox, and seven Creative Music System/Game Blaster.
Hardware manufacturers.
One of the first manufacturers of sound cards for the IBM PC was AdLib, which produced a card based on the Yamaha YM3812 sound chip, also known as the OPL2. The AdLib had two modes: A 9-voice mode where each voice could be fully programmed, and a less frequently used "percussion" mode with 3 regular voices producing 5 independent percussion-only voices for a total of 11. (The percussion mode was considered inflexible by most developers; it was used mostly by AdLib's own composition software.)
Creative Labs also marketed a sound card about the same time called the Creative Music System. Although the "C/MS " had twelve voices to AdLib's nine, and was a stereo card while the AdLib was mono, the basic technology behind it was based on the Philips SAA1099 chip which was essentially a square-wave generator. It sounded much like twelve simultaneous PC speakers would have except for each channel having amplitude control, and failed to sell well, even after Creative renamed it the Game Blaster a year later, and marketed it through RadioShack in the US. The Game Blaster retailed for under $100 and was compatible with many popular games, such as Silpheed.
A large change in the IBM PC compatible sound card market happened when Creative Labs introduced the Sound Blaster card. Recommended by Microsoft to developers creating software based on the Multimedia PC standard, the Sound Blaster cloned the AdLib and added a sound coprocessor for recording and play back of digital audio (likely to have been an Intel microcontroller relabeled by Creative). It was incorrectly called a "DSP" (to suggest it was a digital signal processor), a game port for adding a joystick, and capability to interface to MIDI equipment (using the game port and a special cable). With more features at nearly the same price, and compatibility as well, most buyers chose the Sound Blaster. It eventually outsold the AdLib and dominated the market.
Roland also made sound cards in the late 1980s, most of them being high quality "prosumer" cards, such as the MT-32 and LAPC-I. Roland cards often sold for hundreds of dollars, and sometimes over a thousand. Many games had music written for their cards, such as Silpheed and Police Quest II. The cards were often poor at sound effects such as laughs, but for music were by far the best sound cards available until the mid nineties. Some Roland cards, such as the SCC, and later versions of the MT-32 were made to be less expensive, but their quality was usually drastically poorer than the other Roland cards.
By 1992 one sound card vendor advertised that its product was "Sound Blaster, AdLib, Disney Sound Source and Covox Speech Thing Compatible!". The Sound Blaster line of cards, together with the first inexpensive CD-ROM drives and evolving video technology, ushered in a new era of multimedia computer applications that could play back CD audio, add recorded dialogue to video games, or even reproduce full motion video (albeit at much lower resolutions and quality in early days). The widespread decision to support the Sound Blaster design in multimedia and entertainment titles meant that future sound cards such as Media Vision's Pro Audio Spectrum and the Gravis Ultrasound had to be Sound Blaster compatible if they were to sell well. Until the early 2000s (by which the AC'97 audio standard became more widespread and eventually usurped the SoundBlaster as a standard due to its low cost and integration into many motherboards), Sound Blaster compatibility is a standard that many other sound cards still support to maintain compatibility with many games and applications released.
Industry adoption.
When game company Sierra On-Line opted to support add-on music hardware (instead of built-in hardware such as the PC speaker and built-in sound capabilities of the IBM PCjr and Tandy 1000), what could be done with sound and music on the IBM PC changed dramatically. Two of the companies Sierra partnered with were Roland and AdLib, opting to produce in-game music for King's Quest 4 that supported the MT-32 and AdLib Music Synthesizer. The MT-32 had superior output quality, due in part to its method of sound synthesis as well as built-in reverb. Since it was the most sophisticated synthesizer they supported, Sierra chose to use most of the MT-32's custom features and unconventional instrument patches, producing background sound effects (e.g., chirping birds, clopping horse hooves, etc.) before the Sound Blaster brought playing real audio clips to the PC entertainment world. Many game companies also supported the MT-32, but supported the Adlib card as an alternative because of the latter's higher market base. The adoption of the MT-32 led the way for the creation of the MPU-401/Roland Sound Canvas and General MIDI standards as the most common means of playing in-game music until the mid-1990s.
Feature evolution.
Early ISA bus soundcards were half-duplex, meaning they couldn't record and play digitized sound simultaneously, mostly due to inferior card hardware (e.g., DSPs). Later, ISA cards like the SoundBlaster AWE series and Plug-and-play Soundblaster clones eventually became full-duplex and supported simultaneous recording and playback, but at the expense of using up two IRQ and DMA channels instead of one, making them no different from having two half-duplex sound cards in terms of configuration. Towards the end of the ISA bus' life, ISA soundcards started taking advantage of IRQ sharing, thus reducing the IRQs needed to one, but still needed two DMA channels. Many PCI bus cards do not have these limitations and are mostly full-duplex. It should also be noted that many modern PCI bus cards also do not require free DMA channels to operate.
Also, throughout the years, soundcards have evolved in terms of digital audio sampling rate (starting from 8-bit , to 32-bit, that the latest solutions support). Along the way, some cards started offering 'wavetable' sample-based synthesis, which provides superior MIDI synthesis quality relative to the earlier OPL-based solutions, which uses FM-synthesis. Also, some higher end cards started having their own RAM and processor for user-definable sound samples and MIDI instruments as well as to offload audio processing from the CPU.
For years, soundcards had only one or two channels of digital sound (most notably the Sound Blaster series and their compatibles) with the exception of the E-MU card family, the Gravis GF-1 and AMD Interwave, which had hardware support for up to 32 independent channels of digital audio. Early games and MOD-players needing more channels than a card could support had to resort to mixing multiple channels in software. Even today, the tendency is still to mix multiple sound streams in software, except in products specifically intended for gamers or professional musicians, with a sensible difference in price from "software based" products. Also, in the early era of 'wavetable' sample-based synthesis, soundcard companies would also sometimes boast about the card's polyphony capabilities in terms of MIDI synthesis. In this case polyphony solely refers to the count of MIDI notes the card is capable of synthesizing simultaneously at one given time and not the count of digital audio streams the card is capable of handling.
In regards to physical sound output, the number of physical sound channels has also increased. The first soundcard solutions were mono. Stereo sound was introduced in the early 1980s, and quadraphonic sound came in 1989. This was shortly followed by 5.1 channel audio. The latest soundcards support up to audio channels in the 7.1 speaker setup.
Crippling of features.
Most new soundcards no longer have the audio loopback device commonly called "Stereo Mix"/"Wave out mix"/"Mono Mix"/"What U Hear" that was once very prevalent and that allows users to digitally record speaker output to the microphone input.
Lenovo and other manufacturers fail to implement the chipset feature in hardware, while other manufacturers disable the driver from supporting it. In some cases loopback can be reinstated with driver updates (as in the case of some Dell computers); alternatively software (Total Recorder or "Virtual Audio Cable") can be purchased to enable the functionality. According to Microsoft, the functionality was hidden by default in Windows Vista (to reduce user confusion), but is still available, as long as the underlying sound card drivers and hardware support it. Ultimately, the user can connect the line out directly to the line in (analog hole).
Professional soundcards (audio interfaces).
Professional soundcards are special soundcards optimized for low-latency multichannel sound recording and playback, including studio-grade fidelity. Their drivers usually follow the Audio Stream Input Output protocol for use with professional sound engineering and music software, although ASIO drivers are also available for a range of consumer-grade soundcards.
Professional soundcards are usually described as "audio interfaces", and sometimes have the form of external rack-mountable units using USB, FireWire, or an optical interface, to offer sufficient data rates. The emphasis in these products is, in general, on multiple input and output connectors, direct hardware support for multiple input and output sound channels, as well as higher sampling rates and fidelity as compared to the usual consumer soundcard. In that respect, their role and intended purpose is more similar to a specialized multi-channel data recorder and real-time audio mixer and processor, roles which are possible only to a limited degree with typical consumer soundcards.
On the other hand, certain features of consumer soundcards such as support for environmental audio extensions (EAX), optimization for hardware acceleration in video games, or real-time ambience effects are secondary, nonexistent or even undesirable in professional soundcards, and as such audio interfaces are not recommended for the typical home user.
The typical "consumer-grade" soundcard is intended for generic home, office, and entertainment purposes with an emphasis on playback and casual use, rather than catering to the needs of audio professionals. In response to this, Steinberg (the creators of audio recording and sequencing software, Cubase and Nuendo) developed a protocol that specified the handling of multiple audio inputs and outputs.
In general, consumer grade soundcards impose several restrictions and inconveniences that would be unacceptable to an audio professional. One of a modern soundcard's purposes is to provide an AD/DA converter (analog to digital/digital to analog). However, in professional applications, there is usually a need for enhanced recording (analog to digital) conversion capabilities.
One of the limitations of consumer soundcards is their comparatively large sampling latency; this is the time it takes for the AD Converter to complete conversion of a sound sample and transfer it to the computer's main memory.
Consumer soundcards are also limited in the "effective" sampling rates and bit depths they can actually manage (compare analog versus digital sound) and have lower numbers of less flexible input channels: professional studio recording use typically requires more than the two channels that consumer soundcards provide, and more accessible connectors, unlike the variable mixture of internal—and sometimes virtual—and external connectors found in consumer-grade soundcards.
Sound devices other than expansion cards.
Integrated sound hardware on PC motherboards.
In 1984, the first IBM PCjr had a rudimentary 3-voice sound synthesis chip (the SN76489) which was capable of generating three square-wave tones with variable amplitude, and a pseudo-white noise channel that could generate primitive percussion sounds. The Tandy 1000, initially a clone of the PCjr, duplicated this functionality, with the Tandy TL/SL/RL models adding digital sound recording and playback capabilities. Many games during the 1980s that supported the PCjr's video standard (described as "Tandy-compatible", "Tandy graphics", or "TGA") also supported PCjr/Tandy 1000 audio.
In the late 1990s many computer manufacturers began to replace plug-in soundcards with a "codec" chip (actually a combined audio AD/DA-converter) integrated into the motherboard. Many of these used Intel's AC'97 specification. Others used inexpensive ACR slot accessory cards.
From around 2001 many motherboards incorporated integrated "real" (non-codec) soundcards, usually in the form of a custom chipset providing something akin to full Sound Blaster compatibility, providing relatively high-quality sound.
However, these features were dropped when AC'97 was superseded by Intel's HD Audio standard, which was released in 2004, again specified the use of a codec chip, and slowly gained acceptance. As of 2011, most motherboards have returned to using a codec chip, albeit a HD Audio compatible one, and the requirement for Sound Blaster compatibility relegated to history.
Integrated sound on other platforms.
Various non-IBM PC compatible computers, such as early home computers like the Commodore 64 (1982) and Amiga (1985), NEC's PC-88 and PC-98, Fujitsu's FM-7 and FM Towns, the MSX, Apple's Macintosh, and workstations from manufacturers like Sun, have had their own motherboard integrated sound devices. In some cases, most notably in those of the Amiga, C64, PC-88, PC-98, MSX, FM-7, and FM towns, they provide very advanced capabilities (as of the time of manufacture), in others they are only minimal capabilities. Some of these platforms have also had sound cards designed for their bus architectures that cannot be used in a standard PC.
Several Japanese computer platforms, including the PC-88, PC-98, MSX, and FM-7, featured built-in FM synthesis sound from Yamaha by the mid-1980s. By 1989, the FM Towns computer platform featured built-in PCM sample-based sound and supported the CD-ROM format.
The custom sound chip on Amiga, named Paula, had four digital sound channels (2 for the left speaker and 2 for the right) with 8 bit resolution (although with patches, 14/15bit was accomplishable at the cost of high CPU usage) for each channel and a 6 bit volume control per channel. Sound playback on Amiga was done by reading directly from the chip-RAM without using the main CPU.
Most arcade games have integrated sound chips, the most popular being the Yamaha OPL chip for BGM coupled with a variety of DACs for sampled audio and sound effects.
Sound cards on other platforms.
The earliest known soundcard used by computers was the Gooch Synthetic Woodwind, a music device for PLATO terminals, and is widely hailed as the precursor to sound cards and MIDI. It was invented in 1972.
Certain early arcade machines made use of sound cards to achieve playback of complex audio waveforms and digital music, despite being already equipped with onboard audio. An example of a sound card used in arcade machines is the Digital Compression System card, used in games from Midway. For example, Mortal Kombat II on the Midway T Unit hardware. The T-Unit hardware already has an onboard YM2151 OPL chip coupled with an OKI 6295 DAC, but said game uses an added on DCS card instead. The card is also used in the arcade version of Midway and Aerosmith's Revolution X for complex looping BGM and speech playback (Revolution X used fully sampled songs from the band's album that transparently looped- an impressive feature at the time the game was released).
MSX computers, while equipped with built-in sound capabilities, also relied on sound cards to produce better quality audio. The card, known as Moonsound, uses a Yamaha OPL4 sound chip. Prior to the Moonsound, there were also soundcards called "MSX Music" and "MSX Audio", which uses OPL2 and OPL3 chipsets, for the system.
The Apple II series of computers, which did not have sound capabilities beyond a beep until the IIGS, could use plug-in sound cards from a variety of manufacturers. The first, in 1978, was ALF's Apple Music Synthesizer, with 3 voices; two or three cards could be used to create 6 or 9 voices in stereo. Later ALF created the Apple Music II, a 9-voice model. The most widely supported card, however, was the Mockingboard. Sweet Micro Systems sold the Mockingboard in various models. Early Mockingboard models ranged from 3 voices in mono, while some later designs had 6 voices in stereo. Some software supported use of two Mockingboard cards, which allowed 12-voice music and sound. A 12-voice, single card clone of the Mockingboard called the Phasor was made by Applied Engineering. In late 2005 a company called ReactiveMicro.com produced a 6-voice clone called the Mockingboard v1 and also had plans to clone the Phasor and produce a hybrid card user-selectable between Mockingboard and Phasor modes plus support both the SC-01 or SC-02 speech synthesizers.
External sound devices.
Devices such as the Covox Speech Thing could be attached to the parallel port of an IBM PC and feed 6- or 8-bit PCM sample data to produce audio. Also, many types of professional soundcards (audio interfaces) have the form of an external FireWire or USB unit, usually for convenience and improved fidelity.
Sound cards using the PCMCIA Cardbus interface were available before laptop and notebook computers routinely had onboard sound. Cardbus audio may still be used if onboard sound quality is poor. When Cardbus interfaces were superseded by Expresscard on computers since about 2005, manufacturers followed. Most of these units are designed for mobile DJs, providing separate outputs to allow both playback and monitoring from one system, however some also target mobile gamers, providing high-end sound to gaming laptops who are usually well-equipped when it comes to graphics and processing power, but tend to have audio codecs that are no better than the ones found on regular laptops.
USB sound cards.
USB sound "cards" are external devices that plug into the computer via USB. They are often used in studios and on stage by electronic musicians including live PA performers and DJs.
The USB specification defines a standard interface, the USB audio device class, allowing a single driver to work with the various USB sound devices and interfaces on the market. Mac OS X, Windows, and Linux support this standard. However, many USB sound cards do not conform to the standard and require proprietary drivers from the manufacturer.
Even cards meeting the older, slow, USB 1.1 specification are capable of high quality sound with a limited number of channels, or limited sampling frequency or bit depth, but USB 2.0 or later is more capable.
A USB audio interface may also describe a device allowing a computer which has a sound-card, yet lacks a standard audio socket, to be connected to an external device which requires such a socket, via its USB socket.
Uses.
The main function of a sound card is to play audio, usually music, with varying formats (monophonic, stereophonic, various multiple speaker setups) and degrees of control. The source may be a CD or DVD, a file, streamed audio, or any external source connected to a sound card input.
Audio may be recorded. Sometimes sound card hardware and drivers do not support recording a source that is being played.
A card can also be used, in conjunction with software, to generate arbitrary waveforms, acting as an audio-frequency function generator. Free and commercial software is available for this purpose; there are also online services that generate audio files for any desired waveforms, playable through a sound card.
A card can be used, again in conjunction with free or commercial software, to analyse input waveforms. For example, a very-low-distortion sinewave oscillator can be used as input to equipment under test; the output is sent to a sound card's line input and run through Fourier transform software to find the amplitude of each harmonic of the added distortion. Alternatively, a less pure signal source may be used, with circuitry to subtract the input from the output, attenuated and phase-corrected; the result is distortion and noise only, which can be analysed.
There are programs which allow a sound card to be used as an audio-frequency oscilloscope.
For all measurement purposes a sound card must be chosen with good audio properties. It must itself contribute as little distortion and noise as possible, and attention must be paid to bandwidth and sampling. A typical integrated sound card, the Realtek ALC887, according to its data sheet has distortion of about 80 dB below the fundamental; cards are available with distortion better than -100 dB.
Sound cards with a sampling rate of 192 kHz can be used to synchronize the clock of the computer with a time signal transmitter working on frequencies below 96 kHz like DCF 77 with a special software and a coil at the entrance of the soundcard, working as antenna [http://www.qsl.net/dl4yhf/speclab/timesigs.htm.
Driver architecture.
To use a sound card, the operating system (OS) typically requires a specific device driver, a low-level program that handles the data connections between the physical hardware and the operating system. Some operating systems include the drivers for many cards; for cards not so supported, drivers are supplied with the card, or available for download.

</doc>
<doc id="28186" url="https://en.wikipedia.org/wiki?curid=28186" title="Symmetry group">
Symmetry group

In abstract algebra, the symmetry group of an object (image, signal, etc.) is the group of all transformations under which the object is invariant with composition as the group operation. For a space with a metric, it is a subgroup of the isometry group of the space concerned. If not stated otherwise, this article considers symmetry groups in Euclidean geometry, but the concept may also be studied in more general contexts as expanded below.
Introduction.
The "objects" may be geometric figures, images, and patterns, such as a wallpaper pattern. The definition can be made more precise by specifying what is meant by image or pattern, e.g., a function of position with values in a set of colors. For symmetry of physical objects, one may also want to take their physical composition into account. The group of isometries of space induces a group action on objects in it.
The symmetry group is sometimes also called full symmetry group in order to emphasize that it includes the orientation-reversing isometries (like reflections, glide reflections and improper rotations) under which the figure is invariant. The subgroup of orientation-preserving isometries (i.e. translations, rotations, and compositions of these) that leave the figure invariant is called its proper symmetry group. The proper symmetry group of an object is equal to its full symmetry group if and only if the object is chiral (and thus there are no orientation-reversing isometries under which it is invariant).
Any symmetry group whose elements have a common fixed point, which is true for all finite symmetry groups and also for the symmetry groups of bounded figures, can be represented as a subgroup of the orthogonal group O("n") by choosing the origin to be a fixed point. The proper symmetry group is then a subgroup of the special orthogonal group SO("n"), and is therefore also called rotation group of the figure.
A discrete symmetry group is a symmetry group such that for every point of the space the set of images of the point under the isometries in the symmetry group is a discrete set.
Discrete symmetry groups come in three types: (1) finite point groups, which include only rotations, reflections, inversion and rotoinversion – they are just the finite subgroups of O("n"), (2) infinite lattice groups, which include only translations, and (3) infinite space groups which combines elements of both previous types, and may also include extra transformations like screw axis and glide reflection. There are also "continuous" symmetry groups, which contain rotations of arbitrarily small angles or translations of arbitrarily small distances. The group of all symmetries of a sphere O(3) is an example of this, and in general such continuous symmetry groups are studied as Lie groups. With a categorization of subgroups of the Euclidean group corresponds a categorization of symmetry groups.
Two geometric figures are considered to be of the same symmetry type if their symmetry groups are conjugate subgroups of the Euclidean group E("n") (the isometry group of R"n"), where two subgroups "H"1, "H"2 of a group "G" are "conjugate", if there exists such that . For example:
When considering isometry groups, one may restrict oneself to those where for all points the set of images under the isometries is topologically closed. This includes all discrete isometry groups and also those involved in continuous symmetries, but excludes for example in 1D the group of translations by a rational number. A "figure" with this symmetry group is non-drawable and up to arbitrarily fine detail homogeneous, without being really homogeneous.
One dimension.
The isometry groups in one dimension where for all points the set of images under the isometries is topologically closed are:
See also symmetry groups in one dimension.
Two dimensions.
Up to conjugacy the discrete point groups in two-dimensional space are the following classes:
C1 is the trivial group containing only the identity operation, which occurs when the figure has no symmetry at all, for example the letter F. C2 is the symmetry group of the letter Z, C3 that of a triskelion, C4 of a swastika, and C5, C6, etc. are the symmetry groups of similar swastika-like figures with five, six, etc. arms instead of four.
D1 is the 2-element group containing the identity operation and a single reflection, which occurs when the figure has only a single axis of bilateral symmetry, for example the letter A. 
D2, which is isomorphic to the Klein four-group, is the symmetry group of a non-equilateral rectangle. This figure has four symmetry operations: the identity operation, one twofold axis of rotation, and two nonequivalent mirror planes.
D3, D4 etc. are the symmetry groups of the regular polygons.
The actual symmetry groups in each of these cases have two degrees of freedom for the center of rotation, and in the case of the dihedral groups, one more for the positions of the mirrors.
The remaining isometry groups in two dimensions with a fixed point, where for all points the set of images under the isometries is topologically closed are:
For non-bounded figures, the additional isometry groups can include translations; the closed ones are:
Three dimensions.
Up to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 separate ones. In crystallography they are restricted to be compatible with the discrete translation symmetries of a crystal lattice. This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 from the 7 infinite series, and 5 of the 7 others).
The continuous symmetry groups with a fixed point include those of:
For objects and scalar fields the cylindrical symmetry implies vertical planes of reflection. However, for vector fields it does not: in cylindrical coordinates with respect to some axis, 
formula_1 has cylindrical symmetry with respect to the axis if and only if formula_2 and formula_3 have this symmetry, i.e., they do not depend on φ. Additionally there is reflectional symmetry if and only if formula_4.
For spherical symmetry there is no such distinction, it implies planes of reflection.
The continuous symmetry groups without a fixed point include those with a screw axis, such as an infinite helix. See also subgroups of the Euclidean group.
Symmetry groups in general.
In wider contexts, a symmetry group may be any kind of transformation group, or automorphism group. Once we know what kind of mathematical structure we are concerned with, we should be able to pinpoint what mappings preserve the structure. Conversely, specifying the symmetry can define the structure, or at least clarify what we mean by an invariant, geometric language in which to discuss it; this is one way of looking at the Erlangen programme.
For example, automorphism groups of certain models of finite geometries are not "symmetry groups" in the usual sense, although they preserve symmetry. They do this by preserving "families" of point-sets rather than point-sets (or "objects") themselves.
Like above, the group of automorphisms of space induces a group action on objects in it.
For a given geometric figure in a given geometric space, consider the following equivalence relation: two automorphisms of space are equivalent if and only if the two images of the figure are the same (here "the same" does not mean something like e.g. "the same up to translation and rotation", but it means "exactly the same"). Then the equivalence class of the identity is the symmetry group of the figure, and every equivalence class corresponds to one isomorphic version of the figure.
There is a bijection between every pair of equivalence classes: the inverse of a representative of the first equivalence class, composed with a representative of the second.
In the case of a finite automorphism group of the whole space, its order is the order of the symmetry group of the figure multiplied by the number of isomorphic versions of the figure.
Examples:
Compare Lagrange's theorem (group theory) and its proof.

</doc>
<doc id="28187" url="https://en.wikipedia.org/wiki?curid=28187" title="Singular they">
Singular they

Singular "they" is the use in English of the pronoun "they", or its inflected or derivative forms, such as "them", "their", "themself", or "themselves", as a gender-neutral pronoun to refer to a single person or an antecedent that is grammatically singular. It typically occurs with an antecedent of indeterminate gender, as in sentences such as:
The singular "they" had emerged by the mid-14th century and is common in everyday spoken English, but its use has been the target of criticism since the late nineteenth century. Its use in formal English has increased with the trend toward gender-inclusive language.
Inflected forms and derivative pronouns.
Though the "singular "they"" permits a singular antecedent, it is used with a plural verb form.
Singular "they" has the same inflected forms as the plural "they", i.e. "them" and "their". 
The reflexive form "themselves" is sometimes used, but there is an alternative reflexive form—"themself". "Themself" was common from the 14th to 16th centuries and its use has increased since about the 1970s or 1980s, but it was classified by the 2015 edition of "Fowler's" as "a minority form" and its use in standard dialect was described by "The Cambridge Grammar of the English Language" as "rare and acceptable only to a minority of speakers". Its use is thought to be increasing It is sometimes used when referring to a single person of indeterminate gender, where the plural form "themselves" might seem incongruous, as in
The Canadian government recommends "themselves" as the reflexive form of singular "they" for use in Canadian federal legislative texts and advises against using "themself", but "themself" is also found:
Usage.
"They" with a singular antecedent has remained in common use for centuries in spite of its proscription by traditional grammarians. Such use goes back to the Middle English of the 14th century.
Informal spoken English employs nearly universal use of the singular "they". An examination by Jürgen Gerner of the British National Corpus published in 1998 found that British speakers regardless of social status, age, sex, or region used the singular "they" overwhelmingly more often than the singular "he" or other options.
Older usage.
Singular "they" is found in the writings of many respected authors.
Alongside "they", however, it was also acceptable to use the pronoun "he" as a (purportedly) gender-neutral pronoun, as in the following:
In Thackeray's writings, we find both
and
And Caxton writes
alongside
Trend to prescription of generic "he" from 19th century.
Preferring "he" as a gender-neutral pronoun, nineteenth-century grammarians insisted on a singular pronoun on the grounds of number agreement, while rejecting "he or she" as clumsy.
A recommendation to use the generic "he", rather than "they", in formal English can be found as early as the mid-18th century, in Ann Fisher's "A New Grammar", where she writes:
An 1895 grammar (Baskervill, W.M. and Sewell, J.W.: "An English Grammar for the Use of High School, Academy and College Class") notes the common use of the singular "they" but recommends use of the generic "he", on the basis of number agreement:
Baskervill gives a number of examples of recognized authors using the singular "they", including
but prefers the use of "he":
—Baskervill, An English Grammar
In 1850, the British Parliament passed an act which provided that, when used in acts of Parliament "words importing the masculine gender shall be deemed and taken to include females".
It has been argued that the real motivation for promoting the "generic" "he" was an androcentic world view, with the default sex of humans being male – and the default gender therefore being masculine.
As Wilson wrote in 1560
and Poole wrote in 1646
In spite of continuous attempts on the part of educationalists to proscribe singular "they" in favour of "he", its use remained widespread, and the advice was largely ignored, even by writers of the period, though the advice may have been observed more by American writers.
Use of the purportedly gender-neutral "he" remained acceptable until at least the 1960s, though some uses of "he" were later criticized as being awkward or silly, for instance when referring to:
Contemporary use of "he" to refer to a generic or indefinite antecedent.
"He" is still sometimes found in contemporary writing when referring to a generic or indeterminate antecedent.
In some cases it is clear from the situation that the persons potentially referred to are likely to be male, as in
In some cases the antecedent may refer to persons who are only "probably" male or to occupations traditionally thought of as male:
In other situations, the antecedent may refer to:
In 2010, we still find the use of generic "he" recommended:
In 2015, "Fowler's Dictionary of Modern English Usage" calls this "the now outmoded use of "he" to mean 'anyone'", stating
Trend to gender-neutral language from the 20th century.
In the second half of the 20th century, people expressed concern at the use of sexist and male-oriented language.
Such usage included not only the use of "man" as a false generic but also the use of "he" as a generic pronoun.
It was argued that "he" could not sensibly be used as a generic pronoun understood to include men and women.
William Safire in his "On Language" column in "The New York Times" approved of the use of generic "he", mentioning the mnemonic phrase "the male embraces the female".
C. Badendyck from Brooklyn wrote to the "New York Times" in a reply:
By 1980, the movement had gained wide support, and many organizations, including most publishers, had issued guidelines on the use of gender-neutral language.
Use for specific, known people.
In some situations, an individual may be known but referred to using the pronoun "they" because their gender is unknown or because "they/them" are their preferred pronouns; social media applications, for example, may permit account holders to select a nonbinary gender such as "gender fluid", "agender", or "bigender" and a pronoun, including "they"/"them" which they wish to be used when referring to them. Though "singular "they"" has long been used with antecedents like "everybody" or generic persons of unknown gender, this use, which may be chosen by an individual, is recent.
It was chosen by the American Dialect Society as the Word of the Year 2015, in the meaning "gender-neutral singular pronoun for a known person, as a non-binary identifier". In a press release, the American Dialect Society wrote:
The vote on 8 January 2016 followed the previous year's approval of this use by the "Washington Post" style guide, when Bill Walsh, the "Washington Post" copy editor said that the singular "they" is "the only sensible solution to English’s lack of a gender-neutral third-person singular personal pronoun".
Contemporary usage.
The use of masculine generic nouns and pronouns in written and spoken language has decreased since the 1960s.
In a corpus of spontaneous speech collected in Australia in the 1990s, singular "they" had become the most frequently used generic pronoun. Similarly, a study from 2002 looking at a corpus of American and British newspapers showed a preference for "they" to be used (rather than generic "he" or "he or she") as a singular epicene pronoun.
The increased use of singular "they" may be at least partly due to an increasing desire for gender-neutral language. While writers a hundred years ago might have had no qualm using "he" with a referent of indeterminate gender, writers today often feel uncomfortable with this. One solution in formal writing has often been to write "he or she", or something similar, but this is considered awkward when used excessively, overly politically correct, or both.
In contemporary usage, singular "they" is used to refer to an indeterminate antecedent, for instance when the notional gender or number of the antecedent is indeterminate or the gender of the real-word entity referred to is unknown or unrevealed.
Examples include different types of usage.
Use with a pronoun antecedent.
The singular antecedent can be a pronoun such as everybody, someone, anybody, or an interrogative pronoun such as "who":
Use with a generic noun as antecedent.
The singular antecedent can also be a noun such as "person", "patient", or "student":
Acceptability and prescriptive guidance.
Though both generic "he" and generic "they" have long histories of use, and both are still used, both are also systematically avoided by particular groups.
Style guides that avoid expressing a preference for either approach sometimes recommend recasting a problem sentence, for instance replacing generic expressions with plurals to avoid the criticisms of either party.
The use of singular "they" may be more accepted in British English than in American English, or vice versa.
Usage guidance in British–American style guides.
The Handbook of Non-Sexist Writing by Casey Miller and Kate Swift was first published in the United States but, because of differences in culture and vocabulary, separate British editions have since been published.
These authors accept or recommend singular uses of "they" not just in cases where there is an element of semantic plurality expressed by a word such as "everyone" but also where an indeterminate "person" is referred to, citing examples of such usage even in formal speech. For instance, they quote Ronald Reagan:
In addition to use of singular "they", they – and others – also suggest a number of ways to avoid "the pronoun problem" in gender-neutral writing.
One strategy is to rewrite the sentence to use a plural "they". For instance, in a newspaper story
could have been changed to
Another strategy is to eliminate the pronoun; so
becomes
Other methods of avoiding gender preference include recasting a sentence to use "one", or (for babies) "it".
Usage guidance in American style guides.
"Garner's Modern American Usage" (2003) recommends cautious use of singular "they", and avoidance where possible because its use is stigmatized.
Garner suggests that use of singular "they" is more acceptable in British English:
and apparently regrets the resistance by the American language community:
He regards the trend toward using singular "they" with antecedents like "everybody", "anyone" and "somebody" as inevitable:
In the 14th edition (1993) of The Chicago Manual of Style, the University of Chicago Press explicitly recommended use of singular use of "they" and "their", noting a "revival" of this usage and citing "its venerable use by such writers as Addison, Austen, Chesterfield, Fielding, Ruskin, Scott, and Shakespeare."
From the 15th edition, this was changed. In Chapter 5 of the 16th edition, now written by Bryan A. Garner, the recommendations are:
and
According to The American Heritage Book of English Usage, many Americans avoid use of "they" to refer to a singular antecedent out of respect for a "traditional" grammatical rule, despite use of singular "they" by modern writers of note and mainstream publications:
The Publication Manual of the American Psychological Association explicitly reject the use of singular "they" and gives the following example as "incorrect" usage:
while also specifically taking the position that generic "he" is unacceptable. The APA recommends using "he or she", recasting the sentence with a plural subject to allow correct use of "they", or simply rewriting the sentence to avoid issues with gender or number.
Strunk & White, the authors of The Elements of Style find use of "they" with a singular antecedent unacceptable:
Their assessment, in 1979, was
Joseph M. Williams, who wrote a number of books on writing with "", discusses the advantages and disadvantages of various solutions when faced with the problem of referring to an antecedent such as "someone", "everyone", "no one" or a noun that does not indicate gender and suggests that this will continue to be a problem for some time. He "suspect that eventually we will accept the plural "they" as a correct singular" but states that currently "formal usage requires a singular pronoun".
According to The Little, Brown Handbook, most experts—and some teachers and employers—find use of singular "they" unacceptable:
It recommends using "he or she" or avoiding the problem by rewriting the sentence to use a plural or omit the pronoun.
The Purdue Online Writing Lab (OWL) maintains that singular "they" is incorrect:
The Washington Post style manual, , recommends trying to "write around the problem, perhaps by changing singulars to plurals, before using the singular they as a last resort" and specifically permits use of "they" for a "gender-nonconforming person".
Usage guidance in British style guides.
In the first edition of A Dictionary of Modern English Usage (published in 1926) it is stated that singular "they" is disapproved of by grammarians and should be avoided in favour of the generic "he". Examples of its use by eminent writers are given, but it is suggested that "few good modern writers would flout so conspicuously as Fielding and Thackeray", whose sentences are described as having an "old-fashioned sound".
In the second edition of Fowler's, "Fowler's Modern English Usage" (edited by Sir Ernest Gowers and published in 1965), it is stated that singular "they" is disapproved of by grammarians and, while common in colloquial speech, should preferably be avoided in favour of the generic "he" in prose. Numerous examples of its use by eminent writers are given, but it is still suggested that "few good modern writers would flout so conspicuously as Fielding and Thackeray".
According to the third edition of Fowler's ("The New Fowler's Modern English Usage", edited by Burchfield and published in 1996) singular "they" has not only been widely used by good writers for centuries, but is now generally accepted, except by some conservative grammarians, including the Fowler of 1926, who ignored the evidence:
The Complete Plain Words was originally written in 1948 by Sir Ernest Gowers, a civil servant, in an attempt by the British civil service to improve "official English". A second edition, edited by Sir Bruce Fraser, was published in 1973. It refers to "they" or "them" as the "equivalent of a singular pronoun of common sex" as "common in speech and not unknown in serious writing " but "stigmatized by grammarians as usage grammatically indefensible. The book's advice for "official writers" (civil servants) is to avoid its use and not to be tempted by its "greater convenience", though "necessity may eventually force it into the category of accepted idiom".
A new edition of Plain Words, revised and updated by Sir Ernest Gowers' great granddaughter, Rebecca Gowers, was published in 2014.
It notes that singular "they" and "them" have become much more widespread since Gowers' original comments, but still finds it "safer" to treat a sentence like 'The reader may toss their book aside' as incorrect "in formal English", while rejecting even more strongly sentences like
The Times Style and Usage Guide (first published in 2003 by "The Times" of London) recommends avoiding sentences like
by using a plural construction:
The Cambridge Guide to English Usage (2004) finds singular "they" "unremarkable":
It expresses several preferences.
The Economist Style Guide refers to the use of "they" in sentences like
as "scrambled syntax that people adopt because they cannot bring themselves to use a singular pronoun".
"The New Hart's Rules" is aimed at those engaged in copy editing, and the emphasis is on the formal elements of presentation including punctuation and typeface, rather than on linguistic style but—like "The Chicago Manual of Style"—makes occasional forays into matters of usage.
It advises against use of the purportedly gender-neutral "he", and suggests cautious use of "they" where "he or she" presents problems.
The 2011 edition of the New International Version Bible uses singular "they" instead of the traditional "he" when translating pronouns that apply to both genders in the original Greek or Hebrew. This decision was based on research by a commission that studied modern English usage and determined that singular "they" ("them"/"their") was by far the most common way that English-language speakers and writers today refer back to singular antecedents such as "whoever", "anyone", "somebody", "a person", "no one", and the like."
Australian usage guidance.
The Australian Federation Press Style Guide for use in preparation of book manuscripts recommends "Gender-neutral language should be used", stating that use of "they" and "their" as singular pronouns is acceptable.
Usage guidance in English grammars.
According to A Comprehensive Grammar of the English Language (1985):
The Cambridge Grammar of the English Language discusses the prescriptivist argument that "they" is a plural pronoun and that the use of "they" with a singular "antecedent" therefore violates the rule of agreement between antecedent and pronoun, but takes the view that "they", though "primarily" plural, can also be singular in a secondary "extended" sense, comparable to the purportedly extended sense of "he" to include female gender.
Use of singular "they" is stated to be "particularly common", even "stylistically neutral" with antecedents such as "everyone", "someone", and "no one", but more restricted when referring to common nouns as antecedents, as in
Use of the pronoun "themself" is described as being "rare" and "acceptable only to a minority of speakers", while use of the morphologically plural "themselves" is considered problematic when referring to "someone" rather that "everyone" (since only the latter implies a plural set).
There are also issues of grammatical acceptability when reflexive pronouns refer to singular noun phrases joined by "or", the following all being problematic:
On the motivation for using singular "they", "A Student's Introduction to English Grammar" states
The alternative "he or she" can be "far too cumbersome", as in
or even "flatly ungrammatical", as in
"Among younger speakers", use of singular "they" even with definite noun-phrase antecedents finds increasing acceptance, "sidestepping any presumption about the sex of the person referred to", as in
Grammatical and logical analysis.
Notional agreement.
One explanation given for some uses of "they" referring to a singular antecedent is "notional agreement", when the antecedent is seen as semantically plural: 
In other words, in the Shakespeare quotation "a mother" is syntactically singular but stands for all mothers, and in the Shaw quotation "no man" is syntactically singular (demonstrated by taking the singular form "goes") but is semantically plural ("all" go kill not to be killed), hence idiomatically requiring "they". Such use, which goes back a long way, includes examples where the sex is known, as in the above examples.
Distribution.
Distributive constructions apply a "single" idea to "multiple" members of a group.
They are typically marked in English by words like "each", "every" and "any". The simplest examples are applied to groups of two, and use words like "either" and "or"—"Would you like tea or coffee?". Since distributive constructions apply an idea relevant to each individual in the group, rather than to the group as a whole, they are most often conceived of as singular, and a singular pronoun is used.
However, many languages, including English, show ambivalence in this regard. Because distribution also requires a group with more than one member, plural forms are sometimes used.
Referential and non-referential anaphors.
According to the traditional analysis, English personal pronouns (e.g. "his", "her", "their") are typically used to refer backward (or forward) within a sentence to a noun phrase (which may be a simple noun). This reference is called an "anaphoric" reference, and the referring pronoun is termed an "anaphor".
The so-called singular "they" is morphologically plural, and is accompanied by a plural verb. However, it is often used in circumstances where an indeterminate antecedent is signified by an indefinite singular antecedent; for example,
In some sentences, typically those including words like "every" or "any", the morphologically singular antecedent does not refer to a single entity but is "anaphorically linked" to the associated pronoun to indicate a set of pairwise relationships, as in the sentence:
Linguists like Pinker and Huddleston explain sentences like this (and others) in terms of bound variables, a term borrowed from logic. Pinker prefers the terms "quantifier" and "bound variable" to "antecedent" and " pronoun". He suggests that pronouns used as "variables" in this way are more appropriately regarded as homonyms of the equivalent referential pronouns.
The word "reference" is traditionally used in two different senses:
With a morphologically singular antecedent, there are a number of possibilities, including the following:
Cognitive efficiency.
A study of whether "singular "they"" is more "difficult" to understand than gendered pronouns ("In Search of Gender Neutrality: Is Singular "They" a Cognitively Efficient Substitute for Generic "He"?" by Foertsch and Gernsbacher) found that "singular "they" is a cognitively efficient substitute for generic "he" or "she", particularly when the antecedent is nonreferential" (e.g. "anybody", "a nurse", or "a truck driver") rather than referring to a specific person (e.g. "a runner I knew" or "my nurse"). Clauses with singular "they" were read "just as quickly as clauses containing a gendered pronoun that matched the stereotype of the antecedent" (e.g. "she" for a nurse and "he" for a truck driver) and "much more quickly than clauses containing a gendered pronoun that went against the gender stereotype of the antecedent".
On the other hand, when the pronoun "they" was used to refer to known individuals ("referential antecedents, for which the gender was presumably known", e.g "my nurse", "that truck driver", "a runner I knew"), reading was slowed when compared with use of a gendered pronoun consistent with the "stereotypic gender" (e.g. "he" for a specific truck driver). 
The study concluded that "... the increased use of singular "they" is not problematic for the majority of readers".
Comparison with other pronouns.
The singular and plural use of "they" can be compared with the pronoun "you", which originally was only plural, but by about 1700 replaced "thou" for singular referents, while retaining the plural verb form.

</doc>
<doc id="28189" url="https://en.wikipedia.org/wiki?curid=28189" title="Space Shuttle">
Space Shuttle

The Space Shuttle was a partially reusable low Earth orbital spacecraft system operated by the U.S. National Aeronautics and Space Administration (NASA), as part of the Space Shuttle program. Its official program name was "Space Transportation System (STS)", taken from a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982. They were used on a total of 135 missions from 1981 to 2011, launched from the Kennedy Space Center (KSC) in Florida. Operational missions launched numerous satellites, interplanetary probes, and the Hubble Space Telescope (HST); conducted science experiments in orbit; and participated in construction and servicing of the International Space Station. The Shuttle fleet's total mission time was 1322 days, 19 hours, 21 minutes and 23 seconds.
Shuttle components included the Orbiter Vehicle (OV), a pair of recoverable solid rocket boosters (SRBs), and the expendable external tank (ET) containing liquid hydrogen and liquid oxygen. The Shuttle was launched vertically, like a conventional rocket, with the two SRBs operating in parallel with the OV's three main engines, which were fueled from the ET. The SRBs were jettisoned before the vehicle reached orbit, and the ET was jettisoned just before orbit insertion, which used the orbiter's two Orbital Maneuvering System (OMS) engines. At the conclusion of the mission, the orbiter fired its OMS to de-orbit and re-enter the atmosphere. The orbiter then glided as a spaceplane to a runway landing, usually at the Shuttle Landing Facility of KSC or Rogers Dry Lake in Edwards Air Force Base, California. After landing at Edwards, the orbiter was flown back to the KSC on the Shuttle Carrier Aircraft, a specially modified Boeing 747.
The first orbiter, "Enterprise", was built for Approach and Landing Tests and had no orbital capability. Four fully operational orbiters were initially built: "Columbia", "Challenger", "Discovery", and "Atlantis". Of these, two were lost in mission accidents: "Challenger" in 1986 and "Columbia" in 2003, with a total of fourteen astronauts killed. A fifth operational orbiter, "Endeavour", was built in 1991 to replace "Challenger". The Space Shuttle was retired from service upon the conclusion of "Atlantis"s final flight on July 21, 2011.
Overview.
The Space Shuttle was a partially reusable human spaceflight vehicle capable of reaching low Earth orbit, commissioned and operated by the US National Aeronautics and Space Administration (NASA) from 1981 to 2011. It resulted from shuttle design studies conducted by NASA and the US Air Force in the 1960s and was first proposed for development as part of an ambitious second-generation Space Transportation System (STS) of space vehicles to follow the Apollo program in a September 1969 report of a Space Task Group headed by Vice President Spiro Agnew to President Richard Nixon. Nixon's post-Apollo NASA budgeting withdrew support of all system components except the Shuttle, to which NASA applied the STS name.
The vehicle consisted of a spaceplane for orbit and re-entry, fueled by expendable liquid hydrogen and liquid oxygen tanks, with reusable strap-on solid booster rockets. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982, all launched from the Kennedy Space Center, Florida. The system was retired from service in 2011 after 135 missions, with "Atlantis" making the final launch of the three-decade Shuttle program on July 8, 2011. The program ended after "Atlantis" landed at the Kennedy Space Center on July 21, 2011. Major missions included launching numerous satellites and interplanetary probes, conducting space science experiments, and servicing and construction of space stations. The first orbiter vehicle, named "Enterprise", was built for the initial Approach and Landing Tests phase and lacked engines, heat shielding, and other equipment necessary for orbital flight. A total of five operational orbiters were built, and of these, two were destroyed in accidents.
It was used for orbital space missions by NASA, the US Department of Defense, the European Space Agency, Japan, and Germany. The United States funded Shuttle development and operations except for the Spacelab modules used on D1 and D2sponsored by Germany. SL-J was partially funded by Japan.
At launch, it consisted of the "stack", including the dark orange external tank (ET) (for the first two launches the tank was painted white); two white, slender solid rocket boosters (SRBs); and the Orbiter Vehicle, which contained the crew and payload. Some payloads were launched into higher orbits with either of two different upper stages developed for the STS (single-stage Payload Assist Module or two-stage Inertial Upper Stage). The Space Shuttle was stacked in the Vehicle Assembly Building, and the stack mounted on a mobile launch platform held down by four frangible nuts on each SRB, which were detonated at launch.
The Shuttle stack launched vertically like a conventional rocket. It lifted off under the power of its two SRBs and three main engines, which were fueled by liquid hydrogen and liquid oxygen from the ET. The Space Shuttle had a two-stage ascent. The SRBs provided additional thrust during liftoff and first-stage flight. About two minutes after liftoff, frangible nuts were fired, releasing the SRBs, which then parachuted into the ocean, to be retrieved by ships for refurbishment and reuse. The orbiter and ET continued to ascend on an increasingly horizontal flight path under power from its main engines. Upon reaching 17,500 mph (7.8 km/s), necessary for low Earth orbit, the main engines were shut down. The ET, attached by two frangible nuts was then jettisoned to burn up in the atmosphere. After jettisoning the external tank, the orbital maneuvering system (OMS) engines were used to adjust the orbit.
The orbiter carried astronauts and payloads such as satellites or space station parts into low Earth orbit, the Earth's upper atmosphere or thermosphere. Usually, five to seven crew members rode in the orbiter. Two crew members, the commander and pilot, were sufficient for a minimal flight, as in the first four "test" flights, STS-1 through STS-4. The typical payload capacity was about but could be increased depending on the choice of launch configuration. The orbiter carried its payload in a large cargo bay with doors that opened along the length of its top, a feature which made the Space Shuttle unique among spacecraft. This feature made possible the deployment of large satellites such as the Hubble Space Telescope and also the capture and return of large payloads back to Earth.
When the orbiter's space mission was complete, it fired its OMS thrusters to drop out of orbit and re-enter the lower atmosphere. During descent, the orbiter passed through different layers of the atmosphere and decelerated from hypersonic speed primarily by aerobraking. In the lower atmosphere and landing phase, it was more like a glider but with reaction control system (RCS) thrusters and fly-by-wire-controlled hydraulically actuated flight surfaces controlling its descent. It landed on a long runway as a conventional aircraft. The aerodynamic shape was a compromise between the demands of radically different speeds and air pressures during re-entry, hypersonic flight, and subsonic atmospheric flight. As a result, the orbiter had a relatively high sink rate at low altitudes, and it transitioned during re-entry from using RCS thrusters at very high altitudes to flight surfaces in the lower atmosphere.
Early history.
The formal design of what became the Space Shuttle began with the "Phase A" contract design studies issued in the late 1960s. Conceptualization had begun two decades earlier, before the Apollo program of the 1960s. One of the places the concept of a spacecraft returning from space to a horizontal landing originated was within NACA, in 1954, in the form of an aeronautics research experiment later named the X-15. The NACA proposal was submitted by Walter Dornberger.
In 1958, the X-15 concept further developed into proposal to launch an X-15 into space, and another X-series spaceplane proposal, named X-20 Dyna-Soar, as well as variety of aerospace plane concepts and studies. Neil Armstrong was selected to pilot both the X-15 and the X-20. Though the X-20 was not built, another spaceplane similar to the X-20 was built several years later and delivered to NASA in January 1966 called the HL-10 ("HL" indicated "horizontal landing").
In the mid-1960s, the US Air Force conducted classified studies on next-generation space transportation systems and concluded that semi-reusable designs were the cheapest choice. It proposed a development program with an immediate start on a "Class I" vehicle with expendable boosters, followed by slower development of a "Class II" semi-reusable design and possible "Class III" fully reusable design later. In 1967, George Mueller held a one-day symposium at NASA headquarters to study the options. Eighty people attended and presented a wide variety of designs, including earlier US Air Force designs such as the X-20 Dyna-Soar.
In 1968, NASA officially began work on what was then known as the Integrated Launch and Re-entry Vehicle (ILRV). At the same time, NASA held a separate Space Shuttle Main Engine (SSME) competition. NASA offices in Houston and Huntsville jointly issued a Request for Proposal (RFP) for ILRV studies to design a spacecraft that could deliver a payload to orbit but also re-enter the atmosphere and fly back to Earth. For example, one of the responses was for a two-stage design, featuring a large booster and a small orbiter, called the DC-3, one of several Phase A Shuttle designs. After the aforementioned "Phase A" studies, B, C, and D phases progressively evaluated in-depth designs up to 1972. In the final design, the bottom stage was recoverable solid rocket boosters, and the top stage used an expendable external tank.
In 1969, President Richard Nixon decided to support proceeding with Space Shuttle development. A series of development programs and analysis refined the basic design, prior to full development and testing. In August 1973, the X-24B proved that an unpowered spaceplane could re-enter Earth's atmosphere for a horizontal landing.
Across the Atlantic, European ministers met in Belgium in 1973 to authorize Western Europe's manned orbital project and its main contribution to Space Shuttlethe Spacelab program. Spacelab would provide a multidisciplinary orbital space laboratory and additional space equipment for the Shuttle.
Description.
The Space Shuttle was the first operational orbital spacecraft designed for reuse. It carried different payloads to low Earth orbit, provided crew rotation and supplies for the International Space Station (ISS), and performed satellite servicing and repair. The orbiter could also recover satellites and other payloads from orbit and return them to Earth. Each Shuttle was designed for a projected lifespan of 100 launches or ten years of operational life, although this was later extended. The person in charge of designing the STS was Maxime Faget, who had also overseen the Mercury, Gemini, and Apollo spacecraft designs. The crucial factor in the size and shape of the Shuttle orbiter was the requirement that it be able to accommodate the largest planned commercial and military satellites, and have over 1,000 mile cross-range recovery range to meet the requirement for classified USAF missions for a once-around abort from a launch to a polar orbit. The militarily specified 1,085 nm cross range requirement was one of the primary reasons for the Shuttle's large wings, compared to modern commercial designs with very minimal control surfaces and glide capability. Factors involved in opting for solid rockets and an expendable fuel tank included the desire of the Pentagon to obtain a high-capacity payload vehicle for satellite deployment, and the desire of the Nixon administration to reduce the costs of space exploration by developing a spacecraft with reusable components.
Each Space Shuttle was a reusable launch system composed of three main assemblies: the reusable OV, the expendable ET, and the two reusable SRBs. Only the OV entered orbit shortly after the tank and boosters are jettisoned. The vehicle was launched vertically like a conventional rocket, and the orbiter glided to a horizontal landing like an airplane, after which it was refurbished for reuse. The SRBs parachuted to splashdown in the ocean where they were towed back to shore and refurbished for later Shuttle missions.
Five operational OVs were built: "Columbia" (OV-102), "Challenger" (OV-099), "Discovery" (OV-103), "Atlantis" (OV-104), and "Endeavour" (OV-105). A mock-up, "Inspiration", currently stands at the entrance to the Astronaut Hall of Fame. An additional craft, "Enterprise" (OV-101), was built for atmospheric testing gliding and landing; it was originally intended to be outfitted for orbital operations after the test program, but it was found more economical to upgrade the structural test article STA-099 into orbiter "Challenger" (OV-099). "Challenger" disintegrated 73 seconds after launch in 1986, and "Endeavour" was built as a replacement from structural spare components. Building "Endeavour" cost about US$1.7 billion. "Columbia" broke apart over Texas during re-entry in 2003. A Space Shuttle launch cost around $450 million.
Roger A. Pielke, Jr. has estimated that the Space Shuttle program cost about US$170 billion (2008 dollars) through early 2008; the average cost per flight was about US$1.5 billion. Two missions were paid for by Germany, Spacelab D1 and D2 (D for "Deutschland") with a payload control center in Oberpfaffenhofen. D1 was the first time that control of a manned STS mission payload was not in U.S. hands.
At times, the orbiter itself was referred to as the Space Shuttle. This was not technically correct as the "Space Shuttle" was the combination of the orbiter, the external tank, and the two solid rocket boosters. These components, once assembled in the Vehicle Assembly Building originally built to assemble the Apollo Saturn V rocket, were commonly referred to as the "stack".
Responsibility for the Shuttle components was spread among multiple NASA field centers. The Kennedy Space Center was responsible for launch, landing and turnaround operations for equatorial orbits (the only orbit profile actually used in the program), the US Air Force at the Vandenberg Air Force Base was responsible for launch, landing and turnaround operations for polar orbits (though this was never used), the Johnson Space Center served as the central point for all Shuttle operations, the Marshall Space Flight Center was responsible for the main engines, external tank, and solid rocket boosters, the John C. Stennis Space Center handled main engine testing, and the Goddard Space Flight Center managed the global tracking network.
Orbiter vehicle.
The orbiter resembled a conventional aircraft, with double-delta wings swept 81° at the inner leading edge and 45° at the outer leading edge. Its vertical stabilizer's leading edge was swept back at a 50° angle. The four elevons, mounted at the trailing edge of the wings, and the rudder/speed brake, attached at the trailing edge of the stabilizer, with the body flap, controlled the orbiter during descent and landing.
The orbiter's -long payload bay, comprising most of the fuselage, could accommodate cylindrical payloads up to in diameter. Information declassified in 2011 showed that these measurements were chosen specifically to accommodate the KH-9 HEXAGON spy satellite operated by the National Reconnaissance Office. Two mostly-symmetrical lengthwise payload bay doors hinged on either side of the bay comprised its entire top. Payloads were generally loaded horizontally into the bay while the orbiter was standing upright on the launch pad and unloaded vertically in the near-weightless orbital environment by the orbiter's robotic remote manipulator arm (under astronaut control), EVA astronauts, or under the payloads' own power (as for satellites attached to a rocket "upper stage" for deployment.)
Three Space Shuttle Main Engines (SSMEs) were mounted on the orbiter's aft fuselage in a triangular pattern. The engine nozzles could gimbal 10.5 degrees up and down, and 8.5 degrees from side to side during ascent to change the direction of their thrust to steer the Shuttle. The orbiter structure was made primarily from aluminum alloy, although the engine structure was made primarily from titanium alloy.
The operational orbiters built were OV-102 "Columbia", OV-099 "Challenger", OV-103 "Discovery", OV-104 "Atlantis", and OV-105 "Endeavour".
External tank.
The main function of the Space Shuttle external tank was to supply the liquid oxygen and hydrogen fuel to the main engines. It was also the backbone of the launch vehicle, providing attachment points for the two solid rocket boosters and the orbiter. The external tank was the only part of the Shuttle system that was not reused. Although the external tanks were always discarded, it would have been possible to take them into orbit and re-use them (such as for incorporation into a space station).
Solid rocket boosters.
Two solid rocket boosters (SRBs) each provided of thrust at liftoff, which was 83% of the total thrust at liftoff. The SRBs were jettisoned two minutes after launch at a height of about , and then deployed parachutes and landed in the ocean to be recovered. The SRB cases were made of steel about ½ inch (13 mm) thick. The solid rocket boosters were re-used many times; the casing used in Ares I engine testing in 2009 consisted of motor cases that had been flown, collectively, on 48 Shuttle missions, including STS-1.
Astronauts who have flown on multiple spacecraft report that Shuttle delivers a rougher ride than Apollo or Soyuz. The additional vibration is caused by the solid rocket boosters, as solid fuel does not burn as evenly as liquid fuel. The vibration dampens down after the solid rocket boosters have been jettisoned.
Orbiter add-ons.
The orbiter could be used in conjunction with a variety of add-ons depending on the mission. This included orbital laboratories (Spacelab, Spacehab), boosters for launching payloads farther into space (Inertial Upper Stage, Payload Assist Module), and other functions, such as provided by Extended Duration Orbiter, Multi-Purpose Logistics Modules, or Canadarm (RMS). An upper stage called Transfer Orbit Stage (Orbital Science Corp. TOS-21) was also used once. Other types of systems and racks were part of the modular Spacelab system pallets, igloo, IPS, etc., which also supported special missions such as SRTM.
Spacelab.
A major component of the Space Shuttle Program was Spacelab, primarily contributed by a consortium of European countries, and operated in conjunction with the United States and international partners. Supported by a modular system of pressurized modules, pallets, and systems, Spacelab missions executed on multidisciplinary science, orbital logistics, and international cooperation. Over 29 missions flew on subjects ranging from astronomy, microgravity, radar, and life sciences, to name a few. Spacelab hardware also supported missions such as Hubble (HST) servicing and space station resupply. STS-2 and STS-3 provided testing, and the first full mission was Spacelab-1 (STS-9) launched on November 28, 1983.
Spacelab formally began in 1973, after a meeting in Brussels, Belgium, by European heads of state. Within the decade, Spacelab went into orbit and provided Europe and the United States with an orbital workshop and hardware system. International cooperation, science, and exploration were realized on Spacelab.
Flight systems.
The Shuttle was one of the earliest craft to use a computerized fly-by-wire digital flight control system. This means no mechanical or hydraulic linkages connected the pilot's control stick to the control surfaces or reaction control system thrusters. The control algorithm, which used a classical Proportional Integral Derivative (PID) approach, was developed and maintained by Honeywell. The Shuttle's fly-by-wire digital flight control system was composed of 4 control systems each addressing a different mission phase: Ascent, Descent, On-Orbit and Aborts. Honeywell is also credited with the design and implementation of the Shuttle's Nose Wheel Steering Control Algorithm that allowed the Orbiter to safely land at Kennedy Space Center's Shuttle Runway.
A concern with using digital fly-by-wire systems on the Shuttle was reliability. Considerable research went into the Shuttle computer system. The Shuttle used five identical redundant IBM 32-bit general purpose computers (GPCs), model AP-101, constituting a type of embedded system. Four computers ran specialized software called the Primary Avionics Software System (PASS). A fifth backup computer ran separate software called the Backup Flight System (BFS). Collectively they were called the Data Processing System (DPS).
The design goal of the Shuttle's DPS was fail-operational/fail-safe reliability. After a single failure, the Shuttle could still continue the mission. After two failures, it could still land safely.
The four general-purpose computers operated essentially in lockstep, checking each other. If one computer provided a different result than the other three (i.e. the one computer failed), the three functioning computers "voted" it out of the system. This isolated it from vehicle control. If a second computer of the three remaining failed, the two functioning computers voted it out. A very unlikely failure mode would have been where two of the computers produced result A, and two produced result B (a two-two split). In this unlikely case, one group of two was to be picked at random.
The Backup Flight System (BFS) was separately developed software running on the fifth computer, used only if the entire four-computer primary system failed. The BFS was created because although the four primary computers were hardware redundant, they all ran the same software, so a generic software problem could crash all of them. Embedded system avionic software was developed under totally different conditions from public commercial software: the number of code lines was tiny compared to a public commercial software product, changes were only made infrequently and with extensive testing, and many programming and test personnel worked on the small amount of computer code. However, in theory it could have still failed, and the BFS existed for that contingency. While the BFS could run in parallel with PASS, the BFS never engaged to take over control from PASS during any Shuttle mission.
The software for the Shuttle computers was written in a high-level language called HAL/S, somewhat similar to PL/I. It is specifically designed for a real time embedded system environment.
The IBM AP-101 computers originally had about 424 kilobytes of magnetic core memory each. The CPU could process about 400,000 instructions per second. They had no hard disk drive, and loaded software from magnetic tape cartridges.
In 1990, the original computers were replaced with an upgraded model AP-101S, which had about 2.5 times the memory capacity (about 1 megabyte) and three times the processor speed (about 1.2 million instructions per second). The memory was changed from magnetic core to semiconductor with battery backup.
Early Shuttle missions, starting in November 1983, took along the Grid Compass, arguably one of the first laptop computers. The GRiD was given the name SPOC, for Shuttle Portable Onboard Computer. Use on the Shuttle required both hardware and software modifications which were incorporated into later versions of the commercial product. It was used to monitor and display the Shuttle's ground position, path of the next two orbits, show where the Shuttle had line of sight communications with ground stations, and determine points for location-specific observations of the Earth. The Compass sold poorly, as it cost at least US$8000, but it offered unmatched performance for its weight and size. NASA was one of its main customers.
During its service life, the Shuttle's Control System never experienced a failure. Many of the lessons learned have been used to design today's high speed control algorithms.
Orbiter markings and insignia.
The prototype orbiter "Enterprise" originally had a flag of the United States on the upper surface of the left wing and the letters "USA" in black on the right wing. The name "Enterprise" was painted in black on the payload bay doors just above the hinge and behind the crew module; on the aft end of the payload bay doors was the NASA "worm" logotype in gray. Underneath the rear of the payload bay doors on the side of the fuselage just above the wing is the text "United States" in black with a flag of the United States ahead of it.
The first operational orbiter, "Columbia", originally had the same markings as "Enterprise", although the letters "USA" on the right wing were slightly larger and spaced farther apart. "Columbia" also had black markings which "Enterprise" lacked on its forward RCS module, around the cockpit windows, and on its vertical stabilizer, and had distinctive black "chines" on the forward part of its upper wing surfaces, which none of the other orbiters had.
"Challenger" established a modified marking scheme for the shuttle fleet that was matched by "Discovery", "Atlantis" and "Endeavour". The letters "USA" in black above an American flag were displayed on the left wing, with the NASA "worm" logotype in gray centered above the name of the orbiter in black on the right wing. The name of the orbiter was inscribed not on the payload bay doors, but on the forward fuselage just below and behind the cockpit windows. This would make the name visible when the shuttle was photographed in orbit with the doors open.
In 1983, "Enterprise" had its wing markings changed to match "Challenger", and the NASA "worm" logotype on the aft end of the payload bay doors was changed from gray to black. Some black markings were added to the nose, cockpit windows and vertical tail to more closely resemble the flight vehicles, but the name "Enterprise" remained on the payload bay doors as there was never any need to open them. "Columbia" had its name moved to the forward fuselage to match the other flight vehicles after STS-61-C, during the 1986–88 hiatus when the shuttle fleet was grounded following the loss of "Challenger", but retained its original wing markings until its last overhaul (after STS-93), and its unique black wing "chines" for the remainder of its operational life.
Beginning in 1998, the flight vehicles' markings were modified to incorporate the NASA "meatball" insignia. The "worm" logotype, which the agency had phased out, was removed from the payload bay doors and the "meatball" insignia was added aft of the "United States" text on the lower aft fuselage. The "meatball" insignia was also displayed on the left wing, with the American flag above the orbiter's name, left-justified rather than centered, on the right wing. The three surviving flight vehicles, "Discovery", "Atlantis" and "Endeavour", still bear these markings as museum displays. "Enterprise" became the property of the Smithsonian Institution in 1985 and was no longer under NASA's control when these changes were made, hence the prototype orbiter still has its 1983 markings and still has its name on the payload bay doors.
Upgrades.
The Space Shuttle was initially developed in the 1970s, but received many upgrades and modifications afterward to improve performance, reliability and safety. Internally, the Shuttle remained largely similar to the original design, with the exception of the improved avionics computers. In addition to the computer upgrades, the original analog primary flight instruments were replaced with modern full-color, flat-panel display screens, called a glass cockpit, which is similar to those of contemporary airliners. To facilitate construction of ISS, the internal airlocks of each orbiter except "Columbia" were replaced with external docking systems to allow for a greater amount of cargo to be stored on the Shuttle's mid-deck during station resupply missions.
The Space Shuttle Main Engines (SSMEs) had several improvements to enhance reliability and power. This explains phrases such as "Main engines throttling up to 104 percent." This did not mean the engines were being run over a safe limit. The 100 percent figure was the original specified power level. During the lengthy development program, Rocketdyne determined the engine was capable of safe reliable operation at 104 percent of the originally specified thrust. NASA could have rescaled the output number, saying in essence 104 percent is now 100 percent. To clarify this would have required revising much previous documentation and software, so the 104 percent number was retained. SSME upgrades were denoted as "block numbers", such as block I, block II, and block IIA. The upgrades improved engine reliability, maintainability and performance. The 109% thrust level was finally reached in flight hardware with the Block II engines in 2001. The normal maximum throttle was 104 percent, with 106 percent or 109 percent used for mission aborts.
For the first two missions, STS-1 and STS-2, the external tank was painted white to protect the insulation that covers much of the tank, but improvements and testing showed that it was not required. The weight saved by not painting the tank resulted in an increase in payload capability to orbit. Additional weight was saved by removing some of the internal "stringers" in the hydrogen tank that proved unnecessary. The resulting "light-weight external tank" was first flown on STS-6 and used on the majority of Shuttle missions. STS-91 saw the first flight of the "super light-weight external tank". This version of the tank was made of the 2195 aluminum-lithium alloy. It weighed 3.4 metric tons (7,500 lb) less than the last run of lightweight tanks, allowing the Shuttle to deliver heavy elements to ISS's high inclination orbit. As the Shuttle was always operated with a crew, each of these improvements was first flown on operational mission flights.
The solid rocket boosters underwent improvements as well. Design engineers added a third O-ring seal to the joints between the segments after the 1986 Space Shuttle "Challenger" disaster.
Several other SRB improvements were planned to improve performance and safety, but never came to be. These culminated in the considerably simpler, lower cost, probably safer and better-performing Advanced Solid Rocket Booster. These rockets entered production in the early to mid-1990s to support the Space Station, but were later canceled to save money after the expenditure of $2.2 billion. The loss of the ASRB program resulted in the development of the Super LightWeight external Tank (SLWT), which provided some of the increased payload capability, while not providing any of the safety improvements. In addition, the US Air Force developed their own much lighter single-piece SRB design using a filament-wound system, but this too was canceled.
STS-70 was delayed in 1995, when woodpeckers bored holes in the foam insulation of "Discovery"'s external tank. Since then, NASA has installed commercial plastic owl decoys and inflatable owl balloons which had to be removed prior to launch. The delicate nature of the foam insulation had been the cause of damage to the Thermal Protection System, the tile heat shield and heat wrap of the orbiter. NASA remained confident that this damage, while it was the primary cause of the Space Shuttle "Columbia" disaster on February 1, 2003, would not jeopardize the completion of the International Space Station (ISS) in the projected time allotted.
A cargo-only, unmanned variant of the Shuttle was variously proposed and rejected since the 1980s. It was called the Shuttle-C, and would have traded re-usability for cargo capability, with large potential savings from reusing technology developed for the Space Shuttle. Another proposal was to convert the payload bay into a passenger area, with versions ranging from 30 to 74 seats, three days in orbit, and cost US$1.5 million per seat.
On the first four Shuttle missions, astronauts wore modified US Air Force high-altitude full-pressure suits, which included a full-pressure helmet during ascent and descent. From the fifth flight, STS-5, until the loss of "Challenger", one-piece light blue nomex flight suits and partial-pressure helmets were worn. A less-bulky, partial-pressure version of the high-altitude pressure suits with a helmet was reinstated when Shuttle flights resumed in 1988. The Launch-Entry Suit ended its service life in late 1995, and was replaced by the full-pressure Advanced Crew Escape Suit (ACES), which resembled the Gemini space suit in design, but retained the orange color of the Launch-Entry Suit.
To extend the duration that orbiters could stay docked at the ISS, the Station-to-Shuttle Power Transfer System (SSPTS) was installed. The SSPTS allowed these orbiters to use power provided by the ISS to preserve their consumables. The SSPTS was first used successfully on STS-118.
Specifications.
Orbiter (for "Endeavour", OV-105)
External tank (for SLWT)
Solid Rocket Boosters
System Stack
Mission profile.
Launch preparation.
All Space Shuttle missions were launched from Kennedy Space Center (KSC). The weather criteria used for launch included, but were not limited to: precipitation, temperatures, cloud cover, lightning forecast, wind, and humidity. The Shuttle was not launched under conditions where it could have been struck by lightning. Aircraft are often struck by lightning with no adverse effects because the electricity of the strike is dissipated through its conductive structure and the aircraft is not electrically grounded. Like most jet airliners, the Shuttle was mainly constructed of conductive aluminum, which would normally shield and protect the internal systems. However, upon liftoff the Shuttle sent out a long exhaust plume as it ascended, and this plume could have triggered lightning by providing a current path to ground. The NASA Anvil Rule for a Shuttle launch stated that an anvil cloud could not appear within a distance of 10 nautical miles. The Shuttle Launch Weather Officer monitored conditions until the final decision to scrub a launch was announced. In addition, the weather conditions had to be acceptable at one of the Transatlantic Abort Landing sites (one of several Space Shuttle abort modes) to launch as well as the solid rocket booster recovery area. While the Shuttle might have safely endured a lightning strike, a similar strike caused problems on Apollo 12, so for safety NASA chose not to launch the Shuttle if lightning was possible (NPR8715.5).
Historically, the Shuttle was not launched if its flight would run from December to January (a year-end rollover or YERO). Its flight software, designed in the 1970s, was not designed for this, and would require the orbiter's computers be reset through a change of year, which could cause a glitch while in orbit. In 2007, NASA engineers devised a solution so Shuttle flights could cross the year-end boundary.
Launch.
After the final hold in the countdown at T-minus 9 minutes, the Shuttle went through its final preparations for launch, and the countdown was automatically controlled by the Ground Launch Sequencer (GLS), software at the Launch Control Center, which stopped the count if it sensed a critical problem with any of the Shuttle's onboard systems. The GLS handed off the count to the Shuttle's on-board computers at T minus 31 seconds, in a process called auto sequence start.
At T-minus 16 seconds, the massive sound suppression system (SPS) began to drench the Mobile Launcher Platform (MLP) and SRB trenches with of water to protect the Orbiter from damage by acoustical energy and rocket exhaust reflected from the flame trench and MLP during lift off.
At T-minus 10 seconds, hydrogen igniters were activated under each engine bell to quell the stagnant gas inside the cones before ignition. Failure to burn these gases could trip the onboard sensors and create the possibility of an overpressure and explosion of the vehicle during the firing phase. The main engine turbopumps also began charging the combustion chambers with liquid hydrogen and liquid oxygen at this time. The computers reciprocated this action by allowing the redundant computer systems to begin the firing phase.
The three main engines (SSMEs) started at "T"-6.6 seconds. The main engines ignited sequentially via the Shuttle's general purpose computers (GPCs) at 120 millisecond intervals. All three SSMEs were required to reach 90% rated thrust within three seconds, otherwise the onboard computers would initiate an RSLS abort. If all three engines indicated nominal performance by "T"-3 seconds, they were commanded to gimbal to liftoff configuration and the command would be issued to arm the SRBs for ignition at "T"-0. Between "T"-6.6 seconds and "T"-3 seconds, while the SSMEs were firing but the SRBs were still bolted to the pad, the offset thrust caused the entire launch stack (boosters, tank and orbiter) to pitch down measured at the tip of the external tank. The three second delay after confirmation of SSME operation was to allow the stack to return to nearly vertical. At "T"-0 seconds, the 8 frangible nuts holding the SRBs to the pad were detonated, the SSMEs were commanded to 100% throttle, and the SRBs were ignited. By "T"+0.23 seconds, the SRBs built up enough thrust for liftoff to commence, and reached maximum chamber pressure by "T"+0.6 seconds. The Johnson Space Center's Mission Control Center assumed control of the flight once the SRBs had cleared the launch tower.
Shortly after liftoff, the Shuttle's main engines were throttled up to 104.5% and the vehicle began a combined roll, pitch and yaw maneuver that placed it onto the correct heading (azimuth) for the planned orbital inclination and in a heads down attitude with wings level. The Shuttle flew upside down during the ascent phase. This orientation allowed a trim angle of attack that was favorable for aerodynamic loads during the region of high dynamic pressure, resulting in a net positive load factor, as well as providing the flight crew with a view of the horizon as a visual reference. The vehicle climbed in a progressively flattening arc, accelerating as the mass of the SRBs and main tank decreased. To achieve low orbit requires much more horizontal than vertical acceleration. This was not visually obvious, since the vehicle rose vertically and was out of sight for most of the horizontal acceleration. The near circular orbital velocity at the altitude of the International Space Station is , roughly equivalent to Mach 23 at sea level. As the International Space Station orbits at an inclination of 51.6 degrees, missions going there must set orbital inclination to the same value in order to rendezvous with the station.
Around 30 seconds into ascent, the SSMEs were throttled down—usually to 72%, though this varied—to reduce the maximum aerodynamic forces acting on the Shuttle at a point called Max Q. Additionally, the propellant grain design of the SRBs caused their thrust to drop by about 30% by 50 seconds into ascent. Once the Orbiter's guidance verified that Max Q would be within Shuttle structural limits, the main engines were throttled back up to 104.5%; this throttling down and back up was called the "thrust bucket". To maximize performance, the throttle level and timing of the thrust bucket was shaped to bring the Shuttle as close to aerodynamic limits as possible.
At around "T"+126 seconds, pyrotechnic fasteners released the SRBs and small separation rockets pushed them laterally away from the vehicle. The SRBs parachuted back to the ocean to be reused. The Shuttle then began accelerating to orbit on the main engines. Acceleration at this point would typically fall to .9 "g", and the vehicle would take on a somewhat nose-up angle to the horizonit used the main engines to gain and then maintain altitude while it accelerated horizontally towards orbit. At about five and three-quarter minutes into ascent, the orbiter's direct communication links with the ground began to fade, at which point it rolled heads up to reroute its communication links to the Tracking and Data Relay Satellite system.
At about seven and a half minutes into ascent, the mass of the vehicle was low enough that the engines had to be throttled back to limit vehicle acceleration to 3 "g" (29.4 m/s² or 96.5 ft/s², equivalent to accelerating from zero to in a second). The Shuttle would maintain this acceleration for the next minute, and main engine cut-off (MECO) occurred at about eight and a half minutes after launch. The main engines were shut down before complete depletion of propellant, as running dry would have destroyed the engines. The oxygen supply was terminated before the hydrogen supply, as the SSMEs reacted unfavorably to other shutdown modes. (Liquid oxygen has a tendency to react violently, and supports combustion when it encounters hot engine metal.) A few seconds after MECO, the external tank was released by firing pyrotechnic fasteners.
At this point the Shuttle and external tank were on a slightly suborbital trajectory, coasting up towards apogee. Once at apogee, about half an hour after MECO, the Shuttle's Orbital Maneuvering System (OMS) engines were fired to raise its perigee and achieve orbit, while the external tank fell back into the atmosphere and burned up over the Indian Ocean or the Pacific Ocean depending on launch profile. The sealing action of the tank plumbing and lack of pressure relief systems on the external tank helped it break up in the lower atmosphere. After the foam burned away during re-entry, the heat caused a pressure buildup in the remaining liquid oxygen and hydrogen until the tank exploded. This ensured that any pieces that fell back to Earth were small.
Ascent tracking.
The Shuttle was monitored throughout its ascent for short range tracking (10 seconds before liftoff through 57 seconds after), medium range (7 seconds before liftoff through 110 seconds after) and long range (7 seconds before liftoff through 165 seconds after). Short range cameras included 22 16mm cameras on the Mobile Launch Platform and 8 16mm on the Fixed Service Structure, 4 high speed fixed cameras located on the perimeter of the launch complex plus an additional 42 fixed cameras with 16mm motion picture film. Medium range cameras included remotely operated tracking cameras at the launch complex plus 6 sites along the immediate coast north and south of the launch pad, each with 800mm lens and high speed cameras running 100 frames per second. These cameras ran for only 4–10 seconds due to limitations in the amount of film available. Long range cameras included those mounted on the external tank, SRBs and orbiter itself which streamed live video back to the ground providing valuable information about any debris falling during ascent. Long range tracking cameras with 400-inch film and 200-inch video lenses were operated by a photographer at Playalinda Beach as well as 9 other sites from 38 miles north at the Ponce Inlet to 23 miles south to Patrick Air Force Base (PAFB) and additional mobile optical tracking camera was stationed on Merritt Island during launches. A total of 10 HD cameras were used both for ascent information for engineers and broadcast feeds to networks such as NASA TV and HDNet. The number of cameras significantly increased and numerous existing cameras were upgraded at the recommendation of the Columbia Accident Investigation Board to provide better information about the debris during launch. Debris was also tracked using a pair of Weibel Continuous Pulse Doppler X-band radars, one on board the SRB recovery ship MV Liberty Star positioned north east of the launch pad and on a ship positioned south of the launch pad. Additionally, during the first 2 flights following the loss of Columbia and her crew, a pair of NASA WB-57 reconnaissance aircraft equipped with HD Video and Infrared flew at to provide additional views of the launch ascent. Kennedy Space Center also invested nearly $3 million in improvements to the digital video analysis systems in support of debris tracking.
In orbit.
Once in orbit, the Shuttle usually flew at an altitude of 320 kilometers (200 miles), and occasionally as high as 650 kilometers. In the 1980s and 1990s, many flights involved space science missions on the NASA/ESA Spacelab, or launching various types of satellites and science probes. By the 1990s and 2000s the focus shifted more to servicing the space station, with fewer satellite launches. Most missions involved staying in orbit several days to two weeks, although longer missions were possible with the Extended Duration Orbiter add-on or when attached to a space station.
Re-entry and landing.
Almost the entire Space Shuttle re-entry procedure, except for lowering the landing gear and deploying the air data probes, was normally performed under computer control. However, the re-entry could be flown entirely manually if an emergency arose. The approach and landing phase could be controlled by the autopilot, but was usually hand flown.
The vehicle began re-entry by firing the Orbital maneuvering system engines, while flying upside down, backside first, in the opposite direction to orbital motion for approximately three minutes, which reduced the Shuttle's velocity by about . The resultant slowing of the Shuttle lowered its orbital perigee down into the upper atmosphere. The Shuttle then flipped over, by pushing its nose down (which was actually "up" relative to the Earth, because it was flying upside down). This OMS firing was done roughly halfway around the globe from the landing site.
The vehicle started encountering more significant air density in the lower thermosphere at about , at around Mach 25, . The vehicle was controlled by a combination of RCS thrusters and control surfaces, to fly at a 40-degree nose-up attitude, producing high drag, not only to slow it down to landing speed, but also to reduce reentry heating. As the vehicle encountered progressively denser air, it began a gradual transition from spacecraft to aircraft. In a straight line, its 40-degree nose-up attitude would cause the descent angle to flatten-out, or even rise. The vehicle therefore performed a series of four steep S-shaped banking turns, each lasting several minutes, at up to 70 degrees of bank, while still maintaining the 40-degree angle of attack. In this way it dissipated speed sideways rather than upwards. This occurred during the 'hottest' phase of re-entry, when the heat-shield glowed red and the G-forces were at their highest. By the end of the last turn, the transition to aircraft was almost complete. The vehicle leveled its wings, lowered its nose into a shallow dive and began its approach to the landing site.
The orbiter's maximum glide ratio/lift-to-drag ratio varies considerably with speed, ranging from 1:1 at hypersonic speeds, 2:1 at supersonic speeds and reaching 4.5:1 at subsonic speeds during approach and landing.
In the lower atmosphere, the orbiter flies much like a conventional glider, except for a much higher descent rate, over or 9,800 fpm. At approximately Mach 3, two air data probes, located on the left and right sides of the orbiter's forward lower fuselage, are deployed to sense air pressure related to the vehicle's movement in the atmosphere.
Final approach and landing phase.
When the approach and landing phase began, the orbiter was at a altitude, from the runway. The pilots applied aerodynamic braking to help slow down the vehicle. The orbiter's speed was reduced from , approximately, at touch-down (compared to for a jet airliner). The landing gear was deployed while the Orbiter was flying at . To assist the speed brakes, a drag chute was deployed either after main gear or nose gear touchdown (depending on selected chute deploy mode) at about . The chute was jettisoned once the orbiter slowed to .
Post-landing processing.
After landing, the vehicle stayed on the runway for several hours for the orbiter to cool. Teams at the front and rear of the orbiter tested for presence of hydrogen, hydrazine, monomethylhydrazine, nitrogen tetroxide and ammonia (fuels and by-products of the reaction control system and the orbiter's three APUs). If hydrogen was detected, an emergency would be declared, the orbiter powered down and teams would evacuate the area. A convoy of 25 specially designed vehicles and 150 trained engineers and technicians approached the orbiter. Purge and vent lines were attached to remove toxic gases from fuel lines and the cargo bay about 45–60 minutes after landing. A flight surgeon boarded the orbiter for initial medical checks of the crew before disembarking. Once the crew left the orbiter, responsibility for the vehicle was handed from the Johnson Space Center back to the Kennedy Space Center
If the mission ended at Edwards Air Force Base in California, White Sands Space Harbor in New Mexico, or any of the runways the orbiter might use in an emergency, the orbiter was loaded atop the Shuttle Carrier Aircraft, a modified 747, for transport back to the Kennedy Space Center, landing at the Shuttle Landing Facility. Once at the Shuttle Landing Facility, the orbiter was then towed along a tow-way and access roads normally used by tour buses and KSC employees to the Orbiter Processing Facility where it began a months-long preparation process for the next mission.
Landing sites.
NASA preferred Space Shuttle landings to be at Kennedy Space Center. If weather conditions made landing there unfavorable, the Shuttle could delay its landing until conditions are favorable, touch down at Edwards Air Force Base, California, or use one of the multiple alternate landing sites around the world. A landing at any site other than Kennedy Space Center meant that after touchdown the Shuttle must be mated to the Shuttle Carrier Aircraft and returned to Cape Canaveral. Space Shuttle "Columbia" (STS-3) once landed at the White Sands Space Harbor, New Mexico; this was viewed as a last resort as NASA scientists believe that the sand could potentially damage the Shuttle's exterior.
There were many alternative landing sites that were never used.
Risk contributors.
An example of technical risk analysis for a STS mission is SPRA iteration 3.1 top risk contributors for STS-133:
An internal NASA risk assessment study (conducted by the Shuttle Program Safety and Mission Assurance Office at Johnson Space Center) released in late 2010 or early 2011 concluded that the agency had seriously underestimated the level of risk involved in operating the Shuttle. The report assessed that there was a 1 in 9 chance of a catastrophic disaster during the first nine flights of the Shuttle but that safety improvements had later improved the risk ratio to 1 in 90.
Fleet history.
Below is a list of major events in the Space Shuttle orbiter fleet.
Sources: NASA launch manifest, NASA Space Shuttle archive
Shuttle disasters.
On January 28, 1986, "Challenger" disintegrated 73 seconds after launch due to the failure of the right SRB, killing all seven astronauts on board. The disaster was caused by low-temperature impairment of an O-ring, a mission critical seal used between segments of the SRB casing. The failure of a lower O-ring seal allowed hot combustion gases to escape from between the booster sections and burn through the adjacent external tank, causing it to explode. Repeated warnings from design engineers voicing concerns about the lack of evidence of the O-rings' safety when the temperature was below 53 °F (12 °C) had been ignored by NASA managers.
On February 1, 2003, "Columbia" disintegrated during re-entry, killing its crew of seven, because of damage to the carbon-carbon leading edge of the wing caused during launch. Ground control engineers had made three separate requests for high-resolution images taken by the Department of Defense that would have provided an understanding of the extent of the damage, while NASA's chief thermal protection system (TPS) engineer requested that astronauts on board "Columbia" be allowed to leave the vehicle to inspect the damage. NASA managers intervened to stop the Department of Defense's assistance and refused the request for the spacewalk, and thus the feasibility of scenarios for astronaut repair or rescue by "Atlantis" were not considered by NASA management at the time.
Retirement.
NASA retired the Space Shuttle in 2011, after 30 years of service. The Shuttle was originally conceived of and presented to the public as a "Space Truck", which would, among other things, be used to build a United States space station in low earth orbit in the early 1990s. When the US space station evolved into the International Space Station project, which suffered from long delays and design changes before it could be completed, the service life of the Space Shuttle was extended several times until 2011, serving at least 15 years longer than it was originally designed to do. "Discovery" was the first of NASA's three remaining operational Space Shuttles to be retired.
The final Space Shuttle mission was originally scheduled for late 2010, but the program was later extended to July 2011 when Michael Suffredini of the ISS program said that one additional trip was needed in 2011 to deliver parts to the International Space Station. The Shuttle's final mission consisted of just four astronauts—Christopher Ferguson (Commander), Douglas Hurley (Pilot), Sandra Magnus (Mission Specialist 1), and Rex Walheim (Mission Specialist 2); they conducted the 135th and last space Shuttle mission on board "Atlantis", which launched on July 8, 2011, and landed safely at the Kennedy Space Center on July 21, 2011, at 5:57 AM EDT (09:57 UTC).
Distribution of orbiters and other hardware.
NASA announced it would transfer orbiters to education institutions or museums at the conclusion of the Space Shuttle program. Each museum or institution is responsible for covering the cost of preparing and transporting each vehicle for display. Twenty museums from across the country submitted proposals for receiving one of the retired orbiters. NASA also made Space Shuttle thermal protection system tiles available to schools and universities for less than US$25 each. About 7,000 tiles were available on a first-come, first-served basis, limited to one per institution.
On April 12, 2011, NASA announced selection of locations for the remaining Shuttle orbiters:
Flight and mid-deck training hardware will be taken from the Johnson Space Center and will go to the National Air and Space Museum and the National Museum of the U.S. Air Force. The full fuselage mockup, which includes the payload bay and aft section but no wings, is to go to the Museum of Flight in Seattle. Mission Simulation and Training Facility's fixed simulator will go to the Adler Planetarium in Chicago, and the motion simulator will go to the Texas A&M Aerospace Engineering Department in College Station, Texas. Other simulators used in Shuttle astronaut training will go to the Wings of Dreams Aviation Museum in Starke, Florida and the Virginia Air and Space Center in Hampton, Virginia.
In August 2011, the NASA Office of Inspector General (OIG) published a "Review of NASA's Selection of Display Locations for the Space Shuttle Orbiters"; the review had four main findings: 
The NASA OIG had three recommendations, saying NASA should:
In September 2011, the CEO and two board members of Seattle's Museum of Flight met with NASA Administrator Charles Bolden, pointing out "significant errors in deciding where to put its four retiring Space Shuttles"; the errors alleged include inaccurate information on Museum of Flight's attendance and international visitor statistics, as well as the readiness of the Intrepid Sea-Air-Space Museum's exhibit site.
Space Shuttle successors and legacy.
Until another US manned spacecraft is ready, crews will travel to and from the International Space Station (ISS) exclusively aboard the Russian Soyuz spacecraft.
A planned successor to STS was the "Shuttle II", during the 1980s and 1990s, and later the Constellation program during the 2004–2010 period. CSTS was a proposal to continue to operate STS commercially, after NASA. In September 2011, NASA announced the selection of the design for the new Space Launch System that is planned to launch the Orion spacecraft and other hardware to missions beyond low earth-orbit.
The Commercial Orbital Transportation Services program began in 2006 with the purpose of creating commercially operated unmanned cargo vehicles to service the ISS. The Commercial Crew Development (CCDev) program was started in 2010 to create commercially operated manned spacecraft capable of delivering at least four crew members to the ISS, to stay docked for 180 days, and then return them back to Earth. These spacecraft were to become operational in the 2010s.
In culture.
Space Shuttles have been features of fiction and nonfiction, from children's movies to documentaries. Early examples include the 1979 James Bond film, "Moonraker", the 1982 Activision videogame "Space Shuttle: A Journey into Space" (1982) and G. Harry Stine's 1981 novel "Shuttle Down". In the 1986 film "SpaceCamp", "Atlantis" accidentally launched into space with a group of U.S. Space Camp participants as its crew. The 1998 film "Armageddon" portrayed a combined crew of offshore oil rig workers and US military staff who pilot two modified Shuttles to avert the destruction of Earth by an asteroid. Retired American test pilots visited a Russian satellite in the 2000 Clint Eastwood adventure film "Space Cowboys". In the 2003 film "The Core," the "Endeavour"'s landing is disrupted by the earth's magnetic core, and its crew is selected to pilot the vehicle designed to restart the core. The 2004 Bollywood movie "Swades", where a Space Shuttle was used to launch a special rainfall monitoring satellite, was filmed at Kennedy Space Center in the year following the Columbia disaster that had taken the life of Indian-American astronaut KC Chawla. On television, the 1996 drama "The Cape" portrayed the lives of a group of NASA astronauts as they prepared for and flew Shuttle missions. "Odyssey 5" was a short lived sci-fi series that featured the crew of a Space Shuttle as the last survivors of a disaster that destroyed Earth. The 1997- 2007 Sci-fi series Stargate SG-1 had a shuttle rescue written into an episode. The 2013 film "Gravity" features the fictional space shuttle "Explorer", whose crew are killed or left stranded after it is destroyed by a shower of high speed orbital debris.
The Space Shuttle has also been the subject of toys and models; for example, a large Lego Space Shuttle model was constructed by visitors at Kennedy Space Center, and smaller models have been sold commercially as a standard "LegoLand" set. A 1980 pinball machine "Space Shuttle" was produced by Zaccaria and a 1984 pinball machine "Space Shuttle: Pinball Adventure" was produced by Williams and features a plastic Space Shuttle model among other artwork of astronauts on the play field. The Space Shuttle also appears in a number of flight simulator and space flight simulator games such as "Microsoft Space Simulator", "Orbiter", "FlightGear", "X-Plane" and Space Shuttle Mission 2007.
US postage commemorations.
The U.S. Postal Service has released several postage issues that depict the Space Shuttle. The first such stamps were issued in 1981, and are on display at the National Postal Museum.

</doc>
<doc id="28191" url="https://en.wikipedia.org/wiki?curid=28191" title="Snow">
Snow

Snow is precipitation in the form of flakes of crystalline water ice that falls from clouds.
Since snow is composed of small ice particles, it is a granular material. It has an open and therefore soft, white, and fluffy structure, unless subjected to external pressure. Snowflakes come in a variety of sizes and shapes. Types that fall in the form of a ball due to melting and refreezing, rather than a flake, are hail, ice pellets or snow grains.
The process of precipitating snow is called snowfall. Snowfall tends to form within regions of upward movement of air around a type of low-pressure system known as an extratropical cyclone. Snow can fall poleward of these systems' associated warm fronts and within their comma head precipitation patterns (called such due to the comma-like shape of the cloud and precipitation pattern around the poleward and west sides of extratropical cyclones). Where relatively warm water bodies are present, for example because of water evaporation from lakes, lake-effect snowfall becomes a concern downwind of the warm lakes within the cold cyclonic flow around the backside of extratropical cyclones. Lake-effect snowfall can be heavy locally. Thundersnow is possible within a cyclone's comma head and within lake effect precipitation bands. In mountainous areas, heavy snow is possible where upslope flow is maximized within windward sides of the terrain at elevation, if the atmosphere is cold enough. Snowfall amount and its related liquid equivalent precipitation amount are measured using a variety of different rain gauges.
Forms.
Once on the ground, snow can be categorized as powdery when light and fluffy, fresh when recent but heavier, granular when it begins the cycle of melting and refreezing, and eventually ice once it comes down, after multiple melting and refreezing cycles, into a dense mass called snow pack. When powdery, snow moves with the wind from the location where it originally landed, forming deposits called snowdrifts that may have a depth of several meters. After attaching itself to hillsides, blown snow can evolve into a snow slab—an avalanche hazard on steep slopes. The existence of a snowpack keeps temperatures lower than they would be otherwise, as the whiteness of the snow reflects most sunlight, and any absorbed heat goes into melting the snow rather than increasing its temperature. The water equivalent of snowfall is measured to monitor how much liquid is available to flood rivers from meltwater that will occur during the following spring. Snow cover can protect crops from extreme cold. If snowfall stays on the ground for a series of years uninterrupted, the snowpack develops into a mass of ice called glacier. Fresh snow absorbs sound, lowering ambient noise over a landscape because the trapped air between snowflakes attenuates vibration. These acoustic qualities quickly minimize and reverse, once a layer of freezing rain falls on top of snow cover. Walking across snowfall produces a squeaking sound at low temperatures.
The energy balance of the snowpack itself is dictated by several heat exchange processes. The snowpack absorbs solar shortwave radiation that is partially blocked by cloud cover and reflected by snow surface. A long-wave heat exchange takes place between the snowpack and its surrounding environment that includes overlying air mass, tree cover and clouds. Heat exchange takes place by convection between the snowpack and the overlaying air mass, and it is governed by the temperature gradient and wind speed. Moisture exchange between the snowpack and the overlying air mass is accompanied by latent heat transfer that is influenced by vapor pressure gradient and air wind. Rain on snow can add significant amounts of thermal energy to the snowpack. A generally insignificant heat exchange takes place by conduction between the snowpack and the ground. The small temperature change from before to after a snowfall is a result of the heat transfer between the snowpack and the air. As snow degrades, its surface can develop characteristic ablation textures such as suncups or penitentes.
The term "snow storm" can describe a heavy snowfall, while a "blizzard" involves snow and wind, obscuring visibility. "Snow shower" is a term for an intermittent snowfall, while "flurry" is used for very light, brief snowfalls. Snow can fall more than a meter at a time during a single storm in flat areas, and meters at a time in rugged terrain, such as mountains. When snow falls in significant quantities, travel by foot, car, airplane and other means becomes severely restricted, but other methods of mobility become possible, such as the use of snowmobiles, snowshoes and skis. When heavy snow occurs early in the fall (or, on rarer occasions, late in the spring), significant damage can occur to trees still in leaf. Areas with significant snow each year can store the winter snow within an ice house, which can be used to cool structures during the following summer. A variation on snow has been observed on Venus, though composed of metallic compounds and occurring at a substantially higher temperature.
Cause.
Extratropical cyclones can bring cold and dangerous conditions with heavy rain and snow with winds exceeding , (sometimes referred to as windstorms in Europe). The band of precipitation that is associated with their warm front is often extensive, forced by weak upward vertical motion of air over the frontal boundary, which condenses as it cools off and produces precipitation within an elongated band, which is wide and stratiform, meaning falling out of nimbostratus clouds. When moist air tries to dislodge an arctic air mass, overrunning snow can result within the poleward side of the elongated precipitation band. In the Northern Hemisphere, poleward is towards the North Pole, or north. Within the Southern Hemisphere, poleward is towards the South Pole, or south.
Within the "cold sector", poleward and west of the cyclone center, small scale or mesoscale bands of heavy snow can occur within a cyclone's comma head pattern. The cyclone's comma head pattern is a comma-shaped area of clouds and precipitation found around mature extratropical cyclones. These snow bands typically have a width of . These bands in the comma head are associated with areas of frontogenesis, or zones of strengthening temperature contrast.
Southwest of extratropical cyclones, curved cyclonic flow bringing cold air across the relatively warm water bodies can lead to narrow lake-effect snow bands. Those bands bring strong localized snowfall, which can be understood as follows: Large water bodies such as lakes efficiently store heat that results in significant temperature differences (larger than 13 °C [23 °F]) between the water surface and the air above. Because of this temperature difference, warmth and moisture are transported upward, condensing into vertically oriented clouds (see satellite picture) that produce snow showers. The temperature decrease with height and cloud depth are directly affected by both the water temperature and the large-scale environment. The stronger the temperature decrease with height, the deeper the clouds get, and the greater the precipitation rate becomes.
In mountainous areas, heavy snowfall accumulates when air is forced to ascend the mountains and squeeze out precipitation along their windward slopes, which in cold conditions, falls in the form of snow. Because of the ruggedness of terrain, forecasting the location of heavy snowfall remains a significant challenge.
Snowflakes.
Snow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice. Then the droplet freezes around this "nucleus". Experiments show that this "homogeneous" nucleation of cloud droplets only occurs at temperatures lower than . In warmer clouds an aerosol particle or "ice nucleus" must be present in (or in contact with) the droplet to act as a nucleus. Ice nuclei are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles may be effective, although to what extent is unclear. Artificial nuclei include particles of silver iodide and dry ice, and these are used to stimulate precipitation in cloud seeding.
Once a droplet has frozen, it grows in the supersaturated environment—one where air is saturated with respect to ice when the temperature is below the freezing point. The droplet then grows by diffusion of water molecules in the air (vapor) onto the ice crystal surface where they are collected. Because water droplets are so much more numerous than the ice crystals due to their sheer abundance, the crystals are able to grow to hundreds of micrometers or millimeters in size at the expense of the water droplets by a process known as the Wegner-Bergeron-Findeison process. The corresponding depletion of water vapor causes the ice crystals to grow at the droplets' expense. These large crystals are an efficient source of precipitation, since they fall through the atmosphere due to their mass, and may collide and stick together in clusters, or aggregates. These aggregates are snowflakes, and are usually the type of ice particle that falls to the ground. Guinness World Records list the world's largest snowflakes as those of January 1887 at Fort Keogh, Montana; allegedly one measured wide. Although the ice is clear, scattering of light by the crystal facets and hollows/imperfections mean that the crystals often appear white in color due to diffuse reflection of the whole spectrum of light by the small ice particles.
The shape of the snowflake is determined broadly by the temperature and humidity at which it is formed. The most common snow particles are visibly irregular. Planar crystals (thin and flat) grow in air between and . Between and , the crystals will form needles or hollow columns or prisms (long thin pencil-like shapes). From to the shape reverts to plate-like, often with branched or dendritic features. At temperatures below , the crystal development becomes column-like, although many more complex growth patterns also form such as side-planes, bullet-rosettes and also planar types depending on the conditions and ice nuclei. If a crystal has started forming in a column growth regime, at around , and then falls into the warmer plate-like regime, then plate or dendritic crystals sprout at the end of the column, producing so called "capped columns".
A snowflake consists of roughly 1019 water molecules, which are added to its core at different rates and in different patterns, depending on the changing temperature and humidity within the atmosphere that the snowflake falls through on its way to the ground. As a result, it is extremely difficult to encounter two identical snowflakes. Initial attempts to find identical snowflakes by photographing thousands their images under a microscope from 1885 onward by Wilson Alwyn Bentley found the wide variety of snowflakes we know about today. It is more likely that two snowflakes could become virtually identical if their environments were similar enough. Matching snow crystals were discovered in Wisconsin in 1988. The crystals were not flakes in the usual sense but rather hollow hexagonal prisms.
Types.
Types of snow can be designated by the shape of the flakes, the rate of accumulation, and the way the snow collects on the ground. Types that fall in the form of a ball due to melting and refreezing cycles, rather than a flake, are known as graupel, with ice pellets and snow pellets as types of graupel associated with wintry precipitation. Once on the ground, snow can be categorized as "powdery" when fluffy, "granular" when it begins the cycle of melting and refreezing, and eventually "ice" once it packs down into a dense drift after multiple melting and refreezing cycles. When powdery, snow drifts with the wind from the location where it originally fell, forming deposits with a depth of several meters in isolated locations. Snow fences are constructed in order to help control snow drifting in the vicinity of roads, to improve highway safety. After attaching to hillsides, blown snow can evolve into a snow slab, which is an avalanche hazard on steep slopes. A frozen equivalent of dew known as hoar frost forms on a snow pack when winds are light and there is ample low-level moisture over the snow pack.
Snowfall's intensity is determined by visibility. When the visibility is over , snow is considered light. Moderate snow describes snowfall with visibility restrictions between 0.5 and 1 km. Heavy snowfall describes conditions when visibility is less than 0.5 km. Steady snows of significant intensity are often referred to as "snowstorms". When snow is of variable intensity and short duration, it is described as a "snow shower". The term snow flurry is used to describe the lightest form of a snow shower.
A blizzard is a weather condition involving snow and has varying definitions in different parts of the world. In the United States, a blizzard occurs when two conditions are met for a period of three hours or more: A sustained wind or frequent gusts to , and sufficient snow in the air to reduce visibility to less than . In Canada and the United Kingdom, the criteria are similar. While heavy snowfall often occurs during blizzard conditions, falling snow is not a requirement, as blowing snow can create a ground blizzard.
Density.
Snow remains on the ground until it melts or sublimates. Sublimation of snow directly into water vapor is most likely to occur on a dry and windy day such as when a strong downslope wind, such as a Chinook wind, exists.
Once the snow is on the ground, it will settle under its own weight (largely due to differential evaporation) until its density is approximately 30% of water. Increases in density above this initial compression occur primarily by melting and refreezing, caused by temperatures above freezing or by direct solar radiation. In colder climates, snow lies on the ground all winter. By late spring, snow densities typically reach a maximum of 50% of water. When the snow does not all melt in the summer it evolves into firn, where individual granules become more spherical in nature, evolving into a glacier as the ice flows downhill.
Snow water equivalent.
The "snow water equivalent" is the product of snow depth and the snow bulk density. It is a quantity of type columnar mass density, having units of area density (kg/m2), though it is usually reported normalized by the volumetric density of liquid water (units kg/m3), thus being expressed in units of length (e.g., millimeter or inches). It corresponds to the depth of a layer of water that would accumulate in an area, if all the snow and ice were melted in that given area. For example, if the snow covering a given area has a water equivalent of , then it will melt into a pool of water deep covering the same area. This is a much more useful measurement to hydrologists than snow "depth", as the density of cool freshly fallen snow widely varies. New snow commonly has a density of around 8% of water. This means that of snow melts down to of water. Cloud temperatures and physical processes in the cloud affect the shape of individual snow crystals. Highly branched or dendritic crystals tend to have more space between the arms of ice that form the snowflake and this snow will therefore have a lower density, often referred to as "dry" snow. Conditions that create columnar or plate-like crystals will have much less air space within the crystal and will therefore be denser and feel "wetter".
Acoustic properties.
Newly fallen snow acts as a sound-absorbing material, which minimizes sound over its surface. This is due to the trapped air between the individual crystalline flakes, trapping sound waves and dampening vibrations. Once it is blown around by the wind and exposed to sunshine, snow hardens and its sound-softening quality diminishes. Snow cover as thin as thick changes the acoustic properties of a landscape. Studies concerning the acoustic properties of snow have revealed that loud sounds, such as from a pistol, can be used to measure snow cover permeability and depth. Within motion pictures, the sound of walking through snow is simulated using cornstarch, salt, or cat litter. When the temperature falls below , snow will squeak when walked upon due to the crushing of the ice crystals within the snow. If covered by a layer of freezing rain, the hardened frozen surface acts to echo sounds, similar to concrete.
From under water, snowfall has a unique sound when compared to other forms of precipitation, and the sound varies little with differences in the snowflakes' size and shape.
Snowfall measurement.
Snowfall is defined by the U.S. National Weather Service as a being the maximum depth of snow on a snowboard (typically a piece of plywood painted white) observed during a six-hour period. At the end of the six-hour period, all snow is cleared from the measuring surface. For a daily total snowfall, four six-hour snowfall measurements are summed. Snowfall can be very difficult to measure due to melting, compacting, blowing and drifting.
The liquid equivalent of snowfall may be evaluated using a snow gauge or with a standard rain gauge having a diameter of 100 mm (4 in; plastic) or 200 mm (8 in; metal). Rain gauges are adjusted to winter by removing the funnel and inner cylinder and allowing the snow/freezing rain to collect inside the outer cylinder. Antifreeze liquid may be added to melt the snow or ice that falls into the gauge. In both types of gauges once the snowfall/ice is finished accumulating, or as its height in the gauge approaches , the snow is melted and the water amount recorded.
Another type of gauge used to measure the liquid equivalent of snowfall is the weighing precipitation gauge. The wedge and tipping bucket gauges will have problems with snow measurement. Attempts to compensate for snow/ice by warming the tipping bucket meet with limited success, since snow may sublimate if the gauge is kept much above the freezing temperature. Weighing gauges with antifreeze should do fine with snow, but again, the funnel needs to be removed before the event begins. At some automatic weather stations an ultrasonic snow depth sensor may be used to augment the precipitation gauge.
Spring snow melt is a major source of water supply to areas in temperate zones near mountains that catch and hold winter snow, especially those with a prolonged dry summer. In such places, water equivalent is of great interest to water managers wishing to predict spring runoff and the water supply of cities downstream. Measurements are made manually at marked locations known as "snow courses", and remotely using special scales called "snow pillows".
When a snow measurement is made, various networks exist across the United States and elsewhere where rainfall measurements can be submitted through the Internet, such as CoCoRAHS or GLOBE. If a network is not available in the area where one lives, the nearest local weather office will likely be interested in the measurement.
Records.
The world record for the highest seasonal total snowfall was measured in the United States at Mount Baker Ski Area, outside of the town Bellingham, Washington during the 1998–1999 season. Mount Baker received of snow, thus surpassing the previous record holder, Mount Rainier, Washington, which during the 1971–1972 season received of snow.
The world record for the highest average yearly snowfall is , measured in Sukayu Onsen, Japan for the period of 1981–2010.
The North American record for the highest average yearly snowfall is , measured on Mount Rainier, Washington.
The world record for snow depth is . It was measured on the slope of Mt. Ibuki in Shiga Prefecture, Japan at altitude of on February 14, 1927.
The North American record for snow depth is . It was measured at Tamarack, California at altitude of in March 1911.
The world's snowiest city with a population over one million is Sapporo, Japan, with an average yearly snowfall of .
Snow blindness.
Fresh snow reflects 90% or more of ultraviolet radiation, which causes snow blindness, also reducing absorption of sunlight by the ground. Snow blindness (also known as ultraviolet keratitis, photokeratitis or niphablepsia) is a painful eye condition, caused by exposure of unprotected eyes to the ultraviolet (UV) rays in bright sunlight reflected from snow or ice. This condition is a problem in polar regions and at high altitudes, as with every of elevation (above sea level), the intensity of UV rays increases by 4%. Snow's large reflection of light makes night skies much brighter, since reflected light is directed back up into the sky. However, when there is also cloud cover, light is then reflected back to the ground. This greatly amplifies light emitted from city lights, causing the 'bright night' effect. A similar brightening effect occurs when no snow is falling and there is a full moon and a large amount of snow.
Relation to river flow.
Many rivers originating in mountainous or high-latitude regions receive a significant portion of their flow from snowmelt. This often makes the river's flow highly seasonal resulting in periodic flooding during the spring months and at least in dry mountainous regions like the mountain West of the US or most of Iran and Afghanistan, very low flow for the rest of the year. In contrast, if much of the melt is from glaciated or nearly glaciated areas, the melt continues through the warm season, with peak flows occurring in mid to late summer.
Effects on human society.
Substantial snowfall can disrupt public infrastructure and services, slowing human activity even in regions that are accustomed to such weather. Air and ground transport may be greatly inhibited or shut down entirely. Populations living in snow-prone areas have developed various ways to travel across the snow, such as skis, snowshoes, and sleds pulled by horses, dogs, or other animals and later, snowmobiles. Basic utilities such as electricity, telephone lines, and gas supply can also fail. In addition, snow can make roads much harder to travel and vehicles attempting to use them can easily become stuck. Snowfall can have a small negative effect on yearly yield from solar photovoltaic systems.
The combined effects can lead to a "snow day" on which gatherings such as school or work are officially canceled. In areas that normally have very little or no snow, a snow day may occur when there is only light accumulation or even the threat of snowfall, since those areas are unprepared to handle any amount of snow. In some areas, such as some states in the United States, schools are given a yearly quota of snow days (or "calamity days"). Once the quota is exceeded, the snow days must be made up. In other states, all snow days must be made up. For example, schools may extend the remaining school days later into the afternoon, shorten spring break, or delay the start of summer vacation.
Accumulated snow is removed to make travel easier and safer, and to decrease the long-term impact of a heavy snowfall. This process utilizes shovels, snowplows and snow blowers and is often assisted by sprinkling salt or other chloride-based chemicals, which reduce the melting temperature of snow. In some areas with abundant snowfall, such as Yamagata Prefecture, Japan, people harvest snow and store it surrounded by insulation in ice houses. This allows the snow to be used through the summer for refrigeration and air conditioning, which requires far less electricity than traditional cooling methods.
Agriculture.
Snowfall can be beneficial to agriculture by serving as a thermal insulator, conserving the heat of the Earth and protecting crops from subfreezing weather. Some agricultural areas depend on an accumulation of snow during winter that will melt gradually in spring, providing water for crop growth. If it melts into water and refreezes upon sensitive crops, such as oranges, the resulting ice will protect the fruit from exposure to lower temperatures.
Recreation.
Many winter sports, such as skiing, snowboarding, snowmobiling, and snowshoeing depend upon snow. Where snow is scarce but the temperature is low enough, snow cannons may be used to produce an adequate amount for such sports. Children and adults can play on a sled or ride in a sleigh. Although a person's footsteps remain a visible lifeline within a snow-covered landscape, snow cover is considered a general danger to hiking since the snow obscures landmarks and makes the landscape itself appear uniform.
One of the recognizable recreational uses of snow is in building snowmen. A snowman is created by making a man shaped figure out of snow – often using a large, shaped snowball for the body and a smaller snowball for the head which is often decorated with simple household items – traditionally including a carrot for a nose, and coal for eyes, nose and mouth; occasionally including old clothes such as a top hat or scarf.
Snow can be used to make snow cones, also known as snowballs, which are usually eaten in the summer months. Flat areas of snow can be used to make snow angels, a popular pastime for children.
Snow can be used to alter the format of outdoor games such as Capture the flag, or for snowball fights. The world's biggest snowcastle, the SnowCastle of Kemi, is built in Kemi, Finland every winter. Since 1928 Michigan Technological University in Houghton, Michigan has held an annual Winter Carnival in mid-February, during which a large Snow Sculpture Contest takes place between various clubs, fraternities, and organizations in the community and the university. Each year there is a central theme, and prizes are awarded based on creativity. Snowball softball tournaments are held in snowy areas, usually using a bright orange softball for visibility, and burlap sacks filled with snow for the bases.
Damage.
When heavy, wet snow with a snow-water equivalent (SWE) ratio of between 6:1 and 12:1 (in extreme cases, as heavy as 4:1) and a weight in excess of 10 pounds per square foot (~40 kg/m2) piles onto trees or electricity lines – particularly if the trees have full leaves or are not adapted to snow – significant damage may occur on a scale usually associated with hurricanes. An avalanche can occur upon a sudden thermal or mechanical impact upon snow that has accumulated on a mountain, which causes the snow to rush downhill en masse. Preceding an avalanche is a phenomenon known as an avalanche wind caused by the approaching avalanche itself, which adds to its destructive potential. Large amounts of snow which accumulate on top of man-made structures can lead to structural failure. During snowmelt, acidic precipitation which previously fell into the snow pack is released, which harms marine life.
There is a popular misconception that snow becomes heavier when it starts to melt, so many people take risks by climbing on roofs to remove snow when the weather starts to get warmer, for fear that the roofs will collapse. In fact, when snow starts to melt, its volume decreases as the ice crystals and meltwater move into the spaces between the crystals, which makes the "density" of wet, melting snow greater than that of freshly-fallen snow. This makes it feel heavier to shovel, but its mass does not increase. In fact, it decreases when meltwater runs off the roof, so the weight of snow on a roof actually decreases when it starts to melt.
Design of structures considering snow load.
The designs of all structures and buildings use the ground snow load determined by professional engineers and designers. Data on ground snow in the U.S.A. are provided by the American Society of Civil Engineers (ASCE7-latest edition) for most jurisdictions. This load is typically the governing design factor on roofs and structural elements exposed to the effects of snow in the northern United States. Closer to the Equator, the snow load becomes less important and may or may not be the governing factor.
Extraterrestrial snow.
Very light snow is known to occur at high latitudes on Mars. A "snow" of hydrocarbons is also theorized to occur on Saturn's moon Titan.
While there is little or no water on Venus, there is a phenomenon which is quite similar to snow. The Magellan probe imaged a highly reflective substance at the tops of Venus's highest mountain peaks which bore a strong resemblance to terrestrial snow. This substance arguably formed from a similar process to snow, albeit at a far higher temperature. Too volatile to condense on the surface, it rose in gas form to cooler higher elevations, where it then fell as precipitation. The identity of this substance is not known with certainty, but speculation has ranged from elemental tellurium to lead sulfide (galena).

</doc>
<doc id="28195" url="https://en.wikipedia.org/wiki?curid=28195" title="Symbolics">
Symbolics

Symbolics refers to two companies: now-defunct computer manufacturer Symbolics, Inc., and a privately held company that acquired the assets of the former company and continues to sell and maintain the Open Genera Lisp system and the Macsyma computer algebra system.
The symbolics.com domain was originally registered on March 15, 1985, making it the first .com-domain in the world. In August 2009, it was sold to XF.com Investments.
History.
Symbolics, Inc. was a computer manufacturer headquartered in Cambridge, Massachusetts, and later in Concord, Massachusetts, with manufacturing facilities in Chatsworth, California (a suburban section of Los Angeles). Its first CEO, chairman, and founder was Russell Noftsker. Symbolics designed and manufactured a line of Lisp machines, single-user computers optimized to run the Lisp programming language. Symbolics also made significant advances in software technology, and offered one of the premier software development environments of the 1980s and 1990s, now sold commercially as Open Genera for Tru64 UNIX on the HP Alpha. The Lisp Machine was the first commercially available "workstation" (although that word had not yet been coined).
Symbolics was a spinoff from the MIT AI Lab, one of two companies to be founded by AI Lab staffers and associated hackers for the purpose of manufacturing Lisp machines. The other was Lisp Machines, Inc., although Symbolics attracted most of the hackers, and more funding.
Symbolics' initial product, the LM-2 (introduced in 1981), was a repackaged version of the MIT CADR Lisp machine design. The operating system and software development environment, over 500,000 lines, was written in Lisp from the microcode up, based on MIT's Lisp Machine Lisp.
The software bundle was later renamed ZetaLisp, to distinguish the Symbolics' product from other vendors who had also licensed the MIT software. Symbolics' Zmacs text editor, a variant of Emacs, was implemented in a text-processing package named "ZWEI", an acronym for "Zwei was Eine initially", with "Eine" being an acronym for "Eine Is Not Emacs". Both are recursive acronyms and puns on the German words for "One" ("Eins", "Eine") and "Two" ("Zwei").
The Lisp Machine system software was then copyrighted by MIT, and was licensed to Symbolics. Until 1981, they shared all the source code with MIT and kept it on an MIT server. According to Richard Stallman, Symbolics engaged in a business tactic in which it forced MIT to make all fixes and improvements to the Lisp Machine OS available only to Symbolics, and thereby choke off its competitor LMI, which at that time had insufficient resources to independently maintain or develop the OS and environment.
Symbolics felt that they no longer had sufficient control over their product. At that point, Symbolics began using their own copy of the software, located on their company servers — while Stallman says that Symbolics did that to prevent its Lisp improvements from flowing to Lisp Machines, Inc. From that base, Symbolics made extensive improvements to every part of the software, and continued to deliver almost all the source code to their customers (including MIT). However, the policy prohibited MIT staff from distributing the Symbolics version of the software to others. With the end of open collaboration came the end of the MIT hacker community. As a reaction to this, Stallman initiated the GNU project to make a new community. Eventually, Copyleft and the GNU General Public License would ensure that a hacker's software could remain free software. In this way Symbolics played a key, albeit adversarial, role in instigating the free software movement.
The 3600 series.
In 1983, a year later than planned, Symbolics introduced the 3600 family of Lisp machines. Code-named the "L-machine" internally, the 3600 family was an innovative new design, inspired by the CADR architecture but sharing few of its implementation details. The main processor had a 36 bit word (divided up as 4 or 8 bits of tags, and 32 bits of data or 28 bits of memory address). Memory words were 44 bits, the additional 8 bits being used for error-correcting code (ECC). The instruction set was that of a stack machine. The 3600 architecture provided 4,096 hardware registers, of which half were used as a cache for the top of the control stack; the rest were used by the microcode and time-critical routines of the operating system and Lisp run-time environment. Hardware support was provided for virtual memory, which was common for machines in its class, and for garbage collection, which was unique.
The original 3600 processor was a microprogrammed design like the CADR, and was built on several large circuit boards from standard TTL integrated circuits, both features being common for commercial computers in its class at the time. CPU clock speed varied depending on the particular instruction being executed, but was typically around 5 MHz. Many Lisp primitives could be executed in a single clock cycle. Disk I/O was handled by multitasking at the microcode level. A 68000 processor (known as the "Front-End Processor", or FEP) started the main computer up, and handled the slower peripherals during normal operation. An Ethernet interface was standard equipment, replacing the Chaosnet interface of the LM-2.
The 3600 was roughly the size of a household refrigerator. This was partly due to the size of the processor — the cards were widely spaced to allow wire-wrap prototype cards to fit without interference — and partly due to the limitations of the disk drive technology in the early 1980s. At the 3600's introduction, the smallest disk that could support the ZetaLisp software was 14 inches (356 mm) across (most 3600s shipped with the 10½-inch Fujitsu Eagle). The 3670 and 3675 were slightly shorter in height, but were essentially the same machine packed a little tighter. The advent of , and later , disk drives that could hold hundreds of megabytes led to the introduction of the 3640 and 3645, which were roughly the size of a two-drawer file cabinet.
Later versions of the 3600 architecture were implemented on custom integrated circuits, reducing the five cards of the original processor design to two, at a large manufacturing cost savings and with performance slightly better than the old design. The 3650, first of the "G machines", as they were known within the company, was housed in a cabinet derived from the 3640s. Denser memory and smaller disk drives enabled the introduction of the 3620, about the size of a modern full-size tower PC. The 3630 was a "fat 3620" with room for more memory and video interface cards. The 3610 was a lower priced variant of the 3620, essentially identical in every way except that it was licensed for application deployment rather than general development.
The various models of the 3600 family were popular for AI research and commercial applications throughout the 1980s. The AI commercialization boom of the 1980s led directly to Symbolics' success during the decade. Symbolics computers were widely believed to be the best platform available for developing AI software. The LM-2 used a Symbolics-branded version of the complex space-cadet keyboard, while later models used a simplified version (at right), known simply as the . The Symbolics keyboard featured the many modifier keys used in Zmacs, notably Control/Meta/Super/Hyper in a block, but did not feature the complex symbol set of the space-cadet keyboard.
Also contributing to the 3600 series' success was a line of bit-mapped graphics color video interfaces, combined with extremely powerful animation software. Symbolics' Graphics Division, headquartered in Westwood, California, a stone's throw from the major Hollywood movie and television studios, made its S-Render and S-Paint software into industry leaders in the animation business.
Symbolics developed the first workstations capable of processing HDTV quality video, which enjoyed a popular following in Japan. A 3600 — with the standard black-and-white monitor — made a cameo appearance in the movie Real Genius. The company was also referenced in Michael Crichton's novel Jurassic Park.
Symbolics' Graphics Division was sold to Nichimen Trading Company in the early 1990s, and the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) ported to Franz Allegro Common Lisp on SGI and PC computers running Windows NT. Today it is sold as Mirai by Izware LLC, and continues to be used in major motion pictures (most famously in New Line Cinema's "The Lord of the Rings"), video games, and military simulations.
Symbolic's 3600-series computers were also used as the first front end "controller" computers for the Connection Machine massively parallel computers manufactured by Thinking Machines Inc., another MIT spinoff based in Cambridge, Massachusetts. The Connection Machine ran a parallel variant of Lisp and, initially, was used primarily by the AI community, so the Symbolics Lisp machine was a particularly good fit as a front-end machine.
For a long time, the operating system didn't have a name, but was finally named "Genera" around 1984. The system included a number of advanced dialects of Lisp. Its heritage was MACLISP on the PDP-10, but it included more data types, and multiple-inheritance object-oriented programming features. This Lisp dialect was called Lisp Machine Lisp at MIT. Symbolics used the name ZetaLisp. Symbolics later wrote new software in "Symbolics Common Lisp", its version of the Common Lisp standard.
Ivory and Open Genera.
In the late 1980s (2 years later than planned), the Ivory family of single-chip Lisp Machine processors superseded the G-Machine 3650, 3620, and 3630 systems. The Ivory 390k transistor VLSI implementation designed in Symbolics Common Lisp using NS, a custom Symbolics Hardware Design Language (HDL), addressed a 40-bit word (8 bits tag, 32 bits data/address). Since it only addressed full words and not bytes or half-words, this allowed addressing of 4 Gigawords (GW) or 16 gigabytes (GB) of memory; the increase in address space reflected the growth of programs and data as semiconductor memory and disk space became cheaper. The Ivory processor had 8 bits of ECC attached to each word, so each word fetched from external memory to the chip was actually 48 bits wide. Each Ivory instruction was 18 bits wide and two instructions plus a 2-bit CDR code and 2-bit Data Type were in each instruction word fetched from memory. Fetching two instruction words at a time from memory enhanced the Ivory's performance. Unlike the 3600's microprogrammed architecture, the Ivory instruction set was still microcoded, but was stored in a 1200 x 180-bit ROM inside the Ivory chip. The initial Ivory processors were fabricated by VLSI Technology Inc in San Jose, California, on a 2 µm CMOS process, with later generations fabricated by Hewlett Packard in Corvallis, Oregon, on 1.25 µm and 1 µm CMOS processes. The Ivory had a stack architecture and operated a 4-stage pipeline: Fetch, Decode, Execute and Write Back. Ivory processors were marketed in stand-alone Lisp Machines (the XL400, XL1200, and XL1201), headless Lisp Machines (NXP1000), and on add-in cards for Sun Microsystems (UX400, UX1200) and Apple Macintosh (MacIvory I, II, III) computers. The Lisp Machines with Ivory processors operated at speeds that were between two and six times faster than a 3600 depending on the model and the revision of the Ivory chip.
The Ivory instruction set was later emulated in software for microprocessors implementing the 64-bit Alpha architecture. The "Virtual Lisp Machine" emulator, combined with the operating system and software development environment from the XL machines, is sold as Open Genera.
Sunstone.
Sunstone was a RISC-like processor that was to be released shortly after the Ivory. It was designed by Ron Lebel's group at the Symbolics Westwood office. However, the project was canceled the day it was supposed to tape out.
Endgame.
As quickly as the commercial AI boom of the mid-1980s had propelled Symbolics to success, the "AI Winter" of the late 1980s and early 1990s, combined with the slow down of Reagan's "Star Wars" missile defense program, for which DARPA had invested heavily in AI solutions, severely damaged Symbolics. An internal war between Noftsker and the CEO the board had hired in 1986, Brian Sear, over whether to follow Sun's suggested lead and focus on selling their software, or to re-emphasize their superior hardware, and the ensuing lack of focus when both Noftsker and Sear were fired from the company caused sales to plummet. This fact, combined with some ill-advised real estate deals by company management during the boom years (they had entered into large long-term lease obligations in California), drove Symbolics into bankruptcy. Rapid evolution in mass-market microprocessor technology (the "PC revolution"), advances in Lisp compiler technology, and the economics of manufacturing custom microprocessors severely diminished the commercial advantages of purpose-built Lisp machines. By 1995, the Lisp machine era had ended, and with it Symbolics' hopes for success.
Symbolics continued as an enterprise with very limited revenues, supported mainly by service contracts on the remaining MacIvory, UX-1200, UX-1201, and other machines still used by commercial customers. Symbolics also sold Virtual Lisp Machine (VLM) software for DEC, Compaq, and HP Alpha-based workstations (AlphaStation) and servers (AlphaServer), refurbished MacIvory IIs, and Symbolics keyboards.
In July 2005, Symbolics closed its Chatsworth, California, maintenance facility. The reclusive owner of the company, Andrew Topping, died that same year. The current legal status of Symbolics software is uncertain. An assortment of Symbolics hardware was still available for purchase as of August 2007. The US DoD is still paying Symbolics for regular maintenance work.
First .com domain.
On March 15, 1985, symbolics.com became the first (and currently, since it is still registered, the oldest) registered .com domain of the Internet. The symbolics.com domain was purchased by XF.com in 2009.
Networking.
Genera also featured the most extensive networking interoperability software seen to that point. A local area network system called Chaosnet had been invented for the Lisp Machine (predating the commercial availability of Ethernet). The Symbolics system supported Chaosnet, but also had one of the first TCP/IP implementations. It also supported DECnet and IBM's SNA network protocols. A Dialnet protocol used phone lines and modems. Genera would, using hints from its distributed "namespace" database (somewhat similar to DNS, but more comprehensive, like parts of Xerox's Grapevine), automatically select the best protocol combination to use when connecting to network service. An application program (or a user command) would only specify the name of the host and the desired service. For example, a host name and a request for "Terminal Connection" might yield a connection over TCP/IP using the Telnet protocol (although there were many other possibilities). Likewise, requesting a file operation (such as a Copy File command) might pick NFS, FTP, NFILE (the Symbolics network file access protocol), or one of several others, and it might execute the request over TCP/IP, Chaosnet, or whatever other network was most suitable.
Application programs.
The most popular application program for the Symbolics Lisp Machine was the ICAD computer-aided engineering system. One of the first networked multi-player video games, a version of Spacewar, was developed for the Symbolics Lisp Machine in 1983. Electronic CAD software on the Symbolics Lisp Machine was used to develop the first implementation of the Hewlett-Packard Precision Architecture.
Contributions to computer science.
Symbolics' research and development staff (first at MIT, and then later at the company) produced a number of major innovations in software technology:
Symbolics Graphics Division.
The Symbolics Graphics Division (SGD, founded in 1982, sold to Nichimen Graphics in 1992) developed the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) for Symbolics Genera.
Movies.
This software was also used to create a few computer animated movies and was used for some popular movies.

</doc>
<doc id="28198" url="https://en.wikipedia.org/wiki?curid=28198" title="Surfing">
Surfing

Surfing is a surface water sport in which the wave rider, referred to as a surfer, rides on the forward or deep face of a moving wave, which is usually carrying the surfer towards the shore. Waves suitable for surfing are primarily found in the ocean, but can also be found in lakes or in rivers in the form of a standing wave or tidal bore. However, surfers can also utilize artificial waves such as those from boat wakes and the waves created in artificial wave pools.
The term "surfing" refers to the act of riding a wave, regardless of whether the wave is ridden with a board or without a board, and regardless of the stance used (goofy or regular stance). The native peoples of the Pacific, for instance, surfed waves on alaia, paipo, and other such craft, and did so on their belly and knees. The actual modern-day definition of surfing, however, most often refers to a surfer riding a wave standing up on a surfboard; this is also referred to as stand-up surfing.
Another prominent form of surfing is body boarding, when a surfer rides a wave on a bodyboard, either lying on their belly, drop knee, or sometimes even standing up on a body board. Other types of surfing include knee boarding, surf matting (riding inflatable mats), and using foils. Body surfing, where the wave is surfed without a board, using the surfer's own body to catch and ride the wave, is very common and is considered by some to be the purest form of surfing.
Three major subdivisions within standing-up surfing are long boarding and short boarding and these two have several major differences, including the board design and length, the riding style, and the kind of wave that is ridden.
In tow-in surfing (most often, but not exclusively, associated with big wave surfing), a motorized water vehicle, such as a personal watercraft, tows the surfer into the wave front, helping the surfer match a large wave's speed, which is generally a higher speed than a self-propelled surfer can produce. Surfing-related sports such as paddle boarding and sea kayaking do not require waves, and other derivative sports such as kite surfing and windsurfing rely primarily on wind for power, yet all of these platforms may also be used to ride waves. Recently with the use of V-drive boats, Wakesurfing, in which one surfs on the wake of a boat, has emerged. The Guinness Book of World Records recognized a wave ride by Garrett McNamara at Nazaré, Portugal as the largest wave ever surfed, although this remains an issue of much contention amongst many surfers, given the difficulty of measuring a constantly changing mound of water.
Origins and history.
For centuries, surfing was a central part of ancient Polynesian culture. Surfing may have first been observed by Europeans at Tahiti in 1767 by Samuel Wallis and the crew members of the "Dolphin" who were the first Europeans to visit the island in June of that year. Another candidate is the botanist Joseph Banks being part of the first voyage of James Cook on the HMS "Endeavour", who arrived on Tahiti on 10 April 1769. Lieutenant James King was the first person to write about the art of surfing on Hawaii when he was completing the journals of Captain James Cook upon Cook's death in 1779.
When Mark Twain visited Hawaii in 1866 he wrote,
In one place we came upon a large company of naked natives, of both sexes and all ages, amusing themselves with the national pastime of surf-bathing.
References to surf riding on planks and single canoe hulls are also verified for pre-contact Samoa, where surfing was called "fa'ase'e" or "se'egalu" (see Augustin Krämer, "The Samoa Islands"), and Tonga, far pre-dating the practice of surfing by Hawaiians and eastern Polynesians by over a thousand years.
In July 1885, three teenage Hawaiian princes took a break from their boarding school, St. Mathew’s Hall in San Mateo, and came to cool off in Santa Cruz, California. There, David Kawananakoa, Edward Keliiahonui and Jonah Kuhio Kalaniana'ole surfed the mouth of the San Lorenzo River on custom-shaped redwood boards, according to surf historians Kim Stoner and Geoff Dunn.
George Freeth (8 November 1883 – 7 April 1919) is often credited as being the "Father of Modern Surfing". He is thought to have been the first modern surfer.
In 1907, the eclectic interests of the land baron Henry Huntington brought the ancient art of surfing to the California coast. While on vacation, Huntington had seen Hawaiian boys surfing the island waves. Looking for a way to entice visitors to the area of Redondo Beach, where he had heavily invested in real estate, he hired a young Hawaiian to ride surfboards. George Freeth decided to revive the art of surfing, but had little success with the huge 16-foot hardwood boards that were popular at that time. When he cut them in half to make them more manageable, he created the original "Long board", which made him the talk of the islands. To the delight of visitors, Freeth exhibited his surfing skills twice a day in front of the Hotel Redondo.
In 1975, professional contests started. That year Margo Oberg became the first female professional surfer.
Surf waves.
Swell is generated when wind blows consistently over a large area of open water, called the wind's fetch. The size of a swell is determined by the strength of the wind and the length of its fetch and duration. Because of this, surf tends to be larger and more prevalent on coastlines exposed to large expanses of ocean traversed by intense low pressure systems.
Local wind conditions affect wave quality, since the surface of a wave can become choppy in blustery conditions. Ideal conditions include a light to moderate "offshore" wind, because it blows into the front of the wave, making it a "barrel" or "tube" wave. Waves are Left handed and Right Handed depending upon the breaking formation of the wave.
Waves are generally recognized by the surfaces over which they break. For example, there are Beach breaks, Reef breaks and Point breaks.
The most important influence on wave shape is the topography of the seabed directly behind and immediately beneath the breaking wave. The contours of the reef or bar front becomes stretched by diffraction. Each break is different, since each location's underwater topography is unique. At beach breaks, sandbanks change shape from week to week. Surf forecasting is aided by advances in information technology. Mathematical modeling graphically depicts the size and direction of swells around the globe.
Swell regularity varies across the globe and throughout the year. During winter, heavy swells are generated in the mid-latitudes, when the North and South polar fronts shift toward the Equator. The predominantly Westerly winds generate swells that advance Eastward, so waves tend to be largest on West coasts during winter months. However, an endless train of mid-latitude cyclones cause the isobars to become undulated, redirecting swells at regular intervals toward the tropics.
East coasts also receive heavy winter swells when low-pressure cells form in the sub-tropics, where slow moving highs inhibit their movement. These lows produce a shorter fetch than polar fronts, however they can still generate heavy swells, since their slower movement increases the duration of a particular wind direction. The variables of fetch and duration both influence how long wind acts over a wave as it travels, since a wave reaching the end of a fetch behaves as if the wind died.
During summer, heavy swells are generated when cyclones form in the tropics. Tropical cyclones form over warm seas, so their occurrence is influenced by El Niño & La Niña cycles. Their movements are unpredictable.
Surf travel and some surf camps offer surfers access to remote, tropical locations, where tradewinds ensure offshore conditions. Since winter swells are generated by mid-latitude cyclones, their regularity coincides with the passage of these lows. Swells arrive in pulses, each lasting for a couple of days, with a few days between each swell.
The availability of free model data from the NOAA has allowed the creation of several surf forecasting websites.
Wave intensity.
[[File:Wavemodel.jpg|thumb|300px|The geometry of tube shape can be represented as a ratio between length and width. A perfectly cylindrical vortex has a ratio of 1:1, while the* Tube shape defined by length to width ratio
Artificial reefs.
The value of good surf in attracting surf tourism has prompted the construction of artificial reefs and sand bars. Artificial surfing reefs can be built with durable sandbags or concrete, and resemble a submerged breakwater. These artificial reefs not only provide a surfing location, but also dissipate wave energy and shelter the coastline from erosion. Ships such as Seli 1 that have accidentally stranded on sandy bottoms, can create sandbanks that give rise to good waves.
An artificial reef known as Chevron Reef was constructed in El Segundo, California in hopes of creating a new surfing area. However, the reef failed to produce any quality waves and was removed in 2008. In Kovalam, South West India, an artificial reef has, however, successfully provided the local community with a quality lefthander, stabilized coastal soil erosion, and provided good habitat for marine life. ASR Ltd., a New Zealand-based company, constructed the Kovalam reef and is working on another reef in Boscombe, England.
Even with artificial reefs in place, a tourist's vacation time may coincide with a "flat spell", when no waves are available. Completely artificial Wave pools aim to solve that problem by controlling all the elements that go into creating perfect surf, however there are only a handful of wave pools that can simulate good surfing waves, owing primarily to construction and operation costs and potential liability. Most wave pools generate waves that are too small and lack the power necessary to surf. The Seagaia Ocean Dome, located in Miyazaki, Japan, was an example of a surfable wave pool. Able to generate waves with up to 10-foot faces, the specialized pump held water in 20 vertical tanks positioned along the back edge of the pool. This allowed the waves to be directed as they approach the artificial sea floor. Lefts, Rights, and A-frames could be directed from this pump design providing for rippable surf and barrel rides. The Ocean Dome cost about $2 billion to build and was expensive to maintain. The Ocean Dome was closed in 2007. In England, construction is nearing completion on the The Wave, situated near Bristol, which will enable people unable to get to the coast to enjoy the waves in a controlled environment, set in the heart of nature.
Surfers and surf culture.
Surfers represent a diverse culture based on riding the waves. Some people practice surfing as a recreational activity while others make it the central focus of their lives. Within the United States, surfing culture is most dominant in Hawaii and California because these two states offer the best surfing conditions. However, waves can be found wherever there is coastline, and a tight-knit yet far-reaching subculture of surfers has emerged throughout America. Some historical markers of the culture included the woodie, the station wagon used to carry surfers' boards, as well as boardshorts, the long swim shorts typically worn while surfing. Surfers also wear wetsuits in colder regions.
The sport of surfing now represents a multibillion-dollar industry especially in clothing and fashion markets. The World Surf League (WSL) runs the championship tour, hosting top competitors in some of the best surf spots around the globe. A small number of people make a career out of surfing by receiving corporate sponsorships and performing for photographers and videographers in far-flung destinations; they are typically referred to as freesurfers.
When the waves were flat, surfers persevered with sidewalk surfing, which is now called skateboarding. Sidewalk surfing has a similar feel to surfing and requires only a paved road or sidewalk. To create the feel of the wave, surfers even sneak into empty backyard swimming pools to ride in, known as pool skating. Eventually, surfing made its way to the slopes with the invention of the Snurfer, later credited as the first snowboard. Many other board sports have been invented over the years, but all can trace their heritage back to surfing.
Many surfers claim to have a spiritual connection with the ocean, describing surfing, the surfing experience, both in and out of the water, as a type of spiritual experience or a religion.
Maneuvers.
Standup surfing begins when the surfer paddles toward shore in an attempt to match the speed of the wave (The same applies whether the surfer is standup paddling, bodysurfing, boogie-boarding or using some other type of watercraft, such as a waveski or kayak.). Once the wave begins to carry the surfer forward, the surfer stands up and proceeds to ride the wave. The basic idea is to position the surfboard so it is just ahead of the breaking part (whitewash) of the wave. A common problem for beginners is being able to catch the wave at all.
Surfers' skills are tested by their ability to control their board in difficult conditions, riding challenging waves, and executing maneuvers such as strong turns and cutbacks (turning board back to the breaking wave) and "carving" (a series of strong back-to-back maneuvers). More advanced skills include the "floater" (riding on top of the breaking curl of the wave), and "off the lip" (banking off the breaking wave). A newer addition to surfing is the progression of the "air" whereby a surfer propels off the wave entirely up into the air, and then successfully lands the board back on the wave.
The tube ride is considered to be the ultimate maneuver in surfing. As a wave breaks, if the conditions are ideal, the wave will break in an orderly line from the middle to the shoulder, enabling the experienced surfer to position themselves inside the wave as it is breaking. This is known as a tube ride. Viewed from the shore, the tube rider may disappear from view as the wave breaks over the rider's head. The longer the surfer remains in the tube, the more successful the ride. This is referred to as getting tubed, barreled, shacked or pitted. Some of the world's best known waves for tube riding include Pipeline on the North shore of Oahu, Teahupoo in Tahiti and G-Land in Java. Other names for the tube include "the barrel", and "the pit".
Hanging ten and hanging five are moves usually specific to long boarding. Hanging Ten refers to having both feet on the front end of the board with all of the surfer's toes off the edge, also known as nose-riding. Hanging Five is having just one foot near the front, with five toes off the edge.
Cutback: Generating speed down the line and then turning back to reverse direction.
Floater: Suspending the board atop the wave. Very popular on small waves.
Top-Turn: Turn off the top of the wave. Sometimes used to generate speed and sometimes to shoot spray.
Air / Aerial: Launching the board off the wave entirely, then re-entering the wave. Various airs include ollies, lien airs, method airs, and other skateboard-like maneuvers.
Terms.
The Glossary of surfing includes some of the extensive vocabulary used to describe various aspects of the sport of surfing as described in literature on the subject. In some cases terms have spread to a wider cultural use. These terms were originally coined by people who were directly involved in the sport of surfing.
Learning.
Many popular surfing destinations have surf schools and surf camps that offer lessons. Surf camps for beginners and intermediates are multi-day lessons that focus on surfing fundamentals. They are designed to take new surfers and help them become proficient riders. All-inclusive surf camps offer overnight accommodations, meals, lessons and surfboards. Most surf lessons begin with instruction and a safety briefing on land, followed by instructors helping students into waves on longboards or "softboards". The softboard is considered the ideal surfboard for learning, due to the fact it is safer, and has more paddling speed and stability than shorter boards. Funboards are also a popular shape for beginners as they combine the volume and stability of the longboard with the manageable size of a smaller surfboard.
New and inexperienced surfers typically learn to catch waves on softboards around the 7–8 foot funboard size. Due to the softness of the surfboard the chance of getting injured is substantially minimized. Costco's alaia design Wavestorm surfboard is a cheap and cost efficient option; unlike many surfboards, this one comes with rubber fins, stock leash, pre-installed traction.
Typical surfing instruction is best performed one-on-one, but can also be done in a group setting. The most popular surf locations offer perfect surfing conditions for beginners, as well as challenging breaks for advanced students. The ideal conditions for learning would be small waves that crumble and break softly, as opposed to the steep, fast-peeling waves desired by more experienced surfers. When available, a sandy seabed is generally safer.
Surfing can be broken into several skills: Paddling strength, Positioning to catch the wave, timing, and balance. Paddling out requires strength, but also the mastery of techniques to break through oncoming waves ("duck diving", "eskimo roll"). Take-off positioning requires experience at predicting the wave set and where they will break. The surfer must pop up quickly as soon as the wave starts pushing the board forward. Preferred positioning on the wave is determined by experience at reading wave features including where the wave is breaking. Balance plays a crucial role in standing on a surfboard. Thus, balance training exercises are a good preparation. Practicing with a Balance board or swing boarding helps novices master the art.
Equipment.
Surfing can be done on various equipment, including surfboards, longboards, Stand Up Paddle boards (SUP's), bodyboards, wave skis, skimboards, kneeboards, surf mats and macca's trays. Surfboards were originally made of solid wood and were large and heavy (often up to long and ). Lighter balsa wood surfboards (first made in the late 1940s and early 1950s) were a significant improvement, not only in portability, but also in increasing maneuverability.
Most modern surfboards are made of fiberglass foam (PU), with one or more wooden strips or "stringers", fiberglass cloth, and polyester resin (PE). An emerging board material is epoxy resin and Expanded Polystyrene foam (EPS) which is stronger and lighter than traditional PU/PE construction. Even newer designs incorporate materials such as carbon fiber and variable-flex composites in conjunction with fiberglass and epoxy or polyester resins. Since epoxy/EPS surfboards are generally lighter, they will float better than a traditional PU/PE board of similar size, shape and thickness. This makes them easier to paddle and faster in the water. However, a common complaint of EPS boards is that they do not provide as much feedback as a traditional PU/PE board. For this reason, many advanced surfers prefer that their surfboards be made from traditional materials.
Other equipment includes a leash (to stop the board from drifting away after a wipeout, and to prevent it from hitting other surfers), surf wax, traction pads (to keep a surfer's feet from slipping off the deck of the board), and fins (also known as "skegs") which can either be permanently attached ("glassed-on") or interchangeable. Sportswear designed or particularly suitable for surfing may be sold as "boardwear" (the term is also used in snowboarding). In warmer climates, swimsuits, surf trunks or boardshorts are worn, and occasionally rash guards; in cold water surfers can opt to wear wetsuits, boots, hoods, and gloves to protect them against lower water temperatures. A newer introduction is a rash vest with a thin layer of titanium to provide maximum warmth without compromising mobility. In recent years, there have been advancements in technology that have allowed surfers to pursue even bigger waves with added elements of safety. Big wave surfers are now experimenting with inflatable vests or colored dye packs to help decrease their odds of drowning.
There are many different surfboard sizes, shapes, and designs in use today. Modern longboards, generally in length, are reminiscent of the earliest surfboards, but now benefit from modern innovations in surfboard shaping and fin design. Competitive longboard surfers need to be competent at traditional "walking" manoeuvres, as well as the short-radius turns normally associated with shortboard surfing. The modern shortboard began life in the late 1960s and has evolved into today's common "thruster" style, defined by its three fins, usually around in length. The thruster was invented by Australian shaper Simon Anderson.
Midsize boards, often called funboards, provide more maneuverability than a longboard, with more flotation than a shortboard. While many surfers find that funboards live up to their name, providing the best of both surfing modes, others are critical.
There are also various niche styles, such as the "Egg", a longboard-style short board targeted for people who want to ride a shortboard but need more paddle power. The "Fish", a board which is typically shorter, flatter, and wider than a normal shortboard, often with a split tail (known as a "swallow tail"). The Fish often has two or four fins and is specifically designed for surfing smaller waves. For big waves there is the "Gun", a long, thick board with a pointed nose and tail (known as a pin tail) specifically designed for big waves.
The physics of surfing.
The physics of surfing involves the physical oceanographic properties of wave creation in the surf zone, the characteristics of the surfboard, and the surfer's interaction with the water and the board.
Wave formation.
Ocean waves are defined as a collection of dislocated water parcels that undergo a cycle of being forced past their normal position and being restored back to their normal position. Wind caused ripples and eddies form waves that gradually gain speed and distance (fetch). Waves increase in energy and speed, and then become longer and stronger. The fully developed sea has the strongest wave action that experiences storms lasting 10-hours and creates 15 meter wave heights in the open ocean.
The waves created in the open ocean are classified as deep-water waves. Deep-water waves have no bottom interaction and the orbits of these water molecules are circular; their wavelength is short relative to water depth and the velocity decays before the reaching the bottom of the water basin. Deep waves have depths greater than ½ their wavelengths. Wind forces waves to break in the deep sea.
Deep-water waves travel to shore and become shallow water waves. Shallow water waves have depths less than ½ of their wavelength. Shallow wave's wavelengths are long relative to water depth and have elliptical orbitals. The wave velocity effects the entire water basin. The water interacts with the bottom as it approaches shore and has a drag interaction. The drag interaction pulls on the bottom of the wave, causes refraction, increases the height, decreases the celerity (or the speed of the wave form), and the top (crest) falls over. This phenomenon happens because the velocity of the top of the wave is greater than the velocity of the bottom of the wave.
The surf zone is place of convergence of multiple waves types creating complex wave patterns. A wave suitable for surfing results from maximum speeds of 5 meters per second. This speed is relative because local onshore winds can cause waves to break. In the surf zone, shallow water waves are carried by global winds to the beach and interact with local winds to make surfing waves.
Different onshore and off shore wind patterns in the surf zone create different types of waves. Onshore winds cause random wave breaking patterns and are more suitable for experienced surfers. Light offshore winds create smoother waves, while strong direct offshore winds cause plunging or large barrel waves. Barrel waves are large because the water depth is small when the wave breaks. Thus, the breaker intensity (or force) increases, and the wave speed and height increase. Off shore winds produce non-surfable conditions by flattening a weak swell. Weak swell is made from surface gravity forces and has long wavelengths.
Wave conditions for surfing.
Surfing waves can be analyzed using the following parameters: breaking wave height, wave peel angle (α), wave breaking intensity, and wave section length. The breaking wave height has two measurements, the relative heights estimated by surfers and the exact measurements done by physical oceanographers. Measurements done by surfers were 1.36 to 2.58 times higher than the measurements done by scientists. The scientifically concluded wave heights that are physically possible to surf are 1 to 20 meters.
The wave peel angle is one of the main constituents of a potential surfing wave. Wave peel angle measures the distance between the peel-line and the line tangent to the breaking crest line. This angle controls the speed of the wave crest. The speed of the wave is an addition of the propagation velocity vector (Vw) and peel velocity vector (Vp), which results in the overall velocity of the wave (Vs).
Wave breaking intensity measures the force of the wave as it breaks, spills, or plunges (a plunging wave is termed by surfers as a “barrel wave”). Wave section length is the distance between two breaking crests in a wave set. Wave section length can be hard to measure because local winds, non-linear wave interactions, island sheltering, and swell interactions can cause multifarious wave configurations in the surf zone.
The parameters breaking wave height, wave peel angle (α), and wave breaking intensity, and wave section length are important because they are standardized by past oceanographers who researched surfing; these parameters have been used to create a guide that matches the type of wave formed and the skill level of surfer.
Table 1 shows a relationship of smaller peel angles correlating with a higher skill level of surfer. Smaller wave peel angles increase the velocities of waves. A surfer must know how to react and paddle quickly to match the speed of the wave to catch it. Therefore, more experience is required to catch a low peel angle waves. Also, more experienced surfers can handle longer section lengths, increased velocities, and higher wave heights. Different locations offer different types of surfing conditions for each skill level.
Surf breaks.
A surf break is an area with an obstruction or an object that causes a wave to break. Surf breaks entail multiple scale phenomena. Wave section creation has microscale factors of peel angle and wave breaking intensity. The microscale components influence wave height and variations on wave crests. The mesoscale components of surf breaks are the ramp, platform, wedge, or ledge that may be present at a surf break. Macroscale processes are the global winds that initially produce offshore waves. Types of surf breaks are headlands (point break), beach break, river/estuary entrance bar, reef breaks, and ledge breaks.
Headland (point break).
A headland or point break interacts with the water by causing refraction around the point or headland. The point absorbs the high frequency waves and long period waves persist, which are easier to surf. Examples of locations that have headland or point break induced surf breaks are Dunedin (New Zealand), Raglan, Malibu (California), Rincon (California), and Kirra (Australia).
Beach break.
A beach break happens where waves break from offshore waves, and onshore sandbars and rips. Wave breaks happen successively at beach breaks. Example locations are Tairua and Aramoana Beach (New Zealand) and the Gold Coast (Australia).
River or estuary entrance bar.
A river or estuary entrance bar creates waves from the ebb tidal delta, sediment outflow, and tidal currents. An ideal estuary entrance bar exists in Whangamata Bar, New Zealand.
Reef break.
A reef break is conducive to surfing because large waves consistently break. Reef breaks are present in Padang Padang (Indonesia) and Pipeline (Hawaii).
Ledge break.
A ledge break is formed by steep rocks ledges that makes intense waves because the waves travel through deeper water then abruptly reach shallower water at the ledge. Shark Island, Australia is a location with a ledge break. Ledge breaks create difficult surfing conditions, sometimes only allowing body surfing as the only feasible way to confront the waves.
Jetties and their impacts on wave formation in the surf zone.
Jetties are added to bodies of water to regulate erosion, preserve navigation channels, and make harbors. Jetties are classified into four different types and have two main controlling variables: the type of delta and the size of the jetty.
Type 1 jetty.
The first classification is a type 1 jetty. This type of jetty is significantly longer than the surf zone width and the waves break at the shore end of the jetty. The effect of a Type 1 jetty is sediment accumulation in a wedge formation on the jetty. These waves are large and increase in size as they pass over the sediment wedge formation. An example of a Type 1 jetty is Mission Beach, San Diego, California. This 1000-meter jetty was installed in 1950 at the mouth of Mission Bay. The surf waves happen north of the jetty, are longer waves, and are powerful. The bathymetry of the sea bottom in Mission Bay has a wedge shape formation that causes the waves to refract as they become closer to the jetty. The waves converge constructively after they refract and increase the sizes of the waves.
Type 2 jetty.
A type 2 jetty occurs in an ebb tidal delta, a delta transitioning between high and low tide. This area has shallow water, refraction, and a distinctive seabed shapes that creates large wave heights.
An example of a type 2 jetty is called "The Poles" in Atlantic Beach, Florida. Atlantic Beach is known to have flat waves, with exceptions during major storms. However, "The Poles" has larger than normal waves due to a 500-meter jetty that was installed on the south side of the St. Johns. This jetty was built to make a deep channel in the river. It formed a delta at "The Poles". This is special area because the jetty increases wave size for surfing, when comparing pre-conditions and post-conditions of the southern St. Johns River mouth area.
The wave size at "The Poles" depends on the direction of the incoming water. When easterly waters (from 55°) interact with the jetty, they create waves larger than southern waters (from 100°). When southern waves (from 100°) move toward "The Poles", one of the waves breaks north of the southern jetty and the other breaks south of the jetty. This does not allow for merging to make larger waves. Easterly waves, from 55°, converge north of the jetty and unite to make bigger waves.
Type 3 jetty.
A type 3 jetty is in an ebb tidal area with an unchanging seabed that has naturally created waves. Examples of a Type 3 jetty occurs in “Southside” Tamarack, Carlsbad, California.
Type 4 jetty.
A type 4 jetty is one that no longer functions nor traps sediment. The waves are created from reefs in the surf zone. A type 4 jetty can be found in Tamarack, Carlsbad, California.
Rip currents.
Rip currents are fast, narrow currents that are caused by onshore transport within the surf zone and the successive return of the water seaward. The wedge bathymetry makes a convenient and consistent rip current of 5–10 meters that brings the surfers to the “take off point” then out to the beach.
Oceanographers have two theories on rip current formation. The wave interaction model assumes that two edges of waves interact, create differing wave heights, and cause longshore transport of nearshore currents. The Boundary Interaction Model assumes that the topography of the sea bottom causes nearshore circulation and longshore transport; the result of both models is a rip current.
Rip currents can be extremely strong and narrow as they extend out of the surf zone into deeper water, reaching speeds of 1–2 feet per second to 8 feet per second. The water in the jet is sediment rich, bubble rich, and moves rapidly. The rip head of the rip current has long shore movement. Rip currents are common on beaches with mild slopes that experience sizable and frequent oceanic swell.
The vorticity and inertia of rip currents were studied. From a model of the vorticity of a rip current done at Scripps Institute of Oceanography, it was found that a fast rip current extends away from shallow water, the vorticity of the current increases, and the width of the current decreases. This model also acknowledges that friction plays a role and waves are irregular in nature. From data from Sector-Scanning Doppler Sonar at Scripps Institute of Oceanography, it was found that rip currents in La Jolla, CA lasted several minutes, reoccurred one to four times per hour, and created a wedge with a 45° arch and a radius 200–400 meters.
On the surfboard.
A long surfboard (10 feet) causes more friction with the water; therefore, it will be slower than a smaller lighter board (6 feet). Longer boards are good for beginners who need help balancing. Smaller boards are good for more experienced surfers who want to have more control and maneuverability.
When practicing the sport of surfing, the surfer paddles out past the wave break to wait for a wave. When a surfable wave arrives, the surfer must paddle extremely fast to match the velocity of the wave so the wave can accelerate him or her.
In order to match acceleration of the wave, the surfer must be a strong swimmer. A scientific study was done that measured the optimal distance apart from each finger when swimming in order to gain the most distance and force in the water. The study analyzed evolutionary advancement from the palms and feet of aquatic animals and reptiles and compared them to humans’ hands. The results showed an ideal separation of 0.2 diameters to 0.4 diameters of a human digit between each digit for maximal velocity and minimal friction. This study has implications in surfing for the paddling technique to reach the same velocity as the incoming wave. Therefore, surfers should spread their fingers 0.2 to 0.4 diameters to most efficiently paddle in the water.
When the surfer is at wave speed, the surfer must quickly pop up, stay low, and stay toward the front of the wave to become stable and prevent falling as the wave steepens. The acceleration is less toward the front than toward the back. The physics behind the surfing of the wave involves the horizontal acceleration force (Fsinθ) and the vertical force (Fcosθ=mg). Therefore, the surfer should lean forward to gain more speed, and lean on back foot to brake. Also, to increase the length of the ride of the wave, the surfer should travel parallel to the wave crest.
Famous locations.
In Australia.
Margaret River – Yallingup and Prevelly Park, WA (Western Australia).
260 km south of Perth, the tiny resort village of Yallingup marks the beginning of the famed Margaret River winery region, where wine enthusiasts and ‘waxheads’ (board-riders) have long converged in equal numbers. With several breaks that range from mild to monstrous depending on the swell, Yallingup is considered the best all-round surfing destination on Australia’s west coast.
Further south, Prevelly Park is the heart of serious Margaret River surfing territory, where swells up to six metres get spun into perfect barrels across the treacherous offshore reef. No place for beginners or the faint-of-heart, "Surfers Point" at Prevelly even attracts the big-name big-wave lunatics from the US, and it’s one of the few places in Australia where board-riders wear helmets and nobody laughs at them.
Crescent Head, NSW.
The coastline beginning just north of Port Macquarie through to Crescent Head is accessed via Point Plomer Road, which ribbons the coast for 25 km. The point break at Crescent Head itself is revered by long-boarders the world over, and some of the sport’s best have been filmed here "Hanging Ten" or cross-stepping the length of their 10-foot planks, however, the accumulated sand necessary to enable this wave to run has been significantly depleted in recent years, the result being that the break has suffered in consistency. There is no longer an actual ride-able wave on the point, its glory days only living on in the memory of local surfers. Halfway between Crescent Head and Point Plomer is the brilliantly named Delicate Nobby, a wedge-shaped rock formation that starts just off the beach and spears out into the Pacific, creating beach breaks on either side.
Northern Beaches, NSW.
Beginning at Manly Beach and running 20 km north to Palm Beach, Sydney’s northern peninsula offers a succession of surf beaches unmatched by a city environment anywhere else on earth. Manly itself has playful beach breaks and punchy barrels, plus the offshore Queenscliff "Bommie" (bombora), joy for big wave riders. Neighbouring Freshwater Beach is much loved by bodysurfers and youngsters on body-boards; this is also where surfboard-riding was first introduced to Australia by Hawaiian surfer Duke Kahanamoku, on 15 January 1915. Continuing north, the 6 km coastal corridor between Dee Why Beach and North Narrabeen is widely considered Sydney’s blue-ribbon surfing belt, with the legendary Long Reef bombora (known locally as "Butter-box") situated smack in the middle. The surfing tribes of Mona Vale Beach, Newport Beach, Whale Beach and Avalon Beach can all make a good case for choosing their own local breaks over their neighbours’, or you could try all four beaches in a lazy half-day. Finally, the distinctive burnt-orange sands of Palm Beach mark the end of the peninsula, its 1.5 km procession of beach breaks offering thrills and spills for surfers, body-boarders and wave-ski paddlers.
Seal Rocks and Pacific Palms, NSW.
Lighthouse Beach and Treachery Beach at Seal Rocks are south-facing and known for generating epic waves when a south swell rolls in. Just 22 km up the road at Pacific Palms, Boomerang Beach and Bluey’s Beach are blessed with their own postcard waves shaped by prominent headlands, and often visited by cheeky dolphins that love showing the rest of us how surfing should really be done. This part of the NSW coast has remained miraculously undeveloped too; there’s nary a high-rise, nightclub or casino in sight, making it the perfect place for a true ‘soul surfer’ experience.
Snapper Rocks, QLD.
Snapper Rocks is a sand bottom point break considered as a world-renowned surfing spot on the Gold Coast. Snapper, located at Rainbow Bay, is home to the world-famous ‘Super Bank’, regarded in surfing circles as the longest, most consistent and most hollow wave in the world. The swell here often reaches six to eight feet, and one good, clean wave can transport you from Snapper to Kirra, a distance of almost two kilometres. Snapper Rocks hosts elite international surfing events such as the Quiksilver and Roxy Pro, Rip Curl Masters, and MP Classic. It is also a favourite surfing spot of local world champs, Mick Fanning, Joel Parkinson and Stephanie Gilmore, who enjoy nothing more than surfing their own ‘local’ break when they’re at home.
Noosa – Point Break, QLD.
One of the best and most photogenic long-board breaks in the world, the point at Noosa is capable of producing a genuine 200 metre ride on its best days. In a decent swell especially there’s always a big crew of locals riding it who really know how to "walk the plank", but when it’s smaller it’s perfect for beginners – a long, easy-rolling cruise.
North Coast – Angourie to Byron Bay, NSW.
When the surfing counter-culture took hold in Australia in the late 1960s, the NSW north coast quickly became the promised land for anyone with a board and a hankering for an alternative lifestyle communing with the waves. "Discovered" in the early 1970s, the point break at Angourie remained relatively unheralded for the next two decades, but it’s world famous nowadays as home break of Aussie surfing legend Nat Young. Endlessly filmed and fawned over, the right-hand point-break at Lennox Head rates a mention in any discussion of Australia’s best wave.
Bells Beach, Victoria.
Although the final scene of the film "Point Break" is supposedly set at Bells Beach, the scene was not filmed there. Bells Beach is a straight stretch and the beach in the film is a cove with spruce trees atop a hill. The actual location of the film was a beach called Indian Beach, in Ecola State Park, located in Cannon Beach, Oregon, USA. Bells Beach is visited in the 1966 documentary film The Endless Summer.
Bells Beach is the home of the world's longest-running surfing competition – the Rip Curl Pro Surf & Music Festival. The event was formerly known as the Bells Beach Surf Classic. The competition was first held in January 1961 and then at Easter every year since although occasionally, when conditions at Bells are unsuitable, the competition has been transferred to other breaks such as Johanna.
As early as 1939, surfers from Torquay made their way to Bells, but access was a considerable problem until 1960 when Torquay surfers and Olympic wrestler Joe Sweeney hired a bulldozer and cleared a road along the Bells cliff from the Cobb & Co Road, where the concrete wave now stands, down to the beach. He charged one pound per surfer to recover his expenses. This is now part of the Torquay to Anglesea walking track.
Nearby surf breaks include "Southside", "Centreside", "Rincon", "Winki Pop", (Uppers and Lowers), Boobs and Steps. Although Bells is known internationally as one of the best breaks in Victoria, "Winki Pop" often works better under more diverse conditions than the other nearby breaks.
In 1988, a group of local surfers who were concerned about the human impact that tourism was having on the Bells Beach Surfing Reserve started a group called Surfers Appreciating the Natural Environment. Since 1988 they have met monthly to revegetate the reserve in an effort to bring it back to its original state. They have planted over 100,000 plants there.
In Asia.
Arugam Bay, Ullae (Pottuvil, Sri Lanka).
Arugam Bay is a small fishing village that was, for many years, only known to a small group of surfers, who considered the area to be Asia's surfing "mecca" ever since the 1960s. Due to Sri Lanka's long-running civil war, this remote half-moon-shaped bay was mostly unknown to visitors and tourists. The consistent swell; shark-free, permanently warm (), clear water; and budget accommodation brought Arugam Bay to the attention of international surfers.
In June 2010, the ASP held an international competition—the "6-Star SriLankan Airlines Pro"—at Arugam Bay that was won by Australian Julian Wilson. Prior to the commencement of the inaugural ASP event, the location's warm waters and "high performance sand bottom point waves" were highlighted.
Following a tsunami in 2004, most of the hotels in Arugam Bay were destroyed. By 2008, most of the tourism infrastructure was restored and the Community Eco-Guide Association (CEGA)—a thirteen-member collaboration between local community-based organizations (CBOs), cooperatives and associations—promotes sustainable community-based tourism in the area.
In the South Pacific.
Teahupoʻo (Tahiti).
Teahupoʻo (pronounced cho-po) is a world-renowned surf spot off the South West of the island of Tahiti, French Polynesia, southern Pacific Ocean. It is known for its heavy, glassy waves, often reaching and higher. It is the site of the annual Billabong Pro Tahiti surf competition, part of the World Championship Tour (WCT) of the ASP World Tour professional surfing circuit.
In South Africa.
Jeffreys Bay (Eastern Cape).
The break is regarded as one of the best right-hand point breaks in the entire world, in both consistency and quality, in season. It has been divided up into several sections, including, from the top of the point, Kitchen Windows, Magna tubes, Boneyards, Supertubes, Impossibles, Tubes, the Point, and Albatross. "Supertubes", which itself breaks for about 300m or more, is regarded as the best part of the wave. On rare occasions (large wave sizes, wide-breaking waves, and even swells), Boneyards can link up all the way to the Point for a ride over one kilometer long. The most consistent waves occur between about May to mid September, also often coinciding with offshore winds, although good waves can occasionally occur at other times of the year.
The initial discovery and promotion of the wave is curious. Another nearby right-hand point wave at St Francis Bay (Bruce's Beauty) was first idolised and promoted in the cult classic surf movie "The Endless Summer" in the 1960s (although both Jeffreys Bay and St. Francis Bay had probably been surfed much earlier). Surfers who travelled to the area soon stumbled upon the nearby Jeffreys Bay surf break, which was found to be not only a faster, more powerful, and hollower wave, but also much more consistent.
In North America.
Zicatela Beach (Mexican Pipeline).
Zicatela is a beach located in the town of Puerto Escondido, Oaxaca. Nicknamed the "Mexican Pipeline" due to the similar power and shape of the Banzai Pipeline on the North Shore of Oahu, the wave that breaks on Zicatela Beach draws an international crowd of surfers, bodyboarders and their entourages. Mid to late summer is low season for tourists, but a prime time for waves and international tournaments. A number of international competitions such as the ESPN X Games, and the MexPipe Challenge have taken place.
In Central America.
Costa Rica.
Costa Rica is home to some of the best waves on the planet, crashing along beaches that are clean and blissfully empty. The local Ticos have picked up the skills to surf and win international competitions. Many now run their own surf camps while training for the National Surf Circuit, which inspires the next generation of surfers.
The Caribbean
For those surfing on the Caribbean coast, keep in mind that the surfing season doesn't last long, and most of the waves are produced by tropical storms off the Mexican coast. However, these storms produce some of the biggest waves in the country and should only be attempted by veteran riders.
Puerto Viejo also known as Salsa Brava, is home to some of Costa Rica's biggest waves. A coral reef off the coast produces Hawaiian-style breaks that often carry over 20 surfers at a time. Located in the Limon province, Puerto Viejo is a vibrant seaside town with gorgeous waters and waves that tempt surfing pros.Westfalia extends south from Limon to Cahuita, offering breaks from the left and right. The best part of Westfalia is its reliability -- it offers consistently good surf, but be forewarned of the riptides and jellyfish.
Portete beach is located very close to Limon, so you'll have easy access to the small bay. It's also a great spot for swimming and snorkeling if you're the only surfer in the family. Portete is also close to Playa Bonita, where you can test out some bigger waves.
North Pacific Coast
Is encompassed by the Guanacaste Province and is the country's most visited region. However, this area, home to some of the best and most easily accessible diving and surfing spots, is also one of Costa Rica's least populated regions.
Playa Naranjo, more commonly known as Witch's Rock, is one of the best breaks in the country, with some very strong offshore winds between December and March. There aren't any resorts or hotels located here, but nearby Witch's Rock Surf Camp is the perfect place to stay and learn. Instructors will teach you the basics and shuttle you and other newbies to the famous swells of Playa Naranjo. You can also camp out, but 4-wheel drive is a must and don't forget a mosquito net.
Playa Grande provides consistent swells and breaks, making it one of the best overall surf spots in the country. It also offers a quieter spot than beaches like Playa Tamarindo and the beach's north side is a great location for swimming, safe from the dozens of surfers tearing through the Costa Rican waters.
Playa Negra is a right-handed point break that the locals describe as epic. The waves are as powerful as they are reliable. For experienced surfers, the ride is best when the waves are breaking overhead, so head to Playa Negra on days when storms are kicking up some aggressive surf.
Central Pacific Coast
This region is the most easily accessible, just a short distance from San Jose. The resort towns offer a bit more nightlife than the other coasts, and the surf here is fantastic, making Central Pacific the place for a more lively vacation.
A beach that offers one of Costa Rica's longest rides, Boca Barranca is a left break in front of a river mouth. You can ride these waves over half a mile when the conditions are right, so if you longboard, this is the beach for you. There's great access to this beach, so it's best to arrive early to avoid crowds.
Playa Escondida is an excellent point break for rights and lefts. For members of the beach club, the surf is accessible by land. If you're just visiting, rent a boat from Playa Jaco or Playa Herradura. These waves are best surfed at high tide, so check tide times before hitting the surf.
Hermosa beach juts out into the Pacific Ocean, giving the waters there constant swell. The beach stretches for several miles, so you should be able to stake out your own territory. Some of the best breaks occur off a sandbar located in front of a large tree known as the Almendro. Ask a local to point out this primo spot.
South Pacific Coast
Unlike the Central Pacific Coast, the South Pacific is home to some of the largest last remaining stretches of Pacific Coastal rainforests. This is the coast surf if you're only packing a wetsuit, a board and a sleeping bag -- you're surfing with Madre Nature.
Playa Dominical offers beautiful lush landscapes and waves for all skill levels. On the north end of the beach in front of the Baru River mouth, the surf is a little bigger and provides a challenge for skilled surfers.
Pavones is a left point that offers one of the longest rides in the world. This beach is located 8 hours from San Jose by car, but you're welcome to pack your camping gear and stay a few nights. Rides here can last over three-quarters of a mile, considered one of the longest in the world. The surf here can be fickle, with the best surf occurring between April and November.
Cabo Matapalo is located directly across the bay from Pavones and catches the same ocean swells. Matapalo is much less surfed than Pavones, so if you're looking to avoid crowds or just learning, this is an excellent beach for you. It's drivable from Pavones, so make sure to hit both of these spots on your surfing tour of Costa Rica.
El Salvador.
El Puerto is home to one of the best right points in Central America, known for its fast hollow, pulsing, over 30-second ride waves. Punta Roca (also called "La Punta" by local surfers) has been the perfect spot for many known surfers who back in the 1970s discovered the point with only a few local surfers brave enough to venture into its rocky bottom plane. It is known that legend Gerry Lopez, traveled frequently to this surf spot back in the 1970s encouraging a new wave of locals to get into the sport.
By the 1980s, El Salvador went through a civil war, and getting to the point was rather dangerous slowing visitors, and with that, a scarcity of surf boards to the locals whose only means of getting a surf board was by travelers leaving them behind in exchange of guidance and accommodations. Local legend, "Yepi" was one of the first of his generation to take on full self-support and help maintain the sport, a popular activity among locals. Locals have also been increasing the popularity of the sport throughout the country by offering custom surf tours to tourists and visitors in the region.
The main wave extends from La Punta to the township, a distance of about 800m, although single rides do not normally connect along this whole distance. On a good 6 to 8 feet day (Hawaiian scale), the top part of the point produces the best waves, giving a ride of about 300m or more. The wave features a relatively easy takeoff with long, fast, powerful walls, with longer hollow barrels on the best days. This wave works from about 3 to 12 feet (Hawaiian scale), and can barrel anywhere along the point, but most often closest to the takeoff area. The main takeoff is close to a dangerous rock which often sticks out of the water, and has caused injuries. It works on all tides, although low tide probably has more barrels. The wave is unusual in that it often breaks at a slight angle to the shoreline, hitting it slightly squarely, creating powerful and fast walls. It can be difficult to get out the back in large swells, and the rocky shoreline is notorious for its rather difficult entry.
Further down the point are a few other breaks, including next to the cemetery and in the town itself. These are less crowded and can produce waist-high waves on occasions, but the world-class section of the point is way on the outside. Other surf spots around the region include: Conchalio, La Paz, San Diego, El Tunco, El Zunzal, La Bocana, El Zonte.
In South America.
Chicama (La Libertad-Peru).
This is one of the longest waves in the world with up to 4 km of left waves over more than 3 separate sections of surf. The different sections on the long cape don't link up, and the longest rides are usually only up to about 1 km.
Montanita (Ecuador).
Montanita has been the venue of many diverse international surfing competitions in recent years. This beach has something for everyone, offering several places for jumps and long rides. The swell direction is north-northwest; the water ranges medium to deep, and the bottom is rocky.
In the USA.
Mavericks (California).
Maverick's or Mavericks is a world-famous surfing location in Northern California. It is located approximately one-half mile (0.8 km) from shore in Pillar Point Harbor, just North of Half Moon Bay at the village of Princeton-By-The-Sea. After a strong winter storm in the Northern Pacific Ocean, waves can routinely crest at over and top out at over . The break is caused by an unusually-shaped underwater rock formation.
Huntington Beach (California).
Huntington also known as Surf City, USA is a world-famous surfing location in Southern California. The south side of the pier is where the annual U.S. surfing championship is held. The famous annual U.S. Open of Surfing is the largest surfing competition in the world. Both sides of the pier are popular for surfing as Huntington Beach is known as a year-round surfing spot. Near the pier, the ocean waves here are enhanced by a natural effect caused by the edge-diffraction of open ocean swells around Catalina Island, creating consistent surf year-round.
Trestles (San Clemente).
Located at the northern end of San Diego County and named for a nearby railroad bridge, Trestles is a series of breaks known for their unique quality (wave shape), particularly Lowers (also called Lower Trestles). Each break is popular depending on swell direction, season, and each surfer's preferred riding style. Lowers is frequently the venue of world-class surfing events, including the top-level of professional surfing. Lowers is often considered the best summertime high-performance wave in California.
54th St at Newport Beach (Echo Beach).
Located between jetties 52 and 56, 54th St has been a major epicenter for the surf industry. Due to its photogenic nature, where the waves break close to the beach, hollow surf and fast rides, 54th St has been a mainstay for surfers both local and traveling. Not to mention, which iconic surf brands such as Volcom, Quiksilver and Hurley all being born right near this patch of sand, 54th is an iconic Southern California beach break.
Pipeline (Hawaii).
Pipeline is a surf reef break located in Hawaii, off Ehukai Beach Park in Pupukea on O'ahu's North Shore. The spot is notorious and famous for its huge waves breaking in shallow water just above its sharp and cavernous reef, forming large, hollow and thick curls of water that surfers can ride inside of. There are three reefs at Pipeline in progressively deeper water further out to sea that activate at various power levels applied by ocean swells.
In Europe.
Costa da Caparica (Almada, Portugal).
Caparica Beaches are popular Atlantic beaches located on Portugal's Almada coast, near Lisbon. The Caparica Coast, with part of the "Protected Landscape of the Ancient Beach of Costa da Caparica", is visible the Convent of the Capuchos. The beach has preferred surfing conditions and is also popular for windsurfing, and kitesurfing. The International Surf Center is based in Caparica.
Supertubos (Peniche, Portugal).
The little fishing town of Peniche is probably the most renowned surfing area in Portugal. Originally an island, Peniche became one with the mainland due to the silting up of the shallow channel that divided it from the rest of the country. Today that short and narrow spit of land contains an obscene amount of wave variety that can provide the goods in almost any conditions. Most famous is Supertubos, regarded by many as one of Europe’s best beach breaks, but there are plenty of other barrels to pull into around Peniche. Peniche is a year round destination with swell exposure on the north side of the town and shelter on the south. The town also sits at the dividing point between the cooler and wetter north and the dry, sunny south meaning that summers are long but tempered by cool sea breezes and the winters mild though occasionally stormy. Supertubos is considered the best wave in Portugal and one of the best in Europe. It is a fast and tubular wave which breaks on a hollow sand bank. It works best with SW swells and N, NE or NW winds. Andy Irons, Kelly Slater and Mick Fanning made frequent appearances in the WSL Supertubos surf competitions.
Nazaré (Portugal).
Nazaré has become a popular tourist attraction, advertising itself, internationally, as a picturesque seaside village. Located on the Atlantic coast, it has long sandy beaches (considered by some to be among the best beaches in Portugal), with lots of tourists in the summer. Nazaré is where the largest wave ever surfed, in 2011, by the American surfer Garrett McNamara. The town used to be known for its traditional costumes worn by the fishermen and their wives who wore a traditional headscarf and embroidered aprons over seven flannel skirts in different colours. These dresses can still occasionally be seen.
Golden Bay (Malta).
The tiny island in the middle of the Mediterranean offer a lot of surf spots. One of the most popular ones are Golden Bay. The west side beach offers a long left when the wind and swell conditions are working in your favour. Westerly, south-westerly and north-westerly winds all work, and swell holds up to roughly 8ft.
Varberg (Sweden)
Varberg is a surf town (the only one of its kind in Sweden) on the Swedish west coast. It got famous in the ‘80s for windsurfing, in the mid-2000s for kitesurfing and, and in the last 10 years for surfing. Normally there is limited amount a good waves. In the Nordics every decade or so there are some big storms. The storms generate good waves that are locally called as the aalbun waves.
Dangers.
Drowning.
Surfing, like all water sports, carries the inherent danger of drowning. Anyone at any age can learn to surf, but should have at least intermediate swimming skills. Although the board assists a surfer in staying buoyant, it can become separated from the user. A leash, attached to the ankle or knee, can keep a board from being swept away, but does not keep a rider on the board or above water. In some cases, possibly including the drowning of professional surfer Mark Foo, a leash can even be a cause of drowning by snagging on a reef or other object and holding the surfer underwater. By keeping the surfboard close to the surfer during a wipeout, a leash also increases the chances that the board may strike the rider, which could knock him or her unconscious and lead to drowning. A fallen rider's board can become trapped in larger waves, and if the rider is attached by a leash, he or she can be dragged for long distances underwater. Surfers should be careful to remain in smaller surf until they have acquired the advanced skills and experience necessary to handle bigger waves and more challenging conditions. However, even world-class surfers have drowned in extremely challenging conditions.
Collisions.
Under the wrong set of conditions, anything that a surfer's body can come in contact with is potentially a danger, including sand bars, rocks, small ice, reefs, surfboards, and other surfers. Collisions with these objects can sometimes cause injuries such as cuts and scrapes and in rare instances, death.
A large number of injuries, up to 66%, are caused by collision with a surfboard (nose or fins). Fins can cause deep lacerations and cuts, as well as bruising. While these injuries can be minor, they can open the skin to infection from the sea; groups like Surfers Against Sewage campaign for cleaner waters to reduce the risk of infections. Local bugs and disease can be a dangerous factor when surfing around the globe.
Falling off a surfboard or colliding with others is commonly referred to as a "wipeout".
Marine life.
Sea life can sometimes cause injuries and even fatalities. Animals such as sharks, stingrays, Weever fish, seals and jellyfish can sometimes present a danger. Warmer-water surfers often do the "stingray shuffle" as they walk out through the shallows, shuffling their feet in the sand to scare away stingrays that may be resting on the bottom.
Rip currents.
Rip currents are water channels that flow away from the shore. Under the wrong circumstances these currents can endanger both experienced and inexperienced surfers. Since a rip current appears to be an area of flat water, tired or inexperienced swimmers or surfers may enter one and be carried out beyond the breaking waves. Although many rip currents are much smaller, the largest rip currents have a width of forty or fifty feet. However, by paddling parallel to the shore, a surfer can easily exit a rip current. Alternatively, some surfers actually ride on a rip current because it is a fast and effortless way to get out beyond the zone of breaking waves.
Seabed.
The seabed can pose dangers for surfers. If a surfer falls while riding a wave, the wave tosses and tumbles the surfer around, often in a downwards direction. At reef breaks and beach breaks, surfers have been seriously injured and even killed because of a violent collision with the sea bed, the water above which can sometimes be very shallow, especially at beach breaks or reef breaks during low tide. Cyclops, Western Australia, for example is one of the biggest and thickest reef breaks in the world, with waves measuring up to 10 metres high, but the reef below is only about below the surface of the water.

</doc>
<doc id="28202" url="https://en.wikipedia.org/wiki?curid=28202" title="September 24">
September 24


</doc>
<doc id="28203" url="https://en.wikipedia.org/wiki?curid=28203" title="September 25">
September 25


</doc>
<doc id="28204" url="https://en.wikipedia.org/wiki?curid=28204" title="September 29">
September 29


</doc>
<doc id="28207" url="https://en.wikipedia.org/wiki?curid=28207" title="Short Message Service">
Short Message Service

Short Message Service (SMS) is a text messaging service component of phone, Web, or mobile communication systems. It uses standardized communications protocols to allow fixed line or mobile phone devices to exchange short text messages.
SMS was the most widely used data application, with an estimated 3.5 billion active users, or about 80% of all mobile phone subscribers at the end of 2010. The term "SMS" is used for both the user activity and all types of short text messaging in many parts of the world. SMS is also employed in direct marketing, known as SMS marketing. As of September 2014, global SMS messaging business is said to be worth over USD 100 billion, and SMS accounts for almost 50 percent of all the revenue generated by mobile messaging.
SMS as used on modern handsets originated from radio telegraphy in radio memo pagers using standardized phone protocols. These were defined in 1985 as part of the Global System for Mobile Communications (GSM) series of standards as a means of sending messages of up to 160 characters to and from GSM mobile handsets. Though most SMS messages are mobile-to-mobile text messages, support for the service has expanded to include other mobile technologies, such as ANSI CDMA networks and Digital AMPS, as well as satellite and landline networks.
History.
Initial concept.
Adding text messaging functionality to mobile devices began in the early 1980s. The first action plan of the CEPT Group GSM was approved in December 1982, requesting that, "The services and facilities offered in the public switched telephone networks and public data networks ... should be available in the mobile system." This plan included the exchange of text messages either directly between mobile stations, or transmitted via message handling systems in use at that time.
The SMS concept was developed in the Franco-German GSM cooperation in 1984 by Friedhelm Hillebrand and Bernard Ghillebaert. The GSM is optimized for telephony, since this was identified as its main application. The key idea for SMS was to use this telephone-optimized system, and to transport messages on the signalling paths needed to control the telephone traffic during periods when no signalling traffic existed. In this way, unused resources in the system could be used to transport messages at minimal cost. However, it was necessary to limit the length of the messages to 128 bytes (later improved to 160 seven-bit characters) so that the messages could fit into the existing signalling formats. Based on his personal observations and on analysis of the typical lengths of postcard and Telex messages, Hillebrand argued that 160 characters was sufficient to express most messages succinctly.
SMS could be implemented in every mobile station by updating its software. Hence, a large base of SMS-capable terminals and networks existed when people began to use SMS. A new network element required was a specialized short message service centre, and enhancements were required to the radio capacity and network transport infrastructure to accommodate growing SMS traffic.
Early development.
The technical development of SMS was a multinational collaboration supporting the framework of standards bodies. Through these organizations the technology was made freely available to the whole world.
The first proposal which initiated the development of SMS was made by a contribution of Germany and France into the GSM group meeting in February 1985 in Oslo. This proposal was further elaborated in GSM subgroup WP1 Services (Chairman Martine Alvernhe, France Telecom) based on a contribution from Germany. There were also initial discussions in the subgroup WP3 network aspects chaired by Jan Audestad (Telenor). The result was approved by the main GSM group in a June '85 document which was distributed to industry. The input documents on SMS had been prepared by Friedhelm Hillebrand (Deutsche Telekom) with contributions from Bernard Ghillebaert (France Télécom). The definition that Friedhelm Hillebrand and Bernard Ghillebaert brought into GSM called for the provision of a message transmission service of alphanumeric messages to mobile users "with acknowledgement capabilities". The last three words transformed SMS into something much more useful than the prevailing messaging paging that some in GSM might have had in mind.
SMS was considered in the main GSM group as a possible service for the new digital cellular system. In GSM document ""Services and Facilities to be provided in the GSM System,"" Here a rudimentary description of the three services was given:
The material elaborated in GSM and its WP1 subgroup was handed over in Spring 1987 to a new GSM body called IDEG (the Implementation of Data and Telematic Services Experts Group), which had its kickoff in May 1987 under the chairmanship of Friedhelm Hillebrand (German Telecom). The technical standard known today was largely created by IDEG (later WP4) as the two recommendations GSM 03.40 (the two point-to-point services merged) and GSM 03.41 (cell broadcast).
WP4 created a Drafting Group Message Handling (DGMH), which was responsible for the specification of SMS. Finn Trosby of Telenor chaired the draft group through its first 3 years, in which the design of SMS was established. DGMH had five to eight participants, and Finn Trosby mentions as major contributors Kevin Holley, Eija Altonen, Didier Luizard and Alan Cox. The first action plan mentions for the first time the Technical Specification 03.40 "Technical Realisation of the Short Message Service". Responsible editor was Finn Trosby. The first and very rudimentary draft of the technical specification was completed in November 1987. However, drafts useful for the manufacturers followed at a later stage in the period. A comprehensive description of the work in this period is given in.
The work on the draft specification continued in the following few years, where Kevin Holley of Cellnet (now Telefónica O2 UK) played a leading role. Besides the completion of the main specification GSM 03.40, the detailed protocol specifications on the system interfaces also needed to be completed.
Support in other architectures.
The Mobile Application Part (MAP) of the SS7 protocol included support for the transport of Short Messages through the Core Network from its inception. MAP Phase 2 expanded support for SMS by introducing a separate operation code for Mobile Terminated Short Message transport. Since Phase 2, there have been no changes to the Short Message operation packages in MAP, although other operation packages have been enhanced to support CAMEL SMS control.
From 3GPP Releases 99 and 4 onwards, CAMEL Phase 3 introduced the ability for the Intelligent Network (IN) to control aspects of the Mobile Originated Short Message Service, while CAMEL Phase 4, as part of 3GPP Release 5 and onwards, provides the IN with the ability to control the Mobile Terminated service. CAMEL allows the gsmSCP to block the submission (MO) or delivery (MT) of Short Messages, route messages to destinations other than that specified by the user, and perform real-time billing for the use of the service. Prior to standardized CAMEL control of the Short Message Service, IN control relied on switch vendor specific extensions to the Intelligent Network Application Part (INAP) of SS7.
Early implementations.
The first SMS message was sent over the Vodafone GSM network in the United Kingdom on 3 December 1992, from Neil Papworth of Sema Group (now Mavenir Systems) using a personal computer to Richard Jarvis of Vodafone using an Orbitel 901 handset. The text of the message was "Merry Christmas."
The first commercial deployment of a short message service center (SMSC) was by Aldiscon part of Logica (now part of Acision) with Telia (now TeliaSonera) in Sweden in 1993, followed by Fleet Call (now Nextel) in the US, Telenor in Norway and BT Cellnet (now O2 UK) later in 1993. All first installations of SMS gateways were for network notifications sent to mobile phones, usually to inform of voice mail messages.
The first commercially sold SMS service was offered to consumers, as a person-to-person text messaging service by Radiolinja (now part of Elisa) in Finland in 1993. Most early GSM mobile phone handsets did not support the ability to send SMS text messages, and Nokia was the only handset manufacturer whose total GSM phone line in 1993 supported user-sending of SMS text messages. According to Matti Makkonen, the inventor of SMS text messages, Nokia 2010, which was released in January 1994, was the first mobile phone to support composing SMSes easily.
Initial growth was slow, with customers in 1995 sending on average only 0.4 messages per GSM customer per month. One factor in the slow takeup of SMS was that operators were slow to set up charging systems, especially for prepaid subscribers, and eliminate billing fraud which was possible by changing SMSC settings on individual handsets to use the SMSCs of other operators. Initially, networks in the UK only allowed customers to send messages to other users on the same network, limiting the usefulness of the service. This restriction was lifted in 1999.
Over time, this issue was eliminated by switch billing instead of billing at the SMSC and by new features within SMSCs to allow blocking of foreign mobile users sending messages through it. By the end of 2000, the average number of messages reached 35 per user per month, and on Christmas Day 2006, over 205 million messages were sent in the UK alone.
It is also alleged that the fact that roaming customers, in the early days, rarely received bills for their SMSs after holidays abroad which gave a boost to text messaging as an alternative to voice calls.
Text messaging outside GSM.
SMS was originally designed as part of GSM, but is now available on a wide range of networks, including 3G networks. However, not all text messaging systems use SMS, and some notable alternative implementations of the concept include J-Phone's "SkyMail" and NTT Docomo's "Short Mail", both in Japan. Email messaging from phones, as popularized by NTT Docomo's i-mode and the RIM BlackBerry, also typically uses standard mail protocols such as SMTP over TCP/IP.
SMS today.
, 6.1 trillion (6.1 × 1012) SMS text messages were sent. This translates into an average of 193,000 SMS per second. SMS has become a huge commercial industry, earning $114.6 billion globally in 2010. The global average price for an SMS message is US$0.11, while mobile networks charge each other interconnect fees of at least US$0.04 when connecting between different phone networks.
In 2015, the actual cost of sending an SMS in Australia was found to be $0.00016 per SMS.
In 2014, Caktus Group developed the world's first SMS-based voter registration system in Libya. So far, more than 1.5 million people have registered using that system, providing Libyan voters with unprecedented access to the democratic process.
While SMS is still a growing market, traditional SMS is becoming increasingly challenged by alternative messaging services such as Facebook Messenger, WhatsApp and Viber available on smart phones with data connections, especially in Western countries where these services are growing in popularity. Enterprise SMS-messaging, also known as application-to-peer messaging (A2P Messaging) or 2-way SMS, continue to grow steadily at a rate of 4% annually. Enterprise SMS applications are primarily focused on CRM and delivering highly targeted service messages such as parcel-delivery alerts, real-time notification of credit/debit card purchase confirmations to protect against fraud, and appointment confirmations. Another primary source of growing A2P message volumes is two-step verification (alternatively referred to as 2-factor authentication) processes whereby users are delivered a one-time passcode over SMS and then are asked to enter that passcode online in order to verify their identity.
Technical details.
GSM.
The "Short Message Service—Point to Point (SMS-PP)"—was originally defined in GSM recommendation 03.40, which is now maintained in 3GPP as TS 23.040. GSM 03.41 (now 3GPP TS 23.041) defines the "Short Message Service—Cell Broadcast (SMS-CB)", which allows messages (advertising, public information, etc.) to be broadcast to all mobile users in a specified geographical area.
Messages are sent to a short message service center (SMSC), which provides a "store and forward" mechanism. It attempts to send messages to the SMSC's recipients. If a recipient is not reachable, the SMSC queues the message for later retry. Some SMSCs also provide a "forward and forget" option where transmission is tried only once. Both mobile terminated (MT, for messages sent "to" a mobile handset) and mobile originating (MO, for those sent "from" the mobile handset) operations are supported. Message delivery is "best effort," so there are no guarantees that a message will actually be delivered to its recipient, but delay or complete loss of a message is uncommon, typically affecting less than 5 percent of messages. Some providers allow users to request delivery reports, either via the SMS settings of most modern phones, or by prefixing each message with *0# or *N#. However, the exact meaning of confirmations varies from reaching the network, to being queued for sending, to being sent, to receiving a confirmation of receipt from the target device, and users are often not informed of the specific type of success being reported.
SMS is a stateless communication protocol in which every SMS message is considered entirely independent of other messages. Enterprise applications using SMS as a communication channel for stateful dialogue (where an MO reply message is paired to a specific MT message) requires that session management be maintained external to the protocol.
Message size.
Transmission of short messages between the SMSC and the handset is done whenever using the Mobile Application Part (MAP) of the SS7 protocol. Messages are sent with the MAP MO- and MT-ForwardSM operations, whose payload length is limited by the constraints of the signaling protocol to precisely 140 octets (140 octets * 8 bits / octet = 1120 bits). Short messages can be encoded using a variety of alphabets: the default GSM 7-bit alphabet, the 8-bit data alphabet, and the 16-bit UCS-2 alphabet. Depending on which alphabet the subscriber has configured in the handset, this leads to the maximum individual short message sizes of 160 7-bit characters, 140 8-bit characters, or 70 16-bit characters. GSM 7-bit alphabet support is mandatory for GSM handsets and network elements, but characters in languages such as Arabic, Chinese, Korean, Japanese, or Cyrillic alphabet languages (e.g., Ukrainian, Serbian, Bulgarian, etc.) must be encoded using the 16-bit UCS-2 character encoding (see Unicode). Routing data and other metadata is additional to the payload size.
Larger content (concatenated SMS, multipart or segmented SMS, or "long SMS") can be sent using multiple messages, in which case each message will start with a User Data Header (UDH) containing segmentation information. Since UDH is part of the payload, the number of available characters per segment is lower: 153 for 7-bit encoding, 134 for 8-bit encoding and 67 for 16-bit encoding. The receiving handset is then responsible for reassembling the message and presenting it to the user as one long message. While the standard theoretically permits up to 255 segments, 6 to 8 segment messages are the practical maximum, and long messages are often billed as equivalent to multiple SMS messages. Some providers have offered length-oriented pricing schemes for messages, however, the phenomenon is disappearing.
Gateway providers.
SMS gateway providers facilitate SMS traffic between businesses and mobile subscribers, including SMS for enterprises, content delivery, and entertainment services involving SMS, e.g. TV voting. Considering SMS messaging performance and cost, as well as the level of messaging services, SMS gateway providers can be classified as aggregators or SS7 providers.
The aggregator model is based on multiple agreements with mobile carriers to exchange two-way SMS traffic into and out of the operator's SMSC, also known as local termination model. Aggregators lack direct access into the SS7 protocol, which is the protocol where the SMS messages are exchanged. SMS messages are delivered to the operator's SMSC, but not the subscriber's handset; the SMSC takes care of further handling of the message through the SS7 network.
Another type of SMS gateway provider is based on SS7 connectivity to route SMS messages, also known as international termination model. The advantage of this model is the ability to route data directly through SS7, which gives the provider total control and visibility of the complete path during SMS routing. This means SMS messages can be sent directly to and from recipients without having to go through the SMSCs of other mobile operators. Therefore, it is possible to avoid delays and message losses, offering full delivery guarantees of messages and optimized routing. This model is particularly efficient when used in mission-critical messaging and SMS used in corporate communications. Moreover, these SMS gateway providers are providing branded SMS services with masking but after misuse of these gateways most countries's Governments have taken serious steps to block these gateways.
Interconnectivity with other networks.
Message Service Centers communicate with the Public Land Mobile Network (PLMN) or PSTN via Interworking and Gateway MSCs.
Subscriber-originated messages are transported from a handset to a service center, and may be destined for mobile users, subscribers on a fixed network, or Value-Added Service Providers (VASPs), also known as application-terminated. Subscriber-terminated messages are transported from the service center to the destination handset, and may originate from mobile users, from fixed network subscribers, or from other sources such as VASPs.
On some carriers nonsubscribers can send messages to a subscriber's phone using an Email-to-SMS gateway. Additionally, many carriers, including AT&T Mobility, T-Mobile USA, Sprint, and Verizon Wireless, offer the ability to do this through their respective websites.
For example, an AT&T subscriber whose phone number was 555-555-5555 would receive e-mails addressed to 5555555555@txt.att.net as text messages. Subscribers can easily reply to these SMS messages, and the SMS reply is sent back to the original email address. Sending email to SMS is free for the sender, but the recipient is subject to the standard delivery charges. Only the first 160 characters of an email message can be delivered to a phone, and only 160 characters can be sent from a phone.
Text-enabled fixed-line handsets are required to receive messages in text format. However, messages can be delivered to nonenabled phones using text-to-speech conversion.
Short messages can send binary content such as ringtones or logos, as well as Over-the-air programming (OTA) or configuration data. Such uses are a vendor-specific extension of the GSM specification and there are multiple competing standards, although Nokia's Smart Messaging is common. An alternative way for sending such binary content is EMS messaging, which is standardized and not dependent on vendors.
SMS is used for M2M (Machine to Machine) communication. For instance, there is an LED display machine controlled by SMS, and some vehicle tracking companies use SMS for their data transport or telemetry needs. SMS usage for these purposes is slowly being superseded by GPRS services owing to their lower overall cost. GPRS is offered by smaller telco players as a route of sending SMS text to reduce the cost of SMS texting internationally.
AT commands.
Many mobile and satellite transceiver units support the sending and receiving of SMS using an extended version of the Hayes command set, a specific command language originally developed for the Hayes Smartmodem 300-baud modem in 1977.
The connection between the terminal equipment and the transceiver can be realized with a serial cable (e.g., USB), a Bluetooth link, an infrared link, etc. Common AT commands include AT+CMGS (send message), AT+CMSS (send message from storage), AT+CMGL (list messages) and AT+CMGR (read message).
However, not all modern devices support receiving of messages if the message storage (for instance the device's internal memory) is not accessible using AT commands.
Premium-rated short messages.
Short messages may be used normally to provide premium rate services to subscribers of a telephone network.
Mobile-terminated short messages can be used to deliver digital content such as news alerts, financial information, logos, and ring tones. The first premium-rate media content delivered via the SMS system was the world's first paid downloadable ringing tones, as commercially launched by Saunalahti (later Jippii Group, now part of Elisa Grous), in 1998. Initially only Nokia branded phones could handle them. By 2002 the ringtone business globally had exceeded $1 billion of service revenues, and nearly $5 billion by 2008. Today, they are also used to pay smaller payments online—for example, for file-sharing services, in mobile application stores, or VIP section entrance. Outside the online world, one can buy a bus ticket or beverages from ATM, pay a parking ticket, order a store catalog or some goods (e.g., discount movie DVDs), make a donation to charity, and much more.
Premium-rated messages are also used in Donors Message Service to collect money for charities and foundations. DMS was first launched at April 1, 2004, and is very popular in the Czech Republic. For example, the Czech people sent over 1.5 million messages to help South Asia recover from the 2004 Indian Ocean earthquake and tsunami.
The Value-added service provider (VASP) providing the content submits the message to the mobile operator's SMSC(s) using an TCP/IP protocol such as the short message peer-to-peer protocol (SMPP) or the External Machine Interface (EMI). The SMSC delivers the text using the normal Mobile Terminated delivery procedure. The subscribers are charged extra for receiving this premium content; the revenue is typically divided between the mobile network operator and the VASP either through revenue share or a fixed transport fee. Submission to the SMSC is usually handled by a third party.
Mobile-originated short messages may also be used in a premium-rated manner for services such as televoting. In this case, the VASP providing the service obtains a short code from the telephone network operator, and subscribers send texts to that number. The payouts to the carriers vary by carrier; percentages paid are greatest on the lowest-priced premium SMS services. Most information providers should expect to pay about 45 percent of the cost of the premium SMS up front to the carrier. The submission of the text to the SMSC is identical to a standard MO Short Message submission, but once the text is at the SMSC, the Service Center (SC) identifies the Short Code as a premium service. The SC will then direct the content of the text message to the VASP, typically using an IP protocol such as SMPP or EMI. Subscribers are charged a premium for the sending of such messages, with the revenue typically shared between the network operator and the VASP. Short codes only work within one country, they are not international.
An alternative to inbound SMS is based on long numbers (international number format, e.g. +44 762 480 5000), which can be used in place of short codes for SMS reception in several applications, such as TV voting, product promotions and campaigns. Long numbers work internationally, allow businesses to use their own numbers, rather than short codes, which are usually shared across many brands. Additionally, long numbers are nonpremium inbound numbers.
Threaded SMS.
Threaded SMS is a visual styling orientation of SMS message history that arranges messages to and from a contact in chronological order on a single screen. It was first invented by a developer working to implement the SMS client for the BlackBerry, who was looking to make use of the blank screen left below the message on a device with a larger screen capable of displaying far more than the usual 160 characters, and was inspired by threaded Reply conversations in email. Visually, this style of representation provides a back-and-forth chat-like history for each individual contact. Hierarchical-threading at the conversation-level (as typical in blogs and on-line messaging boards)is not widely supported by SMS messaging clients. This limitation is due to the fact that there is no session identifier or subject-line passed back and forth between sent and received messages in the header data (as specified by SMS protocol) from which the client device can properly thread an incoming message to a specific dialogue, or even to a specific message within a dialogue. Most smart phone text-messaging-clients are able to create some contextual threading of "group messages" which narrows the context of the thread around the common interests shared by group members. On the other hand, advanced enterprise messaging applications which push messages from a remote server often display a dynamically changing reply number (multiple numbers used by the same sender), which is used along with the sender's phone number to create session-tracking capabilities analogous to the functionality that cookies provide for web-browsing. As one pervasive example, this technique is used to extend the functionality of many Instant Messenger (IM) applications such that they are able to communicate over two-way dialogues with the much larger SMS user-base. In cases where multiple reply numbers are used by the enterprise server to maintain the dialogue, the visual conversation threading on the client may be separated into multiple threads.
Application-to-person (A2P) SMS.
While SMS reached its popularity as a person-to-person messaging, another type of SMS is growing fast: application-to-person (A2P) messaging. A2P is a type of SMS sent from a subscriber to an application or sent from an application to a subscriber. It is commonly used by financial institutions, airlines, hotel booking sites, social networks, and other organizations sending SMS from their systems to their customers. 
In the USA, A2P messages must be sent using a short code rather than a standard long code.
Satellite phone networks.
All commercial satellite phone networks except ACeS and OptusSat support SMS. While early Iridium handsets only support incoming SMS, later models can also send messages. The price per message varies for different networks. Unlike some mobile phone networks, there is no extra charge for sending international SMS or to send one to a different satellite phone network. SMS can sometimes be sent from areas where the signal is too poor to make a voice call.
Satellite phone networks usually have web-based or email-based SMS portals where one can send free SMS to phones on that particular network.
Unreliability.
Unlike dedicated texting systems like the Simple Network Paging Protocol and Motorola's ReFLEX protocol, SMS message delivery is not guaranteed, and many implementations provide no mechanism through which a sender can determine whether an SMS message has been delivered in a timely manner. SMS messages are generally treated as lower-priority traffic than voice, and various studies have shown that around 1% to 5% of messages are lost entirely, even during normal operation conditions, and others may not be delivered until long after their relevance has passed. The use of SMS as an emergency notification service in particular has been questioned.
Vulnerabilities.
The Global Service for Mobile communications (GSM), with the greatest worldwide number of users, succumbs to several security vulnerabilities. In the GSM, only the airway traffic between the Mobile Station (MS) and the Base Transceiver Station (BTS) is optionally encrypted with a weak and broken stream cipher (A5/1 or A5/2). The authentication is unilateral and also vulnerable. There are also many other security vulnerabilities and shortcomings. Such vulnerabilities are inherent to SMS as one of the superior and well-tried services with a global availability in the GSM networks. SMS messaging has some extra security vulnerabilities due to its store-and-forward feature, and the problem of fake SMS that can be conducted via the Internet. When a user is roaming, SMS content passes through different networks, perhaps including the Internet, and is exposed to various vulnerabilities and attacks. Another concern arises when an adversary gets access to a phone and reads the previous unprotected messages.
In October 2005, researchers from Pennsylvania State University published an analysis of vulnerabilities in SMS-capable cellular networks. The researchers speculated that attackers might exploit the open functionality of these networks to disrupt them or cause them to fail, possibly on a nationwide scale.
SMS spoofing.
The GSM industry has identified a number of potential fraud attacks on mobile operators that can be delivered via abuse of SMS messaging services. The most serious threat is SMS Spoofing, which occurs when a fraudster manipulates address information in order to impersonate a user that has roamed onto a foreign network and is submitting messages to the home network. Frequently, these messages are addressed to destinations outside the home network—with the home SMSC essentially being "hijacked" to send messages into other networks.
The only sure way of detecting and blocking spoofed messages is to screen incoming mobile-originated messages to verify that the sender is a valid subscriber and that the message is coming from a valid and correct location. This can be implemented by adding an intelligent routing function to the network that can query originating subscriber details from the HLR before the message is submitted for delivery. This kind of intelligent routing function is beyond the capabilities of legacy messaging infrastructure.
Limitation.
In an effort to limit telemarketers who had taken to bombarding users with hordes of unsolicited messages India introduced new regulations in September 2011, including a cap of 3,000 SMS messages per subscriber per month, or an average of 100 per subscriber per day. Due to representations received from some of the service providers and consumers, TRAI (Telecom Regulatory Authority of India) has raised this limit to 200 SMS messages per SIM per day in case of prepaid services, and up to 6,000 SMS messages per SIM per month in case of postpaid services with effect from 1 November 2011. However, it was ruled unconstitutional by the Delhi high court, but there are some limitations.
Flash SMS.
A Flash SMS is a type of SMS that appears directly on the main screen without user interaction and is not automatically stored in the inbox. It can be useful in emergencies, such as a fire alarm or cases of confidentiality, as in delivering one-time passwords.
Silent SMS.
In Germany in 2010 almost half a million "silent SMS" messages were sent by the federal police, customs and the secret service "Verfassungsschutz" (offices for protection of the constitution). These silent messages, also known as "silent TMS", "stealth SMS" or "stealth ping", are used to locate a person and thus to create a complete movement profile. They do not show up on a display, nor trigger any acoustical signal when received. Their primary purpose was to deliver special services of the network operator to any cell phone. The mobile provider, often at the behest of the police, will capture data such as subscriber identification IMSI.

</doc>
<doc id="28208" url="https://en.wikipedia.org/wiki?curid=28208" title="Santa Monica, California">
Santa Monica, California

Santa Monica is a beachfront city in western Los Angeles County, California, United States. The city is named after the Christian saint, Monica. Situated on Santa Monica Bay, it is bordered on three sides by the city of Los Angeles – Pacific Palisades to the north, Brentwood on the northeast, Sawtelle on the east, Mar Vista on the southeast, and Venice on the south. Santa Monica is well known for its affluent single-family neighborhoods but also has many neighborhoods consisting primarily of condominiums and apartments. Over two-thirds of Santa Monica's residents are renters. The Census Bureau population for Santa Monica in 2010 was 89,736.
Partly because of its agreeable climate, Santa Monica had become a famed resort town by the early 20th century. The city has experienced a boom since the late 1980s through the revitalization of its downtown core, significant job growth and increased tourism. The Santa Monica Pier remains a popular and iconic destination.
History.
Santa Monica was long inhabited by the Tongva people. Santa Monica was called Kecheek in the Tongva language. The first non-indigenous group to set foot in the area was the party of explorer Gaspar de Portolà, who camped near the present day intersection of Barrington and Ohio Avenues on August 3, 1769. There are two different versions of the naming of the city. One says that it was named in honor of the feast day of Saint Monica (mother of Saint Augustine), but her feast day is actually May 4. Another version says that it was named by Juan Crespí on account of a pair of springs, the Kuruvungna Springs (Serra Springs), that were reminiscent of the tears that Saint Monica shed over her son's early impiety.
In Los Angeles, several battles were fought by the Californios. Following the Mexican–American War, Mexico signed the Treaty of Guadalupe Hidalgo, which gave Mexicans and Californios living in state certain unalienable rights. US government sovereignty in California began on February 2, 1848.
In the 1870s the Los Angeles and Independence Railroad, connected Santa Monica with Los Angeles, and a wharf out into the bay. The first town hall was a modest 1873 brick building, later a beer hall, and now part of the Santa Monica Hostel. It is Santa Monica's oldest extant structure. By 1885, the town's first hotel, was the Santa Monica Hotel.
Amusement piers became enormously popular in the first decades of the 20th century and the extensive Pacific Electric Railroad brought people to the city's beaches from across the Greater Los Angeles Area.
Around the start of the 20th century, a growing population of Asian Americans lived in or near Santa Monica and Venice. A Japanese fishing village was located near the Long Wharf while small numbers of Chinese lived or worked in both Santa Monica and Venice. The two ethnic minorities were often viewed differently by White Americans who were often well-disposed towards the Japanese but condescending towards the Chinese. The Japanese village fishermen were an integral economic part of the Santa Monica Bay community.
Donald Wills Douglas, Sr. built a plant in 1922 at Clover Field (Santa Monica Airport) for the Douglas Aircraft Company. In 1924, four Douglas-built planes took off from Clover Field to attempt the first aerial circumnavigation of the world. Two planes made it back, after having covered in 175 days, and were greeted on their return September 23, 1924, by a crowd of 200,000 (generously estimated). The Douglas Company (later McDonnell Douglas) kept facilities in the city until the 1960s.
The Great Depression hit Santa Monica deeply. One report gives citywide employment in 1933 of just 1,000. Hotels and office building owners went bankrupt. In the 1930s, corruption infected Santa Monica (along with neighboring Los Angeles).The federal Works Project Administration helped build several buildings in the city, most notably "City Hall". The main "Post Office" and "Barnum Hall" (Santa Monica High School auditorium) were also among several other WPA projects.
Douglas's business grew astronomically with the onset of World War II, employing as many as 44,000 people in 1943. To defend against air attack set designers from the Warner Brothers Studios prepared elaborate camouflage that disguised the factory and airfield. The RAND Corporation began as a project of the Douglas Company in 1945, and spun off into an independent think tank on May 14, 1948. RAND eventually acquired a 15-acre (61,000 m²) campus centrally located between the Civic Center and the pier entrance.
The completion of the Santa Monica Freeway in 1966 brought the promise of new prosperity, though at the cost of decimating the Pico neighborhood that had been a leading African American enclave on the Westside.
Beach volleyball is believed to have been developed by Duke Kahanamoku in Santa Monica during the 1920s.
Attractions and cultural resources.
The Santa Monica Looff Hippodrome (carousel) is a National Historic Landmark. It sits on the Santa Monica Pier, which was built in 1909. The La Monica Ballroom on the pier was once the largest ballroom in the US and the source for many New Year's Eve national network broadcasts.
The Santa Monica Civic Auditorium was an important music venue for several decades and hosted the Academy Awards in the 1960s. McCabe's Guitar Shop is still a leading acoustic performance space as well as retail outlet. Bergamot Station is a city-owned art gallery compound that includes the Santa Monica Museum of Art. The city is also home to the California Heritage Museum and the Angels Attic dollhouse and toy museum.
Santa Monica has three main shopping districts, Montana Avenue on the north side of the city, the Downtown District in the city's core, and Main Street on the south end of the city. Each of these districts has its own unique feel and personality. Montana Avenue is a stretch of luxury boutique stores, restaurants, and small offices that generally features more upscale shopping. The Main Street district offers an eclectic mix of clothing, restaurants, and other specialty retail.
The Downtown District is the home of the Third Street Promenade, a major outdoor pedestrian-only shopping district that stretches for three blocks between Wilshire Blvd. and Broadway (not the same Broadway in downtown and south Los Angeles).
Third Street is closed to vehicles for those three blocks to allow people to stroll, congregate, shop and enjoy street performers.
Santa Monica Place, featuring Bloomingdale's and Nordstrom in a three-level outdoor environment, is located at the south end of the Promenade. After a period of redevelopment, the mall reopened in the fall of 2010 as a modern shopping, entertainment and dining complex with more outdoor space.
Santa Monica hosts the annual Santa Monica Film Festival.
The oldest movie theater in the city is the Majestic. Also known as the Mayfair Theatre, the theater which opened in 1912 has been closed since the 1994 Northridge earthquake. The Aero Theater (now operated by the American Cinematheque) and Criterion Theater were built in the 1930s and still show movies. The Santa Monica Promenade alone supports more than a dozen movie screens.
Palisades Park stretches out along the crumbling bluffs overlooking the Pacific and is a favorite walking area to view the ocean. It includes a totem pole, camera obscura, artwork, benches, picnic areas, pétanque courts, and restrooms.
Tongva Park occupies 6 acres between Ocean Avenue and Main Street, just south of Colorado Avenue. The park includes an overlook, amphitheater, playground, garden, fountains, picnic areas, and restrooms.
The Santa Monica Stairs, a long, steep staircase that leads from north of San Vicente down into Santa Monica Canyon, is a popular spot for all-natural outdoor workouts. Some area residents have complained that the stairs have become too popular, and attract too many exercisers to the wealthy neighborhood of multimillion-dollar properties.
Natives and tourists alike have enjoyed the Santa Monica Rugby Club since 1972. The club has been very successful since its conception, most recently winning back-to-back national championships in 2005 and 2006. Santa Monica defeated the Boston Irish Wolfhounds 57-19 in the Division 1 final, convincingly claiming its second consecutive American title on June 4, 2006, in San Diego. They offer Men's, Women's and a thriving children's programs. The club recently joined the Rugby Super League.
Every fall the Santa Monica Chamber of Commerce hosts "The Taste of Santa Monica" on the Santa Monica Pier. Visitors can sample food and drinks from Santa Monica restaurants. Other annual events include the Business and Consumer Expo, Sustainable Quality Awards, Santa Monica Cares Health and Wellness Festival, and the State of the City. The swanky Shutters on the Beach Hotel offers a trip to the famous Santa Monica Farmers Market to select and influence the materials that will become that evening's special "Market Dinner."[http://www.santamonica.com/includes/docs/Entree-Nov-Dec.pdf]
Santa Monica is a mecca for skateboarding culture.
Santa Monica has two hospitals: Saint John's Health Center and Santa Monica-UCLA Medical Center. Its cemetery is Woodlawn Memorial.
Santa Monica has several newspapers and magazines, including the "Santa Monica Star", "Santa Monica Daily Press", the "Santa Monica Mirror", the "Santa Monica Observer", "Santa Monica Magazine", and the "Santa Monica Sun".
Geography.
The City of Santa Monica rests on a mostly flat slope that angles down towards Ocean Avenue and towards the south. High bluffs separate the north side of the city from the beaches. Santa Monica borders the L.A. neighborhoods of Pacific Palisades to the north and Venice to the south. To the west, Santa Monica has the 3-mile coastline fronting the Santa Monica Bay, and to the east of the city borders are the Los Angeles communities of West Los Angeles and Brentwood.
Climate.
Classified as a Subtropical Mediterranean climate (Köppen Csb), Santa Monica enjoys an average of 310 days of sunshine a year. It is located in USDA plant hardiness zone 11a. Because of its location, nestled on the vast and open Santa Monica Bay, morning fog is a common phenomenon in May, June and early July (caused by ocean temperature variations and currents). Like other inhabitants of the greater Los Angeles area, residents have a particular terminology for this phenomenon: the "May Gray" and the "June Gloom". Overcast skies are common during June mornings, but usually the strong sun burns the fog off by noon. In the late winter/early summer, daily fog is a phenomenon too. It happens suddenly and it may last some hours or past sunset time. Nonetheless, it will sometimes stay cloudy and cool all day during June, even as other parts of the Los Angeles area enjoy sunny skies and warmer temperatures. At times, the sun can be shining east of 20th Street, while the beach area is overcast. As a general rule, the beach temperature is from 5 to 10 degrees Fahrenheit (3 to 6 degrees Celsius) cooler than it is inland during summer days, and 5–10 degrees warmer during winter nights.
It is also in September that highest temperatures tend to be reached. It is winter, however, when the hot, dry winds of the Santa Anas are most common. In contrast, temperatures exceeding 10 degrees below average are rare.
The rainy season is from late October through late March. Winter storms usually approach from the northwest and pass quickly through the Southland. There is very little rain during the rest of the year. Yearly rainfall totals are unpredictable as rainy years are occasionally followed by droughts. There has never been any snow or frost, but there has been hail.
Santa Monica usually enjoys cool breezes blowing in from the ocean, which tend to keep the air fresh and clean. Therefore, smog is less of a problem for Santa Monica than elsewhere around Los Angeles. However, in the autumn months of September through November, the Santa Ana winds will sometimes blow from the east, bringing smoggy inland air to the beaches.
Environment.
Santa Monica is one of the most environmentally activist municipalities in the nation. The city first proposed its Sustainable City Plan in 1992 and in 1994, was one of the first cities in the nation to formally adopt a comprehensive sustainability plan, setting waste reduction and water conservation policies for both public and private sector through its Office of Sustainability and the Environment. Eighty-two percent of the city's public works vehicles now run on alternative fuels, including nearly 100% of the municipal bus system, making it among the largest such fleets in the country. Santa Monica fleet vehicles and Buses now source their natural gas from Redeem, a Southern California-based supplier of renewable and sustainable natural gas obtained from non-fracked methane biogas generated from organic landfill waste.
Santa Monica has adopted a Community Energy Independence Initiative, with a goal of achieving complete energy independence by 2020 (vs. California's already ambitious 33% renewables goal). In the last 15 years, greenhouse gas emissions have been cut city-wide by nearly 10% relative to 1990 levels, with further reductions being planned by the Office of Sustainability.
An urban runoff facility (SMURFF), the first of its kind in the US, catches and treats of water each week that would otherwise flow into the bay via storm-drains and sells it back to end-users within the city for reuse as gray-water, while bio-swales throughout the city allow rainwater to percolate into and replenish the groundwater supply. The groundwater supply in turn plays an important role in the city's Sustainable Water Master Plan, whereby Santa Monica has set a goal of attaining 100% water independence by 2020. The city has numerous programs designed to promote water conservation among residents, including a rebate of $1.50 per square foot for those who convert water intensive lawns to more local drought-tolerant gardens that require less water.
Santa Monica has also instituted a green building-code whereby merely constructing to code automatically renders a building equivalent to the US Green Building Council's LEED Silver standards. The city's Main Library, for example, is one of many LEED certified or LEED equivalent buildings in the city. It is built over a 200,000 gallon cistern that collects filtered storm water from the roof. The water is used for landscape irrigation.
Since 2009, Santa Monica has been developing the Zero Waste Strategic Operations Plan by which the city will set a goal of diverting at least 95% of all waste away from landfills, and toward recycling and composting, by 2030. The plan includes a food waste composting program, which diverts 3 million pounds of restaurant food waste away from landfills annually. Currently, 77% of all solid waste produced city-wide is diverted from landfills.
The city is also in the process of implementing a 5-year and 20 year Bike Action Plan with a goal of attaining 14 to 35% bicycle transportation mode share by 2030 through the installation of enhanced bicycle infrastructure throughout the city. Other environmentally focused initiatives include curbside recycling, curbside composting bins (in addition to trash, yard-waste, and recycle bins), farmers' markets, community gardens, garden-share, an urban forest initiative, a hazardous materials home-collection service, green business certification, and a municipal bus system which is currently being revamped to integrate with the soon-to-open Expo Line.
Demographics.
Santa Monica's population has grown from 417 in 1880 to 89,736 in 2010.
2010.
The 2010 United States Census reported that Santa Monica had a population of 89,736. The population density was 10,662.6 people per square mile (4,116.9/km²). The racial makeup of Santa Monica was 69,663 (77.6%) White (70.1% Non-Hispanic White), 3,526 (3.9%) African American, 338 (0.4%) Native American, 8,053 (9.0%) Asian, 124 (0.1%) Pacific Islander, 4,047 (4.5%) from other races, and 3,985 (4.4%) from two or more races. Hispanic or Latino of any race were 11,716 persons (13.1%).
The Census reported that 87,610 people (97.6% of the population) lived in households, 1,299 (1.4%) lived in non-institutionalized group quarters, and 827 (0.9%) were institutionalized.
There were 46,917 households, out of which 7,835 (16.7%) had children under the age of 18 living in them, 13,092 (27.9%) were opposite-sex married couples living together, 3,510 (7.5%) had a female householder with no husband present, 1,327 (2.8%) had a male householder with no wife present. There were 2,867 (6.1%) unmarried opposite-sex partnerships, and 416 (0.9%) same-sex married couples or partnerships. 22,716 households (48.4%) were made up of individuals and 5,551 (11.8%) had someone living alone who was 65 years of age or older. The average household size was 1.87. There were 17,929 families (38.2% of all households); the average family size was 2.79.
The population was spread out with 12,580 people (14.0%) under the age of 18, 6,442 people (7.2%) aged 18 to 24, 32,552 people (36.3%) aged 25 to 44, 24,746 people (27.6%) aged 45 to 64, and 13,416 people (15.0%) who were 65 years of age or older. The median age was 40.4 years. For every 100 females there were 93.2 males. For every 100 females age 18 and over, there were 91.2 males.
There were 50,912 housing units at an average density of 6,049.5 per square mile (2,335.7/km²), of which 13,315 (28.4%) were owner-occupied, and 33,602 (71.6%) were occupied by renters. The homeowner vacancy rate was 1.1%; the rental vacancy rate was 5.1%. 30,067 people (33.5% of the population) lived in owner-occupied housing units and 57,543 people (64.1%) lived in rental housing units.
According to the 2010 United States Census, Santa Monica had a median household income of $73,649, with 11.2% of the population living below the federal poverty line.
2000.
As of the census of 2000, there are 84,084 people, 44,497 households, and 16,775 families in the city. The population density is 10,178.7 inhabitants per square mile (3,930.4/km²). There are 47,863 housing units at an average density of 5,794.0 per square mile (2,237.3/km²). The racial makeup of the city is 78.29% White, 7.25% Asian, 3.78% African American, 0.47% Native American, 0.10% Pacific Islander, 5.97% from other races, and 4.13% from two or more races. 13.44% of the population are Hispanic or Latino of any race.
There are 44,497 households, out of which 15.8% have children under the age of 18, 27.5% are married couples living together, 7.5% have a female householder with no husband present, and 62.3% are non-families. 51.2% of all households are made up of individuals and 10.6% have someone living alone who is 65 years of age or older. The average household size is 1.83 and the average family size is 2.80.
The city of Santa Monica is consistently among the most educated cities in the United States, with 23.8 percent of all residents holding graduate degrees.
The population is diverse in age, with 14.6% under 18, 6.1% from 18 to 24, 40.1% from 25 to 44, 24.8% from 45 to 64, and 14.4% 65 years or older. The median age is 39 years. For every 100 females, there are 93.0 males. For every 100 females age 18 and over, there are 91.3 males.
According to a 2009 estimate, the median income for a household in the city is $71,095, and the median income for a family is $109,410. Males have a median income of $55,689 versus $42,948 for females. The per capita income for the city is $42,874. 10.4% of the population and 5.4% of families are below the poverty line. Out of the total population, 9.9% of those under the age of 18 and 10.2% of those 65 and older are living below the poverty line.
Education.
Elementary and secondary schools.
The Santa Monica-Malibu Unified School District provides public education at the elementary and secondary levels. In addition to the traditional model of early education school houses, SMASH (Santa Monica Alternative School House) is "a K-8 public school of choice with team teachers and multi-aged classrooms".
Elementary schools.
The district maintains eight public elementary schools in Santa Monica:
Middle schools.
The district maintains three public middle schools in Santa Monica: John Adams Middle School, Lincoln Middle School and SMASH.
High schools.
The district maintains two high schools in Santa Monica: Olympic High School and Santa Monica High School.
Private schools.
Private schools in the city include:
Miscellaneous education.
Asahi Gakuen, a weekend Japanese supplementary school system, operates its Santa Monica campus (サンタモニカ校･高等部 "Santamonika-kō kōtōbu") at Webster Middle in the Sawtelle neighborhood of Los Angeles. All high school classes in the Asahi Gakuen system are held at the Santa Monica campus. As of 1986 students take buses from as far away as Orange County to go to the high school classes of the Santa Monica campus.
Post-secondary.
Santa Monica College is a junior college originally founded in 1929. Many SMC graduates transfer to the University of California system. It occupies 35 acres (14 hectares) and enrolls 30,000 students annually. The Frederick S. Pardee RAND Graduate School, associated with the RAND Corporation, is the U.S.'s largest producer of public policy PhDs. The Art Institute of California – Los Angeles is also located in Santa Monica near the Santa Monica Airport.
Universities and colleges within a radius from Santa Monica include Santa Monica College, Antioch University Los Angeles, Loyola Marymount University, Mount St. Mary's College, Pepperdine University, California State University, Northridge, California State University, Los Angeles, UCLA, USC, West Los Angeles College, California Institute of Technology (Caltech), Occidental College (Oxy), Los Angeles City College, Los Angeles Southwest College, Los Angeles Valley College, and Emperor's College of Traditional Oriental Medicine.
Public library system.
The Santa Monica Public Library consists of a Main Library in the downtown area, plus four neighborhood branches: Fairview, Montana Avenue, Ocean Park, and Pico Boulevard.
Transportation.
Bicycles.
Santa Monica has a bike action plan and recently launched a Bicycle sharing system in November 2015. The city is traversed by the Marvin Braude Bike Trail. Santa Monica has received the Bicycle Friendly Community Award (Bronze in 2009, Silver in 2013) by the League of American Bicyclists. Local bicycle advocacy organizations include Santa Monica Spoke, a local chapter of the Los Angeles County Bicycle Coalition. Santa Monica is thought to be one of the leaders for bicycle infrastructure and programming in Los Angeles County.
In terms of number of bicycle accidents, Santa Monica ranks as one of the worst (#2) out of 102 California cities with population 50,000–100,000, a ranking that is consistent with the composite ranking for the city.
In 2007 and 2008, local police cracked down on Santa Monica Critical Mass rides that had become controversial, putting a damper on the tradition.
Motorized vehicles.
The Santa Monica Freeway (Interstate 10) begins in Santa Monica near the Pacific Ocean and heads east. The Santa Monica Freeway between Santa Monica and downtown Los Angeles has the distinction of being one of the busiest highways in all of North America. After traversing Los Angeles County, I-10 crosses seven more states, terminating at Jacksonville, Florida. In Santa Monica, there is a road sign designating this route as the Christopher Columbus Transcontinental Highway. State Route 2 (Santa Monica Boulevard) begins in Santa Monica, barely grazing State Route 1 at Lincoln Boulevard, and continues northeast across Los Angeles County, through the Angeles National Forest, crossing the San Gabriel Mountains as the Angeles Crest Highway, ending in Wrightwood. Santa Monica is also the western (Pacific) terminus of historic U.S. Route 66. Close to the eastern boundary of Santa Monica, Sepulveda Boulevard reaches from Long Beach at the south, to the northern end of the San Fernando Valley. Just east of Santa Monica is Interstate 405, the "San Diego Freeway", a major north-south route in Los Angeles County and Orange County, California.
The City of Santa Monica has purchased the first ZeroTruck all-electric medium-duty truck. The vehicle will be equipped with a Scelzi utility body, it is based on the Isuzu N series chassis, a UQM PowerPhase 100 advanced electric motor and is the only US built electric truck offered for sale in the United States in 2009.
Bus.
The city of Santa Monica runs its own bus service, the Big Blue Bus, which also serves much of West Los Angeles and the University of California, Los Angeles (UCLA). A Big Blue Bus was featured prominently in the action movie "Speed".
The city of Santa Monica is also served by the Los Angeles County Metropolitan Transportation Authority's bus lines. Metro also complements Big Blue service, as when Big Blue routes are not operational overnight, Metro buses make many Big Blue Bus stops, in addition to MTA stops.
Light rail.
Design and construction on the of the Expo Line from Culver City to Santa Monica started in September 2011 with service scheduled to begin in early 2016. Santa Monica Metro stations include 26th Street/Bergamot, 17th Street/Santa Monica College, and Downtown Santa Monica. Anticipated travel time between the downtown Santa Monica and the downtown Los Angeles termini is 46 minutes.
Historical aspects of the Expo line route are noteworthy. It uses the right-of-way for the Santa Monica Air Line that provided electric-powered freight and passenger service between Los Angeles and Santa Monica beginning in the 1920s. Service was discontinued in 1953 but diesel-powered freight deliveries to warehouses along the route continued until March 11, 1988. The abandonment of the line spurred concerns within the community and the entire right-of-way was purchased from Southern Pacific by Los Angeles Metropolitan Transportation Authority. The line was built in 1875 as the steam-powered Los Angeles and Independence Railroad to bring mining ore to ships in Santa Monica harbor and as a passenger excursion train to the beach.
Subway.
Since the mid-1980s, various proposals have been made to extend the Purple Line subway to Santa Monica under Wilshire Boulevard. There are no current plans to complete the "subway to the sea," an estimated $5 billion project.
Airport and ports.
The city owns and operates a general aviation airport, Santa Monica Airport, which has been the site of several important aviation achievements. Commercial flights are available for residents at Los Angeles International Airport, a few miles south of Santa Monica.
Like other cities in Los Angeles County, Santa Monica is dependent upon the Port of Long Beach and the Port of Los Angeles for international ship cargo. In the 1890s, Santa Monica was once in competition with Wilmington, California, and San Pedro for recognition as the "Port of Los Angeles" (see History of Santa Monica, California).
Emergency services.
Two major hospitals are within the Santa Monica city limits, UCLA Santa Monica Hospital and St. John's Hospital. There are four fire stations providing medical and fire response within the city staffed with 6 Paramedic Engines, 1 Truck company, 1 Hazardous Materials team and 1 Urban Search & Rescue team. Santa Monica Fire Department has its own Dispatch Center. Ambulance transportation is provided by AmeriCare Ambulance Services.
The Los Angeles County Department of Health Services operates the Simms/Mann Health and Wellness Center in Santa Monica. The Department's West Area Health Office is in the Simms/Mann Center.
Government.
Santa Monica is governed by the Santa Monica City Council, a Council-Manager governing body with seven members elected at-large. The current mayor is Tony Vazquez.
In the California State Legislature, Santa Monica is in , and in .
In the United States House of Representatives, Santa Monica is in .
Economy.
Santa Monica is home to the headquarters of many notable businesses, including Universal Music Group, Lionsgate Films, the RAND Corporation, Beachbody, and Macerich. Supermarine (now Atlantic Aviation) is at the Santa Monica Airport. National Public Radio member station KCRW is located at the Santa Monica College campus.
A number of game development studios are based in Santa Monica, making it a major location for the industry. These include:
Fatburger's headquarters are in Santa Monica. TOMS Shoes has its headquarters in Santa Monica.
Former Santa Monica businesses include Douglas Aircraft (now merged with Boeing), MySpace (now headquartered in Beverly Hills), and Metro-Goldwyn-Mayer. In December 1996, GeoCities was headquartered on the third floor of 1918 Main Street in Santa Monica.
Recently, Santa Monica has emerged as the center of the Los Angeles region called Silicon Beach, and serves as the home of hundreds of venture-capital funded startup companies.
Top employers.
According to the City's 2012–2013 Comprehensive Annual Financial Report, the top employers in the city are:
Crime.
In 2006, crime in Santa Monica affected 4.41% of the population, slightly lower than the national average crime rate that year of 4.48%.
The majority of this was property crime,
which affected 3.74% of Santa Monica's population in 2006;
this was higher than
the rates for Los Angeles County (2.76%)
and California (3.17%),
but lower than the national average (3.91%).
These per-capita crime rates are computed based on Santa Monica's full-time population of about 85,000.
However,
the Santa Monica Police Department has suggested the actual per-capita crime rate is much lower,
as tourists, workers, and beachgoers can increase the city's daytime population to between 250,000 and
450,000 people.
Violent crimes affected 0.67% of the population in Santa Monica in 2006,
in line with Los Angeles County (0.65%),
but higher than the averages for California (0.53%)
and the nation (0.55%).
Hate crime has typically been minimal in Santa Monica, with only one reported incident in 2007.
However, the city experienced a spike of anti-Islamic hate crime in 2001, following the attacks of September 11.
Hate crime levels returned to their minimal 2000 levels by 2002.
In 2006, Santa Monica voters passed "Measure Y" with a 65% majority,
which moved the issuance of citations for marijuana smoking to the bottom of the police priority list. A 2009 study by the Santa Monica Daily Press showed that since the law took effect in 2007, the Santa Monica Police had
"not issued any citations for offenses involving the adult, personal use of marijuana inside private residences."
In June 2011, the infamous Boston gangster Whitey Bulger was arrested in Santa Monica after being a fugitive for 16 years. He had been living in the area for 15 years.
A shooting in Santa Monica in 2013 left six (including the perpetrator) dead and five more injured.
Gang activity.
The Pico neighborhood of Santa Monica (south of the Santa Monica Freeway) experiences some gang activity. The city estimates that there are about 50 gang members based in Santa Monica, although some community organizers dispute this claim. Gang activity has been prevalent for decades in the Pico neighborhood.
In October 1998, alleged Culver City 13 gang member Omar Sevilla, 21, of Culver City was killed. A couple of hours after the shooting of Sevilla, German tourist Horst Fietze was killed. Several days later Juan Martin Campos, age 23, a Santa Monica city employee, was shot and killed. Police believe this was a retaliatory killing in response to the death of Omar Sevilla. Less than twenty-four hours later, Javier Cruz was wounded in a drive-by shooting outside his home on 17th and Michigan.
In 1999, there was a double homicide in the Westside Clothing store on Lincoln Boulevard. During the incident, Culver City gang members David "Puppet" Robles and Jesse "Psycho" Garcia entered the store masked and began opening fire, killing Anthony and Michael Juarez. They then ran outside to a getaway vehicle driven by a third Culver City gang member, who is now also in custody. The clothing store was believed to be a local hang out for Santa Monica gang members. The dead included two men from Northern California who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area. Police say the incident was in retaliation for a shooting committed by the Santa Monica 13 gang days before the Juarez brothers were gunned down.
Aside from the rivalry with the Culver City gang, gang members also feud with the Venice and West Los Angeles gangs. The main rivals in these regions include Venice 13, Graveyard Gangster Crips, and Venice Shoreline Crips gangs located in the Oakwood area of Venice, California.
Sport.
The men's and women's marathon ran through parts of Santa Monica during the 1984 Summer Olympics. The Santa Monica Track Club has many prominent track athletes, including many Olympic gold medalists. Santa Monica is also home to the Santa Monica Rugby Club, a semi-professional team that competes in the Pacific Rugby Premiership, the highest-level rugby union club competition in the United States.
In popular culture.
Film and television.
Hundreds of movies have been shot or set in part within the city of Santa Monica. One of the oldest exterior shots in Santa Monica is Buster Keaton's "Spite Marriage" (1929) which shows much of 2nd Street. The comedy "It's a Mad, Mad, Mad, Mad World" (1963) included several scenes shot in Santa Monica, including those along the California Incline, which led to the movie's treasure spot, "The Big W". The Sylvester Stallone film "Rocky III" (1982) shows Rocky Balboa and Apollo Creed training to fight Clubber Lang by running on the Santa Monica Beach, and Stallone's "Demolition Man" (1993) includes Santa Monica settings. Henry Jaglom's indie "Someone to Love" (1987), the last film in which Orson Welles appeared, takes place in Santa Monica's venerable Mayfair Theatre. "Heathers" (1989) used Santa Monica's John Adams Middle School for many exterior shots. "The Truth About Cats & Dogs" (1996) is set entirely in Santa Monica, particularly the Palisades Park area, and features a radio station that resembles KCRW at Santa Monica College. "17 Again" (2009) was shot at Samohi. Other films that show significant exterior shots of Santa Monica include "Fletch" (1985), "Species" (1995), "Get Shorty" (1995), and "Ocean's Eleven" (2001). Richard Rossi's biopic "Aimee Semple McPherson" opens and closes at the beach in Santa Monica. "Iron Man" features the Santa Monica pier and surrounding communities as Tony Stark tests his experimental flight suit.
The documentary "Dogtown and Z-Boys" (2001) and the related dramatic film "Lords of Dogtown" (2005) are both about the influential skateboarding culture of Santa Monica's Ocean Park neighborhood in the 1970s.
The Santa Monica Pier is shown in many films, including "They Shoot Horses, Don't They?" (1969), "The Sting" (1973), "Ruthless People" (1986), "Beverly Hills Cop III" (1994), "Clean Slate" (1994), "Forrest Gump" (1994), "The Net" (1995), "Love Stinks" (1999), "Cellular" (2004), ' (2006), "Iron Man" (2008) and ' (2009).
A number of television series have been set in Santa Monica, including "Baywatch", "Three's Company", "Pacific Blue", and "Private Practice". The Santa Monica pier is shown in the main theme of CBS series "". In "Buffy the Vampire Slayer", the main exterior set of the town of Sunnydale, including the infamous "sun sign", was located in Santa Monica in a lot on Olympic Boulevard.
The film "The Doors" (1991) and "Speed" (1994) featured vehicles from Santa Monica's Big Blue Bus line, relative to the eras depicted in the films.
The city of Santa Monica (and in particular the Santa Monica Airport) was featured in Roland Emmerich's disaster film "2012" (2009). A magnitude 10.9 earthquake destroys the airport and the surrounding area as a group of survivors escape in a personal plane. The Santa Monica Pier and the whole city sinks into the Pacific Ocean after the earthquake.
Literature.
Raymond Chandler's most famous character, private detective Philip Marlowe, frequently has a portion of his adventures in a place called "Bay City", which is modeled on depression-era Santa Monica. In Marlowe's world, Bay City is "a wide-open town", where gambling and other crimes thrive due to a massively corrupt and ineffective police force.
The setting on a certain portion of Mitch Albom's book, The Five People You Meet in Heaven, has similarities to the Pacific Pier located along the Santa Monica beach. In the book, it is named Ruby Pier. Mitch Albom even acknowledged the Pacific Pier for its cooperation.
The main character from Edgar Rice Burroughs' The Land That Time Forgot (novel) was a shipbuilder from Santa Monica.
In "Al Capone Does My Shirts", the Flanagans move to Alcatraz from Santa Monica.
Tennessee Williams lived (while working at MGM Studios) in a hotel on Ocean Avenue in the 1940s. At that location he wrote The Glass Menagerie. His short story titled "" was set near Santa Monica Beach, and mentions the clock visible in much of the city, high up on The Broadway Building, on Broadway near 2nd Street.
Also featured in Rick Riordains "Percy Jackson" Novels, specifically the Santa Monica pier.
Video games.
Santa Monica is featured in the video games
' (2003), ' (2004), "Grand Theft Auto San Andreas" (2004) as a fictional district - Santa Maria Beach, " Destroy All Humans! " (2004), "Tony Hawk's American Wasteland" (2005), "L.A. Rush" (2005), ' (2008), "Cars Race-O-Rama" (2009), "Grand Theft Auto V" (2013) as a fictional district – Del Perro, ' (2013) as a fictional U.S. military base – Fort Santa Monica, "The Crew" (2014), "Need for Speed" (2015)

</doc>
<doc id="28209" url="https://en.wikipedia.org/wiki?curid=28209" title="Shot put">
Shot put

The shot put (pronounced ) is a track and field event involving "throwing"/"putting" (throwing in a pushing motion) a heavy spherical object —the "shot"—as far as possible. The shot put competition for men has been a part of the modern Olympics since their revival in 1896, and women's competition began in 1948.
History.
Homer makes mention of competitions of rock throwing by soldiers during the Siege of Troy but there is no record of any dead weights being thrown in Greek competitions. The first evidence for stone- or weight-throwing events were in the Scottish Highlands, and date back to approximately the first century. In the 16th century King Henry VIII was noted for his prowess in court competitions of weight and hammer throwing. 
The first events resembling the modern shot put likely occurred in the Middle Ages when soldiers held competitions in which they hurled cannonballs. Shot put competitions were first recorded in early 19th century Scotland, and were a part of the British Amateur Championships beginning in 1866.
Competitors take their throw from inside a marked circle in diameter, with a stopboard about high at the front of the circle. The distance thrown is measured from the inside of the circumference of the circle to the nearest mark made in the ground by the falling shot, with distances rounded down to the nearest centimetre under IAAF and WMA rules.
Legal throws.
The following rules are adhered to for a legal throw:
The athlete may enter the ring wherever they chose.
Foul throws occur when an athlete:
At any time if the shot loses contact with the neck then it is technically an illegal throw.
Regulation Misconceptions.
The following are either obsolete or non-existent but commonly believed rules within professional competition:
Competition.
Shot put competitions have been held at the modern Summer Olympic Games since their inception in 1896, and it is also included as an event in the World Athletics Championships.
Each competition has a set number of rounds of throws. Typically there are three preliminary rounds to determine qualification for the final, and then three more rounds in the final. Each competitor is credited with their longest throw, regardless of whether it was achieved in the preliminary or final rounds. The competitor with the longest legal put is declared the winner.
In open competitions the men's shot weighs , and the women's shot weighs . Junior, school, and masters competitions often use different weights of shots, typically below the weights of those used in open competitions; the individual rules for each competition should be consulted in order to determine the correct weights to be used.
Putting styles.
Two putting styles are in current general use by shot put competitors: the "glide" and the "spin". With all putting styles, the goal is to release the shot with maximum forward velocity at an angle of approximately forty degrees.
Glide.
The origin of the glide dates to 1951, when Parry O'Brien from the United States invented a technique that involved the putter facing backwards, rotating 180 degrees across the circle, and then tossing the shot.
With this technique, a right-hand thrower would begin facing the rear of the circle, and then kick to the front with the left leg, while pushing off forcefully with the right. As the thrower crosses the circle, the hips twist toward the front, the left arm is swung out then pulled back tight, followed by the shoulders, and they then strike in a putting motion with their right arm. The key is to move quickly across the circle with as little air under the feet as possible, hence the name 'glide'.
Spin.
In 1972 Aleksandr Baryshnikov set his first USSR record using a new putting style, the spin ("круговой мах" in Russian), invented by his coach Viktor Alexeyev. The spin involves rotating like a discus thrower and using rotational momentum for power. In 1976 Baryshnikov went on to set a world record of with his spin style, and was the first shot putter to cross the 22 meter mark.
With this technique, a right-hand thrower faces the rear, and begins to spin on the ball of the left foot. The thrower comes around and faces the front of the circle and drives the right foot into the middle of the circle. Finally, the thrower reaches for the front of the circle with the left foot, twisting the hips and shoulders like in the glide, and puts the shot.
When the athlete executes the spin, the upper body is twisted hard to the right, so the imaginary lines created by the shoulders and hips are no longer parallel. This action builds up torque, and stretches the muscles, creating an involuntary elasticity in the muscles, providing extra power and momentum. When the athlete prepares to release, the left foot is firmly planted, causing the momentum and energy generated to be conserved, pushing the shot in an upward and outward direction.
Another purpose of the spin is to build up a high rotational speed, by swinging the right leg initially, then to bring all the limbs in tightly, similar to a figure skater bringing in their arms while spinning to increase their speed. Once this fast speed is achieved the shot is released, transferring the energy into the shot put.
Usage.
Currently, most top male shot putters use the spin. However the glide remains popular, especially among Olympic and World Champions and among women, since the technique leads to greater consistency compared to the rotational technique. Almost all throwers start by using the glide. Tomasz Majewski notes that although most athletes use the spin, he and some other top shot putters achieved success using this classic method (for example he became first to defend the Olympic title in 56 years).
The world record by a male putter of by Randy Barnes was completed with the spin technique, while the second-best all-time put of by Ulf Timmermann was completed with the glide technique.
Measuring which technique can provide the most potential is difficult, as many of the best throws recorded with each technique have been completed by athletes under doping suspicions, or with a record of drug violations. The decision to glide or spin may need to be decided on an individual basis, determined by the thrower's size and power. Short throwers may benefit from the spin and taller throwers may benefit from the glide, but many throwers do not follow this guideline.
Types of shots.
The shot put ball is made of different kinds of materials depending on its intended use. Materials used include sand, iron, cast iron, solid steel, stainless steel, brass, and synthetic materials like polyvinyl. Some metals are more dense than others making the size of the shot vary, for example, indoor shots are larger than outdoor shots, so different materials are used to make them. There are various size and weight standards for the implement that depend on the age and gender of the competitors as well as the national customs of the governing body.
World records.
The current world record holders are:
Continental records.
The current records held on each continent are:
Season's bests.
As of June 5, 2015

</doc>
<doc id="28211" url="https://en.wikipedia.org/wiki?curid=28211" title="Stan Kelly-Bootle">
Stan Kelly-Bootle

Stanley Bootle, known as Stan Kelly-Bootle (15 September 1929  – 16 April 2014) was a British author, singer-songwriter and computer scientist. He took his stage name Stan Kelly (he was not known as Stan Kelly-Bootle in folk music circles) from the Irish folk song "Kelly, the boy from Killane". His best-known song is the "Liverpool Lullaby" or "The Mucky Kid" which was recorded in 1965 on the "Three City Four" LP and sung by Marian McKenzie. It was also sung by the Ian Campbell Folk Group on the "Contemporary Campbells" LP. It was later a song which Judy Collins recorded in 1966 for her album, "In My Life". Cilla Black recorded it three years later as the B-side to her pop hit "Conversations". Kelly-Bootle achieved the first postgraduate degree in computer science in 1954.
Early life.
Stan Kelly-Bootle was born Stanley Bootle in Liverpool, Lancashire, on 15 September 1929 and grew up in the Wavertree area of the city. His parents were Arthur Bootle and Ada Gallagher.
Education.
Kelly-Bootle was schooled at the Liverpool Institute. He spent 1948–1950 doing his national service in the British Army, achieving the rank of Sgt. Instructor in RADAR. He attended Downing College, Cambridge, graduating with a first class degree in Numerical Analysis and Automatic Computing in 1954, the first postgraduate degree in computer science.
Folk singing career.
In 1950, Kelly-Bootle helped found the St. Lawrence Folk Song Society at Cambridge University. As a folk singer-songwriter, he performed under the name Stan Kelly. He wrote some of his own tunes and also wrote lyrics set to traditional tunes. In the course of his musical career, he made over 200 radio and television appearances, and released several recordings, as well as having his songs recorded by others.
Discography.
Solo releases include:
Other audio recordings include:
Computing career.
He started his computing career programming the pioneering EDSAC computer, designed and built at Cambridge University. He worked for IBM in the United States and the UK from 1955 to 1970. From 1970 to 1973, he worked as Manager for University Systems for Sperry-UNIVAC. He also lectured at the University of Warwick.
Writing career.
In 1973, Kelly-Bootle left Sperry-UNIVAC and became a freelance consultant, writer and programmer. He was known in the computer community for "The Devil's DP Dictionary" and its second edition, "The Computer Contradictionary" (1995), which he authored. These works are cynical lexicographies in the vein of Ambrose Bierce's "The Devil's Dictionary". Kelly-Bootle authored or co-authored several serious textbooks and tutorials on subjects such as the Motorola 68000 family of CPUs, programming languages including various C compilers, and the Unix operating system. He authored the "Devil's Advocate" column in "UNIX Review" from 1984-2000, and had columns in "Computer Language" ("Bit by Bit", 1989-1994), "OS/2 Magazine" ("End Notes", 1994–97) and "Software Development" ("Seamless Quanta", October 1995–May 1997). He contributed columns and articles to several other computer industry magazines, as well.
Kelly-Bootle's articles for magazines such as "ACM Queue", "AI/Expert", and "UNIX Review" contain stunning examples of word-play, criticism of silly marketing and usage (he refers often to the computer "laxicon") and commentary on the industry in general. He wrote an online monthly column posted on the Internet. While most of his writing was oriented towards the computer industry, he wrote a few books relating to his other interests, including 
Death.
Stan Kelly-Bootle died on 16 April 2014, aged 84, in hospital in Oswestry, Shropshire.

</doc>
<doc id="28212" url="https://en.wikipedia.org/wiki?curid=28212" title="Skewness">
Skewness

In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive or negative, or even undefined.
The qualitative interpretation of the skew is complicated. For a unimodal distribution, negative skew indicates that the "tail" on the left side of the probability density function is longer or fatter than the right side – it does not distinguish these shapes. Conversely, positive skew indicates that the tail on the right side is longer or fatter than the left side. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value indicates that the tails on both sides of the mean balance out, which is the case for a symmetric distribution, but is also true for an asymmetric distribution where the asymmetries even out, such as one tail being long but thin, and the other being short but fat. Further, in multimodal distributions and discrete distributions, skewness is also difficult to interpret. Importantly, the skewness does not determine the relationship of mean and median.
Introduction.
Consider the two distributions in the figure just below. Within each graph, the bars on the right side of the distribution taper differently than the bars on the left side. These tapering sides are called "tails", and they provide a visual means for determining which of the two kinds of skewness a distribution has:
Skewness in a data series may be observed not only graphically but by simple inspection of the values. For instance, consider the numeric sequence (49, 50, 51), whose values are evenly distributed around a central value of (50). We can transform this sequence into a negatively skewed distribution by adding a value far below the mean, as in e.g. (40, 49, 50, 51). Similarly, we can make the sequence positively skewed by adding a value far above the mean, as in e.g. (49, 50, 51, 60).
Relationship of mean and median.
The skewness is not strictly connected with the relationship between the mean and median: a distribution with negative skew can have the mean greater than or less than the median, and likewise for positive skew.
In the older notion of nonparametric skew, defined as formula_1 where "µ" is the mean, "ν" is the median, and "σ" is the standard deviation, the skewness is defined in terms of this relationship: positive/right nonparametric skew means the mean is greater than (to the right of) the median, while negative/left nonparametric skew means the mean is less than (to the left of) the median. However, the modern definition of skewness and the traditional nonparametric definition do not in general have the same sign: while they agree for some families of distributions, they differ in general, and conflating them is misleading.
If the distribution is symmetric, then the mean is equal to the median, and the distribution has zero skewness. If, in addition, the distribution is unimodal, then the mean = median = mode. This is the case of a coin toss or the series 1,2,3,4... Note, however, that the converse is not true in general, i.e. zero skewness does not imply that the mean is equal to the median.
Paul T. von Hippel points out: "Many textbooks, teach a rule of thumb stating that the mean is right of the median under right skew, and left of the median under left skew. This rule fails with surprising frequency. It can fail in multimodal distributions, or in distributions where one tail is long but the other is heavy. Most commonly, though, the rule fails in discrete distributions where the areas to the left and right of the median are not equal. Such distributions not only contradict the textbook relationship between mean, median, and skew, they also contradict the textbook interpretation of the median."
Definition.
Pearson's moment coefficient of skewness.
The skewness of a random variable "X" is the moment coefficient of skewness. It is sometimes referred to as Pearson's moment coefficient of skewness, not to be confused with Pearson's other skewness statistics (see below). It is the third standardized moment. It is denoted "γ"1 and defined as
where "μ"3 is the third central moment, "μ" is the mean, "σ" is the standard deviation, and "E" is the expectation operator. The last equality expresses skewness in terms of the ratio of the third cumulant "κ"3 and the 1.5th power of the second cumulant "κ"2. This is analogous to the definition of kurtosis as the fourth cumulant normalized by the square of the second cumulant.
The skewness is also sometimes denoted Skew["X"].
The formula expressing skewness in terms of the non-central moment E["X"3] can be expressed by expanding the previous formula,
Examples.
Skewness can be infinite, as when
or undefined, as when
In this latter example, the third cumulant is undefined. One can also have distributions such as
where both the second and third cumulants are infinite, so the skewness is again undefined.
Properties.
Starting from a standard cumulant expansion around a normal distribution, one can show that
If "Y" is the sum of "n" independent and identically distributed random variables, all with the distribution of "X", then the third cumulant of "Y" is "n" times that of "X" and the second cumulant of "Y" is "n" times that of "X", so formula_7. This shows that the skewness of the sum is smaller, as it approaches a Gaussian distribution in accordance with the central limit theorem. Note that the assumption that the variables be independent for the above formula is very important because it is possible even for the sum of two Gaussian variables to have a skewed distribution (see this example).
Sample skewness.
For a sample of "n" values, a natural method of moments estimator of the population skewness is
where formula_9 is the sample mean, "s" is the sample standard deviation, and the numerator "m"3 is the sample third central moment.
Another common definition of the "sample skewness" is
where formula_11 is the unique symmetric unbiased estimator of the third cumulant and formula_12 is the symmetric unbiased estimator of the second cumulant (i.e. the variance).
In general, the ratios formula_13 and formula_14 are both biased estimators of the population skewness formula_15; their expected values can even have the opposite sign from the true skewness. (For instance, a mixed distribution consisting of very thin Gaussians centred at −99, 0.5, and 2 with weights 0.01, 0.66, and 0.33 has a skewness of about −9.77, but in a sample of 3, formula_14 has an expected value of about 0.32, since usually all three samples are in the positive-valued part of the distribution, which is skewed the other way.) Nevertheless, formula_13 and formula_14 each have obviously the correct expected value of zero for any symmetric distribution with a finite third moment, including a normal distribution.
The variance of the skewness of a random sample of size "n" from a normal distribution is
An approximate alternative is 6/"n", but this is inaccurate for small samples.
In normal samples, formula_13 has the smaller variance of the two estimators, with
where "m"2 in the denominator is the (biased) sample second central moment.
The adjusted Fisher–Pearson standardized moment coefficient formula_22 is the version found in Excel and several statistical packages including Minitab, SAS and SPSS.
Applications.
Skewness has benefits in many areas. Many models assume normal distribution; i.e., data are symmetric about the mean. The normal distribution has a skewness of zero. But in reality, data points may not be perfectly symmetric. So, an understanding of the skewness of the dataset indicates whether deviations from the mean are going to be positive or negative.
D'Agostino's K-squared test is a goodness-of-fit normality test based on sample skewness and sample kurtosis.
Other measures of skewness.
Other measures of skewness have been used, including simpler calculations suggested by Karl Pearson (not to be confused with Pearson's moment coefficient of skewness, see above). These other measures are:
Pearson's first skewness coefficient (mode skewness).
The Pearson mode skewness, or first skewness coefficient, is defined as
Pearson's second skewness coefficient (median skewness).
The Pearson median skewness, or second skewness coefficient, is defined as
The latter is a simple multiple of the nonparametric skew.
Quantile-based measures.
Bowley's measure of skewness (from 1901), also called Yule's coefficient (from 1912) is defined as: 
When writing it as formula_24, it is easier to see that the nominator is the average of the upper and lower quartiles (a measure of location) minus the median while the denominator is (Q3-Q1)/2 which (for symmetric distributions) is the MAD measure of dispersion.
Other names for this measure are Galton's measure of skewness, the Yule–Kendall index and the quartile skewness ,
A more general formulation of a skewness function was described by Groeneveld, R. A. and Meeden, G. (1984):
where "F" is the cumulative distribution function. This leads to a corresponding overall measure of skewness defined as the supremum of this over the range 1/2 ≤ "u" < 1. Another measure can be obtained by integrating the numerator and denominator of this expression. The function "γ"("u") satisfies −1 ≤ "γ"("u") ≤ 1 and is well defined without requiring the existence of any moments of the distribution.
Bowley's measure of skewness is γ("u") evaluated at "u" = 3/4. Kelley's measure of skewness uses "u" = 0.1.
Groeneveld & Meeden’s coefficient.
Groeneveld & Meeden have suggested, as an alternative measure of skewness,
where "μ" is the mean, "ν" is the median, |…| is the absolute value, and "E"() is the expectation operator. This is closely related in form to Pearson's second skewness coefficient.
L-moments.
Use of L-moments in place of moments provides a measure of skewness known as the L-skewness.
Distance skewness.
A value of skewness equal to zero does not imply that the probability distribution is symmetric. Thus there is a need for another measure of asymmetry that has this property: such a measure was introduced in 2000. It is called distance skewness and denoted by dSkew. If "X" is a random variable taking values in the "d"-dimensional Euclidean space, "X" has finite expectation, "X" is an independent identically distributed copy of "X", and formula_27 denotes the norm in the Euclidean space, then a simple "measure of asymmetry" is
and dSkew("X") := 0 for "X" = 0 (with probability 1). Distance skewness is always between 0 and 1, equals 0 if and only if "X" is diagonally symmetric ("X" and −"X" have the same probability distribution) and equals 1 if and only if X is a nonzero constant with probability one. Thus there is a simple consistent statistical test of diagonal symmetry based on the sample distance skewness:
Medcouple.
The medcouple is a scale-invariant robust measure of skewness, with a breakdown point of 25%. It is the median of the values of the kernel function
taken over all couples formula_31 such that formula_32, where formula_33 is the median of the sample formula_34.

</doc>
<doc id="28215" url="https://en.wikipedia.org/wiki?curid=28215" title="Saint Columba (disambiguation)">
Saint Columba (disambiguation)

Saint Columba may refer to:

</doc>
<doc id="28217" url="https://en.wikipedia.org/wiki?curid=28217" title="Serial Experiments Lain">
Serial Experiments Lain

Plot.
The series focuses on Lain Iwakura, an adolescent girl living in suburban Japan, and her introduction to the Wired, a global communications network similar to the Internet. Lain lives with her middle-class family, which consists of her inexpressive older sister Mika, her emotionally distant mother, and her computer-obsessed father. Lain herself is somewhat awkward, introverted, and socially isolated from most of her school peers, but the status-quo of her life becomes upturned by a series of bizarre incidents which start to take place after she learns that girls from her school have received an e-mail from Chisa Yomoda, a schoolmate who had committed suicide. When Lain receives the message herself at home, Chisa tells her that she is not dead, but has merely "abandoned her physical body and flesh", and is alive deep within the virtual reality-world of the Wired itself where she has found the almighty and divine God. From this point, Lain is caught up in a series of cryptic and surreal events that see her delving deeper into the mystery of the network in a narrative that explores themes of consciousness, perception, and the nature of reality.
The 'Wired' is a virtual reality-world that contains and supports the very sum of "all" human communication and networks, created with the telegraph, televisions, and telephone services, and expanded with the Internet, cyberspace, and subsequent networks. The series assumes that the Wired could be linked to a system that enables unconscious communication between people and machines without physical interface. The storyline introduces such a system with the Schumann resonance, a property of the Earth's magnetic field that theoretically allows for unhindered long distance communications. If such a link were created, the network would become equivalent to Reality as the general consensus of all perceptions and knowledge. The increasingly thin invisible line between what is real and what is virtual/digital begins to slowly shatter.
Masami Eiri is introduced as the project director on Protocol Seven (the next generation internet protocol in the series' time-frame) for major computer company Tachibana General Laboratories. He had secretly included code of his very own creation to give himself absolute control of the Wired through the wireless system described above. He then "uploaded" his own brain, conscience, consciousness, memory, feelings, emotions; his very self into the Wired and "died" a few days after, leaving only his physical, living body behind. These details are unveiled around the middle of the series, but this is the point where the story of "Serial Experiments Lain" begins. Masami later explains that Lain is the artifact by which the wall between the virtual and material worlds is to fall, and that he needs her to get to the Wired and "abandon the flesh", as he did, to achieve his plan. The series sees him trying to convince her through interventions, using the promise of unconditional love, romantic seduction and charm, and even, when all else fails, threats and force.
In the meantime, the anime follows a complex game of hide-and-seek between the "Knights of the Eastern Calculus," hackers whom Masami claims are "believers that enable him to be a God in the Wired", and Tachibana General Laboratories, who try to regain control of Protocol Seven. In the end, the viewer sees Lain realizing, after much introspection, that she has absolute control over everyone's mind and over reality itself. Her dialogue with different versions of herself shows how she feels shunned from the material world, and how she is afraid to live in the Wired, where she has the possibilities and responsibilities of an almighty goddess. The last scenes feature her erasing everything connected to herself from everyone's memories. She is last seen, unchanged, encountering her oldest and closest friend Arisu once again, who is now married. Lain promises herself that she and Arisu will surely meet again anytime as Lain can literally go and be anywhere she desires between both worlds.
Production.
"Serial Experiments Lain" was conceived, as a series, to be original to the point of it being considered "an enormous risk" by its producer Yasuyuki Ueda.
Producer Ueda had to answer repeated queries about a statement made in an Animerica interview. The controversial statement said "Lain" was ""a sort of cultural war against American culture and the American sense of values we adopted after World War II"".
He later explained in numerous interviews that he created "Lain" with a set of values he took as distinctly Japanese; he hoped Americans would not understand the series as the Japanese would. This would lead to a "war of ideas" over the meaning of the anime, hopefully culminating in new communication between the two cultures. When he discovered that the American audience held the same views on the series as the Japanese, he was disappointed.
The "Lain" franchise was originally conceived to connect across forms of media (anime, video games, manga). Producer Yasuyuki Ueda said in an interview, "the approach I took for this project was to communicate the essence of the work by the total sum of many media products." The scenario for the video game was written first, and the video game was produced at the same time as the anime series, though the series was released first. A doujinshi named "The Nightmare of Fabrication" was produced by Yoshitoshi ABe and released in Japanese in the artbook "Omnipresence in The Wired". Ueda and Konaka declared in an interview that the idea of a multimedia project was not unusual in Japan, as opposed to the contents of "Lain", and the way they are exposed.
Writing.
The authors were asked in interviews if they had been influenced by "Neon Genesis Evangelion", in the themes and graphic design. This was strictly denied by writer Chiaki J. Konaka in an interview, arguing that he had not seen "Evangelion" until he finished the fourth episode of "Lain". Being primarily a horror movies writer, his stated influences are Godard (especially for using typography on screen), "The Exorcist", "Hell House", and Dan Curtis's "House of Dark Shadows". Alice's name, like the names of her two friends Julie and Reika, came from a previous production from Konaka, "Alice in Cyberland", which in turn was largely influenced by "Alice in Wonderland". As the series developed, Konaka was "surprised" by how close Alice's character became to the original "Wonderland" character.
Vannevar Bush (and Memex), John C. Lilly, Timothy Leary and his 8-Circuit Model of Consciousness, Ted Nelson and Project Xanadu are cited as precursors to the Wired. Douglas Rushkoff and his book "Cyberia" were originally to be cited as such, and in "Lain" Cyberia became the name of a nightclub populated with hackers and techno-punk teenagers. Likewise, the series' Deus ex machina lies in the conjunction of the Schumann resonance and Jung's collective unconscious (the authors chose this term over Kabbalah and Akashic Record). Majestic 12 and the Roswell UFO incident are used as examples of how a hoax might still have an impact on history, even after having been exposed as such, by creating sub-cultures. This links again to Vannevar Bush, the alleged "brains" of MJ12. Two of the literary references in "Lain" are quoted through Lain's father: he first logs onto a website with the password "Think Bule Count One Tow" ("Think Blue, Count Two" is an Instrumentality of Man story featuring virtual persons projected as real ones in people's minds); and his saying that "madeleines would be good with the tea" in the last episode makes "Lain" "perhaps the only cartoon to allude to Proust".
Character design.
Yoshitoshi ABe confesses to have never read manga as a child, as it was "off-limits" in his household. His major influences are "nature and everything around him". Specifically speaking about "Lain" character, ABe was inspired by Kenji Tsuruta, Akihiro Yamada, Range Murata, and Yukinobu Hoshino. In a broader view, he has been influenced in his style and technique by Japanese artists Chinai-san and Tabuchi-san.
The character design of Lain was not ABe's sole responsibility. Her distinctive left forelock for instance was a demand from Yasuyuki Ueda. The goal was to produce asymmetry to reflect Lain's unstable and disconcerting nature. It was designed as a mystical symbol, as it is supposed to prevent voices and spirits from being heard by the left ear. The bear pajamas she wears were a demand from character animation director Takahiro Kishida. Though bears are a trademark of the Konaka brothers, Chiaki Konaka first opposed the idea. Director Nakamura then explained how the bear motif could be used as a shield for confrontations with her family. It is a key element of the design of the shy "real world" Lain ("see "mental illness" under themes"). When she first goes to the Cyberia night club, she wears a bear hat for similar reasons. The pajamas were finally considered as possible fan-service by Konaka, in the way they enhance Lain's nymph aspect.
ABe's original design was generally more complicated than what finally appeared on screen. As an example, the X-shaped hairclip was to be an interlocking pattern of gold links. The links would open with a snap, or rotate around an axis until the moment the " X " became a " = ". This was not used as there is no scene where Lain takes her hairclip off.
Themes.
"Serial Experiments Lain" is not a conventionally linear story, but "an alternative anime, with modern themes and realization". Themes range from theological to psychological and are dealt with in a number of ways: from classical dialogue to image-only introspection, passing by direct interrogation of imaginary characters.
Communication, in its wider sense, is one of the main themes of the series, not only as opposed to loneliness, but also as a subject in itself. Writer Konaka said he wanted to directly "communicate human feelings". Director Nakamura wanted to show the audience — and particularly viewers between 14 and 15 — "the multidimensional wavelength of the existential self: the relationship between self and the world".
Loneliness, if only as representing a lack of communication, is recurrent through "Lain". Lain herself (according to Anime Jump) is "almost painfully introverted with no friends to speak of at school, a snotty, condescending sister, a strangely apathetic mother, and a father who seems to want to care but is just too damn busy to give her much of his time". Friendships turn on the first rumor; and the only insert song of the series is named "Kodoku no shigunaru", literally "signal of loneliness".
Mental illness, especially dissociative identity disorder, is a significant theme in "Lain": the main character is constantly confronted with alter-egos, to the point where writer Chiaki Konaka and Lain's voice actress Kaori Shimizu had to agree on subdividing the character's dialogues between three different orthographs. The three names designate distinct "versions" of Lain: the real-world, "childish" Lain has a shy attitude and bear pajamas. The "advanced" Lain, her Wired personality, is bold and questioning. Finally, the "evil" Lain is sly and devious, and does everything she can to harm Lain or the ones close to her. As a writing convention, the authors spelled their respective names in kanji, katakana, and roman characters (see picture).
Reality never has the pretense of objectivity in "Lain". Acceptations of the term are battling throughout the series, such as the "natural" reality, defined through normal dialogue between individuals; the material reality; and the tyrannic reality, enforced by one person onto the minds of others. A key debate to all interpretations of the series is to decide whether matter flows from thought, or the opposite. The production staff carefully avoided "the so-called God's Eye Viewpoint" to make clear the "limited field of vision" of the world of "Lain".
Theology plays its part in the development of the story too. "Lain" has been viewed as a questioning of the possibility of an infinite spirit in a finite body. From self-realization as a goddess to deicide, religion (the title of a layer) is an inherent part of "Lain" background.
Apple computers.
"Lain" contains extensive references to Apple computers, as the brand was used at the time by most of the creative staff, such as writers, producers, and the graphical team. As an example, the title at the beginning of each episode is announced by the Apple Computer Speech synthesis program PlainTalk, using the voice ""Whisper"", e.g. codice_1. Tachibana Industries, the company that creates the NAVI computers, is a reference to Apple computers: "tachibana" means "Mandarin orange" in Japanese. NAVI is the abbreviation of Knowledge Navigator, and the HandiNAVI is based on the Apple Newton, one of the world's first PDAs. The NAVIs are seen to run "Copland OS Enterprise" (this reference to Copland was an initiative of Konaka, a declared Apple fan), and Lain's and Alice's NAVIs closely resembles the Twentieth Anniversary Macintosh and the iMac respectively. The HandiNAVI programming language, as seen on the seventh episode, is a dialect of Lisp. Notice that the Newton also used a Lisp dialect (NewtonScript). The program being typed by Lain can be found in the CMU AI repository, it is a simple implementation of Conway's Game of Life in Common Lisp.
During a series of disconnected images, an iMac and the Think Different advertising slogan appears for a short time, while the "Whisper" voice says it. This was an unsolicited insertion from the graphic team, also Mac-enthusiasts. Other subtle allusions can be found: "Close the world, Open the nExt" is the slogan for the "Serial Experiments Lain" video game. NeXT was the company that produced NeXTSTEP, which later evolved into Mac OS X after Apple bought NeXT. Another example is "To Be Continued." at the end of episodes 1–12, with a blue "B" and a red "e" on "Be": "this" "Be" is the original logo of Be Inc., a company founded by ex-Apple employees and NeXT's main competitor in its time.
Media.
Anime.
"Serial Experiments Lain" was first aired on TV Tokyo on July 6, 1998 and concluded on September 28, 1998 with the thirteenth and final episode. The series consists of 13 episodes (referred to in the series as "Layers") of 24 minutes each, except for the sixth episode, "Kids" (23 minutes 14 seconds). In Japan, the episodes were released in LD, VHS, and DVD with a total of five volumes. A DVD compilation named ""Serial Experiments Lain DVD-BOX Яesurrection"" was released along with a promo DVD called ""LPR-309"" in 2000. As this box set is now discontinued, a rerelease was made in 2005 called ""Serial Experiments Lain TV-BOX"". A 4-volume DVD box set was released in the US by Pioneer/Geneon. A Blu-ray release of the anime was made on December 2009 called ""Serial Experiments Lain Blu-ray Box | RESTORE"". The anime series returned to US television on October 15, 2012 on the Funimation Channel.
The series' opening theme, "Duvet", was written and performed by Jasmine Rodgers and the British band Bôa. The ending theme, , was written and composed by Reichi Nakaido.
The anime series was licensed in North America by Pioneer Entertainment (later Geneon USA) on VHS, DVD and LaserDisc in 1999. However, the company closed its USA division in December 2007 and the series went out-of-print as a result. However, at Anime Expo 2010, North American distributor Funimation announced that it had obtained the license to the series and re-released it in 2012. It was also released in Singapore by Odex.
Soundtracks.
The first original soundtrack, "Serial Experiments Lain Soundtrack", features music by Reichi Nakaido: the ending theme and part of the television series' score. The series' opening theme, "Duvet", was written and performed in English by the British rock band Bôa. The second, "Serial Experiments Lain Soundtrack: Cyberia Mix", features electronica songs inspired by the television series, including a remix of the opening theme 'Duvet'. The third, "lain BOOTLEG", consists of two CDs with more than forty-five tracks, containing ambient music from the series. One of the CDs is a mixed-mode data and audio disk, containing a clock program and a game. It was released by Pioneer Records. Because the word "bootleg" appears in its title, it is easily confused with the Sonmay counterfeit edition of itself, which contains one CD of forty-five tracks, some of which are shorter than on the original.
Video game.
On November 26, 1998, Pioneer LDC released a video game with the same name as the anime for the PlayStation. It was designed by Konaka and Yasuyuki, and made to be a "network simulator" in which the player would navigate to explore Lain's story. The creators themselves did not call it a game, but "Psycho-Stretch-Ware", and it has been described as being a kind of graphic novel: the gameplay is limited to unlocking pieces of information, and then reading/viewing/listening to them, with little or no puzzle needed to unlock. Lain distances itself even more from classical games by the random order in which information is collected. The aim of the authors was to let the player get the feeling that there are myriads of informations that they would have to sort through, and that they would have to do with less than what exists to understand. As with the anime, the creative team's main goal was to let the player "feel" Lain, and "to understand her problems, and to love her". A guidebook to the game called "Serial Experiments Lain Official Guide" (ISBN 4-07-310083-1) was released the same month by MediaWorks.
Reception.
"Serial Experiments Lain" was first broadcast in Tokyo at 1:15 a.m. JST. The word "weird" appears almost systematically in English language reviews of the series, or the alternatives "bizarre", and "atypical", due mostly to the freedoms taken with the animation and its unusual science fiction themes, and due to its philosophical and psychological context. Critics responded positively to these thematic and stylistic characteristics, and it was awarded an Excellence Prize by the 1998 Japan Media Arts Festival for "its willingness to question the meaning of contemporary life" and the "extraordinarily philosophical and deep questions" it asks.
According to Christian Nutt from "Newtype USA", the main attraction to the series is its keen view on "the interlocking problems of identity and technology". Nutt saluted ABe's "crisp, clean character design" and the "perfect soundtrack" in his 2005 review of series, saying that ""Serial Experiments Lain" might not yet be considered a true classic, but it's a fascinating evolutionary leap that helped change the future of anime." "Anime Jump" gave it 4.5/5, and Anime on DVD gave it A+ on all criteria for volume 1 and 2, and a mix of A and A+ for volume 3 and 4.
"Lain" was subject to commentary in the literary and academic worlds. The "Asian Horror Encyclopedia" calls it "an outstanding psycho-horror anime about the psychic and spiritual influence of the Internet". It notes that the red spots present in all the shadows look like blood pools (see picture). It notes the death of a girl in a train accident is "a source of much ghost lore in the twentieth century", more so in Tokyo.
The "Anime Essentials" anthology by Gilles Poitras describes it as a "complex and somehow existential" anime that "pushed the envelope" of anime diversity in the 1990s, alongside the much better known "Neon Genesis Evangelion" and "Cowboy Bebop". Professor Susan J. Napier, in her 2003 reading to the American Philosophical Society called "The Problem of Existence in Japanese Animation" (published 2005), compared "Serial Experiments Lain" to "Ghost in the Shell" and Hayao Miyazaki's "Spirited Away". According to her, the main characters of the two other works cross barriers; they can cross back to our world, but Lain cannot. Napier asks whether there is something to which Lain should return, "between an empty 'real' and a dark 'virtual'". Mike Toole of Anime News Network named "Serial Experiments Lain" as one of the most important anime of the 1990s.
Unlike the anime, the video game drew little attention from the public. Criticized for its (lack of) gameplay, as well as for its "clunky interface", interminable dialogues, absence of music and very long loading times, it was nonetheless remarked for its (at the time) remarkable CG graphics, and its beautiful backgrounds.
Despite the positive feedback the television series had received, Anime Academy gave this series a 75%, partly due to the "lifeless" setting it had. Michael Poirier of EX magazine stated that the last three episodes fail to resolve the questions in other DVD volumes. Justin Sevakis of Anime News Network noted that the English dub was decent, but that the show relied so little on dialogue that it hardly mattered.
Cultural references.
Lain's artificial intelligent personal assistant "NAVI" was the basis of the Siri-like guide in the Tokyo Realtime: Akihabara audio tour by White Rabbit Press.

</doc>
<doc id="28219" url="https://en.wikipedia.org/wiki?curid=28219" title="Spontaneous emission">
Spontaneous emission

Spontaneous emission is the process by which a quantum system such as an atom, molecule, nanocrystal or nucleus in an excited state undergoes a transition to a state with a lower energy (e.g., the ground state) and emits quanta of energy. Light or luminescence from an atom is a fundamental process that plays an essential role in many phenomena in nature and forms the basis of many applications, such as fluorescent tubes, older television screens (cathode ray tubes), plasma display panels, lasers, and light emitting diodes. Lasers start by spontaneous emission, and then normal continuous operation works by stimulated emission.
Introduction.
If a light source ('the atom') is in an excited state with energy formula_1, it may spontaneously decay to a lower lying level (e.g., the ground state) with energy formula_2, releasing the difference in energy between the two states as a photon. The photon will have angular frequency formula_3 and an energy energy formula_4:
where formula_6 is the reduced Planck constant. Note: formula_7, where formula_8 is the Planck constant and formula_9 is the linear frequency. The phase of the photon in spontaneous emission is random as is the direction in which the photon propagates. This is not true for stimulated emission. An energy level diagram illustrating the process of spontaneous emission is shown below:
If the number of light sources in the excited state at time formula_10 is given by formula_11, the rate at which formula_12 decays is:
where formula_14 is the rate of spontaneous emission. In the rate-equation formula_14 is a proportionality constant for this particular transition in this particular light source. The constant is referred to as the "Einstein A coefficient", and has units formula_16. 
The above equation can be solved to give:
where formula_18 is the initial number of light sources in the excited state, formula_10 is the time and formula_20 is the radiative decay rate of the transition. The number of excited states formula_12 thus decays exponentially with time, similar to radioactive decay. After one lifetime, the number of excited states decays to 36.8% of its original value (formula_22-time). The radiative decay rate formula_20 is inversely proportional to the lifetime formula_24:
Theory.
Spontaneous transitions were not explainable within the framework of the Schroedinger equation, in which the electronic energy levels were quantized, but the electromagnetic field was not. Given that the eigenstates of an atom are properly diagonalized, the overlap of the wavefunctions between the excited state and the ground state of the atom is zero. Thus, in the absence of a quantized electromagnetic field, the excited state atom can not decay to the ground state. In order to explain spontaneous transitions, quantum mechanics must be extended to a quantum field theory, wherein the electromagnetic field is quantized at every point in space. The quantum field theory of electrons and electromagnetic fields is known as quantum electrodynamics.
In quantum electrodynamics (or QED), the electromagnetic field has a ground state, the QED vacuum, which can mix with the excited stationary states of the atom (for more information, see Ref. ). As a result of this interaction, the "stationary state" of the atom is no longer a true eigenstate of the combined system of the atom plus electromagnetic field. In particular, the electron transition from the excited state to the electronic ground state mixes with the transition of the electromagnetic field from the ground state to an excited state, a field state with one photon in it. Spontaneous emission in free space depends upon vacuum fluctuations to get started.
Although there is only one electronic transition from the excited state to ground state, there are many ways in which the electromagnetic field may go from the ground state to a one-photon state. That is, the electromagnetic field has infinitely more degrees of freedom, corresponding to the different directions in which the photon can be emitted. Equivalently, one might say that the phase space offered by the electromagnetic field is infinitely larger than that offered by the atom. This infinite degrees of freedom for the emission of the photon results in the apparent irreversible decay, i.e., spontaneous emission. 
In presence of the electromagnetic vacuum modes, the combined atom-vacuum system is explained by the superposition of the wavefunctions of the excited state atom with no photon and the ground state atom with a single emitted photon:
where formula_27 and formula_28 are the atomic excited state-electromagnetic vacuum wavefunction and its probability amplitude, formula_29 and formula_30 are the ground state atom with a single photon (of mode formula_31) wavefunction and its probability amplitude, formula_32 is the atomic transition frequency, and formula_33 is the frequency of the photon. The sum is over formula_34 and formula_35, which are the wavenumber and polarization of the emitted photon, respectively. As mentioned above, the emitted photon has a chance to be emitted with different wavenumbers and polarizations, and the resulting wavefunction is a superposition of these possibilities. To calculate the probability of the atom at the ground state (formula_36), one needs to solve the time evolution of the wavefunction with an appropriate Hamiltonian (see external link 1 for the detailed calculations). To solve for the transition amplitude, one needs to average over (integrate over) all the vacuum modes, since one must consider the probabilities that the emitted photon occupies various parts of phase space equally. The "spontaneously" emitted photon has infinite different modes to propagate into, thus the probability of the atom re-absorbing the photon and returning to the original state is negligible, making the atomic decay practically irreversible. Such irreversible time evolution of the atom-vacuum system is responsible for the apparent spontaneous decay of an excited atom. If one were to keep track of all the vacuum modes, the combined atom-vacuum system would undergo unitary time evolution, making the decay process reversible. Cavity quantum electrodynamics is one such system where the vacuum modes are modified resulting in the reversible decay process, see also Quantum revival. The theory of the spontaneous emission under the QED framework was first calculated by Weisskopf and Wigner. 
In spectroscopy one can frequently find that atoms or molecules in the excited states dissipate their energy in the absence of any external source of photons. This is not spontaneous emission, but is actually nonradiative relaxation of the atoms or molecules caused by the fluctuation of the surrounding molecules present inside the bulk.
Rate of spontaneous emission.
The rate of spontaneous emission (i.e., the radiative rate) can be described by Fermi's golden rule. The rate of emission depends on two factors: an 'atomic part', which describes
the internal structure of the light source and a 'field part', which describes the density of electromagnetic modes of the environment. The atomic part describes the strength of a transition between two states in terms of transition moments. In a homogeneous medium, such as free space, the rate of spontaneous emission in the dipole approximation is given by: 
where formula_3 is the emission frequency, formula_40 is the index of refraction, formula_41 is the transition dipole moment, formula_42 is the vacuum permittivity, formula_6 is the reduced Planck constant, formula_44 is the vacuum speed of light, and formula_45 is the fine structure constant. (This approximation breaks down in the case of inner shell electrons in high-Z atoms.) The above equation clearly shows that the rate of spontaneous emission in free space increases proportionally to formula_46. 
In contrast with atoms, which have a discrete emission spectrum, quantum dots can be tuned continuously by changing their size. This property has been used to check the formula_46-frequency dependence of the spontaneous emission rate as described by Fermi's golden rule.
Radiative and nonradiative decay: the quantum efficiency.
In the rate-equation above, it is assumed that decay of the number of excited states formula_12 only occurs under emission of light. In this case one speaks of full radiative decay and this means that the quantum efficiency is 100%. Besides radiative decay, which occurs under the emission of light, there is a second decay mechanism; nonradiative decay. To determine the total decay rate formula_49, radiative and nonradiative rates should be summed:
where formula_49 is the total decay rate, formula_20 is the radiative decay rate and formula_53 the nonradiative decay rate. The quantum efficiency (QE) is defined as the fraction of emission processes in which emission of light is involved:
In nonradiative relaxation, the energy is released as phonons, more commonly known as heat. Nonradiative relaxation occurs when the energy difference between the levels is very small, and these typically occur on a much faster time scale than radiative transitions. For many materials (for instance, semiconductors), electrons move quickly from a high energy level to a meta-stable level via small nonradiative transitions and then make the final move down to the bottom level via an optical or radiative transition. This final transition is the transition over the bandgap in semiconductors. Large nonradiative transitions do not occur frequently because the crystal structure generally cannot support large vibrations without destroying bonds (which generally doesn't happen for relaxation). Meta-stable states form a very important feature that is exploited in the construction of lasers. Specifically, since electrons decay slowly from them, they can be deliberately piled up in this state without too much loss and then stimulated emission can be used to boost an optical signal.

</doc>
<doc id="28220" url="https://en.wikipedia.org/wiki?curid=28220" title="Nicolas Léonard Sadi Carnot">
Nicolas Léonard Sadi Carnot

Nicolas Léonard Sadi Carnot (; 1 June 1796 – 24 August 1832) was a French military engineer and physicist, often described as the "father of thermodynamics". In his only publication, the 1824 monograph "Reflections on the Motive Power of Fire", Carnot gave the first successful theory of the maximum efficiency of heat engines. Carnot's work attracted little attention during his lifetime, but it was later used by Rudolf Clausius and Lord Kelvin to formalize the second law of thermodynamics and define the concept of entropy.
Life.
Nicolas Léonard Jonel Sadi Carnot was born in Paris into a family that was distinguished in both science and politics. He was the first son of Lazare Carnot, an eminent mathematician, military engineer and leader of the French Revolutionary Army. Lazare chose his son's third given name (by which he would always be known) after the Persian poet Sadi of Shiraz. Sadi was the elder brother of statesman Hippolyte Carnot and the uncle of Marie François Sadi Carnot, who would serve as President of France from 1887 to 1894.
At the age of 16, Sadi Carnot became a cadet in the École Polytechnique in Paris, where his classmates included Michel Chasles and Gaspard-Gustave Coriolis. The École Polytechnique was intended to train engineers for military service, but its professors included such eminent scientists as André-Marie Ampère, François Arago, Joseph Louis Gay-Lussac, Louis Jacques Thénard and Siméon Denis Poisson, and the school had become renowned for its mathematical instruction. After graduating in 1814, Sadi became an officer in the French army's corps of engineers. His father Lazare had served as Napoleon's minister of the interior during the "Hundred Days", and after Napoleon's final defeat in 1815 Lazare was forced into exile. Sadi's position in the army, under the restored Bourbon monarchy of Louis XVIII, became increasingly difficult.
Sadi Carnot was posted to different locations, he inspected fortifications, tracked plans and wrote many reports. It appears his recommendations were ignored and his career was stagnating. On 15 September 1818 he took a six-month leave to prepare for the entrance examination of Royal Corps of Staff and School of Application for the Service of the General Staff.
In 1819, Sadi transferred to the newly formed General Staff, in Paris. He remained on call for military duty, but from then on he dedicated most of his attention to private intellectual pursuits and received only two-thirds pay. Carnot befriended the scientist Nicolas Clément and attended lectures on physics and chemistry. He became interested in understanding the limitation to improving the performance of steam engines, which led him to the investigations that became his "Reflections on the Motive Power of Fire", published in 1824.
Carnot retired from the army in 1828, without a pension. He was interned in a private asylum in 1832 as suffering from "mania" and "general delirum", and he died of cholera shortly thereafter, aged 36, at the hospital in Ivry-sur-Seine.
"Reflections on the Motive Power of Fire".
Background.
When Carnot began working on his book, steam engines had achieved widely recognized economic and industrial importance, but there had been no real scientific study of them. Newcomen had invented the first piston-operated steam engine over a century before, in 1712; some 50 years after that, James Watt made his celebrated improvements, which were responsible for greatly increasing the efficiency and practicality of steam engines. Compound engines (engines with more than one stage of expansion) had already been invented, and there was even a crude form of internal-combustion engine, with which Carnot was familiar and which he described in some detail in his book. Although there existed some intuitive understanding of the workings of engines, scientific theory for their operation was almost nonexistent. In 1824 the principle of conservation of energy was still poorly developed and controversial, and an exact formulation of the first law of thermodynamics was still more than a decade away; the mechanical equivalence of heat would not be formulated for another two decades. The prevalent theory of heat was the caloric theory, which regarded heat as a sort of weightless and invisible fluid that flowed when out of equilibrium.
Engineers in Carnot's time had tried, by means such as highly pressurized steam and the use of fluids, to improve the efficiency of engines. In these early stages of engine development, the efficiency of a typical engine — the useful work it was able to do when a given quantity of fuel was burned — was only 3%.
Carnot cycle.
Carnot sought to answer two questions about the operation of heat engines: "Is the work available from a heat source potentially unbounded?" and "Can heat engines in principle be improved by replacing the steam with some other working fluid or gas?" He attempted to answer these in a memoir, published as a popular work in 1824 when he was only 28 years old. It was entitled "Réflexions sur la Puissance Motrice du Feu" ("Reflections on the Motive Power of Fire"). The book was plainly intended to cover a rather wide range of topics about heat engines in a rather popular fashion; equations were kept to a minimum and called for little more than simple algebra and arithmetic, except occasionally in the footnotes, where he indulged in a few arguments involving some calculus. He discussed the relative merits of air and steam as working fluids, the merits of various aspects of steam engine design, and even included some ideas of his own regarding possible improvements of the practical nature. The most important part of the book was devoted to an abstract presentation of an idealized engine that could be used to understand and clarify the fundamental principles that are generally applied to all heat engines, independent of their design.
Perhaps the most important contribution Carnot made to thermodynamics was his abstraction of the essential features of the steam engine, as they were known in his day, into a more general and idealized heat engine. This resulted in a model thermodynamic system upon which exact calculations could be made, and avoided the complications introduced by many of the crude features of the contemporary steam engine. By idealizing the engine, he could arrive at clear and indisputable answers to his original two questions.
He showed that the efficiency of this idealized engine is a function only of the two temperatures of the reservoirs between which it operates. He did not, however, give the exact form of the function, which was later shown to be (T1−T2)/T1, where T1 is the absolute temperature of the hotter reservoir. (Note: This equation probably came from Kelvin.) No thermal engine operating any other cycle can be more efficient, given the same operating temperatures.
The Carnot cycle is the most efficient possible engine, not only because of the (trivial) absence of friction and other incidental wasteful processes; the main reason is that it assumes no conduction of heat between parts of the engine at different temperatures. Carnot knew that the conduction of heat between bodies at different temperatures is a wasteful and irreversible process, which must be eliminated if the heat engine is to achieve maximum efficiency.
Regarding the second point, he also was quite certain that the maximum efficiency attainable did not depend upon the exact nature of the working fluid. He stated this for emphasis as a general proposition:
For his "motive power of heat", we would today say "the efficiency of a reversible heat engine", and rather than "transfer of caloric" we would say "the reversible transfer of entropy ∆S" or "the reversible transfer of heat at a given temperature Q/T". He knew intuitively that his engine would have the maximum efficiency, but was unable to state what that efficiency would be.
He concluded:
and
In an idealized model, the caloric transported from a hot to a cold body by a frictionless heat engine that lacks of conductive heat flow, driven by a difference of temperature, yielding work, could also be used to transport the caloric back to the hot body by reversing the motion of the engine consuming the same amount of work, a concept subsequently known as thermodynamic reversibility. Carnot further postulated that no caloric is lost during the operation of his idealized engine. The process being completely reversible, executed by this kind of heat engine is the most efficient possible process. The assumption that heat conduction driven by a temperature difference cannot exist, so that no caloric is lost by the engine, guided him to design the Carnot-cycle to be operated by his idealized engine. The cycle is consequently composed of adiabatic processes where no heat/caloric ∆S = 0 flows and isothermal processes where heat is transferred ∆S > 0 but no temperature difference ∆T = 0 exist. The proof of the existence of a maximum efficiency for heat engines is as follows:
As the cycle named after him doesn't waste caloric, the reversible engine has to use this cycle. Imagine now two large bodies, a hot and a cold one. He postulates now the existence of a heat machine with a greater efficiency. We couple now two idealized machine but of different efficiencies and connect them to the same hot and the same cold body. The first and less efficient one lets a constant amount of entropy ∆S = Q/T flow from hot to cold during each cycle, yielding an amount of work denoted W. If we use now this work to power the other more efficient machine, it would, using the amount of work W gained during each cycle by the first machine, make an amount of entropy ∆S' > ∆S flow from the cold to the hot body. The net effect is a flow of ∆S' − ∆S ≠ 0 of entropy from the cold to the hot body, while no net work is done. Consequently, the cold body is cooled down and the hot body rises in temperature. As the difference of temperature rises now the yielding of work by the first is greater in the successive cycles and due to the second engine difference in temperature of the two bodies stretches by each cycle evan more. In the end this set of machines would be a perpetuum mobile that cannot exist. This proves that the assumption of the existence of a more efficient engine was wrong so that an heat engine that operates the Carnot cycle must be the most efficient one. This means that a frictionless heat engine that lacks of conductive heat flow driven by a difference of temperature shows maximum possible efficiency.
He concludes further that the choice of the working fluid, its density or the volume occupied by it cannot change this maximum efficiency. Using the equivalence of any working gas used in heat engines he deduced that the difference in the specific heat of a gas measured at constant pressure and at constant volume must be constant for all gases.
By comparing the operation of his hypothetical heat engines for two different volumes occupied by the same amount of working gas he correctly deduces the relation between entropy and volume for an isothermal process:
formula_1
Reception and later life.
Carnot's book received very little attention from his contemporaries. The only reference to it within a few years after its publication was in a review in the periodical "Revue Encyclopédique", which was a journal that covered a wide range of topics in literature. The impact of the work had only become apparent once it was modernized by Émile Clapeyron in 1834 and then further elaborated upon by Clausius and Kelvin, who together derived from it the concept of entropy and the second law of thermodynamics.
On Carnot's religious views, he was a Philosophical theist. As a deist, he believed in divine causality, stating that "what to an ignorant man is chance, cannot be chance to one better instructed," but he did not believe in divine punishment. He criticized established religion, though at the same time spoke in favor of "the belief in an all-powerful Being, who loves us and watches over us."
He was a reader of Blaise Pascal, Molière and Jean de La Fontaine.
Death.
Carnot died during a cholera epidemic in 1832, at the age of 36. 
Because of the contagious nature of cholera, many of Carnot's belongings and writings were buried together with him after his death. As a consequence, only a handful of his scientific writings survived.
After the publication of "Reflections on the Motive Power of Fire", the book quickly went out of print and for some time was very difficult to obtain. Kelvin, for one, had a difficult time getting a copy of Carnot's book. In 1890 an English translation of the book was published by R. H. Thurston; this version has been reprinted in recent decades by Dover and by Peter Smith, most recently by Dover in 2005. Some of Carnot's posthumous manuscripts have also been translated into English.
Carnot published his book in the heyday of steam engines. His theory explained why steam engines using superheated steam were better because of the higher temperature of the consequent hot reservoir. Carnot's theories and efforts did not immediately help improve the efficiency of steam engines; his theories only helped to explain why one existing practice was superior to others. It was only towards the end of the nineteenth century that Carnot's ideas, namely that a heat engine can be made more efficient if the temperature of its hot reservoir is increased, were put into practice. Carnot's book did, however, eventually have a real impact on the design of practical engines. Rudolf Diesel, for example, used Carnot's theories to design the diesel engine, in which the temperature of the hot reservoir is much higher than that of a steam engine, resulting in an engine which is more efficient.
Bibliography.
The text of part of an earlier version of this article was taken from the public domain resource "A Short Account of the History of Mathematics" by W. W. Rouse Ball (4th Edition, 1908)

</doc>
