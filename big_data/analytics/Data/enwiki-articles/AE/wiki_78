<doc id="29166" url="https://en.wikipedia.org/wiki?curid=29166" title="Galgo Español">
Galgo Español

The Galgo Español ("Spanish galgo") or Spanish greyhound is an ancient breed of dog, specifically a member of the sighthound family.
The English greyhound is possibly a descendant of the Spanish greyhound and, for several years in the 20th century, some breeders did cross-breed Galgos and Greyhounds in order to produce faster and more powerful Galgos, specifically for track racing purposes. 
Description.
Appearance.
Galgos are similar in appearance to Greyhounds, but are distinctly different in their conformation. Galgos are higher in the rear than in the front, and have flatter muscling than a Greyhound, which is characteristic of endurance runners. They also tend to be smaller, lighter in build, have longer tails and have a very long, streamlined head that gives the impression of larger ears. Their chests are not as deep as a Greyhound's and should not reach the point of the elbow 
Unlike Greyhounds, Galgos come in two coat types: smooth and rough. The rough coat can provide extra protection from skin injuries while running in the field. They come in a variety of colors and coat patterns. Main colors are "barcino" or "atigrado" (brindle), "negro" (black), "barquillo"(golden), "tostado"(toasted), "canela" (cinnamon), "amarillo"(yellow), "rojo"(red), "blanco" (white), "berrendo" (white with patches) or "pío" (any colour with white muzzle and forehead).
Temperament.
Galgos have a very similar nature to Greyhounds. They are calm, quiet, gentle and laid back; happy to sleep their day away on their backs on a sofa. More than 90% of Galgos can be considered cat-friendly and are therefore an ideal choice for the hound lover who also owns cats. Almost all Galgos are also friendly towards other dogs and small dogs. Galgos are also very good with children, being calm in the house so there is less risk of a child being knocked over or jumped on than with a more excitable breed. They are very gentle and tolerate the often over-enthusiastic attentions of children with little risk of retaliation from the dog.
Galgos have a very reserved personality and they have a tendency towards shyness, so it is very important that they be socialized early in life so that they grow up to be comfortable around strange people, dogs and locations.
Health.
Like many other sighthounds, Galgos are a fairly healthy breed although they are sensitive to anaesthesia. As such, proper care should be taken by the owner to ensure that the attending veterinarian is aware of this issue.
Although Galgos are big dogs, their history of selection as a working sighthound, their light weight, and their anatomy keep them safe from hip dysplasia.
These dogs must run regularly to keep in perfect health, combined with their characteristic tendency to sleep all the rest of the day.
Although Galgos are big dogs, their history of selection as a working sighthound, their light weight, and their anatomy keep them in principle safe from hip dysplasia.
History.
The Galgo is not only "the Spanish greyhound" but also "the Spanish dog".
Its name is probably derived from the Latin "Canis Gallicus" or "Dog from Gaul". The Spanish word for all kinds of Greyhounds - including the Galgo - is "lebrel", which means "harrier" or "dog for chasing hares", since "liebre" is Spanish for hare. We can see the same derivative in the Italian "levriero" and the French "lévrier". 
The first written references to an ancient Celtic sighthound, the "vertragus", in the "Cynegeticus" of Flavius Arrianus (Arrian), Roman proconsul of Baetica in the second century, may refer to the Galgo, or more likely to its antecedent.
The author Arrian, during his personal experience in Spain, describes hare hunting with Galgos in a manner almost identical to that used nowadays in Spain, adding that it was a general Celtic tradition not related to a social class. 
He indicates that there were not only smooth haired types of the vertragus but also coated ones.
There is little evidence on the Galgo or its antecedent in the first centuries of the Middle Age but it appeared to survive and flourish in the second half of this period.
In the 9th and 10th centuries great spaces in Castile were colonized, coinciding with the Reconquista, resulting in the Christian military repossession of the Iberian Peninsula from the Muslims. This open land introduces a new character to hunting with dogs: while the North of Spain is mountainous, the regions progressively recovered from the Muslims were flat, open areas full of small animals like hares, which provided the Galgo a useful opportunity to hunt. At this time, it is considered a noble dog, and kept mainly by aristocracy, both in the Christian and the Muslim Kingdoms in which the Spanish territory was still divided at the time. It is likely that the Galgo and Sloughi were interbred at this period.
The great esteem in which the Galgo was held is visible in the many laws of the time designed to punish the killing or theft of this dog: Fuero of Salamanca (9th century); Fuero of Cuenca; Fuero of Zorita de los Canes; Fuero of Molina de Aragón (12th century); Fuero of Usagre (12th century). 
In the Cartuario of Slonza we can read a will written in Villacantol, in which, using an odd mixture of Latin and Spanish, the Mayor Gutiérrez bequeaths a Galgo to Diego Citid in the year 1081:
The fact that this dog was a significant item in a noble's will, demonstrates the great value that it was given at the time.
The mural paintings at the Hermitage of San Baudelio de Berlanga, in Soria, dating from the 12th century show a hunting scene with three Galgos apparently identical to the ones that we can see today.
In the Renaissance Martínez del Espinar writes in his book "Arte de Ballestería y Montería" ("The Art of Hunting and Archery"):
The Galgo appears to have developed first in the Castillian plains, both in the north (Valladolid, Zamora, Ávila Salamanca, Segovia, Soria, Burgos and Palencia) and the south (Toledo, Cuenca, Guadalajara, Madrid and Ciudad Real) of Castilla. And, afterwards, in more southern territories: La Mancha and Andalusia.
It became the typical dog type of the Spanish interior, while the bloodhound plays the same role in the coast regions.
The Galgo appears not only in hunting books but also in common Spanish expressions, as well as in Literature. Maybe the most famous reference is the one contained in the opening sentence of "Don Quixote de La Mancha":
There are plenty of common expressions in Spain that name the Galgo. For example
""A galgo viejo, echadle liebre, no conejo"" which means "" use old Galgos for chasing hares instead of rabbits"" suggests that it is best to use experienced people for hard tasks and challenges.
"Galgo que va tras dos liebres, sin ninguna vuelve" meaning "if a Galgo tries to chase two hares, it will return with none" recommends focussing on a single effort, otherwise by distraction, failing.
Although the breed did not apparently experience any significant change in the 18th and 19th centuries, and was kept in its vocation as a swift hunting dog, maybe the most telling proverb which mentions the Galgo, is the one dating from the first years of the nineteenth Century:
Meaning
Which was used at first to satirize the corrupt Government of Fernando VII, considered to cheat in everything it did.
In the first years of the 20th century, large scale crossbreeding occurred between the Galgo and the English Greyhound in order to create faster dogs for professional track racing. This certainly affected the purity of the breed, the resulting dogs were just a bit faster, but did lose their long-distance-running abilities. Finally breeders came to the conclusion that it was not worth crossbreeding. 
The pure bred Galgo kept its major presence in the Spanish villages as an excellent hunting type.
Despite its antiquity and importance, the Spanish Galgo has only recently been acknowledged by the cynological associations. The English Greyhound has tended to outshine the Galgo. Spain has suffered catastrophic events during the last century, such as the Spanish Civil War and the 40-year-long Francisco Franco fascist dictatorship, which allowed this breed to be kept relatively unknown both inside and outside of its native country, at least until democracy led to greater social and cultural equality and development.
The breed faces the 21st century being progressively more appreciated at home and abroad, as contemporary Spain becomes more conscious of the uniqueness and heritage of this splendid animal.
Roles.
Hunting use.
The Galgo was used for hunting in monterías and for hunting of hare in open field, where the dog hunt the piece without intervention of the man after a chase. This type of hunting, which now has sporty character, in the past it was an act of social prestige in which hunting was a pretext to prove who was the best specimen holder.
Because of its very particular conditions, Spain is probably the country where the greyhound is used in a greater number of hunting and sports., and is commonly found in any of the towns and cities of the vast geography of the flat Spain.
The Galgo of field moves in Spain annually on the order of ten billion pesetas , calculation refers only to a part of fans in the country included in galgueras societies. This small part of galgueros prepared a year between three thousand and four thousand galgos on the occasion of its participation in the various Open Field Championships. This type of events, where each year it rewards the most characteristic example, seem to show that the galguero turns to a breed that was lost years ago.
The crossbreeding with English Greyhound for faster racing has stopped and eradicated for several reasons:
On the one hand, this hunting activity is evolving by leaps and bounds to become a real sport, where the killing of hare is secondary for the beauty of the race. The hybrid Galgo loses much of that beauty, what is increasingly valued the purity of the Galgo Español.
On the other hand, the fact that the hare is protected more than ever against the guns and it increasingly stronger by natural selection (has lost its natural places of refuge by rising plowed land and roads of concentration) It makes it necessary hunting with harder Galgos.
In fact, the particular circumstances within geography have an impact, and never a Galgo that leading running generations in Andalusia have the same characteristics as other that leads running so many generations in Castile.
Thus in Andalusia and many parts of La Mancha dominates the vineyard and olive having the hare perdederos next. Similarly being the mild climate, will throughout the year with plenty of food without having to travel long distances. Additionally it less annoying to be within large farms without neighbor roads. All these issues will make the Galgo prevails in these areas is smaller and muscles somewhat shorter and rounded, ie, higher power in the hindquarters. This is because it must be a Galgo that reaches quickly and out of cuts more easily. Lighter by the softness of the ground where step and definitely more rapid and less strong, although a property does not necessarily exclude the other.
In Castile, where large spaces, the hare travels great distances in search of food and its only defense being away its perdederos are its legs and its heart. In this type of land it is to impose other Galgo harder footprint, deeper breast, longer and flatter muscles, ultimately a greyhound higher strength. These greyhounds are heavier have more appeal, and in fact have been considered purer of face to the stabilization of the race.
Also, outside of Spain, in Chile (mainly in the Central Zone), these dogs are used in competitions that take place on national holidays.
Galgos as pets.
Due to their primary role as hunting dogs in the Spanish countryside, the Spanish Galgos are sometimes treated a little better than commodities. However, most people know that Galgos are mistreated and abused in their native Spain. Galgueros (breeders), as they are normally called, will often select puppies from a litter that show the most propensity for hunting or racing, while abandoning the rest in the streets. The puppies that do get selected often do not live very long lives, as the galgueros often consider the dog too old to hunt once it has achieved two or three years of age-often after hunting season ends, they are either abandoned, shot, or hanged. For all these reasons, many associations in defense of the Galgo have appeared with the aim to save these dogs from a terrible fate, provide much needed rehabilitation, and adoptive homes, usually in the cities. Some associations will adopt them to other locations in Europe, including France, the UK, Germany, Belgium and the Netherlands.
Because they tend to be quiet and docile, Galgos make very nice house pets. In Spain they have a well earned reputation as gentle dogs, with sweet temperaments and solid health. They tend to get along well with people and other dogs, and they can be well-behaved around cats if properly socialized. Outside of sunny Spain, they require a warm coat to keep them warm in cold winter weather: like all greyhound type breeds, they have little body fat and short coats, so extra warmth is preferred for colder climates.
Galgos excel at performance activities like lure coursing and racing. They are eligible to compete in lure coursing events sanctioned by the American Sighthound Field Association, entered in the Limited class. They also make very nice show dogs and have enjoyed success in the European show ring, although they are not as well known in the American show world due to their rarity outside Europe.

</doc>
<doc id="29167" url="https://en.wikipedia.org/wiki?curid=29167" title="Stephen Bachiler">
Stephen Bachiler

Stephen Bachiler (circa 1561buried October 31, 1656) was an English clergyman who was an early proponent of the separation of church and state in America.
Early life.
Bachiler was born circa 1561. An early graduate of Oxford (St. John's College, 1586), he was vicar of Wherwell, Hampshire (1587–1605) when ousted for Puritanical leanings under James I. Bachiler is said to have married _____ Bates, sister of Rev. John Bates (who succeeded Bachiler as Vicar at Wherwell), about 1590, with whom he had six children: Nathaniel, Deborah, Stephen, Samuel, Ann, and Theodate.
Bachiler had a second marriage to Christian Weare, widow, in 1623. She died before 1627. His third marriage, in 1627, was to Helena Mason, the widow of Revd. Thomas Mason of Odiham, Hampshire; Mary, the daughter of Helena and Thomas Mason, was married to Richard Dummer, who also became involved in the founding of the Plough Company.
Plough Company and immigration.
In 1630 he was a member of the "Company of Husbandmen" in London and with them, as the Plough Company, obtained a 1,600 mile² (4,000 km²) grant of land in Maine from the Plymouth Council for New England. The colony was called "Lygonia" after Cecily Lygon, mother of New England Council president Sir Ferdinando Gorges. Bachiler was to be its minister and leader. Although the settlers sailed to America in the winter of 1630-1631, the project was abandoned.
Bachiler was accompanied to America, on the ship William & Francis (5 June 1632) after an 88 day jouney, by his third wife, Helena and his "family". Exactly who came with Rev. Bachiler is unknown. The families of his children Nathaniel, Deborah, Ann & Theodate are all later found in New England.
Lynn, MA.
Bachiler was 70 years old when he reached Boston in 1632, and gathered his followers to establish the First Church of Lynn (then Saugus). He incurred the hostility of the Puritan theocracy in Boston, being believed to have cast the only dissenting vote among ministers against the expulsion of Roger Williams. Despite his age, he was uncommonly energetic, and throughout some two decades pursued settlement and church endeavors, always engaged in controversy and confrontation with Bay Colony leaders.
New Hampshire.
In 1638, Bachiler and others successfully petitioned to begin a new plantation at Winnacunnet, to which he gave the name Hampton when the town was incorporated in 1639. His ministry there became embroiled in controversy when Timothy Dalton was sent to the town as "teaching assistant" by the Boston church after New Hampshire was absorbed by Massachusetts in 1641. Shortly thereafter, Bachiler was excommunicated by the Hampton church on unfounded charges of "scandal", but protested to Governor Winthrop and was later reinstated. In other respects, Bachiler's reputation was such that in 1642, he was asked by Thomas Gorges, deputy governor of the Province of Maine, to act as arbitration "umpire" (deciding judge) in a Saco Court land dispute between George Cleeve and John Winter.
Maine.
By 1644 Cleeve had become deputy governor of "Lygonia", a rival province to that of Gorges' in Maine established from a resurrected Plough Patent, and asked Bachiler to be its minister at Casco. Bachiler deferred, having already received a call to be minister for the new town of Exeter. Once again Massachusetts intervened in his affairs when the General Court ordered deferral of any church at Exeter. Frustrated in his attempts at a new ministry, Bachiler left Hampton and went as missionary to Strawbery Banke (now Portsmouth, New Hampshire) probably that same year 1644. While there, he married in 1648 (as fourth wife) a young widow, Mary Beedle of Kittery, Maine. In 1651, she was indicted and sentenced for adultery with a neighbor.
Emigration to England and Death.
Denied a divorce by the Massachusetts Court, Bachiler finally returned to England about 1653. His children who had stayed in England, were well off and able to take care of him. Bachiler died near London, and was buried at All Hallows Staining on October 31, 1656.
Perhaps the best summation of his career is in the biographical entry in Robert Charles Anderson's look at the early immigrants: "Among the many remarkable lives lived by early New Englanders, Bachiler's is the most remarkable." 

</doc>
<doc id="29168" url="https://en.wikipedia.org/wiki?curid=29168" title="Soap (disambiguation)">
Soap (disambiguation)

Soap is a surfactant cleaning compound used for personal or other cleaning.
Soap may also refer to:

</doc>
<doc id="29171" url="https://en.wikipedia.org/wiki?curid=29171" title="Declaration of Sentiments">
Declaration of Sentiments

The Declaration of Sentiments, also known as the Declaration of Rights and Sentiments, is a document signed in 1848 by 68 women and 32 men—100 out of some 300 attendees at the first women's rights convention to be organized by women. The convention was held in Seneca Falls, New York, now known as the Seneca Falls Convention. The principal author of the Declaration was Elizabeth Cady Stanton, who based it on the form of the United States Declaration of Independence. She was a key organizer of the convention along with Lucretia Coffin Mott, and Martha Coffin Wright. 
According to the "North Star," published by Frederick Douglass, whose attendance at the convention and support of the Declaration helped pass the resolutions put forward, the document was the "grand movement for attaining the civil, social, political, and religious rights of women."
At a time when traditional roles were still very much in place, the Declaration caused much controversy. Many people respected the courage and abilities behind the drafting of the document, but were unwilling to abandon conventional mindsets. An article in the "Oneida Whig" published soon after the convention described the document as "the most shocking and unnatural event ever recorded in the history of womanity." Many newspapers insisted that the Declaration was drafted at the expense of women's more appropriate duties. At a time when temperance and female property rights were major issues, even many supporters of women's rights believed the Declaration's endorsement of women's suffrage would hinder the nascent women's rights movement, causing it to lose much needed public support.
Signers.
Signers of the Declaration at Seneca Falls in order:
The men signed under the heading "…the gentlemen present in favor of this new movement:
References.
Notes
Bibliography

</doc>
<doc id="29174" url="https://en.wikipedia.org/wiki?curid=29174" title="Social class">
Social class

Social class (or, simply, class), as in class society, is a set of concepts in the social sciences and political theory centered on models of social stratification in which people are grouped into a set of hierarchical social categories, the most common being the upper, middle, and lower classes.
Class is an essential object of analysis for sociologists, political scientists, anthropologists, and social historians. However, there is not a consensus on the best definition of the "class," and the term has different contextual meanings. In common parlance, the term "social class" is usually synonymous with "socio-economic class," defined as "people having the same social, economic, or educational status," e.g., "the working class"; "an emerging professional class." However, academics distinguish social class and socioeconomic status, with the former referring to one’s relatively stable sociocultural background and the latter referring to one’s current social and economic situation and, consequently, being more changeable over time.
The precise measurements of what determines social class in society has varied over time. According to philosopher Karl Marx, "class" is determined entirely by one's relationship to the means of production (their relations of production). The classes in modern capitalist society, according to Marx, are the proletariat, those who work but do not own the means of production; and the bourgeoisie, those who invest and live off of the surplus generated by the former. This contrasts with the view of the sociologist Max Weber, who argued "class" is determined by economic position, in contrast to "social status" or "Stand" which is determined by social prestige rather than simply just relations of production.
The term "class" is etymologically derived from the Latin "classis", which was used by census takers to categorize citizens by wealth, in order to determine military service obligations.
In the late 18th century, the term "class" began to replace classifications such as estates, rank, and orders as the primary means of organizing society into hierarchical divisions. This corresponded to a general decrease in significance ascribed to hereditary characteristics, and increase in the significance of wealth and income as indicators of position in the social hierarchy.
History.
Historically social class and behavior was sometimes laid down in law. For example, permitted mode of dress in some times and places was strictly regulated, with sumptuous dressing only for the high ranks of society and aristocracy; sumptuary laws stipulated the dress and jewelry appropriate for a person's social rank and station.
Theoretical models.
Definitions of social classes reflect a number of sociological perspectives, informed by anthropology, economics, psychology, and sociology. The major perspectives historically have been Marxism and Structural functionalism. The common "stratum model of class" divides society into a simple hierarchy of working class, middle class and upper class. Within academia, two broad schools of definitions emerge: those aligned with 20th-century sociological stratum models of class society, and those aligned with the 19th-century historical materialist economic models of the Marxists and anarchists.
Another distinction can be drawn between "analytical" concepts of social class, such as the "Marxist" and "Weberian" traditions, and the more "empirical" traditions such as socio-economic status approach, which notes the correlation of income, education and wealth with social outcomes without necessarily implying a particular theory of social structure.
Marxist.
For Marx, class is a combination of objective and subjective factors. Objectively, a class shares a common relationship to the means of production.
Subjectively, the members will necessarily have some perception ("class consciousness") of their similarity and common interest. Class consciousness is not simply an awareness of one's own class interest but is also a set of shared views regarding how society should be organized legally, culturally, socially and politically. These class relations are reproduced through time.
In Marxist theory, the class structure of the capitalist mode of production is characterized by the conflict between two main classes: the bourgeoisie, the capitalists who own the means of production, and the much larger proletariat (or 'working class') who must sell their own labour power (See also: wage labour). This is the fundamental economic structure of work and property, a state of inequality that is normalized and reproduced through cultural ideology.
Marxists explain the history of "civilized" societies in terms of a war of classes between those who control production and those who produce the goods or services in society. In the Marxist view of capitalism, this is a conflict between capitalists (bourgeoisie) and wage-workers (the proletariat). For Marxists, class antagonism is rooted in the situation that control over social production necessarily entails control over the class which produces goods—in capitalism this is the exploitation of workers by the bourgeoisie.
Furthermore, "in countries where modern civilisation has become fully developed, a new class of petty bourgeois has been formed". "An industrial army of workmen, under the command of a capitalist, requires, like a real army, officers (managers) and sergeants (foremen, over-lookers) who, while the work is being done, command in the name of the capitalist".
Marx makes the argument that the bourgeoisie reach a point of wealth accumulation they hold enough power as the dominant class to shape political institutions an society according to its own interests. Marx then goes on to claim that the non-elite class, just because of the mass amount of people who make up the class, have the power to overthrow the elite and create an equal society.
Marx himself argued that it was the goal of the proletariat itself to displace the capitalist system with socialism, changing the social relationships underpinning the class system and then developing into a future communist society in which: "..the free development of each is the condition for the free development of all." (Communist Manifesto) This would mark the beginning of a classless society in which human needs rather than profit would be motive for production. In a society with democratic control and production for use, there would be no class, no state and no need for money.
Weberian.
Max Weber formulated a three-component theory of stratification, that saw social class as emerging from an interplay between "class", "status" and "politicsr". Weber believed that class position was determined by a person's relationship to the means of production, while status or "Stand" emerged from estimations of honor or prestige. 
Weber derived many of his key concepts on social stratification by examining the social structure of many countries. He noted that contrary to Marx's theories, stratification was based on more than simply ownership of capital. Weber pointed out that some members of the aristocracy lack economic wealth yet might nevertheless have political power. Likewise in Europe, many wealthy Jewish families in lack prestige and honor, because they were a member of a "pariah group" like the Jews. 
Great British Class Survey.
On April 2, 2013 the results of a survey conducted by BBC Lab UK developed in collaboration with academic experts and slated to be published in the journal "Sociology" were published online. The results released were based on a survey of 160,000 residents of the United Kingdom most of whom lived in England and described themselves as "white." Class was defined and measured according to the amount and kind of economic, cultural, and social resources reported. Economic capital was defined as income and assets; cultural capital as amount and type of cultural interests and activities, and social capital as the quantity and social status of their friends, family and personal and business contacts. This theoretical framework was developed by Pierre Bourdieu who first published his theory of social distinction in 1979.
The common three-stratum model.
Today, concepts of social class often assume three general categories: a very wealthy and powerful "upper class" that owns and controls the means of production; a "middle class" of professional workers, small business owners, and low-level managers; and a "lower class", who rely on low-paying wage jobs for their livelihood and often experience poverty.
Upper class.
The upper class is the social class composed of those who are rich, well-born, powerful, or a combination of those. They usually wield the greatest political power. In some countries, wealth alone is sufficient to allow entry into the upper class. In others, only people who are born or marry into certain aristocratic bloodlines are considered members of the upper class, and those who gain great wealth through commercial activity are looked down upon by the "old rich" as "nouveau riche". In the United Kingdom, for example, the upper classes are the aristocracy and royalty, with wealth playing a less important role in class status. Many aristocratic peerages or titles have 'seats' attached to them, with the holder of the title (e.g. Earl of Bristol) and his family being the custodians of the house, but not the owners. Many of these require high expenditures, so wealth is typically needed. Many aristocratic peerages and their homes are parts of estates, owned and run by the title holder with moneys generated by the land, rents, or other sources of wealth. In America, however, where there is no aristocracy or royalty, the upper class status belongs to the extremely wealthy, the so-called 'super-rich', though there is some tendency even in America for those with old family wealth to look down on those who have earned their money in business, the struggle between New Money and Old Money.
The upper class is generally contained within the richest one or two percent of the population. Members of the upper class are often born into it, and are distinguished by immense wealth which is passed from generation to generation in the form of estates. Sometimes members of the upper class are called "the one percent".
Middle class.
The middle class is the most contested of the three categories, the broad group of people in contemporary society who fall socio-economically between the lower and upper classes. One example of the contest of this term is that in the United States "middle class" is applied very broadly and includes people who would elsewhere be considered working class. Middle class workers are sometimes called "white-collar workers".
Theorists such as Ralf Dahrendorf have noted the tendency toward an enlarged middle class in modern Western societies, particularly in relation to the necessity of an educated work force in technological economies. Perspectives concerning globalization and neocolonialism, such as dependency theory, suggest this is due to the shift of low-level labour to developing nations and the Third World.
Candidates in the 2016 Presidential election have avoided using the term "middle class" because it evokes feelings of anxiety and instability within the largest political faction. After World War II, the middle class was a solid position to hold and perpetuated the American Dream, but since the recession of 2008, the middle class, especially the working class, has not felt secure in their ability to work hard to guarantee better lives for future generations. Reports show that when politicians cite the middle class, their rhetoric connotes fears of falling behind; Hillary Clinton now uses terms like "everyday Americans" to evoke the average American worker.
Lower class.
Lower class (occasionally described as working class) are those employed in low-paying wage jobs with very little economic security. The term "lower class" also refers to persons with low income.
The working class is sometimes separated into those who are employed but lacking financial security, and an underclass—those who are long-term unemployed and/or homeless, especially those receiving welfare from the state. The latter is analogous to the Marxist term ""lumpenproletariat"". Members of the working class are sometimes called blue-collar workers.
Social class in the untied states.
There are usually four social classes that are described in America: the upper class, the middle class, the working class, and the lower class. The upper class typically earns above $250,000 per year; the middle class earns between $48,000 and $249,000 per year; the working class up to $48,000; and the lower class up generally receives a minimal income that is not enough to sustain themselves. These large income gaps are thought to be one of several root causes of class warfare. . While income is a large indicator of class, general wealth and accumulated assets plays a large role in class position because those things have value that can be exchanged for money and thus grant power.
Consequences of class position.
A person's socioeconomic class has wide-ranging effects. It may determine the schools they are able to attend, the jobs open to them, who they may marry, and their treatment by police and the courts.
Doctors Angus Deaton and Anne Case have analyzed the mortality rates related to the group of white, middle-aged Americans between the ages of 45 and 54 and its relation to class. There has been a growing number of suicides and deaths by substance abuse in this particular group of middle class Americans. This group also has been recorded to have an increase in reports of chronic pain and poor general health. Dr. Deaton and Dr. Case came to the conclusion from these observation that because of the constant stress that these white, middle aged Americans feel fighting poverty and wavering between the lower and working class, these strains have taken a toll on these people and affected their whole bodies.
Education.
A person's social class has a significant impact on their educational opportunities. Not only are upper-class parents able to send their children to exclusive schools that are perceived to be better, but in many places state-supported schools for children of the upper class are of a much higher quality than those the state provides for children of the lower classes. This lack of good schools is one factor that perpetuates the class divide across generations.
In 1977, British cultural theorist Paul Willis published a study titled "Learning to Labour", in which he investigated the connection between social class and education. In his study, he found that a group of working class schoolchildren had developed an antipathy towards the acquisition of knowledge as being outside their class, and therefore undesirable, perpetuating their presence in the working class.
Health and nutrition.
A person's social class has a significant impact on their physical health, their ability to receive adequate medical care and nutrition, and their life expectancy.
Lower-class people experience a wide array of health problems as a result of their economic status. They are unable to use health care as often, and when they do it is of lower quality, even though they generally tend to experience a much higher rate of health issues. Lower-class families have higher rates of infant mortality, cancer, cardiovascular disease, and disabling physical injuries. Additionally, poor people tend to work in much more hazardous conditions, yet generally have much less (if any) health insurance provided for them, as compared to middle and upper class workers.
Employment.
The conditions at a person's job vary greatly depending on class. Those in the upper-middle class and middle class enjoy greater freedoms in their occupations. They are usually more respected, enjoy more diversity, and are able to exhibit some authority. Those in lower classes tend to feel more alienated and have lower work satisfaction overall. The physical conditions of the workplace differ greatly between classes. While middle-class workers may "suffer alienating conditions" or "lack of job satisfaction", blue-collar workers are more apt to suffer alienating, often routine, work with obvious physical health hazards, injury, and even death.
A recent UK government study has suggested that a 'glass floor' exists in British Society which prevents those who are less able, but whom come from wealthier backgrounds, from slipping down the social ladder. This is due to the fact that those from wealthier backgrounds have more opportunities available to them. In fact, the article shows that less able, better-off kids are 35% more likely to become high earners than bright poor kids.
Class conflict.
Class conflict, frequently referred to as "class warfare" or "class struggle," is the tension or antagonism which exists in society due to competing socioeconomic interests and desires between people of different classes.
For Marx, the history of class society was a history of class conflict. He pointed to the successful rise of the bourgeoisie, and the necessity of revolutionary violence—a heightened form of class conflict—in securing the bourgeoisie rights that supported the capitalist economy.
Marx believed that the exploitation and poverty inherent in capitalism were a pre-existing form of class conflict. Marx believed that wage labourers would need to revolt to bring about a more equitable distribution of wealth and political power.
Classless society.
"Classless society" refers to a society in which no one is born into a social class. Distinctions of wealth, income, education, culture, or social network might arise and would only be determined by individual experience and achievement in such a society.
Since these distinctions are difficult to avoid, advocates of a classless society (such as anarchists and communists) propose various means to achieve and maintain it and attach varying degrees of importance to it as an end in their overall programs/philosophy.
Relationship between ethnicity and class.
Race and other large-scale groupings can also influence class standing. The association of particular ethnic groups with class statuses is common in many societies. As a result of conquest or internal ethnic differentiation, a ruling class is often ethnically homogenous and particular races or ethnic groups in some societies are legally or customarily restricted to occupying particular class positions. Which ethnicities are considered as belonging to high or low classes varies from society to society. In modern societies strict legal links between ethnicity and class have been drawn, such as in apartheid, the caste system in Africa, the position of the Burakumin in Japanese society, and the Casta system in Latin America.

</doc>
<doc id="29175" url="https://en.wikipedia.org/wiki?curid=29175" title="Solomon Schechter">
Solomon Schechter

Solomon Schechter (Hebrew: שניאור זלמן הכהן שכטר‎; 7 December 1847 – 19 November 1915) was a Moldavian-born American kohen, rabbi, academic scholar, and educator, most famous for his roles as founder and President of the United Synagogue of America, President of the Jewish Theological Seminary of America, and architect of the American Conservative Jewish movement.
Early life.
He was born in Focşani, Moldavia (now Romania) to Rabbi Yitzchok Hakohen, a shochet and member of Chabad hasidim. He was named after its founder, Shneur Zalman of Liadi. Schechter received his early education from his father who was a shochet ("ritual slaughterer"). Reportedly, he learned to read Hebrew by age 3, and by 5 mastered Chumash. He went to a yeshiva in Piatra Neamţ at age 10 and at age thirteen studied with one of the major Talmudic scholars, Rabbi Joseph Saul Nathanson of Lemberg. In his 20s, he went to the Rabbinical College in Vienna, where he studied under the more modern Talmudic scholar Meir Friedmann, before moving on in 1879 to undertake further studies at the "Berlin Hochschule für die Wissenschaft des Judentums" and at the University of Berlin. In 1882, he was invited to Britain, to be tutor of rabbinics under Claude Montefiore in London.
Academic career.
In 1890, after the death of Solomon Marcus Schiller-Szinessy, he was appointed to the faculty at Cambridge University, serving as a lecturer in Talmudics and reader in Rabbinics. To this day, the students of the Cambridge University Jewish Society hold an annual Solomon Schechter Memorial Lecture.
His greatest academic fame came from his excavation in 1896 of the papers of the Cairo Geniza, an extraordinary collection of over 100,000 pages of rare Hebrew religious manuscripts and medieval Jewish texts that were preserved at an Egyptian synagogue. The find revolutionized the study of Medieval Judaism.
Jacob Saphir was the first Jewish researcher to recognize the significance of the Cairo Geniza, as well as the first to publicize the existence of the Midrash ha-Gadol. Schechter was alerted to the existence of the Geniza's papers in May 1896 by two Scottish sisters, Mrs. Lewis and Mrs. Gibson, who showed him some leaves from the Geniza that contained the Hebrew text of Sirach, which had for centuries only been known in Greek and Latin translation. Letters, written at Schechter's prompting, by Agnes Smith to "The Athenaeum" and "The Academy" quickly revealed the existence of another nine leaves of the same manuscript in the possession of Archibald Sayce at Oxford University. Schechter quickly found support for another expedition to the Cairo Geniza, and arrived there in December 1896 with an introduction from the Chief Rabbi, Hermann Adler, to the Chief Rabbi of Cairo, Aaron Raphael Ben Shim'on. He carefully selected for the Cambridge University Library a trove three times the size of any other collection: this is now part of the Taylor-Schechter Collection. The find was instrumental in Schechter resolving a dispute with David Margoliouth as to the likely Hebrew language origins of Sirach.
Charles Taylor took a great interest in Solomon Schechter's work in Cairo, and the "genizah" fragments presented to the University of Cambridge are known as the Taylor-Schechter Collection. He was joint editor with Schechter of "The Wisdom of Ben Sira", 1899. He published separately "Cairo Genizah Palimpsests", 1900.
He became a Professor of Hebrew at University College London in 1899 and remained until 1902 when he moved to the United States and was replaced by Israel Abrahams.
American Jewish community.
In 1902, traditional Jews reacting against the progress of the American Reform Judaism movement, which was trying to establish an authoritative "synod" of American rabbis, recruited Schechter to become President of the Jewish Theological Seminary of America (JTSA).
Schechter served as the second President of the JTSA, from 1902 to 1915, during which time he founded the United Synagogue of America, later renamed as the United Synagogue of Conservative Judaism.
Religious and cultural beliefs.
Schechter emphasized the centrality of Jewish law (Halakha) in Jewish life in a speech in his inaugural address as President of the JTSA in 1902:
Schechter, on the other hand, believed in what he termed "Catholic Israel." The basic idea being that Jewish law, Halacha, is formed and evolves based on the behavior of the people. This concept of modifying the law based on national consensus is an untraditional viewpoint.
Schechter was an early advocate of Zionism. He was the chairman of the committee that edited the Jewish Publication Society of America Version of the Hebrew Bible.
Legacy.
Schechter's name is synonymous with the findings of the Cairo Geniza. He placed the JTSA on an institutional footing strong enough to endure for over a century. He became identified as the foremost personality of Conservative Judaism and is regarded as its founder. A network of Conservative Jewish day schools is named in his honor, as well as a summer camp in Olympia, Washington. There are several dozen Solomon Schechter Day Schools across the United States and Canada.

</doc>
<doc id="29177" url="https://en.wikipedia.org/wiki?curid=29177" title="Spaghetti Western">
Spaghetti Western

Spaghetti Western, also known as Italian Western or Macaroni Western (primarily in Japan), is a broad subgenre of Western films that emerged in the mid-1960s in the wake of Sergio Leone's film-making style and international box-office success. The term was used by American critics and other countries because most of these Westerns were produced and directed by Italians.
According to veteran Spaghetti Western actor Aldo Sambrell, the phrase 'Spaghetti Western' was coined by Spanish journalist Alfonso Sánchez. The denomination for these films in Italy is western all'italiana (Italian-Style Western). Italo-Western is also used, especially in Germany. The term Eurowesterns may be used to also include Western movies that were produced in Europe but not called Spaghetti Westerns, like the West German Winnetou films or Ostern Westerns. The majority of the films were international co-productions between Italy and Spain, and sometimes France, Germany, Yugoslavia, or the United States.
These movies were originally released in Italian, but as most of the films featured multilingual casts and sound was post-synched, most "western all'italiana" do not have an official dominant language.
The typical Spaghetti Western team was made up of an Italian director, Italo-Spanish technical staff, and a cast of Italian, Spanish, German and American actors, sometimes a fading Hollywood star and sometimes a rising one like the young Clint Eastwood in three of Sergio Leone's films.
Over six hundred European Westerns were made between 1960 and 1980. The best-known Spaghetti Westerns were directed by Sergio Leone and scored by Ennio Morricone, notably the three films of the "Dollars Trilogy" – "A Fistful of Dollars" (1964), "For a Few Dollars More" (1965), and "The Good, the Bad and the Ugly" (1966) – as well as "Once Upon a Time in the West" (1968). In fact these are consistently listed among the best Westerns of any variety.
Common elements.
Sergio Leone's "A Fistful of Dollars" established the Spaghetti Western as a novel kind of Western. In this seminal film the hero enters a town that is ruled by two outlaw gangs and ordinary social relations are non-existent. He betrays and plays the gangs against one another in order to make money. Then he uses his cunning and exceptional weapons skill to assist a family threatened by both gangs. His treachery is exposed and he is severely beaten, but in the end he defeats the remaining gang. The interaction in this story between cunning and irony (the tricks, deceits, unexpected actions and sarcasms of the hero) on the one hand, and pathos (terror and brutality against defenseless people and against the hero after his double play has been revealed) on the other, was aspired to and sometimes attained by the imitations that soon flooded the cinemas.
Italian Cinema often borrowed from other films without regard for infringement and Leone famously borrowed the plot for "Fistful", receiving a letter from Japanese director Akira Kurosawa congratulating him on making "...a very fine film. But it is my film". Leone had wisely imitated one of the most highly respected directors in the world by remaking his film "Yojimbo" as "Fistful" and consequently surrendered Asian rights to Kurosawa, plus 15 percent of the international box office. Leone quickly moved on from borrowing and established his own oft imitated style and plots. Leone's films and other "core" Spaghetti Westerns are often described as having eschewed, criticised or even "demythologized" many of the conventions of traditional US Westerns. This was partly intentional and partly the context of a different cultural background.
Use of pathos received a big boost with Sergio Corbucci's influential "Django". However, in the following years use of cunning and irony became more prominent. This was seen in Leone's next two Westerns, with their emphasis on unstable partnerships. In the last phase of the Spaghetti Western, with the Trinity films, the Leone legacy had been transformed almost beyond recognition, as terror and deadly violence gave way to harmless brawling and low comedy.
Just as seminal and imitated was Ennio Morricone's music from "Fistful", that expresses a similar duality between quirky and unusual sounds and instruments on the one hand and sacral dramatizing for the big confrontation scenes, on the other.
Filming locations.
Most Spaghetti Westerns were made on low budgets, using inexpensive locales. Many of the stories take place in the semiarid landscapes of the American Southwest and Northern Mexico, so a popular setting was the Tabernas Desert in the Province of Almería in southeastern Spain, at the studios of Texas Hollywood, Mini Hollywood, and Western Leone. Other filming locations used were in central and southern Italy, such as the parks of Valle del Treja (between Rome and Viterbo), the area of Camposecco (next to Camerata Nuova, characterized by a karst topography), the hills around Castelluccio, the area around the Gran Sasso mountain, and the Tivoli's quarries and Sardinia.
Reception.
In the 1960s, critics recognized that the American genres were rapidly changing. The genre most identifiably American, the Western, seemed to be evolving into a new rougher beast. For many critics, Sergio Leone's films were part of the problem. Leone's "Dollars Trilogy" (1964–1966) was not the beginning of the "Spaghetti Western" cycle in Italy, but for Americans Leone's films represented the true beginning of the Italian invasion of their privileged cultural form.
Sir Christopher Frayling, in his noted book on the Italian Western, describes American critical reception of the Spaghetti Western cycle as, to "a large extent, confined to a sterile debate about the 'cultural roots' of the American/Hollywood Western." He remarks that few critics dared admit that they were, in fact, "bored with an exhausted Hollywood genre."
Pauline Kael, she notes, was willing to acknowledge this critical ennui and thus appreciate how a film such as Akira Kurosawa's "Yojimbo" (1961) "could exploit the conventions of the Western genre, while debunking its morality." Frayling and other film scholars such as Bondanella argue that this revisionism was the key to Leone's success and, to some degree, to that of the Spaghetti Western genre as a whole.
Rise and fall.
European Westerns from the beginning.
European Westerns are as old as filmmaking itself. The Lumière brothers made their first public screening of films in 1895 and already in 1896 Gabriel Veyre shot "Repas d'Indien" ("Indian Banquet") for them. Joe Hamman starred as Arizona Bill in films made in the French horse country of Camargue 1911–12.
In Italy, the American West as a dramatic setting for spectacles goes back at least as far as Giacomo Puccini's 1910 opera "La fanciulla del West"; it is sometimes considered to be the first Spaghetti Western.
The first Italian Western movie was "La Vampira Indiana" (1913) – a combination of Western and vampire film. It was directed by Vincenzo Leone, father of Sergio Leone, and starred his mother Bice Walerian in the title role as Indian princess fatale. The Italians also made Wild Bill Hickok films, while the German twenties saw back-woods Westerns featuring Bela Lugosi as Uncas.
Of the Western-related European films before 1964, the one attracting most attention is probably Luis Trenker’s "Der Kaiser von Kalifornien" (1936), about John Sutter. During and after the Second World War there were scattered European uses of Western settings, mostly for comedy or even musical comedy.
The first Spaghetti Western.
The first American-British western filmed in Spain was "The Sheriff of Fractured Jaw" (1958), directed by Raoul Walsh. It was followed in 1961 by "Savage Guns", this time a British-Spanish western, again filmed in Spain. This marked the beginning of Spain as a suitable film shooting location for any kind of European western.
In Italy a cycle of Western comedies was initiated 1959 with "La sceriffa" and "Il terrore dell’Oklahoma", followed by other films starring comedy specialists like Walter Chiari, Ugo Tognazzi, Raimondo Vianello or Fernandel. An Italian critic has compared these comedies to American Bob Hope vehicles.
In 1963, three non-comedy Italo-Spanish westerns were produced: "Gunfight at Red Sands", "Magnificent Three" and "Gunfight at High Noon". On the other hand, in 1961 an Italian company co-produced the French "Taste of Violence", with a Mexican Revolution theme.
Since there is no real consensus about where to draw the exact line between Spaghetti Westerns and other Eurowesterns (or other Westerns in general) one cannot say which one of the films mentioned so far really was the first Spaghetti Western. However, it is obvious that 1964 saw the breakthrough of this genre, with more than twenty productions or co-productions from Italian companies and also more than half a dozen Westerns by Spanish or Spanish/American companies. Furthermore, by far the most commercially successful of this lot was Sergio Leone's "A Fistful of Dollars" whose innovations in cinematic style, music, acting, and story decided the future for the genre.
The impact of "A Fistful of Dollars".
The Spaghetti Western was born, flourished and faded in a highly commercial production environment. The Italian "low" popular film production was basically low-budget and low-profit, and the easiest way to success was imitating a proven success. When the typically low-budget production "A Fistful of Dollars" turned into a remarkable box office success, the industry eagerly lapped up its innovations. Most succeeding Spaghetti Westerns tried to get a ragged, laconic hero with superhuman weapon skill, preferably one who looked like Clint Eastwood: Franco Nero, John Garko and Terence Hill started out that way; Anthony Steffen and others stayed that way all their Spaghetti Western career.
Whoever the hero was, he would join an outlaw gang to further his own secret agenda, like in "A Pistol for Ringo", "Blood for a Silver Dollar", "Vengeance Is a Dish Served Cold", "Payment in Blood" and others, while "Beyond the Law" instead has a bandit infiltrate society and become a sheriff. There would be a flamboyant Mexican bandit (Gian Maria Volontè from "A Fistful of Dollars", otherwise Tomas Milian or most often Fernando Sancho) and a grumpy old man – more often than not an undertaker, to serve as sidekick for the hero. For love interest, rancher's daughters, schoolmarms and barroom maidens were overshadowed by young Latin women (sometimes mothers) desired by dangerous men, where actresses like Nicoletta Machiavelli or Rosalba Neri carried on Marianne Koch's role of Marisol in the Leone film. The terror of the villains against their defenseless victims became just as ruthless as in "A Fistful of Dollars", or more, and their brutalization of the hero when his treachery is disclosed became just as merciless, or more – just like the cunning used to secure the latter's retribution.
In the beginning some films mixed some of these new devices (more or less uneasily) with the borrowed US Western devices typical for most of the 1963–64 Spaghetti Westerns. For example, already in Sergio Corbucci's "Minnesota Clay" (1964) that appeared only two months after "A Fistful of Dollars", you find an American style "tragic gunfighter" hero confronting two evil gangs, one Mexican and one Anglo, and (just as in "A Fistful of Dollars") the leader of the latter is also the town sheriff.
In the same director's "Johnny Oro" (1966) a traditional Western sheriff and a half-breed bounty killer are forced into an uneasy alliance when Mexican bandits and Indians (!) together assault the town. In "A Pistol for Ringo" a traditional sheriff commissions a money-oriented hero – played by Giuliano Gemma with more pleasing manners than the Eastwood character but just as devious and deadly – to (typically) infiltrate a gang of Mexican bandits whose leader is played by (typically) Fernando Sancho.
"For a Few Dollars More" and its followers.
Likewise, after 1965 when Leone's second Western "For a Few Dollars More" brought a still larger box office bonanza, bounty killer suddenly became the choice profession of Spaghetti Western heroes in films like "Arizona Colt", "Vengeance is Mine", "Ten Thousand Dollars for a Massacre", "The Ugly Ones", "Dead Men Don't Count" and "Any Gun Can Play". In "The Great Silence" and "A Minute to Pray, a Second to Die", the heroes instead fight bounty killers.
This was also the time when every other hero or villain in Spaghetti Westerns started carrying a musical watch, after its ingenious use in "For a Few Dollars More".
Spaghetti Westerns also began featuring a pair of different heroes. In Leone's film Eastwood's character is an unshaven bounty hunter, dressed similarly to his character in "A Fistful of Dollars", who enters an unstable partnership with Mortimer, an older bounty killer who uses more sophisticated weaponry and wears a suit (Lee Van Cleef). In the end he turns out to also be an avenger. In the following years there was a deluge of Spaghetti Westerns with a pair of heroes with (most often) conflicting motives.
Some examples: a lawman and an outlaw ("And the Crows Will Dig Your Grave"), an army officer and an outlaw ("Bury Them Deep"), an avenger and a (covert) army officer ("The Hills Run Red"), an avenger and a (covert) guilty party ("Viva! Django"), an avenger and a con-man ("The Dirty Outlaws"), an outlaw posing as a sheriff and a bounty hunter ("Man With the Golden Pistol") and (even) an outlaw posing as his twin and a bounty hunter posing as a sheriff ("Few Dollars for Django").
The theme of age in "For a Few Dollars More", where the younger bounty killer eventually bests his more experienced colleague, is taken up in "Day of Anger" and "Death Rides a Horse". In both cases Lee Van Cleef carries on as the older hero versus Giuliano Gemma and John Phillip Law, respectively.
Zapata Westerns.
One variant of the hero pair was a revolutionary Mexican bandit (or semi-bandit) and a (mostly) money-oriented Anglo. These films are sometimes called Zapata Westerns. The first was Damiano Damiani's "A Bullet for the General" and then followed Sergio Sollima's trilogy: "The Big Gundown", "Face to Face" and "Run, Man, Run".
Sergio Corbucci's "The Mercenary" and "Compañeros" also belong here, as does "Tepepa" by Giulio Petroni – among others. Many of these films enjoyed both good takes at the box office and attention from critics. They are often interpreted as a leftist critique of the typical Hollywood handling of Mexican revolutions, and of imperialism in general.
Betrayal stories.
In Leone's "The Good, the Bad and the Ugly" there is still the scheme of a pair of heroes vs. a villain but it is somewhat relaxed, as here all three parties were driven by a money motive. In subsequent films like "Any Gun Can Play", "One Dollar Too Many" and "Kill Them All and Come Back Alone" several main characters repeatedly form alliances and betray each other for monetary gain.
"Sabata" and "If You Meet Sartana Pray for Your Death", directed by Gianfranco Parolini, introduce into similar betrayal environments a kind of hero molded on the Mortimer character from "For a Few Dollars More", only without any vengeance motive and with more outrageous trick weapons. Fittingly enough Sabata is performed by Lee Van Cleef himself, while John Garko plays the very similar Sartana protagonist. Parolini made some more Sabata movies while Giuliano Carnimeo made a whole series of Sartana films with Garko.
"Django" and the tragic heroes.
Beside the first three Spaghetti Westerns by Leone, a most influential film was Sergio Corbucci's "Django" starring Franco Nero. This hero (who increases the violence by mowing down his enemies with a machine gun and later having his hands broken by horse hoofs) is torn between several motives – money or revenge – and his choices bring misery to him and to a woman close to him. Indicative of this film's influence on the Spaghetti Western style, Django is the hero's name in a plenitude of subsequent westerns.
Even though his character is not named Django (in the Italian versions, that is), Franco Nero brings a similar ambience to "Texas, Adios" and "Massacre Time" where the hero must confront surprising (and dangerous) family relations. Similar "prodigal son" stories followed, including "Chuck Moll", "Keoma", "The Return of Ringo", "The Forgotten Pistolero", "One Thousand Dollars on the Black", "Johnny Hamlet" and also "Seven Dollars on the Red" (where the hero is a father).
Another type of wronged hero is set up and must clear himself from accusations. Giuliano Gemma starred in a series of successful films carrying this theme – "Adiós gringo", "For a Few Extra Dollars", "I lunghi giorni della vendetta", "Wanted", and to some extent "Blood for a Silver Dollar" – where most often his character is called "Gary".
The wronged hero who becomes an avenger appears in many Spaghetti Westerns. Among the more commercially successful films with a hero dedicated to vengeance – "For a Few Dollars More", "Once Upon a Time in the West", "Today We Kill… Tomorrow We Die!", "A Reason to Live, a Reason to Die", "Death Rides a Horse", "Viva Django", "The Devil's Backbone", "Hate for Hate", "Greatest Robbery in the West" – those with whom he cooperates typically have conflicting motivations.
Comedy Westerns.
In 1968, the wave of Spaghetti Westerns reached its crest, comprising one-third of the Italian film production, only to collapse to one-tenth in 1969. However, the considerable box office success of Enzo Barboni's "They Call Me Trinity" and the pyramidal one of its follow-up "Trinity Is Still My Name" gave Italian filmmakers a new model to emulate. The main characters were played by Terence Hill and Bud Spencer, who had already cooperated as hero pair in the "old style" Spaghetti Westerns "God Forgives... I Don't!", "Ace High" and "Boot Hill" directed by Giuseppe Colizzi. The Barboni films are burlesque comedies that replace gunplay and blood with set piece brawls. They feature the quick but lazy Trinity (Hill) and his big, strong and irritable brother Bambino (Spencer).
The stories make fun of U.S. Western-style diligent farmers and Spaghetti Western-style bounty hunters. There was a wave of Trinity-inspired films with quick and strong heroes, the former kind often called Trinity or perhaps coming from "a place called Trinity", and with no (or few) killings. Because the two model stories contained religious pacifists to account for the absence of gunplay, all the successors contained religious groups or at least priests (sometimes as one of the heroes). The Latino presence – Mexican bandits and peons – became marginalized or disappeared.
The music for the two Trinity westerns (composed by Franco Micalizzi and Guido & Maurizio De Angelis, respectively) also reflected the change into a lighter and more sentimental mood. As can be expected, the Trinity-inspired films also adopted this style.
Some critics deplore these post-Trinity films as a degeneration of the "real" Spaghetti Westerns, and it is true that Hill's and Spencer's skilful use of body language was a hard act to follow. It is significant that the most successful of the post-Trinity films featured Hill ("Man of the East", "A Genius, Two Partners and a Dupe"), Spencer ("It Can Be Done Amigo") and a pair of Hill/Spencer look-alikes in "Carambola". Spaghetti Western old hand Franco Nero also worked in this subgenre with "Cipolla Colt" and Tomas Milian plays an outrageous "quick" bounty hunter modeled on Charlie Chaplin's Little Tramp in "Sometimes Life Is Hard – Right Providence?" and "Here We Go Again, Eh, Providence?"
Twilight of the Spaghetti Western.
Leone's later Westerns "Once Upon a Time in the West", "Duck, You Sucker!" and the produced and co-directed "My Name is Nobody" in 1973 did very well at the Italian box-office but did not inspire the industry to imitations like his first three did. In fact, "Duck, You Sucker!" has been interpreted as a critical comment on the Zapata Westerns and "My Name is Nobody" includes Terence Hill as a Trinity-like character.
By the mid-seventies a few productions, like "Keoma" and "Four of the Apocalypse", tried to revive the pre-Trinity formulas but basically the Spaghetti Western was dead as an active genre.
Later years have seen some "return of stories" "Django 2" with Franco Nero and "Troublemakers" with Terence Hill and Bud Spencer.
Other notable films.
Some movies that were not very successful at the (Italian) box office still earn a "cult" status in some segment of the audience because of certain exceptional features in story and/or presentation. One "cult" Spaghetti Western that also has drawn attention from critics is Giulio Questi's "Django Kill". Other "cult" items are Cesare Canevari's "Matalo!", Tony Anthony's "Blindman" and Joaquín Luis Romero Marchent's "Cut-Throats Nine" (the latter among gore film audiences).
Special interest audiences might also nurture a cult of the "Worst", as exemplified in the interest for a director like Ed Wood. His Spaghetti Western equivalent would be the Western œuvre of Demofilo Fidani. The Stranger (1995 film) is essentially, the Woman with No Name, with a motorcycle instead of a horse.
The few Spaghetti Westerns containing historical characters like Buffalo Bill, Wyatt Earp, Billy the Kid etc. mainly appear before "A Fistful of Dollars" had put its mark on the genre. Likewise, and in contrast to the contemporary German Westerns, few films feature Indians. When they appear they are more often portrayed as victims of discrimination than as dangerous foes. The only fairly successful Spaghetti Western with an Indian main character (played by Burt Reynolds in his only European Western outing) is Sergio Corbucci's "Navajo Joe", where the Indian village is wiped out by bandits during the first minutes, and the avenger hero spends the rest of the film dealing mostly with Anglos and Mexicans until the final showdown at an Indian burial ground.
Several Spaghetti Westerns are inspired by classical myths and dramas. Titles like "Fedra West" (also called "Ballad of a Bounty Hunter") and "Johnny Hamlet" signify the connection to the Greek myth and possibly the plays by Euripides and Racine and the play by William Shakespeare, respectively. The latter also inspired "Dust in the Sun", which follows its original more closely than Johnny Hamlet, where the hero survives. "The Forgotten Pistolero" is based on the vengeance of Orestes. There are similarities between the story of "The Return of Ringo" and the last canto of Homer's "Odyssey". "Fury of Johnny Kid" follows Shakespeare's "Romeo and Juliet", but (again) with a different ending – the loving couple leave together while their families annihilate each other.
It is acknowledged that the story of "A Fistful of Dollars" closely resembles Akira Kurosawa's "Yojimbo". The less well-known "Requiem for a Gringo" shows many traces from another well-known Japanese film, Masaki Kobayashi's "Harakiri". When Asian martial arts films started to draw crowds in European cinema houses, the producers of Spaghetti Westerns tried to hang on, this time not by adapting story-lines but rather by directly including martial arts in the films, performed by Eastern actors – for example Chen Lee in "My Name Is Shanghai Joe" or Lo Lieh teaming up with Lee Van Cleef in "The Stranger and the Gunfighter".
Some Italian Western films were made as vehicles for musical stars, like Ferdinando Baldi's "Rita of the West" featuring Rita Pavone and Terence Hill. In non-singing roles were Ringo Starr as a villain in "Blindman" and French rock 'n' roll veteran Johnny Hallyday as the gunfighter/avenger hero in Sergio Corbucci's "The Specialists".
A celebrity from another sphere of culture is Italian author/film director Pier Paolo Pasolini, who plays a revolutionary man of the church in "Requiescant". This film concerns oppression of poor Mexicans by rich Anglos and ends on a call for arms but it does not fit easily as a Zapata Western. The same can be said for "The Price of Power", a political allegory where an American president is assassinated in Dallas by a conspiracy of Southern racists who frame an innocent Afro-American. They are opposed by an unstable partnership between a whistle-blower (Giuliano Gemma) and a political aide.
Though the Spaghetti Westerns from "A Fistful of Dollars" and on featured more violence and killings than earlier American Western films, they generally shared the parental genre's restrictive attitude toward explicit sexuality. However, in response to the growing commercial success of various shades of sex films, there was a greater exposure of naked skin in some Spaghetti Westerns, among others "Dead Men Ride" and "Heads or Tails". In the former and (partly) the latter, the sex scenes feature coercion and violence against women.
Even though it is hinted at in some films, like "Django Kill" and "Requiescant", open homosexuality plays a marginal part in Spaghetti Westerns. The exception is Giorgio Capitani's "The Ruthless Four" – in effect a gay version of John Huston's "The Treasure of the Sierra Madre" – where the explicit homosexual relation between two of its (male) main characters and some gay cueing scenes are embedded with other forms of man-to-man relations through the story.
Legacy.
Spaghetti Westerns have left their mark on popular culture, strongly influencing numerous works produced outside of Italy.
Clint Eastwood's first American Western film, "Hang 'Em High", incorporates elements of Spaghetti Westerns.
The 1985 Japanese film "Tampopo" was promoted as a "ramen Western".
The Bollywood film "Sholay" was often referred to as a "curry western". Japanese director Takashi Miike paid tribute to the genre with "Sukiyaki Western Django", a Western set in Japan which drives influence from both "Django" and Sergio Leone's "Dollars Trilogy".
American director Quentin Tarantino has utilized elements of Spaghetti Westerns in his films "Kill Bill" (combined with kung fu movies), "Inglourious Basterds" (set in Nazi-occupied France) and "Django Unchained" (set in the American South during the time of slavery).
The American animated film "Rango" incorporates elements of Spaghetti Westerns, including a character modeled after The Man With No Name.
The American heavy metal band Metallica has used Ennio Morricone's composition "The Ecstacy of Gold", from the Spaghetti Western "The Good, The Bad and the Ugly", to open several of their concerts. The Australian band The Tango Saloon combines elements of Tango music with influences from Spaghetti Western scores. The psychobilly band Ghoultown also derives influence from Spaghetti Westerns. The music video for the song "Knights of Cydonia" by the English rock band Muse was influenced by Spaghetti Westerns. The band Big Audio Dynamite used music samples from Spaghetti Westerns when mixing their song "Medicine Show". Within the song you can hear samples from Spaghetti Western movies such as, "A Fistful of Dollars", "The Good, The Bad and The Ugly", and "Duck You Sucker".

</doc>
<doc id="29178" url="https://en.wikipedia.org/wiki?curid=29178" title="Spaghetti">
Spaghetti

Spaghetti () is a long, thin, cylindrical, solid pasta. It is a staple food of traditional Italian cuisine. Like other pasta, spaghetti is made of milled wheat and water. Italian spaghetti is made from durum wheat semolina, but elsewhere it may be made with other kinds of flour.
Originally spaghetti was notably long, but shorter lengths gained in popularity during the latter half of the 20th century and now spaghetti is most commonly available in lengths. A variety of pasta dishes are based on it.
Etymology.
"Spaghetti" is the plural form of the Italian word "spaghetto", which is a diminutive of "spago", meaning "thin string" or "twine".
History.
Pasta in the West may have first been worked into long, thin forms in Sicily around the 12th century, as the Tabula Rogeriana of Muhammad al-Idrisi attested, reporting some traditions about the Sicilian kingdom. In the 5th century AD, it was known that pasta could be cooked through boiling. The popularity of spaghetti spread throughout Italy after the establishment of spaghetti factories in the 19th century, enabling the mass production of spaghetti for the Italian market.
In the United States around the end of the 19th century, spaghetti was offered in restaurants as "Spaghetti Italienne" (which likely consisted of noodles cooked past "al dente", and a mild tomato sauce flavored with easily found spices and vegetables such as cloves, bay leaves, and garlic) and it was not until decades later that it came to be commonly prepared with oregano or basil.
Ingredients.
Spaghetti is made from ground grain (flour) and water. Whole-wheat and multigrain spaghetti are also available.
Production.
Fresh spaghetti.
At its simplest, spaghetti can be formed using no more than a rolling pin and a knife. A home pasta machine simplifies the rolling, and makes the cutting more uniform. Fresh spaghetti would normally be cooked within hours of being formed. Commercial versions of 'fresh' spaghetti are manufactured.
Dried spaghetti.
The bulk of dried spaghetti is produced in factories using auger extruders. While essentially simple, the process requires attention to detail to ensure that the mixing and kneading of the ingredients produces a homogeneous mix, without air bubbles. The forming dies have to be water cooled to prevent spoiling of the pasta by overheating. Drying of the newly formed spaghetti has to be carefully controlled to prevent strands sticking together, and to leave it with sufficient moisture so that it is not too brittle. Packaging for protection and display has developed from paper wrapping to plastic bags and boxes.
Preparation.
Fresh and dry spaghetti is cooked in a large pot of salted, boiling water and then drained in a colander ().
In Italy, spaghetti is generally cooked "al dente" (Italian for "to the tooth"), fully cooked and still firm, it may also be cooked to a softer consistency.
"Spaghettoni" is a thicker spaghetti which takes more time to cook. "Spaghettini" is a very thin form of spaghetti (it may be called "angel hair spaghetti" in English) which takes less time to cook.
Utensils used in spaghetti preparation include the spaghetti scoop and spaghetti tongs.
"Spaghetti" can also be cooked successfully in a microwave oven.
Serving.
Italian cuisine.
An emblem of Italian cuisine, spaghetti is frequently served with tomato sauce, which may contain various herbs, (especially oregano and basil), olive oil, meat, or vegetables. Other spaghetti preparations include amatriciana or carbonara. Grated hard cheeses, such as Pecorino Romano, Parmesan and Grana Padano, are often sprinkled on top.
International cuisine.
In some countries, spaghetti is sold in cans/tins with sauce.
In the United States, it is sometimes served with chili con carne. Unlike in Italy, abroad spaghetti is often served with Bolognese sauce.
"Sapaketti phat khi mao" (Spaghetti fried drunken noodle style) is a popular dish in Thai cuisine.
Market.
Consumption.
By 1955, annual consumption of spaghetti in Italy doubled from per person before World War II to . By that year, Italy produced 1,432,990 tons of spaghetti, of which 74,000 were exported, and had a production capacity of 3 million tons.
Nutrition.
Pasta provides carbohydrate, along with some protein, iron, dietary fiber, potassium and B vitamins. Pasta prepared with whole wheat grain provides more dietary fiber than that prepared with degermed flour.
Records.
The world record for the largest bowl of spaghetti was set in March 2009 and reset in March 2010 when a Buca di Beppo restaurant in Garden Grove, California, filled a swimming pool with more than of pasta.
In popular culture.
Spaghetti Westerns have little to do with spaghetti other than using the name as a shorthand for Italian.
The BBC television program 'Panorama' featured a hoax program about the spaghetti harvest in Switzerland on April Fools' Day, 1957.

</doc>
<doc id="29180" url="https://en.wikipedia.org/wiki?curid=29180" title="System Shock">
System Shock

System Shock is a 1994 first-person action role-playing video game developed by Looking Glass Technologies and published by Origin Systems. It was directed by Doug Church with Warren Spector serving as producer. The game is set aboard a space station in a cyberpunk vision of the year 2072. Assuming the role of a nameless hacker, the player attempts to hinder the plans of a malevolent artificial intelligence called SHODAN.
"System Shock" 3D engine, physics simulation and complex gameplay have been cited as both innovative and influential. The developers sought to build on the emergent gameplay and immersive environments of their previous games, ' and ', by streamlining their mechanics into a more "integrated whole".
Critics praised "System Shock" and hailed it as a major breakthrough in its genre. It was later placed on multiple hall of fame lists. The game was a moderate commercial success, with sales exceeding 170,000 copies, but Looking Glass ultimately lost money on the project. A sequel, "System Shock 2", was released by Looking Glass Studios and offshoot developer Irrational Games in 1999. The 2000 game "Deus Ex" and the 2007 game "BioShock" are spiritual successors to the two games.
Gameplay.
"System Shock" takes place from a first-person perspective in a three-dimensional (3D) graphical environment. The game is set inside a large, multi-level space station, in which players explore, combat enemies and solve puzzles. Progress is largely non-linear and the game is designed to allow for emergent gameplay. As in "", the player uses a freely movable mouse cursor to aim weapons, to interact with objects and to manipulate the heads-up display (HUD) interface. View and posture controls on the HUD allow the player to lean left or right, look up or down, crouch, and crawl. Practical uses for these actions include taking cover, retrieving items from beneath the player character and navigating small passages, respectively. The HUD also features three "Multi-Function Displays", which may be configured to display information such as weapon readouts, an automap and an inventory.
The player advances the plot by acquiring log discs and e-mails: the game contains no non-player characters with which to converse. Throughout the game, an evil artificial intelligence called SHODAN hinders the player's progress with traps and blocked pathways. Specific computer terminals allow the player to temporarily enter Cyberspace; inside, the player moves weightlessly through a wire frame 3D environment, while collecting data and fighting SHODAN's security programs. Actions in Cyberspace sometimes cause events in the game's physical world; for example, certain locked doors may only be opened in Cyberspace. Outside of Cyberspace, the player uses the game's sixteen weapons, of which a maximum of seven may be carried at one time, to combat robots, cyborgs and mutants controlled by SHODAN. Projectile weapons often have selectable ammunition types with varying effects; for example, the "dart pistol" may fire either explosive needles or tranquilizers. Energy weapons and several types of explosives may also be found, with the latter ranging from percussion grenades to land mines.
Along with weapons, the player collects items such as dermal patches and first-aid kits. Dermal patches provide the character with beneficial effects—such as regeneration or increased melee attack power—but can cause detrimental side-effects, such as fatigue and distorted color perception. Attachable "hardware" may also be found, including energy shields and head-mounted lanterns. Increasingly advanced versions of this hardware may be obtained as the game progresses. When activated, most hardware drains from a main energy reserve, which necessitates economization. Certain hardware displays the effectiveness of attacks when active, with messages such as "Normal damage". When an enemy is attacked, the damage is calculated by armor absorption, vulnerabilities, critical hits and a degree of randomness. Weapons and munitions deal specific kinds of damage, and certain enemies are immune, or more vulnerable, to particular types. For example, electromagnetic pulse weapons heavily damage robots, but do not affect mutants. Conversely, gas grenades are effective against mutants, but do not damage robots.
Plot.
In the game's prologue, the protagonist—a nameless hacker—is caught while attempting to access files concerning Citadel Station, a space station owned by the fictional TriOptimum Corporation. The hacker is taken to Citadel Station and brought before Edward Diego, a TriOptimum executive. Diego offers to drop all charges against the hacker in exchange for a confidential hacking of SHODAN, the artificial intelligence that controls the station. Diego secretly plans to steal an experimental mutagenic virus being tested on Citadel Station, and to sell it on the black market as a biological weapon. To entice cooperation, Diego promises the hacker a valuable military grade neural implant. After hacking SHODAN, removing the AI's ethical constraints, and handing control over to Diego, the protagonist undergoes surgery to implant the promised neural interface. Following the operation, the hacker is put into a six-month healing coma. The game begins as the protagonist awakens from his coma, and finds that SHODAN has commandeered the station. All robots aboard have been reprogrammed for hostility, and the crew have been either mutated, transformed into cyborgs, or killed.
Rebecca Lansing, a TriOptimum counter-terrorism consultant, contacts the player and claims that Citadel Station's mining laser is being powered up to attack Earth. SHODAN's plan is to destroy all major cities on the planet, in a bid to become a kind of god. Rebecca says that a certain crew member knows how to deactivate the laser, and promises to destroy the records of the hacker's incriminating exchange with Diego if the strike is stopped. With information gleaned from log discs, the hacker destroys the laser by firing it into Citadel Station's own shields. Foiled by the hacker's work, SHODAN prepares to seed Earth with a mutagenic virus—the same one responsible for turning the station's crew into mutants. The hacker, while attempting to jettison the chambers used to cultivate the virus, confronts and defeats Diego, who has been transformed into a powerful cyborg by SHODAN. Next, SHODAN begins an attempt to download itself into Earth's computer networks. Following Rebecca's advice, the hacker prevents the download's completion by destroying the four antennas that SHODAN is using to send data.
Soon after, Rebecca contacts the hacker, and says that she has convinced TriOptimum to authorize the station's destruction; she provides him with details on how to do this. After obtaining the necessary codes, the hacker initiates the station's self-destruct sequence and flees to the escape pod bay. There, the hacker defeats Diego a second time, then attempts to disembark. However, SHODAN prevents the pod from launching; it seeks to keep the player aboard the station, while the bridge—which contains SHODAN—is jettisoned to a safe distance. Rebecca tells the hacker that he can still escape if he reaches the bridge; SHODAN then intercepts and jams the transmission. After defeating Diego for the third time and killing him for good, the hacker makes it to the bridge as it is released from the main station, which soon detonates. He is then contacted by a technician who managed to circumvent SHODAN's jamming signal. The technician informs him that SHODAN can only be defeated in cyberspace, due to the powerful shields that protect its mainframe computers. Using a terminal near the mainframe, the hacker enters cyberspace and destroys SHODAN. After his rescue, the hacker is offered a job at TriOptimum, but he declines in favor of continuing his life as a hacker.
Development.
Initial design.
"System Shock" was first conceived during the final stages of "" development, between December 1992 and January 1993. Designer and programmer Doug Church spent this period at the Texas headquarters of publisher Origin Systems, and discussions about Looking Glass Technologies' next project occurred between him and producer Warren Spector, with input from designer Austin Grossman and company head Paul Neurath in Massachusetts. According to Church, the team believed that they had made "too many dungeon games"; and Neurath later explained that they were experiencing burnout after the rushed development of "Ultima Underworld II". As a result, they decided to create another "immersive simulation game", but without a fantasy setting. They briefly considered placing the game in modern day, but Church said that the idea was rejected because "it just beg so many questions: why can't I pick up the phone, why can't I get on the train, and so on". Church returned to Looking Glass in Massachusetts, where he, Neurath and Grossman brainstormed possible science fiction settings for the game. According to Spector, the game was initially titled ""Alien Commander"" and was a spin-off of the "Wing Commander" series; however, this idea was soon replaced entirely. Spector said that they enjoyed not being attached to an existing franchise, because it meant that they "could basically do whatever [they liked".
The four collaborated to write numerous "minutes of gameplay" documents, which conveyed how the game would feel. Church later gave the example, "You hear the sound of a security camera swiveling, and then the beep of it acquiring you as a target, so you duck behind the crate and then you hear the door open so you throw a grenade and run out of the way". The documents would "hint" at the gameplay systems involved, and at the emergent possibilities in each situation. Although Neurath was involved in these initial design sessions, he believed that the project "was always Doug Church's vision at heart". Church and Grossman refined several of the team's documents and defined the game's design and direction, and Grossman wrote the game's original design document. Grossman built on ideas that he first explored while writing and designing "Ultima Underworld II"'s tomb dimension, which he later called a "mini-prototype" for "System Shock". These concepts included the minimization of dialogue trees and a greater focus on exploration. The team believed that dialogue trees "broke the fiction" of games; Church later commented that the dialogue trees in the "Ultima Underworld" series were like separate games in themselves, disconnected from main experience of being immersed in the environment. There were also concerns about realism.
To eliminate dialogue trees from "System Shock", the team prevented the player from ever meeting a living non-player character (NPC): the plot is instead conveyed by e-mail messages and log discs, many of which were recorded by dead NPCs. Here, Grossman took influence from Edgar Lee Masters' "Spoon River Anthology", a collection of poems written as the epitaphs of fictional individuals. Grossman later summarized the idea as "a series of short speeches from people, that when put together, gave you a history of a place." The removal of conversations was an attempt by the team to make the game a more "integrated whole" than was "Ultima Underworld"--one with a greater focus on immersion, atmosphere and "the feeling of 'being there'". They sought to "plunge into the fiction and never provide an opportunity for breaking that fiction"; and so they tried to erase the distinction between plot and exploration. Church considered this direction to be an organic progression from "Ultima Underworld", and he later said, "On some level it's still just a dungeon simulator, and we're still just trying to evolve that idea." Shortly before production began, Tribe bassist Greg LoPiccolo was contracted to work on the game's music. He had visited his friend Rex Bradford at the company, and was spontaneously asked by the game's programmers—many of whom were fans of the band—if he would take the role. The game entered production in February 1993. Although Grossman was heavily involved in the game's early planning, he had little to do with its production, aside from providing assistance with writing and voice acting.
Production.
After production began, the team's first task was to develop a new game engine—one that could display a true 3D environment and allow for advanced gameplay. The team abandoned the engine used for the "Ultima Underworld" games and coded one from scratch in Watcom C/C++, using 32-bit code. The new engine is capable of processing texture maps, sloped architecture and light-emitting objects; and it allows the player to look in any direction, whereas "Ultima Underworld"'s engine was "very limited" in this regard. It also enables the player character to jump, crawl, climb walls and lean, among other things. The designers utilized loopholes in the engine's renderer to create more diverse and striking environments. Despite having coded the renderer, Church said that "at first glance even I couldn't see how they did them". However, this added to the performance issues already being caused by the engine's advanced nature, and the team struggled to optimize the game throughout development. 3D polygonal character models were planned, but they could not be implemented on schedule. Church said that the team's ultimate goal was to create a "rich, exciting, active environment" in which the player could be immersed, and that this required "a coherent story and a world that you can interact with as much as possible."
Church later said that the team "stumbled into a nice villain" with SHODAN, in that she could routinely and directly affect the player's gameplay "in non-final ways". Through triggered events and through objects in the environment, such as security cameras that the player must destroy, the team made SHODAN's presence part of the player's exploration of the world. Because SHODAN interacts with the player as a "recurring, consistent, palpable enemy", Church believed that she meaningfully connects the player to the story. "System Shock" concept artist Robb Waters created SHODAN's visual design, and LoPiccolo recruited his bandmate Terri Brosius to voice the character. Brosius said that her goal during the recording sessions was to speak "without emotion, but with some up and down inflections". Afterward, her voice was heavily edited in post-production, which created a robotic effect inspired by the voice of Max Headroom. LoPiccolo later said that the large number of effects on Brosius's voice were "laboriously hand-done" with Sound Designer, which lacked the features that a sound editor would normally use to achieve such results. SHODAN's dialogue early in the game was given "a few glitches" to hint at her corrupted status. LoPiccolo increased the number of these effects throughout the game, which creates an "arc" that ends with SHODAN "completely out of her mind [... and] collapsing as an entity". The character of the hacker arose as a reaction against the protagonist of the "Ultima" series, the Avatar. According to Grossman, they wanted to cast the player as someone "interestingly morally compromised" who had a stake in the situation.
Seamus Blackley designed the game's physics system, which is a modified version of the one he wrote for Looking Glasses' flight simulator Flight Unlimited. At the time, Church described it as "far more sophisticated than what you would normally use for an indoor game". The system governs, among other things, weapon recoil and the arc of thrown objects; the latter behave differently based on their weight and velocity. The game's most complex physics model is that of the player character. Church explained that the character's head "tilts forward when you start to run, and jerks back a bit when you stop", and that, after an impact against a surface or object, its "head is knocked in the direction opposite the hit, with proportion to mass and velocity of the objects involved". On coding physics for Looking Glass Technologies games, Blackley later said, "If games don't obey physics, we somehow feel that something isn't right", and that "the biggest compliment to me is when a gamer doesn't notice the physics, but only notices that things feel the way they should".
Spector's role as a producer gave him the job of explaining the game to the publisher, which he called his "biggest challenge". He explained that they "didn't always get what the team was trying to do", and said, "You don't want to know how many times the game came "this" close to being killed (or how late in the project)". According to Church, Looking Glass' internal management largely ignored "System Shock", in favor of the concurrently-developed "Flight Unlimited"—the game "that had to be the hit, because it was the self-published title". Spector organized a licensing deal between Electronic Arts and Looking Glass that gave the former the trademark to the game, but the latter the copyright. His goal was to ensure that neither party could continue the franchise without the other's involvement. While Cyberspace was originally conceived as a realistic hacking simulation—which could even be used to reimplement SHODAN's ethical constraints—it was simplified after Origin Systems deemed it too complicated. The game's star field system was written by programmer James Fleming. Marc LeBlanc was the main creator of the game's HUD, which he later believed was too complicated. He said that it was "very much the Microsoft Word school of user interface", in that there was no "feature that you not see on the screen and touch and play with".
LoPiccolo composed the game's score—called "dark", "electronic" and "cyberpunk" by the "Boston Herald"—on a Macintosh computer and inexpensive synthesizer, using Audio Vision. It dynamically changes according to the player's actions, a decision made in keeping with the team's focus on emergent gameplay. Each track was "written at three different intensity levels", which change depending on the player's nearness to enemies; and certain events, such as victory in combat, trigger special music. The game's tracks were composed of four-bar segments that could be rearranged dynamically in reaction to game events, with "melodies through-composed on top". LoPiccolo noted that, when using this method, it is necessary to write music that "still flows with the overall theme and doesn't jump around". Because the score was closely tied to the gameplay, LoPiccolo had to work closely with Church and Rob Fermier, the latter of whom wrote the "interactive scoring module" that allowed for dynamic music. After recording the music, LoPiccolo recorded all of the game's sound effects. He later recalled visiting an automobile repair shop with "portable recorder and a mic", and "having mechanic [... hit things with wrenches and so forth, just to get the raw material". He developed the game's audio over 16 months, working on a contractual basis until Tribe disbanded in May 1994; Ned Lerner gave him a full-time job as audio director the next day. Tim Ries composed the "Elevator" music.
The original September 1994 floppy disk release of "System Shock" had no support for spoken dialogue. The enhanced CD-ROM was released in December 1994, which featured full speech for logs and e-mails, multiple display resolutions, and more detailed graphics. The CD-ROM version is often considered to be superior to the floppy version. After completing work on the sound and music for the floppy version, LoPiccolo recorded all of the spoken dialogue for the CD release, using company employees and his friends' voices, which he mixed with ambient sounds to create "audio vignettes". Doug Church later said, "We tried to keep them from shipping the floppy version and instead just ship the CD version, but Origin would have none of it". "System Shock" producer Warren Spector later expressed regret concerning the floppy version, stating, "I wish I could go back and make the decision not to ship the floppy version months before the full-speech CD version. The additional audio added so much it might as well have been a different game. The CD version seemed so much more, well, modern. And the perception of "Shock" was cemented in the press and in people's minds by the floppy version (the silent movie version!). I really think that cost us sales..."
Reception.
"System Shock" was critically acclaimed, and was given high scores by some of the gaming critics. On the review aggregator GameRankings, the game has an average score of 88%. The game sold over 170,000 copies. "Maximum PC" believed that the game did not reach "blockbuster" status, but was successful enough to "keep Looking Glass afloat". GameSpy's Bill Hiles said, "Though it sold well, it never reached the frenzied popularity of ["Doom"]". Paul Neurath later said that the game "was not a flop", but that it ultimately "lost money" for the company, which he attributed to its steep learning curve. "Computer Gaming World" praised the game's scale, physics system, and true 3D environments; the magazine extolled the presentation of Cyberspace as "nothing short of phenomenal". However, the reviewer believed that the game had "little sense of urgency" and "confusing level layouts". "Computer Shopper" wrote that, while the game's controls were difficult to master compared to "simple run-and-shoot game like "Doom"", they were "worth the time and effort". The reviewer noted that the game "grows on you, and it will keep you intrigued for weeks".
The "Boston Herald" noted superficial similarities between "System Shock" and "Doom", but called "System Shock" "much more elaborate". The reviewer noted its high system requirements and complex controls; of the latter, he said, "There's no way you can play "System Shock" without first studying the manual for at least 20 minutes". The paper believed that the game would "set a new standard for computer games with its combination of action and puzzle-solving". "The Atlanta Journal-Constitution" said that the game "is like a well-prepared hamburger—familiar stuff, but good to the last byte". The reviewer noted the game's "somewhat clumsy control", but said, "That, however, is all I can find to complain about. Graphics and sound are outstanding, and the game is well-paced and riveting".
"PC Gamer US" wrote, ""System Shock" smokes. It is the most fully immersive game world I have ever experienced". The reviewer praised the game's story and control system, and believed that "no matter what kind of game you're looking for, you'll find something in "System Shock" to delight you". He finished his review by stating that the game "unquestionably raises computer gaming to a new level". "Next Generation Magazine" summarized the game as "a great blend of strategy and action backed up with all the extras". Various sources have ranked SHODAN as one of the most effective antagonists and female characters in the history of video gaming. In the years following its release, "System Shock" has been inducted into many lists of the best video games of all time, including those by "PC Gamer", GameSpy, and "Computer Gaming World".
"Enhanced Edition".
On September 22, 2015, an updated version of the game titled "System Shock: Enhanced Edition" was released on GOG.com by Night Dive Studios for Microsoft Windows. This version is intended to run on modern systems significantly easier among several other technical improvements such as the original resolution of 320x200, now boosted up to 1024x768 and 854x480 pixels in widescreen mode. The release also includes the original version of the game, titled "System Shock: Classic", with support for Microsoft Windows, OS X and Linux. "System Shock: Enhanced Edition" received very positive reviews. Metacritic calculated an average score of 85 out of 100, and GameRankings assigned it an average review score of 82%. COGconnected said, "If you haven’t played System Shock before, there’s never been a better time. Whether you’re into shooters or RPGs; or just want to experience a cyberpunk romp with a good beat, this one is for you."
Legacy.
In a Gamasutra feature, Patrick Redding of Ubisoft attested that "the fact that so many of "System Shock" features are now virtually "de rigueur" in modern sci-fi shooters is a testament to the influence exerted by this one game." GameSpy argued that the game "is the progenitor of today's story-based action games, a group with titles as diverse as "Metal Gear Solid", "Resident Evil", and even "Half-Life"." Eurogamer called the "System Shock" series "the benchmark for intelligent first-person gaming", and noted that it "kick-startthe revolution which ... has influenced the design of countless other games." The game has been cited as a key popularizer of emergent gameplay. Certain game developers have acknowledged "System Shock" influence on their products. With "Deus Ex", developer Warren Spector revealed a desire to "build on the foundation laid by the Looking Glass guys in games like ... "System Shock"." Developer Ken Levine has commented that the "spirit of "System Shock" is player-powered gameplay: the spirit of letting the player drive the game, not the game designer", and at Irrational Games "... that's always the game we ideally want to make." A sequel to "System Shock", titled "System Shock 2", was released in 1999 to further acclaim and award. The two games were the inspiration behind the 2007 game "BioShock". In late 2015 it was revealed that a third System Shock game was in the works. On February 10, 2016 Night Dive Studios announced that the recovered source code of the game will be released to the game's community.

</doc>
<doc id="29181" url="https://en.wikipedia.org/wiki?curid=29181" title="Spherical coordinate system">
Spherical coordinate system

In mathematics, a spherical coordinate system is a coordinate system for three-dimensional space where the position of a point is specified by three numbers: the radial distance of that point from a fixed origin, its polar angle measured from a fixed zenith direction, and the azimuth angle of its orthogonal projection on a reference plane that passes through the origin and is orthogonal to the zenith, measured from a fixed reference direction on that plane.
The radial distance is also called the radius or radial coordinate. The polar angle may be called co-latitude, zenith angle, normal angle, or inclination angle.
The use of symbols and the order of the coordinates differs between sources. In one system frequently encountered in physics ("r", "θ", "φ") gives the radial distance, polar angle, and azimuthal angle, whereas in another system used in many mathematics books ("r", "θ", "φ") gives the radial distance, azimuthal angle, and polar angle. In both systems "ρ" is often used instead of "r". Other conventions are also used, so great care needs to be taken to check which one is being used. 
A number of different spherical coordinate systems following other conventions are used outside mathematics. In a geographical coordinate system positions are measured in latitude, longitude and height or altitude. There are a number of different celestial coordinate systems based on different fundamental planes and with different terms for the various coordinates. The spherical coordinate systems used in mathematics normally use radians rather than degrees and measure the azimuthal angle counter-clockwise from the x-axis to the y-axis rather than clockwise from North (0 degrees) to the East (+90 degrees) like the horizontal coordinate system. The polar angle is often replaced by the elevation angle measured from the reference plane. Elevation angle of zero is at the horizon.
The spherical coordinate system generalises the two-dimensional polar coordinate system. It can also be extended to higher-dimensional spaces and is then referred to as a hyperspherical coordinate system.
Definition.
To define a spherical coordinate system, one must choose two orthogonal directions, the "zenith" and the "azimuth reference", and an "origin" point in space. These choices determine a reference plane that contains the origin and is perpendicular to the zenith. The spherical coordinates of a point "P" are then defined as follows:
The sign of the azimuth is determined by choosing what is a "positive" sense of turning about the zenith. This choice is arbitrary, and is part of the coordinate system's definition.
The "elevation" angle is 90 degrees (π/2 radians) minus the inclination angle.
If the inclination is zero or 180 degrees (π radians), the azimuth is arbitrary. If the radius is zero, both azimuth and inclination are arbitrary.
In linear algebra, the vector from the origin "O" to the point "P" is often called the "position vector" of "P".
Conventions.
Several different conventions exist for representing the three coordinates, and for the order in which they should be written. The use of ("r", "θ", "φ") to denote radial distance, inclination (or elevation), and azimuth, respectively, is common practice in physics, and is specified by ISO standard 80000-2 :2009, and earlier in ISO 31-11 (1992).
However, some authors (including mathematicians) use "φ" for inclination (or elevation) and "θ" for azimuth, which "provides a logical extension of the usual polar coordinates notation". Some authors may also list the azimuth before the inclination (or elevation), and/or use "ρ" (rho) instead of "r" for radial distance. Some combinations of these choices result in a left-handed coordinate system. The standard convention ("r", "θ", "φ") conflicts with the usual notation for the two-dimensional polar coordinates, where "θ" is often used for the azimuth. It may also conflict with the notation used for three-dimensional cylindrical coordinates.
<ref name="http://mathworld.wolfram.com/SphericalCoordinates.html" />
The angles are typically measured in degrees (°) or radians (rad), where 360° = 2π rad. Degrees are most common in geography, astronomy, and engineering, whereas radians are commonly used in mathematics and theoretical physics. The unit for radial distance is usually determined by the context.
When the system is used for physical three-space, it is customary to use positive sign for azimuth angles that are measured in the counter-clockwise sense from the reference direction on the reference plane, as seen from the zenith side of the plane. This convention is used, in particular, for geographical coordinates, where the "zenith" direction is north and positive azimuth (longitude) angles are measured eastwards from some prime meridian.
Unique coordinates.
Any spherical coordinate triplet ("r", "θ", "φ") specifies a single point of three-dimensional space. On the other hand, every point has infinitely many equivalent spherical coordinates. One can add or subtract any number of full turns to either angular measure without changing the angles themselves, and therefore without changing the point. It is also convenient, in many contexts, to allow negative radial distances, with the convention that (−"r", "θ", "φ") is equivalent to ("r", "θ" + 180°, "φ") for any "r", "θ", and "φ". Moreover, ("r", −"θ", "φ") is equivalent to ("r", "θ", "φ" + 180°).
If it is necessary to define a unique set of spherical coordinates for each point, one may restrict their ranges. A common choice is:
However, the azimuth "φ" is often restricted to the interval , or in radians, instead of . This is the standard convention for geographic longitude.
The range 180° for inclination is equivalent to [−90°, +90°] for elevation (latitude).
Even with these restrictions, if "θ" is zero or 180° (elevation is 90° or −90°) then the azimuth angle is arbitrary; and if "r" is zero, both azimuth and inclination/elevation are arbitrary. To make the coordinates unique, one can use the convention that in these cases the arbitrary coordinates are zero.
Plotting.
To plot a dot from its spherical coordinates ("r", "θ", "φ"), where "θ" is inclination, move "r" units from the origin in the zenith direction, rotate by "θ" about the origin towards the azimuth reference direction, and rotate by "φ" about the zenith in the proper direction.
Applications.
The geographic coordinate system uses the azimuth and elevation of the spherical coordinate system to express locations on Earth, calling them respectively longitude and latitude. Just as the two-dimensional Cartesian coordinate system is useful on the plane, a two-dimensional spherical coordinate system is useful on the surface of a sphere. In this system, the sphere is taken as a unit sphere, so the radius is unity and can generally be ignored. This simplification can also be very useful when dealing with objects such as rotational matrices.
Spherical coordinates are useful in analyzing systems that have some degree of symmetry about a point, such as volume integrals inside a sphere, the potential energy field surrounding a concentrated mass or charge, or global weather simulation in a planet's atmosphere. A sphere that has the Cartesian equation "x"2 + "y"2 + "z"2 = "c"2 has the simple equation "r" = "c" in spherical coordinates.
Two important partial differential equations that arise in many physical problems, Laplace's equation and the Helmholtz equation, allow a separation of variables in spherical coordinates. The angular portions of the solutions to such equations take the form of spherical harmonics.
Another application is ergonomic design, where "r" is the arm length of a stationary person and the angles describe the direction of the arm as it reaches out.
Three dimensional modeling of loudspeaker output patterns can be used to predict their performance. A number of polar plots are required, taken at a wide selection of frequencies, as the pattern changes greatly with frequency. Polar plots help to show that many loudspeakers tend toward omnidirectionality at lower frequencies.
The spherical coordinate system is also commonly used in 3D game development to rotate the camera around the player's position.
In geography.
To a first approximation, the geographic coordinate system uses elevation angle (latitude) in degrees north of the equator plane, in the range −90° ≤ "φ" ≤ 90°, instead of inclination. Latitude is either geocentric latitude, measured at the Earth's center and designated variously by "ψ", "q", "φ"′, "φ"c, "φ"g or geodetic latitude, measured by the observer's local vertical, and commonly designated "φ". The azimuth angle (longitude), commonly denoted by "λ", is measured in degrees east or west from some conventional reference meridian (most commonly the IERS Reference Meridian), so its domain is −180° ≤ "λ" ≤ 180°. For positions on the Earth or other solid celestial body, the reference plane is usually taken to be the plane perpendicular to the axis of rotation. 
The polar angle, which is 90° minus the latitude and ranges from 0 to 180°, is called colatitude in geography.
Instead of the radial distance, geographers commonly use altitude above some reference surface, which may be the sea level or "mean" surface level for planets without liquid oceans. The radial distance "r" can be computed from the altitude by adding the mean radius of the planet's reference surface, which is approximately 6,360 ± 11 km for Earth.
However, modern geographical coordinate systems are quite complex, and the positions implied by these simple formulae may be wrong by several kilometers. The precise standard meanings of latitude, longitude and altitude are currently defined by the World Geodetic System (WGS), and take into account the flattening of the Earth at the poles (about 21 km) and many other details.
In astronomy.
In astronomy there are a series of spherical coordinate systems that measure the elevation angle from different fundamental planes. These reference planes are the observer's horizon, the celestial equator (defined by Earth's rotation), the plane of the ecliptic (defined by Earth's orbit around the Sun), the plane of the earth terminator (normal to the instantaneous direction to the Sun), and the galactic equator (defined by the rotation of the Milky Way).
Coordinate system conversions.
As the spherical coordinate system is only one of many three-dimensional coordinate systems, there exist equations for converting coordinates between the spherical coordinate system and others.
Cartesian coordinates.
The spherical coordinates of a point in the ISO convention ("radius r", "inclination θ", "azimuth φ") can be obtained from its Cartesian coordinates ("x", "y", "z") by the formulae
The inverse tangent denoted in must be suitably defined, taking into account the correct quadrant of ("x","y"). See the article on atan2.
Alternatively, the conversion can be considered as two sequential rectangular to polar conversions: the first in the Cartesian "x"–"y" plane from ("x","y") to ("R","φ"), where "R" is the projection of "r" onto the "x"–"y" plane, and the second in the Cartesian "z"–"R" plane from ("z","R") to ("r","θ"). The correct quadrants for "φ" and "θ" are implied by the correctness of the planar rectangular to polar conversions.
These formulae assume that the two systems have the same origin, that the spherical reference plane is the Cartesian "x"–"y" plane, that "θ" is inclination from the "z" direction, and that the azimuth angles are measured from the Cartesian "x" axis (so that the "y" axis has "φ" = +90°). If "θ" measures elevation from the reference plane instead of inclination from the zenith the arccos above becomes an arcsin, and the cos "θ" and sin "θ" below become switched.
Conversely, the Cartesian coordinates may be retrieved from the spherical coordinates ("radius r", "inclination θ", "azimuth φ"), where , , , by:
Cylindrical coordinates.
Cylindrical coordinates ("radius ρ", "azimuth φ", "elevation z") may be converted into spherical coordinates ("radius r", "inclination θ", "azimuth φ"), by the formulas
Conversely, the spherical coordinates may be converted into cylindrical coordinates by the formulae
These formulae assume that the two systems have the same origin and same reference plane, measure the azimuth angle "φ" in the same sense from the same axis, and that the spherical angle "θ" is inclination from the cylindrical "z" axis.
Integration and differentiation in spherical coordinates.
The following equations assume that "θ" is inclination from the z (polar) axis (ambiguous since "x", "y", and "z" are mutually normal):
The line element for an infinitesimal displacement from formula_13 to formula_14 is
where
are the local orthogonal unit vectors in the directions of increasing , , and , respectively,
and formula_19, formula_20, and formula_21 are the unit vectors in Cartesian coordinates.
The surface element spanning from formula_22 to formula_23 and formula_24 to formula_25 on a spherical surface at (constant) radius formula_26 is
Thus the differential solid angle is
The surface element in a surface of polar angle formula_22 constant (a cone with vertex the origin) is
The surface element in a surface of azimuth formula_24 constant (a vertical half-plane) is
The volume element spanning from formula_26 to formula_34, formula_22 to formula_23, and formula_24 to formula_25 is
Thus, for example, a function formula_40 can be integrated over every point in R3 by the triple integral
The del operator in this system leads to the following expressions for gradient, divergence, curl and Laplacian:
Kinematics.
In spherical coordinates the position of a point is written
Its velocity is then
and its acceleration is
In the case of a constant "φ" or formula_46, this reduces to vector calculus in polar coordinates.

</doc>
<doc id="29183" url="https://en.wikipedia.org/wiki?curid=29183" title="Sleipnir (disambiguation)">
Sleipnir (disambiguation)

Sleipnir is an eight-legged horse in Norse mythology. Sleipnir or Sleipner may also refer to:

</doc>
<doc id="29185" url="https://en.wikipedia.org/wiki?curid=29185" title="GAM-87 Skybolt">
GAM-87 Skybolt

The Douglas GAM-87 Skybolt (AGM-48 under the 1962 Tri-service system) was an air-launched ballistic missile (ALBM), equipped with a thermonuclear warhead, developed by the United States during the late 1950s. The UK joined the program in 1960, intending to use it on their V bomber force. A series of test failures and the development of submarine-launched ballistic missiles (SLBMs) eventually led to its cancellation in December 1962. The UK had decided to base its entire 1960s deterrent force on Skybolt, and its cancellation led to a major disagreement between the UK and US, known today as the "Skybolt Crisis". This was resolved during a series of meetings that led to the Royal Navy gaining the UGM-27 Polaris missile and construction of the s to launch them.
History.
Background.
Nuclear weapons theorists had speculated about how to integrate the flexibility of the manned bomber with the invulnerability (in the attack) of the ballistic missile. The introduction of useful surface-to-air missiles in the 1950s rendered flight over enemy territory much more dangerous and had greatly reduced the effective deterrent power of a bomber force. Yet the Air Force and military planners were, in the mid-1950s, reluctant to simply hand over the nuclear strike capability to missiles. After launch, missiles were no longer under positive control, could not be recalled or redirected, and would reach their targets within a matter of minutes after the order to fire. Bombers, in comparison, could be re-directed in flight, and their longer flight times offered greater chance of a negotiated settlement during the attack.
Furthermore the missiles of the day were all required to be loaded with their fuels immediately prior to launch, and they could only be launched from above ground after long pre-launch checkouts. This made them vulnerable to attack from the air while they prepared – the first American ICBMs, Atlas 1 and Titan 1, were of this type (Soviet R-7 suffered from the same problem as well). In contrast, a bomber could be ordered into the air long in advance of an attack, rendering them effectively invulnerable to attack while they "loitered" awaiting orders. With aerial refuelling, the loiter times were on the order of a day if need be.
In addition, the inaccuracy of missiles in the 1950s made them useless as precision strike weapons. They could attack area targets like cities, but could not reliably and accurately attack precision strike targets like enemy bomber bases, hardened command and control centers, naval bases, or weapons storage areas. Initially, Western ballistic missiles could not even reach such targets, which would be located deep within interior of the Sino-Soviet land mass in Asia. Therefore the potential integration of aircraft with the invulnerability of the ballistic missile was an intriguing prospect to 1950s military planners.
ALBMs.
Basing the strike package on aircraft offered a flexibility that missiles could not match. For instance, the bombers could stand off from the targets and wait for instructions from secure command centers to attack targets that were missed in an initial strike. Additionally, the bombers could use long-range weapons to strike known air defenses, and then overfly them to deliver precision strikes with freefall nuclear bombs.
Secondly, and most importantly, this mode of deployment meant that the strike force was rendered almost invulnerable. The bombers could fly to staging areas well outside the range of even the longest-legged defenses, and strike with impunity. This allowed for gradual escalation and a possible backing down through diplomacy. A ground-based missile cannot be used in the same fashion; it is either launched or not. If threatened with a nuclear strike, this presents their owners with the 'use them or lose them' predicament.
For the British, their dilemma was a matter of geography and financial resources. No fixed land-based missile system could be credibly installed in the British Isles; they were well within the range of Soviet air strikes. The limited land mass available meant it would be relatively easy for missile sites to be spotted no matter what security measures were taken. Suitable locations for construction also carried a social and political cost. Fixed land based ballistic missile sites need many thousands of acres per squadron (typically ten missiles); and the squadrons need to be apportioned over many thousands of square miles, so that no single attack could conceivably destroy them all in one strike.
Development.
In 1958 several American contractors demonstrated that large ballistic missiles could be launched from strategic bombers at high altitude. The use of astronavigation systems for mid-flight corrections of an inertial guidance platform (astro-inertial guidance), similar to that of the US Navy's SLBM systems, led to an accuracy similar to that of their existing ground-based missiles.
The US Air Force was interested and began accepting bids for development systems in early 1959. Douglas Aircraft received the prime contract in May, and in turn subcontracted to Northrop for the guidance system, Aerojet for the propulsion system, and General Electric for the reentry vehicle. The system was initially known as WS-138A and was given the official name GAM-87 Skybolt in 1960.
At the same time the Royal Air Force was having problems with their MRBM missile project, the Blue Streak, which was long overdue. This left the deterrent based on their own bomber force, the V bomber fleet, and like the Americans there were concerned about its survivability in the face of improving Soviet SAMs. The long-range Skybolt would eliminate the need for both the Blue Streak and the Blue Steel II standoff missile, then under development. The Blue Steel II was cancelled in December 1959 and the British cabinet decided in February 1960 to cancel the Blue Streak. 
Prime Minister Macmillan met President Eisenhower in March 1960 and agreed to purchase 144 Skybolts for the RAF. By agreement, British funding for research and development was limited to that required to modify the V bombers to take the missile, but the British were allowed to fit their own warheads and the Americans were given nuclear submarine basing facilities in Scotland. Following the agreement the Blue Streak programme was formally cancelled in April 1960 and in May 1960 an agreement for an initial order of 100 Skybolts was concluded.
Avro were made an associate contractor to manage the Skybolt programme for the United Kingdom and four different schemes were submitted to find a platform for the missile. A number of different aircraft platforms were considered including a variant of the Vickers VC10 airliner and two of the current V bombers, the Avro Vulcan and Handley Page Victor. It was decided to use the Vulcan to initially carry two missiles each on hardpoints outboard of the main landing gear.
Tests.
By 1961, several test articles were ready for testing from USAF B-52 bombers, with drop-tests starting in January. In January 1961 a Vulcan visited the Douglas plant at Santa Monica to make sure the modifications to the aircraft were electrically compatible with the missile. In Britain, compatibility trials with mockups started on the Vulcan. Powered tests started in April 1962, but the test series went badly, with the first five trials ending in failure of one sort or another. The first fully successful flight occurred on December 19, 1962.
Cancellation.
By this point the value of the Skybolt system had been seriously eroded. The US Navy's Polaris submarine-launched ballistic missile had recently gone into service, with overall capabilities similar to Skybolt, but with "loiter" times on the order of months instead of hours. Additionally, the US Air Force itself was well into the process of developing the Minuteman missile, whose improved accuracy reduced the need for any bomber attacks. Robert McNamara was particularly opposed to the bomber force and repeatedly stated he felt that the combination of SLBMs and ICBMs would render them useless. He pressed for the cancellation of Skybolt as an unnecessary program.
The British, on the other hand, had cancelled all other projects to concentrate fully on Skybolt. When McNamara informed them that they were considering cancelling the program in November 1962, a firestorm of protest broke out in the House of Commons. Jo Grimond noted "Does not this mark the absolute failure of the policy of the independent deterrent? Is it not the case that everybody else in the world knew this, except the Conservative Party in this country?" President Kennedy officially cancelled the program on December 22, 1962. As the political row grew into a major crisis, an emergency meeting between parties from the US and UK was called, leading to the Nassau agreement.
Over the next few days a new plan was hammered out that saw the UK purchase the Polaris SLBM, but equipped with British warheads that lacked the dual-key system. The UK would thus retain its independent deterrent force, although its control passed from the RAF largely to the Royal Navy. The Polaris, a much better weapon system for the UK, was a major "scoop" and has been referred to as “almost the bargain of the century”. The RAF kept a tactical nuclear capability with the WE.177 which armed V bombers and later the Panavia Tornado force. The "Skybolt Crisis" was a major event in the eventual downfall of the Macmillan government.
A B-52G launched the last XGAM-87A missile down the Atlantic Missile Range a day after the program was cancelled. In June 1963, the XGAM-87A was redesignated as XAGM-48A.
Description.
The GAM-87 was powered by a two-stage solid-fuel rocket motor. Each B-52 was to carry four missiles, two under each wing on side-by-side pylons, while the Avro Vulcan carried two, on smaller pylons under each wing. The missile was fitted with a tailcone to reduce drag while on the pylon, which was ejected shortly after being dropped from the plane. After first stage burnout, the Skybolt coasted for a while before the second stage ignited. First stage control was by eight movable tail fins, while the second stage was equipped with a gimballed nozzle.
Guidance was entirely by inertial platform. The current position was constantly updated from the host aircraft though accurate fixes, meaning that the accuracy of the platform inside the missile was not as critical.

</doc>
<doc id="29186" url="https://en.wikipedia.org/wiki?curid=29186" title="Strategic Defense Initiative">
Strategic Defense Initiative

The Strategic Defense Initiative (SDI) was a proposed missile defense system intended to protect the United States from attack by ballistic strategic nuclear weapons (Intercontinental ballistic missiles and Submarine-launched ballistic missiles). The system, which was to combine ground-based units and orbital deployment platforms, was first publicly announced by President Ronald Reagan on March 23, 1983. The initiative focused on strategic defense rather than the prior strategic offense doctrine of mutual assured destruction (MAD). The Strategic Defense Initiative Organization (SDIO) was set up in 1984 within the United States Department of Defense to oversee the Strategic Defense Initiative.
Reagan was a vocal critic of the doctrine of mutual assured destruction, and the Strategic Defense Initiative was an important part of his defense policy intended to end MAD as a nuclear deterrence strategy, as well as a strategic initiative to neutralize the military component of Soviet nuclear defenses.
The ambitious initiative was criticized for allegedly threatening to destabilize the MAD -approach and to possibly re-ignite "an offensive arms race". SDI was nicknamed, largely in the mainstream media, as "Star Wars", after the popular 1977 film by George Lucas. In 1987, the American Physical Society concluded that a global shield such as "Star Wars" was extremely ambitious and with existing technology not directly feasible for operational status, and that about ten more years of research was needed to learn about such a comprehensive and complex system to set up and make it fully operational.
However, the United States now holds a significant advantage in the field of comprehensive advanced missile defense systems through years of extensive research and testing. The U.S. and the U.K also have laser weapons, as well as 360 degree laser shields in development, which are expected to be ready for military use as early as 2020. Many of the obtained technological insights were transferred to subsequent programs and would find use in follow-up programs.
Under the administration of President Bill Clinton in 1993, its name was changed to the Ballistic Missile Defense Organization (BMDO) and its emphasis was shifted from national missile defense to theater missile defense; and its scope from global to more regional coverage. It was never truly developed or deployed, though certain aspects of SDI research and technologies paved the way for some anti-ballistic missile systems of today. BMDO was renamed to the Missile Defense Agency in 2002. This article covers defense efforts under the SDIO.
Under the SDIO's Innovative Sciences and Technology Office, headed by physicist and engineer Dr. James Ionson, the investment was predominantly made in basic research at national laboratories, universities, and in industry; these programs have continued to be key sources of funding for top research scientists in the fields of high-energy physics, supercomputing/computation, advanced materials, and many other critical science and engineering disciplines — funding which indirectly supports other research work by top scientists, and which was most politically viable to fund within the budget environment.
Laser research funded by the SDI office was disclosed at laser conferences that also included panel discussions on the subject with the participation of James Ionson, Edward Teller, and other prominent advocates of SDI.
History.
Precursor to SDI.
George Shultz, Secretary of State under Reagan, suggests that a 1967 lecture by physicist Edward Teller (the so-called "father of the hydrogen bomb") was an important precursor to SDI. In the lecture Teller talked about the idea of defending against nuclear missiles by using nuclear weapons. Held at Lawrence Livermore National Laboratory, Reagan attended it shortly after becoming the governor of California. In 1979, Ronald Reagan visited the NORAD command base, Cheyenne Mountain Complex, where he was first introduced to the extensive tracking and detection systems extending throughout the world and into space. However, he was struck by their comments that while they could track the attack down to the individual targets, there was nothing one could do to stop it. Reagan felt that in the event of an attack this would place the president in a terrible position, having to choose between immediate counterattack or attempting to absorb the attack and then maintain an upper hand in the post-attack era. Shultz suggests that this feeling of helplessness, coupled with the defensive ideas proposed by Teller a decade earlier, combined to form the impetus of the SDI. In the fall of 1979, at Reagan's request, Lieutenant General Daniel O. Graham conceived a concept he called the High Frontier, an idea of strategic defense using ground- and space-based weapons theoretically possible because of emerging technologies. It was designed to replace the doctrine of Mutual assured destruction, a doctrine that Reagan and his aides described as a suicide pact.
The initial focus of the strategic defense initiative was a nuclear explosion-powered X-ray laser designed at Lawrence Livermore National Laboratory by a scientist named Peter L. Hagelstein who worked with a team called 'O Group', doing much of the work in the late 1970s and early 1980s. O Group was headed by physicist Lowell Wood, a protégé and friend of Edward Teller.
Ronald Reagan was told of Hagelstein's breakthrough by Teller in 1983, which prompted Reagan's March 23, 1983, "Star Wars" speech. Reagan announced, "I call upon the scientific community who gave us nuclear weapons to turn their great talents to the cause of mankind and world peace: to give us the means of rendering these nuclear weapons impotent and obsolete." This speech, along with Reagan's Evil Empire speech on March 8, 1983, in Florida, ushered in the final major escalation in rhetoric of the Cold War prior to a thawing of relations in the mid-to-late-1980s.
The concept for the space-based portion was to use lasers to shoot down incoming Soviet intercontinental ballistic missiles (ICBMs) armed with nuclear warheads. Nobel Prize-winning physicist Hans Bethe went to Livermore in February 1983 for a two-day briefing on the X-ray laser, and "Although impressed with its scientific novelty, Bethe went away highly skeptical it would contribute anything to the nation's defense."
Frances Fitzgerald claimed that Reagan may also have been inspired to create SDI by a fictional secret weapon, a ray that can paralyze electrical currents, in "Murder in the Air", a movie he made in 1940.
Bombers to ICBMs.
Although Nazi Germany put considerable effort into the first surface-to-air missiles (SAMs) after 1943, they did not have enough time to develop operational-level weapons before the end of World War II.
Their research proved valuable to teams in the US and Soviet Union, where missile programs slowly developed in the immediate post-war era. As the Cold War started, the Soviets found themselves facing massive USAF and Royal Air Force strategic bomber fleets they could not hope to counter in the air. In response they dramatically increased their efforts in SAM development, deploying the S-25 Berkut system around Moscow as early as 1955. This was followed by the dramatically improved and semi-mobile S-75 Dvina, a weapon that remained in service in the 2000s. Similar US and UK weapons soon followed. By the late 1950s, as missiles developed both in quality and number, the ability for the US air fleet to penetrate Soviet airspace was increasingly at risk.
In response, both sides increased their efforts to develop long-range missiles. The Soviets, with no effective bomber force of their own, put considerable effort into their program and quickly brought their basic R-7 Semyorka ballistic system into operation in 1959. The US's SM-65 Atlas followed almost immediately thereafter. These early examples were useful only for attacking large targets like cities or ports, but their relative invulnerability and low cost provided both sides with a credible force in an era of stiffening air defenses.
ABMs.
At first it appeared that the ICBM could be countered by systems similar to the ever-evolving SAMs already in operation. The ICBM's high trajectory meant they became visible to defensive radars not long after being launched, which meant that defensive systems would have time to prepare. Although they moved quickly in flight, early re-entry systems slowed dramatically once they reached the lower atmosphere, which gave time for a fast missile to attack it. By the early 1960s both nations were working on their first anti-ballistic missile (ABM) systems.
As ABMs were being developed, countermeasures were also being studied. As the systems generally used long-range radars to find and track the incoming warheads, the simplest solution was to add radar reflectors and other decoys to the launch. These took up little room or weight, but would make a radar return that looked like an additional warhead. This would force the defender to use more ABMs to ensure the "right" object was hit, or wait until they started to re-enter, when the lighter objects would slow down faster and leave the warhead racing ahead. Neither option was particularly attractive in cost terms, generally requiring more and faster missiles.
A better understanding of Nuclear electromagnetic pulse (NEMP) presented new problems; a warhead set off at high altitudes and long ranges from the defensive missiles could blind the radars, making the incoming warheads only become visible at lower altitudes. This would further reduce the amount of time the ABM system had to react. Systems using non-obvious approaches might be able to blind the radars in a sneak attack; the Soviets developed the R-36 with a system called Fractional Orbital Bombardment System to allow attacks on US missile fields from low altitudes and/or from the south, while the US relied on manned bombers for the same role.
Making matters worse was the continual increase in ICBM numbers. Even before the systems were ready for use, the number of interceptor missiles needed to effectively deter an attack kept increasing. As the ABM systems were expensive, it appeared the simplest way to defeat them was to simply make more ICBMs and deliberately start an arms race the defender could not win. The introduction of MIRV systems dramatically upset this in the favour of the attack; missiles now carried several warheads that would be dispersed over wide areas, so now every new ICBM built would require a small fleet of ABMs to counter it. Both the US and USSR rushed to introduce new weapons with MIRV systems, and the number of warheads in the world rapidly proliferated.
Whether or not deploying an ABM system was worthwhile was a highly contentious issue. The US scaled back their plans significantly and their Sentinel Program aimed only to counter the small Chinese ICBM force, a limited Soviet attack or an accidental launch. By the late 1960s, widespread efforts were underway to solve the problem diplomatically instead of with more missiles. The Anti-Ballistic Missile Treaty, signed in 1972, placed limits on the number of ABM systems, later followed by limits on the number of warheads. Both countries continued to deploy a single ABM site; the US briefly deployed a single site under their Safeguard Program, while the Soviets deployed A35/A135 missile defense system around Moscow.
Attack from above.
Throughout the development of the ABM, another possibility existed that avoided most of these problems. If the interceptors were placed in orbit, some of them could be positioned over the Soviet Union at all times. These would fly "down hill" to attack the missiles, so they could be considerably smaller and cheaper than an interceptor that needed to launch up from the ground. It was also much easier to track the ICBMs during launch, due to their huge infrared signatures, and disguising these signatures would require the construction of large rockets instead of small radar decoys. Moreover, each interceptor could kill one ICBM; MIRV had no effect. As long as the interceptor missile was inexpensive, the advantage was on the side of the defense.
The US Air Force had studied these concepts under "Project Defender" as early as 1958, which included work on the "Ballistic Missile Boost Interceptor", or BAMBI. BAMBI interceptors would be deployed on a series of satellites, and would be launched towards ICBMs as they climbed. As they approached the ICBM, they would open a large metal net, which would destroy the missile on impact. Depending on assumptions about the accuracy of the system and the number of missiles it would have to face, between 400 and 3,600 such satellites would be needed in order to keep enough above the USSR at any one time. The Air Force concluded that there was simply no way to launch the required number of satellites, let alone have any way to service them. As their space logistical abilities improved through the 1960s they continued to study the problem, but in each case the problem of increasing ICBM numbers meant the numbers of interceptors needed grew to overwhelm any possible launch capability.
However, the introduction of the laser in the 1960s appeared to offer the possibility of a way out of the problem. The amount of time needed to attack any one missile was known as the "dwell time", and if a powerful laser had a short dwell time, say 10 seconds, it would be able to attack multiple missiles during the minutes while the ICBM was launching. Given current laser energies this was impractical, but the concept was studied throughout the 1960s and later.
X-ray laser.
In 1979 Edward Teller contributed to a Hoover Institution publication where he claimed that the US would be facing an emboldened USSR due to their work on civil defense. Two years later at a conference in Italy, he made the same claims about their ambitions, but with a subtle change; now he claimed that the reason for their boldness was their development of new space-based weapons. In fact, according to popular author Frances FitzGerald, there was absolutely no evidence that such research was being carried out. What had really changed was that Teller was now selling his latest nuclear weapon, the X-ray laser. Finding limited success in his efforts to get funding for the project, his speech in Italy was a new attempt to create a missile gap.
However, according to a 1983 US Interagency Intelligence Assessment, there was good evidence that in the late 1960s the Soviets were devoting serious thought to both explosive and non-explosive nuclear power sources for lasers.
Moreover, it has since emerged that detailed study on a Soviet space-based LASER system began no later than 1976 as the "Skif (laser)", a 1 MW Carbon dioxide laser along with the anti-satellite "Kaskad (system)", an in-orbit missile platform. With both devices reportedly designed to pre-emptively destroy any US satellites that might be launched in the future which could otherwise aid US missile defense. In 1987 a disguised Mir space station module was lifted on the inaugural flight of the Energia booster as the Polyus (spacecraft) and it has since been revealed that this craft housed a number of systems of the Skif laser and it was intended that they clandestinely be tested in orbit, if it had not been for the spacecraft's attitude control system malfunctioning upon separation from the booster and it failing to reach orbit. More tentatively, it is also suggested that the Zarya module of the International Space Station, capable of station keeping and providing sizable battery power, was initially part of the Skif laser system, as the module was lifted to orbit cheaper and faster than many commentators thought possible in the post-Soviet era, indicating that it had been largely constructed in advance for the mothballed Skif laser.
The US project was the result of a 1977 development by George Chapline, Jr. of Lawrence Livermore's "O-Group". Livermore had been working on X-ray lasers for some time, but Chapline found a new solution that used the massive release of X-rays from a thermonuclear weapon as the source of light for a small baseball-bat sized lasing crystal in the form of a metal rod. The concept was first tried out in the 1978 underground nuclear test "Diablo Hawk" but had failed. Peter Hagelstein, new to O Group, set about creating computer simulations of the system in order to understand why. At first he demonstrated that Chapline's original calculations were simply wrong and the Diablo Hawk system could not possibly work. But as he continued his efforts, he found that using heavier metals appeared to make a machine that would work. Through 1979 a new test was planned to take advantage of his work. The follow-up test in November 1980s "Dauphin" appeared to be a success, and plans were made for a major series of experiments in the early 1980s under "Excalibur".
Since the lasing medium was fairly small, a single bomb could host a number of them and attack multiple ICBMs in a single burst. The Soviet ICBM fleet had tens of thousands of "warheads", but only about 1,400 "missiles". If each satellite had two dozen lasers, two dozen satellites on-station would significantly blunt any attack. In Molniya orbits, where the satellites would spend much of their time over the USSR, only a few dozen satellites would be needed, in total. An article in "Aviation Week and Space Technology" described how the devices "... are so small that a single payload bay on the space shuttle could carry to orbit a number sufficient to stop a Soviet nuclear weapons attack". Some time later Teller used similar language in a letter to Paul Nitze, who was preparing a new round of strategic limitations talks, stating that "A single X-ray laser module the size of an executive desk... could potentially shoot down the entire Soviet land-based missile force..."
Livermore is just one of several major US weapons labs. Other labs had been working on ideas of their own, from new space or ground-based missiles, to chemical lasers, to particle beam weapons. Angelo Codevilla argued for similar funding for powerful chemical lasers as well. None of these efforts was taken very seriously by members of the Carter administration. In a meeting with Teller and Lowell Wood, a critic noted that the Soviets could easily defeat the system by attacking the satellite, whose only defense, if it had been unarmed, was to destroy itself. However this may have been rectified if the satellite also included a means of self-defense, and would not have been the first satellite to have a defense system, as a revolver cannon(Rikhter R-23) was mounted on the 1974 Soviet Salyut 3 space station, a satellite that successfully test fired its cannon in orbit. The critics also suggested that the US public would be unlikely to accept nuclear bombs in space, regardless of the potential benefits. At the time Teller was stymied by these arguments; the concept was later adapted to a "pop-up", or launch on warning system, where the X-ray laser would be launched, or "popped-up" into space from ballistic missile submarines.
Project and proposals.
In 1984, the Strategic Defense Initiative Organization (SDIO) was established to oversee the program, which was headed by Lt. General James Alan Abrahamson USAF, a past Director of the NASA Space Shuttle program. Research and development initiated by the SDIO created significant technological advances in computer systems, component miniaturization, sensors and missile systems that form the basis for current systems.
Initially, the program focused on large scale systems designed to defeat a Soviet offensive strike. However, as the threat diminished, the program shifted towards smaller systems designed to defeat limited or accidental launches.
By 1987, the SDIO had developed a national missile defense concept called the Strategic Defense System Phase I Architecture. This concept consisted of ground and space based sensors and weapons, as well as a central battle management system. The ground-based systems operational today trace their roots back to this concept.
In his 1991 State of the Union Address George H. W. Bush shifted the focus of SDI from defense of North America against large scale strikes to a system focusing on theater missile defense called Global Protection Against Limited Strikes (GPALS).
In 1993, the Clinton administration further shifted the focus to ground-based interceptor missiles and theater scale systems, forming the Ballistic Missile Defense Organization (BMDO) and closing the SDIO. The Ballistic Missile Defense Organization was renamed again by the George W. Bush administration as the Missile Defense Agency and focused onto limited National Missile Defense.
Ground-based programs.
Extended Range Interceptor (ERINT).
The Extended Range Interceptor (ERINT) program was part of SDI's Theater Missile Defense Program and was an extension of the Flexible Lightweight Agile Guided Experiment (FLAGE), which included developing hit-to-kill technology and demonstrating the guidance accuracy of a small, agile, radar-homing vehicle.
FLAGE scored a direct hit against a MGM-52 Lance missile in flight, at White Sands Missile Range in 1987. ERINT was a prototype missile similar to the FLAGE, but it used a new solid-propellant rocket motor that allowed it to fly faster and higher than FLAGE.
Under BMDO, ERINT was later chosen as the Patriot Advanced Capability-3 (PAC-3) missile.
Homing Overlay Experiment (HOE).
Given concerns about the previous programs using nuclear-tipped interceptors, in the 1980s the U.S. Army began studies about the feasibility of hit-to-kill vehicles, i.e. interceptor missiles that would destroy incoming ballistic missiles just by colliding with them head-on.
The Homing Overlay Experiment (HOE) was the first hit-to-kill system tested by the US Army, and also the first successful hit-to-kill intercept of a mock ballistic missile warhead outside the Earth’s atmosphere.
The HOE used a Kinetic Kill Vehicle (KKV) to destroy a ballistic missile. The KKV was equipped with an infrared seeker, guidance electronics and a propulsion system. Once in space, the KKV could extend a folded structure similar to an umbrella skeleton of 4 m (13 ft) diameter to enhance its effective cross section. This device would destroy the ICBM reentry vehicle on collision.
Four test launches were conducted in 1983 and 1984 at Kwajalein Missile Range in the Republic of the Marshall Islands. For each test a Minuteman missile was launched from Vandenberg Air Force Base in California carrying a single mock re-entry vehicle targeted for Kwajalein lagoon more than away.
After test failures with the first three flight tests because of guidance and sensor problems, the DOD reported that the fourth and final test on June 10, 1984 was successful, intercepting the Minuteman RV with a closing speed of about 6.1 km/s at an altitude of more than 160 km.
Although the fourth test was described as a success, the New York Times in August 1993 reported that the HOE4 test was rigged to increase the likelihood of a successful hit. At the urging of Senator David Pryor, the General Accounting Office investigated the claims and concluded that though steps were taken to make it easier for the interceptor to find its target (including some of those alleged by the New York Times), the available data indicated that the interceptor had been successfully guided by its onboard infrared sensors in the collision, and not by an onboard radar guidance system as alleged. Per the GAO report, the net effect of the DOD enhancements increased the infrared signature of the target vessel by 110% over the realistic missile signature initially proposed for the HOE program, but nonetheless the GAO concluded the enhancements to the target vessel were reasonable given the objectives of the program and the geopolitical consequences of its failure. Id. Further, the report concluded that the DOD's subsequent statements before Congress about the HOE program "fairly characterize" the success of HOE4, but confirmed that the DOD never disclosed to Congress the enhancements made to the target vessel. Id. 
The technology developed for the HOE system was later used by the SDI and expanded into the Exoatmospheric Reentry-vehicle Interception System (ERIS) program.
Exoatmospheric Reentry-vehicle Interceptor Subsystem (ERIS).
Developed by Lockheed as part of the ground-based interceptor portion of SDI, the Exoatmospheric Reentry-vehicle Interceptor Subsystem (ERIS) began in 1985, with at least two tests occurring in the early 1990s. This system was never deployed, but the technology of the system was used in the Terminal High Altitude Area Defense (THAAD) system and the Ground Based Interceptor currently deployed as part of the Ground-Based Midcourse Defense (GMD) system.
Directed-energy weapon (DEW) programs.
X-ray laser.
An early focus of the project was a curtain of X-ray lasers powered by nuclear explosions. The curtain was to be deployed using a series of missiles launched from submarines or, later on, satellites, during the critical seconds following a Soviet attack. The satellites would be powered by built-in nuclear warheads – in theory, the energy from the warhead detonation would be used to pump a series of laser emitters in the missiles or satellites, allowing each satellite to shoot down many incoming warheads simultaneously. The attraction of this approach was that it was thought to be faster than an optical laser, which could only shoot down warheads one at a time, limiting the number of warheads each laser could destroy in the short time 'window' of an attack.
However, on March 26, 1983, the first test, known as the Cabra event, was performed in an underground shaft and resulted in marginally positive readings that could be dismissed as being caused by a faulty detector. Since a nuclear explosion was used as the power source, the detector was destroyed during the experiment and the results therefore could not be confirmed. Technical criticism based upon unclassified calculations suggested that the X-ray laser would be of at best marginal use for missile defense. Such critics often cite the X-ray laser system as being the primary focus of SDI, with its apparent failure being a main reason to oppose the program. However, the laser was never more than one of the many systems being researched for ballistic missile defense.
Despite the apparent failure of the Cabra test, the long term legacy of the X-ray laser program is the knowledge gained while conducting the research. A parallel developmental program advanced laboratory X-ray lasers for biological imaging and the creation of 3D holograms of living organisms. Other spin-offs include research on advanced materials like SEAgel and Aerogel, the Electron-Beam Ion Trap facility for physics research, and enhanced techniques for early detection of breast cancer.
Chemical laser.
Beginning in 1985, the Air Force tested an SDIO-funded deuterium fluoride laser known as Mid-Infrared Advanced Chemical Laser (MIRACL) at White Sands Missile Range. During a simulation, the laser successfully destroyed a Titan missile booster in 1985, however the test setup had the booster shell pressurized and under considerable compression loads. These test conditions were used to simulate the loads a booster would be under during launch. The system was later tested on target drones simulating cruise missiles for the US Navy, with some success. After the SDIO closed, the MIRACL was tested on an old Air Force satellite for potential use as an anti-satellite weapon, with mixed results. The technology was also used to develop the Tactical High Energy Laser, (THEL) which is being tested to shoot down artillery shells.
During the mid-to-late 1980s a number of panel discussions on lasers and SDI took place at various laser conferences. Proceedings of these conferences include papers on the status of chemical and other high power lasers at the time.
The Missile Defense Agency's Airborne Laser program uses a chemical laser which has successfully intercepted a missile taking off, so an offshoot of SDI could be said to have successfully implemented one of the key goals of the program.
Neutral Particle Beam.
In July 1989, the Beam Experiments Aboard a Rocket (BEAR) program launched a sounding rocket containing a neutral particle beam (NPB) accelerator. The experiment successfully demonstrated that a particle beam would operate and propagate as predicted outside the atmosphere and that there are no unexpected side-effects when firing the beam in space. After the rocket was recovered, the particle beam was still operational. According to the BMDO, the research on neutral particle beam accelerators, which was originally funded by the SDIO, could eventually be used to reduce the half-life of nuclear waste products using accelerator-driven transmutation technology.
Laser and mirror experiments.
The High Precision Tracking Experiment (HPTE), launched with the Space Shuttle Discovery on STS-51-G, was tested June 21, 1985 when a Hawaii-based low-power laser successfully tracked the experiment and bounced the laser off of the HPTE mirror.
The Relay mirror experiment (RME), launched in February 1990, demonstrated critical technologies for space-based relay mirrors that would be used with an SDI directed-energy weapon system. The experiment validated stabilization, tracking, and pointing concepts and proved that a laser could be relayed from the ground to a 60 cm mirror on an orbiting satellite and back to another ground station with a high degree of accuracy and for extended durations.
Launched on the same rocket as the RME, the Low-power Atmospheric Compensation Experiment (LACE) satellite was built by the United States Naval Research Laboratory (NRL) to explore atmospheric distortion of lasers and real-time adaptive compensation for that distortion. The LACE satellite also included several other experiments to help develop and improve SDI sensors, including target discrimination using background radiation and tracking ballistic missiles using Ultraviolet Plume Imaging (UVPI). LACE was also used to evaluate ground-based adaptive optics, a technique now used in civilian telescopes to remove atmospheric distortions.
Hypervelocity Rail Gun (CHECMATE).
Research out of hypervelocity railgun technology was done to build an information base about rail guns so that SDI planners would know how to apply the technology to the proposed defense system. The SDI rail gun investigation, called the Compact High Energy Capacitor Module Advanced Technology Experiment (CHECMATE), had been able to fire two projectiles per day during the initiative. This represented a significant improvement over previous efforts, which were only able to achieve about one shot per month. Hypervelocity rail guns are, at least conceptually, an attractive alternative to a space-based defense system because of their envisioned ability to quickly shoot at many targets. Also, since only the projectile leaves the gun, a railgun system can potentially fire many times before needing to be resupplied.
A hypervelocity railgun works very much like a particle accelerator insofar as it converts electrical potential energy into kinetic energy imparted to the projectile. A conductive pellet (the projectile) is attracted down the rails by electric current flowing through a rail. Through the magnetic forces that this system achieves, a force is exerted on the projectile moving it down the rail. Railguns can generate muzzle-velocities in excess of 2.4 kilometers per second. At this velocity, even a rifle-bullet sized projectile will penetrate the front armor of a main battle tank, let alone a thinly protected missile guidance system.
Rail guns face a host of technical challenges before they will be ready for battlefield deployment. First, the rails guiding the projectile must carry very high power. Each firing of the railgun produces tremendous current flow (almost half a million amperes) through the rails, causing rapid erosion of the rail's surfaces (through ohmic heating, and even vaporization of the rail surface.) Early prototypes were essentially single-use weapons, requiring complete replacement of the rails after each firing. Another challenge with the rail gun system is projectile survivability. The projectiles experience acceleration force in excess of 100,000 g. In order to be effective, the fired projectile must first survive the mechanical stress of firing and the thermal effects of a trip through the atmosphere at many times the speed of sound before its subsequent impact with the target. In-flight guidance, if implemented, would require the onboard navigation system to be built to the same level of sturdiness as the main mass of the projectile.
In addition to being considered for destroying ballistic missile threats, rail guns were also being planned for service in space platform (sensor and battle station) defense. This potential role reflected defense planner expectations that the rail guns of the future would be capable of not only rapid fire, but also of multiple firings (on the order of tens to hundreds of shots).
Space-based programs.
Space-Based Interceptor (SBI).
Groups of interceptors were to be housed in orbital modules. Hover testing was completed in 1988 and demonstrated integration of the sensor and propulsion systems in the prototype SBI. It also demonstrated the ability of the seeker to shift its aiming point from a rocket's hot plume to its cool body, a first for infrared ABM seekers. Final hover testing occurred in 1992 using miniaturized components similar to what would have actually been used in an operational interceptor. These prototypes eventually evolved into the Brilliant Pebbles program.
Brilliant Pebbles.
Brilliant Pebbles was a non-nuclear system of satellite-based interceptors designed to use high-velocity, watermelon-sized, teardrop-shaped projectiles made of tungsten as kinetic warheads. It was designed to operate in conjunction with the Brilliant Eyes sensor system. The project was conceived in November 1986 by Lowell Wood at Lawrence Livermore National Laboratory. Detailed studies were undertaken by several advisory boards, including the Defense Science Board and JASON, in 1989.
The Pebbles were designed in such a way that autonomous operation, without further external guidance from planned SDI sensor systems, was possible. This was attractive as a cost saving measure, as it would allow scaling back of those systems, and was estimated to save $7 to $13 billion versus the standard Phase I Architecture. Brilliant Pebbles later became the centerpiece of a revised architecture under the Bush Administration SDIO.
John H. Nuckolls, director of Lawrence Livermore National Laboratory from 1988 to 1994, described the system as “The crowning achievement of the Strategic Defense Initiative”. Some of the technologies developed for SDI were used in numerous later projects. For example, the sensors and cameras that were developed for Brilliant Pebbles became components of the Clementine mission and SDI technologies may also have a role in future missile defense efforts.
Though regarded as one of the most capable SDI systems, the Brilliant Pebbles program was canceled in 1994 by the BMDO.
Sensor programs.
SDIO sensor research encompassed visible light, ultraviolet, infrared, and radar technologies, and eventually led to the Clementine mission though that mission occurred just after the program transitioned to the BMDO. Like other parts of SDI, the sensor system initially was very large-scale, but after the Soviet threat diminished it was cut back.
Boost Surveillance and Tracking System (BSTS).
Boost Surveillance and Tracking System was part of the SDIO in the late 1980s, and was designed to assist detection of missile launches, especially during the boost phase. However, once the SDI program shifted toward theater missile defense in the early 1990s, the system left SDIO control and was transferred to the Air Force.
Space Surveillance and Tracking System (SSTS).
Space Surveillance and Tracking System was a system originally designed for tracking ballistic missiles during their mid-course phase. It was designed to work in conjunction with BSTS, but was later scaled down in favor of the Brilliant Eyes program.
Brilliant Eyes.
Brilliant Eyes was a simpler derivative of the SSTS that focused on theater ballistic missiles rather than ICBMs and was meant to operate in conjunction with the Brilliant Pebbles system.
Brilliant Eyes was renamed Space and Missile Tracking System (SMTS) and scaled back further under BMDO, and in the late 1990s it became the low earth orbit component of the Air Force's Space Based Infrared System (SBIRS).
Other sensor experiments.
The Delta 183 program used a satellite known as "Delta Star" to test several sensor related technologies. Delta Star carried a thermographic camera, a long-wave infrared imager, an ensemble of imagers and photometers covering several visible and ultraviolet bands as well as a laser detector and ranging device. The satellite observed several ballistic missile launches including some releasing liquid propellant as a countermeasure to detection. Data from the experiments led to advances in sensor technologies.
Countermeasures.
In war-fighting, countermeasures can have a variety of meanings:
Countermeasures of various types have long been a key part of warfighting strategy. However, with SDI they attained a special prominence due to the system cost, scenario of a massive sophisticated attack, strategic consequences of a less-than-perfect defense, outer spacebasing of many proposed weapons systems, and political debate.
Whereas the current United States national missile defense system is designed around a relatively limited and unsophisticated attack, SDI planned for a massive attack by a sophisticated opponent. This raised significant issues about economic and technical costs associated with defending against anti-ballistic missile defense countermeasures used by the attacking side.
For example, if it had been much cheaper to add attacking warheads than to add defenses, an attacker of similar economic power could have simply outproduced the defender. This requirement of being "cost effective at the margin" was first formulated by Paul Nitze in November 1985.
In addition, SDI envisioned many space-based systems in fixed orbits, ground-based sensors, command, control and communications facilities, etc. In theory, an advanced opponent could have targeted those, in turn requiring self-defense capability or increased numbers to compensate for attrition.
A sophisticated attacker having the technology to use decoys, shielding, maneuvering warheads, defense suppression, or other countermeasures would have multiplied the difficulty and cost of intercepting the real warheads. SDI design and operational planning had to factor in these countermeasures and the associated cost.
Impact upon Soviet Union.
The Soviet response to the SDI during the period March 1983 through November 1985 provided indications of their view of the program both as a threat and as an opportunity to weaken NATO. SDI was likely seen not only as a threat to the physical security of the Soviet Union, but also as part of an effort by the United States to seize the strategic initiative in arms controls by neutralizing the military component of Soviet strategy. The Kremlin though masked their real concerns, advocating that space-based missile defenses would make nuclear war inevitable.
A major objective of that strategy was the political separation of Western Europe from the United States, which the Soviets sought to facilitate by aggravating allied concern over the SDI's potential implications for European security and economic interests. The Soviet predisposition to see deception behind the SDI was reinforced by their assessment of US intentions and capabilities and the utility of military deception in furthering the achievement of political goals.
In 1986 Carl Sagan summarized what he heard Soviet commentators were saying about SDI, with a common argument being that it was equivalent to starting an economic war through a defensive arms race to further cripple the Soviet economy with extra military spending, while another less plausible interpretation was that it served as a disguise for the US wish to initiate a first strike on the Soviet Union.
Controversy and criticism.
Historians from the Missile Defense Agency attribute the term "Star Wars" to a "Washington Post" article published March 24, 1983, the day after the speech, which quoted Democratic Senator Ted Kennedy describing the proposal as "reckless Star Wars schemes." Some critics used that term derisively, implying it was an impractical science fiction fantasy. In addition, the American media's liberal use of the moniker (despite President Reagan's request that they use the program's official name) did much to damage the program's credibility. In comments to the media on March 7, 1986, Acting Deputy Director of SDIO, Dr. Gerold Yonas, described the name "Star Wars" as an important tool for Soviet disinformation and asserted that the nickname gave an entirely wrong impression of SDI. However, supporters have adopted the usage as well on the grounds that yesterday's science fiction is often tomorrow's engineering. 
Jessica Savitch reported on the technology in episode No.111 of Frontline, "Space: The Race for High Ground" on PBS on 4/11/1983. [http://www.imdb.com/title/tt1078741/fullcredits#cast] The opening sequence shows Jessica Savitch seated next to a laser that she used to destroy a model of a communication satellite. The demonstration was perhaps the first televised use of a weapons grade laser. No theatrical effects were used. The model was actually destroyed by the heat from the laser. The model and the laser were realized by Marc Palumbo, a High Tech Romantic artist from the Center for Advanced Visual Studies at MIT.
Ashton Carter, a board member at MIT and current Secretary of Defense, assessed SDI for Congress in 1984, saying there were a number of difficulties in creating an adequate missile defense shield, with or without lasers. Carter said X-rays have a limited scope because they become diffused through the atmosphere, much like the beam of a flashlight spreading outward in all directions. This means the X-rays needed to be close to the Soviet Union, especially during the critical few minutes of the booster phase, in order for the Soviet missiles to be both detectable to radar and targeted by the lasers themselves. Opponents disagreed, saying advances in technology, such as using very strong laser beams, and by "bleaching" the column of air surrounding the laser beam, could increase the distance that the X-ray would reach to successfully hit its target.
Physicist Hans Bethe, who worked with Edward Teller on both the atomic bomb and hydrogen bomb at Los Alamos, claimed a laser defense shield was unfeasible. He said that a defensive system was costly and difficult to build yet simple to destroy, and claimed that the Soviets could easily use thousands of decoys to overwhelm it during a nuclear attack. He believed that the only way to stop the threat of nuclear war was through diplomacy and dismissed the idea of a "technical solution" to the Cold War, saying that a defense shield could be viewed as threatening because it would limit or destroy Soviet offensive capabilities while leaving the American offense intact. In March 1984, Bethe coauthored a 106-page report for the Union of Concerned Scientists that concluded "the X-ray laser offers no prospect of being a useful component in a system for ballistic missile defense."
In response to this when Teller testified before Congress he stated that "instead of objecting on scientific and technical grounds, which he thoroughly understands, he now objects on the grounds of politics, on grounds of military feasibility of military deployment, on other grounds of difficult issues which are quite outside the range of his professional cognizance or mine."
On June 28, 1985, David Lorge Parnas resigned from SDIO's Panel on Computing in Support of Battle Management, arguing in eight short papers that the software required by the Strategic Defense Initiative could never be made to be trustworthy and that such a system would inevitably be unreliable and constitute a menace to humanity in its own right. Parnas said he joined the panel with the desire to make nuclear weapons "impotent and obsolete" but soon concluded that the concept was "a fraud".
Treaty obligations.
Another criticism of SDI was that it would require the United States to modify previously ratified treaties. The Outer Space Treaty of 1967, which requires "States Parties to the Treaty undertake not to place in orbit around the Earth any objects carrying nuclear weapons or any other kinds of weapons of mass destruction, install such weapons on celestial bodies, or station such weapons in outer space in any other manner" and would forbid the US from pre-positioning in Earth orbit any devices powered by nuclear weapons and any devices capable of "mass destruction". Only the space stationed nuclear pumped X-ray laser concept would have violated this treaty, since other SDI systems, did not require the pre-positioning of nuclear explosives in space.
The Anti-Ballistic Missile Treaty and its subsequent protocol, which limited missile defenses to one location per country at 100 missiles each (which the USSR had and the US did not), would have been violated by SDI ground-based interceptors. The Nuclear Non-Proliferation Treaty requires "Each of the Parties to the Treaty undertakes to pursue negotiations in good faith on effective measures relating to cessation of the nuclear arms race at an early date and to nuclear disarmament, and on a treaty on general and complete disarmament under strict and effective international control." Many viewed favoring deployment of ABM systems as an escalation rather than cessation of the nuclear arms race, and therefore a violation of this clause. On the other hand, many others did not view SDI as an escalation.
SDI and MAD.
SDI was criticized for potentially disrupting the strategic doctrine of Mutual assured destruction. MAD postulated that intentional nuclear attack was inhibited by the certainty of ensuing mutual destruction. Even if a nuclear first strike destroyed many of the opponent's weapons, sufficient nuclear missiles would survive to render a devastating counter-strike against the attacker. The criticism was that SDI could have potentially allowed an attacker to survive the lighter counter-strike, thus encouraging a first strike by the side having SDI. Another destabilizing scenario was countries being tempted to strike first before SDI was deployed, thereby avoiding a disadvantaged nuclear posture. Proponents of SDI argued that SDI development might instead cause the side that did not have the resources to develop SDI, too, rather than launching a suicidal nuclear first strike attack before the SDI system was deployed, instead come to the bargaining table with the country that did have those resources, and, hopefully, agree to a real, sincere disarmament pact that would drastically decrease all forces, both nuclear and conventional. Furthermore, the MAD argument was criticized on the grounds that MAD only covered intentional, full-scale nuclear attacks by a rational, non-suicidal opponent with similar values. It did not take into account limited launches, accidental launches, rogue launches, or launches by non-state entities or covert proxies.
During the Reykjavik talks with Gorbachev in 1986, Ronald Reagan addressed Gorbachev's concerns about imbalance by stating that SDI would be given to the Soviet Union to prevent the imbalance from occurring. Gorbachev answered that he could not take this claim seriously.
Non-ICBM delivery.
Another criticism of SDI was that it would not be effective against non-space faring weapons, namely cruise missiles, bombers, short-range ballistic missile submarines and non-conventional delivery methods. However, it was never intended to act as a defense against non-space faring weapons.
Fiction and popular culture.
Because of public awareness of the program and its controversial nature, SDI has been the subject of many fictional and pop culture references. This is not intended to be a complete list of those references.

</doc>
<doc id="29187" url="https://en.wikipedia.org/wiki?curid=29187" title="SDI">
SDI

SDI may refer to:

</doc>
<doc id="29190" url="https://en.wikipedia.org/wiki?curid=29190" title="Solomon Northup">
Solomon Northup

Solomon Northup (July 10, 1807, or 1808–1863?) was an American abolitionist and the primary author of the memoir "Twelve Years a Slave". A free-born African American from New York, he was the son of a freed slave and free woman of color. A farmer and professional violinist, Northup had been a landowner in Hebron, New York. In 1841, he was offered a traveling musician's job and went to Washington, D.C. (where slavery was legal); there he was drugged, kidnapped, and sold as a slave. He was shipped to New Orleans, purchased by a planter, and held as a slave for 12 years in the Red River region of Louisiana, mostly in Avoyelles Parish. He remained in slavery until he met a Canadian working on his plantation who helped get word to New York, where state law provided aid to free New York citizens kidnapped into slavery. Family and friends enlisted the aid of the Governor of New York, Washington Hunt, and Northup regained his freedom on January 3, 1853.
The slave trader in Washington, D.C., James H. Birch, was arrested and tried, but acquitted because District of Columbia law prohibited Northup as a black man from testifying against white people. Later, in New York State, his northern kidnappers were located and charged, but the case was tied up in court for two years due to jurisdictional challenges and finally dropped when Washington, D.C., was found to have jurisdiction. The D.C. government did not pursue the case. Those who had kidnapped and enslaved Northup received no punishment.
In his first year of freedom, Northup wrote and published a memoir, "Twelve Years a Slave" (1853). He lectured on behalf of the abolitionist movement, giving more than two dozen speeches throughout the Northeast about his experiences, to build momentum against slavery. He largely disappears from the historical record in 1857 (although a letter later reported him alive in early 1863); some commentators thought he had been kidnapped again, but historians believe it unlikely, as he would have been considered too old to bring a good price. The details of his death have never been documented.
Northup's memoir was adapted and produced as the 1984 PBS television movie "Solomon Northup's Odyssey," and the 2013 feature film "12 Years a Slave". The latter won an Academy Award in 2014 for Best Picture.
Life.
Family history and education.
Solomon's father Mintus was a freedman who had been a slave in his early life in service to the Northup family. Born in Rhode Island, he was taken with the Northups when they moved to Hoosick, New York, in Rensselaer County. His master, Capt. Henry Northup, a great grandson of Stephen Northup, manumitted Mintus in his will. After being freed by Henry Northup, Mintus adopted the surname Northup as his own. The name appears interchangeably in records as Northup and Northrup.
Mintus Northup married and moved with his wife, a free woman of color, to the town of Minerva in Essex County, New York. Their two sons, Solomon and Joseph, were born free according to the principle of "partus sequitur ventrem", as their mother was a free woman. Solomon described his mother as a quadroon, meaning that she was one-quarter African American, and three-quarters European. A farmer, Mintus Northup was successful enough to own land and thus meet the state's property requirements. From 1821 on, when it revised its constitution, the state retained the property requirement for black people, but dropped it for white men, thus expanding their franchise. It is notable that Mintus Northup was able to save enough money as a freedman to buy land that satisfied this requirement, and registered to vote. He provided an education for his two sons at a level considered high for free black people at that time. As boys, Northup and his brother worked on the family farm. Mintus and his wife last lived near Fort Edward. He died on November 22, 1829, and his grave is in Hudson Falls Baker Cemetery.
Marriage and family.
In 1829 Solomon Northup married Anne Hampton. A "woman of color", she was of African, European, and Native American descent. Between 1830 to 1834 the couple lived in Fort Edward and Kingsbury, small communities in Washington County, New York.
They had three children: Elizabeth, Margaret, and Alonzo. They owned a farm in Hebron and supplemented their income by various jobs. In his later memoir, Northup describes his love for his wife as "sincere and unabated", since the time of their marriage, and his children as "beloved".
Work.
Northup held various jobs, including working as a raftsman. He built a fine reputation as a fiddler and was in high demand to play for local dances. Anne became notable as a cook and worked for local taverns, which served food and drink.
After selling their farm in 1834, the Northups moved 20 miles to Saratoga Springs, New York, for its employment opportunities. Northup played his violin at several well-known hotels in Saratoga Springs, though he found its seasonal cycles of employment difficult. He was busy during the summer, but work was scarce at other times. He worked at an assortment of jobs, constructing the Champlain Canal and the railroad, and as a skilled carpenter. Anne worked from time to time as a cook at the United States Hotel and other public houses, and she was highly praised for her culinary skills. When court was in session at the county seat of Fort Edward, she worked at Sherrill's Coffee House in Sandy Hill (now Hudson Falls) to earn extra money.
Kidnapped and sold into slavery.
In 1841, at age 32, Northup met two men, who introduced themselves as Merrill Brown and Abram Hamilton. Saying they were entertainers, members of a circus company, they offered him a job as a fiddler for several performances in New York City. Expecting the trip to be brief, Northup did not notify Anne, who was working in Sandy Hill. When they reached New York City, the men persuaded Northup to continue with them for a gig with their circus in Washington, D.C., offering him a generous wage and the cost of his return trip home. They stopped so that he could get a copy of his "free papers," which documented his status as a free man. His status was a concern as he was traveling to Washington, where slavery was legal.
The city had one of the nation's largest slave markets, and slave catchers were not above kidnapping free black people. At this time, 20 years before the Civil War, the expansion of cotton cultivation in the Deep South had led to a continuing high demand for healthy slaves. Kidnappers used a variety of means, from forced abduction to deceit, and frequently abducted children, who were easier to control.
It is possible that "Brown" and "Hamilton" incapacitated Northup—his symptoms suggest that he was drugged with belladonna or laudanum, or with a mixture of both—and sold him to Washington slave trader James H. Birch for $650, claiming that he was a fugitive slave. However, Northup stated in his account of the ordeal in "Twelve Years a Slave" in Chapter II, "hether they were accessory to my misfortunes – subtle and inhuman monsters in the shape of men – designedly luring me away from home and family, and liberty, for the sake of gold – those who read these pages will have the same means of determining as myself." Birch and Ebenezer Radburn, his jailer, severely beat Northup to stop him from saying he was a free man. Birch then wrongfully presented Northup as a slave from Georgia. Northup was held in the slave pen of trader William Williams, close to the United States Capitol. Birch shipped Northup and other slaves by sea to New Orleans, in what was called the coastwise slave trade, where his partner Theophilus Freeman would sell them. During the voyage, Northup and the other slaves caught smallpox. A slave named Robert died of the disease en route.
Northup persuaded John Manning, an English sailor, to send to Henry B. Northup, upon reaching New Orleans, a letter that told of his kidnapping and illegal enslavement. Henry was a lawyer, the son of the man who had once held Solomon's father as a slave and freed him, and a childhood friend of Solomon's. The New York State Legislature had passed a law in 1840 to protect its African-American residents by providing legal and financial assistance to aid the recovery of any who were kidnapped and taken out of state and illegally enslaved. Henry Northup was willing to help but could not act without knowing where Solomon was held.
At the New Orleans slave market, Birch's partner Theophilus Freeman sold Northup (who had been renamed Platt) to William Prince Ford, a preacher who engaged in small farming on Bayou Boeuf of the Red River in northern Louisiana. Ford was then a Baptist preacher. (In 1843, he led his congregation in converting to the closely related Churches of Christ, after they were influenced by the writings of Alexander Campbell.) In his memoir, Northup characterized Ford as a good man, considerate of his slaves. In spite of his situation, Northup wrote:
In my opinion, there never was a more kind, noble, candid, Christian man than William Ford. The influences and associations that had always surrounded him, blinded him to the inherent wrong at the bottom of the system of Slavery.
Northup's regard for Ford did not lessen his drive to escape. He made many attempts to get word to his family and friends as to where he was held, and numerous efforts to run away. He had no access to writing paper or the postal service, which made communication almost impossible. The slaves were constantly watched, and the punishments for non-compliance were brutal.
At Ford's place in Pine Woods, Northup assessed the problem of getting timber off Ford's farm to market. He proposed making log rafts to move lumber down the narrow Indian Creek, in order to transport the logs more easily and less expensively than overland. He was familiar with this process from previous work in New York, and Ford was delighted to see his project was a success. Northup used his carpentry skills to build looms, copying from one nearby, so that Ford could set up mills on the creek. With Ford, Northup found his efforts appreciated. But the planter came into financial difficulties and had to sell 18 slaves to settle his debts.
In the winter of 1842, Ford sold Northup to John M. Tibaut, a carpenter who had been working for Ford on the mills. He also had helped construct a weaving-house and corn mill on Ford's Bayou Boeuf plantation. As Tibaut did not have the full purchase price, Ford held a $400 chattel mortgage on Northup. Tibaut owed Ford $400 and Northup was the security for the loan.
Under Tibaut, Northup suffered cruel and capricious treatment. Tibaut used him to help complete construction at Ford's plantation. At one point, Tibaut whipped Northup because he did not like the nails Northup was using. But Northup fought back, beating Tibaut severely. Enraged, Tibaut recruited two friends to lynch and hang the slave, which a master was legally entitled to do. Ford's overseer Chapin interrupted and prevented the men from killing Northup, reminding Tibaut of his debt to Ford, and chasing them off at gunpoint. Northup was left bound and noosed for hours until Ford returned home to cut him down. Northup believed that Tibaut's debt to Ford saved his life. Historian Walter Johnson suggests that Northup may well have been the first slave Tibaut ever bought, marking his transition from itinerant employee to property-owning master.
Tibaut, who had a low reputation locally, decided at another point to kill Northup. When the two men were alone, Tibaut seized an axe and swung it to hit Northup, but he again defended himself. With his bare hands, he strangled Tibaut to the point of unconsciousness. Northup ran away through swamps so that dogs could not track him, making his way back to Ford, with whom he stayed for four days. The planter convinced Tibaut to "hire out" Northup to limit their conflict and take the fees he could generate.
Tibaut hired Northup out to a planter named Eldret, who lived about 38 miles south on the Red River. At what he called "The Big Cane Brake", Eldret had Northup and other slaves clear cane, trees, and undergrowth in the bottomlands in order to develop cotton fields for cultivation. With the work unfinished, after about five weeks, Tibaut sold Northup to Edwin Epps.
Epps held Northup for almost 10 years, until 1853, in Avoyelles Parish. He was a cruel master who frequently and indiscriminately punished slaves and drove them hard. His policy was to whip slaves if they did not meet daily work quotas he set for pounds of cotton to be picked and other goals. Northup wrote that the sounds of whipping were heard every day on Epps' plantation, from sundown until lights out. Epps sexually abused a young enslaved woman named Patsey, repeatedly raping her. This led to additional severe physical and mental abuse prompted by Epps's wife, the mistress of the plantation.
In 1852, itinerant Canadian carpenter Samuel Bass came to do some work for Epps. Hearing Bass express his abolitionist views, Northup eventually decided to confide his secret to him. Bass was the first person he told of his true name and origins as a free man since he was first enslaved. Along with mailing a letter written by Northup, Bass wrote several letters at his request to Northup's friends, providing general details of his location at Bayou Boeuf, in hopes of gaining his rescue.
Bass did this at great personal risk as the local people would not take kindly to a person helping a slave and depriving a man of his property. In addition, Bass's help came after passage of the Fugitive Slave Law of 1850, which increased federal penalties against people assisting slaves to escape.
Restoration of freedom.
Bass wrote several letters: one reached Cephas Parker and William Perry, storekeepers in Saratoga who knew Northup. They contacted attorney Henry B. Northup, the son of Solomon's father's former master. Henry B. Northup contacted New York Governor Washington Hunt, who took up the case, appointing the attorney general as his legal agent. In 1840, the New York State Legislature had passed a law committing the state to help any African-American residents kidnapped into slavery, as well as guaranteeing a jury trial to alleged fugitive slaves. Once Northup's family was notified, his rescuers still had to do detective work to find the enslaved man, as he had partially tried to hide his location for protection in case the letters fell into the wrong hands, and Bass had not used his real name. They had to find documentation of his free status as a citizen and New York resident; Henry B. Northup also collected sworn affidavits from people who knew Solomon Northup. During this time, Northup did not know if Bass had reached anyone with the letters. There was no means of communicating, due to the secrecy they needed to maintain, and the necessity of preventing Northup's owner from knowing their plans.
Bass was itinerant and had no local family. (Unbeknownst to his friends in Louisiana, he had left a wife and children in Canada. He also lived with a free woman of color in Louisiana.) Because of the risk, Bass did not reveal his own name in the letter. Henry Northup still managed to find him in Louisiana, and Bass revealed that Solomon Northup was held by Edwin Epps on his plantation. Henry B. Northup took the precaution of bringing with him the sheriff of Marksville, the parish seat, to enforce the law.
In cooperation with U.S. Senator Pierre Soulé from Louisiana and other local authorities, Henry B. Northup arrived in Marksville on January 1, 1853. Tracing Northup was difficult as he was known locally only by his slave name of Platt. When the attorney confronted Epps with the evidence that Platt/Northup was a free man, with a wife and children, Epps first demanded of the enslaved man why he had not told him this at the time of purchase. Then Epps said, had he known that men were coming to take "Platt," he would have ensured they could never take the slave alive. Epps cursed the man (unknown to him) who had helped Northup, and threatened to kill him if he ever learned his identity.
Northup later wrote, "He thought of nothing but his loss, and cursed me for having been born free." Attorney Henry B. Northup convinced Epps that it would be futile to contest the free papers in a court of law, so the planter conceded the case. He signed papers giving up all claim to Northup. Finally on January 4, 1853, four months after meeting Bass, Northup regained his freedom.
Court cases and memoir.
Northup was one of the few kidnapped free black people to regain freedom after being sold into slavery. Represented by attorneys Senator Salmon P. Chase of Ohio, a General Clark, and Henry B. Northup, Solomon Northup sued Birch and other men involved in selling him into slavery in Washington, DC.
As Solomon Northup and Henry Northup made their way back to New York, they first stopped in Washington DC to file a legal complaint with the police magistrate against James H. Birch, the man who had first enslaved him. Birch was immediately arrested and tried on criminal charges. But, Northup was unable to testify at the trial due to laws in Washington DC against black men testifying in court. Birch and several confederates, who were also in the slave trade, testified that Northup had approached them, saying he was a slave from Georgia and was for sale. No note of his purchase was made in Birch's accounting ledger, however. The prosecution consisted of Henry B. Northup and another white man asserting that they had known Northup for many years, and he was born and lived a free man in New York until his abduction. With no one legally able to testify against Birch's tale, Birch was found not guilty. However, the sensational case immediately attracted national attention, and "The New York Times" published an article about the trial on January 20, 1853, just days after its conclusion and only two weeks after Northup's rescue.
Following his acquittal, Birch demanded charges be filed against Solomon Northup for trying to defraud him of Northup's $625 purchase price by falsely claiming he was a Georgia slave for sale. Northup, eager to prove the veracity of his own story, urged the trial to proceed. Upon the advice of his lawyer, Birch withdrew the complaint, against the protests of Northup. Northup knew that a trial related to Birch's complaint could only rebound against Birch and make him look bad. If Northup had in fact claimed to be a slave from Georgia, it would not have made sense for him to risk his freedom, days after regaining it, by contacting the law to bring charges against Birch.
At the time, Northup did not file a legal complaint against the men with the circus, Alexander Merrill and Joseph Russell, because they could not be found, having used false names with Northup. At first, Northup had trouble believing they could be complicit.
Later that same year, Solomon Northup wrote and published his memoir, "Twelve Years a Slave" (1853). The book was written in three months with the help of David Wilson, a local writer and journalist. Published by Derby & Miller of Auburn, New York In the period when questions of slavery generated debate and the novel "Uncle Tom's Cabin" (1852) by Harriet Beecher Stowe was a bestseller, Northup's book sold 30,000 copies within three years, also becoming a bestseller.
When the book and case were publicized, Thaddeus St. John, a county court judge in nearby Fonda, New York, recalled having seen two old friends, Alexander Merrill and Joseph Russell, traveling with a black man to Washington, DC at the time of the late President Harrison's funeral in 1841. He saw them again while returning from Washington, but they were without the black man. They wore and carried new extravagantly expensive items, and he recalled an odd conversation with them during the first trip. They had asked him then to call them Brown and Hamilton when in company with the black man, rather than Merrill and Russell, as he knew them. After contacting authorities, St. John met with Northup. The two recognized each other from the first encounter on the train in 1840. With this identification, Merrill and Russell were located and arrested.
The New York trial opened on October 4, 1854. Both Northup and St. John testified against the two men. The case brought widespread illegal practices in the domestic slave trade to light. Through testimony during the court case, various details of Northup's account of his experience were confirmed. The respective counsels argued over whether the crime had been committed in New York (where Northup could testify), or in Washington, DC, outside the jurisdiction of New York courts. After more than two years of appeals, a new district attorney in New York failed to continue with the case, and it was dropped in May 1857. Washington, DC authorities declined to prosecute Merrill and Russell, and no further legal action was taken against those who had kidnapped and sold Northup into slavery.
Last years.
After regaining his freedom, Solomon Northup rejoined his wife and children. By 1855 he was living with his daughter Margaret Stanton and her family in Queensbury, Warren County, New York. He was working again as a carpenter. He became active in the abolitionist movement and lectured on slavery on nearly two dozen occasions throughout the northeastern United States in the years before the American Civil War.
During the summer of 1857 Northup was in Canada for a series of lectures. It was widely reported that Northup was in Streetsville, Ontario, but that a hostile Canadian crowd prevented him from speaking. There is no contemporaneous documentation of his whereabouts after that time. The location and circumstances of his death are unknown. Rumors ran rife. In 1858 a newspaper reported, "It is said that Solomon Northup, who was kidnapped, sold as a slave, and afterwards recovered and restored to freedom has been again decoyed South, and is again a slave." Shortly thereafter, even his benefactor Henry B. Northup is said to have believed Solomon had been kidnapped from Canada while drunk.
Years later, in "The Bench and Bar of Saratoga County" (1879), E. R. Mann mistakenly wrote that the Saratoga County kidnapping case against Merrill and Russell had been dismissed because Northup had disappeared. Mann speculated, "What his fate was is unknown to the public, but the desperate kidnappers no doubt knew." In 1909 John Henry Northup, Henry's nephew, wrote: "The last I heard of him, Sol was lecturing in Boston to help sell his book. All at once, he disappeared. We believe that he was kidnapped and taken away or killed." According to John R. Smith, in letters written in the 1930s, he said that his father Rev. John L. Smith, a Methodist minister in Vermont, had worked with Northup and former slave Tabbs Gross in the early 1860s, during the American Civil War, aiding fugitive slaves on the Underground Railroad. Northup was said to have visited Rev. Smith after Lincoln's Emancipation Proclamation, which was made in January 1863.
Northup was not listed with his family in the 1860 United States Census. The New York state census of 1865 records his wife Anne Northup (but not Solomon); she was recorded as married, not widowed, and living with their daughter and son-in-law, Margaret and Philip Stanton, in nearby Moreau in Saratoga County.
In 1870, Northup's wife was enumerated as a cook in the household of Burton C. Dennis. At the time Dennis kept the Middleworth House hotel in Sandy Hill, New York. Solomon Northup is not listed among those living at the hotel. That same year, his daughter, Margaret Stanton, and his son-in-law appear in the census schedule for Moreau, New York, but Northup's name is not there, either. Northup's son, Alonzo, is included in the 1870 census for Fort Edward, New York; his household includes only him, his wife and his daughter.
In 1875 Anne Northup was living in Kingsbury/Sandy Hill in Washington County, New York, and, in census information, her marital status was given as "now widowed." When Anne Northup died in 1876, some newspaper notices of her death said that she was a widow. One obituary, while praising Anne, says of Solomon Northup that "after exhibiting himself through the country became a worthless vagabond."
The 21st-century historians Clifford Brown and Carol Wilson believe it is likely that he died of natural causes. They think a kidnapping for slavery in the late 1850s was unlikely, as he was too old to be of interest to slave catchers, but his disappearance remains unexplained.
Historiography.
Although the memoir is often classified among the genre of slave narratives, the scholar Sam Worley says that it does not fit the standard format of the genre. Northup was assisted in the writing by David Wilson, a white man, and, according to Worley, some believed he would have biased the material. Worley discounted concerns that Wilson was pursuing his own interests in the book. He writes of the memoir:
"Twelve Years" is convincingly Northup's tale and no one else's because of its amazing attention to empirical detail and unwillingness to reduce the complexity of Northup's experience to a stark moral allegory.
Northup's biographer, David Fiske, has investigated Northup's role in the book's writing and asserts that he made it authentic. Northup's full and descriptive account has been used by numerous historians researching slavery. His description of the "Yellow House" (also known as 'The Williams Slave Pen'), in view of the Capitol, has helped researchers document the history of slavery in the District of Columbia.

</doc>
<doc id="29192" url="https://en.wikipedia.org/wiki?curid=29192" title="Space elevator">
Space elevator

A space elevator is a proposed type of space transportation system. The main component would be a cable (also called a tether) anchored to the surface and extending into space. The design would permit vehicles to travel along the cable from a planetary surface, such as the Earth's, directly into space or orbit, without the use of large rockets. An Earth-based space elevator would consist of a cable with one end attached to the surface near the equator and the other end in space beyond geostationary orbit (35,800 km altitude). The competing forces of gravity, which is stronger at the lower end, and the outward/upward centrifugal force, which is stronger at the upper end, would result in the cable being held up, under tension, and stationary over a single position on Earth. With the tether deployed, climbers could repeatedly climb the tether to space by mechanical means, releasing their cargo to orbit. Climbers could also descend the tether to return cargo to the surface from orbit.
The concept of a space elevator was first published in 1895 by Konstantin Tsiolkovsky. His proposal was for a free-standing tower reaching from the surface of Earth to the height of geostationary orbit. Like all buildings, Tsiolkovsky's structure would be under compression, supporting its weight from below. Since 1959, most ideas for space elevators have focused on purely tensile structures, with the weight of the system held up from above by centrifugal forces. In the tensile concepts, a space tether reaches from a large mass (the counterweight) beyond geostationary orbit to the ground. This structure is held in tension between Earth and the counterweight like an upside-down plumb bob.
To construct a space elevator on Earth the cable material would need to be both stronger and lighter (have greater specific strength) than any known material. Development of new materials which could meet the demanding specific strength requirement is required for designs to progress beyond discussion stage. Carbon nanotubes (CNTs) have been identified as possibly being able to meet the specific strength requirements for an Earth space elevator. Other materials considered have been boron nitride nanotubes, and diamond nanothreads which were first constructed in 2014.
The concept is applicable to other planets and celestial bodies. For locations in the solar system with weaker gravity than Earth's (such as the Moon or Mars), the strength-to-density requirements for tether materials are not as problematic. Currently available materials (such as Kevlar) are strong and light enough that they could be used as the tether material for elevators there.
History.
Early concepts.
The key concept of the space elevator appeared in 1895 when Russian scientist Konstantin Tsiolkovsky was inspired by the Eiffel Tower in Paris. He considered a similar tower that reached all the way into space and was built from the ground up to the altitude of 35,790 kilometers, the height of geostationary orbit. He noted that the top of such a tower would be circling Earth as in a geostationary orbit. Objects would attain horizontal velocity as they rode up the tower, and an object released at the tower's top would have enough horizontal velocity to remain there in geostationary orbit. Tsiolkovsky's conceptual tower was a compression structure, while modern concepts call for a tensile structure (or "tether").
20th century.
Building a compression structure from the ground up proved an unrealistic task as there was no material in existence with enough compressive strength to support its own weight under such conditions. In 1959 another Russian scientist, Yuri N. Artsutanov, suggested a more feasible proposal. Artsutanov suggested using a geostationary satellite as the base from which to deploy the structure downward. By using a counterweight, a cable would be lowered from geostationary orbit to the surface of Earth, while the counterweight was extended from the satellite away from Earth, keeping the cable constantly over the same spot on the surface of the Earth. Artsutanov's idea was introduced to the Russian-speaking public in an interview published in the Sunday supplement of "Komsomolskaya Pravda" in 1960, but was not available in English until much later. He also proposed tapering the cable thickness so that the stress in the cable was constant. This gave a thinner cable at ground level that became thickest at the level of geostationary orbit.
Both the tower and cable ideas were proposed in the quasi-humorous "Ariadne" column in "New Scientist", December 24, 1964.
In 1966, Isaacs, Vine, Bradner and Bachus, four American engineers, reinvented the concept, naming it a "Sky-Hook," and published their analysis in the journal "Science". They decided to determine what type of material would be required to build a space elevator, assuming it would be a straight cable with no variations in its cross section, and found that the strength required would be twice that of any then-existing material including graphite, quartz, and diamond.
In 1975 an American scientist, Jerome Pearson, reinvented the concept yet again, publishing his analysis in the journal Acta Astronautica. He designed a tapered cross section that would be better suited to building the elevator. The completed cable would be thickest at the geostationary orbit, where the tension was greatest, and would be narrowest at the tips to reduce the amount of weight per unit area of cross section that any point on the cable would have to bear. He suggested using a counterweight that would be slowly extended out to , almost half the distance to the Moon as the lower section of the elevator was built. Without a large counterweight, the upper portion of the cable would have to be longer than the lower due to the way gravitational and centrifugal forces change with distance from Earth. His analysis included disturbances such as the gravitation of the Moon, wind and moving payloads up and down the cable. The weight of the material needed to build the elevator would have required thousands of Space Shuttle trips, although part of the material could be transported up the elevator when a minimum strength strand reached the ground or be manufactured in space from asteroidal or lunar ore.
A fictional organic form of space elevator appears in Brian Aldiss' 1962 "Hothouse" or "The Long Afternoon of Earth" - spider-like plants called "traversers" spun webs connecting Earth and Moon, which were "locked face to face, and so would be, until the sands of time ran out or the sun ceased to shine." In 1979, space elevators were introduced to a broader audience with the simultaneous publication of Arthur C. Clarke's novel, "The Fountains of Paradise", in which engineers construct a space elevator on top of a mountain peak in the fictional island country of "Taprobane" (loosely based on Sri Lanka, albeit moved south to the Equator), and Charles Sheffield's first novel, "The Web Between the Worlds", also featuring the building of a space elevator. Three years later, in Robert A. Heinlein's 1982 novel "Friday" the principal character makes use of the "Nairobi Beanstalk" in the course of her travels. In Kim Stanley Robinson's 1993 novel "Red Mars", colonists build a space elevator on Mars that allows both for more colonists to arrive and also for natural resources mined there to be able to leave for Earth. In David Gerrold's 2000 novel, "Jumping Off The Planet", a family excursion up the Ecuador "beanstalk" is actually a child-custody kidnapping. Gerrold's book also examines some of the industrial applications of a mature elevator technology. In a biological version, Joan Slonczewski's novel "The Highest Frontier" depicts a college student ascending a space elevator constructed of self-healing cables of anthrax bacilli. The engineered bacteria can regrow the cables when severed by space debris.
After the development of carbon nanotubes in the 1990s, engineer David Smitherman of NASA/Marshall's Advanced Projects Office realized that the high strength of these materials might make the concept of a space elevator feasible, and put together a workshop at the Marshall Space Flight Center, inviting many scientists and engineers to discuss concepts and compile plans for an elevator to turn the concept into a reality.
In 2000, another American scientist, Bradley C. Edwards, suggested creating a long paper-thin ribbon using a carbon nanotube composite material. He chose the wide-thin ribbon-like cross-section shape rather than earlier circular cross-section concepts because that shape would stand a greater chance of surviving impacts by meteoroids. The ribbon cross-section shape also provided large surface area for climbers to climb with simple rollers. Supported by the NASA Institute for Advanced Concepts, Edwards' work was expanded to cover the deployment scenario, climber design, power delivery system, orbital debris avoidance, anchor system, surviving atomic oxygen, avoiding lightning and hurricanes by locating the anchor in the western equatorial Pacific, construction costs, construction schedule, and environmental hazards.
21st century.
To speed space elevator development, proponents have organized several competitions, similar to the Ansari X Prize, for relevant technologies. Among them are , which organized annual competitions for climbers, ribbons and power-beaming systems from 2005 to 2009, the Robogames Space Elevator Ribbon Climbing competition, as well as NASA's Centennial Challenges program, which, in March 2005, announced a partnership with the Spaceward Foundation (the operator of Elevator:2010), raising the total value of prizes to US$400,000.
The first European Space Elevator Challenge (EuSEC) to establish a climber structure took place in August 2011.
In 2005, "the LiftPort Group of space elevator companies announced that it will be building a carbon nanotube manufacturing plant in Millville, New Jersey, to supply various glass, plastic and metal companies with these strong materials. Although LiftPort hopes to eventually use carbon nanotubes in the construction of a space elevator, this move will allow it to make money in the short term and conduct research and development into new production methods." Their announced goal was a space elevator launch in 2010. On February 13, 2006 the LiftPort Group announced that, earlier the same month, they had tested a mile of "space-elevator tether" made of carbon-fiber composite strings and fiberglass tape measuring wide and 1 mm (approx. 13 sheets of paper) thick, lifted with balloons.
In 2007, held the 2007 Space Elevator games, which featured US$500,000 awards for each of the two competitions, (US$1,000,000 total) as well as an additional US$4,000,000 to be awarded over the next five years for space elevator related technologies. No teams won the competition, but a team from MIT entered the first 2-gram (0.07 oz), 100-percent carbon nanotube entry into the competition. Japan held an international conference in November 2008 to draw up a timetable for building the elevator.
In 2008 the book "Leaving the Planet by Space Elevator" by Dr. Brad Edwards and Philip Ragan was published in Japanese and entered the Japanese best-seller list. This led to Shuichi Ono, chairman of the Japan Space Elevator Association, unveiling a space-elevator plan, putting forth what observers considered an extremely low cost estimate of a trillion yen (£5 billion/ $8 billion) to build one.
In 2012, the Obayashi Corporation announced that in 38 years it could build a space elevator using carbon nanotube technology. At 200 kilometers per hour, the design's 30-passenger climber would be able to reach the GEO level after a 7.5 day trip. No cost estimates, finance plans, or other specifics were made. This, along with timing and other factors, hinted that the announcement was made largely to provide publicity for the opening of one of the company's other projects in Tokyo.
In 2013, the International Academy of Astronautics published a technological feasibility assessment which concluded that the critical capability improvement needed was the tether material, which was projected to achieve the necessary strength-to-weight ratio within 20 years. The four-year long study looked into many facets of space elevator development including missions, development schedules, financial investments, revenue flow, and benefits. It was reported that it would be possible to operationally survive smaller impacts and avoid larger impacts, with meteors and space debris, and that the estimated cost of lifting a kilogram of payload to GEO and beyond would be $500.
In 2014, Google X's Rapid Evaluation R&D team began the design of a Space Elevator, eventually finding that no one had yet manufactured a perfectly formed carbon nanotube strand longer than a meter. They thus decided to put the project in "deep freeze" and also keep tabs on any advances in the carbon nanotube field.
Physics.
Apparent gravitational field.
A space elevator cable rotates along with the rotation of the Earth. Therefore, objects attached to the cable would experience upward centrifugal force in the direction opposing the downward gravitational force. The higher up the cable the object is located, the less the gravitational pull of the Earth, and the stronger the upward centrifugal force due to the rotation, so that more centrifugal force opposes less gravity. The centrifugal force and the gravity are balanced at geosynchronous equatorial orbit (GEO). Above GEO, the centrifugal force is stronger than gravity, causing objects attached to the cable there to pull "upward" on it.
The net force for objects attached to the cable is called the "apparent gravitational field". The apparent gravitational field for attached objects is the (downward) gravity minus the (upward) centrifugal force. The apparent gravity experienced by an object on the cable is zero at GEO, downward below GEO, and upward above GEO.
The apparent gravitational field can be represented this way:
where
At some point up the cable, the two terms (downward gravity and upward centrifugal force) are equal and opposite. Objects fixed to the cable at that point put no weight on the cable. This occurs at the altitude of geostationary orbit. This altitude (r1) depends on the mass of the planet and its rotation rate. Setting actual gravity equal to centrifugal acceleration gives:
On Earth, this distance is above the surface, the altitude of geostationary orbit.
On the cable "below" geostationary orbit, downward gravity would be greater than the upward centrifugal force, so the apparent gravity would pull objects attached to the cable downward. Any object released from the cable below that level would initially accelerate downward along the cable. Then gradually it would deflect eastward from the cable. On the cable "above" the level of stationary orbit, upward centrifugal force would be greater than downward gravity, so the apparent gravity would pull objects attached to the cable "upward". Any object released from the cable "above" the geosynchronous level would initially accelerate "upward" along the cable. Then gradually it would deflect westward from the cable.
Cable section.
Historically, the main technical problem has been considered the ability of the cable to hold up, with tension, the weight of itself below any given point. The greatest tension on a space elevator cable is at the point of geostationary orbit, above the Earth's equator. This means that the cable material, combined with its design, must be strong enough to hold up its own weight from the surface up to . A cable which is thicker in cross section at that height than at the surface could better hold up its own weight over a longer length. How the cross section area tapers from the maximum at to the minimum at the surface is therefore an important design factor for a space elevator cable.
To maximize the usable excess strength for a given amount of cable material, the cable's cross section area would need to be designed for the most part in such a way that the stress (i.e., the tension per unit of cross sectional area) is constant along the length of the cable. The constant-stress criterion is a starting point in the design of the cable cross section as it changes with altitude. Other factors considered in more detailed designs include thickening at altitudes where more space junk is present, consideration of the point stresses imposed by climbers, and the use of varied materials. To account for these and other factors, modern detailed cross section designs seek to achieve the largest "safety margin" possible, with as little variation over altitude and time as possible. In simple starting-point designs, that equates to constant-stress.
In the constant-stress case, the cross-section follows this differential equation:
or
or
where
The value of "g" is given by the first equation, which yields:
the variation being taken between "r0" (ground) and "r1" (geostationary).
Between these two points, this quantity can be expressed as:
formula_9, or
where formula_11 is the ratio between the centrifugal force on the equator and the gravitational force.
Cable material.
To compare materials, the "specific strength" of the material for the space elevator can be expressed in terms of the "characteristic length", or "free breaking length": the length of an un-tapered cylindrical cable at which it will break under its own weight under constant gravity. For a given material, that length is Lc = "σ/(ρ g0)", 
where σ, ρ and g0 are a defined above.
The free breaking length needed is given by the equation
where formula_13
If one does not take into account the "x" factor (which reduces the strength needed by about 30 percent), this equation also says that the section ratio equals "e" (exponential one) when:
If the material can support a free breaking length of only one tenth this, the section needed at a geosynchronous orbit will be "e"10 (a factor of 22026) times the ground section.
Structure.
There are a variety of space elevator designs. Almost every design includes a base station, a cable, climbers, and a counterweight. Earth's rotation creates upward centrifugal force on the counterweight. The counterweight is held down by the cable while the cable is held up and taut by the counterweight. The base station anchors the whole system to the surface of the Earth. Climbers climb up and down the cable with cargo.
Base station.
Modern concepts for the base station/anchor are typically mobile stations, large oceangoing vessels or other mobile platforms. Mobile base stations would have the advantage over the earlier stationary concepts (with land-based anchors) by being able to maneuver to avoid high winds, storms, and space debris. Oceanic anchor points are also typically in international waters, simplifying and reducing cost of negotiating territory use for the base station.
Stationary land based platforms would have simpler and less costly logistical access to the base. They also would have an advantage of being able to be at high altitude, such as on top of mountains. In an alternate concept, the base station could be a tower, forming a space elevator which comprises both a compression tower close to the surface, and a tether structure at higher altitudes. Combining a compression structure with a tension structure would reduce loads from the atmosphere at the Earth end of the tether, and reduce the distance into the Earth's gravity field the cable needs to extend, and thus reduce the critical strength-to-density requirements for the cable material, all other design factors being equal.
Cable.
A space elevator cable would need to carry its own weight as well as the additional weight of climbers. The required strength of the cable would vary along its length. This is because at various points it would have to carry the weight of the cable below, or provide a downward force to retain the cable and counterweight above. Maximum tension on a space elevator cable would be at geosynchronous altitude so the cable would have to be thickest there and taper carefully as it approaches Earth. Any potential cable design may be characterized by the taper factor – the ratio between the cable's radius at geosynchronous altitude and at the Earth's surface.
The cable would need to be made of a material with a large tensile strength/density ratio. For example, the Edwards space elevator design assumes a cable material with a specific strength of at least 100,000 kN/(kg/m). This value takes into consideration the entire weight of the space elevator. An untapered space elevator cable would need a material capable of sustaining a length of of its own weight "at sea level" to reach a geostationary altitude of without yielding. Therefore, a material with very high strength and lightness is needed.
For comparison, metals like titanium, steel or aluminium alloys have breaking lengths of only 20–30 km. Modern fibre materials such as kevlar, fibreglass and carbon/graphite fibre have breaking lengths of 100–400 km. Nanoengineered materials such as carbon nanotubes and, more recently discovered, graphene ribbons (perfect two-dimensional sheets of carbon) are expected to have breaking lengths of 5000–6000 km at sea level, and also are able to conduct electrical power.
For a space elevator on Earth, with its comparatively high gravity, the cable material would need to be stronger and lighter than currently available materials. For this reason, there has been a focus on the development of new materials that meet the demanding specific strength requirement. For high specific strength, carbon has advantages because it is only the 6th element in the periodic table. Carbon has comparatively few of the protons and neutrons which contribute most of the dead weight of any material. Most of the interatomic bonding forces of any element are contributed by only the outer few electrons. For carbon, the strength and stability of those bonds is high compared to the mass of the atom. The challenge in using carbon nanotubes remains to extend to macroscopic sizes the production of such material that are still perfect on the microscopic scale (as microscopic defects are most responsible for material weakness).
In 2014, diamond nanothreads were first synthesized. Since they have strength properties similar to carbon nanotubes, diamond nanothreads were quickly seen as candidate cable material as well.
Climbers.
A space elevator cannot be an elevator in the typical sense (with moving cables) due to the need for the cable to be significantly wider at the center than at the tips. While various designs employing moving cables have been proposed, most cable designs call for the "elevator" to climb up a stationary cable.
Climbers cover a wide range of designs. On elevator designs whose cables are planar ribbons, most propose to use pairs of rollers to hold the cable with friction.
Climbers would need to be paced at optimal timings so as to minimize cable stress and oscillations and to maximize throughput. Lighter climbers could be sent up more often, with several going up at the same time. This would increase throughput somewhat, but would lower the mass of each individual payload.
The horizontal speed, i.e. due to orbital rotation, of each part of the cable increases with altitude, proportional to distance from the center of the Earth, reaching low orbital speed at a point approximately 66 percent of the height between the surface and geostationary orbit (a height of about 23,400 km). A payload released at this point would go into a highly eccentric elliptical orbit, staying just barely clear from atmospheric reentry, with the periapsis at the same altitude as LEO and the apoapsis at the release height. With increasing release height the orbit would become less eccentric as both periapsis and apoapsis increase, becoming circular at geostationary level.
When the payload has reached GEO, the horizontal speed is exactly the speed of a circular orbit at that level, so that if released, it would remain adjacent to that point on the cable. The payload can also continue climbing further up the cable beyond GEO, allowing it to obtain higher speed at jettison. If released from 100,000 km, the payload would have enough speed to reach the asteroid belt.
As a payload is lifted up a space elevator, it would gain not only altitude, but horizontal speed (angular momentum) as well. The angular momentum is taken from the Earth's rotation. As the climber ascends, it is initially moving slower than each successive part of cable it is moving on to. This is the Coriolis force: the climber "drags" (westward) on the cable, as it climbs, and slightly decreases the Earth's rotation speed. The opposite process would occur for descending payloads: the cable is tilted eastwards, thus slightly increasing Earth's rotation speed.
The overall effect of the centrifugal force acting on the cable would cause it to constantly try to return to the energetically favorable vertical orientation, so after an object has been lifted on the cable, the counterweight would swing back towards the vertical like an inverted pendulum. Space elevators and their loads would be designed so that the center of mass is always well-enough above the level of geostationary orbit to hold up the whole system. Lift and descent operations would need to be carefully planned so as to keep the pendulum-like motion of the counterweight around the tether point under control.
Climber speed would be limited by the Coriolis force, available power, and by the need to ensure the climber's accelerating force does not break the cable. Climbers would also need to maintain a minimum average speed in order to move material up and down economically and expeditiously. At the speed of a very fast car or train of it will take about 5 days to climb to geosynchronous orbit.
Powering climbers.
Both power and energy are significant issues for climbers—the climbers would need to gain a large amount of potential energy as quickly as possible to clear the cable for the next payload.
Various methods have been proposed to get that energy to the climber:
Wireless energy transfer such as laser power beaming is currently considered the most likely method. Using megawatt powered free electron or solid state lasers in combination with adaptive mirrors approximately wide and a photovoltaic array on the climber tuned to the laser frequency for efficiency. For climber designs powered by power beaming, this efficiency is an important design goal. Unused energy would need to be re-radiated away with heat-dissipation systems, which add to weight.
Yoshio Aoki, a professor of precision machinery engineering at Nihon University and director of the Japan Space Elevator Association, suggested including a second cable and using the conductivity of carbon nanotubes to provide power.
Counterweight.
Several solutions have been proposed to act as a counterweight:
Extending the cable has the advantage of some simplicity of the task and the fact that a payload that went to the end of the counterweight-cable would acquire considerable velocity relative to the Earth, allowing it to be launched into interplanetary space. Its disadvantage is the need to produce greater amounts of cable material as opposed to using anything that has mass.
Launching into deep space.
An object attached to a space elevator at a radius of approximately 53,100 km would be at escape velocity when released. Transfer orbits to the L1 and L2 Lagrangian points could be attained by release at 50,630 and 51,240 km, respectively, and transfer to lunar orbit from 50,960 km.
At the end of Pearson's cable, the tangential velocity is 10.93 kilometers per second (6.79 mi/s). That is more than enough to escape Earth's gravitational field and send probes at least as far out as Jupiter. Once at Jupiter, a gravitational assist maneuver could permit solar escape velocity to be reached.
Extraterrestrial elevators.
A space elevator could also be constructed on other planets, asteroids and moons.
A Martian tether could be much shorter than one on Earth. Mars' surface gravity is 38 percent of Earth's, while it rotates around its axis in about the same time as Earth. Because of this, Martian stationary orbit is much closer to the surface, and hence the elevator could be much shorter. Current materials are already sufficiently strong to construct such an elevator. Building a Martian elevator would be complicated by the Martian moon Phobos, which is in a low orbit and intersects the Equator regularly (twice every orbital period of 11 h 6 min).
On the near side of the Moon, the strength-to-density required of the tether of a lunar space elevator exists in currently available materials. A lunar space elevator would be about long. Since the Moon does not rotate fast enough, there is no effective lunar-stationary orbit, but the Lagrangian points could be used. The near side would extend through the Earth-Moon L1 point from an anchor point near the center of the visible part of Earth's Moon.
On the far side of the Moon, a lunar space elevator would need to be very long—more than twice the length of an Earth elevator—but due to the low gravity of the Moon, could also be made of existing engineering materials.
Rapidly spinning asteroids or moons could use cables to eject materials to convenient points, such as Earth orbits; or conversely, to eject materials to send a portion of the mass of the asteroid or moon to Earth orbit or a Lagrangian point. Freeman Dyson, a physicist and mathematician, has suggested using such smaller systems as power generators at points distant from the Sun where solar power is uneconomical.
A space elevator using presently available engineering materials could be constructed between mutually tidally locked worlds, such as Pluto and Charon or the components of binary asteroid Antiope, with no terminus disconnect, according to Francis Graham of Kent State University. However, spooled variable lengths of cable must be used due to ellipticity of the orbits.
Construction.
The construction of a space elevator would need reduction of some technical risk. Some advances in engineering, manufacturing and physical technology are required. Once a first space elevator is built, the second one and all others would have the use of the previous ones to assist in construction, making their costs considerably lower. Such follow-on space elevators would also benefit from the great reduction in technical risk achieved by the construction of the first space elevator.
Prior to the work of Edwards in 2000 most concepts for constructing a space elevator had the cable manufactured in space. That was thought to be necessary for such a large and long object and for such a large counterweight. Manufacturing the cable in space would be done in principle by using an asteroid or Near-Earth object for source material. These earlier concepts for construction require a large preexisting space-faring infrastructure to maneuver an asteroid into its needed orbit around Earth. They also required the development of technologies for manufacture in space of large quantities of exacting materials.
Since 2001, most work has focused on simpler methods of construction requiring much smaller space infrastructures. They conceive the launch of a long cable on a large spool, followed by deployment of it in space. The spool would be initially parked in a geostationary orbit above the planned anchor point. A long cable would be dropped "downward" (toward Earth) and would be balanced by a mass being dropped "upward" (away from Earth) for the whole system to remain on the geosynchronous orbit. Earlier designs imagined the balancing mass to be another cable (with counterweight) extending upward, with the main spool remaining at the original geosynchronous orbit level. Most current designs elevate the spool itself as the main cable is paid out, a simpler process. When the lower end of the cable is so long as to reach the surface of the Earth (at the equator), it would be anchored. Once anchored, the center of mass would be elevated more (by adding mass at the upper end or by paying out more cable). This would add more tension to the whole cable, which could then be used as an elevator cable.
One plan for construction uses conventional rockets to place a "minimum size" initial seed cable of only 19,800 kg. This first very small ribbon would be adequate to support the first 619 kg climber. The first 207 climbers would carry up and attach more cable to the original, increasing its cross section area and widening the initial ribbon to about 160 mm wide at its widest point. The result would be a 750-ton cable with a lift capacity of 20 tons per climber.
Safety issues and construction challenges.
For early systems, transit times from the surface to the level of geosynchronous orbit would be about five days. On these early systems, the time spent moving through the Van Allen radiation belts would be enough that passengers would need to be protected from radiation by shielding, which would add mass to the climber and decrease payload.
A space elevator would present a navigational hazard, both to aircraft and spacecraft. Aircraft could be diverted by air-traffic control restrictions. All objects in stable orbits that have perigee below the maximum altitude of the cable that are not synchronous with the cable would impact the cable eventually, unless avoiding action is taken. One potential solution proposed by Edwards is to use a movable anchor (a sea anchor) to allow the tether to "dodge" any space debris large enough to track.
Impacts by space objects such as meteoroids, micrometeorites and orbiting man-made debris, pose another design constraint on the cable. A cable would need to be designed to maneuver out of the way of debris, or absorb impacts of small debris without breaking.
Economics.
With a space elevator, materials might be sent into orbit at a fraction of the current cost. As of 2000, conventional rocket designs cost about US$25,000 per kilogram (US$11,000 per pound) for transfer to geostationary orbit. Current proposals envision payload prices starting as low as $220 per kilogram ($100 per pound), similar to the $5–$300/kg estimates of the Launch loop, but higher than the $310/ton to 500 km orbit quoted to Dr. Jerry Pournelle for an orbital airship system.
Philip Ragan, co-author of the book "Leaving the Planet by Space Elevator", states that "The first country to deploy a space elevator will have a 95 percent cost advantage and could potentially control all space activities."
International Space Elevator Consortium (ISEC).
The International Space Elevator Consortium (ISEC) was formed to promote the development, construction, and operation of a space elevator as "a revolutionary and efficient way to space for all humanity". It was formed after the Space Elevator Conference in Redmond, Washington in July 2008 and became an affiliate organization with the National Space Society in August 2013.
ISEC coordinates with the two other major societies focusing on space elevators: the Japanese Space Elevator Association and EuroSpaceward. ISEC supports symposia and presentations at the International Academy of Astronautics and the International Astronautical Federation Congress each year. The organization published two issues of a peer-reviewed journal on space elevators called "CLIMB".
ISEC also conducts one-year studies focusing on individual topics. The process involves experts for one year of discussions on the topic of choice and culminates in a draft report that is presented and reviewed at the ISEC Space Elevator conference workshop. This review of the major conclusions allows input from space elevator enthusiasts as well as other experts. Topics that have concluded are: 2010 - Space Elevator Survivability, Space Debris Mitigation, 2012 - Space Elevator Concept of Operations, 2013 - Design Consideration for Tether Climbers, and 2014 - Space Elevator Architectures and Roadmaps.
Related concepts.
The conventional current concept of a "Space Elevator" has evolved from a static compressive structure reaching to the level of GEO, to the modern baseline idea of a static tensile structure anchored to the ground and extending to well above the level of GEO. In the current usage by practitioners (and in this article), a "Space Elevator" means the Tsiolkovsky-Artsutanov-Pearson type as considered by the International Space Elevator Consortium. This conventional type is a static structure fixed to the ground and extending into space high enough that cargo can climb the structure up from the ground to a level where simple release will put the cargo into an orbit.
Some concepts related to this modern baseline are not usually termed a "Space Elevator", but are similar in some way and are sometimes termed "Space Elevator" by their proponents. For example, Hans Moravec published an article in 1977 called "A Non-Synchronous Orbital Skyhook" describing a concept using a rotating cable. The rotation speed would exactly match the orbital speed in such a way that the tip velocity at the lowest point was zero compared to the object to be "elevated". It would dynamically grapple and then "elevate" high flying objects to orbit or low orbiting objects to higher orbit.
The original concept envisioned by Tsiolkovsky was a compression structure, a concept similar to an aerial mast. While such structures might reach space (100 km, 62 mi), they are unlikely to reach geostationary orbit. The concept of a Tsiolkovsky tower combined with a classic space elevator cable (reaching above the level of GEO) has been suggested. Other ideas use very tall compressive towers to reduce the demands on launch vehicles. The vehicle is "elevated" up the tower, which may extend as high as above the atmosphere, and is launched from the top. Such a tall tower to access near-space altitudes of has been proposed by various researchers.
Other concepts for non-rocket spacelaunch related to a space elevator (or parts of a space elevator) include an orbital ring, a pneumatic space tower, a space fountain, a launch loop, a Skyhook, a space tether, and a buoyant "SpaceShaft".

</doc>
<doc id="29193" url="https://en.wikipedia.org/wiki?curid=29193" title="Spawn (comics)">
Spawn (comics)

Spawn is a fictional character, an anti-hero that appears in a monthly comic book of the same name published by Image Comics as well as annual compilations, mini-series specials written by guest authors and artists and numerous cross-over story-lines in other comic books. Created by writer/artist Todd McFarlane, the character first appeared in "Spawn" #1 (May 1992). Spawn was ranked 60th on "Wizard" magazine's list of the Top 200 Comic Book Characters of All Time, 50th on "Empire" magazine's list of The 50 Greatest Comic Book Characters and 36th on IGN's 2011 Top 100 Comic Book Heroes.
The series has spun off several other comics, including "Angela", "Curse of the Spawn", "Sam & Twitch", and the Japanese manga "Shadows of Spawn." Spawn was adapted into a 1997 feature film and portrayed by Michael Jai White, an HBO animated series lasting from 1997 until 1999, and a series of action figures whose high level of detail made McFarlane Toys known in the toy industry.
Publication history.
"Spawn" enjoyed considerable popularity upon its initial release in the 1990s. Comic book collecting was enjoying a marked upswing at the time, fueled by the speculator boom looking for the next hot book that would jump in value after its release. McFarlane had enjoyed superstar status among comic fans with his work on "Spider-Man", which had featured McFarlane's name prominently as both writer and artist. McFarlane's subsequent break with Marvel and the formation of Image Comics was seen by many as a sea-change event, changing the very way in which comics were produced. "Wizard", on May 2008, rated "The Launch of Image Comics" as No.1 in the list of events that rocked the Comic Industry from 1991 to 2008.
The first issue of "Spawn" was very popular with sales of 1.7 million copies. During "Spawn"'s second year of publication, "Wizard" noted that "The top dog at Image is undoubtedly Todd McFarlane's "Spawn", which, without the added marketing push of fancy covers, polybagged issues, or card inserts has become the best-selling comic on a consistent basis that is currently being published." Sales slumped around the time of "Spawn" #25, but by "Spawn" #45 it was again a consistently strong seller.
The popularity of the franchise peaked with the 1997 "Spawn" feature film, the pre-release publicity for which helped make "Spawn" the top selling comic book for May 1997; in addition, the spin-off "Curse of the Spawn" #9 came in at fifth best-selling for the same month. However, the film was only a mild commercial success and failed to start a film franchise based on the character. A 2008 issue, "Spawn" #174, ranked 99th best-selling comic of the month with retail orders of 22,667. In October 2008, issue #185, which marked both a new creative direction and Todd McFarlane's return to the book, sold out at the distribution level and received a second printing. By issue #191 in May 2009, with estimated sales of 19,803 copies, "Spawn" had dropped below Top 100 titles sold monthly to comic shops as reported by Diamond Comic Distributors. As of August 2010 Spawn no longer was ranked in the top 300 sales figures chart reported by Diamond Comic Distributors. On the day of its release in 2011, issue #200 sold out. This issue featured work by Greg Capullo, David Finch, Michael Golden, Jim Lee, Rob Liefeld, Marc Silvestri, Danny Miki, and Ashley Wood. A second printing was released the next month. Despite its remarkable sales, it received a negative review from IGN.
Fictional character biography.
Origin.
Lt. Colonel Albert Francis ″Al″ Simmons, USMC (Ret.), was a highly trained Marine who was at his most successful point when he saved the President from an attempted assassination. He was promoted to a high level within the CIA devoted to black ops. Once there, he began to question the morality of what his agency was doing.
Jason Wynn hired Bruce Stinson (codenamed Chapel), Simmons' friend and partner, to kill him. In a blazing inferno, Simmons was killed and his soul sent to Hell, because he had knowingly killed innocents while working for the CIA.
Simmons made a deal with an evil being known as Malebolgia: in exchange for his soul, he would get to see once again his wife, Wanda. However, when Simmons returned to the human world, five years had passed, and he had been transformed into a demonic creature with little memory of his former life. After regaining his memories, he sought out his wife, only to find she had moved on and married his best friend, Terry Fitzgerald, and that they now had a daughter named Cyan.
After this event, the Violator appeared, and revealed to Simmons the purpose of his resurrection. They fought, but the battle was interrupted by Malebolgia.
Early History of Spawn.
In his early battles Spawn faced street thugs and gangs, becoming a dark and brutal anti-hero, culminating in his brutal murder of a pedophile and child murderer named Billy Kincaid. As a result, he gained the attention of the detective duo of Sam Burke and Twitch Williams. It is around this time that Spawn becomes "King of Rat City", a gathering of alleys where bums and the homeless live. There he meets the bum Cogliostro, who seems to know much more about Spawn than he first lets on, and becomes his mentor.
He would also be hunted by the warrior angel, Angela, who hunted Hellspawns for sport, and would soon battle the cyborg mob enforcer Overt-Kill. This confrontation almost killed Spawn, but he was able to emerge victorious. He was again hunted by Angela and would fight the angelic warrior called the Anti-Spawn a.k.a. the Redeemer who was in fact Jason Wynn.
First Metamorphosis.
After a confrontation with the Redeemer, Spawn's suit mutated and became more advanced. Its new cape and chains were able to shape shift into different things to confuse his opponents. The cape now looked slightly ripped, and the costume had lost its red glow, having evolved to what is now black and white. Spawn's boots and gloves had also changed, having been replaced with spikes. Spawn used his new equipment to slash off the Redeemer's hand and defeated him. Spawn thought that the battle was over until he met someone more powerful than The Redeemer known as the Freak. Spawn fought with the Freak but was caught unaware by The Freak's ability to create nightmares, which he did to torture Spawn with his past. During the battle a creature appearing to have the original appearance of Spawn, fights other beings and creatures, and knocks out Spawn. When Spawn woke up the creature tells Spawn that ever since the metamorphosis the suit will feed off souls. Spawn later finds himself in New York City. He would fight with The Curse, meet Harry Houdini who taught him about magic and also meets Batman.
After this Tony Twist sends a reprogrammed Overt-Kill after Terry, blaming him for the recent attacks on the mob and Spawn was forced to reveal his identity while saving his friend. However it was a well-placed shot from Twitch Williams that brought Overkill down this time. He would later bring back his friend Bobby after he was killed in another fight with Chapel. Spawn would be a part of Angela’s trial and later traveled to the South and had an encounter with the KKK and an abusive father of two boys. When he returned to New York he was attacked by a new Redeemer. This caused his costume to evolve and defeated the Redeemer with its new found power. After another encounter with the Curse the suit began to go wild and after saving Terry from cancer sent him to Hell, but Malebolgia sent him back with full control of the suit.
Spawn wears a living symbiotic costume, Leetha of the 7th House of K (also known as K7-Leetha). While wearing it, the host assumes a dominant role over his suit. His shroud, spikes, chains, and skulls are all part of an organism bonded to his central nervous system that will protect Spawn even if he is unconscious.
War of Heaven and Hell.
Urizen, Battle for Hell.
Due to increasing attacks from Heaven and Hell, Spawn began to lose himself to evil. However thanks to the arrival of the Heap he was able to regain his goal, the Heap was an emissary of the Greenworld, a dimension whose power was equal to both Heaven and Hell’s and did not care for their war, but wanted to stop the destruction caused by it. They gave new powers to Spawn so he could better understand the world and its people.
It was around this time that Spawn battled the powerful god Urizen. Spawn once stopped a cult from summoning Urizen, but this time he had been summoned by two gate opener demons named Ab and Zab. Urizen was causing massive destruction to the world and after losing a battle to him, Spawn came back and used his new abilities to use the Earth itself to swallow Urizen and imprison him.
After his battle, Spawn learned that Malebolgia had caused Urizen’s release in an attempt to start Armageddon and conquer the forces of Heaven. Spawn and Angela then journeyed to Hell to stop him. During battle, Angela managed to mortally wound Malebolgia, but gets killed by him in retaliation. Consumed with anger, Spawn takes Malebolgia's head.
As Spawn struggled to find a way to get rid of hell's control and regain control, he noticed that the attacks were coming from both Heaven and Hell. Spawn found himself losing the battle at first due to the Salvation that had arrived in a new form, The Heap, a creature from the green world which is one of Spawns weaknesses. Heap's power was equal to Heaven and Hell which Spawn had never faced before, as it gave him new abilities which allowed him to overcome the worst from both Heaven and Hell. Though these powers were not really known they seem to give control of all the elements around the world. It was explained that the Greenworld had no interest in the war of Heaven and Hell, but was getting frustrated with the destruction that it brought. This caused earth "pain", and later Spawn became aware of it. Spawn did not change his powers though, but rather, "listened" to the earth, until being attacked by Urizen. After recovering, Spawn learns that the Greenworld had imbued him with a gift — which he uses to contain Urizen by splitting the ground and imprisoning him inside the earth.
King of Hell.
Upon killing Malebolgia, Spawn learns that Hell's throne is rightfully his, offered to him by the demon Mammon. Though he refuses the offer, Spawn eventually deliberates with his teacher Cog and decides to turn Hell into a new paradise. During this act, Cog reveals that he is in reality the biblical Cain who was the first person to go to hell, having murdered his own brother in envy. His true goal had always been to take over Hell and use a Hellspawn to do it. Having betrayed Spawn, Cog took the throne for himself, but gave his former student his human form back, a parting gift.
After returning he meets a young Wiccan named Nyx. With her help he regains his suit, though he remains weaker than before due to still having a human form. However, Mammon tricks and betrays Nyx and usurps her control over Spawn's union with his suit, removing all of Spawn's past memories in the process. With no memory, Spawn wanders the Earth, and during this time releases a group of angels who are called the Forgotten and take no sides in the war between Heaven and Hell. He discovers that Mammon is a member of the Fallen who was sent to Hell.
Spawn in Armageddon.
Spawn regains his memories thanks to the power of the Greenworld. His suit also evolves once more and now it seems one with his body but as time progresses, he begins to hate himself. Both Heaven and Hell have rejected him and now he sits in the back alleys, the city streets, sitting upon a stage prop in an abandoned warehouse as maggots and other horrible insects crawl inside his body. Nesting inside the empty shell that he has become where is he to go? His first thought is to return to the Dead Zone however that land is Heaven's territory and a Hellspawn is not welcomed. Immediately, upon entering Spawn is confronted by the Disciple who promptly greets Spawn by tearing his brain out and throwing his heart into the Greenworld whilst Spawn's body is thrown to Hell. Spawn is then captured and tortured by Mammon so he can understand the secrets inside him.
When Spawn's heart fell to the Green World, a soul was freed, as all the souls that died within the hour Al died went into the Spawn suit (thus why Malebolgia could not control Spawn). Chris meets with his mother once more as the Man of Miracles instructed him and then he travels to hell along with Sam Burke and Twitch Williams to rescue Spawn from Mammon. Spawn escapes and when he returns to earth, Signs of Armageddon begin to appear and Spawn begins looking for a way to stop it. Spawn discovers that Wanda’s twin children are responsible and he stops them from killing their entire family, but is unable to destroy them. Zera reveals that Jake is God and Katie is Satan.
Spawn finds out that the Mother removed them of their powers and positions and sent them to Earth due to their hatred of each other and constant fighting. She tells him he cannot stop Armageddon, but he has the potential to be elevated to the power of a God and preserve the human race.
He has to eat a piece of Forbidden Fruit from the Garden of Eden to gain such power. She tells Spawn he must prove himself first and has to fight against The Disciple. It is revealed that there are twelve disciples, each one representing one of Jesus’. His power is also weakened as a demon is never to enter the Garden and it has taken the form of a counter starting at 9:9:9:9. However, with guidance from Cyan he defeats all the Disciples except for the last, Judas, who Cyan tells him not to kill. He then stabs him in the heart, but the Mother gives Spawn a piece of the fruit and resurrects him. He gains a more angelic form and greater power.
He returns to Earth finding it destroyed by the Four Horsemen and all the humans dead and Angels and Demons in their place waiting to fight in the final battle. After defeating Zera he finds dead warriors of Heaven and one of them is Granny Blake having been betrayed by her faith. Spawn then battles the forces of Satan and God. He uses all the power given to him by the Mother to destroy the forces of Heaven and Hell and even all humanity. He has stopped Armageddon by taking away their armies; he is then killed by the two who then fight alone on Earth.
However, Spawn comes back and by opening himself to the power of the Mother, resurrects everyone with the knowledge of what happened. He left God and Satan to fight in their own little world and closed the doors to Heaven, Hell and Earth. He asked to be turned into a human again by the Mother, but later asked to once again become a Hellspawn.
Back in the mortal World.
The New Clown.
After a series of odd murders Spawn finds that the Clown has come back possessing the body of man named Barney Saunders. It was revealed that Saunders was having an affair with a woman named Wilma Barbara and got trapped in a garbage chute when he was trying to hide from her husband. He was there for sometime until Spawn destroyed and remade the world. He was rescued by Clown so he could use his body for himself. He then brought out the dark urges inside the tenants of an apartment building and used this to form a doorway to Hell to bring back his brothers. However, before he could form a portal Wilma showed up and his love for her allowed Saunders to take back control. He then intended to close the portal by going through it, but he took Wilma with him due to being angry over her leaving him in the chute.
Zera reappeared only her head survived and she was suspended in a jar. Spawn was summoned by a Voodoo priestess named Mambo Suzanne. Zera was attempting to take over Nyx’s body and fight Spawn, but she was killed when Suzanne used their fighting as a distraction and threw her head into the streets where it was eaten by demon dogs. Nyx was freed and her and Spawn became friends again.
Ab and Zab create a hell where visitors are forced to view their deepest fears due to demons called sin eaters they feed on guilt from the visitors then making them face the evil that they had ever done. A woman who was in the house both had delusions of their fear caused by the sin eaters. They fed on the guilt giving them more power when Spawn confronted Ab and Zab he was faced with his own sin against Wanda and his unborn child. Spawn was unable to break free from the guilt and was slowly fed on by a sin eater. When Nyx interfered he was able to break a sin eater illusion. Nyx realized that he was getting more powerful. Spawn breaks all the others illusions then he comes across one who learned is Albert Simmons brother, Richard who was going through his worst sin Spawn who was still not yet recovering from all those memories as Albert Simmons.
Spawn decides to allow Richard to feel his sins. It is realized that it was Mammon (as Mr. Malefick) that put an influence on Albert Simmons and taught Albert Simmons to torture and murder small animals, attempting to make him become a servant who would feel no pity in taking a being's life. Malefick had also influenced Richard Simmons to take drugs and introduced him to the drug dealer Weasel. It was only Marc Simmons who was able to save himself from Mammon but was unable to help the others. Richard Simmons' delusion of his past climaxed with him stabbing and injuring Weasel with a knife that was given by Mammon while intoxicated with drugs. Richard calls his brothers to help him save the drug dealer's life and Albert Simmons, not wanting his brothers getting in the crime decides instead of calling an ambulance and pulls the knife from Weasel's body and kills him with it. Mammon appears sending the brothers home while he hides the drug dealer's body.
Nyx and Spawn then kill the last sin eater and Spawn discovers that Richard cannot remember his parents due to a spell placed by Mammon and Spawn goes to find out about them. He finds their home under a spell placed by Mammon keeping them there for years. His mother is not shocked by his form, but his father is. It was revealed his mother planned with Mammon to create a hellspawn stronger than the others, but his father was grief-stricken that he couldn’t stop her. He is then given a journal by his father that his mother prevented him from seeing. It revealed that his ancestor came across a hellspawn in the past known as the Gunslinger Spawn.
The Tale of Two Brothers
Spawn at his Prime.
The comic series during the continuation from the last issue Spawn was left helpless and at a little weakened against his enemy Erskine's psychic powers,and although Spawn had psychic powers of his own he was in trouble due to Erskine's creation of a tentacle creature that had the ability of impaling his enemies. This creature also showed the features of Mammon that was coaxing. Nyx had come out of nowhere to stab the creature with a sword, but not killing the creature just stunning it to get its attention, but it was a mistake by Nyx only making herself the creatures target. Next she uses her telepathic powers to contact Marc Simmons, asking him to shoot Erskine. Erskine retaliates and ceases Marc's attack and accidentally grabbed the trigger of Marc's special gun causing the creature to disappear, while Spawn and Nyx go to the hospital where Erskine is.
Spawn is given an opportunity of whether or not whether he should cause Erskine's death. Which threatens Spawn to enter the bubble dome, a dimension if Erskine does not reveal the location of Mammon, who Spawn believed was causing the attacks. Mammon then appears only to complicate the situation and make things worse by allowing Erskine to complete his last murder, by using a devastating psychic projection of a strange woman to attack the last victim. After all that had happened Mammon confesses that it was him who taught Erskine how to use his powers. Which been shown in his recent killing. Erskine tries to commit suicide leaving behind his possessions of the doctor who he was taken care by after he was brought to the hospital. Spawn continues to attack Mammon, and ask what Mammon wanted from him. Mammon tells him that he was a special being with great divine powers.
Later a man wakes in a mortuary with no memory of who he is. Soon some doctors working on him discover he is alive and panic only to be killed by someone in a robe. The person was previously seen with Mammon and is revealed to be Morana. Severin is the man's name and he finds that he is one of the first species of vampires called the Vrykolakas. However he wants to die and Morana promises him death in return for his help. While Marc, Nyx and Spawn talk Severin appears defeats Marc and Nyx and bites Spawn to transfer his vampirism. He retreats back to Morana hoping for death, but is cursed to have to go over his last few moments repeatedly for all time.
It is revealed Cyan has been experiencing horrid visions mostly of her mother covered in blood and even Spawn's current torment. Severin's attack has caused Spawn to battle the suit in his mind. It tells him it was always in control not Al. It tells him of its anger of relinquishing his godlike powers and that Wanda had been having affairs before his death. The two fight in Spawn's mind him in his human form as the suit tells him Wanda never wanted his child soon Al gives in and gets up. Meanwhile, Mammon with Morana and his adopted mother and father Lucian and Daciana as they prepare for the next step in their plans. Cyan is having more visions that cause her to be scared of Spawn and after seeing her door in blood she opens it to find Spawn asking if he's going to kill her mom. Luckily Granny had warned Cyan and she used a knife with the old shoe lace she got from Spawn to send Spawn into an illusionary world to talk to Wanda non-violently.
Morana.
There, he speaks with Wanda while they ride on a boat in a lake. He says he wanted to move on but can't. Wanda wonders if he could go if she forgave him. She says she cannot forgive Al for the death of their child, but that she still loves the man inside him. Spawn tries to take out Cyan's knife, but only pulls out his wedding ring and somehow suppresses the suit. Soon, Cyan and Nyx show up, and they get to the shore only for K7-Leetha to appear and take over Nyx. It says it has been working with Mammon from the start to build Spawn and in return would get to live on freely on earth. Now controlled by the K7, Nyx tries to kill Cyan and Wanda, but is halted by Mammon, who promises even more power if K7 still obeys his wishes. The entire group goes to a castle, where Mammon says his plan to make a perfect Hellspawn. He wanted the Rapture to occur so that Satan, God and Malebolgia would be gone, and then revealed his perfect Hellspawn was Al and Wanda's miscarried child: Morana. He chose those two due to the fact that both their families have been filled with Spawns' from previous times.
After washing herself in virgin's blood, Morana is now fully mature. She bonds with the uniform and gets ready to consume her parents' souls. With none of the most powerful magic being usable, Cyan taps into her powers and goes to the future. She talks with an old woman who gives her a message for Al. Returning, she has Spawn summon the last twelve (but most powerful) members of the Legion. They are beaten by Morana one by one. Mammon insults Al, and in turn, he disowns Morana, enraging her, which gives Cyan the chance to tell Nyx a spell to trap Mammon and Morana. It works, both demons are sealed away, and the others return home, save Al, who jumped into another dimension to be reborn as a weapon against both heaven and hell.
Endgame.
Spawn passes Vickie through that reality and is soon ready to enter the human dimension. He makes his way to a certain point in the alleys (probably the spot he first returned from hell) and then blows off his head. Meanwhile, a man under the named Jim Downing, who is without his memories and is healing (unusually fast) at a hospital, wakes up. He seems to know Spawn, as he was thinking about him before his awakening. A janitor tries to make money off Jim's story and calls a lawyer. The lawyer then calls someone else and tells the janitor not to go to the papers, after which the lawyer commits suicide. Soon, a thug is paid to capture Jim. While he is talking to a nurse who prayed for his recovery, the thug attacks with a flaming skull. Jim flees while the thug causes a massive amount of damage to the hospital. Soon, the thug catches up to Jim, but Jim transforms into Spawn, kills his attacker, then escapes the burning hospital back in human form, eventually being taken in by firefighters.
He is later transported to another hospital with other patients. He later leaves and reunites with his previous nurse Sara. After remembering his first transformation, he begins to turn again. He calls to Sara, but leaves before she can see him. Meanwhile, a reporter begins to ask questions about what happened at the hospital. Wandering the city Jim is attacked and effortlessly kills his would be killers. This draws the attention of Sam and Twitch, who recognize the chaos as something Spawn may do.
After again reuniting with Sara and telling him about his transformation, he then wanders off again, only to run into Wanda Blake, Al Simmons' ex-wife. She recognizes Jim as the form Al took when he originally came back to earth, and she realizes he is a Spawn like Al.
On her advice, he travels to Rat City and finds Spawn's throne, where Spawn ruled, and is met by an angel. He ends up fighting the angel, who calls him a traitor during the fight. Fearing the angel has done something to Sara, he beats the angel and leaves. Jim finds out that a man was asking questions about him, later finding out the man was working from another man named Gilbert Sanchez.
The angel Spawn beat is later attacked by Clown who removes her wings. When Spawn returns he's sees the angel both powerless and insane and is surprised to find Freak and later Violator himself. Despite Freak's warnings Clown begins to deceive Jim and informs him that his suit is a living being but then vanishes. Clown later allows himself to be arrested by Sam and Twitch to meet the leader of a vampire group to attempt to form a unity between the leader, Clown and Spawn.
Gilbert Sanchez is later killed by other mob members for his information on the new Spawn. Spawn attempts to learn more about him, but after being assaulted uses his powers on his attacker. Jim later finds the man who had been asking Sara questions. He tracks to his families home which is suddenly bombed killing everyone inside but Spawn. He is later able to find more mob members and questions them further and then finishing them when he's done.
Legal disputes.
Dispute with Neil Gaiman.
In 1993, McFarlane contracted Neil Gaiman to write "Spawn" #9. While doing so, Gaiman introduced the characters Cogliostro, Angela, and Medieval Spawn. All three characters were designed and co-created by Todd McFarlane and continued to be featured in the series after Gaiman's involvement, and some had tie-ins with McFarlane's toy company. Cogliostro had a prominent role in the live-action movie in 1997. McFarlane had agreed that Gaiman was a co-creator of the characters and paid him royalties for reprints, graphic novels, and action figures. After a few years he ceased the payment of royalties and gave Gaiman notice that he owned all rights to the characters, citing the copyright notice from #9 and claimed that Gaiman's work had been work-for-hire and that McFarlane was the sole owner.
In 2002, Gaiman filed suit against McFarlane and in response McFarlane counter-sued. Gaiman had partnered with Marvel Comics to form Marvels and Miracles, LLC which bankrolled the lawsuit. The main goal was to determine the issue of ownership for another character Gaiman felt he had a stake in, Miracleman, which at the time McFarlane was believed to hold a sizable stake in after his buyout of the assets of Eclipse Comics. This issue was thrown out. Instead the court chose to rule on the breach of contract issue, the rights of ownership, and the copyrightability of the characters from "Spawn" #9. Several arguments were presented by McFarlane and all were rejected, leading to a sizable judgment against McFarlane and Image Comics. The matter went to appeal and the judgment was upheld in a 2003 decision.
Gaiman's rights as co-creator and co-owner of Cogliostro, Angela, and Medieval Spawn were acknowledged. The court's view was that Gaiman and McFarlane's collaboration led to each contributing half of the work. Gaiman wrote the story while McFarlane illustrated the character; because of this each held a 50% stake in the characters. Issue 9 was reprinted for the first time since the lawsuit was filed in the hardcover edition of "Spawn Origins: Volume 1". In a reprint collection of the first twelve issues of "Spawn", the contentious issue (along with Dave Sim's #10, featuring copyrighted character Cerebus) was excluded, but both issues have been reprinted in the hardcover and deluxe editions "Spawn Origins: Volume 1". In 2012, McFarlane and Gaiman settled their dispute, and Gaiman was given full ownership of Angela.
Tony Twist suit.
Todd McFarlane created a mob enforcer character named "Antonio 'Tony Twist' Twistelli," who McFarlane acknowledged was named after hockey player Tony Twist. Twist won a $15 million verdict in 2004 when a St. Louis, Missouri jury found Todd McFarlane Productions had profited from Twist's likeness. The verdict was upheld after two appeals in June 2006.
Collected editions.
Many issues of Spawn have been gathered together in various trade paperbacks collections since the mid-nineties. The original US and UK trade releases contain Issue 9, but not 10 (Cerebus' appearance).
US releases.
Each containing four to five issues, the original "Spawn" trade paperbacks started in 1995 under a different trade cover design. After the live-action 1997 movie, a new trade cover design was created, with Brent Ashe providing new covers for Books 1–7, and Ashley Wood for Books 8–12. These reissues were retitled with subtitles. The sequential trades stopped after Book 12, but several new volumes appeared in 2006–2008, collecting various story arcs. Beginning in 2009, a new series of volumes was released, collecting the "Endgame" storyline.
"Spawn Collection".
In 2005 the entire "Spawn" series began to appear in massive trade paperback releases under the title "Spawn Collection", each containing (with the exception of Volume 1) approximately twenty issues. Released after the Gaiman lawsuit, these editions do not contain either Issue 9 (featuring the first appearance of Angela and Cogliostro, both created by Neil Gaiman) or Issue 10 (featuring Dave Sim's Cerebus).
"Spawn Collection" Volumes 1 and 2 were published in both hardcover and trade paperback formats, while Volume 3 onward were only released as trade paperbacks. As of 2009, "Spawn Collection Volume 1" is currently out of print, with its fourth printing released in June 2007. In 2009 it was announced that the "Spawn Collection" would end with Volume 6, to be replaced by a new TBP format that includes soft- and hardcover versions, reprinting the entire "Spawn" series from the early issues once again (see below).
"Spawn Collection Volume 1" was ranked 17 in the top 100 graphic novels for December 2005 period, with pre-order sales of 3,227.
"Spawn Origins Collection".
In 2009, a line of newly redesigned and reformatted trade paperbacks was announced, replacing the "Spawn Collection" line (see above) and once again collecting the early issues of "Spawn". These new trades feature new cover art by Greg Capullo, recreating classic "Spawn" covers. In addition to the 6 issue trade paperbacks, this line features three oversized 12–13 issue hardcovers, and two large 25-issue limited slipcased deluxe editions (which come in both a standard edition and a signed and numbered edition limited to 500 copies). The 12-issue hardcover edition of Volume One was the first to reprint both Issues 9 and 10, and the 25-issue hardcover editions did as well.
UK releases.
These releases were originally published in fifteen 5–6–issue volumes in the UK by Titan Books, with titles named by religious theme. The following books contained original series issues 1–82, with the exception of the previously mentioned Issue 10.
South African releases.
Originally published by a South African publisher named Battle Axe Press in the early 1990s. Only the first 10 issues were published due to legal matters. The comic book prints were released on standard paper as opposed to the original glossy paper from Image comics.
Related collected editions.
Spin–off Trade Paperback Collections.
Several "Spawn"–related mini–series have been collected in TPB editions.
"Spin-off Hardcover Collections".
A few "Spawn"-related mini-series have been collected in HC editions.
"Curse of the Spawn".
Most of the "Curse of the Spawn" spin–off series has been collected in TPB editions.
In other media.
Video games.
Spawn has starred in several video games:

</doc>
<doc id="29196" url="https://en.wikipedia.org/wiki?curid=29196" title="SDL">
SDL

SDL may refer to:

</doc>
<doc id="29198" url="https://en.wikipedia.org/wiki?curid=29198" title="Samba (software)">
Samba (software)

Samba is a free software re-implementation of the SMB/CIFS networking protocol, and was originally developed by Andrew Tridgell. Samba provides file and print services for various Microsoft Windows clients and can integrate with a Microsoft Windows Server domain, either as a Domain Controller (DC) or as a domain member. As of version 4, it supports Active Directory and Microsoft Windows NT domains.
Samba runs on most Unix, OpenVMS and Unix-like systems, such as Linux, Solaris, AIX and the BSD variants, including Apple's OS X Server, and OS X client (version 10.2 and greater). Samba is standard on nearly all distributions of Linux and is commonly included as a basic system service on other Unix-based operating systems as well. Samba is released under the terms of the GNU General Public License. The name "Samba" comes from SMB (Server Message Block), the name of the standard protocol used by the Microsoft Windows network file system.
Early history.
Andrew Tridgell developed the first version of Samba Unix in December 1991 and January 1992, as a PhD student at the Australian National University, using a packet sniffer to do network analysis of the protocol used by DEC Pathworks server software. At the time of the first releases, versions 0.1, 0.5 and 1.0, all from the first half of January 1992, it did not have a proper name, and Tridgell just called it "a Unix file server for Dos Pathworks". At the time of version 1.0, he realized that he "had in fact implemented the netbios protocol" and that "this software could be used with other PC clients".
With a focus on interoperability with Microsoft's LAN Manager, Tridgell released "netbios for unix", nbserver, version 1.5 in December 1993. This release was the first to include client-software as well as a server. Also, at this time GPL2 was chosen as license.
Midway through the 1.5-series, the name was changed to "smbserver". However, Tridgell got a trademark notice from the company "Syntax", who sold a product named "TotalNet Advanced Server" and owned the trademark for "SMBserver". The name "Samba" was derived by running the Unix command grep through the system dictionary looking for words that contained the letters S, M, and B, in that order (i.e. ).
Versions 1.6, 1.7, 1.8, and 1.9 followed relatively quickly, with the latter being released in January 1995. Tridgell considers the adoption of CVS in May 1996 to mark the birth of the Samba Team, though there had been contributions from other people, especially Jeremy Allison, previously.
Version 2.0.0 was released in January 1999, and version 2.2.0 in April 2001.
Version history.
Version 3.0.0, released on 23 September 2003, was a major upgrade. Samba gained the ability to join Active Directory as a member, though not as a domain controller. Subsequent point-releases to 3.0 have added minor new features. Currently, the latest release in this series is 3.0.37, released 1 October 2009, and shipped on a voluntary basis. The 3.0.x series officially reached end-of-life on 5 August 2009.
Version 3.1 was used only for development.
With version 3.2, the project decided to move to time-based releases. New major releases, such as 3.3, 3.4, etc. will appear every 6 months. New features will only be added when a major release is done, point-releases will be only for bug fixes. Also, 3.2 marked a change of license from GPL2 to GPL3, with some parts released under LGPL3. The main technical change in version 3.2 was to autogenerate much of the DCE/RPC-code that used to be handcrafted. Version 3.2.0 was released on 1 July 2008. and its current release is 3.2.15 from 1 October 2009. The 3.2.x series officially reached end-of-life on 1 March 2010.
Security.
Some versions of Samba 3.6.3 and lower suffer serious security issues which can allow anonymous users to gain root access to a system from an anonymous connection, through the exploitation of an error in Samba's remote procedure call.
On April 12, 2016 Badlock, a crucial security bug in Windows and Samba was disclosed. Badlock for Samba is referenced by CVE-2016-2118 (SAMR and LSA man in the middle attacks possible).
Features.
Samba allows file and print sharing between computers running Microsoft Windows and computers running Unix. It is an implementation of dozens of services and a dozen protocols, including:
All these services and protocols are frequently incorrectly referred to as just NetBIOS or SMB. The NBT (NetBIOS over TCP/IP) and WINS protocols are deprecated on Windows.
Samba sets up network shares for chosen Unix directories (including all contained subdirectories). These appear to Microsoft Windows users as normal Windows folders accessible via the network. Unix users can either mount the shares directly as part of their file structure using the smbmount command or, alternatively, can use a utility, smbclient (libsmb) installed with Samba to read the shares with a similar interface to a standard command line FTP program. Each directory can have different access privileges overlaid on top of the normal Unix file protections. For example: home directories would have read/write access for all known users, allowing each to access their own files. However they would still not have access to the files of others unless that permission would normally exist. Note that the netlogon share, typically distributed as a read only share from /etc/samba/netlogon, is the logon directory for user logon scripts.
Samba services are implemented as two daemons:
Samba configuration is achieved by editing a single file (typically installed as /etc/smb.conf or /etc/samba/smb.conf). Samba can also provide user logon scripts and group policy implementation through poledit.
Samba is included in most Linux distributions and is started during the boot process. On Red Hat, for instance, the /etc/rc.d/init.d/smb script runs at boot time, and starts both daemons. Samba is not included in Solaris 8, but a Solaris 8-compatible version is available from the Samba website.
Samba includes a web administration tool called "Samba Web Administration Tool" (SWAT).
SWAT was removed starting with version 4.1.
Samba TNG.
Samba TNG (The Next Generation) was forked in late 1999, after disagreements between the Samba Team leaders and Luke Leighton about the directions of the Samba project. They failed to come to an agreement on a development transition path which allowed the research version of Samba he was developing (known at the time as Samba-NTDOM) to slowly be integrated into Samba.
Since the project started, development has been minimal, due to a lack of developers. As such the Samba TNG team frequently recommends to people who are unsure of which program to use to try Samba instead, as they have more developers and are able to support more platforms and situations.
One of the key goals of the Samba TNG project is to rewrite all of the NT Domains services as FreeDCE projects. Making this rewriting goal difficult is the fact that services were all developed manually through network reverse-engineering, with limited or no reference to DCE/RPC documentation. 
The key differences between the two programs are in the implementation of the NT Domains suite of protocols and MSRPC services. Samba makes all the NT Domains services available from a single place, whereas Samba TNG has separated each service into its own program. 
ReactOS has started using Samba TNG services for its SMB implementation. The developers of both projects were interested in seeing the Samba TNG design used to help get ReactOS talking to Windows networks. They have been working together to adapt the network code and build system. The multi-layered and modular approach made it easy to port each service to ReactOS.

</doc>
<doc id="29199" url="https://en.wikipedia.org/wiki?curid=29199" title="Simple DirectMedia Layer">
Simple DirectMedia Layer

Simple DirectMedia Layer (SDL) is a cross-platform software development library designed to provide a low level hardware abstraction layer to computer multimedia hardware components. Software developers can use it to write high-performance computer games and other multimedia applications that can run on many operating systems such as Android, iOS, Linux, Mac OS X, Windows and other platforms.
SDL manages video, audio, input devices, CD-ROM, threads, shared object loading, networking and timers. For 3D graphics it can handle an OpenGL or Direct3D context.
The library is internally written in C and, depending on the target platform, C++ or Objective-C, and provides the application programming interface in C, with bindings to other languages available. It is free and open-source software subject to the requirements of the zlib License since version 2.0 and with prior versions subject to the GNU Lesser General Public License. Under the zlib License, SDL 2.0 is freely available for static linking in closed-source projects, unlike SDL 1.2.
SDL is extensively used in the industry in both large and small projects. Over 700 games, 180 applications, and 120 demos have also been posted on the library website.
A common misconception is that SDL is a game engine, but this is not true. However, the library is well-suited for building an engine on top of it.
History.
Sam Lantinga created the library, first releasing it in early 1998, while working for Loki Software. He got the idea while porting a Windows application to Macintosh. He then used SDL to port "Doom" to BeOS (see Doom source ports). Several other free libraries were developed to work alongside SDL, such as SMPEG and OpenAL. He also founded Galaxy Gameworks in 2008 to help commercially support SDL, although the company plans are currently on hold due to time constraints. Soon after putting Galaxy Gameworks on hold, Lantinga announced that SDL 1.3 (which would then later become SDL 2.0) would be licensed under the zlib License. Lantinga announced SDL 2.0 on 14 July 2012, at the same time announcing that he was joining Valve Software, the first version of which was announced the same day he joined the company. Lantinga announced the stable release of SDL 2.0.0 on 13 August 2013.
SDL 2.0 is a major update to the SDL 1.2 codebase with a different, not backwards-compatible API. It replaces several parts of the 1.2 API with more general support for multiple input and output options.
Some feature additions include multiple window support, hardware-accelerated 2D graphics, and better Unicode support.
Support for Mir and Wayland was added in SDL 2.0.2 and enabled by default in SDL 2.0.4.
Better support for Android in forthcoming 2.0.4.
Software architecture.
SDL is a wrapper around the operating-system-specific functions that the game needs to access. The only purpose of SDL is to provide a common framework for accessing these functions for multiple operating systems (cross-platform). SDL provides support for 2D pixel operations, sound, file access, event handling, timing and threading. It is often used to complement OpenGL by setting up the graphical output and providing mouse and keyboard input, since OpenGL comprises only rendering.
A game using the Simple DirectMedia Layer will "not" automatically run on every operating system, further adaptations must be applied. These are reduced to the minimum, since SDL also contains a few abstraction APIs for frequent functions offered by an operating system.
The syntax of SDL is function-based: all operations done in SDL are done by passing parameters to subroutines (functions). Special structures are also used to store the specific information SDL needs to handle. There are a few different subsystems SDL categorizes its functions under.
SDL can be used instead of XInput and XAudio2.
Subsystems.
SDL is divided into several subsystems:
Besides this basic, low-level support, there also are a few separate official libraries that provide some more functions. These comprise the "standard library", and are provided on the official website and included in the official documentation:
Other, non-standard libraries also exist. For example: SDL_Collide on Sourceforge created by Amir Taaki.
Language bindings.
The SDL 2.0 library has language bindings for C, C++, Pascal, Python (via PySDL2.0), C#, Lua, OCaml, Rust, Nim, Vala and Genie.
Supported back-ends.
Because of the way SDL is designed, much of its source code is split into separate modules for each operating system, to make calls to the underlying system. When SDL is compiled, the appropriate modules are selected for the target system. Following back-ends are available:
SDL 1.2 has support for RISC OS (dropped in 2.0).
An unofficial Sixel back-end is available for SDL 1.2.
Reception and adoption.
Over the years SDL was used for many commercial and non-commercial video game projects, for instance MobyGames listed 120 games using SDL in 2013 and the SDL website itself listed around 700 games in 2012. Important commercial examples are "Angry Birds" or "Unreal Tournament", from the open source domain "OpenTTD", "The Battle for Wesnoth" or "Freeciv".
The cross-platform game releases of the popular Humble Indie Bundles for Linux, Mac and Android are often SDL based.
SDL is also often used for later ports on new platforms with existing legacy code, for instance the PC game Homeworld was ported to the Pandora handheld and Jagged Alliance 2 for Android via SDL.
Also, several non video game software uses SDL, examples are the emulators DOSBox and VisualBoyAdvance.
There were several books written for the development with SDL (see further readings).
SDL is used in university courses teaching multimedia and computer science, for instance, in a workshop about game programming using libSDL at the University of Cadiz in 2010, or a Game Design discipline at UTFPR (Ponta Grossa campus) in 2015.

</doc>
<doc id="29200" url="https://en.wikipedia.org/wiki?curid=29200" title="Seattle University">
Seattle University

Seattle University (SU), commonly referred to as Seattle U, is a Jesuit Catholic university in the northwestern United States, located in the First Hill neighborhood of Seattle, Washington.
SU is the largest independent university in the Northwest US, with over 7,500 students enrolled in undergraduate and graduate programs within eight schools, and is one of 28 member institutions of the Association of Jesuit Colleges and Universities. In its "Best Colleges 2015" edition, "U.S. News & World Report" ranked Seattle University the 5th best school in the West, in a category for institutions that offer a full range of programs up to master's degree and some doctoral programs. Seattle University School of Law has the #1 legal writing program in the nation, a rank held for six consecutive years. In 2014, Bloomberg Businessweek ranked Seattle University #1 in the nation for macroeconomics.
Among all colleges nationally, Seattle University graduates, with a degree in either the Liberal Arts or Sciences, are the 10th highest paid in the country.
History.
In 1891, Adrian Sweere, S.J., took over a small parish near downtown Seattle at Broadway and Madison. At first, the school was named after the surrounding Immaculate Conception parish and did not offer higher education. In 1898, the school was named Seattle College after both the city and Chief Seattle, and it granted its first bachelor's degrees 11 years later. Initially, the school served as both a high school and college. From 1919 to 1931, the college moved to Interlaken Blvd, but in 1931 it returned to First Hill permanently. In 1931, Seattle College created a "night school" for women in order to allow them to attend; becoming coeducational was a highly controversial decision at the time.
In 1948, Seattle College changed its name to Seattle University under Father Albert A. Lemieux, S.J. In 1993, the Seattle University School of Law was established through purchase of the Law School from the University of Puget Sound in Tacoma, and the School of Law moved to the Seattle campus in 1999.
In 2009, SU completed the largest capital campaign in the university's history, raising almost $169 million and surpassing the original campaign goal by almost $20 million. The campaign has resulted in new scholarships for students, academic programs and professorships, a fitness complex, an arts center and more. The centerpiece of the capital projects is the $56 million Lemieux Library and McGoldrick Learning Commons, completed in fall 2010.
Campus.
The Seattle University campus is and is located in the Capitol Hill neighborhood, near downtown Seattle, Wash. The SU campus has been recognized by the city of Seattle, the EPA and many organizations for its commitment to sustainability through pesticide-free grounds, a food waste compost facility, recycling program and energy conservation program.
The most well-known building on campus is the Chapel of St. Ignatius, designed by New York architect Steven Holl. The building won a national Honor Award from the American Institute of Architects in 1998. The use of natural lighting and illuminating multi-colored lights at night transforms the chapel into a beacon of multicolored light radiating outward towards the campus.
The campus includes numerous works by well-known artists (including the Centennial Fountain by Seattle artist George Tsutakawa—recipient of an honorary doctorate from Seattle U.—and a large glass sculpture in the PACCAR Atrium of Piggot Hall by Tacoma, Washington artist Dale Chihuly, as well as works by Chuck Close, Jacob Lawrence, Gwendolyn Knight, William Morris (glass artist) and David Mach) and several architecturally notable buildings.
Almost half of the overall student body represent diverse groups, in 2009–10 the makeup of the university was:
49% White
19% Asian/Pacific Islander
7% Latino/Hispanic
5% African American
1% Native American
9.7% International Students
Lemieux Library.
The Lemieux Library was founded in 1991. As of 2011 it contained 216,677 books and subscribed to 1604 periodicals. It participates in the American Theological Library Association and the Association of Jesuit Colleges and Universities.
Academics.
Seattle University offers 61 bachelor's degree programs, 31 graduate degree programs and 27 certificate programs, plus a law school and a doctoral program in education. The university consists of nine colleges: the College of Arts and Sciences, the Albers School of Business and Economics, the College of Education, the School of Law, Matteo Ricci College, the College of Nursing, the College of Science and Engineering, the School of New and Continuing Studies, and the School of Theology and Ministry. A Seattle University education is estimated to cost $150,000, although much of this is covered by financial aid.
Albers School of Business and Economics.
Albers School of Business and Economics was ranked 46th in the U.S. and among the Top 25 private universities in the BusinessWeek 2010 rankings of undergraduate Business Schools. Albers' faculty received an "A" rating from students and recruiters ranked Albers as the top school in the nation. The school ranked seventh in the West and was the only private university in the Northwest appearing in the Top 50. The 2009 "U.S. News & World Report" ranking of undergraduate business programs puts Albers in the top 30% of AACSB accredited schools and one of the top 20 private business schools in the U.S. Albers's part-time MBA program has been recognized as one of the top 50 in the nation, according to the latest issue of "U.S. News & World Report"'s "America's Best Graduate Schools 2009." The Executive Leadership Program was ranked by CRO Corporate Responsibility Officer magazine among the top 10 executive training programs in corporate responsibility. In addition, the Albers EDGE program (Education for Global Executives) was honored in 2008 as the only academic institution to receive the President's "E" Award, which recognizes persons, firms, or organizations that contribute significantly in the effort to increase United States exports.
Seattle University's Albers School of Business and Economics, started in 1945, was named after the Albers family. George and Eva Albers were generous donors to the university and in 1971 Eva bequeathed $3 million to the school. Their daughter, Genevieve Albers, attended SU and also sponsored a business forum, established an eponymous professorship, and donated funds to create scholarships. In 1967, the business school added an MBA program. "BusinessWeek" ranked Albers's Part-time MBA Program #25 in the nation and the undergraduate program in the top 50 in 2010. Both the Leadership Executive MBA Program and the part-time MBA Program are recognized among the Top 25 in their categories by ""U.S. News & World Report's" 2010 America's Best Graduate Schools." US News also ranks the Albers School among the top 10% of undergraduate business schools nationwide. The Albers School is accredited with the Association to Advance Collegiate Schools of Business AACSB.
College of Arts and Sciences.
The Seattle University College of Arts and Sciences in Seattle, Washington is the oldest and the largest undergraduate and graduate college affiliated with Seattle University, the Northwest's largest independent university. The College offers 41 undergraduate majors, 36 undergraduate minors, 6 graduate degrees, and 1 post-graduate certificate. In the College of Arts and Sciences, Seattle University's graduate program in psychology is notable as one of the few schools in the country to focus on existential phenomenology as a therapeutic method.
Seattle University also has a notable Communications Department consisting of Strategic Communications, Journalism, and Communication Studies majors. The Communication Department's website also features a menu listing internship opportunities for students enrolled under this department.
Matteo Ricci College.
The Matteo Ricci College was founded in 1973 and named after Italian Jesuit missionary, Matteo Ricci. The program allows high school students from the affiliated Seattle Preparatory School and other area high schools to graduate with a bachelor's degree in humanities or teaching after as little as three years in high school and three years in college. It also provides students the opportunity to obtain a second bachelor's degree in any other discipline with only one more year of study.
School of Law.
The Seattle University School of Law is the largest and most diverse in the Pacific Northwest. The School of Law was founded in 1972 as part of the University of Puget Sound (UPS) in Tacoma, Wash. In 1993, the University of Puget Sound and Seattle University agreed on a transfer of the law school to Seattle University; in August 1994 the transfer was completed, and the school physically moved to the Seattle University campus in 1999. The 2012 "U.S. News & World Report" Law School rankings lists the school at number eighty-two in the nation overall, adding that the school has the number one legal writing program in the nation, as well as top-twenty rankings for its part-time program and its clinical programs.
College of Nursing.
Seattle University's College of Nursing celebrated its 75th anniversary in 2010. It is housed in the completely renovated Garrand building, the site of the original Seattle College and the oldest building on campus. The "state of the art" Clinical Performance Lab is located in the James Tower of Swedish on Cherry Hill, a few blocks away from the main campus. Undergraduate and Graduate students use this lab to practice skills necessary for clinical nursing.
The BSN program attracts students who begin as Freshmen as well as transfer students from community colleges and those with degrees from other universities. The MSN program welcomes registered nurses with bachelor's degrees. The Advanced Practice Nursing Immersion program (MSN) offers an accelerated program for those with a bachelor's degree in another field.
Specialties available in the MSN program are Family Nurse Practitioner, Adult/Gerontological Nurse Practitioner, Psych-Mental Health Nurse Practitioner, Nurse-Midwifery and Advanced Community/Public Health Nursing.
College of Education.
The College of Education was founded in 1935 and offers programs that include a Doctorate in Educational Leadership, Masters in Adult Education and Training, Counseling, Curriculum and Instruction, Educational Administration, Literacy for Special Needs, Master in Teaching, Master in Teaching with Special Education Endorsement, Special Education, Student Development Administration, and Teaching English to Students of Other Languages. Educational specialist degree programs include Educational Administration – Principal Preparation, School Psychology, and Special Education and Certificate programs offered include Superintendent, Principal, and Professional Development.
The College of Education is accredited by the National Council of Accreditation of Teacher Education and the National Association of School Psychologists and approved by the National Association of School Psychologists.
College of Science & Engineering.
The College of Science and Engineering focuses on basic sciences, mathematics and their applications. Students can major in basic science disciplines, computer science or one of the engineering departments – civil and environmental engineering, mechanical engineering, or computer and electrical engineering. Students may also obtain an interdisciplinary general science degree, or prepare for graduate work in the health professions.
The College of Science and Engineering is ranked among the top 50 in the nation as one of U.S. News & World Report's Best Undergraduate Engineering Programs (2008 edition). The college is ranked second in the nation in terms of the percentage of women faculty members, according to Prism, a publication of the American Society for Engineering Education.
School of Theology & Ministry.
The School of Theology and Ministry is an ecumenical program with relationships with 10 Protestant denominations and the Catholic Archdiocese of Seattle. The school offers a number of master's degrees and certificates, including a Master of Divinity.
School of New and Continuing Studies.
Established in 2014.
Community Investment.
The number of service learning courses at SU has nearly doubled since 2004. 
The economic impact of SU in the Seattle area in 2008 was $580.4 million. This figure is drawn from the total spending by the university, its students and visitors.
Environmental Sustainability.
Among Seattle University's many environmental undertakings, there are projects ranging from composting initiatives to water conservation. There are also solar panels on buildings, and a central recycling yard with an extensive recycling program. The university has been composting since 1995, and in 2003 it built the first composting facility in the state on an urban campus.
SU received the Sustainability Innovator Award in 2007 from the Sustainable Endowments Institute for SU's pre-consumer food waste composting program and the Green Washington Award in 2008 from Washington CEO Magazine for SU's sustainable landscape practices and pre-consumer food waste composting program. The Princeton Review's 2009 Green Rating gave the school a 97 out of a possible 99.
SU's move to a pesticide-free campus began in the early 1980s when Ciscoe Morris, now a local gardening celebrity, was head of the SU Grounds Department in the 1980s. He put a halt to chemical spraying and in its place released more than 20,000 beneficial insects called lacewings to eat the aphids that had infested trees on campus. It worked and that led to a whole host of pesticide-free gardening practices. 
Athletics.
Between 1950 and 1971, Seattle University competed as a Division I independent school. In the 1950s, the basketball team was a powerhouse with brothers Johnny and Eddie O'Brien, who led Seattle University as one of the few teams in history to defeat the world famous Harlem Globetrotters. In 1958, future NBA Hall of Famer Elgin Baylor paced a men's basketball team that advanced to the Final Four and defeated top-ranked Kansas State University before losing to the University of Kentucky. Seattle University was also a leader in the area of racial diversity, with an integrated squad known as "the United Nations team."
The success of men's basketball, in addition to men's golf and baseball, continued into to the 1960s with players Eddie Miles, Clint Richardson, and Tom Workman, all of whom went on to successful careers in the NBA. The 1966 basketball squad led Texas Western University to its only defeat in a championship season celebrated in the film "Glory Road". In the course of the 1960s, Seattle University produced more NBA players than any other school.
During that time women's tennis star Janet Hopps Adkisson was the first female to be the top-ranked player for both the men and women nationally. In women's golf, Pat Lesser was twice named to the Curtis Cup in the mid-1950s and was later inducted into the State of Washington Sports Hall of Fame.
Before 1980, more than 25 SU baseball players went on to play professionally in both the major and minor leagues. Men's golf and a Tom Gorman -led tennis team were also very strong national programs. Gorman went on to lead the US Davis Cup team, where he captained a record 18 match wins and one Davis Cup title (1972) as a player and two more Davis Cup championships as a coach (1990 and 1992).
SU joined the West Coast Conference in 1971. In 1980, SU left the West Coast Conference and Division I membership and entered the NAIA, where it remained for nearly 20 years. In the late 1990s, President Fr. Sundborg started restoring the university's NCAA membership. The athletic program moved into Division II in the fall of 2002.
The school is reclassified from Division II to Division I from 2009-2013. In 2009, the university hired men's basketball coach Cameron Dollar, former assistant at University of Washington, and women's coach Joan Bonvicini, former University of Arizona coach and one of the winningest women's college basketball coaches. In 2013 Coach Bonvicini led the Redhawks to the regular Western Athletic Conference champions.
In 1938 the mascot switched from the Maroons to the Chieftains. The name was selected to honor the college's namesake, Chief Seattle. In 2000 the university changed its mascot to the Redhawks.
In June 14, 2011, Seattle U accepted an invitation to join the Western Athletic Conference and will be a full member for the 2012–2013 season.

</doc>
<doc id="29201" url="https://en.wikipedia.org/wiki?curid=29201" title="Seattle Colleges District">
Seattle Colleges District

The Seattle Colleges District (previously Seattle Community Colleges District) is a group of colleges located in Seattle, Washington. It consists of three colleges—North Seattle College, Seattle Central College (including the Wood Technology Center and Seattle Maritime Academy), South Seattle College (including the Georgetown Campus)—and the Seattle Vocational Institute. Together the colleges form the second largest institution of higher education in the state, behind the University of Washington, to which many of their graduates transfer.
The district's origins can be traced to 1902, with the opening of Broadway High School on Capitol Hill. It operated as a traditional high school until the end of World War II, when it was converted to a vocational and adult education institution for the benefit of veterans who wanted to finish high school but no longer fit in at regular schools. As a result, in 1946, Broadway High School was renamed Edison Technical School. Edison started offering college-level courses 21 years later, and it was reconstituted as Seattle Community College in September 1966.
North Seattle Community College and South Seattle Community College opened their doors in 1970, whereupon Seattle Community College was renamed Seattle Central Community College.
Seattle Central Community College was named Time magazine's Community College of the Year in 2001.
In March 2014, the Board of Trustees voted unanimously to change the name from "Seattle Community Colleges District" to "Seattle Colleges District" and to change the names of the colleges to "Seattle Central College", "North Seattle College" and "South Seattle College".
See also.
Running Start program

</doc>
<doc id="29204" url="https://en.wikipedia.org/wiki?curid=29204" title="Summer of Love">
Summer of Love

The Summer of Love was a social phenomenon that occurred during the summer of 1967, when as many as 100,000 people converged in the Haight-Ashbury neighborhood of San Francisco. Although hippies also gathered in major cities across the U.S., Canada and Europe, San Francisco remained the center of the hippie movement. Like its sister enclave of Greenwich Village, the city became even more of a melting pot of politics, music, drugs, creativity, and the total lack of sexual and social inhibition than it already was. As the hippie counterculture movement came further forward into public awareness, the activities centered therein became a defining moment of the 1960s, causing numerous 'ordinary citizens' to begin questioning everything and anything about them and their environment as a result.
This unprecedented gathering of young people is often considered to have been a social experiment, because of all the alternative lifestyles which became more common and accepted such as gender equality, communal living, and free love. Many of these types of social changes reverberated on into the early 1970s, and effects echo throughout modern society.
The hippies, sometimes called flower children, were an eclectic group. Many were suspicious of the government, rejected consumerist values, and generally opposed the Vietnam War. A few were interested in politics; others focused on art (music, painting, poetry in particular) or religious and meditative movements. All were eager to integrate new ideas and insights into daily life, both public and private.
Early 1967.
Inspired by the Beats of the 1950s, who had flourished in the North Beach area of San Francisco, those who gathered in Haight-Ashbury in 1967 rejected the conformist values of Cold War America. These hippies rejected the material values of modern life; there was an emphasis on sharing and community. The Diggers established a Free Store, and a Free Clinic for medical treatment was started.
The prelude to the Summer of Love was the Human Be-In at Golden Gate Park on January 14, 1967, which was produced and organized by artist Michael Bowen as a "gathering of tribes".
James Rado and Gerome Ragni were in attendance and absorbed the whole experience; this became the basis for the musical "Hair". Rado recalled, "There was so much excitement in the streets and the parks and the hippie areas, and we thought `If we could transmit this excitement to the stage it would be wonderful...' We hung out with them and went to their Be-Ins let our hair grow. It was very important historically, and if we hadn't written it, there'd not be any examples. You could read about it and see film clips, but you'd never experience it. We thought, 'This is happening in the streets,' and we wanted to bring it to the stage.'"
Also at this event, Timothy Leary voiced his phrase, "turn on, tune in, drop out", that persisted throughout the Summer of Love.
The event was announced by the Haight-Ashbury's psychedelic newspaper, the "San Francisco Oracle":
A new concept of celebrations beneath the human underground must emerge, become conscious, and be shared, so a revolution can be formed with a renaissance of compassion, awareness, and love, and the revelation of unity for all mankind.
The gathering of approximately 30,000 like-minded people made the Human Be-In the first event that confirmed there was a viable hippie scene.
The term "Summer of Love" originated with the formation of the Council for the Summer of Love in the spring of 1967 as a response to the convergence of young people on the Haight-Ashbury district. The Council was composed of The Family Dog, The Straight Theatre, The Diggers, The San Francisco Oracle, and approximately twenty-five other people, who sought to alleviate some of the problems anticipated from the influx of people expected in the summer. The Council also supported the Free Clinic and organized housing, food, sanitation, music and arts, along with maintaining coordination with local churches and other social groups to fill in as needed, a practice that continues today.
Popularization.
The ever-increasing numbers of youth making a pilgrimage to the Haight-Ashbury district alarmed the San Francisco authorities, whose public stance was that they would keep the hippies away. Adam Kneeman, a long-time resident of the Haight-Ashbury, recalls that the police did little to help; organization of the hordes of newcomers fell to the overwhelmed residents themselves.
College and high-school students began streaming into the Haight during the spring break of 1967 and the local government leaders, determined to stop the influx of young people once schools let out for the summer, unwittingly brought additional attention to the scene, and an ongoing series of articles in local papers alerted the national media to the hippies' growing numbers. By spring, Haight community leaders responded by forming the Council of the Summer of Love, giving the word-of-mouth event an official-sounding name.
The mainstream media's coverage of hippie life in the Haight-Ashbury drew the attention of youth from all over America. Hunter S. Thompson labeled the district "Hashbury" in "The New York Times Magazine", and the activities in the area were reported almost daily.
The movement was also fed by the counterculture's own media, particularly the "San Francisco Oracle", whose pass-around readership is thought to have topped a half-million at its peak that year.
The media's fascination with the "counterculture" continued with Fantasy Fair and Magic Mountain Music Festival in Marin County and the Monterey Pop Festival, both in June 1967. At Monterey, approximately 30,000 people gathered for the first day of the music festival, with the number swelling to 60,000 on the final day. In addition, media coverage of the Monterey Pop Festival facilitated the Summer of Love even further as large numbers of fledgling hippies headed to San Francisco to hear their favorite bands such as The Who, Grateful Dead, the Animals, Jefferson Airplane, Quicksilver Messenger Service, The Jimi Hendrix Experience, Otis Redding, The Byrds, and Big Brother and the Holding Company featuring Janis Joplin.
"San Francisco (Be Sure to Wear Flowers in Your Hair)".
John Phillips of The Mamas & the Papas wrote the song "San Francisco (Be Sure to Wear Flowers in Your Hair)" for his friend Scott McKenzie. It served well to promote both the Monterey Pop Festival that Phillips was helping to organise, and to popularise the flower children of San Francisco, who came to epitomise the hippie dream. Released on May 13, 1967, the song was an instant hit. By the week ending July 1, 1967, it reached the number four spot on the "Billboard" Hot 100 in the United States, where it remained for four consecutive weeks. Meanwhile, the song rose to number one in the United Kingdom and most of Europe. The single is purported to have sold over 7 million copies worldwide.
Event.
In New York City, an event in Tompkins Square Park in Manhattan on Memorial Day in 1967 sparked the beginning of the summer of love there. During this concert in the park, some police officers asked for the music to be turned down. In response, some in the crowd threw various objects, and thirty-eight police arrests ensued. A debate about the threat of the hippie ensued between Mayor John Lindsay and Police Commissioner Howard Leary. After this event, Allan Katzman, the editor of the "East Village Other", predicted that 50,000 hippies would enter the area for the summer.
Double that amount, as many as 100,000 young people from around the world, flocked to San Francisco's Haight-Ashbury district, as well as to nearby Berkeley and to other San Francisco Bay Area cities, to join in a popularized version of the hippie experience. Free food, free drugs, and free love were available in Golden Gate Park, a Free Clinic was established for medical treatment, and a Free Store gave away basic necessities to anyone who needed them.
The Summer of Love attracted a wide range of people of various ages: teenagers and college students drawn by their peers and the allure of joining a cultural utopia; middle-class vacationers; and even partying military personnel from bases within driving distance. The Haight-Ashbury could not accommodate this rapid influx of people, and the neighborhood scene quickly deteriorated, with overcrowding, homelessness, hunger, drug problems, and crime afflicting the neighborhood.
In London.
Hot on the tails of being crowned 'Swinging London', the Summer Of Love cemented London's reputation for being a major component of the burgeoning youth movement. And it was in London that the Summer of Love was most keenly experienced. Most of the rest of the country carried on mostly unaware of the changes happening in the capital that summer.
The Summer of Love was centred on some key events and places in London. The UFO Club in Tottenham Court Road, open from December 1966 to October 1967, was a gathering place for the psychedelic crowd, where groups like Pink Floyd and the Soft Machine played, accompanied by light shows. Pink Floyd performed their 'Games For May' concert in the Queen Elizabeth Hall in May. The 14 Hour Technicolour Dream in Alexandra Palace on April 29 was another seminal event, where amongst others, Pink Floyd, The Crazy World of Arthur Brown, Soft Machine, The Move, Tomorrow, and The Pretty Things played.
A Legalise Pot Rally was held at Speaker’s Corner on 16 July, featuring Allen Ginsberg and assorted London policemen.
It was soundtracked by songs such as 'A Whiter Shade of Pale' by Procol Harum, 'Itchycoo Park' by the Small Faces, 'All You Need Is Love' by the Beatles and 'Hole In My Shoe' by Traffic. The Beatles were a major influence, particularly by releasing 'Sgt Pepper's Lonely Hearts Band' on June 1. The mouthpieces for the music were the pirate radio stations, particularly Radio Caroline and Radio London, which introduced the DJ John Peel and his 'Perfumed Garden' show.
Events, attitudes and the zeitgeist were recorded and promoted by the newspaper, International Times, also known as IT, and the magazine, Oz.
Notable graphic artists included Hapshash and the Coloured Coat (who were Nigel Weymouth and Michael English), The Fool (a Dutch group, supported by the Beatles), and Martin Sharp.
Many of the fashion shops, known as boutiques, such as Granny Takes A Trip, Hung On You and Dandie Fashions were centred on the Kings Road. These were where the psychedelic clothing, such as kaftans, Victoriana, mini skirts and everything floral could be found.
Key people included John 'Hoppy' Hopkins, who helped establish the International Times, or IT, which became the voice of the hippie movement. He set up the London Free School, established the UFO psychedelic club and promoted the legendary 14 Hour Technicolor Dream with Barry Miles, a writer who established the Indica Gallery and Bookshop. Paul McCartney was particularly vocal in his support for the new movement.
The establishment was mystified by and frightened of the 'beautiful people', and harassed and arrested them as much as they could, spurred on by the tabloid press. Some notable arrests included Mick Jagger, Keith Richards, Robert Fraser and John 'Hoppy' Hopkins. The Times published an editorial headed ‘Who Breaks A Butterfly On A Wheel?’ denouncing Mick Jagger and Keith Richard's arrest.
Use of drugs.
Psychedelic drug use became one of several means of finding or creating a new reality. Grateful Dead guitarist Bob Weir commented:
Haight Ashbury was a ghetto of bohemians who wanted to do anything—and we did but I don't think it has happened since. Yes there was LSD. But Haight Ashbury was not about drugs. It was about exploration, finding new ways of expression, being aware of one's existence.
After losing his untenured position as an Instructor on the Psychology Faculty at Harvard, Timothy Leary became a major advocate for the recreational use of the drug, spreading his beliefs up and down the East Coast. After taking psilocybin, a drug extracted from certain mushrooms that causes effects similar to those of LSD, Leary supported the use of all psychedelics for personal development. He often invited friends as well as the odd graduate student to trip along with him and colleague Richard Alpert.
On the West Coast, author Ken Kesey, a prior volunteer for a CIA-sponsored LSD experiment, also advocated the use of the drug. Shortly after participating, he was inspired to write the bestselling novel "One Flew Over the Cuckoo's Nest". Subsequently, after buying an old school bus, painting it with psychedelic graffiti and attracting a group of similarly-minded individuals he dubbed the Merry Pranksters, Kesey and his group traveled across the country, often hosting "acid tests" where they would fill a large container with a diluted low dose form of the drug and give out diplomas to those who passed their test.
Along with LSD, cannabis was also used heavily during this period of time. With the various all-organic movements beginning to expand, this drug was even more appealing than LSD because apart from creating a euphoric high, it was all-natural as well. However, as a result, crime rose among users because several laws were subsequently enacted to control the use of both drugs. The users thereof often had sessions to oppose the laws, including The Human Be-In referenced above as well as various smoke-ins in July and August, however, their efforts at repeal were unsuccessful.
Funeral and aftermath.
After many people left in the fall to resume their college studies, those remaining in the Haight wanted to signal the conclusion of the scene not only to themselves and their friends, but also to those still in transit or still considering making the trek as well. A mock funeral entitled "The Death of the Hippie" ceremony was staged on October 6, 1967, and organizer Mary Kasper explained the intended message:
In New York, the rock musical "Hair", which told the story of the hippie counterculture and sexual revolution of the 1960s, opened Off-Broadway on October 17, 1967.
Legacy.
Second Summer of Love.
The Second Summer of Love was a renaissance of acid house music and rave parties in Britain. The culture supported MDMA use and some LSD use. The art had a generally psychedelic emotion reminiscent of the 1960s. The term generally refers to the summers of both 1988 and 1989
40th anniversary.
During the summer of 2007, San Francisco celebrated the 40th anniversary of the Summer of Love by holding numerous events around the region, culminating on September 2, 2007, when over 150,000 people attended the 40th anniversary of the Summer of Love concert, held in Golden Gate Park in Speedway Meadows. It was produced by 2b1 Multimedia and the Council of Light.
Bands/Performers included:
Country Joe McDonald, Taj Mahal, Lester Chambers (from Chambers Brothers), Canned Heat, New Riders of the Purple Sage, Jesse Colin Young (from the Youngbloods), Jerry Miller Band (from Moby Grape) featuring Tiran Porter and Dale Ockerman (from the Doobie Brothers) and Fuzzy John Oxendine (from the Sons of Champlin), Banana (from the Youngbloods), Michael McClure and Ray Manzarek (from the Doors), San Francisco’s First Family of Rock (TBA), Brian Auger, Nick Gravenites Band with David LaFlamme, Dickie Peterson of Blue Cheer, Chris and Lorin of the Rowan Brothers, The Alameda All Stars (from Gregg Allman band), Brad Jenkins, Terry Haggerty (from the Sons of Champlin), Tony Lindsay (Santana), Dan Hicks and the Hot Licks, George Michalski – Pete Sears “Dueling Keys”, Freddie Roulette, Ron Thompson, The Charlatans, Leigh Stephens (Blue Cheer), Greg Douglass (from Steve Miller), Pete Sears (Jefferson Starship), Essra Mohawk (from Mothers of Invention), Barry “The Fish” Melton, All Night Flight featuring David Denny and Steve McCarty (from Steve Miller), Jack King (from Cold Blood) and Dale Ockerman (from Doobie Brothers), Merl Saunders (supporting the event), Squid B. Vicious with Buddy Miles, Jim Post (Friend and Lover, Siegel Schwall Blues Band), David Harris, Fayette Hauser and the Cockettes, Terence Hallinan (former San Francisco DA), Ruth Weiss (Beat Poet), Richard Eastman (Marijuana Initiative), Lenore Kandel (Beat Poet), Paul “Lobster” Wells, Dr Hip (Eugene Schoenfeld), Artie Kornfeld (Producer of Woodstock), Wavy Gravy, Mouse man (Bagpipes), Leigh Davidson (Haight Ashbury Free Medical Clinic), Bruce Latimer (Bruce Latimer Show), Rabbi Joseph Langer, Bruce Barthol (Mime Troupe), Doug Green, Howard Hesseman (schedule permitting), Benjamin Hernandez (Hearts Hands and Elders), American Indigenous Peoples, Agnes Pilgrim and 13 Grandmas (schedule permitting), Lakota War Pony’s, Merle Tendoy (6th generation of Sacagawea) Shonie, Harry Riverbottom (Chippewa), Chief Sunne Reyna, Iroquois Tribe, Dakota Tribe, Seminole Tribe, Emmit Powell and the Gospel Elites, and representatives of the Dalai Lama.
50th anniversary.
During the summer of 2017, San Francisco plans to celebrate the 50th anniversary of the Summer of Love by holding numerous events around the region. These are planned to culminate in October, 2017, when over 150,000 people are expected to attend the 50th anniversary of the Summer of Love concert in Golden Gate Park, San Francisco, and webcast/broadcast worldwide. For this 50th anniversary event producers 2b1 Multimedia and the Council of Light are in pre-production, organizing with communities in New York and the UK to plan an event worthy of the occasion. To quote the "Council of Light of 1967-2017", "We call upon the world to celebrate the infinite holiness of Life." 
References.
Notes

</doc>
<doc id="29207" url="https://en.wikipedia.org/wiki?curid=29207" title="Skyhooks (band)">
Skyhooks (band)

Skyhooks were an Australian rock band formed in Melbourne in March 1973 by mainstays Greg Macainsh on bass guitar and backing vocals, and Imants "Freddie" Strauks on drums. They were soon joined by Bob "Bongo" Starkie on guitar and backing vocals, and Red Symons on guitar, vocals and keyboards; Graeme "Shirley" Strachan became lead vocalist in March 1974. Described as a glam rock band, because of flamboyant costumes and make-up, Skyhooks addressed teenage issues including buying drugs "Carlton (Lygon Street Limbo)", suburban sex "Balwyn Calling", the gay scene "Toorak Cowboy" and loss of girlfriends "Somewhere in Sydney" by namechecking Australian locales. According to music historian, Ian McFarlane " made an enormous impact on Australian social life".
Skyhooks had #1 albums on the Australian Kent Music Report with their 1974 debut, "Living in the 70's" (for 16 weeks), and its 1975 follow-up, "Ego Is Not a Dirty Word" (11 weeks). Their #1 singles were "Horror Movie" (January 1975) and "Jukebox in Siberia" (November 1990).
Symons left Skyhooks in 1977 and became a radio and television personality. Strachan had solo releases since 1976 and finally left the band in 1978 and was also a radio and television presenter. With altered line-ups, Skyhooks continued until they disbanded on 8 June 1980; they briefly reformed in 1983, 1984, 1990 and 1994. In 1992, Skyhooks were inducted into the Australian Recording Industry Association (ARIA) Hall of Fame. Lead singer, Strachan died on 29 August 2001, aged 49, in a helicopter crash while solo piloting. Their original lead singer, Steve Hill, died in October 2005, aged 52, of liver cancer.
In 2011, the Skyhooks album "Living in the 70s" was added to the National Film and Sound Archive of Australia's Sounds of Australia registry.
History.
Early years.
Greg Macainsh and Imants "Freddie" Strauks both attended Norwood High School in the Melbourne suburb of Ringwood and formed Spare Parts in 1966 with Macainsh on bass guitar and Strauks on lead vocals. Spare Parts was followed by Sound Pump in 1968, Macainsh formed Reuben Tice in Eltham, with Tony Williams on vocals. By 1970 Macainsh was back with Strauks, now on drums, first in Claptrap and by 1971 in Frame which had Graeme "Shirley" Strachan as lead vocalist. Frame also included Pat O'Brien on guitar and Cynthio Ooms on guitar. Strachan had befriended Strauks earlier—he sang with Strauks on the way to parties—and was asked to join Claptrap which was renamed as Frame. Strachan stayed in Frame for about 18 months but left for a career in carpentry and a hobby of surfing in Phillip Island.
Skyhooks formed in March 1973 in Melbourne with Steve Hill on vocals (ex-Lillee), Peter Ingliss on guitar (The Captain Matchbox Whoopee Band), Macainsh on bass guitar and backing vocals, Peter Starkie on guitar and backing vocals (Lipp & the Double Dekker Brothers) and Strauks on drums and backing vocals. The name, Skyhooks, came from a fictional organisation in the 1956 film "Earth vs. the Flying Saucers". Their first gig was on 16 April 1973 at St Jude's Church hall in Carlton. At a later gig, former Daddy Cool frontman, Ross Wilson was playing in his group Mighty Kong with Skyhooks as a support act. Wilson was impressed with the fledgling band and signed Macainsh to a publishing deal. In August, Bob "Bongo" Starkie (Mary Jane Union) on guitar replaced his older brother Peter (later in Jo Jo Zep & The Falcons) and Ingliss was replaced by Red Symons (Scumbag) on guitar, vocals and keyboards. The two new members added a touch of theatre and humour to the band's visual presence. By late 1973, Wilson had convinced Michael Gudinski to sign the band to his booking agency, Australian Entertainment Exchange, and eventually to Gudinski's label, Mushroom Records.
Skyhooks gained a cult following around Melbourne including university intelligentsia and pub rockers, but a poorly received show at the January 1974 Sunbury Pop Festival saw the group booed off stage. Two tracks from their live set, "Hey What's the Matter?" and "Love on the Radio" appeared on Mushroom's "Highlights of Sunbury '74". After seeing his performance on TV, Hill phoned Macainsh and resigned. To replace Hill, in March, Macainsh recruited occasional singer, surfer and carpenter Strachan from his Frame era. Strachan had been dubbed "Shirley" by fellow surfers due to his curly blond hair "a la" Shirley Temple.
"Living in the 70's".
For Skyhooks, the replacement of Hill by Strachan was a pivotal moment, as Strachan had remarkable vocal skills, and a magnetic stage and screen presence. Alongside Macainsh's lyrics, another facet of the group was the twin-guitar sound of Starkie and Symons. Adopting elements of glam rock in their presentation, and lyrics that presented frank depictions of the social life of young Australia in the 1970s, the band shocked conservative middle Australia with their outrageous (for the time) costumes, make-up, lyrics, and on-stage activities. A 1.2 metre (4 ft) high mushroom-shaped phallus was confiscated by Adelaide police after a performance. Six of the ten tracks on their debut album, "Living in the 70's", were banned by the Federation of Australian Commercial Broadcasters for their sex and drug references, "Toorak Cowboy", "Whatever Happened to the Revolution?", "You Just Like Me Cos I'm Good in Bed", "Hey What's the Matter", "Motorcycle Bitch" and "Smut". Much of the group's success derived from its distinctive repertoire, mostly penned by bass guitarist Macainsh, with an occasional additional song from Symons—who wrote "Smut" and performed its lead vocals. Although Skyhooks were not the first Australian rock band to write songs in a local setting—rather than ditties about love or songs about New York or other foreign lands—they were the first to become commercially successful doing so. Skyhooks songs addressed teenage issues including buying drugs ("Carlton (Lygon Street Limbo)"), suburban sex ("Balwyn Calling"), the gay scene ("Toorak Cowboy") and loss of girlfriends ("Somewhere in Sydney") by namechecking Australian locales. Radio personality, Billy Pinnell described the importance of their lyrics in tackling Australia's cultural cringe:
The first Skyhooks single, "Living in the 70's", was released in August, ahead of the album, and peaked at #7 on the Australian Kent Music Report Singles Charts. "Living in the 70's" initially charted only in Melbourne upon its release on 28 October 1974. It went on to spend 16 weeks at the top of the Australian Kent Music Report Albums Charts from February to June 1975. The album was produced by Wilson, and became the best selling Australian album, to that time, with 226,000 copies sold in Australia.
Skyhooks returned to the Sunbury Pop Festival in January 1975. They were declared the best performers by "Rolling Stone Australia" and "The Age" reviewers, and Gudinski now took over their management. The second single, "Horror Movie", reached #1 for two weeks in March. The band's success was credited by Gudinski with saving his struggling Mushroom Records and enabled it to develop into the most successful Australian label of its time.
The success of the album was also due to support by a new pop music television show "Countdown" on national public broadcaster ABC Television, rather than promotion by commercial radio. "Horror Movie" was the first song played on the first colour transmission of "Countdown" in early 1975. Despite the radio ban, the ABC's newly established 24-hour rock music station Double Jay chose the album's fifth track, the provocatively titled "You Just Like Me Cos I'm Good in Bed", as its first ever broadcast on 19 January.
"Ego Is Not a Dirty Word".
Skyhooks' 1975 national tour promoting "Living in the 70's" finished at Melbourne's Festival Hall with their ANZAC Day (25 April) performance. They were supported by comedy singer Bob Hudson, heavy rockers AC/DC and New Zealand band Split Enz. Strachan then took two weeks off and considered leaving the band, however he returned—newly married—and they continued recording the follow-up album, "Ego Is Not a Dirty Word". Initially, they were locked out of the recording studio until their manager, Gudinski, sent down the money still owed for recording the first album. "Ego Is Not a Dirty Word" spent 11 weeks at the top of the Australian album chart from 21 July 1975, and sold 210,000 copies. It was produced by Wilson again, with the single, "Ego is Not a Dirty Word" issued in March ahead of the album, peaking at #1. The next single, "All My Friends Are Getting Married" reached #5 in July, and was followed by "Million Dollar Riff" at #2 in October. Macainsh's then girlfriend, Jenny Brown, described the band in her 1975 book, "Skyhooks : Million Dollar Riff". A live version of Chuck Berry's "Let It Rock" from a December performance was released as a single in March 1976 and reached #13.
With Australian commercial success achieved, Skyhooks turned to the US market. Gudinski announced a $1.5 million deal with Mercury Records/Phonogram Records, which released a modified international version of "Ego Is Not a Dirty Word" with "Horror Movie" and "You Just Like Me Cos I'm Good in Bed" from their first Australian album replacing two tracks. A US tour followed in March–April 1976, but critics described them as imitators of Kiss due to the similarity of Symons' make-up & stage act to that of Gene Simmons, and despite limited success in Boston, Massachusetts and Jacksonville, Florida they failed to make in-roads into the general US market.
Later years to break-up.
After completing their 1976 US tour, the band remained in San Francisco and recorded their third album with Wilson producing, "Straight in a Gay Gay World"—called "Living in the 70's" for US release with "Living in the 70's" replacing "The Girl Says She's Bored"—which appeared in August and peaked at #3 on the Australian album charts. In July, upon return to Australia they launched The Brats Are Back Tour with a single, "This is My City", which reached the Top 20. "Blue Jeans" followed in August and peaked at #13 on the singles chart. By October, Strachan provided his debut solo single, "Every Little Bit Hurts" (a cover of Brenda Holloway's 1964 hit), which reached #3. In February 1977, Symons left the band and was replaced on guitar by Bob Spencer (Finch later called Contraband). With Symons' departure the band dropped the glam rock look and used a more straight forward hard rock approach.
During 1977 Skyhooks toured nationally three times, while their first single with Spencer, "Party to End All Parties", peaked into the top 30 in May. Strachan released his second solo single, a cover of Smokey Robinson's "Tracks of My Tears", which reached the top 20 in July. Meanwhile, Mushroom released a singles anthology, "The Skyhooks Tapes", which entered the top 50 in September. The band's mass popularity had declined although they still kept their live performances exciting and irreverent.
In January 1978 they toured New Zealand and performed at the Nambassa festival. In February their next single, "Women in Uniform", was issued and peaked at #8, while its album "Guilty Until Proven Insane" followed in March and reached #6. The album was produced by Americans Eddie Leonetti and Jack Douglas. The second single from the album, "Megalomania" issued in May, did not peak into the top 40. Strachan told band members he intended to leave—but it was not officially announced for six months—he continued regular shows until his final gig with Skyhooks on 29 July. Strachan released further solo singles, "Mr Summer" in October and "Nothing but the Best" in January 1979, but neither charted in the top 50. Strachan's replacement in Skyhooks, on lead vocals, was Tony Williams (ex-Reuben Tice with Macainsh).
Williams' first single for Skyhooks, "Over the Border", a political song about the state of the Queensland Police Force at the time, reached the top 40 in April, and their fifth studio album, "Hot for the Orient", appeared in May 1980, but failed to peak into the top 50. From 1975 to 1977, Skyhooks were—alongside Sherbet—the most commercially successful group in Australia, but over the next few years, Skyhooks rapidly faded from the public eye with the departure of key members, and in 1980 the band announced its break-up in controversial circumstances. Ian "Molly" Meldrum, usually a supporter of Skyhooks, savaged "Hot for the Orient" on his "Humdrum" segment of "Countdown"—viewers demanded that the band appear on a following show to defend it. Poor reception of the album both by the public and reviewers led the band to take out a page-sized ad in the local music press declaring "Why Don't You All Get Fu**ed" (title of one of their songs) and they played their last performance on 8 June, not in their hometown of Melbourne, but in the mining town of Kalgoorlie in Western Australia.
Reformations and later releases.
In December 1982, Mushroom released a medley of Skyhooks songs as "Hooked on Hooks" which peaked at #21. Demands for the "classic" line-up of the band—Macainsh, Bob Starkie, Strachan, Strauks and Symons—to reform were successful and on 23 April 1983, they started the Living in the 80's Tour. Support acts for the first concert included The Church, Mental as Anything, The Party Boys, The Sunnyboys, and Midnight Oil—who acknowledged, "Hooks were the only Australian band they would let top the bill above them". This tour was released on LP as "Live in the 80's".
A one-off reunion concert took place in October 1984, and in 1990 the band finally recorded new material, including "Jukebox in Siberia", released in September, which peaked at the top of the ARIA Singles Charts for two weeks. In November, "The Latest and Greatest", a compilation album, was released, which peaked at #4 on the ARIA Albums Charts. The tracks were taken from Skyhooks' first four studio albums along with two recent singles, "Jukebox In Siberia" and the uncharted "Tall Timber".
In 1992, Skyhooks were inducted into the Australian Recording Industry Association (ARIA) Hall of Fame, while their manager, Gudunski, and record label, Mushroom Records, received a 'Special Achievement Award'. Producer of their first three albums, Wilson, had been inducted into the Hall of Fame in 1989 as an individual and again as a member of Daddy Cool in 2006.
The final release of new Skyhooks material came in June 1999 when a twin-CD, "Skyhooks: The Collection", was issued. Disc one contained a greatest hits package, very similar to "The Latest and Greatest", with additional tracks. Disc two is referred to by fans as "The Lost Album", with previously unreleased songs from their 1990 and 1994 recording sessions.
After Skyhooks.
Strachan and Symons each went on to successful careers in Australian media including radio and television. Symons works on ABC radio and writes humorous newspaper columns. Starkie played locally with different bands including Ol' Skydaddys, and Ram Band. Strauks was drummer for Melbourne rock band The Sports, Jo Jo Zep & The Falcons, folk band The Bushwackers and the Ol' Skydaddys. Macainsh played with John Farnham on his Whispering Jack Tour and with Dave Warner's from the Suburbs, in 1988 he put together and managed a very successful AC/DC tribute band called Back in Black who went on to support Skyhooks on their comeback tour. He was a board member of Australasian Performing Right Association (APRA) (1997–2000) and Phonographic Performance Company of Australia (PPCA) (2001–2006), and is an intellectual property lawyer.
Strachan was killed in an air crash on 29 August 2001, when the helicopter he was learning to fly solo crashed into Mount Archer near Kilcoy, northwest of Brisbane. A memorial concert was held on 11 September 2001 at the Palais Theatre, tributes were paid and some remaining members—Strauks, Macainsh, Starkie, Symons and Spencer—performed with guest vocalists Daryl Braithwaite and Wilson. It is the only time Symons and his replacement, Spencer performed together on stage. Braithwaite performed "All My Friends Are Getting Married" with the band whilst Wilson sang the rare Skyhooks track "Warm Wind in the City".
The 30th anniversary of the release of the "Living In The 70's" album was commemorated in 2004, with different incarnations of the band performing. Absent were Strachan, Hill and Ingliss. Vocals were by Wilson, Williams and Bob Starkie. The original line up of Skyhooks including Hill reformed in 2005 at the Annandale Hotel in Sydney for a one-off gig, a benefit for Hill, who had been diagnosed with liver cancer. The line-up of Ingliss, Peter Starkie, Strauks and Macainsh joined him onstage—Hill died six weeks later. In November 2009, the "Skyhooks Tour Archive", displayed on the band's website, listed 925 live shows.
Macainsh, Starkie and Strauks appeared as Skyhooks at the 2009 Helpmann Awards in Sydney. They performed "Women in Uniform" with Australian rock icon Jimmy Barnes providing vocals. Red Symons was also slated to perform with the band, but was replaced by Diesel after withdrawing a few days before the show.
On 7 April 2010, 3AW reported that Skyhooks were to appear on the first episode of the new series of "Hey Hey It's Saturday" with Leo Sayer on vocals. Sayer later appeared on air and denied the claims.

</doc>
<doc id="29208" url="https://en.wikipedia.org/wiki?curid=29208" title="Square root">
Square root

In mathematics, a square root of a number "a" is a number "y" such that , in other words, a number "y" whose "square" (the result of multiplying the number by itself, or ) is "a". For example, 4 and −4 are square roots of 16 because .
Every non-negative real number "a" has a unique non-negative square root, called the "principal square root", which is denoted by , where √ is called the "radical sign" or "radix". For example, the principal square root of 9 is 3, denoted = 3, because and 3 is non-negative. The term whose root is being considered is known as the "radicand". The radicand is the number or expression underneath the radical sign, in this example 9.
Every positive number "a" has two square roots: , which is positive, and −, which is negative. Together, these two roots are denoted ± (see ± shorthand). Although the principal square root of a positive number is only one of its two square roots, the designation ""the" square root" is often used to refer to the "principal square root". For positive "a", the principal square root can also be written in exponent notation, as "a"1/2.
Square roots of negative numbers can be discussed within the framework of complex numbers. More generally, square roots can be considered in any context in which a notion of "squaring" of some mathematical objects is defined (including algebras of matrices, endomorphism rings, etc.)
History.
The Yale Babylonian Collection YBC 7289 clay tablet was created between 1800 BC and 1600 BC, showing and 30 as 1;24,51,10 and 42;25,35 base 60 numbers on a square crossed by two diagonals.
The Rhind Mathematical Papyrus is a copy from 1650 BC of an earlier Berlin Papyrus and other texts – possibly the Kahun Papyrus – that shows how the Egyptians extracted square roots by an inverse proportion method.
In Ancient India, the knowledge of theoretical and applied aspects of square and square root was at least as old as the "Sulba Sutras", dated around 800–500 BC (possibly much earlier). A method for finding very good approximations to the square roots of 2 and 3 are given in the "Baudhayana Sulba Sutra". Aryabhata in the "Aryabhatiya" (section 2.4), has given a method for finding the square root of numbers having many digits.
It was known to the ancient Greeks that square roots of positive whole numbers that are not perfect squares are always irrational numbers: numbers not expressible as a ratio of two integers (that is to say they cannot be written exactly as "m/n", where "m" and "n" are integers). This is the theorem "Euclid X, 9" almost certainly due to Theaetetus dating back to circa 380 BC.
The particular case Square root of 2 is assumed to date back earlier to the Pythagoreans and is traditionally attributed to Hippasus. It is exactly the length of the diagonal of a square with side length 1.
In the Chinese mathematical work "Writings on Reckoning", written between 202 BC and 186 BC during the early Han Dynasty, the square root is approximated by using an "excess and deficiency" method, which says to "...combine the excess and deficiency as the divisor; (taking) the deficiency numerator multiplied by the excess denominator and the excess numerator times the deficiency denominator, combine them as the dividend."
Mahāvīra, a 9th-century Indian mathematician, was the first to state that square roots of negative numbers do not exist.
A symbol for square roots, written as an elaborate R, was invented by Regiomontanus (1436–1476). An R was also used for Radix to indicate square roots in Gerolamo Cardano's Ars Magna.
According to historian of mathematics D.E. Smith, Aryabhata's method for finding the square root was first introduced in Europe by Cataneo in 1546.
Nevertheless, Jeffrey A. Oaks states that arabs used the letter 'Jim' the first letter in the word jidhr (“root”), that was placed over a number to indicate its square root.The letter 'Jim' resembles the present square root shape. Its usage goes as far as the end of the twelfth century in the works of the moroccan mathematician Ibn al-Yasamin.
The symbol '√' for the square root was first used in print in 1525 in Christoph Rudolff's "Coss" (which was also the first to use the then-new signs, '+' and '−').
Properties and uses.
The principal square root function "f"("x") = (usually just referred to as the "square root function") is a function that maps the set of non-negative real numbers onto itself. In geometrical terms, the square root function maps the area of a square to its side length.
The square root of "x" is rational if and only if "x" is a rational number that can be represented as a ratio of two perfect squares. (See square root of 2 for proofs that this is an irrational number, and quadratic irrational for a proof for all non-square natural numbers.) The square root function maps rational numbers into algebraic numbers (a superset of the rational numbers).
For all real numbers "x" 
For all non-negative real numbers "x" and "y",
and
The square root function is continuous for all non-negative "x" and differentiable for all positive "x". If "f" denotes the square-root function, its derivative is given by:
The Taylor series of about "x" = 0 converges for ≤ 1 and is given by
The square root of a non-negative number is used in the definition of Euclidean norm (and distance), as well as in generalizations such as Hilbert spaces. It defines an important concept of standard deviation used in probability theory and statistics. It has a major use in the formula for roots of a quadratic equation; quadratic fields and rings of quadratic integers, which are based on square roots, are important in algebra and have uses in geometry. Square roots frequently appear in mathematical formulas elsewhere, as well as in many physical laws.
Computation.
Most pocket calculators have a square root key. Computer spreadsheets and other software are also frequently used to calculate square roots. Pocket calculators typically implement efficient routines, such as the Newton's method (frequently with an initial guess of 1), to compute the square root of a positive real number. When computing square roots with logarithm tables or slide rules, one can exploit the identity
where and 10 are the natural and base-10 logarithms.
By trial-and-error, one can square an estimate for and raise or lower the estimate until it agrees to sufficient accuracy. For this technique it's prudent to use the identity
as it allows one to adjust the estimate "x" by some amount "c" and measure the square of the adjustment in terms of the original estimate and its square. Furthermore, ("x" + "c")2 ≈ "x"2 + 2"xc" when "c" is close to 0, because the tangent line to the graph of "x"2 + 2"xc" + "c"2 at "c"=0, as a function of "c" alone, is "y" = 2"xc" + "x"2. Thus, small adjustments to "x" can be planned out by setting 2"xc" to "a", or "c"="a"/(2"x").
The most common iterative method of square root calculation by hand is known as the "Babylonian method" or "Heron's method" after the first-century Greek philosopher Heron of Alexandria, who first described it.
The method uses the same iterative scheme as the Newton–Raphson method yields when applied to the function y = f("x")="x"2 − "a", using the fact that its slope at any point is d"y"/d"x"="f"'("x")=2"x", but predates it by many centuries.
The algorithm is to repeat a simple calculation that results in a number closer to the actual square root each time it is repeated with its result as the new input. The motivation is that if "x" is an overestimate to the square root of a non-negative real number "a" then "a"/"x" will be an underestimate and so the average of these two numbers is a better approximation than either of them. However, the inequality of arithmetic and geometric means shows this average is always an overestimate of the square root (as noted below), and so it can serve as a new overestimate with which to repeat the process, which converges as a consequence of the successive overestimates and underestimates being closer to each other after each iteration. To find "x" :
That is, if an arbitrary guess for is "x"0, and , then each xn is an approximation of which is better for large "n" than for small "n". If "a" is positive, the convergence is quadratic, which means that in approaching the limit, the number of correct digits roughly doubles in each next iteration. If , the convergence is only linear.
Using the identity
the computation of the square root of a positive number can be reduced to that of a number in the range . This simplifies finding a start value for the iterative method that is close to the square root, for which a polynomial or piecewise-linear approximation can be used.
The time complexity for computing a square root with "n" digits of precision is equivalent to that of multiplying two "n"-digit numbers.
Another useful method for calculating the square root is the Shifting nth root algorithm, applied for .
Square roots of negative and complex numbers.
The square of any positive or negative number is positive, and the square of 0 is 0. Therefore, no negative number can have a real square root. However, it is possible to work with a more inclusive set of numbers, called the complex numbers, that does contain solutions to the square root of a negative number. This is done by introducing a new number, denoted by "i" (sometimes "j", especially in the context of electricity where ""i"" traditionally represents electric current) and called the imaginary unit, which is "defined" such that . Using this notation, we can think of "i" as the square root of −1, but notice that we also have and so −"i" is also a square root of −1. By convention, the principal square root of −1 is "i", or more generally, if "x" is any non-negative number, then the principal square root of −"x" is
The right side (as well as its negative) is indeed a square root of −"x", since
For every non-zero complex number "z" there exist precisely two numbers "w" such that : the principal square root of "z" (defined below), and its negative.
Square root of an imaginary number.
The square root of i is given by
This result can be obtained algebraically by finding "a" and "b" such that
or equivalently
This gives the two simultaneous equations
with solutions
The choice of the principal root then gives
The result can also be obtained by using de Moivre's formula and setting
which produces
Principal square root of a complex number.
To find a definition for the square root that allows us to consistently choose a single value, called the principal value, we start by observing that any complex number "x" + "iy" can be viewed as a point in the plane, ("x", "y"), expressed using Cartesian coordinates. The same point may be reinterpreted using polar coordinates as the pair ("r", φ), where "r" ≥ 0 is the distance of the point from the origin, and φ is the angle that the line from the origin to the point makes with the positive real ("x") axis. In complex analysis, this value is conventionally written "r" "e""iφ". If 
then we define the principal square root of "z" as follows:
The principal square root function is thus defined using the nonpositive real axis as a branch cut. The principal square root function is holomorphic everywhere except on the set of non-positive real numbers (on strictly negative reals it isn't even continuous). The above Taylor series for remains valid for complex numbers "x" with .
The above can also be expressed in terms of trigonometric functions:
Algebraic formula.
When the number is expressed using Cartesian coordinates the following formula can be used for the principal square root:
where the two-digit pattern {3, 6} repeats over and over again in the partial denominators. Since , the above is also identical to the following generalized continued fractions:
Geometric construction of the square root.
The square root of a positive number is usually defined as the side length of a square with the area equal to the given number. But the square shape is not necessary for it: if one of two similar planar Euclidean objects has the area "a" times greater than another, then the ratio of their linear sizes is .
A square root can be constructed with a compass and straightedge. In his Elements, Euclid (fl. 300 BC) gave the construction of the geometric mean of two quantities in two different places: Proposition II.14 and Proposition VI.13. Since the geometric mean of "a" and "b" is formula_21, one can construct formula_22 simply by taking .
The construction is also given by Descartes in his "La Géométrie", see figure 2 on page 2. However, Descartes made no claim to originality and his audience would have been quite familiar with Euclid.
Euclid's second proof in Book VI depends on the theory of similar triangles. Let AHB be a line segment of length with and . Construct the circle with AB as diameter and let C be one of the two intersections of the perpendicular chord at H with the circle and denote the length CH as "h". Then, using Thales' theorem and, as in the proof of Pythagoras' theorem by similar triangles, triangle AHC is similar to triangle CHB (as indeed both are to triangle ACB, though we don't need that, but it is the essence of the proof of Pythagoras' theorem) so that AH:CH is as HC:HB, i.e. formula_23 from which we conclude by cross-multiplication that formula_24 and finally that formula_25. Note further that if you were to mark the midpoint O of the line segment AB and draw the radius OC of length formula_26 then clearly OC > CH, i.e. formula_27 (with equality if and only if ), which is the arithmetic–geometric mean inequality for two variables and, as noted above, is the basis of the Ancient Greek understanding of "Heron's method".
Another method of geometric construction uses right triangles and induction: can, of course, be constructed, and once has been constructed, the right triangle with 1 and for its legs has a hypotenuse of . The Spiral of Theodorus is constructed using successive square roots in this manner.

</doc>
<doc id="29209" url="https://en.wikipedia.org/wiki?curid=29209" title="SS Kaiser Wilhelm der Grosse">
SS Kaiser Wilhelm der Grosse

Kaiser Wilhelm der Grosse (Ger. orth. "Kaiser Wilhelm der Große") was a German transatlantic ocean liner named after Wilhelm I, German Emperor, the first head of state of the German Empire. Constructed in Stettin for the North German Lloyd (NDL), she entered service in 1897 and was the first liner to have four funnels. The first of four sister ships built between 1903 and 1907 by NDL (the others being , and ) she marked the beginning of a change in the way maritime supremacy was demonstrated in Europe at the beginning of the 20th century.
The ship began a new era in ocean travel and the novelty of having four funnels was quickly associated with size, strength, speed and above all luxury. Quickly established on the Atlantic, she gained the Blue Riband for Germany, a notable prize for the quickest trip from Europe to America which had been previously dominated by the British. In 1900, she was involved in a fire in the port of New York which resulted in several deaths. She was also the victim of a naval ram in the French port of Cherbourg in 1906. With the advent of her sister ships, she was then converted to an all-third-class ship to take advantage of the lucrative immigrant market travelling to the United States.
Converted into an auxiliary cruiser during World War I, she was given orders to capture and destroy enemy ships within the first months of the war. Relatively successful, she destroyed several enemy ships before eventually being destroyed in the Battle of Río de Oro in August 1914, the first month of the war, by the British cruiser . Her wreck was rediscovered in 1952 and then dismantled.
99 years after its sinking, the Sahrawi association called "SALAM", chaired by Mr Ahmed Bazaid Cheikh el Mami, discovered an interesting part that contains the name of the boat "Kaiser Wilhelm der Grosse".
History.
Origins, conception and construction.
At the end of the 19th century, the United Kingdom dominated maritime trade with the ocean liners of the principal maritime companies such as the Cunard and the White Star Line. Having gained more influence in Europe after William I, German Emperor, his grandfather, had created the German Empire in 1870, Emperor Wilhelm II wished to consolidate German influence on the sea and thus decrease that of the British. In 1889, the Emperor himself had attended a naval review in honour of the jubilee of his grandmother Queen Victoria. There he saw the strength and size of these British ships, notably the latest and then-largest liner owned by White Star, . He particularly admired that these ships could easily be converted to auxiliary cruisers in time of conflict. Leaving a lasting impression, the emperor was heard to say that "We must have some of these...", clearly showing they had had a lasting effect.
The "Norddeutscher Lloyd", commonly known as NDL or North German Lloyd, was one of only two German maritime companies which had any influence in the hugely profitable transatlantic shipping market. Neither of these lines had up until now shown any interest in operating large liners. NDL, however, was the first company to name any of their liners in honour of members of the Imperial family, purely to flatter the emperor. The company also had important links with the naval architects AG Vulkan of Stettin. NDL then approached Vulkan and commissioned them to construct a new "superliner", which was to be named "Kaiser Wilhelm der Grosse". The new ship would set a new style for ocean liners. She was the largest and longest liner afloat and would have been the largest ever had it not been for of 1860.
The launching of the ship took place on 4 May 1897 in the presence of the Imperial family; it was the emperor who baptised the ship whose name honoured his grandfather Emperor William I, ""the Great"". Construction and the internal decoration of the liner took place in Bremerhaven and before long she was ready to begin her regular crossings, her maiden voyage being scheduled for September the same year. The most striking feature of "Kaiser Wilhelm der Grosse" was her four funnels, the first ship ever to sport such a quartet, which for the next two decades would be a symbol of size and safety.
Career.
"Kaiser Wilhelm der Grosse" set out on her maiden voyage on 19 September 1897, travelling from Bremerhaven to Southampton and thence to New York. With a capacity of 800 third-class passengers, the NDL had ensured that they would profit greatly from the immigrants wishing to leave the continent for a better standard of living in the United States. From her maiden voyage, she was the only superliner to cross the Atlantic with such speed and such media attention. In March 1898, she successfully gained the Blue Riband with an average crossing speed of , thus establishing the new German competitiveness. The Blue Riband, an award given for the fastest crossing of the North Atlantic, east and westbound, had previously been held by the Cunard liner . This turn of events was closely watched by the maritime world of the era, who were eager to see how the British would retaliate. However, the NDL soon lost the riband in 1900 to the new German superliner, of the Hamburg America Line. This change in events was acceptable to Germans, who were able to relax in the knowledge that they were still the owners of the fastest liner; however, NDL promptly ordered that "Kaiser Wilhelm der Grosse" undergo a refit to ensure that they were the dominant German company. This refit included the installation of wireless communication, then new technology which allowed "Kaiser Wilhelm der Grosse" to transmit telegraphic messages to a port, emphasising her image of security.
The NDL took the battle even further. 1901 saw the addition to their fleet of another four-funnel liner, named in honour of Crown Prince William, heir to the German throne, and they subsequently commissioned another two superliners, and of 1903 and 1907 respectively. From 1903 to 1907 the Blue Riband was held by SS "Kaiser Wilhelm II". The company stated that the four liners were of the renowned Kaiser class and decided to market them as the "Four Flyers", a reference to their speed and associations with the Blue Riband.
The career of "Kaiser Wilhelm der Grosse", despite its prestige, was not without incident. In June 1900 at her quay in Hoboken, New Jersey, she was the victim of a fire which killed one hundred staff who were trying to remove the threat. Six years later, on 21 November 1906, she was the victim of a naval ram inflicted by , a British ship of the Royal Mail, in Cherbourg. Five passengers lost their lives in the incident and the liner was found to have an tear in her hull. To make matters worse, ever growing technological evolution of steamships soon made NDL's express steamers outdated. Cunard's and her sister outmatched their German rivals in all fields, and when the future White Star's entered service in 1911, luxury on the high seas was taken one step further. As a result, "Kaiser Wilhelm der Grosse" was rebuilt in 1913 to carry third-class passengers only. It seemed that her glory was fading regardless of her career as the first "four stacker". From 26 January 1907, she was charged with carrying passengers between the Mediterranean Sea and New York, effectively ending the public career of the first of the "four flyers".
First World War.
From 1908, German naval captains had been receiving orders to make preparations in the event of a sudden war. In fact, "Kaiser Wilhelm der Grosse" was soon fitted with cannons and thus transformed into an auxiliary cruiser. Across the world, supply ships carrying weapons and provisions were ready to convert merchant vessels into armed auxiliary cruisers. In August 1914, international relations reached crisis point. The United Kingdom and France declared war on Germany after the Germans invaded Belgium and Luxembourg. "Kaiser Wilhelm der Grosse" was requisitioned and turned into an armed cruiser, painted in grey and black. Her commander at the time, Captain Reymann, operated not only under the rules of war, but also the rules of mercy.
Reymann soon sank three ships, "Tubal Cain", "Kaipara", and "Nyanza", but only after taking their occupants on board. Further south in the Atlantic, "Kaiser Wilhelm der Grosse" encountered two passenger liners: "Galician" and "Arlanza". Reymann's first intention was to sink both vessels, but, discovering that they had many women and children on board, he let them go. In this early stage of the war, it was thought that it could be fought in a chivalrous fashion. However, soon it was to become a total war and ships would no longer be warned before being fired upon. As "Kaiser Wilhelm der Grosse" approached the west coast of Africa, her coal bunkers were almost empty and needed refilling. She stopped at Río de Oro, (Villa Cisneros, former Spanish Sahara) where German and Austrian colliers started the task of refuelling her.
The task of coaling was still going on on 26 August, when the British cruiser appeared. Reymann quickly prepared his ship and crew for battle and steamed out to engage the enemy after disembarking his prisoners of war. A fierce battle took place, but came to a dramatic end when "Kaiser Wilhelm der Grosse" ran out of ammunition. According to the Germans, rather than let the enemy capture the onetime pride of Germany, Reymann ordered the ship to be scuttled using dynamite, which was already in position should this situation ever arise. On detonation, the explosives tore a massive hole in the ship, causing her to capsize. This version of events was disputed by the British, who stated that "Kaiser Wilhelm der Grosse" had been badly damaged and sinking when Reymann ordered it to be abandoned. The British firmly believed that it was gunfire from HMS "Highflyer" which sank the German ship. Reymann managed to swim to shore, and he made his way back to Germany by working as a stoker on a neutral vessel. (Most of the crew were taken prisoner and held in the Amherst Internment Camp in Nova Scotia for the remainder of the war.)
The downfall of such great liners in the event of war was their huge fuel consumption. Most liners were subsequently converted from cruisers to hospital ships or troopships.
Characteristics.
Technical aspects.
"Kaiser Wilhelm der Grosse" measured long and had a beam of . Her overall weight was 14,349 gross tonnes. In fact, her dimensions were similar to those of the 1860 "Great Eastern", which was the largest ship of its time. As already noted, her four funnels were her most unusual feature. People associated the safety of an ocean liner with the number of "stacks" or funnels they had. Some passengers would in fact refuse to board ships if they did not have four funnels. In an age when ocean travel was not as safe as today, it was important to ensure that passengers felt at ease.
The special improvement in the arrangement of this steamer, as compared with other express steamers previously built by the NDL or other companies, consisted in the entire upper deck. Like many four-funnelled liners, "Kaiser Wilhelm der Grosse" did not actually require that many. She had only two uptake shafts from the boiler rooms, which each branched into two to connect to the four funnels—this design is the reason for the funnels being unequally spaced.
"Kaiser Wilhelm der Grosse" became the first liner to have a commercial wireless telegraphy system when the Marconi Company installed one in February 1900. Communications were demonstrated with systems installed at the Borkum Island lighthouse and Borkum Riff lightship northwest of the island, as well as with British stations. The ship was powered by with two triple expansion reciprocating engines and had two propellers, allowing her to reach speeds of over . The engines were noted for their stability. The engines were balanced on the Schlick system, which prevented movement being transferred to the body of the ship, thus reducing unpleasant vibration.
Interiors.
As a large passenger ship, "Kaiser Wilhelm der Grosse" was built to carry a maximum of 1,506 passengers: 206 first class; 226 second class; 1,074 third class. At the time of her construction, she had a crew numbering a mere 488. However, following her refit of 1913, her crew space was increased to 800. The décor of ship was in the style of Baroque revival, overseen by Johann Poppe, who carried out all of the interior decoration. This was unique as usually a ship would have several interior designers.
The interiors were graced with statues, mirrors, tapestries, gilding, and various portraits of the Imperial family. The interiors of her sister ships were also placed in the hands of Poppe. The first class salon was noted for its tapestries and its blue seating. The smoking room, a traditionally male preserve, was made to look like a typical German inn. The dining room, capable of holding all passengers in one sitting, rose several decks and was crowned with a dome. The room also had columns and had its chairs fixed to the deck, a typical feature of ocean liners of the era.
Wreck.
On 6 September 2013 the Salam association Salam Association for the Protection of the Environment and Sustainable Development in Western Sahara found the part of the wreck that contains the name "Der Grosse Wilhelm" when they filmed the wreck under water. This was confirmed by the Moroccan Ministry of Culture on 8 October 2013.

</doc>
<doc id="29210" url="https://en.wikipedia.org/wiki?curid=29210" title="Sydney Swans">
Sydney Swans

The Sydney Swans is an Australian rules football club which plays in the Australian Football League (AFL). The club has been based in Sydney since the South Melbourne Football Club was relocated to Sydney in 1982. Sydney was the first club in the competition to be based outside Victoria. The Swans play their home games at the Sydney Cricket Ground.
The club has proven to be one of the most consistent teams over the last twenty years of AFL football, only failing to make the finals in three seasons since 1995 and boasting a finals winning record of over 50% in the same time period.
The South Melbourne Football Club was founded in 1874 and was strong through the 1880s. It won three premierships in 1909, 1918 and 1933 before experiencing 72 years without a premiership, the longest premiership drought of any club. The club broke the drought in 2005, and has won one more premiership since, in 2012.
South Melbourne history.
Origins: 1874–1876.
The inauguration date of the club is officially 19 June 1874, and it adopted the name "South Melbourne Football Club" four weeks later, on 15 July. In 1880, South Melbourne amalgamated with the nearby Albert-park Football Club, which had a senior football history dating back to May 1867 (Albert-park had, in fact, been known as South Melbourne during its first year of existence). Following the amalgamation, the club retained the name South Melbourne, and adopted the club's now familiar red and white colours from Albert-park. Nicknamed the "Southerners", the team was more colourfully known as the "Bloods", in reference to the bright red sash on their white jumpers (the sash was replaced with a red "V” in 1932). The colorful epithet the "Bloodstained Angels" was also in use. The club was based at Lake Oval, also home of the South Melbourne Cricket Club.
VFA era: 1877–1896.
South Melbourne was a junior foundation club of the Victorian Football Association in 1877, and attained senior status in 1879; The South Melbourne amalgamation with neighbouring Albert-park Football Club in 1880, formed a club that became the strongest in metropolitan Melbourne. Over its first decade as an amalgamated club, South Melbourne won five VFA premierships – in 1881, 1885 (undefeated), and three-in-a-row in 1888, 1889 and 1890 – and was runner-up to the provincial Geelong Football Club in 1880, 1883 and 1886.
At the end of the 1896 season, Collingwood and South Melbourne finished equal at the top of the VFA's premiership ladder with records of 14–3–1, requiring a playoff match to determine the season's premiership; this was the first time this had occurred in VFA history. The match took place on 3 October 1896 at the East Melbourne Cricket Ground. Collingwood won the match, six goals to five, in front of an estimated crowd of 12,000.
This Grand Final would be the last match South Melbourne would play in the VFA, as the following season they would be one of eight founding clubs forming the breakaway Victorian Football League. The other clubs were St Kilda Football Club, Essendon Football Club, Fitzroy Football Club, Melbourne Football Club, Geelong Football Club, Carlton Football Club and Collingwood Football Club.
VFL entry: 1897–1909.
South Melbourne was one of the original founding clubs of the Victorian Football League that was formed in 1897.
Premiership success: 1909–1945.
The club had early success and won three VFL premierships in 1909, 1918 and 1933. The club was at its most successful in the 1930s, when key recruits from both Victoria and interstate led to a string of appearances in the finals, including four successive grand final appearances from 1933 to 1936, albeit with only one premiership in 1933. On Grand Final eve, 1935, as the Swans prepared to take on Collingwood, star full-forward Bob Pratt was clipped by a truck moments after stepping off a tram and subsequently missed the match for South. Ironically, the truck driver was a South Melbourne supporter.
It was during this period that the team became known as the Swans. The nickname, which was suggested by a Herald and Weekly Times artist in 1933, was inspired by the number of Western Australians in the team (the black swan being the state emblem of Western Australia), and was formally adopted by the club before the following season 1934. The name stuck, in part due to the club's association with nearby Albert Park and Lake, also known for its swans (although there are no longer any non-native white swans and only black, indigenous swans in the lake).
After several years with only limited success, South Melbourne next reached the grand final in 1945. The match, played against Carlton, was to become known as "the Bloodbath", courtesy of the brawl that overshadowed the match, with a total of 9 players being reported by the umpires. Carlton won the match by 28 points, and from then on, South Melbourne struggled.
Struggling times: 1946–1981.
In the following years, South Melbourne consistently struggled, as their traditional inner-city recruiting district largely emptied as a result of demographic shifts. The club missed the finals in 1946 and continued to fall such that by 1950 they were second-last on the ladder. They nearly made the finals in 1952, but in the following seventeen years South Melbourne did not finish above eighth position. By the 1960s it was clear that South Melbourne's financial resources would not be capable of allowing them to compete in the growing market for country and interstate players, and their own local zone was never strong enough to compensate for this. The introduction of country zoning failed to help, as the Riverina Football League proved to be one of the least profitable zones.
Between 1945 and 1981, South Melbourne played finals only twice: under legendary coach Norm Smith, South Melbourne finished fourth in 1970, but lost the first semi-final; and, in 1977, the club finished fifth under coach Ian Stewart, but lost the elimination final – but there were three wooden spoons in the intervening period, and between Round 7, 1972 and Round 13, 1973, the team lost 29 consecutive games. By the end of the 1970s South Melbourne had massive debts after struggling for such a long period of time.
Sydney history.
Early years in Sydney: 1982–1987.
In the late 1970s and early 1980s, the VFL was strategically interested in seeing a club based in Sydney, as part of a long-term plan to broaden the appeal of the game in Queensland and New South Wales. The league had started moving a few premiership matches to the Sydney Cricket Ground annually since 1979, and in 1981 was preparing to establish an entirely new, 13th VFL club in Sydney; but these plans halted when the South Melbourne board, recognising the difficulties it faced with viability and financial stability in Melbourne, made the decision to play all 1982 home games in Sydney. On 29 July 1981, the VFL formally accepted the proposal, and paved the way for the Swans to shift to Sydney in 1982.
The move caused great internal difficulties, as a group of supporters known as Keep South at South campaigned throughout the rest of 1981 to stop the move; and, at an extraordinary general meeting on 22 September, the group democratically took control of the club's board. However, the new board did not have the power to unilaterally stop the move to Sydney: under the VFL constitution, to rescind the decision that had been made on 29 July required a three-quarters majority in a vote of all twelve clubs, and at a meeting on 14 October it failed to obtain this majority. The new board also lacked the support of the players, the vast majority of whom were in favour of a long-term move to Sydney; in early November, after the board promised that it would try to bring the club back to Melbourne in 1983, the players went on strike, seeking to force the new board commit to Sydney in the long term, as well as seeking payments that the cash-strapped club owed them from the previous season. The board ended up undermining its own position when it accepted a $400,000 loan from the VFL in late November to stay solvent, under the condition that it commit to Sydney for two years. Finally, in early December, the Keep South at South board resigned, and a board in favour of the move to Sydney was installed.
Upon moving, the club played at the Sydney Cricket Ground. In 1982, the club was technically a Melbourne-based club which played all of its home games in Sydney; it dropped the name 'South Melbourne' in June 1982, becoming known as simply 'the Swans' for the rest of that season. It was not until 1983 that the club formally moved its operations to Sydney and became the Sydney Swans. Its physical 'home club' was the 'Southern Cross Social Club' at 120a Clovelly Road, Randwick, New South Wales which became bankrupt in 1987; new Sydney Swans Offices were then set up in the Sydney Football Stadium.
On 31 July 1985, for what was thought to be $6.3 million, Geoffrey Edelsten "bought" the Swans; in reality it was $2.9 million in cash with funding and other payments spread over five years. Edelsten resigned as chairman in less than twelve months, but had already made his mark. He immediately recruited former Geelong coach Tom Hafey. Hafey, in turn, used his knowledge of Geelong's contracts to recruit David Bolton, Bernard Toohey and Greg Williams, who would all form a key part of the Sydney side, at a league-determined total fee of $240,000 (less than the $500,000 Geelong demanded and even the $300,000 Sydney offered). The likes of Gerard Healy, Merv Neagle and Paul Morwood were also poached from other clubs, and failed approaches were made to Simon Madden, Terry Daniher, Andrew Bews and Maurice Rioli.
During the Edelsten years, the Swans were seen by the Sydney public as a flamboyant club, typified by the style of its spearhead, Warwick Capper, his long bright blond mullet and bright pink boots made him unmissable on the field and his pink Lamborghini, penchant for fashion models and eccentricity made him notorious off the field – all somewhat fashionable in the 1980s. During Capper's peak years, the Swans had made successive finals appearances for the first time since relocating. His consistently spectacular aerial exploits earned him consecutive Mark of the Year awards while his goalkicking efforts (amassing 103 goals in 1987) made him runner up in the Coleman Medal two years running. The Swans' successive finals appearances saw crowds during this time peak at an average of around 25,000 per game. Edelsten also introduced the "Swanettes", becoming the sole such cheerleading group among VFL teams following the disbandment of Carlton's Blue Birds in 1986. The Swanettes did not get much performance time, owing to the short intervals between quarters of play in the AFL and the lack of space in which they might perform while other activities take place on the field. The Swanettes were soon discontinued and no AFL club has had cheerleaders since then.
In 1987, the Swans scored 201 points against the West Coast Eagles and the following week scored 236 points against the Essendon Football Club. Both games were at the SCG. The Swans are one of the few teams to have scored two scores above 200 points in a row, the feat also being achieved by Geelong in 1992.
Dark times: 1988–1994.
The club's form was to slump in the following year.
Losses were in the millions. A group of financial backers including Mike Willessee, Basil Sellers, Peter Weinert and Craig Kimberley purchased the licence and bankrolled the club until 1993, when the AFL stepped in.
Morale at the side plummeted as players were asked to take pay cuts. Legendary coach Tom Hafey was sacked by the club in 1988 after a player-led rebellion at his tough training methods (unusual in the semi-professional days of that era).
Capper was sold to the Brisbane Bears for $400,000 in a desperate attempt to improve the club's finances. Instead, it only led to disastrous on-field performances. Instead of a 100-goal-a-season forward, Sydney's goalkicking was led by defender Bernard Toohey with 29 in 1989, then Jim West with 34 in 1990. Players left the club in droves, including Brownlow Medalist Greg Williams, Bernard Toohey and Barry Mitchell. The careers of stars such as Dennis Carroll, David Bolton, Ian Roberts, Tony Morwood and David Murphy came to an end, while promising young players like Jamie Lawson, Robert Teal and Paul Bryce had their careers cut short by injury.
Attendances consistently dropped below 10,000 when the team performed poorly between 1990 and 1994, with the side winning the wooden spoon in 1992, 1993 and 1994.
The AFL stepped in to save the Swans, offering substantial monetary and management support. The club survived, despite strong rumours in 1992 that it would merge with the Brisbane Bears to form a combined New South Wales/Queensland team, fold altogether, or even move back to South Melbourne. With draft and salary cap concessions in the early 1990s and a series of notable recruits, the team were competitive after the early part of the decade.
During this time, the side was largely held together by two inspirational skippers, both from the Wagga Wagga region of country New South Wales, Dennis Carroll and later the courageous captain Paul Kelly.
Desperate to hang on, the club was keen to enlist the biggest names and identities in the AFL, and recruited legendary coach Ron Barassi who helped save the club from extinction while serving them as coach from Round 7, 1993 to 1995. At roughly the same time, Hawthorn legend Dermott Brereton was also recruited, albeit with little on-field impact. On a much brighter side for the Swans, their captain Paul Kelly won the League's highest individual honour, the Brownlow Medal, in 1995.
Tony Lockett and Grand Final return: 1995–2001.
A big coup for the club was recruitment of St Kilda Football Club champion Tony "Plugger" Lockett in 1995. Lockett became a cult figure in Sydney, with an instant impact and along with the Super League war in the struggling rival rugby league football code in Australia, helped the Swans to become a powerhouse Sydney icon.
1995 would be Barassi's last year in charge. The Swans won 8 games- as much as they did in the previous three years combined- and finished with a percentage of over 100 (in fact, they have managed such consistently ever since). They were also one of only two teams to defeat the all-conquering Carlton side of that year. Swans great Paul Kelly also won the Brownlow Medal that year. Barassi left an improving team, a club in a much better state than he found them.
Former Hawthorn player Rodney Eade took over the reins in 1996 and after a slow start (they lost their first two games of the season), turned the club around into a competitive force. The Swans ended the minor round on top of the premiership table with 16 wins, 5 losses, and 1 draw. In the finals, the Swans won one of the most thrilling AFL preliminary finals in history after Plugger Lockett kicked a behind after the siren to win the game. The Swans lost the grand final to , which had been their first appearance in a grand final since 1945. The game was played in front of 93,102 at the MCG.
The Swans then made the finals for four of the next five full years that Rodney Eade was in charge of. In 1998 they finished 3rd on the AFL ladder; despite beating in their first final the Swans were then beaten by eventual premiers in the semi-final at the SCG.
The 1999 season was a largely uneventful year for the club, the only real highlight being Tony Lockett kicking his record-breaking 1300th goal against in Round 10. The 1999 season ended with a 69-point mauling at the hands of minor premiers .
After missing the finals in 2000, the Swans rebounded to finish 7th in 2001, but were beaten by by 55 points in their elimination final at Colonial Stadium.
Rebuilding and finals return: 2002–2004.
Former Swans favourite son Paul Roos was appointed coach midway through the 2002 season, replacing Rodney Eade who was removed after Round 12. Roos had an immediate impact, winning six of the remaining 10 games that year (including the last four of the season), and continued a record as a successful coach with the Swans for the eight full seasons that would follow.
A new home ground in ANZ Stadium (then known as Telstra Stadium) provided increased capacity over the SCG. The Swans' first game played at the Stadium in Round 9, 2002 against attracted 54,169 spectators. The Sydney Swans v Collingwood match on 23 August 2003 set an attendance record for the largest crowd to watch an AFL game outside of Victoria with an official attendance of 72,393 and was the largest home and away AFL crowd at any stadium for 2003. A preliminary final against the Brisbane Lions in 2003 attracted 71,019 people. The Swans lost all three of those significant matches.
2004 saw an average year for Sydney, however one highlight was when they ended 's undefeated start to the season in Round 11. The match was notable for Leo Barry's effort in nullifying the impact of St Kilda full-forward and eventual Coleman Medallist Fraser Gehrig, whom Barry restricted to only two possessions for the entire match.
Sydney was able to recruit another St Kilda export in the Lockett mould, Barry Hall. There were obvious parallels to the signing of Lockett (a powerful, tough forward from St Kilda with questions over his discipline and attitude), which left Hall with much to live up to. He flourished in his new surroundings and eventually became a cult figure and club leader in his own right.
As the new century dawned, Sydney implemented a policy of giving up high order draft picks in exchange for players who struggled at other clubs. It was during this era that the Swans picked up the likes of Paul Williams, Barry Hall, Craig Bolton, Darren Jolly, Ted Richards, Peter Everitt, Martin Mattner, Rhyce Shaw, Shane Mumford, Ben McGlynn and Mitch Morton, amongst others, and giving up higher order draft picks meant the Swans missed out on the likes of Daniel Motlop, Nick Dal Santo, James Kelly, Courtenay Dempsey and Sam Lonergan who went to , , and the latter two to respectively. This policy is said to have paid off in the Roos era, as they implemented a strict culture of discipline at the club.
Premiership glory: 2005.
Sydney played the AFL Grand Final on 24 September 2005 against the West Coast Eagles, defeating them by four points, final score 8.10 (58) to West Coast's 7.12 (54). In the last few minutes, the Sydney defence held strong, with Leo Barry marking the ball just before the siren to stop the Eagles' final desperate shot at goal. The premiership was the Swans' first in 72 years and their first since being based in Sydney. It was also the fifth premiership in succession to be won by a team from outside Victoria.
In 2005, the Swans came under enormous public scrutiny, even from AFL CEO Andrew Demetriou for their unorthodox, "boring" defense-oriented tactics that included tightly controlling the tempo of the game and starving the opposition of possession (in fact, seven teams that season had their lowest possession total whilst playing against the Swans). The coach Paul Roos maintained that playing contested football was the style used by all recent premiership winning teams, and felt that it was ironic that the much criticised strategy proved ultimately successful.
On Friday, 30 September 2005 a ticker tape parade down Sydney's George Street was held in honour of the Swans' achievements, which ended with a rally at Town Hall, where Sydney Lord Mayor Clover Moore presented the team with the key to the city. The flag of the Swans also flew on top of the Sydney Harbour Bridge during the week; the same flag was later given to WA premier Geoff Gallop to fly on top of the state legislature in Perth as part of the friendly wager between Gallop and NSW premier Morris Iemma.
Grand Final loss: 2006.
The 2006 AFL Grand Final was contested between the Sydney Swans and West Coast Eagles at the Melbourne Cricket Ground on 30 September 2006. The West Coast Eagles avenged their 2005 Grand Final defeat by beating the Sydney Swans by one point, only the fourth one-point Grand Final margin in the competition's history.
The rivalry between the Sydney Swans and West Coast Eagles has become one of the great modern rivalries. The six games between the two sides (from the start of the 2005 finals to the first round of 2007 inclusive) were decided by a combined margin of 13 points. Four of those six games were finals, and 2 Grand Finals.
Finals goal: 2007–2010.
Sydney finished the 2007 home and away season in 7th place, and advanced to the finals, where they faced and were defeated by by 38 points in the elimination final. It was their earliest exit from the finals since 2001 and was a culmination of a mostly disappointing season, as only victories against lesser teams saw them through to a fifth consecutive finals campaign.
The conclusion of the 2007 trade saw the loss of Adam Schneider and Sean Dempster to St Kilda, the delisting of Simon Phillips, Jonathan Simpkin and Luke Vogels, and the gain of Henry Playfair from Geelong and Martin Mattner from Adelaide.
The Swans spent the middle part of the 2008 season inside the top four, however a late form slump which yielded only three wins in the last nine rounds saw the Swans drop to sixth at the conclusion of the 2008 regular season. Having qualified for the finals for a sixth consecutive season, the Swans defeated in the elimination final before losing to the Western Bulldogs the following week.
2009 saw the club register only eight victories as they failed to reach the finals for the first time since 2002, finishing 12th with a percentage of below 100% for the first time since 1994. Barry Hall, Leo Barry, Jared Crouch, Michael O'Loughlin, Amon Buchanan and Darren Jolly all departed at the conclusion of the season, with Mark Seaby, Daniel Bradshaw and Shane Mumford, among others, joining the club during the trade period.
The 2010 season saw Sydney return to the finals by virtue of a fifth-place finish at the end of the regular season. The club defeated by five points in the elimination final before losing to the Western Bulldogs in the semi-finals for the second time in three seasons. The loss signalled the end of the Swans coaching career of Paul Roos as well as that of the playing career of Brett Kirk.
John Longmire era: 2011–present.
Former premiership-winning forward John Longmire took over as coach of the Swans as part of a succession plan initiated by Paul Roos in 2009 prior to the beginning of the 2011 season. He led the club to a seventh-place finish at the end of the regular season, therefore qualifying for the finals for the 13th time in the past 16 seasons. The Swans defeated in an elimination final at Docklands Stadium before losing to in the semi-finals the following week.
It was during the regular season that the Swans caused the upset of the season, defeating the star-studded Geelong Cats on its home ground, Skilled Stadium, where the home tenant had won its past 29 games in succession, and its past two matches at the ground by a combined margin of 336 points, in Round 23. It was the Swans' first win over the Cats since 2006 and its first win at the ground since Round 8, 1999. The Swans were also the only team to defeat the West Coast Eagles at Patersons Stadium during the regular season. The Swans' victory over Geelong was overshadowed by the news that co-captain Jarrad McVeigh's baby daughter had died in the week leading up to the match, forcing him to miss that match.
The 2012 season began for the Swans with the inaugural Sydney Derby against AFL newcomers . After an even and physical first half, Sydney went on to win by 63 points. Subsequent wins over , , and saw the Swans sit second behind on percentage after Round 5, but the Swans would proceed to lose three of their next four matches before embarking on a nine-match winning streak between Rounds 10 and 19 inclusive. The Swans eventually finished the regular season in third place after losing three of their final four matches, all against their fellow top-four rivals (Collingwood, Hawthorn and Geelong in Rounds 20, 22 and 23 respectively).
The Swans defeated by 29 points in their qualifying final at AAMI Stadium, thus earning a week off and a home preliminary final, where they then defeated by 26 points to qualify for their first Grand Final since 2006, ending an eleven-match losing streak against the Magpies in the process.
In the Grand Final, the Swans defeated Hawthorn by ten points in front of 99,683 people at the MCG, with Nick Malceski kicking a snap goal with 34 seconds left to seal the Swans' fifth premiership and first since 2005. Ryan O'Keefe was named the Norm Smith Medallist and the Swan's best player in September
The Swans' 2013 season was marred by long-term injuries to many of its key players, namely Adam Goodes, Sam Reid, Lewis Jetta, Rhyce Shaw and Lewis Roberts-Thomson, among others; despite this setback, the team were still able to reach the finals for the fifteenth time in 18 seasons, reaching the preliminary finals where they were defeated by at Patersons Stadium, its first loss at the venue since 2009.
The 2014 AFL season began with some difficulties for the Swans. Sydney lost their first game against and then to Collingwood before becoming the first non-South Australian team to win at Adelaide Oval defeating Adelaide by 63 points with Lance Franklin and Luke Parker kicking 4 goals each. After a loss to North Melbourne in Round 4, the Swans' won twelve games in a row, including victories against 2013 Grand Finalists Fremantle and Hawthorn, Geelong by 110 points at the SCG and then ladder leaders Port Adelaide. In Round 17, the Swans defeated Carlton to match a winning streak set three times in club history, the last of which came way back in 1935, and eventually closed out the season with their first minor premiership in 18 years and a club record 17 wins for the season, eclipsing the previous highest of 16, which was achieved on six past occasions in 2012, 1996, 1986, 1945, 1936 and 1935. In 2014 the Swans were minor premiers, and also qualified for the 2014 AFL Grand Final. They defeated Fremantle at home in the first qualifying final in Round one of the finals series and so earned a one-week break. In the first preliminary final the Swans had a convincing win against North Melbourne, which led them to their fourth grand final in 10 years. The 2014 AFL Grand Final was played on Saturday 27 September 2014 in near perfect weather conditions, with Sydney seen as favourites leading up to the match. This was the first time in a finals series that former Hawk player Lance Franklin would play against his former team, one of very few players to have played back to back grand finals for two different teams. The Hawks dominated the game quite early and eventually defeated the Swans 11.8.(74)to 21.11.(137). The 63-point loss was Sydney's biggest ever loss in a grand final and their biggest defeat all season, meaning Hawthorn would become back to back premiers for the second time in their history.
The Swans started the 2015 AFL season well, winning their first three, before losing their next 2 games against Fremantle, where they trailed by as many as 8 goals before half-time, and the Western Bulldogs. They won their next 6 leading into the bye, including home wins against Geelong and North Melbourne, and an upset away win against Hawthorn in the grand final replay. The Swans lost their first game after the bye, their 3rd of the season to Richmond at the SCG, 11.11 (77) to 14.11. (95). The Swans rebounded with unconvincing wins against Port Adelaide and Brisbane Lions, before suffering their heaviest defeat for 17 seasons against the Hawks by 89 points. The following week was no better with a road trip to Perth and another loss, this time to the Eagles by 52 points, the scoreline ultimately flattering the Swans. The Swans bounced back against Adelaide with a convincing win 52 point win, but lost their next game to Geelong at Simmonds stadium; a close affair that Geelong blew apart in the 3rd quarter. The Swans won their final 4 games to secure a top 4 finish, against Collingwood, , St Kilda and .
The Swans faced minor premiers Fremantle in the first qualifying final, their first finals match without Franklin, who had withdrawn from the finals due to illness. Ultimately the Swans would go down in a low scoring affair, effectively kicking themselves out of the game after losing Sam Reid to a hamstring injury midway through the 2nd quarter. The following week the Swans were knocked out of the finals in a one-sided contest against North Melbourne, struggling to score throughout the first half with the game effectively over by half-time. For the first time since 2011, the Swans failed to make a preliminary final.
The Swans' continued period of success, in which it has missed the finals only three times since 1995, has led to some criticism about a salary cap concession which the club receives; the concession is in the form of an additional Cost of Living Allowance (COLA), due to the higher cost of living in Sydney compared with any other Australian city. It was, however, announced in March 2014 that this allowance would be scrapped. The trade ban was fought by the club before the 2015 season and a reprieve was won, with the AFL allowing the club to participate in the 2015 AFL draft. There was a catch however, with the league imposing an edict that the club could only recruit players at or below current average wage of $340,000 (adjusted figures for 2015 was $349,000). During the 2015 season, with the Swans team stretched by aging players and injuries, it had become apparent that the trade restrictions that had prevented the Swans from participating in the 2014 draft, had impacted the list. With the trade period looming, Andrew Pridham lobbied the AFL to lift the trade restrictions, labeling the ban as a restraint of trade. In response to continued discussions between the club and league, as well as lobbying by the AFLPA, the league further relaxed the trade restrictions for the Swans during the 2015 AFL Finals. The AFL changed the sanctions so that the Swans could replace a player that leaves the club as either a free agent, or through trade, with another player on a contract up to $450,000 per year. This allowed the Swans to trade for Callum Sinclair in a swap deal, as well as trade a late pick for out-of-contract defender, Michael Talia from the Western Bulldogs.
The Swans started off the 2016 season with a convincing 80 point round 1 win against Collingwood, with new Swans recruit Michael Talia suffering a long term foot injury. They followed up the next round with a 60 point win against the Blues, with new recruit Callum Sinclair kicking 3 goals. The following week they defeated GWS by 25 points, with Lance Franklin kicking 4 goals. In the following match against the Crows, Isaac Heeney starred with 18 touches and 4 goals in a losing case.
Club identity.
Guernsey.
The jumper is white with a red back and a red yoke with a silhouette of the Sydney Opera House at the point of the yoke. The Opera House design was first used at the start of the 1987 season, replacing the traditional red "V" on white design. Until 1991, the back of the jumper was white with the yoke only extending to the back of the shoulders and each side of the jumper had a red vertical stripe. The current predominantly red design appeared at the start of the 1992 season. The club's major sponsor is QBE Insurance. In 2004 the club added the initials 'SMFC' in white lettering at the back of the collar to honour the club's past as South Melbourne Football Club. The move was welcomed by Melbourne-based fans. The clash guernsey is a predominantly white version of the home guernsey, including a white back, but it is rarely used, since Melbourne, Gold Coast and Brisbane are the only clubs with which there is a clash.
International Sports Clothing have manufactured the Swans' apparel since 2010, replacing long-time sponsor Puma SE.
Mascot.
The Sydney Swans' mascot for the AFL's Mascot Manor is Syd 'Swannie' Skilton.
He is named after Swans legend Bob Skilton. The actual mascot at Sydney's home games is, however, still known as "Cyggy" (as in cygnet).
Home ground.
Since the 2016 AFL season, the Swans have played all their home games at the Sydney Cricket Ground, a 48,000 capacity venue located in inner-east Sydney. The venue has been home to Swans home games since the club's relocation to Sydney in 1982. In the years 2002–2015, the Swans played between three and four home matches per season and most home finals matches at Stadium Australia (commercially known as ANZ Stadium), an 80,000 capacity stadium located in the west of the city. During the first five years at the ground average crowds were high, but issues with the surface as well as fan and player disengagement resulted in the club ending its association with the venue.
Supporter base.
As the only AFL club in Sydney prior to the 2012 entry of , the Swans have a large population base to draw on.
In 2006, following the first premiership in 72 years, the club achieved a record membership and the biggest since 1999. There is still a healthy Melbourne following for the Swans, particularly a revival in the late 1990s. The club experiences good support when the team plays in Melbourne and many also make the long trip to Sydney for home games as well. The club recently celebrated in 2007 their 25th anniversary since relocating from South Melbourne, with parties hosted both in Sydney and their former home.
Some famous fans include movie star Nicole Kidman, singers Shannon Noll, Delta Goodrem, Australian band Human Nature, Australian duo The Presets, radio personalities Peter Stubbs and Adam Spencer, television personality Ian "Dicko" Dickson, media personalities Sandra Sully, Neil Cordy and John Mangos, former cricket legend Glenn McGrath, television presenter Tom Williams, musician Holly Throsby, politician Malcolm Turnbull, Philadelphia Eagles punter Mat McBriar, sports journalist Tony Squires, television actress Cornelia Frances and former Prime Minister Tony Abbott, amongst others.
¹following finals matches
Rivalries.
Greater Western Sydney.
The introduction of the GWS Giants to the AFL in 2012 has resulted in the formation of the Sydney Derby. The Swans compete against their cross-city rivals twice every season. The best performed player from every derby match is awarded the Brett Kirk Medal.
West Coast Eagles.
The Swans developed a famous modern rivalry against the Perth-based West Coast Eagles between 2005 and 2007, when six consecutive games between the two teams, including two Qualifying Finals and two Grand Finals, were decided by less than a goal. The rivalry was highlighted by Sydney's four-point win against West Coast in the 2005 Grand Final, and West Coast's one-point win against Sydney in the 2006 Grand Final.
Reserves team.
The Swans currently field a reserves team in the North East Australian Football League. Previously a reserves team was first created for South Melbourne in 1919 and continued to compete in the Victorian reserves competition until 1999 despite the team relocating to Sydney in 1982. The team enjoyed little success in the Victorian competition; it was the only reserves team never to win a premiership, and its best performances were losing Grand Finals in 1927, 1956 and 1980. In 2000 the Swans entered a reserves team in the Sydney AFL competition but withdrew prior to the finals series because the club felt the difference in standard was too greatly in favour of the Swans. Between 2001–2002 the Swans affiliated themselves with the Port Melbourne Football Club in the VFL while also starting a new stand-alone team named the Redbacks in the Sydney AFL competition. Little success resulted and the Swans entered a stand-alone reserves team in the AFL Canberra competition in 2003 which resulted in four consecutive premierships between 2005–2008. In 2011 the Swans reserves team joined the North East Australian Football League with the rest of the AFL Canberra competition and now have regular matches against AFL reserve teams from the Brisbane Lions, Gold Coast Suns and GWS Giants. The team plays home games at the Sydney Cricket Ground and will often play as a curtain raiser to senior AFL games.
In 2011 the Swans reserves finished the home and away season with the Eastern Conference minor premiership. In the Eastern Conference Grand Final Ainslie caused a major upset when they defeated the Swans by 52 points. The team suffered the same fate in 2012 when Queanbeyan defeated them by 30 points in the Eastern Conference Grand Final.
Honour roll.
1: Relocated to Sydney<br>
²: Six rounds into the 2005 season, Stuart Maxfield ended his playing career due to chronic injury. Six players rotated as captain throughout the rest of the season: Brett Kirk (Rounds 7, 8, 19 and 20), Leo Barry (Rounds 9, 10, 21 and 22), Barry Hall (Rounds 11, 12 and the entire finals series), Ben Mathews (Rounds 13 and 14), Adam Goodes (Rounds 15 and 16) and Jude Bolton (Rounds 17 and 18).
Team records.
As of 2015, the Sydney Swans have not lost a premiership match by more than 100 points since Round 10, 1998.
Individual awards.
Brownlow Medal winners.
Despite its historical lack of success, South Melbourne/Sydney has provided more Brownlow Medal winners (14) than any other club.
Norm Smith Medal winners.
The Norm Smith Medal is awarded to the player judged best-on-ground in the AFL Grand Final:
Team of the Century.
Sydney announced its team of the century on 8 August 2003:
Corporate.
Administration.
Directors:
CEOs:
Media Coverage.
Print.
The Sydney Swans receive regular exposure from Sydney's two major daily newspapers, The Daily Telegraph, the Sydney Morning Herald and their respective counterpart publications, The Sunday Telegraph and The Sun-Herald. Articles about the Swans can occasionally be found in local community newspapers, free magazines and Sydney street press publications.
Radio.
The Sydney Swans are sponsored by radio station Triple M which broadcasts all of its games, including finals, live. Occasionally, 702 ABC Sydney may cover Swans matches if they are played on a Saturday afternoon, regardless of where they are playing. If they play in Sydney during that time schedule, appropriately 702 ABC Sydney will cover the match. Matches played at other times and days are broadcast on the ABC NewsRadio station's analogue AM/FM frequencies for listeners in Sydney, Newcastle, the NSW Central Coast and Canberra. Most Swans matches can be heard by listeners in the Riverina region of N.S.W. via the ABC Riverina – Wagga Wagga (2RVR) service, on the 675 AM frequency. Match coverage can be heard anywhere in the world via live streaming at the official AFL website or by downloading the AFL app for smartphones such as the iPhone and Samsung Galaxy.
Television.
From 2002 – 2011 Network Ten would televise all Swans games played in Melbourne and outside of N.S.W. live, but on a half-hour delay when played in Sydney for Sydney viewers and via affiliated stations in N.S.W and Canberra. In past and recent years the Seven Network would broadcast Swans games to viewers in Sydney and most of N.S.W. and Canberra via the Prime TV network (now branded as Prime7). Matches were telecast either live, on a 30–90-minute delayed broadcast or late-night replay. Commencing 2002 all their games were broadcast live or on same day delay by Subscription television provider Foxtel across Australia on either the Fox Footy Channel or Fox Sports channels.
From 2012 – 2016, the AFL commenced a new broadcast deal requiring the Seven Network and their affiliate station Prime7 to broadcast all Sydney Swans (and Greater Western Sydney Giants) games live to viewers in Sydney and most of regional New South Wales and Canberra. These games are screened on the 7mate channel in these regions. Foxtel also signed a new broadcast deal for the 2012 – 2016 seasons which included screening all AFL matches (including all Swans games) live across Australia on their Fox Sports and Fox Footy channels.

</doc>
<doc id="29212" url="https://en.wikipedia.org/wiki?curid=29212" title="Supersessionism">
Supersessionism

Supersessionism, also called replacement theology or fulfillment theology, is a Christian theological view on the current status of the church in relation to the Jewish people and Judaism. It holds that the Christian Church has succeeded the Israelites as the definitive people of God or that the New Covenant has replaced or superseded the Mosaic covenant. From a supersessionist's "point of view, just by continuing to exist the Church, the Jews dissent". This view directly contrasts with dual-covenant theology which holds that the Mosaic covenant remains valid for Jews.
Supersessionism formed a core tenet of the Church for the majority of its existence, and remains a common assumption among Christians. Subsequent to and because of the Holocaust, some mainstream Christian theologians and denominations have rejected supersessionism.
Etymology.
The word "supersessionism" comes from the English verb "to supersede", from the Latin verb "sedeo, sedere, sedi, sessum", "to sit", plus "super", "upon". It thus signifies one thing being replaced or supplanted by another.
The word "supercession" is used by Sydney Thelwall in the title of chapter three of his 1870 translation of Tertullian's "Adversus Iudaeos". (Tertullian wrote between 198 and 208 AD.) The title is provided by Thelwall; it is not in the original Latin.
Types.
Both Christian and Jewish theologians have identified different types of supersessionism in Christian reading of the Bible.
R. Kendall Soulen notes three categories of supersessionism identified by Christian theologians: punitive, economic, and structural:
These three views are neither mutually exclusive, nor logically dependent, and it is possible to hold all of them or any one with or without the others. The work of Matthew Tapie attempts a further clarification of the language of supersessionism in modern theology that Peter Ochs has called "the clearest teaching on supersessionism in modern scholarship." Tapie argued that Soulen's view of economic supersessionism shares important similarities with those of Jules Isaac's thought (the French-Jewish historian well-known for his identification of "the teaching of contempt" in the Christian tradition) and can ultimately be traced to the medieval concept of the "cessation of the law" - the idea that Jewish observance of the ceremonial law (Sabbath, circumcision, and dietary laws) ceases to have a positive significance for Jews after the passion of Christ. According to Soulen, Christians today often repudiate supersessionism but they do not always carefully examine just what that is supposed to mean. Soulen thinks Tapie’s work is a remedy to this situation.
Christian views.
Many Christian theologians saw the New Covenant in Christ as a replacement for the Mosaic Covenant. Historically, statements on behalf of the Roman Catholic Church have claimed it's ecclesiastical structures to be a fulfillment and replacement of Jewish ecclesiastical structures (see also Jerusalem as an allegory for the Church). As recently as 1965 Vatican Council II affirmed, "the Church is the new people of God," without intending to make "Israel according to the flesh", the Jewish people, irrelevant in terms of eschatology (see "Roman Catholicism," below). Modern Protestants hold to a range of positions on the topic.
In the wake of the Holocaust, mainstream Christian communities began the work of "undoing" supersessionism.
New Testament.
In the New Testament, Jesus and others repeatedly give Jews priority in their mission, as in Jesus's expression of him coming to the Jews rather than to Gentiles and in Paul's formula "first for the Jew, then for the Gentile." Yet after the death of Jesus, the inclusion of the Gentiles as equals in this burgeoning sect of Judaism also caused problems, particularly when it came to Gentiles keeping the Mosaic Law, which was both a major issue at the Council of Jerusalem and a theme of Paul's Epistle to the Galatians, though the relationship of Paul of Tarsus and Judaism is still disputed today.
Paul's views on "the Jews" are complex, but he is generally regarded as the first person to make the claim that by not accepting claims of Jesus's divinity, known as Christology, Jews disqualified themselves from salvation. Paul himself was born a Jew, but after a conversion experience he came to accept Jesus's divinity later in his life. In the opinion of Roman Catholic reformer James Carroll, accepting Jesus's divinity, for Paul, was dichotomous with being a Jew. His personal conversion and his understanding of the dichotomy between being Jewish and accepting Jesus's divinity, was the religious philosophy he wanted to see adopted among other Jews of his time. Christians quickly adopted Paul's views.
For most of Christian history, supersessionism has been the mainstream interpretation of the New Testament of all three major historical traditions within Christianity — Orthodox, Catholic and Protestant. The text most often quoted in favor of the supersessionist view is Hebrews 8:13: "In speaking of 'a new covenant' 31.31-32 he has made the first one obsolete."
Church fathers.
Many Early Christian commentators taught that the Old Covenant was fulfilled and replaced (superseded) by the New Covenant in Christ, for instance:
Augustine (354–430) follows these views of the earlier Church Fathers, but he emphasizes the importance to Christianity of the continued existence of the Jewish people: "The Jews ... are thus by their own Scriptures a testimony to us that we have not forged the prophecies about Christ." The Catholic church built its system of eschatology on his theology, where Christ rules the earth spiritually through his triumphant church. Like his anti-Jewish teacher, St. Ambrose of Milan, he defined Jews as a special subset of those damned to hell, calling them "Witness People": "Not by bodily death, shall the ungodly race of carnal Jews perish (..) 'Scatter them abroad, take away their strength. And bring them down O Lord". Augustine mentioned to "love" the Jews but as a means to convert them to Christianity. Jeremy Cohen, followed by John Y. B. Hood and James Carroll, sees this as having had decisive social consequences, with Carroll saying, "It is not too much to say that, at this juncture, Christianity 'permitted' Judaism to endure because of Augustine."
Roman Catholicism.
Supersessionism is not the name of any official Roman Catholic doctrine and the word appears in no Church documents, but official Catholic teaching has reflected varying levels supersessionist thought throughout its history, especially prior to the mid-twentieth century. Supersessionist theology is extensive in Catholic liturgy and literature. The "Codex Justinianus" (1:5:12) for example defines "everyone who is not devoted to the Catholic Church and to our Orthodox holy Faith" a heretic and Catholic liturgy contains echoes of supersessionist theology. The Second Vatican Council (1962–65) marked a shift in official Catholic teaching about Judaism, a shift which may be described as a move from “hard” to “soft” supersessionism, to use the terminology of David Novak (below).
Prior to Vatican II, Catholic doctrine on the matter was characterized by “displacement” or “substitution” theologies, according to which the Church and its New Covenant took the place of Judaism and its “Old Covenant,” the latter being rendered void by the coming of Jesus. The nullification of the Old Covenant was often explained in terms of the “deicide charge” that Jews forfeited their covenantal relationship with God by executing the divine Christ. As recently as 1943, Pope Pius XII stated in his encyclical “Mystici corporis Christi”:
By the death of our Redeemer, the New Testament took the place of the Old Law which had been abolished; then the Law of Christ together with its mysteries, enactments, institutions, and sacred rites was ratified for the whole world in the blood of Jesus Christ… the gibbet of His death Jesus made void the Law with its decrees fastened the handwriting of the Old Testament to the Cross, establishing the New Testament in His blood shed for the whole human race.
At the Second Vatican Council, which convened within two decades of the Holocaust, there emerged a different framework for thinking about the status of the Jews’ covenant. The declaration "Nostra aetate", promulgated in 1965, made several statements which signaled a shift away from “hard supersessionist” replacement thinking which posited that the Jews’ covenant was no longer acknowledged by God. Retrieving Paul’s language in chapter 11 of his Epistle to the Romans, the declaration states, “God holds the Jews most dear for the sake of their Fathers; He does not repent of the gifts He makes or of the calls He issues… Although the Church is the new people of God, the Jews should not be presented as rejected or accursed by God, as if this followed from the Holy Scriptures.” Notably, a draft of the declaration contained a passage which originally called for the “the entry of that people into the fullness of the people of God established by Christ;” however, at the suggestion of Catholic priest (and convert from Judaism) John M. Oesterreicher, it was replaced in the final promulgated version with the following language: “the Church awaits that day, known to God alone, on which all peoples will address the Lord in a single voice and ‘serve him shoulder to shoulder’ (Zeph 3:9).”.
Further developments in Catholic thinking on the covenantal status of Jews were led by Pope John Paul II. Among his most noteworthy statements on the matter is that which occurred during his historic visit to the synagogue in Mainz (1980), where he called Jews the “people of God of the Old Covenant, which has never been abrogated by God (cf. Romans 11:29, "for the gifts and the calling of God are irrevocable" name="vatican.va">Pontifical Biblical Commission, [http://www.vatican.va/roman_curia/congregations/cfaith/pcb_documents/rc_con_cfaith_doc_20020212_popolo-ebraico_en.html “The Jewish People and their Sacred Scriptures” (2002).</ref> In 1997, John Paul II again affirmed the Jews’ covenantal status: “This people continues in spite of everything to be the people of the covenant and, despite human infidelity, the Lord is faithful to his covenant.”
The post-Vatican II shift toward acknowledging the Jews as a covenanted people has led to heated discussions in the Catholic Church over the issue missionary activity directed toward Jews, with some Catholics theologians reasoning that “if Christ is the redeemer of the world, every tongue should confess him,” while others vehemently oppose “targeting Jews for conversion.” Weighing in on this matter, Cardinal Walter Kasper, then President of the Pontifical Commission for Religious Relations with the Jews, reaffirmed the validity of the Jews’ covenant and then continued:"as Christians we know that God’s covenant with Israel by God's faithfulness is not broken (Rom 11,29; cf. 3,4), mission understood as call to conversion from idolatry to the living and true God (1 Thes 1,9) does not apply and cannot be applied to Jews…. This is not a merely abstract theological affirmation, but an affirmation that has concrete and tangible consequences; namely, that there is no organised Catholic missionary activity towards Jews as there is for all other non–Christian religions.”
Recently, in his apostolic exhortation "Evangelii gaudium" (2013), Pope Francis’s own teaching on the matter closely mirrored these words of Cardinal Kasper:
In 2011, Kasper specifically repudiated the notion of “displacement” theology, clarifying that the “New Covenant for Christians is not the replacement (substitution), but the fulfillment of the Old Covenant.”
These statements from Catholic officials signal a shift away from a “hard” supersessionist model of displacement. Nevertheless, the references to the Church as the “new People of God” and the New Covenant as “fulfilling” the Old Covenant (irrevocable though it might be) imply a clear Christian superiority and thus comport with a “soft” supersessionism. It should be noted that fringe Catholic groups, such as the Society of St. Pius X, strongly oppose the theological developments concerning Judaism made at Vatican II and retain “hard” supersessionist views. Even among mainstream Catholic groups and official Catholic teaching, elements of “soft” supersessionism remain:
Protestant.
Protestant opinions on supersessionism vary. These differences arise from dissimilar literal versus figurative approaches to understanding the relationships between the covenants of the Bible, particularly the relationship between the covenants of the Old Testament and the New Covenant. In consequence, there is a range of viewpoints, including:
Three prominent Protestant views on this relationship are covenant theology, New Covenant theology, and dispensationalism. Extensive discussion is found in Christian views on the Old Covenant and in the respective articles for each of these viewpoints: for example, there is a section within Dispensationalism detailing that perspective's concept of Israel. Differing approaches influence how the land promise in Genesis 12, 15 and 17 is understood, whether it is interpreted literally or figuratively, both with regard to the land and the identity of people who inherit it.
Adherents to these various views are not restricted to a single denomination, although covenant theology is particularly important within the Presbyterian and Reformed traditions. In the US, a difference of approach has been perceived between the Presbyterian Church and the Episcopal Church, the Evangelical Lutheran Church of America, and the United Methodist Church which have worked to develop a non-supersessionist theology.
Paul van Buren developed a thoroughly nonsupersessionist position, in contrast to Karl Barth, his mentor. He wrote, "The reality of the Jewish people, fixed in history by the reality of their election, in their faithfulness in spite of their unfaithfulness, is as solid and sure as that of the gentile church."
Mormonism.
The Latter Day Saint movement rejects supersessionism.
Jewish views.
From a Jewish perspective, the Torah was given to the Jewish people as an eternal covenant (for example , ) and will never be replaced or added to (for example , ). For religious Jews and other critics, supersessionism is usually perceived as a theology of replacement, which substitutes the Christian church, consisting of Christians, for the Jewish and B'nei Noah people. While some modern Jews are offended by the traditional Christian belief in supersessionism, a different viewpoint has been offered by Rabbi and Jewish theologian David Novak, who has stated that "Christian supersessionism need not denigrate Judaism" and that some subsets of Christian supersessionism can affirm that God has not annulled his everlasting covenant with the Jewish people, neither past nor present nor future."

</doc>
<doc id="29213" url="https://en.wikipedia.org/wiki?curid=29213" title="Software cracking">
Software cracking

Software cracking (known as "breaking" in the 1980s) is the modification of software to remove or disable features which are considered undesirable by the person cracking the software, especially copy protection features (including protection against the manipulation of software, serial number, hardware key, date checks and disc check) or software annoyances like nag screens and adware.
A crack refers to the mean of achieving software cracking, for example a stolen serial number or a tool that performs that act of cracking. Some of these tools are called keygen, patch or loader. A keygen is a handmade product license generator that often offers the ability to generate legitimate licenses in your own name. A patch is a small computer program that modifies the machine code of another program. This has the advantage for a cracker to not include a large executable in a release when only a few bytes are changed. A loader modifies the startup flow of a program and does not remove the protection but circumvents it. A well known example of a loader is a trainer used to cheat in games. Fairlight pointed out in one of their .nfo files that these type of cracks are not allowed for warez scene game releases. A nukewar has shown that the protection may not kick in at any point for it to be a valid crack.
The distribution of cracked copies is illegal in most countries. There have been lawsuits over cracking software. It might be legal to use cracked software in certain circumstances.
History.
The first software copy protection was applied to software for Apple II, Atari 800 and Commodore 64 computers. Software publishers have implemented increasingly complex methods to try to stop unauthorized copying of software.
On the Apple II, unlike modern computers that use standardized device drivers to manage device communications, the operating system directly controlled the step motor that moves the floppy drive head, and also directly interpreted the raw data, called "nibbles", read from each track to identify the data sectors. This allowed complex disk-based software copy protection, by storing data on half tracks (0, 1, 2.5, 3.5, 5, 6...), quarter tracks (0, 1, 2.25, 3.75, 5, 6...), and any combination thereof. In addition, tracks did not need to be perfect rings, but could be sectioned so that sectors could be staggered across overlapping offset tracks, the most extreme version being known as spiral tracking. It was also discovered that many floppy drives did not have a fixed upper limit to head movement, and it was sometimes possible to write an additional 36th track above the normal 35 tracks. The standard Apple II copy programs could not read such protected floppy disks, since the standard DOS assumed that all disks had a uniform 35-track, 13- or 16-sector layout. Special nibble-copy programs such as Locksmith and Copy II Plus could sometimes duplicate these disks by using a reference library of known protection methods; when protected programs were cracked they would be completely stripped of the copy protection system, and transferred onto a standard format disk that any normal Apple II copy program could read.
One of the primary routes to hacking these early copy protections was to run a program that simulates the normal CPU operation. The CPU simulator provides a number of extra features to the hacker, such as the ability to single-step through each processor instruction and to examine the CPU registers and modified memory spaces as the simulation runs (any modern disassembler/debugger can do this). The Apple II provided a built-in opcode disassembler, allowing raw memory to be decoded into CPU opcodes, and this would be utilized to examine what the copy-protection was about to do next. Generally there was little to no defense available to the copy protection system, since all its secrets are made visible through the simulation. However, because the simulation itself must run on the original CPU, in addition to the software being hacked, the simulation would often run extremely slowly even at maximum speed.
On Atari 8-bit computers, the most common protection method was via "bad sectors". These were sectors on the disk that were intentionally unreadable by the disk drive. The software would look for these sectors when the program was loading and would stop loading if an error code was not returned when accessing these sectors. Special copy programs were available that would copy the disk and remember any bad sectors. The user could then use an application to spin the drive by constantly reading a single sector and display the drive RPM. With the disk drive top removed a small screwdriver could be used to slow the drive RPM below a certain point. Once the drive was slowed down the application could then go and write "bad sectors" where needed. When done the drive RPM was sped up back to normal and an uncracked copy was made. Of course cracking the software to expect good sectors made for readily copied disks without the need to meddle with the disk drive. As time went on more sophisticated methods were developed, but almost all involved some form of malformed disk data, such as a sector that might return different data on separate accesses due to bad data alignment. Products became available (from companies such as Happy Computers) which replaced the controller BIOS in Atari's "smart" drives. These upgraded drives allowed the user to make exact copies of the original program with copy protections in place on the new disk.
On the Commodore 64, several methods were used to protect software. For software distributed on ROM cartridges, subroutines were included which attempted to write over the program code. If the software was on ROM, nothing would happen, but if the software had been moved to RAM, the software would be disabled. Because of the operation of Commodore floppy drives, one write protection scheme would cause the floppy drive head to bang against the end of its rail, which could cause the drive head to become misaligned. In some cases, cracked versions of software were desirable to avoid this result. A misaligned drive head was rare usually fixing itself by smashing against the rail stops. Another brutal protection scheme was grinding from track 1 to 40 and back a few times.
Most of the early software crackers were computer hobbyists who often formed groups that competed against each other in the cracking and spreading of software. Breaking a new copy protection scheme as quickly as possible was often regarded as an opportunity to demonstrate one's technical superiority rather than a possibility of money-making. Some low skilled hobbyists would take already cracked software and edit various unencrypted strings of text in it to change messages a game would tell a game player, often something considered vulgar. Uploading the altered copies on file sharing networks provided a source of laughs for adult users. The cracker groups of the 1980s started to advertise themselves and their skills by attaching animated screens known as crack intros in the software programs they cracked and released. Once the technical competition had expanded from the challenges of cracking to the challenges of creating visually stunning intros, the foundations for a new subculture known as demoscene were established. Demoscene started to separate itself from the illegal "warez scene" during the 1990s and is now regarded as a completely different subculture. Many software crackers have later grown into extremely capable software reverse engineers; the deep knowledge of assembly required in order to crack protections enables them to reverse engineer drivers in order to port them from binary-only drivers for Windows to drivers with source code for Linux and other free operating systems. Also because music and game intro was such an integral part of gaming the music format and graphics became very popular when hardware became affordable for the home user.
With the rise of the Internet, software crackers developed secretive online organizations. In the latter half of the nineties, one of the most respected sources of information about "software protection reversing" was Fravia's website.
Most of the well-known or "elite" cracking groups make software cracks entirely for respect in the "The Scene", not profit. From there, the cracks are eventually leaked onto public Internet sites by people/crackers who use well-protected/secure FTP release archives, which are made into full copies and sometimes sold illegally by other parties.
The Scene today is formed of small groups of skilled people, who informally compete to have the best crackers, methods of cracking, and reverse engineering.
+HCU.
The "High Cracking University" (+HCU), was founded by Old Red Cracker (+ORC), considered a genius of reverse engineering and a legendary figure in RCE, to advance research into Reverse Code Engineering (RCE). He had also taught and authored many papers on the subject, and his texts are considered classics in the field and are mandatory reading for students of RCE.
The addition of the "+" sign in front of the nickname of a reverser signified membership in the +HCU. Amongst the students of +HCU were the top of the elite Windows reversers worldwide. +HCU published a new reverse engineering problem annually and a small number of respondents with the best replies qualified for an undergraduate position at the university.
+Fravia was a professor at +HCU. Fravia's website was known as "+Fravia's Pages of Reverse Engineering" and he used it to challenge programmers as well as the wider society to "reverse engineer" the "brainwashing of a corrupt and rampant materialism". In its heyday, his website received millions of visitors per year and its influence was "widespread".
Nowadays most of the graduates of +HCU have migrated to Linux and few have remained as Windows reversers. The information at the university has been rediscovered by a new generation of researchers and practitioners of RCE who have started new research projects in the field.
Methods.
The most common software crack is the modification of an application's binary to cause or prevent a specific key branch in the program's execution. This is accomplished by reverse engineering the compiled program code using a debugger such as SoftICE, x64dbg, OllyDbg, GDB, or MacsBug until the software cracker reaches the subroutine that contains the primary method of protecting the software (or by disassembling an executable file with a program such as IDA). The binary is then modified using the debugger or a hex editor or monitor in a manner that replaces a prior branching opcode with its complement or a NOP opcode so the key branch will either always execute a specific subroutine or skip over it. Almost all common software cracks are a variation of this type. Proprietary software developers are constantly developing techniques such as code obfuscation, encryption, and self-modifying code to make this modification increasingly difficult. Even with these measures being taken, developers struggle to combat software cracking. This is because it is very common for a professional to publicly release a simple cracked EXE or Retrium Installer for public download, eliminating the need for inexperienced users to crack the software themselves.
A specific example of this technique is a crack that removes the expiration period from a time-limited trial of an application. These cracks are usually programs that alters the program executable and sometimes the .dll or .so linked to the application. Similar cracks are available for software that requires a hardware dongle. A company can also break the copy protection of programs that they have legally purchased but that are licensed to particular hardware, so that there is no risk of downtime due to hardware failure (and, of course, no need to restrict oneself to running the software on bought hardware only).
Another method is the use of special software such as CloneCD to scan for the use of a commercial copy protection application. After discovering the software used to protect the application, another tool may be used to remove the copy protection from the software on the CD or DVD. This may enable another program such as Alcohol 120%, CloneDVD, Game Jackal, or Daemon Tools to copy the protected software to a user's hard disk. Popular commercial copy protection applications which may be scanned for include SafeDisc and StarForce.
In other cases, it might be possible to decompile a program in order to get access to the original source code or code on a level higher than machine code. This is often possible with scripting languages and languages utilizing JIT compilation. An example is cracking (or debugging) on the .NET platform where one might consider manipulating CIL to achieve one's needs. Java's bytecode also works in a similar fashion in which there is an intermediate language before the program is compiled to run on the platform dependent machine code.
Advanced reverse engineering for protections such as SecuROM, SafeDisc or StarForce requires a cracker, or many crackers to spend much time studying the protection, eventually finding every flaw within the protection code, and then coding their own tools to "unwrap" the protection automatically from executable (.EXE) and library (.DLL) files.
There are a number of sites on the Internet that let users download cracks for popular games and applications (although at the danger of acquiring malicious software that is sometimes distributed via such sites). Although these cracks are used by legal buyers of software, they can also be used by people who have downloaded or otherwise obtained unauthorized copies (often through P2P networks).

</doc>
<doc id="29215" url="https://en.wikipedia.org/wiki?curid=29215" title="SOAP">
SOAP

SOAP, originally an acronym for Simple Object Access Protocol, is a protocol specification for exchanging structured information in the implementation of web services in computer networks. It uses XML Information Set for its message format, and relies on application layer protocols, most notably Hypertext Transfer Protocol (HTTP) or Simple Mail Transfer Protocol (SMTP), for message negotiation and transmission.
Characteristics.
SOAP can form the foundation layer of a web services protocol stack, providing a basic messaging framework for web services. This XML-based protocol consists of three parts: 
SOAP has three major characteristics:
As an example of what SOAP procedures can do, an application can send a SOAP request to a server that has web services enabled—such as a real-estate price database—with the parameters for a search. The server then returns a SOAP response (an XML-formatted document with the resulting data), e.g., prices, location, features. Since the generated data comes in a standardized machine-parsable format, the requesting application can then integrate it directly.
The SOAP architecture consists of several layers of specifications for:
SOAP evolved as a successor of XML-RPC, though it borrows its transport and interaction neutrality and the envelope/header/body from elsewhere (probably from WDDX).
History.
SOAP was designed as an object-access protocol in 1998 by Dave Winer, Don Box, Bob Atkinson, and Mohsen Al-Ghosein for Microsoft, where Atkinson and Al-Ghosein were working. Due to politics within Microsoft, the specification was not made available until it was submitted to IETF 13 September 1999. Because of Microsoft's hesitation, Dave Winer shipped XML-RPC in 1998.
The submitted Internet Draft did not reach RFC status and is therefore not considered a "standard" as such. Version 1.1 of the specification was published as a W3C Note on 8 May 2000. Since version 1.1 did not reach W3C Recommendation status, it can not be considered a "standard" either. Version 1.2 of the specification, however, became a W3C recommendation on June 24, 2003.
The SOAP specification was maintained by the XML Protocol Working Group of the World Wide Web Consortium until the group was closed 10 July 2009. "SOAP" originally stood for "Simple Object Access Protocol" but version 1.2 of the standard dropped this acronym.
After SOAP was first introduced, it became the underlying layer of a more complex set of Web Services, based on Web Services Description Language (WSDL), XML Schema and Universal Description Discovery and Integration (UDDI). These different services, especially UDDI, have proved to be of far less interest, but an appreciation of them gives a more complete understanding of the expected role of SOAP compared to how web services have actually evolved.
Specification.
The SOAP specification defines the messaging framework, which consists of:
Processing model.
The SOAP processing model describes a distributed processing model, its participants, the "SOAP nodes", and how a SOAP receiver processes a SOAP message. The following SOAP nodes are defined:
SOAP Building Blocks.
A SOAP message is an ordinary XML document containing the following elements:
Transport methods.
Both SMTP and HTTP are valid application layer protocols used as transport for SOAP, but HTTP has gained wider acceptance as it works well with today's internet infrastructure; specifically, HTTP works well with network firewalls. SOAP may also be used over HTTPS (which is the same protocol as HTTP at the application level, but uses an encrypted transport protocol underneath) with either simple or mutual authentication; this is the advocated WS-I method to provide web service security as stated in the WS-I Basic Profile 1.1.
This is a major advantage over other distributed protocols like GIOP/IIOP or DCOM, which are normally filtered by firewalls. SOAP over AMQP is yet another possibility that some implementations support. SOAP also has an advantage over DCOM that it is unaffected by security rights configured on the machines that require knowledge of both transmitting and receiving nodes. This lets SOAP be loosely coupled in a way that is not possible with DCOM. There is also the SOAP-over-UDP OASIS standard.
Message format.
XML Information Set was chosen as the standard message format because of its widespread use by major corporations and open source development efforts. Typically, XML Information Set is serialized as XML. A wide variety of freely available tools significantly eases the transition to a SOAP-based implementation. The somewhat lengthy syntax of XML can be both a benefit and a drawback. While it promotes readability for humans, facilitates error detection, and avoids interoperability problems such as byte-order (endianness), it can slow processing speed and can be cumbersome. For example, CORBA, GIOP, ICE, and DCOM use much shorter, binary message formats. On the other hand, hardware appliances are available to accelerate processing of XML messages. Binary XML is also being explored as a means for streamlining the throughput requirements of XML.
XML messages by their self-documenting nature usually have more 'overhead' (Headers, footers, nested tags, delimiters) than actual data in contrast to earlier protocols where the overhead was usually a relatively small percentage of the overall message.
In financial messaging SOAP was found to result in a 2–4 times larger message than previous protocols FIX (Financial Information Exchange) and CDR (Common Data Representation).
XML Information Set does not have to be serialized in XML. For instance, a CSV or JSON XML-infoset representation exists. There is also no need to specify a generic transformation framework. The concept of SOAP bindings allows for specific bindings for a specific application. The drawback is that both the senders and receivers have to support this newly defined binding.
Example message.
<syntaxhighlight lang="http">
POST /InStock HTTP/1.1
Host: www.example.org
Content-Type: application/soap+xml; charset=utf-8
Content-Length: 299
SOAPAction: "http://www.w3.org/2003/05/soap-envelope"
<?xml version="1.0"?>
<soap:Envelope xmlns:soap="http://www.w3.org/2003/05/soap-envelope">
</soap:Envelope>
</syntaxhighlight>

</doc>
<doc id="29218" url="https://en.wikipedia.org/wiki?curid=29218" title="Sodium thiopental">
Sodium thiopental

Sodium thiopental, also known as Sodium Pentothal (a trademark of Abbott Laboratories, not to be confused with pentobarbital), thiopental, thiopentone, or Trapanal (also a trademark), is a rapid-onset short-acting barbiturate general anesthetic that is an analogue of thiobarbital. Sodium thiopental was a core medicine in the World Health Organization's "Essential Drugs List", which is a list of minimum medical needs for a basic healthcare system, but was supplanted by propofol. It was previously the first of three drugs administered during most lethal injections in the United States, but the U.S. manufacturer Hospira stopped manufacturing the drug and the EU banned the export of the drug for this purpose.
Uses.
Anesthesia.
Sodium thiopental is an ultra-short-acting barbiturate and has been used commonly in the induction phase of general anesthesia. Its use has been largely replaced with that of propofol. Following intravenous injection, the drug rapidly reaches the brain and causes unconsciousness within 30–45 seconds. At one minute, the drug attains a peak concentration of about 60% of the total dose in the brain. Thereafter, the drug distributes to the rest of the body, and in about 5–10 minutes the concentration is low enough in the brain such that consciousness returns.
A normal dose of sodium thiopental (usually 4–6 mg/kg) given to a pregnant woman for operative delivery (caesarian section) rapidly makes her unconscious, but the baby in her uterus remains conscious. However, larger or repeated doses can depress the baby.
Sodium thiopental is not used to maintain anesthesia in surgical procedures because, in infusion, it displays zero-order elimination kinetics, leading to a long period before consciousness is regained. Instead, anesthesia is usually maintained with an inhaled anesthetic (gas) agent. Inhaled anesthetics are eliminated relatively quickly, so that stopping the inhaled anesthetic will allow rapid return of consciousness. Sodium thiopental would have to be given in large amounts to maintain an anesthetic plane, and because of its 11.5–26 hour half-life, consciousness would take a long time to return.
In veterinary medicine, sodium thiopental is used to induce anesthesia in animals. Since it is redistributed to fat, certain lean breeds of dogs such as sight hounds will have prolonged recoveries from sodium thiopental due to their lack of body fat and their lean body mass. Conversely, obese animals will have rapid recoveries, but it will be some time before it is entirely removed (metabolized) from their bodies. Sodium thiopental is always administered intravenously, as it can be fairly irritating; severe tissue necrosis and sloughing can occur if it is injected incorrectly into the tissue around a vein.
Medically induced coma.
In addition to anesthesia induction, sodium thiopental was historically used to induce medical comas. It has now been superseded by drugs such as propofol because their effects wear off more quickly than thiopental.
Patients with brain swelling, causing elevation of intracranial pressure, either secondary to trauma or following surgery, may benefit from this drug. Sodium thiopental, and the barbiturate class of drugs, decrease neuronal activity and therefore decrease the production of osmotically active metabolites, which in turn decreases swelling. Patients with significant swelling have improved outcomes following the induction of coma. Reportedly, thiopental has been shown to be superior to pentobarbital in reducing intracranial pressure.This phenomenon is also termed as Reverse steal Effect.
Status epilepticus.
In refractory status epilepticus, thiopental may be used to terminate a seizure.
Euthanasia.
Sodium thiopental is used intravenously for the purposes of euthanasia. In both Belgium and the Netherlands, where active euthanasia is allowed by law, the standard protocol recommends sodium thiopental as the ideal agent to induce coma, followed by pancuronium bromide.
Intravenous administration is the most reliable and rapid way to accomplish euthanasia. A coma is first induced by intravenous administration of 20 mg/kg thiopental sodium (Nesdonal) in a small volume (10 ml physiological saline). Then, a triple dose of a non-depolarizing neuromuscular blocking drug is given, such as 20 mg pancuronium bromide (Pavulon) or 20 mg vecuronium bromide (Norcuron). The muscle relaxant should be given intravenously to ensure optimal availability but pancuronium bromide may be administered intramuscularly at an increased dosage level of 40 mg.
Lethal injection.
Along with pancuronium bromide and potassium chloride, thiopental is used in 34 states of the U.S. to execute prisoners by lethal injection. A very large dose is given to ensure rapid loss of consciousness. Although death usually occurs within ten minutes of the beginning of the injection process, some have been known to take longer. The use of sodium thiopental in execution protocols was challenged in court after a study in the medical journal "The Lancet" reported autopsies of executed inmates showed the level of thiopental in their bloodstream was insufficient to cause unconsciousness.
On December 8, 2009, the State of Ohio became the first to use a single dose of sodium thiopental for its capital execution, following the failed use of the standard three-drug cocktail during a recent execution, due to inability to locate suitable veins. Kenneth Biros was executed using the single-drug method.
The state of Washington is now the second state in the U.S. to use the single-dose sodium thiopental injections for death penalty executions. On September 10, 2010, Cal Coburn Brown was executed. This was the first execution in the state to use a single dose, single drug injection. His death was pronounced approximately one and a half minutes after the intravenous administration of five grams of the drug.
After its use for execution of Jeffrey Landrigan in the U.S., the UK introduced a ban on the export of sodium thiopental in December 2010, after it was established that no European supplies to the U.S. were being used for any other purpose. The restrictions were based on "the European Union Torture Regulation (including licensing of drugs used in execution by lethal injection)". From 21 December 2011 the European Union extended trade restrictions to prevent the export of certain medicinal products for capital punishment, stating that "the Union disapproves of capital punishment in all circumstances and works towards its universal abolition".
Truth serum.
Thiopental (Pentothal) is still used in some places as a truth serum to weaken the resolve of a subject and make them more compliant to pressure. The barbiturates as a class decrease higher cortical brain functioning. Some psychiatrists hypothesize that because lying is more complex than telling the truth, suppression of the higher cortical functions may lead to the uncovering of the truth. The drug tends to make subjects loquacious and cooperative with interrogators; however, the reliability of confessions made under thiopental is questionable. Sodium thiopental is featured as a truth serum in several Hollywood films, in comics and other literature, and even in popular music.
Psychiatry.
Psychiatrists have used thiopental to desensitize patients with phobias, and to "facilitate the recall of painful repressed memories." One psychiatrist who worked with thiopental is the Dutch Professor Jan Bastiaans, who used this procedure to help relieve trauma in surviving victims of the Holocaust.
Mechanism of action.
Sodium thiopental is a member of the barbiturate class of drugs, which are relatively non-selective compounds that bind to an entire superfamily of ligand-gated ion channels, of which the GABAA receptor channel is one of several representatives. This superfamily of ion channels includes the neuronal nAChR channel, the 5HT3R channel, the GlyR channel and others. Surprisingly, while GABAA receptor currents are increased by barbiturates (and other general anesthetics), ligand-gated ion channels that are predominantly permeable for cationic ions are blocked by these compounds. For example, neuronal nAChR channels are blocked by clinically relevant anesthetic concentrations of both sodium thiopental and pentobarbital. Such findings implicate (non-GABA-ergic) ligand-gated ion channels, e.g. the neuronal nAChR channel, in mediating some of the (side) effects of barbiturates. The GABAA receptor is an inhibitory channel that decreases neuronal activity, and barbiturates enhance the inhibitory action of the GABAA receptor.
Controversies.
Following a shortage that led a court to delay an execution in California, a company spokesman for Hospira, the sole American manufacturer of the drug, objected to the use of thiopental in lethal injection. "Hospira manufactures this product because it improves or saves lives, and the company markets it solely for use as indicated on the product labeling. The drug is not indicated for capital punishment, and Hospira does not support its use in this procedure." On January 21, 2011, the company announced that it would stop production of sodium thiopental from its plant in Italy because it could not guarantee Italian authorities that the drug would not be used in executions. Italy was the only viable place where the company could produce sodium thiopental, leaving the United States without a supplier.
Metabolism.
Thiopental rapidly and easily crosses the blood brain barrier as it is a lipophilic molecule. As with all lipid-soluble anaesthetic drugs, the short duration of action of sodium thiopental is due almost entirely to its redistribution away from central circulation towards muscle and fat tissue, due to its very high fat:water partition coefficient (aprx 10), leading to sequestration in fat tissue. Once redistributed, the free fraction in the blood is metabolized in the liver. Sodium thiopental is mainly metabolized to pentobarbital, 5-ethyl-5-(1'-methyl-3'-hydroxybutyl)-2-thiobarbituric acid, and 5-ethyl-5-(1'-methyl-3'-carboxypropyl)-2-thiobarbituric acid.
Dosage.
The usual dose range for induction of anesthesia using thiopental is from 3 to 7 mg/kg; however, there are many factors that can alter this. Premedication with sedatives such as benzodiazepines or clonidine will reduce requirements, as do specific disease states and other patient factors. Among patient factors are: age, sex, and lean body mass. Specific disease conditions that can alter the dose requirements of thiopentone and for that matter any other intravenous anaesthetic are: hypovolemia, burns, azotemia, hepatic failure, hypoproteinemia, etc.
Side effects.
As with nearly all anesthetic drugs, thiopental causes cardiovascular and respiratory depression resulting in hypotension, apnea and airway obstruction. For these reasons, only suitably trained medical personnel should give thiopental in an environment suitably equipped to deal with these effects. Side effects include headache, agitated emergence, prolonged somnolence, and nausea. Intravenous administration of sodium thiopental is followed instantly by an odor and/or taste sensation, sometimes described as being similar to rotting onions, or to garlic. The hangover from the side effects may last up to 36 hours.
Although individual molecules of thiopental contain one sulfur atom, it is not a sulfonamide, and does not show allergic reactions of sulfa/sulpha drugs.
Contraindications.
Thiopental should be used with caution in cases of liver disease, Addison's disease, myxedema, severe heart disease, severe hypotension, a severe breathing disorder, or a family history of porphyria.
Co-administration of pentoxifylline and thiopental causes death by acute pulmonary edema in rats. This pulmonary edema was not mediated by cardiac failure or by pulmonary hypertension but was due to increased pulmonary vascular permeability.
History.
Sodium thiopental was discovered in the early 1930s by Ernest H. Volwiler and Donalee L. Tabern, working for Abbott Laboratories. It was first used in human beings on March 8, 1934, by Dr. Ralph M. Waters in an investigation of its properties, which were short-term anesthesia and surprisingly little analgesia. Three months later, Dr. John S. Lundy started a clinical trial of thiopental at the Mayo Clinic at the request of Abbott. Abbott continued to make the drug until 2004, when it spun off its hospital-products division as Hospira.
Thiopental is famously associated with a number of anesthetic deaths in victims of the attack on Pearl Harbor. These deaths, relatively soon after the drug's introduction, were said to be due to excessive doses given to shocked trauma patients. However, recent evidence available through freedom of information legislation was reviewed in the "British Journal of Anaesthesia", which has suggested that this story was grossly exaggerated. Of the 344 wounded that were admitted to the Tripler Army Hospital only 13 did not survive and it is unlikely that thiopentone overdose was responsible for more than a few of these.
Thiopental is still rarely used as a recreational drug, usually stolen from veterinarians or other legitimate users of the drug; however, more common sedatives such as benzodiazepines are usually preferred as recreational drugs, and abuse of thiopental tends to be uncommon and opportunistic.

</doc>
<doc id="29219" url="https://en.wikipedia.org/wiki?curid=29219" title="Stone Age">
Stone Age

The Stone Age was a broad prehistoric period during which stone was widely used to make implements with a sharp edge, a point, or a percussion surface. The period lasted roughly 3.4 million years, and ended between 6000 BC (or BCE) and 2000 BC (BCE) with the advent of metalworking. 
Stone Age artifacts include tools used by modern humans and by their predecessor species in the genus "Homo", and possibly by the earlier partly contemporaneous genera "Australopithecus" and "Paranthropus". Bone tools were used during this period as well but are rarely preserved in the archaeological record. The Stone Age is further subdivided by the types of stone tools in use.
The Stone Age is the first of the three-age system of archaeology, which divides human technological prehistory into three periods:
Historical significance.
The Stone Age is contemporaneous with the evolution of the genus "Homo", the only exception possibly being at the very beginning, when species prior to "Homo" may have manufactured tools. According to the age and location of the current evidence, the cradle of the genus is the East African Rift System, especially toward the north in Ethiopia, where it is bordered by grasslands. The closest relative among the other living Primates, the genus "Pan", represents a branch that continued on in the deep forest, where the primates evolved. The rift served as a conduit for movement into southern Africa and also north down the Nile into North Africa and through the continuation of the rift in the Levant to the vast grasslands of Asia.
Starting from about 3 million years ago (mya) a single biome established itself from South Africa through the rift, North Africa, and across Asia to modern China, which has been called "transcontinental 'savannahstan'" recently. Starting in the grasslands of the rift, "Homo erectus", the predecessor of modern humans, found an ecological niche as a tool-maker and developed a dependence on it, becoming a "tool equipped savanna dweller."
The Stone Age in archaeology.
Beginning of the Stone Age.
During 2010, fossilised animal bones bearing marks from stone tools were found in the Lower Awash Valley in Ethiopia. Discovered by an international team led by Shannon McPherron, at 3.4 million years old they are the oldest indirect evidence of stone tool use ever found anywhere in the world. Archaeological discoveries in Kenya in 2015, identifying possibly the oldest known evidence of hominin use of tools to date, have indicated that Kenyanthropus platyops ( a 3.2 to 3.5-million-year-old Pliocene hominin fossil discovered in Lake Turkana, Kenya in 1999 ) may have been the earliest tool-users known.
The oldest known stone tools have been excavated from the site of Lomekwi 3 in West Turkana, northwestern Kenya, and date to 3.3 million years old. Prior to the discovery of these "Lomekwian" tools, the oldest known stone tools had been found several sites at Gona, Ethiopia, on the sediments of the paleo-Awash River, which serve to date them. All the tools come from the Busidama Formation, which lies above a disconformity, or missing layer, which would have been from 2.9 to 2.7 mya. The oldest sites containing tools are dated to 2.6–2.55 mya. One of the most striking circumstances about these sites is that they are from the Late Pliocene, where previous to their discovery tools were thought to have evolved only in the Pleistocene. Rogers and Semaw, excavators at the locality, point out that:
The species who made the Pliocene tools remains unknown. Fragments of "Australopithecus garhi", "Australopithecus aethiopicus" and "Homo", possibly "Homo habilis", have been found in sites near the age of the Gona tools.
End of the Stone Age.
Innovation of the technique of smelting ore ended the Stone Age and began the Bronze Age. The first most significant metal manufactured was bronze, an alloy of copper and tin, each of which was smelted separately. The transition from the Stone Age to the Bronze Age was a period during which modern people could smelt copper, but did not yet manufacture bronze, a time known as the Copper Age, or more technically the Chalcolithic, "copper-stone" age. The Chalcolithic by convention is the initial period of the Bronze Age and is unquestionably part of the Age of Metals. The Bronze Age was followed by the Iron Age. During this entire time stone remained in use in parallel with the metals for some objects, including those also used in the Neolithic, such as stone pottery.
The transition out of the Stone Age occurred between 6000 BCE and 2500 BCE for much of humanity living in North Africa and Eurasia. The first evidence of human metallurgy dates to between the 5th and 6th millennium BCE in the archaeological sites of Majdanpek, Yarmovac, and Pločnik in modern-day Serbia (a copper axe from 5500 BCE belonging to the Vinca culture), though not conventionally considered part of the Chalcolithic or "Copper Age", this provides the earliest known example of copper metallurgy. Note the Rudna Glava mine in Serbia. Ötzi the Iceman, a mummy from about 3300 BCE carried with him a copper axe and a flint knife.
In regions such as Subsaharan Africa, the Stone Age was followed directly by the Iron Age. The Middle East and southeastern Asian regions progressed past Stone Age technology around 6000 BCE. Europe, and the rest of Asia became post–Stone Age societies by about 4000 BCE. The proto-Inca cultures of South America continued at a Stone Age level until around 2000 BCE, when gold, copper and silver made their entrance, the rest following later. Australia remained in the Stone Age until European contact in the 17th century. Stone tool manufacture continued even after the Stone Age ended in a given area. In Europe and North America, millstones were in use until well into the 20th century, and still are in many parts of the world.
The concept of Stone Age.
The terms "Stone Age", "Bronze Age", and "Iron Age" were never meant to suggest that advancement and time periods in prehistory are only measured by the type of tool material, rather than, for example, social organization, food sources exploited, adaptation to climate, adoption of agriculture, cooking, settlement and religion. Like pottery, the typology of the stone tools combined with the relative sequence of the types in various regions provide a chronological framework for the evolution of man and society. They serve as diagnostics of date, rather than characterizing the people or the society.
Lithic analysis is a major and specialised form of archaeological investigation. It involves the measurement of the stone tools to determine their typology, function and the technology involved. It includes scientific study of the lithic reduction of the raw materials, examining how the artifacts were made. Much of this study takes place in the laboratory in the presence of various specialists. In experimental archaeology, researchers attempt to create replica tools, to understand how they were made. Flintknappers are craftsmen who use sharp tools to reduce flintstone to flint tool.
In addition to lithic analysis, the field prehistorian utilizes a wide range of techniques derived from multiple fields. The work of the archaeologist in determining the paleocontext and relative sequence of the layers is supplemented by the efforts of the geologic specialist in identifying layers of rock over geologic time, of the paleontological specialist in identifying bones and animals, of the palynologist in discovering and identifying plant species, of the physicist and chemist in laboratories determining dates by the carbon-14, potassium-argon and other methods. Study of the Stone Age has never been mainly about stone tools and archaeology, which are only one form of evidence. The chief focus has always been on the society and the physical people who belonged to it.
Useful as it has been, the concept of the Stone Age has its limitations. The date range of this period is ambiguous, disputed, and variable according to the region in question. While it is possible to speak of a general 'stone age' period for the whole of humanity, some groups never developed metal-smelting technology, so remained in a 'stone age' until they encountered technologically developed cultures. The term was innovated to describe the archaeological cultures of Europe. It may not always be the best in relation to regions such as some parts of the Indies and Oceania, where farmers or hunter-gatherers used stone for tools until European colonisation began.
The archaeologists of the late 19th and early 20th centuries CE, who adapted the three-age system to their ideas, hoped to combine cultural anthropology and archaeology in such a way that a specific contemporaneous tribe can be used to illustrate the way of life and beliefs of the people exercising a specific Stone-Age technology. As a description of people living today, the term "stone age" is controversial. The Association of Social Anthropologists discourages this use, asserting:"To describe any living group as 'primitive' or 'Stone Age' inevitably implies that they are living representatives of some earlier stage of human development that the majority of humankind has left behind."
The three-stage system.
In the 1920s, South African archaeologists organizing the stone tool collections of that country observed that they did not fit the newly detailed Three-Age System. In the words of J. Desmond Clark,
Consequently, they proposed a new system for Africa, the Three-stage System. Clark regarded the Three-age System as valid for North Africa; in sub-Saharan Africa, the Three-stage System was best. In practice, the failure of African archaeologists either to keep this distinction in mind, or to explain which one they mean, contributes to the considerable equivocation already present in the literature. There are in effect two Stone Ages, one part of the Three-age and the other constituting the Three-stage. They refer to one and the same artifacts and the same technologies, but vary by locality and time.
The Three-stage System was proposed in 1929 by Astley John Hilary Goodwin, a professional archaeologist, and Clarence van Riet Lowe, a civil engineer and amateur archaeologist, in an article titled "Stone Age Cultures of South Africa" in the journal "Annals of the South African Museum". By then, the dates of the Early Stone Age, or Paleolithic, and Late Stone Age, or Neolithic ("neo" = new), were fairly solid and were regarded by Goodwin as absolute. He therefore proposed a relative chronology of periods with floating dates, to be called the Earlier and Later Stone Age. The Middle Stone Age would not change its name, but it would not mean Mesolithic.
The duo thus reinvented the Stone Age. In Sub-Saharan Africa, however, it was ended by the intrusion of the Iron Age from the north. The Neolithic and the Bronze Age never occurred. Moreover, the technologies included in those 'stages', as Goodwin called them, were not exactly the same. Since then, the original relative terms have become identified with the technologies of the Paleolithic and Mesolithic, so that they are no longer relative. Moreover, there has been a tendency to drop the comparative degree in favor of the positive: resulting in two sets of Early, Middle and Late Stone Ages of quite different content and chronologies.
By voluntary agreement, archaeologists respect the decisions of the Pan-African Congress of Prehistory, which meets every four years to resolve archaeological business brought before it. Delegates are actually international; the organization takes its name from the topic. Louis Leakey hosted the first one in Nairobi in 1947. It adopted Goodwin and Lowe's 3-stage system at that time, the stages to be called Early, Middle and Later.
The problem of the transitions.
The problem of the transitions in archaeology is a branch of the general philosophic continuity problem, which examines how discrete objects of any sort that are contiguous in any way can be presumed to have a relationship of any sort. In archaeology, the relationship is one of causality. If Period B can be presumed to descend from Period A, there must be a boundary between A and B, the A–B boundary. The problem is in the nature of this boundary. If there is no distinct boundary, then the population of A suddenly stopped using the customs characteristic of A and suddenly started using those of B, an unlikely scenario in the process of evolution. More realistically, a distinct border period, the A/B transition, existed, in which the customs of A were gradually dropped and those of B acquired. If transitions do not exist, then there is no proof of any continuity between A and B.
The Stone Age of Europe is characteristically in deficit of known transitions. The 19th and early 20th-century innovators of the modern three-age system recognized the problem of the initial transition, the "gap" between the Paleolithic and the Neolithic. Louis Leakey provided something of an answer by proving that man evolved in Africa. The Stone Age must have begun there to be carried repeatedly to Europe by migrant populations. The different phases of the Stone Age thus could appear there without transitions. The burden on African archaeologists became all the greater, because now they must find the missing transitions in Africa. The problem is difficult and ongoing.
After its adoption by the First Pan African Congress in 1947, the Three-Stage Chronology was amended by the Third Congress in 1955 to include a First Intermediate Period between Early and Middle, to encompass the Fauresmith and Sangoan technologies, and the Second Intermediate Period between Middle and Later, to encompass the Magosian technology and others. The chronologic basis for definition was entirely relative. With the arrival of scientific means of finding an absolute chronology, the two intermediates turned out to be will-of-the-wisps. They were in fact Middle and Lower Paleolithic. Fauresmith is now considered to be a facies of Acheulean, while Sangoan is a facies of Lupemban. Magosian is "an artificial mix of two different periods."
Once seriously questioned, the intermediates did not wait for the next Pan African Congress two years hence, but were officially rejected in 1965 (again on an advisory basis) by Burg Wartenstein Conference #29, "Systematic Investigation of the African Later Tertiary and Quaternary", a conference in anthropology held by the Wenner-Gren Foundation, at Burg Wartenstein Castle, which it then owned in Austria, attended by the same scholars that attended the Pan African Congress, including Louis Leakey and Mary Leakey, who was delivering a pilot presentation of her typological analysis of Early Stone Age tools, to be included in her 1971 contribution to "Olduvai Gorge", "Excavations in Beds I and II, 1960–1963."
However, although the intermediate periods were gone, the search for the transitions continued.
Chronology.
In 1859 Jens Jacob Worsaae first proposed a division of the Stone Age into older and younger parts based on his work with Danish kitchen middens that began in 1851. In the subsequent decades this simple distinction developed into the archaeological periods of today. The major subdivisions of the Three-age Stone Age cross two epoch boundaries on the geologic time scale:
The succession of these phases varies enormously from one region (and culture) to another.
Three-age chronology.
The Paleolithic or Palaeolithic (from Greek: παλαιός, "palaios", "old"; and λίθος, "lithos", "stone" lit. "old stone", coined by archaeologist John Lubbock and published in 1865) is the earliest division of the Stone Age. It covers the greatest portion of humanity's time (roughly 99% of "human technological history", where "human" and "humanity" are interpreted to mean the genus "Homo"), extending from 2.5 or 2.6 million years ago, with the first documented use of stone tools by hominans such as "Homo habilis", to the end of the Pleistocene around 10,000 BCE. The Paleolithic era ended with the Mesolithic, or in areas with an early neolithisation, the Epipaleolithic.
Lower Paleolithic.
At sites dating from the Lower Paleolithic Period (about 2,500,000 to 200,000 years ago), simple pebble tools have been found in association with the remains of what may have been the earliest human ancestors. A somewhat more sophisticated Lower Paleolithic tradition, known as the Chopper chopping-tool industry, is widely distributed in the Eastern Hemisphere. This tradition is thought to have been the work of the hominin species named Homo erectus. Although no such fossil tools have yet been found, it is believed that H. erectus probably made tools of wood and bone as well as stone.
About 700,000 years ago, a new Lower Paleolithic tool, the hand ax, appeared. The earliest European hand axes are assigned to the Abbevillian industry, which developed in northern France in the valley of the Somme River; a later, more refined hand-ax tradition is seen in the Acheulian industry, evidence of which has been found in Europe, Africa, the Middle East, and Asia. Some of the earliest known hand axes were found at Olduvai Gorge (Tanzania) in association with remains of H. erectus. Alongside the hand-axe tradition there developed a distinct and very different stone-tool industry, based on flakes of stone: special tools were made from worked (carefully shaped) flakes of flint. In Europe, the Clactonian industry is one example of a flake tradition. The early flake industries probably contributed to the development of the Middle Paleolithic flake tools of the Mousterian industry, which is associated with the remains of Neanderthal man.
Oldowan in Africa.
The earliest documented stone tools have been found in eastern Africa, manufacturers unknown, at the 3.3 million year old site of Lomekwi 3 in Kenya. Better known are the later tools belonging to an industry known as Oldowan, after the type site of Olduvai Gorge in Tanzania.
The tools were formed by knocking pieces off a river pebble, or stones like it, with a hammerstone to obtain large and small pieces with one or more sharp edges. The original stone is called a core; the resultant pieces, flakes. Typically, but not necessarily, small pieces are detached from a larger piece, in which case the larger piece may be called the core and the smaller pieces the flakes. The prevalent usage, however, is to call all the results flakes, which can be confusing. A split in half is called bipolar flaking.
Consequently, the method is often called "core-and-flake". More recently, the tradition has been called "small flake" since the flakes were small compared to subsequent Acheulean tools."The essence of the Oldowan is the making and often immediate use of small flakes."
Another naming scheme is "Pebble Core Technology (PBC)":"Pebble cores are ... artifacts that have been shaped by varying amounts of hard-hammer percussion."
Various refinements in the shape have been called choppers, discoids, polyhedrons, subspheroid, etc. To date no reasons for the variants have been ascertained:"From a functional standpoint, pebble cores seem designed for no specific purpose."
However, they would not have been manufactured for no purpose:"Pebble cores can be useful in many cutting, scraping or chopping tasks, but ... they are not particularly more efficient in such tasks than a sharp-edged rock ..."
The whole point of their utility is that each is a "sharp-edged rock" in locations where nature has not provided any. There is additional evidence that Oldowan, or Mode 1, tools were utilized in "percussion technology"; that is, they were designed to be gripped at the blunt end and strike something with the edge, from which use they were given the name of choppers. Modern science has been able to detect mammalian blood cells on Mode 1 tools at Sterkfontein, Member 5 East, in South Africa. As the blood must have come from a fresh kill, the tool users are likely to have done the killing and used the tools for butchering. Plant residues bonded to the silicon of some tools confirm the use to chop plants.
Although the exact species authoring the tools remains unknown, Mode 1 tools in Africa were manufactured and used predominantly by "Homo habilis". They cannot be said to have developed these tools or to have contributed the tradition to technology. They continued a tradition of yet unknown origin. As chimpanzees sometimes naturally use percussion to extract or prepare food in the wild, and may use either unmodified stones or stones that they have split, creating an Oldowan tool, the tradition may well be far older than its current record.
Towards the end of Oldowan in Africa a new species appeared over the range of "Homo habilis": "Homo erectus". The earliest "unambiguous" evidence is a whole cranium, KNM-ER 3733 (a find identifier) from Koobi Fora in Kenya, dated to 1.78 mya. An early skull fragment, KNM-ER 2598, dated to 1.9 mya, is considered a good candidate also. Transitions in paleoanthropology are always hard to find, if not impossible, but based on the "long-legged" limb morphology shared by "H. habilis" and "H. rudolfensis" in East Africa, an evolution from one of those two has been suggested.
The most immediate cause of the new adjustments appears to have been an increasing aridity in the region and consequent contraction of parkland savanna, interspersed with trees and groves, in favor of open grassland, dated 1.8–1.7 mya. During that transitional period the percentage of grazers among the fossil species increased from 15–25% to 45%, dispersing the food supply and requiring a facility among the hunters to travel longer distances comfortably, which "H. erectus" obviously had. The ultimate proof is the "dispersal" of "H. erectus" "across much of Africa and Asia, substantially before the development of the Mode 2 technology and use of fire ..." "H. erectus" carried Mode 1 tools over Eurasia.
According to the current evidence (which may change at any time) Mode 1 tools are documented from about 2.6 mya to about 1.5 mya in Africa, and to 0.5 mya outside of it. The genus Homo is known from "H. habilis" and "H. rudolfensis" from 2.3 to 2.0 mya, with the latest habilis being an upper jaw from Koobi Fora, Kenya, from 1.4 mya. "H. erectus" is dated 1.8–0.6 mya.
According to this chronology Mode 1 was inherited by "Homo" from unknown Hominans, probably "Australopithecus" and "Paranthropus", who must have continued on with Mode 1 and then with Mode 2 until their extinction no later than 1.1 mya. Meanwhile, living contemporaneously in the same regions "H. habilis" inherited the tools around 2.3 mya. At about 1.9 mya "H. erectus" came on stage and lived contemporaneously with the others. Mode 1 was now being shared by a number of Hominans over the same ranges, presumably subsisting in different niches, but the archaeology is not precise enough to say which.
Oldowan out of Africa.
Tools of the Oldowan tradition first came to archaeological attention in Europe, where, being intrusive and not well defined, compared to the Acheulean, they were puzzling to archaeologists. The mystery would be elucidated by African archaeology at Olduvai, but meanwhile, in the early 20th century, the term "Pre-Acheulean" came into use in climatology. C.E.P, Brooks, a British climatologist working in the United States, used the term to describe a "chalky boulder clay" underlying a layer of gravel at Hoxne, central England, where Acheulean tools had been found. Whether any tools would be found in it and what type was not known. Hugo Obermaier, a contemporary German archaeologist working in Spain, quipped:"Unfortunately, the stage of human industry which corresponds to these deposits cannot be positively identified. All we can say is that it is pre-Acheulean..." This uncertainty was clarified by the subsequent excavations at Olduvai; nevertheless, the term is still in use for pre-Acheulean contexts, mainly across Eurasia, that are yet unspecified or uncertain but with the understanding that they are or will turn out to be pebble-tool.
There are ample associations of Mode 2 with "H. erectus" in Eurasia. "H. erectus" – Mode 1 associations are scantier but they do exist, especially in the Far East. One strong piece of evidence prevents the conclusion that only "H. erectus" reached Eurasia: at Yiron, Israel, Mode 1 tools have been found dating to 2.4 mya, about 0.5 my earlier than the known "H. erectus" finds. If the date is correct, either another Hominan preceded "H. erectus" out of Africa or the earliest "H. erectus" has yet to be found.
After the initial appearance at Gona in Ethiopia at 2.7 mya, pebble tools date from 2.0 mya at Sterkfontein, Member 5, South Africa, and from 1.8 mya at El Kherba, Algeria, North Africa. The manufacturers had already left pebble tools at Yiron, Israel, at 2.4 mya, Riwat, Pakistan, at 2.0 mya, and Renzidong, South China, at over 2 mya. The identification of a fossil skull at Mojokerta, Pernung Peninsula on Java, dated to 1.8 mya, as "H. erectus", suggests that the African finds are not the earliest to be found in Africa, or that, in fact, erectus did not originate in Africa after all but on the plains of Asia. The outcome of the issue waits for more substantial evidence. Erectus was found also at Dmanisi, Georgia, from 1.75 mya in association with pebble tools.
Pebble tools are found the latest first in southern Europe and then in northern. They begin in the open areas of Italy and Spain, the earliest dated to 1.6 mya at Pirro Nord, Italy. The mountains of Italy are rising at a rapid rate in the framework of geologic time; at 1.6 mya they were lower and covered with grassland (as much of the highlands still are). Europe was otherwise mountainous and covered over with dense forest, a formidable terrain for warm-weather savanna dwellers. Similarly there is no evidence that the Mediterranean was passable at Gibraltar or anywhere else to "H. erectus" or earlier hominans. They might have reached Italy and Spain along the coasts.
In northern Europe pebble tools are found earliest at Happisburgh, United Kingdom, from 0.8 mya. The last traces are from Kent's Cavern, dated 0.5 mya. By that time "H. erectus" is regarded as having been extinct; however, a more modern version apparently had evolved, "Homo heidelbergensis", who must have inherited the tools. He also explains the last of the Acheulean in Germany at 0.4 mya.
In the late 19th and early 20th centuries archaeologists worked on the assumptions that a succession of Hominans and cultures prevailed, that one replaced another. Today the presence of multiple hominans living contemporaneously near each other for long periods is accepted as proved true; moreover, by the time the previously assumed "earliest" culture arrived in northern Europe, the rest of Africa and Eurasia had progressed to the Middle and Upper Palaeolithic, so that across the earth all three were for a time contemporaneous. In any given region there was a progression from Oldowan to Acheulean, Lower to Upper, no doubt.
Acheulean in Africa.
The end of Oldowan in Africa was brought on by the appearance of Acheulean, or Mode 2, stone tools. The earliest known instances are in the 1.7–1.6 mya layer at Kokiselei, West Turkana, Kenya. At Sterkfontein, South Africa, they are in Member 5 West, 1.7–1.4 mya. The 1.7 is a fairly certain, fairly standard date. Mode 2 is often found in association with "H. erectus". It makes sense that the most advanced tools should have been innovated by the most advanced Hominan; consequently, they are typically given credit for the innovation.
A Mode 2 tool is a biface consisting of two concave surfaces intersecting to form a cutting edge all the way around, except in the case of tools intended to feature a point. More work and planning go into the manufacture of a Mode 2 tool. The manufacturer hits a slab off a larger rock to use as a blank. Then large flakes are struck off the blank and worked into bifaces by hard-hammer percussion on an anvil stone. Finally the edge is retouched: small flakes are hit off with a bone or wood soft hammer to sharpen or resharpen it. The core can be either the blank or another flake. Blanks are ported for manufacturing supply in places where nature has provided no suitable stone.
Although most Mode 2 tools are easily distinguished from Mode 1, there is a close similarity of some Oldowan and some Acheulean, which can lead to confusion. Some Oldowan tools are more carefully prepared to form a more regular edge. One distinguishing criterion is the size of the flakes. In contrast to the Oldowan "small flake" tradition, Acheulean is "large flake:" "The primary technological distinction remaining between Oldowan and the Acheulean is the preference for large flakes (>10 cm) as blanks for making large cutting tools (handaxes and cleavers) in the Acheulean." "Large Cutting Tool (LCT)" has become part of the standard terminology as well.
In North Africa, the presence of Mode 2 remains a mystery, as the oldest finds are from Thomas Quarry in Morocco at 0.9 mya. Archaeological attention, however, shifts to the Jordan Rift Valley, an extension of the East African Rift Valley (the east bank of the Jordan is slowly sliding northward as East Africa is thrust away from Africa). Evidence of use of the Nile Valley is in deficit, but Hominans could easily have reached the palaeo-Jordan river from Ethiopia along the shores of the Red Sea, one side or the other. A crossing would not have been necessary, but it is more likely there than over a theoretical but unproven land bridge through either Gibraltar or Sicily.
Meanwhile, Acheulean went on in Africa past the 1.0 mya mark and also past the extinction of "H. erectus" there. The last Acheulean in East Africa is at Olorgesailie, Kenya, dated to about 0.9 mya. Its owner was still "H. erectus", but in South Africa, Acheulean at Elandsfontein, 1.0–0.6 mya, is associated with Saldanha man, classified as "H. heidelbergensis", a more advanced, but not yet modern, descendant most likely of "H. erectus". The Thoman Quarry Hominans in Morocco similarly are most likely Homo rhodesiensis, in the same evolutionary status as "H. heidelbergensis".
Acheulean out of Africa.
Mode 2 is first known out of Africa at 'Ubeidiya, Israel, a site now on the Jordan River, then frequented over the long term (hundreds of thousands of years) by Homo on the shore of a variable-level palaeo-lake, long since vanished. The geology was created by successive "transgression and regression" of the lake resulting in four cycles of layers. The tools are located in the first two, Cycles Li (Limnic Inferior) and Fi (Fluviatile Inferior), but mostly in Fi. The cycles represent different ecologies and therefore different cross-sections of fauna, which makes it possible to date them. They appear to be the same faunal assemblages as the Ferenta Faunal Unit in Italy, known from excavations at Selvella and Pieterfitta, dated to 1.6–1.2 mya.
At 'Ubeidiya the marks on the bones of the animal species found there indicate that the manufacturers of the tools butchered the kills of large predators, an activity that has been termed "scavenging". There are no living floors, nor did they process bones to obtain the marrow. These activities cannot be understood therefore as the only or even the typical economic activity of Hominans. Their interests were selective: they were primarily harvesting the meat of Cervids, which is estimated to have been available without spoiling for up to four days after the kill.
The majority of the animals at the site were of "Palaearctic biogeographic origin". However, these overlapped in range on 30–60% of "African biogeographic origin". The biome was Mediterranean, not savanna. The animals were not passing through; there was simply an overlap of normal ranges. Of the Hominans, "H. erectus" left several cranial fragments. Teeth of undetermined species may have been "H. ergaster". The tools are classified as "Lower Acheulean" and "Developed Oldowan". The latter is a disputed classification created by Mary Leakey to describe an Acheulean-like tradition in Bed II at Olduvai. It is dated 1.53–1.27 mya. The date of the tools therefore probably does not exceed 1.5 mya; 1.4 is often given as a date. This chronology, which is definitely later than in Kenya, supports the "out of Africa" hypothesis for Acheulean, if not for the Hominans.
From Southwest Asia, as the Levant is now called, the Acheulean extended itself more slowly eastward, arriving at Isampur, India, about 1.2 mya. It does not appear in China and Korea until after 1mya and not at all in Indonesia. There is a discernible boundary marking the furthest extent of the Acheulean eastward before 1 mya, called the Movius Line, after its proposer, Hallam L. Movius. On the east side of the line the small flake tradition continues, but the tools are additionally worked Mode 1, with flaking down the sides. In Athirampakkam at Chennai in Tamil Nadu the Acheulean age started at 1.51 mya and it is also prior than North India and Europe.
The cause of the Movius Line remains speculative, whether it represents a real change in technology or a limitation of archeology, but after 1 mya evidence not available to Movius indicates the prevalence of Acheulean. For example, the Acheulean site at Bose, China, is dated 0.803±3K mya. The authors of this chronologically later East Asian Acheulean remain unknown, as does whether it evolved in the region or was brought in.
There is no named boundary line between Mode 1 and Mode 2 on the west; nevertheless, Mode 2 is equally late in Europe as it is in the Far East. The earliest comes from a rock shelter at Estrecho de Quípar in Spain, dated to greater than 0.9 mya. Teeth from an undetermined Hominan were found there also. The last Mode 2 in Southern Europe is from a deposit at Fontana Ranuccio near Anagni in Italy dated to 0.45 mya, which is generally linked to "Homo cepranensis", a "late variant of "H. erectus"", a fragment of whose skull was found at Ceprano nearby, dated 0.46 mya.
Middle Paleolithic.
This period is best known as the era during which the Neanderthals lived in Europe and the Near East (c. 300,000–28,000 years ago). Their technology is mainly the Mousterian, but Neanderthal physical characteristics have been found also in ambiguous association with the more recent Châtelperronian archeological culture in Western Europe and several local industries like the Szeletian in Eastern Europe/Eurasia. There is no evidence for Neanderthals in Africa, Australia or the Americas.
Neanderthals nursed their elderly and practised ritual burial indicating an organised society. The earliest evidence (Mungo Man) of settlement in Australia dates to around 40,000 years ago when modern humans likely crossed from Asia by island-hopping. Evidence for symbolic behavior such as body ornamentation and burial is ambiguous for the Middle Paleolithic and still subject to debate. The Bhimbetka rock shelters exhibit the earliest traces of human life in India, some of which are approximately 30,000 years old.
Upper Paleolithic.
From 50,000 to 10,000 years ago in Europe, the Upper Paleolithic ends with the end of the Pleistocene and onset of the Holocene era (the end of the last ice age). Modern humans spread out further across the Earth during the period known as the Upper Paleolithic.
The Upper Paleolithic is marked by a relatively rapid succession of often complex stone artifact technologies and a large increase in the creation of art and personal ornaments. During period between 35 and 10 kya evolved: from 38 to 30 kya Châtelperronian, 40–28 Aurignacian, 28–22 Gravettian, 22–17 Solutrean, and 18–10 Magdalenian. All of these industries except the Châtelperronian are associated with anatomically modern humans. Authorship of the Châtelperronian is still the subject of much debate.
Most scholars date the arrival of humans in Australia at 40,000 to 50,000 years ago, with a possible range of up to 125,000 years ago. The earliest anatomically modern human remains found in Australia (and outside of Africa) are those of Mungo Man; they have been dated at 42,000 years old.
The Americas were colonised via the Bering land bridge which was exposed during this period by lower sea levels. These people are called the Paleo-Indians, and the earliest accepted dates are those of the Clovis culture sites, some 13,500 years ago. Globally, societies were hunter-gatherers but evidence of regional identities begins to appear in the wide variety of stone tool types being developed to suit very different environments.
Epipaleolithic/Mesolithic.
The period starting from the end of the last ice age, 10,000 years ago, to around 6,000 years ago was characterized by rising sea levels and a need to adapt to a changing environment and find new food sources. The development of Mode 5 (microlith) tools began in response to these changes. They were derived from the previous Paleolithic tools, hence the term Epipaleolithic, or were intermediate between the Paleolithic and the Neolithic, hence the term Mesolithic (Middle Stone Age). The choice of a word depends on exact circumstances and the inclination of the archaeologists excavating the site. Microliths were used in the manufacture of more efficient composite tools, resulting in an intensification of hunting and fishing and with increasing social activity the development of more complex settlements, such as Lepenski Vir. Domestication of the dog as a hunting companion probably dates to this period.
The earliest known battle occurred during the Mesolithic period at a site in Egypt known as Cemetery 117.
Neolithic.
The Neolithic, New Stone Age, was approximately characterized by the adoption of agriculture, the shift from food gathering to food producing in itself is one of the most revolutionary changes in human history so-called Neolithic Revolution, the development of pottery, polished stone tools and more complex, larger settlements such as Göbekli Tepe and Çatal Hüyük. Some of these features began in certain localities even earlier, in the transitional Mesolithic. The first Neolithic cultures started around 7000 BCE in the fertile crescent and spread concentrically to other areas of the world; however, the Near East was probably not the only nucleus of agriculture, the cultivation of maize in Meso-America and of rice in the Far East being others.
Due to the increased need to harvest and process plants, ground stone and polished stone artifacts became much more widespread, including tools for grinding, cutting, and chopping. Skara Brae located on Orkney island off Scotland is one of Europe's best examples of a Neolithic village. The community contains stone beds, shelves and even an indoor toilet linked to a stream. The first large-scale constructions were built, including settlement towers and walls, e.g., Jericho and ceremonial sites, e.g.: Stonehenge. The Ġgantija temples of Gozo in the Maltese archipelago are the oldest surviving free standing structures in the world, erected c. 3600–2500 BCE. The earliest evidence for established trade exists in the Neolithic with newly settled people importing exotic goods over distances of many hundreds of miles.
These facts show that there were sufficient resources and co-operation to enable large groups to work on these projects. To what extent this was a basis for the development of elites and social hierarchies is a matter of ongoing debate. Although some late Neolithic societies formed complex stratified chiefdoms similar to Polynesian societies such as the Ancient Hawaiians, based on the societies of modern tribesmen at an equivalent technological level, most Neolithic societies were relatively simple and egalitarian. A comparison of art in the two ages leads some theorists to conclude that Neolithic cultures were noticeably more hierarchical than the Paleolithic cultures that preceded them.
Three-stage chronology.
The Earlier or Early Stone Age (ESA).
This period is not to be identified with "Old Stone Age", a translation of Paleolithic, or with Paleolithic, or with the "Earlier Stone Age" that originally meant what became the Paleolithic and Mesolithic. In the initial decades of its definition by the Pan-African Congress of Prehistory, it was parallel in Africa to the Upper and Middle Paleolithic. However, since then Radiocarbon dating has shown that the Middle Stone Age is in fact contemporaneous with the Middle Paleolithic. The Early Stone Age therefore is contemporaneous with the Lower Paleolithic and happens to include the same main technologies, Oldowan and Acheulean, which produced Mode 1 and Mode 2 stone tools respectively. A distinct regional term is warranted, however, by the location and chronology of the sites and the exact typology.
The Middle Stone Age (MSA).
The Middle Stone Age was a period of African prehistory between Early Stone Age and Late Stone Age. It began around 300,000 years ago and ended around 50,000 years ago. It is considered as an equivalent of European Middle Paleolithic. It is associated with anatomically modern or almost modern "Homo sapiens". Early physical evidence comes from Omo and Herto, both in Ethiopia and dated respectively at c. 195 ka and at c. 160 ka.
The Later Stone Age (LSA).
The Later Stone Age (LSA, sometimes also called the Late Stone Age) refers to a period in African prehistory. Its beginnings are roughly contemporaneous with the European Upper Paleolithic. It lasts until historical times and this includes cultures corresponding to Mesolithic and Neolithic in other regions.
Material culture.
Tools.
Stone tools were made from a variety of stones. For example, flint and chert were shaped (or "chipped") for use as cutting tools and weapons, while basalt and sandstone were used for ground stone tools, such as quern-stones. Wood, bone, shell, antler (deer) and other materials were widely used, as well. During the most recent part of the period, sediments (such as clay) were used to make pottery. Agriculture was developed and certain animals were domesticated as well.
Some species of non-primates are able to use stone tools, such as the sea otter, which breaks abalone shells with them. Primates can both use and manufacture stone tools. This combination of abilities is more marked in apes and men, but only men, or more generally Hominans, depend on tool use for survival. The key anatomical and behavioral features required for tool manufacture, which are possessed only by Hominans, are the larger thumb and the ability to hold by means of an assortment of grips.
Food and drink.
Food sources of the Palaeolithic hunter-gatherers were wild plants and animals harvested from the environment. They liked animal organ meats, including the livers, kidneys and brains. Large seeded legumes were part of the human diet long before the agricultural revolution, as is evident from archaeobotanical finds from the Mousterian layers of Kebara Cave, in Israel. Moreover, recent evidence indicates that humans processed and consumed wild cereal grains as far back as 23,000 years ago in the Upper Paleolithic.
Near the end of the Wisconsin glaciation, 15,000 to 9,000 years ago, mass extinction of Megafauna such as the Wooly mammoth occurred in Asia, Europe, North America and Australia. This was the first Holocene extinction event. It possibly forced modification in the dietary habits of the humans of that age and with the emergence of agricultural practices, plant-based foods also became a regular part of the diet. A number of factors have been suggested for the extinction: certainly over-hunting, but also deforestation and climate change. The net effect was to fragment the vast ranges required by the large animals and extinguish them piecemeal in each fragment.
Shelter and habitat.
Around 2 million years ago, "Homo habilis" is believed to have constructed the first man-made structure in East Africa, consisting of simple arrangements of stones to hold branches of trees in position. A similar stone circular arrangement believed to be around 380,000 years old was discovered at Terra Amata, near Nice, France. (Concerns about the dating have been raised, see Terra Amata). Several human habitats dating back to the Stone Age have been discovered around the globe, including:
Art.
Prehistoric art is visible in the artifacts. Prehistoric music is inferred from found instruments, while parietal art can be found on rocks of any kind. The latter are petroglyphs and rock paintings. The art may or may not have had a religious function.
Petroglyphs.
Petroglyphs appeared in the Neolithic. A Petroglyph is an intaglio abstract or symbolic image engraved on natural stone by various methods, usually by prehistoric peoples. They were a dominant form of pre-writing symbols. Petroglyphs have been discovered in different parts of the world, including Asia (Bhimbetka, India), North America (Death Valley National Park), South America (Cumbe Mayo, Peru), and Europe (Finnmark, Norway).
Rock paintings.
In paleolithic times, mostly animals were painted, in theory ones that were used as food or represented strength, such as the rhinoceros or large cats (as in the Chauvet Cave). Signs such as dots were sometimes drawn. Rare human representations include handprints and half-human/half-animal figures. The Cave of Chauvet in the Ardèche "département", France, contains the most important cave paintings of the paleolithic era, dating from about 31,000 BCE. The Altamira cave paintings in Spain were done 14,000 to 12,000 BCE and show, among others, bisons. The hall of bulls in Lascaux, Dordogne, France, dates from about 15,000 to 10,000 BCE.
The meaning of many of these paintings remains unknown. They may have been used for seasonal rituals. The animals are accompanied by signs that suggest a possible magic use. Arrow-like symbols in Lascaux are sometimes interpreted as calendar or almanac use, but the evidence remains interpretative.
Some scenes of the Mesolithic, however, can be typed and therefore, judging from their various modifications, are fairly clear. One of these is the battle scene between organized bands of archers. For example, "the marching Warriors", a rock painting at Cingle de la Mola, Castellón in Spain, dated to about 7,000–4,000 BCE, depicts about 50 bowmen in two groups marching or running in step toward each other, each man carrying a bow in one hand and a fistful of arrows in the other. A file of five men leads one band, one of whom is a figure with a "high crowned hat". In other scenes elsewhere, the men wear head-dresses and knee ornaments but otherwise fight nude. Some scenes depict the dead and wounded, bristling with arrows. One is reminded of Ötzi the Iceman, a Copper Age mummy revealed by an Alpine melting glacier, who collapsed from loss of blood due to an arrow wound in the back.
Stone Age rituals and beliefs.
Modern studies and the in-depth analysis of finds dating from the Stone Age indicate certain rituals and beliefs of the people in those prehistoric times. It is now believed that activities of the Stone Age humans went beyond the immediate requirements of procuring food, body coverings, and shelters. Specific rites relating to death and burial were practiced, though certainly differing in style and execution between cultures. 
Modern popular culture and the Stone Age.
The image of the caveman is commonly associated with the Stone Age. For example, the 2003 documentary series showing the evolution of humans through the Stone Age was called "Walking with Cavemen", although only the last programme showed humans living in caves. While the idea that human beings and dinosaurs coexisted is sometimes portrayed in popular culture in cartoons, films and computer games, such as "The Flintstones", "One Million Years B.C." and "Chuck Rock", the notion of hominids and non-avian dinosaurs co-existing is not supported by any scientific evidence.
Other depictions of the Stone Age include the best-selling "Earth's Children" series of books by Jean M. Auel, which are set in the Paleolithic and are loosely based on archaeological and anthropological findings. The 1981 film "Quest for Fire" by Jean-Jacques Annaud tells the story of a group of neanderthals searching for their lost fire. A twenty first century series, "Chronicles of Ancient Darkness" by Michelle Paver tells of two New Stone Age children fighting to fulfil a prophecy and save their clan.

</doc>
<doc id="29221" url="https://en.wikipedia.org/wiki?curid=29221" title="Single document interface">
Single document interface

SDI applications allow only one open document frame window at a time. It's made up of one or more independent windows, which appears separately on the windows desktop. An example of this would be a simple text document(Notepad).
MDI applications allow multiple document frame windows to be open in the same instance of an application. An MDI application has a window within which multiple MDI child windows, which are frame windows themselves, can be opened, each containing a separate document. In some applications, the child windows can be of different types, such as chart windows and spreadsheet windows. In that case, the menu bar can change as MDI child windows of different types are activated.
https://msdn.microsoft.com/en-us/library/b2kye6c4.aspx

</doc>
