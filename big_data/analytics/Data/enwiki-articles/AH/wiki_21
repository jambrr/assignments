<doc id="49555" url="https://en.wikipedia.org/wiki?curid=49555" title="Batholith">
Batholith

A batholith (from Greek "bathos", depth + "lithos", rock) is a large emplacement of igneous intrusive (also called plutonic) rock that forms from cooled magma deep in the Earth's crust. Batholiths are almost always made mostly of felsic or intermediate rock-types, such as granite, quartz monzonite, or diorite (see also "granite dome").
Formation.
Although they may appear uniform, batholiths are in fact structures with complex histories and compositions. They are composed of multiple masses, or "plutons", bodies of igneous rock of irregular dimensions (typically at least several kilometers) that can be distinguished from adjacent igneous rock by some combination of criteria including age, composition, texture, or mappable structures. Individual plutons are crystallized from magma that traveled toward the surface from a zone of partial melting near the base of the Earth's crust.
Traditionally, these plutons have been considered to form by ascent of relatively buoyant magma in large masses called "plutonic diapirs". Because the diapirs are liquified and very hot, they tend to rise through the surrounding native "country" rock, pushing it aside and partially melting it. Most diapirs do not reach the surface to form volcanoes, but instead slow down, cool, and usually solidify 5 to 30 kilometers underground as plutons (hence the use of the word "pluton"; in reference to the Roman god of the underworld Pluto). An alternate view is that plutons commonly are formed not by diapiric ascent of large magma diapirs, but rather by aggregation of smaller volumes of magma that ascended as dikes.
A batholith is formed when many plutons converge to form a huge expanse of granitic rock. Some batholiths are mammoth, paralleling past and present subduction zones and other heat sources for hundreds of kilometers in continental crust. One such batholith is the Sierra Nevada Batholith, which is a continuous granitic formation that makes up much of the Sierra Nevada in California. An even larger batholith, the Coast Plutonic Complex is found predominantly in the Coast Mountains of western Canada, and extends for 1,800 kilometers and reaches into southeastern Alaska.
Surface expression and erosion.
A batholith is an exposed area of (mostly) continuous plutonic rock that covers an area larger than 100 square kilometers (40 square miles). Areas smaller than 100 square kilometers are called "stocks". However, the majority of batholiths visible at the surface (via outcroppings) have areas far greater than 100 square kilometers. These areas are exposed to the surface through the process of erosion accelerated by continental uplift acting over many tens of millions to hundreds of millions of years. This process has removed several tens of square kilometers of overlying rock in many areas, exposing the once deeply buried batholiths.
Batholiths exposed at the surface are subjected to huge pressure differences between their former location deep in the earth and their new location at or near the surface. As a result, their crystal structure expands slightly over time. This manifests itself by a form of mass wasting called exfoliation. This form of weathering causes convex and relatively thin sheets of rock to slough off the exposed surfaces of batholiths (a process accelerated by frost wedging). The result is fairly clean and rounded rock faces. A well-known result of this process is Half Dome, located in Yosemite Valley.

</doc>
<doc id="49557" url="https://en.wikipedia.org/wiki?curid=49557" title="Castle">
Castle

A castle (from ) is a type of fortified structure built in Europe and the Middle East during the Middle Ages by nobility. Scholars debate the scope of the word "castle", but usually consider it to be the private fortified residence of a lord or noble. This is distinct from a palace, which is not fortified; from a fortress, which was not always a residence for nobility; and from a fortified settlement, which was a public defence – though there are many similarities among these types of construction. Usage of the term has varied over time and has been applied to structures as diverse as hill forts and country houses. Over the approximately 900 years that castles were built, they took on a great many forms with many different features, although some, such as curtain walls and arrowslits, were commonplace.
A European innovation, castles originated in the 9th and 10th centuries, after the fall of the Carolingian Empire resulted in its territory being divided among individual lords and princes. These nobles built castles to control the area immediately surrounding them, and were both offensive and defensive structures; they provided a base from which raids could be launched as well as protection from enemies. Although their military origins are often emphasised in castle studies, the structures also served as centres of administration and symbols of power. Urban castles were used to control the local populace and important travel routes, and rural castles were often situated near features that were integral to life in the community, such as mills and fertile land.
Many castles were originally built from earth and timber, but had their defences replaced later by stone. Early castles often exploited natural defences, and lacked features such as towers and arrowslits and relied on a central keep. In the late 12th and early 13th centuries, a scientific approach to castle defence emerged. This led to the proliferation of towers, with an emphasis on flanking fire. Many new castles were polygonal or relied on concentric defence – several stages of defence within each other that could all function at the same time to maximise the castle's firepower. These changes in defence have been attributed to a mixture of castle technology from the Crusades, such as concentric fortification, and inspiration from earlier defences such as Roman forts. Not all the elements of castle architecture were military in nature, and devices such as moats evolved from their original purpose of defence into symbols of power. Some grand castles had long winding approaches intended to impress and dominate their landscape.
Although gunpowder was introduced to Europe in the 14th century, it did not significantly affect castle building until the 15th century, when artillery became powerful enough to break through stone walls. While castles continued to be built well into the 16th century, new techniques to deal with improved cannon fire made them uncomfortable and undesirable places to live. As a result, true castles went into decline and were replaced by artillery forts with no role in civil administration, and country houses that were indefensible. From the 18th century onwards, there was a renewed interest in castles with the construction of mock castles, part of a romantic revival of Gothic architecture, but they had no military purpose.
Definition.
Etymology.
The word "castle" is derived from the Latin word "castellum" which is a diminutive of the word "castrum", meaning "fortified place". The Old English "castel", Old French "castel" or "chastel", French "château", Spanish "castillo", Italian "castello", and a number of words in other languages also derive from "castellum". The word "castle" was introduced into English shortly before the Norman Conquest to denote this type of building, which was then new to England.
Defining characteristics.
In its simplest terms, the definition of a castle accepted amongst academics is "a private fortified residence". This contrasts with earlier fortifications, such as Anglo Saxon burhs and walled cities such as Constantinople and Antioch in the Middle East; castles were not communal defences but were built and owned by the local feudal lords, either for themselves or for their monarch. Feudalism was the link between a lord and his vassal where, in return for military service and the expectation of loyalty, the lord would grant the vassal land. In the late 20th century, there was a trend to refine the definition of a castle by including the criterion of feudal ownership, thus tying castles to the medieval period; however, this does not necessarily reflect the terminology used in the medieval period. During the First Crusade (1096–1099), the Frankish armies encountered walled settlements and forts that they indiscriminately referred to as castles, but which would not be considered as such under the modern definition.
Castles served a range of purposes, the most important of which were military, administrative, and domestic. As well as defensive structures, castles were also offensive tools which could be used as a base of operations in enemy territory. Castles were established by Norman invaders of England for both defensive purposes and to pacify the country's inhabitants. As William the Conqueror advanced through England, he fortified key positions to secure the land he had taken. Between 1066 and 1087, he established 36 castles such as Warwick Castle, which he used to guard against rebellion in the English Midlands.
Towards the end of the Middle Ages, castles tended to lose their military significance due to the advent of powerful cannons and permanent artillery fortifications; as a result, castles became more important as residences and statements of power. A castle could act as a stronghold and prison but was also a place where a knight or lord could entertain his peers. Over time the aesthetics of the design became more important, as the castle's appearance and size began to reflect the prestige and power of its occupant. Comfortable homes were often fashioned within their fortified walls. Although castles still provided protection from low levels of violence in later periods, eventually they were succeeded by country houses as high status residences.
Terminology.
"Castle" is sometimes used as a catch-all term for all kinds of fortifications, and as a result has been misapplied in the technical sense. An example of this is Maiden Castle which, despite the name, is an Iron Age hill fort, which had a very different origin and purpose.
Although "castle" has not become a generic term for a manor house (like château in French and Schloss in German) many manor houses contain "castle" in their name while having few if any of the architectural characteristics, usually as their owners liked to maintain a link to the past and felt the term "castle" was a masculine expression of their power. In scholarship the castle, as defined above, is generally accepted as a coherent concept, originating in Europe and later spreading to parts of the Middle East, where they were introduced by European Crusaders. This coherent group shared a common origin, dealt with a particular mode of warfare, and exchanged influences.
In different areas of the world, analogous structures shared features of fortification and other defining characteristics associated with the concept of a castle, though they originated in different periods and circumstances and experienced differing evolutions and influences. For example, "shiro" in Japan, described as castles by historian Stephen Turnbull, underwent "a completely different developmental history, were built in a completely different way and were designed to withstand attacks of a completely different nature". While European castles built from the late 12th and early 13th century onwards were generally stone, "shiro" were predominantly timber buildings into the 16th century.
By the time Japanese and European cultures met in the late 16th century, fortification in Europe had moved beyond castles and relied on innovations such as the Italian "trace italienne" and star forts. Forts in India present a similar case; when they were encountered by the British in the 17th century castles in Europe had generally fallen out of use militarily. Like "shiro", the Indian forts, "durga" or "durg" in Sanskrit, shared features with castles in Europe such as acting as a domicile for a lord as well as being fortifications. They too developed differently from the structures known as castles that had their origins in Europe.
Common features.
Motte.
A motte was an earthen mound with a flat top. It was often artificial, although sometimes it incorporated a pre-existing feature of the landscape. The excavation of earth to make the mound left a ditch around the motte, called a moat (which could be either wet or dry). "Motte" and "moat" derive from the same Old French word, indicating that the features were originally associated and depended on each other for their construction. Although the motte is commonly associated with the bailey to form a motte-and-bailey castle, this was not always the case and there are instances where a motte existed on its own.
"Motte" refers to the mound alone, but it was often surmounted by a fortified structure, such as a keep, and the flat top would be surrounded by a palisade. It was common for the motte to be reached over a flying bridge (a bridge over the ditch from the counterscarp of the ditch to the edge of the top of the mound), as shown in the Bayeux Tapestry's depiction of Château de Dinan. Sometimes a motte covered an older castle or hall, whose rooms became underground storage areas and prisons beneath a new keep.
Bailey and enceinte.
A bailey, also called a ward, was a fortified enclosure. It was a common feature of castles, and most had at least one. The keep on top of the motte was the domicile of the lord in charge of the castle and a bastion of last defence, while the bailey was the home of the rest of the lord's household and gave them protection. The barracks for the garrison, stables, workshops, and storage facilities were often found in the bailey. Water was supplied by a well or cistern. Over time the focus of high status accommodation shifted from the keep to the bailey; this resulted in the creation of another bailey that separated the high status buildings – such as the lord's chambers and the chapel – from the everyday structures such as the workshops and barracks.
From the late 12th century there was a trend for knights to move out of the small houses they had previously occupied within the bailey to live in fortified houses in the countryside. Although often associated with the motte-and-bailey type of castle, baileys could also be found as independent defensive structures. These simple fortifications were called ringworks. The enceinte was the castle's main defensive enclosure, and the terms "bailey" and "enceinte" are linked. A castle could have several baileys but only one enceinte. Castles with no keep, which relied on their outer defences for protection, are sometimes called enceinte castles; these were the earliest form of castles, before the keep was introduced in the 10th century.
Keep.
A keep was a great tower and usually the most strongly defended point of a castle before the introduction of concentric defence. "Keep" was not a term used in the medieval period – the term was applied from the 16th century onwards – instead "donjon" was used to refer to great towers, or "turris" in Latin. In motte-and-bailey castles, the keep was on top of the motte. "Dungeon" is a corrupted form of "donjon" and means a dark, unwelcoming prison. Although often the strongest part of a castle and a last place of refuge if the outer defences fell, the keep was not left empty in case of attack but was used as a residence by the lord who owned the castle, or his guests or representatives.
At first this was usual only in England, when after the Norman Conquest of 1066 the "conquerors lived for a long time in a constant state of alert"; elsewhere the lord's wife presided over a separate residence ("domus", "aula" or "mansio" in Latin) close to the keep, and the donjon was a barracks and headquarters. Gradually, the two functions merged into the same building, and the highest residential storeys had large windows; as a result for many structures, it is difficult to find an appropriate term. The massive internal spaces seen in many surviving donjons can be misleading; they would have been divided into several rooms by light partitions, as in a modern office building. Even in some large castles the great hall was separated only by a partition from the lord's "chamber", his bedroom and to some extent his office.
Curtain wall.
Curtain walls were defensive walls enclosing a bailey. They had to be high enough to make scaling the walls with ladders difficult and thick enough to withstand bombardment from siege engines which, from the 15th century onwards, included gunpowder artillery. A typical wall could be thick and tall, although sizes varied greatly between castles. To protect them from undermining, curtain walls were sometimes given a stone skirt around their bases. Walkways along the tops of the curtain walls allowed defenders to rain missiles on enemies below, and battlements gave them further protection. Curtain walls were studded with towers to allow enfilading fire along the wall. Arrowslits in the walls did not become common in Europe until the 13th century, for fear that they might compromise the wall's strength.
Gatehouse.
The entrance was often the weakest part in a circuit of defences. To overcome this, the gatehouse was developed, allowing those inside the castle to control the flow of traffic. In earth and timber castles, the gateway was usually the first feature to be rebuilt in stone. The front of the gateway was a blind spot and to overcome this, projecting towers were added on each side of the gate in a style similar to that developed by the Romans. The gatehouse contained a series of defences to make a direct assault more difficult than battering down a simple gate. Typically, there were one or more portcullises – a wooden grille reinforced with metal to block a passage – and arrowslits to allow defenders to harry the enemy. The passage through the gatehouse was lengthened to increase the amount of time an assailant had to spend under fire in a confined space and unable to retaliate.
It is a popular myth that so-called murder-holes – openings in the ceiling of the gateway passage – were used to pour boiling oil or molten lead on attackers; the price of oil and lead and the distance of the gatehouse from fires meant that this was impractical. They were most likely used to drop objects on attackers, or to allow water to be poured on fires to extinguish them. Provision was made in the upper storey of the gatehouse for accommodation so the gate was never left undefended, although this arrangement later evolved to become more comfortable at the expense of defence.
During the 13th and 14th centuries the barbican was developed. This consisted of a rampart, ditch, and possibly a tower, in front of the gatehouse which could be used to further protect the entrance. The purpose of a barbican was not just to provide another line of defence but also to dictate the only approach to the gate.
Moat.
A moat was a defensive ditch with steep sides, and could be either dry or filled with water. Its purpose was twofold; to stop devices such as siege towers from reaching the curtain wall and to prevent the walls from being undermined. Water moats were found in low-lying areas and were usually crossed by a drawbridge, although these were often replaced by stone bridges. Fortified islands could be added to the moat, adding another layer of defence. Water defences, such as moats or natural lakes, had the benefit of dictating the enemy's approach to the castle. The site of the 13th-century Caerphilly Castle in Wales covers over and the water defences, created by flooding the valley to the south of the castle, are some of the largest in Western Europe.
Other features.
Battlements were most often found surmounting curtain walls and the tops of gatehouses, and comprised several elements: crenellations, hoardings, machicolations, and loopholes. Crenellation is the collective name for alternating crenels and merlons: gaps and solid blocks on top of a wall. Hoardings were wooden constructs that projected beyond the wall, allowing defenders to shoot at, or drop objects on, attackers at the base of the wall without having to lean perilously over the crenellations, thereby exposing themselves to retaliatory fire. Machicolations were stone projections on top of a wall with openings that allowed objects to be dropped on an enemy at the base of the wall in a similar fashion to hoardings.
Arrowslits, also commonly called loopholes, were narrow vertical openings in defensive walls which allowed arrows or crossbow bolts to be fired on attackers. The narrow slits were intended to protect the defender by providing a very small target, but the size of the opening could also impede the defender if it was too small. A smaller horizontal opening could be added to give an archer a better view for aiming. Sometimes a sally port was included; this could allow the garrison to leave the castle and engage besieging forces. It was usual for the latrines to empty down the external walls of a castle and into the surrounding ditch.
History.
Antecedents.
According to historian Charles Coulson the accumulation of wealth and resources, such as food, led to the need for defensive structures. The earliest fortifications originated in the Fertile Crescent, the Indus Valley, Egypt, and China where settlements were protected by large walls. Northern Europe was slower than the East to develop defensive structures and it was not until the Bronze Age that hill forts developed and began to spread across Europe. In the medieval period castles were influenced by earlier forms of elite architecture, contributing to regional variations. Importantly, while castles had military aspects, they contained a recognisable household structure within their walls, reflecting the multi-functional use of these buildings.
Origins (9th and 10th centuries).
The subject of the emergence of castles is a complex matter which has led to considerable debate. Discussions have typically attributed the rise of the castle to a reaction to attacks by Magyars, Muslims, and Vikings and a need for private defence. The breakdown of the Carolingian Empire led to the privatisation of government, and local lords assumed responsibility for the economy and justice. However, while castles proliferated in the 9th and 10th centuries the link between periods of insecurity and building fortifications is not always straightforward. Some high concentrations of castles occur in secure places, while some border regions had relatively few castles.
It is likely that the castle evolved from the practice of fortifying a lordly home. The greatest threat to a lord's home or hall was fire as it was usually a wooden structure. To protect against this, and keep other threats at bay, there were several courses of action available: create encircling earthworks to keep an enemy at a distance; build the hall in stone; or raise it up on an artificial mound, known as a motte, to present an obstacle to attackers. While the concept of ditches, ramparts, and stone walls as defensive measures is ancient, raising a motte is a medieval innovation.
A bank and ditch enclosure was a simple form of defence, and when found without an associated motte is called a ringwork; when the site was in use for a prolonged period, it was sometimes replaced by a more complex structure or enhanced by the addition of a stone curtain wall. Building the hall in stone did not necessarily make it immune to fire as it still had windows and a wooden door. This led to the elevation of windows to the first floor – to make it harder to throw objects in – and to change the entrance from ground floor to first floor. These features are seen in many surviving castle keeps, which were the more sophisticated version of halls. Castles were not just defensive sites but also enhanced a lord's control over his lands. They allowed the garrison to control the surrounding area, and formed a centre of administration, providing the lord with a place to hold court.
Building a castle sometimes required the permission of the king or other high authority. In 864 the King of West Francia, Charles the Bald, prohibited the construction of "castella" without his permission and ordered them all to be destroyed. This is perhaps the earliest reference to castles, though military historian R. Allen Brown points out that the word "castella" may have applied to any fortification at the time.
In some countries the monarch had little control over lords, or required the construction of new castles to aid in securing the land so was unconcerned about granting permission – as was the case in England in the aftermath of the Norman Conquest and the Holy Land during the Crusades. Switzerland is an extreme case of there being no state control over who built castles, and as a result there were 4,000 in the country. There are very few castles dated with certainty from the mid-9th century. Converted into a donjon around 950, Château de Doué-la-Fontaine in France is the oldest standing castle in Europe.
11th century.
From 1000 onwards, references to castles in texts such as charters increased greatly. Historians have interpreted this as evidence of a sudden increase in the number of castles in Europe around this time; this has been supported by archaeological investigation which has dated the construction of castle sites through the examination of ceramics. The increase in Italy began in the 950s, with numbers of castles increasing by a factor of three to five every 50 years, whereas in other parts of Europe such as France and Spain the growth was slower. In 950 Provence was home to 12 castles, by 1000 this figure had risen to 30, and by 1030 it was over 100. Although the increase was slower in Spain, the 1020s saw a particular growth in the number of castles in the region, particularly in contested border areas between Christian and Muslim.
Despite the common period in which castles rose to prominence in Europe, their form and design varied from region to region. In the early 11th century, the motte and keep – an artificial mound surmounted by a palisade and tower – was the most common form of castle in Europe, everywhere except Scandinavia. While Britain, France, and Italy shared a tradition of timber construction that was continued in castle architecture, Spain more commonly used stone or mud-brick as the main building material.
The Muslim invasion of the Iberian Peninsula in the 8th century introduced a style of building developed in North Africa reliant on "tapial", pebbles in cement, where timber was in short supply. Although stone construction would later become common elsewhere, from the 11th century onwards it was the primary building material for Christian castles in Spain, while at the same time timber was still the dominant building material in north-west Europe.
Historians have interpreted the widespread presence of castles across Europe in the 11th and 12th centuries as evidence that warfare was common, and usually between local lords. Castles were introduced into England shortly before the Norman Conquest in 1066. Before the 12th century, castles were as uncommon in Denmark as they had been in England before the Norman Conquest. The introduction of castles to Denmark was a reaction to attacks from Wendish pirates, and they were usually intended as coastal defences. The motte and bailey remained the dominant form of castle in England, Wales, and Ireland well into the 12th century. At the same time, castle architecture in mainland Europe became more sophisticated.
The donjon was at the centre of this change in castle architecture in the 12th century. Central towers proliferated, and typically had a square plan, with walls thick. Their decoration emulated Romanesque architecture, and sometimes incorporated double windows similar to those found in church bell towers. Donjons, which were the residence of the lord of the castle, evolved to become more spacious. The design emphasis of donjons changed to reflect a shift from functional to decorative requirements, imposing a symbol of lordly power upon the landscape. This sometimes led to compromising defence for the sake of display.
Innovation and scientific design (12th century).
Until the 12th century, stone-built and earth and timber castles were contemporary, but by the late 12th century the number of castles being built went into decline. This has been partly attributed to the higher cost of stone-built fortifications, and the obsolescence of timber and earthwork sites, which meant it was preferable to build in more durable stone. Although superseded by their stone successors, timber and earthwork castles were by no means useless. This is evidenced by the continual maintenance of timber castles over long periods, sometimes several centuries; Owain Glyndŵr's 11th-century timber castle at Sycharth was still in use by the start of the 15th century, its structure having been maintained for four centuries.
At the same time there was a change in castle architecture. Until the late 12th century castles generally had few towers; a gateway with few defensive features such as arrowslits or a portcullis; a great keep or donjon, usually square and without arrowslits; and the shape would have been dictated by the lay of the land (the result was often irregular or curvilinear structures). The design of castles was not uniform, but these were features that could be found in a typical castle in the mid-12th century. By the end of the 12th century or the early 13th century, a newly constructed castle could be expected to be polygonal in shape, with towers at the corners to provide enfilading fire for the walls. The towers would have protruded from the walls and featured arrowslits on each level to allow archers to target anyone nearing or at the curtain wall.
These later castles did not always have a keep, but this may have been because the more complex design of the castle as a whole drove up costs and the keep was sacrificed to save money. The larger towers provided space for habitation to make up for the loss of the donjon. Where keeps did exist, they were no longer square but polygonal or cylindrical. Gateways were more strongly defended, with the entrance to the castle usually between two half-round towers which were connected by a passage above the gateway – although there was great variety in the styles of gateway and entrances – and one or more portcullis.
A peculiar feature of Muslim castles in the Iberian Peninsula was the use of detached towers, called Albarrana towers, around the perimeter as can be seen at the Alcazaba of Badajoz. Probably developed in the 12th century, the towers provided flanking fire. They were connected to the castle by removable wooden bridges, so if the towers were captured the rest of the castle was not accessible.
When seeking to explain this change in the complexity and style of castles, antiquarians found their answer in the Crusades. It seemed that the Crusaders had learned much about fortification from their conflicts with the Saracens and exposure to Byzantine architecture. There were legends such as that of Lalys – an architect from Palestine who reputedly went to Wales after the Crusades and greatly enhanced the castles in the south of the country – and it was assumed that great architects such as James of Saint George originated in the East. In the mid-20th century this view was cast into doubt. Legends were discredited, and in the case of James of Saint George it was proven that he came from Saint-Georges-d'Espéranche, in France. If the innovations in fortification had derived from the East, it would have been expected for their influence to be seen from 1100 onwards, immediately after the Christians were victorious in the First Crusade (1096–1099), rather than nearly 100 years later. Remains of Roman structures in Western Europe were still standing in many places, some of which had flanking round-towers and entrances between two flanking towers.
The castle builders of Western Europe were aware of and influenced by Roman design; late Roman coastal forts on the English "Saxon Shore" were reused and in Spain the wall around the city of Ávila imitated Roman architecture when it was built in 1091. Historian Smail in "Crusading warfare" argued that the case for the influence of Eastern fortification on the West has been overstated, and that Crusaders of the 12th century in fact learned very little about scientific design from Byzantine and Saracen defences. A well-sited castle that made use of natural defences and had strong ditches and walls had no need for a scientific design. An example of this approach is Kerak. Although there were no scientific elements to its design, it was almost impregnable, and in 1187 Saladin chose to lay siege to the castle and starve out its garrison rather than risk an assault.
After the First Crusade, Crusaders who did not return to their homes in Europe helped found the Crusader states of the Principality of Antioch, the County of Edessa, the Kingdom of Jerusalem, and the County of Tripoli. The castles they founded to secure their acquisitions were designed mostly by Syrian master-masons. Their design was very similar to that of a Roman fort or Byzantine "tetrapyrgia" which were square in plan and had square towers at each corner that did not project much beyond the curtain wall. The keep of these Crusader castles would have had a square plan and generally be undecorated.
While castles were used to hold a site and control movement of armies, in the Holy Land some key strategic positions were left unfortified. Castle architecture in the East became more complex around the late 12th and early 13th centuries after the stalemate of the Third Crusade (1189–1192). Both Christians and Muslims created fortifications, and the character of each was different. Saphadin, the 13th-century ruler of the Saracens, created structures with large rectangular towers that influenced Muslim architecture and were copied again and again, however they had little influence on Crusader castles.
13th to 15th centuries.
In the early 13th century, Crusader castles were mostly built by Military Orders including the Knights Hospitaller, Knights Templar, and Teutonic Knights. The orders were responsible for the foundation of sites such as Krak des Chevaliers, Margat, and Belvoir. Design varied not just between orders, but between individual castles, though it was common for those founded in this period to have concentric defences.
The concept, which originated in castles such as Krak des Chevaliers, was to remove the reliance on a central strongpoint and to emphasise the defence of the curtain walls. There would be multiple rings of defensive walls, one inside the other, with the inner ring rising above the outer so that its field of fire was not completely obscured. If assailants made it past the first line of defence they would be caught in the killing ground between the inner and outer walls and have to assault the second wall.
Concentric castles were widely copied across Europe, for instance when Edward I of England – who had himself been on Crusade – built castles in Wales in the late 13th century, four of the eight he founded had a concentric design. Not all the features of the Crusader castles from the 13th century were emulated in Europe. For instance, it was common in Crusader castles to have the main gate in the side of a tower and for there to be two turns in the passageway, lengthening the time it took for someone to reach the outer enclosure. It is rare for this bent entrance to be found in Europe.
One of the effects of the Livonian Crusade in the Baltic was the introduction of stone and brick fortifications. Although there were hundreds of wooden castles in Prussia and Livonia, the use of bricks and mortar was unknown in the region before the Crusaders. Until the 13th century and start of the 14th centuries, their design was heterogeneous, however this period saw the emergence of a standard plan in the region: a square plan, with four wings around a central courtyard. It was common for castles in the East to have arrowslits in the curtain wall at multiple levels; contemporary builders in Europe were wary of this as they believed it weakened the wall. Arrowslits did not compromise the wall's strength, but it was not until Edward I's programme of castle building that they were widely adopted in Europe.
The Crusades also led to the introduction of machicolations into Western architecture. Until the 13th century, the tops of towers had been surrounded by wooden galleries, allowing defenders to drop objects on assailants below. Although machicolations performed the same purpose as the wooden galleries, they were probably an Eastern invention rather than an evolution of the wooden form. Machicolations were used in the East long before the arrival of the Crusaders, and perhaps as early as the first half of the 8th century in Syria.
The greatest period of castle building in Spain was in the 11th to 13th centuries, and they were most commonly found in the disputed borders between Christian and Muslim lands. Conflict and interaction between the two groups led to an exchange of architectural ideas, and Spanish Christians adopted the use of detached towers. The Spanish Reconquista, driving the Muslims out of the Iberian Peninsula, was complete in 1492.
Although France has been described as "the heartland of medieval architecture", the English were at the forefront of castle architecture in the 12th century. French historian François Gebelin wrote: "The great revival in military architecture was led, as one would naturally expect, by the powerful kings and princes of the time; by the sons of William the Conqueror and their descendants, the Plantagenets, when they became dukes of Normandy. These were the men who built all the most typical twelfth-century fortified castles remaining to-day". Despite this, by the beginning of the 15th century, the rate of castle construction in England and Wales went into decline. The new castles were generally of a lighter build than earlier structures and presented few innovations, although strong sites were still created such as that of Raglan in Wales. At the same time, French castle architecture came to the fore and led the way in the field of medieval fortifications. Across Europe – particularly the Baltic, Germany, and Scotland – castles were built well into the 16th century.
Advent of gunpowder.
Artillery powered by gunpowder was introduced to Europe in the 1320s and spread quickly. Handguns, which were initially unpredictable and inaccurate weapons, were not recorded until the 1380s. Castles were adapted to allow small artillery pieces – averaging between  – to fire from towers. These guns were too heavy for a man to carry and fire, but if he supported the butt end and rested the muzzle on the edge of the gun port he could fire the weapon. The gun ports developed in this period show a unique feature, that of a horizontal timber across the opening. A hook on the end of the gun could be latched over the timber so the gunner did not have to take the full recoil of the weapon. This adaptation is found across Europe, and although the timber rarely survives, there is an intact example at Castle Doornenburg in the Netherlands. Gunports were keyhole shaped, with a circular hole at the bottom for the weapon and a narrow slit on top to allow the gunner to aim.
This form is very common in castles adapted for guns, found in Egypt, Italy, Scotland, and Spain, and elsewhere in between. Other types of port, though less common, were horizontal slits – allowing only lateral movement – and large square openings, which allowed greater movement. The use of guns for defence gave rise to artillery castles, such as that of Château de Ham in France. Defences against guns were not developed until a later stage. Ham is an example of the trend for new castles to dispense with earlier features such as machicolations, tall towers, and crenellations.
Bigger guns were developed, and in the 15th century became an alternative to siege engines such as the trebuchet. The benefits of large guns over trebuchets – the most effective siege engine of the Middle Ages before the advent of gunpowder – were those of a greater range and power. In an effort to make them more effective, guns were made ever bigger, although this hampered their ability to reach remote castles. By the 1450s guns were the preferred siege weapon, and their effectiveness was demonstrated by Mehmed II at the Fall of Constantinople.
The response towards more effective cannons was to build thicker walls and to prefer round towers, as the curving sides were more likely to deflect a shot than a flat surface. While this sufficed for new castles, pre-existing structures had to find a way to cope with being battered by cannon. An earthen bank could be piled behind a castle's curtain wall to absorb some of the shock of impact.
Often, castles constructed before the age of gunpowder were incapable of using guns as their wall-walks were too narrow. A solution to this was to pull down the top of a tower and to fill the lower part with the rubble to provide a surface for the guns to fire from. Lowering the defences in this way had the effect of making them easier to scale with ladders. A more popular alternative defence, which avoided damaging the castle, was to establish bulwarks beyond the castle's defences. These could be built from earth or stone and were used to mount weapons.
Bastions and star forts (16th century).
Around 1500, the innovation of the angled bastion was developed in Italy. With developments such as these, Italy pioneered permanent artillery fortifications, which took over from the defensive role of castles. From this evolved star forts, also known as "trace italienne". The elite responsible for castle construction had to choose between the new type that could withstand cannon fire and the earlier, more elaborate style. The first was ugly and uncomfortable and the latter was less secure, although it did offer greater aesthetic appeal and value as a status symbol. The second choice proved to be more popular as it became apparent that there was little point in trying to make the site genuinely defensible in the face of cannon. For a variety of reasons, not least of which is that many castles have no recorded history, there is no firm number of castles built in the medieval period. However, it has been estimated that between 75,000 and 100,000 were built in western Europe; of these around 1,700 were in England and Wales and around 14,000 in German-speaking areas.
Some true castles were built in the Americas by the Spanish and French colonies. The first stage of Spanish fort construction has been termed the "castle period", which lasted from 1492 until the end of the 16th century. Starting with Fortaleza Ozama, "these castles were essentially European medieval castles transposed to America". Among other defensive structures (including forts and citadels), castles were also built in New France towards the end of the 17th century. In Montreal the artillery was not as developed as on the battle-fields of Europe, some of the region's outlying forts were built like the fortified manor houses of France. Fort Longueuil, built from 1695–1698 by a baronial family, has been described as "the most medieval-looking fort built in Canada". The manor house and stables were within a fortified bailey, with a tall round turret in each corner. The "most substantial castle-like fort" near Montréal was Fort Senneville, built in 1692 with square towers connected by thick stone walls, as well as a fortified windmill. Stone forts such as these served as defensive residences, as well as imposing structures to prevent Iroquois incursions.
Although castle construction faded towards the end of the 16th century, castles did not necessarily all fall out of use. Some retained a role in local administration and became law courts, while others are still handed down in aristocratic families as hereditary seats. A particularly famous example of this is Windsor Castle in England which was founded in the 11th century and is home to the monarch of the United Kingdom. In other cases they still had a role in defence. Tower houses, which are closely related to castles and include pele towers, were defended towers that were permanent residences built in the 14th to 17th centuries. Especially common in Ireland and Scotland, they could be up to five storeys high and succeeded common enclosure castles and were built by a greater social range of people. While unlikely to provide as much protection as a more complex castle, they offered security against raiders and other small threats.
Later use and revival castles.
According to archaeologists Oliver Creighton and Robert Higham, "the great country houses of the seventeenth to twentieth centuries were, in a social sense, the castles of their day". Though there was a trend for the elite to move from castles into country houses in the 17th century, castles were not completely useless. In later conflicts, such as the English Civil War (1641–1651), many castles were refortified, although subsequently slighted to prevent them from being used again.
Revival or mock castles became popular as a manifestation of a Romantic interest in the Middle Ages and chivalry, and as part of the broader Gothic Revival in architecture. Examples of these castles include Chapultepec in Mexico, Neuschwanstein in Germany, and Edwin Lutyens' Castle Drogo (1911–1930) – the last flicker of this movement in the British Isles. While churches and cathedrals in a Gothic style could faithfully imitate medieval examples, new country houses built in a "castle style" differed internally from their medieval predecessors. This was because to be faithful to medieval design would have left the houses cold and dark by contemporary standards.
Artificial ruins, built to resemble remnants of historic edifices, were also a hallmark of the period. They were usually built as centre pieces in aristocratic planned landscapes. Follies were similar, although they differed from artificial ruins in that they were not part of a planned landscape, but rather seemed to have no reason for being built. Both drew on elements of castle architecture such as castellation and towers, but served no military purpose and were solely for display.
Construction.
Once the site of a castle had been selected – whether a strategic position or one intended to dominate the landscape as a mark of power – the building material had to be selected. An earth and timber castle was cheaper and easier to erect than one built from stone. The costs involved in construction are not well-recorded, and most surviving records relate to royal castles. A castle with earthen ramparts, a motte, and timber defences and buildings could have been constructed by an unskilled workforce. The source of man-power was probably from the local lordship, and the tenants would already have the necessary skills of felling trees, digging, and working timber necessary for an earth and timber castle. Possibly coerced into working for their lord, the construction of an earth and timber castle would not have been a drain on a client's funds. In terms of time, it has been estimated that an average sized motte – high and wide at the summit – would have taken 50 people about 40 working days. An exceptionally expensive motte and bailey was that of Clones in Ireland, built in 1211 for £20. The high cost, relative to other castles of its type, was because labourers had to be imported.
The cost of building a castle varied according to factors such as their complexity and transport costs for material. It is certain that stone castles cost a great deal more than those built from earth and timber. Even a very small tower, such as Peveril Castle, would have cost around £200. In the middle were castles such as Orford, which was built in the late 12th century for £1,400, and at the upper end were those such as Dover, which cost about £7,000 between 1181 and 1191. Spending on the scale of the vast castles such as Château Gaillard (an estimated £15,000 to £20,000 between 1196 and 1198) was easily supported by The Crown, but for lords of smaller areas, castle building was a very serious and costly undertaking. It was usual for a stone castle to take the best part of a decade to finish. The cost of a large castle built over this time (anywhere from £1,000 to £10,000) would take the income from several manors, severely impacting a lord's finances. Costs in the late 13th century were of a similar order, with castles such as Beaumaris and Rhuddlan costing £14,500 and £9,000 respectively. Edward I's campaign of castle-building in Wales cost £80,000 between 1277 and 1304, and £95,000 between 1277 and 1329. Renowned designer Master James of Saint George, responsible for the construction of Beaumaris, explained the cost:
Not only were stone castles expensive to build in the first place, but their maintenance was a constant drain. They contained a lot of timber, which was often unseasoned and as a result needed careful upkeep. For example, it is documented that in the late 12th century repairs at castles such as Exeter and Gloucester cost between £20 and £50 annually.
Medieval machines and inventions, such as the treadwheel crane, became indispensable during construction, and techniques of building wooden scaffolding were improved upon from Antiquity. When building in stone a prominent concern of medieval builders was to have quarries close at hand. There are examples of some castles where stone was quarried on site, such as Chinon, Château de Coucy and Château Gaillard. When it was built in 992 in France the stone tower at Château de Langeais was high, wide, and long with walls averaging . The walls contain of stone and have a total surface (both inside and out) of . The tower is estimated to have taken 83,000 average working days to complete, most of which was unskilled labour.
Many countries had both timber and stone castles, however Denmark had few quarries, and as a result, most of its castles are earth and timber affairs, or later on built from brick. Brick-built structures were not necessarily weaker than their stone-built counterparts. Brick castles are less common in England than stone or earth and timber constructions, and often it was chosen for its aesthetic appeal or because it was fashionable, encouraged by the brick architecture of the Low Countries. For example, when Tattershall Castle was built between 1430 and 1450, there was plenty of stone available nearby, but the owner, Lord Cromwell, chose to use brick. About 700,000 bricks were used to build the castle, which has been described as "the finest piece of medieval brick-work in England". Most Spanish castles were built from stone, whereas castles in Eastern Europe were usually of timber construction.
Social centre.
Due to the lord's presence in a castle, it was a centre of administration from where he controlled his lands. He relied on the support of those below him, as without the support of his more powerful tenants a lord could expect his power to be undermined. Successful lords regularly held court with those immediately below them on the social scale, but absentees could expect to find their influence weakened. Larger lordships could be vast, and it would be impractical for a lord to visit all his properties regularly so deputies were appointed. This especially applied to royalty, who sometimes owned land in different countries.
To allow the lord to concentrate on his duties regarding administration, he had a household of servants to take care of chores such as providing food. The household was run by a chamberlain, while a treasurer took care of the estate's written records. Royal households took essentially the same form as baronial households, although on a much larger scale and the positions were more prestigious. An important role of the household servants was the preparation of food; the castle kitchens would have been a busy place when the castle was occupied, called on to provide large meals. Without the presence of a lord's household, usually because he was staying elsewhere, a castle would have been a quiet place with few residents, focused on maintaining the castle.
As social centres castles were important places for display. Builders took the opportunity to draw on symbolism, through the use of motifs, to evoke a sense of chivalry that was aspired to in the Middle Ages amongst the elite. Later structures of the Romantic Revival would draw on elements of castle architecture such as battlements for the same purpose. Castles have been compared with cathedrals as objects of architectural pride, and some castles incorporated gardens as ornamental features. The right to crenellate, when granted by a monarch – though it was not always necessary – was important not just as it allowed a lord to defend his property but because crenellations and other accoutrements associated with castles were prestigious through their use by the elite. Licences to crenellate were also proof of a relationship with or favour from the monarch, who was the one responsible for granting permission.
Courtly love was the eroticisation of love between the nobility. Emphasis was placed on restraint between lovers. Though sometimes expressed through chivalric events such as tournaments, where knights would fight wearing a token from their lady, it could also be private and conducted in secret. The legend of Tristan and Iseult is one example of stories of courtly love told in the Middle Ages. It was an ideal of love between two people not married to each other, although the man might be married to someone else. It was not uncommon or ignoble for a lord to be adulterous – Henry I of England had over 20 bastards for instance – but for a lady to be promiscuous was seen as dishonourable.
The purpose of marriage between the medieval elites was to secure land. Girls were married in their teens, but boys did not marry until they came of age. There is a popular conception that women played a peripheral role in the medieval castle household, and that it was dominated by the lord himself. This derives from the image of the castle as a martial institution, but most castles in England, France, Ireland, and Scotland were never involved in conflicts or sieges, so the domestic life is a neglected facet. The lady was given a "marriage portion" of her husband's estates – usually about a third – which was hers for life, and her husband would inherit on her death. It was her duty to administer them directly, as the lord administered his own land. Despite generally being excluded from military service, a woman could be in charge of a castle, either on behalf of her husband or if she was widowed. Because of their influence within the medieval household, women influenced construction and design, sometimes through direct patronage; historian Charles Coulson emphasises the role of women in applying "a refined aristocratic taste" to castles due to their long term residence.
Location and landscape.
The positioning of castles was influenced by the available terrain. Whereas hill castles such as Marksburg were common in Germany, where 66 per cent of all known medieval were highland area while 34 per cent were on low-lying land, they formed a minority of sites in England. Because of the range of functions they had to fulfil, castles were built in a variety of locations. Multiple factors were considered when choosing a site, balancing between the need for a defendable position with other considerations such as proximity to resources. For instance many castles are located near Roman roads, which remained important transport routes in the Middle Ages, or could lead to the alteration or creation of new road systems in the area. Where available it was common to exploit pre-existing defences such as building with a Roman fort or the ramparts of an Iron Age hillfort. A prominent site that overlooked the surrounding area and offered some natural defences may also have been chosen because its visibility made it a symbol of power. Urban castles were particularly important in controlling centres of population and production, especially with an invading force, for instance in the aftermath of the Norman Conquest of England in the 11th century the majority of royal castles were built in or near towns.
As castles were not simply military buildings but centres of administration and symbols of power, they had a significant impact on the surrounding landscape. Placed by a frequently-used road or river, the toll castle ensured that a lord would get his due toll money from merchants. Rural castles were often associated with mills and field systems due to their role in managing the lord's estate, which gave them greater influence over resources. Others were adjacent to or in royal forests or deer parks and were important in their upkeep. Fish ponds were a luxury of the lordly elite, and many were found next to castles. Not only were they practical in that they ensured a water supply and fresh fish, but they were a status symbol as they were expensive to build and maintain.
Although sometimes the construction of a castle led to the destruction of a village, such as at Eaton Socon in England, it was more common for the villages nearby to have grown as a result of the presence of a castle. Sometimes planned towns or villages were created around a castle. The benefits of castle building on settlements was not confined to Europe. When the 13th-century Safad Castle was founded in Galilee in the Holy Land, the 260 villages benefitted from the inhabitants' newfound ability to move freely. When built, a castle could result in the restructuring of the local landscape, with roads moved for the convenience of the lord. Settlements could also grow naturally around a castle, rather than being planned, due to the benefits of proximity to an economic centre in a rural landscape and the safety given by the defences. Not all such settlements survived, as once the castle lost its importance – perhaps succeeded by a manor house as the centre of administration – the benefits of living next to a castle vanished and the settlement depopulated.
During and shortly after the Norman Conquest of England, castles were inserted into important pre-existing towns to control and subdue the populace. They were usually located near any existing town defences, such as Roman walls, although this sometimes resulted in the demolition of structures occupying the desired site. In Lincoln, 166 houses were destroyed to clear space for the castle, and in York agricultural land was flooded to create a moat for the castle. As the military importance of urban castles waned from their early origins, they became more important as centres of administration, and their financial and judicial roles. When the Normans invaded Ireland, Scotland, and Wales in the 11th and 12th centuries, settlement in those countries was predominantly non-urban, and the foundation of towns was often linked with the creation of a castle.
The location of castles in relation to high status features, such as fish ponds, was a statement of power and control of resources. Also often found near a castle, sometimes within its defences, was the parish church. This signified a close relationship between feudal lords and the Church, one of the most important institutions of medieval society. Even elements of castle architecture that have usually been interpreted as military could be used for display. The water features of Kenilworth Castle in England – comprising a moat and several satellite ponds – forced anyone approaching a water castle entrance to take a very indirect route, walking around the defences before the final approach towards the gateway. Another example is that of the 14th-century Bodiam Castle, also in England; although it appears to be a state of the art, advanced castle it is in a site of little strategic importance, and the moat was shallow and more likely intended to make the site appear impressive than as a defence against mining. The approach was long and took the viewer around the castle, ensuring they got a good look before entering. Moreover, the gunports were impractical and unlikely to have been effective.
Warfare.
As a static structure, castles could often be avoided. Their immediate area of influence was about and their weapons had a short range even early in the age of artillery. However, leaving an enemy behind would allow them to interfere with communications and make raids. Garrisons were expensive and as a result often small unless the castle was important. Cost also meant that in peace time garrisons were smaller, and small castles were manned by perhaps a couple of watchmen and gate-guards. Even in war, garrisons were not necessarily large as too many people in a defending force would strain supplies and impair the castle's ability to withstand a long siege. In 1403, a force of 37 archers successfully defended Caernarfon Castle against two assaults by Owain Glyndŵr's allies during a long siege, demonstrating that a small force could be effective.
Early on, manning a castle was a feudal duty of vassals to their magnates, and magnates to their kings, however this was later replaced with paid forces. A garrison was usually commanded by a constable whose peace-time role would have been looking after the castle in the owner's absence. Under him would have been knights who by benefit of their military training would have acted as a type of officer class. Below them were archers and bowmen, whose role was to prevent the enemy reaching the walls as can be seen by the positioning of arrowslits.
If it was necessary to seize control of a castle an army could either launch an assault or lay siege. It was more efficient to starve the garrison out than to assault it, particularly for the most heavily defended sites. Without relief from an external source, the defenders would eventually submit. Sieges could last weeks, months, and in rare cases years if the supplies of food and water were plentiful. A long siege could slow down the army, allowing help to come or for the enemy to prepare a larger force for later. Such an approach was not confined to castles, but was also applied to the fortified towns of the day. On occasion, siege castles would be built to defend the besiegers from a sudden sally and would have been abandoned after the siege ended one way or another.
If forced to assault a castle, there were many options available to the attackers. For wooden structures, such as early motte-and-baileys, fire was a real threat and attempts would be made to set them alight as can be seen in the Bayeux Tapestry. Projectile weapons had been used since antiquity and the mangonel and petraria – from Roman and Eastern origins respectively – were the main two that were used into the Middle Ages. The trebuchet, which probably evolved from the petraria in the 13th century, was the most effective siege weapon before the development of cannons. These weapons were vulnerable to fire from the castle as they had a short range and were large machines. Conversely, weapons such as trebuchets could be fired from within the castle due to the high trajectory of its projectile, and would be protected from direct fire by the curtain walls.
Ballistas or springalds were siege engines that worked on the same principles as crossbows. With their origins in Ancient Greece, tension was used to project a bolt or javelin. Missiles fired from these engines had a lower trajectory than trebuchets or mangonels and were more accurate. They were more commonly used against the garrison rather than the buildings of a castle. Eventually cannons developed to the point where they were more powerful and had a greater range than the trebuchet, and became the main weapon in siege warfare.
Walls could be undermined by a sap. A mine leading to the wall would be dug and once the target had been reached, the wooden supports preventing the tunnel from collapsing would be burned. It would cave in and bring down the structure above. Building a castle on a rock outcrop or surrounding it with a wide, deep moat helped prevent this. A counter-mine could be dug towards the besiegers' tunnel; assuming the two converged, this would result in underground hand-to-hand combat. Mining was so effective that during the siege of Margat in 1285 when the garrison were informed a sap was being dug they surrendered. Battering rams were also used, usually in the form of a tree trunk given an iron cap. They were used to force open the castle gates, although they were sometimes used against walls with less effect.
As an alternative to the time-consuming task of creating a breach, an escalade could be attempted to capture the walls with fighting along the walkways behind the battlements. In this instance, attackers would be vulnerable to arrowfire. A safer option for those assaulting a castle was to use a siege tower, sometimes called a belfry. Once ditches around a castle were partially filled in, these wooden, movable towers could be pushed against the curtain wall. As well as offering some protection for those inside, a siege tower could overlook the interior of a castle, giving bowmen an advantageous position from which to unleash missiles.

</doc>
<doc id="49559" url="https://en.wikipedia.org/wiki?curid=49559" title="Yalu River">
Yalu River

The Yalu River, also called the Amnok River (), is a river on the border between North Korea and China. Together with the Tumen River to its east, and a small portion of Paektu Mountain, the Yalu forms the border between North Korea and China and is notable as a site involved in military conflicts in the First Sino-Japanese War, the Russo-Japanese War and the Korean War.
Name.
There are two versions regarding the origin of the river name. One version is that the name derived from "Yalv ula" in the Manchu language. The Manchu word "Yalu" means "the boundary between two countries". In Mandarin Chinese, "Yalu" phonetically approximates the original Manchu word "Yalu", but literally means "Duck Green", which was said to have been once the color of the river. The other version is that the river was named after the combination of its two upper branches, which was called "Ya" and "Lu" respectively.
Geography.
From 2500 m above sea level on Paektu Mountain on the China–North Korea border, the river flows south to Hyesan before sweeping 130 km northwest to Linjiang and then returning to a more southerly route for a further 300 km to empty into the Korea Bay between Dandong (China) and Sinuiju (North Korea). The bordering Chinese provinces are Jilin and Liaoning.
The river is 795 km (493 mi) long and receives the water from over 30,000 km² of land. The Yalu's most significant tributaries are the Changjin (장진강/), the Hochon (허천강/), the Tokro (독로강/) and the Ai (瑷河) rivers. The river is not easily navigable for most of its length.
The depth of the Yalu River varies from some of the more shallow parts on the eastern side in Hyesan (1 metre) to the deeper parts of the river near the Yellow Sea (2.5 metres). The estuary is the site of the Amrok River estuary Important Bird Area, identified as such by BirdLife International.
There are 205 islands on the Yalu. A 1962 border treaty between North Korea and China split the islands according to which ethnic group were living on each island. North Korea possesses 127 and China 78. Due to the division criteria, some islands such as Hwanggumpyong Island belong to North Korea but abut the Chinese side of the river.
History.
The river basin is the site where the ancient kingdom of Goguryeo rose to power. Many former fortresses are located along the river and the former capital of that kingdom was situated at what is now the medium-sized city of Ji'an, Jilin along the Yalu, a site rich in Goguryeo era relics.
Wihwa Island on the river is historically famous as the place where in 1388, General Yi Songgye (later Taejo of Joseon) decided to turn back his army southward to Kaesong in the first of a series of revolts that eventually led to the establishment of the House of Yi.
The river has been the site of several battles because of its strategic location between Korea and China, including:
The Korean side of the river was heavily industrialized during the period of Japanese rule (1910–1945), and by 1945 almost 20% of Imperial Japan's total industrial output originated in Korea. During the Korean War, the movement of United Nations troops approaching the river precipitated massive Chinese intervention from around Dandong. In the course of the conflict every bridge across the river except one was destroyed. The one remaining bridge was the Sino–Korean Friendship Bridge connecting Sinuiju, North Korea to Dandong, China. During the war the valley surrounding the western end of the river also became the focal point of a series of dogfights for air superiority over North Korea, earning the nickname "MiG Alley" in reference to the MiG-15 fighters flown by the combined North Korean, Chinese and Soviet forces.
It was the advance of UN forces during the Korean War toward the Yalu which prompted Chairman Mao Zedong to involve China in the war for fear of an American invasion, since toppling communism was one of America's stated goals and Douglas MacArthur had expressed his desire to expand the war into China.
The river has frequently been crossed by North Koreans fleeing to China since the early 1990s.
Economy.
The river is important for hydroelectric power, and one of the largest hydroelectric dams in Asia is in Sup'ung Dam, 106 m high and over 850 m long, located upstream from Sinuiju, North Korea. The dam has created an artificial lake over a portion of the river, called Sapung Lake. In addition the river is used for transportation, particularly of lumber from its forested banks. The river provides fish for the local population. Downstream of Sup'ung is the Taipingwan Dam. Upstream of Sup'ung is the Yunfeng Dam. Both dams produce hydroelectric power as well.

</doc>
<doc id="49562" url="https://en.wikipedia.org/wiki?curid=49562" title="Nicolae Ceaușescu">
Nicolae Ceaușescu

Nicolae Ceaușescu (; 26 January 1918 – 25 December 1989) was a Romanian Communist politician. He was General Secretary of the Romanian Communist Party from 1965 to 1989, and as such was the country's second and last Communist leader. He was also the country's head of state from 1967 to 1989.
A member of the Romanian Communist youth movement, Ceaușescu rose up through the ranks of Gheorghe Gheorghiu-Dej's Socialist government and, upon the death of Gheorghiu-Dej in 1965, he succeeded to the leadership of Romania’s Communist Party as General Secretary.
After a brief period of relatively moderate rule, Ceaușescu's regime became increasingly brutal and repressive. By some accounts, his rule was the most rigidly Stalinist in the Soviet bloc. He maintained controls over speech and the media that were very strict even by Soviet-bloc standards, and internal dissent was not tolerated. His secret police, the Securitate, was one of the most ubiquitous and brutal secret police forces in the world. In 1982, with the goal of paying off Romania's large foreign debt, Ceaușescu ordered the export of much of the country’s agricultural and industrial production. The resulting extreme shortages of food, fuel, energy, medicines, and other basic necessities drastically lowered living standards and intensified unrest. Ceaușescu's regime was also marked by an extensive and ubiquitous cult of personality, nationalism, a continuing deterioration in foreign relations even with the Soviet Union, and nepotism.
Ceaușescu’s regime collapsed after he ordered his security forces to fire on anti-government demonstrators in the city of Timișoara on 17 December 1989. The demonstrations spread to Bucharest and became known as the Romanian Revolution, which was the only violent removal of a Communist government in the course of the revolutions of 1989. Ceaușescu and his wife, Elena, fled the capital in a helicopter but were captured by the armed forces. On 25 December the couple were hastily tried and convicted by a special military tribunal on charges of genocide and sabotage of the Romanian economy in an approximately one-hour long court session. Ceaușescu and his wife were then shot by a firing squad.
Early life and career.
Ceaușescu was born in the village of Scornicești, Olt County, on 26 January 1918 being one of the ten children of a poor peasant family (see Ceaușescu family). His father, Andruță Ceaușescu, owned of agricultural land, a few sheep, and he also supplemented his large family's income through tailoring. Nicolae studied at the village school until at the age of 11, when he ran away from his abusive, alcoholic father to Bucharest. He initially lived with his sister, Niculina Rusescu, and then became an apprentice shoemaker.
He worked in the workshop of Alexandru Săndulescu, a shoemaker who was an active member in the then-illegal Communist Party. Ceaușescu was soon involved in the Communist Party activities (becoming a member in early 1932), but, as a teenager, he was given only small tasks. He was first arrested in 1933, at the age of 15 for street fighting during a strike and again, in 1934, first for collecting signatures on a petition protesting the trial of railway workers and twice more for other similar activities. By the mid-1930s, he had been in missions in Bucharest, Craiova, Câmpulung, and Râmnicu Vâlcea, being arrested several times.
The profile file from the secret police, Siguranța Statului, named him "a dangerous Communist agitator" and "distributor of Communist and antifascist propaganda materials". For these charges he was convicted on 6 June 1936 by the Brașov Tribunal to 2 years in prison, an additional 6 months for contempt of court, and one year of forced residence in Scornicești. He spent most of his sentence in Doftana Prison. While out of jail in 1940, he met Elena Petrescu, whom he married in 1946 and who would play an increasing role in his political life over the years.
Soon after being freed, he was arrested again and sentenced for "conspiracy against social order", spending the time during the war in prisons and internment camps: Jilava (1940), Caransebeș (1942), Văcărești (1943), and Târgu Jiu (1943). In 1943, he was transferred to Târgu Jiu internment camp where he shared a cell with Gheorghe Gheorghiu-Dej, becoming his protégé. Enticed with substantial bribes, the camp authorities gave the Communist prisoners much freedom in running their cell block, provided they did not attempt to break out of prison. At Târgu Jiu, Gheorghiu-Dej ran "self-criticism sessions" where various Party members had to confess before the other Party members to misunderstanding the dogma of Marx-Engels-Lenin-Stalin as interpreted by Gheorghiu-Dej; journalist Edward Behr claimed Ceaușescu's role in these "self-criticism sessions" was that of the enforcer, the young man allegedly beating those Party members who refused to go with or were insufficiently enthusiastic about the "self-criticism" sessions. These "self-criticism sessions" not only helped to cement Gheorghiu-Dej's control over the Party, but also endeared his protégé Ceaușescu to him. It was Ceaușescu's time at Târgu Jiu that marked the beginning of his rise to power. After World War II, when Romania was beginning to fall under Soviet influence, Ceaușescu served as secretary of the Union of Communist Youth (1944–1945).
After the Communists seized power in Romania in 1947, he headed the ministry of agriculture, then served as deputy minister of the armed forces under Gheorghe Gheorghiu-Dej, becoming a major-general. In 1952, Gheorghiu-Dej brought him onto the Central Committee months after the party's "Muscovite faction" led by Ana Pauker had been purged. In the late 1940s-early 1950s, the Party had been divided into the "home communists" headed by Gheorghiu-Dej who remained inside Romania prior to 1944 and the "Muscovites" who had gone into exile in the Soviet Union. With the partial exception of Poland where the Polish October crisis of 1956 brought to power the previously imprisoned "home communist" Władysław Gomułka, Romania was the only Eastern European nation where the "home communists" triumphed over the "Muscovites". In the rest of the Soviet bloc, there were a series of purges in this period that led to the "home communists" being executed or imprisoned. That Stalin decided in favor of the "home communists" in Romania stemmed largely out of anti-Semitism as Pauker, the leader of the "Muscovites" was Jewish, and thus unacceptable to an increasingly anti-Semitic Stalin. Like his patron Gheorghiu-Dej, Ceaușescu was a "home communist" who benefited from the fall of the "Muscovites" in 1952. In 1954, Ceaușescu became a full member of the Politburo and eventually rose to occupy the second-highest position in the party hierarchy.
Leadership of Romania.
Ceaușescu was not the obvious successor to Gheorghiu-Dej when he died on 19 March 1965, despite his closeness to the longtime leader, but amid widespread infighting among older and more connected officials the Politburo turned to Ceaușescu as a compromise candidate. He was elected general secretary on 22 March 1965, three days after Gheorghiu-Dej's death. One of his first acts was to change the name of the party from the Romanian Workers' Party back to the Communist Party of Romania, and declare the country a socialist republic rather than a people's republic. In 1967, he consolidated his power by becoming president of the State Council, making him de jure head of state.
Initially, Ceaușescu became a popular figure in Romania and also in the West, because of his independent foreign policy, challenging the authority of the Soviet Union. In the 1960s, he eased press censorship and ended Romania's active participation in the Warsaw Pact (though Romania formally remained a member). He not only refused to take part in the 1968 invasion of Czechoslovakia by Warsaw Pact forces, but actively and openly condemned that action in his 21 August 1968 speech. He even traveled to Prague a week before the invasion to offer moral support to his Czechoslovak counterpart, Alexander Dubček. Although the Soviet Union largely tolerated Ceaușescu's recalcitrance, his seeming independence from Moscow earned Romania a maverick status within the Eastern Bloc. Ceaușescu's main aim as leader was to make Romania a world power, and all of his economic, foreign and demographic policies were meant to achieve Ceaușescu's ultimate goal of turning Romania into one of the world's great powers. For the "Conducător" (the "Leader"), as Ceaușescu liked to call himself, "demography was destiny" and countries with rising populations were rising powers. In October 1966, Ceaușescu banned abortion and brought in one of the world's harshest anti-abortion laws.
During the following years Ceaușescu pursued an open policy towards the United States and Western Europe. Romania was the first Warsaw Pact country to recognize West Germany, the first to join the International Monetary Fund, and the first to receive a US President, Richard Nixon. In 1971, Romania became a member of the General Agreement on Tariffs and Trade (GATT). Romania and Yugoslavia were also the only Eastern European countries that entered into trade agreements with the European Economic Community before the fall of the Eastern Bloc.
A series of official visits to Western countries (including the US, France, the United Kingdom, and Spain) helped Ceaușescu to present himself as a reforming Communist, pursuing an independent foreign policy within the Soviet Bloc. He also became eager to be seen as an enlightened international statesman, able to mediate in international conflicts, and to gain international respect for Romania. Ceaușescu negotiated in international affairs, such as the opening of US relations with China in 1969 and the visit of Egyptian president Anwar Sadat to Israel in 1977. Also Romania was the only country in the world to maintain normal diplomatic relations with both Israel and the PLO. In 1980, Romania participated in the 1980 Moscow Olympics with its other Soviet bloc allies but in 1984 was one of the few Communist countries to participate in the 1984 Summer Olympics when most of the East bloc's nations boycotted this event.
The 1966 decree.
In 1967, Ceaușescu, in an attempt to boost the country's population, made abortion illegal and introduced Decree 770 to reverse the very low birth rate and fertility rate. Mothers of at least five children would be entitled to significant benefits, while mothers of at least ten children were declared "heroine mothers" by the Romanian state. Few women ever sought this status; instead, the average Romanian family during the time had two to three children (see Demographics of Romania).
The government also targeted rising divorce rates and made divorce much more difficult—it was decreed that a marriage could be dissolved only in exceptional cases. By the late 1960s, the population began to swell. In turn, a new problem was created by child abandonment, which swelled the orphanage population (see Cighid). Transfusions of untested blood led to Romania accounting for many of Europe's pediatric HIV/AIDS cases at the turn of the 21st century despite having a population that only makes up around 3% of Europe's total population.
Speech of 21 August 1968.
Ceaușescu's speech of 21 August 1968 represented the apogee of Ceaușescu's regime. It marked the highest point in Ceaușescu's popularity, when he openly condemned the Warsaw Pact invasion of Czechoslovakia.
July Theses.
Ceaușescu visited China, North Korea, the Mongolian People's Republic and North Vietnam in 1971. He took great interest in the idea of total national transformation as embodied in the programs of North Korea's "Juche" and China's Cultural Revolution. He was also inspired by the personality cults of North Korea's Kim Il-sung and China's Mao Zedong. Journalist Edward Behr claimed that Ceaușescu admired both Mao and Kim as leaders who not only totally dominated their nations, but had also used totalitarian methods coupled with generous shots of ultra-nationalism mixed in with communism to make both China and North Korea into major world powers. Furthermore, that Kim and even more so Mao had broken free of Soviet control were additional sources of admiration for Ceaușescu. According to Behr, Elena Ceaușescu allegedly bonded with Mao's wife, Jiang Qing. The British journalist wrote that the possibility that what Ceaușescu had seen in both China and North Korea were "vast Potemkin villages for the hoodwinking of gullible foreign guests" was something that never seemed to have crossed his mind. Shortly after returning home, he began to emulate North Korea's system. North Korean books on "Juche" were translated into Romanian and widely distributed inside the country.
On 6 July 1971, he delivered a speech before the Executive Committee of the PCR. This quasi-Maoist speech, which came to be known as the July Theses, contained seventeen proposals. Among these were: continuous growth in the "leading role" of the Party; improvement of Party education and of mass political action; youth participation on large construction projects as part of their "patriotic work"; an intensification of political-ideological education in schools and universities, as well as in children's, youth and student organizations; and an expansion of political propaganda, orienting radio and television shows to this end, as well as publishing houses, theatres and cinemas, opera, ballet, artists' unions, promoting a "militant, revolutionary" character in artistic productions. The liberalisation of 1965 was condemned and an index of banned books and authors was re-established.
The Theses heralded the beginning of a "mini cultural revolution" in Romania, launching a Neo-Stalinist offensive against cultural autonomy, reaffirming an ideological basis for literature that, in theory, the Party had hardly abandoned. Although presented in terms of "Socialist Humanism", the Theses in fact marked a return to the strict guidelines of Socialist Realism, and attacks on non-compliant intellectuals. Strict ideological conformity in the humanities and social sciences was demanded. Competence and aesthetics were to be replaced by ideology; professionals were to be replaced by agitators; and culture was once again to become an instrument for political-ideological propaganda and hardline measures. In a 1972 speech, Ceaușescu stated he wanted " a certain blending of party and state activities...in the long run we shall witness an ever closer blending of the activities of the party, state and other social bodies." In practice, a number of joint party-state organizations were founded such as the Council for Socialist Education and Culture, which had no precise counterpart in any of the other communist states of Eastern Europe, and the Romanian Communist Party was embedded into the daily life of the nation in a way that it never had been before. In 1974, the party programme of the Romanian Communist Party announced that structural changes in society were insufficient to create a full socialist consciousness in the people, and that a full socialist consciousness could only come about if the entire population was made aware of socialist values that guided society. The Communist Party was to be the agency that would so "enlighten" the population and in the words of the British historian Richard Crampton "...the party would merge state and society, the individual and the collective, and would promote 'the ever more organic participation of party members in the entire social life'".
President of Romania.
In 1974, Ceaușescu converted his post of president of the State Council to a full-fledged executive presidency. He was first elected to this post in 1974, and would be reelected every five years until 1989.
Although Ceaușescu had been nominal head of state since 1967, he had merely been first among equals on the State Council, with his real power coming from his status as party leader. The new post, however, made him the nation's top decision-maker both in name and in fact. He was empowered to carry out those functions of the State Council that didn't require plenums. He also appointed and dismissed the president of the Supreme Court and the prosecutor general whenever the legislature was not in session. In practice, from 1974 onward Ceaușescu frequently ruled by decree. For all intents and purposes, Ceaușescu now held all governing power in the nation; virtually all party and state institutions were subordinated to his will.
Starting with the 1973–74 Arab oil embargo against the West, a period of prolonged high oil prices set in that characterised the rest of the 1970s. Romania as a major oil-producer greatly benefited from the high oil prices of the 1970s, which led Ceaușescu to embark on an ambitious plan to invest heavily in oil-refining plants. Ceaușescu's plan was to make Romania into Europe's number one oil refiner not only of its oil, but also of oil from Middle Eastern states like Iraq and Iran, and then to sell all of the refined oil at a profit on the Rotterdam spot market. As Romania lacked the money to build the necessary oil refining plants and Ceaușescu chose to spend the windfall from the high oil prices on aid to the Third World in an attempt to buy Romania international influence, Ceaușescu borrowed heavily from Western banks on the assumption that when the loans came due, the profits from the sales of the refined oil would be more than enough to pay off the loans. A major problem with Ceaușescu's oil-refining plan which led to Romania taking enormous loans was the low productivity of Romanian workers, which meant that the oil-refining plants were finished years behind schedule. The 1977 earthquake which destroyed much of Bucharest also led to delays in the oil plan. By the time the oil refining plans were finished in the early 1980s, a slump in oil prices had set in, leading to major financial problems for Romania.
In August 1977 over 30,000 miners went on strike in the Jiu river valley complaining of low pay and poor working conditions. The Jiu valley miners' strike was the most significant expression of opposition to the Ceaușescu dictatorship prior to the late 1980s. The striking miners were inspired by similar strikes along Poland's Baltic coast in December 1970, and just as in Poland in 1970, the striking Romanian miners demanded face-to-face negotiations with their nation's leader. When Ceaușescu appeared before the miners on the third day of the strike, he was greeted in the words of the British historian Richard Crampton "... once again "á la polonaise", with cries of 'Down with the Red Bourgeoisie!'". Hearing reports that his soldiers were reluctant to fire on fellow Romanians led Ceaușescu to negotiate a compromise solution to the strike. In the years after the strike, the majority of its leaders died of cancer. After 1989, it was revealed that the "Securitate" had doctors give the strike leaders 5-minute chest X-rays to ensure the development of cancer.
He continued to follow an independent policy in foreign relations—for example, in 1984, Romania was one of few communist states (notably including the People's Republic of China, and Yugoslavia) to take part in the American-organized 1984 Summer Olympics in Los Angeles.
Also, the Socialist Republic of Romania was the first of the Eastern bloc nations to have official relations with the Western bloc and the European Community: an agreement including Romania in the Community's Generalised System of Preferences was signed in 1974 and an Agreement on Industrial Products was signed in 1980. On 4 April 1975, Ceaușescu visited Japan and met with Emperor Hirohito.
Pacepa defection.
In 1978, Ion Mihai Pacepa, a senior member of the Romanian political police (Securitate, State Security), defected to the United States. A 3-star general, he was the highest ranking defector from the Eastern Bloc during the Cold War. His defection was a powerful blow against the regime, forcing Ceaușescu to overhaul the architecture of the Security. Pacepa's 1986 book, "Red Horizons: Chronicles of a Communist Spy Chief" (ISBN 0-89526-570-2), claims to expose details of Ceaușescu's regime, such as massive spying on American industry and elaborate efforts to rally Western political support.
Foreign debt.
Ceaușescu's political independence from the Soviet Union and his protest against the invasion of Czechoslovakia in 1968 drew the interest of Western powers, whose governments briefly believed that he was an anti-Soviet maverick and hoped to create a schism in the Warsaw Pact by funding him. Ceaușescu did not realise that the funding was not always favorable. Ceaușescu was able to borrow heavily (more than $13 billion) from the West to finance economic development programs, but these loans ultimately devastated the country's finances. He also secured a deal for cheap oil from Iran, but that deal fell through after the Shah was overthrown.
In an attempt to correct this, Ceaușescu decided to repay Romania's foreign debts. He organised a referendum and managed to change the constitution, adding a clause that barred Romania from taking foreign loans in the future. According to official results, the referendum yielded a nearly unanimous "yes" vote.
In the 1980s, Ceaușescu ordered the export of much of the country's agricultural and industrial production in order to repay its debts. The resulting domestic shortages made the everyday life of Romanians a fight for survival as food rationing was introduced and heating, gas and electricity blackouts became the rule. During the 1980s, there was a steady decrease in the Romanian population's standard of living, especially in the availability and quality of food and general goods in shops. During this time, all regional radio stations were closed, and television was limited to a single channel broadcasting for only two hours a day.
The debt was fully paid in the summer of 1989, shortly before Ceaușescu was overthrown. However, heavy exports continued until the revolution in December.
1984 failed coup d'état attempt.
A tentative coup d'état planned in October 1984 failed when the military unit assigned to carry out the plan was sent to harvest maize instead.
Revolution.
In November 1989, the XIVth Congress of the Romanian Communist Party (PCR) saw Ceaușescu, then aged 71, re-elected for another five years as leader of the PCR. During the Congress, Ceaușescu made a speech denouncing the anti-Communist revolutions happening throughout the rest of Eastern Europe. The following month, Ceaușescu's regime itself collapsed after a series of violent events in Timișoara and Bucharest in December 1989.
Timișoara.
Demonstrations in the city of Timișoara were triggered by the government-sponsored attempt to evict László Tőkés, an ethnic Hungarian pastor, accused by the government of inciting ethnic hatred. Members of his ethnic Hungarian congregation surrounded his apartment in a show of support.
Romanian students spontaneously joined the demonstration, which soon lost nearly all connection to its initial cause and became a more general anti-government demonstration. Regular military forces, police and Securitate fired on demonstrators on 17 December 1989, killing and injuring men, women and children.
On 18 December 1989, Ceaușescu departed for a state visit to Iran, leaving the duty of crushing the Timișoara revolt to his subordinates and his wife. Upon his return to Romania on the evening of 20 December, the situation became even more tense, and he gave a televised speech from the TV studio inside Central Committee Building (CC Building), in which he spoke about the events at Timișoara in terms of an "interference of foreign forces in Romania's internal affairs" and an "external aggression on Romania's sovereignty".
The country, which had little or no information of the Timișoara events from the national media, learned about the Timișoara revolt from U.S. propaganda radio stations such as Voice of America and Radio Free Europe, and by word of mouth. On the next day, 21 December, Ceaușescu staged a mass meeting in Bucharest. Official media presented it as a "spontaneous movement of support for Ceaușescu", emulating the 1968 meeting in which Ceaușescu had spoken against the invasion of Czechoslovakia by Warsaw Pact forces.
Overthrow.
Speech on 21 December.
The mass meeting of 21 December, held in what is now Revolution Square, began like many of Ceaușescu's speeches over the years. Ceaușescu spoke of the achievements of the "Socialist revolution" and Romanian "multi-laterally developed Socialist society." He also blamed the Timișoara riots on "fascist agitators who want to destroy socialism."
However, Ceaușescu had misjudged the crowd's mood. Roughly eight minutes into his speech, several people began jeering, booing and others began chanting "Timișoara!". He tried to silence them by raising his right hand and calling for the crowd's attention before order was temporarily restored, then proceeded to announce social benefit reforms that included raising of the national minimum wage by 200 lei per month. Images of Ceaușescu's facial expression as the crowd began to boo and heckle him were among the most widely broadcast of the collapse of Communism in Eastern Europe.
Failing to control the crowds, the Ceaușescus finally took cover inside the building that housed the Central Committee of the Romanian Communist Party, where they remained until the next day. The rest of the day saw an open revolt of Bucharest's population, which had assembled in University Square and confronted the police and army at barricades. The rioters were no match for the military apparatus concentrated in Bucharest, which cleared the streets by midnight and arrested hundreds of people in the process.
Flight on 22 December.
By the morning of 22 December, the rebellion had already spread to all major cities across the country. The suspicious death of Vasile Milea, the defense minister, later confirmed as a suicide (he tried to incapacitate himself with a flesh wound but a bullet severed his artery), was announced by the media. Immediately thereafter, Ceaușescu presided over the CPEx (Political Executive Committee) meeting and assumed the leadership of the army.
Believing that Milea had been murdered, rank-and-file soldiers switched sides to the revolution almost "en masse". The commanders wrote off Ceaușescu as a lost cause and made no effort to keep their men loyal to the regime. Ceaușescu made a last desperate attempt to address the crowd gathered in front of the Central Committee building, but the people in the square began throwing stones and other projectiles at him, forcing him to take refuge in the building once more. One group of protesters forced open the doors of the building, by now left unprotected. They managed to overpower Ceaușescu's bodyguards and rushed through his office and onto the balcony. Although they did not know it, they were only a few meters from Ceaușescu, who was trapped in an elevator. He, Elena and four others managed to get to the roof and escaped by helicopter, only seconds ahead of a group of demonstrators who had followed them there. The PCR disappeared soon afterward; unlike its kindred parties in the former Soviet bloc, it has never been revived.
During the course of the revolution, the western press published estimates of the number of people killed by Securitate forces in attempting to support Ceaușescu and quell the rebellion. The count increased rapidly until an estimated 64,000 fatalities were widely reported across front pages. The Hungarian military attaché expressed doubt regarding these figures, pointing out the unfeasible logistics of killing such a large number of people in such a short period of time. After Ceaușescu's death, hospitals across the country reported a death toll of fewer than 1,000, and probably much lower than that.
Death.
Ceaușescu and his wife Elena fled the capital with Emil Bobu and Manea Mănescu and headed, by helicopter, for Ceaușescu's Snagov residence, whence they fled again, this time for Târgoviște. Near Târgoviște they abandoned the helicopter, having been ordered to land by the army, which by that time had restricted flying in Romania's airspace. The Ceaușescus were held by the police while the policemen listened to the radio. They were eventually turned over to the army.
On Christmas Day, 25 December 1989, in a small room the Ceaușescus were tried before a kangaroo court convened on orders of the National Salvation Front, Romania's provisional government. They faced charges including illegal gathering of wealth and genocide. Ceaușescu repeatedly denied the court's authority to try him, and asserted he was still legally president of Romania. At the end of the quick show trial the Ceaușescus were found guilty and sentenced to death. A soldier standing guard in the proceedings was ordered to take the Ceaușescus out back one by one and shoot them, but the Ceaușescus demanded to die together. The soldiers agreed to this and began to tie their hands behind their back which the Ceaușescus protested against but were powerless to prevent.
The Ceaușescus were executed by a gathering of soldiers: Captain Ionel Boeru, Sergeant-Major Georghin Octavian and Dorin-Marian Cîrlan, while reportedly hundreds of others also volunteered. The firing squad began shooting as soon as the two were in position against a wall. A TV crew who were to film the execution only managed to catch the end of it as the Ceaușescus lay on the ground shrouded by dust kicked up by the bullets striking the wall and ground. Before his sentence was carried out, Nicolae Ceaușescu sang "The Internationale" while being led up against the wall. After the shooting, the bodies were covered with canvas.
The hasty show trial and the images of the dead Ceaușescus were videotaped and the footage promptly released in numerous western countries two days after the execution. Later that day, it was also shown on Romanian television.
The manner in which the trial was conducted was widely criticised inside and outside Romania. However, Ion Iliescu, Romania's provisional president, said in 2009 that the trial was "quite shameful, but necessary" in order to end the state of near-anarchy that had gripped the country in the three days since the Ceaușescus fled Bucharest. Similarly, Victor Stănculescu, who had been defense minister before going over to the revolution, said in 2009 that the alternative would have been seeing the Ceaușescus lynched on the streets of Bucharest.
The Ceaușescus were the last people to be executed in Romania before the abolition of capital punishment on 7 January 1990.
Nicolae and Elena Ceaușescu were originally buried in simple graves at Ghencea Cemetery in Bucharest, on opposite sides of a path; their graves were often decorated with flowers and symbols of their regime. In April 2007, their son, Valentin Ceaușescu, lost an appeal for an investigation into whether the graves were genuine. Upon his death in 1996, the younger son, Nicu, was buried nearby in the same cemetery. According to "Jurnalul Național", requests were made by the Ceaușescus' daughter Zoia and by supporters of their political views to move their remains to mausoleums or to purpose-built churches. These demands were denied by the government.
Exhumation and reburial.
On 21 July 2010, forensic scientists exhumed the bodies to perform DNA tests to conclusively prove that they were indeed the remains of the Ceaușescus. Elena's body had decayed too much to allow for a positive identification, but Nicolae was easily identifiable, wearing the bullet-riddled black winter coat he had been wearing when he was killed, and DNA was able to conclusively prove his identity. His family organized a funeral service for the couple, and they were reburied together at Ghencea, under a modest tombstone.
"Ceaușism": Ceaușescu's policies.
While the term "Ceaușism" became widely used inside Romania, usually as a pejorative, it never achieved status in academia. This can be explained by the largely crude and syncretic character of the dogma. Ceaușescu attempted to include his views in mainstream Marxist theory, to which he added his belief in a "multilaterally developed Socialist society" as a necessary stage between the Leninist concepts of Socialist and Communist societies (a critical view reveals that the main reason for the interval is the disappearance of the State and Party structures in Communism). A Romanian Encyclopedic Dictionary entry in 1978 underlines the concept as "a new, superior, stage in the Socialist development of Romania [...] begun by the 1971–1975 Five-Year Plan, prolonged over several and projected Five-Year Plans".
Ceaușism's main trait was a form of Romanian nationalism, one which arguably propelled Ceaușescu to power in 1965, and probably accounted for the Party leadership gathered around Ion Gheorghe Maurer choosing him over the more orthodox Gheorghe Apostol. Although he had previously been a careful supporter of the official lines, Ceaușescu came to embody Romanian society's wish for independence after what many considered years of Soviet directives and purges, during and after the SovRom fiasco. He carried this nationalist option inside the Party, manipulating it against the nominated successor Apostol. This nationalist policy had more timid precedents: for example, the Gheorghiu-Dej regime had overseen the withdrawal of the Red Army in 1958.
It had also engineered the publishing of several works that subverted the Russian and Soviet image, such as the final volumes of the official "History of Romania", no longer glossing over traditional points of tension with Russia and the Soviet Union (even alluding to an unlawful Soviet presence in Bessarabia). In the final years of Gheorghiu-Dej's rule more problems were openly discussed, with the publication of a collection of Karl Marx texts that dealt with Romanian topics, showing Marx's previously censored, politically uncomfortable views of Russia.
Ceaușescu was prepared to take a more decisive step in questioning Soviet policies. In the early years of his rule, he generally relaxed political pressures inside Romanian society, which led to the late 1960s and early 1970s being the most liberal decade in Socialist Romania. Gaining the public's confidence, Ceaușescu took a clear stand against the 1968 crushing of the Prague Spring by Leonid Brezhnev. After a visit from Charles de Gaulle earlier in the same year, during which the French President gave recognition to the incipient maverick, Ceaușescu's public speech in August deeply impressed the population, not only through its themes, but also because, uniquely, it was unscripted. He immediately attracted Western sympathies and backing, which lasted well beyond the 'liberal' phase of his regime; at the same time, the period brought forward the threat of armed Soviet invasion: significantly, many young men inside Romania joined the "Patriotic Guards" created on the spur of the moment, in order to meet the perceived threat. President Richard Nixon was invited to Bucharest in 1969, which was the first visit of a United States president to a Socialist country after the start of the Cold War.
Alexander Dubček's version of "Socialism with a human face" was never suited to Romanian Communist goals. Ceaușescu found himself briefly aligned with Dubček's Czechoslovakia and Josip Broz Tito's Yugoslavia. The latter friendship was to last until Tito's death in 1980, with Ceaușescu adapting the Titoist doctrine of "independent Socialist development" to suit his own objectives. Romania proclaimed itself a "Socialist" (in place of "People's") Republic to show that it was fulfilling Marxist goals without Moscow's oversight.
The system's nationalist traits grew and progressively blended with North Korean "Juche" and Maoist ideals. In 1971, the Party, which had already been completely purged of internal opposition (with the possible exception of Gheorghe Gaston Marin), approved the "July Theses", expressing Ceaușescu's disdain of Western models as a whole, and the reevaluation of the recent liberalisation as "bourgeois". The 1974 XIth Party Congress tightened the Party's grip on Romanian culture, guiding it towards Ceaușescu's nationalist principles. Notably, it demanded that Romanian historians refer to Dacians as having "an unorganised State", part of a political continuum that culminated in the Socialist Republic. The regime continued its cultural dialogue with ancient forms, with Ceaușescu connecting his cult of personality to figures such as Mircea cel Bătrân (lit. "Mircea the Elder", whom he styled "Mircea the Great") and Mihai Viteazul (Michael the Brave). It also started adding Dacian or Roman versions to the names of cities and towns ("Drobeta" to Turnu Severin, "Napoca" to Cluj). Although Ceaușescu maintained an independent, "national Communist" course, his absolute control over the country led many non-Romanian observers to describe his regime as one of the closest things to an old-style Stalinist regime. The last edition of the Country Study on Romania, for instance, referred to the PCR's "Stalinist repression of individual liberties."
A new generation of committed supporters on the outside confirmed the regime's character. Ceaușescu probably never emphasized that his policies constituted a paradigm for theorists of National Bolshevism such as Jean-François Thiriart, but there was a publicised connection between him and Iosif Constantin Drăgan, an Iron Guardist Romanian-Italian émigré millionaire (Drăgan was already committed to a Dacian Protochronism that largely echoed the official cultural policy).
Nicolae Ceaușescu had a major influence on modern-day Romanian populist rhetoric. In his final years, he had begun to rehabilitate the image of pro-Nazi dictator Ion Antonescu. Although Antonescu's image was never a fully official myth in Ceaușescu's time, today's politicians such as Corneliu Vadim Tudor have coupled the images of the two leaders into their versions of a national Pantheon. The conflict with Hungary over the treatment of the Magyar minority in Romania had several unusual aspects: not only was it a vitriolic argument between two officially Socialist states, it also marked the moment when Hungary, a state behind the Iron Curtain, appealed to the Organisation for Security and Co-operation in Europe for sanctions to be taken against Romania. This meant that the later 1980s were marked by a pronounced anti-Hungarian discourse, which owed more to nationalist tradition than to Marxism, and the ultimate isolation of Romania on the world stage.
The strong opposition of his regime to all forms of "perestroika" and "glasnost" placed Ceaușescu at odds with Mikhail Gorbachev. He was very displeased when other Warsaw Pact countries decided to try their own versions of Gorbachev's reforms. In particular, he was incensed when Poland's leaders opted for a power-sharing arrangement with the Solidarity trade union. He even went as far as to call for a Warsaw Pact invasion of Poland—a significant reversal, considering how violently he opposed the invasion of Czechoslovakia 20 years earlier. For his part, Gorbachev made no secret of his distaste for Ceaușescu, whom he called "the Romanian führer." At a meeting between the two, Gorbachev upbraided Ceaușescu for his inflexible attitude. "You are running a dictatorship here," the Soviet leader warned.
In November 1989, at the XIVth and last congress of the PCR, Ceaușescu condemned the Molotov–Ribbentrop Pact and asked for the annulment of its consequences. In effect, this amounted to a demand for the return of Bessarabia (most of which was then a Soviet republic and since 1991 has been independent Moldova) and northern Bukovina, both of which had been occupied by the Soviet Union in 1940 and again at the end of World War II.
Non-aligned policy feats.
Ceaușescu's Romania was the only Eastern Bloc country that retained diplomatic relations with Israel and did not sever diplomatic relations after Israel's pre-emptive strike against Egypt at the start of the Six-Day War in 1967. Ceaușescu made efforts to act as a mediator between the PLO and Israel.
Similarly, Romania was the only Soviet bloc country to attend the 1984 Summer Olympics in Los Angeles, which had been boycotted by the Soviets and their allies in response to the U.S.-led boycott of the 1980 Summer Olympics in Moscow.
Ceaușescu's Romania was the only Warsaw Pact country that did not sever diplomatic relations with Chile after Augusto Pinochet's coup.
Nicolae Ceaușescu was a close ally and personal friend of dictator Mobutu Sese Seko of Zaïre. Relations were in fact not just state-to-state, but party-to-party between their respective political machineries, the MPR and the Romanian Communist Party. Many believe that Ceaușescu's death played a role in influencing Mobutu to "democratise" Zaïre in 1990.
Ceaușescu reduced the size of the Romanian Army by 15%, for which he organised a mock referendum. In line with his policy of keeping a facade of "popular democracy" he also ordered large rallies for peace to be held.
Bessarabia.
In August 1976, Nicolae Ceaușescu was the first high-level Romanian visitor to Bessarabia since World War II. In December 1976, at one of his meetings in Bucharest, Ivan Bodiul said that "the good relationship was initiated by Ceaușescu's visit to Soviet Moldova". The final volumes of the official "History of Romania" alluded to an unlawful Soviet presence in Bessarabia.
Personality cult and authoritarianism.
Ceaușescu created a pervasive personality cult, giving himself such titles as "Conducător" ("Leader") and "Geniul din Carpați" ("The Genius of the Carpathians"), with inspiration from Proletarian Culture (Proletkult). After his election as President of Romania, he even had a king-like sceptre made for himself.
The most important day of the year during Ceaușescu's rule was his birthday, 26 January — a day which saw Romanian media saturated with praise for him. According to historian Victor Sebestyen, it was one of the few days of the year when the average Romanian put on a happy face, since appearing miserable on this day was too risky to contemplate.
Such excesses prompted painter Salvador Dalí to send a congratulatory telegram to the "Conducător", in which he sarcastically congratulated Ceaușescu on his "introducing the presidential sceptre". The Communist Party daily "Scînteia" published the message, unaware that it was a work of satire. To lessen the chance of further treason after Pacepa's defection, Ceaușescu also invested his wife Elena and other members of his family with important positions in the government. This led Romanians to joke that Ceaușescu was creating "socialism in one family".
Not surprisingly, Ceaușescu was greatly concerned about his public image. For years, nearly all official photographs of him showed him in his late 40s. Romanian state television was under strict orders to portray him in the best possible light. Additionally, producers had to take great care to make sure that Ceaușescu's height (he was only tall) was never emphasized on screen. Consequences for breaking these rules were severe; one producer showed footage of Ceaușescu blinking and stuttering, and was banned for three months.
As part of a propaganda ploy arranged by the Ceaușescus through the consular cultural attachés of Romanian embassies, they managed to receive orders and titles from numerous states and institutions. France granted Nicolae Ceaușescu the Legion of Honour. In 1978 he became a Knight Grand Cross of the Order of the Bath (GCB) in the UK, a title of which he was stripped in 1989. Elena Ceaușescu was arranged to be "elected" to membership of a Science Academy in the USA.
Legacy.
Nicolae and Elena Ceaușescu had three children: Valentin Ceaușescu (born 1948), a nuclear physicist; Nicu Ceaușescu (1951–1996), also a physicist; and a daughter, Zoia Ceaușescu (1949–2006), who was a mathematician. After the death of his parents, Nicu Ceaușescu ordered the construction of an Orthodox church, the walls of which are decorated with portraits of his parents.
Praising the crimes of totalitarian regimes and denigrating their victims is forbidden by law in Romania; this includes the Ceaușescu regime. Dinel Staicu was fined 25,000 lei (approx. 9,000 United States dollars) for praising Ceaușescu and displaying his pictures on his private television channel ("3TV Oltenia"). Nevertheless, according to opinion polls held in 2010, 41% of Romanians would vote for Ceaușescu and 63% think that their lives were better before 1989. In 2014, the percentage of those who would vote for Ceaușescu reached 66%.
Cultural depictions.
He was played by Constantin Cojocaru in the 2011 Swiss docudrama, "Die letzten Tage der Ceausescus".
Honours and awards.
Ceaușescu received the Danish Order of the Elephant, but this award was revoked on 23 December 1989 by the queen of Denmark, Margrethe II.
Ceaușescu was likewise stripped of his honorary GCB (Knight Grand Cross of the Most Honourable Order of the Bath) status by Queen Elizabeth II of the United Kingdom on the day before his execution. Queen Elizabeth II also returned the Romanian order Ceaușescu had bestowed upon her.
On his 70th birthday in 1988, Ceaușescu was decorated with the Karl-Marx-Orden by then Socialist Unity Party of Germany (SED) chief Erich Honecker; through this he was honoured for his rejection of Mikhail Gorbachev's reforms.
All titles and decorations were revoked by the provisional government on December 26, 1989.
Several foreign decorations were revoked at the time of the collapse of the Ceaușescu regime.<br>
- Argentina
- Austria
- Brazil
- Bulgaria
- Cuba
- Denmark
- France
- Germany (East)
- Germany (West)
- Greece
- Iran
- Italy
- Norway
- Philippines
- Portugal
- Soviet Union: all Soviet decorations were revoked in 1990
- Sweden
- United Kingdom
Honorary degrees from the University of Bucharest (1973), Lebanese University (1974), University of Buenos Aires (1974), Autonomous University of Yucatan (1975), University of Nice Sophia Antipolis (1975), University of Liberia (1988) and North Korea (1988).

</doc>
<doc id="49565" url="https://en.wikipedia.org/wiki?curid=49565" title="President pro tempore of the United States Senate">
President pro tempore of the United States Senate

The president pro tempore ( or ), also president pro tem, is the second-highest-ranking official of the United States Senate. According to the United States Constitution, the Vice President of the United States is the President of the Senate, despite not being a senator, and the Senate must choose a president "pro tempore" to act in his absence. Since 1890, the most senior senator in the majority party has generally been chosen to be president pro tempore; this tradition has been observed without interruption since 1949.
During the vice president's absence, the president pro tempore is empowered to preside over Senate sessions. In practice, neither the vice president nor the president pro tempore usually presides; instead, the duty of presiding officer is rotated among junior senators of the majority party to give them experience in parliamentary procedure.
The president pro tempore is third in the line of succession to the presidency, after the vice president and the Speaker of the House of Representatives and ahead of the Secretary of State.
Orrin Hatch, a Republican and senior senator from Utah, is the current president pro tempore of the Senate, having assumed office in January 2015.
Power and responsibilities.
The office of president pro tempore is created by Article I, Section 3 of the Constitution:
The Senate shall choose their other Officers, and also a President pro tempore, in the absence of the Vice President, or when he shall exercise the Office of President of the United States.
Although the position is in some ways analogous to the Speaker of the House of Representatives, the powers of the president pro tempore are far more limited. In the Senate, most power rests with party leaders and individual senators, but as the chamber's presiding officer, the president pro tempore is authorized to perform certain duties in the absence of the vice president, including ruling on points of order. Additionally, under the 25th Amendment to the Constitution, the president pro tempore and the speaker are the two authorities to whom declarations must be transmitted that the president is unable to perform the duties of the office, or is able to resume doing so. The president pro tempore is third in the line of presidential succession, following the vice president and the speaker. Additional duties include appointment of various congressional officers, certain commissions, advisory boards, and committees and joint supervision of the congressional page school. The president pro tempore is the designated legal recipient of various reports to the Senate, including War Powers Act reports under which he or she, jointly with the speaker, may have the president call Congress back into session. The officeholder is an ex officio member of various boards and commissions. With the secretary and sergeant at arms, the president pro tempore maintains order in Senate portions of the Capitol and Senate buildings.
History.
The office of president pro tempore was established by the Constitution of the United States in 1789. The first president pro tempore, John Langdon, was elected on April 6 the same year. Originally, the president pro tempore was appointed on an intermittent basis when the vice president was not present to preside over the Senate. Until the 1960s, it was common practice for the vice president to preside over daily Senate sessions, so the president pro tempore rarely presided unless the vice presidency became vacant.
Until 1891, the president pro tempore only served until the return of the vice president to the chair or the adjournment of a session of Congress. Between 1792 and 1886, the president pro tempore was second in the line of presidential succession following the vice president and preceding the speaker.
When President Andrew Johnson, who had no vice president, was impeached and tried in 1868, Senate President pro tempore Benjamin Franklin Wade was next in line to the presidency. Wade's radicalism is thought by many historians to be a major reason why the Senate, which did not want to see Wade in the White House, acquitted Johnson. The president pro tempore and the speaker were removed from the line of succession in 1886, but were restored in 1947. This time, however, the president pro tempore followed the speaker.
Following the resignation (for health reasons) of President pro tempore William P. Frye, a Senate divided among progressive Republicans, conservative Republicans, and Democrats reached a compromise by which each of their candidates would rotate holding the office from 1911 to 1913 (see below, 62nd Congress).
Only three former presidents pro tempore ever became vice president: John Tyler, William R. King and Charles Curtis. Tyler is also the only one to have become president, when he succeeded William Henry Harrison in 1841.
Related officials.
Acting president pro tempore.
While the president pro tempore does have other official duties, the holders of the office have, like the vice president, over time ceased presiding over the Senate on a daily basis, owing to the mundane and ceremonial nature of the position. Furthermore, as the president pro tempore is now usually the most senior senator of the majority party, he or she most likely also chairs a major Senate committee and has other significant demands on his or her time. Therefore, the president pro tempore has less time now than in the past to preside daily over the Senate. Instead, junior senators from the majority party are designated acting president pro tempore to preside over the Senate. This allows junior senators to learn proper parliamentary procedure.
Permanent acting president pro tempore.
In June 1963, because of the illness of president pro tempore Carl Hayden, Senator Lee Metcalf was designated permanent acting president pro tempore. No term was imposed on this designation, so Metcalf retained it until he died in office in 1978.
Deputy president pro tempore.
The ceremonial post of deputy president pro tempore was created for Hubert Humphrey, a former vice president, in 1977 following his losing bid to become the Senate majority leader. The Senate resolution creating the position stated that any former president or former vice president serving in the Senate would be entitled to this position, though none has served since Humphrey's death in 1978, and former vice president Walter Mondale, who sought his former Senate seat in Minnesota in 2002, is the only one to have tried. Andrew Johnson is the only former president to have subsequently served in the Senate.
George J. Mitchell was elected deputy president pro tempore in 1987, because of the illness of president pro tempore John C. Stennis, similar to Metcalf's earlier designation as permanent acting president pro tempore. The office has remained vacant since 1988, and no senator other than Humphrey and Mitchell has held it since its creation.
The post is largely honorary and ceremonial, but comes with a salary increase. By statute, the compensation granted to the position holder equals the rate of annual compensation paid to the president pro tempore, majority leader, and minority leader. ("See" .)
President pro tempore emeritus.
Since 2001, the honorary title of president pro tempore emeritus has been given to a senator of the minority party who has previously served as president pro tempore. The position has been held by Strom Thurmond (R-South Carolina) (2001–2003), Robert Byrd (D-West Virginia) (2003–2007), Ted Stevens (R-Alaska) (2007–2009) and Patrick Leahy (D-Vermont) (2015–present). From 2009 to 2015, no Senator met all of the requirements of the position, and the office was vacant.
The position was created for Thurmond when the Democratic Party regained a majority in the Senate in June 2001. With the change in party control, Democrat Robert Byrd of West Virginia replaced Thurmond as president pro tempore, reclaiming a position he had previously held from 1989 to 1995 and briefly in January 2001. Thurmond's retirement from the Senate on January 3, 2003, coincided with a change from Democratic to Republican control, making Stevens president pro tempore and Byrd the second president pro tempore emeritus. Byrd returned as president pro tempore, and Stevens became the third president pro tempore emeritus, when the Democrats gained control of the Senate in 2007. While a president pro tempore emeritus has no official duties, he is entitled to an increase in staff and advises party leaders on the functions of the Senate.
The office's accompanying budget increase was removed toward the end of the 113th Congress, shortly before Patrick Leahy was to become the first holder of the title in six years. Quoted in "CQ Roll Call", Leahy commented, "They didn't keep their commitment. They want to treat us differently than we treated them, and so they've got that right. It seems kind of petty, but it really doesn't matter to me. I've got plenty of funding, plenty of good staff."
Salary.
The salary of the president pro tempore for 2012 was $193,400, equal to that of the majority leaders and minority leaders of both houses of Congress. If there is a vacancy in the office of vice president, then the salary would be the same as that of the vice president, $230,700.
Note.
Arthur Vandenberg (serving in 1947–1949) was the last president pro tempore not to be the senior member of the majority party, aside from the single day accorded Milton Young (serving in 1980), who was the retiring senior member of the party who had been elected to a majority in the incoming congress.

</doc>
<doc id="49568" url="https://en.wikipedia.org/wiki?curid=49568" title="Montgomery">
Montgomery

Montgomery may refer to:

</doc>
<doc id="49569" url="https://en.wikipedia.org/wiki?curid=49569" title="Bayes' theorem">
Bayes' theorem

In probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on conditions that might be related to the event. For example, suppose one is interested in whether a person has cancer, and knows the person's age. If cancer is related to age, then, using Bayes' theorem, information about the person's age can be used to more accurately assess the probability that they have cancer.
When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. In one of these interpretations, the theorem is used directly as part of a particular approach to statistical inference. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for evidence: this is Bayesian inference, which is fundamental to Bayesian statistics. However, Bayes' theorem has applications in a wide range of calculations involving probabilities, not just in Bayesian inference.
Bayes' theorem is named after Rev. Thomas Bayes (; 1701–1761), who first provided an equation that allows new evidence to update beliefs. It was further developed by Pierre-Simon Laplace, who first published the modern formulation in his 1812 "Théorie analytique des probabilités". Sir Harold Jeffreys put Bayes' algorithm and Laplace's formulation on an axiomatic basis. Jeffreys wrote that Bayes' theorem "is to the theory of probability what the Pythagorean theorem is to geometry".
Statement of theorem.
Bayes' theorem is stated mathematically as the following equation:
where "A" and "B" are events.
Examples.
Cancer at age 65.
Suppose we want to know an individual's probability of having cancer, but we know nothing about them. Despite not knowing anything about that person, a probability can be assigned based on the general prevalence of cancer. For the sake of this example, suppose it is 1%. This is known as the base rate or prior probability of having cancer. "Prior" refers to the time before being informed about the particular case at hand.
Next, suppose we find out that person is 65 years old. If we assume that cancer and age are related, this new piece of information can be used to better assess that person's risk of having cancer. More precisely, we'd like to know the probability that a person has cancer when it is known that they are 65 years old. This quantity is known as the current probability, where "current" refers to the theorised situation upon finding out information about the particular case at hand.
In order to apply knowledge of that person's age in conjunction with Bayes' Theorem, two additional pieces of information are needed. Note, however, that the additional information is not specific to that person. The needed information is as follows:
Knowing this, along with the base rate, we can calculate that a person who is age 65 has a probability of having cancer equal to
formula_2
It may come as a surprise that even though being 65 years old increases the risk of having cancer, that person's probability of having cancer is still fairly low. This is because the base rate of cancer (regardless of age) is low. This illustrates both the importance of base rate, as well as that it is commonly neglected. Base rate neglect leads to serious misinterpretation of statistics; therefore, special care should be taken to avoid such mistakes. Becoming familiar with Bayes' theorem is one way to combat the natural tendency to neglect base rates.
Problems using Bayes' Theorem are often more easily grasped by applying the conditions given in the problem to a large pool of observations. Assume, for example, that a community consists of 100,000 people. According to the statement of the problem, 1% of the population, or 1,000 people will have cancer. 0.2% of the population, or 200 people, will be 65 years old. Of the 1000 people with cancer, only .5%, or 5 people, will be 65 years old. Thus, of the 200 people who are 65, only 5 can be expected to have cancer. 5/200 = 2.5%.
Drug testing.
Suppose a drug test is 99% sensitive and 99% specific. That is, the test will produce 99% true positive results for drug users and 99% true negative results for non-drug users. Suppose that 0.5% of people are users of the drug. If a randomly selected individual tests positive, what is the probability that they are a user?
Despite the apparent accuracy of the test, if an individual tests positive, it is more likely that they do "not" use the drug than that they do. This again illustrates the importance of base rates, and how the formation of policy can be egregiously misguided if base rates are neglected.
This surprising result arises because the number of non-users is very large compared to the number of users; thus the number of false positives (0.995%) outweighs the number of true positives (0.495%). To use concrete numbers, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.01 × 995 ≃ 10 false positives are expected. From the 5 users, 0.99 × 5 ≃ 5 true positives are expected. Out of 15 positive results, only 5, about 33%, are genuine.
Note: The importance of specificity can be illustrated by showing that even if sensitivity is 100% and specificity is at 99% the probability of the person being a drug user is ≈33% but if the specificity is changed to 99.5% and the sensitivity is dropped down to 99% the probability of the person being a drug user rises to 49.8%.
A more complicated example.
The entire output of a factory is produced on three machines. The three machines account for 20%, 30%, and 50% of the output, respectively. The fraction of defective items produced is this: for the first machine, 5%; for the second machine, 3%; for the third machine, 1%. If an item is chosen at random from the total output and is found to be defective, what is the probability that it was produced by the third machine?
A solution is as follows. Let "Ai" denote the event that a randomly chosen item was made by the "i"th machine (for "i" = 1,2,3). Let "B" denote the event that a randomly chosen item is defective. Then, we are given the following information:
If the item was made by machine "A"1, then the probability that it is defective is 0.05; that is, "P"("B" | "A"1) = 0.05. Overall, we have
To answer the original question, we first find "P"("B"). That can be done in the following way:
Hence 2.4% of the total output of the factory is defective.
We are given that "B" has occurred, and we want to calculate the conditional
probability of "A"3. By Bayes' theorem,
Given that the item is defective, the probability that it was made by the third
machine is only 5/24. Although machine 3 produces half of the total output, it
produces a much smaller fraction of the defective items. Hence the knowledge
that the item selected was defective enables us to replace the prior probability
"P"("A"3) = 1/2 by the smaller posterior probability "P"("A"3 | "B") = 5/24.
Once again, the answer can be reached without recourse to the formula by applying the conditions to any hypothetical number of cases. For example, in 100,000 items produced by the factory, 20,000 will be produced by Machine A, 30,000 by Machine B, and 50,000 by Machine C. Machine A will produce 1000 defective items, Machine B 900, and Machine C 500. Of the total 2400 defective items, only 500, or 5/24 were produced by Machine C.
Interpretations.
The interpretation of Bayes' theorem depends on the interpretation of probability ascribed to the terms. The two main interpretations are described below.
Bayesian interpretation.
In the Bayesian (or epistemological) interpretation, probability measures a "degree of belief". Bayes' theorem then links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief may rise, fall or remain the same depending on the results.
For proposition "A" and evidence "B",
For more on the application of Bayes' theorem under the Bayesian interpretation of probability, see Bayesian inference.
Frequentist interpretation.
In the frequentist interpretation, probability measures a "proportion of outcomes". For example, suppose an experiment is performed many times. "P" ("A" ) is the proportion of outcomes with property "A", and "P" ("B" ) that with property "B". "P" ("B" |"A" ) is the proportion of outcomes with property "B" "out of" outcomes with property "A", and "P" ("A" |"B" ) the proportion of those with "A" "out of" those with "B".
The role of Bayes' theorem is best visualized with tree diagrams, as shown to the right. The two diagrams partition the same outcomes by "A" and "B" in opposite orders, to obtain the inverse probabilities. Bayes' theorem serves as the link between these different partitionings.
Example.
An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back. In the rare subspecies, 98% have the pattern, or "P" (Pattern|Rare) = 98%. In the common subspecies, 5% have the pattern. The rare subspecies accounts for only 0.1% of the population. How likely is the beetle having the pattern to be rare, or what is "P" (Rare|Pattern)?
From the extended form of Bayes' theorem (since any beetle can be only rare or common),
formula_4
Forms.
Events.
Simple form.
For events "A" and "B", provided that "P"("B") ≠ 0,
In many applications, for instance in Bayesian inference, the event "B" is fixed in the discussion, and we wish to consider the impact of its having been observed on our belief in various possible events "A". In such a situation the denominator of the last expression, the probability of the given evidence "B", is fixed; what we want to vary is "A". Bayes' theorem then shows that the posterior probabilities are proportional to the numerator:
In words: posterior is proportional to prior times likelihood.
If events "A"1, "A"2, ..., are mutually exclusive and exhaustive, i.e., one of them is certain to occur but no two can occur together, and we know their probabilities up to proportionality, then we can determine the proportionality constant by using the fact that their probabilities must add up to one. For instance, for a given event "A", the event "A" itself and its complement ¬"A" are exclusive and exhaustive. Denoting the constant of proportionality by "c" we have
Adding these two formulas we deduce that
Alternative form.
Another form of Bayes' Theorem that is generally encountered when looking at two competing statements or hypotheses is:
For an epistemological interpretation:
For proposition "A" and evidence or background "B",
Extended form.
Often, for some partition {"Aj"} of the sample space, the event space is given or conceptualized in terms of "P"("Aj") and "P"("B" | "Aj"). It is then useful to compute "P"("B") using the law of total probability:
In the special case where "A" is a binary variable:
Random variables.
Consider a sample space Ω generated by two random variables "X" and "Y". In principle, Bayes' theorem applies to the events "A" = {"X" = "x"} and "B" = {"Y" = "y"}. However, terms become 0 at points where either variable has finite probability density. To remain useful, Bayes' theorem may be formulated in terms of the relevant densities (see Derivation).
Simple form.
If "X" is continuous and "Y" is discrete,
If "X" is discrete and "Y" is continuous,
If both "X" and "Y" are continuous,
Extended form.
A continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the law of total probability. For "fY"("y"), this becomes an integral:
Bayes' rule.
Bayes' rule is Bayes' theorem in odds form.
where
is called the Bayes factor or likelihood ratio and the odds between two events is simply the ratio of the probabilities of the two events. Thus
So the rule says that the posterior odds are the prior odds times the Bayes factor, or in other words, posterior is proportional to prior times likelihood.
Derivation.
For events.
Bayes' theorem may be derived from the definition of conditional probability:
For random variables.
For two continuous random variables "X" and "Y", Bayes' theorem may be analogously derived from the definition of conditional density:
History.
Bayes' theorem was named after the Reverend Thomas Bayes (1701–61), who studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). Bayes' unpublished manuscript was significantly edited by Richard Price before it was posthumously read at the Royal Society. Price edited Bayes' major work "An Essay towards solving a Problem in the Doctrine of Chances" (1763), which appeared in "Philosophical Transactions", and contains Bayes' Theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of Bayesian statistics. In 1765 he was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes.
The French mathematician Pierre-Simon Laplace reproduced and extended Bayes' results in 1774, apparently quite unaware of Bayes' work. Stephen Stigler suggested in 1983 that Bayes' theorem was discovered by Nicholas Saunderson some time before Bayes; that interpretation, however, has been disputed.
Martyn Hooper and Sharon McGrayne have argued that Richard Price's contribution was substantial:

</doc>
<doc id="49571" url="https://en.wikipedia.org/wiki?curid=49571" title="Bayesian inference">
Bayesian inference

Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called "Bayesian probability".
Introduction to Bayes' rule.
Formal.
Bayesian inference derives the posterior probability as a consequence of two antecedents, a prior probability and a "likelihood function" derived from a statistical model for the observed data. Bayesian inference computes the posterior probability according to Bayes' theorem:
where
Note that, for different values of formula_3, only the factors formula_5 and formula_12 affect the value of formula_8. As both of these factors appear in the numerator, the posterior probability is proportional to both. In words:
Note that Bayes' rule can also be written as follows:
where the factor formula_27 represents the impact of formula_28 on the probability of formula_29.
Informal.
If the evidence does not match up with a hypothesis, one should reject the hypothesis. But if a hypothesis is extremely unlikely "a priori", one should also reject it, even if the evidence does appear to match up.
For example, imagine that I have various hypotheses about the nature of a newborn baby of a human friend, including:
Then consider two scenarios:
The critical point about Bayesian inference, then, is that it provides a principled way of combining new evidence with prior beliefs, through the application of Bayes' rule. (Contrast this with frequentist inference, which relies only on the evidence as a whole, with no reference to prior beliefs.) Furthermore, Bayes' rule can be applied iteratively: after observing some evidence, the resulting posterior probability can then be treated as a prior probability, and a new posterior probability computed from new evidence. This allows for Bayesian principles to be applied to various kinds of evidence, whether viewed all at once or over time. This procedure is termed "Bayesian updating".
Alternatives to Bayesian updating.
Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered rational.
Ian Hacking noted that traditional "Dutch book" arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. Hacking wrote "And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour."
Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "probability kinematics" following the publication of Richard C. Jeffrey's rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability. The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory.
Formal description of Bayesian inference.
Bayesian inference.
Note that this is expressed in words as "posterior is proportional to likelihood times prior", or sometimes as "posterior = likelihood times prior, over evidence".
Bayesian prediction.
Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned. Only this way is the entire posterior distribution of the parameter(s) used. By comparison, prediction in frequentist statistics often involves finding an optimum point estimate of the parameter(s)—e.g., by maximum likelihood or maximum a posteriori estimation (MAP)—and then plugging this estimate into the formula for the distribution of a data point. This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the variance of the predictive distribution.
Note that both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood). In fact, if the prior distribution is a conjugate prior, and hence the prior and posterior distributions come from the same family, it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the conjugate prior article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.
Inference over exclusive and exhaustive possibilities.
If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole.
General formulation.
Suppose a process is generating independent and identically distributed events formula_53, but the probability distribution is unknown. Let the event space formula_54 represent the current state of belief for this process. Each model is represented by event formula_55. The conditional probabilities formula_56 are specified to define the models. formula_57 is the degree of belief in formula_55. Before the first inference step, formula_59 is a set of "initial prior probabilities". These must sum to 1, but are otherwise arbitrary.
Suppose that the process is observed to generate formula_60. For each formula_61, the prior formula_62 is updated to the posterior formula_63. From Bayes' theorem:
Upon observation of further evidence, this procedure may be repeated.
Multiple observations.
For a sequence of independent and identically distributed observations formula_65, it can be shown by induction that repeated application of the above is equivalent to
Where
Parametric formulation.
By parameterizing the space of models, the belief in all models may be updated in a single step. The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space. The distributions in this section are expressed as continuous, represented by probability densities, as this is the usual situation. The technique is however equally applicable to discrete distributions.
Let the vector formula_68 span the parameter space. Let the initial prior distribution over formula_68 be formula_70, where formula_71 is a set of parameters to the prior itself, or "hyperparameters". Let formula_65 be a sequence of independent and identically distributed event observations, where all formula_73 are distributed as formula_74 for some formula_68. Bayes' theorem is applied to find the posterior distribution over formula_68:
Where
Mathematical properties.
Interpretation of factor.
formula_79. That is, if the model were true, the evidence would be more likely than is predicted by the current state of belief. The reverse applies for a decrease in belief. If the belief does not change, formula_80. That is, the evidence is independent of the model. If the model were true, the evidence would be exactly as likely as predicted by the current state of belief.
Cromwell's rule.
If formula_81 then formula_82. If formula_83, then formula_84. This can be interpreted to mean that hard convictions are insensitive to counter-evidence.
The former follows directly from Bayes' theorem. The latter can be derived by applying the first rule to the event "not formula_85" in place of "formula_85", yielding "if formula_87, then formula_88", from which the result immediately follows.
Asymptotic behaviour of posterior.
Consider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials. For sufficiently nice prior probabilities, the Bernstein-von Mises theorem gives that in the limit of infinite trials, the posterior converges to a Gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by Joseph L. Doob in 1948, namely if the random variable in consideration has a finite probability space. The more general results were obtained later by the statistician David A. Freedman who published in two seminal research papers in 1963 and 1965 when and under what circumstances the asymptotic behaviour of posterior is guaranteed. His 1963 paper treats, like Doob (1949), the finite case and comes to a satisfactory conclusion. However, if the random variable has an infinite but countable probability space (i.e., corresponding to a die with infinite many faces) the 1965 paper demonstrates that for a dense subset of priors the Bernstein-von Mises theorem is not applicable. In this case there is almost surely no asymptotic convergence. Later in the 1980s and 1990s Freedman and Persi Diaconis continued to work on the case of infinite countable probability spaces. To summarise, there may be insufficient trials to suppress the effects of the initial choice, and especially for large (but finite) systems the convergence might be very slow.
Conjugate priors.
In parameterized form, the prior distribution is often assumed to come from a family of distributions called conjugate priors. The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family, and the calculation may be expressed in closed form.
Estimates of parameters and predictions.
It is often desired to use a posterior distribution to estimate a parameter or variable. Several methods of Bayesian estimation select measurements of central tendency from the posterior distribution.
For one-dimensional problems, a unique median exists for practical continuous problems. The posterior median is attractive as a robust estimator.
If there exists a finite mean for the posterior distribution, then the posterior mean is a method of estimation.
Taking a value with the greatest probability defines maximum "a posteriori" (MAP) estimates:
There are examples where no maximum is attained, in which case the set of MAP estimates is empty.
There are other methods of estimation that minimize the posterior "risk" (expected-posterior loss) with respect to a loss function, and these are of interest to statistical decision theory using the sampling distribution ("frequentist statistics").
The posterior predictive distribution of a new observation formula_45 (that is independent of previous observations) is determined by
Examples.
Probability of a hypothesis.
Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?
Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let formula_93 correspond to bowl #1, and formula_94 to bowl #2.
It is given that the bowls are identical from Fred's point of view, thus formula_95, and the two must add up to 1, so both are equal to 0.5.
The event formula_28 is the observation of a plain cookie. From the contents of the bowls, we know that formula_97 and formula_98 Bayes' formula then yields
Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, formula_100, which was 0.5. After observing the cookie, we must revise the probability to formula_101, which is 0.6.
Making a prediction.
An archaeologist is working at a site thought to be from the medieval period, between the 11th century to the 16th century. However, it is uncertain exactly when in this period the site was inhabited. Fragments of pottery are found, some of which are glazed and some of which are decorated. It is expected that if the site were inhabited during the early medieval period, then 1% of the pottery would be glazed and 50% of its area decorated, whereas if it had been inhabited in the late medieval period then 81% would be glazed and 5% of its area decorated. How confident can the archaeologist be in the date of inhabitation as fragments are unearthed?
The degree of belief in the continuous variable formula_102 (century) is to be calculated, with the discrete set of events formula_103 as evidence. Assuming linear variation of glaze and decoration with time, and that these variables are independent,
Assume a uniform prior of formula_108, and that trials are independent and identically distributed. When a new fragment of type formula_109 is discovered, Bayes' theorem is applied to update the degree of belief for each formula_110:
formula_111
A computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph. In the simulation, the site was inhabited around 1420, or formula_112. By calculating the area under the relevant portion of the graph for 50 trials, the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries, about 1% chance that it was inhabited during the 13th century, 63% chance during the 14th century and 36% during the 15th century. Note that the Bernstein-von Mises theorem asserts here the asymptotic convergence to the "true" distribution because the probability space corresponding to the discrete set of events formula_103 is finite (see above section on asymptotic behaviour of the posterior).
In frequentist statistics and decision theory.
A decision-theoretic justification of the use of Bayesian inference was given by Abraham Wald, who proved that every unique Bayesian procedure is admissible. Conversely, every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.
Wald characterized admissible procedures as Bayesian procedures (and limits of Bayesian procedures), making the Bayesian formalism a central technique in such areas of frequentist inference as parameter estimation, hypothesis testing, and computing confidence intervals. For example:
Applications.
Computer applications.
Bayesian inference has applications in artificial intelligence and expert systems. Bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s. There is also an ever growing connection between Bayesian methods and simulation-based Monte Carlo techniques since complex models cannot be processed in closed form by a Bayesian analysis, while a graphical model structure "may" allow for efficient simulation algorithms like the Gibbs sampling and other Metropolis–Hastings algorithm schemes. Recently Bayesian inference has gained popularity amongst the phylogenetics community for these reasons; a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously.
As applied to statistical classification, Bayesian inference has been used in recent years to develop algorithms for identifying e-mail spam. Applications which make use of Bayesian inference for spam filtering include CRM114, DSPAM, Bogofilter, SpamAssassin, SpamBayes, Mozilla, XEAMS, and others. Spam classification is treated in more detail in the article on the naive Bayes classifier.
Solomonoff's Inductive inference is the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. It is a formal inductive framework that combines two well-studied principles of inductive inference: Bayesian statistics and Occam’s Razor.
Solomonoff's universal prior probability of any prefix "p" of a computable sequence "x" is the sum of the probabilities of all programs (for a universal computer) that compute something starting with "p". Given some "p" and any computable but unknown probability distribution from which "x" is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of "x" in optimal fashion.
In the courtroom.
Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant, and to see whether, in totality, it meets their personal threshold for 'beyond a reasonable doubt'. Bayes' theorem is applied successively to all evidence presented, with the posterior from one stage becoming the prior for the next. The benefit of a Bayesian approach is that it gives the juror an unbiased, rational mechanism for combining evidence. It may be appropriate to explain Bayes' theorem to jurors in odds form, as betting odds are more widely understood than probabilities. Alternatively, a logarithmic approach, replacing multiplication with addition, might be easier for a jury to handle.
If the existence of the crime is not in doubt, only the identity of the culprit, it has been suggested that the prior should be uniform over the qualifying population. For example, if 1,000 people could have committed the crime, the prior probability of guilt would be 1/1000.
The use of Bayes' theorem by jurors is controversial. In the United Kingdom, a defence expert witness explained Bayes' theorem to the jury in "R v Adams". The jury convicted, but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes' theorem. The Court of Appeal upheld the conviction, but it also gave the opinion that "To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task."
Gardner-Medwin argues that the criterion on which a verdict in a criminal trial should be based is "not" the probability of guilt, but rather the "probability of the evidence, given that the defendant is innocent" (akin to a frequentist p-value). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:
Gardner-Medwin argues that the jury should believe both A and not-B in order to convict. A and not-B implies the truth of C, but the reverse is not true. It is possible that B and C are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also Lindley's paradox.
Bayesian epistemology.
Bayesian epistemology is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic.
Karl Popper and David Miller have rejected the alleged rationality of Bayesianism, i.e. using Bayes rule to make epistemological inferences: It is prone to the same vicious circle as any other justificationist epistemology, because it presupposes what it attempts to justify. According to this view, a rational interpretation of Bayesian inference would see it merely as a probabilistic version of falsification, rejecting the belief, commonly held by Bayesians, that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt, or even with likelihood greater than 0.
Bayes and Bayesian inference.
The problem considered by Bayes in Proposition 9 of his essay, "An Essay towards solving a Problem in the Doctrine of Chances", is the posterior distribution for the parameter "a" (the success rate) of the binomial distribution.
History.
The term "Bayesian" refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem. However, it was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence. Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called "inverse probability" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, "inverse probability" was largely supplanted by a collection of methods that came to be called frequentist statistics.
In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to "objective" and "subjective" currents in Bayesian practice. In the objective or "non-informative" current, the statistical analysis depends on only the model assumed, the data analyzed, and the method assigning the prior, which differs from one objective Bayesian to another objective Bayesian. In the subjective or "informative" current, the specification of the prior depends on the belief (that is, propositions on which the analysis is prepared to act), which can summarize information from experts, previous studies, etc.
In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods, which removed many of the computational problems, and an increasing interest in nonstandard, complex applications. Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics. Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of machine learning.
Further reading.
Elementary.
The following books are listed in ascending order of probabilistic sophistication:

</doc>
<doc id="49572" url="https://en.wikipedia.org/wiki?curid=49572" title="Hooliganism">
Hooliganism

Hooliganism is disruptive or unlawful behavior such as rioting, bullying, and vandalism.
Etymology.
There are several theories regarding the origin of the word "hooliganism," which is a derivative of the word hooligan. "The Compact Oxford English Dictionary" states that the word may have originated from the surname of a fictional rowdy Irish family in a music hall song of the 1890s. Clarence Rook, in his 1899 book, "Hooligan Nights", wrote that the word came from Patrick Hoolihan (or Hooligan), an Irish bouncer and thief who lived in London. In 2015, it was said in the BBC Scotland TV programme "The Secret Life of Midges" that the English commander-in-chief during the Jacobite rising of 1745, General Wade, misheard the local Scots Gaelic word for midge—"meanbh-chuileag"—and coined the word "hooligan" to describe his fury and frustration at the way the tiny biting creatures made the life of his soldiers and himself a misery; this derivation may be apocryphal.
Early usage.
The first use of the term is unknown, but the word first appeared in print in London police-court reports in 1894 referring to the name of a gang of youths in the Lambeth area of London—the "Hooligan Boys", and later—the "O'Hooligan Boys".
In August 1898 a murder in Lambeth committed by a member of the gang drew further attention to the word which was immediately popularised by the press. The London newspaper "The Daily Graphic" wrote in an article on 22 August 1898, "The avalanche of brutality which, under the name of 'Hooliganism' ... has cast such a dire slur on the social records of South London."
Arthur Conan Doyle wrote in his 1904 short story "The Adventure of the Six Napoleons", "It seemed to be one of those senseless acts of Hooliganism which occur from time to time, and it was reported to the constable on the beat as such." H. G. Wells wrote in his 1909 semi-autobiographical novel "Tono-Bungay", "Three energetic young men of the hooligan type, in neck-wraps and caps, were packing wooden cases with papered-up bottles, amidst much straw and confusion."
According to "Life" magazine (30 July 1941), the comic strip artist and political cartoonist Frederick Burr Opper introduced a character called Happy Hooligan in 1900; "hapless Happy appeared regularly in U.S. newspapers for more than 30 years," a "naive, skinny, baboon-faced tramp who invariably wore a tomato can for a hat." "Life" brought this up by way of criticizing the Soviet U.N. delegate Yakov A. Malik for misusing the word. Malik had indignantly referred to anti-Soviet demonstrators in New York as "hooligans". Happy Hooligan, "Life" reminded its readers, "became a national hero, not by making trouble, which Mr. Malik understands is the function of a hooligan, but by getting himself into it."
Modern usage.
Later, as the meaning of the word shifted slightly, none of the possible alternatives had precisely the same undertones of a person, usually young, who belongs to an informal group and commits acts of vandalism or criminal damage, starts fights, and who causes disturbances but is not a thief.
Violence in sports.
The word "hooliganism" and "hooligan" began to be associated with violence in sports, in particular from the 1970s in the UK with football hooliganism. The phenomenon, however, long preceded the modern term; for example, one of the earliest known instances of crowd violence at a sporting event took place in ancient Constantinople. Two chariot racing factions, the Blues and the Greens, were involved in the Nika riots which lasted around a week in 532 CE; nearly half the city was burned or destroyed, in addition to tens of thousands of deaths.
Sports crowd violence continues to be a worldwide concerning phenomenon exacting at times an inexcusable number of injuries, damage to property and casualties. No single account on its own can be used to understand or explain sports collective violence. Rather, individual, social and environmental factors interact and influence one another through a dynamic process occurring at different levels. Furthermore, any form of sport fan aggression should always be considered in reference to the wider social-structural and environmental context in which it takes place. Macro-sociological accounts suggest that structural strains, experiences of deprivation or a low socio-economic background can at times be instrumental to the acceptance and reproduction of norms that tolerate great levels of violence and territoriality, which is a common feature of football hooliganism. Furthermore, social cleavages within societies facilitate the development of strong in-groups bonds and intense feelings of antagonism towards outsiders which in turn can facilitate group identification and affect the likelihood of fan violence.
In the Soviet Union and Russia.
In the Soviet Union the word "khuligan" was used to refer to scofflaws. Hooliganism (, "khuliganstvo") was listed as a criminal offense, similar to disorderly conduct in some other jurisdictions, and used as a catch-all charge for prosecuting unapproved behavior. Hooliganism is defined generally in the Criminal Code of Russia as an average gravity crime.
Olympic medalist Vasiliy Khmelevskiy was convicted of hooliganism for setting a masqueraded person on fire during a celebration in 1979 and sentenced to five years imprisonment. Mathias Rust was convicted of hooliganism, among other things, for his 1987 Cessna landing in Red Square. More recently, the same charge has been leveled against members of the feminist punk group Pussy Riot for which three members have each received a two-year sentence on 17 August 2012. Hooliganism charges have also been levelled against the Greenpeace protestors in October 2013.
Hooliganism in film.
See also

</doc>
<doc id="49574" url="https://en.wikipedia.org/wiki?curid=49574" title="Abbie Hoffman">
Abbie Hoffman

Abbot Howard "Abbie" Hoffman (November 30, 1936 – April 12, 1989) was an American political and social activist and anarchist who co-founded the Youth International Party ("Yippies").
Hoffman was arrested and tried for conspiracy and inciting to riot as a result of his role in protests that led to violent confrontations with police during the 1968 Democratic National Convention, along with Jerry Rubin, David Dellinger, Tom Hayden, Rennie Davis, John Froines, Lee Weiner, and Bobby Seale. The group was known collectively as the "Chicago Eight"; when Seale's prosecution was separated from the others, they became known as the Chicago Seven. While the defendants were initially convicted of intent to incite a riot, the verdicts were overturned on appeal.
Hoffman continued his activism into the 1970s, and remains an icon of the anti-war movement and the counterculture era.
Early life and education.
Hoffman was born November 30, 1936 in Worcester, Massachusetts, to John Hoffman and Florence Schanberg. Hoffman was raised in a middle-class household and had two younger siblings. As a child in the 1940s–50s, he was a member of what has been described as "the transitional generation between the beatniks and hippies". He described his childhood as "idyllic" and the '40s as "a great time to grow up in." On June 3, 1954, 17-year-old Hoffman was arrested for the first time, for driving without a license. During his school days, he became known as a troublemaker who started fights, played pranks, vandalized school property, and referred to teachers by their first names. In his sophomore year, Hoffman was expelled from Classical High School, a now-closed public high school in Worcester. As an atheist, Hoffman wrote a paper declaring that "God could not possibly exist, for if he did, there wouldn't be any suffering in the world." The irate teacher ripped up the paper and called him "a Communist punk." Hoffman jumped on the teacher and started fighting him until he was restrained and removed from the school. After his expulsion, he attended Worcester Academy, graduating in 1955. Hoffman engaged in many behaviors typical of rebellious teenagers in the 1950s such as riding motorcycles, wearing leather jackets, and sporting a ducktail haircut. Upon graduating, he enrolled in Brandeis University, where he studied under professors such as noted psychologist Abraham Maslow, often considered the father of humanistic psychology. He was also a student of Marxist theorist Herbert Marcuse, whom Hoffman said had a profound effect on his political outlook. Hoffman would later cite Marcuse's influence during his activism and his theories on revolution. He was on the Brandeis tennis team, which was coached by journalist Bud Collins. Hoffman graduated with a B.A. in psychology in 1959. That fall, he enrolled at the University of California, Berkeley, where he completed coursework toward a master's degree in psychology. Soon after, he married his pregnant girlfriend Sheila Karklin in May 1960.
Early protests.
Prior to his days as a leading member of the Yippie movement, Hoffman was involved with the Student Nonviolent Coordinating Committee (SNCC), and organized Liberty House, which sold items to support the Civil Rights Movement in the southern United States. During the Vietnam War, Hoffman was an anti-war activist, using deliberately comical and theatrical tactics.
In late 1966, Hoffman met with a radical community-action group called the Diggers and studied their ideology. He later returned to New York and published a book with this knowledge. Doing so was considered a violation by the Diggers. Diggers co-founder Peter Coyote explained:
One of Hoffman's well-known stunts was on August 24, 1967, when he led members of the movement to the gallery of the New York Stock Exchange (NYSE). The protesters threw fistfuls of real and fake dollar bills down to the traders below, some of whom booed, while others began to scramble frantically to grab the money as fast as they could.
In October 1967, David Dellinger of the National Mobilization Committee to End the War in Vietnam asked Jerry Rubin to help mobilize and direct a March on the Pentagon. The protesters gathered at the Lincoln Memorial as Dellinger and Dr. Benjamin Spock gave speeches to the mass of people. From there, the group marched towards the Pentagon. As the protesters neared the Pentagon, they were met by soldiers of the 82nd Airborne Division who formed a human barricade blocking the Pentagon steps. Not to be dissuaded, Hoffman vowed to levitate the Pentagon claiming he would attempt to use psychic energy to levitate the Pentagon until it would turn orange and begin to vibrate, at which time the war in Vietnam would end. Allen Ginsberg led Tibetan chants to assist Hoffman.
Hoffman's theatrics were successful at convincing many young people to become more active in the politics of the time.
Chicago Eight conspiracy trial.
Hoffman was arrested and tried for conspiracy and inciting to riot as a result of his role in anti-Vietnam War protests, which were met by a violent police response during the 1968 Democratic National Convention in Chicago. He was among the group that came to be known as the Chicago Seven (originally known as the Chicago Eight), which included fellow Yippie Jerry Rubin, David Dellinger, Rennie Davis, John Froines, Lee Weiner, future California state senator Tom Hayden and Black Panther Party co-founder Bobby Seale (before his trial was severed from the others).
Presided over by Judge Julius Hoffman (no relation to Hoffman, about which he joked throughout the trial), Abbie Hoffman's courtroom antics frequently grabbed the headlines; one day, defendants Hoffman and Rubin appeared in court dressed in judicial robes, while on another day, Hoffman was sworn in as a witness with his hand giving the finger. Judge Hoffman became the favorite courtroom target of the Chicago Seven defendants, who frequently would insult the judge to his face. Abbie Hoffman told Judge Hoffman "you are a 'shande fur de Goyim' in front of the gentiles. You would have served Hitler better." He later added that "your idea of justice is the only obscenity in the room." Both Davis and Rubin told the Judge "this court is bullshit." When Hoffman was asked in what state he resided, he replied the "state of mind of my brothers and sisters".
Other celebrities were called as "cultural witnesses" including Allen Ginsberg, Phil Ochs, Arlo Guthrie, Norman Mailer and others. Hoffman closed the trial with a speech in which he quoted Abraham Lincoln, making the claim that the President himself, if alive today, would also be arrested in Chicago's Lincoln Park.
On February 18, 1970, Hoffman and four of the other defendants (Rubin, Dellinger, Davis, and Hayden) were found guilty of intent to incite a riot while crossing state lines. All seven defendants were found not guilty of conspiracy. At sentencing, Hoffman suggested the judge try LSD and offered to set him up with "a dealer he knew in Florida" (the judge was known to be headed to Florida for a post-trial vacation). Each of the five was sentenced to five years in prison and a $5,000 fine.
However, all convictions were subsequently overturned by the Seventh Circuit Court of Appeals. The Walker Commission later found that in fact it had been a "police riot."
Controversy at Woodstock.
At Woodstock in 1969, Hoffman reportedly interrupted The Who's performance to attempt to speak against the jailing of John Sinclair of the White Panther Party. He grabbed a microphone and yelled, "I think this is a pile of shit while John Sinclair rots in prison ..." Pete Townshend was adjusting his amplifier between songs and turned to look at Hoffman over his left shoulder. Townshend shouted "Fuck off! Fuck off my stage!" and reportedly ran at Hoffman with his guitar and hit Hoffman in the back, although Townshend later denied attacking Hoffman. Townshend later said that while he actually agreed with Hoffman on Sinclair's imprisonment, he would have knocked him offstage regardless of the content of his message, given that Hoffman had violated the "sanctity of the stage," i.e., the right of the band to perform uninterrupted by distractions not relevant to the show. The incident took place during a camera change, and was not captured on film. The audio of this incident, however, can be heard on The Who's box set, "Thirty Years of Maximum R&B" (Disc 2, Track 20, "Abbie Hoffman Incident").
In 1971's "Steal This Book" in the section "Free Communication," Hoffman encourages his readership to take to the stage at rock concerts to use the pre-assembled audience and PA system to get their message out. However he mentions that "interrupting the concert is frowned upon since it is only spitting in the faces of people you are trying to reach."
In "Woodstock Nation", Hoffman mentions the incident, and says he was on a bad LSD trip at the time. Joe Shea, then a reporter for the "Times Herald-Record", a Dow Jones-Ottaway newspaper that covered the event on-site, said he saw the incident. He recalled that Hoffman was actually hit in the back of the head by Townshend's guitar and toppled directly into the pit in front of the stage. He does not recall any "shove" from Townshend, and discounts both men's accounts.
Underground.
In 1971, Hoffman published "Steal This Book", which advised readers on how to live basically for free. Many of his readers followed Hoffman's advice and stole the book, leading many bookstores to refuse to carry it. He was also the author of several other books, including "Vote!", co-written with Rubin and Ed Sanders. Hoffman was arrested August 28, 1973 on drug charges for intent to sell and distribute cocaine. He always maintained that undercover police agents entrapped him into a drug deal and planted suitcases of cocaine in his office. In the spring of 1974, Hoffman skipped bail, underwent cosmetic surgery to alter his appearance, and hid from authorities for several years.
Some believed Hoffman made himself a target. In 1998, Peter Coyote opined:
Despite being "in hiding" during part of this period (Hoffman lived in Fineview, New York near Thousand Island Park, a private resort on Wellesley Island on the St. Lawrence River under the name "Barry Freed"), he helped coordinate an environmental campaign to preserve the Saint Lawrence River (Save the River organization). During his time on the run, he was also the "travel" columnist for "Crawdaddy!" magazine. On September 4, 1980, he surrendered to authorities; on the same date, he appeared on a pre-taped edition of ABC-TV's "20/20" in an interview with Barbara Walters. Hoffman received a one-year sentence, but was released after four months.
Back to visibility.
In November 1986, Hoffman was arrested along with 14 others, including Amy Carter, the daughter of former President Jimmy Carter, for trespassing at the University of Massachusetts Amherst. The charges stemmed from a protest against the Central Intelligence Agency's recruitment on the UMass campus. Since the university's policy limited campus recruitment to law-abiding organizations, the defense argued that the CIA engaged in illegal activities. The federal district court judge permitted expert witnesses, including former Attorney General Ramsey Clark and a former CIA agent who testified that the CIA carried on an illegal Contra war against the Sandinista regime in Nicaragua in violation of the Boland Amendment.
In three days of testimony, more than a dozen defense witnesses, including Daniel Ellsberg, and former Contra leader Edgar Chamorro, described the CIA's role in more than two decades of covert, illegal and often violent activities. In his closing argument, Hoffman, acting as his own attorney, placed his actions within the best tradition of American civil disobedience. He quoted from Thomas Paine, "the most outspoken and farsighted of the leaders of the American Revolution: 'Every age and generation must be as free to act for itself, in all cases, as the ages and generations which preceded it. Man has no property in man, neither has any generation a property in the generations which are to follow.'"
Hoffman concluded: "Thomas Paine was talking about this Spring day in this courtroom. A verdict of not guilty will say, 'When our country is right, keep it right; but when it is wrong, right those wrongs.'" On April 15, 1987, the jury found Hoffman and the other defendants not guilty.
After his acquittal, Hoffman acted in a cameo appearance in Oliver Stone's later-released anti-Vietnam War movie, "Born on the Fourth of July." He essentially played himself in the movie, waving a flag on the ramparts of an administration building during a campus protest that was being teargassed and crushed by state troopers.
In 1987 Hoffman summed up his views.
Later that same year, Hoffman and Jonathan Silvers wrote "Steal This Urine Test" (published October 5, 1987), which exposed the internal contradictions of the War on Drugs and suggested ways to circumvent its most intrusive measures. He stated, for instance, that Federal Express, which received high praise from management guru Tom Peters for "empowering" workers, in fact subjected most employees to random drug tests, firing any who got a positive result, with no retest or appeal procedure, despite the fact that FedEx chose a drug lab (the lowest bidder) with a proven record of frequent false positive results.
Stone's "Born on the Fourth of July" was released on December 20, 1989, more than eight months after Hoffman's suicide on April 12, 1989. At the time of his death, Hoffman was at the height of a renewed public visibility, one of the few 1960s radicals who still commanded the attention of all kinds of mass media. He regularly lectured audiences about the CIA's covert activities, including assassinations disguised as suicides. His "Playboy" article (October, 1988) outlining the connections that constitute the "October Surprise," brought that alleged conspiracy to the attention of a wide-ranging American readership for the first time.
Personal life.
In 1960, Hoffman married Sheila Karklin and had two children: Andrew (born 1960) and Amy (1962–2007), who later went by the name Ilya. They divorced in 1966.
In 1967, Hoffman married Anita Kushner in Manhattan's Central Park. They had one son, Hoffman, deliberately named using a lowercase "a" to indicate both patriotism and non-jingoistic intent. Although Hoffman and Kushner were effectively separated after Hoffman became a fugitive, starting in 1973, they were not formally divorced until 1980. He subsequently fell in love with Johanna Lawrenson in 1974, while a fugitive.
His personal life drew a great deal of scrutiny from the Federal Bureau of Investigation. By their own admission, they kept a file on him that was 13,262 pages long.
Death.
Hoffman was 52 at the time of his death on April 12, 1989, which was caused by swallowing 150 phenobarbital tablets and liquor. He had been diagnosed with bipolar disorder in 1980. At the time he had recently changed treatment medications and was reportedly depressed when his 83-year-old mother was diagnosed with cancer (she died in 1996 at the age of 90). Some close to Hoffman claimed that as a natural prankster who valued youth, he was also unhappy about reaching middle age, combined with the fact that the ideas of the 1960s had given way to a conservative backlash in the 1980s. In 1984 he had expressed dismay that the current generation of young people were not as interested in protesting and social activism as youth had been during the 1960s. Hoffman's body was found in his apartment in a converted turkey coop on Sugan Road in Solebury Township, near New Hope, Pennsylvania. At the time of his death, he was surrounded by about 200 pages of his own handwritten notes, many about his own moods.
His death was officially ruled as suicide. As reported by "The New York Times," "Among the more vocal doubters at the service today was Mr. Dellinger, who said, 'I don't believe for one moment the suicide thing.' He said he had been in fairly frequent touch with Mr. Hoffman, who had 'numerous plans for the future.'" Yet the same "New York Times" article reported that the coroner found the residue of about 150 pills and quoted the coroner in a telephone interview saying "There is no way to take that amount of phenobarbital without intent. It was intentional and self-inflicted."
A week after Hoffman's death, a thousand friends and relatives gathered for a memorial in Worcester, Massachusetts at Temple Emanuel, the synagogue he attended as a child. Two of his colleagues from the Chicago Seven conspiracy trial were there: David Dellinger and Jerry Rubin, Hoffman's co-founder of the Yippies, by then a businessman.
As "The New York Times" reported: "Indeed, most of the mourners who attended the formal memorial at Temple Emanuel here were more yuppie than yippie and there were more rep ties than ripped jeans among the crowd..."
The "Times" report continued:
Bill Walton, the radical Celtic of basketball renown, told of a puckish Abbie, then underground evading a cocaine charge in the '70s, leaping from the shadows on a New York street to give him an impromptu basketball lesson after a loss to the Knicks. 'Abbie was not a fugitive from justice,' said Mr. Walton. 'Justice was a fugitive from him.' On a more traditional note, Rabbi Norman Mendell said in his eulogy that Mr. Hoffman's long history of protest, antic though much of it had been, was 'in the Jewish prophetic tradition, which is to comfort the afflicted and afflict the comfortable.'
Media.
Appearances in documentary films.
Hoffman is featured in interviews and archival news footage in the following documentaries:
Legacy.
Theatre Festival.
The Mary-Archie Theatre Company in Chicago started the "Abbie Hoffman Died For Our Sins" Theatre Festival in 1988. This festival runs every year for 3 consecutive days as a celebration of the Woodstock Music and Art Fair of 1969.
Festival website

</doc>
<doc id="49582" url="https://en.wikipedia.org/wiki?curid=49582" title="Battle of Lewes">
Battle of Lewes

The Battle of Lewes was one of two main battles of the conflict known as the Second Barons' War. It took place at Lewes in Sussex, on 14 May 1264. It marked the high point of the career of Simon de Montfort, 6th Earl of Leicester, and made him the "uncrowned King of England". Henry III left the safety of Lewes Castle and St. Pancras Priory to engage the Barons in battle and was initially successful, his son Prince Edward routing part of the baronial army with a cavalry charge. However Edward pursued his quarry off the battlefield and left Henry's men exposed. Henry was forced to launch an infantry attack up Offham Hill where he was defeated by the barons' men, defending the hilltop. The royalists fled back to the castle and priory and the King was forced to sign the Mise of Lewes, ceding many of his powers to Montfort.
Background.
Henry III was an unpopular monarch due to his autocratic style, displays of favouritism and his refusal to negotiate with his barons. The barons eventually imposed a constitutional reform known as the Provisions of Oxford upon Henry that called for a thrice-yearly meeting led by Simon de Montfort to discuss matters of government. Henry sought to escape the restrictions of the provisions and applied to Louis IX of France to arbitrate in the dispute. Louis agreed with Henry and annulled the provisions. Montfort was angered by this and rebelled against the King along with other barons in the Second Barons' War. 
The war was not initially openly fought, each side toured the country to raise support for their army. By May the King's force had reached Lewes where they intended to halt for a while to allow reinforcements to reach them. The King encamped at St. Pancras Priory with a force of infantry, but his son, Prince Edward (later King Edward I), commanded the cavalry at Lewes Castle to the north. De Montfort approached the King with the intention of negotiating a truce or failing that to draw him into open battle. The King rejected the negotiations and de Montfort moved his men from Fletching to Offam Hill, a mile to the north-west of Lewes, in a night march that surprised the royalist forces.
Deployment.
The royalist army was up to twice the size of de Montfort's. Henry held command of the centre, with Prince Edward, William de Valence, 1st Earl of Pembroke, and John de Warenne, 6th Earl of Surrey, on the right; and Richard, 1st Earl of Cornwall, and his son, Henry of Almain, on the left. The barons held the higher ground, overlooking Lewes and had ordered their men to wear white crosses as a distinguishing emblem. De Montfort split his forces into four parts, giving his son, Henry de Montfort command of one quarter; Gilbert de Clare with John FitzJohn and William of Montchensy another; a third portion consisting of Londoners was placed under Nicholas de Segrave whilst de Montfort himself led the fourth quarter with Thomas of Pelveston.
Battle.
The baronial forces commenced the engagement with a surprise dawn attack on foragers sent out from the royalist forces. The King then made his move. Edward led a cavalry charge against Seagrave's Londoners, placed on the left of the baronial line, that caused them to break and run to the village of Offham. Edward pursued his foe for some four miles, leaving the King unsupported. Henry was forced to launch an attack with his centre and right divisions straight up Offham Hill into the baronial line which awaited them at the defensive. Cornwall's division faltered almost immediately but Henry's men fought on until compelled to retreat by the arrival of de Montfort's men that had been held as the baronial reserve.
The King's men were forced down the hill and into Lewes where they engaged in a fighting retreat to the castle and priory. Edward returned with his weary cavalrymen and launched a counterattack but upon locating his father was persuaded that, with the town ablaze and many of the King's supporters having fled, it was time to accept de Montfort's renewed offer of negotiations. The Earl of Cornwall was captured by the barons when he was unable to reach the safety of the priory and, being discovered in a windmill, was taunted with cries of "Come down, come down, thou wicked miller."
Aftermath.
The King was forced to sign the so-called Mise of Lewes. Though the document has not survived, it is clear that Henry was forced to accept the Provisions of Oxford, while Prince Edward remained a hostage of the barons. This put Montfort in a position of ultimate power, which would last until Prince Edward's escape, and Montfort's subsequent defeat at the Battle of Evesham in August 1265.
In 1994, an archaeological survey of the cemetery of St Nicholas Hospital, in Lewes, revealed the remains of bodies that were thought to be combatants from the battle of Lewes. However, in 2014 it was revealed that some of the skeletons may actually be much older with a skeleton known as "skeleton 180", being contemporary with the Norman invasion.
Location.
There remains some uncertainty over the location of the battle with Offham Hill's eastern and lower slopes covered by modern housing. The top and southern slopes remain accessible by footpaths through agricultural land and the ruins of the priory and castle are also open to visitors.

</doc>
<doc id="49583" url="https://en.wikipedia.org/wiki?curid=49583" title="Link awareness">
Link awareness

Link awareness is defined as the ability to discover, view, search and update global hyperlink information about any resource with a URL on the World Wide Web. This global link information is a shared information resource.
Implementing link awareness is difficult. In practice, an implementation only approximates link awareness. There are at least two qualitative axes on which we can classify these implementations.

</doc>
<doc id="49584" url="https://en.wikipedia.org/wiki?curid=49584" title="Stanley Baldwin">
Stanley Baldwin

Stanley Baldwin, 1st Earl Baldwin of Bewdley, (3 August 186714 December 1947) was a British Conservative politician, who dominated the government in his country between the two world wars. Three times Prime Minister, he is the only premier to have served under three monarchs (George V, Edward VIII and George VI).
Baldwin first entered the House of Commons in 1908 as the Member of Parliament for Bewdley. His father Alfred Baldwin had held the seat since 1892, but died in office in 1908, and the younger Baldwin was first selected as a candidate by the local Conservative association, and then acclaimed, holding the seat until his political retirement in 1937. He held government office in the coalition ministry of David Lloyd George. In 1922, Baldwin was one of the prime movers in the withdrawal of Conservative support from Lloyd George; he subsequently became Chancellor of the Exchequer in Andrew Bonar Law's Conservative ministry. Upon Bonar Law's resignation due to health reasons in May 1923, Baldwin became Prime Minister and Conservative Party leader. He called an election on the issue of tariffs and lost the Conservatives' majority, after which Ramsay MacDonald formed a minority Labour government.
After winning the 1924 General Election Baldwin formed his second government, which saw important tenures of office by Sir Austen Chamberlain (Foreign Secretary), Winston Churchill (at the Exchequer) and Neville Chamberlain (Health). That government also saw the General Strike in 1926 and the 1927 Trades Disputes Act to curb the powers of trade unions, although Baldwin was supportive of Labour politicians forming minority governments at Westminster. 
Baldwin narrowly lost the 1929 General Election. and his continued leadership of the party was subject to extensive criticism by the press barons Lord Rothermere and Lord Beaverbrook. In 1931, Labour Prime Minister Ramsay MacDonald formed a National Government, most of whose ministers were Conservatives and which won an enormous majority at the 1931 General Election. As Lord President of the Council, and one of four Conservatives among the small ten-member Cabinet, Baldwin took over many of the Prime Minister's duties due to MacDonald's failing health. This government saw an Act delivering increased self-government for India, a measure opposed by Churchill and by many rank-and-file Conservatives. The Statute of Westminster 1931 gave Dominion status to Canada, Australia, New Zealand and South Africa, while establishing the first step towards the Commonwealth of Nations. As party leader, Baldwin made many striking innovations, such as clever use of radio and film, that made him highly visible to the public and strengthened Conservative appeal.
In 1935, Baldwin replaced MacDonald as Prime Minister of the National Government, and won the 1935 General Election with another large majority. During this time, he oversaw the beginning of the re-armament process of the British military, as well as the very difficult abdication of King Edward VIII. Baldwin's third government saw a number of crises in foreign affairs, including the public uproar over the Hoare-Laval Pact, Hitler's re-occupation of the Rhineland and the outbreak of the Spanish Civil War.
Baldwin retired in 1937 and was succeeded by Neville Chamberlain. At that time, he was regarded as a popular and successful prime minister, but for the final decade of his life, and for many years afterwards, he was vilified for having presided over high unemployment in the 1930s and as one of the "Guilty Men" who had tried to appease Adolf Hitler and who had – supposedly – not rearmed sufficiently to prepare for the Second World War. By 2004, however, historians generally painted a positive portrait of his governments. Stuart Ball says, Baldwin is now seen as having done more than most and perhaps as much as was possible in the context, but the fact remains that it was not enough to deter the aggressors or ensure their defeat. Less equivocal was his rediscovery as a moderate and inclusive Conservative for the modern age, part of a 'one nation tradition'.
This more positive outlook on Baldwin's time as Prime Minister is reflected in evaluations by scholars, where he generally ranked in the upper half of British Prime Ministers.
Early life: family, education and marriage.
Baldwin was born at Lower Park House, Lower Park, Bewdley in Worcestershire, England to Alfred Baldwin and Louisa Baldwin (née MacDonald), and through his Scottish mother was a first cousin of the writer and poet Rudyard Kipling, with whom he was close for their entire lives. The family was prosperous, and owned the eponymous iron and steel making business that in later years became part of Richard Thomas and Baldwins.
Baldwin's schools were St Michael's followed by Harrow School and finally Brighton College. He later wrote that "all the king's horses and all the king's men would have failed to have drawn me into the company of school masters, and in relation to them I once had every qualification as a passive resister." Baldwin then went on to the University of Cambridge, where he studied history at Trinity College. His time at university was blighted by the presence, as Master of Trinity, of Montagu Butler, his former headmaster who had punished him at Harrow for writing a piece of schoolboy smut. He was asked to resign from the Magpie & Stump (the Trinity College debating society) for never speaking, and, after receiving a third-class degree in history, he went into the family business of iron manufacturing. His father sent him to Mason College (the future University of Birmingham) for one session of technical training in metallurgy as preparation. As a young man he served briefly as a Second Lieutenant in the Artillery Volunteers at Malvern, and in 1897 became a JP for the county of Worcestershire.
Baldwin married Lucy Ridsdale on 12 September 1892. The couple had six children.
Baldwin proved to be adept as a businessman, and acquired a reputation as a modernising industrialist. He inherited £200,000 and a directorship of the Great Western Railway on the death of his father in 1908.
Early political career.
In the 1906 general election he contested Kidderminster but lost amidst the Conservative landslide defeat after the party split on the issue of free trade. In a by-election in 1908 he was elected Member of Parliament (MP) for Bewdley in which role he succeeded his father, who had died earlier that year. During the First World War he became Parliamentary Private Secretary to the party leader Andrew Bonar Law, and in 1917 he was appointed to the junior ministerial post of Financial Secretary to the Treasury where he sought to encourage voluntary donations by the rich to repay the United Kingdom's war debt, writing letters to "The Times" under the pseudonym 'FST', much of which were published. He relinquished to the Treasury one fifth of his own fortune, estimated at own account as £580,000, held in the form of war loan stock worth £120,000.
Joins Cabinet.
He served jointly with Sir Hardman Lever, who had been appointed in 1916, but after 1919 Baldwin carried out the duties largely alone. He was appointed to the Privy Council in the 1920 Birthday Honours. In 1921 he was promoted to the Cabinet as President of the Board of Trade.
Chancellor of the Exchequer.
In late 1922 dissatisfaction was steadily growing within the Conservative Party over its coalition with the Liberal David Lloyd George. At a meeting of Conservative MPs at the Carlton Club in October, Baldwin announced that he would no longer support the coalition, and famously condemned Lloyd George for being a "dynamic force" that was bringing destruction across politics. The meeting chose to leave the coalition, against the wishes of most of the party leadership. As a direct result Bonar Law was forced to search for new ministers for a Cabinet which he would lead, and so promoted Baldwin to the position of Chancellor of the Exchequer. In the November 1922 general election the Conservatives were returned with a majority in their own right.
Prime Minister: First term (1923–1924).
In May 1923 Bonar Law was diagnosed with terminal cancer and retired immediately; he died five months later. With many of the party's senior leading figures standing aloof and outside of the government, there were only two candidates to succeed him: Lord Curzon, the Foreign Secretary, and Baldwin. The choice formally fell to King George V acting on the advice of senior ministers and officials.
It is not entirely clear what factors proved most crucial, but some Conservative politicians felt that Curzon was unsuitable for the role of Prime Minister because he was a member of the House of Lords (though this did not stop other lords running credible campaigns for the position until World War II). Curzon was strong and experienced in international affairs, but his lack of experience in domestic affairs, his personal character (found objectionable), and his substantial inherited wealth and many directorships at a time when the Conservative Party was seeking to shed its patrician image were all deemed impediments. Much weight at the time was given to the intervention of Arthur Balfour.
The King turned to Baldwin to become Prime Minister. Initially Baldwin was also Chancellor of the Exchequer whilst he sought to recruit the former Liberal Chancellor Reginald McKenna to join the government. When this failed he appointed Neville Chamberlain to that position.
The Conservatives now had a clear majority in the House of Commons and could govern for five years before holding a general election, but Baldwin felt bound by Bonar Law's pledge at the previous election that there would be no introduction of tariffs without a further election. Thus Baldwin turned towards a degree of classical Tory protectionism which would remain a key party message during his lifetime. With the country facing growing unemployment in the wake of free trade imports driving down prices and profits, Baldwin decided to call an early general election in December 1923 to seek a mandate to introduce protectionist tariffs which, he hoped, would drive down unemployment and spur an economic recovery. Protectionism was a divisive issue in the Conservative Party in 1923: "one must speak of the election being fought by a divided party."
The election outcome was inconclusive: the Conservatives had 258 MPs, Labour 191 and the reunited Liberals 159. Whilst the Conservatives retained a plurality in the House of Commons, they had been clearly defeated on the central issue: tariffs. Baldwin remained Prime Minister until the opening session of the new Parliament in January 1924, at which time the government was defeated in a motion of confidence vote. He resigned immediately.
Leader of the Opposition.
Baldwin successfully held on to the party leadership amid some colleagues' calls for his resignation. For the next ten months, an unstable minority Labour government under Prime Minister Ramsay MacDonald held office. On 13 March 1924, the Labour government was defeated for the first time in the Commons, although the Conservatives decided to vote with Labour later that day against the Liberals.
During a debate on the naval estimates the Conservatives opposed Labour but supported them on 18 March in a vote on cutting expenditure on the Singapore military base. Baldwin also cooperated with MacDonald over Irish policy to stop it becoming a party-political issue.
The Labour government was negotiating with the Soviet government over intended commercial treaties -- 'the Russian Treaties' -- to provide most favoured nation privileges and diplomatic status for the UK trade delegation; and a treaty that would settle the claims of pre-revolutionary British bondholders and holders of confiscated property, after which the British government would guarantee a loan to the Soviet Union. Baldwin decided to vote against the government over the Russian Treaties, which brought the government down on 8 October.
The general election held in October 1924 brought a landslide majority of 223 for the Conservative party, primarily at the expense of an unpopular Liberal Party. Baldwin campaigned on the "impracticability" of socialism, the Campbell Case, the Zinoviev Letter (which Baldwin thought was genuine) and the Russian Treaties. In a speech during the campaign Baldwin said:
It makes my blood boil to read of the way which Mr. Zinoviev is speaking of the Prime Minister today. Though one time there went up a cry, "Hands off Russia", I think it's time somebody said to Russia, "Hands off England".
Prime Minister: Second term (1924–1929).
Baldwin's new Cabinet now included many former political associates of Lloyd George: former Coalition Conservatives: Austen Chamberlain (as Foreign Secretary), Lord Birkenhead (Secretary for India) and Arthur Balfour (Lord President after 1925), and the former Liberal Winston Churchill as Chancellor of the Exchequer. This period included the General Strike of 1926, a crisis that the government managed to weather, despite the havoc it caused throughout the UK. Baldwin created the Organisation for the Maintenance of Supplies, a volunteer body of those opposed to the strike which was intended to complete essential work.
At Baldwin's instigation Lord Weir headed a committee to "review the national problem of electrical energy". It published its report on 14 May 1925 and in it Weir recommended the setting up of a Central Electricity Board, a state monopoly half-financed by the Government and half by local undertakings. Baldwin accepted Weir's recommendations and they became law by the end of 1926.
The Board was a success. By 1939 electrical output was up fourfold and generating costs had fallen. Consumers of electricity rose from three-quarters of a million in 1920 to nine million in 1938, with annual growth of 700,000 to 800,000 a year (the fastest rate of growth in the world).
One of his legislative reforms was a paradigm shift in his party. This was the Widows, Orphans and Old Age Contributory Pensions Act of 1925, which provided a pension of 10 shillings a week for widows with extra for children, and 10 shillings a week for insured workers and their wives at 65. This transformed Toryism, away from its historic reliance on community (particularly religious) charities, and towards acceptance of a humanitarian welfare state which would guarantee a minimum living standard for those unable to work or who took out national insurance. In 1927, he was made a Fellow of the Royal Society.
Leader of the Opposition.
In 1929 Labour returned to office, the largest party in the House of Commons (although without an overall majority) despite obtaining fewer votes than the Conservatives. In opposition, Baldwin was almost ousted as party leader by the press barons Lords Rothermere and Beaverbrook, whom he accused of enjoying "power without responsibility, the prerogative of the harlot throughout the ages".
Ramsden argues that Baldwin made dramatic permanent improvements to the organisation and effectiveness of the Conservative party. He enlarged the headquarters with professionals, professionalised the party agents, raised ample funds, and was an innovative user of the new mass media of radio and film.
Lord President of the Council.
By 1931, as the economy headed towards crisis, both in Britain and around the world, with the onset of the Great Depression, Baldwin and the Conservatives entered into a coalition with Labour Prime Minister Ramsay MacDonald. This decision led to MacDonald's expulsion from his own party, and Baldwin, as Lord President of the Council became "de facto" Prime Minister deputising for the increasingly senile MacDonald, until he once again officially became Prime Minister in 1935. 
One central and vitally important agreement was the Statute of Westminster 1931, which conferred full self-government upon the Dominions Canada, South Africa, Australia and New Zealand, while preparing the first steps towards the eventual Commonwealth of Nations, and away from the designation 'British Empire'. In 1930, the first British Empire Games sports competition was held successfully among Empire nations in Hamilton, Canada; this would eventually evolve into the present-day Commonwealth Games, held every four years, and in effect became a 'Summer Olympics' for these affiliated nations.
His government then secured with great difficulty the passage of the landmark Government of India Act 1935, in the teeth of opposition from Winston Churchill, whose views enjoyed much support among rank-and-file Conservatives.
Disarmament.
Baldwin did not advocate total disarmament but believed that, as Sir Edward Grey had stated in 1925, "great armaments lead inevitably to war". However he came to believe that, as he put it on 9 November 1932: "the time has now come to an end when Great Britain can proceed with unilateral disarmament". On 10 November 1932 Baldwin said:
I think it is well also for the man in the street to realise that there is no power on earth that can protect him from being bombed. Whatever people may tell him, the bomber will always get through, The only defence is in offence, which means that you have to kill more women and children more quickly than the enemy if you want to save yourselves...If the conscience of the young men should ever come to feel, with regard to this one instrument that it is evil and should go, the thing will be done; but if they do not feel like that – well, as I say, the future is in their hands. But when the next war comes, and European civilisation is wiped out, as it will be, and by no force more than that force, then do not let them lay blame on the old men. Let them remember that they, principally, or they alone, are responsible for the terrors that have fallen upon the earth.
This speech was often used against Baldwin as allegedly demonstrating the futility of rearmament or disarmament, depending on the critic.
With the second part of the Disarmament Conference starting in January 1933, Baldwin attempted to see through his hope of air disarmament. However he became alarmed at Britain's lack of defence against air raids and German rearmament, saying it "would be a terrible thing, in fact, the beginning of the end". In April 1933 the Cabinet agreed to follow through with the construction of the Singapore military base.
On 15 September 1933 the German delegate at the Disarmament Conference refused to return to the Conference and Germany left altogether in October. On 6 October Baldwin, in a speech to the Conservative Party Conference in Birmingham, pleaded for a Disarmament Convention and then said:
when I speak of a Disarmament Convention I do not mean disarmament on the part of this country and not on the part of any other. I mean the limitation of armaments as a real limitation...and if we find ourselves on some lower rating and that some other country has higher figures, that country has to come down and we have to go up until we meet.
On 14 October Germany left the League of Nations. The Cabinet decided on 23 October that Britain should still attempt to cooperate with other states, including Germany, in international disarmament. However between mid-September 1933 and the beginning of 1934 Baldwin's mind changed from hoping for disarmament to favouring rearmament, including parity in aircraft. In late 1933 and early 1934 he rejected an invitation from Hitler to meet him, believing that visits to foreign capitals were the job of Foreign Secretaries. On 8 March 1934 Baldwin defended the creation of four new squadrons for the Royal Air Force against Labour criticisms and said of international disarmament:
If all our efforts for an agreement fail, and if it is not possible to obtain this equality in such matters as I have indicated, then any Government of this country—a National Government more than any, and "this" Government—will see to it that in air strength and air power this country shall no longer be in a position inferior to any country within striking distance of our shores.
On 29 March 1934 Germany published its defence estimates' which showed a total increase of one-third and an increase of 250% in its air force.
A series of by-elections with massive swings against government candidates—most famous was Fulham East with a 26.5% swing—in late 1933 and early 1934 convinced Baldwin that the British public was profoundly pacifist. Baldwin also rejected the "belligerent" views of those like Churchill and Robert Vansittart because he believed that the Nazis were rational men who would appreciate the logic of mutual and equal deterrence. He also believed war to be "the most fearful terror and prostitution of man's knowledge that ever was known".
Prime Minister: Third term: (1935–1937).
With MacDonald's physical powers failing him, he and Baldwin changed places in June 1935; Baldwin was now Prime Minister, MacDonald Lord President of the Council. In October that year Baldwin called a general election. Neville Chamberlain advised Baldwin to make rearmament the leading issue in the election campaign against Labour, saying that, if a rearmament programme were not announced until after the election, his government would be seen as having deceived the people. However, Baldwin did not make rearmament the central issue in the election. He said he would support the League of Nations, modernise Britain's defences and remedy deficiencies but also said: "I give you my word that there will be no great armaments". The main issues in the election were housing, unemployment and the special areas of economic depression. The election gave 430 seats to National government supporters (386 of these Conservative) and 154 seats to Labour.
Rearmament.
Baldwin's younger son A. Windham Baldwin, writing in 1955, argued that his father Stanley planned a rearmament programme as early as 1934, but had to do so quietly to avoid antagonizing the pacifistic public revealed by the Peace Ballot of 1934–35 and endorsed by both the Labour and the Liberal oppositions. His thorough presentation of the case for rearmament in 1935, the son argues, defeated pacifism and secured a victory that allowed rearmament to move ahead.
On 31 July 1934, the Cabinet approved a report that called for expansion of the Royal Air Force to the 1923 standard by creating 40 new squadrons over the following five years. On 26 November 1934, six days after receiving the news that the German air force would be as large as the RAF within one year, the Cabinet decided to speed up air rearmament from four years to two. On 28 November 1934 Churchill moved an amendment to the vote of thanks for the King's Speech, which read: "...the strength of our national defences, and especially our air defences, is no longer adequate". His motion was known eight days before it was moved, and a special Cabinet meeting decided how to deal with this motion; it dominated two other Cabinet meetings. Churchill said Germany was rearming; requested that the money spent on air armaments be doubled or tripled in order to deter an attack; and that the "Luftwaffe" was nearing equality with the RAF. Baldwin responded by denying that the Luftwaffe was approaching equality and that it was "not 50 per cent" of the RAF. He added that by the end of 1935 the RAF would still have "a margin of nearly 50 per cent" in Europe. After Baldwin said the government would ensure the RAF had parity with the future German air force Churchill withdrew his amendment. In April 1935 the Air Secretary reported that although Britain's strength in the air would be ahead of Germany for at least three years, air rearmament needed to be increased so the Cabinet agreed to the creation of an extra 39 squadrons for home defence by 1937. However, on 8 May 1935 the Cabinet heard that it was estimated that the RAF was inferior to the Luftwaffe by 370 aircraft and that in order to reach parity the RAF must have 3,800 aircraft by April 1937—an extra 1,400 on the existing air programme. It was learnt that Germany was easily able to outbuild this revised programme as well. On 21 May 1935, the Cabinet agreed to expanding the home defence force of the RAF to 1,512 aircraft (840 bombers and 420 fighters). On 22 May 1935 Baldwin confessed in the Commons: "I was wrong in my estimate of the future. There I was completely wrong."
On 25 February 1936, the Cabinet approved a report calling for expansion of the Royal Navy and the re-equipment of the British Army (though not its expansion), along with the creation of "shadow factories" built by public money and managed by industrial companies. These factories came into operation in 1937. In February 1937 the Chiefs of Staff reported that by May 1937 the Luftwaffe would have 800 bombers compared to the RAF's 48.
In the debate in the Commons on 12 November 1936, Churchill attacked the government on rearmament as being "decided only to be undecided, resolved to be irresolute, adamant for drift, solid for fluidity, all-powerful to be impotent. So we go on, preparing more months and years – precious, perhaps vital, to the greatness of Britain – for the locusts to eat". Baldwin replied:
I put before the whole House my own views with an appalling frankness. From 1933, I and my friends were all very worried about what was happening in Europe. You will remember at that time the Disarmament Conference was sitting in Geneva. You will remember at that time there was probably a stronger pacifist feeling running through the country than at any time since the War. I am speaking of 1933 and 1934. You will remember the election at Fulham in the autumn of 1933...That was the feeling of the country in 1933. My position as a leader of a great party was not altogether a comfortable one. I asked myself what chance was there...within the next year or two of that feeling being so changed that the country would give a mandate for rearmament? Supposing I had gone to the country and said that Germany was rearming and we must rearm, does anybody think that this pacific democracy would have rallied to that cry at that moment! I cannot think of anything that would have made the loss of the election from my point of view more certain...We got from the country – with a large majority – a mandate for doing a thing that no one, twelve months before, would have believed possible.
Churchill wrote to a friend: "I have never heard such a squalid confession from a public man as Baldwin offered us yesterday". In 1935 Baldwin wrote to J. C. C. Davidson (in a letter now lost) saying of Churchill: "If there is going to be a war – and no one can say that there is not – we must keep him fresh to be our war Prime Minister". Thomas Dugdale also claimed Baldwin said to him: "If we do have a war, Winston must be Prime Minister. If he is in Cabinet now we shan't be able to engage in that war as a united nation". The General Secretary of the Trades Union Congress, Walter Citrine, recalled a conversation he had had with Baldwin on 5 April 1943: "Baldwin thought his [Churchill's] political recovery was marvellous. He, personally, had always thought that if war came Winston would be the right man for the job".
The Labour Party strongly opposed the rearmament programme. Clement Attlee said on 21 December 1933: "For our part, we are unalterably opposed to anything in the nature of rearmament". On 8 March 1934 Attlee said, after Baldwin defended the Air Estimates, "we on our side are out for total disarmament". On 30 July 1934 Labour moved a motion of censure against the government because of its planned expansion of the RAF. Attlee spoke for it: "We deny the need for increased air arms...and we reject altogether the claim of parity". Sir Stafford Cripps also said on this occasion that it was fallacy that Britain could achieve security through increasing air armaments. On 22 May 1935, the day after Hitler had made a speech claiming that German rearmament offered no threat to peace, Attlee asserted that Hitler's speech gave "a chance to call a halt in the armaments race". Attlee also denounced the Defence White Paper of 1937: "I do not believe the Government are going to get any safety through these armaments".
Abdication of Edward VIII.
The accession of King Edward VIII, and the ensuing abdication crisis, brought Baldwin's last major test in office. The new monarch was "an ardent exponent of the cause of Anglo-German understanding", and had "strong views on his right to intervene in affairs of state," but the "Government's main fears ... were of indiscretion." The King proposed to marry Wallis Simpson, an American woman who was twice divorced. The high-minded Baldwin felt that he could tolerate her as "a respectable whore," so long as she stayed behind the throne, but not as "Queen Wally". Mrs. Simpson was also distrusted by the government for her known pro-German sympathies, and she was believed to be in "close contact with German monarchist circles".
During October through November 1936, Baldwin joined the royal family in trying to dissuade the King from that marriage, arguing that the idea of having a twice-divorced woman as the Queen would be rejected by the government, by the country, and by the Empire; and that "the voice of the people must be heard." As the public standing of the King would be gravely compromised, the Prime Minister gave him time to reconsider the notion of this marriage. According to the historian Philip Williamson, "The offence lay in the implications of King's attachment to Mrs. Simpson for the broader public morality and the constitutional integrity which were now perceived—especially by Baldwin—as underpinning the nation's unity and strength."
News of the affair was broken in the papers on 2 December. There was some support for the wishes of the King, especially in and around London. The romantic royalists Churchill, Mosley, and the press barons, Lord Beaverbrook of the "Daily Express" and Lord Rothermere of the "Daily Mail", all declared that the king had a right to marry whichever woman he wished to. This crisis assumed a political dimension when Beaverbrook and Churchill tried to rally support for the marriage in the Parliament. However, the King's party could only muster 40 Members of Parliament in support, and the majority opinion sided with Baldwin and his conservative government. The Labour leader, Clement Attlee, told Baldwin "that while Labour people had no objection to an American becoming Queen, was certain they would not approve of Mrs. Simpson for that position," especially in the provinces and in the Commonwealth countries. The Archbishop of Canterbury, Cosmo Lang, held that the King, as the head of the Church of England, should not marry a divorcée, while "The Times" argued that the monarchy’s prestige would be destroyed if "private inclination were to come into open conflict with public duty and be allowed to prevail."
While some recent critics have complained that "Baldwin refused the reasonable request for time to reflect, preferring to keep the pressure on the King – once again suggesting that his own agenda was to force the crisis to a head," and that he "never mentioned that the alternative the marriage was abdication," the House of Commons immediately and overwhelmingly came out against this marriage. The Labour and Liberal parties, the Trades Union Congress, and the Dominions of Australia and Canada, all joined the British cabinet in rejecting the King's compromise, originally made on 16 November, for a morganatic marriage. The crisis threatened the unity of the British Empire, since the King's personal relationship with the Dominions was their "only remaining constitutional link."
Baldwin still hoped that the King would choose the throne over Mrs. Simpson. For the King to act against the wishes of the cabinet would have precipitated a constitutional crisis. Baldwin would have had to resign, and no other party leader would have served as the Prime Minister under this King, with the Labour Party having already having indicated that it would not form a ministry to uphold impropriety. Baldwin told the Cabinet one Labour MP had asked, "Are we going to have a fascist monarchy?" When the Cabinet refused the morganatic marriage, King Edward decided on his own volition to abdicate.
The King's final plea, on 4 December, that he should broadcast an appeal to the nation, was rejected by the Prime Minister as too divisive. Nevertheless, at his final audience with King Edward on 7 December, Baldwin offered to strive all night with the King's conscience, but he found him to be determined to go. Baldwin announced the King's abdication in the Commons on 10 December. Harold Nicolson, an MP who witnessed Baldwin's speech, wrote in his diary:
There is no moment when he overstates emotion or indulges in oratory. There is intense silence broken only by the reporters in the gallery scuttling away to telephone the speech...When it was over... file out broken in body and soul, conscious that we have heard the best speech that we shall ever hear in our lives. There was no question of applause. It was the silence of Gettysburg...No man has ever dominated the House as he dominated it tonight, and he knows it.
After the speech, the House was adjourned and Nicolson bumped into Baldwin as he was leaving, who asked him what he thought of the speech. Nicolson said it was superb, to which Baldwin replied: "Yes...it was a success. I know it. It was almost wholly unprepared. I had a success, my dear Nicolson, at the moment I most needed it. "Now is the time to go"".
The King abdicated on 11 December, and he was succeeded by his brother, George VI. Edward VIII was assigned the title of the Duke of Windsor by his brother, and then he married Mrs. Simpson in France in June 1937, after her divorce from Ernest Simpson had become final.
Baldwin had defused a political crisis by turning it into a constitutional question. His discreet resolution met with general approval and restored his popularity. He was praised on all sides for his tact and patience, and was not in the least put out by the protestors' cries of "God save the King—from Baldwin!" "Flog Baldwin! Flog him!! We—want—Edward."
Retirement.
Leaving office and peerage.
After the coronation of George VI, Baldwin announced on 27 May 1937 that he would resign the premiership the next day. His last act as Prime Minister was to raise the salaries of MPs from £400 a year to £600 and to give the Leader of the Opposition a salary. This was the first rise in MPs wages since their introduction in 1911 and it particularly benefited Labour MPs. Harold Nicolson wrote in his diary that it "was done with Baldwin's usual consummate taste. No man has ever left in such a blaze of affection". Baldwin was knighted as a Knight of the Garter (KG) on 28 May and ennobled as Earl Baldwin of Bewdley and Viscount Corvedale, "of Corvedale in the County of Salop" on 8 June.
Attitude to appeasement.
Baldwin supported the Munich Agreement and said to Chamberlain on 26 September 1938: "If you can secure peace, you may be cursed by a lot of hotheads but my word you will be blessed in Europe and by future generations". Baldwin made a rare speech in the House of Lords on 4 October where he said he could not have gone to Munich but praised Chamberlain's courage and said the responsibility of a Prime Minister was not to commit the country to war until he was sure that it was ready to fight. If there was a 95% chance of war in the future, he would still choose peace. He also said he would put industry on a war footing tomorrow as the opposition to such a move had disappeared. Churchill said in a speech: "He says he would mobilise tomorrow. I think it would have been much better if Earl Baldwin had said that two and a half years ago when everyone demanded a Ministry of Supply".
Two weeks after Munich, Baldwin said (prophetically) in a conversation with Lord Hinchingbrooke: "Can't we turn Hitler East? Napoleon broke himself against the Russians. Hitler might do the same".
Baldwin's years in retirement were quiet. After Chamberlain's death in 1940, Baldwin's perceived part in pre-war appeasement made him an unpopular figure during and after World War II. With a succession of British military failures in 1940, Baldwin started to receive critical letters: "insidious to begin with, then increasingly violent and abusive; then the newspapers; finally the polemicists who, with time and wit at their disposal, could debate at leisure how to wound the deepest." He did not have a secretary and so was not shielded from the often unpleasant letters sent to him. After a bitterly critical letter was sent to him by a member of the public, Baldwin wrote: "I can understand his bitterness. He wants a scapegoat and the men provided him with one". His biographers Middlemas and Barnes claim that "the men" almost certainly meant the authors of "Guilty Men".
Letter to Lord Halifax.
After Lord Halifax made a speech on the strength of prayer as the instrument which could be invoked by the humblest to use in their country's service, Baldwin wrote to him on 23 July 1940:
With millions of others I had prayed hard at the time of Dunkirk and never did prayer seem to be more speedily answered to the full. And we prayed for France and the next day she surrendered. I thought much, and when I went to bed I lay for a long time vividly awake. And I went over in my mind what had happened, concentrating on the thoughts that you had dwelt on, that prayer to be effective must be in accordance with God's will, and that by far the hardest thing to say from the heart and indeed the last lesson we learn (if we ever do) is to say and mean it, ‘Thy will be done.’ And I thought what mites we all are and how we can never see God's plan, a plan on such a scale that it "must" be incomprehensible. And suddenly for what must have been a couple of minutes I seemed to see with extraordinary and vivid clarity and to hear someone speaking to me. The words at the time were clear, but the recollection of them had passed when I seemed to come to, as it were, but the sense remained, and the sense was this. ‘You cannot see the plan’; then ‘Have you not thought there is a purpose in stripping you one by one of all the human props on which you depend, that you are being left alone in the world? You have now one upon whom to lean and I have chosen you as my instrument to work with my will. Why then are you afraid?’ And to prove ourselves worthy of that tremendous task is our job.
Iron gates crisis.
In September 1941, Baldwin's old enemy, Lord Beaverbrook, asked all local authorities to survey their area's iron and steel railings and gates that could be used for the war effort. Owners of such materials could appeal for an exemption on grounds of artistic or historic merit, which would be decided by a panel set up by local authorities. Baldwin applied for exemption for the iron gates of his country home on artistic grounds and his local council sent an architect to assess them. In December, the architect advised that they be exempt, but, in February 1942, the Ministry of Supply overruled this and said all his gates must go except the ones at the main entrance. A newspaper campaign hounded him for not donating the gates to war production. The "Daily Mirror" columnist "Cassandra" denounced Baldwin:
Here was the country in deadly peril with half the Empire swinging in the wind like a busted barn door hanging on one hinge. Here was Old England half smothered in a shroud crying for steel to cut her way out, and right in the heart of beautiful Worcestershire was a one-time Prime Minister, refusing to give up the gates of his estate to make guns for our defence – and his. Here was an old stupid politician who had tricked the nation into complacency about rearmament for fear of losing an election... Here is the very shrine of stupidity... This National Park of Failure...
There were fears that if the gates were not taken by the proper authorities, "others without authority might". Thus, months before any other collections were made, Baldwin's gates were removed except for those at the main entrance. Two of Beaverbrook's friends after the war claimed that this was Beaverbrook's decision despite Churchill saying, "Lay off Baldwin's gates". At Question Time in the House of Commons the Conservative MP Captain Alan Graham said: "Is the honourable Member aware that it is very necessary to leave Lord Baldwin his gates in order to protect him from the just indignation of the mob?"
Comments on politics.
During the war, Winston Churchill consulted him only once, in February 1943, on the advisability of his speaking out strongly against the continued neutrality of Éamon de Valera's Ireland. Baldwin saw the draft of Churchill's speech and advised against it, which advice Churchill followed. A few months after this visit to Churchill, Baldwin told Harold Nicolson, "I went into Downing Street... a happy man. Of course it was partly because an old buffer like me enjoys feeling that he is still not quite out of things. But it was also pure patriotic joy that my country at such a time should have found such a leader. The furnace of the war has smeltered out all base metals from him". To D. H. Barber, Baldwin wrote of Churchill: "You can take it from me he is a really big man, the War has brought out the best that was in him. His head isn't turned the least little bit by the great position he occupies in the eyes of the world. I pray he is spared to see us through".
In private, Baldwin defended his conduct in the 1930s:
the critics have no historical sense. I have no Cabinet papers by me and do not want to trust my memory. But recall the Fulham election, the peace ballot, Singapore, sanctions, Malta. The English will only learn by example. When I first heard of Hitler, when Ribbentrop came to see me, I thought they were all crazy. I think I brought Ramsay and Simon to meet Ribbentrop. Remember that Ramsay's health was breaking up in the last two years. He had lost his nerve in the House in the last year. I had to take all the important speeches. The moment he went, I prepared for a general election and got a bigger majority for rearmament. No power on earth could have got rearmament without a general election except by a big split. Simon was inefficient. I had to lead the House, keep the machine together with those Labour fellows.
In December 1944, strongly advised by friends, Baldwin decided to respond to criticisms of him through a biographer. He asked G. M. Young, who accepted, and asked Churchill to grant permission to Young to see Cabinet papers. Baldwin wrote:
I am the last person to complain of fair criticism, but when one book after another appears and I am compared, for example, to Laval, my gorge rises; but I am crippled and cannot go and examine the files of the Cabinet Office. Could G. M. Young go on my behalf?
Last years and death.
In June 1945, Baldwin's wife Lucy died. Baldwin himself by now suffered from arthritis and needed a stick to walk. When he made his final public appearance in London in October 1947 at the unveiling of a statue of George V, a crowd of people recognised and cheered him, but by this time he was deaf and asked: "Are they booing me?" Having been made Chancellor of the University of Cambridge in 1930, he continued in this capacity until his death in his sleep at Astley Hall near Stourport-on-Severn, Worcestershire, on 14 December 1947. He was cremated at Golders Green Crematorium and his ashes buried in Worcester Cathedral.
Baldwin was a member of the Oddfellows and Foresters Friendly Society.
Legacy.
Upon his retirement in 1937, he had received a great deal of praise; the onset of World War II would change his public image for the worse. Rightly or wrongly, Baldwin, Chamberlain and MacDonald were held responsible for the United Kingdom's military unpreparedness on the eve of war in 1939. Peter Howard, writing in the "Sunday Express" (3 September 1939), accused Baldwin of deceiving the country of the dangers that faced it in order not to re-arm and so win the 1935 general election. During the ill-fated Battle of France, in May 1940, Lloyd George in conversation with Winston Churchill and General Ironside railed against Baldwin and said "he ought to be hanged". In July 1940, a bestseller "Guilty Men" appeared, which blamed Baldwin for failing to re-arm enough. In May 1941 Hamilton Fyfe wrote an article ("Leadership and Democracy") for "Nineteenth Century and After" which also laid these charges against Baldwin. In 1941, A. L. Rowse criticised Baldwin for lulling the people into a false sense of security; as a practitioner in "the art of taking the people in":
what can this man think in the still watches of the night, when he contemplates the ordeal his country is going through as the result of the years, the locust years, in which he held power?
Churchill firmly believed that Baldwin's conciliatory stance toward Hitler gave the German dictator the impression that Britain would not fight if attacked. Though known for his magnanimity toward political rivals such as Chamberlain, Churchill had none to spare for Baldwin. "I wish Stanley Baldwin no ill," Churchill said when declining to send him 80th birthday greetings in 1947, "but it would have been much better had he never lived." Churchill also believed that Baldwin, rather than Chamberlain, would be most blamed by subsequent generations for the policies that led to "the most unnecessary war in history". An index entry in the first volume of Churchill's "History of the Second World War" ("The Gathering Storm") records Baldwin "admitting to putting party before country" for his alleged admission that he would not have won the 1935 election if he had pursued a more aggressive policy of rearmament. Churchill selectively quoted a speech in the Commons by Baldwin that gave the false impression that Baldwin was speaking of the general election when he was speaking of the Fulham by-election in 1933, and omits Baldwin's actual comments about the 1935 election: "We got from the country, a mandate for doing a thing substantial rearmament programme that no one, twelve months before, would have believed possible". In his speech on Baldwin's death, Churchill paid him a double-edged yet respectful tribute: "He was the most formidable politician I ever encountered in public life".
In 1948, Reginald Bassett published an essay disputing the claim that Baldwin "confessed" to putting party before country, and claimed that Baldwin was referring to 1933/34 when a general election on rearmament would have been lost.
In 1952, G. M. Young published a biography of Baldwin, which Baldwin had asked him to write. He asserted that Baldwin united the nation and helped moderate the policies of the Labour Party. However he accepted the criticism of Baldwin; that he failed to re-arm early enough and that he put party before country. Young contends that Baldwin should have retired in 1935. Churchill and Beaverbrook threatened to sue if certain passages in the biography were not removed or altered. With the help of lawyer Arnold Goodman an agreement was reached to replace the offending sentences, and the publisher Rupert Hart-Davis had the "hideously expensive" job of removing and replacing seven leaves from 7,580 copies.
In response to Young's biography, D. C. Somervell published "Stanley Baldwin: An examination of some features of Mr. G. M. Young's biography" in 1953 with a foreword by Ernest Brown. This attempted to defend Baldwin against the charges made by Young. Both Young and Somervell were criticised by C. L. Mowat in 1955, who claimed they both failed to rehabilitate Baldwin's reputation.
In 1956, Baldwin's son A. W. Baldwin published a biography entitled "My Father: The True Story". It has been written that his son "evidently could not decide whether he was answering the charge of inanition and deceit which grew out of the war, or the radical "dissenters" of the early 1930s who thought the Conservatives were warmongers and denounced them for rearming at all".
In an article written to commemorate the centenary of Baldwin's birth, in "The Spectator" ("Don't Let's Be Beastly to Baldwin", 14 July 1967) Rab Butler defended Baldwin's moderate policies which, he claimed, helped heal social divisions. In 1969 the first major biography of Baldwin appeared, of over 1,000 pages, written by Keith Middlemas and John Barnes, both Conservatives who wished to defend Baldwin.
In 1999, Philip Williamson published a collection of essays on Baldwin which attempted to explain his beliefs and defended his policies as Prime Minister. Williamson asserted that Baldwin had helped create "a moral basis for rearmament in the mid 1930s" that contributed greatly to "the national spirit of defiance after Munich". His defenders counter that the moderate Baldwin felt he could not start a programme of aggressive re-armament without a national consensus on the matter. Certainly, pacifist appeasement was the dominant mainstream political view of the time in Britain, France, and the United States. Williamson admits that there was a clear postwar consensus that repudiated and denigrated all inter-war governments: Baldwin was targeted with the accusation that he had failed to rearm Britain in the 1930s despite Hitler's threat. Williamson says the negative reputation was chiefly the product of partisan politics, the bandwagon of praise for Churchill, selective recollections, and the need for scapegoats to blame for Britain's very close call in 1940. Only during the 1960s did political distance and then the opening of government records lead to more balanced historical assessments; yet the myth had become so central to larger myths about the 1930s and 1940s that it persists as conventional wisdom about the period.
By 2004 Ball could report that among historians, "The pendulum has swung almost completely towards a positive view." He says "Baldwin is now seen as having done more than most and perhaps as much as was possible in the context, but the fact remains that it was not enough to deter the aggressors or ensure their defeat. Less equivocal was his rediscovery as a moderate and inclusive Conservative for the modern age, part of a 'one nation tradition'."
In film, television and literature.
Baldwin has been portrayed in the following film and television productions:

</doc>
<doc id="49588" url="https://en.wikipedia.org/wiki?curid=49588" title="James Longstreet">
James Longstreet

James Longstreet (January 8, 1821January 2, 1904) was one of the foremost Confederate generals of the American Civil War and the principal subordinate to General Robert E. Lee, who called him his "Old War Horse." He served under Lee as a corps commander for many of the famous battles fought by the Army of Northern Virginia in the Eastern Theater, but also with Gen. Braxton Bragg in the Army of Tennessee in the Western Theater. Biographer and historian Jeffry D. Wert wrote that "Longstreet ... was the finest corps commander in the Army of Northern Virginia; in fact, he was arguably the best corps commander in the conflict on either side."
Longstreet's talents as a general made significant contributions to the Confederate victories at Second Bull Run (Second Manassas), Fredericksburg, and Chickamauga, in both offensive and defensive roles. He also performed strongly during the Seven Days Battles, the Battle of Antietam, and until he was seriously wounded, at the Battle of the Wilderness. His performance in semiautonomous command during the Knoxville Campaign resulted in a Confederate defeat. His most controversial service was at the Battle of Gettysburg, where he openly disagreed with General Lee on the tactics to be employed and reluctantly supervised the disastrous infantry assault known as Pickett's Charge.
He enjoyed a successful post-war career working for the U.S. government as a diplomat, civil servant, and administrator. However, his conversion to the Republican Party and his cooperation with his old friend, President Ulysses S. Grant, as well as critical comments he wrote in his memoirs about General Lee's wartime performance, made him anathema to many of his former Confederate colleagues. His reputation in the South further suffered when he led African-American militia against the anti-Reconstruction White League at the Battle of Liberty Place in 1874. Authors of the Lost Cause movement focused on Longstreet's actions at Gettysburg as a primary reason for the Confederacy's loss of the war. His reputation in the South was damaged for over a century and has only recently begun a slow reassessment.
Early life and career.
In 1821, Longstreet was born in Edgefield District, South Carolina, an area that is now part of North Augusta, Edgefield County. He was the fifth child and third son of James Longstreet (1783-1833), of Dutch descent, and Mary Ann Dent (1793-1855) of English descent, originally from New Jersey and Maryland respectively, who owned a cotton plantation close to where the village of Gainesville would be founded in northeastern Georgia. James's ancestor Dirck Stoffels Langestraet immigrated to the Dutch colony of New Netherland in 1657, but the name became Anglicized over the generations. James's father was impressed by his son's "rocklike" character on the rural plantation, giving him the nickname Peter, and he was known as Pete or Old Pete for the rest of his life.
Longstreet's father decided on a military career for his son, but felt that the local education available to him would not be adequate preparation. At the age of nine, James was sent to live with his aunt and uncle in Augusta, Georgia. His uncle, Augustus Baldwin Longstreet, was a newspaper editor, educator, and a Methodist minister. James spent eight years on his uncle's plantation, Westover, just outside the city while he attended the Academy of Richmond County. His father died from a cholera epidemic while visiting Augusta in 1833; although James's mother and the rest of the family moved to Somerville, Alabama, following his father's death, James remained with uncle Augustus.
In 1837, Augustus attempted to obtain an appointment for James to the United States Military Academy, but the vacancy for his congressional district had already been filled so James was appointed in 1838 by a relative, Reuben Chapman, who represented the First District of Alabama (where Mary Longstreet lived). James was a poor student academically and a disciplinary problem at West Point, ranking 54th out of 56 cadets when he graduated in 1842. He was popular with his classmates, however, and befriended a number of men who would become prominent during the Civil War, including George Henry Thomas, William S. Rosecrans (his West Point roommate), John Pope, D.H. Hill, Lafayette McLaws, George Pickett, and Ulysses S. Grant of the class of 1843. Longstreet was commissioned a brevet second lieutenant in the 4th U.S. Infantry. Longstreet spent his first two years of service at Jefferson Barracks, Missouri, where he was soon joined by his friend, Lieutenant Ulysses Grant.
Soon after, Longstreet met his future first wife Maria Louisa Garland, called Louise by her family. She was the daughter of Longstreet's regimental commander, Lt. Col. John Garland. They married in March 1848, after the Mexican-American War. Although their marriage would last for over 40 years and produce 10 children, Longstreet never mentioned Louise in his memoirs and most anecdotes about their relationship came to historians through the writings of his second wife, Helen Dortch Longstreet.
At about the same time as Longstreet began courting Garland, Grant became acquainted with and courted Longstreet's fourth cousin, Julia Dent, and the couple eventually married. Historians agree that Longstreet attended the Grant wedding on August 22, 1848 in St. Louis, but his role at the ceremony remains unclear. Grant biographer Jean Edward Smith asserted that Longstreet served as Grant's best man at the wedding. John Y. Simon, editor of Julia Grant's memoirs, concluded that Longstreet "may have been a groomsman," and Longstreet biographer Donald Brigman Sanger called the role of best man "uncertain" while noting that neither Grant nor Longstreet mentioned any such role in either of their memoirs.
Mexican-American War.
Longstreet served with distinction in the Mexican-American War with the 8th U.S. Infantry. Early in the war, he served as a lieutenant in Zachary Taylor's army at the 1846 Battle of Monterrey. He received brevet promotions to captain for Contreras and Churubusco and to major for Molino del Rey. In the Battle of Chapultepec on September 12, 1847, he was wounded in the thigh while charging up the hill with his regimental colors; falling, he handed the flag to his friend, Lt. George E. Pickett, who was able to reach the summit.
Longstreet was a veteran member of the Aztec Club of 1847. The Aztec Club was a military society for officers who had served in the Mexican War. Many distinguished officers, both Union and Confederate, who served the American Civil War were members of the Aztec Club.
After the war and his recovery from the Chapultepec wound, Longstreet and his new wife served on frontier duty in Texas, primarily at Fort Martin Scott near Fredericksburg and Fort Bliss in El Paso. He performed scouting missions and also served as major and paymaster for the 8th Infantry from July 1858. Author Kevin Phillips claims that during this period Longstreet was involved in a plot to draw the Mexican state of Chihuahua into the Union as a slave state.
Longstreet was not enthusiastic about secession from the Union, but he had learned from his uncle Augustus about the doctrine of states' rights early in his life and had seen his uncle's passion for it. Although he was born in South Carolina and reared in Georgia, he offered his services to the state of Alabama, which had appointed him to West Point and where his mother still lived. Furthermore, he was the senior West Point graduate from that state, which implied a commensurate rank in the state's forces would be available. He resigned from the U.S. Army in June 1861 to cast his lot with the Confederacy in the Civil War.
American Civil War.
First Bull Run and the Peninsula.
Longstreet arrived in Richmond, Virginia with a commission as a lieutenant colonel in the Confederate States Army. He met with Confederate President Jefferson Davis at the executive mansion on June 22, 1861, where he was informed that he had been appointed a brigadier general with date of rank on June 17, a commission he accepted on June 25. He was ordered to report to Brig. Gen. P.G.T. Beauregard at Manassas, where he was given command of a brigade of three Virginia regiments—the 1st, 11th, and 17th Virginia Infantry regiments.
Longstreet assembled his staff and trained his brigade incessantly. They saw their first action at Blackburn's Ford on July 18, resisting a Union Army reconnaissance in force that preceded the First Battle of Bull Run (First Manassas). When the main attack came at the opposite end of the line on July 21, the brigade played a relatively minor role, although it endured artillery fire for nine hours. Longstreet was infuriated that his commanders would not allow a vigorous pursuit of the defeated Union Army. His trusted staff officer, Moxley Sorrel, recorded that he was "in a fine rage. He dashed his hat furiously to the ground, stamped, and bitter words escaped him." He quoted Longstreet as saying, "Retreat! Hell, the Federal army has broken to pieces." On October 7, Longstreet was promoted to major general and assumed command of a division in the Confederate Army of Northern Virginia —four infantry brigades and Hampton's Legion.
Tragedy struck the Longstreet family in January 1862. A scarlet fever epidemic in Richmond claimed the lives of his one-year-old daughter Mary Anne, his four-year-old son James, and eleven-year-old Augustus ("Gus"), all within a week. His 13-year-old son Garland almost succumbed. The losses were devastating for Longstreet and he became withdrawn, both personally and socially. In 1861 his headquarters were noted for parties, drinking, and poker games. After he returned from the funeral the headquarters social life became more somber, he rarely drank, and he became a devout Episcopalian.
Longstreet turned in a mixed performance in the Peninsula Campaign that spring. He executed well as a rear guard commander at Yorktown and Williamsburg, delaying the advance of Union Maj. Gen. George B. McClellan's army toward Richmond. During the Battle of Seven Pines he marched his men in the wrong direction down the wrong road, causing congestion and confusion with other Confederate units, diluting the effect of the massive Confederate counterattack against McClellan. His report unfairly blamed fellow Maj. Gen. Benjamin Huger for the mishaps. Gen. Joseph E. Johnston was wounded during the battle and he was replaced in command of the Army of Northern Virginia by Gen. Robert E. Lee.
During the Seven Days Battles that followed in late June, Longstreet had operational command of nearly half of Lee's army—15 brigades—as it drove McClellan back down the Peninsula. Longstreet performed aggressively and well in his new, larger command, particularly at Gaines' Mill and Glendale. Lee's army in general suffered from weak performances by Longstreet's peers, including, uncharacteristically, Maj. Gen. Thomas J. "Stonewall" Jackson, and was unable to destroy the Union Army. Moxley Sorrel wrote of Longstreet's confidence and calmness in battle: "He was like a rock in steadiness when sometimes in battle the world seemed flying to pieces." Gen. Lee said, "Longstreet was the staff in my right hand." He had been established as Lee's principal lieutenant.
Second Bull Run, Maryland, and Fredericksburg.
The military reputations of Lee's corps commanders are often characterized as Stonewall Jackson representing the audacious, offensive component of Lee's army, whereas Longstreet more typically advocated and executed defensive strategies and tactics. Jackson has been described as the army's hammer, Longstreet its anvil. In the Northern Virginia Campaign of August 1862, this stereotype did not hold true. Longstreet commanded the Right Wing (later to become known as the First Corps) and Jackson commanded the Left Wing. Jackson started the campaign under Lee's orders with a sweeping flanking maneuver that placed his corps into the rear of Union Maj. Gen. John Pope's Army of Virginia, but he then took up a defensive position and effectively invited Pope to assault him. On August 28 and August 29, the start of the Second Battle of Bull Run (Second Manassas), Pope pounded Jackson as Longstreet and the remainder of the army marched north to reach the battlefield. Postwar criticism of Longstreet claimed that he marched his men too slowly, leaving Jackson to bear the brunt of the fighting for two days, but they covered roughly in a little over 24 hours and Gen. Lee did not attempt to get his army concentrated any faster.
When Longstreet's men arrived around midday on August 29, Lee planned a flanking attack on the Union Army, which was concentrating its attention on Jackson. Longstreet demurred against three suggestions from Lee, urging him to attack, recommending instead that a reconnaissance in force be conducted to survey the ground in front of him. By 6:30 p.m. the division of Brig. Gen. John Bell Hood moved forward against the troops of the Union V Corps, and Longstreet withdrew them at 8:30 p.m., having a better idea of the terrain and enemy soldiers in the area. On the next day, Longstreet's preparations paid dividends, as his artillery was a major factor in helping Jackson resist the V Corps attack, and he capitalized on Federal confusion by launching an attack of his own, anticipating an order from Lee that had not yet arrived. Despite the smashing victory that followed, Longstreet's performance at the battle was criticized by postbellum advocates of the Lost Cause, claiming that his slowness, reluctance to attack, and disobedience to Gen. Lee were a harbinger of his controversial performance to come on July 2, 1863, at the Battle of Gettysburg. Lee's biographer, Douglas Southall Freeman, wrote: "The seeds of much of the disaster at Gettysburg were sown in that instant—when Lee yielded to Longstreet and Longstreet discovered that he would."
Despite this criticism, the following day, August 30, was one of Longstreet's finest performances of the war. Pope came to believe that Jackson was starting to retreat and Longstreet took advantage of this by launching a massive assault on the Union army's left flank with over 25,000 men. For over four hours they "pounded like a giant hammer" with Longstreet actively directing artillery fire and sending brigades into the fray. Longstreet and Lee were together during the assault and both of them came under Union artillery fire. Although the Union troops put up a furious defense, Pope's army was forced to retreat in a manner similar to the embarrassing Union defeat at First Bull Run (First Manassas), fought on roughly the same battleground. Longstreet gave all of the credit for the victory to Lee, describing the campaign as "clever and brilliant." It established a strategic model he believed to be ideal—the use of defensive tactics within a strategic offensive.
Longstreet's actions in the final two major Confederate defensive battles of 1862 would be the proving grounds for his development of dominant defensive tactics. In the Maryland Campaign of September, at the Battle of Antietam, Longstreet held his part of the Confederate defensive line against Union forces twice as numerous. After the delaying action Longstreet's corps fought at South Mountain, he retired to Sharpsburg to join Stonewall Jackson, and prepared to fight a defensive battle. Using terrain to his advantage, Longstreet validated his idea that the tactical defense was now vastly superior to the exposed offense. While the offense dominated in the time of Napoleon, the technological advancements had overturned this. Lt. Col. Harold M. Knudsen claims that Longstreet was one of the few Civil War officers truly aware of this. At the end of that bloodiest day of the Civil War, Lee greeted his subordinate by saying, "Ah! Here is Longstreet; here's my old "war-horse!"" On October 9, a few weeks after Antietam, Longstreet was promoted to lieutenant general. Lee arranged for Longstreet's promotion to be dated one day earlier than Jackson's, making the Old War-Horse the senior lieutenant general in the entire Confederate Army. In an army reorganization in November, Longstreet's command, now designated the First Corps, consisted of five divisions, approximately 41,000 men.
In December, Longstreet's First Corps played the decisive role in the Battle of Fredericksburg. Since Lee moved Longstreet to Fredericksburg early, it allowed Longstreet to take the time to dig in portions of his line, methodically site artillery, and set up a kill zone over the axis of advance he thought the Union attack would come. Remembering the slaughter at Antietam, in which the Confederates did not construct defensive works, Longstreet ordered trenches, abatis, and fieldworks to be constructed, which would set a precedent for future defensive battles of the Army of Northern Virginia. Additionally, Longstreet positioned his men behind a stone wall at the foot of Marye's Heights and held off fourteen assaults by Union forces. The Union army suffered almost 8,000 casualties at Marye's Heights, Longstreet only 1,000. His great defensive success was not based entirely on the advantage of terrain; this time it was the combination of terrain, defensive works, and a centralized coordination of artillery.
Suffolk.
In the early spring of 1863, Longstreet suggested to Lee that his corps be detached from the Army of Northern Virginia and sent to reinforce the Army of Tennessee, where Gen. Braxton Bragg was being challenged in Middle Tennessee by Union Maj. Gen. William S. Rosecrans, Longstreet's roommate at West Point. It is possible that Longstreet believed that an independent command in the West offered better opportunities for advancement than a corps under Lee's shadow. Lee did detach two divisions from the First Corps, but ordered them to Richmond, not Tennessee. Seaborne movements of the Union IX Corps potentially threatened vital ports on the mid-Atlantic coast. The division of George Pickett started for the capital in mid-February, was followed by John Hood's, and then Longstreet himself was told to take command of the detached divisions and the Departments of North Carolina and Southern Virginia.
In April, Longstreet besieged Union forces in the city of Suffolk, Virginia, a minor operation, but one that was very important to Lee's army, still stationed in war-devastated central Virginia. It enabled Confederate authorities to collect huge amounts of provisions that had been under Union control. However, this operation caused Longstreet and 15,000 men of the First Corps to be absent from the Battle of Chancellorsville in May. Despite Lee's brilliant victory at Chancellorsville, Longstreet once again came under criticism, claiming that he could have marched his men back from Suffolk in time to join Lee. However, from the Chancellorsville and Suffolk scenario, Longstreet brought forward the beginnings of a new Confederate strategy. These events proved that the Army of Northern Virginia could manage with fewer troops for periods of time, and units could be shifted to create windows of opportunity in other theaters. Longstreet advocated the first strategic movements to utilize rail, interior lines, and create temporary numerical advantages in Mississippi or Tennessee prior to Gettysburg.
Gettysburg.
Campaign plans.
Following Chancellorsville and the death of Stonewall Jackson, Longstreet and Lee met in mid-May to discuss options for the army's summer campaign. Longstreet advocated, once again, detachment of all or part of his corps to be sent to Tennessee. The justification for this course of action was becoming more urgent as Union Maj. Gen. Ulysses S. Grant was advancing on the critical Confederate stronghold on the Mississippi River, Vicksburg. Longstreet argued that a reinforced army under Bragg could defeat Rosecrans and drive toward the Ohio River, which would compel Grant to break his hold on Vicksburg. Lee was opposed to a division of his army and instead advocated a large-scale offensive or raid into Pennsylvania. In his memoirs, Longstreet described his reaction to Lee's proposal:
This was written years after the campaign and is affected by hindsight, both of the results of the battle and of the postbellum criticism of the Lost Cause authors. In letters of the time Longstreet made no reference to such a bargain with Lee. In April 1868, Lee said that he "had never made any such promise, and had never thought of doing any such thing." Yet in his post-battle report, Lee wrote, "It had not been intended to fight a general battle at such a distance from our base, unless attacked by the enemy."
The Army of Northern Virginia was reorganized after Jackson's death. Two division commanders, Richard S. Ewell and A.P. Hill, were promoted to lieutenant general and assumed command of the Second and the newly created Third Corps respectively. Longstreet's First Corps gave up the division of Maj. Gen. Richard H. Anderson during the reorganization, leaving him with the divisions of Lafayette McLaws, George Pickett, and John Hood.
In the initial movements of the campaign, Longstreet's corps followed Ewell's through the Shenandoah Valley. A spy he had hired, Henry Thomas Harrison who went by just "Harrison", was instrumental in warning the Confederates that the Union Army of the Potomac was advancing north to meet them more quickly than they had anticipated, prompting Lee to order the immediate concentration of his army near Gettysburg, Pennsylvania.
Battle of Gettysburg.
Longstreet's actions at the Battle of Gettysburg would be the centerpiece of the controversy that surrounded him for over a century. Ahead of his troops he arrived on the battlefield late in the afternoon of the first day, July 1, 1863. By then, two Union corps had been driven by Ewell and Hill back through the town into defensive positions on Cemetery Hill. Lee had not intended to fight before his army was fully concentrated, but chance and questionable decisions by A.P. Hill brought on the battle, which- on the first day- was an impressive Confederate victory. Meeting with Lee, Longstreet was concerned about the strength of the Union defensive position and advocated a strategic movement around the left flank of the enemy, to "secure good ground between him and his capital," which would presumably compel the Union commander, Maj. Gen. George G. Meade, to attack defensive positions erected by the Confederates. Instead, Lee exclaimed, "If the enemy is there tomorrow, we must attack him."
Lee's plan for July 2 called for Longstreet to attack the Union's left flank, which would be followed up by Hill's attack on Cemetery Ridge near the center, while Ewell demonstrated on the Union right. Longstreet was not ready to attack as early as Lee envisioned. He received permission from Lee to wait for Brig. Gen. Evander M. Law's brigade (Hood's division) to reach the field before he advanced any of his other brigades; Law marched his men quickly, but did not arrive until noon. Three of Longstreet's brigades were still in march column, and some distance from the attack positions they would need to reach. All of Longstreet's divisions were forced to take a long detour while approaching the enemy position, misled by inadequate reconnaissance that failed to identify a completely concealed route.
Postbellum criticism of Longstreet claims that he was ordered by Lee to attack in the early morning and that his delays were a significant contributor to the loss of the battle. However, Lee agreed to the delays for arriving troops and did not issue his formal order for the attack until 11 a.m. Although Longstreet's motivations have long been clouded by the vitriol of the Lost Cause partisans (see Legacy), many historians agree that Longstreet did not aggressively pursue Lee's orders to launch an attack as early as possible. Biographer Jeffry D. Wert wrote, "Longstreet deserves censure for his performance on the morning of July 2. He allowed his disagreement with Lee's decision to affect his conduct. Once the commanding general determined to assail the enemy, duty required Longstreet to comply with the vigor and thoroughness that had previously characterized his generalship. The concern for detail, the regard for timely information, and the need for preparation were absent." Military historians Herman Hattaway and Archer Jones wrote, "Unenthusiastic about the attack, Longstreet consumed so much time in properly assembling and aligning the corps that the assault did not commence until 4 p.m. During all the time that passed, Meade continued to move in troops to bring about a more and more complete concentration; by 6 p.m. he had achieved numerical superiority and had his left well covered." Campaign historian Edwin Coddington presents a lengthy description of the approach march, which he described as "a comedy of errors such as one might expect of inexperienced commanders and raw militia, but not of Lee's "War Horse" and his veteran troops." He called the episode "a dark moment in Longstreet's career as a general." Gettysburg historian Harry Pfanz concluded that "Longstreet's angry dissidence had resulted in further wasted time and delay." David L. Callihan, in a 2002 reassessment of Longstreet's legacy, wrote, "It is appalling that a field commander of Longstreet's experience and caliber would so cavalierly and ineptly march and prepare his men for battle." An alternative view has been expressed by John Lott, "General Longstreet did all that could be expected on the 2nd day and any allegations of failing to exercise his duty by ordering a morning assault can be repudiated. It would have been impossible to have commenced an attack much earlier than it occurred, and it is doubtful that the Confederacy could have placed the attack in any more secure hands than General Longstreet." But Longstreet's command of the operation had for the most part, been reasonable, since taking the route he should have would have alerted the whole Union army of his assault. Regardless of the controversy regarding the preparations, however, once the assault began around 4 p.m., Longstreet pressed the assault by McLaws and Hood (Pickett's division had not yet arrived) competently against fierce Union resistance, but it was largely unsuccessful, with significant casualties.
On the night of July 2, Longstreet did not follow his usual custom of meeting Gen. Lee at his headquarters to discuss the day's battle, claiming that he was too fatigued to make the ride. Instead, he spent part of the night planning for a movement around Big Round Top that would allow him to attack the enemy's flank and rear. (Longstreet, despite his use of scouting parties, was apparently unaware that a considerable body of troops from the Union VI Corps was in position to block this move.) Shortly after issuing orders for the attack, around sunrise, Longstreet was joined at his headquarters by Lee, who was dismayed at this turn of events. The commanding general had intended for Longstreet to attack the Union left early in the morning in a manner similar to the attack of July 2, using Pickett's newly arrived division, in concert with a resumed attack by Ewell on Culp's Hill. What Lee found was that no one had ordered Pickett's division forward from its bivouac in the rear and that Longstreet had been planning an independent operation without consulting with him. Lee wrote with some restraint in his after-battle report that Longstreet's "dispositions were not completed as early as was expected."
Since his plans for an early-morning coordinated attack were now infeasible, Lee instead ordered Longstreet to coordinate a massive assault on the center of the Union line, employing the division of George Pickett and brigades from A.P. Hill's corps. Longstreet knew this assault had little chance of success. The Union Army was in a position reminiscent of the one Longstreet had taken at Fredericksburg to defeat Burnside's assault. The Confederates would have to cover almost a mile of open ground and spend time negotiating sturdy fences under fire. The lessons of Fredericksburg and Malvern Hill were lost to Lee on this day. In his memoirs, Longstreet claims to have told Lee that he believed the attack on the Union center would fail:
During the artillery barrage that preceded the infantry assault, Longstreet began to agonize over an assault that was going to cost dearly. He attempted to pass the responsibility for launching Pickett's division to his artillery chief, Col. Edward Porter Alexander. When the time came to actually order Pickett forward, Longstreet could only nod in assent, unable to verbalize the order. The assault, known as Pickett's Charge, suffered the heavy casualties that Longstreet anticipated. It was the decisive point in the Confederate loss at Gettysburg and Lee ordered a retreat back to Virginia the following day.
Criticism of Longstreet after the war was based not only on his reputed conduct at the Battle of Gettysburg, but also intemperate remarks he made about Robert E. Lee and his strategies, such as:
For years after the war Longstreet's reputation suffered and was blamed for the failed attack even though Lee ordered the advance after Longstreet's repeated advice to cancel the attack.
Tennessee.
In mid-August 1863, Longstreet resumed his attempts to be transferred to the Western Theater. He wrote a private letter to Secretary of War James Seddon, requesting that he be transferred to serve under his old friend Gen. Joseph E. Johnston. He followed this up in conversations with his congressional ally, Senator Louis Wigfall, who had long considered Longstreet a suitable replacement for Braxton Bragg. Since Bragg's army was under increasing pressure from Rosecrans outside of Chattanooga, Lee and President Davis agreed to the request on September 5. In one of the most daunting logistical efforts of the Confederacy, Longstreet, with the divisions of Lafayette McLaws and John Hood, a brigade from George Pickett's division, and Porter Alexander's 26-gun artillery battalion, traveled over 16 railroads on a route through the Carolinas to reach Bragg in northern Georgia. Although the entire operation would take over three weeks, Longstreet and lead elements of his corps arrived on September 17.
The First Corps veterans arrived in the early stages of the Battle of Chickamauga. Bragg had already begun an unsuccessful attempt to interpose his army between Rosecrans and Chattanooga before the arrival of Longstreet's corps. When the two met at Bragg's headquarters in the evening, Bragg placed Longstreet in command of the Left Wing of his army; Lt. Gen. Leonidas Polk commanded the Right. On September 20, 1863, Longstreet lined up eight brigades in a deep column against a narrow front, an attack very similar to future German tank tactics in World War II. By chance, a mistaken order from General Rosecrans caused a gap to appear in the Union line and Longstreet took additional advantage of it to increase his chances of success. The organization of the attack was well suited to the terrain and would have penetrated the Union line regardless. The Union right collapsed and Rosecrans fled the field, as units began to retreat in panic. Maj. Gen. George H. Thomas managed to rally the retreating units and solidify a defensive position on Snodgrass Hill. He held that position against repeated afternoon attacks by Longstreet, who was not adequately supported by the Confederate right wing. Once night fell, the battle was over, and Thomas was able to extricate the units under his control to Chattanooga. Bragg's failure to coordinate the right wing and cavalry to further envelop Thomas prevented a total rout of the Union Army. Bragg also neglected to pursue the retreating Federals aggressively, resulting in the futile siege of Chattanooga. Nevertheless, Chickamauga was the greatest Confederate victory in the Western Theater and Longstreet deserved a good portion of the credit.
Longstreet soon clashed with Bragg and became leader of the group of senior commanders of the army who conspired to have him removed. Bragg's subordinates had long been dissatisfied with his leadership and abrasive personality; the arrival of Longstreet (the senior lieutenant general in the Army) and his officers, added credibility to the earlier claims, and was a catalyst toward action. Longstreet wrote to Seddon, "I am convinced that nothing but the hand of God can save us or help us as long as we have our present commander." The situation became so grave that President Davis was forced to intercede in person. What followed was one of the most bizarre scenes of the war, with Bragg sitting red faced as a procession of his commanders condemned him. Longstreet stated that Bragg "was incompetent to manage an army or put men into a fight" and that he "knew nothing of the business." Davis sided with Bragg and did nothing to resolve the conflict.
Bragg retained his position, relieving or reassigning the generals who had testified against him, and retaliated against Longstreet by reducing his command to only those units that he brought with him from Virginia. Despite the dysfunctional command climate under Bragg, and the lack of support from the War Department and President Davis concerning Bragg's removal, Longstreet did the best he could to continue to seek options in the Chattanooga Campaign. While Bragg resigned himself and his army to the siege of the Union Army of the Cumberland in Chattanooga, Longstreet devised a strategy to prevent reinforcement and a lifting of the siege by Grant. He knew this Union reaction was underway, and that the nearest railhead was Bridgeport, Alabama, where portions of two Union corps would soon arrive. After sending his artillery commander, Porter Alexander, to reconnoiter the Union-occupied town, he devised a plan to shift most of the Army of Tennessee away from the siege, setting up logistical support in Rome, Georgia, to go after Bridgeport to take the railhead, possibly catching Maj. Gen. Joseph Hooker and arriving Union troops from the Eastern Theater in a disadvantageous position. The plan was well received and approved by President Davis, but it was disapproved by Bragg, who objected to the significant logistical challenges it posed. Longstreet accepted Bragg's arguments and agreed to a plan in which he and his men were dispatched to East Tennessee to deal with an advance by Union Maj. Gen. Ambrose Burnside. Longstreet was selected for this assignment partially due to enmity on Bragg's part, but also because the War Department intended for Longstreet's men to return to Lee's army and this movement was in that direction.
Longstreet was criticized for the slow pace of his advance toward Knoxville in November and some of his troops began using the nickname "Peter the Slow" to describe him. Burnside evaded him at the Battle of Campbell's Station and settled into entrenchments around the city, which Longstreet besieged unsuccessfully. The Battle of Fort Sanders failed to bring a Confederate breakthrough. When Bragg was defeated by Grant at Chattanooga on November 25, Longstreet was ordered to join forces with the Army of Tennessee in northern Georgia. He demurred and began to move back to Virginia, soon pursued by Maj. Gen. William T. Sherman in early December. The armies went into winter quarters and the First Corps rejoined the Army of Northern Virginia in the spring. The only real effect of the minor campaign was to deprive Bragg of troops he sorely needed in Chattanooga. Longstreet's second independent command (after Suffolk) was a failure and his self-confidence was damaged. He reacted to the failure of the campaign by blaming others, as he had done at Seven Pines. He relieved Lafayette McLaws from command and requested the court martial of Brig. Gens. Jerome B. Robertson and Evander M. Law. He also submitted a letter of resignation to Adjutant General Samuel Cooper on December 30, 1863, but his request to be relieved was denied.
His corps suffered through a severe winter in Eastern Tennessee with inadequate shelter and provisions. Writing to Georgia's Quartermaster General, Ira Roe Foster on January 24, 1864, Longstreet noted: "There are five Georgia Brigades in this Army – Wofford's, G.T. Anderson's, Bryan's, Benning's, and Crews' cavalry brigade. They are all alike in excessive need of shoes, clothing of all kinds, and blankets. All that you can send will be thankfully received." Meanwhile, Longstreet again developed strategic plans. He called for an offensive through Tennessee into Kentucky in which his command would be bolstered by P.G.T. Beauregard and 20,000 men. Although he had the concurrence of Gen. Lee, Longstreet was unable to convince President Davis or his newly appointed military advisor, Braxton Bragg.
Wilderness to Appomattox.
Finding out that his old friend Ulysses Grant was in command of the Union Army, he told his fellow officers that "he will fight us every day and every hour until the end of the war." Longstreet helped save the Confederate Army from defeat in his first battle back with Lee's army, the Battle of the Wilderness in May 1864, in which he launched a powerful flanking attack along the Orange Plank Road against the Union II Corps and nearly drove it from the field. Once again he developed innovative tactics to deal with difficult terrain, ordering the advance of six brigades by heavy skirmish lines, which allowed his men to deliver a continuous fire into the enemy, while proving to be elusive targets themselves. Wilderness historian Edward Steere attributed much of the success of the Army to "the display of tactical genius by Longstreet which more than redressed his disparity in numerical strength." After the war, the Union II Corps commander that day, Maj. Gen. Winfield S. Hancock, said to Longstreet of this flanking maneuver: "You rolled me up like a wet blanket."
Longstreet was wounded during the assault—accidentally shot by his own men only about away from the place where Jackson suffered the same fate a year earlier. A bullet passed through his shoulder, severing nerves, and tearing a gash in his throat. Micah Jenkins, who was riding with Longstreet, was also shot and died from his wounds. The momentum of the attack subsided. As he was taken from the field, Longstreet urged Lee to press the attack. Instead, Lee delayed further movement until units could be realigned, giving the Union defenders adequate time to reorganize. The subsequent attack was a failure. E.P. Alexander called the removal of Longstreet the critical juncture of the battle: "I have always believed that, but for Longstreet's fall, the panic which was fairly underway in Hancock's Corps would have been extended & have resulted in Grant's being forced to retreat back across the Rapidan."
Longstreet missed the rest of the 1864 spring and summer campaign, where Lee sorely missed his skill in handling the army. He was treated in Lynchburg, Virginia, and recuperated in Augusta, Georgia, with his niece, Emma Eve Longstreet Sibley, the daughter of his brother Gilbert. While in Augusta, he participated in the funeral service for Lt. Gen. Leonidas Polk at Saint Paul's Church, joining the Bishops of Mississippi and Arkansas in casting earth onto the coffin. He rejoined Lee in October 1864, with his right arm paralyzed and in a sling, initially unable to ride a horse. He had taught himself to write with his left hand; by periodically pulling on his arm, as advised by doctors, he was able to regain use of his right hand in later years. For the remainder of the Siege of Petersburg he commanded the defenses in front of the capital of Richmond, including all forces north of the James River and Pickett's Division at Bermuda Hundred. He retreated with Lee in the Appomattox Campaign, commanding both the First and Third Corps, following the death of A.P. Hill on April 2. As Lee considered surrender, Longstreet advised him of his belief that Grant would treat them fairly, but as Lee rode toward Appomattox Court House on April 9, 1865, Longstreet said, "General, if he does not give us good terms, come back and let us fight it out."
Post-bellum life.
After the war, Longstreet and his family settled in New Orleans, a location popular with a number of former Confederate generals. He entered into a cotton brokerage partnership there and also became the president of the newly created Great Southern and Western Fire, Marine and Accident Insurance Company. He actively sought the presidency of the Mobile and Ohio Railroad but was unsuccessful, and also failed in an attempt to get investors for a proposed railroad from New Orleans to Monterrey, Mexico. (In 1870, he was named president of the newly organized New Orleans and Northeastern Railroad.) He applied for a pardon from President Andrew Johnson, endorsed by his old friend Ulysses S. Grant. Johnson refused, however, telling Longstreet in a meeting: "There are three persons of the South who can never receive amnesty: Mr. Davis, General Lee, and yourself. You have given the Union cause too much trouble." Regardless of such opposition the United States Congress restored his rights of citizenship in June 1868.
Longstreet was the only senior Confederate officer to join the Republican Party during Reconstruction. He endorsed Grant for president in 1868, attended his inauguration ceremonies, and six days later received an appointment as surveyor of customs in New Orleans. For these acts he lost favor with many white Southerners. His old friend Harvey Hill wrote to a newspaper: "Our scalawag is the local leper of the community." Unlike Northerners who moved South and were sometimes referred to as "Carpetbaggers," Hill wrote, Longstreet "is a native, which is so much the worse." The Republican governor of Louisiana appointed Longstreet the adjutant general of the state militia and by 1872 he became a major general in command of all militia and state police forces within New Orleans. During protests of election irregularities in 1874, referred to as the Battle of Liberty Place, an armed force of 8,400 White League members advanced on the State House. Longstreet commanded a force of 3,600 Metropolitan Police, city policemen, and African-American militia troops, armed with two Gatling guns and a battery of artillery. He rode to meet the protesters but was pulled from his horse, shot by a spent bullet, and taken prisoner. The White League charged, causing many of Longstreet's men to flee or surrender. There were casualties of 38 killed and 79 wounded. Federal troops were required to restore order. Longstreet's use of black troops during the disturbances increased the denunciations by anti-Reconstructionists.
In 1875 the Longstreet family left New Orleans with concerns over health and safety, returning to Gainesville, Georgia. By this time Louise had given birth to ten children, five of whom lived to adulthood. He applied for various jobs through the Rutherford B. Hayes administration and was briefly considered for Secretary of the Navy. He served briefly as deputy collector of internal revenue and as postmaster of Gainesville. In 1880 Hayes appointed Longstreet as his ambassador to the Ottoman Empire, and later he served from 1897 to 1904, under Presidents William McKinley and Theodore Roosevelt, as U.S. Commissioner of Railroads, succeeding Wade Hampton III.
On one of his frequent return trips to New Orleans on business, Longstreet converted to Catholicism in 1877 and was a devout believer until his death. He served as a U.S. Marshal from 1881 to 1884, but the return of a Democratic administration ended his political careers and he went into semiretirement on a farm near Gainesville, where he raised turkeys and planted orchards and vineyards on terraced ground that his neighbors referred to jokingly as "Gettysburg." A devastating fire on April 9, 1889 (the 24th anniversary of Lee's surrender at Appomattox) destroyed his house and many of his personal possessions, including his personal Civil War documents and memorabilia. That December Louise Longstreet died. He remarried in 1897, in a ceremony at the governor's mansion in Atlanta, to Helen Dortch, age 34. Although Longstreet's children reacted poorly to the marriage, Helen became a devoted wife and avid supporter of his legacy after his death. She outlived him by 58 years, dying in 1962.
After Louise's death, and after bearing criticism of his war record from other Confederates for decades, Longstreet refuted most of their arguments in his memoirs entitled "From Manassas to Appomattox", a labor of five years that was published in 1896. His final years were marked by poor health and partial deafness. In 1902 he suffered from severe rheumatism and was unable to stand for more than a few minutes at a time. His weight diminished from 200 to 135 pounds by January 1903. Cancer developed in his right eye, and in December he had X-ray therapy in Chicago to treat it. He contracted pneumonia and died in Gainesville, six days before his 83rd birthday. Longstreet's remains are buried in Alta Vista Cemetery. He outlived most of his detractors, and was one of only a few general officers from the Civil War to live into the 20th century.
Legacy.
Criticism from authors in the Lost Cause movement attacked Longstreet's war career for many years after his death. Knudsen maintains that because Longstreet became a "reconstructed rebel", embraced equal rights for blacks, unification of the nation, and reconstruction, he became the target of those who wanted to maintain racist policies and otherwise could not accept the verdict of the battlefield. The attacks formally began on January 19, 1872, the anniversary of Robert E. Lee's birth, and less than two years after Lee's death. Jubal Early, in a speech at Washington College, exonerated Lee of his failure at Gettysburg and falsely accused Longstreet of attacking late on the second day and of being responsible for the debacle on the third. The following year William N. Pendleton, Lee's artillery chief, claimed in the same venue that Longstreet disobeyed an explicit order to attack at sunrise on July 2. Both of these allegations were fabrications; however, Longstreet failed to challenge these lies publicly until 1875. The delay was damaging to his reputation, as the Lost Cause mythology had taken hold in common opinion by this time. In the 20th century Douglas Southall Freeman kept criticism of Longstreet foremost in Civil War scholarship in his biography of Lee. Clifford Dowdey, a Virginia newspaperman and novelist, was noted for his severe criticism of Longstreet in the 1950s and 1960s.
After Longstreet's death, his second wife Helen privately published "Lee and Longstreet at High Tide" in his defense, in which she stated "the South was seditiously taught to believe that the Federal Victory was wholly the fortuitous outcome of the culpable disobedience of General Longstreet."
The publication of Michael Shaara's novel "The Killer Angels" in 1974, based in part on Longstreet's memoirs, followed by its 1993 film adaptation, "Gettysburg", have been credited with helping to restore Longstreet's reputation as a general and to dramatically raise his public visibility. The 1982 work by Thomas L. Connolly and Barbara L. Bellows, "God and General Longstreet", provided a "further upgrading of Longstreet through an attack on Lee, the Lost Cause, and the Virginia revisionists."
Jeffry D. Wert wrote that "Longstreet ... was the finest corps commander in the Army of Northern Virginia; in fact, he was arguably the best corps commander in the conflict on either side." Richard L. DiNardo wrote "Even Longstreet's most virulent critics have conceded that he put together the best staff employed by any commander, and that his "de facto" chief of staff, Lieutenant Colonel G. Moxley Sorrel, was the best staff officer in the Confederacy." DiNardo cited the effective way in which Longstreet delegated responsibilities for control of battlefield movements to his staff and how they were able to communicate with him more effectively during battles than the staffs of other Confederate generals during the war.
In memoriam.
Longstreet is remembered through numerous places that bear his name in and around Gainesville, Georgia, including Longstreet Bridge, a portion of U.S. Route 129 that crosses the Chattahoochee River (later dammed to form Lake Sidney Lanier), and the local Longstreet Chapter of the United Daughters of the Confederacy.
In 1998, one of the last monuments erected at Gettysburg National Military Park was dedicated as a belated tribute to Longstreet, an equestrian statue by sculptor Gary Casteel. He is shown riding on a depiction of his favorite horse, Hero, at ground level in a grove of trees in Pitzer Woods—unlike most generals, who are elevated on tall bases overlooking the battlefield.
The Longstreet Society is an organization and museum in Gainesville, dedicated to the celebration and study of his life and career.
The General Longstreet Recognition Project is an educational project of the Agribusiness Council Heritage Preservation Committee aimed at broadening public awareness of Longstreet's military and public service.
Longstreet's Billet, the house in Russellville, Tennessee, that Longstreet occupied during the winter of 1863–64, has been converted into The Longstreet Museum, open to the public.

</doc>
<doc id="49592" url="https://en.wikipedia.org/wiki?curid=49592" title="Fortune cookie">
Fortune cookie

A fortune cookie is a crisp cookie usually made from flour, sugar, vanilla, and sesame seed oil with a piece of paper, a "fortune", on which is an aphorism, or a vague prophecy. The message inside may also include a Chinese phrase with translation and/or a list of lucky numbers used by some as lottery numbers, some of which have become actual winning numbers. Fortune cookies are often served as a dessert in Chinese restaurants in the United States and some other countries, but are absent in China. The exact origin of fortune cookies is unclear, though various immigrant groups in California claim to have popularized them in the early 20th century.
Origin.
As far back as the 19th century, a cookie very similar in appearance to the modern fortune cookie was made in Kyoto, Japan; and there is a Japanese temple tradition of random fortunes, called omikuji. The Japanese version of the cookie differs in several ways: they are a little bit larger; are made of darker dough; and their batter contains sesame and miso rather than vanilla and butter. They contain a fortune; however, the small slip of paper was wedged into the bend of the cookie rather than placed inside the hollow portion. This kind of cookie is called and is still sold in some regions of Japan, especially in Kanazawa, Ishikawa. It is also sold in the neighborhood of Fushimi Inari-taisha shrine in Kyoto.
Makoto Hagiwara of Golden Gate Park's Japanese Tea Garden in San Francisco is reported to have been the first person in the USA to have served the modern version of the cookie when he did so at the tea garden in the 1890s or early 1900s. The fortune cookies were made by a San Francisco bakery, Benkyodo.
David Jung, founder of the Hong Kong Noodle Company in Los Angeles, has made a competing claim that he invented the cookie in 1918. San Francisco's Court of Historical Review attempted to settle the dispute in 1983. During the proceedings, a fortune cookie was introduced as a key piece of evidence with a message reading, "S.F. Judge who rules for L.A. Not Very Smart Cookie". A federal judge of the Court of Historical Review determined that the cookie originated with Hagiwara and the court ruled in favor of San Francisco. Subsequently, the city of Los Angeles condemned the decision.
Seiichi Kito, the founder of Fugetsu-do of Little Tokyo in Los Angeles, also claims to have invented the cookie. Kito claims to have gotten the idea of putting a message in a cookie from Omikuji (fortune slip) which are sold at temples and shrines in Japan. According to his story, he sold his cookies to Chinese restaurants where they were greeted with much enthusiasm in both the Los Angeles and San Francisco areas. Thus Kito's main claim is that he is responsible for the cookie being so strongly associated with Chinese restaurants.
Up to around World War II, fortune cookies were known as "fortune tea cakes"—likely reflecting their origins in Japanese tea cakes.
Fortune cookies moved from being a confection dominated by Japanese-Americans to one dominated by Chinese-Americans sometime around World War II. One theory for why this occurred is because of the Japanese American internment during World War II, which forcibly put over 100,000 Japanese-Americans in internment camps, including those who had produced fortune cookies. This gave an opportunity for Chinese manufacturers.
Fortune cookies before the early 20th century were all made by hand. However, the fortune cookie industry changed dramatically after the fortune cookie machine was invented by Shuck Yee from Oakland, California. The machine allowed for mass production of fortune cookies which subsequently allowed the cookies to drop in price to become the novelty and courtesy dessert many Americans are familiar with after their meals at most Chinese restaurants today.
Chinese legend.
Rumors that fortune cookies were invented in China are seen as false. In 1989, fortune cookies were reportedly imported into Hong Kong and sold as "genuine American fortune cookies". Wonton Food attempted to expand its fortune cookie business into China in 1992, but gave up after fortune cookies were considered "too American".
Many view the mooncake hidden message system that was used in the Ming revolution to be a precursor to the modern day fortune cookie. By adding the covert element to the myths of the fortune cookie some have found more meaning behind the simple treat. This led to the act of removing and replacing the fortune inside without breaking for an added bit of good luck.
Manufacturers.
There are approximately 3 billion fortune cookies made each year around the world, the vast majority of them used for consumption in the United States. The largest manufacturer of the cookies is Wonton Food Inc., headquartered in Brooklyn, New York. They make over 4.5 million fortune cookies per day. Another large manufacturer are Baily International in the Midwest and Peking Noodle in the Los Angeles area. There are other smaller, local manufacturers including Tsue Chong Co. in Seattle, Keefer Court Food in Minneapolis and Sunrise Fortune Cookie in Philadelphia. Many smaller companies will also sell custom fortunes.
Around the world.
Fortune cookies, while largely an American item, have been served in Chinese restaurants in Australia, Brazil, Canada, Chile, Colombia, Finland, France, Germany, India, Italy, Mexico, United Kingdom, as well as other countries.
Asian stereotype.
Fortune cookies are sometimes viewed as a stereotype of East Asians by Westerners. "I think it does go to what people think when they think of Asians. They think of food. Because that is really their only point of contact, or awareness, with the Asian-American community." said Andrew Kang, senior staff attorney at the Asian-American Institute in Chicago. The Asian American Journalists Association discourages associating ethnic foods with Asian Americans in news coverage.
Translations of name.
Globally, the cookies are generally called by the English term "fortune cookies", being American in origin.
There is no single accepted Chinese name for the cookies, with a large variety of translations being used to describe them in the Chinese language, all of which being more-or-less literal translations of the English "fortune cookie". Examples include: 幸运籤饼 "xìngyùn qiān bǐng" "good luck lot cookie", 籤语饼 "qiān yǔ bǐng" "fortune words cookie", 幸运饼 "xìngyùn bǐng" "good luck cookie", 幸运籤语饼 "xìngyùn qiān yǔ bǐng" "lucky fortune words cookie", 幸运甜饼 "xìngyùn tián bǐng" "good luck sweet cookie", 幸福饼干 "xìngfú bǐnggān" "good luck biscuit", or 占卜饼 "zhānbǔ bǐng" "divining cookie".
In popular culture.
The non-Chinese origin of the fortune cookie is humorously illustrated in Amy Tan's 1989 novel "The Joy Luck Club", in which a pair of immigrant women from China find jobs at a fortune cookie factory in America. They are amused by the unfamiliar concept of a fortune cookie but, after several hilarious attempts at translating the fortunes into Chinese, come to the conclusion that the cookies contain not wisdom but "bad instruction."
Fortune cookies have become an iconic symbol in American culture, inspiring many products. There are fortune cookie-shaped jewelry, a fortune cookie-shaped Magic 8 Ball, and silver-plated fortune cookies. Fortune cookie toilet paper, with words of wisdom that appear when the paper is moistened, has become popular among university students in Italy and Greece.
There is a common joke in the United States involving fortune cookies that involves appending "between the sheets" or "in bed" to the end of the fortune, usually creating a sexual innuendo or other bizarre messages (e.g., "Our greatest glory is not in never falling but in rising every time we fall [in bed"). A gallows humor variation to this joke involves appending the phrase "in jail" to the end of the fortune.
In "The Simpsons" episode "The Last Temptation of Homer," Homer, who is trying to resist having an affair with a co-worker, receives a fortune reading "You will find happiness with a new love." The scene then cuts to the kitchen, where one waiter notes they're out of "new love" fortunes, only for another to point out a "stick with your wife" barrel. In another episode, "Hunka-Hunka Burns in Love", the family complains to their waiter about getting subpar fortunes (for example, "Every house has a bathroom"), and Homer winds up getting a job writing them himself ("You will be aroused by a shampoo commercial"). Another reference was in the episode, "Goo Goo Gai Pan", when Selma had her adopted baby taken from her in China, Homer said that the fortune cookies are more accurate in China, to which a fortune said "We will take Selma's baby." appears.
In "Are You Afraid of the Dark?" episode "The Tale of the Misfortunate Cookie", David's misreading of a magical fortune cookie sends him to an alternate world where he learns the negative consequences of his wish.
In "Iron Man 3", the sardonically villainous Mandarin, played by Ben Kingsley, mentions the cookie's origin, stating: "True story about fortune cookies—they look Chinese, they sound Chinese. But they're actually an American invention, which is why they're hollow, full of lies and leave a bad taste in the mouth." Later his character says "Did you know fortune cookies aren't even Chinese? They're made by Americans, based on a Japanese recipe."
In the video game "Animal Crossing New Leaf", the player can purchase fortune cookies, which allows them to win Nintendo-themed items.
In 2013, Japanese pop group AKB48 released a single titled "Koisuru Fortune Cookie" which sold 1,095,894 copies on its first day of release, and reached number one on the Oricon weekly charts with over 1.33 million copies. AKB48's Indonesian sister group JKT48 released their own version of the song titled "Fortune Cookie Yang Mencinta" as did the Chinese sister group SNH48 as "Ài de xìngyùn qū qí".
In an episode of cartoon series Rocko's Modern Life, character Filbert Turtle receives a fortune that reads "Bad luck and misfortune will forever torment your pathetic soul for all eternity.". This fortune comes true and the entire episode revolves around his bad luck.

</doc>
<doc id="49593" url="https://en.wikipedia.org/wiki?curid=49593" title="Candiru">
Candiru

Candiru (English and Portuguese or "candirú" in Spanish), "Vandellia cirrhosa", also known as "cañero", "toothpick fish", or "vampire fish", is a species of parasitic freshwater catfish in the family "Trichomycteridae" native to the Amazon Basin where it is found in the countries of Bolivia, Brazil, Colombia, Ecuador and Peru.
The definition of "candiru" differs between authors. The word has been used to refer to only "Vandellia cirrhosa", the entire genus "Vandellia", the subfamily Vandelliinae, or even the two subfamilies Vandelliinae and Stegophilinae.
Although some candiru species have been known to grow to a size of in length, others are considerably smaller. These smaller species are known for an alleged tendency to invade and parasitise the human urethra; however, despite ethnological reports dating back to the late 19th century, the first documented case of the removal of a candiru from a human urethra did not occur until 1997, and even that incident has remained a matter of controversy.
Description.
Candirus are small fish. Adults can grow to around with a rather small head and a belly that can appear distended, especially after a large blood meal. The body is translucent, making it quite difficult to spot in the turbid waters of its home. There are short sensory barbels around the head, together with short, backward pointing spines on the gill covers.
Location and habitat.
Candirus ("Vandellia") inhabit the Amazon and Orinoco basins of lowland Amazonia, where they constitute part of the Neotropical fish fauna. Candirus are hematophagous and parasitize the gills of larger Amazonian fishes, especially catfish of the family Pimelodidae (Siluriformes).
Alleged attacks on humans.
Although lurid anecdotes of attacks on humans abound, very few cases have been verified, and some alleged traits of the fish have been discredited as myth or superstition.
Historical accounts.
The earliest published report of candiru attacking a human host comes from German biologist C. F. P. von Martius in 1829, who never actually observed it, but rather was told about it by the native people of the area, including that men would tie a ligature around their penis while going into the river to prevent this from happening. Other sources also suggest that other tribes in the area used various forms of protective coverings for their genitals while bathing, though it was also suggested that these were to prevent bites from piranha. Martius also speculated that the fish were attracted by the "odor" of urine. Later experimental evidence showed this to be false, as the fish actually hunt by sight and have no attraction to urine at all.
Another report from French naturalist Francis de Castelnau in 1855 relates an allegation by local Araguay fisherman, saying that it is dangerous to urinate in the river as the fish "springs out of the water and penetrates into the urethra by ascending the length of the liquid column." While Castelnau himself dismissed this claim as "absolutely preposterous," and the fluid mechanics of such a thing occurring defy the laws of physics, it remains one of the more stubborn myths about the candiru. It has been suggested this claim evolved out of the real observation that certain species of fish in the Amazon will gather at the surface near the point where a urine stream enters, having been attracted by the noise and agitation of the water.
In 1836 Eduard Poeppig documented a statement by a local physician in Pará, known only as Dr. Lacerda, who offered an eyewitness account of a case where a candiru had entered a human orifice. However, it was lodged in a native woman's vagina, rather than a male urethra. He relates that the fish was extracted after external and internal application of the juice from a Xagua plant (believed to be a name for Genipa americana). Another account was documented by biologist George A. Boulenger from a Brazilian physician named Dr. Bach, who examined a man and several boys whose penises had been amputated. Bach believed this was a remedy performed because of parasitism by candiru, but he was merely speculating as he did not speak his patients' language. American biologist Eugene Willis Gudger noted the area the patients were from did not have candiru in its rivers, and suggested the amputations were much more likely the result of having been attacked by piranha.
In 1891, naturalist Paul Le Cointe provides a rare first-hand account of a candiru entering a human body, and like Lacerda's account, it involved the fish being lodged in the vaginal canal, not the urethra. Le Cointe actually removed the fish himself, by pushing it forward to disengage the spines, turning it around and removing it head-first.
Gudger, in 1930, noted there have been several other cases reported wherein the fish entered the vaginal canal, but not a single case of a candiru entering the anus was ever documented. According to Gudger, this lends credence to the unlikelihood of the fish entering the male urethra, based on the comparatively small opening that would accommodate only the most immature members of the species.
It was also once thought that the fish was attracted to urine, as the candiru's primary prey emits urea from its gills, but this was later discredited in formal experimentation. Indeed, the fish appears not to have any response to any chemical attractants, and primarily hunts by visual tracking.
Modern cases.
To date, there is only one documented case of a candiru entering a human urethra, which took place in Itacoatiara, Brazil in 1997. In this incident, the victim (a 23-year-old man known only as "F.B.C.") claimed a candiru "jumped" from the water into his urethra as he urinated while thigh-deep in a river. After traveling to Manaus on October 28, 1997, the victim underwent a two-hour urological surgery by Dr. Anoar Samad to remove the fish from his body.
In 1999, American marine biologist Stephen Spotte traveled to Brazil to investigate this particular incident in detail. He recounts the events of his investigation in his book "Candiru: Life and Legend of the Bloodsucking Catfishes". Spotte met Dr. Samad in person and interviewed him at his practice and home. Samad gave him photos, the original VHS tape of the cystoscopy procedure, and the actual fish's body preserved in formalin as his donation to the INPA. Spotte and his colleague Paulo Petry took these materials and examined them at the INPA, comparing them with Samad's formal paper. While Spotte did not overtly express any conclusions as to the veracity of the incident, he did remark on several observations that were suspicious about the claims of the patient and/or Samad himself.
When subsequently interviewed, Spotte stated that even if a person were to urinate while "submerged in a stream where candiru live", the odds of that person being attacked by candiru are "(a)bout the same as being struck by lightning while simultaneously being eaten by a shark."

</doc>
<doc id="49597" url="https://en.wikipedia.org/wiki?curid=49597" title="Nathan Bedford Forrest">
Nathan Bedford Forrest

Nathan Bedford Forrest (July 13, 1821 – October 29, 1877), called Bedford Forrest in his lifetime, was a lieutenant general in the Confederate Army during the American Civil War. He is remembered as a self-educated, brutal, and innovative cavalry leader during the war and as a leading Southern advocate in the postwar years. He was a pledged delegate from Tennessee to the New York Democratic national convention of 4 July 1868. He served as the first Grand Wizard of the Ku Klux Klan, but later distanced himself from the organization.
A cavalry and military commander in the war, Forrest is one of the war's most unusual figures. Although less educated than many of his fellow officers, before the war Forrest had already amassed a fortune as a planter, real estate investor, and slave trader. He was one of the few officers in either army to enlist as a private and be promoted to general officer and corps commander during the war. Although Forrest lacked formal military education, he had a gift for leadership, strategy and tactics. He created and established new doctrines for mobile forces, earning the nickname "The Wizard of the Saddle".
Forrest was accused of war crimes at the Battle of Fort Pillow for allowing forces under his command to massacre hundreds of black Union Army and white Southern Unionist prisoners. Union Major General William T. Sherman investigated the allegations and did not charge Forrest with any improprieties.
In their postwar writings, Confederate President Jefferson Davis and General Robert E. Lee both expressed their belief that the Confederate high command had failed to fully use Forrest's talents.
Early life.
Nathan Bedford Forrest was born to a poor Scotch-Irish American family in Bedford County, Tennessee. He and his twin sister, Fanny, were the two eldest of blacksmith William Forrest's 12 children with wife Miriam Beck. The Forrest family had migrated to Tennessee from Virginia, via North Carolina, during the second half of the 18th century, while the Beck family had moved from South Carolina to Tennessee around the same time. After the deaths of his father and Fanny to scarlet fever, Forrest at age 17 became head of the family.
In 1841, Forrest went into business with his uncle Jonathan Forrest in Hernando, Mississippi. His uncle was killed there in 1845 during an argument with the Matlock brothers. In retaliation, Forrest shot and killed two of them with his two-shot pistol and wounded two others with a knife which had been thrown to him. One of the wounded Matlock men survived and served under Forrest during the Civil War.
Forrest became a businessman, planter, and slaveholder. He owned several cotton plantations in the Delta region of West Tennessee. He was also a slave trader, at a time when demand was booming in the Deep South; he had his trading business based on Adams Street in Memphis. In 1858, Forrest (a ), was elected a Memphis city alderman. Forrest supported his mother and put his younger brothers through college. By the time the American Civil War started in 1861, he had become a millionaire and one of the richest men in the South, having amassed a "personal fortune that he claimed was worth $1.5 million".
Before the Civil War, Forrest was well known as a Memphis speculator and Mississippi gambler. He was for some time captain of a boat which ran between Memphis, Tennessee and Vicksburg, Mississippi. As his fortune increased he engaged in plantation speculation, and became the nominal owner of two plantations not far from Goodrich's Landing, above Vicksburg, where he worked some hundred or more slaves ... He was known to his acquaintances as a man of obscure origin and low associations, a shrewd speculator, negro trader, and duelist, but a man of great energy and brute courage.
Marriage and family.
In 1845, Nathan married Mary Ann Montgomery (1826–1893), the daughter of a Presbyterian minister. They had two children together: William Montgomery Bedford Forrest (1846–1908), who enlisted at the age of 15 and served alongside his father in the war, and a daughter Fanny (1849–1854), who died in childhood. His descendants continued the military tradition. A grandson, Nathan Bedford Forrest II (1872–1931), became Commander-in-Chief of the Sons of Confederate Veterans and a Grand Dragon of the Ku Klux Klan. A great-grandson, Nathan Bedford Forrest III (1905–1943), graduated from West Point and rose to the rank of brigadier general in the U.S. Army Air Corps; he was killed during a bombing raid over Nazi Germany in 1943.
Military career.
After the Civil War broke out, Forrest returned to Tennessee from his Mississippi ventures, enlisted in the Confederate States Army (CSA), and trained at Fort Wright in Randolph, Tennessee. On July 14, 1861, he joined Captain Josiah White's Company "E", Tennessee Mounted Rifles as a private, along with his youngest brother and 15-year-old son. Upon seeing how badly equipped the CSA was, Forrest offered to buy horses and equipment with his own money for a regiment of Tennessee volunteer soldiers.
His superior officers and the state Governor Isham G. Harris were surprised that someone of Forrest's wealth and prominence had enlisted as a soldier, especially since major planters were exempted from service. They commissioned him as a lieutenant colonel and authorized him to recruit and train a battalion of Confederate mounted rangers. In October 1861, Forrest was given command of a regiment, the 3rd Tennessee Cavalry. Though Forrest had no prior formal military training or experience, he had exhibited leadership qualities and soon proved he had a gift for successful tactics.
Public debate surrounded Tennessee's decision to join the Confederacy. Both the CSA and the Union armies recruited soldiers from the state. More than 100,000 men from Tennessee served with the Confederacy (more per capita than any other state), and 50,000 served with the Union. Forrest posted ads to join his regiment for "men with good horse and good gun" adding "if you wanna have some fun and to kill some Yankees".
At six feet, two inches (1.88 m) tall and 210 pounds (95 kg; 15 stone), Forrest was physically imposing and intimidating, especially compared to the average height of men at the time. He used his skills as a hard rider and fierce swordsman to great effect; he was known to sharpen both the top and bottom edges of his heavy saber.
Historians have evaluated contemporary records to conclude that Forrest may have killed more than 30 enemy soldiers with saber, pistol, and shotgun. Not all of Forrest's feats of individual combat involved enemy troops. Lt. A. Wills Gould, an artillery officer in Forrest's command, was being transferred, presumably because cannons under his command were spiked by the enemy during the Battle of Day's Gap. On June 14, 1863, Gould confronted Forrest about his transfer, which escalated into a violent exchange. Gould shot Forrest in the hip; Forrest mortally stabbed his assailant.
Forrest's command included his Escort Company (his "Special Forces"), for which he selected the best soldiers available. This unit, which varied in size from 40 to 90 men, was the elite of the cavalry.
Cavalry command.
Forrest received praise for his skill and courage during an early victory in the Battle of Sacramento in Kentucky, where he routed a Union force by personally leading a cavalry charge that was later commended by his commander, Brigadier General Charles Clark. Forrest distinguished himself further at the Battle of Fort Donelson in February 1862. After his cavalry captured a Union artillery battery, he broke out of a Union Army siege headed by Major General Ulysses S. Grant. Forrest rallied nearly 4,000 troops and led them across the river.
A few days after Fort Donelson, with the fall of Nashville to Union forces imminent, Forrest took command of the city. Local industries had several millions of dollars worth of heavy ordnance machinery. Forrest arranged for transport of the machinery and several important government officials to safe locations.
A month later, Forrest was back in action at the Battle of Shiloh (April 6 to 7, 1862). He commanded a Confederate rear guard after the Union victory. In the battle of Fallen Timbers, he drove through the Union skirmish line. Not realizing that the rest of his men had halted their charge when reaching the full Union brigade, Forrest charged the brigade single-handedly, and soon found himself surrounded. He emptied his Colt Army revolvers into the swirling mass of Union soldiers and pulled out his saber, hacking and slashing. A Union infantryman fired a musket ball into Forrest's spine with a point-blank musket shot, nearly knocking him out of the saddle. Forrest galloped back to his incredulous troopers. A surgeon removed the musket ball a week later, without anesthesia, which was unavailable. Forrest would likely have been given a generous dose of alcohol to muffle the pain of the surgery.
By early summer, Forrest commanded a new brigade of "green" cavalry regiments. In July, he led them into Middle Tennessee under orders to launch a cavalry raid. On July 13, 1862, he led them into the First Battle of Murfreesboro, which Forrest is said to have won.
According to a report by a Union commander:
The forces attacking my camp were the First Regiment Texas Rangers Texas Cavalry, Terry's Texas Rangers, ed., Colonel Wharton, and a battalion of the First Georgia Rangers, Colonel Morrison, and a large number of citizens of Rutherford County, many of whom had recently taken the oath of allegiance to the United States Government. There were also quite a number of negroes attached to the Texas and Georgia troops, who were armed and equipped, and took part in the several engagements with my forces during the day.
Promoted in July 1862 to brigadier general, Forrest was given command of a Confederate cavalry brigade. In December 1862, Forrest's veteran troopers were reassigned by Gen. Braxton Bragg to another officer, against his protest. Forrest had to recruit a new brigade, composed of about 2,000 inexperienced recruits, most of whom lacked weapons. Again, Bragg ordered a raid, this one into west Tennessee to disrupt the communications of the Union forces under Grant, which were threatening the city of Vicksburg, Mississippi. Forrest protested that to send such untrained men behind enemy lines was suicidal, but Bragg insisted, and Forrest obeyed his orders. On the ensuing raid, he showed his brilliance, leading thousands of Union soldiers in west Tennessee on a "wild goose chase" to try to locate his fast-moving forces. Never staying in one place long enough to be attacked, Forrest led his troops in raids as far north as the banks of the Ohio River in southwest Kentucky. He returned to his base in Mississippi with more men than he had started with. By then, all were fully armed with captured Union weapons. As a result, General Grant was forced to revise and delay the strategy of his Vicksburg campaign. "He was the only Confederate cavalryman of whom Grant stood in much dread," a friend of Ulysses was quoted as saying.
The Union Army occupied Tennessee in 1862 and for the duration of the war, taking control of strategic cities and railroads. Forrest continued to lead his men in small-scale operations until April 1863. The Confederate army dispatched him with a small force into the backcountry of northern Alabama and west Georgia to defend against an attack of 3,000 Union cavalrymen commanded by Colonel Abel Streight. Streight had orders to cut the Confederate railroad south of Chattanooga, Tennessee, to cut off Bragg's supply line and force him to retreat into Georgia. Forrest chased Streight's men for 16 days, harassing them all the way. Streight's goal changed to escape the pursuit. On May 3, Forrest caught up with Streight's unit east of Cedar Bluff, Alabama. Forrest had fewer men than the Union side, but he repeatedly paraded some of them around a hilltop to appear a larger force, and convinced Streight to surrender his 1,500 exhausted troops.
Forrest served with the main army at the Battle of Chickamauga (September 18 to 20, 1863). He pursued the retreating Union army and took hundreds of prisoners. Like several others under Bragg's command, he urged an immediate follow-up attack to recapture Chattanooga, which had fallen a few weeks before. Bragg failed to do so, upon which Forrest was quoted as saying, "What does he fight battles for?" The story that Forrest confronted and threatened the life of Bragg in the fall of 1863, following the battle of Chickamauga, and that Bragg transferred Forrest to command in Mississippi as a direct result, is now considered to be apocryphal and the invention of Dr. J. B. Cowan. On December 4, 1863, Forrest was promoted to the rank of major general.
On March 25, 1864, Forrest was at Paducah, Kentucky where he unsuccessfully demanded surrender of U.S. Col. Stephen G. Hicks:
... if I have to storm your works, you may expect no quarter.
Fort Pillow.
On April 12, 1864, General Forrest led his forces in the attack and capture of Fort Pillow, in Henning, Tennessee, on the Mississippi River. Many African American Union soldiers were killed in the battle. A controversy arose May 3, 1864 about whether Forrest conducted or condoned a massacre of negro soldiers, white Tennessee Unionists, and Confederate deserters, who had surrendered there. President Abraham Lincoln asked his cabinet for opinions as to how the Union should respond to the massacre.
According to reports filed by Union Captain Goodman, Union forces never surrendered; he said it was agreed that if the fort was surrendered, the whole garrison, white and black, would be treated as prisoners of war. General Forrest sent additional communiques to Major Lionel F. Booth demanding total surrender, but Major Booth had been fatally shot in the battle and the command of Fort Pillow had already been assumed by Major William F. Bradford. The delayed reply to Forrest's demands bore the name of Major Booth, asking for more time to decide about surrendering the fort and the gunboat "Olive Branch". General Forrest replied that the gunboat was not expected to be surrendered, but the fort alone. Hours later during the truce, after many communiques, the Union sent their answer, "a brief but positive refusal to capitulate".
Forrest's men insisted that the Union soldiers, although fleeing, kept their weapons and frequently turned to shoot, forcing the Confederates to keep firing in self defense. Confederates said the Union flag was still flying over the fort, which indicated that the force had not formally surrendered. A contemporary newspaper account from Jackson, Tennessee, stated that "General Forrest begged them to surrender", but "not the first sign of surrender was ever given." Similar accounts were reported in many Southern newspapers at the time.
These statements, however, were contradicted by Union survivors, as well as the letter of a Confederate soldier who graphically recounted a massacre. Achilles Clark, a soldier with the 20th Tennessee cavalry, wrote to his sister immediately after the battle: 
The slaughter was awful. Words cannot describe the scene. The poor, deluded, negroes would run up to our men, fall upon their knees, and with uplifted hands scream for mercy, but they were ordered to their feet and then shot down. I, with several others, tried to stop the butchery, and at one time had partially succeeded, but General Forrest ordered them shot down like dogs and the carnage continued. Finally our men became sick of blood and the firing ceased.
Ulysses S. Grant, in his "Personal Memoirs," says of the battle:
These troops fought bravely, but were overpowered. I will leave Forrest in his dispatches to tell what he did with them. 'The river was dyed,' he says, 'with the blood of the slaughtered for two hundred yards. The approximate loss was upward of five hundred killed, but few of the officers escaping. My loss was about twenty killed. It is hoped that these facts will demonstrate to the Northern people that negro soldiers cannot cope with Southerners.' Subsequently Forrest made a report in which he left out the part which shocks humanity to read.
At the time of the massacre General Grant was no longer in Tennessee but had transferred to the east to command all Union troops. General William Tecumseh Sherman, Commander of the Military Division of the Mississippi, which included Tennessee, wrote:
Historians have differed on interpretation of events. Richard Fuchs, author of "An Unerring Fire", concludes: The affair at Fort Pillow was simply an orgy of death, a mass lynching to satisfy the basest of conduct – intentional murder – for the vilest of reasons – racism and personal enmity. Andrew Ward downplays the controversy: Whether the massacre was premeditated or spontaneous does not address the more fundamental question of whether a massacre took place... it certainly did, in every dictionary sense of the word. John Cimprich states: 
The new paradigm in social attitudes and the fuller use of available evidence has favored a massacre interpretation... Debate over the memory of this incident formed a part of sectional and racial conflicts for many years after the war, but the reinterpretation of the event during the last thirty years offers some hope that society can move beyond past intolerance.The site is now a State Historic Park.
Brice's Crossroads.
Forrest's greatest victory came on June 10, 1864, when his 3,500-man force clashed with 8,500 men commanded by Union Brig. Gen. Samuel D. Sturgis at the Battle of Brice's Crossroads. Here, his mobility of force and superior tactics led to victory. He swept the Union forces from a large expanse of southwest Tennessee and northern Mississippi. Forrest set up a position for an attack to repulse a pursuing force commanded by Sturgis, who had been sent to impede Forrest from destroying Union supplies and fortifications. When Sturgis's Federal army came upon the crossroad, they collided with Forrest's cavalry. Sturgis ordered his infantry to advance to the front line to counteract the cavalry. The infantry, tired and weary and suffering under the heat, were quickly broken and sent into mass retreat. Forrest sent a full charge after the retreating army and captured 16 artillery pieces, 176 wagons, and 1,500 stands of small arms. In all, the maneuver cost Forrest 96 men killed and 396 wounded. The day was worse for Union troops, which suffered 223 killed, 394 wounded, and 1,623 men missing. The losses were a deep blow to the black regiment under Sturgis's command. In the hasty retreat, they stripped off commemorative badges that read "Remember Fort Pillow", to avoid goading the Confederate force pursuing them.
Conclusion of the war.
One month later, while serving under General Stephen D. Lee, Forrest experienced tactical defeat at the Battle of Tupelo in 1864. Concerned about Union supply lines, Maj. Gen. William T. Sherman sent a force under the command of Maj. Gen. Andrew J. Smith to deal with Forrest. The Union forces drove the Confederates from the field and Forrest was wounded in the foot, but his forces were not wholly destroyed. He continued to oppose Union efforts in the West for the remainder of the war.
Forrest led other raids that summer and fall, including a famous one into Union-held downtown Memphis in August 1864 (the Second Battle of Memphis), and another on a Union supply depot at Johnsonville, Tennessee, on October 3, 1864 (the Battle of Johnsonville), causing millions of dollars in damage. In December, during the disastrous Franklin-Nashville Campaign, he fought alongside General John Bell Hood, the newest (and last) commander of the Confederate Army of Tennessee in the Second Battle of Franklin. Forrest argued bitterly with Hood (his superior officer) demanding permission to cross the river and cut off the escape route of Union Maj. Gen. John M. Schofield's army. He made the belated attempt, but was defeated.
After his bloody defeat at Franklin, Hood continued to Nashville. Hood ordered Forrest to conduct an independent raid against the Murfreesboro garrison. After success in achieving the objectives specified by Gen. Hood, Forrest engaged Union forces near Murfreesboro on December 5, 1864. In what would be known as the Third Battle of Murfreesboro, a portion of Forrest's command broke and ran. After Hood's Army of Tennessee was all but destroyed at the Battle of Nashville, Forrest distinguished himself by commanding the Confederate rear guard in a series of actions that allowed what was left of the army to escape. For this, he earned promotion to the rank of lieutenant general. A portion of his command, now dismounted, was surprised and captured in their camp at Verona, Mississippi, on December 25, 1864, during a raid of the Mobile and Ohio Railroad by a brigade of Brig. Gen. Benjamin Grierson's cavalry division.
In 1865, Forrest attempted, without success, to defend the state of Alabama against Wilson's raid. His opponent, Brig. Gen. James H. Wilson, defeated Forrest in battle. When he received news of Lee's surrender, Forrest also chose to surrender. On May 9, 1865, at Gainesville, Forrest read his farewell address.
Forrest's farewell address to his troops, May 9, 1865.
The following text is excerpted from Forrest's farewell address to his troops:
Forrest's Military Doctrines.
Forrest grasped the doctrines of "mobile warfare" that became prevalent in the 20th century. Paramount in his strategy was fast movement, even if it meant pushing his horses at a killing pace, which he did more than once. Noted Civil War scholar Bruce Catton writes:
Forrest ... used his horsemen as a modern general would use motorized infantry. He liked horses because he liked fast movement, and his mounted men could get from here to there much faster than any infantry could; but when they reached the field they usually tied their horses to trees and fought on foot, and they were as good as the very best infantry.
Forrest is often erroneously quoted as saying his strategy was to "git thar fustest with the mostest." Now often recast as "Getting there "firstest" with the mostest", this misquote first appeared in print in a "New York Tribune" article written to provide colorful comments in reaction to European interest in Civil War generals. The aphorism was addressed and corrected by a "New York Times" story in 1918 to be: "Ma'am, I got there first with the most men." Though a novel and succinct condensation of the military principles of mass and maneuver, Bruce Catton writes:
Do not, under any circumstances whatever, quote Forrest as saying 'fustest' and 'mostest'. He did not say it that way, and nobody who knows anything about him imagines that he did.
Forrest became well known for his early use of "maneuver" tactics as applied to a mobile horse cavalry deployment. He sought to constantly harass the enemy in fast-moving raids, and to disrupt supply trains and enemy communications by destroying railroad track and cutting telegraph lines, as he wheeled around the Union Army's flank.
Postwar years.
Business ventures.
With slavery abolished after the war, Forrest suffered a major financial setback as a former slave trader. He became interested in the area around Crowley's Ridge during the war and settled in Memphis, Tennessee. In 1866 Forrest and C. C. McCreanor contracted to finish the Memphis & Little Rock Railroad. He built a commissary in a town forming along the rail route which most residents were calling "Forrest's Town," incorporated as Forrest City, Arkansas in 1870.
He later found employment at the Selma-based Marion & Memphis Railroad and eventually became the company president. He was not as successful in railroad promoting as in war, and under his direction, the company went bankrupt. Nearly ruined as the result of the failure of the Marion & Memphis, Forrest spent his final days running a prison work farm on President's Island in the Mississippi River. There were financial failures across the country in the Panic of 1873. Forrest's health was in steady decline. He and his wife lived in a log cabin they had salvaged from his plantation.
Offers services to Gen. Sherman.
During the Virginius Affair of 1873, some of Forrest's old Southern friends were filibusterers aboard the vessel, so he wrote a letter to then General-in-Chief of the United States Army William Tecumseh Sherman and offered his services in case of war with Spain. Sherman, who in the Civil War had recognized what a deadly foe Forrest was, replied after the crisis settled down by thanking Forrest for the offer and stating that had war broken out he would have considered it an honor to have served side-by-side with him.
Ku Klux Klan Membership.
Klan Membership.
Forrest was an early member of the Ku Klux Klan (KKK). Historian and Forrest biographer Brian Steel Wills writes, "While there is no doubt that Forrest joined the Klan, there is some question as to whether he actually was the Grand Wizard of the Ku Klux Klan." The KKK (the Klan) was formed by veterans of the Confederate Army in Pulaski, Tennessee in 1866 and soon expanded throughout the state and beyond. Forrest became involved sometime in late 1866 or early 1867. A common report is that Forrest arrived in Nashville in April 1867 while the Klan was meeting at the Maxwell House Hotel, probably at the encouragement of a state Klan leader, former Confederate general George Gordon. The organization had grown to the point where an experienced commander was needed, and Forrest fit the bill. In Room 10 of the Maxwell, Forrest was sworn in as a member. In 1869, Forrest distanced himself from the Klan, when it got ugly, hoping to dissolve this incarnation of the KKK as a way of placating Republicans he needed for his new railroad ventures.
After the Civil War had ended, the United States Congress began passing the Reconstruction Acts to lay out requirements for the former Confederate States to be re-admitted to the Union, to include ratification of the Fourteenth Amendment to the United States Constitution. One of its stipulations was specifically granting voting rights to black men. According to Wills, in the August 1867 state elections the Klan was relatively restrained in its actions. White Americans who made up the KKK hoped to persuade black voters that a return to their state of repression and slavery, as it existed before the war, was in their best interest. Forrest assisted in maintaining order. It was after these efforts failed that Klan violence and intimidation escalated and became widespread. Author Andrew Ward, however, writes, "In the spring of 1867, Forrest and his dragoons launched a campaign of midnight parades; 'ghost' masquerades; and 'whipping' and even 'killing Negro voters and white Republicans, to scare blacks off voting and running for office.'"
In an 1868 interview by a Cincinnati newspaper, Forrest claimed that the Klan had 40,000 members in Tennessee and 550,000 total members throughout the Southern states. He said he sympathized with them, but denied any formal connection. He claimed he could muster thousands of men himself. He described the Klan as "a protective political military organization... The members are sworn to recognize the government of the United States... Its objects originally were protection against Loyal Leagues and the Grand Army of the Republic..." Forrest dissolved the first incarnation of the Ku Klux Klan in 1869, although many local groups continued their activities for several years.
Congressional Testimony.
Forrest testified before the Congressional investigation on Klan activities on June 27, 1871. Forrest denied membership, but his individual role in the KKK was beyond the scope of the investigating committee which wrote:
The committee also noted, "The natural tendency of all such organizations is to violence and crime; hence it was that General Forrest and other men of influence in the state, by the exercise of their moral power, induced them to disband."
Speaks to Black Southerners.
In July 1875, Forrest demonstrated that his personal sentiments on the issue of race now differed from that of the Klan, when he was invited to give a speech before an organization of black Southerners advocating racial reconciliation, called the Independent Order of Pole-Bearers Association. At this, his last public appearance, he made what the "New York Times" described as a "friendly speech" during which, when offered a bouquet of flowers by a black woman, he accepted them as a token of reconciliation between the races and espoused a radical agenda (for the time) of equality and harmony between black and white Americans. His speech was as follows:
In response to that Pole-Bearers speech, the Cavalry Survivors Association of Augusta, the first Confederate organization formed after the war, called a meeting in which Captain F. Edgeworth Eve gave a speech expressing unmitigated disapproval of Forrest's remarks promoting inter-ethnic harmony, by ridiculing his faculties and judgement and berating the woman who gifted Forrest flowers as ""a mulatto wench"". The association voted unanimously to amend its constitution to expressly forbid publicly advocating for or hinting at any association of white women and girls as being in the same classes as ""females of the negro race"". The Macon Weekly Telegraph newspaper also condemned Forrest for his speech, describing the event as ""the recent disgusting exhibition of himself at the negro [sic.] jamboree,"" and quoting part of a Charlotte, North Carolina Observer article which read ""We have infinitely more respect for Longstreet, who fraternizes with negro men on public occasions, with the pay for the treason to his race in his pocket, than with Forrest and Pillow, who equalize with the negro women, with only 'futures' in payment.""
Death.
Forrest died in Memphis in October 1877, at the home of his brother Jesse, reportedly from acute complications of diabetes. His eulogy was delivered by his recent spiritual mentor and former Confederate chaplain, George Tucker Stainback, who declared in his eulogy: "Lieutenant-General Nathan Bedford Forrest, though dead, yet speaketh. His acts have photographed themselves upon the hearts of thousands, and will speak there forever."
Forrest was buried at Elmwood Cemetery. In 1904 the remains of Forrest and his wife Mary were disinterred from Elmwood and moved to a Memphis city park originally named Forrest Park in his honor, that has since been renamed Health Sciences Park.
On July 7, 2015, the Memphis City Council unanimously approved to move the remains of Nathan Bedford Forrest and his wife from Health Sciences Park. However, there are laws that protect him and his statue, the Tennessee Heritage Protection Act of 2013 and the U.S. Public Law 85-425: Sec. 410 approved May 23, 1958.
Posthumous legacy.
Many memorials were erected to Forrest in Tennessee, but only in Mississippi is there a county named after him. Obelisks in his memory were placed at his birthplace in Chapel Hill and at Nathan Bedford Forrest State Park near Camden. A statue of General Forrest was erected in Memphis's Forrest Park (renamed Health Sciences Park on February 5, 2013 amid great controversy). A bust sculpted by Jane Baxendale is on display at the Tennessee State Capitol building in Nashville. The World War II Army base Camp Forrest in Tullahoma, Tennessee was named after him. It is now the site of the Arnold Engineering Development Center.
, Tennessee had 32 dedicated historical markers linked to Nathan Bedford Forrest, more than are dedicated to the three former Presidents associated with the state: Andrew Jackson, James K. Polk, and Andrew Johnson (none of whom were born in Tennessee).
Finally, the Tennessee legislature established July 13 as "Nathan Bedford Forrest Day."
A monument to Forrest in the Confederate Circle section of Old Live Oak Cemetery in Selma, Alabama, reads "Defender of Selma, Wizard of the Saddle, Untutored Genius, The first with the most. This monument stands as testament of our perpetual devotion and respect for Lieutenant General Nathan Bedford Forrest. CSA 1821–1877, one of the south's finest heroes. In honor of Gen. Forrest's unwavering defense of Selma, the great state of Alabama, and the Confederacy, this memorial is dedicated. DEO VINDICE." As armory for the Confederacy, Selma provided most of the South's ammunition. The bust of Forrest was stolen from the cemetery monument in March 2012 and efforts are currently underway to restore the monument.
A monument to Forrest in the Myrtle Hill Cemetery in Rome, Georgia, was erected by the United Daughters of the Confederacy in 1909 to honor his bravery for saving Rome from Union Army Colonel Abel Streight and his cavalry.
High schools named for Forrest were built in Chapel Hill, Tennessee, and Jacksonville, Florida. In 2008 the Duval County School Board voted 5–2 against a push to change the name of Nathan Bedford Forrest High School in Jacksonville. In 2013, the Board voted 7-0 to begin the process to rename the school. The school was named for Forrest in 1959 at the urging of the Daughters of the Confederacy because they were upset about the 1954 Brown v. Board of Education decision. At the time the school was all white, but now more than half the student body is black. After several public forums and discussions, Westside High School was unanimously approved in January 2014 as the school's new name.
In August 2000, a road on Fort Bliss named for Forrest decades earlier was renamed for former post commander Richard T. Cassidy.
In 2005, Shelby County Commissioner Walter Bailey started an effort to move the statue over Forrest's grave and rename Forrest Park. Former Memphis Mayor Willie Herenton, who is black, blocked the move. Others have tried to get a bust of Forrest removed from the Tennessee House of Representatives chamber. Leaders in other localities have tried to remove or eliminate Forrest monuments, with mixed success.
The ROTC building at Middle Tennessee State University was named Forrest Hall in his honor. In 2006, the frieze depicting General Forrest on horseback that had adorned the side of this building was removed amid protests, but a major push to change its name failed. Also, the university's Blue Raiders' athletic mascot was changed to a pegasus from a cavalier, in order to avoid its mistaken association with General Forrest.
Forrest's great-grandson, Nathan Bedford Forrest III, pursued a military career, first in cavalry, then in aviation, and attained the rank of brigadier general in the United States Army Air Forces during World War II. On June 13, 1943, Forrest III was killed in action while participating in a bombing raid over Germany, the first U.S. General to be killed in action in World War II. His family was awarded his Distinguished Service Cross (second only to the Medal of Honor) for staying with the controls of his B-17 bomber while his crew bailed out. The aircraft exploded before Forrest could bail out. By the time German air-sea rescue could arrive, only one of the crew was still alive in the water.
In popular culture.
William Faulkner's 1943 short story "My Grandmother Millard and General Bedford Forrest and The Battle of Harrykin Creek" features Forrest as a character. Faulkner's 1938 novel "The Unvanquished" is set against the backdrop of Forrest's engagements with Union general Smith (presumably Andrew J. Smith).
"Bedford Forrest: Boy on Horseback" by Aileen Wells Parks in 1952 is part of the "Childhood of Famous Americans" series.
The 1987 novel "Fightin' With Forrest" tells the story of two young men who ride with Forrest during the War.
In the 1990 PBS documentary "The Civil War" by Ken Burns, historian Shelby Foote states in Episode 7 that the Civil War produced two "authentic geniuses": Abraham Lincoln and Nathan Bedford Forrest; when expressing this opinion to one of General Forrest's granddaughters, she replied after a pause, "You know, we never thought much of Mr. Lincoln in my family." Foote also used Forrest as a major character in his novel "Shiloh". 
Forrest is an important character in Harry Turtledove's 1992 alternate history/science fiction novel "The Guns of the South", where he goes through a sea change as a character. Forrest's great-grandson, Nathan Bedford Forrest III, appears in other Turtledove works.
In the 1994 film "Forrest Gump", the titular character says that he was named after his ancestor General Nathan Bedford Forrest, who "... started up this club called the Ku Klux Klan." Tom Hanks who plays Gump, also makes a cameo as General Forrest, inserted into scenery from "Birth of a Nation".
The children's science fiction series "Animorphs" has flashback scenes of an ancestor of one of the main characters fighting a battle against Forrest's brigade.
In the 2004 mockumentary "" a slave narrator cites Nathan Bedford Forrest as the leader of a Confederate army that massacred hundreds of freed slaves in the North shortly after the Civil War, possibly an alternate reference to the Fort Pillow Massacre.
The 2006 song "The Decline and Fall of Country and Western Civilization" by Lambchop begins with the lines: "I hate Nathan Bedford Forrest / He's the featured artist in the Devil's chorus."
The song "Horse Soldier, Horse Soldier" from the 2007 album of the same name by Corb Lund and the Hurtin' Albertans references Forrest: "I's the firstest with the mostest when I fought for Bedford Forrest."
The Fort Pillow Massacre.
There are conflicting reports about what occurred at Fort Pillow. Only 90 out of approximately 262 US Colored Troops survived the battle. Casualties were also high among white defenders of the fort, with 205 out of about 500 surviving. Forrest's Confederate forces were accused of subjecting captured soldiers to brutality, with allegations that some were burned to death. Forrest's men were alleged to have set fire to Union barracks with wounded Union soldiers inside; however, the report of Union Lieutenant Daniel Van Horn said that act was due to orders carried out by Union Lieutenant John D. Hill. Van Horn also reported that, "There never was a surrender of the fort, both officers and men declaring they never would surrender or ask for quarter."
Following the cessation of hostilities, Forrest transferred the 14 most seriously wounded United States Colored Troops (USCT) to the U.S. Steamer Silver Cloud. He sent 39 USCT taken as prisoners to higher command.
On October 30, 1877, "The New York Times" reported that "General Bedford Forrest, the great Confederate cavalry officer, died at 7:30 o'clock this evening at the residence of his brother, Colonel Jesse Forrest."
But "The Times" also reported that it would not be for military victories that Forrest would pass into history:
These claims were directly disputed in letters, written by Confederate soldiers to their own families, which described wanton brutality on the part of Confederate troops.
The New York newspaper obituary further stated:
Continuing controversies.
On February 10, 2011 Fox News Channel reported that there is a proposal in Mississippi to issue specialty license plates, honoring Forrest, to mark the 150th anniversary of the "War Between the States". Forrest's legacy still draws heated public debate, as he has been called "one of the most controversial – and popular – icons of the war." The Sons of Confederate Veterans helped sponsor a set of Mississippi license plates commemorating the Civil War, for which the 2014 version featuring Forrest drew controversy in 2011. The Mississippi NAACP petitioned Governor Haley Barbour to denounce the plates and prevent their distribution. Barbour refused to denounce the honor, noting instead that the state legislature would not be likely to approve the plate anyway.
In 2000, a monument to Forrest in Selma, Alabama, was unveiled.
On March 10, 2012, it was vandalized and the bronze bust of the general vanished. 
In August, a historical society called Friends of Forrest moved forward with plans for a new, larger monument, which was to be 12 feet high, illuminated by L.E.D. lights, surrounded by a wrought-iron fence and protected by 24-hour security cameras. 
The plans triggered outrage and a group of around 20 protesters attempted to block construction of the new monument by lying in the path of a concrete truck. 
Local lawyer and radio host Rose Sanders said, "Glorifying Nathan B. Forrest here is like glorifying a Nazi in Germany. For Selma, of all places, to have a big monument to a Klansman is totally unacceptable." An online petition at Change.org asking the City Council to ban the monument collected more than 285,000 signatures by mid-September.
In 2015, as a result of the June 17 church shooting in Charleston, South Carolina, some Tennessee lawmakers advocated removing a bust of Forrest located in the State's Capitol building. Subsequently, then-Mayor A.C. Wharton urged removal of the statue of Confederate Lieutenant General Nathan Bedford Forrest in Health Sciences Park and suggested relocation of Bedford Forrest and his wife to their original burial site in nearby Elmwood Cemetery. In a nearly unanimous vote on July 7, the Memphis City Council passed a resolution in favor of removing the statue and securing the couple's remains for transfer. The issue now goes before the Tennessee Historical Commission for final approval to carry out the decision. The commission was given this authority under the Tennessee Heritage Protection Act of 2013. Thomas Robb, national director of the Knights of the Ku Klux Klan and pastor of the Christian Revival Center in Harrison, Arkansas, announced on August 12, 2015 that he is requesting transfer of the remains of Forrest and his wife, as well as the monument in Health Sciences Park, to his church. He also stated that the Christian Revival Center would take the burden of all cost involved in the removal and transfer and would be willing to pay reasonable compensation. The formal request was made through the law office of Jason M. Robb.

</doc>
<doc id="49598" url="https://en.wikipedia.org/wiki?curid=49598" title="Pigment">
Pigment

A pigment is a material that changes the color of reflected or transmitted light as the result of wavelength-selective absorption. This physical process differs from fluorescence, phosphorescence, and other forms of luminescence, in which a material emits light.
Many materials selectively absorb certain wavelengths of light. Materials that humans have chosen and developed for use as pigments usually have special properties that make them ideal for coloring other materials. A pigment must have a high tinting strength relative to the materials it colors. It must be stable in solid form at ambient temperatures.
For industrial applications, as well as in the arts, permanence and stability are desirable properties. Pigments that are not permanent are called fugitive. Fugitive pigments fade over time, or with exposure to light, while some eventually blacken.
Pigments are used for coloring paint, ink, plastic, fabric, cosmetics, food, and other materials. Most pigments used in manufacturing and the visual arts are dry colorants, usually ground into a fine powder. This powder is added to a binder (or vehicle), a relatively neutral or colorless material that suspends the pigment and gives the paint its adhesion.
A distinction is usually made between a pigment, which is insoluble in its vehicle (resulting in a suspension), and a dye, which either is itself a liquid or is soluble in its vehicle (resulting in a solution). A colorant can act as either a pigment or a dye depending on the vehicle involved. In some cases, a pigment can be manufactured from a dye by precipitating a soluble dye with a metallic salt. The resulting pigment is called a lake pigment. The term biological pigment is used for all colored substances independent of their solubility.
In 2006, around 7.4 million tons of inorganic, organic and special pigments were marketed worldwide. Asia has the highest rate on a quantity basis followed by Europe and North America. By 2020, revenues will have risen to approx. US$34.2 billion. The global demand on pigments was roughly US$20.5 billion in 2009, around 1.5-2% up from the previous year. It is predicted to increase in a stable growth rate in the coming years. The worldwide sales are said to increase up to US$24.5 billion in 2015, and reach US$27.5 billion in 2018.
Physical basis.
Pigments appear the colors they are because they selectively reflect and absorb certain wavelengths of visible light. White light is a roughly equal mixture of the entire spectrum of visible light with a wavelength in a range from about 375 or 400 nanometers to about 760 or 780 nm. When this light encounters a pigment, parts of the spectrum are absorbed by the molecules or ions of the pigment. In organic pigments such as diazo or phthalocyanine compounds the light is absorbed by the conjugated systems of double bonds in the molecule. Some of the inorganic pigments such as vermilion (mercury sulfide) or cadmium yellow (cadmium sulfide) absorb light by transferring an electron from the negative ion (S2-) to the positive ion (Hg2+ or Cd2+). Such compounds are designated as charge-transfer complexes, with broad absorption bands that subtract most of the colors of the incident white light. The other wavelengths or parts of the spectrum are reflected or scattered. The new reflected light spectrum creates the appearance of a color. Pigments, unlike fluorescent or phosphorescent substances, can only subtract wavelengths from the source light, never add new ones.
The appearance of pigments is intimately connected to the color of the source light. Sunlight has a high color temperature, and a fairly uniform spectrum, and is considered a standard for white light. Artificial light sources tend to have great peaks in some parts of their spectrum, and deep valleys in others. Viewed under these conditions, pigments will appear different colors.
Color spaces used to represent colors numerically must specify their light source. Lab color measurements, unless otherwise noted, assume that the measurement was taken under a D65 light source, or "Daylight 6500 K", which is roughly the color temperature of sunlight.
Other properties of a color, such as its saturation or lightness, may be determined by the other substances that accompany pigments. Binders and fillers added to pure pigment chemicals also have their own reflection and absorption patterns, which can affect the final spectrum. Likewise, in pigment/binder mixtures, individual rays of light may not encounter pigment molecules, and may be reflected as is. These stray rays of source light contribute to a slightly less saturated color. Pure pigment allows very little white light to escape, producing a highly saturated color. A small quantity of pigment mixed with a lot of white binder, however, will appear desaturated and pale, due to the high quantity of escaping white light.
History.
Naturally occurring pigments such as ochres and iron oxides have been used as colorants since prehistoric times. Archaeologists have uncovered evidence that early humans used paint for aesthetic purposes such as body decoration. Pigments and paint grinding equipment believed to be between 350,000 and 400,000 years old have been reported in a cave at Twin Rivers, near Lusaka, Zambia.
Before the Industrial Revolution, the range of color available for art and decorative uses was technically limited. Most of the pigments in use were earth and mineral pigments, or pigments of biological origin. Pigments from unusual sources such as botanical materials, animal waste, insects, and mollusks were harvested and traded over long distances. Some colors were costly or impossible to mix with the range of pigments that were available. Blue and purple came to be associated with royalty because of their expense.
Biological pigments were often difficult to acquire, and the details of their production were kept secret by the manufacturers. Tyrian Purple is a pigment made from the mucus of one of several species of Murex snail. Production of Tyrian Purple for use as a fabric dye began as early as 1200 BCE by the Phoenicians, and was continued by the Greeks and Romans until 1453 CE, with the fall of Constantinople. The pigment was expensive and complex to produce, and items colored with it became associated with power and wealth. Greek historian Theopompus, writing in the 4th century BCE, reported that "purple for dyes fetched its weight in silver at Colophon Asia Minor."
Mineral pigments were also traded over long distances. The only way to achieve a deep rich blue was by using a semi-precious stone, lapis lazuli, to produce a pigment known as ultramarine, and the best sources of lapis were remote. Flemish painter Jan van Eyck, working in the 15th century, did not ordinarily include blue in his paintings. To have one's portrait commissioned and painted with ultramarine blue was considered a great luxury. If a patron wanted blue, they were obliged to pay extra. When Van Eyck used lapis, he never blended it with other colors. Instead he applied it in pure form, almost as a decorative glaze. The prohibitive price of lapis lazuli forced artists to seek less expensive replacement pigments, both mineral (azurite, smalt) and biological (indigo).
Spain's conquest of a New World empire in the 16th century introduced new pigments and colors to peoples on both sides of the Atlantic. Carmine, a dye and pigment derived from a parasitic insect found in Central and South America, attained great status and value in Europe. Produced from harvested, dried, and crushed cochineal insects, carmine could be, and still is, used in fabric dye, food dye, body paint, or in its solid lake form, almost any kind of paint or cosmetic.
Natives of Peru had been producing cochineal dyes for textiles since at least 700 CE, but Europeans had never seen the color before. When the Spanish invaded the Aztec empire in what is now Mexico, they were quick to exploit the color for new trade opportunities. Carmine became the region's second most valuable export next to silver. Pigments produced from the cochineal insect gave the Catholic cardinals their vibrant robes and the English "Redcoats" their distinctive uniforms. The true source of the pigment, an insect, was kept secret until the 18th century, when biologists discovered the source.
While Carmine was popular in Europe, blue remained an exclusive color, associated with wealth and status. The 17th-century Dutch master Johannes Vermeer often made lavish use of lapis lazuli, along with Carmine and Indian yellow, in his vibrant paintings.
Development of synthetic pigments.
The earliest known pigments were natural minerals. Natural iron oxides give a range of colors and are found in many Paleolithic and Neolithic cave paintings. Two examples include Red Ochre, anhydrous Fe2O3, and the hydrated Yellow Ochre (Fe2O3.H2O). Charcoal, or carbon black, has also been used as a black pigment since prehistoric times.
Two of the first synthetic pigments were white lead (basic lead carbonate, (PbCO3)2Pb(OH)2) and blue frit (Egyptian Blue). White lead is made by combining lead with vinegar (acetic acid, CH3COOH) in the presence of CO2. Blue frit is calcium copper silicate and was made from glass colored with a copper ore, such as malachite. These pigments were used as early as the second millennium BCE Later premodern additions to the range of synthetic pigments included vermillion, verdigris and lead-tin-yellow.
The Industrial and Scientific Revolutions brought a huge expansion in the range of synthetic pigments, pigments that are manufactured or refined from naturally occurring materials, available both for manufacturing and artistic expression. Because of the expense of Lapis Lazuli, much effort went into finding a less costly blue pigment.
Prussian Blue was the first modern synthetic pigment, discovered by accident in 1704. By the early 19th century, synthetic and metallic blue pigments had been added to the range of blues, including French ultramarine, a synthetic form of lapis lazuli, and the various forms of Cobalt and Cerulean Blue. In the early 20th century, organic chemistry added Phthalo Blue, a synthetic, organometallic pigment with overwhelming tinting power.
Discoveries in color science created new industries and drove changes in fashion and taste. The discovery in 1856 of mauveine, the first aniline dye, was a forerunner for the development of hundreds of synthetic dyes and pigments like azo and diazo compounds which are the source of a wide spectrum of colors. Mauveine was discovered by an 18-year-old chemist named William Henry Perkin, who went on to exploit his discovery in industry and become wealthy. His success attracted a generation of followers, as young scientists went into organic chemistry to pursue riches. Within a few years, chemists had synthesized a substitute for madder in the production of Alizarin Crimson. By the closing decades of the 19th century, textiles, paints, and other commodities in colors such as red, crimson, blue, and purple had become affordable.
Development of chemical pigments and dyes helped bring new industrial prosperity to Germany and other countries in northern Europe, but it brought dissolution and decline elsewhere. In Spain's former New World empire, the production of cochineal colors employed thousands of low-paid workers. The Spanish monopoly on cochineal production had been worth a fortune until the early 19th century, when the Mexican War of Independence and other market changes disrupted production. Organic chemistry delivered the final blow for the cochineal color industry. When chemists created inexpensive substitutes for carmine, an industry and a way of life went into steep decline.
New sources for historic pigments.
Before the Industrial Revolution, many pigments were known by the location where they were produced. Pigments based on minerals and clays often bore the name of the city or region where they were mined. Raw Sienna and Burnt Sienna came from Siena, Italy, while Raw Umber and Burnt Umber came from Umbria. These pigments were among the easiest to synthesize, and chemists created modern colors based on the originals that were more consistent than colors mined from the original ore bodies. But the place names remained.
Historically and culturally, many famous natural pigments have been replaced with synthetic pigments, while retaining historic names. In some cases, the original color name has shifted in meaning, as a historic name has been applied to a popular modern color. By convention, a contemporary mixture of pigments that replaces a historical pigment is indicated by calling the resulting color a hue, but manufacturers are not always careful in maintaining this distinction. The following examples illustrate the shifting nature of historic pigment names:
Manufacturing and industrial standards.
Before the development of synthetic pigments, and the refinement of techniques for extracting mineral pigments, batches of color were often inconsistent. With the development of a modern color industry, manufacturers and professionals have cooperated to create international standards for identifying, producing, measuring, and testing colors.
First published in 1905, the Munsell color system became the foundation for a series of color models, providing objective methods for the measurement of color. The Munsell system describes a color in three dimensions, hue, value (lightness), and chroma (color purity), where chroma is the difference from gray at a given hue and value.
By the middle years of the 20th century, standardized methods for pigment chemistry were available, part of an international movement to create such standards in industry. The International Organization for Standardization (ISO) develops technical standards for the manufacture of pigments and dyes. ISO standards define various industrial and chemical properties, and how to test for them. The principal ISO standards that relate to all pigments are as follows:
Other ISO standards pertain to particular classes or categories of pigments, based on their chemical composition, such as ultramarine pigments, titanium dioxide, iron oxide pigments, and so forth.
Many manufacturers of paints, inks, textiles, plastics, and colors have voluntarily adopted the Colour Index International (CII) as a standard for identifying the pigments that they use in manufacturing particular colors. First published in 1925, and now published jointly on the web by the Society of Dyers and Colourists (United Kingdom) and the American Association of Textile Chemists and Colorists (USA), this index is recognized internationally as the authoritative reference on colorants. It encompasses more than 27,000 products under more than 13,000 generic color index names.
In the CII schema, each pigment has a generic index number that identifies it chemically, regardless of proprietary and historic names. For example, Phthalocyanine Blue BN has been known by a variety of generic and proprietary names since its discovery in the 1930s. In much of Europe, phthalocyanine blue is better known as Helio Blue, or by a proprietary name such as Winsor Blue. An American paint manufacturer, Grumbacher, registered an alternate spelling (Thalo Blue) as a trademark. Colour Index International resolves all these conflicting historic, generic, and proprietary names so that manufacturers and consumers can identify the pigment (or dye) used in a particular color product. In the CII, all phthalocyanine blue pigments are designated by a generic color index number as either PB15 or PB16, short for pigment blue 15 and pigment blue 16; these two numbers reflect slight variations in molecular structure that produce a slightly more greenish or reddish blue.
Scientific and technical issues.
Selection of a pigment for a particular application is determined by cost, and by the physical properties and attributes of the pigment itself. For example, a pigment that is used to color glass must have very high heat stability in order to survive the manufacturing process; but, suspended in the glass vehicle, its resistance to alkali or acidic materials is not an issue. In artistic paint, heat stability is less important, while lightfastness and toxicity are greater concerns.
The following are some of the attributes of pigments that determine their suitability for particular manufacturing processes and applications:
Swatches.
Swatches are used to communicate colors accurately. For different media like printing, computers, plastics, and textiles, different type of swatches are used. Generally, the media which offers widest gamut of color shades is widely used across different media.
Printed swatches.
There are many reference standards providing printed swatches of color shades. PANTONE, RAL, Munsell etc. are widely used standards of color communication across different media like printing, plastics, and textiles.
Plastic swatches.
Companies manufacturing color masterbatches and pigments for plastics offer plastic swatches in injection molded color chips. These color chips are supplied to the designer or customer to choose and select the color for their specific plastic products.
Plastic swatches are available in various special effects like pearl, metallic, fluorescent, sparkle, mosaic etc. However, these effects are difficult to replicate on other media like print and computer display. wherein they have created plastic swatches on website by 3D modelling to including various special effects.
Computer swatches.
Pure pigments reflect light in a very specific way that cannot be precisely duplicated by the discrete light emitters in a computer display. However, by making careful measurements of pigments, close approximations can be made. The Munsell Color System provides a good conceptual explanation of what is missing. Munsell devised a system that provides an objective measure of color in three dimensions: hue, value (or lightness), and chroma. Computer displays in general are unable to show the true chroma of many pigments, but the hue and lightness can be reproduced with relative accuracy. However, when the gamma of a computer display deviates from the reference value, the hue is also systematically biased.
The following approximations assume a display device at gamma 2.2, using the sRGB color space. The further a display device deviates from these standards, the less accurate these swatches will be. Swatches are based on the average measurements of several lots of single-pigment watercolor paints, converted from Lab color space to sRGB color space for viewing on a computer display. Different brands and lots of the same pigment may vary in color. Furthermore, pigments have inherently complex reflectance spectra that will render their color appearancegreatly different depending on the spectrum of the source illumination; a property called metamerism. Averaged measurements of pigment samples will only yield approximations of their true appearance under a specific source of illumination. Computer display systems use a technique called chromatic adaptation transforms to emulate the correlated color temperature of illumination sources, and cannot perfectly reproduce the intricate spectral combinations originally seen. In many cases, the perceived color of a pigment falls outside of the gamut of computer displays and a method called gamut mapping is used to approximate the true appearance. Gamut mapping trades off any one of lightness, hue, or saturation accuracy to render the color on screen, depending on the priority chosen in the conversion's ICC rendering intent.
Biological pigments.
In biology, a pigment is any colored material of plant or animal cells. Many biological structures, such as skin, eyes, fur, and hair contain pigments (such as melanin).
Animal skin coloration often comes about through specialized cells called chromatophores, which animals such as the octopus and chameleon can control to vary the animal's color. Many conditions affect the levels or nature of pigments in plant, animal, some protista, or fungus cells. For instance, the disorder called albinism affects the level of melanin production in animals.
Pigmentation in organisms serves many biological purposes, including camouflage, mimicry, aposematism (warning), sexual selection and other forms of signalling, photosynthesis (in plants), as well as basic physical purposes such as protection from sunburn.
Pigment color differs from structural color in that pigment color is the same for all viewing angles, whereas structural color is the result of selective reflection or iridescence, usually because of multilayer structures. For example, butterfly wings typically contain structural color, although many butterflies have cells that contain pigment as well.

</doc>
<doc id="49600" url="https://en.wikipedia.org/wiki?curid=49600" title="Mantophasmatidae">
Mantophasmatidae

Mantophasmatidae is a family of carnivorous insects within the order Notoptera, which was discovered in Africa in 2001. Originally, the group was regarded as an order in its own right, and named Mantophasmatodea, but based on recent evidence indicating a sister group relationship with Grylloblattidae (formerly classified in the order Grylloblattodea), Arillo & Engel have combined the two groups into a single order, Notoptera.
Overview.
The most common vernacular name for this order is gladiators, although they also are called rock crawlers, heelwalkers, mantophasmids, and colloquially, mantos. Their modern centre of endemism is western South Africa and Namibia (Brandberg Massif), although a relict population, and Eocene fossils suggest a wider ancient distribution.
Mantophasmatodea are wingless even as adults, making them relatively difficult to identify. They resemble a mix between praying mantids and phasmids, and molecular evidence indicates that they are most closely related to the equally enigmatic group Grylloblattodea. Initially, the gladiators were described from old museum specimens that originally were found in Namibia ("Mantophasma zephyrum") and Tanzania ("M. subsolanum"), and from a 45-million-year-old specimen of Baltic amber ("Raptophasma kerneggeri").
Live specimens were found in Namibia by an international expedition in early 2002; "Tyrannophasma gladiator" was found on the Brandberg Massif, and "Mantophasma zephyrum" was found on the Erongoberg Massif.
Classification.
The most recent classification recognizes numerous genera, including fossils:
Some taxonomists assign full family status to the subfamilies and tribes, and sub-ordinal status to the family.

</doc>
<doc id="49603" url="https://en.wikipedia.org/wiki?curid=49603" title="Colosseum">
Colosseum

The Colosseum or Coliseum ( ), also known as the Flavian Amphitheatre (Latin: "Amphitheatrum Flavium"; Italian: "Anfiteatro Flavio" or "Colosseo" ), is an oval amphitheatre in the centre of the city of Rome, Italy. Built of concrete and sand, it is the largest amphitheatre ever built. The Colosseum is situated just east of the Roman Forum. Construction began under the emperor Vespasian in AD 72, and was completed in AD 80 under his successor and heir Titus. Further modifications were made during the reign of Domitian (81–96). These three emperors are known as the Flavian dynasty, and the amphitheatre was named in Latin for its association with their family name "(Flavius)".
The Colosseum could hold, it is estimated, between 50,000 and 80,000 spectators, having an average audience of some 65,000; it was used for gladiatorial contests and public spectacles such as mock sea battles (for only a short time as the "hypogeum" was soon filled in with mechanisms to support the other activities), animal hunts, executions, re-enactments of famous battles, and dramas based on Classical mythology. The building ceased to be used for entertainment in the early medieval era. It was later reused for such purposes as housing, workshops, quarters for a religious order, a fortress, a quarry, and a Christian shrine.
Although partially ruined because of damage caused by earthquakes and stone-robbers, the Colosseum is still an iconic symbol of Imperial Rome. It is one of Rome's most popular tourist attractions and has also links to the Roman Catholic Church, as each Good Friday the Pope leads a torchlit "Way of the Cross" procession that starts in the area around the Colosseum.
The Colosseum is also depicted on the Italian version of the five-cent euro coin.
The Colosseum's original Latin name was "Amphitheatrum Flavium", often anglicized as "Flavian Amphitheater". The building was constructed by emperors of the Flavian dynasty, following the reign of Nero. This name is still used in modern English, but generally the structure is better known as the Colosseum. In antiquity, Romans may have referred to the Colosseum by the unofficial name "Amphitheatrum Caesareum" (with "Caesareum" an adjective pertaining to the title "Caesar"), but this name may have been strictly poetic as it was not exclusive to the Colosseum; Vespasian and Titus, builders of the Colosseum, also constructed an amphitheater of the same name in Puteoli (modern Pozzuoli).
The name "Colosseum" has long been believed to be derived from a colossal statue of Nero nearby (the statue of Nero was named after the Colossus of Rhodes). This statue was later remodeled by Nero's successors into the likeness of Helios ("Sol") or Apollo, the sun god, by adding the appropriate solar crown. Nero's head was also replaced several times with the heads of succeeding emperors. Despite its pagan links, the statue remained standing well into the medieval era and was credited with magical powers. It came to be seen as an iconic symbol of the permanence of Rome.
In the 8th century, a famous epigram attributed to the Venerable Bede celebrated the symbolic significance of the statue in a prophecy that is variously quoted: "Quamdiu stat Colisæus, stat et Roma; quando cadet colisæus, cadet et Roma; quando cadet Roma, cadet et mundus" ("as long as the Colossus stands, so shall Rome; when the Colossus falls, Rome shall fall; when Rome falls, so falls the world"). This is often mistranslated to refer to the Colosseum rather than the Colossus (as in, for instance, Byron's poem "Childe Harold's Pilgrimage"). However, at the time that the Pseudo-Bede wrote, the masculine noun "coliseus" was applied to the statue rather than to what was still known as the Flavian amphitheatre.
The Colossus did eventually fall, possibly being pulled down to reuse its bronze. By the year 1000 the name "Colosseum" had been coined to refer to the amphitheatre. The statue itself was largely forgotten and only its base survives, situated between the Colosseum and the nearby Temple of Venus and Roma.
The name further evolved to "Coliseum" during the Middle Ages. In Italy, the amphitheatre is still known as "il Colosseo", and other Romance languages have come to use similar forms such as "Coloseumul" (Romanian), "le Colisée" (French), "el Coliseo" (Spanish) and "o Coliseu" (Portuguese).
History.
Construction, inauguration, and Roman renovations.
The site chosen was a flat area on the floor of a low valley between the Caelian, Esquiline and Palatine Hills, through which a canalised stream ran. By the 2nd century BC the area was densely inhabited. It was devastated by the Great Fire of Rome in AD 64, following which Nero seized much of the area to add to his personal domain. He built the grandiose "Domus Aurea" on the site, in front of which he created an artificial lake surrounded by pavilions, gardens and porticoes. The existing "Aqua Claudia" aqueduct was extended to supply water to the area and the gigantic bronze Colossus of Nero was set up nearby at the entrance to the Domus Aurea. Although the Colossus was preserved, much of the Domus Aurea was torn down. The lake was filled in and the land reused as the location for the new Flavian Amphitheatre. Gladiatorial schools and other support buildings were constructed nearby within the former grounds of the Domus Aurea. Vespasian's decision to build the Colosseum on the site of Nero's lake can be seen as a populist gesture of returning to the people an area of the city which Nero had appropriated for his own use. In contrast to many other amphitheatres, which were located on the outskirts of a city, the Colosseum was constructed in the city centre; in effect, placing it both symbolically and precisely at the heart of Rome.
Construction was funded by the opulent spoils taken from the Jewish Temple after the Great Jewish Revolt in 70 AD led to the Siege of Jerusalem. According to a reconstructed inscription found on the site, "the emperor Vespasian ordered this new amphitheatre to be erected from his general's share of the booty." Along with the spoils, estimated 100,000 Jewish prisoners were brought back to Rome after the war, and many contributed to the massive workforce needed for construction. The slaves undertook manual labor such as working in the quarries at Tivoli where the travertine was quarried, along with lifting and transporting the quarried stones 20 miles from Tivoli to Rome. Along with this free source of unskilled labor, teams of professional Roman builders, engineers, artists, painters and decorators undertook the more specialized tasks necessary for building the Colosseum.
Construction of the Colosseum began under the rule of Vespasian in around 70–72 AD (73-75 AD according to some sources) The Colosseum had been completed up to the third story by the time of Vespasian's death in 79. The top level was finished by his son, Titus, in 80, and the inaugural games were held in A.D. 80 or 81. Dio Cassius recounts that over 9,000 wild animals were killed during the inaugural games of the amphitheatre. Commemorative coinage was issued celebrating the inauguration. The building was remodelled further under Vespasian's younger son, the newly designated Emperor Domitian, who constructed the "hypogeum", a series of underground tunnels used to house animals and slaves. He also added a gallery to the top of the Colosseum to increase its seating capacity.
In 217, the Colosseum was badly damaged by a major fire (caused by lightning, according to Dio Cassius) which destroyed the wooden upper levels of the amphitheatre's interior. It was not fully repaired until about 240 and underwent further repairs in 250 or 252 and again in 320. Gladiatorial fights are last mentioned around 435. An inscription records the restoration of various parts of the Colosseum under Theodosius II and Valentinian III (reigned 425–455), possibly to repair damage caused by a major earthquake in 443; more work followed in 484 and 508. The arena continued to be used for contests well into the 6th century. Animal hunts continued until at least 523, when Anicius Maximus celebrated his consulship with some "venationes", criticised by King Theodoric the Great for their high cost.
Medieval.
The Colosseum underwent several radical changes of use during the medieval period. By the late 6th century a small chapel had been built into the structure of the amphitheater, though this apparently did not confer any particular religious significance on the building as a whole. The arena was converted into a cemetery. The numerous vaulted spaces in the arcades under the seating were converted into housing and workshops, and are recorded as still being rented out as late as the 12th century. Around 1200 the Frangipani family took over the Colosseum and fortified it, apparently using it as a castle.
Severe damage was inflicted on the Colosseum by the great earthquake in 1349, causing the outer south side, lying on a less stable alluvial terrain, to collapse. Much of the tumbled stone was reused to build palaces, churches, hospitals and other buildings elsewhere in Rome. A religious order moved into the northern third of the Colosseum in the mid-14th century and continued to inhabit it until as late as the early 19th century. The interior of the amphitheater was extensively stripped of stone, which was reused elsewhere, or (in the case of the marble façade) was burned to make quicklime. The bronze clamps which held the stonework together were pried or hacked out of the walls, leaving numerous pockmarks which still scar the building today.
Modern.
During the 16th and 17th century, Church officials sought a productive role for the Colosseum. Pope Sixtus V (1585–1590) planned to turn the building into a wool factory to provide employment for Rome's prostitutes, though this proposal fell through with his premature death. In 1671 Cardinal Altieri authorized its use for bullfights; a public outcry caused the idea to be hastily abandoned.
In 1749, Pope Benedict XIV endorsed the view that the Colosseum was a sacred site where early Christians had been martyred. He forbade the use of the Colosseum as a quarry and consecrated the building to the Passion of Christ and installed Stations of the Cross, declaring it sanctified by the blood of the Christian martyrs who perished there ("see Significance in Christianity"). However, there is no historical evidence to support Benedict's claim, nor is there even any evidence that anyone prior to the 16th century suggested this might be the case; the "Catholic Encyclopedia" concludes that there are no historical grounds for the supposition, other than the reasonably plausible conjecture that some of the many martyrs may well have been.
Later popes initiated various stabilization and restoration projects, removing the extensive vegetation which had overgrown the structure and threatened to damage it further. The façade was reinforced with triangular brick wedges in 1807 and 1827, and the interior was repaired in 1831, 1846 and in the 1930s. The arena substructure was partly excavated in 1810–1814 and 1874 and was fully exposed under Benito Mussolini in the 1930s.
The Colosseum is today one of Rome's most popular tourist attractions, receiving millions of visitors annually. The effects of pollution and general deterioration over time prompted a major restoration programme carried out between 1993 and 2000, at a cost of 40 billion Italian lire ($19.3m / €20.6m at 2000 prices).
In recent years the Colosseum has become a symbol of the international campaign against capital punishment, which was abolished in Italy in 1948. Several anti–death penalty demonstrations took place in front of the Colosseum in 2000. Since that time, as a gesture against the death penalty, the local authorities of Rome change the color of the Colosseum's night time illumination from white to gold whenever a person condemned to the death penalty anywhere in the world gets their sentence commuted or is released, or if a jurisdiction abolishes the death penalty. Most recently, the Colosseum was illuminated in gold in November 2012 following the abolishment of capital punishment in the American state of Connecticut in April 2012.
Because of the ruined state of the interior, it is impractical to use the Colosseum to host large events; only a few hundred spectators can be accommodated in temporary seating. However, much larger concerts have been held just outside, using the Colosseum as a backdrop. Performers who have played at the Colosseum in recent years have included Ray Charles (May 2002), Paul McCartney (May 2003), Elton John (September 2005), and Billy Joel (July 2006).
Physical description.
Exterior.
Unlike earlier Greek theatres that were built into hillsides, the Colosseum is an entirely free-standing structure. It derives its basic exterior and interior architecture from that of two Roman theatres back to back. It is elliptical in plan and is 189 meters (615 ft / 640 Roman feet) long, and 156 meters (510 ft / 528 Roman feet) wide, with a base area of . The height of the outer wall is 48 meters (157 ft / 165 Roman feet). The perimeter originally measured 545 meters (1,788 ft / 1,835 Roman feet). The central arena is an oval 87 m (287 ft) long and 55 m (180 ft) wide, surrounded by a wall 5 m (15 ft) high, above which rose tiers of seating.
The outer wall is estimated to have required over of travertine stone which were set without mortar; they were held together by 300 tons of iron clamps. However, it has suffered extensive damage over the centuries, with large segments having collapsed following earthquakes. The north side of the perimeter wall is still standing; the distinctive triangular brick wedges at each end are modern additions, having been constructed in the early 19th century to shore up the wall. The remainder of the present-day exterior of the Colosseum is in fact the original interior wall.
The surviving part of the outer wall's monumental façade comprises three stories of superimposed arcades surmounted by a podium on which stands a tall attic, both of which are pierced by windows interspersed at regular intervals. The arcades are framed by half-columns of the Doric, Ionic, and Corinthian orders, while the attic is decorated with Corinthian pilasters. Each of the arches in the second- and third-floor arcades framed statues, probably honoring divinities and other figures from Classical mythology.
Two hundred and forty mast corbels were positioned around the top of the attic. They originally supported a retractable awning, known as the "velarium", that kept the sun and rain off spectators. This consisted of a canvas-covered, net-like structure made of ropes, with a hole in the center. It covered two-thirds of the arena, and sloped down towards the center to catch the wind and provide a breeze for the audience. Sailors, specially enlisted from the Roman naval headquarters at Misenum and housed in the nearby "Castra Misenatium", were used to work the "velarium".
The Colosseum's huge crowd capacity made it essential that the venue could be filled or evacuated quickly. Its architects adopted solutions very similar to those used in modern stadiums to deal with the same problem. The amphitheatre was ringed by eighty entrances at ground level, 76 of which were used by ordinary spectators. Each entrance and exit was numbered, as was each staircase. The northern main entrance was reserved for the Roman Emperor and his aides, whilst the other three axial entrances were most likely used by the elite. All four axial entrances were richly decorated with painted stucco reliefs, of which fragments survive. Many of the original outer entrances have disappeared with the collapse of the perimeter wall, but entrances XXIII (23) to LIV (54) still survive.
Spectators were given tickets in the form of numbered pottery shards, which directed them to the appropriate section and row. They accessed their seats via "vomitoria" (singular "vomitorium"), passageways that opened into a tier of seats from below or behind. These quickly dispersed people into their seats and, upon conclusion of the event or in an emergency evacuation, could permit their exit within only a few minutes. The name "vomitoria" derived from the Latin word for a rapid discharge, from which English derives the word vomit.
Interior seating.
According to the Codex-Calendar of 354, the Colosseum could accommodate 87,000 people, although modern estimates put the figure at around 50,000. They were seated in a tiered arrangement that reflected the rigidly stratified nature of Roman society. Special boxes were provided at the north and south ends respectively for the Emperor and the Vestal Virgins, providing the best views of the arena. Flanking them at the same level was a broad platform or "podium" for the senatorial class, who were allowed to bring their own chairs. The names of some 5th century senators can still be seen carved into the stonework, presumably reserving areas for their use.
The tier above the senators, known as the "maenianum primum", was occupied by the non-senatorial noble class or knights ("equites"). The next level up, the "maenianum secundum", was originally reserved for ordinary Roman citizens ("plebeians") and was divided into two sections. The lower part (the "immum") was for wealthy citizens, while the upper part (the "summum") was for poor citizens. Specific sectors were provided for other social groups: for instance, boys with their tutors, soldiers on leave, foreign dignitaries, scribes, heralds, priests and so on. Stone (and later marble) seating was provided for the citizens and nobles, who presumably would have brought their own cushions with them. Inscriptions identified the areas reserved for specific groups.
Another level, the "maenianum secundum in legneis", was added at the very top of the building during the reign of Domitian. This comprised a gallery for the common poor, slaves and women. It would have been either standing room only, or would have had very steep wooden benches. Some groups were banned altogether from the Colosseum, notably gravediggers, actors and former gladiators.
Each tier was divided into sections ("maeniana") by curved passages and low walls ("praecinctiones" or "baltei"), and were subdivided into "cunei", or wedges, by the steps and aisles from the vomitoria. Each row ("gradus") of seats was numbered, permitting each individual seat to be exactly designated by its gradus, cuneus, and number.
Arena and hypogeum.
The arena itself was 83 meters by 48 meters (272 ft by 157 ft / 280 by 163 Roman feet). It comprised a wooden floor covered by sand (the Latin word for sand is "harena" or "arena"), covering an elaborate underground structure called the "hypogeum" (literally meaning "underground"). The hypogeum was not part of the original construction but was ordered to be built by Emperor Domitian. Little now remains of the original arena floor, but the "hypogeum" is still clearly visible. It consisted of a two-level subterranean network of tunnels and cages beneath the arena where gladiators and animals were held before contests began. Eighty vertical shafts provided instant access to the arena for caged animals and scenery pieces concealed underneath; larger hinged platforms, called "hegmata", provided access for elephants and the like. It was restructured on numerous occasions; at least twelve different phases of construction can be seen.
The "hypogeum" was connected by underground tunnels to a number of points outside the Colosseum. Animals and performers were brought through the tunnel from nearby stables, with the gladiators' barracks at the Ludus Magnus to the east also being connected by tunnels. Separate tunnels were provided for the Emperor and the Vestal Virgins to permit them to enter and exit the Colosseum without needing to pass through the crowds.
Substantial quantities of machinery also existed in the "hypogeum". Elevators and pulleys raised and lowered scenery and props, as well as lifting caged animals to the surface for release. There is evidence for the existence of major hydraulic mechanisms and according to ancient accounts, it was possible to flood the arena rapidly, presumably via a connection to a nearby aqueduct. However, the construction of the hypogeum at Domitian's behest put an end to the practise of flooding, and thus also to naval battles, early in the Colosseum's existence.
Supporting buildings.
The Colosseum and its activities supported a substantial industry in the area. In addition to the amphitheatre itself, many other buildings nearby were linked to the games. Immediately to the east is the remains of the "Ludus Magnus", a training school for gladiators. This was connected to the Colosseum by an underground passage, to allow easy access for the gladiators. The "Ludus Magnus" had its own miniature training arena, which was itself a popular attraction for Roman spectators. Other training schools were in the same area, including the "Ludus Matutinus" (Morning School), where fighters of animals were trained, plus the Dacian and Gallic Schools.
Also nearby were the "Armamentarium", comprising an armory to store weapons; the "Summum Choragium", where machinery was stored; the "Sanitarium", which had facilities to treat wounded gladiators; and the "Spoliarium", where bodies of dead gladiators were stripped of their armor and disposed of.
Around the perimeter of the Colosseum, at a distance of 18 m (59 ft) from the perimeter, was a series of tall stone posts, with five remaining on the eastern side. Various explanations have been advanced for their presence; they may have been a religious boundary, or an outer boundary for ticket checks, or an anchor for the "velarium" or awning.
Right next to the Colosseum is also the Arch of Constantine.
Use.
The Colosseum was used to host gladiatorial shows as well as a variety of other events. The shows, called "munera", were always given by private individuals rather than the state. They had a strong religious element but were also demonstrations of power and family prestige, and were immensely popular with the population. Another popular type of show was the animal hunt, or "venatio". This utilized a great variety of wild beasts, mainly imported from Africa and the Middle East, and included creatures such as rhinoceros, hippopotamuses, elephants, giraffes, aurochs, wisents, Barbary lions, panthers, leopards, bears, Caspian tigers, crocodiles and ostriches. Battles and hunts were often staged amid elaborate sets with movable trees and buildings. Such events were occasionally on a huge scale; Trajan is said to have celebrated his victories in Dacia in 107 with contests involving 11,000 animals and 10,000 gladiators over the course of 123 days. During lunch intervals, executions "ad bestias" would be staged. Those condemned to death would be sent into the arena, naked and unarmed, to face the beasts of death which would literally tear them to pieces. Other performances would also take place by acrobats and magicians, typically during the intervals.
During the early days of the Colosseum, ancient writers recorded that the building was used for "naumachiae" (more properly known as "navalia proelia") or simulated sea battles. Accounts of the inaugural games held by Titus in AD 80 describe it being filled with water for a display of specially trained swimming horses and bulls. There is also an account of a re-enactment of a famous sea battle between the Corcyrean (Corfiot) Greeks and the Corinthians. This has been the subject of some debate among historians; although providing the water would not have been a problem, it is unclear how the arena could have been waterproofed, nor would there have been enough space in the arena for the warships to move around. It has been suggested that the reports either have the location wrong, or that the Colosseum originally featured a wide floodable channel down its central axis (which would later have been replaced by the hypogeum).
"Sylvae" or recreations of natural scenes were also held in the arena. Painters, technicians and architects would construct a simulation of a forest with real trees and bushes planted in the arena's floor, and animals would then be introduced. Such scenes might be used simply to display a natural environment for the urban population, or could otherwise be used as the backdrop for hunts or dramas depicting episodes from mythology. They were also occasionally used for executions in which the hero of the story – played by a condemned person – was killed in one of various gruesome but mythologically authentic ways, such as being mauled by beasts or burned to death.
Today.
The Colosseum today is now a major tourist attraction in Rome with thousands of tourists each year paying to view the interior arena, though entrance for citizens of the European Union (EU) is partially subsidised, and entrance is free for EU citizens under eighteen or over sixty-five years of age. There is now a museum dedicated to Eros located in the upper floor of the outer wall of the building. Part of the arena floor has been re-floored. Beneath the Colosseum, a network of subterranean passageways once used to transport wild animals and gladiators to the arena opened to the public in summer 2010.
The Colosseum is also the site of Roman Catholic ceremonies in the 20th and 21st centuries. For instance, Pope Benedict XVI led the Stations of the Cross called the Scriptural Way of the Cross (which calls for more meditation) at the Colosseum on Good Fridays.
Restoration.
In 2011 Diego Della Valle, head of the shoe firm Tod's, entered into an agreement with local officials to sponsor a €25 million restoration of the Colosseum. Work was planned to begin at the end of 2011, taking up to two and a half years. Due to the controversial nature of using a public-private partnership to fund the restoration, work was delayed and began in 2013. the restoration is estimated to be complete by 2016. The restoration is the first full cleaning and repair in the Colosseum's history. The first stage is to clean and restore the Colosseum's arcaded façade and replace the metal enclosures that block the ground-level arches. The project plans to create a services center and to restore the galleries and underground spaces inside the Colosseum.
Significance in Christianity.
The Colosseum is generally regarded by Christians as a site of the martyrdom of large numbers of believers during the persecution of Christians in the Roman Empire, as evidenced by Church history and tradition. On the other hand, other scholars believe that the majority of martyrdoms may have occurred at other venues within the city of Rome, rather than at the Colosseum, citing a lack of still-intact physical evidence or historical records. These scholars assert that "some Christians were executed as common criminals in the Colosseum—their crime being refusal to reverence the Roman gods", but most Christian martyrs of the early Church were executed for their faith at the Circus Maximus. According to Irenæus (died about 202), Ignatius of Antioch was fed to the lions in Rome around 107 A.D and although Irenaeus says nothing about this happening at the Colosseum, tradition ascribes it to that place.
In the Middle Ages, the Colosseum was not regarded as a monument, and was used as what some modern sources label a "quarry," which is to say that stones from the Colosseum were taken for the building of other sacred sites. This fact is used to support the idea that, at a time when sites associated with martyrs were highly venerated the Colosseum was not being treated as a sacred site. It was not included in the itineraries compiled for the use of pilgrims nor in works such as the 12th century "Mirabilia Urbis Romae" ("Marvels of the City of Rome"), which claims the Circus Flaminius – but not the Colosseum – as the site of martyrdoms. Part of the structure was inhabited by a Christian religious order, but it is not known whether this was for any particular religious reason.
Pope Pius V (1566–1572) is said to have recommended that pilgrims gather sand from the arena of the Colosseum to serve as a relic, on the grounds that it was impregnated with the blood of martyrs, although some of his contemporaries did not share his conviction. A century later Fioravante Martinelli listed the Colosseum at the head of a list of places sacred to the martyrs in his 1653 book "Roma ex ethnica sacra". Martinelli's book evidently had an effect on public opinion; in response to Cardinal Altieri's proposal some years later to turn the Colosseum into a bullring, Carlo Tomassi published a pamphlet in protest against what he regarded as an act of desecration. The ensuing controversy persuaded Pope Clement X to close the Colosseum's external arcades and declare it a sanctuary.
At the insistence of St. Leonard of Port Maurice, Pope Benedict XIV (1740–1758) forbade the quarrying of the Colosseum and erected Stations of the Cross around the arena, which remained until February 1874. Benedict Joseph Labre spent the later years of his life within the walls of the Colosseum, living on alms, prior to his death in 1783. Several 19th century popes funded repair and restoration work on the Colosseum, and it still retains its Christian connection today. A Christian cross stands in the Colosseum, with a plaque, stating:
The amphitheater, one consecrated to triumphs, entertainments, and the impious worship of pagan gods, is now dedicated to the sufferings of the martyrs purified from impious superstitions.
Other Christian crosses stand in several points around the arena and every Good Friday the Pope leads a Via Crucis procession to the amphitheater.
Flora.
The Colosseum has a wide and well-documented history of flora ever since Domenico Panaroli made the first catalogue of its plants in 1643. Since then, 684 species have been identified there. The peak was in 1855 (420 species). Attempts were made in 1871 to eradicate the vegetation, because of concerns over the damage that was being caused to the masonry, but much of it has returned. 242 species have been counted today and of the species first identified by Panaroli, 200 remain.
The variation of plants can be explained by the change of climate in Rome through the centuries. Additionally, bird migration, flower blooming, and the growth of Rome that caused the Colosseum to become embedded within the modern city centre rather than on the outskirts of the ancient city, as well as deliberate transport of species, are also contributing causes. One other romantic reason often given is their seeds being unwittingly transported on the animals brought there from all corners of the empire.
Popular culture references.
The iconic status of the Colosseum has led it to be featured in numerous films and other items of popular culture:

</doc>
<doc id="49604" url="https://en.wikipedia.org/wiki?curid=49604" title="Hearing loss">
Hearing loss

Hearing loss, also known as hearing impairment, is a partial or total inability to hear. A deaf person has little to no hearing. Hearing loss may occur in one or both ears. In children hearing problems can affect the ability to learn language and in adults it can cause work related difficulties. In some people, particularly older people, hearing loss can result in loneliness. Hearing loss can be temporary or permanent.
Hearing loss may be caused by a number of factors, including: genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins. --> A common condition that results in hearing loss is chronic ear infections. --> Certain infections during pregnancy such as rubella may also cause problems. --> Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear. Testing for poor hearing is recommended for all newborns. Hearing loss can be categorised as mild, moderate, severe, or profound.
Half of hearing loss is preventable. --> This includes by immunisation, proper care around pregnancy, avoiding loud noise, and avoiding certain medications. The World Health Organization recommends that young people limit the use of personal audio players to an hour a day in an effort to limit exposure to noise. Early identification and support are particularly important in children. --> For many hearing aids, sign language, cochlear implants and subtitles are useful. --> Lip reading is another useful skill some develop. --> Access to hearing aids, however, is limited in many areas of the world.
As of 2013 hearing loss affects about 1.1 billion people to some degree. It causes disability in 5% (360 to 538 million) and moderate to severe disability in 124 million people. Of those with moderate to severe disability 108 million live in low and middle income countries. Of those with hearing loss it began in 65 million during childhood. Those who use sign language and are members of Deaf culture see themselves as having a difference rather than an illness. Most members of Deaf culture oppose attempts to cure deafness and some within this community view cochlear implants with concern as they have the potential to eliminate their culture. The term hearing impairment is often viewed negatively as it emphasises what people cannot do.
Definition.
Use of the terms "hearing impaired," "deaf-mute," or "deaf and dumb" to describe deaf and hard of hearing people is discouraged by advocacy organizations as they are offensive to many deaf and hard of hearing people.
Hearing standards.
Human hearing extends in frequency from 20-20,000 Hz, and in amplitude from 0 dB to 130 dB or more. 0 dB does not represent absence of sound, but rather the softest sound an average unimpaired human ear can hear; some people can hear down to -5 or even -10 dB. 130 dB represents the threshold of pain. But the ear doesn't hear all frequencies equally well; hearing sensitivity peaks around 3000 Hz. There are many qualities of human hearing besides frequency range and amplitude that can't easily be measured quantitatively. But for many practical purposes, normative hearing is defined by a frequency versus amplitude graph, or audiogram, charting sensitivity thresholds of hearing at defined frequencies. Because of the cumulative impact of age and exposure to noise and other acoustic insults, 'typical' hearing may not be normative.
ANSI 7029:2000/BS 6951 Acoustics - Statistical distribution of hearing thresholds as a function of age
ANSI S3.5-1997 Speech Intelligibility Index (SII)
Signs and symptoms.
Hearing loss is sensory, but may have accompanying symptoms:
There may also be accompanying secondary symptoms:
Causes.
Hearing loss has multiple causes, including ageing, genetics, perinatal problems and acquired causes like noise and disease. For some kinds of hearing loss the cause may be classified as of unknown cause.
Age.
There is a progressive loss of ability to hear high frequencies with ageing known as presbycusis. For men, this can start as early as 25 and women at 30. Although genetically variable it is a normal concomitant of ageing and is distinct from hearing losses caused by noise exposure, toxins or disease agents. While everyone loses hearing with age, the amount and type of hearing lost is variable.
Noise.
Noise is the cause of approximately half of all cases of hearing loss, causing some degree of problems in 5% of the population globally.
The National Institute for Occupational Safety and Health (NIOSH) recognizes that the majority of hearing loss is not due to age, but due to noise exposure. By correcting for age in assessing hearing, one tends to overestimate the hearing loss due to noise for some and underestimate it for others.
Hearing loss due to noise may be temporary, called a 'temporary threshold shift', a reduced sensitivity to sound over a wide frequency range resulting from exposure to a brief but very loud noise like a gunshot, firecracker, jet engine, jackhammer, etc. or to exposure to loud sound over a few hours such as during a pop concert or nightclub session. Recovery of hearing is usually within 24 hours, but may take up to a week. Serial exposure to very loud sounds may eventually result in a permanent loss of hearing. Extremely loud sounds may cause instant and permanent hearing loss.
Noise-induced hearing loss (NIHL) typically manifests as elevated hearing thresholds (i.e. less sensitivity or muting) between 3000 and 6000 Hz, centered at 4000 Hz. As noise damage progresses, damage spreads to affect lower and higher frequencies. On an audiogram, the resulting configuration has a distinctive notch, called a 'noise' notch. As aging and other effects contribute to higher frequency loss (6–8 kHz on an audiogram), this notch may be obscured and entirely disappear.
Various governmental, industry and standards organizations set noise standards. The U.S. Environmental Protection Agency has identified the level of 70 dB(A) (40% louder to twice as loud as normal conversation; typical level of TV, radio, stereo; city street noise) for 24‑hour exposure as the level necessary to protect the public from hearing loss and other disruptive effects from noise, such as sleep disturbance, stress-related problems, learning detriment, etc. Populations living near airports or freeways are exposed to levels of noise typically in the 65 to 75 dB(A) range. If lifestyles include significant outdoor or open window conditions, these exposures over time can degrade hearing.
Louder sounds cause damage in a shorter period of time. Estimation of a "safe" duration of exposure is possible using an "exchange rate" of 3 dB. As 3 dB represents a doubling of intensity of sound, duration of exposure must be cut in half to maintain the same energy dose. For workplace noise regulation, the "safe" daily exposure amount at 85 dB A, known as an exposure action value, is 8 hours, while the "safe" exposure at 91 dB(A) is only 2 hours. 
Different standards use exposure action values between 80dBA and 90dBA. Note that for some people, sound may be damaging at even lower levels than 85 dB A. Exposures to other ototoxins (such as pesticides, some medications including chemotherapy agents, solvents, etc.) can lead to greater susceptibility to noise damage, as well as causing their own damage. This is called a "synergistic" interaction. Since noise damage is cumulative over long periods of time, persons who are exposed to non-workplace noise, like recreational activities or environmental noise, may have compounding damage from all sources.
Some national and international organizations and agencies use an exchange rate of 4 dB or 5 dB. While these exchange rates may indicate a wider zone of comfort or safety, they can significantly underestimate the damage caused by loud noise. For example, at 100 dB (nightclub music level), a 3 dB exchange rate would limit exposure to 15 minutes; the 5 dB exchange rate allows an hour.
Many people are unaware of the presence of environmental sound at damaging levels, or of the level at which sound becomes harmful. Common sources of damaging noise levels include car stereos, children's toys, motor vehicles, crowds, lawn and maintenance equipment, power tools, gun use, musical instruments, and even hair dryers. Noise damage is cumulative; all sources of damage must be considered to assess risk. If one is exposed to loud sound (including music) at high levels or for extended durations (85 dB A or greater), then hearing loss will occur. Sound intensity (sound energy, or propensity to cause damage to the ears) increases dramatically with proximity according to an inverse square law: halving the distance to the sound quadruples the sound intensity.
In the USA, 12.5% of children aged 6–19 years have permanent hearing damage from excessive noise exposure. The World Health Organization estimates that half of those between 12 and 35 are at risk from using personal audio devices that are too loud.
Hearing loss due to noise has been described as primarily a condition of modern society. In preindustrial times, humans had far less exposure to loud sounds. Studies of primitive peoples indicate that much of what has been attributed to age-related hearing loss may be long term cumulative damage from all sources, especially noise. People living in preindustrial society have considerably less hearing loss than similar populations living in modern society. Among primitive people who have migrated into modern society, hearing loss is proportional to the number of years spent in modern society. Military service in World War II, the Korean War, and the Vietnam War, has likely also caused hearing loss in large numbers of men from those generations, though proving hearing loss was a direct result of military service is problematic without entry and exit audiograms.
Genetic.
Hearing loss can be inherited. Around 75–80% of all these cases are inherited by recessive genes, 20–25% are inherited by dominant genes, 1–2% are inherited by X-linked patterns, and fewer than 1% are inherited by mitochondrial inheritance.
When looking at the genetics of deafness, there are 2 different forms, syndromic and nonsyndromic. Syndromic deafness occurs when there are other signs or medical problems aside from deafness in an individual. This accounts for around 30% of deaf individuals who are deaf from a genetic standpoint. Nonsyndromic deafness occurs when there are no other signs or medical problems associated with an individual other than deafness. From a genetic standpoint, this accounts for the other 70% of cases, and represents the majority of hereditary hearing loss. Syndromic cases occur with diseases such as Usher syndrome, Stickler syndrome, Waardenburg syndrome, Alport's syndrome, and neurofibromatosis type 2. These are diseases that have deafness as one of the symptoms or as a common feature associated with it. Many of the genetic mutations giving rise to syndromic deafness have been identified. In nonsyndromic cases, where deafness is the only finding, it is more difficult to identify the genetic mutation although some have been discovered.
Medications.
Some medications may reversibly affect hearing. This includes loop diuretics such as furosemide and bumetanide, non-steroidal anti-inflammatory drugs (NSAIDs) both over-the-counter (aspirin, ibuprofin, naproxin) as well as prescription (celecoxib, diclofenac, etc.), acetaminophen, quinine, and macrolide antibiotics. The link between NSAIDs and hearing loss tends to be greater in women, especially those who take ibuprofen six or more times a week. Others may cause permanent hearing loss. The most important group is the aminoglycosides (main member gentamicin) and platinum based chemotherapeutics such as cisplatin and carboplatin.
On October 18, 2007, the U.S. Food and Drug Administration (FDA) announced that a warning about possible sudden hearing loss would be added to drug labels of PDE5 inhibitors, which are used for erectile dysfunction. There have been reports of sensorineural hearing loss associated with the use of opioids such as codeine and heroin.
Chemicals.
In addition to medications, hearing loss can also result from specific chemicals: metals, such as lead; solvents, such as toluene (found in crude oil, gasoline and automobile exhaust, for example); and asphyxiants. Combined with noise, these ototoxic chemicals have an additive effect on a person’s hearing loss.
Hearing loss due to chemicals starts in the high frequency range and is irreversible. It damages the cochlea with lesions and degrades central portions of the auditory system. For some ototoxic chemical exposures, particularly styrene, the risk of hearing loss can be higher than being exposed to noise alone. 
Physical trauma.
There can be damage either to the ear itself or to the brain centers that process the aural information conveyed by the ears.
People who sustain head injury are especially vulnerable to hearing loss or tinnitus, either temporary or permanent.
Pathophysiology.
From a neurobiological perspective, there are three reasons that could cause a person to have hearing loss: either there is something wrong with the mechanical portion of the process, meaning the conductive portions of the ear (external and middle ear), or there is something wrong with the sensory portion of the process (inner ear or cochlea and related structures) or there is something wrong with the neural portion of the process, meaning the nerves or brain.
The process of understanding how sound travels to the brain is imperative in understanding how and why disease can cause a person to develop hearing loss. The process is as follows: sound waves are transmitted to the outer ear, sound waves are conducted down to ear canal, bringing the sound waves to the eardrum which they cause to vibrate, these vibrations are now passed through the 3 tiny ear bones in the middle ear, which transfer the vibrations to the fluid in the inner ear, the fluid moves the hair cells, the movement of the hair cells cause the vibrations to be converted into nerve impulses, the nerve impulses are taken to the brain by the auditory nerve, the auditory nerve takes the impulses to the medulla oblongata, the brainstem send the impulses to the midbrain, which finally goes to the auditory cortex of the temporal lobe to be interpreted as sound.
This process is complex and involves several steps that depend on the previous step in order for the vibrations or nerve impulses to be passed on. This is why if anything goes wrong at either the mechanical or neural portion of the process, it could result in sound not being processed by the brain, hence, leading to hearing loss.
Lesions to the auditory association cortex produced by physical trauma can result in deafness and other problems in auditory perception. The place where the lesion occurs on the auditory cortex plays an important role in what type of hearing deficit will occur in a person. A study conducted by Clarke et al. (2000) tested three subjects for the ability to identify a produced environmental sound, the source of the sound, and whether or not the source is moving. All three subjects had trauma to different parts of the auditory cortex, and each patient demonstrated a different set of auditory deficits, suggesting that different parts of the auditory cortex controlled different parts of the hearing process. This means, lesion one part of auditory cortex and it could result in one or two deficits. It would take larger lesions at the right parts to produce deafness.
Diagnosis.
Identification of a hearing loss is usually conducted by a general practitioner medical doctor, otolaryngologist, certified and licensed audiologist, school or industrial audiometrist, or other audiology technician. Diagnosis of the cause of a hearing loss is carried out by a specialist physician (audiovestibular physician) or otorhinolaryngologist
Case history.
A case history (usually a written form, with questionnaire) can provide valuable information about the context of the hearing loss, and indicate what kind of diagnostic procedures to employ. Case history will include such items as:
Laboratory testing.
In case of infection or inflammation, blood or other body fluids may be submitted for laboratory analysis.
Hearing tests.
Hearing loss is generally measured by playing generated or recorded sounds, and determining whether the person can hear them. Hearing sensitivity varies according to the frequency of sounds. To take this into account, hearing sensitivity can be measured for a range of frequencies and plotted on an audiogram.
Another method for quantifying hearing loss is a speech-in-noise test. As the name implies, a speech-in-noise test gives an indication of how well one can understand speech in a noisy environment. A person with a hearing loss will often be less able to understand speech, especially in noisy conditions. This is especially true for people who have a sensorineural loss – which is by far the most common type of hearing loss. As such, speech-in-noise tests can provide valuable information about a person's hearing ability, and can be used to detect the presence of a sensorineural hearing loss. A recently developed digit-triple speech-in-noise test may be a more efficient screening test.
Otoacoustic emissions test is an objective hearing test that may be administered to toddlers and children too young to cooperate in a conventional hearing test. The test is also useful in older children and adults.
Auditory brainstem response testing is an electrophysiological test used to test for hearing deficits caused by pathology within the ear, the cochlear nerve and also within the brainstem. This test can be used to identify delay in the conduction of neural impulses due to tumours or inflammation but can also be an objective test of hearing thresholds. Other electophysiological tests, such as cortical evoked responses, can look at the hearing pathway up to the level of the auditory cortex.
Scans.
MRI and CT scans can be useful to identify the pathology of many causes of hearing loss. They are only needed in selected cases.
Classification.
Hearing loss is categorized by type, severity, and configuration. Furthermore, a hearing loss may exist in only one ear (unilateral) or in both ears (bilateral). Hearing loss can be temporary or permanent, sudden or progressive.
Severity.
The severity of a hearing loss is ranked according to the additional intensity above a nominal threshold that a sound must be before being detected by an individual; it is measured in decibels of hearing loss, or dB HL. Hearing loss may be ranked as slight, mild, moderate, moderately severe, severe or profound as defined below:
Hearing loss may affect one or both ears. If both ears are affected, then one ear may be more affected than the other. Thus it is possible, for example, to have normal hearing in one ear and none at all in the other, or to have mild hearing loss in one ear and moderate hearing loss in the other.
For certain legal purposes such as insurance claims, hearing loss is described in terms of percentages. Given that hearing loss can vary by frequency and that audiograms are plotted with a logarithmic scale, the idea of a percentage of hearing loss is somewhat arbitrary, but where decibels of loss are converted via a legally recognized formula, it is possible to calculate a standardized "percentage of hearing loss", which is suitable for legal purposes only.
Type.
There are four main types of hearing loss, conductive hearing loss, sensorineural hearing loss, central deafness and combinations of conductive and sensorienural hearing losses which is called mixed hearing loss. An additional problem which is increasingly recognised is auditory processing disorder which is not a hearing loss as such but a difficulty perceiving sound.
Conductive hearing loss is present when the sound is not reaching the inner ear, the cochlea. This can be due to external ear canal malformation, dysfunction of the eardrum or malfunction of the bones of the middle ear. The ear drum may show defects from small to total resulting in hearing loss of different degree. Scar tissue after ear infections may also make the ear drum dysfunction as well as when it is retracted and adherent to the medial part of the middle ear.
Dysfunction of the three small bones of the middle ear – malleus, incus, and stapes – may cause conductive hearing loss. The mobility of the ossicles may be impaired for different reasons and disruption of the ossicular chain due to trauma, infection or ankylosis may also cause hearing loss.
Sensorineural hearing loss is one caused by dysfunction of the inner ear, the cochlea or the nerve that transmits the impulses from the cochlea to the hearing centre in the brain. The most common reason for sensorineural hearing loss is damage to the hair cells in the cochlea. Depending on the definition it could be estimated that more than 50% of the population over the age of 70 has impaired hearing.
Damage to the brain can lead to a central deafness. The peripheral ear and the auditory nerve may function well but the central connections are damaged by tumour, trauma or other disease and the patient is unable to hear.
Mixed hearing loss is a combination of conductive and sensorineural hearing loss. Chronic ear infection (a fairly common diagnosis) can cause a defective ear drum or middle-ear ossicle damages, or both. In addition to the conductive loss, a sensory component may be present.
This is not an actual hearing loss but gives rise to significant difficulties in hearing. One kind of auditory processing disorder is King-Kopetzky syndrome, which is characterized by an inability to process out background noise in noisy environments despite normal performance on traditional hearing tests.
Configuration.
The shape of an audiogram shows the relative configuration of the hearing loss, such as a Carhart notch for otosclerosis, 'noise' notch for noise-induced damage, high frequency rolloff for presbycusis, or a flat audiogram for conductive hearing loss. In conjunction with speech audiometry, it may indicate central auditory processing disorder, or the presence of a schwannoma or other tumor.
There are four general configurations of hearing loss:
1. Flat: thresholds essentially equal across test frequencies.
2. Sloping: lower (better) thresholds in low-frequency regions and higher (poorer) thresholds in high-frequency regions.
3. Rising: higher (poorer) thresholds in low-frequency regions and lower (better) thresholds in higher-frequency regions.
4. Trough-shaped ("cookie-bite" or "U" shaped): greatest hearing loss in the mid-frequency range, with lower (better) thresholds in low- and high-frequency regions.
Unilateral and bilateral.
People with unilateral hearing loss or single-sided deafness (SSD) have difficulty in:
In quiet conditions, speech discrimination is approximately the same for normal hearing and those with unilateral deafness; however, in noisy environments speech discrimination varies individually and ranges from mild to severe.
One reason for the hearing problems these patients often experience is due to the head shadow effect. Newborn children with no hearing on one side but one normal ear could still have problems. Speech development could be delayed and difficulties to concentrate in school are common. More children with unilateral hearing loss have to repeat classes than their peers. Taking part in social activities could be a problem. Early aiding is therefore of utmost importance.
Prevention.
It is estimated that half of cases of hearing loss are preventable. A number of preventative strategies are effective including: immunisation against rubella to prevent congenital rubella syndrome, immunization against "H. influenza" and "S. pneumoniae" to reduce cases of meningitis, and avoiding or protecting against excessive noise exposure. The World Health Organization also recommends immunization against measles, mumps, and meningitis, efforts to prevent premature birth, and avoidance of certain medication as prevention.
The use of antioxidants is being studied for the prevention of noise-induced hearing loss.
Hearing protectors.
Education regarding noise exposure increases the use of hearing protectors.
Workplace noise regulation.
In addition, steps can be taken to avoid the production of excessive noise in the workplace. By purchasing less noisy power tools like those found on the NIOSH Power Tools Database and limiting exposure to ototoxic chemicals, great strides can be made in preventing hearing loss. Companies can also provide personal hearing protector devices tailored to both the worker and type of employment. Some hearing protectors universally block out all noise, and some allow for certain noises to be heard. Workers are more likely to wear hearing protector devices when they are properly fitted.
Better enforcement of laws can decrease levels of noise at work.
Screening.
The United States Preventive Services Task Force recommends screening for all newborns.
The American Academy of Pediatrics advises that children should have their hearing tested several times throughout their schooling:
There is not enough evidence to determine the utility of screening in adults over 50 years old who do not have any symptoms.
Treatment.
Treatment depends on the specific cause if known as well as the extent, type and configuration of the hearing loss. Most hearing loss, that resulting from age and noise, is progressive and irreversible, and there are currently no approved or recommended treatments; management is by hearing aid. A few specific kinds of hearing loss are amenable to surgical treatment. In other cases, treatment is addressed to underlying pathologies, but any hearing loss incurred may be permanent.
There are a number of devices that can improve hearing in those who are deaf or hard of hearing or allow people with these conditions to manage better in their lives.
Hearing aids.
Hearing aids are devices that work to improve the hearing and speech comprehension of those with hearing loss. It works by magnifying the sound vibrations in the ear so that one can understand what is being said around them. The use of this technological device may or may not have an effect on one's sociability. Some people feel as if they cannot live without one because they say it is the only thing that keeps them engaged with the public. Others dislike hearing aids very much because they feel wearing them is embarrassing or weird. Due to their low-esteem, they avoid hearing aid usage altogether and would rather remain quiet and to themselves in a social environment.
Assistive devices.
Many deaf and hard of hearing individuals use assistive devices in their daily lives:
Wireless hearing aids.
A wireless device has two main components: a transmitter and a receiver. The transmitter broadcasts the captured sound, and the receiver detects the broadcast audio and enables the incoming audio stream to be connected to accommodations such as hearing aids or captioning systems.
Three types of wireless systems are commonly used: FM, audio induction loop, and InfraRed. Each system has advantages and benefits for particular uses. FM systems can be battery operated or plugged into an electrical outlet. FM system produce an analog audio signal, meaning they have extremely high fidelity. Many FM systems are very small in size, allowing them to be used in mobile situations. The audio induction loop permits the listener with hearing loss to be free of wearing a receiver provided that the listener has a hearing aid or cochlear implant processor with an accessory called a "telecoil". If the listener does not have a telecoil, then he or she must carry a receiver with an earpiece. As with FM systems, the infrared (IR) system also requires a receiver to be worn or carried by the listener. An advantage of IR wireless systems is that people in adjoining rooms cannot listen in on conversations, making it useful for situations where privacy and confidentiality are required. Another way to achieve confidentiality is to use a hardwired amplifier, which contains or is connected to a microphone and transmits no signal beyond the earpiece plugged directly into it.
Surgical.
There is no treatment surgical or otherwise for hearing lost due to the most common causes (age, noise and genetic defects). For a few specific conditions, surgical intervention can provide a remedy:
Surgical and implantable hearing aids are an alternative to conventional external hearing aids. 
If the ear is dry and not infected, an air conduction aid could be tried; if the ear is draining, a direct bone condition hearing aid is often the best solution. If the conductive part of the hearing loss is more than 30–35 dB, an air conduction device could have problems overcoming this gap. A bone-anchored hearing aid could, in this situation, be a good option.
The active bone conduction hearing implant Bonebridge is also an option. This implant is invisible under the intact skin and therefore minimises the risk of skin irritations.
Cochlear implants improve outcomes in people with hearing loss in either one or both ears. They work by artificial stimulation of the cochlear nerve by providing an electric impulse substitution for the firing of hair cells. They are expensive, and require programming along with extensive training for effectiveness.
Cochlear implants as well as bone conduction implants can help with single sided deafness.
Middle ear implants or bone conduction implants can help with conductive hearing loss.
People with cochlear implants are at a higher risk for bacterial meningitis. Thus, meningitis vaccination is recommended. People who have hearing loss, especially those who develop a hearing problem in childhood or old age, may need support and technical adaptations as part of the rehabilitation process. Recent research shows variations in efficacy but some studies show that if implanted at a very young age, some profoundly impaired children can acquire effective hearing and speech, particularly if supported by appropriate rehabilitation.
Classroom.
For a classroom setting, children with hearing loss often benefit from direct instruction and communication. One option for students is to attend a school for the Deaf, where they will have access to the language, communication, and education. Another option is to have the child attend a mainstream program, with special accommodation such as providing favorable seating for the child. Having the student sit as close to the teacher as possible improves the student's ability to hear the teacher's voice and to more easily read the teacher's lips. When lecturing, teachers can help the student by facing them and by limiting unnecessary noise in the classroom. In particular, the teacher can avoid talking when their back is turned to the classroom, such as while writing on a whiteboard.
Some other approaches for classroom accommodations include pairing deaf or hard of hearing students with hearing students. This allows the deaf or hard of hearing student to ask the hearing student questions about concepts that they have not understood. The use of CART (Communication Access Real Time) systems, where an individual types a captioning of what the teacher is saying, is also beneficial. The student views this captioning on their computer. Automated captioning systems are also becoming a popular option. In an automated system, software, instead of a person, is used to generate the captioning. Unlike CART systems, automated systems generally do not require an Internet connection and thus they can be used anywhere and anytime. Another advantage of automated systems over CART is that they are much lower in cost. However, automated systems are generally designed to only transcribe what the teacher is saying and to not transcribe what other students say. An automated system works best for situations where just the teacher is speaking, whereas a CART system will be preferred for situations where there is a lot of classroom discussion.
For those students who are completely deaf, one of the most common interventions is having the child communicate with others through an interpreter using sign language.
Epidemiology.
[[File:Hearing loss (adult onset) world map - DALY - WHO2004.svg|thumb|Disability-adjusted life year for hearing loss (adult onset) per 100,000 inhabitants in 2004.
Globally hearing loss affects about 10% of the population to some degree. It caused moderate to severe disability in 124.2 million people as of 2004 (107.9 million of whom are in low and middle income countries). Of these 65 million acquired the condition during childhood. At birth ~3 per 1000 in developed countries and more than 6 per 1000 in developing countries have hearing problems.
Hearing loss increases with age. In those between 20 and 35 rates of hearing loss are 3% while in those 44 to 55 it is 11% and in those 65 to 85 it is 43%.
History.
Abbé Charles-Michel de l'Épée opened the first school for the deaf in Paris at the deaf school. The American Thomas Gallaudet witnessed a demonstration of deaf teaching skills from Épée's successor Abbé Sicard and two of the school's deaf faculty members, Laurent Clerc and Jean Massieu; accompanied by Clerc, he returned to the United States, where in 1817 they founded American School for the Deaf in Hartford, Connecticut. American Sign Language (ASL) started to evolve from primarily French Sign Language (LSF), and other outside influences.
Society and culture.
After language.
"Post-lingual deafness" is hearing loss that is sustained after the acquisition of language, which can occur due to disease, trauma, or as a side-effect of a medicine. Typically, hearing loss is gradual and often detected by family and friends of affected individuals long before the patients themselves will acknowledge the disability. Post-lingual deafness is far more common than pre-lingual deafness. Those who lose their hearing later in life, such as in late adolescence or adulthood, face their own challenges, living with the adaptations that allow them to live independently.
Before language.
"Prelingual deafness" is hearing loss that is sustained before the acquisition of language, which can occur due to a congenital condition or through hearing loss in early infancy. Prelingual deafness impairs an individual's ability to acquire a "spoken" language. Children born into signing families rarely have delays in language development, but most prelingual hearing loss is acquired via either disease or trauma rather than genetically inherited, so families with deaf children nearly always lack previous experience with sign language. Cochlear implants allow prelingually deaf children to acquire an oral language with remarkable success if implantation is performed within the first 2–4 years.
Jack Gannon, a professor at Gallaudet University, said this about Deaf culture. "Deaf culture is a set of learned behaviors and perceptions that shape the values and norms of deaf people based on their shared or common experiences." Some doctors believe that being deaf makes a person more social. Bill Vicar, from ASL University, shared his experiences as a deaf person, "people tend to congregate around the kitchen table rather than the living room sofa… our good-byes take nearly forever, and our hellos often consist of serious hugs. When two of us meet for the first time we tend to exchange detailed biographies." Deaf culture is not about contemplating what deaf people cannot do and how to fix their problems, an approach known as the "pathological view of the deaf." Instead deaf people celebrate what they can do. There is a strong sense of unity between deaf people as they share their experiences of suffering through a similar struggle. This celebration creates a unity between even deaf strangers. Bill Vicars expresses the power of this bond when stating, "if given the chance to become hearing most people would choose to remain deaf."
Views of treatments.
There has been considerable controversy within the culturally deaf community over cochlear implants. For the most part, there is little objection to those who lost their hearing later in life, or culturally deaf adults choosing to be fitted with a cochlear implant.
Many in the deaf community strongly object to a deaf child being fitted with a cochlear implant (often on the advice of an audiologist); new parents may not have sufficient information on raising deaf children and placed in an oral-only program that emphasizes the ability to speak and listen over other forms of communication such as sign language or total communication. Many Deaf people view cochlear implants and other hearing devices as confusing to one's identity. A Deaf person will never be a hearing person and therefore would be trying to fit into a way of living that is not their own. Other concerns include loss of Deaf culture and identity and limitations on hearing restoration.
The National Association of the Deaf has a statement on its website regarding cochlear implants. The NAD asserts that the choice to implant is up to the individual (or the parents), yet strongly advocates a fully informed decision in all aspects of a cochlear implant. Much of the negative reaction to cochlear implants stems from the medical viewpoint that deafness is a condition that needs to be "cured," while the Deaf community instead regards deafness a defining cultural characteristic.
Many other assistive devices are more acceptable to the Deaf community, including but not limited to, hearing aids, closed captioning, email and the Internet, text telephones, and video relay services.
Sign language.
Sign languages convey meaning through manual communication and body language instead of acoustically conveyed sound patterns. This involves the simultaneous combination of hand shapes, orientation and movement of the hands, arms or body, and facial expressions to express a speaker's thoughts.
Government policies.
Those who are deaf (by either state or federal standards) have access to a free and appropriate public education. If a child does qualify as being deaf or hard of hearing and receives an individualized education plan, the IEP team must consider, "the child's language and communication needs. The IEP must include opportunities for direct communication with peers and professionals. It must also include the student’s academic level, and finally must include the students full range of needs"
In part, the Department of Education defines deafness as "… a hearing impairment that is so severe that the child is impaired in processing linguistic information through hearing, with or without amplification …." Hearing impairment is defined as "… an impairment in hearing, whether permanent or fluctuating, that adversely affects a child's educational performance but that is not included under the definition of deafness …."
Inclusion versus pullout.
It is commonly misunderstood that least restrictive environment means mainstreaming or inclusion. Sometimes the resources available at the public schools do not match up to the resources at a residential school for the deaf. Many hearing parents choose to have their deaf child educated in the general education classroom as much as possible because they are told that mainstreaming is the least restrictive environment, which is not always the case. However, there are parents that live in Deaf communities who feel that the general education classroom is not the least restrictive environment for their child. These parents feel that placing their child in a residential school where all children are deaf may be more appropriate for their child because the staff tend to be more aware of the needs and struggles of deaf children. Another reason that these parents feel a residential school may be more appropriate is because in a general education classroom, the student will not be able to communicate with their classmates due to the language barrier.
In a residential school where all the children use the same communication system (whether it is a school using ASL, Total Communication or Oralism), students will be able to interact normally with other students, without having to worry about being criticized. An argument supporting inclusion, on the other hand, exposes the student to people who aren't just like them, preparing them for adult life. Through interacting, children with hearing disabilities can expose themselves to other cultures which in the future may be beneficial for them when it comes to finding jobs and living on their own in a society where their disability may put them in the minority. These are some reasons why a person may or may not want to put their child in an inclusion classroom.
Communication barriers.
The most predominant forms of communication barriers originate from one's own personal self and they are directly the result of the hearing loss condition. These barriers are associated specifically with speech and language. In terms of speech, hearing loss has an effect on speech sound production, for example distortion caused by the omission of various letters from words. The pitch of their voice may sound too high or low and their volume may be louder or quieter than is intended. Resonance of voice is also affected, as it can be hypernasal or denasal. Prosody, which represents the patterns of stress and rhythm in the voice, will often become irregular. As a result of such changes to speech, the receiver during a conversation is likely to deem the communicator's speech unintelligible. The placement of improper stresses on syllables makes it more difficult for the receiver to clearly perceive and hear the intended words. Three major problems in terms of language are present for those with hearing loss. First, there are problems with language formation, where individuals may overuse nouns and verbs and they may improperly place words within a sentence. Second, the actual content of the language is troubling, for example the interpretation of synonyms and antonyms. This results in a limited vocabulary. The third major problem is associated with Pragmatics, which includes the inability of individuals to recognize that a message has been delivered to them, therefore resulting in inappropriate questions being asked. All of these speech and language barriers make it difficult for those with hearing loss to control their own speech and understand what others have to say, therefore making it quite hard to hold a conversation altogether.
Family.
The communication limitations between people who are deaf and their hearing family members can often cause difficulties in family relationships, and affect the strength of relationships among individual family members. It was found that most people who are deaf have hearing parents, which means that the channel that the child and parents communicate through can be very different, often affecting their relationship in a negative way. If a parent communicates best verbally, and their child communicates best using sign language, this could result in ineffective communication between parents and children. Ineffective communication can potentially lead to fights caused by misunderstanding, less willingness to talk about life events and issues, and an overall weaker relationship. Even if individuals in the family made an effort to learn deaf communication techniques such as sign language, a deaf family member often will feel excluded from casual banter; such as the exchange of daily events and news at the dinner table. It is often difficult for people who are deaf to follow these conversations due to the fast paced and overlapping nature of these exchanges. This can cause a deaf individual to become frustrated and take part in less family conversations. This can potentially result in weaker relationships between the hearing individual and their immediate family members. This communication barrier can have a particularly negative effect on relationships with extended family members as well. Communication between a deaf individual and their extended family members can be very difficult due to the gap in verbal and non-verbal communication. This can cause the individuals to feel frustrated and unwilling to put effort into communicating effectively. The lack of effort put into communicating can result in anger, miscommunication, and unwillingness to build a strong relationship.
Community.
People who have hearing loss can often experience many difficulties as a result of communication barriers among them and other hearing individuals in the community. Some major areas that can be impacted by this are involvement in extracurricular activities and social relationships. For young people, extracurricular activities are vehicles for physical, emotional, social, and intellectual development. However, it is often the case that communication barriers between people who are deaf and their hearing peers and coaches/club advisors limit them from getting involved. These communication barriers make it difficult for someone with a hearing loss to understand directions, take advice, collaborate, and form bonding relationships with other team or club members. As a result, extracurricular activities such as sports teams, clubs, and volunteering are often not as enjoyable and beneficial for individuals who have hearing loss, and they may engage in them less often. A lack of community involvement through extracurricular activities may also limit the individual’s social network. In general, it can be difficult for someone who is deaf to develop and maintain friendships with their hearing peers due to the communication gap that they experience. They can often miss the jokes, informal banter, and "messing around" that is associated with the formation of many friendships among young people. Conversations between people who are deaf and their hearing peers can often be limited and short due to their differences in communication methods and lack of knowledge on how to overcome these differences. Deaf individuals can often experience rejection by hearing peers who are not willing to make an effort to find their way around communication difficulties. Patience and motivation to overcome such communication barriers is required by both the deaf or hard of hearing and hearing individuals in order to establish and maintain good friendships.
Many people tend to forget about the difficulties that deaf children encounter, as they view the deaf child differently from a deaf adult. Deaf children grow up being unable to fully communicate with their parents, siblings and other family members. Examples include being unable to tell their family what they have learned, what they did, asking for help, or even simply being unable to interact in daily conversation. Deaf children have to learn sign language and to read lips at a young age, however they cannot communicate with others using it unless the others are educated in sign language as well. Children who are deaf or hard of hearing are faced with many complications while growing up, for example some children have to wear hearing aids and others require assistance from sign language (ASL) interpreters. The interpreters help them to communicate with other individuals until they develop the skills they need to efficiently communicate on their own. Although growing up for deaf children may entitle more difficulties than for other children, there are many support groups that allow deaf children to interact with other children. This is where they develop friendships. There are also classes for young children to learn sign language in an environment that has other children in their same situation and around their same age. These groups and classes can be very beneficial in providing the child with the proper knowledge and not to mention the societal interactions that they need in order to live a healthy, young, playful and carefree life that any child deserves.
There are three typical adjustment patterns adopted by adults with hearing loss. The first one is to remain withdrawn into your own self. This provides a sense of safety and familiarity which can be a comforting way to lead your life. The second is to act "as if" one does not even have hearing loss. A positive attitude will help people to live a life with no barriers and thus, engage in optimal interaction. The final and third pattern is for the person to accept their hearing loss as a part of them without undervaluing oneself. This means understanding that one is forced to live life with this disability, however it is not the only thing that constitutes life’s meaning. Furthermore, many feel as if their inability to hear others during conversation is their fault. It's important that these individuals learn how to become more assertive individuals who do not lack fear when it comes to asking someone to repeat something or to speak a little louder. Although there is much fatigue and frustration that is produced from one’s inability to hear, it is important to learn from personal experiences in order to improve on one’s communication skills. In essence, these patterns will help adults with hearing loss deal with the communication barriers that are present.
Workplace.
In most instances, people who are deaf find themselves working with hearing colleagues, where they can often be cut off from the communication going on around them. Interpreters can be provided for meetings and workshops, however are seldom provided for everyday work interactions. Communication of important information needed for jobs typically comes in the form of written or verbal summaries, which do not convey subtle meanings such as tone of voice, side conversations during group discussions, and body language. This can result in confusion and misunderstanding for the worker who is deaf, therefore making it harder to do their job effectively. Additionally, deaf workers can be unintentionally left out of professional networks, informal gatherings, and casual conversations among their collogues. Information about informal rules and organizational culture in the workplace is often communicated though these types of interactions, which puts the worker who is deaf at a professional and personal disadvantage. This could sever their job performance due to lack of access to information and therefore, reduce their opportunity to form relationships with their co-workers. Additionally, these communication barriers can all affect a deaf person’s career development. Since being able to effectively communicate with one's co-workers and other people relevant to one's job is essential to managerial positions, people with hearing loss can often be denied such opportunities.
To avoid these situations in the workplace, individuals can take full-time or part-time sign language courses. In this way, they can become better able to communicate with the deaf and hard of hearing. Such courses teach the American Sign Language (ASL) language as most North Americans use this particular language to communicate. It is a visual language made up of specific gestures (signs), hand shapes, and facial expressions that contain their own unique grammatical rules and sentence structures By completing sign language courses, it ensures that deaf individuals feel a part of the workplace and have the ability to communicate with their co-workers and employer in the manner as other hearing employees do.
Health care.
Not only can communication barriers between deaf and hearing people affect family relationships, work, and school, but they can also have a very significant effect on a deaf individual’s health care. As a result of poor communication between the health care professional and the deaf or hard of hearing patient, many patients report that they are not properly informed about their disease and prognosis. 
This lack of or poor communication could also lead to other issues such as misdiagnosis, poor assessments, mistreatment, and even possibly harm to patients. Poor communication in this setting is often the result of health care providers having the misconception that all people who are deaf or hard of hearing have the same type of hearing loss, and require the same type of communication methods. In reality, there are many different types and range of hearing loss, and in order to communicate effectively a health care provider needs to understand that each individual with hearing loss has unique needs. This affects how individuals have been educated to communicate, as some communication methods work better depending on an individual’s severity of hearing loss. For example, assuming every deaf or hard of hearing patient knows American Sign Language would be incorrect because there are different types of sign language, each varying in signs and meanings. A patient could have been educated to use cued speech which is entirely different from ASL. Therefore, in order to communicate effectively, a health care provider needs to understand that each individual has unique needs when communicating.
Although there are specific laws and rules to govern communication between health care professionals and people who are deaf, they are not always followed due to the health care professional’s insufficient knowledge of communication techniques. This lack of knowledge can lead them to make assumptions about communicating with someone who is deaf, which can in turn cause them to use an unsuitable form of communication. 
Acts in countries such as the Americans with Disabilities Act (ADA) state that all health care providers are required to provide reasonable communication accommodations when caring for patients who are deaf. These accommodations could include qualified sign language interpreters, CDIs, and technology such as Internet interpretation services. A qualified sign language interpreter will enhance communication between a deaf individual and a health care professional by interpreting not only a health professional’s verbal communication, but also their non-verbal such as expressions, perceptions, and body language. A Certified Deaf Interpreter (CDI) is a sign language interpreter who is also a member of the Deaf community. They accompany a sign language interpreter and are useful for communication with deaf individuals who also have language or cognitive deficits. A CDI will transform what the health care professional communicates into basic, simple language. This method takes much longer, however it can also be more effective than other techniques. Internet interpretation services are convenient and less costly, but can potentially pose significant risks. They involve the use of a sign language interpreter over a video device rather than directly in the room. This can often be an inaccurate form of communication because the interpreter may not be licensed, is often unfamiliar with the patient and their signs, and can lack knowledge of medical terminology.
Aside from utilizing interpreters, healthcare professionals can improve their communication with deaf or hard of hearing patients by educating themselves on common misconceptions and proper practices depending on the patient’s needs. For example, a common misconception is that exaggerating words and speaking loudly will help the patient understand more clearly. However, many individuals with hearing loss depend on lip-reading to identify words. Exaggerated pronunciation and a raised voice can distort the lips, making it even more difficult to understand. Another common mistake health care professionals make are the use of single words rather than full sentences. Although language should be kept simple and short, keeping context is important because certain homophonous words are difficult to distinguish by lip-reading. Health care professionals can further improve their own communication with their patients by eliminating any background noise and positioning themselves in a way where their face is clearly visible to the patient, and suitably lit. The healthcare professional should know how to use body language and facial expressions to properly communicate different feelings.
Research.
Stem cell transplant and gene therapy.
A 2005 study achieved successful regrowth of cochlea cells in guinea pigs. However, the regrowth of cochlear hair cells does not imply the restoration of hearing sensitivity, as the sensory cells may or may not make connections with neurons that carry the signals from hair cells to the brain. A 2008 study has shown that gene therapy targeting Atoh1 can cause hair cell growth and attract neuronal processes in embryonic mice. Some hope that a similar treatment will one day ameliorate hearing loss in humans.
Recent research, reported in 2012 achieved growth of cochlear nerve cells resulting in hearing improvements in gerbils, using stem cells. Also reported in 2013 was regrowth of hair cells in deaf adult mice using a drug intervention resulting in hearing improvement. The Hearing Health Foundation in the US has embarked on a project called the Hearing Restoration Project. Also Action on Hearing Loss in the UK is also aiming to restore hearing.
Researchers reported in 2015 that genetically deaf mice which were treated with TMC1 gene therapy recovered some of their hearing.
Audition.
Besides research studies seeking to improve hearing, such as the ones listed above, research studies on the deaf have also been carried out in order to understand more about audition. Pijil and Shwarz (2005) conducted their study on the deaf who lost their hearing later in life and, hence, used cochlear implants to hear. They discovered further evidence for rate coding of pitch, a system that codes for information for frequencies by the rate that neurons fire in the auditory system, especially for lower frequencies as they are coded by the frequencies that neurons fire from the basilar membrane in a synchronous manner. Their results showed that the subjects could identify different pitches that were proportional to the frequency stimulated by a single electrode. The lower frequencies were detected when the basilar membrane was stimulated, providing even further evidence for rate coding.

</doc>
<doc id="49607" url="https://en.wikipedia.org/wiki?curid=49607" title="Jargon">
Jargon

Jargon is a type of language that is used in a particular context and may not be well understood outside of it. The context is usually a particular occupation (that is, a certain trade, profession, or academic field), but any ingroup can have jargon. The main trait that distinguishes jargon from the rest of a language is special vocabulary—including some words specific to it and, often, narrower senses of words that outgroups would tend to take in a broader sense. Jargon is thus "the technical terminology or characteristic idiom of a special activity or group". Most jargon is technical terminology, involving terms of art or industry terms, with particular meaning within a specific industry. A main driving force in the creation of technical jargon is precision and efficiency of communication when a discussion must easily range from general themes to specific, finely differentiated details without circumlocution. A side effect of this is a higher threshold for comprehensibility, which is usually accepted as a trade-off but is sometimes even used as a means of social exclusion (reinforcing ingroup-outgroup barriers) or social aspiration (when intended as a way of showing off).
The philosopher Étienne Bonnot de Condillac observed in 1782 that "every science requires a special language because every science has its own ideas". As a rationalist member of the Enlightenment, he continued: "It seems that one ought to begin by composing this language, but people begin by speaking and writing, and the language remains to be composed."
Various kinds of language peculiar to ingroups can be named across a semantic field. Slang can be either culture-wide or known only within a certain group or subculture. Argot is slang and jargon purposely used to obscure meaning to outsiders. Conversely, a lingua franca is used for the opposite effect, helping communicators to overcome unintelligibility, as are pidgins and creole languages. For example, the Chinook Jargon was a pidgin. Although technical jargon's primary purpose is to aid technical communication, not to exclude outsiders by serving as an argot, it can have both effects at once and can provide a technical ingroup with shibboleths. For example, medieval guilds could use this as one means of informal protectionism. On the other hand, jargon that once was obscure outside a small ingroup can become generally known over time. For example, the terms "bit, byte," and "hexadecimal" (which are terms from computing jargon) are now recognized by many people outside computer science.
Etymology.
The French word is believed to have been derived from the Latin word "gaggire", meaning "to chatter", which was used to describe speech that the listener did not understand. Middle English also has the verb jargounen meaning "to chatter", which comes from the French word. The word may also come from Old French "jargon" meaning "chatter of birds".
Industry term.
An industry term is a type of technical terminology that has a particular meaning within a specific industry. The phrase implies that a word or phrase is a typical one within a particular industry or business and people within the industry or business will be familiar with and use the term.
Precise technical terms and their definitions are formally recognised, documented, and taught by educators in the field. Other terms are more colloquial, coined and used by practitioners in the field, and are similar to slang. The boundaries between formal and slang jargon, as in general English, are quite fluid. This is especially true in the rapidly developing world of computers and networking. For instance, the term "firewall" (in the sense of a device used to filter network traffic) was at first technical slang. As these devices became more widespread and the term became widely understood, the word was adopted as formal terminology.
Technical terminology evolves due to the need for experts in a field to communicate with precision and brevity, but often has the effect of excluding those who are unfamiliar with the particular specialized language of the group. This can cause difficulties as, for example, when a patient is unable to follow the discussions of medical practitioners, and thus cannot understand his own condition and treatment. Differences in jargon also cause difficulties where professionals in related fields use different terms for the same phenomena.
Accessibility issues.
With the rise of the self-advocacy movement within the disability movement, "jargonised" language has been much objected to by advocates and self-advocates. Jargon is largely present in every day language, in newspapers, government documents and official forms. Several advocacy organisations work on influencing public agents to offer accessible information in different formats. One accessible format that offers an alternative to jargonised language is Easy Read, which consists of a combination of plain English and images. Another alternative is a jargon buster, incorporated to certain technical documents. There is a balance to be struck, as excessive removal of technical terminology from a document leads to an equally undesirable outcome—dumbing down.
Examples.
Many examples of jargon exist because of its use among specialists and subcultures alike. In the professional world, those who are in the business of filmmaking may use words like "vorkapich" to refer to a montage when talking to colleagues. In Rhetoric, rhetoricians use words like "arete" to refer to a person of power's character when speaking with one another.

</doc>
<doc id="49609" url="https://en.wikipedia.org/wiki?curid=49609" title="Bay of Biscay">
Bay of Biscay

The Bay of Biscay (, ) is a gulf of the northeast Atlantic Ocean located south of the Celtic Sea. It lies along the western coast of France from Brest south to the Spanish border, and the northern coast of Spain west to Cape Ortegal.
The average depth is and the greatest depth is .
Name.
The Bay of Biscay is named (for English speakers) after Biscay on the northern Spanish coast, probably standing for the western Basque districts ("Biscay" up to the early 19th century). Its name in other languages is:
Geography.
Parts of the continental shelf extend far into the bay, resulting in fairly shallow waters in many areas and thus the rough seas for which the region is known. The Bay of Biscay is home to some of the Atlantic Ocean's fiercest weather. Large storms occur in the bay, especially during the winter months. Up until recent years it was a regular occurrence for merchant vessels to founder in Biscay storms.
Extent.
The International Hydrographic Organization defines the limits of the Bay of Biscay as "a line joining Cap Ortegal () to Penmarch Point ()".
Rivers.
The main rivers that empty into the Bay of Biscay are Loire, Charente, Garonne, Dordogne, Adour, Nivelle, Bidasoa, Oiartzun, Urumea, Oria, Urola, Deba, Artibai, Oka, Nervión, Agüera, Asón, Miera, Pas, Saja, Nansa, Deva, Sella, Nalón, Navia, Esva, Eo, Landro and Sor.
Climate.
The phenomenon of June Gloom is common. In late spring and early summer a large fog triangle fills the southwestern half of the bay, covering just a few kilometres inland.
As winter begins, weather becomes severe. Depressions enter from the west very frequently and they either bounce north to the British Isles or they enter the Ebro Valley, dry out, and are finally reborn in the form of powerful thunderstorms as they reach the Mediterranean Sea. These depressions cause severe weather at sea and bring light though very constant rain to its shores (known as "orballo", "sirimiri", "morrina", "orbayu", "orpin" or "calabobos"). Sometimes powerful windstorms form if the pressure falls rapidly, traveling along the Gulf Stream at great speed, resembling a hurricane, and finally crashing in this bay with their maximum power, such as the Klaus storm.
The Gulf Stream enters the bay following the continental shelf's border anti-clockwise (the Rennell Current), keeping temperatures moderate all year long.
Main cities.
The main cities on the shores of the Bay of Biscay are Bordeaux, Bayonne, Biarritz, Brest, Nantes, La Rochelle, Donostia-San Sebastián, Bilbao, Santander, Gijón and Avilés.
History.
The southern end of the gulf is also called in Spanish ""Mar Cantábrico"" (Cantabrian Sea), from the Estaca de Bares, as far as the mouth of Adour river, but this name is not generally used in English. It was named by Romans in the 1st century BC as "Sinus Cantabrorum" (Bay of the Cantabri) and also, "Mare Gallaecum" (the Sea of the Galicians). On some medieval maps, the Bay of Biscay is marked as "El Mar del los Vascos" (the Basque Sea).
The Bay of Biscay has been the site of many famous naval engagements over the centuries. In 1592 the Spanish defeated an English fleet during the eponymous Battle of the Bay of Biscay. The Biscay campaign of June 1795 consisted of a series of manoeuvres and two battles fought between the British Channel Fleet and the French Atlantic Fleet off the southern coast of Brittany during the second year of the French Revolutionary Wars. The USS Californian sank here after striking a naval mine on June 22, 1918. On December 28, 1943, the Battle of the Bay of Biscay was fought between HMS Glasgow and HMS Enterprise and a group of German destroyers as part of Operation Stonewall during World War II. "U-667" sunk on 25 August 1944 in position , when she struck a mine. All hands were lost.
Wildlife.
Marine mammals.
The car ferries from Gijón to Nantes/Saint-Nazaire, Portsmouth to Bilbao and from Plymouth, Portsmouth and Poole to Santander provide one of the most convenient ways to see cetaceans in European waters. Often specialist groups take the ferries to hear more information.
Volunteers and employees of Biscay Dolphin Research regularly observe and monitor cetacean activity from the bridge of the ships on the P&O Ferries Portsmouth to Bilbao route. Many species of whales and dolphins can be seen in this area. Most importantly, it is one of the few places where the beaked whales, such as the Cuvier's beaked whale, have been observed relatively frequently. This is the best study area in the world for beaked whales.
North Atlantic Right Whales, one of the most endangered whales, once came to the bay for feeding and probably for calving as well, but whaling activities by Basques and others almost wiped them out sometime prior to 1850s. In modern days, the eastern population of this species are considered to be almost extinct, and there has been no record of right whales in the Bay of Biscay except for possible few sightings made in modern time, a pair which was possibly a mother-calf pair in 1977 at , and another pair in earlier June 1980 seen by merchantmen. Other records in the late 20th century include one off Galicia at in September 1977 reported by a whaling company and another one seen off the Iberian Peninsula.
The best areas to see the larger cetaceans lie in the deep waters beyond the continental shelf, particularly over the Santander Canyon and Torrelavega Canyon in the south of the Bay.
The three-day round trip also gives sightings of good numbers of several species of seabirds, particularly gannets.
The alga "Colpomenia peregrina" was introduced and first noticed in 1906 by oyster fishermen in the Bay of Biscay.
The "Grammatostomias flagellibarba" commonly known as the scaleless dragonfish are native to these waters.

</doc>
