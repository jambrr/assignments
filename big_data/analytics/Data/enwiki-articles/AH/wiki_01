<doc id="47763" url="https://en.wikipedia.org/wiki?curid=47763" title="Environmental economics">
Environmental economics

Environmental Economics is a sub-field of economics that is concerned with environmental issues. Quoting from the National Bureau of Economic Research Environmental Economics program:
[...] Environmental Economics [...] undertakes theoretical or empirical studies of the economic effects of national or local environmental policies around the world [...]. Particular issues include the costs and benefits of alternative environmental policies to deal with air pollution, water quality, toxic substances, solid waste, and global warming.
Environmental economics is distinguished from ecological economics in that ecological economics emphasizes the economy as a subsystem of the ecosystem with its focus upon preserving natural capital. One survey of German economists found that ecological and environmental economics are different schools of economic thought, with ecological economists emphasizing "strong" sustainability and rejecting the proposition that natural capital can be substituted by human-made capital.
Topics and concepts.
Market failure.
Central to environmental economics is the concept of market failure. Market failure means that markets fail to allocate resources efficiently. As stated by Hanley, Shogren, and White (2007) in their textbook "Environmental Economics": "A market failure occurs when the market does not allocate scarce resources to generate the greatest social welfare. A wedge exists between what a private person does given market prices and what society might want him or her to do to protect the environment. Such a wedge implies wastefulness or economic inefficiency; resources can be reallocated to make at least one person better off without making anyone else worse off." Common forms of market failure include externalities, non-excludability and non-rivalry.
Externality.
An externality exists when a person makes a choice that affects other people in a way that is not accounted for in the market price. An externality can be positive or negative, but is usually associated with negative externalities in environmental economics. For instance, water seepage in residential buildings happen in upper floor affect the lower floor. Or a firm emitting pollution will typically not take into account the costs that its pollution imposes on others. As a result, pollution may occur in excess of the 'socially efficient' level, which is the level that would exist if the market was required to account for the pollution. A classic definition influenced by Kenneth Arrow and James Meade is provided by Heller and Starrett (1976), who define an externality as “a situation in which the private economy lacks sufficient incentives to create a potential market in some good and the nonexistence of this market results in losses of Pareto efficiency.” In economic terminology, externalities are examples of market failures, in which the unfettered market does not lead to an efficient outcome.
Common goods and public goods.
When it is too costly to exclude some people from access to an environmental resource, the resource is either called a common property resource (when there is rivalry for the resource, such that one person's use of the resource reduces others' opportunity to use the resource) or a public good (when use of the resource is non-rivalrous). In either case of non-exclusion, market allocation is likely to be inefficient.
These challenges have long been recognized. Hardin's (1968) concept of the tragedy of the commons popularized the challenges involved in non-exclusion and common property. "Commons" refers to the environmental asset itself, "common property resource" or "common pool resource" refers to a property right regime that allows for some collective body to devise schemes to exclude others, thereby allowing the capture of future benefit streams; and "open-access" implies no ownership in the sense that property everyone owns nobody owns.
The basic problem is that if people ignore the scarcity value of the commons, they can end up expending too much effort, over harvesting a resource (e.g., a fishery). Hardin theorizes that in the absence of restrictions, users of an open-access resource will use it more than if they had to pay for it and had exclusive rights, leading to environmental degradation. See, however, Ostrom's (1990) work on how people using real common property resources have worked to establish self-governing rules to reduce the risk of the tragedy of the commons.
The mitigation of climate change effects is an example of a public good, where the social benefits are not reflected completely in the market price. This is a public good since the risks of climate change are both non-rival and non-excludable. Such efforts are non-rival since climate mitigation provided to one does not reduce the level of mitigation that anyone else enjoys. They are non-excludable actions as they will have global consequences from which no one can be excluded. A country's incentive to invest in carbon abatement is reduced because it can "free ride" off the efforts of other countries. Over a century ago, Swedish economist Knut Wicksell (1896) first discussed how public goods can be under-provided by the market because people might conceal their preferences for the good, but still enjoy the benefits without paying for them.
Valuation.
Assessing the economic value of the environment is a major topic within the field. Use and indirect use are tangible benefits accruing from natural resources or ecosystem services (see the nature section of ecological economics). Non-use values include existence, option, and bequest values. For example, some people may value the existence of a diverse set of species, regardless of the effect of the loss of a species on ecosystem services. The existence of these species may have an option value, as there may be the possibility of using it for some human purpose (certain plants may be researched for drugs). Individuals may value the ability to leave a pristine environment to their children.
Use and indirect use values can often be inferred from revealed behavior, such as the cost of taking recreational trips or using hedonic methods in which values are estimated based on observed prices. Non-use values are usually estimated using stated preference methods such as contingent valuation or choice modelling. Contingent valuation typically takes the form of surveys in which people are asked how much they would pay to observe and recreate in the environment (willingness to pay) or their willingness to accept (WTA) compensation for the destruction of the environmental good. Hedonic pricing examines the effect the environment has on economic decisions through housing prices, traveling expenses, and payments to visit parks.
Solutions.
Solutions advocated to correct such externalities include:
Relationship to other fields.
Environmental economics is related to ecological economics but there are differences. Most environmental economists have been trained as economists. They apply the tools of economics to address environmental problems, many of which are related to so-called market failures—circumstances wherein the "invisible hand" of economics is unreliable. Most ecological economists have been trained as ecologists, but have expanded the scope of their work to consider the impacts of humans and their economic activity on ecological systems and services, and vice versa. This field takes as its premise that economics is a strict subfield of ecology. Ecological economics is sometimes described as taking a more pluralistic approach to environmental problems and focuses more explicitly on long-term environmental sustainability and issues of scale.
Environmental economics is viewed as more pragmatic in a price system; ecological economics as more idealistic in its attempts not to use money as a primary arbiter of decisions. These two groups of specialists sometimes have conflicting views which may be traced to the different philosophical underpinnings.
Another context in which externalities apply is when globalization permits one player in a market who is unconcerned with biodiversity to undercut prices of another who is - creating a race to the bottom in regulations and conservation. This in turn may cause loss of natural capital with consequent erosion, water purity problems, diseases, desertification, and other outcomes which are not efficient in an economic sense. This concern is related to the subfield of sustainable development and its political relation, the anti-globalization movement.
Environmental economics was once distinct from resource economics. Natural resource economics as a subfield began when the main concern of researchers was the optimal commercial exploitation of natural resource stocks. But resource managers and policy-makers eventually began to pay attention to the broader importance of natural resources (e.g. values of fish and trees beyond just their commercial exploitation;, externalities associated with mining). It is now difficult to distinguish "environmental" and "natural resource" economics as separate fields as the two became associated with sustainability. Many of the more radical green economists split off to work on an alternate political economy.
Environmental economics was a major influence for the theories of natural capitalism and environmental finance, which could be said to be two sub-branches of environmental economics concerned with resource conservation in production, and the value of biodiversity to humans, respectively. The theory of natural capitalism (Hawken, Lovins, Lovins) goes further than traditional environmental economics by envisioning a world where natural services are considered on par with physical capital.
The more radical Green economists reject neoclassical economics in favour of a new political economy beyond capitalism or communism that gives a greater emphasis to the interaction of the human economy and the natural environment, acknowledging that "economy is three-fifths of ecology" - Mike Nickerson.
These more radical approaches would imply changes to money supply and likely also a bioregional democracy so that political, economic, and ecological "environmental limits" were all aligned, and not subject to the arbitrage normally possible under capitalism.
An emerging sub-field of environmental economics studies its intersection with development economics. Dubbed "envirodevonomics" by Michael Greenstone and B. Kelsey Jack in their paper "Envirodevonomics: A Research Agenda for a Young Field," the sub-field is primarily interested in studying "why environmental quality so poor in developing countries." A strategy for better understanding this correlation between a country's GDP and its environmental quality involves analyzing how many of the central concepts of environmental economics, including market failures, externalities, and willingness to pay, may be complicated by the particular problems facing developing countries, such as political issues, lack of infrastructure, or inadequate financing tools, among many others.
Professional bodies.
The main academic and professional organizations for the discipline of Environmental Economics are the Association of Environmental and Resource Economists (AERE) and the European Association for Environmental and Resource Economics (EAERE). The main academic and professional organization for the discipline of Ecological Economics is the International Society for Ecological Economics (ISEE). The main organization for Green Economics is the Green Economics Institute.

</doc>
<doc id="47766" url="https://en.wikipedia.org/wiki?curid=47766" title="CDR coding">
CDR coding

In computer science CDR coding is a compressed data representation for Lisp linked lists. It was developed and patented by the MIT Artificial Intelligence Laboratory, and implemented in computer hardware in a number of Lisp machines derived from the MIT CADR.
CDR coding is in fact a fairly general idea; whenever a data object "A" ends in a reference to another data structure "B", we can instead place the structure "B" itself there, overlapping and running off the end of "A". By doing this we free the space required by the reference, which can add up if done many times, and also improve locality of reference, enhancing performance on modern machines. The transformation is especially effective for the cons-based lists it was created for; we free about half of the space for each node we perform this transformation on.
It is not always possible to perform this substitution, because there might not be a large enough chunk of free space beyond the end of A. Thus, some objects will end in a real reference, and some with the referenced object, and the machine must be able to tell by reading the final cell which one it is. This can be accomplished with some inefficiency in software by the use of tagged pointers, which allow a pointer in a final position to be specifically tagged as such, but is best done in hardware.
In the presence of mutable objects, CDR coding becomes more complex. If a reference is updated to point to another object, but currently has an object stored in that field, the object must be relocated, along with any other pointers to it. Not only are such moves typically expensive or impossible, but over time they cause fragmentation of the store. This problem is typically avoided by using CDR coding only on immutable data structures.
Unrolled linked lists are simpler and often higher-performance than CDR coding (no "tagged pointers"; typically less fragmentation). For short lists, CDR coding uses the least amount of space.

</doc>
<doc id="47767" url="https://en.wikipedia.org/wiki?curid=47767" title="Threaded code">
Threaded code

In computer science, the term threaded code refers to a programming technique where the code has a form that essentially consists entirely of calls to subroutines. It is often, but not only, found in compiler implementations that generate code in that form and/or are implemented in that form themselves. The code may be processed by an interpreter, or may simply be a sequence of machine code call instructions.
Threaded code has better code density than code generated by alternative code generation techniques and alternative calling conventions, sometimes at the expense of slightly slower execution speed. However, a program small enough to fit fully in a computer processor's cache may run faster than a larger program that suffers many cache misses.
Threaded code is best known as the implementation technique commonly used in some programming languages, such as Forth, many implementations of BASIC, some implementations of COBOL, early versions of B,
and other languages for small minicomputers and amateur radio satellites.
History.
The common way to make computer programs is to 'translate' a computer program written in some symbolic language to machine code using a compiler. The code is typically fast but nonportable since the binary code is designed for a specific computer hardware platform. A different approach uses a virtual machine instruction set, which has no particular target hardware. An interpreter executes it on each new target hardware.
Early computers had relatively little memory. For example, most Data General Nova, IBM 1130, and many of the first Apple II computers had only 4 KB of RAM installed. Consequently, a lot of time was spent trying to find ways to reduce the size of programs so they would fit in the memory available. At the same time, computers were relatively slow, so simple interpretation was very noticeably slower than executing machine code. Instead of writing out every step of an operation in every part of the program where it was needed, programmers saved memory by writing each step of such operations once (see "Don't repeat yourself") and placing it in a subroutine. This processcode refactoringis used today, although for different reasons. The top-level application in these programs may consist of nothing but subroutine calls. Many of these subroutines, in turn, also consist of nothing but lower level subroutine calls.
Mainframes and some early microprocessors such as the RCA 1802 required several instructions to call a subroutine. In the top-level application and in many subroutines, that sequence is constantly repeated, only the subroutine address changing from one call to the next. Using memory to store the same instructions repeatedly is wasteful. To save space, programmers squeezed that series of subroutine calls into a list containing only contiguous addresses of the sub-routines, and used a tiny "interpreter" to call each subroutine in turn. This is identical to the way other programmers squeezed a series of jumps in a branch table, dispatch table, or virtual method table into a list containing only the destination addresses, and used a small selector to branch to the selected destination.
In threaded code and these other techniques, the program becomes a list of entry points to the actual code to be executed.
Over the years, programmers have created many variations on that "interpreter" or "small selector".
The particular address in the list of addresses may be extracted using an index, general purpose register or pointer. The addresses may be direct or indirect, contiguous or non-contiguous (linked by pointers), relative or absolute, resolved at compile time or dynamically built.
No one variation is "best".
Development.
To save space, programmers squeezed the lists of subroutine calls into simple lists of subroutine addresses, and used a small loop to call each subroutine in turn. For example:
In this case, decoding the bytecodes is performed once, during program compilation or program load, so it is not repeated each time an instruction is executed. This can save much time and space when decode and dispatch overhead is large compared to the execution cost.
Note, however, addresses in codice_1 for codice_2, codice_3, etc., are two or more bytes, compared to one byte, typically, for the decode and dispatch interpreter described above. In general, instructions for a decode and dispatch interpreter may be any size. For example, a decode and dispatch interpreter to simulate an Intel Pentium decodes instructions that range from 1 to 16 bytes. However, bytecoded systems typically choose 1-byte codes for the most-common operations. Thus, the thread often has a higher space cost than bytecodes. In most uses, the reduction in decode cost outweighs the increase in space cost.
Note also that while bytecodes are nominally machine-independent, the format and value of the pointers in threads generally depend on the target machine which is executing the interpreter. Thus, an interpreter might load a portable bytecode program, decode the bytecodes to generate platform-dependent threaded code, then execute threaded code without further reference to the bytecodes.
The loop is simple, so is duplicated in each handler, removing codice_4 from the list of machine instructions needed to execute each interpreter instruction. For example:
This is called direct threaded code (DTC). Although the technique is older, the first widely circulated use of the term "threaded code" is probably Bell's article "Threaded Code" from 1973.
Charles H. Moore invented a more compact notation in 1970 for his Forth virtual machine: indirect threaded code (ITC). Originally, Moore invented this because it was easy and fast on Nova minicomputers, which have an indirection bit in every address. He said (in published remarks, Byte Magazine's Forth Issue) that he found it so convenient that he propagated it into all later Forth designs.
Some Forth compilers compile Forth programs into direct-threaded code, while others make indirect-threaded code. The programs act the same either way.
Threading models.
Practically all executable threaded code uses one or another of these methods for invoking subroutines (each method is called a "threading model").
Direct threading.
Addresses in the thread are the addresses of machine language. This form is simple, but may have overheads because the thread consists only of machine addresses, so all further parameters must be loaded indirectly from memory. Some Forth systems produce direct-threaded code. On many machines direct-threading is faster than subroutine threading (see reference below).
An example of a stack machine might execute the sequence "push A, push B, add". That might be translated to the following thread and routines, where codice_5 is initialized to the address codice_6.
Alternatively, operands may be included in the thread. This can remove some indirection needed above, but makes the thread larger:
Indirect threading.
Indirect threading uses pointers to locations that in turn point to machine code. The indirect pointer may be followed by operands which are stored in the indirect "block" rather than storing them repeatedly in the thread. Thus, indirect code is often more compact than direct-threaded code, but the indirection also typically makes it slower, though still usually faster than bytecode interpreters. Where the handler operands include both values and types, the space savings over direct-threaded code may be significant. Older FORTH systems typically produce indirect-threaded code.
For example, if the goal is to execute "push A, push B, add", the following might be used. Here, codice_5 is initialized to address codice_6, each code fragment (codice_9, codice_10) is found by double-indirecting through codice_5; and operands to each code fragment are found in the first-level indirection following the address of the fragment.
Subroutine threading.
So-called "subroutine-threaded code" (also "call-threaded code") consists of a series of machine-language "call" instructions (or addresses of functions to "call", as opposed to direct threading's use of "jump"). Early compilers for ALGOL, Fortran, Cobol and some Forth systems often produced subroutine-threaded code. The code in many of these systems operated on a last-in-first-out (LIFO) stack of operands, which had well-developed compiler theory. Most modern processors have special hardware support for subroutine "call" and "return" instructions, so the overhead of one extra machine instruction per dispatch is somewhat diminished. Anton Ertl has stated "that, in contrast to popular myths, subroutine threading is usually slower than direct threading." However, Ertl's most recent tests show that subroutine threading is faster than direct threading in 15 out of 25 test cases. Ertl's most recent tests show that direct threading is the fastest threading model on Xeon, Opteron, and Athlon processors; indirect threading is the fastest threading model on Pentium M processors; and subroutine threading is the fastest threading model on Pentium 4, Pentium III, and PPC processors.
As an example of call threading "push A, push B, add":
Token threading.
Token threaded code uses lists of 8 or 12-bit indexes to a table of pointers. Token threaded code is notably compact, without much special effort by a programmer. It is usually half to three-fourths the size of other threaded-codes, which are themselves a quarter to an eighth the size of compiled code. The table's pointers can either be indirect or direct. Some Forth compilers produce token threaded code. Some programmers consider the "p-code" generated by some Pascal compilers, as well as the bytecodes used by .NET, Java, BASIC and some C compilers, to be token-threading.
A common approach historically is bytecode, which uses 8-bit opcodes and, often, a stack-based virtual machine. A typical interpreter is known as a "decode and dispatch interpreter", and follows the form
If the virtual machine uses only byte-size instructions, codice_12 is simply a fetch from codice_13, but often there are commonly used 1-byte instructions plus some less-common multibyte instructions, in which case codice_12 is more complex. The decoding of single byte opcodes can be very simply and efficiently handled by a branch table using the opcode directly as an index.
For instructions where the individual operations are simple, such as "push" and "add", the overhead involved in deciding what to execute is larger than the cost of actually executing it, so such interpreters are often much slower than machine code. However, for more complex ("compound") instructions, the overhead percentage is proportionally less significant.
Huffman threading.
Huffman threaded code consists of lists of tokens stored as Huffman codes. A Huffman code is a variable length bit string used to identify a unique token. A Huffman-threaded interpreter locates subroutines using an index table or tree of pointers that can be navigated by the Huffman code. Huffman threaded code is one of the most compact representations known for a computer program. Basically the index and codes are organized by measuring the frequency that each subroutine occurs in the code. Frequent calls are given the shortest codes. Operations with approximately equal frequencies are given codes with nearly equal bit-lengths. Most Huffman-threaded systems have been implemented as direct-threaded Forth systems, and used to pack large amounts of slow-running code into small, cheap microcontrollers. Most published uses have been in smart cards, toys, calculators, and watches.
The bit-oriented tokenized code used in PBASIC can be seen as a kind of Huffman threaded code.
Lesser-used threading.
An example is string threading, in which operations are identified by strings, usually looked-up by a hash table. This was used in Charles H. Moore's earliest Forth implementations and in the University of Illinois's experimental hardware-interpreted computer language. It is also used in Bashforth.
Branches.
Examples above show no branches. For all interpreters, a branch changes the thread pointer (codice_5 above). As example, a conditional branch when the top-of-stack value is zero might be encoded as follows. Note that codice_6 is the location to jump to, not the address of a handler, and so must be skipped (codice_17) whether or not the branch is taken.
Common amenities.
Separating the data and return stacks in a machine eliminates a great deal of stack management code, substantially reducing the size of the threaded code. The dual-stack principle was originated three times independently: for Burroughs large systems, Forth and PostScript, and is used in some Java virtual machines.
Three registers are often present in a threaded virtual machine. Another one exists for passing data between subroutines ('words'). These are:
Often, threaded virtual machines such as implementations of Forth have a simple virtual machine at heart, consisting of three "primitives". Those are:
In an indirect-threaded virtual machine, the one given here, the operations are:
This is perhaps the simplest and fastest interpreter or virtual machine.

</doc>
<doc id="47768" url="https://en.wikipedia.org/wiki?curid=47768" title="Texas Instruments">
Texas Instruments

Texas Instruments Inc. (TI) is an American electronics company that designs and makes semiconductors, which it sells to electronics designers and manufacturers globally. Headquartered in Dallas, Texas, United States, TI is the third largest manufacturer of semiconductors worldwide after Intel and Samsung, the second largest supplier of chips for cellular handsets after Qualcomm, and the largest producer of digital signal processors (DSPs) and analog semiconductors, among a wide range of other semiconductor products, including calculators, microcontrollers and multi-core processors. Texas Instruments is among the Top 20 Semiconductor producing companies in the world.
Texas Instruments was founded in 1951. It emerged after a reorganization of Geophysical Service, a company that manufactured equipment for use in the seismic industry as well as defense electronics. TI began research in transistors in the early 1950s and produced the world's first commercial silicon transistor. In 1954, Texas Instruments designed and manufactured the first transistor radio and Jack Kilby invented the integrated circuit in 1958 while working at TI's Central Research Labs. The company produced the first integrated circuit-based computer for the U.S. Air Force in 1961. TI researched infrared technology in the late 1950s and later made radar systems as well as guidance and control systems for both missiles and bombs. The hand-held calculator was introduced to the world by TI in 1967.
In the 1970s and 1980s the company focused on consumer electronics including digital clocks, watches, hand-held calculators, home computers as well as various sensors. In 1997, its defense business was sold to Raytheon. In 2007, Texas Instruments was awarded the Manufacturer of the Year for Global Supply Chain Excellence by World Trade magazine. Texas Instruments is considered to be one of the most ethical companies in the world.
After the acquisition of National Semiconductor in 2011, the company has a combined portfolio of nearly 45,000 analog products and customer design tools, making it the world's largest maker of analog technology components. In 2011, Texas Instruments ranked 175 in the Fortune 500. TI is made up of two main divisions: Semiconductors (SC) and Educational Technology (ET) of which Semiconductor products account for approximately 96% of TI's revenue.
History.
Texas Instruments was founded by Cecil H. Green, J. Erik Jonsson, Eugene McDermott, and Patrick E. Haggerty in 1951. McDermott was one of the original founders of Geophysical Service Inc. (GSI) in 1930. McDermott, Green, and Jonsson were GSI employees who purchased the company in 1941. In November, 1945, Patrick Haggerty was hired as general manager of the Laboratory and Manufacturing (L&M) division. By 1951, the L&M division, with its defense contracts, was growing faster than GSI's Geophysical division. The company was reorganized and initially renamed General Instruments Inc. Because there already existed a firm named General Instrument, the company was renamed Texas Instruments that same year. From 1956 to 1961, Fred Agnich of Dallas, later a Republican member of the Texas House of Representatives, was the Texas Instruments president. Geophysical Service, Inc. became a subsidiary of Texas Instruments. Early in 1988 most of GSI was sold to the Halliburton Company.
Geophysical Service Incorporated.
In 1930, J. Clarence Karcher and Eugene McDermott founded Geophysical Service, an early provider of seismic exploration services to the petroleum industry. In 1939 the company reorganized as Coronado Corp., an oil company with Geophysical Service Inc (GSI), now as a subsidiary. On December 6, 1941, McDermott along with three other GSI employees, J. Erik Jonsson, Cecil H. Green, and H.B. Peacock purchased GSI. During World War II, GSI expanded their services to include electronics for the U.S. Army, Signal Corps, and the U.S. Navy. In 1951 the company changed its name to Texas Instruments, GSI becoming a wholly owned subsidiary of the new company.
An early success story for TI-GSI came in 1965 when GSI was able (under a Top Secret government contract) to monitor the Soviet Union's underground nuclear weapons testing under the ocean in Vela Uniform, a subset of Project Vela, to verify compliance of the Partial Nuclear Test Ban Treaty.
Texas Instruments also continued to manufacture equipment for use in the seismic industry, and GSI continued to provide seismic services. After selling (and repurchasing) GSI, TI finally sold the company to Halliburton in 1988, at which point GSI ceased to exist as a separate entity.
Defense electronics.
Texas Instruments entered the defense electronics market in 1942 with submarine detection equipment, based on the seismic exploration technology previously developed for the oil industry. The division responsible for these products was known at different points in time as the Laboratory & Manufacturing Division, the Apparatus Division, the Equipment Group and the Defense Systems & Electronics Group (DSEG).
During the early 1980s Texas Instruments instituted a quality program which included Juran training, as well as promoting statistical process control, Taguchi methods and Design for Six Sigma. In the late '80s, the company, along with Eastman Kodak and Allied Signal, began involvement with Motorola institutionalizing Motorola's Six Sigma methodology. Motorola, who originally developed the Six Sigma methodology, began this work in 1982. In 1992 the DSEG division of Texas Instruments' quality improvement efforts were rewarded by winning the Malcolm Baldrige National Quality Award for manufacturing.
The followings are some of the major programs of the former TI defense group.
Infrared and radar systems.
TI developed the AAA-4 infra-red search and track (IRST) in the late 50's and early 60's for the F-4B Phantom for passive scanning of jet engine emissions but possessed limited capabilities and was eliminated on F-4D's and later models.
In 1956 TI began research on infrared technology that led to several line scanner contracts and with the addition of a second scan mirror the invention of the first forward looking infrared (FLIR) in 1963 with production beginning in 1966. In 1972 TI invented the Common Module FLIR concept, greatly reducing cost and allowing reuse of common components.
TI went on to produce side-looking radar systems, the first terrain following radar and surveillance radar systems for both the military and FAA. TI demonstrated the first solid-state radar called Molecular Electronics for Radar Applications (MERA). In 1976 TI developed a microwave landing system prototype. In 1984 TI developed the first inverse synthetic aperture radar (ISAR). The first single-chip gallium arsenide radar module was developed. In 1991 the Military Microwave Integrated Circuit (MIMIC) program was initiated – a joint effort with Raytheon.
Missiles and laser-guided bombs.
In 1961 TI won the guidance and control system contract for the defense suppression AGM-45 Shrike anti-radiation missile. This led later to the prime on the High-speed Anti-Radiation Missile (AGM-88 HARM) development contract in 1974 and production in 1981.
In 1964 TI began development of the first laser guidance system for precision-guided munitions (PGM) leading to the Paveway series of laser-guided bombs (LGBs). The first LGB was the BOLT-117.
In 1969 TI won the Harpoon (missile) Seeker contract. In 1986 TI won the Army FGM-148 Javelin fire-and-forget man portable anti-tank guided missile in a joint venture with Martin Marietta. In 1991 TI was awarded the contract for the AGM-154 Joint Standoff Weapon (JSOW).
Military computers.
Because of TI's research and development of military temperature range silicon transistors and integrated circuits (ICs), TI won contracts for the first IC-based computer for the U.S. Air Force in 1961 and for ICs for the Minuteman Missile the following year. In 1968 TI developed the data systems for Mariner Program. In 1991 TI won the F-22 Radar and Computer development contract.
Divestiture to Raytheon.
As the defense industry consolidated, TI sold its defense business to Raytheon in 1997 for $2.95 billion. The Department of Justice required that Raytheon divest the TI Monolithic Microwave Integrated Circuit (MMIC) operations after closing the transaction. The TI MMIC business accounted for less than $40 million in 1996 revenues, or roughly two percent of the $1.8 billion in total TI defense revenues was sold to TriQuint Semiconductor, Inc. Raytheon retained its own existing MMIC capabilities and has the right to license TI's MMIC technology for use in future product applications from TriQuint.
Shortly after Raytheon acquired TI DSEG, Raytheon then acquired Hughes Aircraft from General Motors. Raytheon then owned TI's mercury cadmium telluride detector business and Infrared (IR) systems group. In California, it also had Hughes infrared detector and an IR systems business. When again the US government forced Raytheon to divest itself of a duplicate capability, the company kept the TI IR systems business and the Hughes detector business. As a result of these acquisitions these former arch rivals of TI systems and Hughes detectors work together.
Immediately after acquisition, DSEG was known as Raytheon TI Systems (RTIS). It is now fully integrated into Raytheon and this designation no longer exists.
Semiconductors.
Early in 1952 Texas Instruments purchased a patent license to produce germanium transistors from Western Electric Co., the manufacturing arm of AT&T, for $25,000, beginning production by the end of the year.
On January 1, 1953, Haggerty brought Gordon Teal to the company as a research director. Gordon brought with him his expertise in growing semiconductor crystals. Teal's first assignment was to organize what became TI's Central Research Laboratories (CRL), which Teal based on his prior experience at Bell Labs.
Among his new hires was Willis Adcock who joined TI early in 1953. Adcock, who like Teal was a physical chemist, began leading a small research group focused on the task of fabricating "grown-junction "silicon" single-crystal small-signal transistors. Adcock later became the first TI Principal Fellow.
First silicon transistor and integrated circuits.
On January 26, 1954, M Tanenbaum et al. at Bell Labs created the first workable silicon transistor. This work was reported in the spring of 1954 at the IRE off-the-record conference on Solid State Devices and later published in the Journal of Applied Physics, 26, 686–691(1955). Working independently in April 1954, Gordon Teal at TI created the first commercial silicon transistor and tested it on April 14, 1954. On May 10, 1954 at the Institute of Radio Engineers (IRE) National Conference on Airborne Electronics, in Dayton, Ohio. Teal also presented a paper, "Some Recent Developments in Silicon and Germanium Materials and Devices," at this conference.
In 1954, Texas Instruments designed and manufactured the first transistor radio. The Regency TR-1 used germanium transistors, as silicon transistors were much more expensive at the time. This was an effort by Haggerty to increase market demand for transistors.
Jack Kilby, an employee at TI's Central Research Labs, invented the integrated circuit in 1958. Kilby recorded his initial ideas concerning the integrated circuit in July 1958 and successfully demonstrated the world's first working integrated circuit on September 12, 1958. Six months later Robert Noyce of Fairchild Semiconductor (who went on to co-found Intel) independently developed the integrated circuit with integrated interconnect, and is also considered an inventor of the integrated circuit. Kilby won the 2000 Nobel Prize in Physics for his part of the invention of the integrated circuit. Noyce's chip, made at Fairchild, was made of silicon, while Kilby's chip was made of germanium. In 2008 TI named its new development laboratory "Kilby Labs" after Jack Kilby.
In 2011, Intel, Samsung, LG, ST-Ericsson, Huawei's HiSilicon Technologies subsidiary, Via Telecom and three other undisclosed chipmakers licensed the C2C link specification developed by Arteris Inc. and Texas Instruments.
Standard TTL.
The 7400 series of transistor-transistor logic (TTL) chips, developed by Texas Instruments in the 1960s, popularized the use of integrated circuits in computer logic. The military grade version of this was the 5400 series. 
Microprocessor.
Texas Instruments invented the hand-held calculator (a prototype called "Cal Tech") in 1967 and the single-chip microcomputer in 1971, was assigned the first patent on a single-chip microprocessor (invented by Gary Boone) on September 4, 1973. This was disputed by Gilbert Hyatt, formerly of the Micro Computer Company, in August 1990 when he was awarded a patent superseding TI's. This was over-turned on June 19, 1996 in favor of TI (note: Intel is usually given credit with Texas Instruments for the almost-simultaneous invention of the microprocessor).
First speech synthesis chip.
In 1978, Texas Instruments introduced the first single-chip LPC speech synthesizer. In 1976 TI began a feasibility study memory intensive applications for bubble memory then being developed. They soon focused on speech applications. This resulted in the development the TMC0280 one-chip linear predictive coding (LPC) speech synthesizer which was the first time a single silicon chip had electronically replicated the human voice. This was used in several TI commercial products beginning with Speak & Spell which was introduced at the Summer Consumer Electronics Show in June 1978. In 2001 TI left the speech synthesis business, selling it to Sensory Inc. of Santa Clara, California.
Consumer electronics and computers.
In May 1954, Texas Instruments designed and built a prototype of the world's first transistor radio, and, through a partnership with Industrial Development Engineering Associates (I.D.E.A.) of Indianapolis, Indiana, the 100% solid-state radio was sold to the public beginning in November of that year.
TI continued to be active in the consumer electronics market through the 1970s and 1980s. Early on, this also included two digital clock models; one for desk, and the other a bedside alarm. From this sprang what became the Time Products Division, which made LED watches. Though these LED watches enjoyed early commercial success thanks to excellent quality, it was short lived due to poor battery life. LEDs were replaced with LCD watches for a short time, but these could not compete because of styling issues, excessive makes and models, and price points. The watches were manufactured in Dallas and then Lubbock, Texas. In 1978, Texas Instruments introduced the first single chip speech synthesizer, and incorporated it in a product called Speak & Spell, which was later immortalized in the movie "E.T. the Extra-Terrestrial". Several spin-offs, such as the Speak & Read and Speak & Math, were introduced soon thereafter.
In 1979, TI entered the home computer market with the TI99/4, a competitor to such entries as the Apple II, Tandy/RadioShack TRS-80 and the later Atari 400/800 series and Commodore VIC-20. It discontinued the TI-99/4A (1981), the sequel to the 99/4, in late 1983 amidst an intense price war waged primarily against Commodore. At the 1983 Winter CES, TI showed models 99/2 and the Compact Computer 40 (CC-40), the latter aimed at professional users. The TI Professional (1983) ultimately joined the ranks of the many unsuccessful DOS and x86-based—but non-compatible—competitors to the IBM PC (the founders of Compaq, an early leader in PC compatibles, all came from TI). The company for years successfully made and sold PC-compatible laptops before withdrawing from the market and selling its product line to Acer in 1997.
Artificial intelligence.
Texas Instruments was active in the 1980s in the area of artificial intelligence. In addition to ongoing developments in speech and signal processing and recognition, it developed and sold the Explorer computer family of LISP machines. For the Explorer a special 32-bit LISP microprocessor was developed, which was used in the Explorer II and the TI MicroExplorer (a LISP Machine on a NuBus board for the Apple Macintosh). AI application software developed by TI for the Explorer included the Gate Assignment system for United Airlines, described as "an artificial intelligence program that captures the combined experience and knowledge of a half-dozen United operations experts." In software for the PC, they introduced "Personal Consultant" a rule-based expert system development tool and runtime engine, followed by "Personal Consultant Plus" written in the LISP-like language from MIT known as Scheme, and the natural language menu system NLMenu.
Sensors and controls.
Texas Instruments was a major OEM of sensor, control, protection, and RFID products for the automotive, appliance, aircraft, and other industries. The S&C division was headquartered in Attleboro, Massachusetts.
In 2006, Bain Capital LLC, a private equity firm, purchased the Sensors & Controls division for $3.0 billion in cash. The RFID portion of the division remained part of TI, transferring to the Application Specific Products business unit of the Semiconductor division, with the newly formed independent company based in Attleboro taking the name Sensata Technologies.
Software.
TI sold its software division along with its main product such as the IEF to Sterling Software in 1997. It is now part of Computer Associates. TI still owns small pieces of software though such as the software for calculators like TI Interactive!. TI also creates a significant amount of target software for its digital signal processors, along with host-based tools for creating DSP applications.
Divisions.
Today, TI is made up of three divisions: Semiconductors (SC), Educational Technology (ET), and Digital Light Processing (DLP).
Semiconductors.
Semiconductor products account for approximately 96% of TI's revenues. TI's semiconductor-related product areas include digital signal processors in the TMS320 series, high speed digital-to-analog and analog-to-digital converters, power management solutions, and high performance analog circuits.
TI’s Wireless Business Unit (WBU) produces wireless solutions for products such as smartphones and eBooks, tablets, consumer electronics and other portable devices. Wireless communications has been a primary focus for TI, with around 50% of all cellular phones sold worldwide containing TI chips. 
The Mixed Signal Automotive group is a business unit that manufactures mixed signal and analog solutions for transportation and automotive applications. In the power space, this unit produces DC/DC controllers and converters, low dropout voltage regulators (LDOs), voltage references and voltage supervisors. In the networking space, MSA has solutions for CAN and LIN. Safety-related solutions include airbags and anti-lock braking.
Signal processing.
Digital Light Processing is a trademark under which Texas Instruments sells technology regarding TVs, video projectors and digital cinema: on February 2, 2000, Philippe Binant, technical manager of Digital Cinema Project at Gaumont in France, realized the first digital cinema projection in Europe with the DLP CINEMA technology developed by TI.
Another business unit of the Semiconductor division called Application Specific Products (ASP) develops specific products that cater to a broad range of DSP applications, such as digital still cameras, cable modems, Voice over IP (VOIP), streaming media, speech compression and recognition, wireless LAN and gateway products (residential and central office), and RFID.
TI makes a broad range of digital signal processors and a suite of tools called eXpressDSP, used to develop applications on these chips.
Microcontrollers and processors.
Texas Instruments maintains several lines of processors, including the Sitara ARM processor family featuring ARM Cortex-A8 and ARM9 to serve a broad base of applications.
Texas Instruments also offers a portfolio of microcontrollers, including:
In the past, TI has also sold microcontrollers based on ARM7 (TMS470) and 8051 cores.
In addition to its microcontrollers, Texas Instruments also produces several multi-core processor lines.
Educational technology.
Texas Instruments produces a range of calculators, with the TI-30 being one of the most popular early calculators. TI has also developed a line of graphing calculators, the first being the TI-81, and most popular being the TI-83 Plus (with the TI-84 Plus being an updated equivalent).
There are many TI calculators still selling without graphing capabilities. The TI-30 has been replaced by the TI-30X IIS. There are some financial calculators for sale on the TI website.
In 2007, TI released the TI-Nspire family of calculators, as well as computer software that has similar capabilities to the calculators.
Texas Instruments calculator community.
In the 1990s, with the advent of TI's graphing calculator series, programming became popular among some students. The TI-8x series of calculators (beginning with the TI-81) came with a built-in BASIC interpreter, through which simple programs could be created. The TI-85 was the first TI calculator to allow assembly programming (via a shell called "ZShell"), and the TI-83 was the first in the series to receive native assembly. While the earlier BASIC programs were relatively simple applications or small games, the modern assembly-based programs rival what one might find on a Game Boy or PDA.
Around the same time that these programs were first being written, personal web pages were becoming popular (through services such as Angelfire and GeoCities), and programmers began creating websites to host their work, along with tutorials and other calculator-relevant information. This led to the formation of TI calculator webrings and eventually a few large communities, including the now-defunct TI-Files and still-active ticalc.org.
The TI community reached the height of its popularity in the early 2000s, with new websites and programming groups being started almost daily. In fact, the aforementioned community sites were exploding with activity, with close to 100 programs being uploaded daily by users of the sites. There was also a competition between both sites to be the top site in the community, which helped increase interest and activity in the community.
One of the common unifying forces that has united the community over the years has been the rather contentious relationship with Texas Instruments regarding control over its graphing calculators. TI graphing calculators generally fall into two distinct groups: those powered by the Zilog Z80 and those running on the Motorola 68000 series. Both lines of calculators are locked by TI with checks in the hardware and through the signing of software to disable use of custom flash applications and operating systems.
However, users employed brute force to find the keys and publish them in 2009. TI responded by sending invalid DMCA takedown notices, causing the Texas Instruments signing key controversy. Enthusiasts had already been creating their own operating systems before the finding of the keys, which could be installed with other methods.
Competitors.
TI has the largest market share in the analog semiconductor industry which has an estimated market TAM exceeding US$37 billion. TI is reported to have 14% of the market, leading ahead of competitors STMicroelectronics, Infineon and NXP Semiconductors according to latest reports. from Gartner
Industry recognition.
In 2007, Texas Instruments was awarded the Manufacturer of the Year for Global Supply Chain Excellence by World Trade magazine.
In six consecutive years (2007 through 2012), TI made it to the list of most ethical companies in the world, compiled by Ethisphere Institute. TI is the only company to appear for five consecutive years in the Electronics/Semiconductor category.
A more complete list of TI's awards and recognition can be found at the Texas Instruments website.
Acquisitions.
National Semiconductor acquisition.
On April 4, 2011, Texas Instruments announced that it had agreed to buy National Semiconductor for $6.5 billion in cash. Texas Instruments would pay $25 per share of National Semiconductor stock. It was an 80% premium over the share price of $14.07 as of April 4, 2011 close. The deal made Texas Instruments the world's largest maker of analog technology components.
The companies formally merged on September 23, 2011.
Restatement.
On August 6, 1999, Texas Instruments Inc. announced the restatement of its results for parts of 1998 and the first quarter of 1999 after a review by the Securities & Exchange Commission over the timing of charges for a plant closing and writedown.

</doc>
<doc id="47769" url="https://en.wikipedia.org/wiki?curid=47769" title="Transistor–transistor logic">
Transistor–transistor logic

Transistor–transistor logic (TTL) is a class of digital circuits built from bipolar junction transistors (BJT) and resistors. It is called "transistor–transistor logic" because both the logic gating function (e.g., AND) and the amplifying function are performed by transistors (contrast with resistor–transistor logic (RTL) and diode–transistor logic (DTL)).
TTL is notable for being a widespread integrated circuit (IC) family used in many applications such as computers, industrial controls, test equipment and instrumentation, consumer electronics, synthesizers, etc. The designation "TTL" is sometimes used to mean TTL-compatible logic levels, even when not associated directly with TTL integrated circuits, for example as a label on the inputs and outputs of electronic instruments.
After their introduction in integrated circuit form in 1963 by Sylvania, TTL integrated circuits were manufactured by several semiconductor companies, with the 7400 series (also called 74xx) by Texas Instruments becoming particularly popular. TTL manufacturers offered a wide range of logic gate, flip-flops, counters, and other circuits. Several variations of the original bipolar TTL concept were developed, yielding circuits with higher speed or lower power dissipation to allow optimization of a design. TTL circuits simplified system design compared to earlier logic families, offering speed superior to RTL. The design of the input and outputs of TTL gates allowed many elements to be interconnected.
TTL became the foundation of computers and other digital electronics. Even after much larger scale integrated circuits made multiple-circuit-board processors obsolete, TTL devices still found extensive use as the "glue" logic interfacing more densely integrated components. TTL devices were originally made in ceramic and plastic dual-in-line (DIP) packages, and flat-pack form. TTL chips are now also made in surface-mount packages. Successors to the original bipolar TTL logic often are interchangeable in function with the original circuits, but with improved speed or lower power dissipation.
History.
TTL was invented in 1961 by James L. Buie of TRW, "particularly suited to the newly developing integrated circuit design technology", and it was originally named "transistor-coupled transistor logic" (TCTL). The first commercial integrated-circuit TTL devices were manufactured by Sylvania in 1963, called the Sylvania Universal High-Level Logic family (SUHL). The Sylvania parts were used in the controls of the Phoenix missile. TTL became popular with electronic systems designers after Texas Instruments introduced the 5400 series of ICs, with military temperature range, in 1964 and the later 7400 series, specified over a narrower range, and with inexpensive plastic packages in 1966.
The Texas Instruments 7400 family became an industry standard. Compatible parts were made by Motorola, AMD, Fairchild, Intel, Intersil, Signetics, Mullard, Siemens, SGS-Thomson, Rifa, National Semiconductor, and many other companies, even in the Eastern Bloc (Soviet Union, GDR, Poland, Czechoslovakia, Hungary, Romania - for details see 7400 series). Not only did others make compatible TTL parts, but compatible parts were made using many other circuit technologies as well. At least one manufacturer, IBM, produced non-compatible TTL circuits for its own use; IBM used the technology in the IBM System/38, IBM 4300, and IBM 3081.
The term "TTL" is applied to many successive generations of bipolar logic, with gradual improvements in speed and power consumption over about two decades. The most recently introduced family 74Fxx is still sold today, and was widely used into the late 90s. 74AS/ALS Advanced Schottky, was introduced in 1985. As of 2008, Texas Instruments continues to supply the more general-purpose chips in numerous obsolete technology families, albeit at increased prices. Typically, TTL chips integrate no more than a few hundred transistors each. Functions within a single package generally range from a few logic gates to a microprocessor bit-slice. TTL also became important because its low cost made digital techniques economically practical for tasks previously done by analog methods.
The Kenbak-1, ancestor to the first personal computers, uses TTL for its CPU instead of a microprocessor chip, which was not available in 1971. The Datapoint 2200 from 1970 use TTL components for its CPU and is the basis for the 8008 and later the x86 instruction set. The 1973 Xerox Alto and 1981 Star workstations, which introduced the graphical user interface, use TTL circuits integrated at the level of Arithmetic logic units (ALU)s and bitslices, respectively. Most computers used TTL-compatible "glue logic" between larger chips well into the 1990s. Until the advent of programmable logic, discrete bipolar logic was used to prototype and emulate microarchitectures under development.
Implementation.
Fundamental TTL gate.
TTL inputs are the emitters of a multiple-emitter transistor. This IC structure is functionally equivalent to multiple transistors where the bases and collectors are tied together. The output is buffered by a common emitter amplifier.
Inputs both logical ones. When all the inputs are held at high voltage, the base–emitter junctions of the multiple-emitter transistor are reverse-biased. Unlike DTL, a small “collector” current (approximately 10µA) is drawn by each of the inputs. This is because the transistor is in reverse-active mode. An approximately constant current flows from the positive rail, through the resistor and into the base of the multiple emitter transistor. This current passes through the base-emitter junction of the output transistor, allowing it to conduct and pulling the output voltage low (logical zero).
An input logical zero. Note that the base-collector junction of the multiple-emitter transistor and the base-emitter junction of the output transistor are in series between the bottom of the resistor and ground. If one input voltage becomes zero, the corresponding base-emitter junction of the multiple-emitter transistor is in parallel with these two junctions. A phenomenon called current steering means that when two voltage-stable elements with different threshold voltages are connected in parallel, the current flows through the path with the smaller threshold voltage. That is, current flows out of this input and into the zero (low) voltage source. As a result, no current flows through the base of the output transistor, causing it to stop conducting and the output voltage becomes high (logical one). During the transition the input transistor is briefly in its active region; so it draws a large current away from the base of the output transistor and thus quickly discharges its base. This is a critical advantage of TTL over DTL that speeds up the transition over a diode input structure.
The main disadvantage of TTL with a simple output stage is the relatively high output resistance at output logical "1" that is completely determined by the output collector resistor. It limits the number of inputs that can be connected (the fanout). Some advantage of the simple output stage is the high voltage level (up to VCC) of the output logical "1" when the output is not loaded.
A common variation omits the collector resistor of the output transistor, making an open collector output. This allows the designer to fabricate logic by connecting the open collector outputs of several logic gates together and providing a single external pull-up resistor. If any of the logic gates becomes logic low (transistor conducting), the combined output will be low. Examples of this type of gate are the 7401 and 7403 series. Open collector outputs of some gates have a higher maximum voltage, such as 15V for the 7426, useful when driving other than TTL loads.
TTL with a "totem-pole" output stage.
To solve the problem with the high output resistance of the simple output stage the second schematic adds to this a "totem-pole" ("push–pull") output. It consists of the two n-p-n transistors V3 and V4, the "lifting" diode V5 and the current-limiting resistor R3 (see the figure on the right). It is driven by applying the same "current steering" idea as above.
When V2 is "off", V4 is "off" as well and V3 operates in active region as a voltage follower producing high output voltage (logical "1"). When V2 is "on", it activates V4, driving low voltage (logical "0") to the output. V2 and V4 collector–emitter junctions connect V4 base–emitter junction in parallel to the series-connected V3 base–emitter and V5 anode–cathode junctions. V3 base current is deprived; the transistor turns "off" and it does not impact on the output. In the middle of the transition, the resistor R3 limits the current flowing directly through the series connected transistor V3, diode V5 and transistor V4 that are all conducting. It also limits the output current in the case of output logical "1" and short connection to the ground. The strength of the gate may be increased without proportionally affecting the power consumption by removing the pull-up and pull-down resistors from the output stage.
The main advantage of TTL with a "totem-pole" output stage is the low output resistance at output logical "1". It is determined by the upper output transistor V3 operating in active region as a voltage follower. The resistor R3 does not increase the output resistance since it is connected in the V3 collector and its influence is compensated by the negative feedback. A disadvantage of the "totem-pole" output stage is the decreased voltage level (no more than 3.5 V) of the output logical "1" (even if the output is unloaded). The reason of this reduction are the voltage drops across the V3 base–emitter and V5 anode–cathode junctions.
Interfacing considerations.
Like DTL, TTL is a "current-sinking logic" since a current must be drawn from inputs to bring them to a logic 0 level. At low input voltage, the TTL input sources current which must be absorbed by the previous stage. The maximum value of this input current is about 1.6 mA for a standard TTL gate. The input source has to be low-resistive enough (<500 Ω) so that the flowing current creates only a negligible voltage drop (<0.4 V) across it, for the input to be considered as a logical "0" (with a 0.4 V "noise margin", see below). The output stage of the most common TTL gates are specified to function correctly when driving up to 10 standard input stages (a fanout of 10). TTL inputs are sometimes simply left floating to provide a logical "1", though this usage is not recommended.
Standard TTL circuits operate with a 5-volt power supply. A TTL input signal is defined as "low" when between 0 V and 0.8 V with respect to the ground terminal, and "high" when between 2.2 V and VCC (5 V), and if a voltage signal ranging between 0.8 V and 2.0 V is sent into the input of a TTL gate, there is no certain response from the gate and therefore it is considered "uncertain" (precise logic levels vary slightly between sub-types and by temperature). TTL outputs are typically restricted to narrower limits of between 0.0 V and 0.4 V for a "low" and between 2.6 V and VCC for a "high", providing at least 0.4 V of noise immunity. Standardization of the TTL levels is so ubiquitous that complex circuit boards often contain TTL chips made by many different manufacturers selected for availability and cost, compatibility being assured; two circuit board units off the same assembly line on different successive days or weeks might have a different mix of brands of chips in the same positions on the board; repair is possible with chips manufactured years (sometimes over a decade) later than original components. Within usefully broad limits, logic gates can be treated as ideal Boolean devices without concern for electrical limitations. The 0.4V noise margins are adequate because of the low output impedance of the driver stage, that is, a large amount of noise power superimposed on the output is needed to drive an input into an undefined region.
In some cases (e.g., when the output of a TTL logic gate needs to be used for driving the input of a CMOS gate), the voltage level of the "totem-pole" output stage at output logical "1" can be increased up to VCC by connecting an external resistor between the V3 collector and the positive rail. It pulls up the V5 cathode and cuts-off the diode. However, this technique actually converts the sophisticated "totem-pole" output into a simple output stage having significant output resistance when driving a high level (determined by the external resistor).
Packaging.
Like most integrated circuits of the period 1963–1990, commercial TTL devices are usually packaged in dual in-line packages (DIPs), usually with 14 to 24 pins, for through-hole or socket mounting. The DIPs were usually made of epoxy plastic (PDIP) for commercial-grade parts or of ceramic (CDIP) for military-grade parts.
Beam-lead chip dies without packages were made for assembly into larger arrays as hybrid integrated circuits. Parts for military and aerospace applications were packaged in flatpacks, a form of surface-mount package, with leads suitable for welding or soldering to printed circuit boards. Today, many TTL-compatible devices are available in surface-mount packages, which are available in a wider array of types than through-hole packages.
TTL is particularly well suited to bipolar integrated circuits because additional inputs to a gate merely required additional emitters on a shared base region of the input transistor. If individually packaged transistors were used, the cost of all the transistors would discourage one from using such an input structure. But in an integrated circuit, the additional emitters for extra gate inputs add only a small area.
At least one computer manufacturer, IBM, built its own flip chip integrated circuits with TTL; these chips were mounted on ceramic multi-chip modules.
Comparison with other logic families.
TTL devices consume substantially more power than equivalent CMOS devices at rest, but power consumption does not increase with clock speed as rapidly as for CMOS devices. Compared to contemporary ECL circuits, TTL uses less power and has easier design rules but is substantially slower. Designers can combine ECL and TTL devices in the same system to achieve best overall performance and economy, but level-shifting devices are required between the two logic families. TTL is less sensitive to damage from electrostatic discharge than early CMOS devices.
Due to the output structure of TTL devices, the output impedance is asymmetrical between the high and low state, making them unsuitable for driving transmission lines. This drawback is usually overcome by buffering the outputs with special line-driver devices where signals need to be sent through cables. ECL, by virtue of its symmetric low-impedance output structure, does not have this drawback.
The TTL "totem-pole" output structure often has a momentary overlap when both the upper and lower transistors are conducting, resulting in a substantial pulse of current drawn from the power supply. These pulses can couple in unexpected ways between multiple integrated circuit packages, resulting in reduced noise margin and lower performance. TTL systems usually have a decoupling capacitor for every one or two IC packages, so that a current pulse from one TTL chip does not momentarily reduce the supply voltage to another.
Several manufacturers now supply CMOS logic equivalents with TTL-compatible input and output levels, usually bearing part numbers similar to the equivalent TTL component and with the same pinouts. For example, the 74HCT00 series provides many drop-in replacements for bipolar 7400 series parts, but uses CMOS technology.
Sub-types.
Successive generations of technology produced compatible parts with improved power consumption or switching speed, or both. Although vendors uniformly marketed these various product lines as TTL with Schottky diodes, some of the underlying circuits, such as used in the LS family, could rather be considered DTL.
Variations of and successors to the basic TTL family, which has a typical gate propagation delay of 10ns and a power dissipation of 10 mW per gate, for a power–delay product (PDP) or switching energy of about 100 pJ, include:
Most manufacturers offer commercial and extended temperature ranges: for example Texas Instruments 7400 series parts are rated from 0 to 70 °C, and 5400 series devices over the military-specification temperature range of −55 to +125 °C.
Special quality levels and high-reliability parts are available for military and aerospace applications.
Radiation-hardened devices are offered for space applications.
Applications.
Before the advent of VLSI devices, TTL integrated circuits were a standard method of construction for the processors of mini-computer and mainframe processors; such as the DEC VAX and Data General Eclipse, and for equipment such as machine tool numerical controls, printers and video display terminals. As microprocessors became more functional, TTL devices became important for "glue logic" applications, such as fast bus drivers on a motherboard, which tie together the function blocks realized in VLSI elements.
Analog applications.
While originally designed to handle logic-level digital signals, a TTL inverter can be biased as an analog amplifier. Connecting a resistor between the output and the input biases the TTL element as a negative feedback amplifier. Such amplifiers may be useful to convert analog signals to the digital domain but would not ordinarily be used where analog amplification is the primary purpose. TTL inverters can also be used in crystal oscillators where their analog amplification ability is significant.
A TTL gate may operate inadvertently as an analog amplifier if the input is connected to a slowly changing input signal that traverses the unspecified region from 0.8 V to 2.2 V. The output can be erratic when the input is in this range. A slowly changing input like this can also cause excess power dissipation in the output circuit. If such an analog input must be used, there are specialized TTL parts with Schmitt trigger inputs available that will reliably convert the analog input to a digital value, effectively operating as a one bit A to D converter.

</doc>
<doc id="47770" url="https://en.wikipedia.org/wiki?curid=47770" title="TTL">
TTL

TTL may refer to:

</doc>
<doc id="47772" url="https://en.wikipedia.org/wiki?curid=47772" title="Instruction set">
Instruction set

An instruction set, or instruction set architecture (ISA), is the part of the computer architecture related to programming, including the native data types, instructions, registers, addressing modes, memory architecture, interrupt and exception handling, and external I/O. An ISA includes a specification of the set of opcodes (machine language), and the native commands implemented by a particular processor.
Overview.
An instruction set architecture is distinguished from a microarchitecture, which is the set of processor design techniques used, in a particular processor, to implement the instruction set. Processors with different microarchitectures can share a common instruction set. For example, the Intel Pentium and the AMD Athlon implement nearly identical versions of the x86 instruction set, but have radically different internal designs.
The concept of an "architecture", distinct from the design of a specific machine, was developed by Fred Brooks at IBM during the design phase of System/360. 
Some virtual machines that support bytecode as their ISA such as Smalltalk, the Java virtual machine, and Microsoft's Common Language Runtime, implement this by translating the bytecode for commonly used code paths into native machine code. In addition, these virtual machines execute less frequently used code paths by interpretation (see: Just-in-time compilation). Transmeta implemented the x86 instruction set atop VLIW processors in this fashion.
Classification of instruction sets.
A complex instruction set computer (CISC) has many specialized instructions, some of which may only be rarely used in practical programs. A reduced instruction set computer (RISC) simplifies the processor by efficiently implementing only the instructions that are frequently used in programs, while the less common operations are implemented as subroutines, having their resulting additional processor execution time offset by infrequent use.
Theoretically important types are the minimal instruction set computer and the one instruction set computer, but these are not implemented in commercial processors. Another variation is the very long instruction word (VLIW) where the processor receives many instructions encoded and retrieved in one instruction word.
Machine language.
Machine language is built up from discrete "statements" or "instructions". On the processing architecture, a given instruction may specify:
More complex operations are built up by combining these simple instructions, which are executed sequentially, or as otherwise directed by control flow instructions.
Instruction types.
Examples of operations common to many instruction sets include:
Complex instructions.
Processors may include "complex" instructions in their instruction set. A single "complex" instruction does something that may take many instructions on other computers. Such instructions are typified by instructions that take multiple steps, control multiple functional units, or otherwise appear on a larger scale than the bulk of simple instructions implemented by the given processor. Some examples of "complex" instructions include:
Complex instructions are more common in CISC instruction sets than in RISC instruction sets, but RISC instruction sets may include them as well. RISC instruction sets generally do not include ALU operations with memory operands, or instructions to move large blocks of memory, but most RISC instruction sets include SIMD or vector instructions that perform the same arithmetic operation on multiple pieces of data at the same time. SIMD instructions have the ability of manipulating large vectors and matrices in minimal time. SIMD instructions allow easy parallelization of algorithms commonly involved in sound, image, and video processing. Various SIMD implementations have been brought to market under trade names such as MMX, 3DNow! and AltiVec.
Specialised processor types like GPUs for example also provide complex instruction sets. Nonetheless many of these specialised processor complex instruction sets do not have a publicly available native instruction set and native assembly language for proprietary hardware related reasons and are usually only accessible to software developers through standardized higher level languages and APIs. The OpenGL virtual instruction set and virtual assembly language ARB assembly language and CUDA are examples of such hardware abstraction layers on top of the specialised processor native instruction set.
Parts of an instruction.
On traditional architectures, an instruction includes an opcode that specifies the operation to perform, such as "add contents of memory to register"—and zero or more operand specifiers, which may specify registers, memory locations, or literal data. The operand specifiers may have addressing modes determining their meaning or may be in fixed fields. In very long instruction word (VLIW) architectures, which include many microcode architectures, multiple simultaneous opcodes and operands are specified in a single instruction.
Some exotic instruction sets do not have an opcode field (such as Transport Triggered Architectures (TTA) or the Forth virtual machine), only operand(s).
Other unusual "0-operand" instruction sets lack any operand specifier fields, such as some stack machines including NOSC.
Conditional instructions often have a predicate field—a few bits that encode the specific condition to cause the operation to be performed rather than not performed. For example, a conditional branch instruction will be executed, and the branch taken, if the condition is true, so that execution proceeds to a different part of the program, and not executed, and the branch not taken, if the condition is false, so that execution continues sequentially. Some instruction sets also have conditional moves, so that the move will be executed, and the data stored in the target location, if the condition is true, and not executed, and the target location not modified, if the condition is false. Similarly, IBM z/Architecture has a conditional store. A few instruction sets include a predicate field in every instruction; this is called branch predication.
Instruction length.
The size or length of an instruction varies widely, from as little as four bits in some microcontrollers to many hundreds of bits in some VLIW systems. Processors used in personal computers, mainframes, and supercomputers have instruction sizes between 8 and 64 bits. The longest possible instruction on x86 is 15 bytes (120 bits). Within an instruction set, different instructions may have different lengths. In some architectures, notably most reduced instruction set computers (RISC), instructions are a fixed length, typically corresponding with that architecture's word size. In other architectures, instructions have variable length, typically integral multiples of a byte or a halfword. Some such as the ARM with "Thumb-extension" have "mixed" variable encoding, that is two fixed, usually 32-bit and 16-bit encodings, where instructions can not be mixed freely but must be switched between on a branch (or exception boundary in ARMv8).
A RISC instruction set normally has a fixed instruction width (often 4 bytes = 32 bits), whereas a typical CISC instruction set may have instructions of widely varying length (1 to 15 bytes for x86). Fixed-width instructions are less complicated to handle than variable-width instructions for several reasons (not having to check whether an instruction straddles a cache line or virtual memory page boundary for instance), and are therefore somewhat easier to optimize for speed.
Representation.
The instructions constituting a program are rarely specified using their internal, numeric form (machine code); they may be specified by programmers using an assembly language or, more commonly, may be generated from programming languages by compilers.
Design.
The design of instruction sets is a complex issue. There were two stages in history for the microprocessor. The first was the CISC (Complex Instruction Set Computer), which had many different instructions. In the 1970s, however, places like IBM did research and found that many instructions in the set could be eliminated. The result was the RISC (Reduced Instruction Set Computer), an architecture that uses a smaller set of instructions. A simpler instruction set may offer the potential for higher speeds, reduced processor size, and reduced power consumption. However, a more complex set may optimize common operations, improve memory/cache efficiency, or simplify programming.
Some instruction set designers reserve one or more opcodes for some kind of system call or software interrupt. For example, MOS Technology 6502 uses 00H, Zilog Z80 uses the eight codes C7,CF,D7,DF,E7,EF,F7,FFH while Motorola 68000 use codes in the range A000..AFFFH. 
Fast virtual machines are much easier to implement if an instruction set meets the
Popek and Goldberg virtualization requirements.
The NOP slide used in Immunity Aware Programming is much easier to implement if the "unprogrammed" state of the memory is interpreted as a NOP.
On systems with multiple processors, non-blocking synchronization algorithms are much easier to implement if the instruction set includes support for something such as "fetch-and-add", "load-link/store-conditional" (LL/SC), or "atomic compare and swap".
Instruction set implementation.
Any given instruction set can be implemented in a variety of ways. All ways of implementing a particular instruction set provide the same programming model, and all implementations of that instruction set are able to run the same binary executables. The various ways of implementing an instruction set give different tradeoffs between cost, performance, power consumption, size, etc.
When designing the microarchitecture of a processor, engineers use blocks of "hard-wired" electronic circuitry (often designed separately) such as adders, multiplexers, counters, registers, ALUs etc. Some kind of register transfer language is then often used to describe the decoding and sequencing of each instruction of an ISA using this physical microarchitecture.
There are two basic ways to build a control unit to implement this description (although many designs use middle ways or compromises):
Some designs use a combination of hardwired design and microcode for the control unit.
Some CPU designs use a writable control store—they compile the instruction set to a writable RAM or flash inside the CPU (such as the Rekursiv processor and the Imsys Cjip), or an FPGA (reconfigurable computing).
An ISA can also be emulated in software by an interpreter. Naturally, due to the interpretation overhead, this is slower than directly running programs on the emulated hardware, unless the hardware running the emulator is an order of magnitude faster. Today, it is common practice for vendors of new ISAs or microarchitectures to make software emulators available to software developers before the hardware implementation is ready.
Often the details of the implementation have a strong influence on the particular instructions selected for the instruction set. For example, many implementations of the instruction pipeline only allow a single memory load or memory store per instruction, leading to a load-store architecture (RISC). For another example, some early ways of implementing the instruction pipeline led to a delay slot.
The demands of high-speed digital signal processing have pushed in the opposite direction—forcing instructions to be implemented in a particular way. For example, to perform digital filters fast enough, the MAC instruction in a typical digital signal processor (DSP) must use a kind of Harvard architecture that can fetch an instruction and two data words simultaneously, and it requires a single-cycle multiply–accumulate multiplier.
Code density.
In early computers, memory was expensive, so minimizing the size of a program to make sure it would fit in the limited memory was often central. Thus the combined size of all the instructions needed to perform a particular task, the "code density", was an important characteristic of any instruction set. Computers with high code density often have complex instructions for procedure entry, parameterized returns, loops etc. (therefore retroactively named "Complex Instruction Set Computers", CISC). However, more typical, or frequent, "CISC" instructions merely combine a basic ALU operation, such as "add", with the access of one or more operands in memory (using addressing modes such as direct, indirect, indexed etc.). Certain architectures may allow two or three operands (including the result) directly in memory or may be able to perform functions such as automatic pointer increment etc. Software-implemented instruction sets may have even more complex and powerful instructions.
"Reduced instruction-set computers", RISC, were first widely implemented during a period of rapidly growing memory subsystems. They sacrifice code density to simplify implementation circuitry, and try to increase performance via higher clock frequencies and more registers. A single RISC instruction typically performs only a single operation, such as an "add" of registers or a "load" from a memory location into a register. A RISC instruction set normally has a fixed instruction width, whereas a typical CISC instruction set has instructions of widely varying length.
However, as RISC computers normally require more and often longer instructions to implement a given task, they inherently make less optimal use of bus bandwidth and cache memories.
Certain embedded RISC ISAs like Thumb and AVR32 typically exhibit very high density owing to a technique called code compression. This technique packs two 16-bit instructions into one 32-bit instruction, which is then unpacked at the decode stage and executed as two instructions.
Minimal instruction set computers (MISC) are a form of stack machine, where there are few separate instructions (16-64), so that multiple instructions can be fit into a single machine word. These type of cores often take little silicon to implement, so they can be easily realized in an FPGA or in a multi-core form. The code density of MISC is similar to the code density of RISC; the increased instruction density is offset by requiring more of the primitive instructions to do a task.
There has been research into executable compression as a mechanism for improving code density. The mathematics of Kolmogorov complexity describes the challenges and limits of this.
Number of operands.
Instruction sets may be categorized by the maximum number of operands "explicitly" specified in instructions.
codice_1
Due to the large number of bits needed to encode the three registers of a 3-operand instruction, RISC processors using 16-bit instructions are invariably 2-operand machines, such as the Atmel AVR, the TI MSP430, and some versions of the ARM Thumb. RISC processors using 32-bit instructions are usually 3-operand machines, such as processors implementing the Power Architecture, the SPARC architecture, the MIPS architecture, the ARM architecture, and the AVR32 architecture.
Each instruction specifies some number of operands (registers, memory locations, or immediate values) "explicitly". Some instructions give one or both operands implicitly, such as by being stored on top of the stack or in an implicit register. If some of the operands are given implicitly, fewer operands need be specified in the instruction. When a "destination operand" explicitly specifies the destination, an additional operand must be supplied. Consequently, the number of operands encoded in an instruction may differ from the mathematically necessary number of arguments for a logical or arithmetic operation (the arity). Operands are either encoded in the "opcode" representation of the instruction, or else are given as values or addresses following the instruction.
Register pressure.
"Register pressure" measures the availability of free registers at any point in time during the program execution. Register pressure is high when a large number of the available registers are in use; thus, the higher the register pressure, the more often the register contents must be spilled into memory. Increasing the number of registers in an architecture decreases register pressure but increases the cost.
While embedded RISC ISAs like Thumb suffer from extremely high register pressure due to lack of available programmable registers, general-purpose RISC ISAs like MIPS and Alpha enjoy low register pressure. CISC ISAs like x86-64 offer low register pressure despite the fact that they have fewer programmable registers. This is a manifestation of the many addressing modes and optimizations such as sub-register addressing, memory operands in ALU instructions, absolute addressing, PC-relative addressing, and register-to-register spills, which these ISAs offer.

</doc>
<doc id="47774" url="https://en.wikipedia.org/wiki?curid=47774" title="Maclisp">
Maclisp

MACLISP (or Maclisp, sometimes styled MacLisp or MacLISP) is a dialect of the Lisp programming language. It originated at MIT's Project MAC (from which it derived its prefix) in the late 1960s and was based on Lisp 1.5. Richard Greenblatt was the main developer of the original codebase for the PDP-6; Jon L. White was responsible for its later maintenance and development. The name 'Maclisp' started being used in the early 1970s to distinguish it from other forks of PDP-6 Lisp, notably BBN Lisp.
History.
Maclisp is a descendant of Lisp 1.5. Maclisp departs from Lisp 1.5 by using a "value cell" to access and store the dynamic values of variables; Lisp 1.5 used a linear search of an association list to determine a variable's value. The Maclisp variable evaluation is faster but has different variable semantics. Maclisp also employed reader macros to make more readable input and output. Instead of entering codice_1, one could enter codice_2 to get the same s-expression. Although both implementations put functions on the property list, Maclisp uses different syntax to define functions. Maclisp also has a load-on-demand feature.
Maclisp started on Digital Equipment Corporation PDP-6 and PDP-10 computers running the Incompatible Timesharing System (ITS); later it was ported to all other PDP-10 operating systems, for example, TOPS-10 and TOPS-20. The original implementation was in assembly language, but a later implemented on Multics used PL/I. Maclisp developed considerably in its lifetime. Major features were added which in other language systems would typically correspond to major release numbers.
Maclisp was used to implement the Macsyma symbolic algebra program; Macsyma's development also drove a number of features in Maclisp. The SHRDLU blocks-world program was written in Maclisp, and so the language was in widespread use in the artificial intelligence research community through the early 1980s. It was also used to implement other programming languages, such as Planner and Scheme. Multics Maclisp was used to implement the first Lisp-based Emacs.
Maclisp was an influential Lisp implementation, but is no longer actively maintained. It now runs on PDP-10 emulators and can be used for experimenting with early AI programs.
Characteristics.
Maclisp started with a small, fixed number of data types: cons cell, atom (later called "symbol"), integer, and floating-point number. Later additions included: arrays, which were however never first-class data-types; arbitrary-precision integers (bignums); strings; and tuples. All objects (except inums) were implemented as pointers, and their data type was determined by the block of memory into which it pointed, with a special case for small numbers (inums).
Programs could be interpreted or compiled. Compiled behavior was the same as interpreted except that local variables were lexical by default in compiled code, and no error checking was done for inline operations such as CAR and CDR. The Ncomplr compiler (mid-1970s) introduced fast numeric support to the Lisp world, generating machine instructions for arithmetic rather than calling interpretive routines which dispatched on data type. This made Lisp arithmetic comparable in speed to Fortran for scalar operations (though Fortran array and loop implementation remained much better).
The original version was limited by the 18-bit word address of the PDP-10, and considerable effort was expended in keeping the implementation lean and simple. Multics Maclisp had a far larger address space, but was expensive to use. When the memory and processing power of the PDP-10 were exceeded, the Lisp Machine was invented: Lisp Machine Lisp is the direct descendant of Maclisp. Several other Lisp dialects were also in use, and the need to unify the community resulted in the modern Common Lisp language.
Name.
MACLISP was named for Project MAC, and is unrelated to Apple's Macintosh ("Mac") computer, which it predates by decades. The various Lisp systems for the Macintosh have no particular similarity to Maclisp.

</doc>
<doc id="47775" url="https://en.wikipedia.org/wiki?curid=47775" title="Lisp Machine Lisp">
Lisp Machine Lisp

Lisp Machine Lisp is a dialect of the Lisp programming language. A direct descendant of Maclisp, it was initially developed in the mid to late 1970s as the systems programming language for the MIT Lisp machines. Lisp Machine Lisp was also the Lisp dialect with the most influence on the design of Common Lisp.
Lisp Machine Lisp itself branched into three dialects. Symbolics named their variant ZetaLisp. Lisp Machines, Inc. and later Texas Instruments (with the TI Explorer) would share a common code base, but their dialect of Lisp Machine Lisp would differ from the version maintained at the MIT AI Lab by Richard Stallman and others.
The Lisp Machine Manual describes the Lisp Machine Lisp language in detail. The manual was popularly known as the "Chine Nual", because the full title was printed across the front and back covers such that only those letters appeared on the front.[http://www.jargon.net/jargon/jargonfile/c/chinenual.html] This name is sometimes further abbreviated by blending the two words into "Chinual".
Some Lisp Machine Lisp features:

</doc>
<doc id="47778" url="https://en.wikipedia.org/wiki?curid=47778" title="Maskun">
Maskun

Maskun may refer to:

</doc>
<doc id="47779" url="https://en.wikipedia.org/wiki?curid=47779" title="Pohnpei">
Pohnpei

Pohnpei "upon ("pohn") a stone altar ("pei")" (formerly known as Ponape) is the name of an island of the Senyavin Islands which are part of the larger Caroline Islands group. It belongs to Pohnpei State, one of the four states in the Federated States of Micronesia (FSM). Major population centers on Pohnpei include Palikir, the FSM's capital, and Kolonia, the capital of Pohnpei State. Pohnpei Island is the largest (334 km²), highest (almost 800m), most populous (34,000 people), and most developed single island in the FSM. The islanders of Pohnpei have a reputation as being the most welcoming of outsiders among residents of the island group.
Pohnpei also contains a wealth of biodiversity. It is also one of the wettest places on earth with annual recorded rainfall exceeding each year in certain mountainous locations. It is home to the ka tree (Terminalia carolinensis) found only in Pohnpei and Kosrae.
Geography.
The highest point of the island is Mount Nanlaud at 772 or 782 metres. Pohnpei is home to several dozen bird species including four endemic species, the Pohnpei lorikeet, the Pohnpei fantail, the Pohnpei flycatcher and the long-billed white-eye. A fifth endemic, the Pohnpei starling, is thought to have recently gone extinct. The only land reptiles are a few species of lizard. Originally, only three mammals existed: rats, bats and dogs. Pigs were introduced, some are now feral, as are the deer brought during German times. The lagoons are rich in fish, mollusks, turtles and other marine fauna.
Climate.
Pohnpei belongs to the Tropical rainforest climate zone (Köppen: "Af"). It is also one of the wettest places on earth with annual recorded rainfall exceeding each year in certain mountainous locations.
History.
On Pohnpei, pre-colonial history is divided into three eras: "Mwehin Kawa" or "Mwehin Aramas" (Period of Building, or Period of Peopling, before c. 1100); "Mwehin Sau Deleur" (Period of the Lord of Deleur, c. 1100 to c. 1628); and "Mwehin Nahnmwarki" (Period of the Nahnmwarki, c. 1628 to c. 1885). Pohnpeian legend recounts that the Saudeleur rulers, the first to bring government to Pohnpei, were of foreign origin. The Saudeleur centralized form of absolute rule is characterized in Pohnpeian legend as becoming increasingly oppressive over several generations. Arbitrary and onerous demands, as well as a reputation for offending Pohnpeian deities, sowed resentment among Pohnpeians. The Saudeleur Dynasty ended with the invasion of Isokelekel, another semi-mythical foreigner, who replaced the Saudeleur rule with the more decentralized "nahnmwarki" system in existence today.
Pohnpeian historic society was highly structured into five tribes, various clans and sub-clans; each tribe headed by two principal chiefs. The tribes were organized on a feudal basis. In theory, "all land belonged to the chiefs, who received regular tribute and whose rule was absolute." Punishments administered by chiefs included death and banishment. Tribal wars included looting, destruction of houses and canoes and killing of prisoners.
Pohnpei's first European visitor was Álvaro de Saavedra on 14 September 1529 shortly before his death, when trying to find the way back to New Spain. He charted it as "San Bartolomé" and called this one and the surrounding islands as "Los Pintados" (literally, "the painted ones" in Spanish) because the natives were frequently tattooed. It was later visited by the Portuguese navigator Pedro Fernandes de Queirós, commanding the Spanish ship "San Jeronimo". on 23 December 1595; his description is brief, he made no attempt to land. Another visitor, for whom good documentation exists, was the Australian John Henry Rowe, his barque "John Bull" did not arrive until 10 September 1825, he did not land either as his vessel was chased off by native canoes. The first lengthy description of the island and its inhabitants is presented by the Russian explorer Fyodor Litke, whose ship "Senyavin" gave the island group of Pohnpei, Ant and Pakin its name. From 14 to 19 January 1828, his boats attempted to land but could not due to the hostility shown by the islanders, but natives then came aboard his ship, "some trading occurred, a short vocabulary was compiled, and a map made." F.H. von Kittlitz, a member of the Litke expedition made a further descriptive account, including the offshore ruins of Nan Madol, and the two reports together provided the first real knowledge of Pohnpei. It is not clear who the next visitors were; however, when Capt. J.H. Eagleston of the barque "Peru" sighted the island on 3 January 1832 it was already on his charts as "Ascension Island;" Riesenberg writes that it is uncertain who first called it Ascension Island, but the name became established until the Spanish period.
"By Spanish times with colonial troops had become considerably bloodier," a report of one battle alone in 1890 lists 300 Spanish casualties. In the German period officials attempted to identify agitators from both tribal sides and then ship them off to Rabaul in German New Guinea for one year of penal servitude.
Pre-Spanish population estimates are deemed unreliable.
From this time onward whaling and trading vessels came in increasing numbers. Very soon a "large colony of beachcombers, escaped convicts, and ship's deserters became established ashore," identified as "chiefly bad characters," according to the log of the Swedish frigate "Eugenie". The first missionary to arrive was Father Louis Désiré Maigret, a Roman Catholic priest. He had sailed from Honolulu on the schooner "Notre Dame de Paix" and began his efforts in December 1837, but he departed on 29 July 1838 for Valparaíso after seven unsuccessful months. In his company were "several Mangarevans and Tahitians," some of whom remained on Pohnpei and left descendants. Ten years later Maigret returned to the Hawaiian kingdom as Bishop of Honolulu. A group of Protestant missionaries from New England established themselves permanently on Pohnpei in 1852. Their letters and journals contain a wealth of information about the island and are preserved at Harvard University.
A drastic population decline occurred after 1854 due to a smallpox epidemic after European contact.
During the American Civil War, to counteract the United States blockade of their ports, Confederate States Navy ships hunted Yankee merchant shipping. On April 1, 1865, the "CSS Shenandoah" surprised four United States whalers at then- Ascension Island (Pohnpei), and destroyed them all. The local king, Nananierikie, was delighted to receive much of the spoils from this action.
By 1886 the Spaniards claimed the Caroline Islands which were part of the Manila-based Spanish East Indies and began to exert political authority. They founded the city "Santiago de la Ascensión" in what today is "Kolonia" (from Spanish "colonia" or colony). The Spanish built several government buildings, a fort, a church and a school. Spanish Capuchin friars were also sent from Manila to Pohnpei to preach the Catholic faith. After the 1898 Spanish–American War, the German Empire purchased the Caroline island group from Spain in 1899 together with the Marianas (except Guam) and 4 years later the Marshall Islands for 17 million goldmark.
During the German administration a fundamental change in land ownership was implemented on Pohnpei and throughout the Carolines. Beginning in 1907 the feudal system, where all land is held in fief, was gradually replaced with the issuance of individual deeds to land. The chief's economic advantages were thus reduced, and only force of tradition granted a first harvest tribute to chiefs. With land holding, taxes came due and new owners, in lieu of payment, were obliged to work 15 days per year on public projects, such as wharf construction, road building, etc. One such work for taxes engagement sparked the Sokehs Rebellion. It began as an insubordination event during road construction on Sokehs Island, then escalated into the murder of 9 persons, the subsequent apprehension and trial of 36 Sokehs rebels, the execution of 15 insurgents, and banishment for others to Babelthuap in the German Palau Islands.
The German census of 1911–12 shows 3,190 Pohnpeians, 585 Central Carolinians and 279 Melanesians. Many of the outer islands were resettled (mainly on Sokehs Island) in consequence of destructive typhoons in their home islands. A special census conducted in late 1947 shows a total population of 5,628, of which 4,451 were Pohnpeians, and 1,177 were natives of other Pacific islands. By 1963 the population had grown to nearly 10,000.
With the Treaty of Versailles Japan as mandatory power assumed control of all German colonial possessions north of the equator, having occupied Pohnpei along with the rest of the Carolines, the Marshalls, the Marianas (except for American-owned Guam) and Kiautschou Bay during World War I. In subsequent years and during World War II the Japanese garrison strength was composed of about 2,000 men of the IJN under Captain Jun Naito and 5,984 IJA men under Lieutenant General Masao Watanabe. However, Pohnpei was bypassed by the US Navy during the island-hopping amphibious campaigns of 1943–1945. The island was shelled on several occasions, including by the battleships USS "Massachusetts", USS Alabama, and , as well as air attacks launched from USS "Cowpens". In 1945 all Japanese citizens were forced off the island; many of their Pohnpeian family members remained.
In 1980, a group of twelve artists including Marina Abramović, John Cage and Laurie Anderson were invited to Pohnpei as part of a project called "Word of Mouth", designed to collect the artists' conversations. According to Anderson's recording for this project, as well as the track "Word of Mouth" from her album "The Ugly One with the Jewels", while the artists were there a number of prisoners managed to escape from the jail, after which they broke into the radio station and shot the DJ before going on a rampage through the jungle armed with lawnmower blades. Four people were murdered overall. Television was also introduced to Pohnpei around this time.
The Federated States of Micronesia achieved independence in 1986 after being administered by the United States under UN auspices since 1947 as part of the Trust Territory of the Pacific Islands.
The natives of Pohnpei, especially the 'older' generations, often refer to events in their past as having occurred, e.g., in "German times" or "before the Spaniards," which identifies the historical periods as follows:
Demographics.
The population of the state in 2010 was approximately 34,000. While the majority of the population consider themselves ethnic Pohnpeians, Pohnpei is more ethnically diverse than any other island in the FSM. This is largely due to more than a century of foreign colonial occupation, bringing in Spanish, German, Japanese, Chamorro, Filipino, US, Australian, other western Europeans, and it being home to the capital of the national government, which employs hundreds of people from the other three FSM States (Yap, Chuuk, Kosrae) having distinct ethnic and cultural origins. The indigenous makeup also includes the multiple regional ethnicities of the outer islands within Pohnpei State, resulting in a mix of Australasian Pacific Islanders and hence making Pohnpei Island the FSM's melting pot.
Languages.
The Pohnpeian language (formerly called "Ponapean") and its dialects are the indigenous languages of Pohnpei. The Federated States of Micronesia government also uses English as an official language.
Administrative divisions.
The municipalities on the island of Pohnpei are:
Transportation.
Pohnpei International Airport (IATA code PNI) is located near Kolonia, on a small island named Deketik off the northern coast of the main island.
Sport.
The FSM is part of the international Olympic movement and sent teams to the last two summer games in Beijing and Sydney with athletes participating in track and field, swimming and weightlifting. The most notable Pohnpeian athlete is marathoner Elias Rodriguez who ran for the FSM at the Sydney Olympics. Rodriguez finished last in the marathon but was cheered on by tens of thousands of spectators and watched by millions of television viewers as he entered the Olympic stadium for a final lap immediately prior to the closing ceremony which was delayed to allow his finish.
Pohnpei in fiction.
Pohnpei (as Ponape) plays a role in several stories in Cthulhu Mythos by H. P. Lovecraft and others. Its role in "Out of the Aeons", by Lovecraft and Hazel Heald, was inspired by the ruins of Nan Madol (see above), which had already been used as the setting for a lost race story by Abraham Merritt, "The Moon Pool", in which the islands are called Nan-Matal.
Pohnpei is a central location in "South Sea Adventure" (1952), the second of Willard Price's Young Adult Adventure Series books featuring Hal and Roger Hunt.
Pohnpei, or "Ponape" as it is spelled, is stated as the home island of "Mike" on the popular blog "Dunce Upon A Time", authored by BC Woods.

</doc>
<doc id="47780" url="https://en.wikipedia.org/wiki?curid=47780" title="Ponape">
Ponape

Ponape may refer to:

</doc>
<doc id="47782" url="https://en.wikipedia.org/wiki?curid=47782" title="Białystok">
Białystok

Białystok [ (; , , , ) is the largest city in northeastern Poland and the capital of the Podlaskie Voivodeship.
Located in the Białystok Uplands () of the Podlaskie Plain () on the banks of the Biała River, Białystok ranks second in terms of population density, eleventh in population, and thirteenth in area, of the cities of Poland. It has historically attracted migrants from elsewhere in Poland and beyond, particularly from Central and Eastern Europe. This is facilitated by the fact that the nearby border with Belarus is also the eastern border of the European Union, as well as the Schengen Area. The city and its adjacent municipalities constitute Metropolitan Białystok. The city has a , characterized by warm summers and long frosty winters. Forests are an important part of Białystok's character, and occupy around 1,756 ha (4,340 acres) (17.2% of the administrative area of the city) which places it as the fifth most forested city in Poland.
The first settlers arrived in the 14th century. A town grew up and received its municipal charter in 1692. Białystok has traditionally been one of the leading centers of academic, cultural, and artistic life in Podlaskie and the most important economic center in northeastern Poland. In the nineteenth century Białystok was an important center for light industry, which was the reason for the substantial growth of the city's population. But after the fall of communism in 1989 many of these factories faced severe problems and subsequently closed down. Through the infusion of EU investment funds, the city continues to work to reshape itself into a modern metropolis. Białystok in 2010, was on the short-list, but ultimately lost the competition to become a finalist for European Capital of Culture in 2016. Over the centuries Białystok has produced a number of people who have provided unique contributions to the fields of science, language, politics, religion, sports, visual arts and performing arts. This environment was created in the mid-eighteenth century by the patronage of Jan Klemens Branicki for the arts and sciences. These include Ryszard Kaczorowski, the last émigré President of the Republic of Poland; L. L. Zamenhof, the creator of Esperanto; and Albert Sabin, the co-developer of the polio vaccine.
Etymology.
The English translation of "Białystok" is "white slope". Due to changing borders over the centuries, the city has been known as ("Byelastok"?, "Biełastok"? ), ("Byalistok", "Bjalistok"), ("Bilostok"), ("Balstogė"), and ("Belostok").
Linguist A. P. Nepokupnyj proposes that the language source for Białystok is Yotvingian. Names with the -"stok" suffix as a second element of a hydronym are localized in the basin of the upper Narew.
History.
Archaeological discoveries show that the first settlements in the area of present-day Białystok occurred during the Stone Age. Tombs of ancient settlers can be found in the district of Dojlidy. In the early Iron Age a mix of Prussians, Yotvingians and Wielbark culture people settled in the area producing kurgans, the tombs of the chiefs in the area located in the current village of Rostołty. Since then, the Białystok area has been at the crossroads of cultures. Trade routes linking the Baltic to the Black Sea favored the development of settlements with Yotvingia-Ruthenian-Polish cultural characteristics.
The city of Białystok has existed for five centuries and during this time the fate of the city has been affected by various political and economic forces.
Surviving documents attest that around 1437 a representative of the Raczków family, Jakub Tabutowicz of the coat of arms Łabędź, received from Michael Žygimantaitis son of Sigismund Kęstutaitis, Duke of Lithuania, a wilderness area along the river Biała that marked the beginning of Białystok as a settlement.
The first brick church and a castle were built between 1617 and 1826. The two-floor castle, designed on a rectangular plan in the Gothic-Renaissance style, was the work of Job Bretfus. Extension of the castle was continued by Krzysztof Wiesiołowski, starost of Tykocin, Grand Marshal of Lithuania since 1635, and husband of Aleksandra Marianna Sobieska. In 1637 he died childless, and as a result Białystok came under the management of his widow. After her death in 1645 the Wiesiołowski estate, including Białystok, passed to the Commonwealth to cover the costs of maintaining Tykocin Castle. In the years 1645–1659 Białystok was managed by the governors of Tykocin and was part of the Grand Duchy of Lithuania.
In 1661 it was given to Stefan Czarniecki as a reward for his service in the victory over the Swedes during the Deluge. Four years later, it was given as a dowry of his daughter Aleksandra, who married Hetman Jan Klemens Branicki, thus passing into the hands of the Branicki family. In 1692, Stefan Mikołaj Branicki, the son of Jan Klemens Branicki (Marshal of the Crown Court), obtained city rights for Białystok from King John III Sobieski. He constructed the Branicki Palace on the foundations of the castle of the Wiesiołowski family. In the second half of the eighteenth century the ownership of the city was inherited by Field Crown Hetman Jan Klemens Branicki. It was he who transformed the palace built by his father into a magnificent residence of a great noble.
The end of the eighteenth century saw the division of the Polish–Lithuanian Commonwealth, in three steps, among the neighboring states. The Kingdom of Prussia acquired Białystok and the surrounding region during the third partition. The city became the capital of the New East Prussia province in 1795. Prussia lost the territory following Napoleon Bonaparte's victory in the War of the Fourth Coalition as the resultant 1807 Treaties of Tilsit awarded the area to the Russian Empire, which organized the region into the Belostok Oblast, with the city as the regional center.
At the end of the nineteenth century, the majority of the city's population was Jewish. According to Russian census of 1897, out of the total population of 66,000, Jews constituted 41,900 (so around 63% percent). This heritage can be seen on the Jewish Heritage Trail in Białystok. The first Anarchist groups to attract a significant following of Russian workers or peasants, were the Anarcho-Communist Chernoe-Znamia groups, founded in Białystok in 1903. The Białystok pogrom occurred between 14–16 June 1906 in the city. During the pogrom between 81 and 88 people were killed, and about 80 people were wounded.
During World War I the Bialystok-Grodno District was the administrative division of German-controlled territory of Ober-Ost. It comprised the city, as the capital, and the surrounding Podlaskie region, roughly corresponding to the territory of the earlier Belostok Oblast. At the end of World War I the city became part of the newly independent Second Polish Republic, as the capital of the Białystok Voivodeship (1919–1939). During the 1919–1920 Polish-Soviet War, possession of the city by the Red Army and the Provisional Polish Revolutionary Committee occurred during the lead up to the Battle of Warsaw. During the resultant counteroffensive, the city returned to Polish control after the Battle of Białystok.
With the beginning of World War II, Poland was invaded by Nazi Germany and the Soviet Union, and initially the city came under Soviet control, as a result of the Molotov-Ribbentrop Pact. It was incorporated into the Byelorussian SSR from 1939 to 1941 as the capital of the Belastok Voblast. After the Nazi attack on Soviet Union in 1941, Białystok was occupied by the German Army on 27 June 1941, during the Battle of Białystok–Minsk, and the city became the capital of Bezirk Białystok, a separate region in German occupied Poland, until 1944.
From the very beginning, the Nazis pursued a ruthless policy of pillage and removal of the non-German population. The 56,000 Jewish residents of the town were confined in a ghetto. On August 15, 1943, the Białystok Ghetto Uprising began, and several hundred Polish Jews and members of the Anti-Fascist Military Organisation () started an armed struggle against the German troops who were carrying out the planned liquidation of the ghetto with deportations to the Treblinka extermination camp.
The city was liberated by the Red Army on 27 July 1944 and on 20 September 1944 transferred to Poland. After the war, the city became capital of the initial Białystok Voivodeship (1945–1975) of the People's Republic of Poland. After the 1975 administrative reorganization, the city was the capital of the smaller Białystok Voivodeship (1975–1998). Since 1999 it has been the capital of the Podlaskie Voivodeship, Republic of Poland.
Geography.
Białystok is situated in the Białystok Uplands () of the Podlaskie Plain (), part of what is known collectively as the "Green Lungs of Poland". The Biała River, a left tributary of the Supraśl River, passes through the city. The landscape of the Białystok Upland is diverse, with high moraine hills and kame in excess of above sea level. Vast areas of outwash, a glacial plain formed of sediments deposited by meltwater at the terminus of a glacier, are covered by forests.
Forests are an important part of the city character, they currently occupy approximately (17.2% of the administrative area of the city) which places it as the fifth most "wooded" city in Poland; behind Katowice (38%), Bydgoszcz (30%), Toruń (22.9%) and Gdańsk (17.6%).
Part of Knyszyn Forest is preserved within the city limits by two nature reserves—a total area of . The Zwierzyniecki Forest Nature Reserve (), which is contained within the city limits, is a fragment, , of the riparian forest with a dominant assemblage of oak and hornbeam. The Antoniuk Nature Reserve () is a park in the city that preserves the natural state of a forest fragment characteristic of the Białystok Upland, with a dominant mixed forest of hazel and spruce.
The of forests lying in the vicinity of the Dojlidy Ponds are administered by the Central Sports and Recreation Center in Białystok ( – MOSiR). The Dojlidy Ponds recreation area includes a public beach, walking trails, birdwatching and fishing.
Climate
The city has a or Hemiboreal climate (Dfb) according to the Köppen climate classification system, characterized by warm temperatures during summer and long and frosty winters. It is substantially different from most of the other Polish lowlands. The region is one of the coldest in Poland, with the average temperature in January being . The average temperature in a year is about . The number of frost days ranges from 50 to 60, with frost from 110 to 138 days and the duration of snow cover from 90 to 110 days. Mean annual rainfall values oscillate around , and the vegetation period lasts 200 to 210 days.
Districts and Metropolitan Region.
Districts of Białystok
The city of Białystok is divided into 28 administrative units, known in Polish as "osiedla". The first 27 of these were created on October 25, 2004. The 28th, Dojlidy Górne, was created by on October 23, 2006, out of three settlements which had been incorporated into the city: Dojlidy Górne, Kolonia Halickie, and Zagórki.
The center of the city, Osiedle Centrum, surrounds Lipowa Street, the "main street" of the city. Lipowa Street extends from Rynek Kościuszki (the corner of Spółdzielczej Street) to Plac Niepodległości im. Romana Dmowskiego (the corner of Krakowska Street). Over the centuries the name of this exclusive and elegant street has taken on a number of different names; Choroszcz, Nowolipie, Lipowa, Józef Piłsudski, Joseph Stalin, Adolf Hitler and Joseph Stalin, once again, to return, after the end of World War II, to its original name – Lipowa Street.
The city covers of which is agricultural land, is urbanized areas, is surface waters and is wasteland. The composition of the districts vary from residential near the city center, with a combination of multi-story apartment buildings and individual houses on small parcels, to industrial and agricultural at the city edges.
Metropolitan Białystok
Metropolitan Białystok was designated by the Voivodeship of the Regulation No. 52/05 of 16 May 2005 to help develop the region economically. In 2006, the metropolitan area population was 450,254 inhabitants. The municipalities adjacent to Białystok are slowly losing their agricultural character, becoming residential suburban neighborhoods with single-family housing and small businesses.
Demographics.
In June 2009, the population of the city was 294,399, among cities of Poland, Białystok is second in terms of population density, eleventh in population, and thirteenth in area.
Historically, Białystok has been a destination for internal and foreign immigration, especially from Central and Eastern Europe. In addition to the Polish minority, there was a significant Jewish majority in Białystok. According to Russian census of 1897, out of the total population of 66,000, Jews constituted 41,900 (around 63% percent). Białystok's pre-World War II Jewish population constituted about 63 percent of the city's total population of 107,000. World War II changed all of this, in 1939, around 107,000 people lived in Białystok, but in 1946 – only 56,759, and to this day there is much less ethnic diversity than in the previous 300 years of the city's history. Currently the city's population is 97% Polish, 2.5% Belarusian and 0.5% of a number of minorities including Russians, Lipka Tatars, Ukrainians and Romani. Most of the modern-day population growth is based on internal migration within Poland and urbanization of surrounding areas.
Governance.
City government
Białystok, like other major cities in Poland, is a city county (). The Legislative power in the city is vested in the unicameral Białystok City Council (), which has 28 members. Council members are elected directly every four years, one of whom is the mayor, or President of Białystok (). Like most legislative bodies, the City Council divides itself into committees which have the oversight of various functions of the city government. Bills passed by a simple majority are sent to the mayor, who may sign them into law. If the mayor vetoes a bill, the Council has 30 days to override the veto by a two-thirds majority vote. The current President of Białystok, elected for his first term in 2006, is Tadeusz Truskolaski won the elections as the Civic Platform's candidate, however, he has no official connection with the party. In the first round of the elections he received 49% of the votes (42,889 votes altogether). In the later runoff he defeated his rival candidate Marek Kozlowski from Law and Justice (), receiving 67% of the votes cast (53,018 votes).
For the 2010–2011 fiscal year the city received revenue (taxes levied + investments) of 1,409,565,525 zł, expended 1,676,459,102 zł leaving a budget deficit of 266,893,577 zł. The deficit was covered by short-term borrowing of 166,893,577 zł and the issuance of 100 million zł in municipal bonds.
Other levels of governmental representation
It is also the seat of government for the Podlaskie Voivodeship. The city is represented by several members of both houses of the Polish Parliament (Sejm and Senat) from the Białystok constituency. Białystok is represented by the Podlaskie and Warmian-Masurian constituency of the European Parliament.
International relations
There are two consulates in Białystok, Belarus has a Consulate General and Romania has an Honorary Consulate. The City of Białystok is a member of several organizations such as Union of Polish Metropolises (), Euroregion Niemen, Polish Green Lungs Foundation (headquarters) and Eurocities.
Białystok is twinned with
Częstochowa, Poland, Dijon, France:, Eindhoven, Netherlands, Hrodno, Belarus, Jelgava, Latvia, Kaliningrad, Russia Kaunas, Lithuania, Milwaukee County, Wisconsin, USA and Tallinn, Estonia.
Military units.
The 18th Reconnaissance Regiment () of the Polish Land Forces is based in Białystok. The heritage of the unit was the former 18th Territorial Defense Battalion () and prior to that the former 18th Mechanized Brigade. December 31, 2001, as a result of the restructuring of the Armed Forces, 18th Mechanized Brigade () was disbanded and in its place created the 18th Territorial Defense Battalion ().
Historical military units
During December 1993 an order of the Chief of the General Staff of the Polish Armed Forces created the 18th Mechanized Brigade () at the garrison in Białystok. The unit was formed from the 3rd Mechanized Regiment () and was subordinated to the commander of the 1st Warsaw Mechanized Division (). On December 31, 2001, as a result of the restructuring of the Armed Forces, the 18th Mechanized Brigade was disbanded and in its place was created the 18th Territorial Defense Battalion.
The Podlaska Cavalry Brigade () was a military unit of the Polish Army, created on April 1, 1937. Its headquarters was located in Białystok and operated as part of Independent Operational Group Narew. It was formed from the Cavalry Brigade "Białystok", which existed between February 1929, and March 30, 1937. After the Soviet invasion of Poland, remnants of the Brigade fought both Wehrmacht and Red Army troops, capitulating on October 6, 1939.
The Cavalry Brigade "Białystok" (BK "Białystok") of the Polish Army Second Republic was formed in February 1929. April 1, 1937 BK "Białystok" was renamed the Podlaska Cavalry Brigade.
Economy.
In the nineteenth century Białystok was an important center for light industry, which was the reason for the substantial growth of the city's population. The tradition continued with many garment factories established in the twentieth century, such as "Fasty" in the district of Bacieczki. However, after the fall of communism in 1989 many of these factories faced severe problems and subsequently closed down.
The unemployment rate for February 2011 in Białystok was 13.2%.
The 2009 average household had a monthly per capita income of 1018.77 zł and monthly per capita expenses of 823.56 zł
The city has a number of nearby border crossings. The border with Belarus is only away, the nearest border crossings are located in; Bobrowniki (road crossing located about from the city limits), Kuźnica Białostocka (road and rail crossing located from the city limits), Siemianówka (railway – freight traffic), Połowce (road) and Czeremcha (railway). Since the border with Belarus is also the eastern border of the European Union, as well as the Schengen Area the city is a center for trade in mainly from the east.
Industry
The leading industries in the city's economy are: food processing (production of meat products, fruit and vegetable products, the production of spirits, the production of frozen food, grain processing), electrical engineering (production tools and equipment for machine tools, production of electric heaters, manufacture and production mixers household appliances). There is also a developed machine industry (electronics, machinery and metal), plastic processing (production of household appliances), textiles (textiles and upholstery, manufacture of underwear, clothing accessories, footwear and backpacks), Wood (production plywood and furniture) building materials.
Some notable major employers who are based in Białystok include:
Culture and tourism.
Białystok is one of the largest cultural centers in the Podlaskie Voivodeship. The attractions include performing arts groups, art museums, historical museums, walking tours of architectural / cultural aspects and a wide variety of parks and green spaces. Białystok in 2010 was on the short-list, but ultimately lost the competition, to become a finalist for European Capital of Culture in 2016.
Performing arts.
The city has a number of performing arts facilities including:
The Białystok Puppet Theater (), established in 1953, is one of the oldest Polish puppet theaters. The facility is located at Kalinowskiego 1 in Białystok. The repertoire includes performances for both children and puppet adaptations of world literature for adults. Because of the high artistic level of productions, the theater has been recognized as one of the best puppetry arts centers in Poland.
The Aleksandra Węgierki Drama Theatre. Housed in a building designed by Jarosław Girina, built in the years 1933–1938.
The Podlasie Opera and Philharmonic – European Art Centre in Białystok is the largest institute of arts in Northeastern Poland, and the most modern cultural center in this region of Europe. In its amphitheatre every year in the end of June Halfway Festival takes place.
Museums.
There are a number of museums in the city including:
The Historical Museum in Białystok () is part of the Podlaskie Museum. The facility has a rich collection of archival materials and iconography illustrating the history of Białystok and Podlasie, and a number of middle-class cultural relics, especially in the field of craft utility. There are also the Numismatic Cabinet of the collection of 16 000 coins, medals and securities. The museum is in possession of the only collections in the country memorabilia connected with the Tatar settlement on the Polish–Lithuanian–Belarusian region.
The Army Museum in Białystok () was established in September 1968 as a branch of the Podlaskie Museum to house the research and collections of many people connected with military history of north-eastern Poland.
The Ludwik Zamenhof Centre () offers the visitors a permanent exhibition, 'Bialystok of Young Ludwik Zamenhof', and various temporary exhibitions, concerts, film projections, and theatre performances. The Centre has a branch of Lukasz Gornicki’s Podlaska Library dedicated to the Esperanto language.
Parks and green spaces.
Around 32% of the city is occupied by parks, squares and forest preserves which creates a unique and healthy climate. The green spaces include:
Branicki Palace () is a historical edifice and park in Białystok. It was developed on the site of an earlier building in the first half of the eighteenth century by Jan Klemens Branicki, a wealthy Polish–Lithuanian Commonwealth hetman, into a residence suitable for a man whose ambition was to be elected king of Poland. The palace complex with gardens, pavillons, sculptures, outbuildings and other structures and the city with churches, city hall and monastery, all built almost at the same time according to French models was the reason why the city was known in the eighteenth century as Versailles of Podlaskie ().
Planty is a park created between 1930 and 1938, under the auspices of the then Voivode Kościałkowskiego Mariana Zyndrama in the areas adjacent to Branicki Palace. The modernist composition of the park was designed by Stanislav Gralla.
Architecture.
The various historically driven changes have had a very significant influence on the architectural space of the city. Most other Polish cities have suffered similarly, but the processes in Białystok, have had a particularly intense course. Numerous historic works of architecture no longer exist, while many others have been rebuilt to their original configuration. Very few historic buildings of the city have been preserved – the sights are merely an echo of the old historical shape of Białystok.
Main sights include:
Sports.
The city has both professional and amateur sports teams, and a number of venues where they are based. Jagiellonia Białystok is a Polish football club, based in Białystok, in the Ekstraklasa League that plays at the Białystok City Stadium. Jagiellonia Białystok won the Polish Cup in 2010, Super Cup and qualified to play in the third round qualification of the UEFA Europa League. A new 22,500 seat stadium was completed at the beginning of 2015.
Hetman Białystok (formerly known as Gwardia Białystok) is a Polish football club based in Podlaskie Voivodeship. They play in the Division IV or the (4th) League.
Lowlanders Białystok is a football club, based in Białystok, that plays in the Polish American Football League () PLFA I Conference. The Lowlanders were the champions of the PLFA II Conference in 2010 with a perfect season (8 wins in eight meetings). Because of the win they were advanced to the upper conference (PLFA I) in 2011.
Media.
Białystok has a wide variety of media outlets serving the city and surrounding region. There are two locally published daily newspapers, Gazeta Współczesna (36.3% market share) and Kurier Poranny (20.3% market share). In addition two national papers have local bureaus. There are a number of national and locally produced television and radio channels available both over-the-air from the nearby RTCN Białystok (Krynice) Mast, the seventh highest structure in Poland, in addition to transmitter sites within the city. TVP Białystok is one of the locally produced, regional branches of the TVP, Poland's public television broadcaster. There is also a cable television system available within the city. The city has two campus radio stations; "Radiosupeł" at the Medical University of Białystok and "Radio Akadera" at Białystok Technical University.
Religion.
In the early 1900s, Białystok was reputed to have the largest concentration of Jews of all the cities in the world. In 1931, 40,000 Jews lived in the city, nearly half the city's inhabitants.
The city is the seat of the Roman Catholic Archdiocese of Białystok. Pope John Paul II on 5 June 1991, during a visit to Białystok, announced the establishment of the Archdiocese of Białystok which ended the period of the temporary church administration of the portion of the Archdiocese of Vilnius that had, after World War II, remained within the Polish borders. The city is also the seat of the Białystok-Gdansk Diocese of the Autocephalous Polish Orthodox Church. Białystok is the largest concentration of Orthodox believers in Poland. In Białystok, the following Protestant churches exist: a Lutheran parish, two Pentecostal churches, Baptist church, a congregation of the Church of God in Christ and a Seventh Day Adventist church.
Białystok is home to more than two thousand Muslims (mainly Tatars). There is an Islamic Centre a House of Prayer, and various organisations such as Muzułmańska Gmina Wyznaniowa, Stowarzyszenie Studentów Muzułmańskich and Muzułmańskie Stowarzyszenie Kształtowania Kulturalnego. There is magazine issued – "Pamięć i trwanie" ("Memory and persistence").
Transport.
The city is, and has been for centuries, the main hub of transportation for the Podlaskie Voivodeship and the entire northeastern section of Poland. It is a major city on the European Union roadways (Via Baltica) and railways (Rail Baltica) to the Baltic Republics and Finland. It is also a main gateway of trade with Belarus due to its proximity to the border and its current and longstanding relationship with Hrodno, Belarus.
Railways
Passenger trains connect from Suvalki, Hrodno and Lithuania to Warsaw and the rest of the European passenger network. Passenger services are provided by two rail service providers, PKP Intercity that provides intercity passengers trains (express, intercity, eurocity, hotel and TLK) and Przewozy Regionalne that operates only regional passenger trains financed by the voivodeship. Passenger trains are mostly run using electrical multiple units (on electrified lines) or rail buses.
Buses
There is an extensive bus network that covers the entire city by three bus services, but no tram or subway exists. The three bus operators are partially owned by the city (KPKM, KPK and KZK) and each shares approximately a third of the lines and the bus fleet.
Roads and highways
The National Roads () running through Białystok:
Airports
A civil airport, Białystok-Krywlany Airport, lies within the city limits, but does not provide regularly scheduled service. There were plans in 2011 to build a new regional airport, "Białystok-Saniki Airport", that would have provided flights within Europe.
Education.
Higher education in the city can be traced back to the second half of the eighteenth century, when the ownership of the city was inherited by Field Crown Hetman Jan Klemens Branicki. As a patron of the arts and sciences, Branicki encouraged numerous artists and scientists to settle in Białystok to take advantage of Branicki's patronage. In 1745 Branicki established Poland's first military college, the School of Civil and Military Engineering, in the city.
Since the fall of communism many privately funded institutions of higher educations have been founded and their number is still increasing. Currently Białystok is home to one principal public university (University of Białystok) and two other public specialist universities (Białystok Technical University and Medical University of Białystok). Some institutions, such as Musical Academy in Białystok, are branches of their parent institutions in other cities, usually in Warsaw.
Notable residents.
Over the centuries Białystok has produced a number of persons who have provided unique contributions to the fields of science, language, politics, religion, sports, visual arts and performing arts. This environment was created in the mid eighteenth century by the patronage of Jan Klemens Branicki for the arts and sciences. A list of recent notable persons includes, but is not limited to; Ryszard Kaczorowski, last émigré President of the Republic of Poland, L. L. Zamenhof, the creator of Esperanto, Albert Sabin, co-developer of the polio vaccine, Izabella Scorupco, actress, Max Weber, painter. Tomasz Bagiński illustrator, animator and director Oscar nominee in 2002 for The Cathedral

</doc>
<doc id="47783" url="https://en.wikipedia.org/wiki?curid=47783" title="575">
575

__NOTOC__
Year 575 (DLXXV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. The denomination 575 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47785" url="https://en.wikipedia.org/wiki?curid=47785" title="Dr. Mario">
Dr. Mario

The game focuses on the player character Mario, who assumes the role of a doctor and is tasked with eradicating deadly viruses. In this falling block puzzle game, the player's objective is to destroy the viruses populating the on-screen playing field by using colored capsules that are dropped into the field. The player manipulates the capsules as they fall so that they are aligned with viruses of matching colors, which removes them from play. The player progresses through the game by eliminating all the viruses on the screen in each level.
"Dr. Mario" received positive reception, appearing on several "Best Nintendo Games of All Time" lists. The game has been ported, remade, or has had a sequel on every Nintendo home console since the NES as well as most portable consoles, including a re-release in 2004 on the Game Boy Advance as part of the "Classic NES Series". Modified versions of "Dr. Mario" exist as minigames in ', ', and "".
Gameplay.
"Dr. Mario" is a falling block tile-matching video game, in which Mario assumes the role of a doctor, dropping two-colored medical capsules into a medicine bottle representing the playing field. This area is populated by viruses of three colors: red, yellow, and blue. In a manner and style considered similar to "Tetris", the player manipulates each capsule as it falls, moving it left or right and rotating it such that it is positioned alongside the viruses and any existing capsules. When four or more capsule halves or viruses of matching color are aligned in vertical or horizontal configurations, they are removed from play. The main objective is to complete levels, which is accomplished by eliminating all viruses from the playing field. A game over occurs if capsules fill up the playing field in a way that obstructs the bottle's narrow neck.
Players can select the degree of starting difficulty any time a new game is started. The initial level chosen is a value between zero and twenty that determines the number of viruses to clear, and the three game speed options change how fast the capsules fall in the bottle. The player's score is based solely on the elimination of viruses, not on the time taken to complete the level or the number of capsules used. If players complete the highest difficulty level, they can continue playing to accumulate a higher score, but the number of viruses to clear remains the same. Additional points are awarded when multiple viruses are eliminated at once, but no additional points are awarded for initiating chain reactions, in which the elimination of one set of objects triggers the elimination of another set. The game speed is also a factor in how the game calculates scoring; higher speed levels yield more points.
"Dr. Mario" offers a multiplayer gaming mode in which two players compete against each other in separate playing fields. In this mode, the player's goal is to clear their own playing field of viruses before the other player does. Eliminating multiple viruses or initiating chain reactions can cause additional capsules to fall onto the opponent's playing field. A player wins a single game upon eliminating all the viruses or if the other playing field fills up. The first player to win three games wins overall.
Development and releases.
"Dr. Mario" was designed by Gunpei Yokoi, creator of the Game Boy and Game & Watch handheld systems, and produced by Takahiro Harada, who also acted as producer of the "Metroid" series. The game's music, later re-used and arranged in games such as "Super Smash Bros. Melee", was composed by Hirokazu Tanaka, who later became president of Creatures Inc., an affiliate of Nintendo that owns one-third of the copyright regarding the "Pokémon" franchise.
Re-releases.
"Dr. Mario" spawned a number of remakes and ports that were released on various Nintendo consoles. The original version's multiplayer portion was ported to two Nintendo arcade systems in 1990: the Nintendo Vs. System (under the title "Vs. Dr. Mario") and the PlayChoice-10. 
An enhanced remake of "Dr. Mario" was paired with "Tetris" in the Super Nintendo Entertainment System compilation game "Tetris & Dr. Mario", released on 30 December 1994. This version of "Dr. Mario" was re-released in Japan on 30 March 1997, as a downloadable title for the Super Famicom's Satellaview peripheral, under the name . It was re-released again in Japan as a downloadable game for the Super Famicom's and Game Boy's Nintendo Power cartridges.
The NES version was ported twice to the Game Boy Advance: first in 2004 as one of thirty games in the "Classic NES Series" (known as the "Famicom Mini Series" in Japan), then bundled with a version of the "Puzzle League" series in 2005 under the title "Dr. Mario & Puzzle League", this time with updated graphics and new music to choose from. On 20 May 2003, Nintendo released the "Nintendo GameCube Preview Disc" for the Nintendo GameCube, which allows players to download the NES version of "Dr. Mario" to their Game Boy Advance consoles using the Nintendo GameCube – Game Boy Advance link cable.
The original Game Boy version was made available on the Nintendo 3DS Virtual Console in 2011 and 2012. The NES version was released on the Wii U Virtual Console in 2014.
Reception.
While some parents were critical of the premise due to its inclusion of medicine in a children's game, "Dr. Mario" and its re-releases received generally positive reviews. One notably negative review, by ACE, scored the Game Boy version 510/1000. It criticized the game's uninspiring graphics and repetitive play. The review also said the game "reeks of plagiarism", stating it is worse than the original games it is modeled after.
Reviewing the NES version, Allgame praised it, stating that on its release, "when puzzle games were flooding the market, Dr. Mario stands out as one of the best, combining a smooth learning curve, playful graphics and memorable tunes." and "fundamental concepts may be simple, but the addictive gameplay becomes progressively more complex as the speed increases and additional viruses are added."
"GamePro" gave the "Tetris & Dr. Mario" compilation a rave review. They praised the Mixed Match mode and the SNES enhanced graphics and sounds, and concluded "Sharp controls and absorbing action are what make these two classics even better as a pair than they were alone."
"Dr. Mario" was rated the 134th best game released on a Nintendo system in "Nintendo Power"'s Top 200 Games list, by ScrewAttack as the seventh best "Mario" game of all time, and by IGN as the 51st best NES game of all time. IGN also rated the game's soundtrack, composed by Hirokazu Tanaka, as seventh in its list of the top ten greatest 8-Bit soundtracks. GamesRadar ranked it the 13th best NES game ever made. The staff called it "one of the most celebrated of the genre." "Game Informer"s Ben Reeves called it the seventh best Game Boy game.
The Game Boy Advance re-release as part of the "Classic NES series" holds a rating of 66% on Metacritic based on 10 reviews. Most reviews pointed out the game's addictiveness and praise the addition of wireless multiplayer, but some questioned the relevance of the game's re-release as a standalone title. Eurogamer said the game was "still as playable, addictive and maddening as it was back in 1990" but criticized Nintendo for re-releasing classic games as standalone titles in the "Classic NES Series" instead of as a compilation, like Atari's "Atari Anthology" or Midway's "Midway Arcade Treasures". Craig Harris, in his review for IGN, sarcastically expressed unease over the game's use of medicine. He enjoyed the addictive gameplay, but criticized the black-and-white manual which made it difficult to understand the color-based gameplay mechanics. While 1UP.com noted that the game's "color-matching action is more engrossing than Mario Bros.' turtle-punching platform hopping", the reviewer strongly questioned whether this re-release is worth its sale price by itself when a version of "Dr. Mario" was included in another Game Boy Advance game, "".
Legacy.
Following the commercial success of this game, Nintendo released several follow-up titles in the "Dr. Mario" series. "Dr. Mario 64", released in 2001 for the Nintendo 64, features Wario and several "Wario Land 3" characters, and offers numerous game modes, including a story-focused single player mode. The game also supports simultaneous multiplayer for up to four players at once. "Dr. Mario 64" was subsequently released in Japan in the compilation game "Nintendo Puzzle Collection" on the Nintendo GameCube. "Dr. Mario Online Rx", released in 2008 on WiiWare, offers online multiplayer via Nintendo Wi-Fi Connection. "Dr. Mario Express", released in 2009 for the Nintendo DSi, does not support multiplayer gameplay. "Dr. Luigi", released in 2013, features Luigi as a playable character and has all the modes in "Dr. Mario Online Rx", as well as a new mode with L-shaped capsules. The latest installment, "", was released 2015 and introduced power-ups to the series.
The character of Dr. Mario appears as a secret fighter in the 2001 fighting game "Super Smash Bros. Melee", where he attacks by throwing capsules. There are two ways to unlock Dr. Mario as a playable character, either by completing Classic, Adventure or All-Star mode with Mario (using no continues) or by completing 100 melee battles. The game's sequel, "Super Smash Bros. Brawl", did not feature Dr. Mario as a playable character, but it includes the "Melee" remix of "Dr. Mario"'s "Fever" background music theme and a version of the "Chill" theme arranged by Masaaki Iwasaki, who had previously composed for Magical Drop as part of the Data East Sound Team. "Dr. Mario" characters also appear in the game as collectible stickers. Dr. Mario would later return as an unlockable playable character in "Super Smash Bros. for Nintendo 3DS and Wii U".
A version of the game called "Dr. Wario", which replaces Mario's character with Wario, is included as an unlockable minigame in the Game Boy Advance game '. A simplified version of "Dr. Mario" also appears in the Nintendo DS game ' as a minigame called "Virus Buster," which is played by using the system's touch screen to drag the capsules around the playing field.
The viruses appear as enemies in ' and '. In that game, they change colors every time they are attacked, and they are all defeated when they are all the same color, in a similar fashion to how they are defeated by the same color of the capsules in Dr. Mario.

</doc>
<doc id="47786" url="https://en.wikipedia.org/wiki?curid=47786" title="Environmental finance">
Environmental finance

Environmental finance is the use of various financial instruments (usually land trusts and emissions trading) to protect the environment. The field is part of both environmental economics and the conservation movement.
The field of Environmental Finance was first defined by Richard L. Sandor, American economist and entrepreneur, when he taught the first ever Environmental Finance course at Columbia University in the fall of 1992. 
Dr. Gretchen Daily, of Stanford University has written a book, "The New Economy of Nature" that addresses the issue of financing ecosystem services.
Dr. Jürg P. Blum, defined the term environmental finance (Dissertation: Corporate Environmental Responsibility and Corporate Economic Performance... 1994 at USIU) as a fairly new field, "concerned mainly with finance and investment regarding the ecological environment. The term environment, although frequently used in areas, such as strategic management (Ansoff, 1968), has been popularized throughout literature synonymously with the term ecological environment."

</doc>
<doc id="47788" url="https://en.wikipedia.org/wiki?curid=47788" title="214">
214

__NOTOC__
Year 214 (CCXIV) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Messalla and Suetrius (or, less frequently, year 967 "Ab urbe condita"). The denomination 214 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="47789" url="https://en.wikipedia.org/wiki?curid=47789" title="Quality of life">
Quality of life

Quality of life (QOL) is the general well-being of individuals and societies. QOL has a wide range of contexts, including the fields of international development, healthcare, politics and employment. It is important not to mix up the concept of QOL with a more recent growing area of health related QOL (HRQOL). When we look at HRQOL we in effect look at QOL and its relationship with health.
Quality of life should not be confused with the concept of standard of living, which is based primarily on income.
Overview.
Standard indicators of the quality of life include not only wealth and employment but also the built environment, physical and mental health, education, recreation and leisure time, and social belonging.
According to ecological economist Robert Costanza:
One approach, called engaged theory, outlined in the journal of "Applied Research in the Quality of Life", posits four domains in assessing quality of life: ecology, economics, politics and culture. In the domain of culture, for example, it includes the following subdomains of quality of life:
Also frequently related are concepts such as freedom, human rights, and happiness. However, since happiness is subjective and difficult to measure, other measures are generally given priority. It has also been shown that happiness, as much as it can be measured, does not necessarily increase correspondingly with the comfort that results from increasing income. As a result, standard of living should not be taken to be a measure of happiness. Also sometimes considered related is the concept of human security, though the latter may be considered at a more basic level and for all people.
Quantitative measurement.
Unlike "per capita" GDP or standard of living, both of which can be measured in financial terms, it is harder to make objective or long-term measurements of the quality of life experienced by nations or other groups of people. Researchers have begun in recent times to distinguish two aspects of personal well-being: "Emotional well-being", in which respondents are asked about the quality of their everyday emotional experiences—the frequency and intensity of their experiences of, for example, joy, stress, sadness, anger, and affection— and "life evaluation", in which respondents are asked to think about their life in general and evaluate it against a scale. Such and other systems and scales of measurement have been in use for some time. Research has attempted to examine the relationship between quality of life and productivity.
Human Development Index.
Perhaps the most commonly used international measure of development is the Human Development Index (HDI), which combines measures of life expectancy, education, and standard of living, in an attempt to quantify the options available to individuals within a given society. The HDI is used by the United Nations Development Programme in their Human Development Report.
World Happiness Report.
Also developed by the United Nations and published recently along with the HDI, this report combines both objective and subjective measures to rank countries by happiness, which is deemed as the ultimate outcome of a high quality of life. It uses surveys from Gallup, real GDP per capita, healthy life expectancy, having someone to count on, perceived freedom to make life choices, freedom from corruption, and generosity to derive the final score.
Other measures.
The Physical Quality of Life Index (PQLI) is a measure developed by sociologist Morris David Morris in the 1970s, based on basic literacy, infant mortality, and life expectancy. Although not as complex as other measures, and now essentially replaced by the Human Development Index, the PQLI is notable for Morris's attempt to show a "less fatalistic pessimistic picture" by focusing on three areas where global quality of life was generally improving at the time, and ignoring gross national product and other possible indicators that were not improving.
The Happy Planet Index, introduced in 2006, is unique among quality of life measures in that, in addition to standard determinants of well-being, it uses each country's ecological footprint as an indicator. As a result, European and North American nations do not dominate this measure. The 2012 list is instead topped by Costa Rica, Vietnam and Colombia.
Gallup researchers trying to find the world's happiest countries found Denmark to be at the top of the list. uSwitch publishes an annual quality of life index for European countries. France has topped the list for the last three years.
A 2010 study by two Princeton University professors looked at 1,000 randomly selected U.S. residents over an extended period. It concludes that their "life evaluations" - that is, their considered evaluations of their life against a stated scale of one to ten - rise steadily with income. On the other hand, their reported quality of "emotional daily experiences" (their reported experiences of joy, affection, stress, sadness, or anger) levels off after a certain income level (approximately $75,000 per year); income above $75,000 does not lead to more experiences of happiness nor to further relief of unhappiness or stress. Below this income level, respondents reported decreasing happiness and increasing sadness and stress, implying the pain of life’s misfortunes, including disease, divorce, and being alone, is exacerbated by poverty.
Gross national happiness and other subjective measures of happiness are being used by the governments of Bhutan and the United Kingdom. The World Happiness report, issued by Columbia University is a meta-analysis of happiness globally and provides an overview of countries and grassroots activists using GNH. The OECD issued a guide for the use of subjective well-being metrics in 2013. In the U.S., cities and communities are using a GNH metric at a grassroots level.
The Social Progress Index measures the extent to which countries provide for the social and environmental needs of their citizens. Fifty-two indicators in the areas of basic human needs, foundations of wellbeing, and opportunity show the relative performance of nations. The index uses outcome measures when there is sufficient data available or the closest possible proxies.
Livability.
The term "quality of life" is also used by politicians and economists to measure the livability of a given city or nation. Two widely known measures of livability are the Economist Intelligence Unit's Where-to-be-born Index and Mercer's Quality of Living Reports. These two measures calculate the livability of countries and cities around the world, respectively, through a combination of subjective life-satisfaction surveys and objective determinants of quality of life such as divorce rates, safety, and infrastructure. Such measures relate more broadly to the population of a city, state, or country, not to individual quality of life. Livability has a long history and tradition in urban design, and neighborhoods design standards such as LEED-ND are often used in an attempt to influence livability.
Crimes.
Some crimes against property (e.g., graffiti and vandalism) and some "victimless crimes" have been referred to as "quality-of-life crimes." American sociologist James Q. Wilson encapsulated this argument as the Broken Window Theory, which asserts that relatively minor problems left unattended (such as litter, graffiti, or public urination by homeless individuals) send a subliminal message that disorder in general is being tolerated, and as a result, more serious crimes will end up being committed (the analogy being that a broken window left broken shows an image of general dilapidation).
Wilson's theories have been used to justify the implementation of zero tolerance policies by many prominent American mayors, most notably Oscar Goodman in Las Vegas, Richard Riordan in Los Angeles, Rudolph Giuliani in New York City and Gavin Newsom in San Francisco. Such policies refuse to tolerate even minor crimes; proponents argue that this will improve the quality of life of local residents. However, critics of zero tolerance policies believe that such policies neglect investigation on a case-by-case basis and may lead to unreasonably harsh penalties for crimes.
Popsicle index.
The Popsicle Index is a quality of life measurement coined by Catherine Austin Fitts as the percentage of people - in a community who believe that a child in their community can safely leave their home, walk to the nearest possible location to buy a popsicle, and walk back home.
In healthcare.
Within the field of healthcare, quality of life is often regarded in terms of how a certain ailment affects a patient on an individual level. This may be a debilitating weakness that is not life-threatening; life-threatening illness that is not terminal; terminal illness; the predictable, natural decline in the health of an elder; an unforeseen mental/physical decline of a loved one; or chronic, end-stage disease processes. Researchers at the University of Toronto's Quality of Life Research Unit define quality of life as "The degree to which a person enjoys the important possibilities of his or her life" (UofT). Their Quality of Life Model is based on the categories "being", "belonging", and "becoming"; respectively who one is, how one is not connected to one's environment, and whether one achieves one's personal goals, hopes, and aspirations.
In international development.
Quality of life is an important concept in the field of international development, since it allows development to be analyzed on a measure broader than standard of living. Within development theory, however, there are varying ideas concerning what constitutes desirable change for a particular society, and the different ways that quality of life is defined by institutions therefore shapes how these organizations work for its improvement as a whole.
Organisations such as the World Bank, for example, declare a goal of "working for a world free of poverty", with poverty defined as a lack of basic human needs, such as food, water, shelter, freedom, access to education, healthcare, or employment. In other words, poverty is defined as a low quality of life. Using this definition, the World Bank works towards improving quality of life through neoliberal means, with the stated goal of lowering poverty and helping people afford a better quality of life.
Other organizations, however, may also work towards improved global quality of life using a slightly different definition and substantially different methods. Many NGOs do not focus at all on reducing poverty on a national or international scale, but rather attempt to improve quality of life for individuals or communities. One example would be sponsorship programs that provide material aid for specific individuals. Although many organizations of this type may still talk about fighting poverty, the methods are significantly different.
Improving quality of life involves action not only by NGOs, but also by governments. Global health has the potential to achieve greater political presence if governments were to incorporate aspects of human security into foreign policy. Stressing individuals’ basic rights to health, food, shelter, and freedom addresses prominent inter-sectoral problems negatively impacting today’s society and may lead to greater action and resources. Integration of global health concerns into foreign policy may be hampered by approaches that are shaped by the overarching roles of defense and diplomacy.

</doc>
<doc id="47791" url="https://en.wikipedia.org/wiki?curid=47791" title="Mars program">
Mars program

The Mars program was a series of unmanned spacecraft launched by the Soviet Union between 1960 and 1973. The spacecraft were intended to explore Mars, and included flyby probes, landers and orbiters.
Early Mars spacecraft were small, and launched by Molniya rockets. Starting with two failures in 1969, the heavier Proton-K rocket was used to launch larger 5 tonne spacecraft, consisting of an orbiter and a lander to Mars. The orbiter bus design was likely somewhat rushed into service and immature, considering that it performed very reliably in the Venera variant after 1975. This reliability problem was common to much Soviet space hardware from the late 1960s and early 1970s and was largely corrected with a deliberate policy, implemented in the mid-1970s, of consolidating (or "debugging") existing designs rather than introducing new ones. The names of the "Mars" missions do not need to be translated, as the word "Mars" is spelled and pronounced approximately the same way in English and Russian.
In addition to the Mars program, the Soviet Union also sent a probe to Mars as part of the Zond program; Zond 2, however it failed en route. Two more spacecraft were sent during the Phobos program. In 1996, Russia launched Mars 96, its first interplanetary mission since the dissolution of the Soviet Union, however it failed to depart Earth orbit.
Spacecraft.
Mars 1M.
The first Soviet attempts to send a probe to Mars were the two Mars 1M spacecraft, which each had a mass of about 650 kg. Both were launched in 1960 and failed to achieve orbit. The spacecraft were dubbed "Marsnik" by the Western media.
Mars 2MV.
Mars 1 was launched in 1962 but failed en route to Mars. Two other Soviet launches at around the same time, Mars 2MV-4 No.1 and Mars 2MV-3 No.1 were spacecraft, however both failed to leave Earth orbit due to problems with the upper stages of their carrier rockets.
Mars 2M.
Mars 2M No.521 and Mars 2M No.522, known in the West as Mars 1969A and B, were heavier spacecraft with masses of . They were launched by Proton-K rockets, and consisted of orbiters. Both were destroyed during launch.
Mars 4M.
The Mars 4M spacecraft; Mars 2 and Mars 3 missions consisted of identical spacecraft, each with an orbiter and an attached lander, which became the first spacecraft to reach the surface of Mars.
The orbiters' primary scientific objectives were to image the Martian surface and clouds, determine the temperature on Mars, study the topography, composition and physical properties of the surface, measure properties of the atmosphere, monitor the solar wind and the interplanetary and Martian magnetic fields, and act as communications relays to send signals from the landers to Earth.
Mars 2.
Mars 2 released the descent module 4.5 hours before reaching Mars on November 27, 1971. The descent module entered the Martian atmosphere at roughly 6.0 km/s at a steeper angle than planned. The descent system malfunctioned and the lander crashed at , delivering the Soviet Union coat of arms to the surface. Meanwhile, the orbiter engine performed a burn to put the spacecraft into a 1380 x 24,940 km, 18‑hour orbit about Mars with an inclination of 48.9 degrees. Scientific instruments were generally turned on for about 30 minutes near periapsis.
Mars 3.
Mars 3's descent module was released at 09:14 UT on December 2, 1971, 4 hours 35 minutes before reaching Mars. The descent module entered the Martian atmosphere at roughly 5.7 km/s. Through aerodynamic braking, parachutes, and retrorockets, the lander achieved a soft landing at and began operations. However, after 20 seconds the instruments stopped working for unknown reasons, perhaps as a result of the massive surface dust storms raging at the time of landing. Mars 3 lander still managed to transmit a portion of the first picture of Martian surface. Meanwhile, the orbiter had suffered from a partial loss of fuel and did not have enough to put itself into a planned 25 hour orbit. The engine instead performed a truncated burn to put the spacecraft into a long 12-day-19-hour period orbit about Mars with an inclination thought to be similar to that of Mars 2 (48.9 degrees).
Both landers had a small Mars 'rover' on board, which would move across the surface on skis while connected to the lander with a 15-meter umbilical. Two small metal rods were used for autonomous obstacle avoidance, as radio signals from Earth would take too long to drive the rovers using remote control. Each rover had both a densitometer and a dynamic penetrometer, to test the density and the bearing strength of the soil. Because of the demise of the landers, neither rover saw action.
The Mars 2 and 3 orbiters sent back a large volume of data covering the period from December 1971 to March 1972, although transmissions continued through August. It was announced that Mars 2 and 3 had completed their missions by August 22, 1972, after 362 orbits completed by Mars 2 and 20 orbits by Mars 3. The probes sent back a total of 60 pictures. The images and data enabled creation of surface relief maps, and gave information on the Martian gravity and magnetic fields.
Mars 3MS.
Kosmos 419 was launched on May 5, 1971. It consisted of only an orbiter, and was intended to become the first spacecraft to orbit Mars (areocentric orbit), thereby beating the American Mariner 8 and Mariner 9 spacecraft.
The Mars 4 and Mars 5 orbiters, launched in 1973, were designed to orbit Mars and return information on the composition, structure, and properties of the Martian atmosphere and surface. The spacecraft were also designed to act as communications links to the Mars 6 and 7 landers. Like earlier heavy spacecraft, they were launched by Proton-K rockets.
Mars 4.
The Mars 4 orbiter reached Mars on February 10, 1974. Due to a flaw in the transistor 2Т-312 which resulted in its degradation during the voyage to Mars, the retro-rockets designed to slow the craft into Mars orbit did not fire, and Mars 4 flew by the planet at a range of 2200 km. It returned one swath of pictures and some radio occultation data which constituted the first detection of the nightside ionosphere on Mars. It continued to return interplanetary data from heliocentric orbit after the flyby.
Mars 5.
Mars 5 reached Mars on February 12, 1974 at 15:45 UT and was inserted into an elliptical 1755 by 32,555 km, 24 h 53 min orbit with an inclination of 35.3 degrees. Nearly synchronized with the rotation of the planet, its two phototelevision cameras could be commanded to take 12 pictures during each close approach. The Vega camera used a wide area 52mm lens with color filters, the Zulfar camera used a telescopic 350mm lens and long-pass orange filter. Images were transmitted in a rapid 220-line mode, and then selected pictures were retransmitted at 880 or 1760 line resolution. Mars 5 collected data for 22 orbits until a loss of pressurization in the transmitter housing ended the mission. About 60 images were returned over a nine-day period showing swaths of the area south of Valles Marineris, from to .
Mars 3MP.
In 1973 the speed required to place a spacecraft in an interplanetary trajectory had to be increased. Thus the Proton could not deliver spacecraft with an orbiter and an attached lander to the necessary trajectory to reach Mars, as had been possible in 1971. To resolve this problem, four spacecrafts were launched. The Mars 4 and 5 orbiters, which had been launched separately, were used to relay communications, and to complete mission objectives which would have been completed by landers. Two landers were launched with orbiter type buses (Mars 6 and 7), but without fuel to enter orbit of the Mars satellite.
Mars 6.
Mars 6 successfully lifted off on August 5, 1973, into an intermediate Earth orbit on a Proton-K/D booster and then launched into a Mars transfer trajectory. Total fueled launch mass of the lander and bus was 3260 kg. It reached Mars on March 12, 1974. The descent module separated from the bus at a distance of 48,000 km from Mars. The bus continued on into a heliocentric orbit after passing within 1600 km of Mars. The descent module entered the atmosphere at 09:05:53 UT at a speed of 5.6 km/s. The parachute opened at 09:08:32 UT after the module had slowed its speed to 600 m/s by aerobraking. During this time the craft was collecting data and transmitting it directly to the bus for immediate relay to Earth. Contact with the descent module was lost at 09:11:05 UT in "direct proximity to the surface", probably either when the retrorockets fired or when it hit the surface at an estimated 61 m/s. Mars 6 landed at in the Margaritifer Terra region of Mars. The landed mass was 635 kg. The descent module transmitted 224 seconds of data before transmissions ceased, the first data returned from the atmosphere of Mars. Much of the data was unreadable due to a flaw in a transistor which led to degradation of the system during its journey to Mars.
Mars 7.
Mars 7 successfully lifted off on August 9, 1973, into an intermediate Earth orbit on a Proton-K/D booster and then launched into a Mars transfer trajectory. Total fueled launch mass of the lander and bus was 3260 kg. It reached Mars on March 9, 1974. Due to a problem in the operation of one of the on-board systems (attitude control or retro-rockets) the landing probe separated prematurely (4 hours before encounter) and missed the planet by 1300 km. The early separation was probably due to a computer error which resulted from degradation of the systems during the trip to Mars. The intended landing site was . The lander and bus continued on into heliocentric orbits.
Mars 4NM and 5NM.
The Mars 4NM and Mars 5NM projects would have seen heavier spacecraft launched by N1 rockets. They would have deployed heavy Marsokhod rovers onto the surface, and conducted sample return missions. The N1 failed on all four of its test flights, and was never used to launch any Mars spacecraft.
Mars 5M.
Mars 5M (Mars 79) was a sample return mission developed in 1977 to be double launched in 1979 by Proton launchers and then docked in Earth orbit for a joint flight of orbital and return modules to Mars. The project was canceled due to the low reliability of the Igla automatic docking system.

</doc>
<doc id="47792" url="https://en.wikipedia.org/wiki?curid=47792" title="Martian">
Martian

A Martian is a native inhabitant of the planet Mars. Although the search for evidence of life on Mars continues, many science fiction writers have imagined what extraterrestrial life on Mars might be like. Some writers also use the word "Martian" to describe a human colonist on Mars.
Martians in fiction.
The earliest known instances of the word "Martian", used as a noun instead of an adjective, were printed in late 1877. They appeared nearly simultaneously in England and the United States, in magazine articles detailing Asaph Hall's discovery of the moons of Mars in August of that year.
The next event to inspire the use of the noun "Martian" in print was the International Exposition of Electricity, which was hosted in Paris in the year 1881. During the four months of the exhibition, many people visited to witness such technological marvels as the incandescent light bulb and the telephone. One visitor came away wondering what kind of world such innovations might engender in the next 200 years. Writing anonymously, s/he assembled some speculations in an essay titled "The Year of Grace 2081", which enjoyed wide circulation. The Martians enter the story late in the narrative. During a rest from international conflict on Earth, humans begin telecommunicating with Martians. "After a brief period passed in the exchange of polite messages", says the essayist, humans will decide to war with the Martians on some pretence of honor. The war that results is cataclysmic:
W. S. Lach-Szyrma's novel "Aleriel, or A Voyage to Other Worlds" (1883) was previously reputed to be the first published work to apply the word "Martian" as a noun. The usage is incidental; it occurs when Aleriel, the novel's protagonist, lands on Mars in a spacecraft called an "ether-car" (an allusion to aether, which was once postulated as a gaseous medium in outer space). Aleriel buries the car in snow "so that it might not be disturbed by any Martian who might come across it."
Fifteen years after Aleriel, H. G. Wells' landmark novel "The War of the Worlds" (1898) was published by William Heinemann, Ltd., then a relatively new publishing house. The novel was revised numerous times, and has since been translated into many languages. In the story, the Martians are a technologically advanced race of octopus-like extraterrestrials who invade Earth because Mars is becoming too cold to sustain them. The Martians' undoing is a fatal vulnerability to Earth bacteria.
In his book "Mars and Its Canals" (1906), astronomer and businessman Percival Lowell conjectured that an extinct Martian race had once constructed a vast network of aqueducts to channel water to their settlements from Mars' polar ice caps, Planum Australe and Planum Boreum. Lowell did not invent this Martian canal hypothesis, but he supported it. The belief that Mars had canals was based on observations Giovanni Schiaparelli made through his reflecting telescope. Although the telescope's image was fuzzy, Schiaparelli thought he saw long, straight lines on the Martian surface; some astronomers came to believe that these lines were structures built by Martians. This idea inspired Lowell, who returned to the subject in "Mars As the Abode of Life" (1910), in which he wrote a fanciful description of what this Martian society may have been like. Although his description was based on almost no evidence, Lowell's words evoked vivid pictures in his readers' imaginations.
One of the people Lowell inspired was Edgar Rice Burroughs, who began writing his own story about Mars in the summer of 1911. The story is a planetary romance in which an American Civil War veteran named John Carter is transported to Mars when he walks inside a cave on Earth. He finds that Mars is populated by two species of warring humanoids, and he becomes embroiled in their conflict. In February 1912, an American pulp magazine called "The All-Story" published Burroughs' story as the first installment of a serial novel, which the editor titled "Under the Moons of Mars" (retitled "A Princess of Mars" in subsequent editions). The book was the first in Burroughs' Barsoom series.
Although the noun "Martian" can describe any organism from Mars, these and later works typically imagine Martians as a humanoid monoculture. "Martian", in this sense, is more like the word "human" than the word "Earthling". (Few writers describe a biodiverse Mars.) In science fiction, Martians are stereotypically imagined in one or more of the following ways: as alien invaders; as humanoids with a civilization that resembles one on Earth; as anthropomorphic animals; as beings with superhuman abilities; as humanoids with a lower intelligence than humans; as human colonists who adopt a Martian identity; and/or as an extinct race who possessed high intelligence.
Martians as invaders of Earth.
H. G. Wells' novel "The War of the Worlds" (1898) and its various adaptations have been an extraordinary influence on science fiction writers for more than 100 years. Wells' Martians are a technologically advanced species with an ancient civilization. They somewhat resemble cephalopods, with large, bulky brown bodies and sixteen snake-like tentacles, in two groups of eight, around a quivering V-shaped mouth; they move around in 100 feet tall tripod fighting-machines they assemble upon landing, killing everything in their path. They invade Earth because Mars is dying, and they need a warmer planet to live on. They attack cities in southern England, including London, with a deadly heat-ray they fire from a camera-like device on an articulated arm attached to their tripods; they also employ chemical warfare, using a poisonous "black smoke" launched from gun-like tubes. Mankind is saved by Earth bacteria, which kill the Martians within three weeks of their landing on Earth.
In "Last and First Men" (1930) Olaf Stapledon revisited Wells' theme of Martian invasion. "Last and First Men" summarizes tens of thousands of years of invasions and war between Martians and humans. Eventually, humans destroy the Martians' empire.
William Cameron Menzies' film "Invaders from Mars" (1953) fuses the tentacles of Wells' Martians with the idea of little green men to conceive a Martian Mastermind who enslaves tall, green, humanoid mutants. Tobe Hooper's remake of the film was released in 1986,
In his 1955 comic novel "Martians, Go Home", Fredric Brown spoofs the Wellsian invasion, and reinterprets the Martian invader as a rude house guest with ulterior motives. Brown, too, employs the "little green men" trope to describe his annoying Martians.
Little green men recur in the 2009 video game "Stalin vs. Martians", a spoof of earlier strategy video games. As the president of the Soviet Union, the player defends Earth from Martian invasion. This time the caricature of the Martians appears to be influenced by H. R. Giger.
In "Spaced Invaders" (1990), a group of Martians invade a town in the Midwestern United States during a re-broadcast of the Orson Wells' 1938 radio dramatization of "The War of the Worlds".
In 1948, Warner Bros. introduced a new villain to their animated films: Marvin the Martian, a short, slender figure with comically oversized eyes, hands, and feet, but with no visible mouth. His big, spherical head is either completely black or overshadowed by his crested helmet. His clothing is patterned on that of Mars, the god of war in Roman mythology. In Marvin's film debut, "Haredevil Hare" (1948), he attempts to blow up Earth because it "obscures view of Venus", but is thwarted by Bugs Bunny.
Another film that imagines is Martians as ineffectual invaders is "Mars Attacks!" (1996), a black comedy based on a Topps trading card series. Here, the Martian invaders are loud, irritating, and dim-witted, despite having over-sized heads with extremely large, protruding brains. The film was written by Jonathan Gems, and directed by Tim Burton.
In the "Superman" story "Black Magic on Mars" (1950), the conception of Martians is more grave: The Martian invaders Superman faces are led by a dictator called Martler, who is an admirer of Adolf Hitler.
In "Will the Real Martian Please Stand Up?", a 1961 episode of "The Twilight Zone", Martians attempt to colonize Earth, but are thwarted by Venusians. Here, the Martians are disguised as humans, but towards the end of the episode we see that they have three arms.
In "Mars Needs Moms" (2007), a picture book by Berkeley Breathed, Martians are squat, humanoid beings with antennae and skin color that varies by individual. When they travel to Earth, they wear transparent helmets and a bulbous, ribbed outer garment. In the story, a five-year-old boy learns to appreciate his mother after three Martians kidnap her while he sleeps. Writer-director Simon Wells and his wife Wendy adapted the picture book into the film "Mars Needs Moms" (2011).
Martians as civilized humanoids.
In April 1911, about a year before "The All-Story" published the first installment of Burroughs' "Under the Moons of Mars", "Modern Electrics" began publishing Hugo Gernsback's own romance, "Ralph 124C 41+", which takes place on Earth. Gernsback's Martians live among the humans on Earth; they are taller and physically stronger than humans, with green skin and large eyes. The serial wasn't republished as a book until 1925.
1923 saw the publication of "Aelita, or The Decline of Mars", a novel that is equally science fiction and political fiction. Its author, Soviet Russian writer Aleksey Nikolayevich Tolstoy, tells a story of a Soviet engineer who builds a rocket and invites an acquaintance to accompany him in it to Mars. There they find a humanoid race of Martians who are the offspring of both an elder Martian species and of humans from Atlantis. The Martians live in a class society; the workers rise up against the ruling class, but the revolution fails. All the while, Mars is entering a phase of climate change that threatens disaster for the population.
"Red Planet" (1932), a play cowritten by John L. Balderston and John Hoare, also deals with radical environmental change on Mars, except in this case it occurs through terraforming. Balderston was one of the few playwrights of the 20th century to adopt Mars or Martians as a subject for the stage. He was, however, also a screenwriter who specialized in fantasy film and horror film. Many years later, United Artists bought a screen adaptation that Balderston and fellow screenwriter Anthony Veiller wrote. Harry Horner directed the film, called "Red Planet Mars", and it was released to cinemas in 1952. In the film, a scientist communicates with Martians by radio, and they tell him that Mars is a utopia. When the news circulates, it causes widespread unrest among the people of Western nations. The US government tries to silence further messages, then later announces that the Martians have informed them that they must all worship God in order to save themselves. After millions of people revolt against their governments, it seems the Martian communiques may have been a hoax.
In four stories by Eric Frank Russell published in the early 1940s and collected in the classic "Men, Martians and Machines" (1955), a crew of humans and humanoid Martians are shipmates and compatriots on an interstellar voyage. During their travels, they encounter hostile aliens.
Ray Bradbury's novel "The Martian Chronicles" (1950) depicts Martians as a refined and artistic race of golden-skinned beings who closely resemble humans. The Martians are almost completely wiped out by the diseases brought to Mars by human invaders. At the end of the book, the human inhabitants of Mars realize that they are the new Martians. The novel's themes and its portrayal of Martians resemble Bradbury's 1949 short story "Dark They Were, and Golden-Eyed".
Brothers Jim Thomas and John Thomas teamed with Graham Yost to write "Mission to Mars" (2000), a film that depicts Martians as tall, feminine, peaceful humanoids who left Mars to escape the havoc caused by a massive meteorite impact.
Edmond Hamilton's "A Conquest of Two Worlds" describes Martians as humanoid creatures with stilt-like limbs and large, bulging chests and heads. They live in tribal groups centered on oases and occasionally fight among themselves. After an accidental confrontation sparks war, they are all killed or enslaved by the invading human population.
Martians as anthropomorphic animals.
C. S. Lewis wrote, in "Out of the Silent Planet" (1938), about three humans who visit Mars and meet three different kinds of intelligent native creatures: The hrossa, the sorns/séroni, and the pfifltriggi. They are dying out, but are resigned to their fate. The books also describe a prey animal called hnakra, which is hunted. The planet is ruled by the Oyarsa, who are also called "eldil".
As a writer for "Doctor Who", Brian Hayles created a Martian species of reptilian humanoids called Ice Warriors, who move stiffly and speak in a husky whisper. Most of the Ice Warriors that the Doctor encounters are brutish and belligerent. As Mars' climate becomes less favorable to sustaining them, the Ice Warriors seek a new planet.
These reptilians debuted in "The Ice Warriors" (1967), a "Doctor Who" television serial about an impending ice age in Earth's future. As British scientists try to slow or avert a glacier encroaching on Great Britain, they find an Ice Warrior near their base, frozen in the glacier, and apparently in suspended animation. No one knows the being's identity, but they understand that it is probably from another planet. When the Ice Warrior revives, he attacks the first person he sees and kidnaps a young woman. Other stories set in the future show the Martians eventually become more peaceful and are members of the Galactic Federation, though some want to return to their warlike ways.
Martians and human colonists.
Many of Robert A. Heinlein's Martian characters are humans born and raised on Mars. In "Red Planet" (1949), boys attend a boarding school in a human colony on Mars. A population of native Martians tolerates them until the colony administrator threatens a Martian child. The Martians demand that the humans leave Mars, but a human doctor convinces them to reconsider.
In Heinlein's 1956 novel "Double Star", humans have colonized the solar system, and a politician on Mars faces the civil rights issue of granting a native Martian species (who are second-class citizens) the right to vote.
In Philip K. Dick's novel "Martian Time-Slip" (1964), a human colony on Mars is trying to cope with arduous environmental conditions. They treat an aboriginal race, whom they call "Bleekmen", with casual racism. In "The Three Stigmata of Palmer Eldritch" (1965), Mars has no indigenous life. To cope with the sterile habitat, the human colonists abuse drugs such as "Can-D" and "Chew-Z".
Dick previously published a shorter version of the story in 1963, called "All We Marsmen". Like Heinlein's "Double Star", "All We Marsmen" was conceived at a time in US history when many marginalized people were fighting especially vehemently for more civil rights. US President Lyndon B. Johnson signed the Civil Rights Act of 1964 into law on 2 July 1964.
"Total Recall" (1990) is a science fiction action film about an apparently unsophisticated construction worker who turns out to be a freedom fighter from Mars who has been relocated to Earth. He later learns of an alien artifact that proves Mars harbored life before the human colonization.
The character designers for "Futurama", a comical American animated series, imagine Mars after human colonization as being like the American frontier; native Martians inhabit zones analogous to Indian reservations. One of the series principal characters, Amy Wong, is a scientist of Chinese descent who was born on Mars. Her parents have amassed an enormous fortune and enterprise there.
Rebecca Bloomer's novel "Unearthed" (2011), the first in a series, describes a futuristic human colony on Mars amidst the populations of native Martians.
1. See also African-American Civil Rights Movement (1955–68)
Martians as an extinct race.
For his Known Space series of novels, Larry Niven conceived humanoid Martians with a primitive material culture who inhabit an environment of red dust and nitric acid, and for whom water is lethal. In the 1973 novel "Protector", a man named Jack Brennan allies himself with a ruthless, xenophobic humanoid species called the Pak. To preclude the possibility future competition for Pak offspring, Brennan engineers a Martian genocide by sending an ice-covered asteroid to collide with Mars.
In Dennis Feltham Jones' 1977 novel "Colossus and the Crab", Martian life predated life on Earth, but faced a process of devolution as conditions on the planet worsened.
"Quatermass and the Pit" (1958–59) is a British television serial in which a crashed spacecraft is discovered in London. The wreck evidences that the human population of Earth resulted from the experiments of a Martian civilisation, now long dead. A film remake was released in 1967.
"Ghosts of Mars" (2001) human invaders war with Martians in an attempt to conquer Mars.
In the "Invader Zim" episode "Battle of the Planets" (2001), Zim discovers that a Martian race died off after converting Mars into a giant spacecraft.
In "Doom 3" (2004), the entire Martian race sacrificed itself many millennia ago in order to prevent a demonic invasion of our universe. By the year 2145, humanity has colonized Mars and begun excavating the ruins of their civilization, recovering several important artifacts. One of these artifacts - known as the Soul Cube - is the player's most valuable tool in combating a second demonic invasion, as it is the only weapon capable of killing the Cyberdemon.
Martians as superbeings.
Isaac Asimov's "David Starr, Space Ranger", the first novel in the Lucky Starr series, features a race of Martians who have retreated into vast artificial underground caverns half a million years ago. These Martians are incorporeal, telepathic beings, peaceful yet curious about humanity. They have access to advanced technologies completely incomprehensible to human beings, like personal energy shield generators the size of a fabric mask.
In Robert Heinlein's "Stranger in a Strange Land" (1961), a man raised by native Martians emigrates to Earth, where he must reacclimate. In the novel he functions as a Christ figure. Soon he demonstrates psychic powers, superhuman intelligence, and an ability to manipulate higher dimensions. He founds a church on Earth based on Martian philosophy, and starts a cultural shift. At the novel's climax, he is murdered by a mob from a rival religious group.
In 1963, American television network CBS premiered a sitcom called "My Favorite Martian". The series proved popular in the US, especially during the first season, and CBS broadcast more than one hundred episodes before canceling it when the third season ended in 1966. In this comedy, a Martian anthropologist (who passes for human in most respects) crashes on Earth, where he is harbored by an American man who keeps the Martian's identity secret. Also secret are the Martian's extraordinary abilities, not the least of which are invisibility and telepathy. CBS's rival networks, NBC and ABC, did not fail to notice the success of "My Favorite Martian", or the comic potential of a character with secret powers. In 1964, ABC introduced "Bewitched" (a sitcom about a married, suburban witch), and NBC countered the following year with "I Dream of Jeannie", a sitcom about an astronaut who discovers and marries a genie.
In "Captain Scarlet and the Mysterons" (1967–68), the Mysterons are a race of invisible superbeings from Mars who are at war with humans from Earth. The conflict begins when Captain Black, a human officer investigating radio signals from Mars, mistakes a surveillance camera for a weapon. In violation of his orders, he attacks, but the Mysterons immediately repair the damage he caused. The conflict escalates, and the Mysterons attempt to assassinate the president of Earth.
DC Comics introduced the first Martian superhero to the DC Universe in 1955. Martian Manhunter (J'onn J'onzz), a green humanoid who is believed to be the last of the peaceful Green Martians, joins the Justice League. Meanwhile, the warlike, shapeshifting White Martians regard the Green Martians as enemies. The White Martians adopt a humanoid form which, they say, expresses their distinctive philosophy. DC introduced a White Martian superhero, Miss Martian, in 2006. A third race, the Yellow Martians, may or may not have survived as long as the Green and White Martians.

</doc>
<doc id="47795" url="https://en.wikipedia.org/wiki?curid=47795" title="Voyager program">
Voyager program

The Voyager program is a continuing American scientific program that employs two robotic probes, Voyager 1 and Voyager 2, to study the outer Solar System. They were launched in 1977 to take advantage of a favorable alignment of Jupiter, Saturn, Uranus, and Neptune, and are now exploring the outer boundary of the heliosphere. Although their original mission was to study only the planetary systems of Jupiter and Saturn, Voyager 2 continued on to Uranus and Neptune, and both Voyagers are now tasked with exploring interstellar space. Their mission has been extended three times, and both probes continue to collect and relay useful scientific data.
On August 25, 2012, data from Voyager 1 indicated that it had become the first human-made object to enter interstellar space, traveling "further than anyone, or anything, in history". , Voyager 1 was moving with a velocity of relative to the Sun. Voyager 2 is expected to enter interstellar space by 2016, and its plasma spectrometer should provide the first direct measurements of the density and temperature of the interstellar plasma.
Data and photographs collected by the Voyagers' cameras, magnetometers, and other instruments revealed previously unknown details about each of the giant planets and their moons. Close-up images from the spacecraft charted Jupiter’s complex cloud forms, winds, and storm systems and discovered volcanic activity on its moon Io. Saturn’s rings were found to have enigmatic braids, kinks, and spokes and to be accompanied by a myriad of "ringlets." At Uranus Voyager 2 discovered a substantial magnetic field around the planet and 10 additional moons. Its flyby of Neptune uncovered three complete rings and six hitherto unknown moons as well as a planetary magnetic field and complex, widely distributed auroras. Voyager 2 is still the only spacecraft to have visited the ice giants.
The Voyager spacecraft were built at the Jet Propulsion Laboratory in Southern California, and they were paid for by the National Aeronautics and Space Administration (NASA), which also paid for their launchings from Cape Canaveral, Florida, their tracking, and everything else concerning the space probes.
History.
The two Voyager space probes were originally conceived as part of the Mariner program, and they were thus initially named Mariner 11 and Mariner 12. They were then moved into a separate program named "Mariner Jupiter-Saturn", later renamed the "Voyager Program" because it was thought that the design of the two space probes had progressed sufficiently beyond that of the Mariner family to merit a separate name.
The Voyager Program was similar to the Planetary Grand Tour planned during the late 1960s and early 70s. The Grand Tour would take advantage of an alignment of the outer planets discovered by Gary Flandro, an aerospace engineer at the Jet Propulsion Laboratory. This alignment, which occurs once every 175 years, would occur in the late 1970s and make it possible to use gravitational assists to explore Jupiter, Saturn, Uranus, Neptune, and Pluto. The Planetary Grand Tour was to send several pairs of probes to fly by all the outer planets (including Pluto, then still considered a planet) along various trajectories, including Jupiter-Saturn-Pluto and Jupiter-Uranus-Neptune. Limited funding ended the Grand Tour program, but elements were incorporated into the "Voyager" Program, which fulfilled many of the flyby objectives of the Grand Tour except a visit to Pluto.
Voyager 2 was the first to launch. Its trajectory was designed to allow flybys of Jupiter, Saturn, Uranus, and Neptune. Voyager 1 was launched after Voyager 2, but along a shorter and faster trajectory that was designed to provide an optimal flyby of Saturn's moon Titan, which was known to be quite large and to possess a dense atmosphere. This encounter sent Voyager 1 out of the plane of the ecliptic, ending its planetary science mission. Had "Voyager 1" been unable to perform the Titan flyby, the trajectory of Voyager 2 could have been altered to explore Titan, forgoing any visit to Uranus and Neptune. Voyager 1 was not launched on a trajectory that would have allowed it to continue to Uranus and Neptune, but could have continued from Saturn to Pluto without exploring Titan.
During the 1990s, Voyager 1 overtook the slower deep-space probes Pioneer 10 and Pioneer 11 to become the most distant human made object from Earth, a record that it will keep for the foreseeable future. The New Horizons probe, which had a higher launch velocity than 
Voyager 1, is traveling more slowly due to the extra speed "Voyager 1" gained from its flybys of Jupiter and Saturn. Voyager 1 and Pioneer 10 are the most widely separated human made objects anywhere, since they are traveling in roughly opposite directions from the Solar System.
In December 2004, Voyager 1 crossed the termination shock, where the solar wind is slowed to subsonic speed, and entered the heliosheath, where the solar wind is compressed and made turbulent due to interactions with the interstellar medium. On December 10, 2007, Voyager 2 also reached the termination shock, about 1 billion miles closer to the sun than from where Voyager 1 first crossed it, indicating that the Solar System is asymmetrical.
In 2010 Voyager 1 reported that the outward velocity of the solar wind had dropped to zero, and scientists predicted it was nearing interstellar space. In 2011, data from the "Voyagers" determined that the heliosheath is not smooth, but filled with giant magnetic bubbles, theorized to form when the magnetic field of the Sun becomes warped at the edge of the Solar System.
On 15 June 2012, scientists at NASA reported that Voyager 1 was very close to entering interstellar space, indicated by a sharp rise in high-energy particles from outside the Solar System. In September 2013, NASA announced that Voyager 1 had crossed the heliopause on August 25, 2012, making it the first spacecraft to enter interstellar space.
Spacecraft design.
The Voyager spacecraft weigh 773 kilograms. Of this, 105 kilograms are scientific instruments. The identical Voyager spacecraft use three-axis-stabilized guidance systems that use gyroscopic and accelerometer inputs to their attitude control computers to point their high-gain antennas towards the Earth and their scientific instruments towards their targets, sometimes with the help of a movable instrument platform for the smaller instruments and the electronic photography system.
The diagram at the right shows the high-gain antenna (HGA) with a 3.7 m diameter dish attached to the hollow decagonal electronics container. There is also a spherical tank that contains the hydrazine monopropellant fuel.
The Voyager Golden Record is attached to one of the bus sides. The angled square panel to the right is the optical calibration target and excess heat radiator. The three radioisotope thermoelectric generators (RTGs) are mounted end-to-end on the lower boom.
The scan platform comprises: the Infrared Interferometer Spectrometer (IRIS) (largest camera at top right); the Ultraviolet Spectrometer (UVS) just above the UVS; the two Imaging Science Subsystem (ISS) vidicon cameras to the left of the UVS; and the Photopolarimeter System (PPS) under the ISS.
Only five investigation teams are still supported, though data is collected for two additional instruments.
The Flight Data Subsystem (FDS) and a single eight-track digital tape recorder (DTR) provide the data handling functions.
The FDS configures each instrument and controls instrument operations. It also collects engineering and science data and formats the data for transmission. The DTR is used to record high-rate Plasma Wave Subsystem (PWS) data. The data is played back every six months.
The Imaging Science Subsystem, made up of a wide angle and a narrow angle camera, is a modified version of the slow scan vidicon camera designs that were used in the earlier Mariner flights. The Imaging Science Subsystem consists of two television-type cameras, each with eight filters in a commandable Filter Wheel mounted in front of the vidicons. One has a low resolution 200 mm focal length wide-angle lens with an aperture of f/3 (the wide angle camera), while the other uses a higher resolution 1500 mm narrow-angle f/8.5 lens (the narrow angle camera).
Computers.
Unlike the other onboard instruments, the operation of the cameras for visible light is not autonomous, but rather it is controlled by an imaging parameter table contained in one of the on-board digital computers, the Flight Data Subsystem (FDS). More recent space probes, since about 1990, usually have completely autonomous cameras.
The computer command subsystem (CCS) controls the cameras. The CCS contains fixed computer programs such as command decoding, fault detection, and correction routines, antenna pointing routines, and spacecraft sequencing routines. This computer is an improved version of the one that was used in the "Viking" orbiter. The hardware in both custom-built CCS subsystems in the "Voyagers" is identical. There is only a minor software modification for one of them that has a scientific subsystem that the other lacks.
The Attitude and Articulation Control Subsystem (AACS) controls the spacecraft orientation (its attitude). It keeps the high-gain antenna pointing towards the Earth, controls attitude changes, and points the scan platform. The custom-built AACS systems on both craft are identical.
It has been erroneously reported on the Internet that the Voyager space probes were controlled by a version of the RCA 1802 (RCA CDP1802 "COSMAC" microprocessor), but such claims are not supported by the primary design documents. The CDP1802 microprocessor was used later in the Galileo space probe, which was designed and built years later. The digital control electronics of the Voyagers were not based on a microprocessor but on a specially designed set of RCA CD4000 radiation-hardened, silicon-on-sapphire (SOS) custom-made integrated circuit chips, combined with standard transistor-transistor logic (TTL) integrated circuits.
Communications.
The uplink communications are executed via S-band microwave communications. The downlink communications are carried out by an X-band microwave transmitter on board the spacecraft, with an S-band transmitter as a back-up. All long-range communications to and from the two Voyagers have been carried out using their 3.7-meter high-gain antennas.
Because of the inverse-square law in radio communications, the digital data rates used in the downlinks from the Voyagers have been continually decreasing the farther that they get from the Earth. For example, the data rate used from Jupiter was about 115,000 bits per second. That was halved at the distance of Saturn, and it has gone down continually since then. Some measures were taken on the ground along the way to reduce the effects of the inverse-square law. In between 1982 and 1985, the diameters of the three main parabolic dish antennas of the Deep Space Network was increased from 64 m to 70 m, dramatically increasing their areas for gathering weak microwave signals.
Then between 1986 and 1989, new techniques were brought into play to combine the signals from multiple antennas on the ground into one, more powerful signal, in a kind of an antenna array. This was done at Goldstone, California, Canberra, and Madrid using the additional dish antennas available there. Also, in Australia, the Parkes Radio Telescope was brought into the array in time for the fly-by of Neptune in 1989. In the United States, the Very Large Array in New Mexico was brought into temporary use along with the antennas of the Deep Space Network at Goldstone. Using this new technology of antenna arrays helped to compensate for the immense radio distance from Neptune to the Earth.
Power.
Electrical power is supplied by three MHW-RTG radioisotope thermoelectric generators (RTGs). They are powered by plutonium-238 (distinct from the Pu-239 isotope used in nuclear weapons) and provided approximately 470 W at 30 volts DC when the spacecraft was launched. Plutonium-238 decays with a half-life of 87.74 years, so RTGs using Pu-238 will lose a factor of 1−0.5(1/87.74) = 0.79% of their power output per year.
In 2011, 34 years after launch, such an RTG would inherently produce 470 W × 2−(34/87.74) ≈ 359 W, about 76% of its initial power. Additionally, the thermocouples that convert heat into electricity also degrade, reducing available power below this calculated level.
By 7 October 2011 the power generated by Voyager 1 and Voyager 2 had dropped to 267.9 W and 269.2 W respectively, about 57% of the power at launch. The level of power output was better than pre-launch predictions based on a conservative thermocouple degradation model. As the electrical power decreases, spacecraft loads must be turned off, eliminating some capabilities.
Voyager Interstellar Mission.
The Voyager primary mission was completed in 1989, with the close flyby of Neptune by "Voyager 2". The Voyager Interstellar Mission (VIM) is a mission extension, which began when the two spacecraft had already been in flight for over 12 years. The Heliophysics Division of the NASA Science Mission Directorate conducted a Heliophysics Senior Review in 2008. The panel found that the VIM "is a mission that is absolutely imperative to continue" and that VIM "funding near the optimal level and increased DSN (Deep Space Network) support is warranted."
As of the present date, the Voyager 2 and Voyager 1 scan platforms, including all of the platform instruments, have been powered down. The ultraviolet spectrometer (UVS) on Voyager 1 was active until 2003, when it too was deactivated. Gyro operations will end in 2016 for Voyager 2 and 2017 for Voyager 1. Gyro operations are used to rotate the probe 360 degrees six times per year to measure the magnetic field of the spacecraft, which is then subtracted from the magnetometer science data.
The two spacecraft continue to operate, with some loss in subsystem redundancy, but retain the capability of returning scientific data from a full complement of Voyager Interstellar Mission (VIM) science instruments.
Both spacecraft also have adequate electrical power and attitude control propellant to continue operating until around 2025, after which there may not be available electrical power to support science instrument operation. At that time, science data return and spacecraft operations will cease.
Telemetry.
The telemetry comes to the telemetry modulation unit (TMU) separately as a "low-rate" 40-bit-per-second (bit/s) channel and a "high-rate" channel.
Low rate telemetry is routed through the TMU such that it can only be downlinked as uncoded bits (in other words there is no error correction). At high rate, one of a set of rates between 10 bit/s and 115.2 kbit/s is downlinked as coded symbols.
The TMU encodes the high rate data stream with a convolutional code having constraint length of 7 with a symbol rate equal to twice the bit rate (k=7, r=1/2)
Voyager telemetry operates at these transmission rates:
Note: At 160 and 600 bit/s different data types are interleaved.
The Voyager craft have three different telemetry formats:
High rate
Low rate
It is understood that there is substantial overlap of EL-40 and CR-5T (ISA 35395) telemetry, but the simpler EL-40 data does not have the resolution of the CR-5T telemetry. At least when it comes to representing available electricity to subsystems, EL-40 only transmits in integer increments—so similar behaviors are expected elsewhere.
Memory dumps are available in both engineering formats. These routine diagnostic procedures have detected and corrected intermittent memory bit flip problems, as well as detecting the permanent bit flip problem that caused a two-week data loss event mid-2010.
Voyager Golden Record.
Both craft carry with them a 12-inch golden phonograph record that contains pictures and sounds of Earth along with symbolic directions on the cover for playing the record and data detailing the location of our planet. The record is intended as a combination of a time capsule and an interstellar message to any civilization, alien or far-future human, that may recover either of the Voyagers. The contents of this record were selected by a committee that included Timothy Ferris and was chaired by Carl Sagan.
Pale blue dot.
The Voyager program's discoveries during the primary phase of its mission, including never-before-seen close-up color photos of the major planets, were regularly documented by both print and electronic media outlets. Among the best-known of these is an image of the Earth as a pale blue dot, taken in 1990 by Voyager 1, and popularized by Carl Sagan with the quote: 
External links.
NASA sites
NASA instrument information pages:
Non-NASA sites

</doc>
<doc id="47796" url="https://en.wikipedia.org/wiki?curid=47796" title="Patience">
Patience

Patience (or forbearing) is the state of endurance under difficult circumstances, which can mean persevering in the face of delay or provocation without acting on negative annoyance/anger; or exhibiting forbearance when under strain, especially when faced with longer-term difficulties. Patience is the level of endurance one can have before negativity. It is also used to refer to the character trait of being steadfast. Antonyms include and impetuousness.
Scientific perspectives.
In psychology and in cognitive neuroscience, patience is studied as a decision-making problem, involving the choice of either a small reward in the short-term, or a more valuable reward in the long-term. When given a choice, all animals, humans included, are "inclined" to favour short-term rewards over long-term rewards. This is despite the often greater benefits associated with long-term rewards.
In a 2005 study involving common marmosets and cottontop tamarins, animals of both species faced a self-control paradigm in which individuals chose between taking an immediate small reward and waiting a variable amount of time for a large reward. Under these conditions, marmosets waited significantly longer for food than tamarins. This difference cannot be explained by life history, social behaviour or brain size. It can, however, be explained by feeding ecology: marmosets rely on gum, a food product acquired by waiting for exudate to flow from trees, whereas tamarins feed on insects, a food product requiring impulsive action. Foraging ecology, therefore, may provide a selective pressure for the evolution of self-control.
Patience of human users in the online world has been the subject of much recent scientific research. In a 2012 study involving tens of millions of users who watched videos on the Internet, Krishnan and Sitaraman show that online users lose patience in as little as two seconds while waiting for their chosen video to start playing. The study also shows that users who are connected to the Internet at faster speeds are less patient than their counterparts connected at slower speeds, demonstrating a link between the human expectation of speed and human patience. These and other scientific studies of patience have led many social commentators to conclude that the rapid pace of technology is rewiring humans to be less and less patient.
Religious perspectives.
Judaism.
Patience and fortitude are prominent themes in Judaism. The Talmud extols patience as an important personal trait. The story of Micah, for example, is that he suffers many challenging conditions and yet endures, saying "I will wait for the God who saves me." Patience in God, it is said, will aid believers in finding the strength to be delivered from the evils that are inherent in the physical life.
In the Hebrew Torah, patience is referred to in several proverbs, such as "The patient man shows much good sense, but the quick-tempered man displays folly at its height" (Proverbs 14:29, NAB); "An ill-tempered man stirs up strife, but a patient man allays discord." (Proverbs 15:18, NAB); and "A patient man is better than a warrior, and he who rules his temper, than he who takes a city." (Proverbs 16:32). The emotion is also discussed in other sections, such as Ecclesiastes: "Better is the patient spirit than the lofty spirit. Do not in spirit become quickly discontented, for discontent lodges in the bosom of a fool." (Ecclesiastes 7:8-9, NAB).
Christianity.
In the Christian religion, patience is one of the most valuable virtues of life. Increasing patience is viewed as the work of the Holy Ghost in the Christian who has accepted the gift of salvation. While patience is not one of the traditional biblical three theological virtues nor one of the traditional cardinal virtues, it is part of the fruit of the Holy Spirit, according to the Apostle Paul in his Epistle to the Galatians.
In the Christian Bible, patience is referred to in several sections. The Book of Proverbs notes that "through patience a ruler can be persuaded, and a gentle tongue can break a bone" (Proverbs 25:14-16, NIV); Ecclesiastes points out that the "end of a matter is better than its beginning, and patience is better than pride" (Ecclesiastes 7:7-9, NIV); and 1 Thessalonians states that we should "be patient with all. See that no one returns evil for evil; rather, always seek what is good for each other and for all" (1 Thessalonians 5:14-15, NAB). In the Epistle of James, the Bible urges Christians to be patient, and " see how the farmer waits for the precious fruit of the earth...until it receives the early and the late rains." (James 5:7-11, NAB). In Galatians, patience is listed as part of the "fruit of the Spirit": "love, joy, peace, patience, kindness, goodness, faithfulness, gentleness and self-control. Against such things there is no law". (Galatians 5:21-23, NIV). In Timothy, the Bible states that "Jesus might display his unlimited patience as an example for those who would believe on him and receive eternal life".(1 Timothy 1:15-17, NIV).
Islam.
Patience with steadfast belief in Allah is called "sabr" (), one of the best virtues of life in Islam. Through sabr, a Muslim believes that an individual can grow closer to God and thus attain true peace. It is also stressed in Islam, that Allah is with those who are patient, more specifically during calamity and suffering. Several verses in Quran urge Muslims to seek Allah's help when faced with fear and loss, with patient prayers and perseverance for Allah. For example:
Similarly, patience is mentioned in hadith Sahih Bukhari:
In Islamic tradition,("Ayyoob") illustrates a story where he demonstrated patience and steadfast belief in Allah. Ibn Kathir narrates the story in the following manner: Job was a very rich person with much land, and many animals and children — all of which were lost and soon he was struck with disease as a test from Allah. He remained steadfast and patient in his prayers to Allah, so Allah eventually relieved him of the disease, gave him double the money he lost, and raised to life twice the number of children who had died before him.
Buddhism.
In Buddhism, patience (Skt.: "kshanti"; Pali: "khanti") is one of the "perfections" ("paramitas") that a bodhisattva trains in and practices to realize perfect enlightenment ("bodhi"). The Buddhist concept of patience is distinct from the English definition of the word. In Buddhism, patience refers to not returning harm, rather than merely enduring a difficult situation. It is the ability to control one's emotions even when being criticized or attacked. In verse 184 of the Dhammapada it is said that 'enduring patience is the highest austerity'.
Hinduism.
Patience and forbearance is considered an essential virtue in Hinduism. In ancient literature of Hinduism, the concept of patience is referred to with the word "pariksaha" (patience and forbearance, Sanskrit: परिषहा), and several other words such as sahiṣṇutā (patient toleration, Sanskrit: सहिष्णुता), "titiksha" (forbearance, Sanskrit: तितिक्षा), "sah" or "sahanshilata" (suffer with patience, Sanskrit: सह, सहनशीलता) and several others.
Patience, in Hindu philosophy, is the cheerful endurance of trying conditions and the consequence of one's action and deeds (karma). It is also the capacity to wait, endure opposites - such as pain and pleasure, cold and heat, sorrows and joys - calmly, without anxiety, and without a desire to seek revenge. In interpersonal relationships, virtuous "titiksha" means that if someone attacks or insults without cause, one must endure it without feeling enmity, anger, resentment or anxiety. The concept of patience is explained as being more than trust, and as a value that reflects the state of one's body and mind. The term "pariksaha" is sometimes also translated as test or exam, in other contexts. Some of these concepts have been carried into the spiritual understanding of yoga. Sandilya Upanishad of Hinduism identifies ten sources of patience and forbearances: Ahimsa, Satya, Asteya, Brahmacharya, Daya, Arjava,Kshama, Dhriti, Mitahara and Saucha. In each of these ten forbearances, the virtuous implicit belief is that our current spirit and the future for everyone, including oneself, will be stronger if these forbearances are one's guide. Each source of those ten pariksaha (patience and forbearances) are:
The classical literature of Hinduism exists in many Indian languages. For example, "Tirukkuṛaḷ" written between 200 BC and 400 AD, and sometimes called the Tamil Veda, is one of the most cherished classics on Hinduism written in a South Indian language. It too discusses patience and forbearance, dedicating Chapter 16 of Book 1 to it. "Tirukkuṛaḷ" suggests "patience" is necessary for an ethical life and one's long term happiness, even if patience is sometimes difficult in the short term. Some of the verse excerpts from this book are: "our conduct must always foster forbearance"; "one must patiently endure rude remarks, because it delivers us to purity"; "if we are unjustly wronged by others, it is best to conquer our hurt with patience, accept suffering, and refrain from unrighteous retaliation"; "it is good to patiently endure injuries done to you, but to forget them is even better"; "just as the Earth bears those who dig into her, one must with patience bear with those who despise us", and so on.
Meher Baba.
The spiritual teacher Meher Baba stated that "of the first requirements of the [spiritual aspirant is that he should combine "unfailing enthusiasm with unyielding patience"...Spiritual effort demands not only physical endurance and courage, but also unshrinking forbearance and unassailable moral courage."
Philosophical perspectives.
In "Human, All Too Human", philosopher Friedrich Nietzsche argued that "being able to wait is so hard that the greatest poets did not disdain to make the inability to wait the theme of their poetry." He notes that "Passion will not wait", and gives the example of cases of duels, in which the "advising friends have to determine whether the parties involved might be able to wait a while longer. If they cannot, then a duel is reasonable ...to wait would be to continue suffering the horrible torture of offended honor...".
Impatience.
Impatience is not just the opposite of patience or the lack of it, it is more than that. Impatience is also a cultural phenomenon in many respects. There are deep-rooted psychological and social reasons for both feelings of and displays of impatience.
Examples of Impatience.
The term 'road rage' is commonly used to refer to situations that are the result of some form of impatience, though of course there can be other factors involved. This phrase refers to circumstances where one or both drivers get into a confrontation in which they are emotionally involved. Driving anywhere in the world can be frustrating. However, attitudes towards annoying circumstances vary widely from one place to another. Excessive honking, as a further example of impatience, is very common in some countries, but not so much in others.
Behavior in restaurants and other public places is another example. People can often be rude and impatient toward other patrons, as well as to the staff serving customers. Although a customer may have legitimate complaints about something, there is really no justification to be difficult about it. On the other hand, there is no reason for staff to be surly or curt with customers, but it does happen, all too often.
Telephone service and support is another situation where customers can become impatient with company representatives. This may partly be the result of changes in how these support services work. In an effort to save expense costs, many organizations have automated part of all of these services. This starts off the process, in the customer's view, on a somewhat unpleasant note. By the time the client has maneuvered their way through the system, including being on hold for a significant length of time, and sometimes eventually reaching the wrong person for whatever reason, the customer is understandably upset.
These examples may explain impatience and related behaviors in various situations, but do not excuse it.
Positive Aspects of Impatience.
Impatience is usually viewed as a negative thing. However, it can have some advantages.
Being patient with one's condition or situation can be a form of complacency. Being impatient to do something useful can indeed be a good thing.

</doc>
<doc id="47798" url="https://en.wikipedia.org/wiki?curid=47798" title="Trogon">
Trogon

The trogons and quetzals are birds in the order Trogoniformes which contains only one family, the Trogonidae. The family contains 39 species in seven genera. The fossil record of the trogons dates back 49 million years to the Early Eocene. They might constitute a member of the basal radiation of the order Coraciiformes or be closely related to mousebirds and owls. The word "trogon" is Greek for "nibbling" and refers to the fact that these birds gnaw holes in trees to make their nests.
Trogons are residents of tropical forests worldwide. The greatest diversity is in the Neotropics, where four genera, containing 24 species occur. The genus "Apaloderma" contains the three African species. The genera "Harpactes" and "Apalharpactes", containing twelve species, are found in southeast Asia.
They feed on insects and fruit, and their broad bills and weak legs reflect their diet and arboreal habits. Although their flight is fast, they are reluctant to fly any distance. Trogons are generally not migratory, although some species undertake partial local movements. Trogons have soft, often colourful, feathers with distinctive male and female plumage. They are the only type of animal with a heterodactyl toe arrangement. They nest in holes dug into trees or termite nests, laying 2–4 white or pastel-coloured eggs.
Evolution and taxonomy.
The position of the trogons within the class Aves has been a long-standing mystery. A variety of relations have been suggested, including the parrots, cuckoos, toucans, jacamars and puffbirds, rollers, owls and nightjars. More recent morphological and molecular evidence has suggested a relationship with the Coliiformes. The unique arrangement of the toes on the foot (see morphology and flight) has led many to consider the trogons to have no close relatives, and to place them in their own order, possibly with the similarly atypical mousebirds as their closest relatives.
The earliest formally described fossil specimen is a cranium from the Fur Formation Lower Eocene in Denmark (54 mya). Other trogoniform fossils have been found in the Messel pit deposits from the mid-Eocene in Germany (49 mya)., in Oligocene rocks from Switzerland and Miocene France. The oldest New World fossil of a trogon is from the comparatively recent Pleistocene (less than 2.588 mya).
The family had been thought to have an Old World origin notwithstanding the current richness of the family, which is more diverse in the Neotropical New World. DNA evidence seemed to support an African origin for the trogons, with the African genus "Apaloderma" seemingly basal in the family, and the other two lineages, the Asian and American, breaking off between 20–36 million years ago. More recent studies show that the DNA evidence gives contradictory results concerning the basal phylogenetic relationships; so it is currently unknown if all extant trogons are descended from an African or an American ancestor or neither.
The trogons are split into three subfamilies, each reflecting one of these splits, Aplodermatinae is the African subfamily and contains a single genus, "Apaloderma." Harpactinae is the Asian subfamily and contains two genera, "Harpactes" and "Apalharpactes". "Apalharpactes", consisting of two species in Java and Sumatra, has only recently been accepted as a separate genus from "Harpactes". The remaining subfamily, the Neotropical Trogoninae, contains the remaining four genera, "Trogon, Priotelus, Pharomachrus" and "Euptilotis".
Distribution and habitat.
The majority of trogons are birds of tropical and subtropical forests. They have a cosmopolitan distribution in the worlds wet tropics, being found in the Americas, Africa and Asia. A few species are distributed into the temperate zone, with one species, the elegant trogon, reaching the south of the United States specifically southern Arizona and the surrounding area. The Narina trogon of Africa is slightly exceptional in that it utilises a wider range of habitats than any other trogon, ranging from dense forest to fairly open savannah, and from the Equator to southern South Africa. It is the most widespread and successful of all the trogons. The eared quetzal of Mexico is also able to use more xeric habitats, but preferentially inhabits forests. Most other species are more restricted in their habitat, with several species being restricted to undisturbed primary forest. Within forests they tend to be found in the mid-story, occasionally in the canopy.
Some species, particularly the quetzals, are adapted to cooler montane forest. There are a number of insular species; these include a number of species found in the Greater Sundas, one species in the Philippines as well as two monophyletic genera endemic to Cuba and Hispaniola respectively. Outside of South Asia and the Caribbean, however, trogons are generally absent from islands, especially oceanic ones.
Trogons are generally sedentary, with no species known to undertake long migrations. A small number of species are known to make smaller migratory movements, particularly montane species which move to lower altitudes during different seasons. This has been demonstrated using radio tracking in the resplendent quetzal in Costa Rica and evidence has been accumulated for a number of other species. The Narina trogon of Africa is thought to undertake some localised short distance migrations over parts of its range, for example birds of Zimbabwe's plateau savannah depart after the breeding season. A complete picture of these movements is however lacking. Trogons are difficult to study as their thick tarsi (feet bones) make ringing studies difficult.
Morphology and flight.
The trogons as a family are fairly uniform in appearance, having compact bodies and long tails (very long in the case of the quetzals), and short necks. Trogons range in size from the 23 cm, 40 gram scarlet-rumped trogon to the 40 cm, 210 gram resplendent quetzal (not including the male quetzal's tail streamers). Their legs and feet are weak and short, and trogons are essentially unable to walk beyond a very occasional shuffle along a branch. They are even incapable of turning around on a branch without using their wings. The ratio of leg muscle to body weight in trogons is only 3 percent, the lowest known ratio of any bird. The arrangement of toes on the feet of trogons is also unique among birds, although essentially resembling the zygodactyl’s two forward two backward arrangement of parrots and other near-passerines, the actual toes are arranged with usually inner hallux being the outer hind toe, an arrangement that is referred to as heterodactylous. The strong bill is short and the gape wide, particularly in the fruit eating quetzals, with a slight hook at the end. There is also a notch at the end of the bill and many species have slight serrations in the mandibles. The skin is exceptionally tender, making preparation of study skins difficult for museum curators. The skeletons of trogons are surprisingly slender, particularly the skulls which are very thin. The plumage of many species is iridescent, although most of the Asian species are not. The African trogons are generally green on the back with red bellies. The New World trogons similarly have green or deep blue upperparts but are more varied in their lowerparts. The Asian species tend towards red underparts and brown backs.
The wings are short but strong, with the wing muscle ratio being around 22% of the body weight. In spite of the strength of their flight, trogons do not fly often or for great distances, generally flying no more than a few hundred metres at a time. Only the montane species tend to make long distance flights. Shorter flights tend to be direct and swift, but longer flights are slightly undulating. Their flight can be surprisingly silent (for observers), although that of a few species is reportedly quite noisy.
Calls.
The calls of trogons are generally loud and uncomplex, consisting of monosyllabic hoots and whistles delivered in varying patterns and sequences. The calls of the quetzals and the two Caribbean genera are the most complex. Among the Asian genera the Sumatran trogon ("Apalharpactes") has the most atypical call of any trogon, research has not yet established whether the closely related Javan trogon has a similar call. The calls of the other Asian genus, "Harpactes", are remarkably uniform. In addition to the territorial and breeding calls given by males and females during the breeding seasons, trogons have been recorded as having aggression calls given by competing males and alarm calls.
Behaviour.
Trogons are generally inactive outside of infrequent feeding flights. Among birdwatchers and biologists it has been noted that "from their great beauty [they are notorious ... for their lack of other immediately engaging qualities". Their lack of activity is possibly a defence against predation; trogons on all continents have been reported to shift about on branches to always keep their less brightly coloured backs turned towards observers, while their heads, which like owls can turn through 180 degrees, keep a watch on the watcher. Trogons have reportedly been preyed upon by hawks and predatory mammals; one report was of a resplendent quetzal taken while brooding young by a margay.
Diet and feeding.
Trogons feed principally on insects, other arthropods, and fruit; to a lesser extent some small vertebrates such as lizards are taken. Among the insect prey taken one of the more important types are caterpillars; along with cuckoos, trogons are one of the few birds groups to regularly prey upon them. Some caterpillars are known to be poisonous to trogons though, like "Arsenura armida. "The extent to which each food type is taken varies depending on geography and species. The three African trogons are exclusively insectivorous, whereas the Asian and American genera consume varying amounts of fruit. Diet is somewhat correlated with size, with larger species feeding more on fruit and smaller species focusing on insects.
Prey is almost always obtained on the wing. The most commonly employed foraging technique is a sally-glean flight, where a trogon flies from an observation perch to a target on another branch or in foliage. Once there the birds hovers or stalls and snatches the item before returning to its perch to consume the item. This type of foraging is commonly used by some types of bird to obtain insect prey; in trogons and quetzals it is also used to pluck fruit from trees. Insect prey may also be taken on the wing, with the trogon pursuing flying insects in a similar manner to drongos and Old World flycatchers. Frogs, lizards and large insects on the ground may also be pounced on from the air. More rarely some trogons may shuffle along a branch to obtain insects, insect eggs and very occasionally nestling birds. Violaceous trogons will consume wasps and wasp larvae encountered while digging nests.
Breeding.
Trogons are territorial and monogamous. Males will respond quickly to playbacks of their calls and will repel other members of the same species and even other hole-nesting species from around their nesting sites. Males attract females by singing, and, in the case of the resplendent quetzal, undertaking display flights. Some species have been observed in small flocks of 3–12 individuals prior to and sometimes during the breeding season, calling and chasing each other, but the function of these flocks is unclear.
Trogons are cavity nesters. Nests are dug into rotting wood or termite nests, with one species, the violaceous trogon, nesting in wasp nests. Nest cavities can either be deep upward slanting tubes that lead to fully enclosed chambers, or much shallower open niches (from which the bird is visible). Nests are dug with the beak, incidentally giving the family its name. Nest digging may be undertaken by the male alone or by both sexes. In the case of nests dug into tree trunks, the wood must be strong enough not to collapse but soft enough to dig out. Trogons have been observed landing on dead tree trunks and slapping the wood with their tails, presumably to test the firmness.
The nests of trogons are thought to usually be unlined. Between two to four eggs are laid in a nesting attempt. These are round and generally glossy white or lightly coloured (buff, grey, blue or green), although they get increasingly dirty during incubation. Both parents incubate the eggs (except in the case of the bare-cheeked trogon, where apparently the male takes no part), with the male taking one long incubation stint a day and the female incubating the rest of the time. Incubation seems to begin after the last egg is laid. The incubation period varies by species, usually lasting between 16–19 days. On hatching the chicks are altricial, blind and naked. The chicks acquire feathers rapidly in some of the montane species, in the case of the mountain trogon in a week, but more slowly in lowland species like the black-headed trogon, which may take twice as long. The nestling period varies by species and size, with smaller species generally taking 16 to 17 days to fledge, whereas larger species may take as long as 30 days, although 23–25 days is more typical.
Relationship with humans.
Trogons and quetzals are considered to be "among the most beautiful of birds", yet they are also often reclusive and seldom seen. Little is known about much of their biology, and much of what is known about them comes from the research of neotropical species by the ornithologist Alexander Skutch. Trogons are nevertheless popular birds with birdwatchers, and there is a modest ecotourism industry in particular to view quetzals in Central America.

</doc>
<doc id="47801" url="https://en.wikipedia.org/wiki?curid=47801" title="H.M.S. Pinafore">
H.M.S. Pinafore

H.M.S. Pinafore; or, The Lass That Loved a Sailor is a comic opera in two acts, with music by Arthur Sullivan and a libretto by W. S. Gilbert. It opened at the Opera Comique in London, on 25 May 1878 and ran for 571 performances, which was the second-longest run of any musical theatre piece up to that time. "H.M.S. Pinafore" was Gilbert and Sullivan's fourth operatic collaboration and their first international sensation.
The story takes place aboard the ship HMS "Pinafore". The captain's daughter, Josephine, is in love with a lower-class sailor, Ralph Rackstraw, although her father intends her to marry Sir Joseph Porter, the First Lord of the Admiralty. She abides by her father's wishes at first, but Sir Joseph's advocacy of the equality of humankind encourages Ralph and Josephine to overturn conventional social order. They declare their love for each other and eventually plan to elope. The captain discovers this plan, but, as in many of the Gilbert and Sullivan operas, a surprise disclosure changes things dramatically near the end of the story.
Drawing on several of his earlier "Bab Ballad" poems, Gilbert imbued this plot with mirth and silliness. The opera's humour focuses on love between members of different social classes and lampoons the British class system in general. "Pinafore" also pokes good-natured fun at patriotism, party politics, the Royal Navy, and the rise of unqualified people to positions of authority. The title of the piece comically applies the name of a garment for girls and women, a pinafore, to the fearsome symbol of a naval warship.
"Pinafore"'s extraordinary popularity in Britain, America and elsewhere was followed by the similar success of a series of Gilbert and Sullivan works, including "The Pirates of Penzance" and "The Mikado". Their works, later known as the Savoy operas, dominated the musical stage on both sides of the Atlantic for more than a decade and continue to be performed today. The structure and style of these operas, particularly "Pinafore", were much copied and contributed significantly to the development of modern musical theatre.
Background.
In 1875, Richard D'Oyly Carte, who was then managing the Royalty Theatre for Selina Dolaro, brought Gilbert and Sullivan together to write their second show, a one-act opera entitled "Trial by Jury". This proved a success, and in 1876 D'Oyly Carte assembled a group of financial backers to establish the Comedy Opera Company, which was devoted to the production and promotion of family-friendly English comic opera. With this theatre company, Carte finally had the financial resources, after many failed attempts, to produce a new full-length Gilbert and Sullivan opera. This next opera was "The Sorcerer", which opened in November 1877. It too was successful, running for 178 performances. Sheet music from the show sold well, and street musicians played the melodies.
Instead of writing a piece for production by a theatre proprietor, as was usual in Victorian theatres, Gilbert, Sullivan and Carte produced the show with their own financial support. They were therefore able to choose their own cast of performers, rather than being obliged to use the actors already engaged at the theatre. They chose talented actors, most of whom were not well-known stars and did not command high fees, and to whom they could teach a more naturalistic style of performance than was commonly used at the time. They then tailored their work to the particular abilities of these performers. The skill with which Gilbert and Sullivan used their performers had an effect on the audience; as critic Herman Klein wrote: "we secretly marvelled at the naturalness and ease with which Gilbertian quips and absurdities were said and done. For until then no living soul had seen upon the stage such weird, eccentric, yet intensely human beings. ... conjured into existence a hitherto unknown comic world of sheer delight."
The success of "The Sorcerer" paved the way for another collaboration by Gilbert and Sullivan. Carte agreed on terms for a new opera with the Comedy Opera Company, and Gilbert began work on "H.M.S. Pinafore" before the end of 1877. Gilbert's father had been a naval surgeon, and the nautical theme of the opera appealed to him. He drew on several of his earlier "Bab Ballad" poems (many of which also have nautical themes), including "Captain Reece" (1868) and "General John" (1867). Some of the characters also have prototypes in the ballads: Dick Deadeye is based on a character in "Woman's Gratitude" (1869); an early version of Ralph Rackstraw can be seen in "Joe Go-Lightly" (1867), with its sailor madly in love with the daughter of someone who far outranks him; and Little Buttercup is taken almost wholesale from "The Bumboat Woman's Story" (1870). On 27 December 1877, while Sullivan was on holiday on the French Riviera, Gilbert sent him a plot sketch accompanied by the following note:
Despite Gilbert's disclaimer, audiences, critics and even the Prime Minister identified Sir Joseph Porter with W. H. Smith (a politician who had recently been appointed First Lord of the Admiralty despite having neither military nor nautical experience). Sullivan was delighted with the sketch, and Gilbert read a first draft of the plot to Carte in mid-January.
Following the example of his mentor, T. W. Robertson, Gilbert strove to ensure that the costumes and sets were as realistic as possible. When preparing the sets for "H.M.S. Pinafore", Gilbert and Sullivan visited Portsmouth in April 1878 to inspect ships. Gilbert made sketches of H.M.S. "Victory" and H.M.S. "St Vincent" and created a model set for the carpenters to work from. This was far from standard procedure in Victorian drama, in which naturalism was still a relatively new concept, and in which most authors had very little influence on how their plays and libretti were staged. This attention to detail was typical of Gilbert's stage management and would be repeated in all of his Savoy Operas. Gilbert's focus on visual accuracy provided a "right-side-up for topsy-turvydom", that is, a realistic point of reference that serves to heighten the whimsicality and absurdity of the situations. Sullivan was "in the full swing" of work on the piece by the middle of April 1878. The bright and cheerful music of "Pinafore" was composed during a time when Sullivan suffered from excruciating pain from a kidney stone. The cast began music rehearsals on 24 April, and at the beginning of May 1878, the two collaborators worked closely together at Sullivan's flat to finalise the piece.
In "Pinafore", Gilbert, Sullivan and Carte used several of the principal cast members that they had assembled for "The Sorcerer". As Gilbert had suggested to Sullivan in December 1877, "Mrs. Cripps Buttercup will be a capital part for Everard ... Barrington will be a capital captain, and Grossmith a first-rate First Lord." However, Mrs Howard Paul, who had played Lady Sangazure in "The Sorcerer", was declining vocally. She was under contract to play the role of Cousin Hebe in "Pinafore". Gilbert made an effort to write an amusing part for her despite Sullivan's reluctance to use her, but by mid-May 1878, both Gilbert and Sullivan wanted her out of the cast; unhappy with the role, she left. With only a week to go before opening night, Carte hired concert singer Jessie Bond to play Cousin Hebe. Since Bond had little experience as an actress, Gilbert and Sullivan cut the dialogue out of the role, except for a few lines in the last scene, which they turned into recitative. Other new cast members were Emma Howson and George Power in the romantic roles, who were improvements on the romantic soprano and tenor in "The Sorcerer".
Gilbert acted as stage director for his own plays and operas. He sought realism in acting, just as he strove for realistic visual elements. He deprecated self-conscious interaction with the audience and insisted on a style of portrayal in which the characters were never aware of their own absurdity but were coherent internal wholes. Sullivan conducted the music rehearsals. As was to be his usual practice in his later operas, Sullivan left the overture for the last moment, sketching it out and entrusting it to the company's music director, in this case Alfred Cellier, to complete. "Pinafore" opened on 25 May 1878 at the Opera Comique.
Synopsis.
Act I.
The British warship H.M.S. "Pinafore" is at anchor off Portsmouth. The sailors are on the quarterdeck, proudly "cleaning brasswork, splicing rope, etc."
Little Buttercup, a Portsmouth "bumboat woman" (dockside vendor) – who is the "rosiest, roundest, and reddest beauty in all Spithead" – comes on board to sell her wares to the crew. She hints that she may be hiding a dark secret under her "gay and frivolous exterior". Ralph Rackstraw, "the smartest lad in all the fleet", enters, declaring his love for the Captain's daughter, Josephine. His fellow sailors (excepting Dick Deadeye, the grim and ugly realist of the crew) offer their sympathies, but they can give Ralph little hope that his love will ever be returned.
The gentlemanly and popular Captain Corcoran greets his "gallant crew" and compliments them on their politeness, saying that he returns the favour by never ("well, hardly ever") using bad language, such as "a big, big D". After the sailors leave, the Captain confesses to Little Buttercup that Josephine is reluctant to consider a marriage proposal from Sir Joseph Porter, the First Lord of the Admiralty. Buttercup says that she knows how it feels to love in vain. As she leaves, the Captain remarks that she is "a plump and pleasing person". Josephine enters and reveals to her father that she loves a humble sailor in his crew, but she assures him that she is a dutiful daughter and will never reveal her love to this sailor.
Sir Joseph comes on board, accompanied by his "admiring crowd of sisters, cousins and aunts". He recounts how he rose from humble beginnings to be "ruler of the Queen's Navee" through persistence, although he has no naval qualifications. He then delivers a humiliating lesson in etiquette, telling the Captain that he must always say "if you please" after giving an order; for "A British sailor is any man's equal" – excepting Sir Joseph's. Sir Joseph has composed a song to illustrate that point, and he gives a copy of it to Ralph. Shortly afterwards, elated by Sir Joseph's views on equality, Ralph decides that he will declare his love to Josephine. This delights his shipmates, except Dick Deadeye, who contends that "when people have to obey other people's orders, equality's out of the question". Shocked by his words, the other sailors force Dick to listen to Sir Joseph's song before they exit, leaving Ralph alone on deck. Josephine now enters, and Ralph confesses his love in terms surprisingly eloquent for a "common sailor". Josephine is touched, but although she has found Sir Joseph's attentions nauseating, she knows that it is her duty to marry Sir Joseph instead of Ralph. Disguising her true feelings, she "haughtily rejects" Ralph's "proffered love".
Ralph summons his shipmates (Sir Joseph's female relatives also arrive) and tells them that he is bent on suicide. The crew expresses sympathy, except for Dick, who provides a stark counterpoint of dissent. Ralph puts a pistol to his head, but as he is about to pull the trigger, Josephine enters, admitting that she loves him after all. Ralph and Josephine plan to sneak ashore to elope that night. Dick Deadeye warns them to "forbear, nor carry out the scheme", but the joyous ship's company ignores him.
Act II.
Later that night, under a full moon, Captain Corcoran reviews his concerns: his "kindly crew rebels", his "daughter to a tar is partial", his friends seem to desert him, and Sir Joseph has threatened a court-martial. Little Buttercup offers sympathy. He tells her that, if it were not for the difference in their social standing, he would have returned her affection. She prophesies that things are not all as they seem and that "a change" is in store for him, but he does not understand her cryptic warning.
Sir Joseph enters and complains that Josephine has not yet agreed to marry him. The Captain speculates that she is probably dazzled by his "exalted rank" and that if Sir Joseph can persuade her that "love levels all ranks", she will accept his proposal. They withdraw, and Josephine enters, still feeling guilty about her planned elopement with Ralph and fearful of giving up a life of luxury. When Sir Joseph makes the argument that "love levels all ranks", a delighted Josephine says that she "will hesitate no longer". The Captain and Sir Joseph rejoice, but Josephine is now more determined than ever to marry Ralph.
Dick Deadeye intercepts the Captain and tells him of the lovers' plans to elope. The Captain confronts Ralph and Josephine as they try to leave the ship. The pair declare their love, justifying their actions because "He is an Englishman!" The furious Captain is unmoved and blurts out, "Why, damme, it's too bad!" Sir Joseph and his relatives, who have overheard this oath, are shocked to hear swearing on board a ship, and Sir Joseph orders the Captain confined to his cabin.
When Sir Joseph asks what had provoked the usually polite officer's outburst, Ralph replies that it was his declaration of love for Josephine. Furious in his turn at this revelation, and ignoring Josephine's plea to spare Ralph, Sir Joseph has the sailor "loaded with chains" and taken to the ship's dungeon. Little Buttercup now comes forward to reveal her long-held secret. Many years ago, when she "practised baby-farming", she had cared for two babies, one "of low condition", the other "a regular patrician". She confesses that she "mixed those children up. ... The wellborn babe was Ralph; your Captain was the other."
Sir Joseph now realises that Ralph should have been the Captain, and the Captain should have been Ralph. He summons both, and they emerge wearing one another's uniforms: Ralph as Captain, in command of the "Pinafore", and Corcoran as a common sailor. Sir Joseph's marriage with Josephine is now "out of the question" in his eyes: "love levels all ranks ... to a considerable extent, but it does not level them as much as that." He hands her to Captain Rackstraw. The former Captain's now-humble social rank leaves him free to marry Buttercup. Sir Joseph settles for his cousin Hebe, and all ends in general rejoicing.
Musical numbers.
1"See discussion of versions, below."
2"Includes reprises of several songs, concluding with "For he is an Englishman"."
Productions.
"Pinafore" opened on 25 May 1878 at the Opera Comique, before an enthusiastic audience, with Sullivan conducting. Soon, however, the piece suffered from weak ticket sales, generally ascribed to a heat wave that made the Opera Comique particularly uncomfortable. Historian Michael Ainger questions this explanation, at least in part, stating that the heat waves in the summer of 1878 were short and transient. In any case, by mid-August, Sullivan wrote to his mother that cooler weather had arrived, which was good for the show. In the meantime, the four partners of the Comedy Opera Company lost confidence in the opera's viability and posted closing notices. Carte publicised the piece by presenting a matinee concert performance on 6 July 1878 at the enormous Crystal Palace.
In late August 1878, Sullivan used some of the "Pinafore" music, arranged by his assistant Hamilton Clarke, during several successful promenade concerts at Covent Garden that generated interest and stimulated ticket sales. By September, "Pinafore" was playing to full houses at the Opera Comique. The piano score sold 10,000 copies, and Carte soon sent two additional companies out to tour in the provinces.
Carte, Gilbert and Sullivan now had the financial resources to produce shows themselves, without outside backers. Carte persuaded the author and composer that a business partnership among the three would be to their advantage, and they hatched a plan to separate themselves from the directors of the Comedy Opera Company. The contract between Gilbert and Sullivan and the Comedy Opera Company gave the latter the right to present "Pinafore" for the duration of the initial run. The Opera Comique was obliged to close for drain and sewer repairs, and was renovated and redecorated by E. W. Bradwell, from Christmas 1878 to the end of January 1879. Gilbert, Sullivan and Carte believed that this break ended the initial run, and, therefore, ended the company's rights. Carte put the matter beyond doubt by taking a six-month personal lease of the theatre beginning on 1 February 1879, the date of its re-opening, when "Pinafore" resumed. At the end of the six months, Carte planned to give notice to the Comedy Opera Company that its rights in the show and the theatre had ended.
Meanwhile, numerous versions of "Pinafore", unauthorised by its creators, began playing in America with great success, beginning with a production in Boston that opened on 25 November 1878. "Pinafore" became a source of popular quotations on both sides of the Atlantic, such as the exchange:
In February 1879, "Pinafore" resumed operations at the Opera Comique. The opera also resumed touring in April, with two companies crisscrossing the British provinces by June, one starring Richard Mansfield as Sir Joseph, the other W. S. Penley in the role. Hoping to join in on the profits to be made in America from "Pinafore", Carte left in June for New York to make arrangements for an "authentic" production there to be rehearsed personally by the author and composer. He arranged to rent a theatre and auditioned chorus members for the American production of "Pinafore" and a new Gilbert and Sullivan opera to be premiered in New York, and for tours.
Sullivan, as had been arranged with Carte and Gilbert, gave notice to the partners of the Comedy Opera Company in early July 1879 that he, Gilbert and Carte would not be renewing the contract to produce "Pinafore" with them and that he would be withdrawing his music from the Comedy Opera Company on 31 July. In return, the Comedy Opera Company gave notice that they intended to play "Pinafore" at another theatre and brought a legal action against Carte and company. They offered the London and touring casts of "Pinafore" more money to play in their production, and although some choristers accepted their offer, only one principal player, Mr Dymott, accepted. They engaged the Imperial Theatre but had no scenery. On 31 July, they sent a group of thugs to seize the scenery and props during Act II of the evening performance at the Opera Comique. Gilbert was away, and Sullivan was recovering from an operation for kidney stones. Stagehands and cast members managed to ward off their backstage attackers and protect the scenery, although the stage manager, Richard Barker, and others, were injured. The cast went on with the show until someone shouted "Fire!" George Grossmith, playing Sir Joseph, went before the curtain to calm the panicked audience. The police arrived to restore order, and the show continued. Gilbert sued to stop the Comedy Opera Company from staging their rival production of "H.M.S. Pinafore". The court permitted the production to go on at the Imperial, beginning on 1 August 1879, and it transferred to the Olympic Theatre in September. Pauline Rita was one of a series of Josephines. The production received good notices and initially sold well but was withdrawn in October after 91 performances. The matter was eventually settled in court, where a judge ruled in Carte's favour about two years later.
After his return to London, Carte formed a new partnership with Gilbert and Sullivan to divide profits equally after the expenses of each of their shows. Meanwhile, "Pinafore" continued to play strongly. On 20 February 1880, "Pinafore" completed its initial run of 571 performances. Only one other work of musical theatre in the world had ever run longer, Robert Planquette's operetta "Les cloches de Corneville".
Taking "Pinafore" to the United States.
Approximately 150 unauthorised productions of "Pinafore" sprang up in the United States in 1878 and 1879, and none of these paid royalties to the authors. Gilbert and Sullivan called them "pirated", although the creators did not have any international copyright protection. The first of these productions, opening at the Boston Museum on 25 November 1878, made such a splash that the piece was quickly produced in major cities and on tour by dozens of companies throughout the country. Boston alone saw at least a dozen productions, including a juvenile version described by Louisa May Alcott in her 1879 story, "Jimmy's Cruise in the Pinafore". In New York, different productions of the piece played simultaneously in eight theatres within five blocks of each other and in six theatres in Philadelphia.
These unauthorised performances took many forms, including burlesques, productions with men playing women's roles and vice versa, spoofs, variety acts, Minstrel show versions, all-black and Catholic productions, German, Yiddish and other foreign-language versions, performances on boats or by church choirs, and productions starring casts of children. Few purported to play the opera as written. Sheet music arrangements were popular, there were "Pinafore"-themed dolls and household items, and references to the opera were common in advertising, news and other media. Gilbert, Sullivan and Carte brought lawsuits in the U.S. and tried for many years to control the American performance copyrights over their operas, or at least to claim some royalties, without success. They made a special effort to claim American rights for their next work after "Pinafore", "The Pirates of Penzance", by giving the official premiere in New York.
Gilbert, Sullivan and Carte met by 24 April 1879 to make plans for a production of "Pinafore" in America. Carte travelled to New York in the summer of 1879 and made arrangements with theatre manager John T. Ford to present, at the Fifth Avenue Theatre, the first authorised American production of "Pinafore". In November, he returned to America with Gilbert, Sullivan and a company of strong singers, including J. H. Ryley as Sir Joseph, Blanche Roosevelt as Josephine, Alice Barnett as Little Buttercup, Furneaux Cook as Dick Deadeye, Hugh Talbot as Ralph Rackstraw and Jessie Bond as Cousin Hebe. To these, he added some American singers, including Signor Brocolini as Captain Corcoran. Alfred Cellier came to assist Sullivan, while his brother François remained in London to conduct "Pinafore" there.
"Pinafore" opened in New York on 1 December 1879 (with Gilbert onstage in the chorus) and ran for the rest of December. After a reasonably strong first week, audiences quickly fell off, since most New Yorkers had already seen local productions of "Pinafore". This was unexpected and forced Gilbert and Sullivan to race to complete and rehearse their new opera, "The Pirates of Penzance", which premièred with much success on 31 December. Shortly thereafter, Carte sent three touring companies around the United States East Coast and Midwest, playing "Pinafore" alongside "Pirates".
Children's production.
The unauthorised juvenile productions of "Pinafore" were so popular that Carte mounted his own children's version, played at matinees at the Opera Comique beginning on 16 December 1879. François Cellier, who had taken over from his brother as Carte's music director in London, adapted the score for children's voices. Between its two Christmas seasons in London, the children's production went on a provincial tour from 2 August 1880 to 11 December 1880.
Carte's children's production earned enthusiastic reviews from critic Clement Scott and the other London critics, as well as the audiences, including children. However, Captain Corcoran's curse "Damme!" was uncensored, shocking such prominent audience members as Lewis Carroll, who later wrote: "a bevy of sweet innocent-looking girls sing, with bright and happy looks, the chorus 'He said, Damn me! He said, Damn me!' I cannot find words to convey to the reader the pain I felt in seeing those dear children taught to utter such words to amuse ears grown callous to their ghastly meaning ... How Mr. Gilbert could have stooped to write, or Sir Arthur Sullivan could have prostituted his noble art to set to music, such vile trash, it passes my skill to understand".
Subsequent productions.
After the opera became successful in London, Richard D'Oyly Carte quickly sent touring companies into the British provinces. At least one D'Oyly Carte company, and sometimes as many as three, played "Pinafore" under Carte's aegis every year between 1878 and 1888, including its first London revival in 1887. The opera was then given a rest, returning to the touring repertory between 1894 and 1900 and again for most of the time between 1903 and 1940. Gilbert directed all the revivals during his lifetime, and after his death, the D'Oyly Carte Opera Company had exclusive performing rights to the Savoy operas until 1962. It continued to hew closely to Gilbert's directions throughout that period, as recorded in Gilbert's prompt books, and it also required its licensees to follow them closely.
Until 1908, revivals of the opera were given in contemporary dress, with ladies' costumes executed by couture houses such as Redfern. After that, designers such as Percy Anderson, George Sheringham and Peter Goffin created Victorian costume designs. The 1887 set was designed by Hawes Craven. In the winter of 1940–41, the D'Oyly Carte Opera Company's scenery and costumes for "Pinafore" and three other operas were destroyed by German bombs during World War II. The opera was revived in London in the summer of 1947. It was then included in the D'Oyly Carte repertory in every season from then on, until the company's closure in 1982. The D'Oyly Carte company performed "Pinafore" before Queen Elizabeth II and the royal family at Windsor Castle on 16 June 1977, during the queen's Silver Jubilee year, the first royal command performance of a Gilbert and Sullivan opera since 1891.
The D'Oyly Carte Opera Company did not allow any other professional company to present the Savoy operas in Britain until the copyrights expired at the end of 1961, although it licensed many amateur and school societies to do so, beginning in the 19th century. After 1961, other professional companies mounted productions of the opera in Britain. These have included Tyrone Guthrie's 1960 production from Stratford, Ontario, seen on Broadway in 1960 and in London in 1962 and a New Sadler's Wells Opera Company production first seen on 4 June 1984 at Sadler's Wells Theatre, which was seen also in New York. Scottish Opera, Welsh National Opera and many of the other British opera companies have mounted productions, as did the reconstituted D'Oyly Carte Opera Company between 1990 and its closure in 2003. In recent years, the Carl Rosa Opera Company has produced "Pinafore" several times, including in 2009, and Opera della Luna and other British companies continue to mount the piece.
The extraordinary initial success of "Pinafore" in America was seen first-hand by J. C. Williamson. He soon made arrangements with D'Oyly Carte to present the opera's first authorised production in Australia, opening on 15 November 1879 at the Theatre Royal, Sydney. Thereafter, his opera company played frequent seasons of the work (and the subsequent Savoy operas) until at least 1963. In the U.S., the piece never lost popularity. The Internet Broadway Database links to forty productions on Broadway alone. Among the professional repertory companies continuing to present "Pinafore" regularly in the U.S. are Opera a la Carte, based in California, Ohio Light Opera and the New York Gilbert and Sullivan Players, which tours the opera annually and often includes it in its New York seasons. "Pinafore" is still performed around the world by opera companies such as the Royal Theatre, Copenhagen; Australian Opera (and Essgee Entertainment and others in Australia); in Kassel, Germany; and even Samarkand, Uzbekistan.
The following table shows the history of the D'Oyly Carte productions (excluding tours) in Gilbert's lifetime:
Reception.
Initial critical reception.
The early reviews were mostly favourable. "The Era" wrote:
"The Era" also lavishly praised Emma Howson as Josephine. "The Entr'acte and Limelight" commented that the opera was reminiscent of "Trial by Jury" and "Sorcerer" but found it diverting and called the music "very charming. To hear so-called grand opera imitated through the medium of the most trifling lyrics, is funny". The paper praised Grossmith as Sir Joseph, noting with amusement that he was made up to look like portraits of Horatio Nelson, "and his good introductory song seems levelled at" W. H. Smith. It opined, further, that "He Is an Englishman" is "an excellent satire on the proposition that a man must necessarily be virtuous to be English". It found the piece, as a whole, well presented and predicted that it would have a long run.
Similarly, "The Illustrated London News" concluded that the production was a success and that the plot, though slight, served as a good vehicle for Gilbert's "caustic humour and quaint satire". It found that there was "much to call forth hearty laughter in the occasional satirical hits. ... Dr. Sullivan's music is as lively as the text to which it is set, with here and there a touch of sentimental expression ... The piece is well performed throughout." The "Daily News", "The Globe", "The Times" (which particularly praised Grossmith, Barrington and Everard) and "The Standard" concurred, the last commenting favourably on the chorus acting, which, it said, "adds to the reality of the illusion". "The Times" also noted that the piece was an early attempt at the establishment of a "national musical stage" with a libretto free from risqué French "improprieties" and without the "aid" of Italian and German musical models.
"The Daily Telegraph" and the "Athenaeum", however, greeted the opera with only mixed praise. "The Musical Times" complained that the ongoing collaboration between Gilbert and Sullivan was "detrimental to the art-progress of either" because, although it was popular with audiences, "something higher is demanded for what is understood as 'comic opera'". The paper commented that Sullivan had "the true elements of an artist, which would be successfully developed were a carefully framed libretto presented to him for composition". It concluded, however, by saying how much it enjoyed the opera: "Having thus conscientiously discharged our duties as art-critics, let us at once proceed to say that "H.M.S. Pinafore" is an amusing piece of extravagance, and that the music floats it on merrily to the end". "The Times" and several of the other papers agreed that, while the piece was entertaining, Sullivan was capable of higher art. Only "The Figaro" was actively hostile to the new piece. Upon the publication of the vocal score, a review by "The Academy" joined the chorus of regret that Sullivan had sunk so low as to compose music for "Pinafore" and hoped that he would turn to projects "more worthy of his great ability". This criticism would follow Sullivan throughout his career.
The many unauthorised American productions of 1878–79 were of widely varying quality, and many of them were adaptations of the opera. One of the more "authentic" ones was the production by the Boston Ideal Opera Company, which was first formed to produce "Pinafore". It engaged well-regarded concert singers and opened on 14 April 1879 at the 3,000-seat Boston Theatre. The critics agreed that the company fulfilled its goals of presenting an "ideal" production. The "Boston Journal" reported that the audience was "wrought up by the entertainment to a point of absolute approval". The paper observed that it is a mistake to consider "Pinafore" a burlesque, "for while irresistibly comical it is not "bouffe" and requires to be handled with great care lest its delicate proportions be marred and its subtle quality of humor be lost". The "Journal" described the opera as "classical" in method and wrote that its "most exquisite satire" lay in its "imitation of the absurdities" of grand opera. The company went on to become one of the most successful touring companies in America. The first children's version in Boston became a sensation with both children and adult audiences, extending its run through the summer of 1879. The "Boston Herald" wrote that "the large audience of children and their elders went fairly wild with delight ... shrieks of laughter were repeatedly heard".
Subsequent reception.
When "Pinafore" was first revived in London in 1887, it was already treated as a classic. "The Illustrated London News" observed that the opera had not been updated with new dialogue, jokes and songs, but concluded that this was for the best, as the public would have missed the "time-honoured jokes, such as 'Hardly Ever.' The Savoy has once more got a brilliant success." "The Theatre" concurred, stating that since the opera "has been heard in almost every part of this habitable globe and been enjoyed everywhere, there is not much occasion to descant". It called the revival a "most brilliant" success and predicted another long run.
Reviewing the 1899 revival, "The Athenaeum" managed to praise the piece while joining in the musical establishment's critique of Sullivan. On the one hand, "The "Pinafore" ... sounds fresher than ever. The musical world has become serious – very serious – and it is indeed refreshing to hear a merry, humorous piece, and music, unassuming in character ... it is delicately scored, and in many ways displays ability of a high order". On the other hand, it wrote that if Sullivan had pursued the path of composing more serious music, like his symphony, "he would have produced still higher results; in like manner "Pinafore" set us wondering what the composer would have accomplished with a libretto of somewhat similar kind, but one giving him larger scope for the exercise of his gifts".
In 1911, H. L. Mencken wrote: "No other comic opera ever written – no other stage play, indeed, of any sort – was ever so popular. ... "Pinafore" ... has been given, and with great success, wherever there are theaters – from Moscow to Buenos Aires, from Cape Town to Shanghai; in Madrid, Ottawa and Melbourne; even in Paris, Rome, Vienna and Berlin." After the deaths of Gilbert and Sullivan, the D'Oyly Carte Opera Company retained exclusive rights to perform their operas in Great Britain until 1962, touring throughout Britain for most of the year and, beginning in 1919, often performing in London for a season of about four months. "The Times" gave the company's 1920 London production an enthusiastic review, saying that the audience was "enraptured", and regretting that "Pinafore" would be played for only two weeks. It praised the cast, singling out Leo Sheffield as the Captain, Henry Lytton as Sir Joseph, Elsie Griffin as Josephine, James Hay as Ralph, Bertha Lewis as Little Buttercup and the "splendid" choral tone. It concluded that the opera made a "rollicking climax to the season". Two years later, it gave an even more glowing report of that season's performances, calling Derek Oldham an "ideal hero" as Ralph, noting that Sydney Granville "fairly brought down the house" with his song, that Darrell Fancourt's Deadeye was "an admirably sustained piece of caricature" and that it was a "great pleasure" to hear the returning principals. A 1961 review of the company's "Pinafore" is much the same.
In 1879, J. C. Williamson acquired the exclusive performing rights to "Pinafore" in Australia and New Zealand. His first production earned public and critical acclaim. Williamson played Sir Joseph, and his wife, Maggie Moore played Josephine. Praising the production and all the performers, the "Sydney Morning Herald" noted that the production though "abounding in fun" was dignified and precise, that many numbers were encored and that laughter and applause from the "immense audience ... was liberally bestowed". Williamson's company continued to produce "Pinafore" in Australia, New Zealand and on tour into the 1960s with much success. As Williamson said, "If you need money, then put on G&S". Meanwhile, "Pinafore" continued to garner praise outside Britain. The 1950s Danish version in Copenhagen, for example, was revived repeatedly, playing for well over 100 performances to "packed houses". Translations into German, Yiddish and many other languages, and professional productions in places as remote as Samarkand in Uzbekistan have been successful.
In the U.S., where Gilbert and Sullivan's performance copyright was never in force, "Pinafore" continued to be produced continuously by both professional and amateur companies. "The New York Times", in a 1914 review, called a large-scale production at the 6,000-seat New York Hippodrome a "royal entertainment" that "comes up smiling". The opera had been turned into a "mammoth spectacle" at with a chorus of hundreds and the famous Hippodrome tank providing a realistic harbour. Buttercup made her entrance to the three-masted Pinafore rowing into sight, and Dick Deadeye was later thrown overboard with a real splash. The "Times" praised the hearty singing but noted that some subtlety is lost when the dialogue needs "fairly to be shouted". The production took some liberties, including interpolating music from other Sullivan works. The paper concluded, "the mild satire of "Pinafore" is entertaining because it is universal". The same paper deemed Winthrop Ames' popular Broadway productions of "Pinafore" in the 1920s and 1930s "spectacular". Modern productions in America continue to be generally well received. "The New York Times" review of The New York Gilbert and Sullivan Players' 2008 season at New York City Center commented, "Gilbert's themes of class inequality, overbearing nationalism and incompetent authorities remain relevant, however absurdly treated. But the lasting appeal of "Pinafore" and its ilk is more a matter of his unmatched linguistic genius and Sullivan's generous supply of addictive melodies."
With the expiry of the copyrights, companies around the world have been free to produce Gilbert and Sullivan works and to adapt them as they please for almost 50 years. Productions of "Pinafore", both amateur and professional, range from the traditional, in the D'Oyly Carte vein, to the broadly adapted, such as that of the very successful Essgee Entertainment (formed by Simon Gallaher) in Australia and Opera della Luna in Britain. Since its original production, "H.M.S. Pinafore" has remained one of Gilbert and Sullivan's most popular comic operas. Productions continue in large numbers around the world. In 2003 alone, The D'Oyly Carte Opera Company rented 224 sets of orchestra parts, mostly for productions of "Pinafore", "Pirates" and "Mikado". This does not take into account other rental companies and the theatre companies that borrow scores or have their own, or that use only one or two pianos instead of an orchestra. Hundreds of productions of "Pinafore" are presented every year worldwide.
Analysis.
Theatre historian John Bush Jones wrote that "Pinafore" has "everything a musical theatregoer could ask for. An engaging and even relatively suspenseful story is populated with varied and well-drawn characters who speak and sing witty, literate, and often outrageously funny dialogue and lyrics has a score that ... has plenty of tunes for the audience to go away humming". Sir George Power, the tenor who created the role of Ralph Rackstraw, opined in later life that the secret of the success of the Savoy operas is the way in which "Sullivan entered into the spirit of Gilbert's topsy-turvy humour, and was pompous when Gilbert was sprightly, or, when Gilbert's satire was keenest and most acid, consciously wallowed in sentiment." Another commentator has suggested that the opera's enduring success lies in its focus on "mirth and silliness". Even the title of the piece is silly, applying the name of a little girl's garment, a pinafore, to the fearsome symbol of a naval warship, which usually bore names like "Victory", "Goliath", "Audacious" and "Minotaur".
Satiric and comic themes.
Biographer Jane Stedman wrote that "Pinafore" is "satirically far more complex" than "The Sorcerer". She commented that Gilbert uses several ideas and themes from his Bab Ballads, including the idea of gentlemanly behaviour of a captain towards his crew from "Captain Reece" (1868) and the exchange of ranks due to exchange at birth from "General John" (1867). Dick Deadeye, based on a character in "Woman's Gratitude" (1869), represents another of Gilbert's favorite (and semi-autobiographical) satiric themes: the misshapen misanthrope whose forbidding "face and form" makes him unpopular although he represents the voice of reason and common sense. Gilbert also borrows from his 1870 opera, "The Gentleman in Black" which includes the device of baby-switching.
Historian H. M. Walbrook wrote in 1921 that "Pinafore" "satirizes the type of nautical drama of which Douglas Jerrold's "Black-Eyed Susan" is a typical instance, and the 'God's Englishman' sort of patriotism which consists in shouting a platitude, striking an attitude, and doing little or nothing to help one's country". G. K. Chesterton agreed that the satire is pointed at the selfishness of "being proud of yourself for being a citizen" of one's country, which requires no virtuous effort of will to resist the "temptations to belong to other nations" but is merely an excuse for pride. In 2005, Australian opera director Stuart Maunder noted the juxtaposition of satire and nationalism in the opera, saying, "they all sing 'He is an Englishman', and you know damn well they're sending it up, but the music is so military ... that you can't help but be swept up in that whole jingoism that is the British Empire." In addition, he argued that the song ties this theme into the main satire of class distinctions in the opera: ""H.M.S. Pinafore" is basically a satire on ... the British love of the class system. ... course [Ralph can marry Captain's daughter, because he's British, and therefore he's great'". Jacobs notes that Gilbert is lampooning the tradition of nautical melodrama in which the sailor's "patriotism guarantees his virtue".
One of Gilbert's favourite comic themes is the elevation of an unqualified person to a position of high responsibility. In "The Happy Land" (1873), for example, Gilbert describes a world in which government offices are awarded to the person who has the least qualification to hold each position. In particular, the one who has never heard of a ship is appointed to the cabinet post of First Lord of the Admiralty. In "Pinafore", Gilbert revisits this theme in the character of Sir Joseph, who rises to the same position by "never goto sea". In later Gilbert and Sullivan operas, the characters Major-General Stanley in "Pirates", and Ko-Ko in "The Mikado", are similarly appointed to high office though lacking the necessary qualifications. Gilbert also pokes fun at party politics, implying that when Sir Joseph "always voted at [his party's call", he sacrificed his personal integrity. The "commercial middle class" (which was Gilbert's main audience) is treated as satirically as are social climbers and the great unwashed. In addition, the apparent age difference between Ralph and the Captain, even though they were babies nursed together, satirises the variable age of Thaddeus in "The Bohemian Girl". "The Times" wrote, in reviewing the 1929 production, that "Pinafore" was quintessentially Gilbertian in that the absurdities of a "paternal" Captain and the "ethics ... of all romanticism" are accepted "unflinchingly" and taken to their logical conclusion: "It is the reference to actuality that is essential; without it, the absurdity will not stand starkly out".
A theme that pervades the opera is the treatment of love across different social ranks. In the previous Gilbert and Sullivan opera, "The Sorcerer", a love potion causes trouble by inducing the villagers and wedding guests to fall in love with people of different social classes. In "Pinafore", the captain's daughter, Josephine, loves and is loved by a common sailor, but she dutifully tells him, "your proffered love I haughtily reject". He expresses his devotion to her in a poetic and moving speech that ends with "I am a British sailor, and I love you". It finally turns out that he is of a higher rank than she. This is a parody of the Victorian "equality" drama, such as Lord Lytton's "The Lady of Lyons" (1838), where the heroine rejects a virtuous peasant who makes a similarly moving speech, ending with "I am a peasant!" It then turns out that he has become her social superior. Furthermore, in "Pinafore", Sir Joseph assures Josephine that "love levels all ranks". In Tom Taylor's "The Serf", the heroine again loves a worthy peasant who turns out to be of high rank, and she declares happily at the end that "love levels all". In a satire of the libertarian traditions of nautical melodrama, Sir Joseph tells the crew of the Pinafore that they are "any man's equal" (excepting his), and he writes a song for them that glorifies the British sailor. Conversely, he brings the proud captain down a notch by making him "dance a hornpipe on the cabin table". Jones notes that the union between Ralph and Josephine "becomes acceptable only through the absurd second-act revelation of Buttercup's inadvertent switching of the infants" and concludes that Gilbert is a "conservative satirist ultimately advocated preserving the status quo ... [and set out to show love definitely "does not" level all ranks".
There is a divide among Gilbert and Sullivan scholars as to whether Gilbert is, as Jones argues, a supporter of the status quo whose focus is merely to entertain or, on the other hand, predominantly to satirise and protest "against the follies of his age". Gilbert scholar Andrew Crowther posits that this disagreement arises from Gilbert's "techniques of inversion – with irony and topsyturvydom", which lead to "the surface meaning of his writings" being "the opposite of their underlying meaning". Crowther argues that Gilbert desires to "celebrate" society's norms while, at the same time, satirising these conventions. In "Pinafore", which established many patterns for the later Savoy operas, Gilbert found a way to express his own conflict that "also had tremendous appeal to the general public". He creates "a highly intelligent parody of nautical melodrama ... controlled by the conventions it mocks". While nautical melodrama exalts the common sailor, in "Pinafore" Gilbert makes the proponent of equality, Sir Joseph, a pompous and misguided member of the ruling class who, hypocritically, cannot apply the idea of equality to himself. The hero, Ralph, is convinced of his equality by Sir Joseph's foolish pronouncements and declares his love for his Captain's daughter, throwing over the accepted "fabric of social order". At this point, Crowther suggests, the logic of Gilbert's satiric argument should result in Ralph's arrest. But to satisfy convention, Gilbert creates an obvious absurdity: the captain and Ralph were switched as babies. By an "accident of birth", Ralph is suddenly an appropriate husband for Josephine, and both the social order and the desire for a romantic happy ending are satisfied at once. Crowther concludes, "We have an opera which uses all the conventions of melodrama and ridicules them; but in the end it is difficult to see which has won out, the conventions or the ridicule." Thus, "Pinafore" found broadbased success by appealing to the intellectual theatregoer seeking satire, the middle-class theatre-goer looking for a comfortable confirmation of the "existing social order" and the working-class audience who saw a satisfying melodramatic victory for the common man.
Songs and musical analysis.
According to musicologist Arthur Jacobs, Gilbert's plot "admirably sparked off Sullivan's genius". Sullivan embraces the nautical setting; in "We Sail the Ocean Blue", for example, he "presents his twist on a traditional sea shanty". In the Captain's opening song, "I am the Captain of the Pinafore", he admits that his gentlemanliness "never ... well, hardly ever" gives way to swearing at his men, and although he has experience at sea, he "hardly ever" suffers from seasickness. Sullivan "unerringly found the right musical setting for the key phrase 'What never?' ... cunningly sharpened ... through the chromatic touch on the bassoon." Audrey Williamson argued that the music of "Pinafore" is quintessentially English and free of European influences throughout most of the score, from the "glee" for Ralph, the Boatswain and the Carpenter, to "For He Is an Englishman".
The best-known songs from the opera include "I'm called Little Buttercup", a waltz tune introducing the character, which Sullivan repeats in the entr'acte and in the Act II finale to imprint the melody on the mind of the audience; and "A British tar" (a glee for three men describing the ideal sailor), composed by Sir Joseph "to encourage independent thought and action in the lower branches of the service, and to teach the principle that a British sailor is any man's equal, excepting mine". Sullivan's voicing advances the satiric lyric, which mocks the "equality" plays while underlining the hypocrisy of Sir Joseph. Another popular number is Sir Joseph's song "When I was a Lad", recounting the meteoric rise of his career, which bears similarities to that of W. H. Smith, the civilian news entrepreneur who had risen to the position of First Lord of the Admiralty in 1877.
In "Pinafore", Sullivan exploits minor keys for comic effect, for instance in "Kind Captain, I've important information". Further, he achieves a musical surprise when he uses the subdominant minor in "Sorry her lot". Biographer Gervase Hughes was impressed with the introduction to the opening chorus which includes "a rousing nautical tune ... in a key of no nonsense, C major ... a modulation to the mediant minor, where to our surprise a plaintive oboe gives us the first verse of "Sorry her lot" in 2/4 . After this closes on the local dominant B major the violins (still in 2/4) introduce us to Little Buttercup ... meeting her under these conditions one would hardly expect her to blossom out later as a queen of the waltz." He continues, "the bassoon and basses ... assert vigorously who is the Captain of the Pinafore ... in the improbable key of A flat minor. ... Buttercup makes a last despairing attempt to make herself heard in D flat minor, but the others have never known that such an outlandish key existed. So in a flash they all go back to C major on a good old 6/4".
According to Jacobs, "Ralph, Captain Corcoran, Sir Joseph and Josephine all live in their interactive music (particularly 'Never mind the why and wherefore'), and almost as much musical resource is lavished on two characters parodied from opera or melodrama, Little Buttercup with 'gypsy blood in her veins' and the heavy-treading Dick Deadeye." Jacobs also opined that the leading tone that begins "Never mind the why and wherefore" "serves to emphasize the phrase like a Johann Strauss-ian grace-note". Sullivan scholar David Russell Hulme noted Sullivan's parody of operatic styles, "particularly the Handelian recitatives and the elopement scene (evocative of so many nocturnal operatic conspiracies), but best of all is the travesty of the patriotic tune in 'For he is an Englishman!'" Buttercup's Act II song, in which she reveals the dark secret of the baby-switching is preceded by a quote from Franz Schubert's "The Erl-King" and also parodies the opera "Il Trovatore". Jacobs notes that Sullivan also adds his own humorous touches to the music by setting commonplace expressions in "Donizettian recitative". But on the serious side, he enhances the moments of true emotional climax, as in Josephine's Act II aria, and added musical interest to concerted numbers by "subtly shifting the rhythms and bar groupings."
Revisions and cut material.
Ballad for Captain Corcoran, "Reflect, my child".
During rehearsals for the original production, Gilbert added a ballad for Captain Corcoran in which he urged his daughter to forget the common sailor with whom she is in love, because "at every step, he would commit solecisms that society would never pardon." The ballad was meant to be sung between No. 5 and No. 6 of the current score, but it was cut before opening night. The words survive in the libretto that was deposited with the Lord Chamberlain for licensing. Before 1999, all that was known to survive of Sullivan's setting was a copy of the leader violin part.
In April 1999, Sullivan scholars Bruce I. Miller and Helga J. Perry announced that they had discovered a nearly complete orchestration – lacking only the second violin part – in a private collection of early band parts. These materials, with a conjectural reconstruction of the partially lost vocal lines and second violin part, were later published and professionally recorded. This piece has now been performed a number of times by amateur and professional companies, although it has not become a standard addition to the traditional scores or recordings.
Dialogue for Cousin Hebe.
In the licensing copy of the libretto, Sir Joseph's cousin Hebe had lines of dialogue in several scenes in Act II. In the scene that follows No. 14 ("Things are seldom what they seem"), she accompanied Sir Joseph onstage and echoed the First Lord's dissatisfaction with Josephine. After several interruptions, Sir Joseph urged her to be quiet, eliciting the response "Crushed again!" Gilbert would later re-use this passage for Lady Jane in "Patience". Hebe was also assigned several lines of dialogue after No. 18 ("Carefully on tiptoe stealing") and again after No. 19 ("Farewell, my own").
Late in rehearsals for the original production, Jessie Bond assumed the role of Hebe, replacing Mrs Howard Paul. Bond, who at this point in her career was known primarily as a concert singer and had little experience as an actress, did not feel capable of performing dialogue, and these passages were revised to cut Hebe's dialogue. Hebe's dialogue is occasionally restored in modern performances, particularly her lines in the scene following No. 14.
Recitative preceding the Act II finale.
The dialogue preceding the Act II finale, starting with "Here, take her sir, and mind you treat her kindly", was originally recitative. The music for this passage was printed in the first edition of the vocal score as No. 20a. Shortly after opening night, the recitative was dropped, and the lines thereafter were performed as spoken dialogue. In modern productions, the recitative is occasionally restored in place of the dialogue.
Recordings.
There have been numerous recordings of "Pinafore" since 1907. Ian Bradley counted seventeen recordings of the opera available on CD in 2005.
The 1930 recording is notable for preserving the performances of the D'Oyly Carte Opera Company stars of the era. The 1960 D'Oyly Carte recording, which contains all the dialogue, has been repeatedly praised by reviewers. The 1994 Mackerras recording, featuring grand opera singers in the principal roles, is musically well-regarded. The 2000 D'Oyly Carte recording also contains complete dialogue and the first recording of the "lost" ballad for Captain Corcoran, "Reflect, my child", as a bonus track. A 1957 Danish-language recording of the opera is one of the few foreign-language professional recordings of Gilbert and Sullivan.
In 1939, "Pinafore" was chosen by NBC as one of the earliest operas ever broadcast on American television, but no recording appears to have been saved. The 1973 D'Oyly Carte video recording, directed by Michael Heyland, demonstrates the company's staging of the period, but some reviewers find it dull. It is, however, one of only three video or film recordings of a Gilbert and Sullivan opera by the D'Oyly Carte Opera Company. The 1982 video of "Pinafore" is considered one of the worst of the Brent Walker Productions series of Gilbert and Sullivan television productions. More recent professional productions have been recorded on video by the International Gilbert and Sullivan Festival.
Adaptations.
"H.M.S. Pinafore" has been adapted many times. W. S. Gilbert wrote a 1909 children's book called "The Pinafore Picture Book", illustrated by Alice Woodward, which retells the story of "Pinafore", in some cases giving considerable backstory that is not found in the libretto. Many other children's books have since been written retelling the story of "Pinafore" or adapting characters or events from "Pinafore".
Many musical theatre adaptations have been produced since the original opera. Notable examples include a 1945 Broadway musical adapted by George S. Kaufman, called "Hollywood Pinafore", using Sullivan's music. This was revived several times, including in London in 1998. Another 1945 Broadway musical adaptation, "Memphis Bound!", was written by Don Walker and starred Bill Robinson and an all-black cast. In 1940, the American Negro Light Opera Association produced the first of several productions set in the Caribbean Sea, "Tropical Pinafore". An early Yiddish adaptation of "Pinafore", called "Der Shirtz" (Yiddish for "apron") was written by Miriam Walowit in 1952 for a Brooklyn, New York Hadassah group, and they recorded 12 of the songs. In the 1970s, Al Grand was inspired by this recording and urged the Gilbert and Sullivan Long Island Light Opera Company to perform these songs. He later translated the missing songs and dialogue, with Bob Tartell, and the show has been toured widely under the name "Der Yiddisher Pinafore". The group have continued to produce this adaptation for over two decades, in which "He is an Englishman" becomes "Er Iz a Guter Yid" ("He is a good Jew").
Essgee Entertainment produced an adapted version of "Pinafore" in 1997 in Australia and New Zealand that has been much revived. Another musical adaptation is "Pinafore! (A Saucy, Sexy, Ship-Shape New Musical)", adapted by Mark Savage. It was first performed at the Celebration Theater in Los Angeles, California on 7 September 2001, directed by Savage, where it ran with great success for nine months. It then played in Chicago and New York in 2003. In this adaptation, only one character is female, and all but one of the male characters are gay. An original cast recording was issued in 2002 by Belva Records. "Pinafore Swing" is a musical with music arranged by Sarah Travis. It premiered at the Watermill Theatre in England in 2004 in a production directed by John Doyle. The adaptation, set in 1944, changes the characters into members of a band entertaining the sailors on a World War II troop ship in the Atlantic. The reduced-size acting cast also serve as the orchestra for the singing roles, and the music is infused with swing rhythms. Numerous productions in recent decades have been set to parody "Star Trek" or "Star Wars".
Cultural impact.
Among its other influences on popular culture, "Pinafore" had perhaps its most profound influence on the development of musical theatre. According to theatre historian John Kenrick, "Pinafore" "became an international sensation, reshaping the commercial theater in both England and the United States." Music writer Andrew Lamb notes, "The success of "H.M.S. Pinafore" in 1879 established British comic opera alongside French opéra bouffe throughout the English-speaking world". Historian John Bush Jones opines that "Pinafore" and the other Savoy operas demonstrate that musical theatre "can address contemporary social and political issues without sacrificing entertainment value" and that "Pinafore" created the model for a new kind of musical theatre, the "integrated" musical, where "book, lyrics, and music combined to form an integral whole". He adds that its "unprecedented ... popularity fostered an American audience for musical theatre, while the show itself became a model for form, content, and even intention of ... musicals ever since, especially socially relevant musicals." Its popularity also led to the musical theatre adaptations of "Pinafore" described above, musicals in which the story line involves a production of "Pinafore" and other musicals that parody the opera or that use or adapt its music. The first such parody was a short-lived burlesque presented at the Opera Comique in 1882, called "The Wreck of the Pinafore" by H. Lingard and Luscombe Searelle; the opera's characters are shipwrecked on a desert island. It was described by "The Era" as "chiefly remarkable for its impudence".
Likewise, the opera's popularity has led to the widespread parody and pastiche of its songs in politics, literature and films, on television and in a variety of other media. Many comedians have used "Pinafore" songs for comic and satiric effect. For example, in his comedy album "My Son, the Celebrity", Allan Sherman parodies "When I Was a Lad" from the point of view of a young man who goes to an Ivy League school and then rises to prominence in business. At the end of the song, he "thanks old Yale", "thanks the Lord" and thanks his father, "who is chairman of the board". Literary references to "Pinafore" songs include Harris's attempt to sing "When I Was a Lad" in Jerome K. Jerome's "Three Men in a Boat". Another is found in the story "Runaround" from "I, Robot" by Isaac Asimov, where a robot sings part of "I'm Called Little Buttercup". "Pinafore" and its songs have been performed by rock musicians such as Todd Rundgren, Taj Mahal and Michele Gray Rundgren, who performed "Never Mind the Why and Wherefore" on "Night Music" ("Sunday Night") in 1989.
Political references include a 1996 satiric pastiche of "When I Was a Lad" aimed at Tony Blair by Virginia Bottomley, heritage secretary under John Major. Sporting references include a racehorse named "H.M.S. Pinafore". "Pinafore" songs and images have been used extensively in advertising. According to Jones, ""Pinafore" launched the first media blitz in the United States" beginning in 1879, and recent ads include a television campaign for Terry's Chocolate Orange featuring a pastiche of "When I Was a Lad". "Pinafore"-themed merchandise includes trading cards that were created in the 1880s.
In recent decades, songs from "Pinafore" have been used frequently to give period flavor to films. Prominent examples include the 1981 historical film "Chariots of Fire", in which the protagonist, Harold Abrahams, and others from Cambridge University, sing "He Is an Englishman". This song also features at the end of the 1983 BBC drama "An Englishman Abroad". In the 2003 movie "Peter Pan", the Darling family sings "When I Was a Lad". In "Wyatt Earp" (1994), the famed lawman meets his future wife when he sees her playing in an early production of "Pinafore". A 1953 biopic, "The Story of Gilbert and Sullivan", uses music from "Pinafore".
Characters also sing songs from "Pinafore" in such popular films as "Raiders of the Lost Ark" (1981) and "" (1998), where Captain Picard and Lt. Commander Worf sing part of "A British Tar" to distract a malfunctioning Lt. Commander Data. Likewise, in "The Good Shepherd" (2006), which depicts an all-male version of "Pinafore" at Yale University, the Matt Damon character plays Little Buttercup, singing her song in falsetto. Judy Garland sings "I Am the Monarch of the Sea" in the 1963 film, "I Could Go On Singing". The soundtrack of the 1992 thriller "The Hand that Rocks the Cradle" prominently features songs and music from "Pinafore", and the father and daughter characters sing "I Am the Captain of the Pinafore" together. An example of a film based on ideas from "Pinafore" is the 1976 animated film by Ronald Searle called "Dick Deadeye, or Duty Done" is based on the character and songs from "Pinafore". In the 1988 drama "Permanent Record", a high school class performs "Pinafore".
Television series that include substantial "Pinafore" references include "The West Wing", for example in the 2000 episode "And It's Surely to Their Credit", where "He Is an Englishman" is used throughout and quoted (or paraphrased) in the episode's title. Among other notable examples of the use of songs from "Pinafore" on television are several popular animated shows. In the "Cape Feare" episode of "The Simpsons", Bart stalls his would-be killer Sideshow Bob with a "final request" that Bob sing him the entire score of "Pinafore". Similarly, the 1993 "HMS Yakko" episode of "Animaniacs" consists of pastiches of songs from "H.M.S. Pinafore" and "The Pirates of Penzance". In a "Family Guy" episode, "The Thin White Line" (2001), Stewie sings a pastiche of "My Gallant Crew". Stewie also sings "I Am the Monarch of the Sea" (including the ladies' part, in falsetto) in "". A 1986 Mr. Belvedere episode, "The Play", concerns a production of "H.M.S. Pinafore", and several of the songs are performed. In the 2009 episode "Broken" of "House", Dr. House must take a urine test and starts singing "He is an Englishman" to cover the sound. In 1955, NBC broadcast a variety special including a 20-minute compressed jazz version, "H.M.S. Pinafore in Jazz", produced and directed by Max Liebman, starring Perry Como, Buddy Hackett, Kitty Kallen, Bill Hayes, Pat Carroll and Herb Shriner.
Historical casting.
The following tables show the most prominent cast members of significant D'Oyly Carte Opera Company productions and tours at various times through to the company's 1982 closure:
1 The Midshipmite, Tom Tucker, is traditionally played by a child. "Fitzaltamont" was likely a pseudonym used to protect the child's identity, as the same name appears on programmes of several provincial touring companies. No names are listed for his role in later productions.

</doc>
<doc id="47804" url="https://en.wikipedia.org/wiki?curid=47804" title="Extracorporeal shock wave lithotripsy">
Extracorporeal shock wave lithotripsy

Extracorporeal shock wave lithotripsy (ESWL) is a non-invasive treatment of kidney stones (urinary calculosis) and biliary calculi (stones in the gallbladder or in the liver) using an acoustic pulse. It is also reported to be used for salivary stones. and pancreatic stones [https://www.nzma.org.nz/journal/read-the-journal/all-issues/2010-2019/2012/vol-125-no-1361/cc-hayes]
It is estimated that more than one million patients are treated annually with ESWL in the USA alone.
History.
It was first used by physicians from the university hospital Großhadern (Munich, Germany) and technicians from Dornier System (Friedrichshafen, Germany) in 1980. The device used is now displayed in the Deutsches Medizinhistorischen Museum in Ingolstadt.
Non-invasive treatment.
The lithotriptor attempts to break up the stone with minimal collateral damage by using an externally applied, focused, high-intensity acoustic pulse. The sedated or anesthetized patient lies down in the apparatus' bed, with the back supported by a water-filled coupling device placed at the level of kidneys. A fluoroscopic x-ray imaging system or an ultrasound imaging system is used to locate the stone and aim the treatment. The first generation lithotriptor known as the Dornier HM3 (Human Model 3), has a half ellipsoid-shaped piece that opens toward the patient. The acoustic pulse is generated at the ellipsoidal focal point that is furthest from the patient and the stone positioned at the opposite focal point receives the focused shock wave. The treatment usually starts at the equipment's lowest power level, with a long gap between pulses, in order to accustom the patient to the sensation. The length of gap between pulses is also controlled to allow cavitation bubbles to disperse minimizing tissue damage. Second and later generation machines use an acoustic lens to focus the shock wave. This functions much like an optical lens, focusing the shock wave at the desired loci. The frequency of pulses are currently left at a slow rate for more effective comminution of the stone and to minimize morbidity while the power levels are then gradually increased, so as to break up the stone. The final power level usually depends on the patient's pain threshold and the observed success of stone breakage. If the stone is positioned near a bone (usually a rib in the case of kidney stones), this treatment may be more uncomfortable because the shock waves can cause a mild resonance in the bone which can be felt by the patient. The sensation of the treatment is likened to an elastic band twanging off the skin. Alternatively the patient may be sedated during the procedure. This allows the power levels to be brought up more quickly and a much higher pulse frequency, often up to 120 shocks per minute.
The successive shock wave pressure pulses result in direct shearing forces, as well as cavitation bubbles surrounding the stone, which fragment the stones into smaller pieces that then can easily pass through the ureters or the cystic duct. The process takes about an hour. A ureteral stent (a kind of expandable hollow tube) may be used at the discretion of the urologist. The stent allows for easier passage of the stone by relieving obstruction and through passive dilatation of the ureter.
Extracorporeal lithotripsy works best with stones between 4 mm and 20 mm (0.4 cm and 2.0 cm) in diameter that are still located in the kidney. ESWL can be used to break up stones located in ureters as well, but with a lower rate of success.
The patients undergoing this procedure can, in some cases, see for themselves the progress of their treatment. If allowed to view the ultrasound or x-ray monitor, they may be able to see their stones change from a distinct bright point (or dark spot depending on whether the fluoro unit is set up in native or bones white) to a fuzzy cloud as the stone is disintegrated into a fine powder.
ESWL is the least invasive of the commonplace modalities for definitive stone treatment, but provides a lower stone-free rate than other more invasive treatment methods, such as ureteroscopic manipulation with laser lithotripsy or percutaneous nephrolithotomy (PCNL) or retrogade intrarenal surgery (RIRS). The passage of stone fragments may take a few days or a week and may cause mild to extreme pain depending on the patient and the success of the operation. Patients may be instructed to drink as much water as practical during this time. Patients are also advised to void through a stone screen in order to capture stone fragments for analysis.
ESWL is not without risks. The shock waves themselves, as well as cavitation bubbles formed by the agitation of the urine medium, can lead to capillary damage, renal parenchymal or subcapsular hemorrhage. This can lead to long-term consequences such as renal failure and hypertension. Overall complication rates of ESWL range from 5–20%. Occasionally, patients have experienced infections and thus are advised by medical professionals to obtain medical help as soon as possible if they develop a fever.

</doc>
<doc id="47805" url="https://en.wikipedia.org/wiki?curid=47805" title="Vector quantization">
Vector quantization

Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.
The density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.
Vector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.
Training.
A simple training algorithm for vector quantization is:
A more sophisticated algorithm reduces the bias in the density matching estimation, and ensures that all points are used, by including an extra sensitivity parameter:
It is desirable to use a cooling schedule to produce convergence: see Simulated annealing. Another (simpler) method is LBG which is based on K-Means.
The algorithm can be iteratively updated with 'live' data, rather than by picking random points from a data set, but this will introduce some bias if the data are temporally correlated over many samples.
A vector is represented either geometrically by an arrow whose length corresponds to its magnitude and points in an appropriate direction, or by two or three numbers representing the magnitude of its components.
Applications.
Vector quantization is used for lossy data compression, lossy data correction, pattern recognition, density estimation and clustering.
Lossy data correction, or prediction, is used to recover data missing from some dimensions. It is done by finding the nearest group with the data dimensions available, then predicting the result based on the values for the missing dimensions, assuming that they will have the same value as the group's centroid.
For density estimation, the area/volume that is closer to a particular centroid than to any other is inversely proportional to the density (due to the density matching property of the algorithm).
Use in data compression.
Vector quantization, also called "block quantization" or "pattern matching quantization" is often used in lossy data compression. It works by encoding values from a multidimensional vector space into a finite set of values from a discrete subspace of lower dimension. A lower-space vector requires less storage space, so the data is compressed. Due to the density matching property of vector quantization, the compressed data has errors that are inversely proportional to density.
The transformation is usually done by projection or by using a codebook. In some cases, a codebook can be also used to entropy code the discrete value in the same step, by generating a prefix coded variable-length encoded value as its output.
The set of discrete amplitude levels is quantized jointly rather than each sample being quantized separately. Consider a "k"-dimensional vector formula_1, with "n" < "k".
All possible combinations of the "n"-dimensional vector formula_2 form the vector space to which all the quantized vectors belong.
Only the index of the codeword in the codebook is sent instead of the quantized values. This conserves space and achieves more compression.
Twin vector quantization (VQF) is part of the MPEG-4 standard dealing with time domain weighted interleaved vector quantization.
Video codecs based on vector quantization.
The usage of video codecs based on vector quantization has declined significantly in favor of those based on motion compensated prediction combined with transform coding, e.g. those defined in MPEG standards, as the low decoding complexity of vector quantization has become less relevant.
Use in pattern recognition.
VQ was also used in the eighties for speech and speaker recognition.
Recently it has also been used for efficient nearest neighbor search 
and on-line signature recognition. 
In pattern recognition applications, one codebook is constructed for each class (each class being a user in biometric applications) using acoustic vectors of this user. In the testing phase the quantization distortion of a testing signal is worked out with the whole set of codebooks obtained in the training phase. The codebook that provides the smallest vector quantization distortion indicates the identified user.
The main advantage of VQ in pattern recognition is its low computational burden when compared with other techniques such as dynamic time warping (DTW) and hidden Markov model (HMM). The main drawback when compared to DTW and HMM is that it does not take into account the temporal evolution of the signals (speech, signature, etc.) because all the vectors are mixed up. In order to overcome this problem a multi-section codebook approach has been proposed. The multi-section approach consists of modelling the signal with several sections (for instance, one codebook for the initial part, another one for the center and a last codebook for the ending part).
Use as clustering algorithm.
As VQ is seeking for centroids as density points of nearby lying samples, it can be also directly used as a prototype-based clustering method: each centroid is then associated with one prototype. 
By aiming to minimize the expected squared quantization error and introducing a decreasing learning gain fulfilling the Robbins-Monro conditions, multiple iterations over the whole data set with a concrete but fixed number of prototypes converges to the solution of k-means clustering algorithm in an incremental manner.
See also.
"Part of this article was originally based on material from the Free On-line Dictionary of Computing and is used with under the GFDL."

</doc>
<doc id="47822" url="https://en.wikipedia.org/wiki?curid=47822" title="Elk (disambiguation)">
Elk (disambiguation)

The elk ("Cervus canadensis") is a mammal very closely related to Eurasian red deer ("Cervus elaphus").
Elk may also refer to:
In wildlife:
In geography:
In other uses:

</doc>
<doc id="47836" url="https://en.wikipedia.org/wiki?curid=47836" title="Averroes">
Averroes

Ibn Rushd (; April 14, 1126 – December 10, 1198), full name (), often Latinized as Averroes (), is a medieval Andalusian polymath. He wrote on logic, Aristotelian and Islamic philosophy, theology, the Maliki school of Islamic jurisprudence, psychology, political and Andalusian classical music theory, geography, mathematics, and the mediæval sciences of medicine, astronomy, physics, and celestial mechanics. Ibn Rushd was born in Córdoba, Al Andalus (present-day Spain), and died at Marrakesh in present-day Morocco. His body was interred in his family tomb at Córdoba. The 13th-century philosophical movement based on Ibn Rushd's work is called Averroism.
Ibn Rushd was a defender of Aristotelian philosophy against Ash'ari theologians led by Al-Ghazali. Although highly regarded as a legal scholar of the Maliki school of Islamic law, Ibn Rushd's philosophical ideas were considered controversial in Ash'arite Muslim circles. Whereas al-Ghazali believed that any individual act of a natural phenomenon occurred only because God willed it to happen, Ibn Rushd insisted phenomena followed natural laws that God created.
Ibn Rushd had a greater impact on Christian Europe: he has been described as the "founding father of secular thought in Western Europe" and was known by the sobriquet the Commentator for his detailed emendations to Aristotle. Latin translations of Ibn Rushd's work led the way to the popularization of Aristotle.
Name.
Averroes is the Medieval Latin form of the Hebrew translation "Aben Rois" or "Rosh" of the Arabic Ibn Rushd. It is also seen as Averroës, Averrhoës, or Averroès to mark that the o and e are separate vowels and not an œ or diphthong. Other forms of the name include "Ibin-Ros-din", "Filius Rosadis", "Ibn-Rusid", "Ben-Raxid", "Ibn-Ruschod", "Den-Resched", "Aben-Rassad", "Aben-Rasd", "Aben-Rust", "Avenrosdy Avenryz", "Adveroys", "Benroist", "Avenroyth", and "Averroysta".
Biography.
Ibn Rushd was born in Córdoba to a family with a long and well-respected tradition of legal and public service. His grandfather Abu Al-Walid Muhammad (d. 1126) was chief judge of Córdoba under the Almoravids. His father, Abu Al-Qasim Ahmad, held the same position until the Almoravids were replaced by the Almohads in 1146.
Ibn Rushd's education followed a traditional path, beginning with studies in Hadith, linguistics, jurisprudence and scholastic theology. Throughout his life he wrote extensively on Philosophy and Religion, attributes of God, origin of the universe, Metaphysics and Psychology. It is generally believed that he was perhaps once tutored by Ibn Bajjah (Avempace). His medical education was directed under Abu Jafar ibn Harun of Trujillo in Seville.
Ibn Rushd began his career with the help of Ibn Tufail ("Aben Tofail" to the West), the author of "Hayy ibn Yaqdhan" and philosophic vizier of Almohad king Abu Yaqub Yusuf who was an amateur of philosophy and science. It was Ibn Tufail who introduced him to the court and to Ibn Zuhr ("Avenzoar" to the West), the great Muslim physician, who became Ibn Rushd's teacher and friend. Ibn Rushd's aptitude for medicine was noted by his contemporaries and can be seen in his major enduring work "Kitab al-Kulyat fi al-Tibb" (Generalities), influenced by the "Kitab al-Taisir fi al-Mudawat wa al-Tadbir" (Particularities) of Ibn Zuhr. Ibn Rushd later reported how it was also Ibn Tufail that inspired him to write his famous commentaries on Aristotle:
Ibn Rushd also studied the works and philosophy of Ibn Bajjah ("Avempace" to the West), another famous Islamic philosopher who greatly influenced his own Averroist thought.
However, while the thought of his mentors Ibn Tufail and Ibn Bajjah were mystic to an extent, the thought of Ibn Rushd was purely rationalist. Together, the three men are considered the greatest Andalusian philosophers. Ibn Rushd devoted the next 30 years to his philosophical writings.
In 1160, Ibn Rushd was made "Qadi" (judge) of Seville and he served in many court appointments in Seville, Cordoba, and Morocco during his career. Sometime during the reign of Yaqub al-Mansur, Averroes's political career was abruptly ended and he faced severe criticism from the Fuqaha (Islamic jurists) of the time.
A contemporary of Ibn Rushd, Abdelwahid al-Marrakushi writing in 1224, reported that there were secret and public reasons for his falling out of favor with Yaqub al-Mansour:
Ibn Rushd's strictly rationalist views collided with the more orthodox views of Abu Yusuf Ya'qub al-Mansur, who therefore eventually banished Ibn Rushd in 1195 and ordered his writings burned, though he had previously appointed him as his personal physician. Ibn Rushd was not allowed to return to Marrakesh until 1197, shortly before his death in the year 1198 AD. His body was returned to Córdoba for burial.
Works.
Ibn Rushd's first writings date from his age of 31 (year 1157). His works were spread over 20,000 pages covering a variety of different subjects, including early Islamic philosophy, logic in Islamic philosophy, Islamic medicine, mathematics, astronomy, Arabic grammar, Islamic theology, Sharia (Islamic law), and Fiqh (Islamic jurisprudence). In particular, his most important works dealt with Islamic philosophy, medicine and Fiqh. He wrote at least 80 original works, which included 28 works on philosophy, 20 on medicine, 8 on law, 5 on theology, and 4 on grammar, in addition to his commentaries on most of Aristotle's works and his commentary on Plato's "The Republic".
Ibn Rushd commentaries on Aristotle were the foundation for the Aristotelian revival in the 12th and 13th centuries. Ibn Rushd wrote short commentaries on Aristotle's work in logic, physics, and psychology. Ibn Rushd long commentaries provided an in depth line by line analysis of Aristotle's "Posterior Analytics," "De Anima," "Physics," "De Caelo," and the "Metaphysics."
His most important original philosophical work was "The Incoherence of the Incoherence" ("Tahafut al-tahafut"), in which he defended Aristotelian philosophy against al-Ghazali's claims in "The Incoherence of the Philosophers" ("Tahafut al-falasifa").
In "Fasl al-Maqal fi ma bayn al-Hikma wa al-Shariah min Ittisal" ("فصل المقال في ما بين الحكمة و الشريعة من إتصال" translated as "The Harmony of Religion and Philosophy", or "The Decisive Treatise, Determining the Nature of the Connection between Religion and Philosophy"), Ibn Rushd proves that philosophy and revelation do not contradict each other, and are essentially different means of reaching the same truth. However, he warns against teaching philosophical methods to the general populace.
Other works include "Kitab al-Kashf an Manahij al-Adilla كتاب الكشف عن مناهج الادلة ".
Ibn Rushd is also a highly regarded legal scholar of the Maliki school. Perhaps his best-known work in this field is "Bidāyat al-Mujtahid wa Nihāyat al-Muqtaṣid" ( بداية المجتهد و نهاية المقتصد), a textbook of Maliki doctrine in a comparative framework.
Jacob Anatoli translated several of the works of Ibn Rushd from Arabic into Hebrew in the 13th century. Many of them were later translated from Hebrew into Latin by Jacob Mantino and Abraham de Balmes. Other works were translated directly from Arabic into Latin by Michael Scot. Many of his works in logic and metaphysics have been permanently lost, while others, including some of the longer Aristotelian commentaries, have only survived in Latin or Hebrew translation, not in the original Arabic. The fullest version of his works is in Latin, and forms part of the multi-volume Juntine edition of Aristotle published in Venice 1562-1574.
Science.
Medicine.
Ibn Rushd wrote a medical encyclopedia called "Kulliyat (Colliget)" ("Generalities", i. e. general medicine), known in its Latin translation as "Colliget". He also made a compilation of the works of Galen, and wrote a commentary on the "The Canon of Medicine" ("Al-Qanun fi 't-Tibb") of Avicenna (Ibn Sina) (980–1037).
Physics.
Ibn Rush also authored three books on physics namely: Short Commentary on the Physics, Middle Commentary on the Physics and Long Commentary on the Physics.
Ibn Rushd defined and measured force as "the rate at which work is done in changing the kinetic condition of a material body" and correctly argued "that the effect and measure of force is change in the kinetic condition of a materially resistant mass". He took a particular and keen interest in the understanding of "motor force".
Ibn Rushd also developed the notion that bodies have a (non-gravitational) inherent resistance to motion into physics. This idea in particular was adopted by Thomas Aquinas and subsequently by Johannes Kepler, who referred to this fact as "Inertia".
In optics, Ibn Rushd followed Alhazen's incorrect explanation that a rainbow is due to reflection, not refraction.
Astronomy.
Regarding his studies in astronomy, Ibn Rushd argued for a strictly concentric model of the universe, and explained sunspots and scientific reasoning regarding the occasional opaque colors of the moon. He also worked on the description of the spheres, and movement of the spheres.
Psychology.
Ibn Rushd also made some studies regarding Active intellect and Passive intellect, both of the following were formerly regarded subjects of Psychology.
Philosophy.
The tradition of Islamic philosophy.
Ibn Rushd furthered the tradition of Greek philosophy in the Islamic world ("falsafa"). His commentaries removed the neo-Platonic bias of his predecessors. Criticizing al-Farabi's attempt to merge Plato and Aristotle's ideas, Ibn Rushd argued that Aristotle's philosophy diverged in significant ways from Plato's. Averroes rejected Avicenna's Neoplatonism which was partly based on the works of neo-Platonic philosophers, Plotinus and Proclus, which were mistakenly attributed to Aristotle.
In metaphysics, or more exactly ontology, Ibn Rushd rejects the view advanced by Avicenna that existence is merely accidental. Avicenna holds that "essence is ontologically prior to existence". The accidental are attributes which are not essential, but rather are additional contingent characteristics. Ibn Rushd, following Aristotle, holds that individual existing substances are primary. One may separate them mentally; however, ontologically speaking, existence and essence are one. According to Fakhry, this represents a change from Plato's theory of Ideas, where ideas precede particulars, to Aristotle's theory where particulars come first and the essence is "arrived at by a process of abstraction."
Commentaries on Aristotle and Plato.
Ibn Rushd wrote commentaries on most of the surviving works of Aristotle working from Arabic translations. He wrote three types of commentaries. The short commentary ("jami") is generally an epitome; the middle commentary ("talkhis") is a paraphrase; the long commentary ("tafsir") includes the whole text with a detailed analysis of each line.
Not having access to Aristotle's Politics, Ibn Rushd substituted Plato's Republic. Ibn Rushd, following Plato's paternalistic model, advances an authoritarian ideal. Absolute monarchy, led by a philosopher-king, creates a justly ordered society. This requires extensive use of coercion, although persuasion is preferred and is possible if the young are properly raised. Rhetoric, not logic, is the appropriate road to truth for the common man. Demonstrative knowledge via philosophy and logic requires special study. Rhetoric aids religion in reaching the masses.
Following Plato, Ibn Rushd accepts the principle of women's equality. They should be educated and allowed to serve in the military; the best among them might be tomorrow's philosophers or rulers. He also accepts Plato's illiberal measures such as the censorship of literature. He uses examples from Arab history to illustrate just and degenerate political orders.
Independent philosophical works.
His most important original philosophical work was "The Incoherence of the Incoherence" ("Tahafut al-tahafut"), in which he defended Aristotelian philosophy against al-Ghazali's claims in "The Incoherence of the Philosophers" ("Tahafut al-falasifa"). Al-Ghazali argued that Aristotelianism, especially as presented in the writings of Avicenna, was self-contradictory and an affront to the teachings of Islam. Ibn Rushd's rebuttal was two-pronged: he contended both that al-Ghazali's arguments were mistaken and that, in any case, the system of Avicenna was a distortion of genuine Aristotelianism so that al-Ghazali was aiming at the wrong target.
Whereas al-Ghazali believed that phenomenon such as cotton burning when coming into contact with fire happened each and every time only because God willed it to happen -- "all earthly occurrences depend on heavenly occurrences"—Ibn Rushd, by contrast insisted while God created the natural law, humans "could more usefully say that fire cause cotton to burn -- because creation had a pattern that they could discern."
In "Fasl al-Maqal", Ibn Rushd argues for the legality of philosophical investigation under Islamic law, and that there is no inherent contradiction between philosophy and religion.
In "Kitab al-Kashf", which argued against the proofs of Islam advanced by the Ash'arite school and discussed what proofs, on the popular level, should be used instead.
Significance.
Ibn Rushd is most famous for his commentaries of Aristotle's works, which had been mostly forgotten in the West. Before 1150, only a few of Aristotle's works existed in translation in Latin Europe, although the tradition of great philosophers and poets of antiquity continued to be studied and copied in the Greek Byzantium. It was to some degree through the Latin translations of Ibn Rushd's work beginning in the thirteenth century, that the legacy of Aristotle was recovered in the Latin West.
Ibn Rushd's work on Aristotle spans almost three decades, and he wrote commentaries on almost all of Aristotle's work except for Aristotle's "Politics", to which he did not have access. Hebrew translations of his work also had a lasting impact on Jewish philosophy. Moses Maimonides, Samuel Ben Tibbon, Juda Ben Solomon Choen, and Shem Tob Ben Joseph Falaquera were Jewish philosophers influenced by Ibn Rushd. His ideas were assimilated by Siger of Brabant and Thomas Aquinas and others (especially in the University of Paris) within the Christian scholastic tradition which valued Aristotelian logic. Famous scholastics such as Aquinas did not refer to him by name, simply calling him "The Commentator" and calling Aristotle "The Philosopher." Ibn Rushd had no discernible influence on Islamic philosophic thought until modern times. His death coincides with a change in the culture of Al-Andalus. In his work "Fasl al-Maqāl" (translated a. o. as "The Decisive Treatise"), he stresses the importance of analytical thinking as a prerequisite to interpret the Qur'an.
Jurisprudence and law.
Ibn Rushd is also a highly regarded legal scholar of the Maliki school. Perhaps his best-known work in this field is "Bidāyat al-Mujtahid wa Nihāyat al-Muqtaṣid, " a textbook of Maliki doctrine in a comparative framework, which is rendered in English as "The Distinguished Jurist's Primer"—. He is also the author of "al-Bayān wa'l-Taḥṣīl, wa'l-Sharḥ wa'l-Tawjīh wa'l-Ta`līl fi Masā'il al-Mustakhraja, " a long and detailed commentary based on the "Mustakhraja" of Muḥammad al-`Utbī al-Qurtubī.

</doc>
<doc id="47858" url="https://en.wikipedia.org/wiki?curid=47858" title="Arabian Peninsula">
Arabian Peninsula

Arabian Peninsula, simplified Arabia ( "", « Arabian island ») is a peninsula of Western Asia situated north-east of Africa on the Arabian plate. From a geological perspective, it is considered a subcontinent of Asia.
It is the largest peninsula in the world, at . The Arabian Peninsula consists of the countries Yemen, Oman, Qatar, Bahrain, Kuwait, Saudi Arabia and the United Arab Emirates as well as parts of southern Iraq and Jordan. The peninsula formed as a result of the rifting of the Red Sea between 56 and 23 million years ago, and is bordered by the Red Sea to the west, the Persian Gulf to the northeast, the Levant to the north and the Indian Ocean to the southeast. The Arabian Peninsula plays a critical geopolitical role in the Middle East and Arab world due to its vast reserves of oil and natural gas.
Before the modern era, it was divided into four distinct regions: Hejaz, Najd, Southern Arabia and Eastern Arabia. Hejaz and Najd make up most of Saudi Arabia. Southern Arabia consists of Yemen and some parts of Saudi Arabia and Oman (Dhofar). Eastern Arabia consists of the entire coastal strip of the Persian Gulf.
Geography.
The Arabian Peninsula is located in the continent of Asia and bounded by (clockwise) the Persian Gulf on the northeast, the Strait of Hormuz and the Gulf of Oman on the east, the Arabian Sea on the southeast and south, the Gulf of Aden on the south, the Bab-el-Mandeb strait on the southwest, and the Red Sea which is located on the southwest and west. The northern portion of the peninsula merges with the Syrian Desert with no clear border line, although the northern boundary of the Arabian Peninsula is generally considered to be the northern borders of Saudi Arabia and Kuwait.
The most prominent feature of the peninsula is desert, but in the southwest there are mountain ranges which receive greater rainfall than the rest of the Arabian Peninsula. Harrat ash Shaam is a large volcanic field that extends from the northwestern Arabian Peninsula into Jordan and southern Syria.
Political boundaries.
The peninsula's constituent countries are (clockwise north to south) Kuwait, Bahrain, Qatar, and the United Arab Emirates (UAE) on the east, Oman on the southeast, Yemen on the south and Saudi Arabia at the center. The island nation of Bahrain lies off the east coast of the peninsula.
Six countries, including Saudi Arabia, Kuwait, Bahrain, Qatar, United Arab Emirates and Oman form the Gulf Cooperation Council (GCC). This is however a disputed term. Iranians assert that it is a historical and internationally recognized convention to name it the Persian Gulf, while Arab States, and most notably the six GCC member countries, have been claiming that the Gulf is Arabian since its shallow marine depths are a geological continuity of the Arabian peninsula's Eastern low-lying coasts, from Kuwait to the UAE's Northern Emirates.
The Kingdom of Saudi Arabia covers the greater part of the peninsula. The majority of the population of the peninsula live in Saudi Arabia and in Yemen. The peninsula contains the world's largest reserves of oil. Saudi Arabia and the UAE are economically the wealthiest in the region. Qatar, a small peninsula in the Persian Gulf on the larger peninsula, is home of the Arabic-language television station Al Jazeera and its English-language subsidiary Al Jazeera English. Kuwait, on the border with Iraq, is an important country strategically, forming one of the main staging grounds for coalition forces mounting the invasion of Iraq in 2003.
Population.
Though historically lightly populated, political Arabia is noted for a high population growth rate - as the result of both very strong inflows of migrant labor as well as sustained high birth rates. The population tends to be relatively young and heavily skewed gender ratio dominated by males. In many states, the number of South Asians exceeds that of the local citizenry. The 4 smaller states (by area) having their entire coastlines on the Persian Gulf exhibit the world's most extreme population growth, roughly tripling every 20 years.
In 2014, the estimated population of the Arabian Peninsula is 77,983,936 (including expatriates).
Y-Chromosome.
Listed here are the human Y-chromosome DNA haplogroups in Arabia (Yemen, Oman, Qatar, Kuwait, Saudi Arabia and the United Arab Emirate).
Haplogroup J is the most abundant component in the Arabian peninsula embracing more than 50% of its Y-chromosomes. Its two main subclades (J1-M267 and J2-M172), show opposite latitudinal gradients in the Middle East. J1-M267 is more abundant in the southern areas, reaching a frequency around 73% in Yemen, whereas J2-M172 is more common in the Levant.
J (L222.2) Accounts for the majority of (L147.1) in Saudi Arabia. Seems to be an exclusively Adnani marker.
Landscape.
Geologically, this region is perhaps more appropriately called the "Arabian subcontinent" because it lies on a tectonic plate of its own, the Arabian Plate, which has been moving incrementally away from the rest of Africa (forming the Red Sea) and north, toward Asia, into the Eurasian plate (forming the Zagros mountains). The rocks exposed vary systematically across Arabia, with the oldest rocks exposed in the Arabian-Nubian Shield near the Red Sea, overlain by earlier sediments that become younger towards the Persian Gulf. Perhaps the best-preserved ophiolite on Earth, the Semail Ophiolite, lies exposed in the mountains of the UAE and northern Oman.
The peninsula consists of:
Arabia has few lakes or permanent rivers. Most areas are drained by ephemeral watercourses called wadis, which are dry except during the rainy season. Plentiful ancient aquifers exist beneath much of the peninsula, however, and where this water surfaces, oases form (e.g. Al-Hasa and Qatif, two of the world's largest oases) and permit agriculture, especially palm trees, which allowed the peninsula to produce more dates than any other region in the world. In general, the climate is extremely hot and arid, although there are exceptions. Higher elevations are made temperate by their altitude, and the Arabian Sea coastline can receive surprisingly cool, humid breezes in summer due to cold upwelling offshore. The peninsula has no thick forests, although desert-adapted wildlife is present throughout the region.
According to NASA's Gravity Recovery and Climate Experiment (GRACE) satellite data (2003 - 2013) analysed in a University of California, Irvine (UCI)-led study published in Water Resources Research on 16 June 2015, the most over-stressed aquifer system in the world is the Arabian Aquifer System, upon which more than 60 million people depend for water. Twenty-one of the thirty seven largest aquifers "have exceeded sustainability tipping points and are being depleted" and thirteen of them are "considered significantly distressed."
A plateau more than high extends across much of the Arabian Peninsula. The plateau slopes eastwards from the massive, rifted escarpment along the coast of the Red Sea, to the shallow waters of the Persian Gulf. The interior is characterised by "cuestas" and valleys, drained by a system of "wadis". A crescent of sand and gravel deserts lies to the east.
Land and sea.
Most of the Arabian Peninsula is unsuited to agriculture, making irrigation and land reclamation projects essential. The narrow coastal plain and isolated oases, amounting to less than 1% of the land area, are used to cultivate grains, coffee and tropical fruits. Goat, sheep, and camel husbandry is widespread elsewhere throughout the rest of the Peninsula. Some areas have a summer humid tropical monsoon climate, in particular the Dhofar and Al Mahrah areas of Oman and Yemen.
These areas allow for large scale coconut plantations. Much of Yemen has a tropical monsoon rain influenced mountain climate. The plains usually have either a tropical or subtropical arid desert climate or arid steppe climate. The sea surrounding the Arabian Peninsula is generally tropical sea with a very rich tropical sea life and some of the world's largest, undestroyed and most pristine coral reefs. In addition, the organisms living in symbiosis with the Red Sea coral, the protozoa and zooxanthellae, have a unique hot weather adaptation to sudden rise (and fall) in sea water temperature. Hence these coral reefs are not affected by coral bleaching caused by rise in temperature as elsewhere in the indopacific coral sea. The reefs are also unaffected by mass tourism and diving or other large scale human interference. However, some reefs were destroyed in the Persian Gulf, mostly caused by phosphate water pollution and resultant increase in algae growth as well as oil pollution from ships and pipeline leakage.
The fertile soils of Yemen have encouraged settlement of almost all of the land from sea level up to the mountains at . In the higher reaches elaborate terraces have been constructed to facilitate grain, fruit, coffee, ginger and khat cultivation.
Arabian peninsula is known for its rich oil, i.e. petroleum production due to its geographical location.
Etymology.
During the Hellenistic period, the area was known as "Arabia" or "Aravia" (). The Romans named three regions with the prefix "Arabia", encompassing a larger area than the current term "Arabian Peninsula":
The Arab inhabitants used a north-south division of Arabia: Al Sham-Al Yaman, or Arabia Deserta-Arabia Felix. Arabia Felix had originally been used for the whole peninsula, and at other times only for the southern region. Because its use became limited to the south, the whole peninsula was simply called Arabia. Arabia Deserta was the entire desert region extending north from Arabia Felix to Palmyra and the Euphrates, including all the area between Pelusium on the Nile and Babylon. This area was also called Arabia and not sharply distinguished from the peninsula.
The Arabs and the Ottoman Empire considered the west of the Arabian Peninsula region where the Arabs lived 'the land of the Arabs' – Bilad al-Arab (Arabia), and its major divisions were the bilad al-Sham (Syria), bilad al-Yaman (the Land of the southern Peninsula), and Bilad al-Iraq and modern-day Kuwait (the Land of the River Banks). The Ottomans used the term Arabistan in a broad sense for the subcontinent itself starting from Cilicia, where the Euphrates river makes its descent into Syria, through Palestine, and on through the remainder of the Sinai and Arabian peninsulas
"The provinces of Arabia were:" Al Tih, the Sinai peninsula, Hedjaz, Asir, Yemen, Hadramaut, Mahra and Shilu, Oman, Hasa, Bahrain, Dahna, Nufud, the Hammad, which included the deserts of Syria, Mesopotamia and Babylonia.
History.
The history of the Arabian Peninsula goes back to the beginnings of human habitation in Arabia up to 130,000 years ago. The region has twice in world history had a global impact. The first was in the 7th century when it became the cradle of Islam. The second was from the mid-20th century when the discovery of vast oil deposits propelled it into a key economic and geo-political role. At other times, the region existed in relative obscurity and isolation, although from the 7th century the cities of Mecca and Medina had the highest spiritual significance for the Islamic world, Mecca being the destination for the Hajj annual pilgrimage.
Pre-Islamic Arabia.
There is evidence that human habitation in the Arabian Peninsula dates back to about 106,000 to 130,000 years ago. However, the harsh climate historically prevented much settlement. In pre-Islamic Saudi Arabia, apart from a small number of urban trading settlements, such as Mecca and Medina, located in the Hejaz in the west of the peninsula, most of what was to become Saudi Arabia was populated by nomadic tribal societies or uninhabitable desert.
However, archaeology has revealed the existence of civilizations in pre-Islamic Arabia (such as Thamud). There were many civilizations in the Arabian Peninsula before Islam, especially in South Arabia. South Arabian civilizations include Sheba, Himyarite Kingdom, Kingdom of Awsan, Kingdom of Ma'īn and Sabaean Kingdom. Central Arabia was the sole location of kingdom of Kinda in the 4th, 5th and early 6th centuries AD. Eastern Arabia was home to the Dilmun civilization. The earliest known events in Arabian history are migrations from the Peninsula into neighbouring areas.
The Arabian peninsula has long been accepted as the original "Urheimat" of the Semitic languages by a majority of scholars.
"Shamir of Dhu-Raydan and Himyar had called in the help of the clans of Habashat for against the kings of Saba; but Ilmuqah granted... the submission of Shamir of Dhu-Raydan and the clans of Habashat."
The Rise of Islam.
The seventh century saw the introduction of Islam in the Arabian Peninsula. The Islamic prophet Muhammad, was born in Mecca in about 570 and first began preaching in the city in 610, but migrated to Medina in 622. From there he and his companions united the tribes of Arabia under the banner of Islam and created a single Arab Muslim religious polity in the Arabian peninsula. established a new unified polity in the Arabian peninsula which under the subsequent Rashidun and Umayyad Caliphates saw a century of rapid expansion of Arab power well beyond the Arabian peninsula in the form of a vast Muslim Arab Empire with an area of influence that stretched from the northwest Indian subcontinent, across Central Asia, the Middle East, North Africa, southern Italy, and the Iberian Peninsula, to the Pyrenees.
Muhammad began preaching Islam at Mecca before migrating to Medina, from where he united the tribes of Arabia into a singular Arab Muslim religious polity. With Muhammad's death in 632 AD, disagreement broke out over who would succeed him as leader of the Muslim community. Umar ibn al-Khattab, a prominent companion of Muhammad, nominated Abu Bakr, who was Muhammad's intimate friend and collaborator. Others added their support and Abu Bakr was made the first caliph. This choice was disputed by some of Muhammad's companions, who held that Ali ibn Abi Talib, his cousin and son-in-law, had been designated his successor. Abu Bakr's immediate task was to avenge a recent defeat by Byzantine (or Eastern Roman Empire) forces, although he first had to put down a rebellion by Arab tribes in an episode known as the Ridda wars, or "Wars of Apostasy".
Following Muhammad's death in 632, Abu Bakr became leader of the Muslims as the first Caliph. After putting down a rebellion by the Arab tribes (known as the Ridda wars, or "Wars of Apostasy"), Abu Bakr attacked the Byzantine Empire. On his death in 634, he was succeeded by Umar as caliph, followed by Uthman ibn al-Affan and Ali ibn Abi Talib. The period of these first four caliphs is known as "al-khulafā' ar-rāshidūn": the Rashidun or "rightly guided" Caliphate. Under the Rashidun Caliphs, and, from 661, their Umayyad successors, the Arabs rapidly expanded the territory under Muslim control outside of Arabia. In a matter of decades Muslim armies decisively defeated the Byzantine army and destroyed the Persian Empire, conquering huge swathes of territory from the Iberian peninsula to India. The political focus of the Muslim world then shifted to the newly conquered territories.
Nevertheless, Mecca and Medina remained the spiritually most important places in the Muslim world. The Qu'ran requires every able-bodied Muslim who can afford it, as one of the five pillars of Islam, to make a pilgrimage, or Hajj, to Mecca during the Islamic month of Dhu al-Hijjah at least once in his or her lifetime. The Masjid al-Haram (the Grand Mosque) in Mecca is the location of the Kaaba, Islam's holiest site, and the Masjid al-Nabawi (the Prophet's Mosque) in Medina is the location of Muhammad tomb; as a result, from the 7th century, Mecca and Medina became the pilgrimage destinations for large numbers of Muslims from across the Islamic world.
The Middle Ages.
Despite its spiritual importance, in political terms Arabia soon became a peripheral region of the Islamic world, in which the most important medieval Islamic states were based at various times in such far away cities as Damascus, Baghdad, and Cairo.
However, from the 10th century (and, in fact, until the 20th century) the Hashemite Sharifs of Mecca maintained a state in the most developed part of the region, the Hejaz. Their domain originally comprised only the holy cities of Mecca and Medina but in the 13th century it was extended to include the rest of the Hejaz. Although, the Sharifs exercised at most times independent authority in the Hejaz, they were usually subject to the suzerainty of one of the major Islamic empires of the time. In the Middle Ages, these included the Abbasids of Baghdad, and the Fatimids, Ayyubids and Mamluks of Egypt.
Modern history.
The provincial Ottoman Army for Arabia (Arabistan Ordusu) was headquartered in Syria, which included Lebanon, Palestine, and the Transjordan region. It was put in charge of Syria, Cilicia, Iraq, and the remainder of the Arabian Peninsula. The Ottomans never had any control over central Arabia, also known as the Najd region.
The Damascus Protocol of 1914 provides an illustration of the regional relationships. Arabs living in one of the existing districts of the Arabian peninsula, the Emirate of Hejaz, asked for a British guarantee of independence. Their proposal included all Arab lands south of a line roughly corresponding to the northern frontiers of present-day Syria and Iraq. They envisioned a new Arab state, or confederation of states, adjoining the southern Arabian Peninsula. It would have comprised Cilicia – İskenderun and Mersin, Iraq with Kuwait, Syria, Lebanon, Jordan, and Palestine.
In the modern era, the term bilad al-Yaman came to refer specifically to the southwestern parts of the peninsula. Arab geographers started to refer to the whole peninsula as 'jazirat al-Arab', or the peninsula of the Arabs.
Late Ottoman rule and the Hejaz Railway.
In the beginning of the 20th century, the Ottomans embarked on an ambitious project: the construction of a railway connecting Istanbul, the capital of the Ottoman Empire and the seat of the Islamic Caliphate, and Hejaz with its holiest shrines of Islam which are the yearly pilgrimage destination of the Hajj. Another important goal was to improve the economic and political integration of the distant Arabian provinces into the Ottoman state, and to facilitate the transportation of military troops in case of need.
The Hejaz Railway was a narrow gauge railway (1050 mm) that ran from Damascus to Medina, through the Hejaz region of Arabia. It was originally planned to reach the holy city of Mecca, but due to the interruption of the construction works caused by the outbreak of World War I, it eventually became able to reach only Medina. It was a part of the Ottoman railway network and was built in order to extend the previously existing line between Istanbul and Damascus (which began from the Haydarpaşa Terminal).
The railway was started in 1900 at the behest of the Ottoman Sultan Abdul Hamid II and was built largely by the Turks, with German advice and support. A public subscription was opened throughout the Islamic world to fund the construction. The railway was to be a waqf, an inalienable religious endowment or charitable trust.
The Arab Revolt and the unification of Saudi Arabia.
The major developments of the early 20th century were the Arab Revolt during World War I and the subsequent collapse and partitioning of the Ottoman Empire. The Arab Revolt (1916–1918) was initiated by the Sherif Hussein ibn Ali with the aim of securing independence from the ruling Ottoman Empire and creating a single unified Arab state spanning from Aleppo in Syria to Aden in Yemen. During World War I, the Sharif Hussein entered into an alliance with the United Kingdom and France against the Ottomans in June 1916.
These events were followed by the unification of Saudi Arabia under King Abdulaziz Ibn Saud. In 1902 Ibn Saud had captured Riyadh. Continuing his conquests, Abdulaziz subdued Al-Hasa, the rest of Nejd, and the Hejaz between 1913 and 1926, defeating the Sherif Hussein ibn Ali, and founded the modern state of Saudi Arabia. Two Saudi states were formed and controlled much of Arabia before Ibn Saud was even born. Ibn Saud however, established the third Saudi state.
Oil reserves.
The second major development has been the discovery of vast reserves of oil in the 1930s. Its production brought great wealth to all countries of the region, with the exception of Yemen.
Civil war in Yemen.
The North Yemen Civil War was fought in North Yemen between royalists of the Mutawakkilite Kingdom of Yemen and factions of the Yemen Arab Republic from 1962 to 1970. The war began with a coup d'état carried out by the republican leader, Abdullah as-Sallal, which dethroned the newly crowned Muhammad al-Badr and declared Yemen a republic under his presidency. The Imam escaped to the Saudi Arabian border and rallied popular support.
The royalist side received support from Saudi Arabia, while the republicans were supported by Egypt and the Soviet Union. Both foreign irregular and conventional forces were also involved. The Egyptian President, Gamal Abdel Nasser, supported the republicans with as many as 70,000 troops. Despite several military moves and peace conferences, the war sank into a stalemate. Egypt's commitment to the war is considered to have been detrimental to its performance in the Six-Day War of June 1967, after which Nasser found it increasingly difficult to maintain his army's involvement and began to pull his forces out of Yemen.
By 1970, King Faisal of Saudi Arabia recognized the republic and a truce was signed. Egyptian military historians refer to the war in Yemen as their Vietnam.
Gulf War.
In 1990 Iraq invaded Kuwait. The invasion of Kuwait by Iraqi forces led to the 1990–91 Gulf War. Egypt, Qatar, Syria and Saudi Arabia joined a multinational coalition that opposed Iraq. Displays of support for Iraq by Jordan and Palestine resulted in strained relations between many of the Arab states. After the war, a so-called "Damascus Declaration" formalized an alliance for future joint Arab defensive actions between Egypt, Syria, and the GCC member states.
Transport and industry.
The extraction and refining of oil and gas are the major industrial activities in the Arabian Peninsula. The region also has an active construction sector, with many cities reflecting the wealth generated by the oil industry. The service sector is dominated by financial and technical institutions, which, like the construction sector, mainly serve the oil industry. Traditional handicrafts such as carpet-weaving are found in rural areas of Arabia.

</doc>
<doc id="47863" url="https://en.wikipedia.org/wiki?curid=47863" title="Destination user">
Destination user

The destination user in an information transfer transaction is the user who receives information from the source, i.e., from the originating user.

</doc>
<doc id="47867" url="https://en.wikipedia.org/wiki?curid=47867" title="Information transfer">
Information transfer

In telecommunications, information transfer is the process of moving messages containing user information from a source to a sink via a Communication channel. In this sense, information transfer is equivalent to data transmission which highlights more practical, technical aspects.
The information transfer rate may or may not be equal to the transmission modulation rate.
Bidirectional information transfer is called information exchange.
Non-technical meaning.
In a non-technical context, information transfer is sometimes used to signify knowledge transfer or teaching.

</doc>
<doc id="47868" url="https://en.wikipedia.org/wiki?curid=47868" title="Data stream">
Data stream

In Connection-oriented communication, a data stream is a sequence of digitally encoded coherent signals (packets of data or data packets) used to transmit or receive information that is in the process of being transmitted.
Formal definition.
In a formal way, a data stream is any ordered pair formula_1 where:

</doc>
<doc id="47876" url="https://en.wikipedia.org/wiki?curid=47876" title="Lorry (disambiguation)">
Lorry (disambiguation)

"Lorry" is the British English term for a truck, a large motor vehicle
Lorry may also refer to:
__NOTOC__

</doc>
<doc id="47877" url="https://en.wikipedia.org/wiki?curid=47877" title="Pickup truck">
Pickup truck

A pickup truck is a light duty truck having an enclosed cab and an open cargo area with low sides and tailgate. Once a work tool with few creature comforts, in the 1950s consumers began purchasing pickups for lifestyle reasons and by the 1990s less than 15 percent of owners reported use in work as the pickup truck's primary purpose. Today in North America, the pickup is mostly used like a passenger car and accounts for about 18 per cent of total vehicles sold in the US.
The term pickup is of unknown origin. It was used by Studebaker in 1913 and by the 1930s "pick-up" (hyphenated) had become the standard term. In Australia and New Zealand ute, short for "utility", is used for both pickups and coupé utilities. In South Africa people of all language groups use the term "bakkie", a diminutive of "bak", Afrikaans for "bowl".
History.
In the early days of automobile manufacturing, vehicles were sold as a chassis only, and third parties added bodies on top. In 1913 the Galion Allsteel Body Company, an early developer of the pickup and dump truck, built and installed hauling boxes on slightly modified Ford Model T chassis, and from 1917 on the Model TT. Seeking part of this market share, Dodge introduced a 3/4-ton pickup with cab and body constructed entirely of wood in 1924. In 1925 Ford followed up with a Model T-based steel-bodied, half-ton with an adjustable tailgate and heavy-duty rear springs. Billed as the "Ford Model T Runabout with Pickup Body," it sold for . 34,000 were built. In 1928 it was replaced by the Model A which had a closed-cab, safety glass windshield, roll-up side windows and three-speed transmission. In 1931 Chevrolet produced its first factory-assembled pickup. Ford Australia produced the first Australian "ute" in 1932. During the Second World War, the United States government halted the production of privately owned pickup trucks. 
In the 1950s consumers began purchasing pickups for lifestyle rather than utilitarian reasons. Car-like smooth-sided fenderless trucks were introduced, such as the Chevrolet Fleetside, the Chevrolet El Camino, the Dodge Sweptline and in 1957, Ford's purpose-built Styleside. Pickups began to feature comfort items like power options and air conditioning. Trucks became more passenger oriented with the introduction of crew cabs in the Toyota Stout and the Hino Briska, was introduced in 1962. Dodge followed with a crew cab in 1963, Ford in 1965, and General Motors in 1973. Many modern pickups are considered luxury cars, having such features as AWD/four wheel drive, Bluetooth and DVD Players, front and rear cameras, heated/cooled leather seating, sunroofs, 22-inch polished aluminum wheels, and remote-release powered tailgates.
In the U.S.A., the 1963 protectionist chicken tax distorted the light truck market in favor of American manufacturers, stopping the import of the Volkswagen Type 2, and effectively "squeezed smaller Asian truck companies out of the American pickup market." Over the intervening years, Detroit lobbied to protect the light-truck tariff, thereby reducing pressure on Detroit to introduce vehicles that polluted less and that offered increased fuel economy.
The US government's 1973 Corporate Average Fuel Economy policy sets higher fuel economy requirements for cars than pickups. CAFE led to the replacement of the station wagon by the minivan, the latter being in the truck category which allowed it compliance with less-strict emissions standards. Eventually, this same idea led to the promotion of the SUV. Pickups, unhindered by the emissions controls regulations on cars, began to replace muscle cars as the performance vehicle of choice. The Dodge Warlock appeared in Dodge's "adult toys" line, along with the Macho Power Wagon and Street Van. The gas guzzler tax, which taxed fuel-inefficient cars while exempting pickup trucks, further distorted the market in favour of pickups.
In the 1980s, the compact Mazda B-series, Isuzu Faster and Mitsubishi Forte appeared. Subsequently, American manufacturers built their own compact pickups for the domestic market: the Ford Ranger, and the Chevrolet S-10. Minivans make inroads into the pickups' market share. In the 1990s Pickups' market share was further eroded by the popularity of sport utility vehicles.
International markets.
While the Ford F-150 has been the best-selling vehicle in the United States since 1982, the Ford F-150, or indeed any full-size pickup truck is a rare sight in Europe, where high fuel prices and very narrow city roads make it difficult to use on a daily basis. In America pickups are favored by low fuel prices, taxes and regulations that distort the market in favor of domestically built trucks, and a cultural attachment to the style.
Full-size pickups and SUVs are an important source of revenue for GM, Ford, and FCA's Ram, accounting for more than two-thirds of their global pre-tax earnings, though the vehicles make up just 16 percent of North American vehicle production. The vehicles have a high profit margin and a high price, with 40 per cent of Ford F-150 selling for US$40,000 or more.
The NOx law prevents pickups from being imported to Japan, but the Japanese Domestic Market Mitsubishi Triton is available. In China (where it is known by the English loanword as 皮卡车 pí kǎ chē) the Great Wall Wingle is manufactured domestically and exported to Australia. In Thailand pickups manufactured for local sale and export include the Isuzu D-Max and the Mitsubishi Triton. In Latin and South America the Toyota Hilux, Ford Ranger, VW Amarok, Dodge Ram, Chevrolet S-10, D-20, and Montana are sold.
In South Africa pickups account for about 17 per cent of the passenger and light commercial vehicle sales, mostly the Toyota Hilux, Ford Ranger, and Isuzu KB (Isuzu D-Max). The Volkswagen Amarok and Nissan Navara are also sold, that latter marketed as the Nissan NP300 Hardbody.
Design and features.
In the US and Canada, nearly all pickups are sold with automatic transmissions. The Cummins diesel-equipped Ram is the only full-size truck available with a clutch. It has an ultra-low first-gear ratio for heavy hauling. While the Chevrolet Colorado, Nissan Frontier, and Toyota Tacoma are offered with a manual transmission, Ford offers no manual transmission pickups in North America.
A regular cab has a single row of seats and a single set of doors, one on each side. Extended or super cab pickups add an extra space behind the main seat, sometimes including small seats. A crew cab, or double cab, seats five or six and has two full-size front-hinged doors on both sides. Cab-over, or cab forward has the cab sitting above the front axle. An early cab-forward drop-side pickup was the Volkswagen Transporter, introduced in 1952. This configuration is more common among European and Japanese manufacturers than in North America, since the style allows a longer cargo area for the same overall length. The design was more popular in North America in the 1950s and '60s, examples including the Jeep Forward Control, Ford Econoline, Chevrolet Corvair, Rampside and Loadside pickups, and the Dodge A-100.
The cargo bed can vary in size according to whether the vehicle is optimized for cargo utility or passenger comfort. Most have fixed side walls and a hinged tailgate. This is termed step-style or well body cargo. A drop-side bed has a flat tray with hinged panels rising up on the sides and the rear.
A pickup with four rear wheels instead of two is called a "dually".
Vehicles similar to the pickup include:
In the American domestic market pickups are general categorized as:
The terms "half-ton" and "three-quarter-ton" are a remnant from a time when the number referred to the maximum cargo capacity by weight.
Uses for pickup trucks.
While in the US and Canada most pickup trucks are used primarily for passenger transport, agriculture, and commercial uses, pickups are also used in law enforcement, the military, fire services, and for pickup truck racing, a form of auto racing using modified versions of pickups mostly on oval tracks. Race pickup trucks are mechanically similar to coupé-shaped stock cars.
A monster truck is a vehicle styled after pickup trucks, but with extremely large wheels and suspension. They are used for competition and popular sports entertainment and in some cases they are featured alongside motocross races, mud bogging, tractor pulls and car-eating robots.
Equipping pickup trucks with camper shells provides a small living space for camping. Slide-in truck campers, on the other hand, give a pickup truck the amenities of a small motorhome, but still allow the operator the option of removal and independent use of the vehicle.
Pickups are often used by practitioners of rolling coal. This is the modification diesel engine, so that the vehicle can emit an under-aspirated visibly polluting sooty diesel exhaust. Vehicles emissions controls are modified in open defiance of environmental regulations. It also may include the intentional removal of the particulate filter Practitioners often additionally modify their vehicles by installing smoke switches and smoke stacks. Modifications to a vehicle to enable rolling coal may cost from $200 to $5,000.

</doc>
<doc id="47878" url="https://en.wikipedia.org/wiki?curid=47878" title="Huntington's disease">
Huntington's disease

Huntington's disease (HD) is a neurodegenerative genetic disorder that affects muscle coordination and leads to mental decline and behavioral symptoms. Symptoms of the disease can vary between individuals and affected members of the same family, but usually progress predictably. The earliest symptoms are often subtle problems with mood or cognition. A general lack of coordination and an unsteady gait often follow. As the disease advances, uncoordinated, jerky body movements become more apparent, along with a decline in mental abilities and behavioral symptoms. Physical abilities gradually worsen until coordinated movement becomes difficult. Mental abilities generally decline into dementia. Complications such as pneumonia, heart disease, and physical injury from falls reduce life expectancy to around twenty years from the point at which symptoms begin. Physical symptoms can begin at any age from infancy to old age, but usually begin between 35 and 44 years of age. The disease may develop earlier in life in each successive generation. About 6% of cases start before the age of 21 years with an akinetic-rigid syndrome; they progress faster and vary slightly. The variant is classified as juvenile, akinetic-rigid, or Westphal variant HD.
HD is the most common genetic cause of abnormal involuntary writhing movements called chorea, which is why the disease used to be called Huntington's chorea. The disease is caused by an autosomal dominant mutation in either of an individual's two copies of a gene called Huntingtin. This means a child of an affected person typically has a 50% chance of inheriting the disease. The "Huntingtin" gene provides the genetic information for a protein that is also called "huntingtin". Expansion of a CAG (cytosine-adenine-guanine) triplet repeat stretch within the "Huntingtin" gene results in a different form of the protein, which gradually damages cells in the brain, through mechanisms that are not fully understood. Genetic testing can be performed at any stage of development, even before the onset of symptoms. This fact raises several ethical debates: the age at which an individual is considered mature enough to choose testing; whether parents have the right to have their children tested; and managing confidentiality and disclosure of test results. Genetic counseling has developed to inform and aid individuals considering genetic testing and has become a model for other genetically dominant diseases.
There is no cure for HD. Full-time care is required in the later stages of the disease. Existing pharmaceutical and non-drug treatments can relieve some symptoms, but are still limited in improving quality of life. It is much more common in people of Western European descent than in those of Asian or African ancestry. The disease can affect both men and women.
The genetic basis of HD was discovered in 1993 by an international collaborative effort spearheaded by the Hereditary Disease Foundation. Research and support organizations, first founded in the 1960s and increasing in number, work to increase public awareness, to provide support for individuals and their families, and to promote and facilitate research. Current research directions include determining the exact mechanism of the disease, improving animal models to expedite research, clinical trials of pharmaceuticals to treat symptoms or slow the progression of the disease, and studying procedures such as stem cell therapy with the goal of repairing damage caused by the disease.
Signs and symptoms.
Symptoms of Huntington's disease commonly become noticeable between the ages of 35 and 44 years, but they can begin at any age from infancy to old age. In the early stages, there are subtle changes in personality, cognition, and physical skills. The physical symptoms are usually the first to be noticed, as cognitive and behavioral symptoms are generally not severe enough to be recognized on their own at the earlier stages. Almost everyone with Huntington's disease eventually exhibits similar physical symptoms, but the onset, progression and extent of cognitive and behavioral symptoms vary significantly between individuals.
The most characteristic initial physical symptoms are jerky, random, and uncontrollable movements called chorea. Chorea may be initially exhibited as general restlessness, small unintentionally initiated or uncompleted motions, lack of coordination, or slowed saccadic eye movements. These minor motor abnormalities usually precede more obvious signs of motor dysfunction by at least three years. The clear appearance of symptoms such as rigidity, writhing motions or abnormal posturing appear as the disorder progresses. These are signs that the system in the brain that is responsible for movement has been affected. Psychomotor functions become increasingly impaired, such that any action that requires muscle control is affected. Common consequences are physical instability, abnormal facial expression, and difficulties chewing, swallowing, and speaking. Eating difficulties commonly cause weight loss and may lead to malnutrition. Sleep disturbances are also associated symptoms. Juvenile HD differs from these symptoms in that it generally progresses faster and chorea is exhibited briefly, if at all, with rigidity being the dominant symptom. Seizures are also a common symptom of this form of HD.
Cognitive abilities are progressively impaired. Especially affected are executive functions which include planning, cognitive flexibility, abstract thinking, rule acquisition, initiation of appropriate actions, and inhibition of inappropriate actions. As the disease progresses, memory deficits tend to appear. Reported impairments range from short-term memory deficits to long-term memory difficulties, including deficits in episodic (memory of one's life), procedural (memory of the body of how to perform an activity) and working memory. Cognitive problems tend to worsen over time, ultimately leading to dementia. This pattern of deficits has been called a subcortical dementia syndrome to distinguish it from the typical effects of cortical dementias e.g. Alzheimer's disease.
Reported neuropsychiatric manifestations are anxiety, depression, a reduced display of emotions (blunted affect), egocentrism, aggression, and compulsive behavior, the latter of which can cause or worsen addictions, including alcoholism, gambling, and hypersexuality. Difficulties in recognizing other people's negative expressions have also been observed. The prevalence of these symptoms is highly variable between studies, with estimated rates for lifetime prevalence of psychiatric disorders between 33% and 76%. For many sufferers and their families, these symptoms are among the most distressing aspects of the disease, often affecting daily functioning and constituting reason for institutionalization. Suicidal thoughts and suicide attempts are more common than in the general population. Often individuals have reduced awareness of chorea, cognitive and emotional impairments.
Mutant Huntingtin is expressed throughout the body and associated with abnormalities in peripheral tissues that are directly caused by such expression outside the brain. These abnormalities include muscle atrophy, cardiac failure, impaired glucose tolerance, weight loss, osteoporosis, and testicular atrophy.
Genetics.
All humans have two copies of the Huntingtin gene ("HTT"), which codes for the protein Huntingtin (Htt). The gene is also called "HD" and "IT15", which stands for 'interesting transcript 15'. Part of this gene is a repeated section called a trinucleotide repeat, which varies in length between individuals and may change length between generations. If the repeat is present in a healthy gene, a dynamic mutation may increase the repeat count and result in a defective gene. When the length of this repeated section reaches a certain threshold, it produces an altered form of the protein, called mutant Huntingtin protein (mHtt). The differing functions of these proteins are the cause of pathological changes which in turn cause the disease symptoms. The Huntington's disease mutation is genetically dominant and almost fully penetrant: mutation of either of a person's "HTT" alleles causes the disease. It is not inherited according to sex, but the length of the repeated section of the gene and hence its severity can be influenced by the sex of the affected parent.
Genetic mutation.
HD is one of several trinucleotide repeat disorders which are caused by the length of a repeated section of a gene exceeding a normal range. The "HTT" gene is located on the short arm of chromosome 4 at 4p16.3. "HTT" contains a sequence of three DNA bases—cytosine-adenine-guanine (CAG)—repeated multiple times (i.e. ... CAGCAGCAG ...), known as a trinucleotide repeat. CAG is the 3-letter genetic code (codon) for the amino acid glutamine, so a series of them results in the production of a chain of glutamine known as a polyglutamine tract (or polyQ tract), and the repeated part of the gene, the "PolyQ region".
Generally, people have fewer than 36 repeated glutamines in the polyQ region which results in production of the cytoplasmic protein Huntingtin. However, a sequence of 36 or more glutamines results in the production of a protein which has different characteristics. This altered form, called mHtt (mutant Htt), increases the decay rate of certain types of neurons. Regions of the brain have differing amounts and reliance on these types of neurons, and are affected accordingly. Generally, the number of CAG repeats is related to how much this process is affected, and accounts for about 60% of the variation of the age of the onset of symptoms. The remaining variation is attributed to environment and other genes that modify the mechanism of HD. 36–39 repeats result in a reduced-penetrance form of the disease, with a much later onset and slower progression of symptoms. In some cases the onset may be so late that symptoms are never noticed. With very large repeat counts, HD has full penetrance and can occur under the age of 20, when it is then referred to as juvenile HD, akinetic-rigid, or Westphal variant HD. This accounts for about 7% of HD carriers.
Inheritance.
Huntington's disease has autosomal dominant inheritance, meaning that an affected individual typically inherits one copy of the gene with an expanded trinucleotide repeat (the mutant allele) from an affected parent. Since penetrance of the mutation is very high, those who have a mutated copy of the gene will have the disease. In this type of inheritance pattern, each offspring of an affected individual has a 50% risk of inheriting the mutant allele and therefore being affected with the disorder (see figure). This probability is sex-independent.
Trinucleotide CAG repeats over 28 are unstable during replication and this instability increases with the number of repeats present. This usually leads to new expansions as generations pass (dynamic mutations) instead of reproducing an exact copy of the trinucleotide repeat. This causes the number of repeats to change in successive generations, such that an unaffected parent with an "intermediate" number of repeats (28–35), or "reduced penetrance" (36–40), may pass on a copy of the gene with an increase in the number of repeats that produces fully penetrant HD. Such increases in the number of repeats (and hence earlier age of onset and severity of disease) in successive generations is known as genetic anticipation. Instability is greater in spermatogenesis than oogenesis; maternally inherited alleles are usually of a similar repeat length, whereas paternally inherited ones have a higher chance of increasing in length. It is rare for Huntington's disease to be caused by a new mutation, where neither parent has over 36 CAG repeats.
In the rare situations where both parents have an expanded HD gene, the risk increases to 75%, and when either parent has two expanded copies, the risk is 100% (all children will be affected). Individuals with both genes affected are rare. For some time HD was thought to be the only disease for which possession of a second mutated gene did not affect symptoms and progression, but it has since been found that it can affect the phenotype and the rate of progression.
Mechanism.
The Htt protein interacts with over 100 other proteins, and appears to have multiple biological functions. The behavior of this mutated protein is not completely understood, but it is toxic to certain cell types, particularly in the brain. Early damage is most evident in the striatum, but as the disease progresses, other areas of the brain are also more conspicuously affected. Early symptoms are attributable to functions of the striatum and its cortical connections—namely control over movement, mood and higher cognitive function.
Htt function.
Htt is expressed in all mammalian cells. The highest concentrations are found in the brain and testes, with moderate amounts in the liver, heart, and lungs. The function of Htt in humans is unclear. It interacts with proteins which are involved in transcription, cell signaling and intracellular transporting. In animals genetically modified to exhibit HD, several functions of Htt have been found. In these animals, Htt is important for embryonic development, as its absence is related to embryonic death. Caspase, an enzyme which plays a role in catalyzing apoptosis, is thought to be activated by the mutated gene through damaging the ubiquitin-protease system. It also acts as an anti-apoptotic agent preventing programmed cell death and controls the production of brain-derived neurotrophic factor, a protein which protects neurons and regulates their creation during neurogenesis. Htt also facilitates vesicular transport and synaptic transmission and controls neuronal gene transcription. If the expression of Htt is increased and more Htt produced, brain cell survival is improved and the effects of mHtt are reduced, whereas when the expression of Htt is reduced, the resulting characteristics are more typical of the presence of mHtt. In humans the disruption of the normal gene does not cause the disease. It is thought that the disease is not caused by inadequate production of Htt, but by a gain of toxic function of mHtt.
Cellular changes due to mHtt.
There are multiple cellular changes through which the toxic function of mHtt may manifest and produce the HD pathology. During the biological process of posttranslational modification of mHtt, cleavage of the protein can leave behind shorter fragments constituted of parts of the polyglutamine expansion. The polar nature of glutamine causes interactions with other proteins when it is overabundant in unmodified Htt proteins or the Htt fragments created from Htt cleavage. Thus, the mHtt molecule strands will form hydrogen bonds with one another, forming a protein aggregate rather than folding into functional proteins. Over time, the aggregates accumulate, ultimately interfering with neuron function because these fragments can then misfold and coalesce, in a process called protein aggregation, to form inclusion bodies within cells. Neuronal inclusions run indirect interference. The excess protein aggregates clump together at axons and dendrites in neurons which mechanically stops the transmission of neurotransmitters because vesicles (filled with neurotransmitters) can no longer move through the cytoskeleton. Ultimately, over time, fewer and fewer neurotransmitters are available for release in signaling other neurons as the neuronal inclusions grow. Inclusion bodies have been found in both the cell nucleus and cytoplasm. Inclusion bodies in cells of the brain are one of the earliest pathological changes, and some experiments have found that they can be toxic for the cell, but other experiments have shown that they may form as part of the body's defense mechanism and help protect cells.
Several pathways by which mHtt may cause cell death have been identified. These include: effects on chaperone proteins, which help fold proteins and remove misfolded ones; interactions with caspases, which play a role in the process of removing cells; the toxic effects of glutamine on nerve cells; impairment of energy production within cells; and effects on the expression of genes. The cytotoxic effects of mHtt are strongly enhanced by interactions with a protein called "Rhes", which is expressed mainly in the striatum. Rhes was found to induce sumoylation of mHtt, which causes the protein clumps to disaggregate—studies in cell culture showed that the clumps were much less toxic than the disaggregated form.
An additional theory that explains another way cell function may be disrupted by HD proposes that damage to mitochondria in striatal cells (numerous accounts of mitochondrial metabolism deficiency have been found) and the interactions of the altered huntingtin protein with numerous proteins in neurons leads to an increased vulnerability of glutamine, which, in large amounts, has been found to be an excitotoxin. Excitotoxins may cause damage to numerous cellular structures. Although glutamine is not found in excessively high amounts, it has been postulated that because of the increased vulnerability, even normal amounts glutamine can cause excitotoxins to be expressed.
Macroscopic changes due to mHtt.
HD affects the whole brain, but certain areas are more vulnerable than others. The most prominent early effects are in a part of the basal ganglia called the neostriatum, which is composed of the caudate nucleus and putamen. Other areas affected include the substantia nigra, layers 3, 5 and 6 of the cerebral cortex, the hippocampus, purkinje cells in the cerebellum, lateral tuberal nuclei of the hypothalamus and parts of the thalamus. These areas are affected according to their structure and the types of neurons they contain, reducing in size as they lose cells. Striatal spiny neurons are the most vulnerable, particularly ones with projections towards the external globus pallidus, with interneurons and spiny cells projecting to the internal pallidum being less affected. HD also causes an abnormal increase in astrocytes and activation of the brain's immune cells, microglia.
The basal ganglia—the part of the brain most prominently affected in early HD—play a key role in movement and behavior control. Their functions are not fully understood, but current theories propose that they are part of the cognitive executive system and the motor circuit. The basal ganglia ordinarily inhibit a large number of circuits that generate specific movements. To initiate a particular movement, the cerebral cortex sends a signal to the basal ganglia that causes the inhibition to be released. Damage to the basal ganglia can cause the release or reinstatement of the inhibitions to be erratic and uncontrolled, which results in an awkward start to motion or motions to be unintentionally initiated, or a motion to be halted before, or beyond, its intended completion. The accumulating damage to this area causes the characteristic erratic movements associated with HD. The spontaneous and erratic physical movements associated with HD are classified as a type of hyperkinetic dysarthria. Because of the basal ganglia's inability to inhibit movements, individuals affected by it will inevitably experience a reduced ability to produce speech and swallow foods and liquids (dysphagia).
Transcriptional dysregulation.
CREB-binding protein (CBP), a transcriptional coregulator, is essential for cell function because as a coactivator at a significant number of promoters, it activates the transcription of genes for survival pathways. Furthermore, the amino acids that form CBP include a strip of 18 glutamines. Thus, the glutamines on CBP interact directly with the increased numbers of glutamine on the Htt chain and CBP gets pulled away from its typical location next to the nucleus. Specifically, CBP contains an acetyltransferase domain to which Htt binds through its polyglutamine-containing domain. Autopsied brains of those who had Huntington's disease also have been found to have incredibly reduced amounts of CBP. Medical imaging, such as computerized tomography (CT) and magnetic resonance imaging (MRI), can show atrophy of the caudate nuclei early in the disease, as seen in the illustration to the right, but these changes are not, by themselves, diagnostic of HD. Cerebral atrophy can be seen in the advanced stages of the disease. Functional neuroimaging techniques, such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET), can show changes in brain activity before the onset of physical symptoms, but they are experimental tools, and are not used clinically.
Predictive genetic testing.
Because HD follows an autosomal dominant pattern of inheritance, there is a strong motivation for individuals who are at risk of inheriting it to seek a diagnosis. The genetic test for HD consists of a blood test which counts the numbers of CAG repeats in each of the "HTT" alleles. Cutoffs are given as follows:
A pre-symptomatic test is a life-changing event and a very personal decision. The main reason given for choosing testing for HD is to aid in career and family decisions. Before 1993 there was not an available test for individuals to learn if they carried the Huntington's gene. At that time surveys indicated that 50–70% of at-risk individuals would have been interested in receiving testing, but since predictive testing has been offered far fewer choose to be tested. Over 95% of individuals at risk of inheriting HD do not proceed with testing, mostly because there is no treatment. A key issue is the anxiety an individual experiences about not knowing whether they will eventually develop HD, compared to the impact of a positive result. Irrespective of the result, stress levels have been found to be lower two years after being tested, but the risk of suicide is increased after a positive test result. Individuals found to have not inherited the disorder may experience survivor guilt with regard to family members who are affected. Other factors taken into account when considering testing include the possibility of discrimination and the implications of a positive result, which usually means a parent has an affected gene and that the individual's siblings will be at risk of inheriting it. In one study genetic discrimination was found in 46% of individuals at risk for Huntington's disease. It occurred at higher rates within personal relationships than health insurance or employment relations. Genetic counseling in HD can provide information, advice and support for initial decision-making, and then, if chosen, throughout all stages of the testing process. Because of the implications of this test, patients who wish to undergo testing must complete three counseling sessions which provide information about Huntington's.
Counseling and guidelines on the use of genetic testing for HD have become models for other genetic disorders, such as autosomal dominant cerebellar ataxias. Presymptomatic testing for HD has also influenced testing for other illnesses with genetic variants such as polycystic kidney disease, familial Alzheimer's disease and breast cancer. The European Molecular Genetics Quality Network have published yearly external quality assessment scheme for molecular genetic testing for this disease and have developed best practice guidelines for genetic testing for HD to assist in testing and reporting of results.
Preimplantation genetic diagnosis.
Embryos produced using in vitro fertilization may be genetically tested for HD using preimplantation genetic diagnosis (PGD). This technique, where one or two cells are extracted from a typically 4 to 8 cell embryo and then tested for the genetic abnormality, can then be used to ensure embryos affected with HD genes are not implanted, and therefore any offspring will not inherit the disease. Some forms of preimplantation genetic diagnosis—non-disclosure or exclusion testing—allow at-risk people to have HD-free offspring "without" revealing their own parental genotype, giving no information about whether they themselves are destined to develop HD. In exclusion testing, the embryos' DNA is compared with that of the parents and grandparents to avoid inheritance of the chromosomal region containing the HD gene from the affected grandparent. In non-disclosure testing, only disease-free embryos are replaced in the uterus while the parental genotype and hence parental risk for HD are never disclosed.
Prenatal testing.
It is also possible to obtain a prenatal diagnosis for an embryo or fetus in the womb, using fetal genetic material acquired through chorionic villus sampling. An amniocentesis can be performed if the pregnancy is further along, within 14–18 weeks. This procedure looks at the amniotic fluid surrounding the baby for indicators of the HD mutation. This, too, can be paired with exclusion testing to avoid disclosure of parental genotype. Prenatal testing can be done when a parent has been diagnosed with HD, when they have had genetic testing showing the expansion of the HTT gene, or when they have a 50% chance of inheriting the disease. The parents can be counseled on their options, which include termination of pregnancy, and on the difficulties of a child with the identified gene.
In addition, in at-risk pregnancies due to an affected male partner, non-invasive prenatal diagnosis can be performed by analyzing cell-free fetal DNA in a blood sample taken from the mother (via venipuncture) between six and twelve weeks of pregnancy. It has no procedure-related risk of miscarriage (excepting via needle contamination).
Differential diagnosis.
About 99% of HD diagnoses based on the typical symptoms and a family history of the disease are confirmed by genetic testing to have the expanded trinucleotide repeat that causes HD. Most of the remaining are called HD-like disorders. Most of these other disorders are collectively labelled HD-like (HDL). The cause of most HDL diseases is unknown, but those with known causes are due to mutations in the prion protein gene (HDL1), the junctophilin 3 gene (HDL2), a recessively inherited "HTT" gene (HDL3—only found in one family and poorly understood), and the gene encoding the TATA box-binding protein (HDL4/SCA17). Other autosomal dominant diseases that can be misdiagnosed as HD are dentatorubral-pallidoluysian atrophy and neuroferritinopathy. There are also autosomal recessive disorders that resemble sporadic cases of HD. Main examples are chorea acanthocytosis, pantothenate kinase-associated neurodegeneration and X-linked McLeod syndrome.
Management.
There is no cure for HD, but there are treatments available to reduce the severity of some of its symptoms. For many of these treatments, evidence to confirm their effectiveness in treating symptoms of HD specifically are incomplete. As the disease progresses the ability to care for oneself declines and carefully managed multidisciplinary caregiving becomes increasingly necessary. Although there have been relatively few studies of exercises and therapies that help rehabilitate cognitive symptoms of HD, there is some evidence for the usefulness of physical therapy, occupational therapy, and speech therapy. An association between caffeine intake and earlier age of onset in Huntington's disease has been found but, since this finding was based on retrospective questionnaire data rather than a blinded, randomized trial or case-control study, this work is a poor basis for guiding lifestyle decisions. 
Physical therapy.
Weight loss and eating difficulties due to dysphagia and other muscle discoordination are common, making nutrition management increasingly important as the disease advances. Thickening agents can be added to liquids as thicker fluids are easier and safer to swallow. Reminding the people to eat slowly and to take smaller pieces of food into the mouth may also be of use to prevent choking. If eating becomes too hazardous or uncomfortable, the option of using a percutaneous endoscopic gastrostomy is available. This is a feeding tube, permanently attached through the abdomen into the stomach, which reduces the risk of aspirating food and provides better nutritional management. Assessment and management by speech and language therapists with experience in Huntington's disease is recommended.
People with Huntington's disease may see a physical therapist for non-invasive and non-medication-based ways of managing the physical symptoms. Physical therapists may implement fall risk assessment and prevention, as well as strengthening, stretching, and cardiovascular exercises. Walking aids may be prescribed as appropriate. Physical therapists also prescribe breathing exercises and airway clearance techniques with the development of respiratory problems. Consensus guidelines on physiotherapy in Huntington's disease have been produced by the European HD Network. Goals of early rehabilitation interventions are prevention of loss of function. Participation in rehabilitation programs during early to middle stage of the disease may be beneficial as it translates into long term maintenance of motor and functional performance. Rehabilitation during the late stage aims to compensate for motor and functional losses. For long-term independent management, the therapist may develop home exercise programs for appropriate people.
Medications.
Tetrabenazine was approved in 2008 for treatment of chorea in Huntington's disease in the US. Other drugs that help to reduce chorea include neuroleptics and benzodiazepines. Compounds such as amantadine or remacemide are still under investigation but have shown preliminary positive results. Hypokinesia and rigidity, especially in juvenile cases, can be treated with antiparkinsonian drugs, and myoclonic hyperkinesia can be treated with valproic acid.
Psychiatric symptoms can be treated with medications similar to those used in the general population. Selective serotonin reuptake inhibitors and mirtazapine have been recommended for depression, while atypical antipsychotic drugs are recommended for psychosis and behavioral problems. Specialist neuropsychiatric input is recommended as people may require long-term treatment with multiple medications in combination.
Education.
The families of individuals who have inherited or are at risk of inheriting HD, have generations of experience of HD which may be outdated and lack knowledge of recent breakthroughs and improvements in genetic testing, family planning choices, care management, and other considerations. Genetic counseling benefits these individuals by updating their knowledge, dispelling any myths they may have and helping them consider their future options and plans.
Prognosis.
The length of the trinucleotide repeat accounts for 60% of the variation in the age symptoms appear and the rate they progress. A longer repeat results in an earlier age of onset and a faster progression of symptoms. Individuals with more than sixty repeats often develop the disease before age 20, while those with fewer than 40 repeats may not ever develop noticeable symptoms. The remaining variation is due to environmental factors and other genes that influence the mechanism of the disease.
Life expectancy in HD is generally around 20 years following the onset of visible symptoms. Most life-threatening complications result from muscle coordination and, to a lesser extent, behavioral changes induced by declining cognitive function. The largest risk is pneumonia, which causes death in one third of those with HD. As the ability to synchronize movements deteriorates, difficulty clearing the lungs and an increased risk of aspirating food or drink both increase the risk of contracting pneumonia. The second greatest risk is heart disease, which causes almost a quarter of fatalities of those with HD. Suicide is the third greatest cause of fatalities, with 7.3% of those with HD taking their own lives and up to 27% attempting to do so. It is unclear to what extent suicidal thoughts are influenced by behavioral symptoms, as they signify sufferers' desires to avoid the later stages of the disease. Other associated risks include choking, physical injury from falls, and malnutrition.
Epidemiology.
The late onset of Huntington's disease means it does not usually affect reproduction. The worldwide prevalence of HD is 5–10 cases per 100,000 persons, but varies greatly geographically as a result of ethnicity, local migration and past immigration patterns. Prevalence is similar for men and women. The rate of occurrence is highest in peoples of Western European descent, averaging around 7 per 100,000 people, and is lower in the rest of the world; e.g., one per million people of Asian and African descent. A 2013 epidemiological study of the prevalence of Huntington's disease in the U.K. between 1990 and 2010 found that the average prevalence for the U.K. was 12.3 per 100,000. Additionally, some localized areas have a much higher prevalence than their regional average. One of the highest incidences is in the isolated populations of the Lake Maracaibo region of Venezuela, where HD affects up to 700 per 100,000 persons. Other areas of high localization have been found in Tasmania and specific regions of Scotland, Wales and Sweden. Increased prevalence in some cases occurs due to a local founder effect, a historical migration of carriers into an area of geographic isolation. Some of these carriers have been traced back hundreds of years using genealogical studies. Genetic haplotypes can also give clues for the geographic variations of prevalence. Iceland, on the contrary, has a rather low prevalence of 1 per 100,000, despite the fact that Icelanders as a people are descended of the early Germanic tribes of Scandinavia which also gave rise to the Swedes; all cases with the exception of one going back nearly two centuries having derived from the offspring of a couple living early in the 19th century. Finland, as well, has a low incidence of only 2.2 per 100,000 people.
Until the discovery of a genetic test, statistics could only include clinical diagnosis based on physical symptoms and a family history of HD, excluding those who died of other causes before diagnosis. These cases can now be included in statistics; and, as the test becomes more widely available, estimates of the prevalence and incidence of the disorder are likely to increase.
History.
Although Huntington's has been recognized as a disorder since at least the Middle Ages, the cause has been unknown until fairly recently. Huntington's was given different names throughout this history as understanding of the disease changed. Originally called simply 'chorea' for the jerky dancelike movements associated with the disease, HD has also been called "hereditary chorea" and "chronic progressive chorea". The first definite mention of HD was in a letter by Charles Oscar Waters, published in the first edition of Robley Dunglison's "Practice of Medicine" in 1842. Waters described "a form of chorea, vulgarly called magrums", including accurate descriptions of the chorea, its progression, and the strong heredity of the disease. In 1846 Charles Gorman observed how higher prevalence seemed to occur in localized regions. Independently of Gorman and Waters, both students of Dunglison at Jefferson Medical College in Philadelphia, Johan Christian Lund also produced an early description in 1860. He specifically noted that in Setesdalen, a secluded mountain valley in Norway, there was a high prevalence of dementia associated with a pattern of jerking movement disorders that ran in families.
The first thorough description of the disease was by George Huntington in 1872. Examining the combined medical history of several generations of a family exhibiting similar symptoms, he realized their conditions must be linked; he presented his detailed and accurate definition of the disease as his first paper. Huntington described the exact pattern of inheritance of autosomal dominant disease years before the rediscovery by scientists of Mendelian inheritance."Of its hereditary nature. When either or both the parents have shown manifestations of the disease ..., one or more of the offspring almost invariably suffer from the disease ... But if by any chance these children go through life without it, the thread is broken and the grandchildren and great-grandchildren of the original shakers may rest assured that they are free from the disease.". Sir William Osler was interested in the disorder and chorea in general, and was impressed with Huntington's paper, stating that "In the history of medicine, there are few instances in which a disease has been more accurately, more graphically or more briefly described." Osler's continued interest in HD, combined with his influence in the field of medicine, helped to rapidly spread awareness and knowledge of the disorder throughout the medical community. Great interest was shown by scientists in Europe, including Louis Théophile Joseph Landouzy, Désiré-Magloire Bourneville, Camillo Golgi, and Joseph Jules Dejerine, and until the end of the century, much of the research into HD was European in origin. By the end of the 19th century, research and reports on HD had been published in many countries and the disease was recognized as a worldwide condition.
During the rediscovery of Mendelian inheritance at the turn of the 20th century, HD was used tentatively as an example of autosomal dominant inheritance. The English biologist William Bateson used the pedigrees of affected families to establish that HD had an autosomal dominant inheritance pattern. The strong inheritance pattern prompted several researchers, including Smith Ely Jelliffe, to attempt to trace and connect family members of previous studies. Jelliffe collected information from across New York and published several articles regarding the genealogy of HD in New England. Jelliffe's research roused the interest of his college friend, Charles Davenport, who commissioned Elizabeth Muncey to produce the first field study on the East Coast of the United States of families with HD and to construct their pedigrees. Davenport used this information to document the variable age of onset and range of symptoms of HD; he claimed that most cases of HD in the USA could be traced back to a handful of individuals. This research was further embellished in 1932 by P. R. Vessie, who popularized the idea that three brothers who left England in 1630 bound for Boston were the progenitors of HD in the USA. The claim that the earliest progenitors had been established and eugenic bias of Muncey's, Davenport's, and Vessie's work contributed to misunderstandings and prejudice about HD. Muncey and Davenport also popularized the idea that in the past some HD sufferers may have been thought to be possessed by spirits or victims of witchcraft, and were sometimes shunned or exiled by society. This idea has not been proven. Researchers have found contrary evidence; for instance, the community of the family studied by George Huntington openly accommodated those who exhibited symptoms of HD.
The search for the cause of this condition was enhanced considerably in 1968 when the Hereditary Disease Foundation (HDF) was created by Milton Wexler, a psychoanalyst based in Los Angeles, California whose wife Leonore Sabin had been diagnosed earlier that year with Huntington's disease. The three brothers of Wexler's wife also suffered from this disease. The foundation was involved in the recruitment of over 100 scientists in the Huntington's Disease Collaborative Research Project who over a 10-year period worked to locate the responsible gene.
Thanks to the HDF, the ongoing US-Venezuela Huntington's Disease Collaborative Research Project was started in 1979, and reported a major breakthrough in 1983 with the discovery of the approximate location of a causal gene. This was the result of an extensive study focusing on the populations of two isolated Venezuelan villages, Barranquitas and Lagunetas, where there was an unusually high prevalence of the disease. It involved over 18,000 people—mostly from a single extended family.
Among other innovations, the project developed DNA-marking methods which were an important step in making the Human Genome Project possible. In 1993, the research group isolated the precise causal gene at 4p16.3, making this the first autosomal disease locus found using genetic linkage analysis.
In the same time frame, key discoveries concerning the mechanisms of the disorder were being made, including the findings by Anita Harding's research group on the effects of the gene's length.
Modelling the disease in various types of animals, such as the transgenic mouse developed in 1996, enabled larger scale experiments. As these animals have faster metabolisms and much shorter lifespans than humans, results from experiments are received sooner, speeding research. The 1997 discovery that mHtt fragments misfold led to the discovery of the nuclear inclusions they cause. These advances have led to increasingly extensive research into the proteins involved with the disease, potential drug treatments, care methods, and the gene itself.
The condition was formerly called 'Huntington's chorea' but this term has been replaced by 'Huntington's disease' because not all patients develop chorea and due to the importance of cognitive and behavioral problems.
Society and culture.
Ethics.
Huntington's disease, particularly the application of the genetic test for the disease, has raised several ethical issues. The issues for genetic testing include defining how mature an individual should be before being considered eligible for testing, ensuring the confidentiality of results, and whether companies should be allowed to use test results for decisions on employment, life insurance or other financial matters. There was controversy when Charles Davenport proposed in 1910 that compulsory sterilization and immigration control be used for people with certain diseases, including HD, as part of the eugenics movement. In vitro fertilization has some issues regarding its use of embryos. Some HD research has ethical issues due to its use of animal testing and embryonic stem cells.
The development of an accurate diagnostic test for Huntington's disease has caused social, legal, and ethical concerns over access to and use of a person's results.
Many guidelines and testing procedures have strict procedures for disclosure and confidentiality to allow individuals to decide when and how to receive their results and also to whom the results are made available. Financial institutions and businesses are faced with the question of whether to use genetic test results when assessing an individual, such as for life insurance or employment. The United Kingdom's insurance companies have agreed that until 2017 they will not use genetic information when writing insurance policies under , but Huntington's is explicitly excluded from this agreement. As with other untreatable genetic conditions with a later onset, it is ethically questionable to perform pre-symptomatic testing on a child or adolescent, as there would be no medical benefit for that individual. There is consensus for testing only individuals who are considered cognitively mature, although there is a counter-argument that parents have a right to make the decision on their child's behalf. With the lack of an effective treatment, testing a person under legal age who is not judged to be competent is considered unethical in most cases.
There are ethical concerns related to prenatal genetic testing or preimplantation genetic diagnosis to ensure a child is not born with a given disease. For example, prenatal testing raises the issue of selective abortion, a choice considered unacceptable by some. As it is a dominant disease, there are difficulties in situations in which a parent does not want to know his or her own diagnosis. This would require parts of the process to be kept secret from the parent.
Support organizations.
In 1968, after experiencing HD in his wife's family, Dr. Milton Wexler was inspired to start the Hereditary Disease Foundation (HDF), with the aim of curing genetic illnesses by coordinating and supporting research. The foundation and Dr. Wexler's daughter, Nancy Wexler, were key parts of the research team in Venezuela which discovered the HD gene.
At roughly the same time as the HDF formed, Marjorie Guthrie helped to found the Committee to Combat Huntington's Disease (now the Huntington's Disease Society of America), after her husband Woody Guthrie died from complications of HD.
Since then, support and research organizations have formed in many countries around the world and have helped to increase public awareness of HD. A number of these collaborate in umbrella organizations, like the International Huntington Association and the European HD network. Many support organizations hold an annual HD awareness event, some of which have been endorsed by their respective governments. For example, 6 June is designated "National Huntington's Disease Awareness Day" by the US Senate.
The largest funder of Huntington's disease research globally, in terms of financial expenditure, is the CHDI Foundation, a US non-profit biomedical foundation that aims to "rapidly discover and develop drugs that delay or slow Huntington's disease". CHDI was formerly known as the High Q Foundation. In 2006, it spent $50 million on Huntington's disease research. CHDI collaborates with many academic and commercial laboratories globally and engages in oversight and management of research projects as well as funding. Many organizations exist to support and inform those affected by HD.
Research directions.
Research into the mechanism of HD has focused on identifying the functioning of Htt, how mHtt differs or interferes with it, and the brain pathology that the disease produces. Research is conducted using "in vitro" methods, animal models and human volunteers. Animal models are critical for understanding the fundamental mechanisms causing the disease and for supporting the early stages of drug development. Animals with chemically induced brain injury exhibit HD-like symptoms and were initially used, but they did not mimic the progressive features of the disease. The identification of the causative gene has enabled the development of many transgenic animal models including nematode worms, "Drosophila" fruit flies, mice, rats, sheep, pigs and monkeys that express mutant huntingtin and develop progressive neurodegeneration and HD-like symptoms.
Research is being conducted on many different approaches to prevent Huntington's disease or slow its progression. Disease-modifying strategies can be broadly grouped into three categories: reducing the level of the mutant huntingtin protein (including gene silencing); approaches aimed at improving neuronal survival by reducing the harm caused by the protein to specific cellular pathways and mechanisms (including protein homeostasis and histone deacetylase inhibition); and strategies to replace lost neurons. In addition, novel therapies to improve brain functioning are under development; these seek to produce symptomatic rather than disease-modifying therapies, and include phosphodiesterase inhibitors.
Reducing huntingtin production.
Gene silencing aims to reduce the production of the mutant protein, since HD is caused by a single dominant gene encoding a toxic protein. Gene silencing experiments in mouse models have shown that when the expression of mHtt is reduced, symptoms improve. Safety of non-allele specific RNAi and ASO gene silencing has now been demonstrated in mice and the large, human-like brains of primates. Allele-specific silencing attempts to silence mutant Htt while leaving wild-type Htt untouched. One way of accomplishing this is to identify polymorphisms present on only one allele and produce gene silencing drugs that target polymorphisms in only the mutant allele. The first 'gene silencing' trial involving human HD patients began in 2015, testing the safety of IONIS-HTTRx, produced by Ionis Pharmaceuticals and led by UCL Institute of Neurology. Mutant huntingtin was detected and quantified for the first time in cerebrospinal fluid from Huntington's disease mutation-carriers in 2015 using a novel 'single-molecule counting' immunoassay, providing a direct way to assess whether huntingtin-lowering treatments are achieving the desired effect.
Improving cell survival.
Among the approaches aimed at improving cell survival in the presence of mutant huntingtin are correction of transcriptional regulation using histone deacetylase inhibitors, modulating aggregation of huntingtin, improving metabolism and mitochondrial function and restoring function of synapses.
Neuronal replacement.
Stem cell therapy is the replacement of damaged neurons by transplantation of stem cells into affected regions of the brain. Experiments have yielded mixed results using this technique in animal models and preliminary human clinical trials. Whatever their future therapeutic potential, stem cells are already a valuable tool for studying Huntington's disease in the laboratory.
Clinical trials.
Several clinical trials of new experimental treatments are underway and planned in Huntington's disease.
Compounds that have failed to prevent or slow progression of Huntington's disease in human trials include remacemide, coenzyme Q10, riluzole, creatine, minocycline, ethyl-EPA, phenylbutyrate and dimebon.

</doc>
<doc id="47880" url="https://en.wikipedia.org/wiki?curid=47880" title="Roguelike">
Roguelike

Roguelike is a subgenre of role-playing video games characterized by a dungeon crawl through procedurally generated game levels, turn-based gameplay, tile-based graphics, and permanent death of the player-character. Most roguelikes are based on a high fantasy narrative, reflecting their influence from tabletop role playing games such as "Dungeons & Dragons". Though the roguelikes "Beneath Apple Manor" and "Sword of Fargoal" predate it, the 1980 game "Rogue" is considered the forerunner and the namesake of the genre, with derivative games mirroring "Rogue"s character- or sprite-based graphics. These games were popularized among college students and computer programmers of the 1980s and 1990s, leading to a large number of variants but adhering to these common gameplay elements, often titled the "Berlin Interpretation". Some of the better-known variants include "Hack", "NetHack", "Ancient Domains of Mystery", "Moria", "Angband", and "Tales of Maj'Eyal". The Japanese series of "Mystery Dungeon" games by Chunsoft, inspired by "Rogue", also fall within the concept of roguelike games.
More recently, with more powerful home computers and gaming systems, new variations of roguelikes incorporating other gameplay genres, thematic elements and graphical styles have become popular, typically retaining the notion of procedural generation and permanent death of the player-character. Indie games like ', "Spelunky", "The Binding of Isaac", ', and "Rogue Legacy" helped to establish the use of roguelike elements in other genres. These titles are sometimes labeled as "roguelike-like", "rogue-lite", or "procedural death labyrinths" to reflect the variation from titles which mimic the gameplay of traditional roguelikes more faithfully. Other games, like "Diablo" and "UnReal World", key titles in the action role-playing and the survival game genres respectively, took inspiration from roguelikes.
Gameplay and design.
General gameplay.
Deriving from the concepts of tabletop role-playing games such as "Dungeons & Dragons", nearly all roguelikes give the player control of a character, which they may customize by selecting a class, race, and gender, and adjusting attributes points and skills. At the start of the game, the character is placed at the top-most level of a dungeon, with basic equipment such as a simple weapon, armor, torches, and food. Following along the role-playing concept of a dungeon crawl, the player moves the character through the dungeon, collecting treasure which can include new weapons, armors, magical devices, potions, scrolls, food, and in-game money, while having to fight monsters that roam the dungeon. Most combat is performed simply by attempting to move the character into the same space as the monster, which the game then calculates the damage that the character and monster deal. Other types of attacks, such as firing an arrow or performing an offensive magic spell, can often be performed as well.
Defeating monsters earns the character experience points, and after earning enough points, the character will gain an experience level, improving their hit points, magic capability, and other attributes. Monsters may drop treasure to be looted. On the other hand, the character dies if they lose all their hit points. As most roguelikes feature the concept of permadeath, this represents the end of the game, and the player will need to restart the game with a newly made character. Roguelikes are nearly always turn-based, with the game only reacting when the player makes an action with the character. This allows players to evaluate a difficult situation, such as being cornered by several monsters, at their own pace and determine the best strategy.
The player generally has to explore the dungeon to reveal its contents similar to a fog of war. Many roguelikes include factors related to light sources such as a torch to provide illumination to see monsters in nearby squares, and line of sight can also limit what monsters are visible. Dungeons tend to be connected by stairs; lower dungeon levels generally are more difficult than higher ones, so that a character that is underdeveloped may have difficulty by progressing too fast. Dungeon levels and the population of monsters and treasure within them are generated randomly using procedural generation, so that no game is the same on subsequent playthroughs. Most roguelikes have an ultimate goal of either claiming an item located at the deepest level of the dungeon, or to defeat a specific monster that lives on that level. Typical roguelikes include a scoreboard, ranking the player's performance once their game is over (be it through permadeath or completing the objective) based on the amount of treasure, money, and experience earned, and how fast the player completed the game in such cases.
Key features.
The genre of roguelike broadly encompasses the gameplay that was popularized by the text-based game "Rogue" (1980), which bore out many variations due to its success; As of 2015, several hundred games claiming to be roguelikes were available through the Steam game catalog, and the user-run wiki RogueBasin tracks hundreds of roguelikes and their development. Because of the expansion of numerous variations on the roguelike theme, the gameplay elements characterizing the roguelike genre were explicitly defined at the International Roguelike Development Conference 2008 held in Berlin, Germany; these factors encompass what is known as the "Berlin Interpretation". Some of the factors used in this definition include:
Early roguelikes.
Early roguelikes were developed to be played on text-based user interfaces, commonly UNIX-based computer mainframes and terminals used at colleges and universities before transitioning to personal computers. Games used a mix of ASCII or ANSI characters to represent elements of the dungeon levels, creatures, and items on the level. These games typically included one or two text lines presenting the player's current status at the bottom of the screen, and text-based menu screens to manage inventory, statistics, and other details. The player's character was nearly always represented by the codice_1 across text-based roguelikes, which had been chosen by the developers of "Rogue" to stand for "where you're at". Other examples would include codice_2 for monetary treasure and codice_3 for a dragon. Later games would take advantage of color-based text graphics to increase the variation of creature types, such as a red codice_3 for a red dragon that would shoot fire, while a green codice_3 could indicate a green dragon that would shoot acid. Players would use the keyboard, using one keypress to enter a command. With modern computer systems, users developed alternate means of displaying the game, such as graphical tilesets and Isometric-based graphical front ends, as well as interfaces that took advantage of keyboard and mouse UI controls.
As computers offered more advanced user interfaces, such as windows and point-and-click menus, many traditional roguelikes were modified to include support for having multiple windows. This was useful to not only show the character-based dungeon, but details on the character's inventory, the monster they were in battle with, and other status messages, in separate windows. Having access to multiple windows also allowed having menus to complete more complex commands.
Roguelike-likes and procedural death labyrinths.
With computers and video game consoles capable of more advanced graphics and gameplay, numerous games have emerged that are loosely based on the classic roguelike design but diverge in one or more features. Many of these games use the concepts of procedurally generated maps and permadeath, while moving away from tile-based movement and turn-based gameplay, often using another gameplay genre such as action games or platformers. As such, the term "roguelike-like" or "rogue-lite" has been used to distinguish these games that possess some, but not all, of the Berlin Interpretation features from those that exactly meet the Berlin roguelike definition. The phrase "procedural death labyrinth" has also been applied to such games, as they retain the notion of permadeath and random level generation but lack the other high-value factors normally associated with roguelike games.
Roguelike-likes are generally much shorter games intended to be winnable within a single gaming session, in contrast to traditional roguelikes that are designed for multiple sessions of gameplay. Associated with their short length, many roguelike-likes feature a metagame, whereby achieving certain goals will unlock features such as the ability to select a new character at the start of the game or the addition of new items and monsters in the procedural generation of the game's levels. Several roguelike-likes feature daily challenges, in which a preset random seed is used to generate the game's levels in a deterministic fashion so that each player will have the same encounters; players attempt to complete the game through those levels or otherwise get the highest score through online leaderboards. Roguelike-likes may also allow the player to enter the random seed directly as to be able to rechallenge the same set of levels or share a difficult set of levels with other players.
"US Gamer" further identified games they consider edge cases of being roguelikes or roguelike-likes, as they are inspired by "Rogue", and "that stray a bit further from the genre but still manage to scratch the same itch as a great roguelike". These include games such as the "Diablo" series, "Dark Souls" and its sequel, "ToeJam & Earl", and "Dwarf Fortress".
History.
Early history (1975–1980).
The creation of roguelike games came from hobbyist programmers and computer hackers, attempting to create games for the nascent computer field in the early 1980s, particularly influenced by the 1975 text adventure game "Colossal Cave Adventure" (often simply titled "Adventure"). Some elements of the roguelike genre were present in dungeon crawlers written for the PLATO system, like the multi-user games "dnd" (1975) and "Moria" (1975), although there is no evidence that the early roguelike creators had access to these games. The core roguelike games were developed independently of each other, many of the developers not learning about their respective projects until several years after the genre took off.
Roguelike games were initially developed for computing environments with limited memory, including shared mainframe systems and early home computers; this limitation prevented developers from retaining all but a few dungeon levels in memory while the game was running, leading to procedural generation to avoid the memory storage issue. Roguelikes were also often written in languages like BASIC, which are not as efficient in memory and data management compared to modern languages, further limiting the scope the game could cover.
Concurrent variants.
Though the term "roguelike" derives from the 1980 game "Rogue", the first known game with the core roguelike gameplay elements was "Beneath Apple Manor" (1978), written by Don Worth for the Apple II; "Beneath Apple Manor" is also recognized as the first commercial roguelike game. The game, inspired by Worth's enjoyment of "Dungeons & Dragons" roleplaying, included procedural generation using a modification of the random maze generator from the game "Dragon Maze", role-playing elements for the characters, tile-based movement and turn-based combat. Though "Beneath Apple Manor" predated "Rogue", it was not as popular as "Rogue": "Rogue" had advantage of being distributed over ARPANET which many college students had easy access to, while "Beneath Apple Manor" was packaged and sold by hand by Worth either at local stores or through mail fulfillment.
Another early roguelike whose development pre-dated "Rogue" was "Sword of Fargoal" (1982), developed by Jeff McCord starting in 1979. The game was based on "GammaQuest", an earlier title McCord had created on the Commodore PET which he shared locally with friends while a student at Henry Clay High School in Kentucky; the game itself was based on a "Dungeons & Dragons" campaign he had run himself in the prior years. Before graduating and attending the University of Tennessee in 1981, he had started work on "GammaQuest II", which required the player to navigate through randomly generated dungeon levels, acquire a sword, and make it back to the surface with that sword through more randomly generated levels. The more advanced computers available at the school, such as the Commodore VIC-20, enabled him to expand out the game further from the highly limited memory on the PET. On seeing the prospects of selling computer software, he eventually got a publication deal with Epyx, where they helped him to refine the marketing of the game, renaming it "Sword of Fargoal", and giving him access to the more powerful Commodore 64, enabling him to use graphics and sound as part of the game. The game was considered a success, and when it was ported to the PC in 1983, it out-shone "Rogue"s PC release the same year due to "Sword of Fargoal"s superior graphics and sound.
"Rogue".
"Rogue" was written by Glenn Wichman and Michael Toy in 1980 while students at the University of California, Santa Cruz. The game was inspired by Toy's prior experience in playing the 1971 "Star Trek" game and programming clones of it for various other computer systems. It was also inspired by interactive fiction "Adventure". While looking for a way to randomize the experience of "Adventure", they came across Ken Arnold's curses library that enabled them to better manipulate characters on the terminal screen, prompting Toy and Wichman to create a graphical-like randomized adventure game. They created the story of the game by having the player seek out the "Amulet of Yendor", "Yendor" being "Rodney" spelled backwards, the name of the wizard they envisioned had created the dungeon. "Rogue" was originally executed on a VAX-11/780 computer; its limited memory forced them to use a simple text-based interface for the game. Toy eventually dropped out of school but got a job at the computing labs at University of California, Berkeley, where he meet with Arnold. Arnold helped to optimize the curses code and implement more features into the game.
"Rogue" proved popular with college students and computer researchers at the time, including Ken Thompson; Dennis Ritchie had joked at the time that "Rogue" was "the biggest waste of CPU cycles in history". Its popularity led to the game's inclusion on BSD UNIX v4.2 in 1984, though at that time, without its source code. Toy and Arnold had anticipated selling "Rogue" commercially and were hesitant about releasing it; Toy would go on to meet Jon Lane at Olivetti, and together they would go on to create the company A.I. Design to port the games for various home systems, later bringing Wichman back to help.
Following evolution (1980–1995).
The hierarchy of the major Roguelike games that are known to descend from "Rogue". Solid lines represent games developed from the parent's source code, while dotted lines represent games that were inspired by the parent game.
The popularity of "Rogue" led developers to create their own versions of the game, though their efforts were originally limited by the lack of access to "Rogue"s source, which was not released until BSD v4.3 in 1986. These developers resorted to building games from scratch similar to "Rogue" but with features that they wanted to see. These versions would be distributed with source code, and along with the original "Rogue" source, other developers were able to create software forks of the games, adding in new monsters, items, and gameplay features, creating several dozen variants. This process was aided by switching code to languages with better data typing, including object-oriented and scripting languages, and cleaning up and modularizing the code so that contributors can better follow where changes can be made.
While there are some direct variants of "Rogue", such as "Brogue", most variants of "Rogue" could be classified into two branches based on two key games, "Moria" and "Hack", that were developed in the spirit of "Rogue".
"Moria"-based.
"Moria" (1983) was developed by Robert Alan Koeneke while a student at University of Oklahoma, inspired by both "Adventure" and "Rogue". Having access to a VAX-11/780, but without the source to "Rogue" due to computer administrator restrictions, he began trying to recreate "Rogue" but specifically flavored with the complex underground cave maze of the same name in J.R.R. Tolkien's Middle Earth stories. Following Tolkien's fiction, the player's goal was to descend to the depths of Moria to defeat the Balrog, akin to a boss battle. As with "Rogue", levels were not persistent: when the player left the level and then tried to return, a new level would be procedurally generated. Among other improvement to "Rogue", Koeneke included a persistent town at the highest level where players could buy and sell equipment, and the use of data structures within the Pascal language allowed him to create a more diverse bestiary within the game. He got help from several playtesters as well as another student, Jimmey Wayne Todd, who helped to program a deeper character generation system. "UMoria" (short for "UNIX Moria") is a close variation on "Moria" by Jim E. Wilson, making the game more portable to a larger variety of computers while fixing various bugs.
"Angband" (1990) was developed by Alex Cutler and Andy Astrand while attending the University of Warwick. Having played "UMoria", they wanted to expand the game even further. Working from "UMoria"s code, they increased the number of levels and monsters, flavored the game based on Angband, the massive fortress controlled by Morgoth from Tolkien's fiction, and incorporated more of the deadlier creatures described within the Middle Earth mythology. They kept the Balrog as a difficult creature that must be overcome at a mid-game level, while Morgoth became the final boss the player must defeat to win the game. Following Cutler and Astrand's graduation, Sean March and Geoff Hill took over the development to see the game through to a public release outside of the university, adding in elements such as giving the player a sense of the rewards and dangers of a level when they entered it the first time.
Once "Angband" was released to the public via USENET, there were efforts to have code maintainers (the "devteam") to fix bugs, clean up the code and implement suggestions into the code. Due to numerous shifts in those maintaining the code (due to other obligations), and the number of potential user suggestions to include, "Angband" would become highly forked, leading to a number of "Angband" variants; at least sixty known variants exist with about a half dozen still under active development. One significant fork was "ZAngband" (1994) (short for "Zelanzy Angband"), which expanded on "Angband" and altered the theme towards Roger Zelazny's "The Chronicles of Amber". The "ZAngband" codebase would be used to create "Tales of Middle Earth" ("TOME") in 2002, which later swapped out the Tolkien and Zelazny fiction setting for a new original one to become "Tales of Maj'Eyal" (2009). The vanilla "Angband" remains in development today by the devteam.
"Hack"-based.
"Hack" (1982) was developed by Jay Fenlason with help from Kenny Woodland, Mike Thome, and Jonathan Payne, students at Lincoln-Sudbury Regional High School at the time, while participating in the school's computer lab overseen by Brian Harvey. Harvey had been able to acquire a PDP-11/7 mainframe for the school and instituted a course curriculum that allowed students to do whatever they wanted on the computers, including playing games, as long as they had completed assignments by the end of each semester. Fenlason, Woodland, Thome, and Payne met through these courses and became a close-group of friends and competent programmers. Harvey had invited the group to the computer labs at UC Berkeley where they had the opportunity to use the mainframe systems there, and were introduced to "Rogue", inspiring them to create their own version as their class project. Fenlason had created a list of features they wanted to improve upon in "Rogue" such as having a level's layout saved once the player moved off that level. They approached Toy and Arnold at a local USENIX conference for the source code to "Rogue", but were refused, forcing them to develop the routines from scratch. The resulting program, "Hack", stayed true to the original "Dungeons and Dragons" influences, and derived its name from being both a "hack and slash" game as well as a programming hack to recreate "Rogue" without having access to its source code. Fenlason was not able to include all the desired features, and his involvement in "Hack"s development concluded after the students had left the school. Fenlason had provided the source code to "Hack" to the USENIX conferences to be distributed on their digital tapes, from which it was later discovered and built upon through USENET newsgroups, porting it to various systems. Like "Angband", the maintainership of the "Hack" code passed through several hands, and some variants were created by different forks.
"Hack" would eventually be dropped in favor of "NetHack" (1987). When Mike Stephenson, an analyst at a computer hardware manufacture, took maintainership of "Hack"s code, he took the opportunity to improve upon it along with suggestions for game improvements from Izchak Miller, a philosophy professor at University of Pennsylvania, and Janet Walz, another computer hacker. Calling themselves the DevTeam, they began to make major modifications to "Hack"s code. They named their new version "NetHack", in part for the collaboration of the game over USENET. "NetHack"s major deviations from "Hack" were the introduction of a wider variety of monsters, borrowing from other mythologies and lores, including anachronistic and contemporary cultural elements (such as a tourist class with a flash-bulb camera inspired by Terry Pratchet's "Discworld" series) in the high fantasy setting, and the use of pre-defined levels with some procedural elements that the player would encounter deeper in the dungeons. Further iterations of the game included branching pathways through the dungeon and optional character-based quests that could grant the player an extremely useful item to complete the game. Though the DevTeam released the code publicly, they carefully maintained who could contribute to the code base to avoid excessive forking of the vanilla game, and remain relatively quiet about suggested improvements to each release, working in relatively secrecy from its player base. There are separate forks of "NetHack", such as "Slash'EM".
"Ancient Domains of Mystery" (1994), or "ADOM" for short, derived from concepts presented in "NetHack". "ADOM" was originally developed by Thomas Biskup while a student at Technical University of Dortmund. After playing through "Rogue" and "Hack", he came to "NetHack" and was inspired by the game but dismayed at the complexity and elements he found unnecessary or distracting. Biskup created "ADOM" from scratch with the aim of creating a more story-driven game than "NetHack" that kept the depth of gameplay with a focused theme and setting. The resulting game featured several different dungeons, many generated procedurally, connected through an overworld map of the fictional realm of Ancardia, and would have the player complete various quests in those dungeons to progress the game. A major feature was the influence of Chaos forces through unsealed portals, which the player would have to close. While in areas affected by Chaos, the player's character would become tainted, causing mutations that could be either detrimental or beneficial. "ADOM", like "NetHack" and "Angband", would gain a devteam to maintain the code and implement updates and patches to avoid excessive forking.
Other variants.
Not all early roguelikes were readily classified as "Hack" or "Moria" descendants. "Larn" (1986), developed by Noah Morgan, borrowed concepts from both "Hack" (in that there are persistent and fixed levels) and "Moria" (in the availability of a shop level and general difficulty increasing with dungeon level), but while these two games have spiraled in size to take multiple play sessions to complete, "Larn" was aimed to be completed in a single session. "Larn" also uses a fixed-time feature, in that the player had only so many turns to complete a goal, though there were ways to jump back in time as to extend play. This was further limited by the stamina decay created by the need to feed the character, as failure to find food would result in the character's death. "Omega", developed by the Laurence Brothers in the late 1980s, is credited with introducing an overworld concept to the roguelike genre, prior to the feature's appearance in "ADOM". "Omega" was often remembered for its odd inventory approach in which the player would have to pick up an object, considering it being held, and then moving that object to a bag or an equipment slot. "Linley's Dungeon Crawl" (1995) was created by Linley Henzell and featured a skill-based character progression system, in which experience points could be used to improve specific skills, such as weapon proficiency or trap detection. One fork of this would form the basis for "Dungeon Crawl Stone Soup" (2006).
"Mystery Dungeon" games (1995–onward).
A variation of the roguelike genre came from Japan, primarily through the "Mystery Dungeon" series by Chunsoft. The first game in this series was based on the "Dragon Quest" series. Producer Koichi Nakamura stated the game was directly inspired by "Rogue", looking to make a "more understandable, more easy-to-play version" of the title. The developers had developed ways to reduce the difficulty of the roguelike by using progressively more difficult dungeons that were randomly generated, and made permadeath an option by selection of difficulty level. To date, over 25 games in the "Mystery Dungeon" series have been made across various platforms, and spanning various franchises, including "Chocobo" series based on "Final Fantasy" and "Pokémon Mystery Dungeon" based on "Pokémon". Several titles in the "Mystery Dungeon" series were popular, and would become a staple of the Japanese video game market. The games were not as successful in Western markets when published there, as the target players – younger players who likely had not experienced games like "Rogue" – found the lack of a traditional role-playing game save system odd.
Other Japanese role-playing games would incorporate random dungeon generation as part of their design, mimicking part of the nature of roguelikes, and were considered roguelike titles when published in Western markets. Such titles include "Vagrant Story", "Shining Soul", and "Baroque".
Continued development in Western markets (2002–onward).
Though new classical roguelike variants would continue to be developed within the Western market, the genre languished as more advanced personal computers capable of improved graphics capabilities and games that utilized these features became popular. However, some of these new graphical games drew influence for roguelike concepts, notably action role-playing games like Blizzard Entertainment's "Diablo" (1996). "Diablo"s creator, David Brevik, acknowledged that games like "Rogue", "Nethack", "Telengard" and other roguelikes influenced the design of "Diablo", including the nature of randomly generated dungeons and loot.
Existing roguelikes continue to be developed: a sequel to "ADOM" successfully received crowd funding in 2012, while "NetHack"s first major release in ten years in 2015 is set to help the DevTeam expand the game further. New roguelikes that adhere to core Berlin Interpretation rules are still being created, including "Dungeon Crawl Stone Soup" (2006), "Dungeons of Dredmor" (2011), and "Dragon Fin Soup" (2015). A subclass of "coffeebreak roguelikes" that could be completed in a short period of time have developed, often derived from entries in the Seven Day Roguelike Challenge; examples include such as "DoomRL" (2013) and "Desktop Dungeons" (2013) Some games would also take advantage of the ease of developing in the tile-based ASCII interfaces common to roguelikes. For example, the highly popular "Dwarf Fortress" (2006) uses the roguelike interface atop a construction and management simulation, and would serve as a major inspiration for "Minecraft", while "SanctuaryRPG" (2014) is a more traditional turn-based role playing game featuring a scripted story that uses an ASCII interface and roguelike gameplay elements. "UnReal World" (1992), the game that is considered to be the forerunner of the survival game genre, and which frequently uses procedural generation to create the worlds that players must survive in, was developed by Sami Maaranen and was influenced by roguelikes, with its initial interface being similar to that of "
NetHack".
Growth of the roguelike-like (2005-onward).
The roguelike genre saw a resurgence in Western markets after 2000 through independent developers that created a new subgenre designated the roguelike-like. Indie developers began to incorporate roguelike elements into genres not normally associated with roguelikes, creating games that would form the basis of this new subgenre. Two of the earliest cited examples of roguelike-likes are "Strange Adventures in Infinite Space" (2002) and its sequel "" (2005) by Digital Eel, both space exploration games that included randomly generated planets and encounters, and permadeath. Digital Eel based their work on the space exploration game "Starflight" along with roguelikes like "Nethack" but wanted to provide a shorter experience that would be easier to replay, akin to tabletop beer and pretzels games like "Deathmaze" and "The Sorcerer's Cave" that has elements in common with roguelikes.
"Spelunky" (2008), released shortly after the formation of the Berlin Interpretation, is considered to be a major contribution to the growth of indie-developed roguelike-likes. "Spelunky" was developed by Derek Yu, who wanted to take the deep gameplay that is offered by roguelikes and combine it with the ease and pick-up-and-play of a platformer. The result was a platform game incorporating the notion of permadeath in which the player takes an explorer character through randomly-generated underground caves. The intent was to create "deep" gameplay in which the game could be replayed over and over again, with the randomly generated situations driving the need for the player to develop novel, emergent strategies on the fly. Developer Jason Rohrer stated that "Spelunky" "totally revamped my thinking about single-player videogame design".
The formula and success of both "Weird Worlds" and "Spelunky" would influence other developers in creating new roguelike-likes. Edmund McMillen, the developer of "The Binding of Isaac" (2011), and Kenny and Teddy Lee, the co-develoeprs of "Rogue Legacy" (2012), credit Yu's approach with "Spelunky" as showing how to distill down the nature of a traditional roguelike to apply it to other gaming genres which they had done for their roguelike-likes. Justin Ma and Matthew Davis, the co-developers of "" (2012), credited both "Weird Worlds: Return to Infinite Space" and "Spelunky" as part of their influence for "FTL". All of these games earned critical praise, and their success has led to a more modern resurgence in roguelike-likes since their release.
The newfound success in roguelike-likes is considered part of a larger trend in those that play both board and computer games, looking for "rich play experiences", as described by "100 Rogues" developer Keith Burgun, that more popular titles may not always offer. David Bamguart of Gaslight Games stated that there is a thrill of the risk inherent in roguelike-likes with random generation and permadeath, helping the player become more invested in the fate of their player character: "The deadly precariousness inherent to the unknown environments of roguelikes gives that investment a great deal of meaning." Additionally, many of these newer roguelike-likes strive to address the apparent high difficulty and ruthlessness that traditional roguelikes were known for, and newer players will be able to find more help through user-generated game guides and walkthroughs made possible through wide Internet accessibility. Fabien Fischer offers that players have taken to independently-developed roguelike-likes as they have tired from "superficial gameplay, whitewashing spectacle, the content craze, and Skinner Box design" in titles produced by AAA developers and publishers.
Particularly for smaller indie developers, the nature of the procedural-generated world allows teams to deliver many hours worth of game content without having to spend resources and development time on fleshing out detailed worlds. This also allows developers to devote more time in building out complex gameplay features and their interacting systems that are part of the enjoyment of roguelike and roguelike-like games. McMillen of "The Binding of Isaac" stated that incorporating roguelike elements into other game genres can be difficult due to overcoming the complex interfaces roguelikes tend to have, but once a means to do so is found, "it becomes an increasingly beautiful, deep, and everlasting design that allows you to generate a seemingly dynamic experience for players, so that each time they play your game they're getting a totally new adventure".
With the expansion of both classical roguelikes and modern roguelike-like titles, there has been more interest in developing roguelikes. The 7 Day Roguelike challenge (7DRL) was born out of a USENET newsgroup in 2005 for roguelike developers, informally challenging them to create the core of a novel roguelike within 7 days to be submitted for judging and play by the public. The competition has continued annually each year, since growing from 5–6 entries in 2005 to over 130 in 2014.

</doc>
<doc id="47886" url="https://en.wikipedia.org/wiki?curid=47886" title="Disruptive innovation">
Disruptive innovation

A disruptive innovation is an innovation that creates a new market and value network and eventually disrupts an existing market and value network, displacing established market leaders and alliances. The term was defined and phenomenon analyzed by Clayton M. Christensen beginning in 1995. More recent sources also include "signiﬁcant societal impact" as an aspect of disruptive innovation.
Not all innovations are disruptive, even if they are revolutionary. For example, the automobile was not a disruptive innovation, because early automobiles were expensive luxury items that did not disrupt the market for horse-drawn vehicles. The market for transportation essentially remained intact until the debut of the lower-priced Ford Model T in 1908. The "mass-produced" automobile was a disruptive innovation, because it changed the transportation market, whereas the first thirty years of automobiles did not.
Disruptive innovations tend to be produced by outsiders. The business environment of market leaders does not allow them to pursue disruption when they first arise, because they are not profitable enough at first and because their development can take scarce resources away from sustaining innovations (which are needed to compete against current competition). A disruptive process can take longer to develop than by the conventional approach and the risk associated to it is higher than the other more incremental or evolutionary forms of innovations, but once it is deployed in the market, it achieves a much faster penetration and higher degree of impact on the established markets.
History and usage of the term.
The term disruptive technologies was coined by Clayton M. Christensen and introduced in his 1995 article "Disruptive Technologies: Catching the Wave", which he cowrote with Joseph Bower. The article is aimed at management executives who make the funding or purchasing decisions in companies, rather than the research community. He describes the term further in his book "The Innovator's Dilemma". "Innovator's Dilemma" explored the cases of the disk drive industry (which, with its rapid generational change, is to the study of business what fruit flies are to the study of genetics, as Christensen was advised in the 1990s) and the excavating equipment industry (where hydraulic actuation slowly displaced cable-actuated movement). In his sequel with Michael E. Raynor, "The Innovator's Solution", Christensen replaced the term "disruptive technology" with "disruptive innovation" because he recognized that few technologies are intrinsically disruptive or sustaining in character; rather, it is the "business model" that the technology enables that creates the disruptive impact. However, Christensen's evolution from a technological focus to a business-modelling focus is central to understanding the evolution of business at the market or industry level. Christensen and Mark W. Johnson, who cofounded the management consulting firm Innosight, described the dynamics of "business model innovation" in the 2008 "Harvard Business Review" article "Reinventing Your Business Model". The concept of disruptive technology continues a long tradition of identifying radical technical change in the study of innovation by economists, and the development of tools for its management at a firm or policy level.
In the late 1990s, the automotive sector began to embrace a perspective of "constructive disruptive technology" by working with the consultant David E. O'Ryan, whereby the use of current off-the-shelf technology was integrated with newer innovation to create what he called "an unfair advantage". The process or technology change as a whole had to be "constructive" in improving the current method of manufacturing, yet disruptively impact the whole of the business case model, resulting in a significant reduction of waste, energy, materials, labor, or legacy costs to the user.
In keeping with the insight that what matters economically is the business model, not the technological sophistication itself, Christensen's theory explains why many disruptive innovations are "not" "advanced technologies", which the technology mudslide hypothesis would lead one to expect. Rather, they are often novel combinations of existing off-the-shelf components, applied cleverly to a small, fledgling value network.
Theory.
Christensen defines a disruptive innovation as a product or service designed for a new set of customers.
"Generally, disruptive innovations were technologically straightforward, consisting of off-the-shelf components put together in a product architecture that was often simpler than prior approaches. They offered less of what customers in established markets wanted and so could rarely be initially employed there. They offered a different package of attributes valued only in emerging markets remote from, and unimportant to, the mainstream."
Christensen argues that disruptive innovations can hurt successful, well-managed companies that are responsive to their customers and have excellent research and development. These companies tend to ignore the markets most susceptible to disruptive innovations, because the markets have very tight profit margins and are too small to provide a good growth rate to an established (sizable) firm. Thus, disruptive technology provides an example of an instance when the common business-world advice to "focus on the customer" (or "stay close to the customer", or "listen to the customer") can be strategically counterproductive.
While Christensen argued that disruptive innovations can hurt successful, well-managed companies, O'Ryan countered that "constructive" integration of existing, new, and forward-thinking innovation could improve the economic benefits of these same well-managed companies, once decision-making management understood the systemic benefits as a whole.
Christensen distinguishes between "low-end disruption", which targets customers who do not need the full performance valued by customers at the high end of the market, and "new-market disruption", which targets customers who have needs that were previously unserved by existing incumbents.
"Low-end disruption" occurs when the rate at which products improve exceeds the rate at which customers can adopt the new performance. Therefore, at some point the performance of the product overshoots the needs of certain customer segments. At this point, a disruptive technology may enter the market and provide a product that has lower performance than the incumbent but that exceeds the requirements of certain segments, thereby gaining a foothold in the market.
In low-end disruption, the disruptor is focused initially on serving the least profitable customer, who is happy with a good enough product. This type of customer is not willing to pay premium for enhancements in product functionality. Once the disruptor has gained a foothold in this customer segment, it seeks to improve its profit margin. To get higher profit margins, the disruptor needs to enter the segment where the customer is willing to pay a little more for higher quality. To ensure this quality in its product, the disruptor needs to innovate. The incumbent will not do much to retain its share in a not-so-profitable segment, and will move up-market and focus on its more attractive customers. After a number of such encounters, the incumbent is squeezed into smaller markets than it was previously serving. And then, finally, the disruptive technology meets the demands of the most profitable segment and drives the established company out of the market.
"New market disruption" occurs when a product fits a new or emerging market segment that is not being served by existing incumbents in the industry.
The extrapolation of the theory to all aspects of life has been challenged, as has the methodology of relying on selected case studies as the principal form of evidence. Jill Lepore points out that some companies identified by the theory as victims of disruption a decade or more ago, rather than being defunct, remain dominant in their industries today (including Seagate Technology, U.S. Steel, and Bucyrus). Lepore questions whether the theory has been oversold and misapplied, as if it were able to explain everything in every sphere of life, including not just business but education and public institutions.
Disruptive technology.
In 2009, Milan Zeleny described high technology as disruptive technology and raised the question of what is being disrupted. The answer, according to Zeleny, is the "support network" of high technology. For example, introducing electric cars disrupts the support network for gasoline cars (network of gas and service stations). Such disruption is fully expected and therefore effectively resisted by support net owners. In the long run, high (disruptive) technology bypasses, upgrades, or replaces the outdated support network.
Technology, being a form of social relationship, always evolves. No technology remains fixed. Technology starts, develops, persists, mutates, stagnates, and declines, just like living organisms. The evolutionary life cycle occurs in the use and development of any technology. A new high-technology core emerges and challenges existing technology support nets (TSNs), which are thus forced to coevolve with it. New versions of the core are designed and fitted into an increasingly appropriate TSN, with smaller and smaller high-technology effects. High technology becomes regular technology, with more efficient versions fitting the same support net. Finally, even the efficiency gains diminish, emphasis shifts to product tertiary attributes (appearance, style), and technology becomes TSN-preserving appropriate technology. This technological equilibrium state becomes established and fixated, resisting being interrupted by a technological mutation; then new high technology appears and the cycle is repeated.
Regarding this evolving process of technology, Christensen said:
Joseph Bower explained the process of how disruptive technology, through its requisite support net, dramatically transforms a certain industry.
The automobile was high technology with respect to the horse carriage; however, it evolved into technology and finally into appropriate technology with a stable, unchanging TSN. The main high-technology advance in the offing is some form of electric car—whether the energy source is the sun, hydrogen, water, air pressure, or traditional charging outlet. Electric cars preceded the gasoline automobile by many decades and are now returning to replace the traditional gasoline automobile.
Milan Zeleny described the above phenomenon. He also wrote that:
High-technology effects.
High technology is a technology core that changes the very architecture (structure and organization) of the components of the technology support net. High technology therefore transforms the qualitative nature of the TSN's tasks and their relations, as well as their requisite physical, energy, and information flows. It also affects the skills required, the roles played, and the styles of management and coordination—the organizational culture itself.
This kind of technology core is different from regular technology core, which preserves the qualitative nature of flows and the structure of the support and only allows users to perform the same tasks in the same way, but faster, more reliably, in larger quantities, or more efficiently. It is also different from appropriate technology core, which preserves the TSN itself with the purpose of technology implementation and allows users to do the same thing in the same way at comparable levels of efficiency, instead of improving the efficiency of performance.
As for the difference between high technology and low technology, Milan Zeleny once said:
However, not all modern technologies are high technologies. They have to be used as such, function as such, and be embedded in their requisite TSNs. They have to empower the individual because only through the individual can they empower knowledge. Not all information technologies have integrative effects. Some information systems are still designed to improve the traditional hierarchy of command and thus preserve and entrench the existing TSN. The administrative model of management, for instance, further aggravates the division of task and labor, further specializes knowledge, separates management from workers, and concentrates information and knowledge in centers.
As knowledge surpasses capital, labor, and raw materials as the dominant economic resource, technologies are also starting to reflect this shift. Technologies are rapidly shifting from centralized hierarchies to distributed networks. Nowadays knowledge does not reside in a super-mind, super-book, or super-database, but in a complex relational pattern of networks brought forth to coordinate human action.
Practical example of disruption.
In the practical world, the popularization of personal computers illustrates how knowledge contributes to the ongoing technology innovation. The original centralized concept (one computer, many persons) is a knowledge-defying idea of the prehistory of computing, and its inadequacies and failures have become clearly apparent. The era of personal computing brought powerful computers "on every desk" (one person, one computer). This short transitional period was necessary for getting used to the new computing environment, but was inadequate from the vantage point of producing knowledge. Adequate knowledge creation and management come mainly from networking and distributed computing (one person, many computers). Each person's computer must form an access point to the entire computing landscape or ecology through the Internet of other computers, databases, and mainframes, as well as production, distribution, and retailing facilities, and the like. For the first time, technology empowers individuals rather than external hierarchies. It transfers influence and power where it optimally belongs: at the loci of the useful knowledge. Even though hierarchies and bureaucracies do not innovate, free and empowered individuals do; knowledge, innovation, spontaneity, and self-reliance are becoming increasingly valued and promoted.

</doc>
<doc id="47889" url="https://en.wikipedia.org/wiki?curid=47889" title="212">
212

__NOTOC__
Year 212 (CCXII) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Asper and Camilius (or, less frequently, year 965 "Ab urbe condita"). The denomination 212 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>

</doc>
<doc id="47891" url="https://en.wikipedia.org/wiki?curid=47891" title="Augur">
Augur

The augur was a priest and official in the classical Roman world. His main role was the practice of augury, interpreting the will of the gods by studying the flight of birds: whether they are flying in groups or alone, what noises they make as they fly, direction of flight and what kind of birds they are. This was known as "taking the auspices." The ceremony and function of the augur was central to any major undertaking in Roman society—public or private—including matters of war, commerce, and religion.
The Roman historian Livy stresses the importance of the augurs: "Who does not know that this city was founded only after taking the auspices, that everything in war and in peace, at home and abroad, was done only after taking the auspices?"
Etymology.
Although ancient authors believed that the term "augur" contained the words "avis" and "gero"—Latin for "directing the birds"—historical-linguistic evidence points instead to the root "aug-": "to increase, to prosper."
Public role.
Roman augurs were part of a college (Latin "collegium") of priests who shared the duties and responsibilities of the position. At the foundation of the Republic in 510 BC, the patricians held sole claim to this office; by 300 BC, the office was open to plebeian occupation as well. Senior members of the "collegium" put forth nominations for any vacancies, and members voted on whom to co-opt.
In the Regal period tradition holds that there were three augurs at a time; by the time of Sulla, they had reached fifteen in number.
Augury sought the divine will regarding any proposed course of action which might affect Rome's "pax", "fortuna" and "salus" (peace, good fortune and wellbeing). 
Political, military and civil actions were sanctioned by augury, historically performed by priests of the college of augurs and by haruspices on behalf of senior magistrates. The presiding magistrate at an augural rite thus held the “right of augury” ("ius augurii"). Magistracies (which included senior military and civil ranks) were therefore religious offices in their own right, and magistrates were directly responsible for the "pax", "fortuna" and "salus" of Rome and everything that was Roman.
The effectiveness of augury could only be judged retrospectively; the divinely ordained condition of peace ("pax deorum") was an outcome of successful augury. Those whose actions had led to divine wrath ("ira deorum") could not have possessed a true right of augury ("ius augurum"). Of all the protagonists in the Civil War, only Octavian could have possessed it, because he alone had restored the "pax deorum" to the Roman people. Lucan, writing during the Principate, described the recent Civil War as "unnatural" - a mirror to supernatural disturbances in the greater cosmos. His imagery is apt to the traditional principles of augury and its broader interpretation by Stoic apologists of the Imperial cult. In the Stoic cosmology, "pax deorum" is the expression of natural order in human affairs.
According to Cicero, the "auctoritas" of "ius augurum" included the right to adjourn and overturn the process of law: consular election could be - and was - rendered invalid by inaugural error. For Cicero, this made the augur the most powerful authority in the Republic. Cicero himself was co-opted into the college only late in his career.
In the later Republic, augury came under the supervision of the college of "pontifices", a priestly-magistral office whose powers were increasingly woven into the "cursus honorum". The office of "pontifex maximus" eventually became a "de facto" consular prerogative. When his colleague Lepidus died, Augustus assumed his office as "pontifex maximus", took priestly control over the State oracles (including the Sibylline books), and used his powers as censor to suppress the circulation of "unapproved" oracles.
Augurs, "auguria" and auspices.
In ancient Rome the "auguria" were considered to be in equilibrium with the "sacra" ("sacred things" or "rites") and were not the only way by which the gods made their will known.
The "augures publici" (public augurs) concerned themselves only with matters related to the state.
According to Varro they used to distinguish five kinds of territory: ager Romanus, ager Gabinus, ager peregrinus, ager hosticus, ager incertus: these distinctions clearly point to the times of the prehistory of Latium and testify the archaic quality of the art of augury.
The "jus augurale" (augural law) was rigorously secret, therefore very little about the technical aspects of ceremonies and rituals has been recorded. We have only the names of some "auguria" (augural rites): e.g. the "augurium salutis" which took place once a year before the magistrates and the people, in which the gods were asked whether it was auspicious to ask to for the welfare of the Romans, the "augurium canarium" and the "vernisera auguria". The first one required the sacrifice of red dogs and took place before wheat grains were shelled but not before they had formed. Of the second we know only the name that implies a ritual related to the harvest.
"Augurium" and "auspicium" are terms used indifferently by the ancient. Modern scholars have debated the issue at length but have failed to find a distinctive definition that may hold for all the known cases. By such considerations Dumezil thinks that the two terms refer in fact to two aspects of the same religious act:
"auspicium" would design the technical process of the operation, i.e. "aves spicere", looking at the birds. His result would be the "augurium", i.e. the determination, acknowledgement of the presence of the "*auges", the favour of the god(s), the intention and the final result of the whole operation. In Varro's words "Agere augurium, aves specit", "to conduct the "augurium", he observed the birds".
Since "auguria publica" and inaugurations of magistrates are strictly connected to political life this brought about the deterioration and abuses that condemned augury to progressive and inarrestable debasement, stripping it of all religious value.
The role of the augur was that of consulting and interpreting the will of gods about some course of action such as accession of kings to the throne, of magistrates and major "sacerdotes" to their functions (inauguration) and all public enterprises.
The prototype of the ritual of inauguration of people is described in Livy's relation of the inauguration of king Numa Pompilius. The augur asks Jupiter (signa belong to Jupiter): "Si fas est (i.e. if it is divine justice to do this)... send me a certain "signum" (sign)", then the augur listed the "auspicia" he wanted to see coming. When they appeared Numa was declared king.
Technically the sky was divided into four sections or regions: dextera, sinistra, antica and postica (right, left, anterior and posterior).
Before taking the "auspicia impetrativa" ("requested" or "sought" auspices; see below) the "templum", or sacred space within which the operation would take place had to be established and delimited (it should be square and have only one entrance) and purified (effari, liberare).
The auspicia were divided into two categories: requested by man ("impetrativa") and offered spontaneously by the gods ("oblativa").
During a ceremony the enunciation of the requested auspicia was technically called "legum dictio".
Magistrates endowed by the law with the right of "spectio" (observation of auspices) would establish the requested the "auspicium".
To the augur was reserved the "nuntiatio" i.e. announcing the appearance of "auspicia oblativa" that would require the interruption of the operation.
The science of interpretation of signs was vast and complex.
Only some species of birds ("aves augurales") could yield valid signs whose meaning would vary according to the species. Among them were ravens, woodpeckers, owls, ossifragae, eagles.
Signs from birds were divided into "alites", from the flight, and "oscines", from the voice.
The "alites" included region of the sky, height and type of flight, behaviour of the bird and place where it would rest.
The "oscines" included the pitch and direction of the sound.
Since the observation was complex conflict among signs was not uncommon.
A hierarchy among signs was devised: e.g. a sign from the eagle would prevail on that from the woodpecker and the ossifragae (parra).
Observation conditions were rigorous and required absolute silence for validity of the operation.
Both "impetrativa" and "oblativa" auspices could be divided into five classes:
ex caelo (thunder,lightning), ex avibus, ex tripudiis (attitude to food and feeding manner of the sacred chickens), ex quadrupedibus (dog, horse, wolf, fox), ex diris (ominous events).
During the last centuries of the republic the auspices "ex caelo" and "ex tripudiis" supplanted other types, as they could be easily used in a fraudulent way, i.e. bent to suit the desire of the asking person. It sufficed to say that the augur or magistrate had heard a clap of thunder to suspend the convocation of the comitia.
Cicero condemned the fraudulent use and denounced the decline in the level of knowledge of the doctrine by the augurs of his time.
In fact the abuse developed from the protective tricks devised to avoid being paralysed by negative signs. For an instance see the conversation between king Numa and Jupiter in Ovid, "Fasti" III, 339–344.
Against the negative "auspicia oblativa" the admitted procedures included:
Attus Navius.
Contrary to other divinatory practices present in Rome (e.g. haruspicina, consultation of the libri Sibyllini) augury appears to be autochthonous and originally Latin or Italic. The art has its roots in the prehistory of the Italic people and is attested in the Iguvine Tables ("avif aseria") and among other Latin tribes. The very story or legend of the foundation of Rome is based on augury, i.e. the ascertaining of the will of gods through observation of the sky and of birds. Romulus and Remus indeed acted as augurs and Romulus was considered a great augur throughout the course of his life.
The character that best represented and portrayed the art however was Attus Navius. His story is related by Cicero.
He was born into a very poor family. One day he lost one of his pigs. He then promised the gods that if he found it, he would offer them the biggest grapes growing in his vineyard. After recovering his pig he stood right at the middle of his grape yard facing South. He divided the sky into four sections and observed birds: when they appeared he walked in that direction and found an extraordinary large grape that he offered to the gods.
His story was immediately famous and he became the augur of the king (see above the episode with king Tarquinius narrated by Livy). Henceforth he was considered the patron of the augurs.

</doc>
<doc id="47892" url="https://en.wikipedia.org/wiki?curid=47892" title="Bay of Bengal">
Bay of Bengal

The Bay of Bengal, the largest bay in the world, forms the northeastern part of the Indian Ocean. Roughly triangular, it is bordered mostly by India and Sri Lanka to the west, Bangladesh to the north, and Myanmar (Burma) and the Andaman and Nicobar Islands to the east.
The Bay of Bengal occupies an area of . A number of large rivers – the Ganges and its tributaries such as the Padma and Hooghly, the Brahmaputra and its tributaries such as the Jamuna and Meghna, other rivers such as the Irrawaddy River, Godavari, Mahanadi, Krishna and Kaveri flow into the Bay of Bengal. Among the important ports are Chennai, Chittagong, Kolkata, Mongla, Paradip, Tuticorin, Visakhapatnam and Yangon.
Extent.
The International Hydrographic Organization defines the limits of the Bay of Bengal as follows:
Etymology.
In ancient Hindu scriptures, this water body is referred to as '"Mahodadhi"' (Sanskrit: महोदधि, lit. "great water receptacle") while it appears as "Sinus Gangeticus" or "Gangeticus Sinus", meaning "Gulf of the Ganges", in ancient maps.
The other Sanskrit names for Bay of Bengal are '"Vangopasagara"' (Sanskrit: वङ्गोपसागर, lit. "Bengal's Bay"), also simply called as '"Vangasagara"' (Sanskrit: वङ्गसागर, lit. "Bengal Sea") and '"Purvapayodhi"' (Sanskrit: पूर्वपयोधि, lit. "Eastern Ocean").
Rivers.
Many major rivers of the Indian subcontinent flow west to east before draining into the Bay of Bengal. The Ganges is the northernmost of theses. Its main channel enters and flows through Bangladesh, where it is known as the Padma River, before joining the Meghna River. However, the Brahmaputra River flows from east to west in Assam before turning south and entering Bangladesh where it is called the Jamuna River. This joins the Padma whereupon the Padma joins the Meghna River that finally drains into Bay of Bengal. The Sundarbans mangrove forest at the delta of the Ganges, Brahmaputra and Meghna rivers lies partly in West Bengal and partly in Bangladesh. The Brahmaputra at is the 28th longest River in the world. It originates in Tibet. The Hooghly River, another channel of the Ganga that flows through Calcutta drains into Bay of Bengal in India itself.
The Ganga–Brahmaputra rivers deposits nearly 1000 million tons of sediment per year. The sediment from these two rivers forms the Bengal Delta and the submarine fan, a vast structure that extends from Bangladesh to south of the Equator, is up to thick, and contains at least 1,130 trillion tonnes of sediment, which has accumulated over the last 17 million years at an average rate of 665 million tons per annum. The Bay of Bengal used to be deeper than the Mariana Trench, the present deepest ocean point. The fan has buried organic carbon at a rate of nearly 1.1 trillion mol/yr since the early Miocene period. The two rivers currently contribute nearly 8% of the total organic carbon (TOC) deposited in the world's oceans. Due to high TOC accumulation in the deep sea bed of the Bay of Bengal, the area is rich in oil and natural gas and gas hydrate reserves. Bangladesh can reclaim land substantially and economically from the sea area by constructing sea dikes, bunds, causeways and by trapping the sediment from its rivers.
Further south of Bengal, the Mahanadi, Godavari, Krishna and Kaveri Rivers are the major rivers that flow from west to east in the Indian subcontinent and drain into the Bay of Bengal. Many small rivers also drain directly into the Bay of Bengal; the shortest of them is the Cooum River at .
The Irrawaddy (or Ayeyarwady) River in Myanmar flows into the Andaman Sea of the Bay of Bengal and once had thick mangrove forest of its own.
Sea ports.
Some of the biggest ports in the world — Chennai, Visakhapatnam, Kolkata in India as well as Chittagong and Mongla in Bangladesh— are located in the bay.
Other Indian ports on the bay include: Kakinada, Pondicherry, Paradip, Dhamra, Gopalpur.
Islands.
The islands in the bay are numerous, including the Andaman Islands, Nicobar and Mergui groups of India. The Cheduba group of islands, in the north-east, off the Burmese coast, are remarkable for a chain of mud volcanoes, which are occasionally active. Great Andaman is the main archipelago or island group of the Andaman Islands, whereas Ritchie's Archipelago consists of smaller islands. Only 37, or 6.5%, of the 572 islands and islets of the Andaman and Nicobar Islands are inhabited.
Oceanography.
The Bay of Bengal is a salt water sea and is a part of the Indian Ocean.
Plate tectonics.
The lithosphere of the earth is broken up into what are called tectonic plates. Underneath the Bay of Bengal is the Indian Plate which is part of the great Indo-Australian Plate and is slowly moving north east. This plate meets the Burma Microplate at the Sunda Trench. The Nicobar Islands, and the Andaman Islands are part of the Burma Microplate. The India Plate subducts beneath the Burma Plate at the Sunda Trench or Java Trench. Here, the pressure of the two plates on each other increase pressure and temperature resulting in the formation of volcanoes such as the volcanoes in Myanmar, and a volcanic arc called the Sunda Arc. The Sumatra-Andaman earthquake and Asian Tsunami was a result of the pressure at this zone causing a submarine earthquake which then resulted in a huge Tsunami.
Marine geology.
A zone 50 m wide extending from the island of Ceylon and the Coromandel coast to the head of the bay, and thence southwards through a strip embracing the Andaman and Nicobar islands, is bounded by the 100 fathom line of sea bottom; some 50 m. beyond this lies the 500-fathom limit. Opposite the mouth of the Ganges, however, the intervals between these depths are very much extended by deltaic influence.
Swatch of No Ground is a 14 km-wide deep sea canyon of the Bay of Bengal. The deepest recorded area of this valley is about 1340 m. The submarine canyon is part of the Bengal Fan, the largest submarine fan in the world.
Marine biology, flora and fauna.
The Bay of Bengal is full of biological diversity, diverging amongst coral reefs, estuaries, fish spawning and nursery areas, and mangroves. The Bay of Bengal is one of the World's 64 largest marine ecosystems.
"Kerilia jerdonii" is a sea snake of the Bay of Bengal. Glory of Bengal cone ("Conus bengalensis") is just one of the seashells which can be photographed along beaches of the Bay of Bengal. An endangered species, the olive ridley sea turtle can survive because of the nesting grounds made available at the Gahirmatha Marine Wildlife Sanctuary, Gahirmatha Beach, Odisha, India. Marlin, barracuda, skipjack tuna, ("Katsuwonus pelamis"), yellowfin tuna, Indo-Pacific humpbacked dolphin ("Sousa chinensis"), and Bryde's whale ("Balaenoptera edeni") are a few of the marine animals. Bay of Bengal hogfish ("Bodianus neilli") is a type of wrass which live in turbid lagoon reefs or shallow coastal reefs. Schools of dolphins can be seen, whether they are the bottle nose dolphin ("Tursiops truncatus"), pantropical spotted dolphin ("Stenella attenuata") or the spinner dolphin ("Stenella longirostris"). Tuna and dolphins usually reside in the same waters. In shallower and warmer coastal waters the Irrawaddy dolphins ("Orcaella brevirostris") can be found.
The Great Nicobar Biosphere Reserve provides sanctuary to many animals some of which include the saltwater crocodile ("Crocodylus porosus"), giant leatherback sea turtle ("Dermochelys coriacea"), and Malayan box turtle ("Cuora amboinensis kamaroma") to name a few.
Another endangered species royal Bengal tiger is supported by Sundarbans a large estuarine delta that holds a mangrove area in the Ganges River Delta.
Chemical oceanography.
Coastal regions bordering the Bay of Bengal are rich in minerals. Sri Lanka, Serendib, or "Ratna – Dweepa" which means Gem Island. Amethyst, beryl, ruby, sapphire, topaz, and garnet are just some of the gems of Sri Lanka. Garnet and other precious gems are also found in abundance in the Indian states of Odisha and Andhra Pradesh.
Physical oceanography - climate.
From January to October, the current is northward flowing, and the clockwise circulation pattern is called the "East Indian Current". The Bay of Bengal monsoon moves in a northwest direction striking the Nicobar Islands, and the Andaman Islands first end of May, then the north eastern coast of India by end of June.
The remainder of the year, the counterclockwise current is southwestward flowing, and the circulation pattern is called the East Indian Winter Jet. September and December see very active weather, season varsha (or monsoon), in the Bay of Bengal producing severe cyclones which affect eastern India. Several efforts have been initiated to cope with storm surge.
Tropical storms and cyclones.
A tropical storm with rotating winds blowing at speeds of 74 miles (119 kilometres) per hour is called a cyclone when they originate over the Bay of Bengal; and called a hurricane in the Atlantic. Between 100,000 and 500,000 residents of Bangladesh were killed because of the 1970 Bhola cyclone.
Religious importance.
The Bay of Bengal in the stretch of Swargadwar , the gateway to heaven in Sanskrit , in the Indian town of Puri is considered holy by Hindus.
The Samudra arati is a daily tradition started by the present Shankaracharya of Puri 9 years ago to honour the sacred sea. The daily practise includes prayer and fire offering to the sea at Swargadwar in Puri by disciples of the Govardhana matha of the Shankaracharya. On Paush Purnima of every year the Shankaracharya himself comes out to offer prayers to the sea.
Economy.
One of the first trading ventures along the Bay of Bengal was The Company of Merchants of London Trading into the East Indies more commonly referred to as British East India Company. Gopalpur-on-Sea was one of their main trading centers. Other trading companies along the Bay of Bengal shorelines were English East India Company and French East India Company.
BIMSTEC Bay of Bengal Initiative for MultiSectoral Technical and Economic Cooperation (BIMSTEC) supports free trade internationally around the Bay of Bengal between Bangladesh, Bhutan, India, Myanmar, Nepal, Sri Lanka, and Thailand.
The Sethusamudram Shipping Canal Project is a new venture proposed which would create a channel for a shipping route to link the Gulf of Mannar with the Bay of Bengal. This would connect India east to west without the necessity of going around Sri Lanka.
Thoni and catamaran fishing boats of fishing villages thrive along the Bay of Bengal shorelines. Fishermen can catch between 26 and 44 species of marine fish. In one year, the average catch is two million tons of fish from the Bay of Bengal alone.. Approximately 31% of the world’s coastal fishermen live and work on the bay.
Strategic importance.
The Bay of Bengal is centrally located in the region from the Middle East to the Philippine Sea. Even from aviations strategic aerial position it lies at the centre. It lies at dead center of two huge economic blocks, the SAARC and ASEAN. China's southern landlocked region in the north, and major sea ports of Bangladesh and India. Bangladesh, China and India has forged naval cooperation agreements with Malaysia, Thailand and Indonesia to increase its cooperation in checking terrorism in the high seas.
The Bay of Bengal is strategically crucial for India since it is a natural extension of its sphere of influence. Secondly because of the presence of outlying islands, namely Andaman islands and Nicobar islands and most importantly several major ports such as Kolkata, Chennai, Visakhapatnam, and Tuticorin along its coast with the Bay of Bengal.
China has recently made some efforts to project influence into the region through tie-ups with Myanmar and Bangladesh. The United States has held several major exercises with Bangladesh, India, Malaysia, Singapore and Thailand. The largest ever wargame in Bay of Bengal, known as Malabar 2007, was held in 2007 and naval warships from US, Singapore, Japan and Australia took part in it. India, was also a participant.
Large deposits of natural gas also incited a serious up for grabs urgency by India. Disputes over rights of some oil and gas blocks have caused brief diplomatic spats between Myanmar and India with Bangladesh.
The disputed maritime boundary between Bangladesh and Myanmar has twice resulted in military tensions, in 2008 and 2009. Bangladesh is currently pursuing a settlement with Myanmar and India to the boundary dispute through the International Tribunal on the Law of the Sea.
It has been argued that the conceptual division of the Bay of Bengal region between South Asia and Southeast Asia may no longer be appropriate and that the rise of India as a major power may lead to a new mental mapping of the Bay of Bengal as a coherent strategic region.
Environmental hazards.
Pollution.
The Asian brown cloud, a layer of air pollution that covers much of South Asia and the Indian Ocean every year between January and March, and possibly also during earlier and later months, hangs over the Bay of Bengal. It is considered to be a combination of vehicle exhaust, smoke from cooking fires, and industrial discharges.
Transboundary issues affecting the marine ecosystem.
A transboundary issue is defined as an environmental problem in which either the cause of the problem and/or its impact is separated by a national boundary; or the problem contributes to a global environmental problem and finding regional solutions is considered to be a global environmental benefit. The eight Bay of Bengal countries have (2012) identified three major transboundary problems (or areas of concern) affecting the health of the Bay, that they can work on together. With the support of the Bay Of Bengal Large Marine Ecosystem Project (BOBLME), the eight countries are now (2012) developing responses to these issues and their causes, for future implementation as the Strategic Action Programme.
Overexploitation of fisheries.
Fisheries production in the Bay of Bengal is six million tonnes per year, more than seven percent of the world's catch. The major transboundary issues relating to shared fisheries are: a decline in the overall availability of fish resources; changes in species composition of catches; the high proportion of juvenile fish in the catch; and changes in marine biodiversity, especially through loss of vulnerable and endangered species. The transboundary nature of these issues are: that many fish stocks are shared between BOBLME countries through the transboundary migration of fish, or larvae. Fishing overlaps national jurisdictions, both legally and illegally – overcapacity and overfishing in one location forces a migration of fishers and vessels to other locations. All countries (to a greater or lesser degree) are experiencing difficulties in implementing fisheries management, especially the ecosystem approach to fisheries. Bay of Bengal countries contribute significantly to the global problem of loss of vulnerable and endangered species.
The main causes of the issues are: open access to fishing grounds; Government emphasis on increasing fish catches; inappropriate government subsidies provided to fishers; increasing fishing effort, especially from trawlers and purse seiners; high consumer demand for fish, including for seed and fishmeal for aquaculture; ineffective fisheries management; and illegal and destructive fishing.
Degradation of critical habitats.
The Bay of Bengal is an area of high biodiversity, with many endangered and vulnerable species.
The major transboundary issues relating to habitats are: the loss and degradation of mangrove habitats; degradation of coral reefs; and the loss of, and damage to, seagrasses. The transboundary nature of these major issues are: that all three critical habitats occur in all BOBLME countries. Coastal development for several varying uses of the land and sea are common in all BOBLME countries. Trade in products from all the habitats is transboundary in nature. Climate change impacts are shared by all BOBLME countries.
The main causes of the issues are: food security needs of the coastal poor; lack of coastal development plans; increasing trade in products from coastal habitats; coastal development and industrialization; ineffective marine protected areas and lack of enforcement; upstream development that affects water-flow; intensive upstream agricultural practices; and increasing tourism.
Pollution and water quality.
The major transboundary issues relating to pollution and water quality are: sewage-borne pathogens and organic load; solid waste/marine litter; increasing nutrient inputs; oil pollution; persistent organic pollutants (POPs) and persistent toxic substances (PTSs); sedimentation; and heavy metals.
The transboundary nature of these issues are: discharge of untreated/partially treated sewage being a common problem. Sewage and organic discharges from the Ganges-Brahmaputra-Meghna River are likely to be transboundary. Plastics and derelict fishing gear can be transported long distances across national boundaries. High nutrient discharges from rivers could intensify largescale hypoxia. Atmospheric transport of nutrients is inherently transboundary. Differences between countries with regard to regulation and enforcement of shipping discharges may drive discharges across boundaries. Tar balls are transported long distances. POPs/PTSs and mercury, including organo-mercury, undergo long-range transport. Sedimentation and most heavy metal contamination tend to be localized and lack a strong transboundary dimension.
The main causes of the issues are: increasing coastal population density and urbanization; higher consumption, resulting in more waste generated per person; insufficient funds allocated to waste management; migration of industry into BOBLME countries; and proliferation of small industries.
History.
Northern Circars occupied the western coast of the Bay of Bengal and is now considered to be India's Madras state. Chola dynasty (9th century to 12th century) when ruled by Rajaraja Chola I occupied the western coastline of the Bay of Bengal circa AD 1014, The Bay of Bengal was also called as the Chola Lake. The Kakatiya dynasty reached the western coastline of the Bay of Bengal between the Godavari and the Krishna rivers. Kushanas about the middle of the 1st century AD invaded northern India perhaps extending as far as the Bay of Bengal. Chandragupta Maurya extended the Maurya Dynasty across northern India to the Bay of Bengal. Hajipur was a stronghold of Portuguese Pirates. In the 16th century the Portuguese built trading posts in the north of the Bay of Bengal at Chittagong (Porto Grande) and Satgaon (Porto Pequeno). Before the arrival of British to India it was also known as "Kalinga Sagar".
British penal colony.
Cellular Jail or "Black Waters" built in 1896 on Ross Island, a part of the Andaman Island Chain. As early as 1858 this island was used as a British penal colony for political prisoners facing life imprisonment.
Marine archaeology.
Maritime archaeology or marine archaeology is the study of material remains of ancient peoples. A specialized branch, Archaeology of shipwrecks studies the salvaged artifacts of ancient ships. Stone anchors, amphorae shards, elephant tusks, hippopotamus teeth, ceramic pottery, a rare wood mast and lead ingots are examples which may survive the test of time for archaeologists to study and place the salvaged findings into a time line of history. coral reefs, tsunamis, cyclones, mangrove swamps, battles and a criss cross of sea routes in a high trading area combined with pirating have all contributed to shipwrecks in the Bay of Bengal.

</doc>
<doc id="47895" url="https://en.wikipedia.org/wiki?curid=47895" title="Stochastic process">
Stochastic process

In probability theory, a stochastic () process, or often random process, is a collection of random variables representing the evolution of some system of random values over time. This is the probabilistic counterpart to a deterministic process (or deterministic system). Instead of describing a process which can only evolve in one way (as in the case, for example, of solutions of an ordinary differential equation), in a stochastic, or random process, there is some indeterminacy: even if the initial condition (or starting point) is known, there are several (often infinitely many) directions in which the process may evolve.
In the simple case of discrete time, as opposed to continuous time, a stochastic process is a sequence of random variables. (For example, see Markov chain, also known as discrete-time Markov chain.) The random variables corresponding to various times may be completely different, the only requirement being that these different random quantities all take values in the same space (the codomain of the function). One approach may be to model these random variables as random functions of one or several deterministic arguments (in most cases, the time parameter). Although the random values of a stochastic process at different times may be independent random variables, in most commonly considered situations they exhibit complicated statistical dependence.
Familiar examples of stochastic processes include stock market and exchange rate fluctuations; signals such as speech; audio and video; medical data such as a patient's EKG, EEG, blood pressure or temperature; and random movement such as Brownian motion or random walks.
A generalization, the random field, is defined by letting the variables be parametrized by members of a topological space instead of time. Examples of random fields include static images, random terrain (landscapes), wind waves and composition variations of a heterogeneous material.
Formal definition and basic properties.
Definition.
Given a probability space formula_1 and a measurable space formula_2,
an "S"-valued stochastic process is a collection of "S"-valued
random variables on formula_3, indexed by a totally ordered set "T" ("time"). That is, a stochastic process "X" is a collection 
where each formula_5 is an "S"-valued random variable on formula_3. The space "S" is then called the state space of the process.
Finite-dimensional distributions.
Let "X" be an "S"-valued stochastic process. For every finite sequence formula_7, the "k"-tuple formula_8 is a random variable taking values in formula_9. The distribution formula_10 of this random variable is a probability measure on formula_9. This is called a finite-dimensional distribution of "X".
Under suitable topological restrictions, a suitably "consistent" collection of finite-dimensional distributions can be used to define a stochastic process (see Kolmogorov extension in the "Construction" section).
History of stochastic processes.
Stochastic processes were first studied rigorously in the late 19th century to aid in understanding financial markets and Brownian motion. The first person to describe the mathematics behind Brownian motion was Thorvald N. Thiele in a paper on the method of least squares published in 1880. This was followed independently by Louis Bachelier in 1900 in his PhD thesis "The theory of speculation", in which he presented a stochastic analysis of the stock and option markets. Albert Einstein (in one of his 1905 papers) and Marian Smoluchowski (1906) brought the solution of the problem to the attention of physicists, and presented it as a way to indirectly confirm the existence of atoms and molecules. Their equations describing Brownian motion were subsequently verified by the experimental work of Jean Baptiste Perrin in 1908.
An excerpt from Einstein's paper describes the fundamentals of a stochastic model:
<poem>
"It must clearly be assumed that each individual particle executes a motion which is independent of the motions of all other particles; it will also be considered that the movements of one and the same particle in different time intervals are independent processes, as long as these time intervals are not chosen too small.
We introduce a time interval formula_12 into consideration, which is very small compared to the observable time intervals, but nevertheless so large that in two successive time intervals formula_12, the motions executed by the particle can be thought of as events which are independent of each other".
</poem>
Construction.
In the ordinary axiomatization of probability theory by means of measure theory, the problem is to construct a sigma-algebra of measurable subsets of the space of all functions, and then put a finite measure on it. For this purpose one traditionally uses a method called Kolmogorov extension.
Kolmogorov extension.
The Kolmogorov extension proceeds along the following lines: assuming that a probability measure on the space of all functions formula_14 exists, then it can be used to specify the joint probability distribution of finite-dimensional random variables formula_15. Now, from this "n"-dimensional probability distribution we can deduce an ("n" − 1)-dimensional marginal probability distribution for formula_16. Note that the obvious compatibility condition, namely, that this marginal probability distribution be in the same class as the one derived from the full-blown stochastic process, is not a requirement. Such a condition only holds, for example, if the stochastic process is a Wiener process (in which case the marginals are all gaussian distributions of the exponential class) but not in general for all stochastic processes. When this condition is expressed in terms of probability densities, the result is called the Chapman–Kolmogorov equation.
The Kolmogorov extension theorem guarantees the existence of a stochastic process with a given family of finite-dimensional probability distributions satisfying the Chapman–Kolmogorov compatibility condition.
Separability, or what the Kolmogorov extension does not provide.
Recall that in the Kolmogorov axiomatization, measurable sets are the sets which have a probability or, in other words, the sets corresponding to yes/no questions that have a probabilistic answer.
The Kolmogorov extension starts by declaring to be measurable all sets of functions where finitely many coordinates formula_17 are restricted to lie in measurable subsets of formula_18. In other words, if a yes/no question about f can be answered by looking at the values of at most finitely many coordinates, then it has a probabilistic answer.
In measure theory, if we have a countably infinite collection of measurable sets, then the union and intersection of all of them is a measurable set. For our purposes, this means that yes/no questions that depend on countably many coordinates have a probabilistic answer.
The good news is that the Kolmogorov extension makes it possible to construct stochastic processes with fairly arbitrary finite-dimensional distributions. Also, every question that one could ask about a sequence has a probabilistic answer when asked of a random sequence. The bad news is that certain questions about functions on a continuous domain don't have a probabilistic answer. One might hope that the questions that depend on uncountably many values of a function be of little interest, but the really bad news is that virtually all concepts of calculus are of this sort. For example:
all require knowledge of uncountably many values of the function.
One solution to this problem is to require that the stochastic process be separable. In other words, that there be some countable set of coordinates formula_19 whose values determine the whole random function "f".
The Kolmogorov continuity theorem guarantees that processes that satisfy certain constraints on the moments of their increments have continuous modifications and are therefore separable.
Filtrations.
Given a probability space formula_20, a filtration is a weakly increasing collection of sigma-algebras on formula_3, formula_22, indexed by some totally ordered set formula_23, and bounded above by formula_24, i.e. for "s","t"  formula_25 with "s" < "t",
A stochastic process formula_27 on the same time set formula_23 is said to be adapted to the filtration if, for every t  formula_25, formula_5 is formula_31-measurable.
Natural filtration.
Given a stochastic process formula_32, the natural filtration for (or induced by) this process is the filtration where formula_31 is generated by all values of formula_34 up to time "s" = "t", i.e. formula_35.
A stochastic process is always adapted to its natural filtration.
Classification.
Stochastic processes can be classified according to the cardinality of its index set (usually interpreted as time) and state space.
Discrete time and discrete state space.
If both formula_36 and formula_5 belong to formula_38, the set of natural numbers, then we have models which lead to Markov chains. For example:
(a) If formula_5 means the bit (0 or 1) in position formula_36 of a sequence of transmitted bits, then formula_5 can be modelled as a Markov chain with two states. This leads to the error-correcting Viterbi algorithm in data transmission.
(b) If formula_5 represents the combined genotype of a breeding couple in the formula_36th generation in an inbreeding model, it can be shown that the proportion of heterozygous individuals in the population approaches zero as formula_36 goes to ∞.
Continuous time and continuous state space.
The paradigm of continuous stochastic process is that of the Wiener process. In its original form the problem was concerned with a particle floating on a liquid surface, receiving "kicks" from the molecules of the liquid. The particle is then viewed as being subject to a random force which, since the molecules are very small and very close together, is treated as being continuous and since the particle is constrained to the surface of the liquid by surface tension, is at each point in time a vector parallel to the surface. Thus, the random force is described by a two-component stochastic process; two real-valued random variables are associated to each point in the index set, time, (note that since the liquid is viewed as being homogeneous the force is independent of the spatial coordinates) with the domain of the two random variables being R, giving the "x" and "y" components of the force. A treatment of Brownian motion generally also includes the effect of viscosity, resulting in an equation of motion known as the Langevin equation.
Discrete time and continuous state space.
If the index set of the process is N (the natural numbers), and the range is R (the real numbers), there are some natural questions to ask about the sample sequences of a process {X"i"}"i" ∈ N, where a sample sequence is
Main applications of discrete time continuous state stochastic models include Markov chain Monte Carlo (MCMC) and the analysis of Time Series.
Continuous time and discrete state space.
Similarly, if the index space "I" is a finite or infinite interval, we can ask about the sample paths {X"t"(ω)}"t " ∈ "I"

</doc>
