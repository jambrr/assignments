<doc id="55266" url="https://en.wikipedia.org/wiki?curid=55266" title="Asia (band)">
Asia (band)

Asia are a British progressive rock band. The band was formed in 1981 as a supergroup of four members from different progressive rock bands: John Wetton (former bassist/vocalist of bands including King Crimson, Family, Roxy Music, Uriah Heep, UK and Wishbone Ash), Steve Howe (guitarist of Yes), Geoff Downes (keyboardist of Yes and The Buggles) and drummer Carl Palmer (of Emerson, Lake & Palmer, The Crazy World of Arthur Brown, and Atomic Rooster). With their debut album "Asia" from 1982 reaching No. 1 in several countries, Asia ranks as one of the most popular progressive rock bands in history.
The band has undergone multiple line-up changes during its history, but in 2006, the original line-up reunited. As a result, a band called Asia Featuring John Payne exists as a continuation of John Payne's career as Asia's frontman from 1991 until Wetton's return in 2006. In 2013, Howe retired from the band to continue with Yes and pursue other projects, and was replaced by guitarist Sam Coulson, completing the current lineup.
History.
Formation.
Asia began in early 1981 with the apparent demise of Yes and Emerson, Lake & Palmer, two of the flagship bands of British progressive rock. After the break-up of King Crimson in 1974, various plans for a supergroup involving bassist John Wetton had been mooted, including the abortive British Bulldog project with Bill Bruford and Rick Wakeman in 1976. Wakeman left the project at the urging of management, according to Bruford. In 1977 Bruford and Wetton were reunited in U.K., augmented by guitarist Allan Holdsworth and keyboardist/violinist Eddie Jobson. Their self-titled debut was released in 1978. But by January 1980, U.K. had folded after one lineup change and three recordings. A new project was then suggested involving Wetton, Wakeman, drummer Carl Palmer and (then little known) South African guitarist/singer Trevor Rabin, but Wakeman also left this project shortly before they were due to sign to Geffen and before they had played together.
In late December 1980, Wetton and former Yes guitarist Steve Howe were brought together by A&R man John Kalodner and Geffen Records to start writing material for a new album. They were eventually joined in early 1981 by drummer Carl Palmer, and finally by Howe's fellow member of Yes, keyboardist Geoff Downes. Two other players auditioned and considered during the band's formation were former The Move and ELO founder Roy Wood and guitarist/singer Trevor Rabin, who would go on to be part of a reformed Yes in 1983. Rabin, in a filmed 1984 interview included in the DVD "9012Live," said that his involvement with Asia never went anywhere because "there was no chemistry" among the participants.
The band's first recordings, under the auspices of Geffen record label head David Geffen and Kalodner, were considered disappointing by music critics and fans of traditional progressive rock, who found the music closer to radio-friendly album-oriented rock (AOR). However, Asia clicked with fans of popular arena acts such as Journey, Boston and Styx; Kalodner had once introduced Wetton to Journey's short-lived frontman Robert Fleischman, with a view to Fleischman becoming Asia's lead-singer. As they worked on material together, Fleischman was impressed by Wetton's singing and felt the voice best suited to the new material was Wetton's own. He left Asia amicably.
Rolling Stone gave "Asia" an indifferent review, while acknowledging the band's musicianship was a cut above the usual AOR expectations.
1981–85: "Heat of the Moment" and early success.
Asia's eponymous debut album "Asia", released in March 1982, gained considerable commercial success, spending nine weeks at number one in the U.S. album chart and selling over four million copies in the States alone. The album sold over 10 million worldwide and has never been out of print. The singles "Only Time Will Tell" and "Heat of the Moment" became huge Top 40 hits, both boosted by popular MTV music videos. Both tracks were stadium favourites at U.S. sporting events. "Sole Survivor" also received heavy air play on rock stations across the U.S., as did "Wildest Dreams" (another MTV video) and "Here Comes The Feeling". The band's best performing single, and perhaps their most recognised and popular hit song, "Heat of the Moment", spent six weeks at #1 on "Billboard"s Album Rock Tracks chart and climbed to #4 on the Hot 100.
In the United States the band sold out every date on their debut tour, which began at Clarkson University in Potsdam, New York on 22 April 1982, continued in theatres but quickly expanded into massive arenas because of high ticket demand. Asia would go on to receive a Grammy Award nomination as Best New Artist of 1982. MTV also played Asia videos on heavy rotation—as many as five times a day. Both "Billboard" and "Cash Box" named Asia's debut the #1 album of the year. Asia's logo and cover art were created by illustrator Roger Dean of Yes and Uriah Heep fame.
However, neither the second album, "Alpha" (released in July 1983), nor any following Asia album could repeat the chart success of the first release. "Don't Cry" was a #1 Album Rock Track and Top 10 Pop hit in the summer of 1983, and the video received considerable attention on MTV, while "The Smile Has Left Your Eyes" was another Top 40 hit for the band. The video for "Smile" also scored heavy MTV play. However, "Rolling Stone" panned "Alpha" as an over-produced commercial album, while others lamented that Howe and Palmer were effectively reduced to session musicians. "Alpha" received indifferent reviews from various critics, while still attaining platinum status and reaching #6 on the Billboard album chart.
In October 1983, Wetton was forced out of the group on the heels of the comparatively disappointing sales of "Alpha". The band stated that Wetton quit and there is no universally agreed upon version of what happened. Wetton later revealed one factor may have been his alcohol dependency. In any event, the next leg of their 1983 U.S. tour (which had begun in the summer but shut down suddenly on 10 September after a performance at Pine Knob in Detroit), scheduled for the autumn, was abruptly cancelled, reportedly because of low ticket sales. Ex-King Crimson and ELP front man Greg Lake replaced Wetton for the highly publicised "Asia in Asia" concert at the Nippon Budokan Hall in Tokyo, Japan on 6 December 1983, which was the first concert broadcast over satellite to MTV in the U.S. and later made into a home video. Some of the songs had to be played in a lower key to suit Lake's voice and he read most of the lyrics from a teleprompter. The Japanese dates were successful financially but not musically. Lake left in early 1984 and Asia reunited with Wetton that same year to start work on their next album. But Howe soon left to be replaced by Krokus guitarist Mandy Meyer. Howe then enjoyed brief success with GTR, another supergroup, formed with Steve Hackett of Genesis and produced by Downes.
1985–91: "Astra", break-up and new lineups.
The third Asia album was tentatively titled "Arcadia", but during production it was discovered that that name was being used by a forthcoming spin-off project from Duran Duran. The retitled "Astra", released in November 1985, was not as commercially successful as the first two albums. The record label cancelled the projected tour because of lack of interest. Howe's replacement, Mandy Meyer of Krokus, provided more of a hard-rock approach. The band charted another single with "Go", featuring Meyer's guitar heroics centre stage. The music video was another hit with MTV. In 1986 this Asia line-up folded, bringing the group to an end for the time being. Singer/bassist/songwriter Wetton is quoted as saying "It ["Astra"] did really well in Sweden ... but Swedish sales aren't that large."
Wetton resurfaced with a 1987 album with guitarist Phil Manzanera, "Wetton-Manzanera", based on material that had been originally intended for Asia. Also in 1987, Wetton played with Phenomena on their "Dream Runner" album and landed a number one hit in South America with the Phenomena single "Did It All for Love", also appearing in the related music video. Asia were also credited with contributing the Giorgio Moroder produced track "Gypsy Soul" to the Sylvester Stallone film soundtrack to "Over the Top", although Wetton was the only band member involved.
Wetton and Downes' attempt to restart the group in 1987 with guitarist Scott Gorham (formerly of Thin Lizzy) and drummer Michael Sturgis (ex-a-ha) fizzled when they were unable to land a worldwide recording deal.
Wetton and Palmer were more successful in reuniting the band for a few tours of Europe in the summer and fall of 1989. Downes (who was working on a project with Greg Lake) was not available, so keyboards were played by John Young. Guitars on this tour were handled by Alan Darby (who was replaced shortly after by German guitarist Holger Larisch) and Zoe Nicholas and Susie Webb were brought aboard to provide back-up vocals. Unlike Wetton's later anger at Asia continuing without him in the 1990s, this lineup was viewed favourably by other Asia band members.
Asia returned to the studio in 1990 with Downes, Toto guitarist Steve Lukather and other musicians (see discography below) and released "Then & Now", a best-of with four new tracks. "Days Like These" from the disc received substantial airplay during the summer of 1990 on AOR radio stations and re-sparked some interest in the band. Pat Thrall joined Downes, Palmer and Wetton on tour and they performed classic material, including King Crimson and U.K. songs. The band toured the Soviet Union in November 1990 to play in front of 20,000 fans on two sold out nights. "Days Like These" charted in the U.S. at No.64 in 1990 and climbed all the way to No.2 on the U.S. Album Rock Tracks chart. A video was planned but scrapped because various problems hampered the single's chance at the Top 40. Asia received the RIAA Gold album award for "Then and Now" many years later, but the initial response was modest as the album failed to dent the Top 100. A DVD and CD are available of the Asia concerts in the USSR (featuring a bonus studio track, "Kari-Anne" recorded by the 1987 Wetton-Downes-Gorham-Sturgis lineup and with Francis Dunnery contributing a guitar solo). Wetton left the group in April 1991 after a South American tour, discouraged by Asia's lack of success in the United States.
1991–2006: Downes/Payne era.
After Wetton's departure, vocalist/bassist John Payne joined the band and, together with Downes, enlisted new musicians and led Asia through to 2006. The first album with this line-up was "Aqua", released in June 1992. In addition to Downes and Payne, the album featured Howe, Palmer, and guitarist Al Pitrelli (of Danger Danger, Megadeth and Alice Cooper). Howe returned during the sessions having just left Yes again, but Palmer would leave soon, committing to an ELP reunion, and was able to play on just three songs. Session men then completed the drumming. Downes' environmentalist single "Who Will Stop the Rain?" (originally written for Max Bacon and the aborted "Rain" project, later appearing on Bacon's album "From the Banks of the River Irwell") attracted some radio attention. The "Aqua" club tour featured Howe (whose presence was heavily promoted), who took the stage after the fifth song. The tour was successful enough to warrant the band's continuation. The 1992–93 tour featured Downes, Howe, Payne, guitarist Vinny Burns and drummer Trevor Thornton. Before a European festival tour in late 1993, Howe and Burns left and were replaced by guitarist Keith More.
The group released "Aria" in May 1994, which featured lead guitarist Al Pitrelli once again, who would leave Asia during the short "Aria" tour. The "Aria" album did not fare well commercially and the ensuing tour was limited to four concerts. Ex-Simply Red guitarist Aziz Ibrahim took over during the tour. "Aria" also introduced new drummer Michael Sturgis, who had been involved during the band's aborted 1987 reunion and had appeared on some of the sessions for "Aqua". "Aria" was not released in the United States until 1995.
Over New Year's Eve 1996, a broken pipe inundated the control room in Downes' and Payne's recording studio, Electric Palace, in London. Amid the lost equipment, a vault containing unreleased material was found intact. The band decided to release the double-disc "Archiva", a collection of unreleased tracks recorded during the first three Downes/Payne albums. Next, "Arena", released in February 1996, featured Downes, Payne, Sturgis, Ibrahim and guest guitarist Elliott Randall (ex-Steely Dan, and Randy Crawford). The album was released on Resurgence Records but there was no tour because of lack of interest. The group's lone promotional performance in conjunction with the album occurred on 19 April 1996, when Downes and Payne appeared with guitarist Elliott Randall on the Virgin FM radio programme "Alive in London" to play the song "Never".
An all acoustic album, "Live at the Town & Country Club", was recorded by the group in September 1997 (and released in 1999) that featured a line-up of Downes, Payne, Ibrahim, and drummer Bob Richards.
In 1999 there was talk of a reunion of the original lineup minus Howe. The original proposition included Dave Kilminster on guitar, who had previously toured and recorded with Wetton. While Howe was interested in participating, he was unable to because of his busy schedule with Yes. This reunion did not take place and John Payne continued to carry on Asia with Downes uninterrupted. Wetton and Palmer did, however, get together to form Qango, which included Kilminster and John Young, although the band was short-lived. Kilminster went on to work with Keith Emerson, The Nice and Roger Waters. In 2000, Geffen/Universal released a best-of entitled "The Very Best of Asia: Heat of the Moment (1982–1990)" which also included three rare B-sides from the early days.
2001's "Aura" featured three different session guitarists, including Ian Crichton (of Canadian progressive rock band Saga) who'd briefly joined Asia in 1998–1999. "Aura" took a more progressive rock form, but still did not recapture the commercial success of the first album. Former members Howe, Thrall, Sturgis and Elliott Randall also made guest appearances. The single "Ready to Go Home" was barely distributed. Asia then signed with Recognition Records. 2001 did see the band with a stable line-up, achieved during the "Aura" sessions featuring Downes, Payne, guitarist Guthrie Govan and ex-Manfred Mann's Earth Band/The Firm/Uriah Heep/Gary Numan/AC/DC drummer Chris Slade (who had first joined Asia in 1999, briefly). Asia would tour for the first time since 1994, including the first U.S. dates since 1993. A live album and DVD, both titled "America: Live in the USA", were released in 2003, recorded at the Classic Rock Festival in Trenton, New Jersey in October 2002, which they co-headlined with Uriah Heep.
In the summer of 2003, Downes and Payne undertook the "Asia Across America Tour" which received some media attention. Performing "unplugged", the duo would reportedly play anywhere in the U.S. that fans requested, provided there was a venue and the fans put up $3,000 to cover costs.
Marking a departure from convention, for the first time a studio release was not titled as a single word starting and ending with the letter A (excepting the partial compilation / partial new album "Then & Now"). Released on Asia's newly signed label SPV/Inside Out Records, 2004's "Silent Nation" (name influenced by the Howard Stern vs. FCC incident) picked up some unexpected exposure on the Internet.
In 2004 an acoustic Asia toured once again featuring only Downes and Payne. In 2005 the full band toured in Europe and the Americas playing settings ranging from small clubs to medium-sized theatres. In the United States attendance was poor at best. Meanwhile, Wetton and Downes released some archival Asia material under the name "Wetton/Downes", and they then reunited to record a full-length album ("Icon", released in 2005), and an accompanying EP and DVD. Two additional Icon projects have since followed: "Icon II - Rubicon" (2006) and "Icon 3" (2009).
In August 2005, Slade left the group to be replaced by drummer Jay Schellen. The new band started work on an album, tentatively entitled "Architect of Time", which was originally planned for release in 2006, though later developments would cause this project to be shelved.
2006–13: "Original Asia" reunion.
In early 2006, the partnership between Downes and Payne was dissolved when Downes left for a reunion of the original band line-up under the Asia name, a breakup that Payne described as "painful". The existing line-up (minus Downes) continued for a short while before morphing into GPS.
When Downes left in 2006, Payne owned a significant portion of the rights to the band name "Asia", until a legal agreement was set by both bands' management. The original members exclusively now perform and record as Asia. On 9 May 2006, John Payne, Geoff Downes, John Wetton, Carl Palmer and Steve Howe contractually agreed that John Payne could continue his 15-year period with Asia as Asia Featuring John Payne. Asia featuring John Payne debuted in 2007 with Payne on vocals/bass, Guthrie Govan on guitar, Erik Norlander on keyboards and Jay Schellen on drums.
The official websites of each band reflect a split between the shared history of Payne's tenure with the band, as the reunited Asia acknowledge only pre- and post-Payne albums, whereas Asia Featuring John Payne claim Payne-era (1991–2006) albums "Aqua" (1991) through "Silent Nation" (2004) as part of their own discography. Asia Featuring John Payne perform songs from the entire history of Asia.
Downes and the other three original members (Wetton, Palmer and Howe) convened a group meeting in England in early 2006 in anticipation of formally reforming for work that year. And after a slew of rumours, they announced that this original line-up of Asia were planning a CD, DVD and world tour to celebrate the band's 25th anniversary. The band appeared in October 2006 on U.S. cable channel VH-1 Classic and began a world tour largely focused on the U.S. The band secured ownership of the Asia name and toured under the description of the Four Original Members of Asia. The set list featured most of the first album as well as a couple of songs from the second, along with one selection each from Yes, ELP, King Crimson and the Buggles to acknowledge the history of each member of the band. In a 2006 interview, guitarist Steve Howe states, "This is the real Asia. There have been other incarnations of the band, but this is the one that the public truly embraced".
The tour began on 29 August 2006 in Rochester, New York. "The Definitive Collection" was released by Geffen/Universal to tie into the tour in September and peaked at No.183 on the U.S. album charts--—the first time Asia had made the charts since 1990. A limited edition release available only at Best Buy stores also included a DVD of all the band's music videos.
The reunion tour continued into 2007 with venue size based on the success of the 2006 shows, where the band was mainly playing in clubs and theatres. Many of these sold out, including all seven dates in Japan. Also in 2007, the band released "Fantasia: Live In Tokyo" on CD and DVD through Eagle Records, commemorating the 25th Anniversary and documenting the success of the 2006–2007 tour.
In mid-2007, all four original members (Wetton, Downes, Howe and Palmer) went into the studio to record a new album, marking the first recorded material from all four original members since 1983's "Alpha". The band continued to tour until major heart surgery for Wetton in the second half of the year saw remaining tour dates rescheduled for 2008. The new studio album, entitled "Phoenix", was released on Frontiers Records on 14 April 2008 (via EMI/Capitol on 15 April in North America), along with a world tour to promote. The 12-track album includes "An Extraordinary Life", based on Wetton's experience of ill health; rockers such "Never Again" and "Nothing's Forever"; and power ballads such as "Heroine" and "I Will Remember You". The world tour also featured a couple of the new songs. The album cover featured Dean's illustration and design. The "Phoenix" album did well in both the American and European/Japanese markets. It debuted at No.73 on the American Billboard 200; the band had not charted with a studio album since 1985.
As a special finale to the US "Phoenix" tour, the band performed, for the first time ever, the entire first Asia album from beginning to end at their San Francisco concert at The Regency Center on 5 May. The album comprised the entire 2nd set of the evening's concert.
In summer 2009, Asia toured the United States with Yes. Asia opened with a 55-minute show, while Yes closed with a 1-hour and 50 minute set. Asia's set included only "An Extraordinary Life" from "Phoenix", the rest of the songs coming from the first two albums plus one cover each from The Buggles ("Video Killed the Radio Star" with Wetton on lead vocals and Downes on vocoder), King Crimson ("The Court of the Crimson King", which was recorded by the original incarnation of that band with Greg Lake on lead vocals) and Emerson, Lake & Palmer ("Fanfare for the Common Man"). Yes songs were omitted from this tour's setlist, though Asia also covered "Roundabout" on earlier legs of the "Four Original Members" tour. Contrary to some early expectations, Downes did not perform with Yes, although their set list included two songs from the 1980 album "Drama", which featured Downes on keys. A series of shows late in the tour featured a special appearance by Ian McDonald (flute and vocals on "The Court Of The Crimson King" which he co-wrote and backing vocals on "Heat Of The Moment").
In late 2009, the band began working on their follow-up CD to "Phoenix". According to Wetton's website in late November 2009: "Good news is that the new album is starting to leap, rather than creep (or sleep) in terms of progress. This week I have two completed lead vocals, with complete harmony/chorus voxes on three. It's just me, Geoff Steve R[ispin, and Mike Paxman in the studio--- Carl is pretty much all done, Steve H[owe is half done, and returns to the fold after Yes tour. It sounds absolutely wonderful".
The follow-up, titled "Omega", was released in the UK on 26 April 2010.
The band finished a new studio album timed to coincide with the band's thirtieth anniversary, titled "XXX", and released in the U.K. on 2 July 2012 and worldwide around the same time. In September 2012 they played 4 shows in Japan and a North American tour started on 11 October 2012. The UK tour, however, had to be cancelled after a number of shows due to Palmer contracting a serious case of E. coli.
2013-present: Howe's retirement, "Gravitas" and hiatus.
On 10 January 2013, Steve Howe announced his retirement from the band to focus on other projects, including Yes, bringing an end to the reunion of the original lineup. Asia in turn announced they would be continuing with new guitarist Sam Coulson, with a new album in the works entitled "Gravitas". The new line-up performed live in 2013.
On the website ultimateclassicrock.com, Howe explained his decision to leave Asia:
The band finished the recording sessions for "Gravitas" in December 2013 and in January 2014 they started shooting the music video for "Valkyrie", which was released as a single. The album's cover artwork was designed by Asia longtime collaborator, Roger Dean. On 30 January 2014, Wetton revealed the album's track listing through Asia's official website and talked about each song from the album. The album was released on 24 March 2014 and reached Number 1 in the Progressive Rock Chart for emusic on 27 March.
Following "Gravitas" and the following tour, the band has been on hiatus due to Wetton fighting with abdominal cancer and undergoing chemotherapy. On 8 December 2015, it was announced on the band's official website that the band will be returning to activity in 2016.
Personnel.
Through the years, Geoff Downes has been the most consistent member of the band, which experienced a revolving roster of noted musicians, particularly in the 1990s.
Certain musicians have joined and left after a short time without recording any studio material with the group. The most notable collaboration of this kind was the participation of Greg Lake in the "Asia in Asia" concert on bass guitar and lead vocals. Yet more musicians have played as session musicians or have guested with the band without formally joining. Some of these artists include: Robert Fleischman, Vinnie Colaiuta, Francis Dunnery, Ant Glynne, Scott Gorham, Tomoyasu Hotei, Luís Jardim, Ron Komie, Tony Levin, Steve Lukather, Thomas Lang, Kim Nielsen-Parsons, Nigel Glockler, Simon Phillips, and Alex Thomas.
Steve Howe of Yes fame was an original member, and rejoined the original lineup in 2006, before departing to pursue other projects in 2013.
Timeline.
This is an approximate timeline of the members of Asia.

</doc>
<doc id="55268" url="https://en.wikipedia.org/wiki?curid=55268" title="Øresund">
Øresund

Øresund (, ; , ) is a strait which forms the Danish–Swedish border,
separating Sjælland (Denmark) from Skåne (Sweden). 
It is wide at its narrowest point between Helsingør in Denmark and Helsingborg in Sweden. 
Øresund is one of three Danish Straits that connect the Baltic Sea to the Atlantic Ocean via Kattegat, Skagerrak, and the North Sea, and is one of the busiest waterways in the world. 
The Øresund Bridge, between the Danish capital Copenhagen and the Swedish city of Malmö, inaugurated on 1 July 2000, connects a bi-national metropolitan area with close to 4 million inhabitants.
The HH Ferry route, between Helsingør, Denmark and Helsingborg, Sweden, in the northern part of Øresund, is one of the world's busiest international ferry routes with more than 70 departures from each harbour per day.
Name.
The strait is called "Øresund" in Danish and "Öresund" in Swedish, informally "Sundet" (lit. "the Strait") in both languages.
The first part of the name is "øre" "ear", and the second part is "sund", i.e. "sound, strait". 
The name is first attested on a Danish runestone dated to ca. AD 1000, where it is written as ura suti, read as Old East Norse (the dative case). 
The Old West Norse (and modern Icelandic) form of the name is "Eyrarsund". 
The Øresund is so called because "øre" "ear" is a term for a small piece of land between two waters, and the Øresund stretches between two such "ears", from "Siellands Øre" to "Skan-Øre".
Boundaries.
The northern boundary between Øresund and Kattegat is a line which goes from Gilleleje at Zealand's northern peak to the westernmost point of Kullaberg at the smaller peninsula north of Helsingborg, known as Kullahalvön. In the south, the boundary towards the Baltic Sea starts at Stevns Klint, at the westernmost peak of the peninsula just south of Køge Bay, Stevns Peninsula to Falsterbo at the Falsterbo peninsula. 
Its eastern boundary is the Swedish coastline; to the west Amager may be considered part of Øresund (in which case it is the largest island) or a part of Zealand. Amager has eight connections with Zealand (two street bridges, a road bridge, a motorway bridge, a dual-track railway tunnel, an underground metro and a bicycle bridge) as well as a combined motorway and dual track railway to Scania and Sweden.
Streams, animals and salinity.
Øresund, like other Danish and Danish-German straits, is at the border between oceanic salt water (which has a salinity of more than 30 PSU) and the less salty Baltic Sea. As the Cattegat in the north has almost oceanic conditions and the Baltic Sea (6–7 PSU, in its main basin) has brackish water, Øresund's water conditions are rather unusual. The streams are very complex, but the "surface" stream is often northbound (from the Baltic Sea) which gives a lower surface salinity, though streams can change from one day to another. The average surface salinity is about 10–12 PSU in the southern part but above 20 PSU north of Helsingør.
Near the seafloor, conditions are more stable and salinity is always oceanic (above 30 PSU) below a certain depth that varies between 10 and 15 metres. In the southern part, however, the depth is 5–6 metres (outside the rather narrow waterways Drogden and Flintrännan), and this is the definite border of oceanic salt water, therefore also a border for many maritime species of animals. Only 52 known salt-water species reside in the central Baltic Sea, compared to around 1500 in the North Sea. Close to 600 species are known to exist in at least some part of Øresund. Well-known examples, for which the bottom salinity makes a distinct breeding border, include lobster, small crabs (Carcinus maenas), several species of flatfish and the lion's mane jellyfish; the latter can sometimes drift into the southwest Baltic Sea, but it cannot reproduce there.
There are daily tides, but the lunar attraction cannot force much water to move from west to east, or vice versa, in narrow waters where the current is either northbound or southbound. So, not much of the difference in water levels in Øresund is due to daily tides, and other circumstances "hide" the little tide that still remains. The current has a much stronger effect than the tide on the water level, but strong winds may also affect the water level. During exceptional conditions, such as storms and hurricanes, oceanic water may suddenly flow into the Baltic Sea at all depths. Such events give deep waters in the southern Baltic Sea higher salinity, which makes it possible for cod to breed there. If no such inflow of oceanic water to the Baltic Sea occurs for around a decade, the breeding of cod becomes endangered.
History.
Political control of Øresund has been an important issue in Danish and Swedish history. Denmark maintained military control with the coastal fortress of Kronborg at Elsinore on the west side and Kärnan at Helsingborg on the east, until the eastern shore was ceded to Sweden in 1658, based on the Treaty of Roskilde. Both fortresses are located where the strait is 4 kilometres wide.
In 1429, King Eric of Pomerania introduced the Sound Dues which remained in effect for more than four centuries, until 1857. Transitory dues on the use of waterways, roads, bridges and crossings were then an accepted way of taxing which could constitute a great part of a state's income. 
The Strait Dues remained the most important source of income for the Danish Crown for several centuries, thus making Danish kings relatively independent of Denmark's Privy Council and aristocracy.
To be independent of the Øresund, Sweden carried out two great projects, the foundation of Göteborg (Gothenborg) in 1621 and the construction of the Göta Canal from 1810 to 1832.
The Copenhagen Convention of 1857 abolished the Dues and made the Danish straits an international waterway.
A fixed connection was opened across the strait in 2000, the Øresund Bridge.

</doc>
<doc id="55271" url="https://en.wikipedia.org/wiki?curid=55271" title="Francisco Pizarro">
Francisco Pizarro

Francisco Pizarro González (; ; "circa" 1471 or 1476 – 26 June 1541) was a Spanish conquistador who led an expedition that caused the fall of the Inca Empire. On his expedition he captured and killed the Incan emperor Atahualpa and claimed the fallen empire for the Spanish.
Francisco Pizarro was born in Trujillo, Spain (then in the Crown of Castile), the illegitimate son of Gonzalo Pizarro, an infantry colonel, and Francisca González, a woman of poor means. His exact date of birth is uncertain, but it is believed to be sometime in the 1470s, probably 1471. Barely any attention was paid to his education and he grew up illiterate. He was a distant cousin of Hernán Cortés. On 10 November 1509, Pizarro sailed from Spain to the New World with Alonzo de Ojeda on an expedition to Urabí. He sailed to Cartagena and joined the fleet of Martín Fernández de Enciso, and, in 1513, accompanied Balboa to the Pacific. In 1514, he found a supporter in Pedrarias Dávila, the Governor of Castilla de Oro, and was rewarded for his role in the arrest of Balboa with the positions of mayor and magistrate in Panama City, serving from 1519 to 1523.
Reports of Peru's riches and Cortés's success in Mexico tantalized Pizarro and he undertook two expeditions to conquer the Incan Empire in 1524 and in 1526. Both failed as a result of native hostilities, bad weather, and lack of provisions. Pedro de los Ríos, the Governor of Panama, made an effort to recall Pizarro, but the "conquistador" resisted and remained in the south. In April 1528, he reached northern Peru and found the natives rich with precious metals. This discovery gave Pizarro the motivation to plan a third expedition to conquer Peru, and he returned to Panama to make arrangements, but the Governor refused to grant permission for the project. Pizarro returned to Spain to appeal directly to King Charles I. His plea was successful, and he received not only a license for the proposed expedition, but also considerable authority over any lands conquered during the venture. He was joined by family and friends, and the expedition left Panama in 1530.
When hostile natives along the coast threatened the expedition, Pizarro moved inland and founded the first Spanish settlement in Peru, San Miguel de Piura. Inca Atahualpa refused to tolerate a Spanish presence in his lands, but was captured by Pizarro during the Battle of Cajamarca on 16 November 1532. A ransom for the emperor's release was demanded and Atahualpa filled a room with gold, but Pizarro charged him with various crimes and executed Atahualpa on 26 July 1533, much to the opposition of his associates, who thought the conquistador was overstepping his authority. The same year, Pizarro entered the Inca capital of Cuzco, and the conquest of Peru was complete. In January 1535, Pizarro founded the city of Lima, a project he considered his greatest achievement. Quarrels between Pizarro and his longtime comrade-in-arms Diego Almagro culminated in the Battle of Las Salinas. Almagro was captured and executed, and, on 26 June 1541, his embittered son- Diego de Almagro "el mozo"- assassinated Pizarro in Lima. The conquistador of Peru was laid to rest in the Lima Cathedral.
Early life.
Pizarro was born in Trujillo, in modern-day Extremadura, Spain. His birth year is uncertain, but is placed sometime in the early 1470s. He was the illegitimate son of Gonzalo Pizarro Rodríguez de Aguilar (1446–1522) and Francisca González Mateos, a poor woman of Trujillo. His father was a colonel of infantry who served in Navarre and in the Italian campaigns under Córdoba. His mother married late in life and had a son Francisco Martín de Alcántara, who was at the conquest of Peru with his half-brother from its inception. Through his father, Francisco was a second cousin once removed to Hernán Cortés. Little attention was paid to Francisco's education and he grew up illiterate. On 10 November 1509, Pizarro sailed from Spain to the New World with Alonso de Ojeda on an expedition to Gulf of Urabá in Tierra Firme. Pizarro became a participant in Ojeda's failed colony, commanding the remnants until he abandoned it with the survivors. He sailed to Cartagena and joined the fleet of Martín Fernández de Enciso in 1513.
Panama.
In 1513, Pizarro accompanied Vasco Núñez de Balboa in his crossing of the Isthmus of Panama to the Pacific coast. The following year, Pedrarias Dávila became the newly appointed governor of Castilla de Oro and succeeded Balboa. During the next five years, Pizarro became a close associate of Dávila and the governor assigned him a "repartimiento" of natives and cattle. When Dávila decided to get rid of Balboa out of distrust, he instructed Pizarro to personally arrest him and bring him to stand trial. Balboa was beheaded in January 1519. For his loyalty to Dávila, Pizarro was rewarded with the positions of mayor (Alcalde) and magistrate of the then recently founded Panama City from 1519 to 1523.
Expeditions to South America.
The first attempt to explore western South America was undertaken in 1522 by Pascual de Andagoya. The native South Americans he encountered told him about a gold-rich territory called Virú, which was on a river called Pirú (later corrupted to Perú) and was from where they came. These reports were related by the Spanish-Inca "mestizo" writer Garcilaso de la Vega in his famous "Comentarios Reales de los Incas" (1609).
Andagoya eventually established contact with several Native American "curacas" (chiefs), some of whom he later claimed were sorcerers and witches. Having reached as far as the San Juan River (part of the present boundary between Ecuador and Colombia), Andagoya fell very ill and decided to return. Back in Panama, he spread the news and stories about "Pirú" – a great land to the south rich with gold (the legendary El Dorado). These revelations, along with the accounts of success of Hernán Cortés in Mexico years before, caught the immediate attention of Pizarro, prompting a new series of expeditions to the south in search of the riches of the Incan Empire.
In 1524, while still in Panama, Pizarro formed a partnership with a priest, Hernando de Luque, and a soldier, Diego de Almagro, to explore and conquer the South. Pizarro, Almagro, and Luque later renewed their compact more explicitly, agreeing to conquer and divide equally among themselves the opulent empire they hoped to discover. While historians agree their accord was strictly oral (no written document exists to prove otherwise), they are known to have dubbed their enterprise the "Empresa del Levante" and determined that Pizarro would command the expedition, Almagro would provide the military and food supplies, and Luque would be in charge of finances and any additional provisions they might need.
First expedition (1524).
In November 1524, the first of three expeditions left from Panama for the conquest of Peru with about 80 men and 40 horses. Juan de Salcedo was the standard bearer, Nicolas de Ribera was the treasurer, and Juan Carvallo was the inspector.
Diego de Almagro was left behind because he was to recruit men, gather additional supplies, and join Pizarro later. The Governor of Panama, Pedro Arias Dávila, at first approved in principle of exploring South America. Pizarro's first expedition, however, turned out to be a failure as his "conquistadores", sailing down the Pacific coast, reached no farther than Colombia before succumbing to such hardships as bad weather, lack of food, and skirmishes with hostile natives, one of which caused Almagro to lose an eye by arrow-shot. Moreover, the place names the Spanish bestowed along their route, including "Puerto Deseado" (desired port), "Puerto del Hambre" (port of hunger), and "Punta Quemado" or "Puebla Quemado" (burned port), only confirm their straits. Fearing subsequent hostile encounters like the one the expedition endured at the Battle of Punta Quemada, Pizarro chose to end his tentative first expedition and return to Panama.
Second expedition (1526).
Two years after the first very unsuccessful expedition, Pizarro, Almagro, and Luque started the arrangements for a second expedition with permission from Pedrarias Dávila. The governor, who himself was preparing an expedition north to Nicaragua, was reluctant to permit another expedition, having lost confidence in the outcome of Pizarro's expeditions. The three associates, however, eventually won his trust and he acquiesced. Also by this time, a new governor was to arrive and succeed Pedrarias Dávila. This was Pedro de los Ríos, who took charge of the post in July 1526 and had manifested his initial approval of Pizarro's expeditions (he would join him several years later in Peru).
On 10 March 1526, after all preparations were ready, Pizarro left Panama with two ships with 160 men and several horses, reaching as far as the Colombian San Juan River. Soon after arriving the party separated, with Pizarro staying to explore the new and often perilous territory off the swampy Colombian coasts, while the expedition's co-commander, Almagro, was sent back to Panama for reinforcements. Pizarro's "Piloto Mayor" (main pilot), Bartolomé Ruiz, continued sailing south and, after crossing the equator, found and captured a "balsa" (raft) under sail, with natives from Tumbes. To everyone's surprise, these carried a load of textiles, ceramic objects, and some much-desired pieces of gold, silver, and emeralds, making Ruiz's findings the central focus of this second expedition which only served to pique the conquistadors' interests for more gold and land. Some of the natives were also taken aboard Ruiz's ship to serve later as interpreters.
He then set sail north for the San Juan River, arriving to find Pizarro and his men exhausted from the serious difficulties they had faced exploring the new territory. Soon Almagro also sailed into the port with his vessel laden with supplies, and a considerable reinforcement of at least eighty recruited men who had arrived at Panama from Spain with the same expeditionary spirit. The findings and excellent news from Ruiz along with Almagro's new reinforcements cheered Pizarro and his tired followers. They then decided to sail back to the territory already explored by Ruiz and, after a difficult voyage due to strong winds and currents, reached Atacames in the Ecuadorian coast. Here, they found a very large native population recently brought under Inca rule. Unfortunately for the "conquistadores", the warlike spirit of the people they had just encountered seemed so defiant and dangerous in numbers that the Spanish decided not to enter the land.
The Famous Thirteen.
After much wrangling between Pizarro and Almagro, it was decided that Pizarro would stay at a safer place, the Isla de Gallo, near the coast, while Almagro would return yet again to Panama with Luque for more reinforcements – this time with proof of the gold they had just found and the news of the discovery of an obvious wealthy land they had just explored. The new governor of Panama, Pedro de los Ríos, had learned of the mishaps of Pizarro's expeditions and the deaths of various settlers who had gone with him. Fearing an unsuccessful outcome, he outright rejected Almagro's application for continued resources. In addition, he ordered two ships commanded by Juan Tafur to be sent immediately with the intention of bringing Pizarro and everyone back to Panama.
The leader of the expedition had no intention of returning, and when Tafur arrived at the now famous Isla de Gallo, Pizarro drew a line in the sand, saying: "There lies Peru with its riches; Here, Panama and its poverty. Choose, each man, what best becomes a brave Castilian. For my part, I go to the south."
Only 13 men decided to stay with Pizarro and later became known as "The Famous Thirteen" ("Los trece de la fama"), while the rest of the expeditioners stayed with Tafur aboard his ships. Ruiz also left in one of the ships with the intention of joining Almagro and Luque in their efforts to gather more reinforcements and eventually return to aid Pizarro. Soon after the ships left, Pizarro and his men constructed a crude boat and journeyed 25 leagues north for La Isla Gorgona, where they would remain for seven months before the arrival of new provisions.
Back in Panama, Pedro de los Ríos (after much convincing by Luque) had finally acquiesced to the requests for another ship, but only to bring Pizarro back within six months and completely abandon the expedition. Both Almagro and Luque quickly grasped the opportunity and left Panama (this time without new recruits) for La Isla Gorgona to once again join Pizarro. On meeting with Pizarro, the associates decided to continue sailing south on the recommendations of Ruiz's Indian interpreters.
By April 1528, they finally reached the northwestern Peruvian Tumbes Region. Tumbes became the territory of the first fruits of success the Spanish had so long desired, as they were received with a warm welcome of hospitality and provisions from the Tumpis, the local inhabitants. On subsequent days two of Pizarro's men, Alonso de Molina and Pedro de Candia, reconnoitered the territory and both, on separate accounts, reported back the incredible riches of the land, including the decorations of silver and gold around the chief's residence and the hospitable attentions with which they were received by everyone. The Spanish also saw, for the first time, the Peruvian llama which Pizarro called the "little camels". The natives also began calling the Spanish the "Children of the Sun" due to their fair complexions and brilliant armor. Pizarro, meanwhile, continued receiving the same accounts of a powerful monarch who ruled over the land they were exploring. These events only served as evidence to convince the expedition of the wealth and power displayed at Tumbes as an example of the riches the Peruvian territory had awaiting to conquer. The conquistadors decided to return to Panama to prepare the final expedition of conquest with more recruits and provisions. Before leaving, however, Pizarro and his followers sailed south not so far along the coast to see if anything of interest could be found. Historian William H. Prescott recounts that after passing through territories they named such as Cabo Blanco, port of Payta, Sechura, Punta de Aguja, Santa Cruz, and Trujillo (founded by Almagro years later), they finally reached for the first time the ninth degree of the southern latitude in South America. On their return towards Panama, Pizarro briefly stopped at Tumbes, where two of his men had decided to stay to learn the customs and language of the natives. Pizarro was also given two boys to learn his language, one of whom was later baptized as Felipillo and served as an important interpreter, the equivalent of Cortés' La Malinche of Mexico, and another called Martinillo. Their final stop was at La Isla Gorgona, where two of his ill men (one had died) had stayed before. After at least 18 months away, Pizarro and his followers anchored off the coasts of Panama to prepare for the final expedition.
Capitulación de Toledo.
When the new governor of Panama, Pedro de los Ríos, had refused to allow for a third expedition to the south, the associates resolved for Pizarro to leave for Spain and appeal to the sovereign in person. Pizarro sailed from Panama for Spain in the spring of 1528, accompanied by Pedro de Candia, some natives and llamas, plus samples of fabric, gold, and silver.
Pizzaro reached Seville in early summer. King Charles I, who was at Toledo, had an interview with Pizarro and heard of his expeditions in South America, a territory the conquistador described as very rich in gold and silver which he and his followers had bravely explored "to extend the empire of Castile." The king, who was soon to leave for Italy, was impressed at the accounts of Pizarro and promised to give his support for the conquest of Peru. Queen Isabel, though, in the absence of the king, signed the "Capitulación de Toledo" on 6 July 1529, a license document which authorized Francisco Pizarro to proceed with the conquest of Peru. Pizarro was officially named the Governor, Captain general, "Adelantado", and "Alguacil Mayor", of the New Castile for the distance of 200 leagues along the newly discovered coast, and invested with all the authority and prerogatives, his associates being left in wholly secondary positions (a fact which later incensed Almagro and would lead to eventual discords with Pizarro). One of the conditions of the grant was that within six months, Pizarro should raise a sufficiently equipped force of 250 men, of whom 100 might be drawn from the colonies.
This gave Pizarro time to leave for his native Trujillo and convince his brother Hernando Pizarro and other close friends to join him on his third expedition. Along with him also came Francisco de Orellana, who would later discover and explore the entire length of the Amazon River. Two more of his brothers from his father, Juan Pizarro and Gonzalo Pizarro, and a brother from his mother, Francisco Martin de Alcantara, would later decide to also join him, as well as his cousin Pedro Pizarro, who served as his page. When the expedition was ready and left the following year, it numbered three ships, 180 men, and 27 horses.
Since Pizarro could not meet the number of men the Capitulación had required, he sailed clandestinely from the port of Sanlúcar de Barrameda for the Canary Island of La Gomera in January 1530. He was there to be joined by his brother Hernando and the remaining men in two vessels that would sail back to Panama. Pizarro's third and final expedition left Panama for Peru on 27 December 1530.
Conquest of Peru (1532).
In 1531, Pizarro once again landed in the coasts near Ecuador, the province of Coaque and the region of "esmeraldas", where some gold, silver, and emeralds were procured and then dispatched to Almagro, who had stayed in Panama to gather more recruits. Sebastián de Belalcázar soon arrived with 30 men. Though Pizarro's main objective was then to set sail and dock at Tumbes like his previous expedition, he was forced to confront the Punian natives in the Battle of Puná, leaving three or four Spaniards dead and many wounded. Soon after, Hernando de Soto, another conquistador who had joined the expedition, arrived with 100 volunteers and horses to aid Pizarro and with him sailed towards Tumbes, only to find the place deserted and destroyed. Their two fellow conquistadors expected they had disappeared or died under murky circumstances. The chiefs explained the fierce tribes of Punians had earlier attacked them and ransacked the place.
As Tumbes no longer afforded the safe accommodations Pizarro sought, he decided to lead an excursion into the interior of the land in May 1532, and established the first Spanish settlement in Peru, San Miguel de Piura, and a "repartimiento". An earlier settlement than this in South America was Santa Marta, Colombia, established in 1526, but this was the first in Peru.
Leaving 50 men back at the settlement under the command of Antonio Navarro, Pizarro proceeded with his conquest accompanied by 200 men on 24 Sept. 1532. After arriving at Zaran, Hernando de Soto was dispatched to a Peruvian garrison at Caxas. After a week, he returned with an envoy from the Inca himself, with some presents, and an invitation to visit the Inca ruler's camp.
Following the defeat of his brother, Huáscar, Atahualpa had been resting in the Sierra of northern Peru, near Cajamarca, in the nearby thermal baths known today as the Inca Baths. Arriving Cajamarca on 15 Nov. 1532, Pizarro had a force of just 110 foot soldiers, 67 cavalry, three arquebuses, and two falconets. He sent Hernando Pizarro and Hernando de Soto to meet with Atahualpa in his camp. Atahuallpa agreed to meet Pizarro in his Cajamarca plaza fortress the next day. Fray Vincente de Valverde and native interpreter Felipillo approached Atahualpa in Cajamarca's central plaza. After the Dominican friar expounded the "true faith" and the need to pay tribute to the Emperor Charles the Fifth, Atahualpa replied, "I will be no man's tributary." His complacency, because fewer than 200 Spanish remained, as opposed to his 50,000-man army, of which 6000 accompanied him to Cajamarca, sealed his fate and that of the Inca empire.
Atahualpa's refusal led Pizarro and his force to attack the Inca army in what became the Battle of Cajamarca on 16 November 1532. The Spanish were successful and Pizarro executed Atahualpa's 12-man honor guard and took the Inca captive at the so-called Ransom Room. By February 1533, Almagro had joined Pizarro in Cajamarca with an additional 150 men with 50 horses.
Despite fulfilling his promise of filling one room () with gold and two with silver, Atahualpa was convicted of 12 charges, including killing his brother, and plotting against Pizarro and his forces. He was executed by garrote on 29 August 1533. Francisco Pizarro and de Soto were opposed to Atahualpa's execution, but Francisco consented to the trial due to the "great agitation among the soldiers", particularly by Almagro. De Soto was on a reconnaissance mission the day of the trial and execution, and upon his return expressed his dismay, stating, "he should have been taken to Castile and judged by the emperor." King Charles later wrote to Pizarro: "We have been displeased by the death of Atahualpa, since he was a monarch, and particularly as it was done in the name of justice."
Pizarro advanced with his army of 500 Spaniards toward Cuzco, accompanied by Chalcuchimac before he was burned at the stake. Manco Inca Yupanqui joined Pizarro after the death of Túpac Huallpa.
During the exploration of Cuzco, Pizarro was impressed and through his officers wrote back to King Charles I of Spain, saying:
"This city is the greatest and the finest ever seen in this country or anywhere in the Indies... We can assure your Majesty that it is so beautiful and has such fine buildings that it would be remarkable even in Spain."
The Spanish sealed the conquest of Peru by entering Cuzco on 15 November 1533. Jauja in the fertile Mantaro Valley was established as Peru's provisional capital in April 1534, but it was too far up in the mountains and far from the sea to serve as the Spanish capital of Peru. Pizarro thus founded the city of Lima in Peru's central coast on 6 January 1535, which he considered as one of the most important things he had created in life.
After the final effort of the Inca to recover Cuzco had been defeated by Almagro, a dispute occurred between Pizarro and him respecting the limits of their jurisdiction; both claimed the city of Cuzco. The king of Spain had awarded the Governorate of New Toledo to Almagro and the Governorate of New Castile to Pizarro. The dispute had originated from a disagreement on how to interpret the limit between both governorates.
This led to confrontations between the Pizarro brothers and Almagro, who was eventually defeated during the Battle of Las Salinas (1538) and executed. Almagro's son, also named Diego and known as "El Mozo", was later stripped of his lands and left bankrupt by Pizarro.
Atahualpa's wife, 10-year-old Cuxirimay Ocllo Yupanqui, was with Atahualpa's army in Cajamarca and had stayed with him while he was imprisoned. Following his execution, she was taken to Cuzco and given the name Dona Angelina. By 1538, it was known she was Pizarro's mistress, having borne him two sons, Juan and Francisco.
Pizarro's death.
In Lima, on 26 June 1541 "a group of 20 heavily armed supporters of Diego de Almagro "el mozo" stormed Pizarro's palace, assassinating him, and then forced the terrified city council to appoint young Almagro as the new governor of Peru", according to Burkholder and Johnson.
"Most of Pizarro's guests fled, but a few fought the intruders, numbered variously between seven and 25. While Pizarro struggled to buckle on his breastplate, his defenders, including his half-brother Martin de Alcántara, were killed. For his part, Pizarro killed two attackers and ran through a third. While trying to pull out his sword, he was stabbed in the throat, then fell to the floor where he was stabbed many times." Pizarro (who now was maybe as old as 70 years, and at least 62), collapsed on the floor, alone, painted a cross in his own blood and cried for Jesus Christ. He died moments after. Diego de Almagro the younger was caught and executed the following year after losing the battle of Chupas.
Pizarro's remains were briefly interred in the cathedral courtyard; at some later time, his head and body were separated and buried in separate boxes underneath the floor of the cathedral. In 1892, in preparation for the anniversary of Columbus' discovery of the Americas, a body believed to be that of Pizarro was exhumed and put on display in a glass coffin. However, in 1977, men working on the cathedral's foundation discovered a lead box in a sealed niche, which bore the inscription "Here is the head of Don Francisco Pizarro Demarkes, Don Francisco Pizarro who discovered Peru and presented it to the crown of Castile." A team of forensic scientists from the United States, led by Dr. William Maples, was invited to examine the two bodies, and they soon determined that the body which had been honored in the glass case for nearly a century had been incorrectly identified. The skull within the lead box not only bore the marks of multiple sword blows, but the features bore a remarkable resemblance to portraits made of the man in life.
Legacy.
By his marriage to N de Trujillo, Pizarro had a son also named Francisco, who married his relative Inés Pizarro, without issue. After Pizarro's death, Inés Yupanqui, whom he took as a mistress, favourite sister of Atahualpa, who had been given to Francisco in marriage by her brother, married a Spanish cavalier named Ampuero and left for Spain, taking her daughter who would later be legitimized by imperial decree. Francisca Pizarro Yupanqui eventually married her uncle Hernando Pizarro in Spain, on 10 October 1537; a third son of Pizarro who was never legitimized, Francisco, by Dona Angelina, a wife of Atahualpa that he had taken as a mistress, died shortly after reaching Spain.
Historians have often compared Pizarro and Cortés' conquests in North and South America as very similar in style and career. Pizarro, however, faced the Incas with a smaller army and fewer resources than Cortés at a much greater distance from the Spanish Caribbean outposts that could easily support him, which has led some to rank Pizarro slightly ahead of Cortés in their battles for conquest. Based on sheer numbers alone, Pizarro's military victory was one of the most improbable in recorded history.
Pizarro is well known in Peru for being the leader behind the Spanish conquest of the Inca Empire, and a growing number of Peruvians of strong indigenous descent which are the majority of Peru regard him negatively. By incorporating the natives into the society of Peru, Pizarro ruled Peru for almost a decade and initiated the decline of Inca culture. The Incas’ polytheistic religion was replaced by Christianity and both Quechua and Aymara — the main Inca languages — were reduced to a marginal role in society for centuries, while Spanish became the official language of Peru, Ecuador, Bolivia, and Chile. The cities of the Inca Empire were transformed into Spanish, Catholic cities. Pizarro is also vilified for having ordered Atahualpa's death despite his paid ransom of filling a room with gold and two with silver which was later split among all his closest Spanish associates after a fifth share had been set aside for the king. Among other once-Spanish nations in the Americas, those which have a large creole or mestizo population with mostly European ancestry notably Mexico, El Salvador, Argentina, and Chile, regard Francisco Pizarro as a hero much like Cortes because they have the religion, culture, and prosperity today because of conquistadors such as them.
Sculptures.
In the early 1930s, sculptor Ramsey MacDonald created three copies of an anonymous European foot soldier resembling a conquistador with a helmet, wielding a sword, and riding a horse. The first copy was offered to Mexico to represent Hernán Cortés, though it was rejected. Since the Spanish conquerors had the same appearance with helmet and beard, the statue was taken to Lima in 1934. One other copy of the statue resides in Wisconsin. The mounted statue of Pizarro in the Plaza Mayor in Trujillo, Spain, was created by Charles Rumsey, an American sculptor. It was presented to the city by his widow in 1926.
In 2003, after years of lobbying by indigenous and mixed-raced majority requesting for the equestrian statue of Pizarro to be removed, the mayor of Lima, Luis Castañeda Lossio, approved the transfer of the statue to another location: an adjacent square to the country's Government Palace. Since 2004, however, Pizarro's statue has been placed in a rehabilitated park surrounded by the recently restored 17th-century walls in the Rímac District. The statue faces the Rímac River and the Government Palace.
Palace of the conquest.
After their return from Peru and notoriously rich, the Pizarro family erected a plateresque-style palace on the corner of the Plaza Mayor in Trujillo. Francisca Pizarro Yupanqui and her uncle/husband Hernando Pizarro ordered the building of the palace; it features busts of them and others. It instantly became a recognizable symbol of the plaza.
The opulent palace is structured in four stands, giving it the significance of the coat of arms of the Pizarro family, which is situated at one of its corner balconies displaying its iconographic content. The building's decor includes plateresque ornaments and balustrades.

</doc>
<doc id="55273" url="https://en.wikipedia.org/wiki?curid=55273" title="Manchukuo">
Manchukuo

Manchukuo () was a puppet state in Northeast China and Inner Mongolia, which was governed under a form of constitutional monarchy. The area, collectively known as Manchuria by westerners and Japanese, was designated by China's erstwhile Qing Dynasty as the "homeland" of the ruling family's ethnic group, the Manchus, but the Manchus themselves never used "Manchuria" (滿洲) as a place name to refer to the area. In 1931, the region was seized by Japan following the Mukden Incident and a pro-Japanese government was installed one year later with Puyi, the last Qing emperor, as the nominal regent and emperor. Manchukuo's government was abolished in 1945 after the defeat of Imperial Japan at the end of World War II. The territories formally claimed by the puppet state were first seized in the Soviet invasion of Manchuria in August 1945, and then formally transferred to Chinese administration in the following year.
Manchus formed a minority in Manchukuo, whose largest ethnic group were Han Chinese. The population of Koreans increased during the Manchukuo period, and there were also Japanese, Mongols, White Russians and other minorities. The Mongol regions of western Manchukuo were ruled under a slightly different system in acknowledgement of the Mongolian traditions there. The southern part of the Liaodong Peninsula was ruled by Japan as the Kwantung Leased Territory.
History.
Terminology.
"Manchuria" is a transcription of the Japanese reading of the Chinese word "滿洲" which means Manchuria, which in Japanese is "Manshū", which in turn dates from the 19th century. The name "Manzhou" was invented and given to the Jurchen people by Hong Taiji in 1635 as a new name for their ethnic group, however, the name "Manchuria" was never used by the Manchus or the Qing dynasty itself to refer to their homeland.
According to the Japanese scholar Junko Miyawaki-Okada, the Japanese geographer Takahashi Kageyasu (高橋景保) was the first to use the term 满洲 (Manshū) as a place name in 1809 in the "Nippon Henkai Ryakuzu" (日本辺海略図), and it was from that work where Westerners adopted the name. 满洲 (Manshū) then began to appear as a place names in more maps created by Japanese like Kondi Jūzō, Takahashi Kageyasu, Baba Sadayoshi and Yamada Ren. These maps were brought to Europe by the Dutch Philipp von Siebold.
According to Nakami Tatsuo, Philip Franz von Siebold was the one who brought the usage of the term Manchuria to Europeans, after borrowing it from the Japanese, who were the first to use it in a geographic manner in the eighteenth century, while neither the Manchu nor Chinese languages had a term in their own language equivalent to "Manchuria" as a geographic place name. According to Bill Sewell, it was Europeans who first started using Manchuria as a name to refer to the location and it is "not a genuine geographic term." The historian Gavan McCormack agreed with Robert H. G. Lee's statement that "The term Manchuria or Man-chou is a modern creation used mainly by westerners and Japanese", with McCormack writing that the term Manchuria is imperialistic in nature and has no "precise meaning", since the Japanese deliberately promoted the use of "Manchuria" as a geographic name to promote its separation from China while they were setting up their puppet state of Manchukuo.
Background.
The Japanese had their own motive for deliberately spreading the usage of the term Manchuria. The historian Norman Smith wrote that "The term "Manchuria" is controversial". Professor Mariko Asano Tamanoi said that she "should use the term in quotation marks", when referring to Manchuria. Herbert Giles wrote that "Manchuria" was unknown to the Manchus themselves as a geographical expression.
The Qing Dynasty, which replaced the Shun and Ming dynasties in China, was founded by Manchus from Manchuria (modern Northeastern China). The Manchu emperors separated their homeland in Jilin and Heilongjiang from the Han Liaoning province with the Willow Palisade. This ethnic division continued until the Qing dynasty encouraged massive immigration of Han in the 19th century during Chuang Guandong to prevent the Russians from seizing the area from the Qing. After conquering the Ming, the Qing identified their state as "China" (中國, Zhongguo; "Central Realm") and referred to it as "Dulimbai Gurun" in Manchu. The Qing equated the lands of the Qing state (including present day Manchuria, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, rejecting the idea that China only meant Han areas, proclaiming that both Han and non-Han peoples were part of "China", using "China" to refer to the Qing in official documents, international treaties, and foreign affairs, and the "Chinese language" (Dulimbai gurun i bithe) referred to Chinese, Manchu, and Mongol languages, and the term "Chinese people" (中國人 Zhongguo ren; Manchu: Dulimbai gurun i niyalma) referred to all Han, Manchus, and Mongol subjects of the Qing. The lands in Manchuria were explicitly stated by the Qing to belong to "China" (Zhongguo, Dulimbai gurun) in Qing edicts and in the Treaty of Nerchinsk.
During the Qing dynasty, the area of Manchuria was known as the "three eastern provinces" (三東省; "Sān dōng shěng") since 1683 when Jilin and Heilongjiang were separated even though it was not until 1907 that they were turned into actual provinces. The area of Manchuria was then converted into three provinces by the late Qing government in 1907. Since then, the "Three Northeast Provinces" () was officially used by the Qing government in China to refer to this region, and the post of Viceroy of Three Northeast Provinces was established to take charge of these provinces.
During its reign the Qing Dynasty became highly integrated with Chinese culture. The dynasty reached its height in the 18th century, during which both territory and population were increased. However, its military power weakened thereafter and, faced with massive rebellions and defeat in wars, the Qing Dynasty declined after the mid-19th century.
As the power of the court in Beijing weakened, many outlying areas either broke free (like Kashgar) or fell under the control of Imperialist powers. In the 19th century, Imperial Russia was most interested in the northern lands of the Qing Empire. In 1858, Russia gained control over a huge tract of land called Outer Manchuria thanks to the Supplementary Treaty of Beijing that ended the Second Opium War. But Russia was not satisfied and, as the Qing Dynasty continued to weaken, they made further efforts to take control of the rest of Manchuria. Inner Manchuria came under strong Russian influence in the 1890s with the building of the Chinese Eastern Railway through Harbin to Vladivostok.
Origins.
As a direct result of the Russo-Japanese War (1904/05) Japanese influence replaced Russia's in Inner Manchuria. In 1906, Japan laid the South Manchurian Railway to Port Arthur (Japanese: Ryojun). Between World War I and World War II Manchuria became a political and military battleground between Russia, Japan, and China. Japan moved into Outer Manchuria as a result of the chaos following the Russian Revolution of 1917. A combination of Soviet military successes and American economic pressure forced the Japanese to withdraw from the area, however, and Outer Manchuria returned to Soviet control by 1925.
During the warlord period in China, the warlord Zhang Zuolin established himself in Inner Manchuria with Japanese backing. Later, the Japanese Kwantung Army found him too independent, so he was assassinated in 1928.
After the Japanese invasion of Manchuria in 1931, Japanese militarists moved forward to separate the region from Chinese control and to create a Japanese-aligned puppet state. To create an air of legitimacy, the last Emperor of China, Puyi, was invited to come with his followers and act as the head of state for Manchuria. One of his faithful companions was Zheng Xiaoxu, a Qing reformist and loyalist.
On 18 February 1932 the "Manchu State" (Manchukuo, Pinyin: "Mǎnzhōuguó") was proclaimed and recognized by Japan on 15 September 1932 through the Japan-Manchukuo Protocol, after the assassination of Japanese Prime Minister Inukai Tsuyoshi. The city of Changchun, renamed Hsinking (Pinyin: Xinjing) (新京, literally "New Capital"), became the capital of the new entity. Chinese in Manchuria organized volunteer armies to oppose the Japanese and the new state required a war lasting several years to pacify the country.
The Japanese initially installed Puyi as Head of State in 1932, and two years later he was declared Emperor of Manchukuo with the era name of "Kangde" ("Tranquility and Virtue"; Wade-Giles: "Kangte"). Manchukuo thus became the Great Manchurian Empire, sometimes termed "Manchutikuo" (Pinyin: "Mǎnzhōu Dìguó"). Zheng Xiaoxu served as Manchukuo's first prime minister until 1935, when Zhang Jinghui succeeded him. Puyi was nothing more than a figurehead and real authority rested in the hands of the Japanese military officials. An imperial palace was specially built for the emperor. The Manchu ministers all served as front-men for their Japanese vice-ministers, who made all decisions.
In this manner, Japan formally detached Manchukuo from China in the course of the 1930s. With Japanese investment and rich natural resources, the area became an industrial powerhouse. Manchukuo had its own issued bank notes and postal stamps. Several independent banks were founded as well.
In 1935, Manchukuo bought the Chinese Eastern Railway from the Soviet Union.
Diplomatic recognition.
China did not recognize Manchukuo but the two sides established official ties for trade, communications and transportation. In 1933, the League of Nations adopted the Lytton Report, declaring that Manchuria remained rightfully part of China, leading Japan to resign its membership. The Manchukuo case persuaded the United States to articulate the so-called Stimson Doctrine, under which international recognition was withheld from changes in the international system created by force of arms.
In spite of the League of Nations' approach, the new state was diplomatically recognised by El Salvador (3 March 1934) and the Dominican Republic (1934), the Soviet Union (de facto 23 March 1935; de jure 13 April 1941), Italy (29 November 1937), Spain (2 December 1937), Germany (12 May 1938) and Hungary (9 January 1939).
It is commonly believed that the Holy See established diplomatic relations with Manchukuo in 1934, but the Holy See never did so. This belief is partly due to the erroneous reference in Bernardo Bertolucci's 1987 film "The Last Emperor" that the Holy See diplomatically recognised Manchukuo. Bishop Auguste Ernest Pierre Gaspais was appointed as "representative "ad tempus" of the Holy See and of the Catholic missions of Manchukuo to the government of Manchukuo" by the Congregation De Propaganda Fide (a purely religious body responsible for missions) and not by the Secretariat of State responsible for diplomatic relations with states.
After the outbreak of World War II, the state was recognised by Slovakia (1 June 1940), Vichy France (12 July 1940), Romania (1 December 1940), Bulgaria (10 May 1941), Finland (18 July 1941), Denmark (August 1941), Croatia (2 August 1941)—all controlled or influenced by Japan's ally Germany — as well as by China's Wang Jingwei government (30 November 1940), Thailand (5 August 1941) and the Philippines (1943) — all under Japanese control.
World War II and aftermath.
Before World War II, the Japanese colonized Manchukuo and used it as a base from which to invade China. In the summer of 1939 a border dispute between Manchukuo and the Mongolian People's Republic resulted in the Battle of Khalkhin Gol. During this battle, a combined Soviet-Mongolian force defeated the Japanese Kwantung Army ("Kantōgun") supported by limited Manchukuoan forces.
On 8 August 1945, the Soviet Union declared war on Japan, in accordance with the agreement at the Yalta Conference, and invaded Manchukuo from outer Manchuria and Outer Mongolia. This was called Manchurian Strategic Offensive Operation. During the Soviet offensive the Army of Manchukuo, on paper a 200,000-man force, performed poorly and whole units surrendered to the Soviets without firing a single shot; there were even cases of armed riots and mutinies against the Japanese forces. Emperor Kangde (known by reign title Xuantong during the Qing Dynasty; his childhood name was Puyi) had hoped to escape to Japan to surrender to the Americans, but the Soviets captured him and eventually extradited him to the communist government in China, where the authorities had him imprisoned as a war criminal along with all other captured Manchukuo officials.
From 1945 to 1948, Manchuria (Inner Manchuria) served as a base area for the People's Liberation Army in the Chinese Civil War against the National Revolutionary Army. The Chinese Communists used Manchuria as a staging ground until the final Nationalist retreat to Taiwan in 1949. Many Manchukuo army and Japanese Kantogun personnel served with the communist troops during the Chinese Civil War against the Nationalist forces. Most of the 1.5 million Japanese who had been left in Manchukuo at the end of World War II were sent back to their homeland in 1946-1948 by U.S. Navy ships in the operation now known as the Japanese repatriation from Huludao.
Politics.
Historians generally consider Manchukuo a puppet state of Imperial Japan because of the Japanese military's strong presence and strict control of the government administration. Chinese historians generally refer to the state as 'Wei Manzhouguo' ('false state of Manchuria'). Some historians see Manchukuo as an effort at building a glorified Japanese state in mainland Asia that deteriorated due to the pressures of war.
The independence of Manchuria was proclaimed on 18 February 1932, and renamed to Manchukuo. The Japanese military commander appointed Puyi as regent (reign name Datong) for the time being, stating that he would become Emperor of Manchukuo but could not reign using the title of Emperor of the Great Qing Empire as he once held. Manchukuo was proclaimed a monarchy on 1 March 1934, with Puyi assuming the throne under the reign name of Emperor Kang-de. Puyi was assisted in his executive duties by a Privy Council (), and a General Affairs State Council (). This State Council was the center of political power, and consisted of several cabinet ministers, each assisted by a Japanese vice-minister.
The commanding officer of the Kwantung Army in Manchukuo was additionally the Japanese ambassador to Manchukuo. He functioned in a manner similar to that of a British resident officer in British overseas protectorates, with the power to veto decisions by the emperor. The Kwangtung Army leadership placed Japanese vice ministers in his cabinet, while all Chinese advisors gradually resigned or were dismissed.
The Legislative Council () was largely a ceremonial body, existing to rubber-stamp decisions issued by the State Council. The only authorized political party was the government-sponsored Concordia Association, although various émigré groups were permitted their own political associations.
When the Japanese surrender was announced on 15 August 1945, Puyi was "asked" to abdicate, which he did.
Administrative divisions.
During its short-lived existence, Manchukuo was divided into between five (in 1932) and 19 (in 1941) provinces, one special ward of Peiman () and two Special cities which were Hsinking () and Harbin (). Each province was divided into between four (Hsingan-tung) and 24 (Fengtien) prefectures. Peiman lasted less than 3 years (1 July 1933 – 1 January 1936) and Harbin was later incorporated into Binkiang province. Lungkiang also existed as a province in the 1932 before being divided into Heiho, Lungkiang and Sankiang in 1934. Antung and Chinchow provinces separated themselves from Fengtien while Binkiang and Chientao from Kirin separated themselves in the same year.
Demographics.
In 1908, the number of residents was 15,834,000, which rose to 30,000,000 in 1931 and 43,000,000 for the Manchukuo state. The population balance remained 123 men to 100 women and the total number in 1941 was 50,000,000. Other statistics indicate that in Manchukuo the population rose by 18,000,000.
In early 1934, the total population of Manchukuo was estimated as 30,880,000, with 6.1 persons the average family, and 122 men for each 100 women. These numbers included 29,510,000 Chinese (96%, which should have included the Manchurian population), 590,760 Japanese (2%), 680,000 Koreans (2%), and 98,431 (<1%) of other nationality: White Russians, Mongols, etc. Around 80% of the population was rural. During the existence of Manchukuo, the ethnic balance did not change significantly, except that Japan increased the Korean population in China. The majority of Han Chinese in Manchukuo believed that Manchuria was rightfully part of China, who both passively and violently resisted Japan's propaganda that Manchukuo was a "multinational state".
From Japanese sources come these numbers: in 1940 the total population in Manchukuo of Lungkiang, Jehol, Kirin, Liaoning (Fengtien) and Hsingan provinces at 43,233,954; or an Interior Ministry figure of 31,008,600. Another figure of the period estimated the total population as 36,933,000 residents.
Around the same time the Soviet Union was advocating the Siberian Jewish Autonomous Oblast across the Manchukuo-Soviet border, some Japanese officials investigated a plan (known as the Fugu Plan) to attract Jewish refugees to Manchukuo as part of their colonisation efforts which was never adopted as official policy.
The Japanese Ueda Kyōsuke labelled all 30 million people in Manchuria as "Manchus", including Han Chinese, despite the fact that most of them were not ethnic Manchu, and the Japanese written "Great Manchukuo" built upon Ueda's argument to claim that all 30 million "Manchus" in Manchukuo had the right to independence to justify splitting Manchukuo from China. In 1942 the Japanese written "Ten Year History of the Construction of Manchukuo" attempted to emphasize the right of ethnic Japanese to the land of Manchukuo while attempting to delegitimize the Manchu's claim to Manchukuo as their native land, noting that most Manchus moved out during the Qing period and only returned later.
Japanese population.
In 1931–2, there were 100,000 Japanese farmers; other sources mention 590,760 Japanese inhabitants. Other figures for Manchukuo speak of a Japanese population 240,000 strong, later growing to 837,000. In Hsinking, they made up 25% of the population. The Japanese government had official plans projecting the emigration of 5 million Japanese to Manchukuo between 1936 and 1956. Between 1938 and 1942 a batch of young farmers of 200,000 arrived in Manchukuo; joining this group after 1936 were 20,000 complete families. When Japan lost sea and air control of the Yellow Sea, this migration stopped.
When the Red Army invaded Manchukuo, they captured 850,000 Japanese settlers. With the exception of some civil servants and soldiers, these were repatriated to Japan in 1946–7. Many Japanese orphans in China were left behind in the confusion by the Japanese government and were adopted by Chinese families. Many, however, integrated well into Chinese society. In the 1980s Japan began to organise a repatriation programme for them but not all chose to go back to Japan.
The majority of Japanese left behind in China were women, and these Japanese women mostly married Chinese men and became known as "stranded war wives" (zanryu fujin). Because they had children fathered by Chinese men, the Japanese women were not allowed to bring their Chinese families back with them to Japan so most of them stayed. Japanese law only allowed children fathered by Japanese fathers to become Japanese citizens.
Economy.
Manchukuo experienced rapid economic growth and progress in its social systems. During the 1920s, the Japanese Army under the influence of the "Wehrstaat" (Defense State) theories popular with the "Reichswehr" had started to advocate their own version of the "Wehrstaat", the totalitarian "national defense state" which would mobilize an entire society for war in peacetime. An additional influence on the Japanese "total war" school who tended to be very anti-capitalist was the First Five Year Plan in the Soviet Union, which provided an example of rapid industrial growth achieved without capitalism. At least part of the reason why the Kwangtung Army seized Manchuria in 1931 was to use it as an laboratory for creating an economic system geared towards the "national defense state"; colonial Manchuria offered up possibilities for the army carrying out drastic economic changes that were not possible in Japan. From the beginning, the Army intended to turn Manchukuo into the industrial heartland of the empire, and starting in 1932, the Army sponsored a policy of forced industrialization that was closely modeled after the Five Year Plan in the Soviet Union. Reflecting their dislike of capitalism, the "Zaibatsu" were excluded from Manchukuo and all of the heavy industrial factories were built and owned by Army-owned corporations. In 1935, there was a change when the "reform bureaucrat" Nobusuke Kishi was appointed Deputy Minister of Industrial Development. Kishi persuaded the Army to allow the "zaibatsu" to invest in Manchukuo, arguing that having the state carry out the entire industrialization of Manchukuo was costing too much money. Kishi pioneered an etatist system where bureaucrats such as himself developed economic plans, which the "zaibatsu" had to then carry out. Kishi succeeded in marshaling private capital in a very strongly state-directed economy to achieve his goal of vastly increased industrial production while at the same time displaying utter indifference to the exploited Chinese workers toiling in Manchukuo's factories; the American historian Mark Driscoll described Kishi's system as a “necropolitical” system where the Chinese workers were literally treated as dehumanized cogs within a vast industrial machine. The system that Kishi pioneered in Manchuria of a state-guided economy where corporations made their investments on government orders later served as the model for Japan's post-1945 development, albeit not with same level of brutal exploitation as in Manchukuo. By the 1930s, Manchukuo's industrial system was among the most advanced making it one of the industrial powerhouses in the region. Manchukuo's steel production exceeded Japan's in the late 1930s. Many Manchurian cities were modernised during Manchukuo era. However, much of the country's economy was often subordinated to Japanese interests and, during the war, raw material flowed into Japan to support the war effort. Traditional lands were taken and redistributed to Japanese farmers with local farmers relocated and forced into collective farming units over smaller areas of land.
See also:
Transport.
The Japanese built an efficient and impressive railway system that still functions well today. Known as the South Manchuria Railway Company or "Mantetsu", this large corporation came to own large stakes in many industrial projects throughout the region. "Mantetsu" personnel were active in the pacification of occupied China during World War II.
Military.
Manchukuo Imperial Army.
The Manchukuo Imperial Army was the armed force of Manchukuo.
Manchukuo Imperial Guards.
The Manchukuo Imperial Guards was the elite unit of the Manchukuo armed forces created in 1933. It was charged with the protection of the emperor and senior members of the civil government. Its headquarters was in the capital of Hsinking, near the Imperial Palace in the centre of the city.
Manchukuo Imperial Navy.
The Manchukuo Imperial Navy ("Manshu Teikoku Kaigun") was the navy of Manchukuo. As Manchukuo was a largely land-locked state, the leadership of the Japanese Kwantung Army regarded the development of a navy to have a very low military priority, although it was politically desirable to create at least a nominal force as a symbol of the legitimacy of the new regime.
Manchukuo Imperial Air Force.
The Manchukuo Imperial Air Force ("Dai Manshū Teikoku Kūgun") was established in February 1937, initially with 30 men selected from the Manchukuo Imperial Army and trained at the Japanese Kwantung Army aircraft arsenal in Harbin. The official air force's predecessor was the Manchukuo Air Transport Company (later renamed the Manchukuo National Airways) a paramilitary airline formed in 1931, which undertook transport and reconnaissance missions for the Japanese military.
War crimes in Manchukuo.
According to a joint study by historians Zhifen Ju, Mitsuyochi Himeta, Toru Kubo and Mark Peattie, more than 10 million Chinese civilians were mobilized by the Kwangtung Army for slave labor in Manchukuo under the supervision of the Kōa-in.
The Chinese slave laborers often suffered illness due to high-intensity manual labor. Some badly ill workers were directly pushed into mass graves in order to avoid the medical expenditure and the world's most serious mine disaster, at Benxihu Colliery, happened in Manchukuo.
Bacteriological weapons were experimented on humans by the infamous unit 731 located near Harbin in Beinyinhe from 1932 to 1936 and to Pingfan until 1945. Victims, mostly Chinese, Russians and Koreans, were subjected to vivisection, sometimes without anesthesia.
Drug trafficking.
In 2007, an article by Reiji Yoshida in the "Japan Times" argued that Japanese investments in Manchukuo were partly financed by selling drugs. According to the article, a document found by Yoshida shows that the Kōa-in was directly implicated in providing funds to drug dealers in China for the benefit of the puppet government of Manchukuo, Nanjing and Mongolia. This document corroborates evidence analyzed earlier by the Tokyo tribunal which stated that
Education.
Manchukuo developed an efficient public education system. The government established many schools and technical colleges, 12,000 primary schools in Manchukuo, 200 middle schools, 140 normal schools (for preparing teachers), and 50 technical and professional schools. In total the system had 600,000 children and young pupils and 25,000 teachers. Local Chinese children and Japanese children usually attended different schools, and the ones who did attend the same school were segregated by ethnicity, with the Japanese students assigned to better-equipped classes.
Confucius's teachings also played an important role in Manchukuo's public school education. In rural areas, students were trained to practice modern agricultural techniques to improve production. Education focused on practical work training for boys and domestic work for girls, all based on obedience to the "Kingly Way" and stressing loyalty to the Emperor. The regime used numerous festivals, sport events, and ceremonies to foster loyalty of citizens. Eventually, Japanese became the official language in addition to the Chinese taught in Manchukuo schools.
Culture.
Film.
The Photograpic Division, part of the public relations section of the South Manchurian Railway was created in 1928 to produce short documentary films about Manchuria to Japanese audiences. In 1937, the Manchukuo Film Association was established by the government and the South Manchurian Railway in a studio in Jilin province. It was founded by Masahiko Amakasu, who also helped the career of Yoshiko Otaka, also known as Ri Koran. He also tried to ensure that Manchukuo would have its own industry and would be catering mainly to Manchurian audiences. The films for the most part usually promote pro-Manchukuo and pro-Japanese views. After World War II, the archives and the equipment of the association were used by the Changchun Film Studio of the People's Republic of China.
Dress.
The Changshan and the Qipao, both derived from traditional Manchu dress, were considered national dresses in Manchukuo.
In a meeting with the Concordia Association, the organizers devised what was termed Concordia Costume, or the kyowafuku, in 1936. Even Japanese like Masahiko Amakasu and Kanji Ishiwara adopted it. It was gray and a civilianized version of Imperial Japanese Army uniform. It was similar to the National Clothes "(kokumin-fuku)" worn by Japanese civilians in World War II as well as the Zhongshan suit. A pin of either a Manchukuo flag or a five-pointed, five colored star with the Manchukuo national colors were worn on the collars.
Court dress resembled those of Meiji-era Japan at that time.
Sport.
The Manchukuo National Physical Education Association was established in 1932 to promote sport.
Manchukuo also had a national football team, and football was considered the country's de facto national sport; the Football Association of Manchukuo was formed around it.
Manchukuo hosted and participated in baseball matches with Japanese teams. Some of the games of the Intercity Baseball Tournament were held in the country, and played with local teams.
Manchukuo originally was to join the 1932 Summer Olympic Games, but one of the athletes who were intended to represent Manchukuo, Liu Changchun, refused to join the team and instead joined as the first Chinese representative in the Olympics. There were attempts by Japanese authorities to let Manchukuo join the 1936 games, but the Olympic Committee persisted in the policy of not allowing an unrecognized state to join the Olympics. Manchukuo had a chance to participate in the planned 1940 Tokyo Olympics, but the outset of World War II killed the idea permanently.
National symbols.
Aside from the national flag, the orchid, reportedly Puyi's favorite flower, became the royal flower of the country, similar to the chrysanthemum in Japan. The sorghum flower also became a national flower by decree in April 1933. "Five Races Under One Union (Manchukuo)" was used as a national motto.
Stamps and postal history.
Manchukuo issued postage stamps from 28 July 1932 until its dissolution following the final defeat of the Japanese Empire in August 1945. The last issue of Manchukuo was on 2 May 1945.
In popular culture.
In Masaki Kobayashi's "The Human Condition" (1959), Kaji, the main protagonist, is a labor supervisor assigned to a workforce consisting of Chinese prisoners in a large mining operation in Japanese-colonized Manchuria.
Bernardo Bertolucci's 1987 film "The Last Emperor" presented a portrait of Manchukuo through the memories of Emperor Puyi, during his days as a political prisoner in the People's Republic of China.
Haruki Murakami's 1995 novel "The Wind-Up Bird Chronicle" deals greatly with Manchukuo through the character of Lieutenant Mamiya. Mamiya recalls, in person and in correspondence, his time as an officer in the Kwantung Army in Manchukuo. While the period covered in these recollections extends over many years, the focus is on the final year of the war and the Soviet invasion of Manchuria. Also, in his 2011 magical realist novel "1Q84", Murakami references the Manchukuo puppet state, Manchuria and the Trans-Siberian Railway in various character backstories.
The 2008 South Korean western "The Good, the Bad, the Weird" is set in the desert wilderness of 1930s Manchuria.

</doc>
<doc id="55275" url="https://en.wikipedia.org/wiki?curid=55275" title="Denotational semantics">
Denotational semantics

In computer science, denotational semantics (initially known as mathematical semantics or Scott–Strachey semantics) is an approach of formalizing the meanings of programming languages by constructing mathematical objects (called "denotations") that describe the meanings of expressions from the languages. Other approaches to providing formal semantics of programming languages include axiomatic semantics and operational semantics.
Broadly speaking, denotational semantics is concerned with finding mathematical objects called domains that represent what programs do. For example, programs (or program phrases) might be represented by partial functions or by games between the environment and the system.
An important tenet of denotational semantics is that "semantics should be compositional": the denotation of a program phrase should be built out of the denotations of its subphrases.
Historical development.
Denotational semantics originated in the work of Christopher Strachey and Dana Scott in the late 1960s. As originally developed by Strachey and Scott, denotational semantics provided the denotation (meaning) of a computer program as a function that mapped input into output. To give denotations to recursively defined programs, Scott proposed working with continuous functions between domains, specifically complete partial orders. As described below, work has continued in investigating appropriate denotational semantics for aspects of programming languages such as sequentiality, concurrency, non-determinism and local state.
Denotational semantics have been developed for modern programming languages that use capabilities like concurrency and exceptions, e.g., Concurrent ML, CSP, and Haskell. The semantics of these languages is compositional in that the denotation of a phrase depends on the denotations of its subphrases. For example, the meaning of the applicative expression f(E1,E2) is defined in terms of semantics of its subphrases f, E1 and E2. In a modern programming language, E1 and E2 can be evaluated concurrently and the execution of one of them might affect the other by interacting through shared objects causing their denotations to be defined in terms of each other. Also, E1 or E2 might throw an exception which could terminate the execution of the other one. The sections below describe special cases of the semantics of these modern programming languages.
Denotations of recursive programs.
Denotational semantics are given to a program phrase as a function from an environment (that has the values of its free variables) to its denotation. For example, the phrase n*m produces a denotation when provided with an environment that has binding for its two free variables: n and m. If in the environment n has the value 3 and m has the value 5, then the denotation is 15.
A function can be modeled as denoting a set of ordered pairs where each ordered pair in the set consists of two parts (1) an argument for the function and (2) the value of the function for that argument. For example, the set of order pairs {1 3} is the denotation of a function with value 1 for argument 0, value 3 for the argument 4, and is otherwise undefined.
The problem to be solved is to provide denotations for recursive programs that are defined in terms of themselves such as the definition of the factorial function as
A solution is to build up the denotation by approximation starting with the empty set of ordered pairs (which in set theory would be written as {}). If {} is plugged into the above definition of factorial then the denotation is {1}, which is a better approximation of factorial. Iterating: If {1} is plugged into the definition then the denotation is {1 1}. So it is convenient to think of an approximation to factorial as an input F in the following way:
It is instructive to think of a chain of "iterates" where "Fi" indicates "i"-many applications of "F".
The least upper bound of this chain is the full factorial function which can be expressed as follows where the symbol "⊔" means "least upper bound":
Denotational semantics of non-deterministic programs.
The concept of power domains has been developed to give a denotational semantics to non-deterministic sequential programs. Writing "P" for a power domain constructor, the domain "P"("D") is the domain of non-deterministic computations of type denoted by "D".
There are difficulties with fairness and unboundedness in domain-theoretic models of non-determinism.
Denotational semantics of concurrency.
Many researchers have argued that the domain theoretic models given above do not suffice for the more general case of concurrent computation. For this reason various new models have been introduced. In the early 1980s, people began using the style of denotational semantics to give semantics for concurrent languages. Examples include Will Clinger's work with the actor model; Glynn Winskel's work with event structures and petri nets; and the work by Francez, Hoare, Lehmann, and de Roever (1979) on trace semantics for CSP. All these lines of inquiry remain under investigation (see e.g. the various denotational models for CSP).
Recently, Winskel and others have proposed the category of profunctors as a domain theory for concurrency.
Denotational semantics of state.
State (such as a heap) and simple imperative features can be straightforwardly modeled in the denotational semantics described above. All the textbooks below have the details. The key idea is to consider a command as a partial function on some domain of states. The denotation of "x:=3" is then the function that takes a state to the state with 3 assigned to x. The sequencing operator ";" is denoted by composition of functions. Fixed-point constructions are then used to give a semantics to looping constructs, such as "while".
Things become more difficult in modelling programs with local variables. One approach is to no longer work with domains, but instead to interpret types as functors from some category of worlds to a category of domains. Programs are then denoted by natural continuous functions between these functors.
Denotations of data types.
Many programming languages allow users to define recursive data types. For example, the type of lists of numbers can be specified by
This section deals only with functional data structures that cannot change. Conventional imperative programming languages would typically allow the elements of such a recursive list to be changed.
For another example: the type of denotations of the untyped lambda calculus is
The problem of "solving domain equations" is concerned with finding domains that model these kinds of datatypes. One approach, roughly speaking, is to consider the collection of all domains as a domain itself, and then solve the recursive definition there. The textbooks below give more details.
Polymorphic data types are data types that are defined with a parameter. For example, the type of α lists is defined by
Lists of natural numbers, then, are of type nat list, while lists of strings are of string list.
Some researchers have developed domain theoretic models of polymorphism. Other researchers have also modeled parametric polymorphism within constructive set theories. Details are found in the textbooks listed below.
A recent research area has involved denotational semantics for object and class based programming languages.
Denotational semantics for programs of restricted complexity.
Following the development of programming languages based on linear logic, denotational semantics have been given to languages for linear usage (see e.g. proof nets, coherence spaces) and also polynomial time complexity.
Denotational semantics of sequentiality.
The problem of full abstraction for the sequential programming language PCF was, for a long time, a big open question in denotational semantics. The difficulty with PCF is that it is a very sequential language. For example, there is no way to define the parallel-or function in PCF. It is for this reason that the approach using domains, as introduced above, yields a denotational semantics that is not fully abstract.
This open question was mostly resolved in the 1990s with the development of game semantics and also with techniques involving logical relations. For more details, see the page on PCF.
Denotational semantics as source-to-source translation.
It is often useful to translate one programming language into another. For example, a concurrent programming language might be translated into a process calculus; a high-level programming language might be translated into byte-code. (Indeed, conventional denotational semantics can be seen as the interpretation of programming languages into the internal language of the category of domains.)
In this context, notions from denotational semantics, such as full abstraction, help to satisfy security concerns.
Abstraction.
It is often considered important to connect denotational semantics with operational semantics. This is especially important when the denotational semantics is rather mathematical and abstract, and the operational semantics is more concrete or closer to the computational intuitions. The following properties of a denotational semantics are often of interest.
Additional desirable properties we may wish to hold between operational and denotational semantics are:
Compositionality.
An important aspect of denotational semantics of programming languages is compositionality, by which the denotation of a program is constructed from denotations of its parts. For example, consider the expression "7 + 4". Compositionality in this case is to provide a meaning for "7 + 4" in terms of the meanings of "7", "4" and "+".
A basic denotational semantics in domain theory is compositional because it is given as follows. We start by considering program fragments, i.e. programs with free variables. A "typing context" assigns a type to each free variable. For instance, in the expression ("x" + "y") might be considered in a typing context ("x":nat,"y":nat). We now give a denotational semantics to program fragments, using the following scheme.
Now, the meaning of the compound expression (7+4) is determined by composing the three functions 〚⊢7:nat〛:1→ℕ⊥, 〚⊢4:nat〛:1→ℕ⊥, and 〚"x":nat,"y":nat⊢"x"+"y":nat〛:ℕ⊥×ℕ⊥→ℕ⊥.
In fact, this is a general scheme for compositional denotational semantics. There is nothing specific about domains and continuous functions here. One can work with a different category instead. For example, in game semantics, the category of games has games as objects and strategies as morphisms: we can interpret types as games, and programs as strategies. For a simple language without general recursion, we can make do with the category of sets and functions. For a language with side-effects, we can work in the Kleisli category for a monad. For a language with state, we can work in a functor category. Milner has advocated modelling location and interaction by working in a category with interfaces as objects and "bigraphs" as morphisms.
Semantics versus implementation.
According to Dana Scott :
According to Clinger (1981):
Connections to other areas of computer science.
Some work in denotational semantics has interpreted types as domains in the sense of domain theory, which can be seen as a branch of model theory, leading to connections with type theory and category theory. Within computer science, there are connections with abstract interpretation, program verification, and model checking.

</doc>
<doc id="55278" url="https://en.wikipedia.org/wiki?curid=55278" title="Warp drive">
Warp drive

Warp drive is a faster-than-light (FTL) spacecraft propulsion system in many science fiction works, most notably "Star Trek". A spacecraft equipped with a warp drive may travel at fantasy speeds greater than that of light by many orders of magnitude. In contrast to other FTL technologies such as a jump drive or hyper drive, the warp drive does not permit instantaneous (or near instantaneous) travel between two points but involves a measurable passage of time which is problematic to the concept. Spacecraft at warp velocity theoretically continue to interact with objects in "normal space". Other science fiction in which warp drive technology is featured include "Stars!", "EVE Online", "Earth and Beyond", "StarCraft", "DarkSpace", "Starship Troopers", "Doctor Who", "WALL-E", "" and "Star Ocean". The general concept of "warp drive" was introduced by John W. Campbell in his 1931 novel "Islands of Space". A similar concept known as the "hyperdrive" in the science fiction franchise "Star Wars" functions also in a similar manner by manipulating the dimensions of spatial vacuum in front of a spaceship and behind it, giving it immense speed by giving it around and slightly beyond light speed capability.
Einstein's theory of special relativity states that energy and mass are interchangeable, thus, speed of light travel is impossible for material objects that weigh more than photons. The problem of a material object exceeding light speed is that an infinitely increasing amount of kinetic energy is required to attempt moving as fast as a massless photon. This problem can theoretically be solved by warping space to move an object instead of increasing the kinetic energy of the object to do so.
"The Original Series": Establishing a background.
Warp drive is one of the fundamental features of the "Star Trek" franchise; in the first pilot episode of "", "The Cage", it is referred to as a "hyperdrive"/"time warp" drive combination, and it is stated that the "time barrier" has been broken, allowing a group of stranded interstellar travelers to return to Earth far sooner than would have otherwise been possible. The light speed time barrier shouldn't be confused with time dilation which occurs when approaching very fast speeds. Warp drive technology avoids time dilation.
The episode "Metamorphosis", also from "The Original Series", establishes a backstory for the invention of warp drive on Earth, in which Zefram Cochrane discovered the "space warp". Cochrane is repeatedly referred to afterwards, but the exact details of the first warp trials were not shown until the second ' movie, '. The movie depicts Cochrane as having first operated warp drive on Earth in 2063 (two years after the date speculated by the first edition of the Star Trek Chronology). By using a matter/antimatter reactor to create plasma, and by sending this plasma through warp coils, he created a warp bubble which he could use to move a craft into subspace, thus allowing it to exceed the speed of light. This successful first trial led directly to first contact with the Vulcans.
"Enterprise": Leading up to "The Original Series".
Later on, a prequel series titled "" describes the warp engine technology as a "Gravimetric Field Displacement Manifold" (Commander Tucker's tour, ""), and describes the device as being powered by a matter/anti-matter reaction which powers the two separate nacelles (one on each side of the ship) to create a displacement field (the aforementioned "bubble").
The episode also firmly establishes that many other civilizations had warp drive before humans; "First Contact" co-writer Ronald D. Moore suggested Cochrane's drive was in some way superior to forms which existed beforehand, and was gradually adopted by the galaxy at large.
"Enterprise", set in 2151 and onwards, follows the voyages of the first human ship capable of traveling at warp factor 5.2, which under the old warp table formula, is about 140 times the speed of light. In the episode, "", which is the series' pilot episode, Capt. Archer equates warp 4.5 to "Neptune and back Earth in six minutes." There are two reasons why this is not in conflict with statements in previously-aired "Star Trek" series, set "after" the time of "Enterprise", which declare that use of warp drive within a solar system presents extreme danger to both ship and surrounding planets. First, Archer was simply mentioning familiar landmarks as a way to comprehend how much faster his new ship was than any Earth vessel which had come before. Second, those future cautions were uttered in an era of hundredfold greater traffic concentrated around major spaceports. Flying randomly at high speed a mile above Manhattan is frowned upon today; in 1916, no one would have been close enough to notice.
"The Next Generation" onwards.
Plots involving the "Enterprise" traveling beyond warp 10 were once in the original series (such as warp 14.1 in "That Which Survives"), but for "The Next Generation" it was decided that these would no longer be featured. A new warp scale was drawn up, with warp factor 10 set as an unattainable maximum. This is described in some technical manuals as "Eugene's limit", in homage to creator/producer Gene Roddenberry. Warp 8 in the original series was the "Never Exceed" speed for the hulls and engines of Constitution class starships, equivalent to the aircraft VNE V-speed. Warp 6 was the VNO "Normal Operation" maximum "safe" cruising speed for that vessel class. The Warp 14.1 incident was the result of runaway engines which brought the hull within seconds of structural failure before power was disengaged.
The limit of 10 did not entirely stop warp inflation. By the mid-24th century, the "Enterprise"-D could travel at warp 9.8 at "extreme risk", while normal maximum operating speed was warp 9.6 and maximum rated cruise was warp 9.2. According to the Deep Space Nine Tech Manual, during the Dominion War, "Galaxy"-class starships were refitted with newer technology including modifications which increased their maximum speed to warp 9.9.
In the episode "Where No One Has Gone Before" the "Enterprise"-D was shown to exceed Warp 10, traveling 2.7 million light-years from their home galaxy in a matter of minutes (though the ship's extreme velocity was due to the influence of an alien being and could not be achieved by starship engines). The "Intrepid"-class starship "Voyager" has a maximum sustainable cruising speed of warp 9.975, the "Enterprise"-E can go even faster at Warp 9.99. In the alternative future depicted in "", the series finale of "The Next Generation", the "future" "Enterprise"-D travels at warp 13, although it is never established whether this is "above" warp ten, or simply the result of another reconfiguration of the warp scale.
Warp velocities.
Warp drive velocity in "Star Trek" is generally expressed in "warp factor" units, which—according to the "Star Trek Technical Manuals"—correspond to the magnitude of the warp field. Achieving warp factor 1 is equal to breaking the light barrier, while the actual velocity corresponding to higher factors is determined using an ambiguous formula. Several episodes of the original series placed the "Enterprise" in peril by having it travel at high warp factors; at one point in "That Which Survives" the "Enterprise" traveled at a warp factor of 14.1 (any faster than warp 10.0 would theoretically send you backwards in time). In the "" episode "The Most Toys" the crew of "Enterprise"-D discovers that the android Data may have been stolen while on board another ship, "Jovis". At this point the "Jovis", which has a maximum warp factor of 3 has had a 23-hour head start, which the Enterprise-D figures puts her anywhere within a 0.102 light year radius of her last known position. However, the velocity (in present dimensional units) of any given warp factor is rarely the subject of explicit expression, and travel times for specific interstellar distances are not consistent through the various series.
According to the "Star Trek" episode writer's guide for "The Original Series", warp factors are converted to multiples of "c" with the cubic function , where "w" is the warp factor, "v" is the velocity, and "c" is the speed of light. Accordingly, "warp 1" is equivalent to the speed of light, "warp 2" is 8 times the speed of light, "warp 3" is 27 times the speed of light, etc.
For "" and the subsequent series, "Star Trek" artist Michael Okuda devised a formula based on the original one but with important differences; for warp factors 1 through 9, . In the half-open interval from warp 9 to warp 10, the exponent of "w" increases toward infinity. Thus, in the Okuda scale, warp velocities approach warp 10 asymptotically.
There is no exact formula for this interval because the quoted velocities are based on a hand-drawn curve; what can be said is that at velocities greater than warp 9, the form of the warp function changes because of an increase in the exponent of the warp factor "w". Due to the resultant increase in the derivative, even minor changes in the warp factor eventually correspond to a greater than exponential change in velocity. In the episode "", Tom Paris breaks the warp 10 threshold.
Exact velocities were given in a few episodes, one being "", where Kathryn Janeway describes "Voyager"'s velocity at warp factor 9.975. "Voyager" was about 70,000 light-years away from home, and crew would often use "75 years" as the time it would take to get back home at top speed. This means the "Voyager" series used the old method of Warp calculation. 70,000/9.9753 is roughly 70.5 years. If delays for refueling, repair, restocking, and downtime are considered, 75 years is a logical rounding. However, in "", Tom Paris achieves warp 10, which is infinite velocity.
Slingshot effect.
A curious extension of warp travel, shown throughout "Star Trek", is the "slingshot effect".
First discovered accidentally in "Tomorrow Is Yesterday" (1967), one of the earlier episodes of the original "Star Trek" series, as a method of time travel. While the actual procedure is intentionally obscure, it involved traveling at a high warp velocity (depicted in "" as more than warp 9.8) in the direction of a star, on a precisely calculated "slingshot" path; if successful, the ship is caused to travel to a desired point, past or future. The same technique was used later in the episode "" (1968) for historic research — in this episode, the warp factor required for ""time warp"" is given the name "light speed breakaway factor". The term ""time warp"" was first used in "The Naked Time" (1966) when a previously untried cold-start intermix of matter and antimatter threw the Enterprise back three days in time. The term was later used in "Star Trek IV" in describing the slingshot effect. The technique was mentioned as a viable method of time travel in the TNG episode "Time Squared" (1989).
This "slingshot" effect has been explored in theoretical physics: it is hypothetically possible to slingshot oneself "around" the event horizon of a black hole. The result of such a maneuver would cause time to pass at a slower rate for the ship near the event horizon relative to the rest of the outside universe. Such a journey would be a trip into the future — the craft would have merely "fast forwarded". It is not possible to travel into the past with this method.
Warp core.
A primary component of the warp drive method of propulsion in the "Star Trek" universe is the "gravimetric field displacement manifold", more commonly referred to as a "warp core". It is a fictional reactor that taps the energy released in a matter-antimatter annihilation to provide the energy necessary to power a starship's warp drive, allowing faster-than-light travel. Starship warp cores generally also serve as powerplants for other primary ship systems.
When matter and antimatter come into contact, they annihilate—both matter and antimatter are converted directly and entirely into enormous quantities of energy, in the form of subnuclear particles and electromagnetic radiation (specifically, mesons and gamma rays). In the "Star Trek" universe, fictional "dilithium crystals" are used to regulate this reaction. These crystals are described as being non-reactive to anti-matter when bombarded with high levels of radiation.
Usually, the reactants are deuterium, which is an isotope of hydrogen, and antideuterium (its antimatter counterpart). In "The Original Series" and in-universe chronologically subsequent series, the warp core reaction chamber is often referred to as the "dilithium intermix chamber" or the "matter/antimatter reaction chamber", depending upon the ship's intermix type. The reaction chamber is surrounded by powerful magnetic fields to contain the anti-matter. If the containment fields ever fail, the subsequent interaction of the antimatter fuel with the container walls would result in a catastrophic release of energy, with the resultant explosion capable of utterly destroying the ship. Such "warp core breaches" are used as plot devices in many Star Trek episodes. An intentional warp core breach can also be deliberately created, as one of the methods by which a starship can be made to self-destruct.
The mechanisms that provide a starship's propulsive force are the "warp nacelles", one (or more) cylindrical pods that are offset from the hull of the ship by large pylons; the nacelles generate the actual 'warp bubble' that surrounds the ship, and destruction of one or both nacelles will cripple the ship, and possibly cause a warp-core breach.
Real-world theories and science.
A theoretical solution for faster-than-light travel which models the warp drive concept, called the Alcubierre drive, was formulated by physicist Miguel Alcubierre in 1994. Subsequent calculations found that such a model would require prohibitive amounts of negative energy, or mass.
In 2012, NASA researcher Harold White hypothesized that by changing the shape of the warp drive, much less negative mass and energy could be used, though the energy required ranges from the mass of Voyager 1 to the mass of the observable universe, or many orders of magnitude greater than anything currently possible by modern technology. NASA engineers have begun preliminary research into such technology.

</doc>
<doc id="55279" url="https://en.wikipedia.org/wiki?curid=55279" title="James Dewar">
James Dewar

Sir James Dewar FRS FRSE LLD (20 September 1842 – 27 March 1923) was a Scottish chemist and physicist. He is probably best-known today for his invention of the vacuum flask, which he used in conjunction with extensive research into the liquefaction of gases. He was also particularly interested in atomic and molecular spectroscopy, working in these fields for more than 25 years.
Early life.
James Dewar was born in Kincardine, Perthshire (now in Fife) in 1842, the youngest of six boys of Thomas Dewar, a vintner, and his wife, Ann Eadie. James was educated at Kincardine Parish School and then Dollar Academy. He lost his parents when he was 15, soon after leaving the Academy, but was still able to attend University of Edinburgh. There he studied chemistry under Lyon Playfair (later Baron Playfair) and became Playfair's personal assistant. Dewar also studied under August Kekulé at Ghent.
Career.
In 1875, Dewar was elected Jacksonian professor of natural experimental philosophy at the University of Cambridge, becoming a fellow of Peterhouse. He became a member of the Royal Institution and later, in 1877, replaced Dr. John Hall Gladstone in the role of Fullerian Professor of Chemistry. Dewar was also the President of the Chemical Society in 1897 and the British Association for the Advancement of Science in 1902, as well as serving on the Royal Commission established to examine London's water supply from 1893 to 1894 and the Committee on Explosives. Whilst he was serving on the Committee on Explosives, Frederick Augustus Abel and he developed cordite, a smokeless gunpowder alternative.
In 1867 Dewar described several chemical formulas for benzene. Ironically, one of the formulae, which does not represent benzene correctly and was not advocated by Dewar, is sometimes still called Dewar benzene. In 1869 he was elected a Fellow of the Royal Society of Edinburgh, his proposer being his former mentor, Lyon Playfair.
His scientific work covers a wide field – his earlier papers cover a wide range of topics: organic chemistry, hydrogen and its physical constants, high-temperature research, the temperature of the Sun and of the electric spark, electrophotometry, and the chemistry of the electric arc.
With professor J. G. McKendrick, of Glasgow, he investigated the physiological action of light and examined the changes that take place in the electrical condition of the retina under its influence. With professor G. D. Liveing, one of his colleagues at Cambridge, he began in 1878 a long series of spectroscopic observations, the later of which were devoted to the spectroscopic examination of various gaseous elements separated from atmospheric air by the aid of low temperatures; he was joined by professor J. A. Fleming, of University College London, in the investigation of the electrical behaviour of substances cooled to very low temperatures.
His name is most widely known in connection with his work on the liquefaction of the so-called permanent gases and his researches at temperatures approaching absolute zero. His interest in this branch of physics and chemistry dates back at least as far as 1874, when he discussed the "Latent Heat of Liquid Gases" before the British Association. In 1878, he devoted a Friday evening lecture at the Royal Institution to the then-recent work of Louis Paul Cailletet and Raoul Pictet, and exhibited for the first time in Great Britain the working of the Cailletet apparatus. Six years later, again at the Royal Institution, he described the researches of Zygmunt Florenty Wróblewski and Karol Olszewski, and illustrated for the first time in public the liquefaction of oxygen and air. Soon afterwards, he built a machine from which the liquefied gas could be drawn off through a valve for use as a cooling agent, before using the liquid oxygen in research work related to meteorites; about the same time, he also obtained oxygen in the solid state.
By 1891, he had designed and built, at the Royal Institution, machinery which yielded liquid oxygen in industrial quantities, and towards the end of that year, he showed that both liquid oxygen and liquid ozone are strongly attracted by a magnet. About 1892, the idea occurred to him of using vacuum-jacketed vessels for the storage of liquid gases – the Dewar flask (otherwise known as a Thermos or vacuum flask) – the invention for which he became most famous. The vacuum flask was so efficient at keeping heat out, it was found possible to preserve the liquids for comparatively long periods, making examination of their optical properties possible. Dewar did not profit from the widespread adoption of his vacuum flask – he lost a court case against Thermos concerning the patent for his invention. While Dewar was recognised as the inventor, because he did not patent his invention, no way to stop Thermos from using the design was possible.
He next experimented with a high-pressure hydrogen jet by which low temperatures were realised through the Joule–Thomson effect, and the successful results he obtained led him to build at the Royal Institution a large regenerative cooling refrigerating machine. Using this machine in 1898, liquid hydrogen was collected for the first time, solid hydrogen following in 1899. He tried to liquefy the last remaining gas, helium, which condenses into a liquid at −268.9 °C, but owing to a number of factors, including a lack of helium with which to work, Dewar was preceded by Heike Kamerlingh Onnes as the first person to produce liquid helium, in 1908. Onnes would later be awarded the Nobel Prize in Physics for his research into the properties of matter at low temperatures – Dewar was nominated several times, but never successful in winning the Nobel Prize.
In 1905, he began to investigate the gas-absorbing powers of charcoal when cooled to low temperatures and applied his research to the creation of high vacuum, which was useful for further experiments in atomic physics. Dewar continued his research work into the properties of elements at low temperatures, specifically low-temperature calorimetry, until the outbreak of World War I. The Royal Institution laboratories lost a number of staff to the war effort, both in fighting and scientific roles, and after the war, Dewar had little interest in restarting the serious research work that went on before the war. Shortages of scholars necessarily compounded the problems. His research during and after the war mainly involved investigating surface tension in soap bubbles, rather than further work into the properties of matter at low temperatures.
Death.
James Dewar died at the Royal Institution in London in 1923, still holding the office of Fullerian Professor of Chemistry at the Royal Institution, having refused to retire. He was cremated at the Golders Green Crematorium, where his ashes remain. He was survived by his wife, Lady Helen Rose Dewar (née Banks).
Family.
He married Helen Rose Banks in 1871. They had no children.
Helen was sister-in-law to both Charles Dickson, Lord Dickson and James Douglas Hamilton Dickson.
His nephew, Dr Thomas William Dewar FRSE (1851–1931) was an amateur artist, who painted a portrait of Sir James Dewar. He is presumably also the same Thomas William Dewar as mentioned as executor in James Dewar's will, ultimately replaced "unopposed" by Dewar's wife. This lack of opposition was of course because Thomas died before Sir James, therefore he could not be his executor.
Royal Institution Christmas Lectures.
Dewar was invited to deliver several Royal Institution Christmas Lectures:<br>
Honours and awards.
Whilst Dewar was never recognised by the Swedish Academy, he was recognised by many other institutions both before and after his death, in Britain and overseas. The Royal Society elected him a Fellow of the Royal Society in June 1877 and bestowed their Rumford (1894), Davy (1909), and Copley Medal (1916) medals upon him for his work, as well as inviting him to deliver their Bakerian Lecture in 1901. In 1899, he became the first recipient of the Hodgkins gold medal of the Smithsonian Institution, Washington, DC, for his contributions to knowledge of the nature and properties of atmospheric air.
In 1904, he was the first British subject to receive the Lavoisier Medal of the French Academy of Sciences, and in 1906, he was the first to be awarded the Matteucci Medal of the Italian Society of Sciences. He was knighted in 1904 and awarded the Gunning Victoria Jubilee Prize for 1900–1904 by the Royal Society of Edinburgh, and in 1908, he was awarded the Albert Medal of The Society of Arts. A lunar crater was named in his honour.
Character.
Dewar's irascibility was legendary. Rowlinson (2012) calls him "ruthless". He could certainly be ruthless with his colleagues, as the Siegfried Ruhemann affair indicates.

</doc>
<doc id="55280" url="https://en.wikipedia.org/wiki?curid=55280" title="Grand Prix">
Grand Prix

Grand Prix (, meaning "Grand Prize"; plural Grands Prix) may refer to:

</doc>
<doc id="55283" url="https://en.wikipedia.org/wiki?curid=55283" title="Canning Stock Route">
Canning Stock Route

The Canning Stock Route is a track that runs from Halls Creek in the Kimberley region of Western Australia to Wiluna in the mid-west region. With a total distance of around 1,850 km (1,150 mi) it is the longest historic stock route in the world.
The stock route was proposed as a way of breaking a monopoly that west Kimberley cattlemen had on the beef trade at the beginning of the 20th century. In 1906, the Government of Western Australia appointed Alfred Canning to survey the route. When the survey party returned to Perth, Canning's treatment of Aboriginal guides came under scrutiny leading to a Royal Commission. Canning had been organising Aboriginal hunts to show the explorer where the waterholes were. Despite condemning Canning's methods, the Royal Commission, after the Lord Mayor of Perth, Alexander Forrest had appeared as a witness for Canning, exonerated Canning and his men of all charges. The cook who made the complaints was dismissed and Canning was sent back to finish the job.
Canning was appointed to lead a construction party and between March 1908 and April 1910, 48 wells were completed along the route. Commercial droving began in 1910, but the stock route did not prove popular and was rarely used for the next twenty years. The wells made it difficult for Aboriginal people to access water and in reprisal they vandalised or dismantled many of the wells.
A 1928 Royal Commission into the price of beef in Western Australia led to the repair of the wells and the re-opening of the stock route. Around 20 droves took place between 1931 and 1959 when the final droving run was completed.
The Canning Stock Route is now a popular but challenging four-wheel drive adventure. On rare occasions, people have traversed the track on foot, by bicycle, and in two-wheel drive vehicles.
The building of the stock route impacted on the cultural and social life of the more than 15 Aboriginal language groups and today the Aboriginal history of the track, recorded through oral and artistic traditions, is increasingly being recognised.
History.
In Western Australia at the beginning of the 20th century, east Kimberley cattlemen were looking for a way to traverse the western deserts of Australia with their cattle as a way to break a west Kimberley monopoly that controlled the supply of beef to Perth and the goldfields in the south of the state. East Kimberley cattle were infested with Boophilus ticks infected with a malaria-like parasitic disease called Babesiosis and were prohibited from being transported to southern markets by sea due to a fear that the ticks would survive the journey and spread. This gave west Kimberley cattlemen a monopoly on the beef trade and resulted in high prices.
With east Kimberley cattlemen keen to find a way to get their cattle to market, and the Government of Western Australia keen for competition to bring prices down, a 1905 proposal of a stock route through the desert was taken seriously. James Isdell, an east Kimberley pastoralist and member of the Western Australian Legislative Assembly, proposed the stock route arguing that ticks would not survive in the dry desert climate on the trip south.
Surveying the route.
Calvert and Carnegie expeditions.
The route, which crossed the territories of nine different Aborigine language groups, had been explored previously in 1896 by the Calvert Expedition led by Lawrence Wells and again later that year by the Carnegie Expedition led by David Carnegie. Two members of the Calvert Expedition perished of thirst and the Carnegie Expedition suffered considerable hardships with camels dying after eating poisonous grass and a member of the party accidentally shooting himself dead. Carnegie investigated the possibility of a stock route and concluded that the route was "too barren and destitute of vegetation" and was impractical.
Wells and Carnegie both mistreated Aborigines they encountered on their expeditions, forcing them to cooperate by tying them up and encouraging them to find water. Carnegie is also believed to have fed them salt, and he was later publicly criticised for this. Evidence supports that Alfred Canning had read both the Calvert and Carnegie expedition accounts to find out about the country (which both had described as extremely difficult terrain) and the use of Aboriginal people to find water, an example Canning followed during his own expedition.Both Wells and Carnegie used ropes to tie Aboriginals up so that they could not escape. In Carnegie's case to help them find water and in Wells's case, for help in finding two lost members of their party. Carnegie also deprived his captives of water or fed them salt beef so that they would lead him to water more quickly and he was publicly criticised for this at the time.</ref>
Canning survey.
After it was determined that ticks could not survive a desert crossing, the government endorsed James Isdell's scheme and funded a survey to find a stock route that would cross the Great Sandy Desert, the Little Sandy Desert and the Gibson Desert. Alfred Canning, a surveyor with the Western Australian Department of Lands and Surveys, was appointed to survey the stock route.
Canning's task was to find a route through 1850 kilometres of desert, from Wiluna in the mid west to the Kimberley in the north. He needed to find significant water sources – enough for up to 800 head of cattle, a day's walk apart – where wells could be dug, and enough good grazing land to sustain this number of cattle during the journey south.
In 1906, with a team of 23 camels, two horses, and eight men, Canning surveyed the route completing the difficult journey from Wiluna to Halls Creek in less than six months. On 1 November 1906, shortly after arriving in Halls Creek, Canning sent a telegram to Perth stating that the finished route would "be about the best watered stock route in Colony". Canning was forced to delay his return journey because of an early wet season in the Kimberley that year. The survey party left Halls Creek in late January 1907 and arrived back in Wiluna in early July 1907. During the 14-month expedition, they had trekked about , relying on Aboriginal guides to help them find water.
Canning had always planned to rely on Aboriginal guides to help him find water and had taken neck chains and handcuffs supplied to him by the Wiluna police to make sure local 'guides' stayed as long as he needed them. In order to gain assistance in locating water along the route, Canning captured several Martu men, chained them by the neck and forced them to lead his party to native water sources (soaks). As many soaks were sacred, the Martu may have misdirected the explorers away from these, resulting in the eventual stock route winding more than was actually necessary.
Royal Commission into treatment of Aboriginal people.
After the Canning survey party returned to Perth, Canning's use of Aboriginal guides came under scrutiny. The expedition's cook, Edward Blake, accused Canning of mistreating many of the Aboriginal people they met during the survey expedition. Blake objected to the use of chains and criticised the "party's 'immoral' pursuit of Aboriginal women, the theft and 'unfair' trade of Aboriginal property and the destruction of native waters". Blake was concerned that the planned wells would prevent Aboriginal people accessing water.
Blake's complaints led to a Royal Commission into the Treatment of Natives by the Canning Exploration Party.
Blake was unable to prove many of his claims, but Canning did admit to the use of chains. Kimberley Explorer and the first Premier of Western Australia, John Forrest, dismissed Canning's actions by claiming that all explorers behaved in this manner. Despite condemning the use of chains, the Royal Commission accepted the survey party's actions as "reasonable" and Canning and his men were exonerated of all charges, including "immorality with native women" and stealing property. The Royal Commission approved the immediate commencement of the stock route's construction. Canning was appointed to lead the construction party.
Construction.
Canning left Perth in March 1908, along with 30 men, 70 camels, four wagons, 100 tonnes of food and equipment and 267 goats (for milk and meat), and travelled the route again to commence the construction of well heads and water troughs at the 54 water sources identified by his earlier expedition. He arrived back in Wiluna in April 1910 having completed the last of 48 wells and bringing the total cost of the route to £22000 (2010: A$2.6 million).
Thirty-seven of the wells were built on or near existing Aboriginal waters and were constructed in the European tradition, which made many of them inaccessible to Aboriginal people. Pulling the heavy buckets up from the bottom of the wells required the strength of three men or use of a camel. Consequentially, many Aboriginal people were injured or died while trying to access the water, either falling in and drowning or breaking bones on the windlass handle. In reprisal, buckets were cut off or timber set on fire, and by 1917 Aboriginal people had vandalised or dismantled approximately half of the wells in a bid to reclaim access to the water or to prevent drovers from using the wells. Canning's party had constructed the wells with the forced help of one of the Aboriginal peoples whose land the route traversed, the Martu.
Canning produced a detailed map of the stock route, Plan of Wiluna–Kimberley stock route exploration (showing positions of wells constructed 1908–9 and 10) on which he also recorded his observations of the land and water sources along the route. The map has become a symbol of Australia's pioneering history.
Using the stock route.
First droving runs.
Commercial droving along the stock route began in 1910. The first few droves were of small groups of horses — the first started out with 42 horses of which only nine survived the journey.
The first mob of bullocks to attempt to use the stock route set out in January 1911; however the party of three drovers, George Shoesmith, James Thompson and an Aboriginal stockman who was known as 'Chinaman', were killed by Aborigines at Well 37. Thomas Cole discovered their bodies later in 1911 during his successful drove along the stock route. In September 1911, Sergeant R.H. Pilmer led a police 'punitive expedition' to find the culprits and ensure the stock route remained open. The police made no arrests, but the expedition was considered a success after Pilmer acknowledged killing at least 10 Aborigines.
On 7 September 1911 it was reported that the first mob of cattle to traverse the entire length of the stock route had successfully arrived in Wiluna. The cattle had apparently gained condition on the long drove.
Despite police protection, drovers were afraid to use the track and it was rarely used for almost 20 years. Between 1911 and 1931, only eight mobs of cattle were driven along the Canning Stock Route.
Reopening of the stock route.
A 1928 Royal Commission into the price of beef in Western Australia led to the re-opening of the stock route. In 1929, William Snell was commissioned to repair the wells and found that the only wells undamaged were the ones that Aboriginal people could use. Snell criticised the construction of Canning's wells because they were difficult for Aboriginal people to use safely, and he put the destruction of the wells down to the anger and frustration people felt at being unable to access traditional water sources. Snell personally committed to making the wells more accessible to Aboriginal people:Natives cannot draw water from the Canning Stock Route wells. It takes three strong white men to land a bucket of water. It is beyond the natives power to land a bucket. They let go the handle some times escape with their life but get an arm and head broken in the attempt to get away. To heal the wounds so severely inflicted and [as a safeguard against the natives destroying the wells again I equipped the wells ... so that the native can draw water from the wells without destroying them.—William Snell
Snell started work on the refurbishment of the wells, fitting some with ladders for easier access, but he abandoned the work after well 35. Reports vary that he either ran out of materials or the desert became too much for him.
In 1930, Alfred Canning (then aged 70) was commissioned to complete the work. While Snell had encountered no hostility, Canning had trouble with the Aborigines from the start but successfully completed the commission in 1931.
With these improvements, the route was used on a more regular basis although in total, it would only be used around 20 times between 1931 and 1959 when the last droving run was completed. None of the larger station owners used the track as it was found that only 600 head of cattle could be supported at a time, which was 200 less than was estimated when first completed. As Carnegie had accurately reported in 1896, the track was impractical for cattle drives.
During the Second World War the track was upgraded at considerable expense in case it was needed for an evacuation of the north if Australia was invaded. Including horse drives there have been only 37 recorded drives between 1910 and the last run in 1959.
Traverses.
In the 1950s horses became scarce in the Kimberley as widespread losses were caused by "Walkabout Poison". This led to the stock route being used to drove horses north from around the Norseman area where they were sold to the stations. Wally Dowling, a drover who had made nine droves along the stock route took what was probably the last group of horses northwards along the route in September 1951.
In 1968 the entire length of the track was driven for the first time by surveyors Russell Wenholz and David Chudleigh.
In 1972, before the route was regularly negotiated in four-wheel drives, ambitious attempts to complete it on foot took place. A New Zealander, Murray Rankin, and two English brothers, John and Peter Waterfall, fashioned homemade trolleys from bicycle wheels and metal tubing, and began their attempt starting from Wiluna in early June 1972. First John and then Peter turned back, but Rankin continued to Lake Disappointment before being forced to abandon the attempt. The remains of one of their trolleys lie 19 km north of Well 15.
In 1973 Rankin tried again, this time starting from Old Halls Creek with Englishman John Foulsham. This time they had professionally built trolleys with motor-cycle wheels. The walk began on 1 June. Soon after reaching Godfrey's Tank they were unable to pull the trolleys over the high sand hills. They left them and walked on to Lake Tobin and there abandoned the attempt and returned to Halls Creek.
Three years later,in 1976, Rankin achieved his ambition to walk the stock route. After driving the route in a Land Rover and establishing food depots along the way, he set out from Halls Creek on 12 July 1976 with three other bushwalkers, Ralph Barraclough, Kathy Borman and Rex Shaw. Barraclough turned back after becoming ill, but the others completed the journey in just under three months. 
In 1977, the first commercial tour completed the drive.
During the 1980s fuel dumps were created and adventurous travellers became interested in the history of the track and the challenge to drive it.
In 1985, a Beach Buggy and a Citroën 2CV became the first two-wheel drive vehicles to complete the entire route.
In 1994 long distance walker Drew Kettle walked the route.
In 1997 Robin Rishworth cycled, with the aid of food drops, in just less than 27 days, and is considered to be the first modern day solo cyclist.
In 2004 Kate Leeming, as part of a longer trek, completed the route with the aid of a support vehicle.
In 2005 Jakub Postrzygacz became the first person to traverse the entire track without either support or the use of food drops, travelling alone by bicycle for 33 days. With large tyres and a single-wheel trailer, he carried all his food with him and replenished his water at wells.
Present.
Tourism.
The Canning Stock route is considered one of the world's great four-wheel drive adventures. Apart from keeping the track open, the route is not maintained. Some wells have been restored but others are in ruins and unusable. While quite a few travellers successfully make the trip, it still requires substantial planning and a convoy of well-equipped four-wheel drives or equivalent vehicles, and is only practical during the cooler months. Fuel drops typically need to be organised in advance and the trip will take two to three weeks. Fuel is now available at Kunawarritji Community near well 33 and Parnngurr Community near well 22.
Aboriginal perspective.
The history of the Canning Stock Route has been well documented from the colonial perspective – accounts of European explorers, drovers, prospectors and law enforcers – but increasingly the Aboriginal history of the track is also being recognised, and Aboriginal people are keen to have their story told:We wanna tell you fellas 'bout things been happening in the past that hasn't been recorded, what old people had in their head. No pencil and paper. The white man history has been told and it's today in the book. But our history is not there properly. We've got to tell 'em through our paintings. — Clifford Brooks, Wiluna, 2006
Archaeologists now believe that the Western Desert has been occupied for around 30,000 years. For Aboriginal people, the history of the stock route is therefore part of a much older story. They have recorded this story, including the changes brought about by the construction of the stock route, through oral and artistic traditions.
The building of the stock route impacted on the cultural and social life of the more than 15 Aboriginal language groups that have a "cultural, familial or historical connection to the route and its custodians, or to sites along the major Dreaming tracks or songlines". Some Dreaming tracks exist within the Country of a single language group, but others cross the territory of many groups and the major Dreaming tracks often mark the territorial boundaries of the Countries they cross. The stock route, and the people and stock it brought with it, inevitably interrupted traditional patterns of movement and connection to Country.
While many Aboriginal people made a determined effort to avoid contact with the people the stock route brought into their Country, the route became a path out of the desert for others. At different times, and for different reasons, people moved away to the outskirts of towns, to pastoral stations and church missions. Many found work with the drovers using the stock route and successful droves relied on the skill of these Aboriginal stockmen and women. Others left looking for more reliable sources of food and water, especially in times of drought, while some were drawn to the changes taking place around the edges of the desert or motivated by a desire to join family already living elsewhere.
Rock art project.
There are a large number of Aboriginal rock paintings and carvings along the stock route. As more and more people visit the area each year, custodians of the Western Desert have become concerned about the protection and management of Aboriginal sites along the route. In 2007, researchers from the Australian National University began a project to draw up the first comprehensive plan of management for the entire Canning Stock Route. The project aimed to develop a series of modules to inform detailed guides and signs for visitors, while also protecting sites that have special significance for Indigenous peoples.
The Canning Stock Route Project.
The Canning Stock Route has a strong connection to the story of Aboriginal art in the Western Desert. When droving along the stock route led to many family groups dispersing to the edges of the desert, communities were established in missions, towns, stations and settlements, and it was here that contemporary painting movements flourished.
In 2006, West Australian independent cultural organisation FORM instigated a contemporary arts and cultural initiative to "explore the complex history of the Canning Stock Route through the prism of contemporary Aboriginal art". Partnerships among nine art centres and communities with direct connections to the stock route region were set up. The project involved several years of research by FORM in collaboration with Aboriginal artists and their art centres and organisations.
A major part of the project's program of bush work was a six-week, 1850-kilometre desert journey from Wiluna to Billiluna. During this trip, and in follow-up workshops and other trips, 80 artists created a collection of paintings, contemporary cultural objects and documentary material.
The historical and artistic value of the project was recognised in 2008 when the National Museum of Australia decided to acquire the entire Canning Stock Route Project collection.
The Canning Stock Route Collection.
The National Museum of Australia acquired a significant collection of artworks and other material collected by the 60 artists who travelled along the Canning Stock Route on a six-week return to country trip in 2007 as part of the Canning Stock Route Project. The Canning Stock Route collection includes over 100 works of art, 120 oral histories, historical research, social and cultural data, artists' biographies, 20,000 photographs and over 200 hours of film footage.
One of the key aims of the Canning Stock Route Project was the development of a travelling exhibition. The National Museum of Australia committed to assisting FORM to develop an exhibition. "Yiwarra Kuju (One Road) – The Canning Stock Route", a joint initiative between the National Museum of Australia and FORM, was held at the museum from July 2010 to January 2011. The exhibition used works of art and stories to tell the story of the stock route's impact on Aboriginal people from an Aboriginal perspective. When it closed in January 2011, "Yiwarra Kuju – The Canning Stock Route" had been the most successful exhibition in the history of the Museum, with over 120,000 visitors.
Journey distances.
The nearest capital city to the Wiluna starting point of the route is Perth, south west of Wiluna by road. Then to return to Perth via sealed roads from Halls Creek it's Including the Canning route this gives a total driving distance of .

</doc>
<doc id="55284" url="https://en.wikipedia.org/wiki?curid=55284" title="Coat of arms">
Coat of arms

A coat of arms is a unique heraldic design on an escutcheon (i.e. shield), surcoat, or tabard. The coat of arms on an escutcheon forms the central element of the full heraldic achievement which consists of shield, supporters, crest, and motto. The design is a symbol unique to an individual person or family (except in the UK), corporation, or state.
History.
The ancient Romans used similar insignia on their shields, but these identified military units rather than individuals. The first evidence of medieval coats of arms is found in the 11th century Bayeux Tapestry in which some of the combatants carry shields painted with crosses. Coats of arms came into general use by feudal lords and knights in battle in the 12th century. By the 13th century, arms had spread beyond their initial battlefield use to become a flag or emblem for families in the higher social classes of Europe, inherited from one generation to the next. Exactly who had a right to use arms, by law or social convention, varied to some degree between countries. In the German-speaking regions both the aristocracy and "burghers" (non-noble free citizens) used arms, while in most of the rest of Europe they were limited to the aristocracy. The use of arms spread to the clergy, to towns as civic identifiers, and to royally chartered organizations such as universities and trading companies. Flags developed from coats of arms, and the arts of vexillology and heraldry are closely related. The coats of arms granted to commercial companies are a major source of the modern logo.
Despite no widespread regulation, heraldry has remained consistent across Europe, where tradition alone has governed the design and use of arms. Some nations, like England and Scotland, still maintain the same heraldic authorities which have traditionally granted and regulated arms for centuries and continue to do so in the present day. In Britain, for example, the granting of arms is and has been controlled by the College of Arms. Unlike seals and other general emblems, heraldic "achievements" have a formal description called a blazon, which uses vocabulary that allows for consistency in heraldic depictions. In the present day, coats of arms are still in use by a variety of institutions and individuals: for example, many European cities and universities have guidelines on how their coats of arms may be used, and protect their use as trademarks. Many societies exist that also aid in the design and registration of personal arms.
Traditions and usage.
In the heraldic traditions of England and Scotland, an individual, rather than a family, had a coat of arms. In those traditions coats of arms are legal property transmitted from father to son; wives and daughters could also bear arms modified to indicate their relation to the current holder of the arms. Undifferenced arms are used only by one person at any given time. Other descendants of the original bearer could bear the ancestral arms only with some difference: usually a colour change or the addition of a distinguishing charge. One such charge is the label, which in British usage (outside the Royal Family) is now always the mark of an heir apparent or (in Scotland) an heir presumptive. Because of their importance in identification, particularly in seals on legal documents, the use of arms was strictly regulated; few countries continue in this today. This has been carried out by heralds and the study of coats of arms is therefore called "heraldry". In time, the use of arms spread from military entities to educational institutes, and other establishments.
In his book, "The Visual Culture of Violence in the Late Middle Ages", Valentin Groebner argues that the images composed on coats of arms are in many cases designed to convey a feeling of power and strength, often in military terms. The author Helen Stuart argues that some coats of arms were a form of corporate logo. Museums on medieval armory also point out that as emblems they may be viewed as precursors to the corporate logos of modern society, used for group identity formation.
When knights were encased in armour that no means of identifying them was left, the practice was introduced of painting their insignia of honour on their shield as an easy method of distinguishing them. Originally these were granted only to individuals, but were afterward made hereditary in England by King Richard I, during his crusade to the Holy Land.
European tradition.
French and British heraldry.
The French system of heraldry greatly influenced the British and Western European systems. Much of the terminology and classifications are taken from it. However, with the fall of the French monarchy (and later Empire) there is not currently a "Fons Honorum" (power to dispense and control honors) to strictly enforce heraldic law. The French Republics that followed have either merely affirmed pre-existing titles and honors or vigorously opposed noble privilege. Coats of arms are considered an intellectual property of a family or municipal body. Assumed arms (arms invented and used by the holder rather than granted by an authority) are considered valid unless they can be proved in court to copy that of an earlier holder.
In Scotland, the Lord Lyon King of Arms has criminal jurisdiction to control the use of arms. In England, Northern Ireland and Wales the use of arms is a matter of civil law and regulated by the College of Arms and the Court of Chivalry.
In reference to a dispute over the exercise of authority over the Officers of Arms in England, Arthur Annesley, 1st Earl of Anglesey, Lord Privy Seal, declared on 16 June 1673 that the powers of the Earl Marshal were "to order, judge, and determine all matters touching arms, ensigns of nobility, honour, and chivalry; to make laws, ordinances, and statutes for the good government of the Officers of Arms; to nominate Officers to fill vacancies in the College of Arms; to punish and correct Officers of Arms for misbehaviour in the execution of their places". It was further declared that no patents of arms or any ensigns of nobility should be granted and no augmentation, alteration, or addition should be made to arms without the consent of the Earl Marshal.
Irish heraldry.
In Ireland the usage and granting of coats of arms was strictly regulated by the Ulster King of Arms from the office's creation in 1552. After Irish independence in 1922 the office was still functioning and working out of Dublin Castle. The last Ulster King of Arms was Sir Nevile Rodwell Wilkinson King of Arms 1908-1940, who held it until his death in 1940. At the Irish government's request, no new King of Arms was appointed. Thomas Ulick Sadleir, the Deputy Ulster King of Arms, then became the Acting Ulster King of Arms. He served until the office was merged with that of Norroy King of Arms in 1943 and stayed on until 1944 to clear up the backlog.
An earlier Ireland King of Arms was created by King Richard II in 1392 and discontinued by King Henry VII in 1487. It didn't grant many coats of arms - the few it did grant were annulled by the other Kings of Arms because they encroached upon their jurisdictions. Its purpose was supposedly to marshal an expedition to fully conquer Ireland that never materialized. Since 1 April 1943 the authority has been split between the Republic of Ireland and Northern Ireland. Heraldry in the Republic of Ireland is regulated by the Government of Ireland, by the Genealogical Office through the Office of the Chief Herald of Ireland. Heraldry in Northern Ireland is regulated by the British Government by the College of Arms through the Norroy and Ulster King of Arms.
German and Scandinavian heraldry.
The heraldic tradition and style of modern and historic Germany and the Holy Roman Empire — including national and civic arms, noble and burgher arms, ecclesiastical heraldry, heraldic displays, and heraldic descriptions — stand in contrast to Gallo-British, Latin and Eastern heraldry, and strongly influenced the styles and customs of heraldry in the Nordic countries, which developed comparatively late.
In the Nordic countries, provinces, regions, cities, and municipalities have coats of arms. These are posted at the borders and on buildings containing official offices, as well as used in official documents and on the uniforms of municipal officers. Arms may also be used on souvenirs or other effects, given that an application has been granted by the municipal council.
Other European countries.
At a national level, "coats of arms" were generally retained by European states with constitutional continuity of more than a few centuries, including constitutional monarchies like Denmark as well as old republics like San Marino and Switzerland.
In Italy the use of coats of arms was only loosely regulated by the states existing before the unification of 1861. Since the Consulta Araldica, the college of arms of the Kingdom of Italy, was abolished in 1948, personal coats of arms and titles of nobility, though not outlawed, are not recognised.
Coats of arms in Spain were generally left up to the owner themselves, but the design was based on military service and the heritage of their grandparents. In France, the coat of arms is based on the Fleur-de-lys and the "Rule of Tinctures" used in English heraldry as well.
Among the states ruled by communist regimes, emblems resembling the Soviet design were adopted in all the Warsaw Pact states except Czechoslovakia and Poland. Since 1989, some of the ex-Communist states, as Romania or Russia have reused their original pre-communist heraldry, often with only the symbols of monarchy removed. Other countries such as Belarus or Tajikistan have retained their communist coats of arms or at least kept some of the old heraldry.
Asia and Africa.
Japanese emblems, called "kamon" (often abbreviated "mon"), are family badges which often date back to the 7th century, and are used in Japan today. The Japanese tradition is independent of the European, but many abstract and floral elements are used.
Sometimes simple items express an origin to a specific design. An example in recent use is the logo of Mitsubishi corporation which started as a shipping and maritime enterprise and whose emblem is based on a water chestnut derived from its maritime history with a military naval influence. The word "mitsu" means the number 3 and the word "hishi" meaning "water chestnut" (pronounced "bishi" in some combinations; see rendaku) originated from the emblem of the warrior Tosa Clan. The battleships of the Tosa Clan had been used in the late 19th century in the First Sino-Japanese War to reach Korea and their name was given to a modern battleship. The Tosa water chestnut leaf mon was then drawn as a rhombus or diamond shape in the Mitsubishi logo.
With the formation of the modern nation states of the Arab World in the second half of the 20th century, European traditions of heraldry were partially adopted for state emblems. These emblems often involve the star and crescent symbol taken from the Ottoman flag.
Another commonly seen symbol is the eagle, which is a symbol attributed to Saladin, and the hawk of the Qureish. These symbols can be found on the Coat of Arms of Egypt and Syria.
New World practices.
The Queen of Canada has delegated her prerogative to grant armorial bearings to the Governor General of Canada. Canada has its own Chief Herald and Herald Chancellor. The Canadian Heraldic Authority is situated at Rideau Hall. The Great Seal of the United States uses on the obverse as its central motif an heraldic achievement described as being the arms of the nation. The seal, and the armorial bearings, were adopted by the Continental Congress on 20 June 1782, and is a shield divided palewise into thirteen pieces, with a blue chief, which is displayed upon the breast of an American bald eagle. The crest is thirteen stars breaking through a glory and clouds, displayed with no helm, torse, or mantling (unlike most European precedents). Only a few of the American states have adopted a coat of arms, which is usually designed as part of the respective state's seal. Vermont has both a state seal and a state coat of arms that are independent of one another (though both contain a pine tree, a cow and sheaves of grain); the seal is used to authenticate documents, whilst the heraldic device represents the state itself.
Ecclesiastic practice.
The Vatican City State and the Holy See each have their own coat of arms. As the papacy is not hereditary, its occupants display their personal arms combined with those of their office. Some popes came from armigerous (noble) families; others adopted coats of arms during their career in the Church. The latter typically allude to their ideal of life, or to specific pontifical programmes. A well-known and widely displayed example in recent times was Pope John Paul II's arms. His selection of a large letter M (for the Virgin Mary) was intended to express the message of his strong Marian devotion. Roman Catholic dioceses are also each assigned a coat of arms, as are basilicas or papal churches, the latter usually displaying these on the building. These may be used in countries which otherwise do not use heraldic devices. In countries like Scotland with a strong statutory heraldic authority, arms will need to be officially granted and recorded.
Flags and banners.
Flags are used to identify ships (where they are called ensigns), embassies and such, and they use the same colors and designs found in heraldry, but they are not usually considered to be heraldic. A country may have both a national flag and a national coat of arms, and the two may not look alike at all. For example, the flag of Scotland (St Andrew's Cross) has a white saltire on a blue field, but the royal arms of Scotland has a red lion within a double tressure on a gold (or) field.

</doc>
<doc id="55285" url="https://en.wikipedia.org/wiki?curid=55285" title="Ernst Mach">
Ernst Mach

Ernst Waldfried Josef Wenzel Mach (; ; February 18, 1838 – February 19, 1916) was an Austrian physicist and philosopher, noted for his contributions to physics such as the Mach number and the study of shock waves. As a philosopher of science, he was a major influence on logical positivism, American pragmatism and through his criticism of Newton, a forerunner of Einstein's relativity.
Biography.
Ernst Waldfried Josef Wenzel Mach was born in Brno-Chrlice (), Moravia (then in the Austrian empire, now part of Brno in the Czech Republic). His father, who had graduated from Charles University in Prague, acted as tutor to the noble Brethon family in Zlín, eastern Moravia. His grandfather, Wenzl Lanhaus, an administrator of the estate Chirlitz, was also master builder of the streets there. His activities in that field later influenced the theoretical work of Ernst Mach. Some sources give Mach's birthplace as Turas/Tuřany (now also part of Brno), the site of the Chirlitz registry-office. Peregrin Weiss baptized Ernst Mach into the Roman Catholic Church in Turas/Tuřany. Despite his Catholic background, he later became an atheist and his theory and life is compared with Buddhism.
Up to the age of 14, Mach received his education at home from his parents. He then entered a Gymnasium in Kroměříž (), where he studied for three years. In 1855 he became a student at the University of Vienna. There he studied physics and for one semester medical physiology, receiving his doctorate in physics in 1860 and his Habilitation the following year. His early work focused on the Doppler effect in optics and acoustics. In 1864 he took a job as Professor of Mathematics at the University of Graz, having turned down the position of a chair in surgery at the University of Salzburg to do so, and in 1866 he was appointed as Professor of Physics. During that period, Mach continued his work in psycho-physics and in sensory perception. In 1867, he took the chair of Experimental Physics at the Charles University, Prague, where he stayed for 28 years before returning to Vienna.
Mach's main contribution to physics involved his description and photographs of spark shock-waves and then ballistic shock-waves. He described how when a bullet or shell moved faster than the speed of sound, it created a compression of air in front of it. Using schlieren photography, he and his son Ludwig were able to photograph the shadows of the invisible shock waves. During the early 1890s Ludwig was able to invent an interferometer which allowed for much clearer photographs. But Mach also made many contributions to psychology and physiology, including his anticipation of gestalt phenomena, his discovery of the oblique effect and of Mach bands, an inhibition-influenced type of visual illusion, and especially his discovery of a non-acoustic function of the inner ear which helps control human balance.
One of the best-known of Mach's ideas is the so-called "Mach principle," concerning the physical origin of inertia. This was never written down by Mach, but was given a graphic verbal form, attributed by Philipp Frank to Mach himself, as, "When the subway jerks, it's the fixed stars that throw you down."
Mach also became well known for his philosophy developed in close interplay with his science. Mach defended a type of phenomenalism recognizing only sensations as real. This position seemed incompatible with the view of atoms and molecules as external, mind-independent things. He famously declared, after an 1897 lecture by Ludwig Boltzmann at the Imperial Academy of Science in Vienna: "I don't believe that atoms exist!" From about 1908 to 1911 Mach's reluctance to acknowledge the reality of atoms was criticized by Max Planck as being incompatible with physics. Einstein's 1905 demonstration that the statistical fluctuations of atoms allowed measurement of their existence without direct individuated sensory evidence marked a turning point in the acceptance of atomic theory. Some of Mach's criticisms of Newton's position on space and time influenced Einstein, but later Einstein realized that Mach was basically opposed to Newton's philosophy and concluded that his physical criticism was not sound.
In 1898 Mach suffered from cardiac arrest and in 1901 retired from the University of Vienna and was appointed to the upper chamber of the Austrian parliament. On leaving Vienna in 1913 he moved to his son's home in Vaterstetten, near Munich, where he continued writing and corresponding until his death in 1916, only one day after his 78th birthday. His current living descendant is Marilyn vos Savant (her father was Joseph Mach).
Physics.
Most of Mach's initial studies in the field of experimental physics concentrated on the interference, diffraction, polarization and refraction of light in different media under external influences. From there followed important explorations in the field of supersonic fluid mechanics. Mach and physicist-photographer Peter Salcher presented their paper on this subject in 1887; it correctly describes the sound effects observed during the supersonic motion of a projectile. They deduced and experimentally confirmed the existence of a shock wave which of conical shape, with the projectile at the apex. The ratio of the speed of a fluid to the local speed of sound "vp"/"vs" is now called the Mach number. It is a critical parameter in the description of high-speed fluid movement in aerodynamics and hydrodynamics. Mach also contributed to cosmology the hypothesis known as Mach's principle.
Philosophy of science.
From 1895 to 1901, Mach held a newly created chair for "the history and philosophy of the inductive sciences" at the University of Vienna. In his historico-philosophical studies, Mach developed a phenomenalistic philosophy of science which became influential in the 19th and 20th centuries. He originally saw scientific laws as summaries of experimental events, constructed for the purpose of making complex data comprehensible, but later emphasized mathematical functions as a more useful way to describe sensory appearances. Thus scientific laws while somewhat idealized have more to do with describing sensations than with reality as it exists beyond sensations.
Mach's positivism also influenced many Russian Marxists, such as Alexander Bogdanov (1873–1928). In 1908, Lenin wrote a philosophical work, "Materialism and Empirio-criticism" (published 1909), in which he criticized Machism and the views of "Russian Machists".
In accordance with this philosophy, Mach opposed Ludwig Boltzmann and others who proposed an atomic theory of physics. Since one cannot observe things as small as atoms directly, and since no atomic model at the time was consistent, the atomic hypothesis seemed to Mach to be unwarranted, and perhaps not sufficiently "economical". Mach had a direct influence on the Vienna Circle philosophers and the school of logical positivism in general.
Mach is attributed with a number of principles that distill his ideal of physical theorisation — what is now called "Machian physics":
The last is singled out, particularly by Albert Einstein as "the" Mach's principle. Einstein cited it as one of the three principles underlying general relativity. In 1930, he stated that "it is justified to consider Mach as the precursor of the general theory of relativity", though Mach, before his death, would reject Einstein's theory. Einstein was aware that his theories did not fulfill all Mach's principles, and no subsequent theory has either, despite considerable effort.
Phenomenological constructivism.
According to Alexander Riegler, Ernst Mach's work was a precursor to the influential perspective known as constructivism. Constructivism holds that all knowledge is constructed rather than received by the learner. He took an exceptionally non-dualist, phenomenological position. The founder of radical constructivism, von Glasersfeld, gave a nod to Mach as an ally.
Physiology.
In 1873, independently of each other
Mach and the physiologist and physician Josef Breuer discovered how the sense of balance (i.e., the perception of the head’s imbalance) functions, tracing its management by information which the brain receives from the movement of a fluid in the semicircular canals of the inner ear. That the sense of balance depended on the three semicircular canals was discovered in 1870 by the physiologist Friedrich Goltz, but Goltz didn't discover how the balance-sensing apparatus functioned. Mach devised a swivel chair to enable him to test his theories, and Floyd Ratcliff has suggested that this experiment may have paved the way to Mach's critique of a physical conception of absolute space and motion.
Psychology.
In the area of sensory perception, psychologists remember Mach for the optical illusion called Mach bands. The effect exaggerates the contrast between edges of the slightly differing shades of gray, as soon as they contact one another, by triggering edge-detection in the human visual system.
More clearly than anyone before (or even since) Mach made the distinction between what he called "physiological" (specifically visual) and "geometrical" spaces.
Mach's views on mediating structures inspired B. F. Skinner's strongly inductive position, which paralleled Mach's in the field of psychology.
Eponyms.
In homage his name was given to:

</doc>
<doc id="55289" url="https://en.wikipedia.org/wiki?curid=55289" title="Caracas">
Caracas

Caracas (), officially Santiago de León de Caracas, is the capital, the center of the Greater Caracas Area, and the largest city of Venezuela. Caracas is located along the Guaire River in the northern part of the country, following the contours of the narrow Caracas Valley on the Venezuelan coastal mountain range (Cordillera de la Costa). Terrain suitable for building lies between above sea level. The valley is close to the Caribbean Sea, separated from the coast by a steep mountain range, Cerro El Ávila; to the south there are more hills and mountains.
The Metropolitan District of Caracas is made up of five municipalities: Libertador Municipality which is the only administrative division of the Venezuelan Capital District, and four other municipalities, which are within in Miranda State: Chacao, Baruta, Sucre, and El Hatillo. Libertador holds many of the government buildings and is the Capital District ("Distrito Capital"). The Distrito Capital had a population of 2,013,366 , while the Metropolitan District of Caracas was estimated at 3,273,863 as of (2013). The Metropolitan Region of Caracas has an estimated population of 5,243,301.
Businesses that are located in the city include service companies, banks, and malls, among others. It has a largely service-based economy, apart from some industrial activity in its metropolitan area. The Caracas Stock Exchange and Petróleos de Venezuela (PDVSA) are headquartered in Caracas. PDVSA is the largest company in Venezuela. Caracas is also Venezuela's cultural capital, with many restaurants, theaters, museums, and shopping centers. Some of the tallest skyscrapers in Latin America are located in Caracas.
History.
At the time of the founding of the city in 1567, the valley of Caracas was populated by indigenous peoples. Francisco Fajardo, the son of a Spanish captain and a Guaiqueri "cacica", attempted to establish a plantation in the valley in 1562 after founding a series of coastal towns. Fajardo's settlement did not last long. It was destroyed by natives of the region led by Terepaima and Guaicaipuro. This was the last rebellion on the part of the natives. On 25 July 1567, Captain Diego de Losada laid the foundations of the city of "Santiago de León de Caracas". The foundation − 1567 – "I take possession of this land in the name of God and the King" These were the words of Don Diego de Losada in founding the city of Caracas on 25 July 1567. In 1577 Caracas became the capital of the Spanish Empire's Venezuela Province under Governor Juan de Pimentel (1576–1583).
During the 17th century, the coast of Venezuela was frequently raided by pirates. With the coastal mountains as a barrier, Caracas was relatively immune to such attacks. However, in 1595, around 200 English privateers including George Sommers and Amyas Preston crossed the mountains through a little-used pass while the town's defenders were guarding the more often-used one. Encountering little resistance, the invaders sacked and set fire to the town after a failed ransom negotiation.
As the cocoa cultivation and exports under the Compañía Guipuzcoana de Caracas grew in importance, the city expanded. In 1777, Caracas became the capital of the Captaincy General of Venezuela.
José María España and Manuel Gual led an attempted revolution aimed at independence, but the rebellion was put down on 13 July 1797. Caracas was ultimately the site of the signing of a Declaration of Independence on 5 July 1811. In 1812, an earthquake destroyed Caracas. The revolutionary war continued until 24 June 1821, when Bolívar defeated royalists in the Battle of Carabobo.
Caracas grew in economic importance during Venezuela's oil boom in the early 20th century. During the 1950s, Caracas began an intensive modernization program which continued throughout the 1960s and early 1970s. The Universidad Central de Venezuela, designed by modernist architect Carlos Raúl Villanueva and declared World Heritage by UNESCO, was built. New working- and middle-class residential districts sprouted in the valley, extending the urban area toward the east and southeast. Joining El Silencio, also designed by Villanueva, were several workers' housing districts, 23 de Enero and Simon Rodriguez. Middle-class developments include Bello Monte, Los Palos Grandes, Chuao, and El Cafetal. The dramatic change in the economic structure of the country, which went from being primarily agricultural to dependent on oil production, stimulated the fast development of Caracas, and made it a magnet for people in rural communities who migrated to the capital city in an unplanned fashion searching for greater economic opportunity. This migration created the "rancho" (slum) belt of the valley of Caracas.
Symbols.
The flag of Caracas consists of a burgundy red field with the version of the Coat of Arms of the City (effective since the 1980s). The red field symbolises the blood spilt by Caraquenian people in favour of independence and the highest ideals of the Venezuelan Nation. Later, in the year 1994, presumably as a result of the change of municipal authorities, it was decided to increase the size of the Caracas coat of arms and move it to the centre of the field. This version of the flag is still in use today.
The coat of arms of the City of Caracas was adopted by the Libertador Municipality to identify itself. Later, the Metropolitan Mayor Office assumed the lion, the scallop and Saint James' Cross for the same purpose.
The anthem of the city is the "Marcha a Caracas", written by the composer Tiero Pezzuti de Matteis with the lyrics by José Enrique Sarabia. The lyrics are said to be inspired by the heroism of the Caraquenian people, and the memory of the "City of Red Roofs". Incidentally, the National Anthem of Venezuela, "Gloria al Bravo Pueblo", includes the lines "...y si el despotismo levanta la voz, seguid el ejemplo que Caracas dio" ("...and if despotism raises its voice, follow the example that Caracas gave"), reflecting the fact that, in addition to generously providing many heroic fighters to the War of Independence, the junta established in Caracas (19 April 1810) served as inspiration for other regions to do the same—as did its declaration of independence a year later.
Local government.
Caracas has five municipalities: Baruta, El Hatillo, Chacao, Libertador and Sucre. Under the constitution of Venezuela, municipal governments have two branches: the executive (governed by a mayor) and the legislative (managed by a municipal council). On 8 March 2000, the year after a new constitution was introduced in Venezuela, it was decreed in "Gaceta Official" N° 36,906 that the Metropolitan District of Caracas would be created, and that some of the powers of these municipalities would be delegated to the "Alcaldía Mayor", physically located in the large Libertador municipality, in the center of the city.
Economy.
Businesses that are located here include service companies, banks, and malls, among others. It has a largely service-based economy, apart from some industrial activity in its metropolitan area. The Caracas Stock Exchange and Petróleos de Venezuela (PDVSA) are headquartered here. The PDVSA is the largest company in Venezuela, and negotiates all the international agreements for the distribution and export of petroleum. When the company existed, the airline Viasa had its headquarters in the Torre Viasa.
Caracas' central business district is Milla de Oro, which is located in the north of the Baruta municipality and the south of the Chacao municipality, it is one of largest financial districts of Latin America, it is home to many companies and is dominated by numerous high-rises. Other important business districts include Plaza Venezuela, Parque Central Complex and El Recreo.
Small and medium-size industry contributes to the Caracas economy. The city provides communication and transportation infrastructure between the metropolitan area and the rest of the country. Important industries in Caracas include chemicals, textiles, leather, food, iron and wood products. There are also rubber and cement factories. Its GDP(Nominal) is 69 billion $ and the GDP(PPP) per Capita is $24,000 
Cost of living.
A 2009 United Nations survey reported that the cost of living in Caracas was 89% of that of its baseline city: New York. However, this statistic is based upon a fixed currency-exchange-rate of 2003 and might not be completely realistic, due to the elevated inflation rates of the last several years.
Geography.
Caracas is contained entirely within a valley of the Venezuelan central range, and separated from the Caribbean coast by a roughly expanse of El Ávila National Park. The valley is relatively small and quite irregular, the altitude with respect to sea level varies from between , with in the historic zone. This, along with the rapid population growth, has profoundly influenced the urban development of the city. The most elevated point of the Capital District, wherein the city is located, is the "Pico El Ávila", which rises to . The main body of water in Caracas is the Guaire River, which flows across the city and empties into the "Tuy River", which is also fed by the "El Valle" and "San Pedro" rivers, in addition to numerous streams which descend from El Ávila. The "La Mariposa" and "Camatagua" reservoirs provide water to the city.
The city is occasionally subject to earthquakes - notably in 1641 and 1967.
Climate.
Under the Köppen climate classification, Caracas has a tropical savanna climate (Aw). Caracas is also intertropical, with precipitation that varies between (annual), in the city proper, and up to in some parts of the Mountain range. While Caracas is within the tropics, due to its altitude temperatures are generally not nearly as high as other tropical locations at sea level. The annual average temperature is approximately , with the average of the coldest month (January) and the average of the warmest month (July) , which gives a small annual thermal amplitude of . In the months of December and January abundant fog may appear, in addition to a sudden nightly drop in temperature, until reaching . This peculiar weather is known by the natives of Caracas as the "Pacheco". In addition, nightly temperatures at any time of the year are much (14 to 20 °C) lower than daytime highs and usually do not remain above , resulting in very pleasant evening temperatures. Hail storms appear in Caracas, although only on rare occasions. Electrical storms are much more frequent, especially between June and October, due to the city being in a closed valley and the orographic action of Cerro El Ávila. Caracas record extremes have been reported in other city's stations to reach a minimum of and a maximum of 
Demographics.
According to the population census of 2011 the Caracas proper (Distrito Capital) is over 3.0 million inhabitants, while that of the Metropolitan District of Caracas is estimated at 5.4 million . The vast majority of the population is composed from immigrants and their descendents primarily from Spain, Italy, Germany and Portugal. There is also a considerable Syrian and Lebanese population present in the country.
Crime.
Venezuela and its capital, Caracas, are reported to both have among the highest per capita murder rates in the world, with 116 homicides per 100,000 inhabitants. Most murders and other violent crimes go unsolved. The low class neighborhoods that cover the hills around Caracas are dangerous at all times.
However, recent research has suggested that the government misreported the actual population of Caracas, which may have skewed crime figures higher. Adjusted for population, the official 2010 homicide rate in Caracas falls to 71 per 100,000. Even though this is still a high number, it is 33% less than the figures usually reported by international media outlets; the discrepancy originates from the outdated population data that was held by the CICPC (a police agency) in Caracas.
Landmarks.
Federal Capitol.
The Federal Capitol occupies an entire city block, and, with its golden domes and neoclassical pediments, can seem even bigger. The building was commissioned by Antonio Guzmán Blanco in the 1870s, and is most famous for its Salón Elíptico, an oval hall with a mural-covered dome and walls lined with portraits of the country's great and good. The nearby Palacio Municipal de Caracas dating from 1696 was renovated in the Neoclassical style in 1906 and now serves as the city hall and the Caracas Museum.
East Park.
The Caracas East Park ("Parque del Este", now officially "Parque Generalísimo Francisco de Miranda") was designed by Brazilian architect Roberto Burle Marx. It is a green paradise in the middle of the city, and it contains a small zoo. A replica of the ship led by Francisco de Miranda, the "Leander", is in the southern part of the park. Before there used to exist a replica of the Santa Maria ship, used by Christopher Colombus in his voyages to America.
Teresa Carreño Cultural Complex.
The Teresa Carreño Cultural Complex ("Complejo Cultural Teresa Carreño"), or more commonly the Teresa Carreño Theatre ("Teatro Teresa Carreño"), is by far the most important theater of Caracas and Venezuela. The theater presents symphonic and popular concerts, operas, ballet, and dramatic works. It is the second largest theater in South America, after the Teatro Colón of Buenos Aires, Argentina.
Simón Bolívar's Birthplace Home.
Skyscrapers may loom overhead, but there is more than a hint of original colonial flavor in this neatly proportioned reconstruction of the house where Simón Bolívar was born on 24 July 1783. The museum's exhibits include period weapons, banners and uniforms.
Much of the original colonial interior has been replaced by monumental paintings of battle scenes, but more personal relics can be seen in the nearby Bolivarian museum. The pride of the place goes to the coffin in which Bolívar's remains were brought from Colombia; his ashes now rest in the National Pantheon.
National Pantheon.
Venezuela's most venerated building is five blocks north of Plaza Bolívar, on the northern edge of the old town. Formerly a church, the building was given its new purpose as the final resting place for eminent Venezuelans by Antonio Guzmán Blanco in 1874.
Parque Central Complex.
At a short distance east of Plaza Bolívar is Parque Central, a concrete complex of five high-rise residential slabs of somewhat apocalyptic-appearing architecture, crowned by two 56-storey octagonal towers, one of them is under repair due to the fire which burnt the building on 17 October 2004.
Parque Central is Caracas' art and culture hub, with museums, cinemas and the Teresa Carreño Cultural Complex. The West Tower balcony, on the 52nd floor, gives a 360° bird's-eye view of Caracas.
El Hatillo.
El Hatillo is a colonial town that is located at the south-east suburbs of Caracas in the municipal area of the same name. This small town, which is one of Venezuela's few well-preserved typical colonial areas, gives an idea of what Caracas was like in centuries past.
Cerro El Ávila.
"Cerro El Ávila" ("Mountain El Ávila") (Indigenous name: Waraira Repano), is a mountain in the mid-North of Venezuela. It rises next to Caracas and separates the city from the Caribbean Sea. It is considered the lungs of Caracas because there is a lot of vegetation on it.
Las Mercedes.
This zone contains restaurants with varied gastronomical specialties, along with pubs, bars, pools and art galleries.
Altamira neighborhood.
Altamira is a neighborhood in the Chacao municipality of Caracas. It has its own Metro Station, many hotels, malls and restaurants, and is an important business and cultural centre. The Francisco de Miranda avenue (a major avenue in Caracas) and the Distibuidor Altamira (a congested highway exit) are both in Altamira.
Religious buildings.
The Iglesia de San Francisco is of historical value. Bolívar's funeral was held here twelve years after his death. Here he was proclaimed "Libertador" in 1813 by the people of Caracas. The church has gilded baroque altarpieces, and retains much of its original colonial interior, despite being given a treatment in the 19th century under the auspices of Antonio Guzmán Blanco, which was intended to be modernizing. It contains some 17th-century masterpieces of art, carvings, sculptures and oil paintings. The Central University of Venezuela, established during the reign of Philip V, was lodged for centuries in the church cloisters next door, which today are the seat of the Language Academy, and the Academies of History, Physics, and Mathematics.
Caracas Cathedral is the seat of the Roman Catholic Archdiocese of Caracas.
The Mosque of Sheikh Ibrahim Al-Ibrahim is the second largest mosque in Latin America. For many years it was the biggest.
The Union Israelita de Caracas is the biggest Synagogue for the Jewish Ashkenazi community in Caracas. Its mission is to host the religious services and preserve the memory of the Jewish heritage in Venezuela. Similarly, Mariperez is the biggest Synagogue for the Jewish Sephardic community in Caracas.
Colleges, universities and international schools.
Central University of Venezuela.
The Central University of Venezuela ("Universidad Central de Venezuela" in Spanish) is a public University. Founded in 1721, it is the oldest university in Venezuela and one of the first in Latin America. The university campus was designed by architect Carlos Raúl Villanueva and it was declared World Heritage by UNESCO in 2000. The Ciudad Universitaria de Caracas, as the main Campus is also known, is considered a masterpiece of architecture and urban planning and it is the only university campus designed in the 20th century that has received such recognition by UNESCO.
Simón Bolívar University.
The Simón Bolívar University (Universidad Simón Bolívar, in Spanish, or USB) is a public institution in Caracas that focuses on science and technology. Its motto is ""La Universidad de la Excelencia"" ("University of Excellence").
Sports.
There are professional association football, baseball and several other sports.
Professional teams include Caracas Fútbol Club, Deportivo Petare, Atletico Venezuela, SD Centro Italo Venezolano, Estrella Roja FC and Real Esppor Club. The Deportivo Petare has reached the semifinals of international tournaments, such as the Copa Libertadores de America, while the Caracas Fútbol Club has reached the quarterfinals.
Baseball teams Tiburones de La Guaira and Leones del Caracas play in the "Estadio Universitario de la UCV", of the Central University of Venezuela, with a capacity of 26,000 spectators.
Another baseball team started in Caracas: the Navegantes del Magallanes. It was moved to Valencia, Carabobo in the 1970s.
Association Football stadiums include:
Caracas is the seat of the National Institute of Sports and of the Venezuelan Olympic Committee.
Caracas hosted the 1983 Pan American Games.
Culture.
Caracas is Venezuela's cultural capital, with many restaurants, theaters, museums, and shopping centers. The city is home to an array of immigrants from but not limited to: Spain, Italy, Portugal, the Middle East, Germany, China, and Latin American countries.
Gastronomy.
Caracas has a gastronomical heritage due to the influence of immigrants, leading to a choice of regional and international cuisine. There are a variety of international restaurants including American, French, Lebanese, Italian, Spanish, Indian, Chinese, Peruvian, Japanese, Mediterranean and Mexican. The district of "La Candelaria" contains Spanish restaurants, resulting from Galician and Canarian immigrants that came to the area in the mid-20th century.
International relations.
Twin towns and Sister Cities.
Caracas is twinned with:
Union of Ibero-American Capital Cities.
Caracas is part of the Union of Ibero-American Capital Cities from 12 October 1982 establishing brotherly relations with the following cities:

</doc>
<doc id="55290" url="https://en.wikipedia.org/wiki?curid=55290" title="Gedeon Burkhard">
Gedeon Burkhard

Gedeon Burkhard (born July 3, 1969) is a German film and television actor. Although he has appeared in numerous films and TV series in both Europe and the US, he is probably best recognised for his role as Alexander Brandtner in the Austrian/German television series "Kommissar Rex" (1997–2001), which has been aired on television in numerous countries around the world, or as Corporal Wilhelm Wicki in the 2009 film "Inglourious Basterds". He is also well recognised for his role as Chris Ritter in the long-running series Alarm für Cobra 11.
Life and career.
Gedeon Burkhard was born in Munich, Germany, the son of German actress Elisabeth von Molo (then Burkhard) and Wolfgang Burkhard, a great-grandson of Aleksandër Moisiu, a famous Italo-Austro-Albanian actor of the 20th century. Gedeon was educated at a boarding school in England and began his acting career in 1979 in the German TV film "Tante Maria". His father, Wolfgang Burkhard, is his manager.
During the 1990s, he lived in the U.S.A, working in several productions, but without much recognition. Burkhard got married in Las Vegas only to divorce 4 months later. After that, he lived in Vienna for "Kommisar Rex" for more than 5 years, before moving to Berlin for work reasons. Burkhard was working in Cologne on the TV series "Alarm für Cobra 11" as the detective Chris Ritter, until the end of his contract in November 2007. His character Chris Ritter had a heroic death. He then returned to Berlin, to be close to his daughter and to work in new projects. He said: "For the moment I will dedicate myself again fully to my artistic vagabond life in Berlin". Burkhard appeared in the Quentin Tarantino movie "Inglourious Basterds". Since February 2009, he has been shooting the film "Massel", made for German television. In 2011, he was in Rome, shooting an Italian mini TV series of 6 chapters "Caccia al Re - La Narcotici", with other Italian actor like Stefano Dionisi, Raffaella Rea and Laura Glavan, in which he plays a drug investigator, Daniele Piazza.
In 2011, he competed in the Italian version of "Dancing with the Stars", accompanied by professional Italian dancer Samanta Togni, in which he danced with his "daughter" Laura Glavan.
Gedeon Burkhard is in a relationship with a German woman, Anika Bormann.

</doc>
<doc id="55293" url="https://en.wikipedia.org/wiki?curid=55293" title="Niagara County, New York">
Niagara County, New York

Niagara County is a county located in the U.S. state of New York. As of the 2010 census, the population was 216,469. The county seat is Lockport. The county name is from the Iroquois word "Onguiaahra"; meaning "the strait" or "thunder of waters".\
Niagara County is part of the Buffalo-Niagara Falls metropolitan area, and its Canadian border is the province of Ontario.
It is the location of Niagara Falls and Fort Niagara, and has many parks and lake shore recreation communities. In the Summer of 2008 Niagara County celebrated its 200th Birthday with the first town of the county, Town of Cambria.
History.
When counties were established in the New York colony in 1683, the present Niagara County was part of Albany County. This was an enormous county, including the northern part of New York State as well as all of the present State of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to honor the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, replacing the name of the hated British governor.
In 1789, Ontario County was split off from Montgomery. In turn, Genesee County was created from Ontario County in 1802.
Niagara County was created from Genesee County in 1808. It was, however, larger than the present Niagara County even though it consisted of only the Town of Cambria.
From 1814 to 1817, records of Cattaraugus County were divided between Belmont (the seat of Allegany County) and Buffalo (then in Niagara County).
In 1821, Erie County was created from Niagara County.
The county has a number of properties on the National Register of Historic Places.
Geography.
According to the U.S. Census Bureau, the county has a total area of , of which is land and (54%) is water.
Niagara County is in the extreme western part of New York State, just north of Buffalo and adjacent to Lake Ontario on its northern border and the Niagara River and Canada on its western border.
The primary geographic feature of the county is Niagara Falls, the riverbed of which has eroded seven miles south over the past 12,000 years since the last Ice Age. The Niagara River and Niagara Falls, are in effect, the drainage ditch for four of the Great Lakes which constitute the largest supply of fresh water in the world. The water flows north from Lake Erie, then through the Niagara River, goes over Niagara Falls, and then on to Lake Ontario and the St. Lawrence River, eventually emptying into the North Atlantic Ocean. Today, tourists and visitors to the Falls see a diminished flow of water over the Falls, since a portion of the flow has been diverted for hydroelectric power purposes. Both the American and Canadian side of the Niagara River have massive electrical power plants.
The spectacular Niagara Gorge is the path Niagara Falls has taken over thousands of years as it continues to erode. Niagara Falls started at the Niagara Escarpment which cuts Niagara County in half in an East-West direction. North of the Escarpment lies the Lake Ontario plain, which is a fertile flatland that is used to grow grapes, apples, peaches and other fruits and vegetables. The grape variety Niagara, source of most American white grape juice but not esteemed for wine, was first grown in the county, in 1868. Viticulture, or wine culture has begun to take place, with several wineries below the escarpment. This has helped to improve the depressed economy of the region. To further capitalize on economic development, the state has created the Niagara Wine Trail.
Government and politics.
Structure.
Niagara County is governed by a 15-member Legislature, with the Chairman of the Legislature as the de facto head of county government. Currently, there are 11 members of the Republican-led Majority Caucus and four members of the Democrat-led Minority Caucus. The Legislature formerly consisted of 19 members, but was downsized to 15 seats effective January 1, 2012 based on the results of a public referendum.
A subordinate county manager reports to the County Legislature. Jeffrey M. Glatz is Niagara County Manager, with a four-year term commencing December 1, 2010.
Niagara County Legislature.
Chairman Wm. Keith McNall
Vice Chairman Clyde L. Burmaster
Majority Leader Randy R. Bradt
Minority Leader Dennis Virtuoso
Legislator Clyde L. Burmaster (1st District—Towns of Lewiston and Porter)
Legislator Rebecca J. Wydysh (2nd District—Towns of Wheatfield and Lewiston)
"Legislator Mark J. Grozio" (3rd District—City of Niagara Falls)
"Legislator Owen Steed" (4th District—City of Niagara Falls)
"Legislator Jason A. Zona" (5th District—City of Niagara Falls)
"Legislator Dennis F. Virtuoso" (6th District—City of Niagara Falls)
Legislator Kathryn L. Lance (7th District—Town of Wheatfield and City of North Tonawanda)
Legislator Richard L. Andres (8th District—City of North Tonawanda)
Legislator Randy R. Bradt (9th District—City of North Tonawanda)
Legislator David E. Godfrey (10th District—Towns of Cambria, Wilson and Wheatfield)
Legislator Anthony J. Nemi (11th District—City of Lockport, Towns of Lockport and Pendleton)
Legislator Will Collins (12th District—Town of Lockport and City of Lockport)
Legislator Wm. Keith McNall (13th District—City of Lockport)
Legislator John Syracuse (14th District—Towns of Newfane and Somerset)
Legislator Michael A. Hill (15th District—Towns of Royalton and Hartland)
Governing functions of the Legislature rely on a committee system; currently, there are five standing committees and one long-term ad hoc committee. The five standing committees are Administration, chaired by Nemi; Community Services, chaired by McNall; Community Safety and Security, chaired by Godfrey; Economic Development, chaired by Lance; and Public Works, chaired by Syracuse. An "ad hoc" Refuse Disposal District Committee is chaired by Hill.
The Administration Committee has oversight of the following government departments: County Manager, County Attorney, Management & Budget, Treasurer, Audit, Real Property, Data Processing, Legislature Office, Printing/Mailing, Human Resources, Civil Service, Risk Management, and Board of Elections.
The Community Services Committee has oversight of the following government departments: Social Services, Employment & Training, Youth Bureau, Office of Aging, Public Health, Mental Health, NCCC, County Clerk/DMV, Historian, and Veterans Services.
The Community Safety and Security Committee has oversight of the following government departments: Sheriff, District Attorney, Public Defender, Probation, Fire Coordinator/Emergency Services, and Coroners.
The Economic Development Committee has oversight of the Niagara County Center for Economic Development and the Niagara County Industrial Development Agency.
The Public Works Committee has oversight of the following government departments: Public Works, Parks/Golf Course, Refuse Disposal District, Sewer District, Water District, and Weights & Measures.
Additionally, the "ad hoc" Refuse Disposal District Committee has oversight of that District.
The dominant political party in the Niagara County Legislature is currently the Republican Party, which is ancestrally the dominant party in Niagara County.
Other entities.
In addition to the areas mentioned above, much of Niagara County is serviced by a Water District and a Sewer District. Both bodies are subordinate to the County Legislature; the former has a direct relationship, while the latter is currently under limited oversight of the town supervisors within the district.
Other elected officers.
County Clerk Joseph A. Jastrzemski (R)
Treasurer Kyle R. Andrews (D)
Sheriff James R. Voutour (D)
District Attorney Michael J. Violante (R)
Coroner, 1st District Cindy Lou Joyce (D)
Coroner, 2nd District Joseph V. Mantione (R)
Coroner, 3rd District Kenneth V. Lederhouse (R) "Lederhouse is also the senior coroner, having served longest of the four county coroners.
Coroner, 4th District Michael Ross (R)
State and federal government.
Niagara County is part of:
Demographics.
As of the census of 2010, there were 216,469 people, 87,846 households, and 58,593 families residing in the county. The population density was 420 people per square mile (162/km²). There were 95,715 housing units at an average density of 183 per square mile (71/km²). The racial makeup of the county was 90.70% White, 6.15% Black or African American, 0.94% Native American, 0.58% Asian, 0.02% Pacific Islander, 0.40% from other races, and 1.21% from two or more races. 1.33% of the population were Hispanic or Latino of any race. 23.6% were of German, 18.1% Italian, 11.3% Irish, 11.2% Polish and 8.3% English ancestry. 94.5% spoke English, 1.6% Spanish and 1.0% Italian as their first language.
There were 87,846 households out of which 30.90% had children under the age of 18 living with them, 50.30% were married couples living together, 12.30% had a female householder with no husband present, and 33.30% were non-families. 28.60% of all households were made up of individuals and 12.00% had someone living alone who was 65 years of age or older. The average household size was 2.45 and the average family size was 3.03.
In the county the population was spread out with 24.70% under the age of 18, 8.50% from 18 to 24, 28.40% from 25 to 44, 23.10% from 45 to 64, and 15.40% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 93.30 males. For every 100 females age 18 and over, there were 89.50 males.
The median income for a household in the county was $38,136, and the median income for a family was $47,817. Males had a median income of $37,468 versus $24,668 for females. The per capita income for the county was $19,219. About 8.20% of families and 10.60% of the population were below the poverty line, including 15.00% of those under age 18 and 7.30% of those age 65 or over.
Education.
Niagara University is located in Lewiston, New York. Niagara County Community College is located in Sanborn, New York. Many Niagara County residents also attend Erie and other Western New York County Schools.
In the Buffalo Metro area there more than 20 public and private colleges and universities in Buffalo and its environs offer programs in technical and vocational training, graduate, and professional studies.

</doc>
<doc id="55294" url="https://en.wikipedia.org/wiki?curid=55294" title="Oneida County, New York">
Oneida County, New York

Oneida County is a county located in the state of New York, in the United States. As of the 2010 census, the population was 234,878. The county seat is Utica. The name is in honor of the Oneida, an Iroquoian tribe that had this territory at the time of European encounter and has a reservation in the region.
Oneida County is part of the Utica-Rome, NY Metropolitan Statistical Area.
History.
When colonial counties were established by England in New York State in 1683, the territory of present Oneida County was included in a very large, mostly undeveloped Albany County. This was an enormous county, including the northern part of New York State as well as all of the present state of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766 by the creation of Cumberland County, and further on March 16, 1770 by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other sections, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately five miles west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County included what are now 37 individual counties of New York State. The county was named for William Tryon, colonial governor of New York.
During and after the Revolution, most of the Loyalists in Tryon County fled to Canada. In 1784, following the peace treaty that ended the American Revolutionary War, Americans changed the name of Tryon County to Montgomery County to honor the general, Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec. They replaced the name of the then hated British governor.
In 1789, the size of Montgomery County was reduced by the splitting off of Ontario County from Montgomery. The actual area split off from Montgomery County was much larger than the present county, also including the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne Counties.
In 1791, Herkimer County was one of three counties split off from Montgomery (the other two being Otsego, and Tioga County). This was much larger than the present county, however, and was reduced by a number of subsequent splits.
In 1794, Herkimer County was reduced in size by the splitting off of Onondaga County. This county was larger than the current Onondaga County, including the present Cayuga, Cortland, and part of Oswego counties.
In 1798, Oneida County was created from a part of Herkimer County. This county was larger than the current Oneida County, including the present Jefferson, Lewis, and part of Oswego counties.
In 1805, Jefferson and Lewis counties were split off from Oneida. In 1816, parts of Oneida and Onondaga counties were taken to form the new Oswego County.
In 1848, John Humphrey Noyes founded a religious and Utopian community, the Oneida Community, near Oneida. Its unconventional views on religion and relations between the sexes led to much controversy. The community lasted until 1881. The Oneida Silver Company was founded here to manufacture sterling silver, silverplate holloware and later stainless steel flatware.
Geography.
According to the U.S. Census Bureau, the county has a total area of , of which is land and (3.6%) is water.
Oneida County is in the central portion of New York State, east of Syracuse, and west of Albany. Oneida Lake is on the northwestern corner of the county, and the Adirondack Park is on the northeast. Part of the Tug Hill Plateau is in the northern part of the county. Interestingly, Oneida County's highest point does not lie on either the plateau nor in the Adirondack Park, but in the county's southern extremity. The peak's name is Tassel Hill. It is located slightly southeast of Hardscrabble Road (Tassel Hill Road), between the villages of Waterville and Cassville.
The Erie Canal bisects the county. Oneida Lake and Oneida Creek form part of the western boundary.
In the early 21st century, it is the only county in New York state with a known presence of 
Chronic wasting disease among wild White-tailed deer.
Demographics.
As of the census of 2000, there were 235,469 people, 90,496 households, and 59,184 families residing in the county. The population density was 194 people per square mile (75/km²). There were 102,803 housing units at an average density of 85 per square mile (33/km²). The racial makeup of the county was 90.21% White, 5.74% African American, 0.23% Native American, 1.16% Asian, 0.02% Pacific Islander, 1.11% from other races, and 1.52% from two or more races. Hispanic or Latino of any race were 3.20% of the population. 21.7% were of Italian, 13.1% Irish, 12.1% German, 9.9% Polish, 8.5% English and 5.6% American ancestry according to Census 2000. 90.6% spoke English, 2.7% Spanish, 1.3% Italian, 1.2% Serbo-Croatian and 1.1% Polish as their first language.
There were 90,496 households out of which 30.40% had children under the age of 18 living with them, 49.10% were married couples living together, 12.00% had a female householder with no husband present, and 34.60% were non-families. 29.50% of all households were made up of individuals and 13.10% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.02.
In the county the population was spread out with 23.90% under the age of 18, 8.60% from 18 to 24, 28.20% from 25 to 44, 22.90% from 45 to 64, and 16.50% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 98.60 males. For every 100 females age 18 and over, there were 96.30 males.
The median income for a household in the county was $35,909, and the median income for a family was $45,341. Males had a median income of $32,194 versus $24,295 for females. The per capita income for the county was $18,516. About 9.80% of families and 13.00% of the population were below the poverty line, including 18.90% of those under age 18 and 8.50% of those age 65 or over.
Government and politics.
Oneida County was governed by a board of supervisors until 1962, when the county charter was changed to create a county executive and a 29-seat county legislature. The county executive is elected by the entire county. All 29 members of the legislature are elected from single member districts. Currently, there are 16 Republicans and 13 Democrats. Effective January 1, 2014, the Oneida County Legislature will be reduced to 23 seats.
Oneida County also leans Republican in major statewide and national elections. In 2008, John McCain won the county by 6,000 votes out of 90,000 cast. He won all municipalities in the county except the city of Utica and the town of Kirkland.
Economy.
Once, the main product of Oneida County was silverware, chiefly manufactured at Oneida Ltd.'s headquarters in Sherrill. In January 2005, the company ceased manufacturing their product, closing its main plant and selling its assets.
Currently the largest non-governmental, non-healthcare product of Oneida County is gambling. Turning Stone Casino Resort is an enterprise of the Oneida Indian Nation of New York, and the largest private employer in Oneida County.

</doc>
<doc id="55295" url="https://en.wikipedia.org/wiki?curid=55295" title="Onondaga County, New York">
Onondaga County, New York

Onondaga County ( ) is a county located in the U.S. state of New York. As of the 2010 census, the population was 467,026. The county seat is Syracuse.
Onondaga County is part of the Syracuse, NY Metropolitan Statistical Area.
Joanie Mahoney (R) is the current County Executive
History.
The name "Onondaga" derives from the name of the Native American tribe who historically lived in this area at the time of European contact, one of the original Five Nations of the "Haudenosaunee". They called themselves (autonym) "Onoda'gega", sometimes spelled "Onontakeka." The word means "People of the Hills." Sometimes the term was "Onondagaono" ("The People of the Hills"). The federally recognized Onondaga Nation has a reservation within the county, on which they have self-government.
When counties were established in New York in 1683, the present Onondaga County was part of Albany County. This was an enormous county, including the northern part of New York State as well as all of the present State of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766, by the creation of Cumberland County, and further on March 16, 1770, by the creation of Gloucester County, both containing territory now in Vermont.
On March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. One of the other pieces, Tryon County, contained the western portion (and thus, since no western boundary was specified, theoretically still extended west to the Pacific). The eastern boundary of Tryon County was approximately west of the present city of Schenectady, and the county included the western part of the Adirondack Mountains and the area west of the West Branch of the Delaware River. The area then designated as Tryon County now includes 37 counties of New York State. The county was named for William Tryon, colonial governor of New York.
In the years prior to 1776, most of the Loyalists in Tryon County fled to Canada. The Onondaga were among four Iroquois tribes that allied with the British against the American colonists, as they hoped to end their encroachment. Instead, they were forced to cede most of their land in New York to the United States after the war. Many Onondaga went with Joseph Brant and other nations to Canada, where they received land grants in compensation and formed the Six Nations of the Grand River First Nation.
In 1784, following the peace treaty that ended the American Revolutionary War, the name of Tryon County was changed to Montgomery County. It honored General Richard Montgomery, who had captured several places in Canada and died attempting to capture the city of Quebec, and replaced the name of the hated British governor.
In 1789, Montgomery County was reduced by the splitting off of Ontario County from Montgomery. The actual area split off from Montgomery County was much larger than the present county, also including the present Allegany, Cattaraugus, Chautauqua, Erie, Genesee, Livingston, Monroe, Niagara, Orleans, Steuben, Wyoming, Yates, and part of Schuyler and Wayne Counties.
In 1791, Herkimer County was one of three counties split off from Montgomery (the other two being Otsego, and Tioga County). This was much larger than the present county, however, and was reduced by a number of subsequent splits.
In 1794, Onondaga County was split off from Herkimer County. This county was larger than the current Onondaga County, including the present Cayuga, Cortland, and part of Oswego Counties.
In 1799, Cayuga County was split off from Onondaga.
In 1808, Cortland County was split off from Onondaga.
In 1816, parts of Oneida and Onondaga Counties were taken to form the new Oswego County.
At the time Onondaga County was originally organized, it was divided into eleven towns: Homer, Pompey, Manlius, Lysander, Marcellus, Ulysses, Milton, Scipio, Ovid, Aurelius and Romulus.
Central New York developed rapidly after the New Military Tract provided land in lieu of payment to Revolutionary War veterans. Migration was largely from the east, mostly from New England states. The Genesee Road, which became the Seneca Turnpike in 1800, provided access. Generally settlers preferred higher land, since they associated lowlands with disease. In time, as hillside soil was eroded by early clearing and farming, valley lands were more fertile and highly prized for agriculture as well as for water power, which was the origin of many communities. An early settler of 1823 was James Hutchinson Woodworth, a native of Washington County, NY. He helped clear land for his family's farm in this region before going on to Chicago where he became Mayor. The completion of the Erie Canal across New York state in 1825 accelerated trade, development and migration.
The city of Syracuse, New York developed relatively late, due to its marshy situation. It was incorporated as a village in 1825 and as a city in 1847; by contrast, the Village of Manlius, along the Cherry Valley and Seneca Turnpikes, was incorporated in 1813. Population of rural towns was greatest in the late nineteenth century, when more people cultivated land and farms were relatively small, supporting large households.
Since that time, agriculture has declined in the county. Some Onondaga County towns like Spafford, New York were largely depopulated, many villages becoming veritable ghost towns. Onondaga County highlands now are more heavily reforested, with public parks and preserves providing recreation. Two Finger Lakes in the county, Skaneateles and Otisco, also attract visitors. The village of Skaneateles on scenic Route 20 has become a major tourist destination.
At the turn of the twenty-first century, population declined in the City of Syracuse while suburban communities generally grew, particularly with tract developments north of the city. Elsewhere, scattered commuter houses appeared, generally on fairly large parcels. The village of Skaneateles and shores of Skaneateles Lake attracted rapid development, demand for property increasing property values remarkably.
Geography.
According to the U.S. Census Bureau, the county has a total area of , of which is land and (3.4%) is water. The geographic dimensions of the county are illustrated as approximately 35 miles in length and 30 miles in width, and comprising 25 miles of the New York State Barge Canal System, in combination with a number of lakes, streams and rivers. Onondaga County is in the central portion of New York State, west of Utica, east of Rochester and north of Ithaca. Onondaga Lake is bordered by many of the larger communities in the county.
The northern part of the county is fairly level lake plain, extending northward to Lake Ontario. Oneida Lake three rivers, as well as the Erie and subsequent Barge Canals are in the lake plain. The main line of the New York Central Railroad and the New York State Thruway extend east and west across the county through the lake plain. The southern part of the county is Appalachian Plateau, with high hills rising at the southern edge of Syracuse. This is the eastern part of the Finger Lakes region. Skaneateles Lake and Otisco Lake are both in Onondaga County. US 20 extends east and west across the county, traversing dramatic hill-and-valley terrain. Between the lake plain and Appalachian highlands is a zone noted for drumlins, smaller, scattered hills formed as mounds of debris left by the last glacier. Tully is geologically noted for the terminal moraine deposited there by the glacier, filling the deep Tully Valley, which might have been another Finger Lake, had the moraine been left closer to Syracuse, impounding water. Tully is at the divide between two major watersheds, one flowing northward to the Atlantic Ocean by way of the St. Lawrence River and the other southward to the ocean vie the Susquehanna River. Oneida Lake, the Finger Lakes, and smaller bodies of water provide recreation. Several ski slopes are located in the Appalachian hills, where there are waterfalls and historic villages as attractions, as well as parks and large forest preserves.
Demographics.
As of the census of 2000, 458,336 people, 181,153 households, and 115,394 families resided in the county. The population density was 587 people per square mile (227/km²). There were 196,633 housing units at an average density of 252 per square mile (97/km²). The racial makeup of the county was 84.78% White, 9.38% African American, 0.86% Native American, 2.09% Asian, 0.03% Pacific Islander, 0.89% from other races, and 1.97% from two or more races. Hispanics or Latinos of any race were 2.44% of the population. About 17.5% were of Italian, 16.2% Irish, 12.4% German, 9.4% English, and 6.0% Polish ancestry according to Census 2000, and 91.4% spoke English, 2.4% Spanish and 1.1% Italian as their first language.
Of the 181,153 households, 31.90% had children under the age of 18 living with them, 46.90% were married couples living together, 12.90% had a female householder with no husband present, and 36.30% were not families. About 29.40% of all households were made up of individuals and 10.80% had someone living alone who was 65 years of age or older. The average household size was 2.46 and the average family size was 3.07.
In the county, the population was distributed as 25.80% under the age of 18, 9.50% from 18 to 24, 28.80% from 25 to 44, 22.10% from 45 to 64, and 13.80% who were 65 years of age or older. The median age was 36 years. For every 100 females, there were 91.70 males. For every 100 females age 18 and over, there were 87.70 males.
The median income for a household in the county was $40,847, and for a family was $51,876. Males had a median income of $39,048 versus $27,154 for females. The per capita income for the county was $21,336. About 8.60% of families and 12.20% of the population were below the poverty line, including 15.50% of those under age 18 and 7.10% of those age 65 or over.
Demographic trends (2006): The county population has decreased from a high in 1970. The increasing number of housing units apparently is due to smaller family units and more individuals living alone. While the City of Syracuse population has declined, some suburban towns have grown.
Government and politics.
Onondaga County was governed exclusively by a board of supervisors until 1961, when voters approved the creation of the county executive. In 1968, the board reorganized into a 24-seat county legislature. In 2001, the legislature was reduced to 19 seats. The county executive is elected in a countywide vote. In a 2010 referendum, voters approved a measure to reduce the legislature to 17 seats. All 17 members are elected from individual districts. Currently, there are 13 Republicans and 4 Democrats.
Historically, Onondaga County was a Republican stronghold. The GOP carried the county in all but one presidential election from 1952 to 1988. However, it has become friendlier to Democrats in recent years; it has gone Democratic in every presidential election since 1992. At the state and local level, however, it is more of a swing county. Generally, Democratic strength is concentrated in Syracuse itself, while Republicans do well in the suburbs.
Communities.
Syracuse, the county seat, is the only city in Onondaga County.
The following is a list of official towns, villages, and hamlets:

</doc>
<doc id="55297" url="https://en.wikipedia.org/wiki?curid=55297" title="Bradford (disambiguation)">
Bradford (disambiguation)

Bradford is a city and metropolitan borough in West Yorkshire, England.
Bradford may also refer to:

</doc>
<doc id="55300" url="https://en.wikipedia.org/wiki?curid=55300" title="Registered nurse">
Registered nurse

A registered nurse (RN) is a nurse who has graduated from a nursing program and met the requirements outlined by a country, state, province or similar licensing body in order to obtain a nursing license. An RN's scope of practice is determined by local legislation governing nurses, and usually regulated by a professional body or council.
Registered nurses are employed in a wide variety of professional settings, often specializing in their field of practice. They may be responsible for supervising care delivered by other healthcare workers including enrolled nurses, licensed practical nurses, unlicensed assistive personnel, nursing students, and less-experienced RNs.
Registered nurses must usually meet a minimum practice hours requirement and undertake continuing education in order to maintain their registration. Furthermore, there is often a requirement that an RN remain free from serious criminal convictions.
History.
The registration of nurses by nursing councils or boards began in the early twentieth century. New Zealand registered the first nurse in 1901 with the establishment of the Nurses Registration Act. Nurses were required to complete three years of training and pass a state-administered examination. Registration ensured a degree of consistency in the education of new nurses, and the title was usually protected by law. After 1905 in California, for example, it became a misdemeanour to claim to be an RN without a certificate of registration.
Registration acts allowed authorities a degree of control over who was admitted to the profession. Requirements varied by location, but often included a stipulation that the applicant must be "of good moral character" and must not have mental or physical conditions that rendered them unable to practice.
As nursing became more of an international profession, with RNs travelling to find work or improved working conditions and wages, some countries began implementing standardized language tests (notably the International English Language Testing System).
By nation.
Australia.
Nursing registration in Australia has been at a national level since 2010, since the inception of the Nursing and Midwifery Board of Australia (NMBA), which forms part of the Australian Health Practitioners Registration Authority (AHPRA). Prior to 2010, Nursing registration in Australia was administered individually by each state and territory.
The title 'Registered Nurse' (also known in the state of Victoria as a 'Division 1 Nurse') is granted to a nurse who has successfully completed a board approved course in the field of nursing, as outlined by education and registration standards defined by the NMBA. Registered Nurses are also required to meet certain other standards in order to fulfil registration standards as outlined by the NMBA, and these can include continuing professional development, recency of practice, criminal history checks and English language competency.
Educational requirements for an entry-level Registered Nurse are at the level of bachelor's degree in Australia, and can range in two to four years in length with three years being the national average. Some universities offer a two-year 'fast track' bachelor's degree, whereby a students study three years worth of coursework compressed in a two-year period. This is made possible by reducing summer and winter semester breaks and utilising three semesters per year compared to two. Some universities also offer combined degrees which allow the graduate to exit the program with a Masters in Nursing, e.g.: Bachelor of Science/Master of Nursing, and these are generally offered over a four-year period.
Postgraduate nursing education is widespread in Australia and is encouraged by employing bodies such as state health services (e.g. New South Wales Health). There are many varying courses and scholarships available which provide a bachelor-level Registered Nurse the opportunity to 'up-skill' and assume an extended scope of practice. Such courses are offered at all levels of the post graduate spectrum and range from graduate certificate to master's degree and provide a theoretical framework for a bachelor level Registered nurse to take up an advanced practice position such as Clinical Nurse Specialist (CNS), Clinical Nurse Consultant (CNC) and Nurse Practitioner (NP).
Canada.
In all Canadian provinces except Quebec, newly registered nurses are required to have a Bachelor of Science in Nursing. This is either achieved through a four-year university (or collaborative) program or through a bridging program for registered practical nurses or licensed practical nurses. Some universities also offer compressed programs for applicants already holding a bachelor's degree in another field.
Prior to 2015, initial licensure as an RN required passing the Canadian Registered Nurse Examination (CRNE) offered by the Canadian Nurses Association. As of 2015, for initial licensure, Canadian RNs must pass the NCLEX-RN exam offered by the National Council of State Boards of Nursing. In Quebec, the 'Ordre des infirmières et infirmiers du Québec' (Quebec Order of Nurses) administers their own licensing exam for registration within the province.
United States.
There are approximately 3.1 million active registered nurses in the United States. In the US, a registered nurse is a clinician who has completed at least an associate degree in nursing or a hospital-based diploma program. The RN has successfully completed the NCLEX-RN examination for initial licensure. Associate degrees in nursing frequently take three years to complete because of the increased volume of undergraduate coursework related to the profession of nursing. Bachelor of Science in Nursing degrees include more thorough coursework in leadership and community health.
Specialty certification is available through organizations such as the American Nurses Credentialing Center, a subsidiary of the American Nurses Association. After meeting the eligibility requirements and passing the appropriate specialty certification exam, the designation of Registered Nurse – Board Certified (RN-BC) credential is granted.
Economics.
As of 2011, there are 2.24 million registered nurses in China. In 2008 the United States had approximately three million nurses and Canada had just over 250,000. In the US and Canada this works out to approximately eight nurses per 1000 people. According to the Bureau of Labor Statistics, the job growth rate of registered nurses is 24%, well above the national average of 14%. The highest paid registered nurses in the United States are in California. California cities often comprise the top five highest paying metropolitan areas for registered nurses in the country.

</doc>
<doc id="55309" url="https://en.wikipedia.org/wiki?curid=55309" title="Blood type">
Blood type

A blood type (also called a blood group) is a classification of blood based on the presence or absence of inherited antigenic substances on the surface of red blood cells (RBCs). These antigens may be proteins, carbohydrates, glycoproteins, or glycolipids, depending on the blood group system. Some of these antigens are also present on the surface of other types of cells of various tissues. Several of these red blood cell surface antigens can stem from one allele (or an alternative version of a gene) and collectively form a blood group system.
Blood types are inherited and represent contributions from both parents. A total of 35 human blood group systems are now recognized by the International Society of Blood Transfusion (ISBT). The two most important ones are ABO and the RhD antigen; they determine someone's blood type (A, B, AB and O, with +, − or Null denoting RhD status).
Many pregnant women carry a fetus with a blood type which is different from their own, which is not a problem. What can matter is whether the baby is RhD positive or negative. Mothers who are RhD- and carry a RhD+ baby can form antibodies against fetal RBCs. Sometimes these maternal antibodies are IgG, a small immunoglobulin, which can cross the placenta and cause hemolysis of fetal RBCs, which in turn can lead to hemolytic disease of the newborn called erythroblastosis fetalis, an illness of low fetal blood counts that ranges from mild to severe. Sometimes this is lethal for the fetus; in these cases it is called hydrops fetalis.
Blood group systems.
A complete blood type would describe a full set of 30 substances on the surface of RBCs, and an individual's blood type is one of many possible combinations of blood-group antigens. Across the 35 blood groups, over 600 different blood-group antigens have been found, but many of these are very rare, some being found mainly in certain ethnic groups.
Almost always, an individual has the same blood group for life, but very rarely an individual's blood type changes through addition or suppression of an antigen in infection, malignancy, or autoimmune disease. Another more common cause in blood type change is a bone marrow transplant. Bone-marrow transplants are performed for many leukemias and lymphomas, among other diseases. If a person receives bone marrow from someone who is a different ABO type (e.g., a type A patient receives a type O bone marrow), the patient's blood type will eventually convert to the donor's type.
Some blood types are associated with inheritance of other diseases; for example, the Kell antigen is sometimes associated with McLeod syndrome. Certain blood types may affect susceptibility to infections, an example being the resistance to specific malaria species seen in individuals lacking the Duffy antigen. The Duffy antigen, presumably as a result of natural selection, is less common in ethnic groups from areas with a high incidence of malaria.
ABO blood group system.
The "ABO system" is the most important blood-group system in human-blood transfusion. The associated anti-A and anti-B antibodies are usually "immunoglobulin M", abbreviated IgM, antibodies. ABO IgM antibodies are produced in the first years of life by sensitization to environmental substances such as food, bacteria, and viruses. The original terminology used by Dr. Karl Landsteiner in 1901 for the classification is A/B/C; in later publications "C" became "O". "O" is often called "0" ("zero", or "null") in other languages. The Austrian Federal Ministry of Health claims the original terminology used by Dr. Karl Landsteiner in 1901 for the classification is 0(Zero)/A/B/AB and that in later publications "0" became "O" in most of English language countries.
Rh blood group system.
The Rh system (Rh meaning "Rhesus") is the second most significant blood-group system in human-blood transfusion with currently 50 antigens. The most significant Rh antigen is the D antigen, because it is the most likely to provoke an immune system response of the five main Rh antigens. It is common for D-negative individuals not to have any anti-D IgG or IgM antibodies, because anti-D antibodies are not usually produced by sensitization against environmental substances. However, D-negative individuals can produce IgG anti-D antibodies following a sensitizing event: possibly a fetomaternal transfusion of blood from a fetus in pregnancy or occasionally a blood transfusion with D positive RBCs. Rh disease can develop in these cases. Rh negative blood types are much less common in proportion of Asian populations (0.3%) than they are in White (15%).
The presence or absence of the Rh(D) antigen is signified by the + or − sign, so that, for example, the A− group is ABO type A and does not have the Rh (D) antigen.
ABO and Rh distribution by country.
As with many other genetic traits, the distribution of ABO and Rh blood groups varies significantly between populations.
Other blood group systems.
33 blood-group systems have been identified, including the ABO and Rh systems. Thus, in addition to the ABO antigens and Rh antigens, many other antigens are expressed on the RBC surface membrane. For example, an individual can be AB, D positive, and at the same time M and N positive (MNS system), K positive (Kell system), Lea or Leb negative (Lewis system), and so on, being positive or negative for each blood group system antigen. Many of the blood group systems were named after the patients in whom the corresponding antibodies were initially encountered.
Clinical significance.
Blood transfusion.
Transfusion medicine is a specialized branch of hematology that is concerned with the study of blood groups, along with the work of a blood bank to provide a transfusion service for blood and other blood products. Across the world, blood products must be prescribed by a medical doctor (licensed physician or surgeon) in a similar way as medicines.
Much of the routine work of a blood bank involves testing blood from both donors and recipients to ensure that every individual recipient is given blood that is compatible and is as safe as possible. If a unit of incompatible blood is transfused between a donor and recipient, a severe acute hemolytic reaction with hemolysis (RBC destruction), renal failure and shock is likely to occur, and death is a possibility. Antibodies can be highly active and can attack RBCs and bind components of the complement system to cause massive hemolysis of the transfused blood.
Patients should ideally receive their own blood or type-specific blood products to minimize the chance of a transfusion reaction. Risks can be further reduced by cross-matching blood, but this may be skipped when blood is required for an emergency. Cross-matching involves mixing a sample of the recipient's serum with a sample of the donor's red blood cells and checking if the mixture "agglutinates", or forms clumps. If agglutination is not obvious by direct vision, blood bank technicians usually check for agglutination with a microscope. If agglutination occurs, that particular donor's blood cannot be transfused to that particular recipient. In a blood bank it is vital that all blood specimens are correctly identified, so labelling has been standardized using a barcode system known as ISBT 128.
The blood group may be included on identification tags or on tattoos worn by military personnel, in case they should need an emergency blood transfusion. Frontline German Waffen-SS had blood group tattoos during World War II.
Rare blood types can cause supply problems for blood banks and hospitals. For example, Duffy-negative blood occurs much more frequently in people of African origin, and the rarity of this blood type in the rest of the population can result in a shortage of Duffy-negative blood for these patients. Similarly for RhD negative people, there is a risk associated with travelling to parts of the world where supplies of RhD negative blood are rare, particularly East Asia, where blood services may endeavor to encourage Westerners to donate blood.
Hemolytic disease of the newborn (HDN).
A pregnant woman can make IgG blood group antibodies if her fetus has a blood group antigen that she does not have. This can happen if some of the fetus' blood cells pass into the mother's blood circulation (e.g. a small fetomaternal hemorrhage at the time of childbirth or obstetric intervention), or sometimes after a therapeutic blood transfusion. This can cause Rh disease or other forms of hemolytic disease of the newborn (HDN) in the current pregnancy and/or subsequent pregnancies. If a pregnant woman is known to have anti-D antibodies, the Rh blood type of a fetus can be tested by analysis of fetal DNA in maternal plasma to assess the risk to the fetus of Rh disease. One of the major advances of twentieth century medicine was to prevent this disease by stopping the formation of Anti-D antibodies by D negative mothers with an injectable medication called Rho(D) immune globulin. Antibodies associated with some blood groups can cause severe HDN, others can only cause mild HDN and others are not known to cause HDN.
Blood products.
To provide maximum benefit from each blood donation and to extend shelf-life, blood banks fractionate some whole blood into several products. The most common of these products are packed RBCs, plasma, platelets, cryoprecipitate, and fresh frozen plasma (FFP). FFP is quick-frozen to retain the labile clotting factors V and VIII, which are usually administered to patients who have a potentially fatal clotting problem caused by a condition such as advanced liver disease, overdose of anticoagulant, or disseminated intravascular coagulation (DIC).
Units of packed red cells are made by removing as much of the plasma as possible from whole blood units.
Clotting factors synthesized by modern recombinant methods are now in routine clinical use for hemophilia, as the risks of infection transmission that occur with pooled blood products are avoided.
Red blood cell compatibility.
Table note
1. Assumes absence of atypical antibodies that would cause an incompatibility between donor and recipient blood, as is usual for blood selected by cross matching.
An Rh D-negative patient who does not have any anti-D antibodies (never being previously sensitized to D-positive RBCs) can receive a transfusion of D-positive blood once, but this would cause sensitization to the D antigen, and a female patient would become at risk for hemolytic disease of the newborn. If a D-negative patient has developed anti-D antibodies, a subsequent exposure to D-positive blood would lead to a potentially dangerous transfusion reaction. Rh D-positive blood should never be given to D-negative women of child bearing age or to patients with D antibodies, so blood banks must conserve Rh-negative blood for these patients. In extreme circumstances, such as for a major bleed when stocks of D-negative blood units are very low at the blood bank, D-positive blood might be given to D-negative females above child-bearing age or to Rh-negative males, providing that they did not have anti-D antibodies, to conserve D-negative blood stock in the blood bank. The converse is not true; Rh D-positive patients do not react to D negative blood.
This same matching is done for other antigens of the Rh system as C, c, E and e and for other blood group systems with a known risk for immunization such as the Kell system in particular for females of child-bearing age or patients with known need for many transfusions.
Plasma compatibility.
Blood plasma compatibility is the inverse of red blood cell compatibility. Type AB plasma carries neither anti-A nor anti-B antibodies and can be transfused to individuals of any blood group; but type AB patients can only receive type AB plasma. Type O carries both antibodies, so individuals of blood group O can receive plasma from any blood group, but type O plasma can be used only by type O recipients.
Table note
1. Assumes absence of strong atypical antibodies in donor plasma
Rh D antibodies are uncommon, so generally neither D negative nor D positive blood contain anti-D antibodies. If a potential donor is found to have anti-D antibodies or any strong atypical blood group antibody by antibody screening in the blood bank, they would not be accepted as a donor (or in some blood banks the blood would be drawn but the product would need to be appropriately labeled); therefore, donor blood plasma issued by a blood bank can be selected to be free of D antibodies and free of other atypical antibodies, and such donor plasma issued from a blood bank would be suitable for a recipient who may be D positive or D negative, as long as blood plasma and the recipient are ABO compatible.
Universal donors and universal recipients.
With regard to transfusions of packed red blood cells, individuals with type O Rh D negative blood are often called universal donors, and those with type AB Rh D positive blood are called universal recipients; however, these terms are only generally true with respect to possible reactions of the recipient's anti-A and anti-B antibodies to transfused red blood cells, and also possible sensitization to Rh D antigens. One exception is individuals with hh antigen system (also known as the Bombay phenotype) who can only receive blood safely from other hh donors, because they form antibodies against the H antigen present on all red blood cells.
Blood donors with exceptionally strong anti-A, anti-B or any atypical blood group antibody may be excluded from blood donation. In general, while the plasma fraction of a blood transfusion may carry donor antibodies not found in the recipient, a significant reaction is unlikely because of dilution.
Additionally, red blood cell surface antigens other than A, B and Rh D, might cause adverse reactions and sensitization, if they can bind to the corresponding antibodies to generate an immune response. Transfusions are further complicated because platelets and white blood cells (WBCs) have their own systems of surface antigens, and sensitization to platelet or WBC antigens can occur as a result of transfusion.
With regard to transfusions of plasma, this situation is reversed. Type O plasma, containing both anti-A and anti-B antibodies, can only be given to O recipients. The antibodies will attack the antigens on any other blood type. Conversely, AB plasma can be given to patients of any ABO blood group due to not containing any anti-A or anti-B antibodies.
Blood group genotyping.
In addition to the current practice of serologic testing of blood types, the progress in molecular diagnostics allows the increasing use of blood group genotyping. In contrast to serologic tests reporting a direct blood type phenotype, genotyping allows the prediction of a phenotype based on the knowledge of the molecular basis of the currently known antigens. This allows a more detailed determination of the blood type and therefore a better match for transfusion, which can be crucial in particular for patients with needs for many transfusions to prevent allo-immunization.
History.
Two blood group systems were discovered by Karl Landsteiner during early experiments with blood transfusion: the ABO group in 1901 and in co-operation with Alexander S. Wiener the Rhesus group in 1937. Development of the Coombs test in 1945,
the advent of transfusion medicine, and the understanding of ABO hemolytic disease of the newborn led to discovery of more blood groups, and now 33 human blood group systems are recognized by the International Society of Blood Transfusion (ISBT), and in the 33 blood groups, over 600 blood group antigens have been found; many of these are rare or are mainly found in certain ethnic groups.
Czech serologist Jan Janský is credited with the first classification of blood into the four types (A, B, AB, O) in 1907, which remains in use today. Blood types have been used in forensic science and were formerly used to demonstrate impossibility of paternity (e.g., a type AB man cannot be the father of a type O infant), but both of these uses are being replaced by genetic fingerprinting, which provides greater certainty.
According to the Austrian Federal Ministry of Health the original terminology used by Karl Landsteiner in 1901 for the classification is A,B and 0 ("zero"); the "O" ("oh") you find in the ABO group system is actually a subsequent variation occurred during the translation process, probably due to the similar shape between the number 0 and the letter O.
Society and culture.
A popular belief in Japan is that a person's ABO blood type is predictive of their personality, character, and compatibility with others. This belief is also widespread in South Korea and Taiwan. Deriving from ideas of historical scientific racism, the theory reached Japan in a 1927 psychologist's report, and the militarist government of the time commissioned a study aimed at breeding better soldiers. The fad faded in the 1930s due to its lack of scientific basis and ultimately the discovery of DNA in the following decades which it later became clear had a vastly more complex and important role in both heredity generally and personality specifically. No evidence has been found to support the theory by scientists, but it was revived in the 1970s by Masahiko Nomi, a broadcaster with a background in law who had no scientific or medical background. Despite these facts, the myth still persists widely in Japanese and South Korean popular culture.

</doc>
<doc id="55311" url="https://en.wikipedia.org/wiki?curid=55311" title="Seattle Wireless">
Seattle Wireless

Seattle Wireless is an American non-profit project created by Matt Westervelt and Ken Caruso in June 2000. It seeks to develop a free, locally owned wireless community network using widely available, license-free technology wireless broadband Internet access. It is a metropolitan area network.
Seattle Wireless is one of the first Community Wireless Networks and one of the first project focused wikis. It also had a short lived (7 episode) online television show, prior to the recent surge in videocasting and podcasting, called Seattle Wireless TV. It was created by Peter Yorke and Michael Pierce and ran July 2003 - June 2004. SWTV was an early adopter of Bittorrent to distribute its shows.

</doc>
<doc id="55312" url="https://en.wikipedia.org/wiki?curid=55312" title="United Federation of Planets">
United Federation of Planets

The United Federation of Planets, abbreviated as UFP and usually referred to as "the Federation", is a fictional interstellar federal republic composed of planetary sovereignties depicted in the "Star Trek" science fiction franchise. The planetary governments agree to exist semi-autonomously under a single central authority based on the Utopian principles of universal liberty, rights, and equality, and to share their knowledge and resources in peaceful cooperation and space exploration; each member world retains its own political and social structure, with the Federation itself serving as a 'United Nations'-type advisory body.
The Federation was first introduced in the 1966–1969 television show "Star Trek" as the organization that sent the starship USS Enterprise on its mission of peaceful exploration. As the Federation has continued to explore the galaxy and expanded its membership, it is increasingly challenged by hostile alien civilizations such as the Borg and the Dominion. The survival, success, and growth of the Federation and its principles of freedom have become some of the "Star Trek" franchise's central themes. 
The Federation was originally conceived as an idealized version of the United Nations. The Federation has been generally well received by critics and fans, becoming one of the most enduring storylines and symbols of the "Star Trek" franchise.
Conception.
The first mention of the United Federation of Planets was in the 1967 episode "A Taste of Armageddon", although other vague references such as just "the Federation" or to the "United Earth Space Probe Agency" were used in prior episodes. As part of the anti-war message he wanted the show to convey, "Star Trek" creator Gene Roddenberry intended to depict the Federation as if it was like an ideal, optimistic version of the United Nations. In several following episodes of the original series that were intended as allegories to the then-current Cold War tensions, the Federation took on the role resembling NATO while the Klingons represented the Soviet Union. Roberto Orci, writer of the 2009 "Star Trek" movie, explained that the utopianism of the series has many times been a thematic foil to ongoing world events, showing that peace is possible in times where there are fears of "perpetual war".
The Federation is described as an interstellar federal polity with, as of the year 2373, more than 150 member planets and thousands of colonies spread across some 8,000,000 cubic light years of the Milky Way galaxy. The social structure within the Federation is classless and operates within a money-less "New World Economy". The Federation is described as stressing, at least nominally, the values of universal liberty, equality, justice, peace, and cooperation. The Federation also maintains its own quasi-militaristic and scientific exploratory agency, known as Starfleet (also written as "Star Fleet" in some texts). Starfleet is seen handling many other governmental processes, sometimes with no other agency's influence, such as border defense, diplomatic envoy and has seen extensive use as an offensive military force.
Depiction.
The television series and films depict Earth and humanity as holding a center-stage political role within the Federation, in some ways first among equals. The legislature, the Federation Council, is located at the Presidio of San Francisco. Several other bodies of the Federation have been depicted. There is an executive branch headed by a Federation President, who keeps offices in the Palais de la Concorde in Paris. There is a judiciary branch as well, the highest court of which is the Federation Supreme Court. The Federation's scientific, diplomatic and defensive/military arm is Starfleet, depicted as being headquartered at Fort Baker, just north of San Francisco across the Golden Gate Bridge. The Federation comes into military conflict with other major powers in the galaxy such as the Klingon Empire, the Romulan Star Empire, the Cardassian Union, the Borg, and the Dominion.
The United Federation of Planets has existed as part of the "Star Trek" universe since the first season of the and is the primary focus of all the "Star Trek" series. Several episodes of "" follow events leading up the creation of the Federation, with the final episode featuring the signing of the Federation Charter.
In the series "", Earth Minister Nathan Samuels advocated the Coalition of Planets and invited other alien species, initially the Vulcans, Andorians and Tellarites, to become a part of this. The formation of the Coalition seems to have been the event that provoked the xenophobic Terra Prime incident in the episodes "" and "Terra Prime". After Terra Prime leader John Frederick Paxton exploited the xenophobia on Earth, many of the aliens were unnerved and nearly abandoned the idea of a coalition. However, they were convinced by a speech from Captain Jonathan Archer to give the idea of a united organization of worlds a chance. Six years later in 2161, the United Federation of Planets was organized.
The Federation is founded under a document known as the Charter of the United Federation of Planets October 9, 2161, which is occasionally referred to informally as the "Constitution". It draws text and inspiration from the United Nations Charter and other sources. An important guiding principle — indeed, it is listed as General Order One in the list of Starfleet general orders — is the Prime Directive, which forbids any interference in the natural development of any pre-warp civilization. This is intended to prevent even well-intentioned Federation personnel from introducing changes which could destabilize or even destroy other pre-warp-era cultures through interference. In practice, however, consistent application of the Prime Directive tends to be a controversial issue, and the Federation does not always abide strictly by it, such as when it attempted to strongarm the Organians into forming an alliance with it, or when it initially approved the forced relocation of the Ba'ku from their adopted homeworld—although it was eventually determined that the Ba'ku were not a pre-warp civilization. Starfleet's Omega Directive supersedes the Prime Directive allowing for any means possible to destroy the Omega particle if encountered. Other aspects of the Articles provide for rule of law, equality among individuals and protection of civil and creative liberties, which appears to be based on principles found in contemporary Western political theory. It includes a set of guarantees of civil rights, the "Seventh Guarantee" being analogous to the Fifth Amendment to the United States Constitution and its protection against self-incrimination.
The Federation also has its own semi-independent black ops agency, referred to only as "Section 31". It can be considered analogous to the Romulan Star Empire's "Tal Shiar" and the Cardassian Union's "Obsidian Order".
The Federation has exacting requirements for prospective member worlds that wish to join. Caste-based discrimination is forbidden, and major systematic violations of sentient rights, such as the unjust peacetime imprisonment of specially modified soldiers on the planet Angosia, are not tolerated for any petitioner. Furthermore, while most member worlds have single, unified world governments, it is not required for entry, as the Federation will consider "associate membership" of non-unified worlds.
Government.
The government of the United Federation of Planets consists of the central government, the Federation, and local planetary governments who share joint-sovereignty with the central government. The chief of state or chief of government of most planets are referred to as Governor, Prime Minister, or First Minister. The central government is composed of the Office of the President, the President's Cabinet, the Federation Council which is composed of an equal number of representatives from each member planet, and the Federation Supreme Court.
President of the United Federation of Planets.
The President of the United Federation of Planets (informally, the Federation President or the President of the Federation) is the elected head of state and head of government of the United Federation of Planets and is responsible for the day-to-day operation of the government, setting and coordinating foreign policy, and dealing with resource distribution issues. The Federation President is also the commander-in-chief of all Starfleet forces.
The President is supported by the Cabinet, a special committee composed of the heads of the executive departments of the Federation government as mentioned in "".
The Federation President's office is located on Earth in the city of Paris, France.
Federation Council.
The Federation Council is the legislature of the United Federation of Planets. Seats on the Council are filled by representatives from the various Federation Member Worlds. In addition to legislation, only the Federation Council may declare war and frequently passes resolutions which the Federation President and his or her staff must carry out and enforce.
The make-up and location of the Council is somewhat vague and open to interpretation based upon canonical evidence.
Federation Supreme Court.
The Federation Supreme Court is the highest court in the Federation headed by an elected Chief Justice. The Federation Supreme Court was first mentioned in "Doctor Bashir, I Presume?".
History of the Federation.
21st century.
After the end of World War III on Earth, scientist Zefram Cochrane built Earth's first warp-capable vessel, the "Phoenix". He launched it on April 5, 2063. The warp-testing of this vessel would garner the attention of a Vulcan science ship operating just outside the Solar System. Vulcans had not previously considered the Solar System of Earth, or Earth itself, worthy of their attention before this time. However, the science ship lands on Earth, and makes first-contact with Zefram Cochrane and the inhabitants of Bozeman, Montana. This contact would be the first time that Earth joins the interstellar community, and begins the road toward the foundation of the United Federation of Planets.
22nd-23rd centuries.
In the year 2119, an aging Zefram Cochrane opens the Warp 5 Complex on Earth, in the hope of building a vessel that would be the fastest human starship at the time. Eventually this project would yield the "Enterprise NX-01", Earth's first deep-space exploration vessel. In 2150, a World Government, "United Earth", was formed that included virtually all of the old nations on Earth.
Although no single individual is responsible for the foundation of the United Federation of Planets, the exploratory vessel "Enterprise NX-01" was a major catalyst. Under the command of Captain Jonathan Archer, it helped forge an alliance between the formerly belligerent Vulcan, Andorian, and Tellarite states, and forged a spirit of unity and cooperation in the Alpha Quadrant, culminating in a formal union in 2161. It was first preceded by the Coalition of Planets, which was mainly opposed by the xenophobic group, Terra Prime. The Federation was formed largely out of the aftermath of the Earth-Romulan War of the late 2150s ending in 2160, when the founding members saw the need for interstellar unity to prevent the horror of further war. Archer was one of the individuals who signed the Federation Charter, after giving a historic speech that was still being studied two centuries later. According to information seen on a viewscreen in a late episode of "", Jonathan Archer later became the Federation ambassador to Andoria, a Federation Councillor, and President of the United Federation of Planets from 2184 to 2192.
Around 2223, tensions thickened between the UFP and the Klingon Empire. In 2267, the Organian Peace Treaty was signed which ended major engagements, but the two interstellar powers remained in a state of cold war with occasional skirmishes over the next couple of decades. In 2293 the Klingons sued for peace after the destruction of the Klingon moon Praxis, leading eventually to the signing of the (the events depicted in ""). This effectively ended the war and ushered in seven decades of relative peace.
During the era of , Captain James Kirk once noted (in the episode "") that humanity was on "a thousand planets and spreading out"; however, this number apparently encompasses Earth's many off-Earth colonies and the various alien worlds on which humans can be found (just as non-humans have been depicted as residing on Earth) and should not be taken to mean that the Federation itself had a thousand members at that time. Considering that many of the Federation's other members have several interplanetary colonies just as Earth does, the full number of planets which the Federation encompasses may be impossible to determine, though "Star Trek: First Contact" establishes that there are 'over one hundred and fifty' planets in the Federation. It is presumed that colony worlds are directly subsidiary to the planetary governments of their homeworlds (much like individual states/provinces in a nation), but this has never been clearly established.
Early 24th century.
In 2311, the Tomed Incident occurred where thousands of Federation civilians and Starfleet personnel were killed by Romulan forces. The unrest was ended by the , which re-affirmed the Neutral Zone and prohibited Federation development of cloaking technology.
In 2344, the Romulan Star Empire launched an assault on the Klingon outpost at Narendra III, but unexpectedly the USS "Enterprise"-C, under the command of Captain Rachel Garrett, came to the Klingons' defense. This "Enterprise" was destroyed in the skirmish, a sacrifice which did great honor to the Klingons, and the burgeoning diplomacy between the two powers soon grew into a formal alliance. (In an alternate timeline, the "Enterprise"-C did not so assist, leading eventually to a full-scale war.)
Exploration and expansion in the 2340s and 2350s brought the Federation into conflict with several minor and major powers including the Talarians, the Sheliak and, eventually, the Cardassians.
Cardassian War.
Federation contact with a race called the Cardassians resulted in an extended conflict. One incident in this conflict was the massacre of Federation civilians on Setlik III in 2347. A truce was reached in the 2360s and a Demilitarized Zone was formed in 2370. A number of Federation and Cardassian colonies found themselves situated within the other’s territory; an agreement was reached for the transfer of those colonies. However, some Federation colonists were opposed to the agreement and formed the Maquis, a rebel movement who resisted the Cardassians.
Mid-24th century.
In 2365, the Federation had first formal contact with the Borg Collective, who threatened the existence of the Federation at the Battle of Wolf 359. Other events of this era include the Klingon Civil War, first contact with the Q, the beginning of relations with the Ferengi and various time travel incidents.
From 2363 to 2371, the USS "Enterprise"-D served as the Federation's flagship.
From 2371 to 2378, the USS "Voyager" (NCC-74656) was lost in the Delta Quadrant after being taken in the Badlands by the Caretaker's Array.
From 2373 to 2375, the Federation fought in the Dominion War. This was by far the largest conflict the Federation had ever been involved in, allying initially with the Klingons, and at a later time in the conflict, the Romulans against the combined forces of the Dominion, the Cardassians, and Breen. The Federation/Klingon/Romulan alliance was victorious, due in no small part to the Cardassians switching sides in the war after some of its officials realized that the Dominion had bloodlessly conquered them, but with substantial casualties on both sides.
In 2379, a Reman Praetor named Shinzon seized control of the Romulan Star Empire. The coup was defeated by the crew of the USS "Enterprise"-E with assistance from dissidents within the Romulan military, opening up the possibility of improved UFP/Romulan relations after over two centuries of general tension. However, this improved relationship came at a cost, as the death of Shinzon may have created a power vacuum.
Future history.
Prominent in some timelines is the Temporal Cold War, waged on a number of fronts throughout time including the 28th and 31st centuries.
By the 29th century, the Federation explores time as it once did space.
Alternate timeline (2009 and 2013 "Star Trek" films).
As depicted in the "Star Trek: Countdown" comic series, in 2387 the star of the Hobus system went supernova and posed a serious threat to the Romulan Star Empire. Ambassador Spock formulated a plan involving red matter to halt the Hobus supernova; saving billions of lives and preventing the political destabilization of the Alpha and Beta Quadrants. However, they did not act soon enough to save Romulus from being destroyed. An augmented Romulan mining ship called the "Narada" captained by Nero attacked the ship in which Spock traveled as Nero blames the Federation (Ambassador Spock in particular) for the destruction of his homeworld and for the death of his wife and child. During the attack, both ships are pulled into the singularity and transported into the past; the appearance of the "Narada" (which arrives farther into the past than Spock's ship) and its subsequent attack on the USS "Kelvin" creates an alternate timeline depicted in the 2009 "Star Trek" film. As depicted in the 2013 sequel "Star Trek Into Darkness", the Federation in this era is on the brink of war with the Klingon Empire just as its counterpart in the prime reality was.
Economics.
The Federation has been portrayed as an economic utopia. In the ' episode "Dark Frontier", Tom Paris describes it as the "New World Economy", which began in the late 22nd century and eventually made money obsolete, as does Jean-Luc Picard while explaining the timeline to Lily Sloane in '.
The first mention of the Federation "not" using money came in ', where Kirk (coming from 2286) says "these people still use money" upon arriving at 20th-century Earth, and says "We don't." when asked whether or not he and his crew use money in the 23rd century. In "", Picard tries to explain to cryogenically preserved people from the late 20th century that 24th century economics are quite different and money as they know it is not used or needed in the Federation. In ', he gives a similar speech to Lily.
In other episodes, especially earlier in the in-universe timespan, a monetary unit known as the "credit" is mentioned. At the Federation space station K-7 in the original series episode "The Trouble with Tribbles", set in 2267, Uhura is offered a Tribble for 10 credits. In ', in 2285, while on Earth, McCoy attempts to hire a ship to take him to the Genesis Planet, is warned it would be expensive, and haggles over payment; we do not know if McCoy could have afforded this or how much it would cost, since he was taken into custody for breaching the secrecy of the Genesis Project immediately afterwards. And in ', Carol Marcus mentions the Federation's decision whether or not to "fund" the Genesis Project itself, though "fund" means something different in this context as credits are not mentioned. In the "Deep Space Nine" episode "You Are Cordially Invited...", Jake Sisko tells Quark he sold his first book, but when Quark asks him how much he earned, Jake answers, "It's just a figure of speech." This explains moments when characters have made similar comments (in "", for instance, Scotty mentioned having "bought" a boat, and during the film "Star Trek Generations", Captain Kirk states that he "sold" his house).
By the time of ', money was considered abhorrent to many members of Starfleet, although in "Encounter at Farpoint", set in 2364, Beverly Crusher buys a bolt of fabric and requests that it be charged to her account on the "Enterprise". Two years later, in 2366, in "", the Federation is willing to pay millions of credits for access to a stable wormhole. Additionally, some officers were shown in "" to visit casinos, particularly near starbases, and poker is shown on a number of occasions to be a favorite pastime of "Enterprise"-D crewmembers, though real money is never said to be part of the game. In the ' episode "", Benjamin Sisko says that when he first entered Starfleet Academy, he rapidly spent an entire month's allotment of transporter credits (which may not be the same thing as 23rd-century credits) on transporting back and forth to his home in New Orleans. He also arranges for his wife's employer to give her a month's "paid" vacation (emphasis in episode) in "The Changing Face of Evil" (although his wife works for the Bajorans, a non-Federation race). And in the pilot episode of "", Tom Paris makes a reference to having someone "pay his bar bills".
Non-canon.
In many non-canon sources like "Star Trek Star Fleet Technical Manual" and "Worlds of the Federation", as well as the and role-playing games, the five founding worlds of the United Federation of Planets were Earth, Vulcan, Tellar, Andoria, and Alpha Centauri. Alpha Centauri being a founding world of the Federation and even having a humanlike native race called Centaurans became a popular fan theory, possibly based on uncertainty as to whether or not Zefram Cochrane (described in "" as "Zefram Cochrane of Alpha Centauri") was a native of Alpha Centauri or a resident of a human colony in that system; the latter has since been revealed to be the case, Cochrane having spent most of his life on Earth but eventually retiring to spend his final years on Alpha Centauri, before his disappearance and presumed death.
The once official, but now non-canon "Star Trek Spaceflight Chronology 1980 - 2188" guide states that the UFP was "incorporated at the first Babel Interplanetary Conference" in the year 2087.
Later, in "" the actual founding of the Federation can be seen in the episodes "" and "These Are the Voyages...", and early negotiations that lead to it in "" and "Terra Prime". Alpha Centauri is not mentioned as part of the founding, which is explicitly said to be between Humans, Vulcans, Andorians, and Tellarites. This leaves open the possibility of the Alpha Centauri colony becoming an independent polity some time between "Terra Prime" and "These Are the Voyages...", and then helping to form the Federation as a separate member. However, Alpha Centauri is only ever mentioned in passing as an Earth colony on screen. In the alternate timeline seen in the "DS9" episode "", where the Federation was never formed, Alpha Centauri is under Romulan control instead.
In the novels "A Time to Kill", "A Time to Heal", "A Time For War", "A Time For Peace", "Errand of Vengeance: Seeds of Rage", and "Articles of the Federation", the Federation Council was shown occupying the floors below the President's office in the Palais de la Concorde. This may be seen as contradicting elements of ' and '.
In some non-canonical works like "Star Trek Star Fleet Technical Manual" and the novel "Articles of the Federation", the Federation's founding document is called the "Articles Of Federation", which has been popular fan tradition. However, in the "Star Trek: Voyager" episode "", the text of the founding document is shown on screen (the preamble is a slightly reworded version of the UN Charter), and it is clearly called the "Charter of the United Federation of Planets", canonically establishing that as the name of the founding document. The term "charter" is also used in ' and in the ' episode "Accession", when discussing membership requirements for the Federation. That latter episode seemed to indicate that the timetable for a world's entry into the Federation is ten years after the request is made, although the Federation was willing to cut that time in half for Bajor in that episode, and has similarly made other exceptions for times of war, as seen in '. In the ' episode "The Drumhead", Captain Picard refers to the founding document in passing as "the Constitution", establishing that it is also known by that name. Novels such as "Articles of the Federation" presume that it is known by all three names.
The novels have also gone into more detail about inner workings of Federation government, such as how member worlds choose their Councillors (it is up to each world to decide how to do so) and how a President is elected (candidates submit their names anonymously, and are vetted by the Federation Council which determines if they are qualified to run; actual election is done by direct popular vote).
Reception.
The United Federation of Planets has been well received by most critics and fans. The optimistic view of the future present in the Federation has been highlighted as unique among most science fiction, showing how "civilized" the future could conceivably be. Much debate has centered around how realistic is the "post-scarcity" economy of the Federation that has evolved beyond Capitalism. It has been described, along with the series as a whole, as a vehicle to explore what it means to be human, as well as exploring mankind's efforts to build a better society. Some writers have also stated that "Star Trek"s vision of the future is outdated, with Federation society having prohibited genetic enhancement when it is not yet clear such technologies will not be a net positive. Other writers have noted that "Star Trek"s Federation has the same logistical and philosophical difficulties of other utopian economic and political schemes that make it seem unrealistic.

</doc>
<doc id="55313" url="https://en.wikipedia.org/wiki?curid=55313" title="Allergy">
Allergy

Allergies, also known as allergic diseases, are a number of conditions caused by hypersensitivity of the immune system to something in the environment that usually causes little problem in most people. These diseases include hay fever, food allergies, atopic dermatitis, allergic asthma, and anaphylaxis. Symptoms may include red eyes, an itchy rash, runny nose, shortness of breath, or swelling. Food intolerances and food poisoning are separate conditions.
Common allergens include pollen and food. --> Metals and other substances may also cause problems. Food, insect stings, and medications are common causes of severe reactions. --> Their development is due to both genetic and environmental factors. The underlying mechanism involves immunoglobulin E antibodies (IgE), part of the body's immune system, binding to an allergen and then to a receptor on mast cells or basophils where it triggers the release of inflammatory chemicals such as histamine. Diagnosis is typically based on a person's medical history. --> Further testing of the skin or blood may be useful in certain cases. Positive tests, however, may not mean there is a significant allergy to the substance in question.
Early exposure to potential allergens may be protective. Treatments for allergies include avoiding known allergens and the use of medications such as steroids and antihistamines. In severe reactions injectable adrenaline (epinephrine) is recommended. Allergen immunotherapy, which gradually exposes people to larger and larger amounts of allergen, is useful for some types of allergies such as hay fever and reactions to insect bites. --> Its use in food allergies is unclear.
Allergies are common. In the developed world, about 20% of people are affected by allergic rhinitis, about 6% of people have at least one food allergy, and about 20% have atopic dermatitis at some point in time. Depending on the country about 1 and 18% of people have asthma. Anaphylaxis occurs in between 0.05–2% of people. Rates of many allergic diseases appear to be increasing. The word "allergy" was first used by Clemens von Pirquet in 1906.
Signs and symptoms.
Many allergens such as dust or pollen are airborne particles. In these cases, symptoms arise in areas in contact with air, such as eyes, nose, and lungs. For instance, allergic rhinitis, also known as hay fever, causes irritation of the nose, sneezing, itching, and redness of the eyes. Inhaled allergens can also lead to increased production of mucus in the lungs, shortness of breath, coughing, and wheezing.
Aside from these ambient allergens, allergic reactions can result from foods, insect stings, and reactions to medications like aspirin and antibiotics such as penicillin. Symptoms of food allergy include abdominal pain, bloating, vomiting, diarrhea, itchy skin, and swelling of the skin during hives. Food allergies rarely cause respiratory (asthmatic) reactions, or rhinitis. Insect stings, antibiotics, and certain medicines produce a systemic allergic response that is also called anaphylaxis; multiple organ systems can be affected, including the digestive system, the respiratory system, and the circulatory system. Depending on the rate of severity, it can cause cutaneous reactions, bronchoconstriction, edema, hypotension, coma, and even death. This type of reaction can be triggered suddenly, or the onset can be delayed. The severity of this type of allergic response often requires injections of epinephrine, sometimes through a device known as the EpiPen or Twinject auto-injector. The nature of anaphylaxis is such that the reaction can seem to be subsiding, but may recur throughout a prolonged period of time.
Substances that come into contact with the skin, such as latex, are also common causes of allergic reactions, known as contact dermatitis or eczema. Skin allergies frequently cause rashes, or swelling and inflammation within the skin, in what is known as a "wheal and flare" reaction characteristic of hives and angioedema.
Cause.
Risk factors for allergy can be placed in two general categories, namely host and environmental factors. Host factors include heredity, sex, race, and age, with heredity being by far the most significant. However, there have been recent increases in the incidence of allergic disorders that cannot be explained by genetic factors alone. Four major environmental candidates are alterations in exposure to infectious diseases during early childhood, environmental pollution, allergen levels, and dietary changes.
Foods.
A wide variety of foods can cause allergic reactions, but 90% of allergic responses to foods are caused by cow's milk, soy, eggs, wheat, peanuts, tree nuts, fish, and shellfish. Other food allergies, affecting less than 1 person per 10,000 population, may be considered "rare". The use of hydrolysed milk baby formula versus standard milk baby formula does not appear to change the risk.
The most common food allergy in the US population is a sensitivity to crustacea. Although peanut allergies are notorious for their severity, peanut allergies are not the most common food allergy in adults or children. Severe or life-threatening reactions may be triggered by other allergens, and are more common when combined with asthma.
Rates of allergies differ between adults and children. Peanut allergies can sometimes be outgrown by children. Egg allergies affect one to two percent of children but are outgrown by about two-thirds of children by the age of 5. The sensitivity is usually to proteins in the white, rather than the yolk.
Milk-protein allergies are most common in children. Approximately 60% of milk-protein reactions are immunoglobulin E-mediated, with the remaining usually attributable to inflammation of the colon. Some people are unable to tolerate milk from goats or sheep as well as from cows, and many are also unable to tolerate dairy products such as cheese. Roughly 10% of children with a milk allergy will have a reaction to beef. Beef contains a small amount of protein that is present in cow's milk. Lactose intolerance, a common reaction to milk, is not a form of allergy at all, but rather due to the absence of an enzyme in the digestive tract.
Those with tree nut allergies may be allergic to one or to many tree nuts, including pecans, pistachios, pine nuts, and walnuts. Also seeds, including sesame seeds and poppy seeds, contain oils in which protein is present, which may elicit an allergic reaction.
Allergens can be transferred from one food to another through genetic engineering; however genetic modification can also remove allergens. Little research has been done on the natural variation of allergen concentrations in the unmodified crops.
Latex.
Latex can trigger an IgE-mediated cutaneous, respiratory, and systemic reaction. The prevalence of latex allergy in the general population is believed to be less than one percent. In a hospital study, 1 in 800 surgical patients (0.125 percent) reported latex sensitivity, although the sensitivity among healthcare workers is higher, between seven and ten percent. Researchers attribute this higher level to the exposure of healthcare workers to areas with significant airborne latex allergens, such as operating rooms, intensive-care units, and dental suites. These latex-rich environments may sensitize healthcare workers who regularly inhale allergenic proteins.
The most prevalent response to latex is an allergic contact dermatitis, a delayed hypersensitive reaction appearing as dry, crusted lesions. This reaction usually lasts 48–96 hours. Sweating or rubbing the area under the glove aggravates the lesions, possibly leading to ulcerations. Anaphylactic reactions occur most often in sensitive patients who have been exposed to a surgeon's latex gloves during abdominal surgery, but other mucosal exposures, such as dental procedures, can also produce systemic reactions.
Latex and banana sensitivity may cross-react. Furthermore, those with latex allergy may also have sensitivities to avocado, kiwifruit, and chestnut. These people often have perioral itching and local urticaria. Only occasionally have these food-induced allergies induced systemic responses. Researchers suspect that the cross-reactivity of latex with banana, avocado, kiwifruit, and chestnut occurs because latex proteins are structurally homologous with some other plant proteins.
Medications.
About 10% of people report that they are allergic to penicillin; however, 90% turn out not to be. Serious allergies only occur in about 0.03%.
Toxins interacting with proteins.
Another non-food protein reaction, urushiol-induced contact dermatitis, originates after contact with poison ivy, eastern poison oak, western poison oak, or poison sumac. Urushiol, which is not itself a protein, acts as a hapten and chemically reacts with, binds to, and changes the shape of integral membrane proteins on exposed skin cells. The immune system does not recognize the affected cells as normal parts of the body, causing a T-cell-mediated immune response. Of these poisonous plants, sumac is the most virulent. The resulting dermatological response to the reaction between urushiol and membrane proteins includes redness, swelling, papules, vesicles, blisters, and streaking.
Estimates vary on the percentage of the population that will have an immune system response. Approximately 25 percent of the population will have a strong allergic response to urushiol. In general, approximately 80 percent to 90 percent of adults will develop a rash if they are exposed to of purified urushiol, but some people are so sensitive that it takes only a molecular trace on the skin to initiate an allergic reaction.
Genetics.
Allergic diseases are strongly familial: identical twins are likely to have the same allergic diseases about 70% of the time; the same allergy occurs about 40% of the time in non-identical twins. Allergic parents are more likely to have allergic children, and those children's allergies are likely to be more severe than those in children of non-allergic parents. Some allergies, however, are not consistent along genealogies; parents who are allergic to peanuts may have children who are allergic to ragweed. It seems that the likelihood of developing allergies is inherited and related to an irregularity in the immune system, but the specific allergen is not.
The risk of allergic sensitization and the development of allergies varies with age, with young children most at risk. Several studies have shown that IgE levels are highest in childhood and fall rapidly between the ages of 10 and 30 years. The peak prevalence of hay fever is highest in children and young adults and the incidence of asthma is highest in children under 10.
Overall, boys have a higher risk of developing allergies than girls, although for some diseases, namely asthma in young adults, females are more likely to be affected. These differences between the sexes tend to decrease in adulthood.
Ethnicity may play a role in some allergies; however, racial factors have been difficult to separate from environmental influences and changes due to migration. It has been suggested that different genetic loci are responsible for asthma, to be specific, in people of European, Hispanic, Asian, and African origins.
Hygiene hypothesis.
Allergic diseases are caused by inappropriate immunological responses to harmless antigens driven by a TH2-mediated immune response. Many bacteria and viruses elicit a TH1-mediated immune response, which down-regulates TH2 responses. The first proposed mechanism of action of the hygiene hypothesis was that insufficient stimulation of the TH1 arm of the immune system leads to an overactive TH2 arm, which in turn leads to allergic disease. In other words, individuals living in too sterile an environment are not exposed to enough pathogens to keep the immune system busy. Since our bodies evolved to deal with a certain level of such pathogens, when they are not exposed to this level, the immune system will attack harmless antigens and thus normally benign microbial objects — like pollen — will trigger an immune response.
The hygiene hypothesis was developed to explain the observation that hay fever and eczema, both allergic diseases, were less common in children from larger families, which were, it is presumed, exposed to more infectious agents through their siblings, than in children from families with only one child. The hygiene hypothesis has been extensively investigated by immunologists and epidemiologists and has become an important theoretical framework for the study of allergic disorders. It is used to explain the increase in allergic diseases that have been seen since industrialization, and the higher incidence of allergic diseases in more developed countries. The hygiene hypothesis has now expanded to include exposure to symbiotic bacteria and parasites as important modulators of immune system development, along with infectious agents.
Epidemiological data support the hygiene hypothesis. Studies have shown that various immunological and autoimmune diseases are much less common in the developing world than the industrialized world and that immigrants to the industrialized world from the developing world increasingly develop immunological disorders in relation to the length of time since arrival in the industrialized world. Longitudinal studies in the third world demonstrate an increase in immunological disorders as a country grows more affluent and, it is presumed, cleaner. The use of antibiotics in the first year of life has been linked to asthma and other allergic diseases. The use of antibacterial cleaning products has also been associated with higher incidence of asthma, as has birth by Caesarean section rather than vaginal birth.
Stress.
Chronic stress can aggravate allergic conditions. This has been attributed to a T helper 2 (TH2)-predominant response driven by suppression of interleukin 12 by both the autonomic nervous system and the hypothalamic–pituitary–adrenal axis. Stress management in highly susceptible individuals may improve symptoms.
Other environmental factors.
International differences have been associated with the number of individuals within a population have allergy. Allergic diseases are more common in industrialized countries than in countries that are more traditional or agricultural, and there is a higher rate of allergic disease in urban populations versus rural populations, although these differences are becoming less defined.
Alterations in exposure to microorganisms is another plausible explanation, at present, for the increase in atopic allergy. Endotoxin exposure reduces release of inflammatory cytokines such as TNF-α, IFNγ, interleukin-10, and interleukin-12 from white blood cells (leukocytes) that circulate in the blood. Certain microbe-sensing proteins, known as Toll-like receptors, found on the surface of cells in the body are also thought to be involved in these processes.
Gutworms and similar parasites are present in untreated drinking water in developing countries, and were present in the water of developed countries until the routine chlorination and purification of drinking water supplies. Recent research has shown that some common parasites, such as intestinal worms (e.g., hookworms), secrete chemicals into the gut wall (and, hence, the bloodstream) that suppress the immune system and prevent the body from attacking the parasite. This gives rise to a new slant on the hygiene hypothesis theory — that co-evolution of humans and parasites has led to an immune system that functions correctly only in the presence of the parasites. Without them, the immune system becomes unbalanced and oversensitive. In particular, research suggests that allergies may coincide with the delayed establishment of gut flora in infants. However, the research to support this theory is conflicting, with some studies performed in China and Ethiopia showing an increase in allergy in people infected with intestinal worms. Clinical trials have been initiated to test the effectiveness of certain worms in treating some allergies. It may be that the term 'parasite' could turn out to be inappropriate, and in fact a hitherto unsuspected symbiosis is at work. For more information on this topic, see Helminthic therapy.
Pathophysiology.
Acute response.
In the early stages of allergy, a type I hypersensitivity reaction against an allergen encountered for the first time and presented by a professional antigen-presenting cell causes a response in a type of immune cell called a TH2 lymphocyte, which belongs to a subset of T cells that produce a cytokine called interleukin-4 (IL-4). These TH2 cells interact with other lymphocytes called B cells, whose role is production of antibodies. Coupled with signals provided by IL-4, this interaction stimulates the B cell to begin production of a large amount of a particular type of antibody known as IgE. Secreted IgE circulates in the blood and binds to an IgE-specific receptor (a kind of Fc receptor called FcεRI) on the surface of other kinds of immune cells called mast cells and basophils, which are both involved in the acute inflammatory response. The IgE-coated cells, at this stage, are sensitized to the allergen.
If later exposure to the same allergen occurs, the allergen can bind to the IgE molecules held on the surface of the mast cells or basophils. Cross-linking of the IgE and Fc receptors occurs when more than one IgE-receptor complex interacts with the same allergenic molecule, and activates the sensitized cell. Activated mast cells and basophils undergo a process called degranulation, during which they release histamine and other inflammatory chemical mediators (cytokines, interleukins, leukotrienes, and prostaglandins) from their granules into the surrounding tissue causing several systemic effects, such as vasodilation, mucous secretion, nerve stimulation, and smooth muscle contraction. This results in rhinorrhea, itchiness, dyspnea, and anaphylaxis. Depending on the individual, allergen, and mode of introduction, the symptoms can be system-wide (classical anaphylaxis), or localized to particular body systems; asthma is localized to the respiratory system and eczema is localized to the dermis.
Late-phase response.
After the chemical mediators of the acute response subside, late-phase responses can often occur. This is due to the migration of other leukocytes such as neutrophils, lymphocytes, eosinophils and macrophages to the initial site. The reaction is usually seen 2–24 hours after the original reaction. Cytokines from mast cells may play a role in the persistence of long-term effects. Late-phase responses seen in asthma are slightly different from those seen in other allergic responses, although they are still caused by release of mediators from eosinophils and are still dependent on activity of TH2 cells.
Allergic contact dermatitis.
Although allergic contact dermatitis is termed an "allergic" reaction (which usually refers to type I hypersensitivity), its pathophysiology actually involves a reaction that more correctly corresponds to a type IV hypersensitivity reaction. In type IV hypersensitivity, there is activation of certain types of T cells (CD8+) that destroy target cells on contact, as well as activated macrophages that produce hydrolytic enzymes.
Diagnosis.
Effective management of allergic diseases relies on the ability to make an accurate diagnosis. Allergy testing can help confirm or rule out allergies. Correct diagnosis, counseling, and avoidance advice based on valid allergy test results reduces the incidence of symptoms and need for medications, and improves quality of life. To assess the presence of allergen-specific IgE antibodies, two different methods can be used: a skin prick test, or an allergy blood test. Both methods are recommended, and they have similar diagnostic value.
Skin prick tests and blood tests are equally cost-effective, and health economic evidence shows that both tests were cost-effective compared with no test. Also, early and more accurate diagnoses save cost due to reduced consultations, referrals to secondary care, misdiagnosis, and emergency admissions.
Allergy undergoes dynamic changes over time. Regular allergy testing of relevant allergens provides information on if and how patient management can be changed, in order to improve health and quality of life. Annual testing is often the practice for determining whether allergy to milk, egg, soy, and wheat have been outgrown, and the testing interval is extended to 2–3 years for allergy to peanut, tree nuts, fish, and crustacean shellfish. Results of follow-up testing can guide decision-making regarding whether and when it is safe to introduce or re-introduce allergenic food into the diet.
Skin prick testing.
Skin testing is also known as "puncture testing" and "prick testing" due to the series of tiny punctures or pricks made into the patient's skin. Small amounts of suspected allergens and/or their extracts ("e.g.", pollen, grass, mite proteins, peanut extract) are introduced to sites on the skin marked with pen or dye (the ink/dye should be carefully selected, lest it cause an allergic response itself). A small plastic or metal device is used to puncture or prick the skin. Sometimes, the allergens are injected "intradermally" into the patient's skin, with a needle and syringe. Common areas for testing include the inside forearm and the back.
If the patient is allergic to the substance, then a visible inflammatory reaction will usually occur within 30 minutes. This response will range from slight reddening of the skin to a full-blown hive (called "wheal and flare") in more sensitive patients similar to a mosquito bite. Interpretation of the results of the skin prick test is normally done by allergists on a scale of severity, with +/- meaning borderline reactivity, and 4+ being a large reaction. Increasingly, allergists are measuring and recording the diameter of the wheal and flare reaction. Interpretation by well-trained allergists is often guided by relevant literature. Some patients may believe they have determined their own allergic sensitivity from observation, but a skin test has been shown to be much better than patient observation to detect allergy.
If a serious life-threatening anaphylactic reaction has brought a patient in for evaluation, some allergists will prefer an initial blood test prior to performing the skin prick test. Skin tests may not be an option if the patient has widespread skin disease, or has taken antihistamines in the last several days.
Patch testing.
Patch testing is a method used to determine if a specific substance causes allergic inflammation of the skin. It tests for delayed reactions. It is used to help ascertain the cause of skin contact allergy, or contact dermatitis. Adhesive patches, usually treated with a number of common allergic chemicals or skin sensitizers, are applied to the back. The skin is then examined for possible local reactions at least twice, usually at 48 hours after application of the patch, and again two or three days later.
Blood testing.
An allergy blood test is quick and simple, and can be ordered by a licensed health care provider ("e.g.", an allergy specialist), GP, or PED. Unlike skin-prick testing, a blood test can be performed irrespective of age, skin condition, medication, symptom, disease activity, and pregnancy. Adults and children of any age can take an allergy blood test. For babies and very young children, a single needle stick for allergy blood testing is often more gentle than several skin tests.
An allergy blood test is available through most laboratories. A sample of the patient's blood is sent to a laboratory for analysis, and the results are sent back a few days later. Multiple allergens can be detected with a single blood sample. Allergy blood tests are very safe, since the person is not exposed to any allergens during the testing procedure.
The test measures the concentration of specific IgE antibodies in the blood. Quantitative IgE test results increase the possibility of ranking how different substances may affect symptoms. A rule of thumb is that the higher the IgE antibody value, the greater the likelihood of symptoms. Allergens found at low levels that today do not result in symptoms can nevertheless help predict future symptom development. The quantitative allergy blood result can help determine what a patient is allergic to, help predict and follow the disease development, estimate the risk of a severe reaction, and explain cross-reactivity.
A low total IgE level is not adequate to rule out sensitization to commonly inhaled allergens. Statistical methods, such as ROC curves, predictive value calculations, and likelihood ratios have been used to examine the relationship of various testing methods to each other. These methods have shown that patients with a high total IgE have a high probability of allergic sensitization, but further investigation with allergy tests for specific IgE antibodies for a carefully chosen of allergens is often warranted.
Other.
Challenge testing: Challenge testing is when small amounts of a suspected allergen are introduced to the body orally, through inhalation, or via other routes. Except for testing food and medication allergies, challenges are rarely performed. When this type of testing is chosen, it must be closely supervised by an allergist.
Elimination/Challenge tests: This testing method is used most often with foods or medicines. A patient with a suspected allergen is instructed to modify his diet to totally avoid that allergen for a set time. If the patient experiences significant improvement, he may then be "challenged" by reintroducing the allergen, to see if symptoms are reproduced.
Unreliable tests: There are other types of allergy testing methods that are unreliable, including applied kinesiology (allergy testing through muscle relaxation), cytotoxicity testing, urine autoinjection, skin titration (Rinkel method), and provocative and neutralization (subcutaneous) testing or sublingual provocation.
Differential diagnosis.
Before a diagnosis of allergic disease can be confirmed, other possible causes of the presenting symptoms should be considered. Vasomotor rhinitis, for example, is one of many maladies that shares symptoms with allergic rhinitis, underscoring the need for professional differential diagnosis. Once a diagnosis of asthma, rhinitis, anaphylaxis, or other allergic disease has been made, there are several methods for discovering the causative agent of that allergy.
Prevention.
The consumption of various foods during pregnancy has been linked to eczema; these include celery, citrus fruit, raw pepper, margarine, and vegetable oil. A high intake of antioxidants, zinc, and selenium during pregnancy may help prevent allergies. This is linked to a reduced risk for childhood-onset asthma, wheezing, and eczema. Further research needs to be conducted. Probiotic supplements taken during pregnancy or infancy may help to prevent atopic dermatitis.
After birth, an early introduction of solid food and high diversity before week 17 could increase a child's risk for allergies. Studies suggest that introduction of solid food and avoidance of highly allergenic food such as peanuts during the first year does not help in allergy prevention.
Management.
Management of allergies typically involves avoiding what triggers the allergy and medications to improve the symptoms. Allergen immunotherapy may be useful for some types of allergies.
Medication.
Several medications maybe used to block the action of allergic mediators, or to prevent activation of cells and degranulation processes. These include antihistamines, glucocorticoids, epinephrine, mast cell stabilizers, and antileukotriene agents are common treatments of allergic diseases. Anti-cholinergics, decongestants, and other compounds thought to impair eosinophil chemotaxis, are also commonly used. Epinephrine is important in anaphylaxis.
Immunotherapy.
Allergen immunotherapy is useful for environmental allergies, allergies to insect bites, and asthma. Its benefit for food allergies is unclear and thus not recommended. Immunotherapy involves exposing people to larger and larger amounts of allergen in an affect to change the immune system's response.
Meta-analyses have found that injections of allergens under the skin is effective in the treatment in allergic rhinitis in children and in asthma. The benefits may last for years after treatment is stopped. It is generally safe and effective for allergic rhinitis and conjunctivitis, allergic forms of asthma, and stinging insects.
The evidence also supports the use of sublingual immunotherapy for rhinitis and asthma but it is less strong. For seasonal allergies the benefit is small. In this form the allergen is given under the tongue and people often prefer it to injections. Immunotherapy is not recommended as a stand-alone treatment for asthma.
Alternative medicine.
An experimental treatment, enzyme potentiated desensitization (EPD), has been tried for decades but is not generally accepted as effective. EPD uses dilutions of allergen and an enzyme, beta-glucuronidase, to which T-regulatory lymphocytes are supposed to respond by favoring desensitization, or down-regulation, rather than sensitization. EPD has also been tried for the treatment of autoimmune diseases but evidence does not show effectiveness.
A review found no effectiveness of homeopathic treatments and no difference compared with placebo. The authors concluded that, based on rigorous clinical trials of all types of homeopathy for childhood and adolescence ailments, there is no convincing evidence that supports the use of homeopathic treatments.
Epidemiology.
The allergic diseases — hay fever and asthma — have increased in the Western world over the past 2–3 decades. Increases in allergic asthma and other atopic disorders in industrialized nations, it is estimated, began in the 1960s and 1970s, with further increases occurring during the 1980s and 1990s, although some suggest that a steady rise in sensitization has been occurring since the 1920s. The number of new cases per year of atopy in developing countries has, in general, remained much lower.
Changing frequency.
Although genetic factors govern susceptibility to atopic disease, increases in atopy have occurred within too short a time frame to be explained by a genetic change in the population, thus pointing to environmental or lifestyle changes. Several hypotheses have been identified to explain this increased rate; increased exposure to perennial allergens due to housing changes and increasing time spent indoors, and changes in cleanliness or hygiene that have resulted in the decreased activation of a common immune control mechanism, coupled with dietary changes, obesity and decline in physical exercise. The hygiene hypothesis maintains that high living standards and hygienic conditions exposes children to fewer infections. It is thought that reduced bacterial and viral infections early in life direct the maturing immune system away from TH1 type responses, leading to unrestrained TH2 responses that allow for an increase in allergy.
Changes in rates and types of infection alone however, have been unable to explain the observed increase in allergic disease, and recent evidence has focused attention on the importance of the gastrointestinal microbial environment. Evidence has shown that exposure to food and fecal-oral pathogens, such as hepatitis A, "Toxoplasma gondii", and "Helicobacter pylori" (which also tend to be more prevalent in developing countries), can reduce the overall risk of atopy by more than 60%, and an increased rate of parasitic infections has been associated with a decreased prevalence of asthma. It is speculated that these infections exert their effect by critically altering TH1/TH2 regulation. Important elements of newer hygiene hypotheses also include exposure to endotoxins, exposure to pets and growing up on a farm.
History.
The concept of "allergy" was originally introduced in 1906 by the Viennese pediatrician Clemens von Pirquet, after he noted that some of his patients were hypersensitive to normally innocuous entities such as dust, pollen, or certain foods. Pirquet called this phenomenon "allergy" from the Ancient Greek words ἄλλος "allos" meaning "other" and ἔργον "ergon" meaning "work".
All forms of hypersensitivity used to be classified as allergies, and all were thought to be caused by an improper activation of the immune system. Later, it became clear that several different disease mechanisms were implicated, with the common link to a disordered activation of the immune system. In 1963, a new classification scheme was designed by Philip Gell and Robin Coombs that described four types of hypersensitivity reactions, known as Type I to Type IV hypersensitivity. With this new classification, the word "allergy" was restricted to type I hypersensitivities (also called immediate hypersensitivity), which are characterized as rapidly developing reactions.
A major breakthrough in understanding the mechanisms of allergy was the discovery of the antibody class labeled immunoglobulin E (IgE). IgE was simultaneously discovered in 1966-7 by two independent groups: Ishizaka's team at the Children's Asthma Research Institute and Hospital in Denver, Colorado, and by Gunnar Johansson and Hans Bennich in Uppsala, Sweden. Their joint paper was published in April 1969.
Diagnosis.
Radiometric assays include the radioallergosorbent test (RAST test) method, which uses IgE-binding (anti-IgE) antibodies labeled with radioactive isotopes for quantifying the levels of IgE antibody in the blood. Other newer methods use colorimetric or fluorescence-labeled technology in the place of radioactive isotopes.
The RAST methodology was invented and marketed in 1974 by Pharmacia Diagnostics AB, Uppsala, Sweden, and the acronym RAST is actually a brand name. In 1989, Pharmacia Diagnostics AB replaced it with a superior test named the ImmunoCAP Specific IgE blood test, which uses the newer fluorescence-labeled technology.
American College of Allergy Asthma and Immunology (ACAAI) and the American Academy of Allergy Asthma and Immunology (AAAAI) issued the Joint Task Force Report "Pearls and pitfalls of allergy diagnostic testing" in 2008, and is firm in its statement that the term RAST is now obsolete:
The term RAST became a colloquialism for all varieties of (in vitro allergy) tests. This is unfortunate because it is well recognized that there are well-performing tests and some that do not perform so well, yet they are all called RASTs, making it difficult to distinguish which is which. For these reasons, it is now recommended that use of RAST as a generic descriptor of these tests be abandoned.
The new version, the ImmunoCAP Specific IgE blood test, is the only specific IgE assay to receive FDA approval to quantitatively report to its detection limit of 0.1kU/l.
Medical specialty.
An allergist is a physician specially trained to manage and treat allergies, asthma and the other allergic diseases.
In the United States physicians holding certification by the American Board of Allergy and Immunology (ABAI) have successfully completed an accredited educational program and evaluation process, including a proctored examination to demonstrate knowledge, skills, and experience in patient care in allergy and immunology. Becoming an allergist/immunologist requires completion of at least nine years of training. After completing medical school and graduating with a medical degree, a physician will undergo three years of training in internal medicine (to become an internist) or pediatrics (to become a pediatrician). Once physicians have finished training in one of these specialties, they must pass the exam of either the American Board of Pediatrics (ABP), the American Osteopathic Board of Pediatrics (AOBP), the American Board of Internal Medicine (ABIM), or the American Osteopathic Board of Internal Medicine (AOBIM). Internists or pediatricians wishing to focus on the sub-specialty of allergy-immunology then complete at least an additional two years of study, called a fellowship, in an allergy/immunology training program. Allergist/immunologists listed as ABAI-certified have successfully passed the certifying examination of the ABAI following their fellowship.
In the United Kingdom, allergy is a subspecialty of general medicine or pediatrics. After obtaining postgraduate exams (MRCP or MRCPCH), a doctor works for several years as a specialist registrar before qualifying for the General Medical Council specialist register. Allergy services may also be delivered by immunologists. A 2003 Royal College of Physicians report presented a case for improvement of what were felt to be inadequate allergy services in the UK. In 2006, the House of Lords convened a subcommittee. It concluded likewise in 2007 that allergy services were insufficient to deal with what the Lords referred to as an "allergy epidemic" and its social cost; it made several recommendations.
Research.
Low-allergen foods are being developed, as are improvements in skin prick test predictions; evaluation of the atopy patch test; in wasp sting outcomes predictions and a rapidly disintegrating epinephrine tablet, and anti-IL-5 for eosinophilic diseases.
Aerobiology is the study of biological particles passively dispersed through the air. One aim is the prevention of allergies due to pollen.

</doc>
<doc id="55314" url="https://en.wikipedia.org/wiki?curid=55314" title="Deacon">
Deacon

Deacon is a ministry in the Christian Church that is generally associated with service of some kind, but which varies among theological and denominational traditions. In many traditions the "diaconate" (or deaconate), the term for a deacon's office, is a clerical office; in others it is for laity.
The word "deacon" is derived from the Greek word "diákonos" (διάκονος), which is a standard ancient Greek word meaning "servant", "waiting-man", "minister", or "messenger". One commonly promulgated speculation as to its etymology is that it literally means "through the dust", referring to the dust raised by the busy servant or messenger.
The title "deaconess" (διακόνισσα) is not found in the Bible. However, a woman, Phoebe, is mentioned at Romans 16:1–2 as a deacon ("diakonos") of the church in Cenchreae. Nothing more specific is said about her duties or authority. It is generally believed that the office of deacon originated in the selection of seven men by the apostles, among them Stephen, to assist with the charitable work of the early church as recorded in . Female deacons are mentioned by Pliny the Younger in a letter to Trajan dated "c". 112. The exact relationship between male and female deacons varies. In some traditions a female deacon is simply a member of the order of deacons; in others, deaconesses constitute a separate order; in others, the title "deaconess" was also given to the wife of a deacon.
A biblical description of the qualities required of a deacon, and of his household, can be found in .
Among the more prominent deacons in history are Stephen, the first Christian martyr (the "protomartyr"); Philip, whose baptism of the Ethiopian eunuch is recounted in ; Saint Lawrence, an early Roman martyr; Saint Vincent of Saragossa, protomartyr of Spain; Saint Francis of Assisi, founder of the mendicant Franciscans; Saint Ephrem the Syrian and Saint Romanos the Melodist, a prominent early hymnographer. Prominent historical figures who played major roles as deacons and went on to higher office include Saint Athanasius of Alexandria, Thomas Becket and Reginald Pole. On June 8, 536 a serving Roman deacon was raised to Pope, Silverius. His father, Pope Agapetus, had died and the office had been vacant for over a month.
The title is also used for the president, chairperson, or head of a trades guild in Scotland; and likewise to two officers of a Masonic Lodge.
Catholicism, Orthodoxy, Anglicanism.
The diaconate is one of the major orders in the Catholic, Anglican, Eastern Orthodox, and Oriental Orthodox churches. The other major orders are those of bishop and presbyter (priest).
While the diaconate as a vocation was maintained from earliest Apostolic times to the present in the Eastern churches (Orthodox and Catholic), it mostly disappeared in the Western church (with a few notable exceptions such as St Francis of Assisi) during the first millennium, with Western churches retaining deacons attached to diocesan cathedrals. The diaconate continued in a vestigial form as a temporary, final step along the course toward ordination to priesthood. In the 20th century, the diaconate was restored as a vocational order in many Western churches, most notably in the Latin Rite of the Catholic Church, the Anglican Communion, and the United Methodist Church.
In Catholic, Orthodox, and Anglican churches, deacons assist priests in their pastoral and administrative duties, but often report indirectly to the bishops of their diocese. They have a distinctive role in the liturgy of the Eastern and Western Churches. In the Eastern Church, deacons have a profound liturgical presence in the Divine Liturgy. In the Western Church, Pope St. Gregory the Great greatly reduced the liturgical role of the deacon in the Roman Rite, limiting them to serving the bishop, the proclamation of the Gospel, assisting the celebrant at the altar aside from the deacon's calling of charity. Today, deacons are also granted permission to preach.
Roman Catholicism.
Beginning around the fifth century, there was a gradual decline in the permanent diaconate in the Latin church. It has however remained a vital part of the Eastern Catholic Churches. From that time until the years just prior to the Second Vatican Council, the only men ordained as deacons were seminarians who were completing the last year or so of graduate theological training, who received the order several months before priestly ordination.
Following the recommendations of the council (in "Lumen gentium", 29), in 1967 Pope Paul VI issued the motu proprio "Sacrum Diaconatus Ordinem", restoring the ancient practice of ordaining to the diaconate men who were not candidates for priestly ordination. These men are known as "permanent deacons" in contrast to those continuing their formation, who were then called "transitional deacons". There is no sacramental difference between the two, however, as there is only one order of deacons.
The permanent diaconate formation period in the Roman Catholic Church varies from diocese to diocese as it is determined by the local ordinary. But it usually entails a year of prayerful preparation, a four- or five-year training period that resembles a collegiate course of study, and a year of post-ordination formation as well as the need for lifelong continuing education credits. Diaconal candidates receive instruction in philosophy, theology, study of the Holy Scriptures (the Bible), homiletics, sacramental studies, evangelization, ecclesiology, counseling, and pastoral care and ministry before ordination. Although they are assigned to work in a parish by the diocesan bishop, once assigned, deacons are under the supervision of the parish pastor. Unlike most clerics, permanent deacons who also have a secular profession have no right to receive a salary for their ministry, but many dioceses opt to remunerate them anyway.
The ministry of the deacon in the Roman Catholic Church is described as one of service in three areas: the Word, the Liturgy and Charity. The deacon's ministry of the Word includes proclaiming the Gospel during the Mass, preaching and teaching. The deacon's liturgical ministry includes various parts of the Mass proper to the deacon, including being an ordinary minister of Holy Communion and the proper minister of the chalice when Holy Communion is administered under both kinds. The ministry of charity involves service to the poor and marginalized and working with parishioners to help them become more involved in such ministry. As clerics, deacons are required to recite the Liturgy of the Hours. Deacons, like priests and bishops, are ordinary ministers of the sacrament of Baptism and can serve as the church's witness at the sacrament of Holy Matrimony, which the bride and groom administer to each other (though if the exchange of vows takes place in a wedding Mass, or Nuptial Mass, the Mass is celebrated by the priest and the deacon acts as another witness). Deacons may preside at funeral rites not involving a Mass (e.g., the final commendation at the gravesite or the reception of the body at a service in the funeral home), and may assist the priest at the Requiem Mass. They can preside over various services such as Benediction of the Blessed Sacrament, and they may give certain blessings. They cannot hear confession and give absolution, anoint the sick, or celebrate Mass.
At Mass, the deacon is the ordinary minister of the proclamation of the Gospel (in fact, a priest, bishop, or even the Pope should not proclaim the Gospel if a deacon is present) and of Holy Communion (primarily, of the Precious Blood). As ordained clerics, and if granted faculties by their bishops, deacons may preach the homily at a public Mass, unless the priest celebrant retains that ministry to himself at a given Mass.
The vestments most particularly associated with the Western Rite Catholic deacon are the alb, stole and dalmatic. Deacons, like priests and bishops, must wear their albs and stoles; deacons place the stole over their left shoulder and it hangs across to their right side, while priests and bishops wear it around their necks. The dalmatic, a vestment especially associated with the deacon, is worn during the celebration of the Mass and other liturgical functions; its use is more liberally applied than the corresponding vestment of the priest, the chasuble. At certain major celebrations, such as ordinations, the diocesan bishop wears a dalmatic under his chasuble.
Permanent deacons often serve in parish or other ministry as their time permits, since they typically have other full-time employment. They may also act as parish administrators (C. 217 of the Code of Canon Law). With the passage of time, more and more deacons are serving in full-time ministries in parishes, hospitals, prisons, and in diocesan positions. Deacons often work directly in ministry to the marginalized inside and outside the church: the poor, the sick, the hungry, the imprisoned.
The transitional diaconate is to be conferred on seminarians (continuing to the priesthood) no sooner than 23 years of age (C. 1031 of the Code of Canon Law). The permanent diaconate can be conferred on single men 25 or older, and on married men 35 or older, but an older age can be required by the episcopal conference. If a married deacon is widowed, he must maintain the celibate state. Under some very rare circumstances, however, deacons who have been widowed can receive permission to remarry. This is most commonly done when the deacon is left as a single father. In some cases, a widowed deacon will seek priestly ordination, especially if his children are grown. ("See also clerical celibacy.") The wife of a permanent deacon may be sometimes considered a partner in his ordained ministry. In many dioceses, the wife of the diaconal candidate undertakes the same education and training her husband does.
A permanent deacon is not styled "Father" as a priest would be, but as "Deacon", abbreviated variously as "Dn." or "Dcn." This preferred method of address is stated in the 2005 document of the United States Conference of Catholic Bishops, National Directory for the Formation, Ministry and Life of Permanent Deacons in the United States. The proper address in written correspondence for all Deacons of the Latin (Roman Rite) Catholic Church in the United States is "Deacon "Name"", although it is not uncommon to see "Rev. Mr." sometimes used. "Rev. Mr.", however, is more often used to indicate a transitional deacon (i.e., preparing for ordination to the priesthood) or one who belongs to a religious institute, while Rev. Deacon is used as the honorific for permanent deacons in many dioceses (e.g. Rev. Deacon John Smith, or Deacon John Smith). The decision as to whether deacons wear the Roman collar as street attire is left to the discretion of each bishop for his own diocese. Where clerical garb is approved by the bishop, the deacon can choose to wear or not wear the "collar". Where it is not permitted, the deacon must wear secular clothing. It is becoming more common to see deacons wearing a clerical suit especially in prisons and jails.
Deacons, like seminarians, religious, and the two other orders, Bishops and priests, recite the Liturgy of the Hours; however, deacons, if they are obliged to do so, are usually only required to participate in Morning and Evening Prayer.
In solemn Masses today and more so in older Rites of the Mass, one deacon will serve as the Deacon of the Word (proclaiming the Gospel and the Kyrie, and some other parts), and the Deacon of the Eucharist, who assists the priest during the Liturgy of the Eucharist.
Eastern Orthodoxy and Eastern Catholicism.
In addition to reading the Gospel and assisting in the administration of Holy Communion, the deacon censes the icons and people, calls the people to prayer, leads the litanies, and has a role in the dialogue of the Anaphora. In keeping with Eastern tradition, he is not permitted to perform any Sacred Mysteries (sacraments) on his own, except for Baptism "in extremis" (in danger of death), conditions under which anyone, including the laity, may baptize. When assisting at a normal baptism, it is often the deacon who goes down into the water with the one being baptized (). In contrast to the Roman Catholic Church, deacons in the Eastern Churches may not preside at the celebration of marriages, as in Eastern theology the sacrament is conferred by the nuptial blessing of a priest.
Diaconal vestments are the sticharion (dalmatic), the orarion (deacon's stole), and the epimanikia (cuffs). The last are worn under his sticharion, not over it as does a priest or bishop. The deacon usually wears a simple orarion which is only draped over the left shoulder but, if elevated to the rank of archdeacon, he wears the "doubled-orarion", meaning it is passed over the left shoulder, under the right arm, and then crossed over the left shoulder (see photograph, right). In modern Greek practice, a deacon wears this doubled orarion from the time of his ordination. Also, in the Greek practice, he wears the clerical kamilavka (cylindrical head covering) with a rim at the top. In Slavic practice, a hierodeacon (monastic deacon) wears the simple black kamilavka of a monk (without the rim), but he removes the monastic veil (see klobuk) when he is vested; a married deacon would not wear a kamilavka unless it is given to him by the bishop as an ecclesiastical award; the honorary kamilavka is purple in colour, and may be awarded to either married or monastic clergy.
As far as street clothing is concerned, immediately following his ordination the deacon receives a blessing to wear the "Exorasson" (Arabic: "Jib'be", Slavonic: "Riassa"), an outer cassock with wide sleeves, in addition to the "Anterion" (Slavonic: "Podraznik"), the inner cassock worn by all orders of clergy. In the Slavic practice, married clergy may wear any of a number of colours, but most often grey, while monastic clergy always wear black. In certain jurisdtictions in North America and Western Europe, a Roman collar is often worn, although this is not a traditional or widespread practice.
A "protodeacon" (Greek: πρωτοδιάκονος: "protodiakonos", "first deacon") is a distinction of honor awarded to senior deacons, usually serving on the staff of the diocesan bishop. An "archdeacon" is similar, but is among the monastic clergy. Protodeacons and archdeacons use a double-length orarion even if it is not the local tradition for all deacons to use it. In the Slavic tradition a deacon may be awarded the doubled-orarion even if he is not a protodeacon or archdeacon.
According to the practice of the Greek Orthodox Church of America, in keeping with the tradition of the Ecumenical Patriarchate, the most common way to address a deacon is "Father". Depending on local tradition, deacons are addressed as either "Father", "Father Deacon", "Deacon Father", or, if addressed by a Bishop, simply as "Deacon".
The tradition of kissing the hands of ordained clergy extends to the diaconate as well. This practice is rooted in the Holy Eucharist and is in acknowledgement and respect of the eucharistic role members of the clergy play in preparing, handling and disbursing the sacrament during the Divine Liturgy, and in building and serving the church as the Body of Christ.
Anciently, the Eastern churches blessed though never consecrated deaconesses. This practice fell into desuetude in the second millennium, but has been revived in some schismatic churches. Saint Nectarios of Pentapolis blessed a number of nuns as deaconesses in convents. Deaconesses would assist in anointing and baptising women, and in ministering to the spiritual needs of the women of the community, but would not serve within the holy altar. As churches discontinued blessing women as deaconesses, these duties largely fell to the nuns and to the priests' wives.
Anglicanism.
In Anglican churches, deacons often work directly in ministry to the marginalized inside and outside the church: the poor, the sick, the hungry, the imprisoned. Unlike Orthodox and Catholic deacons who may be married only before ordination, Anglican deacons are permitted to marry freely before or after ordination, as are Anglican priests. Most deacons are preparing for priesthood and are usually ordained as priests about a year after their diaconal ordination. However, there are some deacons who do not go on to receive priestly ordination. Many provinces of the Anglican Communion ordain both women and men as deacons. Many of those provinces that ordain women to the priesthood previously allowed them to be ordained only to the diaconate. The effect of this was the creation of a large and overwhelmingly female diaconate for a time, as most men proceeded to be ordained priests after a short time as a deacon.
Anglican deacons may baptize and in some dioceses are granted licences to solemnize matrimony, usually under the instruction of their parish priest and bishop. Deacons are not able to preside at the Eucharist (but can lead worship with the distribution of already-consecrated communion elements where this is permitted), nor can they pronounce God's absolution of sin or pronounce the Trinitiarian blessing. In most cases, deacons minister alongside other clergy.
An Anglican deacon wears an identical choir dress to an Anglican priest: cassock, surplice, tippet and academic hood. However, liturgically, deacons usually wear a stole over their left shoulder and fastened on the right side of their waist. This is worn both over the surplice and the alb. A deacon might also wear a dalmatic.
Deaconesses.
The title "women deacon" or "deaconess" appears in many documents from the early Church period, particularly in the East. Their duties were often different from that of male deacons; women deacons prepared adult women for baptism and they had a general apostolate to female Christians and catechumens (typically for the sake of modesty). Women appear to have been ordained as deacons to serve the larger community until about the 6th century in the West and in the East until modern times.
Liturgies for the ordination of women deacons are quite similar to those for male deacons and the ancient ordination rites have been noted by groups like Womenpriests. Although it is sometimes argued that women deacons of history were not sacramentally ordained in the full sense used in the present day in Canons 1008 and 1009 of the Code of Canon Law, some modern scholars argue that the ordination of women deacons would have been equally sacramental to that of male deacons.
Currently, the Catholic Church has not restored women to the diaconate, although Vatican statements have declined to state that this is not possible, as they have in the case of priestly ordination. The Russian Orthodox Church had a female diaconate into the 20th century. The Holy Synod of the Orthodox Church of Greece restored a monastic female diaconate in 2004.
Lutheran churches.
Lutheran Church–Missouri Synod (USA).
The Lutheran Church–Missouri Synod (LCMS) in the United States has special training and certification programs for deaconesses. LC-MS deaconesses are trained at Concordia University - Chicago or one of their two seminaries (St. Louis, MO or Fort Wayne, IN). Internet based classes are also available through the Mission Training Center (MTC).
Deaconesses assist pastors in human care ministry and other roles with the goals of caring for those in need and freeing pastors to focus on word and sacrament ministry. Acts chapter 6, verse 2 describes the function of deacons (servants) then and now, "So the Twelve gathered all the disciples together and said, 'It would not be right for us to neglect the ministry of the word of God in order to wait on tables.'"
Deaconesses are installed, not ordained, and remain lay people. The word "ordain" is to be reserved for the pastoral office.
A professional Deaconess (trained at the seminaries or Concordia University-Chicago) does not ordinarily preach and only ordained pastors may administer the sacraments; although she may perform baptism in cases of emergency.
The Atlantic District of the Lutheran Church–Missouri Synod has a Deacon Training program that prepares men and women for ministries of Word and Service in the local congregation. Students take the following courses of study at the Large Catechism level over the course of two years: Christian Doctrine Summary; Interpreting the Bible in Translation; Lutheran Worship I; OT Bible; Fundamental Pastoral Care; Basic Preaching; NT Bible; Teaching the Faith; Mission Outreach in Context; and Church History I (Christ to 1500 A.D.).
Atlantic District deacon students (male and female) who wish to seek commissioning as a Deacon in a local congregation must complete a pre-internship interview, 200 intern hours and their status as Deacon is under the authority of the local Pastoral Office. and a post-internship interview. Students are commissioned for the local congregation According to guidelines, Deacons shall be reviewed tri-annually by the Pastoral Office and the congregational President of the local congregation where the Deacon serves. Other Districts also train laymen and laywomen for service but the nomenclature varies by District.
Deacons, both the professional Deaconesses and the congregational and District trained Deacons (male and female) are considered to hold ministries of Word and Service (as opposed to Word and Sacrament).
Some with the nomenclature of "Deacon" are those training for ordination, although the terms "seminarian" or "vicar" are preferred. Special exceptions may be made for these Deacons who are vicars (training to become pastors) but must be given by the appropriate District president in writing. (A vicar in the LCMS is a third year seminarian who is doing an internship under a pastor. It should not be confused with the same term in Anglican and other church traditions.)
Evangelical Lutheran Church in America.
Deaconess Community (ELCA/ELCIC).
The Deaconess Community, a community of women serving in the Evangelical Lutheran Church in America (ELCA) and the Evangelical Lutheran Church in Canada (ELCIC) was formed in 1884. These women, who bear the title of 'Sister', proclaim the gospel through ministries of mercy and servant leadership on behalf of both churches for the sake of the world. Since the 1970s the sisters have been allowed to marry.
Diaconal Ministers/Associates in Ministry (ELCA/ELCIC).
The diaconate was recognized and rostered by the ELCA in 1993, creating a fourth 'roster' of recognized ministers (the other three being ordained, associates in ministry, and deaconess) in the churchwide body. The community is still young and as such is still being formed as to what styles and forms of ministry a diaconal minister pursues, as well as practices and traditions of the same.
As in the Anglican Communion, Lutheran diaconal ministers are allowed to wear a stole draped sideways from one shoulder and tied off at the waist, usually with some material left hanging below. Diaconal ministers (the term "deacon" is used occasionally but not officially) are involved in preaching, assisting in worship, leading worship in lieu of an ordained pastor and other congregational duties; they are, however, primarily called to service outside the church, in fields such as campus ministry, chaplaincy, congregational ministry, counseling, social service agency work, spiritual direction, parish and community nursing and a range of other avenues. A diaconal minister is "consecrated", rather than "ordained". This ceremony is usually presided over by a bishop.
Also of note are the 'associates in ministry (AIM), a rostered position within the ELCA consisting of laypersons commissioned into positions of service within the church, most often as educators, musicians, and worship leaders. While there is a trend towards combining the diaconal and associate ministries, the 'AIM' program continues in its own right, and associates are spread across the entirety of the churchwide body. AIMs are "commissioned" in the church and the hierarchy for service.
The ELCA is currently reviewing these rosters and working to identify how the institution can better answer the call to Word and Service ministries.
Calvinistic churches.
Church of Scotland.
There are two distinct offices of Deacon in the Church of Scotland. The best known form of diaconate are trained, paid pastoral workers, often working in parishes with considerable social and economic deprivation. The permanent diaconate was formerly exclusively female, and it was in the centenary year of the Diaconate (1988) that men were admitted to the office of Deacon. Women could not be ordained as Ministers until 1968. The offices of Deacon and Minister are now both open to both women and men; Deacons are now ordained (they were previously "commissioned").
The other office of Deacon can be found in congregations formerly belonging to the pre-1900 Free Church of Scotland, with a "Deacons' Court" having responsibility for financial and administrative oversight of congregations. Only a few congregations still retain this constitutional model, with most having since adopted the Church of Scotland's "Model Constitution" (with a Kirk Session and Congregational Board) or "Unitary Congregation" (with just a Kirk Session). Most of the Free Church congregations united with the United Presbyterian Church of Scotland in 1900 creating the United Free Church of Scotland, which itself united with the Church of Scotland in 1929.
The congregations of the Free Church of Scotland (post 1900) which did not join the UF Church in 1900 continue to have Deacons.
Presbyterian Church (USA/PCA).
Individual congregations of these church denominations also elect deacons, along with elders. However, in some churches the property-functions of the diaconate and session of elders is commended to an independent board of trustees. John Calvin's legacy of restoring a servant-ministry diaconate
Dutch protestant churches.
In many Dutch Protestant churches deacons are charged with ministries of mercy. As such, the deacons are also a member of the local church council. A special feature of the Dutch churches is the fact that the Diaconate of each local church is an own legal entity with own financial means, separated from the church itself, and governed by the deacons.
Methodist churches.
Methodist Church of Great Britain.
Methodist Church of Great Britain has a Permanent Diaconate, based on an understanding of the New Testament that Deacons have an equal, but distinct ministry from Presbyters. The original Wesleyan Deaconess Order was founded by Thomas Bowman Stephenson in 1890, following observation of new ministries in urban areas in the previous years. The order continued as the Wesley Deaconess Order following Methodist Union in 1932, but, following the admission of women to "The Ministry" (as presbyteral ministry is commonly termed in the Methodist Church), a number of Deaconesses transferred and recruitment for the WDO ceased from 1978. The 1986 Methodist Conference re-opened The Order to both men and women and the first Ordinations to the renewed order occurred during the 1990 Conference in Cardiff, which coincided with celebrations of 100 years of diaconal service in British Methodism; deaconesses had previously been ordained at their annual convocation.
The United Methodist Church.
In United Methodism, deacons began as a transitional order before ordination as elders. In 1996, The United Methodist Church ended the transitional deacon and established a new Order of Deacons to be equal in status with the Order of Elders. Both men and women may be ordained as deacons. Deacons serve in a variety of specialized ministries including, but not limited to, Christian education, music, communications and ministries of justice and advocacy. Unlike United Methodist elders, deacons must find their own place of service. Nevertheless, the bishop does officially approve and appoint deacons to their selected ministry. Deacons may assist the elder in the administration of Sacraments, but must receive special approval from a bishop before presiding over Baptism and Holy Communion.
Other traditions.
Deacons are also appointed or elected in other Protestant denominations, though this is less commonly seen as a step towards the clerical ministry. The role of deacon in these denominations varies greatly from denomination to denomination; often, there will be more emphasis on administrative duties than on pastoral or liturgical duties. In some denominations, deacons' duties are only financial management and practical aid and relief. Elders handle pastoral and other administrative duties.
Iglesia Ni Cristo.
Iglesia Ni Cristo's deacons serve as worship service's strict etiquette checkers in male's seatings,deaconesses are their female counterparts. They also serve as offering collectors and other Church duties during worship services. Deacons are required to be married people and have their faith strong and be a good example to others if you want to be a deacon. There is also a Head Deacon,who leads the congregation in prayer before the sermon and the prayer for voluntary offerings.
Amish.
The Amish have deacons, but they are elected by a council and receive no formal training.
Baptists.
Baptists have traditionally followed the principle of the autonomy of the local church congregation, giving each church the ability to discern for themselves the interpretation of scripture. Thus, the views among Baptist churches as to who becomes a deacon and when, as well as what they do and how they go about doing it, vary greatly. Baptists recognize two ordained positions in the church as Elders (Pastors) and Deacons, as per 1 Timothy, third chapter.
There are Baptist churches where the deacons decide many of the church affairs. There are churches where deacons serve in a family ministry only. There are Baptist churches (especially in the United Kingdom, but also in the U.S. and elsewhere) where women are allowed to be deacons; while many Baptist churches do not allow women to serve as deacons. Many Baptists also interpret Scripture as prohibiting divorced men from serving as deacons.
In the General Association of Regular Baptist Churches, deacons can be any adult male member of the congregation who is in good standing.
In some African American Missionary Baptist churches and in churches affiliated with the National Baptist Convention, USA, Inc. male and female deacons serve as one board. Other churches may have two separate boards of deacons and deaconesses. Most often the deacon or deacon candidate is a long-standing member of the church, being middle aged, but younger deacons may be selected from among members of a family that has had several generations in the same church. They are elected by quorum vote annually. Their roles are semi-pastoral in that they fill in for the pastor on occasion, or support the pastor vocally during his sermon. They may also lead a special prayer service, generally known as "The Deacon's Prayer." Their other roles are to accompany the pastor during Communion by handing out the remembrances of bread and wine (or grape juice) and to set a good example for others to follow. Their administrative duties sometimes include oversight of the treasury, Sunday school curriculum, transportation, and various outreach ministries.
See Baptist Distinctives for a more detailed treatment of Deacons in churches in other Associations, particularly the UK.
Uniting Church in Australia.
In the Uniting Church in Australia, the diaconate is one of two offices of ordained ministry. The other is Minister of the Word.
Deacons in the Uniting Church are called to minister to those on the fringes of the church and be involved in ministry in the community. Deacons offer leadership in a ministry of service to the world. The primary focus of the ministry of Deacon is on care and compassion for the poor and oppressed and in 
seeking social justice for all people. They take both an active role in leadership in such actions themselves, but are also play a key role in encouraging other Uniting Church members in similar action.
Some examples of service that Deacons may take include: prison chaplaincy, acting as youth or community workers, in community service agencies, in schools and hospitals, or in mission placements in Australia or overseas. Although the primary responsibility for worship in congregations lies with the Ministers of the Word, Deacons have a liturgical role appropriate to their distinctive ministry, including ministries where their main leadership is within a congregation.
In the Uniting Church both ministers of the word and deacons are styled "The Reverend".
The Uniting Church has recognised deacons since union, but it was not until the 6th Assembly in 1991 that the Uniting Church began ordaining deacons. This was partly because the historical, theological and sociological roles of deaconesses and deacons was being widely discussed in Churches throughout the world at the time that the Basis of Union was being drafted 
The Church of Jesus Christ of Latter-day Saints.
The office of Deacon is generally open to all 12- and 13-year-old male members of the church; all are encouraged to become Deacons. Duties include:
Church of Christ.
In accordance with Church of Christ doctrine and practice, only males may serve as deacons (deaconesses are not recognized), and must meet Biblical qualifications (generally I Timothy 3:8-13 is the Biblical text used to determine if a male is qualified to serve as deacon). A deacon may also be qualified to serve as an elder (and, in fact, may move into that role after a period of time if his service as deacon is considered acceptable).
The role of the deacon varies, depending on the local congregation. Generally a deacon will have responsibility for a specific non-spiritual function (e.g. finance, building and grounds, benevolence); however, the deacons (like the rest of the congregation) are under the subjection of the elders, who have spiritual and administrative authority over the deacon's function.
In congregations which lack qualified elders (where, in their absence, the men of the congregation handle leadership duties), a deacon would have ruling authority, but not due to his position as a deacon.
New Apostolic Church.
In the New Apostolic Church, the deacon ministry is a local ministry. A deacon mostly works in his home congregation to support the priests. If a priest is unavailable, a deacon will hold a divine service, without the act of communion (Only Priests and up can consecrate Holy Communion).
Jehovah's Witnesses.
Deacons among Jehovah's Witnesses are referred to as ministerial servants, claiming it preferable to translate the descriptive Greek term used in the Bible rather than merely transliterate it as though it were a title. Appointed ministerial servants aid elders in congregational duties. Like the elders, they are adult baptized males and serve voluntarily.
Cognates.
The Greek word "diakonos" (διάκονος) gave rise to the following terms from the history of Russia, not to be confused with each other: ""dyak"", ""podyachy"", ""dyachok"", in addition to "deacon" and "protodeacon".
Scots usage.
In Scots, the title "deacon" is used for a head-workman, a master or chairman of a trade guild, or one who is adept, expert and proficient. The term "deaconry" refers to the office of a "deacon" or the trade guild under a "deacon".
The most famous holder of this title was Deacon Brodie who was a cabinet-maker and president of the Incorporation of Wrights and Masons as well as being a Burgh councillor of Edinburgh, but at night led a double life as a burglar. He is thought to have inspired the story of "The Strange Case of Dr Jekyll and Mr Hyde".

</doc>
<doc id="55315" url="https://en.wikipedia.org/wiki?curid=55315" title="USS Voyager (Star Trek)">
USS Voyager (Star Trek)

The fictional "Intrepid"-class starship USS "Voyager" is the primary setting of the science fiction television series "". It is commanded by Captain Kathryn Janeway.
"Voyager" was designed by "Star Trek: Voyager" production designer Richard D. James and illustrator Rick Sternbach. Most of the ship's on-screen appearances result from computer-generated imagery, although Tony Meinenger built a model used in the series. The principal model of "Voyager" used for filming sold at Christie's auction in 2006 for USD $132,000.
Mission.
"Voyager" was launched in 2371. The crew's first orders were to track down a Maquis ship in the Badlands. An alien force called the transports both "Voyager" and the Maquis vessel across 70,000 light-years to the Delta Quadrant, damaging "Voyager" and killing several crewmembers (including first officer Lt. Cmdr. Cavit, the ship's chief medical officer and the rest of the medical staff, helm officer Stadi, and the chief engineer). To prevent a genocide of the Ocampans, Janeway orders the destruction of a device that could transport "Voyager" and the Maquis vessel home. Stranded, and with the Maquis ship also destroyed, both crews must integrate and work together for the anticipated 75-year journey home.
Starfleet Command eventually becomes aware of the ship's presence in the Delta Quadrant and is later able to establish regular communication. After a seven-year journey, the ship returns to the Alpha Quadrant via a Borg transwarp conduit with the aid of the time-traveling Admiral Kathryn Janeway (former Captain of "Voyager") from an alternate future.
The ship's motto, as engraved on its dedication plaque, is a quote from the poem "Locksley Hall" by Alfred, Lord Tennyson: "For I dipt in to the future, far as human eye could see; Saw the vision of the world, and all the wonder that would be."
Crew.
USS "Voyager" was launched with 141 crew on board. After the starship was flung to the Delta Quadrant, several crew members were killed, including several senior officers, namely Lt. Commander Cavit, the first officer; Chief Medical Officer Fitzgerald; Lieutenant Stadi, the Betazoid Helm Officer; and the chief engineer.
As of 2377, the crew complement was at 146, having gained some crew from the Maquis, the "Equinox", Samantha Wildman's child Naomi, and several liberated Borg drones, including Seven of Nine and Icheb.
The senior staff of "Voyager" include Captain Kathryn Janeway (Kate Mulgrew), who commands the ship; Commander (field commission) Chakotay (Robert Beltran), her first officer, who joined from the Maquis; Lt. Tuvok (Tim Russ) (later rising to the rank of Lieutenant Commander), the Security/Tactical officer; Ensign Harry Kim (Garrett Wang), the Operations Officer; Lt JG (junior grade) (field commission) B'Elanna Torres (Roxann Dawson), Chief Engineer; Lt. JG (falling to ensign as a result of demotion but later rising in rank to full lieutenant) Tom Paris (Robert Duncan McNeill), Helm Officer; as well as several noncommissioned personnel, the Emergency Medical Hologram (Robert Picardo) as the Chief Medical Officer, Neelix (Ethan Phillips) as the ship's cook and later Voyager's ambassador, Kes (Jennifer Lien) as the EMH's Medical Assistant, and Seven of Nine (Jeri Ryan), who plays several roles, generally in Astrometrics or Engineering.
Several recurring characters include Naomi Wildman, the "Captain's Assistant" and daughter of Ensign Samantha Wildman, and the Borg children, Icheb, Mezoti, Azan, and Rebi.
Design and capabilities.
The 15-deck (257 rooms), 700,000 metric-tonne "Voyager" was built at the Utopia Planitia Fleet Yards and launched from Earth Station McKinley.
"Voyager" is equipped with bio-neural gel packs, designed to increase processing speed and better organize processed information, that supplement the ship's isolinear optical chips. The ship had 47 spare gel packs and could not replicate additional packs. The ship also has two holodecks. "Voyager" was the first ship to be equipped with a class-9 warp drive, which was intended to be tested in deep space, allowing for a maximum sustainable speed of Warp 9.975. Variable geometry pylons allow Voyager and other Intrepid class ships to exceed warp 5 without damaging subspace. "Voyager" is capable of planetary landings. The ship isn't able to make the saucer separation in case of emergency, instead, it simply ejects the warp core.
"Voyager" includes an Emergency Medical Hologram programmed with a library of more than 5 million different medical treatments from 2,000 medical references and 47 physicians. The hologram itself is generated by a series of holographic emitters installed in sickbay. "Voyager"'s EMH is eventually able to leave sickbay due to a piece of 29th century technology commonly referred to as the "mobile emitter".
The ship is initially equipped with 38 photon torpedoes with type VI warheads and two tricobalt devices, both of which are used to destroy the Caretaker's array. Quantum torpedoes were also compatible with Voyager's launchers, with some modification. Voyager housed five standard torpedo launchers (two fore, two aft, one ventral) able to fire up to four torpedoes per launcher at once. In the final episode, an alternate future Kathryn Janeway equips the ship with transphasic torpedoes and ablative hull armor.
During the years in the Delta Quadrant, the ship is augmented with custom, non-spec upgrades and modifications, some of which are modified from technology of other cultures, an example being Seven of Nine's alcoves and the Delta Flyer which both utilize modified Borg technology. Several pieces of technology from the future were also installed in the final episode, courtesy of Admiral Janeway who went back in time to bring "Voyager" home. Some of the adaptive solutions are to compensate for the disadvantages of being 70,000 light years from port, such as the airponics bay and the transformation of the Captain's dining room to a galley, and the acquisition of enhancements from aliens in the Void that massively increases replicator efficiency.
The Borg are a major source of technological upgrades conducted on "Voyager". Cargo Bay 2 is equipped with several Borg alcoves when Captain Janeway forms an alliance with the Borg and several Borg are forced to work aboard "Voyager" during the alliance. Seven of Nine and Harry Kim build an astrometrics lab from scratch with Borg-enhanced sensors, knowledge of which Seven of Nine retained from the Borg. Additionally, the crew design and build the "Delta Flyer" support craft at the behest of Tom Paris.

</doc>
<doc id="55317" url="https://en.wikipedia.org/wiki?curid=55317" title="First day of issue">
First day of issue

A first day of issue cover or first day cover (FDC) is a postage stamp on a cover, postal card or stamped envelope franked on the first day the issue is authorized for use within the country or territory of the stamp-issuing authority. Sometimes the issue is made from a temporary or permanent foreign or overseas office. There will usually be a first day of issue postmark, frequently a pictorial cancellation, indicating the city and date where the item was first issued, and "first day of issue" is often used to refer to this postmark. Depending on the policy of the nation issuing the stamp, official first day postmarks may sometimes be applied to covers weeks or months after the date indicated.
Postal authorities may hold a first day ceremony to generate publicity for the new issue, with postal officials revealing the stamp, and with connected persons in attendance, such as descendants of the person being honored by the stamp. The ceremony may also be held in a location that has a special connection with the stamp's subject, such as the birthplace of a social movement, or at a stamp show.
Other types of first day covers.
Computer vended postage stamps issued by Neopost had first-day-of-issue ceremonies sponsored by the company, not by an official stamp-issuing entity. Personalised postage stamps of different designs are sometimes also given first-day-of-issue ceremonies and cancellations by the private designer. The stamps issued by private local posts can also have first days of issue, as can artistamps.
Event covers.
Event covers, instead of marking the issuance of a stamp, commemorate events. A design on the left side of the envelope (a "cachet") explains the event or anniversary being celebrated. Ideally the stamp or stamps affixed relate to the event. Cancels are obtained either from the location (e.g., Cape Canaveral, Anytown) or, in the case of the United States, from the Postal Service's Cancellation Services unit in Kansas City.
Earliest known use.
The earliest known use (EKU) of a stamp may or may not be the same as the first day of issue. This can occur if:
The search for EKUs of both old and new stamps is an active area of philately, and new discoveries are regularly announced.
Philatelic covers.
As the collecting of first day covers became more popular they began to appear on prepared envelopes, often with an illustration (commonly referred to by collectors as a cachet) that corresponded with the theme of the stamp. Several printing companies began producing such envelopes and often hired free lance illustrators to design their cachets such as Charles R. Chickering who in his earlier years designed postage stamps for the U.S. Post Office.
See also.
Earliest Reported Postmark on stamped envelopes.

</doc>
<doc id="55319" url="https://en.wikipedia.org/wiki?curid=55319" title="Ambient music">
Ambient music

Ambient music is a genre of music that puts an emphasis on tone and atmosphere over traditional musical structure or rhythm. Ambient music is said to evoke an "atmospheric", "visual," or "unobtrusive" quality. According to Brian Eno, one of its pioneers, "Ambient music must be able to accommodate many levels of listening attention without enforcing one in particular; it must be as ignorable as it is interesting."
As a genre, it originated in the United Kingdom at a time when new sound-making devices were being introduced to a wider market, such as the synthesizer. Ambient developed in the 1970s from the experimental and synthesizer-oriented styles of the period. Mike Oldfield, Jean Michel Jarre, and Vangelis, as well as the psychoacoustic soundscapes of Irv Teibel's "Environments" series, were all influences on the emergence of ambient. Brian Eno popularized ambient music in 1978 with his landmark album . The Orb and Aphex Twin gained commercial success with ambient tracks in the early 1990s. Ambient compositions are often quite lengthy, much longer than more popular, commercial forms of music.
Ambient had a revival towards the late 1980s with the prominence of house and techno music. Eventually, ambient grew a cult following in the 1990s. By the early 1990s, artists such as Aphex Twin were being called ambient house, ambient techno, IDM, or "ambient" by the media. Genre offshoots include dark ambient, ambient house, ambient industrial, ambient dub, psybient, and ambient trance.
History.
Developing in the 1970s, ambient stemmed from the experimental and synthesizer-oriented styles of the period. Although German bands such as Popol Vuh and Tangerine Dream predate him in the creation of Ambient music, Brian Eno played a key role in its development and popularization and is often erroneously cited as ambient's founder. The concept of background or furniture music had already existed some time before, but only in the 70s was ambient music first created, which incorporated New Age ideals with the newly invented modular synthesizer. The impact the rise of the synthesizer in modern music had on ambient as a genre cannot be overstated; as Ralf Hutter of early electronic pioneers Kraftwerk said in a 1977 Billboard interview: "Electronics is beyond nations and colors...with electronics everything is possible. The only limit is with the composer". Similarly, Eno said in a 2013 interview with The Guardian that "One of the important things about the synthesizer was that it came without any baggage. A piano comes with a whole history of music...when you play an instrument that does not have any such historical background you are designing sound basically. You're designing a new instrument. That's what a synthesizer is essentially. It's a constantly unfinished instrument. You finish it when you tweak it, and play around with it, and decide how to use it. You can combine a number of cultural references into one new thing."
As a genre, ambient music usually focuses on creating a mood or atmosphere through synthesizers and timbral qualities. It often lacks the presence of any net composition, beat, or structured melody. Due to its relatively open style, ambient music often takes influences from many other genres, ranging from house, dub, industrial and new age, amongst several others.
Ambient did not achieve large commercial success, being criticized as having a "boring" and "over-intellectual" sound. Nevertheless, it has also attained a certain degree of acclaim throughout the years. It had its first wave of popularity in the 1970s, yet saw a revival towards the late 1980s with the prominence of house and techno music, growing a cult following by the 1990s.
As an early 20th-century French composer, Erik Satie used such Dadaist-inspired explorations to create an early form of ambient / background music that he labeled "furniture music" ("Musique d'ameublement"). This he described as being the sort of music that could be played during a dinner to create a background atmosphere for that activity, rather than serving as the focus of attention. In his own words, he sought to create "a music...which will be part of the noises of the environment, will take them into consideration. I think of it as melodious, softening the noises of the knives and forks at dinner, not dominating them, not imposing itself. It would fill up those heavy silences that sometime fall between friends dining together. It would spare them the trouble of paying attention to their own banal remarks. And at the same time it would neutralize the street noises which so indiscreetly enter into the play of conversation. To make such music would be to respond to a need." Like Satie describes, Brian Eno notes a time when he was too sick to put on music and got a friend to come over and do it for him. Listening to harp music on an old stereo, he was unable to hear it amidst the loud sounds of the rain outside his window. Initially this frustrated him, but eventually he found it interesting the way the music blended with the natural rain sounds in a way that made it not distinctly apparent, and he set out to create compositions that worked in a comparable way. Eno went on to record 1975's Discreet Music with this in mind, suggesting that it be listened to at "comparatively low levels, even to the extent that it frequently falls below the threshold of audibility", referring to Satie's quote about his musique d'ameublement.
Eno's 1973 collaboration with Robert Fripp (No Pussyfooting) can be seen as early experiment in ambient music. Through the manipulation of reel-to-reel tape loops by Eno while Fripp played guitar over them, the duo was able to create multi-layered and dynamic pieces whose textures are similar to those of later ambient pieces, the genre not to be popularized by Eno until years later. Their method of composition, involving the treatment and experimentation of loops, would prove influential and has since been emulated by modern artists such as William Basinski, whose work The Disintegration Loops consists of a series of tape loops that slowly deteriorate over as the songs progress, introducing noise and cracks to the initially pristine quality of sound.
Brian Eno is generally credited with coining the term "Ambient Music" in the mid-1970s to refer to music that, as he stated, can be either "actively listened to with attention or as easily ignored, depending on the choice of the listener", and that exists on the "cusp between melody and texture". Eno, who describes himself as a "non-musician", termed his experiments in sound as "treatments" rather than as traditional performances. Eno used the word "ambient" to describe music that creates an atmosphere that puts the listener into a different state of mind; having chosen the word based on the Latin term "ambire", "to surround".
The album notes accompanying Eno's 1978 release "" include a manifesto describing the philosophy behind his ambient music: "Ambient Music must be able to accommodate many levels of listening attention without enforcing one in particular; it must be as ignorable as it is interesting."
Eno has acknowledged the influence of Erik Satie and John Cage. In particular, Eno was aware of Cage's use of chance such as throwing the "I Ching" to directly affect the creation of a musical composition. Eno then utilised a similar method of weaving randomness into his compositional structures. This approach was manifested in Eno's creation of Oblique Strategies, where he used a set of specially designed cards to create various sound dilemmas that in turn, were resolved by exploring various open ended paths, until a resolution to the musical composition revealed itself. Eno also acknowledged influences of the drone music of La Monte Young (of whom he said, "La Monte Young is the daddy of us all") and of the mood music of Miles Davis and Teo Macero, especially their 1974 epic piece, "He Loved Him Madly" (from "Get Up with It"), about which Eno wrote, "that piece seemed to have the 'spacious' quality that I was after...it became a touchstone to which I returned frequently."
Eno has also been outspoken about the role of the recording studio in ambient music, notably in the freedom it gives in crafting textures of sound. He notes "When I first started recording I didn't have the background of a musician, and in fact it was only because of the recording studio and because of the technology that existed there that I was ever able to become a musician of any kind...the recording studio allows you to become a painter with sound, that's really what you do in a studio, you make pictures with sound..."
Beyond the major influence of Brian Eno, other musicians and bands added to the growing nucleus of music that evolved around the development of "Ambient Music". While not an exhaustive list, one cannot ignore the parallel influences of Wendy Carlos, who produced the original music piece called "Timesteps" which was then used as the filmscore to "A Clockwork Orange", as well as her later work "Sonic Seasonings". Other significant artists such as Mike Oldfield, Jean Michel Jarre and Vangelis, also Russian electronic music pioneer Mikhail Chekalin, have all added to or directly influenced the evolution of ambient music. Adding to these individual artists, works by groups such as Pink Floyd, through their album The Endless River. The Yellow Magic Orchestra developed a distinct style of ambient electronic music that would later be developed into ambient house music.
While muzak can be seen akin to ambient music in that they are both meant to be heard in the background, Eno describes some notable differences between them: "Whereas the extant canned music companies proceed from the basis of regularizing environments by blanketing their acoustic and atmospheric idiosyncrasies, Ambient Music is intended to enhance these. Whereas conventional background music is produced by stripping away all sense of doubt and uncertainty (and thus all genuine interest) from the music, Ambient Music retains these qualities. And whereas their intention is to "brighten" the environment by adding stimulus to it...Ambient Music is intended to produce calm and a space to think."
1990s developments.
By the early 1990s, artists such as The Orb, Aphex Twin, Seefeel, the Irresistible Force, Geir Jenssen's Biosphere, and the Higher Intelligence Agency were being referred to by the popular music press as ambient house, ambient techno, IDM or simply "ambient" according to the liner notes of Brian Eno's "":
So-called 'Chillout' began as a term deriving from British ecstasy culture which was originally applied in relaxed downtempo 'chillout rooms' outside of the main dance floor where ambient, dub and downtempo beats were played to ease the tripping mind.
The London scene artists, such as Aphex Twin (specifically: "Selected Ambient Works Volume II", 1994), Global Communication ("", 1994), FSOL The Future Sound of London ("Lifeforms", "ISDN"), The Black Dog ("Temple of Transparent Balls", 1993), Another Green World ("Invisible Landscape",1996), Autechre ("Incunabula", 1993, "Amber"), Boards of Canada, and The KLF's seminal "Chill Out", 1990, all took a part in popularising and diversifying ambient music where it was used as a calming respite from the intensity of the hardcore and techno popular at that time.
Related and derivative genres.
Dark ambient.
Brian Eno's original vision of ambient music as unobtrusive musical wallpaper, later fused with warm house rhythms and given playful qualities by the Orb in the 1990s, found its opposite in the style known as dark ambient. Populated by a wide assortment of personalities—ranging from aging industrial and metal experimentalists (Scorn's Mick Harris, Current 93's David Tibet, Nurse with Wound's Steven Stapleton) to electronic boffins (Kim Cascone/PGR, Psychick Warriors Ov Gaia), Japanese noise artists (K.K. Null, Merzbow), and latter-day indie rockers (Main, Bark Psychosis) -- dark ambient features toned-down or entirely missing beats with unsettling passages of keyboards, eerie samples, and treated guitar effects. Like most styles related in some way to electronic/dance music of the '90s, it's a very nebulous term; many artists enter or leave the style with each successive release. Related styles include ambient industrial (see below) and isolationist ambient.
Ambient house.
Ambient house is a musical category founded in the late 1980s that is used to describe acid house featuring ambient music elements and atmospheres. Tracks in the ambient house genre typically feature four-on-the-floor beats, synth pads, and vocal samples integrated in an atmospheric style. Ambient house tracks generally lack a diatonic center and feature much atonality along with synthesized chords. The Dutch Brainvoyager is an example of this genre. Illbient is another form of ambient house music.
Ambient industrial.
Ambient industrial is a hybrid genre of ambient and industrial music; the term industrial being used in the original experimental sense, rather than in the sense of industrial metal. A "typical" ambient industrial work (if there is such a thing) might consist of evolving dissonant harmonies of metallic drones and resonances, extreme low frequency rumbles and machine noises, perhaps supplemented by gongs, percussive rhythms, bullroarers, distorted voices or anything else the artist might care to sample (often processed to the point where the original sample is no longer recognizable). Entire works may be based on radio telescope recordings, the babbling of newborn babies, or sounds recorded through contact microphones on telegraph wires.
Space music.
Space music, also spelled "Spacemusic", includes music from the ambient genre as well as a broad range of other genres with certain characteristics in common to create the experience of contemplative spaciousness.
Many of the earliest performers were associated with the Berlin School of electronic music, which continues to inspire the genre.
Space music ranges from simple to complex sonic textures sometimes lacking conventional melodic, rhythmic, or vocal components, generally evoking a sense of "continuum of spatial imagery and emotion", beneficial introspection, deep listening and sensations of floating, cruising or flying.
Space music is used by individuals for both background enhancement and foreground listening, often with headphones, to stimulate relaxation, contemplation, inspiration and generally peaceful expansive moods and soundscapes. Space music is also a component of many film soundtracks and is commonly used in planetariums, as a relaxation aid and for meditation.
Ambient dub.
Ambient dub involves the genre melding of dub styles. It was pioneered by King Tubby and other Jamaican sound artists, who used DJ-inspired ambient electronica, complete with all the inherent drop-outs, echo, equalization and psychedelic electronic effects. It often features layering techniques and incorporates elements of world music, deep bass lines and harmonic sounds. According to David Toop, "Dub music is like a long echo delay, looping through time...turning the rational order of musical sequences into an ocean of sensation." Notable artists within the genre include Dreadzone, Higher Intelligence Agency, The Orb, Ott, Loop Guru, Woob and Transglobal Underground as well as Banco de Gaia.

</doc>
<doc id="55321" url="https://en.wikipedia.org/wiki?curid=55321" title="Postcard">
Postcard

A postcard or post card is a rectangular piece of thick paper or thin cardboard intended for writing and mailing without an envelope. Shapes other than rectangular may also be used. There are novelty exceptions, such as wood postcards, made of thin wood, and copper postcards sold in the Copper Country of the U.S. state of Michigan, and coconut "postcards" from tropical islands.
In some places, one can send a postcard for a lower fee than for a letter. Stamp collectors distinguish between postcards (which require a stamp) and postal cards (which have the postage pre-printed on them). While a postcard is usually printed by a private company, individual or organization, a postal card is issued by the relevant postal authority.
The world's oldest postcard was sent in 1840 to the writer Theodore Hook from Fulham in London, England. The study and collecting of postcards is termed "deltiology".
Early history of postcards.
Cards with messages had been sporadically created and posted by individuals since the beginning of postal services. The earliest known picture postcard was a hand-painted design on card, posted in Fulham in London to the writer Theodore Hook in 1840 bearing a penny black stamp. He probably created and posted the card to himself as a practical joke on the postal service, since the image is a caricature of workers in the post office. In 2002 the postcard sold for a record £31,750.
In the United States, the custom of sending through the mail, at letter rate, a picture or blank card stock that held a message, began with a card postmarked in December 1848 containing printed advertising. The first commercially produced card was created in 1861 by John P. Charlton of Philadelphia, who patented a postal card, and sold the rights to Hymen Lipman, whose postcards, complete with a decorated border, were labeled "Lipman's postal card". These cards had no images.
In Britain, postcards without images were issued by the Post Office in 1870, and were printed with a stamp as part of the design, which was included in the price of purchase. These cards came in two sizes. The larger size was found to be slightly too large for ease of handling, and was soon withdrawn in favour of cards 13mm (½ inch) shorter. The first known printed picture postcard, with an image on one side, was created in France in 1870 at Camp Conlie by Léon Besnardeau (1829–1914). Conlie was a training camp for soldiers in the Franco-Prussian war. The cards had a lithographed design printed on them containing emblematic images of piles of armaments on either side of a scroll topped by the arms of the Duchy of Brittany and the inscription "War of 1870. Camp Conlie. Souvenir of the National Defence. Army of Brittany". While these are certainly the first known picture postcards, there was no space for stamps and no evidence that they were ever posted without envelopes.
In the following year the first known picture postcard in which the image functioned as a souvenir was sent from Vienna. The first advertising card appeared in 1872 in Great Britain and the first German card appeared in 1874. Cards showing images increased in number during the 1880s. Images of the newly built Eiffel Tower in 1889 and 1890 gave impetus to the postcard, leading to the so-called "golden age" of the picture postcard in years following the mid-1890s. Early postcards often showcased photography of nude women. These were commonly known as French postcards, due to the large number of them produced in France.
Early US postcards.
The first American postcard was developed in 1873 by the Morgan Envelope Factory of Springfield, Massachusetts. These first postcards depicted the Interstate Industrial Exposition that took place in Chicago. Later in 1873, Post Master John Creswell introduced the first pre-stamped "Postal Cards", often called "penny postcards". Postcards were made because people were looking for an easier way to send quick notes. The first postcard to be printed as a souvenir in the United States was created in 1893 to advertise the World's Columbian Exposition in Chicago.
The Post Office was the only establishment allowed to print postcards, and it held its monopoly until May 19, 1898, when Congress passed the Private Mailing Card Act, which allowed private publishers and printers to produce postcards. Initially, the United States government prohibited private companies from calling their cards "postcards", so they were known as "souvenir cards". These cards had to be labeled "Private Mailing Cards". This prohibition was rescinded on December 24, 1901, from when private companies could use the word "postcard". Postcards were not allowed to have a divided back and correspondents could only write on the front of the postcard. This was known as the "undivided back" era of postcards. From March 1, 1907 the Post Office allowed private citizens to write on the address side of a postcard. It was on this date that postcards were allowed to have a "divided back".
On these cards the back is divided into two sections: the left section is used for the message and the right for the address. Thus began the Golden Age of American postcards, which peaked in 1910 with the introduction of tariffs on German-printed postcards, and ended by 1915, when World War I ultimately disrupted the printing and import of the fine German-printed cards. The postcard craze between 1907 and 1910 was particularly popular among rural and small-town women in Northern U.S. states.
Postcards, in the form of government postal cards and privately printed souvenir cards, became very popular as a result of the Columbian Exposition, held in Chicago in 1893, after postcards featuring buildings were distributed at the fair. In 1908, more than 677 million postcards were mailed.
The "white border" era, named for borders around the picture area, lasted from about 1916 to 1930.
Mid-20th century US postcards.
Linen postcards were produced in great quantity from 1931 to 1959. Despite the name, linen postcards were not produced on a linen fabric, but used newer printing processes that used an inexpensive card stock with a high rag content, and were then finished with a pattern which resembled linen. The face of the cards is distinguished by a textured cloth appearance which makes them easily recognizable. The reverse of the card is smooth, like earlier postcards. The rag content in the card stock allowed a much more colorful and vibrant image to be printed than the earlier "white border" style. Due to the inexpensive production and bright realistic images they became popular.
One of the better known linen era postcard manufacturers was Curt Teich and Company, who first produced the immensely popular "large letter linen" postcards (among many others). The card design featured a large letter spelling of a state or place with smaller photos inside the letters. The design can still be found in many places today. Other manufacturers include Tichnor and Company, Haynes, Stanley Piltz, E.C Kropp, and the Asheville Postcard Company.
By the late 1920s new colorants had been developed that were very enticing to the printing industry. Though they were best used as dyes to show off their brightness, this proved to be problematic. Where traditional pigment based inks would lie on a paper's surface, these thinner watery dyes had a tendency to be absorbed into a paper's fibers, where it lost its advantage of higher color density, leaving behind a dull blurry finish. To experience the rich colors of dyes light must be able to pass through them to excite their electrons. A partial solution was to combine these dyes with petroleum distillates, leading to faster drying heatset inks. But it was Curt Teich who finally solved the problem by embossing paper with a linen texture before printing. The embossing created more surface area, which allowed the new heatset inks to dry even faster. The quicker drying time allowed these dyes to remain on the paper's surface, thus retaining their superior strength, which give Linens their telltale bright colors. In addition to printing with the usual CYMK colors, a lighter blue was sometimes used to give the images extra punch. Higher speed presses could also accommodate this method, leading to its widespread use. Although first introduced in 1931, their growing popularity was interrupted by the outbreak of war. They were not to be printed in numbers again until the later 1940s, when the war effort ceased consuming most of the country’s resources. Even though the images on linen cards were based on photographs, they contained much handwork of the artists who brought them into production. There is of course nothing new in this; what it notable is that they were to be the last postcards to show any touch of the human hand on them. In their last days, many were published to look more like photo-based chrome cards that began to dominate the market. Textured papers for postcards had been manufactured ever since the turn of the century. But since this procedure was not then a necessary step in aiding card production, its added cost kept the process limited to a handful of publishers. Its original use most likely came from attempts to simulate the texture of canvas, thus relating the postcard to a painted work of fine art.
The United States Postal Service defines a postcard as: rectangular, at least high × long × thick and no more than high × long × thick. However, some postcards have deviated from this (for example, shaped postcards).
Contemporary postcards.
The last and current postcard era, which began about 1939, is the "chrome" era, however these types of cards did not begin to dominate until about 1950. The images on these cards are generally based on colored photographs, and are readily identified by the glossy appearance given by the paper's coating. 'These still photographs made the invisible visible, the unnoticed noticed, the complex simple and the simple complex. The power of the still photograph forms symbolic structures and make the image a reality', as Elizabeth Edwards wrote in her book: The Tourist Image: Myths and Myth Making in Tourism.
In 1973 the British Post Office introduced a new type of card, PHQ Cards, popular with collectors, especially when they have the appropriate stamp affixed and a First day of issue postmark obtained.
Postcards in British India.
In July 1879, the Post Office of India introduced a quarter anna postcard that could be posted from one place to another within British India. This was the cheapest form of post provided to the Indian people to date and proved a huge success. The establishment of a large postal system spanning India resulted in unprecedented postal access: a message on a postcard could be sent from one part of the country to another part (often to a physical address without a nearby post office) without additional postage affixed. This was followed in April 1880 by postcards meant specifically for government use and by reply postcards in 1890. The postcard facility continues to this date in independent India.
British seaside postcards.
In 1894, British publishers were given permission by the Royal Mail to manufacture and distribute picture postcards, which could be sent through the post. It was originally thought that the first UK postcards were produced by printing firm Stewarts of Edinburgh but later research published in Picture Postcard Monthly in 1991, has shown that the first GB picture card was published by ETW Dennis of Scarborough. Two postmarked examples of the September 1894 E T W Dennis card have survived but no cards of Stewart dated 1894 have been found. Early postcards were pictures of landmarks, scenic views, photographs or drawings of celebrities and so on. With steam locomotives providing fast and affordable travel, the seaside became a popular tourist destination, and generated its own souvenir-industry. 
In the early 1930s, cartoon-style saucy postcards became widespread, and at the peak of their popularity the sale of saucy postcards reached a massive 16 million a year. They were often bawdy in nature, making use of innuendo and double entendres and traditionally featured stereotypical characters such as vicars, large ladies and put-upon husbands, in the same vein as the "Carry On" films. In the early 1950s, the newly elected Conservative government were concerned at the apparent deterioration of morals in Britain and decided on a crackdown on these postcards. The main target on their hit list was the postcard artist Donald McGill. In the more liberal 1960s, the saucy postcard was revived and later became to be considered, by some as an art form. However, during the 1970s and 1980s, the quality of the artwork and humour started to deteriorate and, with changing attitudes towards the cards' content, the demise of the saucy postcard occurred. Original postcards are now highly sought after, and rare examples can command high prices at auction. The best-known saucy seaside postcards were created by a publishing company called Bamforths, based in the town of Holmfirth, West Yorkshire, England.
Despite the decline in popularity of postcards that are overtly 'saucy', postcards continue to be a significant economic and cultural aspect of British seaside tourism. Sold by newsagents and street vendors, as well as by specialist souvenir shops, modern seaside postcards often feature multiple depictions of the resort in unusually favourable weather conditions. John Hinde, the British photographer, used saturated colour and meticulously planned his photographs, which made his postcards of the later twentieth century become collected and admired as kitsch. Such cards are also respected as important documents of social history, and have been influential on the work of Martin Parr.
Japan.
In Japan, official postcards have one side dedicated exclusively to the address, and the other side for the content, though commemorative picture postcards and private picture postcards also exist. In Japan today, two particular idiosyncratic postcard customs exist: and . New Year's Day postcards serve as greeting cards, similar to Western Christmas cards, while return postcards function similarly to a self-addressed stamped envelope, allowing one to receive a reply without burdening the addressee with postage fees. Return postcards consist of a single double-size sheet, and cost double the price of a usual postcard – one addresses and writes one half as a usual postcard, writes one's own address on the return card, leaving the other side blank for the reply, then folds and sends. Return postcards are most frequently encountered by non-Japanese in the context of making reservations at certain locations that only accept reservations by return postcard, notably at Saihō-ji (moss temple). For overseas purposes, an international reply coupon is used instead.
In Japan, official postcards were introduced in December 1873, shortly after stamps were introduced to Japan. Return postcards were introduced in 1885, sealed postcards in 1900, and private postcards were allowed from 1900.
Russia.
In the State Standard of the Russian Federation "GOST 51507-99. Postal cards. Technical requirements. Methods of Control" (2000) gives the following definition:
Post Card is a standard rectangular form of a paper for public postings. According to the same state standards, cards are classified according to the type and kind.
Depending on whether or not the image on the card printing postage stamp cards are divided into two types:
Depending on whether or not the card illustrations, cards are divided into two types:
Cards, depending on the location of illustrations divided into:
Depending on the walking area cards subdivided into:
Free postcards.
Specialist marketing companies in many countries produce and distribute advertising postcards which are available for free. These are normally offered on wire rack displays in plazas, coffee shops and other commercial locations, usually not intended to be mailed.
Controversy.
The initial appearance of picture postcards (and the enthusiasm with which the new medium was embraced) raised some legal issues. Picture postcards allowed and encouraged many individuals to send images across national borders, and the legal availability of a postcard image in one country did not guarantee that the card would be considered "proper" in the destination country, or in the intermediate countries that the card would have to pass through. Some countries might refuse to handle postcards containing sexual references (in seaside postcards) or images of full or partial nudity (for instance, in images of classical statuary or paintings).
In response to this new phenomenon, the Ottoman Empire banned the sale or importation of some materials relating to the Islamic prophet Muhammad in 1900. Affected postcards that were successfully sent through the Ottoman Empire before this date (and are postmarked accordingly) have a high rarity value and are considered valuable by collectors.

</doc>
<doc id="55325" url="https://en.wikipedia.org/wiki?curid=55325" title="Total Recall (1990 film)">
Total Recall (1990 film)

Total Recall is a 1990 American science fiction action film directed by Paul Verhoeven, starring Arnold Schwarzenegger, Rachel Ticotin, and Sharon Stone. The film is loosely based on the Philip K. Dick story ""We Can Remember It for You Wholesale"". It tells the story of a construction worker who is having troubling dreams about Mars and a mysterious woman there. It was written by Ronald Shusett, Dan O'Bannon, Jon Povill, and Gary Goldman, and won a Special Achievement Academy Award for its visual effects. The original score composed by Jerry Goldsmith won the BMI Film Music Award.
The film was one of the most expensive films made at the time of its release, although estimates of its production budget vary and it is not certain whether it ever actually held the record.
Plot.
In 2084, Earthbound construction worker Douglas Quaid (Arnold Schwarzenegger) is having troubling dreams about Mars and a mysterious woman there. His wife Lori (Sharon Stone) dismisses the dreams and discourages him from thinking about Mars, where the governor, Vilos Cohaagen (Ronny Cox), is fighting rebels while searching for a rumored alien artifact located in the mines. At Rekall, a company that provides memory implants of vacations, Quaid opts for a memory trip to Mars as a Secret Agent fantasy. However, during the procedure, before the memory is implanted, something goes wrong, and the story diverges between the question of what is real and what is hallucination. Apparently, Quaid starts revealing previously suppressed memories of actually being a Secret Agent. The company sedates him, wipes his memory of the visit, and sends him home. On the way home, Quaid is attacked by his friend Harry (Robert Costanzo) and some construction co-workers; he is forced to kill them, revealing elite fighting-skills. He is then attacked in his apartment by Lori, who reveals that she was never his wife; their marriage was just a false memory implant, and Cohaagen sent her as an agent to monitor Quaid. He is then attacked and pursued by armed thugs led by Richter (Michael Ironside), Lori's real husband and Cohaagen's operative.
After evading his attackers, Quaid is given a suitcase containing money, gadgets, fake IDs, a disguise, and a video recording. The video is of Quaid himself, who identifies himself as "Hauser" and explains that he used to work for Cohaagen but learned about the artifact and underwent the memory wipe to protect himself. "Hauser" instructs Quaid to remove a tracking device located inside his skull before ordering him to go to Mars and check into the Hilton Hotel with a fake ID. Quaid makes his way to Mars and follows clues to Venusville, the colony's red-light district, primarily populated by people mutated as a result of poor radiation shielding. He meets Benny (Mel Johnson, Jr.), a taxi driver, and Melina (Rachel Ticotin), the woman from his dreams; but she spurns him, believing that Quaid is still working for Cohaagen.
Quaid later encounters Dr. Edgemar (Roy Brocksmith) and Lori, who claim Quaid has suffered a "schizoid embolism" and is trapped in a fantasy based on the implanted memories. Edgemar warns that Quaid is headed for lunacy and a lobotomy if he does not return to reality, then offers Quaid a pill that would waken him from the dream. Quaid puts the pill in his mouth, but after seeing Edgemar sweating in fear, he kills Edgemar and spits out the pill. Lori alerts Richter's forces, who burst into the room and capture Quaid, but Melina rescues him, with Quaid killing Lori in the process. The two race back to the Venusville bar and escape into the tunnels with Benny. Unable to locate Quaid, Cohaagen shuts down the ventilation to Venusville, slowly asphyxiating its citizens. Quaid, Melina, and Benny are taken to a resistance base; and Quaid is introduced to Kuato (Marshall Bell), a parasitic twin conjoined to his brother's stomach. Kuato reads Quaid's mind and tells him that the alien artifact is a turbinium reactor that will create a breathable atmosphere for Mars when activated, eliminating Cohaagen's abusive monopoly on breathable air. Cohaagen's forces burst in and kill most of the resistance, including Kuato, who instructs Quaid to start the reactor. Benny reveals that he is also working for Cohaagen.
Quaid and Melina are taken to Cohaagen, who reveals the Quaid persona was a ploy by Hauser to infiltrate the mutants and lead Cohaagen to Kuato, thereby wiping out the resistance. Cohaagen orders Hauser's memory to be re-implanted in Quaid and Melina programmed as Hauser's obedient wife, but Quaid and Melina escape into the mines where the reactor is located. They work their way to the control room of the reactor, and Benny attacks them in an excavation machine. Quaid kills Benny, then confronts Richter and his men, killing them too.
Quaid reaches the reactor control room, where Cohaagen is waiting with a bomb. During the ensuing struggle, Cohaagen triggers the bomb, but Quaid throws it away, blowing out one of the walls of the control room and causing an explosive decompression. While reaching for the reactor controls, Quaid knocks out Cohaagen, which causes him to be sucked out onto the Martian surface, killing him. Quaid manages to activate the reactor before he and Melina are also pulled out. The reactor releases air into the Martian atmosphere, saving Quaid, Melina and the rest of Mars' population. As humans walk onto the surface of the planet in its new atmosphere, Quaid momentarily pauses to wonder whether he is dreaming before turning to kiss Melina.
Production.
The original screenplay was written by Dan O'Bannon and Ronald Shusett, the writers of "Alien", who had bought the rights to Philip K. Dick's short story ""We Can Remember It for You Wholesale"" while Dick was still alive. They were unable to find a backer for the project and it drifted into development hell, passing from studio to studio.
In the mid-1980s, producer Dino De Laurentiis took on the project with Richard Dreyfuss attached to star. Patrick Swayze, who had recently starred in "Dirty Dancing", was also considered for the role. In 1987, it was announced that De Laurentiis would make the film as the first production for his DEL company at the new De Laurentiis film studios on the Gold Coast, with Bruce Beresford to direct from a screenplay by O'Bannon and Shusett. This film did not eventuate.
David Cronenberg was attached to direct but wanted to cast William Hurt in the lead role. Cronenberg described his work on the project and eventual falling out with Shusett: "I worked on it for a year and did about 12 drafts. Eventually we got to a point where Ron Shusett said, 'You know what you've done? You've done the Philip K. Dick version.' I said, 'Isn't that what we're supposed to be doing?' He said, 'No, no, we want to do "Raiders of the Lost Ark Go to Mars".'". When the adaptation of "Dune" flopped at the box office, De Laurentiis similarly lost enthusiasm for the project. Although he went uncredited in the final version of the film, Cronenberg originated the idea of mutants on Mars, including the character of Kuato (spelled Quato in his screenplay).
The collapse of De Laurentiis' company provided an opening for Schwarzenegger, who had unsuccessfully approached the producer about starring in the film. He persuaded Carolco to buy the rights to the film for a comparatively cheap $3 million and negotiated a salary of $10–11 million (plus 15% of the profits) to star, with an unusually broad degree of control over the production. He obtained veto power over the producer, director, screenplay, co-stars and promotion. The first thing Schwarzenegger did was personally recruit Paul Verhoeven to direct the film, having been impressed by the Dutch director's "RoboCop" (for which Schwarzenegger was considered for the title role). By this time the script had been through forty-two drafts but it still lacked a third act. Gary Goldman was therefore brought in by Paul Verhoeven to work with Ronald Shusett to develop the final draft of the screenplay. The director also brought in many of his collaborators on "RoboCop", including actor Ronny Cox, cinematographer Jost Vacano, production designer William Sandell, editor Frank J. Urioste, and special effects designer Rob Bottin.
Filming.
Much of the filming took place on location in Mexico City and at Estudios Churubusco. The futuristic subway station and vehicles are actually part of the Mexican public transportation system, with the subway cars painted gray and television monitors added.
Rating.
The film was initially given an X rating. Violence was trimmed and different camera angles were used in the over-the-top scenes for an R rating.
Soundtrack.
The score was composed and conducted by Jerry Goldsmith, and 40 minutes of it was released by the Varèse Sarabande label in 1990. Ten years later, the same label released a "Deluxe Edition," in chronological order with additional cues that were left out, totaling 74 minutes. As with several Goldsmith scores, the music was performed by the National Philharmonic Orchestra.
The main title theme features a metal percussion pattern that bears similarities to a drum pattern from "Anvil of Crom". The score has been hailed as one of Goldsmith's best, especially as heard in the deluxe edition, and commended for its blend of electronic and orchestral elements.
Reception.
Critical response.
"Total Recall" debuted at number one at the box office. The film grossed $261,299,840 worldwide, a box office success. Critical reaction to "Total Recall" has been mostly positive. It currently holds an 85% positive rating on Rotten Tomatoes, based on 52 reviews. Metacritic rated it 57 out of 100 based on 17 reviews.
Roger Ebert awarded the film three and a half stars (out of four), calling it "one of the most complex and visually interesting science fiction movies in a long time." Owen Gleiberman of "Entertainment Weekly" gave it a score of "B+" and said that it "starts out as mind-bending futuristic satire and then turns relentless becomes a violent, post-punk version of an Indiana Jones cliff-hanger." Film scholar William Buckland considers it one of the more "sublime" Philip K. Dick adaptations, contrasting it with films like "Impostor" and "Paycheck", which he considered "ridiculous".
Mick LaSalle of the "San Francisco Chronicle" said the film is not a classic, "but it's still solid and entertaining." James Berardinelli gave the film two and a half stars (out of four), saying that "neither Schwarzenegger nor Verhoeven have stretched their talents here," but added, "with a script that's occasionally as smart as it is energetic, "Total Recall" offers a little more than wholesale carnage."
Some critics, such as Janet Maslin of "The New York Times", considered the film excessively violent. Rita Kempley of "The Washington Post" gave it a negative review, saying that director Paul Verhoeven "disappoints with this appalling onslaught of blood and boredom." Feminist cultural critic Susan Faludi called it one of "an endless stream of war and action movies" in which "women are reduced to mute and incidental characters or banished altogether."
The film ranked number 79 on Rotten Tomatoes’ Journey Through Sci-Fi (100 Best-Reviewed Sci-Fi Movies).
Accolades.
In 2008, "Total Recall" was nominated for AFI's Top 10 Science Fiction Films list.
Legacy.
Novelization.
The film was novelized by Piers Anthony. The novel and film correspond fairly well, although Anthony was evidently working from an earlier script than the one used for the film, and was criticized for the ending of his book which removed the ambiguity whether the events of "Total Recall" are real or a dream. In addition, the novel had a subplot wherein the aliens planted a fail-safe device within their Mars technology, so that if it were misused or destroyed, the local star would go nova and therefore prevent the species from entering the galactic community. It coincided with a comment earlier in the novel that astronomers were noticing an abnormal number of recent supernovae, giving an indication that the aliens seeded their tech as part of a galactic experiment in technological maturity. Instead of mentioning that he dreamt of her earlier in the film, Melina mentions she was once a model, explaining how Quaid could have seen her on the screen at Rekall.
Video game.
A video game was made based on the film, featuring 2D action, platformer scenes and top-down racing scenes; a version was released for popular 8-bit home computers (Commodore 64, ZX Spectrum and Amstrad CPC), and popular 16-bit home computers (Amiga and Atari ST). The game was developed and released by Ocean Software, reaching number 2 in the UK sales charts, behind "Teenage Mutant Ninja Turtles". There was also a NES version which was notably different from the others, being developed by a different team (Interplay), who were subcontracted by Acclaim Entertainment. Interplay defended the changes, however, claiming that their alteration stuck closer to the spirit of the original short story, which they said "read more like a platformer." In a tie-in with the NES game, the August 1990 version of "Nintendo Power" promoted the game for their well-known monthly mail-in contests, under the Rekall hype "Making the Impossible Possible" whereby first prize would be one of the Martian police uniforms along with a videotaped trip to Hollywood with a chance to meet Schwarzenegger. Years later, the magazine admitted that it was their worst promotion, as "our winner did not get to meet Arnold until late 1991, and even then only for a quick handshake."
Television series.
A television series called "Total Recall 2070" went into production in 1999. The show was meant to be a sequel; however, it had far more similarities with the "Blade Runner" film (also inspired by a Philip K. Dick story) than Verhoeven's film. The two-hour series pilot, released on VHS and DVD for the North American market, borrowed footage from the film, such as the space cruiser arriving on Mars.
Comic series.
In 2011, a four-issue comic book adaptation was released by Dynamite Entertainment, continuing the story from the film.
Sequel.
Due to the film's success, a sequel was written with the script title "Total Recall 2", and with Schwarzenegger's character still Douglas Quaid, now working as a reformed law enforcer. The sequel was based on another Philip K. Dick short story, ""The Minority Report"", which hypothesizes about a future where a crime can be solved before it is committed—in the movie, the clairvoyants would be Martian mutants. In 1994, producer Mario Kassar spoke with director Ronny Yu about possibly helming the sequel. In 1998, actor-director Jonathan Frakes was also attached to the follow-up. The sequel was not filmed, but the script survived and it was changed drastically and contained greater elements from the original short story. The story was eventually adapted into the Steven Spielberg sci-fi thriller "Minority Report", which opened in 2002 to commercial success.
Remake.
In February 2009, "The Hollywood Reporter" stated that Neal H. Moritz and Original Film were in negotiations for developing a contemporary version of "Total Recall" for Columbia. In June, 2009, it was announced that Columbia Pictures had hired Kurt Wimmer to write the script for the remake. Over a year later, Len Wiseman was hired to direct.
On January 9, 2011, it was confirmed that Colin Farrell would be starring in the remake and Bryan Cranston would play the villain, with production starting in Toronto on May 15. According to producer Neal Moritz, this version of the film would be closer to Dick's original story. Moritz also stated that the film would not be shot in 3D, saying: "we decided that it would be too much." Kate Beckinsale was cast in the role of agent Lori, while John Cho was cast as McClane, the smooth-talking rep for the memory company.
The film was released on August 3, 2012 and received mixed reviews.

</doc>
<doc id="55329" url="https://en.wikipedia.org/wiki?curid=55329" title="Neturei Karta">
Neturei Karta

Neturei Karta (Jewish Babylonian Aramaic: "", literally "Guardians of the City") is a religious group that split from normative Judaism, formally created in Jerusalem, British Mandate of Palestine, in 1938, splitting off from Agudas Yisrael. The Neturei Karta see themselves as the "true Jews", but are viewed as a cult on "the farthest fringes of Judaism" by the US-based Anti-Defamation League. Neturei Karta opposes Zionism and calls for a dismantling of the State of Israel, in the belief that Jews are forbidden to have their own state until the coming of the Jewish Messiah. They live in a fashion similar to Haredi communities around the globe.
In Israel some members also pray at affiliated "beit midrash", in Jerusalem's Meah Shearim neighborhood and in Ramat Beit Shemesh Bet. Neturei Karta states that no official count of the number of members exists. The Jewish Virtual Library puts their numbers at 5,000, while the Anti-Defamation League estimates that fewer than 100 members of the community take part in anti-Israel activism. Members of Neturei Karta have a long history of extremist statements and support for notable anti-Semites and Islamic extremists.
According to the US branch Neturei Karta:
"The name Neturei Karta is a name usually given to those people who regularly pray in the Neturei Karta synagogues (Torah Ve'Yirah Jerusalem, Torah U'Tefillah London, Torah U'Tefillah NY, Beis Yehudi Upstate NY, etc.), study in or send their children to educational institutions run by Neturei Karta, or actively participate in activities, assemblies or demonstrations called by the Neturei Karta".
Origin of the name.
Originally the organization was called Chevrat HaChayim (Society of Life) however this name was quickly supplanted in favor of the name Neturei Karta.
The name "Neturei Karta" literally means "Guardians of the City" in Aramaic and is derived from a narrative on page 76c of Tractate Hagigah in the Jerusalem Talmud. There it is related that Rabbi Judah haNasi sent two rabbis on a tour of inspection:
In one town they asked to see the "guardians of the city" and the city guard was paraded before them. They said that these were not the guardians of the city but its destroyers, which prompted the citizens to ask who, then, could be considered the guardians. The rabbis answered, "The scribes and the scholars," referring them to "Tehillim" (Psalms) Chapter 127.
It is this role that Neturei Karta see themselves as fulfilling by defending what they believe is "the position of the Torah and authentic unadulterated Judaism."
History.
Background.
Generally, members of Neturei Karta are descendants of Hungarian Jews who settled in Jerusalem's Old City in the early nineteenth century, and from Lithuanian Jews who were students of the Gaon of Vilna (known as "Perushim"), who had settled earlier. In the late nineteenth century, their ancestors participated in the creation of new neighborhoods outside the city walls to alleviate overcrowding in the Old City, and most are now concentrated in the neighborhood of Batei Ungarin and the larger Meah Shearim neighborhood.
At the time, they were vocal opponents to the new political ideology of Zionism that was attempting to assert Jewish sovereignty in Ottoman-controlled Palestine. They resented the new arrivals, who were predominantly secular and anti-religious, while they asserted that Jewish redemption could only be brought about by the Jewish messiah.
Formation.
Neturei Karta was founded by Rabbi Amram Blau and Rabbi Aharon Katzenelbogen. Rabbi Blau was a native of Meah Shearim in Jerusalem and was active in the Agudat Israel during the British Mandate era. However, by the 1930s, the Aguda began to adopt a more compromising and accommodationist approach to the Zionist movement. This caused Rabbi Blau to split with the Aguda in 1937 and alongside of Rabbi Katzenelbogen found Chevrat HaChayim, which was soon to be known thereafter as Neturei Karta.
Subsequent Development.
Other Orthodox Jewish movements, including some who oppose Zionism, have denounced the activities of the radical branch of Neturei Karta. According to "The Guardian", "among Haredi, or ultra-Orthodox circles, the Neturei Karta are regarded as a wild fringe". Neturei Karta is sometimes confused with Satmar, due to both being anti-Zionist. They are separate groups and have had disagreements. Neturei Karta asserts that the mass media deliberately downplays their viewpoint and makes them out to be few in number. Their protests in America are usually attended by, at most, a few dozen people. In Israel, the group's protests typically attract several hundred participants, depending on the nature of the protest and its location.
In July 2013, the Shin Bet arrested a 46-year-old member of Neturei Karta for allegedly attempting to spy on Israel for Iran. As part of a plea deal, the man was sentenced to 4 years in prison. Neturei Karta has denied that he had ever been a member of their group.
Neturei Karta's website states that its members "frequently participatein public burning of the Israeli flag." On the Jewish holiday of Purim, Neturei Karta members have routinely burned Israeli flags in celebrations in cities such as London, Brooklyn and Jerusalem.
While many in Neturei Karta chose to simply ignore the State of Israel, this became more difficult. Some took steps to condemn Israel and bring about its eventual dismantling until the coming of the Messiah. Chief among these was Moshe Hirsch, leader of an activist branch of Neturei Karta, who served in Yasser Arafat's cabinet as Minister for Jewish Affairs.
Beliefs.
Neturei Karta stresses what is said in the mussaf Shemona Esrei of Yom Tov, that because of their sins, the Jewish people went into exile from the Land of Israel (""umipnei chatoeinu golinu meiartzeinu""). Additionally, they maintain the view – based on the Babylonian Talmud – that any form of forceful recapture of the Land of Israel is a violation of divine will. They believe that the restoration of the Land of Israel to the Jews should only happen with the coming of the Messiah, not by self-determination.
Neturei Karta believes that the exile of the Jews can only end with the arrival of the Messiah, and that human attempts to establish Jewish sovereignty over the Land of Israel are sinful. In Neturei Karta's view, Zionism is a presumptuous affront against God. Among their arguments against Zionism was a Talmudic discussion about portions of the Bible regarding a pact known as the Three Oaths made between God, the Jewish people, and the nations of the world, when the Jews were sent into exile. One provision of the pact was that the Jews would not rebel against the non-Jewish world that gave them sanctuary; a second was that they would not immigrate en masse to the Land of Israel. In return the gentile nations promised not to persecute the Jews. By rebelling against this pact, they argued, the Jewish People were engaging in rebellion against God.
The Neturei Karta synagogues follow the customs of the Gaon of Vilna, due to Neturei Karta's origin within the Lithuanian rather than Hasidic branch of ultra-Orthodox Judaism. Neturei Karta is not a Hasidic but a Litvish group, they are often mistaken for Hasidim because their style of dress (including a shtreimel on Shabbos) is very similar to that of Hasidim. This style of dress is not unique to Neturei Karta, but is also the style of other Jerusalem Litvaks, such as Rabbi Yosef Sholom Eliashiv and his followers. Furthermore, Shomer Emunim, a Hasidic group with a similar anti-Zionist ideology, is often bundled together with Neturei Karta. Typically, the Jerusalem Neturei Karta will keep the customs of the "Old "Yishuv"" of the city of Jerusalem even when living outside of Jerusalem or even when living abroad, as a demonstration of their love and connection to the Holy Land.
Factionalism.
In the United States, the Neturei Karta are led by Moshe Ber Beck of Monsey, New York. They affiliate with the radical branch led by Moshe Hirsch. Beck has courted controversy by meeting with Nation of Islam leader Minister Louis Farrakhan, who has been accused of inciting antisemitism and of describing Judaism as a "gutter religion" (although Farrakhan insists his words were misinterpreted ). In addition, after meeting with the representatives from Neturei Karta, Farrakhan indicated he would be more cautious in his choice of words in the future.
Moshe Hirsch faction.
Relations with the Palestinians.
After two men associated with the radical branch of Neturei Karta participated in a 2004 prayer vigil for Yasser Arafat outside the Percy Military Hospital in Paris, France, where he lay on his death bed, the radical branch of Neturei Karta was widely condemned by other Orthodox Jewish organizations, including many other anti-Zionist Haredi organizations both in New York and Jerusalem. Rabbi Moshe Hirsch, and what Hirsch's faction described as an "impressive contingent" of other members, attended Arafat's funeral in Ramallah.
Almost a year after the Gaza War a group of Neturei Karta members crossed into Gaza as part of the Gaza Freedom March to celebrate Jewish "Shabbos" to show support for Palestinians in the Hamas ruled enclave.
Relations with Iran and President Mahmoud Ahmedinejad.
In October 2005, Neturei Karta leader Rabbi Yisroel Dovid Weiss issued a statement criticising Jewish attacks on Iranian President Mahmoud Ahmadinejad. Weiss wrote that Ahmadinejad's statements were not "indicative of anti-Jewish sentiments", but rather, "a yearning for a better, more peaceful world", and "re-stating the beliefs and statements of Ayatollah Khomeini, who always emphasized and practiced the respect and protection of Jews and Judaism."
In March 2006, several members of a Neturei Karta's faction visited Iran where they met with Iranian leaders, including the Vice-President, and praised Ahmadinejad for calling for the Zionist regime occupying Jerusalem to vanish from the pages of time. The spokesmen commented that they shared Ahmadinejad's aspiration for "a disintegration of the Israeli government". In an interview with Iranian television reporters, Rabbi Weiss remarked, "The Zionists use the Holocaust issue to their benefit. We, Jews who perished in the Holocaust, do not use it to advance our interests. We stress that there are hundreds of thousands Jews around the world who identify with our opposition to the Zionist ideology and who feel that Zionism is not Jewish, but a political agenda. ... What we want is not a withdrawal to the '67 borders, but to everything included in it, so the country can go back to the Palestinians and we could live with them ..."
Tehran Holocaust Conference.
In December 2006, members of Neturei Karta, including Yisroel Dovid Weiss, attended the International Conference to Review the Global Vision of the Holocaust, a controversial conference being held in Tehran, Iran that attracted a number of high-profile Holocaust-deniers.
They praised Iranian president Mahmoud Ahmadinejad, and expressed solidarity with the Iranian position of anti-Zionism. Rabbi Yonah Metzger, the chief Ashkenazi Rabbi of Israel, immediately called for those who went to Tehran to be put into 'cherem', a form of excommunication. Subsequently a group of Rabbis claiming to represent part of the recently split anti-Zionist Satmar Hasidic group called on Jews "to keep away from them and condemn their actions". However the newspaper 'Der Blatt' which represents the largest part of the Satmar group refused to denounce the actions of Neturei Karta. In addition Neturei Karta claim that the late Rabbi Avrohom Leitner, one of the major Poskim (Halcachic decisors) of Brooklyn's large Satmar community publicly supported their activities.
On 21 December, the Edah HaChareidis rabbinical council of Jerusalem also released a statement calling on the public to distance itself from those who went to Iran. The Edah's statement followed, in major lines, the Satmar statement released a few days earlier In January 2007, a group of protesters stood outside the radical Neturei Karta synagogue in Monsey, New York, demanding that they leave Monsey and move to Iran, the Neturei Karta and their sympathisers from Monsey's Orthodox community responded with a counter protest.
2008 Mumbai attack on Nariman House.
One of the targets of the 2008 Mumbai attacks was the Nariman House, which was operated by the Jewish Chabad movement. Neturei Karta subsequently issued a leaflet criticising the Chabad movement for its relations with "the filthy, deplorable traitors – the cursed Zionists that are your friends." It added that the Chabad movement has been imbued with "false national sentiment" and criticised the organisation for allowing all Jews to stay in its centres, without differentiating "between good and evil, right and wrong, pure and impure, a Jew and a person who joins another religion, a believer and a heretic." The leaflet also criticised the invitation of Israeli state officials to the funerals of the victims, claiming that they "uttered words of heresy and blasphemy." The leaflet concluded that "the road have taken is the road of death and it leads to doom, assimilation and the uprooting of the Torah."
Sikrikim.
A radical breakaway faction called "Sikrikim" is based in Israel, mainly in Jerusalem and Beit Shemesh. The group's engagement in acts of vandalism, "mafia-like intimidation" and violent protests caused several people, including authority figures, to push for officially labeling them as a terrorist group, along with Neturei Karta.
Further reading.
Books
978-965-91505-0-2 (Hebrew language)

</doc>
<doc id="55331" url="https://en.wikipedia.org/wiki?curid=55331" title="Westport">
Westport

Westport is the name of several communities around the world. 

</doc>
<doc id="55335" url="https://en.wikipedia.org/wiki?curid=55335" title="Discounting">
Discounting

Discounting is a financial mechanism in which a debtor obtains the right to delay payments to a creditor, for a defined period of time, in exchange for a charge or fee. Essentially, the party that owes money in the present purchases the right to delay the payment until some future date. The discount, or charge, is the difference (expressed as a difference in the same units (absolute) or in percentage terms (relative), or as a ratio) between the original amount owed in the present and the amount that has to be paid in the future to settle the debt.
The discount is usually associated with a "discount rate", which is also called the "discount yield". The discount yield is the proportional share of the initial amount owed (initial liability) that must be paid to delay payment for 1 year. 
It is also the rate at which the amount owed must rise to delay payment for 1 year.
Since a person can earn a return on money invested over some period of time, most economic and financial models assume the discount yield is the same as the rate of return the person could receive by investing this money elsewhere (in assets of similar risk) over the given period of time covered by the delay in payment. The concept is associated with the opportunity cost of not having use of the money for the period of time covered by the delay in payment. The relationship between the discount yield and the rate of return on other financial assets is usually discussed in such economic and financial theories involving the inter-relation between various market prices, and the achievement of Pareto optimality through the operations in the capitalistic price mechanism, as well as in the discussion of the efficient (financial) market hypothesis. The person delaying the payment of the current liability is essentially compensating the person to whom he/she owes money for the lost revenue that could be earned from an investment during the time period covered by the delay in payment. Accordingly, it is the relevant "discount yield" that determines the "discount", and not the other way around.
As indicated, the rate of return is usually calculated in accordance to an annual return on investment. Since an investor earns a return on the original principal amount of the investment as well as on any prior period investment income, investment earnings are "compounded" as time advances. Therefore, considering the fact that the "discount" must match the benefits obtained from a similar investment asset, the "discount yield" must be used within the same compounding mechanism to negotiate an increase in the size of the "discount" whenever the time period of the payment is delayed or extended. The "discount rate" is the rate at which the "discount" must grow as the delay in payment is extended. This fact is directly tied into the time value of money and its calculations.
The "time value of money" indicates there is a difference between the "future value" of a payment and the "present value" of the same payment. The rate of return on investment should be the dominant factor in evaluating the market's assessment of the difference between the future value and the present value of a payment; and it is the market's assessment that counts the most. Therefore, the "discount yield", which is predetermined by a related return on investment that is found in the financial markets, is what is used within the time-value-of-money calculations to determine the "discount" required to delay payment of a financial liability for a given period of time.
Basic calculation.
If we consider the value of the original payment presently due to be "P", and the debtor wants to delay the payment for "t" years, then an "r" Market Rate of Return on a similar Investment Assets means the future value of "P" is formula_1, and the discount would be calculated as
where "r" is also the discount yield.
If "F" is a payment that will be made "t" years in the future, then the "Present Value" of this Payment, also called the "Discounted Value" of the payment, is
To calculate the present value of a single cash flow, it is divided by one plus the interest rate for each period of time that will pass. This is expressed mathematically as raising the divisor to the power of the number of units of time.
Consider the task to find the present value "PV" of $100 that will be received in five years. Or equivalently, to find which amount of money today will grow to $100 in five years when subject to a constant discount rate.
Assuming a 12% per year interest rate, it follows that
Discount rate.
The discount rate which is used in financial calculations is usually chosen to be equal to the cost of capital. The cost of capital, in a financial market equilibrium, will be the same as the market rate of return on the financial asset mixture the firm uses to finance capital investment. Some adjustment may be made to the discount rate to take account of risks associated with uncertain cash flows, with other developments.
The discount rates typically applied to different types of companies show significant differences:
The higher discount rate for start-ups reflects the various disadvantages they face, compared to established companies:
One method that looks into a correct discount rate is the capital asset pricing model. This model takes into account three variables that make up the discount rate:
1. Risk free rate: The percentage of return generated by investing in risk free securities such as government bonds.
2. Beta: The measurement of how a company's stock price reacts to a change in the market. A beta higher than 1 means that a change in share price is exaggerated compared to the rest of shares in the same market. A beta less than 1 means that the share is stable and not very responsive to changes in the market. Less than 0 means that a share is moving in the opposite direction from the rest of the shares in the same market.
3. Equity market risk premium: The return on investment that investors require above the risk free rate.
Discount factor.
The discount factor, "DF(T)", is the factor by which a future cash flow must be multiplied in order to obtain the present value. For a zero-rate (also called spot rate) "r", taken from a yield curve, and a time to cash flow "T" (in years), the discount factor is:
In the case where the only discount rate you have is not a zero-rate (neither taken from a zero-coupon bond nor converted from a swap rate to a zero-rate through bootstrapping) but an annually-compounded rate (for example if your benchmark is a US Treasury bond with annual coupons and you only have its yield to maturity, you would use an annually-compounded discount factor:
However, when operating in a bank, where the amount the bank can lend (and therefore get interest) is linked to the value of its assets (including accrued interest), traders usually use daily compounding to discount cash flows. Indeed, even if the interest of the bonds it holds (for example) is paid semi-annually, the value of its book of bond will increase daily, thanks to accrued interest being accounted for, and therefore the bank will be able to re-invest these daily accrued interest (by lending additional money or buying more financial products). In that case, the discount factor is then (if the usual money market day count convention for the currency is ACT/360, in case of currencies such as United States dollar, euro, Japanese yen), with "r" the zero-rate and "T" the time to cash flow in years:
or, in case the market convention for the currency being discounted is ACT/365 (AUD, CAD, GBP):
Sometimes, for manual calculation, the continuously-compounded hypothesis is a close-enough approximation of the daily-compounding hypothesis, and makes calculation easier (even though it does not have any real application as no financial instrument is continuously compounded). In that case, the discount factor is:
Other discounts.
For discounts in marketing, see discounts and allowances, sales promotion, and pricing. The article on discounted cash flow provides an example about discounting and risks in real estate investments.
References.
Notes

</doc>
<doc id="55336" url="https://en.wikipedia.org/wiki?curid=55336" title="Cash">
Cash

Cash refers to money in the physical form of currency, such as banknotes and coins.
In bookkeeping and finance, cash refers to current assets comprising currency or currency equivalents that can be accessed immediately or near-immediately (as in the case of money market accounts). Cash is seen either as a reserve for payments, in case of a structural or incidental negative cash flow or as a way to avoid a downturn on financial markets.
Etymology.
The English word "cash" is ultimately rooted in the Old Persian "karsha" meaning “a unit of value equivalent to one cash coin” and was first employed during the reign of Cyrus II followed by the establishment of the “formal” banking system and around the same time of the establishment of the credit and checking unions during the reign of Darius I who also minted the first face-coins.This form along with this meaning entered into the Sanskrit language during the post-Vedic era and also entered into Latin giving forms such as "capsa" “money box” (cf. OPers. Kshatrapavan = Satrap “an ancient Persian commercial and state confinement”) and "case" eventually passing into Middle French as "caisse" meaning “money in hand, coin.” 
"Cash" used as a verb means "to convert to cash"; for example in the expression "to cash a cheque".
History.
In Western Europe, after the Collapse of the Western Roman Empire, coins, silver jewelry and hacksilver (silver objects hacked into pieces) were for centuries the only form of money, until Venetian merchants started using silver bars for large transactions in the early Middle Ages. In a separate development, Venetian merchants started using paper bills, instructing their banker to make payments. Similar marked silver bars were in use in lands where the Venetian merchants had established representative offices. The Byzantine empire and several states in the Balkan area and Kievan Rus also used marked silver bars for large payments. As the world economy developed and silver supplies increased, in particular after the colonization of South America, coins became larger and a standard coin for international payment developed from the 15th century: the Spanish and Spanish colonial coin of 8 reales. Its counterpart in gold was the Venetian ducat.
Coin types would compete for markets. By conquering foreign markets, the issuing rulers would enjoy extra income from seigniorage (the difference between the value of the coin and the value of the metal the coin was made of). Successful coin types of high nobility would be copied by lower nobility for seigniorage. Imitations were usually of a lower weight, undermining the popularity of the original. As feudal states coalesced into kingdoms, imitation of silver types abated, but gold coins, in particular the gold ducat and the gold florin were still issued as trade coins: coins without a fixed value, going by weight. Colonial powers also sought to take away market share from Spain by issuing trade coin equivalents of silver Spanish coins, without much success.
In the early part of the 17th century, English East India Company coins were minted in England and shipped to the East. In England over time the word ‘Cash’ was adopted from Sanskrit कर्ष karsa, a weight of gold or silver but akin to Old Persian 𐎣𐎼𐏁 karsha, unit of weight (83.30 grams). East India Company coinage had both Urdu and English writing on it, to facilitate its use within trade. In 1671 the directors of The East India Company ordered a mint to be established at Bombay, known as Bombain. In 1677 this was sanctioned by the Crown, the coins, having received royal sanction were struck as silver Rupees; the inscription runs The Rupee of Bombaim, by authority of Charles II.
At about this time coins were also being produced for The East India Company at the Madras mint. The currency at The Company’s Bombay and Bengal administrative regions was The Rupee. At Madras, however, the Company's accounts were reckoned in “pagodas”, “fractions”, “fanams”, “faluce” and “cash”. This system was maintained until 1818 when the rupee was adopted as the unit of currency for the Company's operations, the relation between the two systems being 1 pagoda = 3-91 rupees and 1 rupee = 12 fanams.
Meanwhile, paper money had been developed. At first, it was thought of for emergency issues, hence were most popular in the colonies of European powers. In the 18th century, important paper issues were made in colonies such as Ceylon and the bordering colonies of Essequibo, Demerara and Berbice. John Law did pioneering work on banknotes with the "Banque Royale". However, the relation between money supply and inflation was still imperfectly understood and the bank went under, while its notes became worthless when they were over-issued. The lessons learned were applied to the Bank of England, which played a crucial role in financing Wellington's Peninsular war, against French troops, hamstrung by a metallic Franc de Germinal.
The ability to create paper money made nation-states responsible for the management of inflation, through control of the money supply. It also made a direct relation between the metal of the coin and its denomination superfluous. From 1816, coins generally became token money, though some large silver and gold coins remained standard coins until 1927. The first world war saw standard coins disappear to a very large extent. Afterwards, standard gold coins, mainly British sovereigns, would still be used in colonies and less developed economies and silver Maria Theresa thalers dated 1780 would be struck as trade coins for countries in East Asia until 1946 and possibly later locally.
Cash has now become a very small part of the money supply. Its remaining role is to provide a form of currency storage and payment for those who do not wish to take part in other systems, and make small payments conveniently and promptly, though this latter role is being replaced more and more frequently by electronic payment systems. Research has found that the demand for cash decreases as debit card usage increases because merchants need to make less change for customer purchases.
Cash is increasing in circulation. The value of the United States dollar in circulation increased by 42% from 2007 to 2012. The value of Pound Sterling banknotes in circulation increased by 29% from 2008 to 2013. The value of the Euro in circulation increased by 34% from August 2008 to August 2013 (2% of the increase was due to the adoption of Euro in Slovakia 2009 and in Estonia 2011).

</doc>
<doc id="55338" url="https://en.wikipedia.org/wiki?curid=55338" title="Rock Ridge">
Rock Ridge

The Rock Ridge Interchange Protocol (RRIP, IEEE P1282) is an extension to the ISO 9660 volume format, commonly used on CD-ROM and DVD media, which adds POSIX file system semantics. The availability of these extension properties allows for better integration with Unix and Unix-like operating systems. 
The standard takes its name from the fictional town "Rock Ridge" in Mel Brooks' film "Blazing Saddles. 
Design and contents.
The RRIP extensions are, briefly:
The RRIP extensions are built upon a related standard System Use Sharing Protocol (SUSP, IEEE P1281). SUSP provides a generic way of including additional properties for any directory entry reachable from the primary volume descriptor (PVD).
In an ISO 9660 volume, every directory entry has an optional "system use area" whose contents are undefined and left to be interpreted by the system. SUSP defines a method to subdivide that area into multiple system use fields, each identified by a two-character signature tag. The idea behind SUSP was that it would enable any number of independent extensions to ISO 9660 (not just RRIP) to be created and included on a volume without conflicting. It also allows for the inclusion of property data that would otherwise be too large to fit within the limits of the system use area.
SUSP defines several common tags and system use fields:
RRIP defines additional SUSP tags for support of POSIX semantics, along with the format and meaning of the corresponding system use fields:
Other known SUSP fields include:
Note that the Apple ISO 9660 Extensions do not technically follow the SUSP standard; however the basic structure of the AA and AB fields defined by Apple are forward compatible with SUSP; so that, with care, a volume can use both Apple extensions as well as RRIP extensions.
Variants.
"Amiga Rock Ridge" is similar to RRIP, except it provides additional properties used by the Amiga operating system. It too is built on the SUSP standard by defining an "AS"-tagged system use field. Thus both Amiga Rock Ridge and the POSIX RRIP may be used simultaneously on the same volume.
Some of the specific properties supported by this extension are the additional Amiga-bits for files. There is support for attribute "P" that stands for "pure" bit (indicating re-entrant command) and attribute "S" for script bit (indicating batch file). This includes the protection flags plus an optional comment field. These extensions were introduced by Angela Schmidt with the help of Andrew Young,
the primary author of the Rock Ridge Interchange Protocol and System Use Sharing Protocol.
The Amiga extensions are recognized by Amiga program MasterISO, and should also be recognized by MakeCD and Frying Pan, but the support by latter two programs is uncredited.
Amiga filesystems supporting the extensions are AmiCDFS, AsimCDFS and CacheCDFS.
Users who want to access comments and protection bits of their Amiga files present on CDs could simply mount some new logical units associated to the same physical unit, but using Amiga CacheCDFS as filesystem.

</doc>
<doc id="55340" url="https://en.wikipedia.org/wiki?curid=55340" title="Song Zheyuan">
Song Zheyuan

Sòng Zhéyuán (宋哲元) (October 30, 1885 – April 5, 1940) was a Chinese general during the Chinese Civil War and Second Sino-Japanese War (1937-1945).
Biography.
Early life and education.
Born in the village of Zhaohong, northwest of the seat of Leling County, Shandong, he was educated under his uncle from his mother's side, a teacher of a traditional Confucian private school in Yanshan County. At the age of 20 (1904) he began studying in the military institute founded by Lu Jianzhang at Beijing and had since become Lu's favorite. In 1912 the troops of Lu and Feng Yuxiang, now subordinates of Yuan Shikai, were regrouped and Feng had then been Song's superior.
Military career.
In 1917, a year after being appointed the head of 1st battalion of Feng's 2nd regiment, his battalion spearheaded the removal of Zhang Xun from his imperial restoration in 1917. As part of the Guominjun he became Governor of Jehol Province in 1926. Following the defeat of the Guominjun in the Anti–Fengtian War Feng Yuxiang participated in the Northern Expedition, Sòng assumed the Chairmanship of Shaanxi province in November 1927 and in April of the same year the head of 4th division under the II Corps of the National Revolutionary Army.
Switching sides to the Kuomintang after the abortive coup d'état in 1930 of Feng against Chiang Kai-shek, his troops were designated as the 29th Army and garrisoned in southern Shanxi province where he was responsible for the frontiers of the Rehe and Chahar provinces against the Japanese in Manchukuo.
Chair of Chahar province.
Song was the chairman of Chahar province when Japan invaded the provinces in the end of year 1932. Though poorly equipped compared to the better armed Japanese, Song led the 29th army to resist the aggression in a war known as the Defense of the Great Wall (熱河長城之戰). Japanese troops then entered the suburbs of Beijing and Tianjin after the predictable victory. Song was relieved from his post but reinstalled as commander after the Ho-Umezu agreement.
Later years.
In the Battle of Lugou Bridge, his 29th Army bore the brunt of the Japanese Guandong Army. His troops were halved after the defeat and chased by the Japanese along the Jinpu Railway into Shandong Province during the Beiping–Hankou Railway Operation. However Han Fuqu, chairman of the province and suspected for his clandestine Japanese liaison, forbade Song to retreat across the Yellow River, resulting in the 29th Army being shattered at Shijiazhuang in December 1937 and January 1938. Remaining forces suffered various losses against the Imperial Japanese Army and were delegated to guerrilla combat after retreating into the mountainous regions at the borders of Henan and Shanxi province in February 1938.
He soon suffered various illnesses and died at the age of 54 in Mianyang County, Sichuan province after several unsuccessful medical treatments in Guilin, Chongqing, and Chengdu.

</doc>
<doc id="55344" url="https://en.wikipedia.org/wiki?curid=55344" title="Hermann Huppen">
Hermann Huppen

Hermann Huppen (born 17 July 1938) is a Belgian comic book artist. He is better known under his pen-name Hermann. He is most famous for his post-apocalyptic comic "Jeremiah" which was made into a television series.
Biography.
Hermann was born in 1938 in Bévercé (now a part of Malmedy) in Liège Province. After studying to become a furniture maker and working as interior architect, Hermann made his debut as comic book artist in 1964 in the Franco-Belgian comics magazine "Spirou" with a four page story. Greg noticed his talent and offered him to work for his studio. In 1966, he began illustrating the "Bernard Prince" series written by Greg, published in "Tintin" magazine. In 1969, also in collaboration with Greg, he began the western series "Comanche". This appeared at the same time as other western series such as "Blueberry".
Hermann began writing his own stories in 1977, starting the post-apocalyptic "Jeremiah" series, which is still produced today. In the same period, he also made three albums of "Nick", inspired by "Little Nemo in Slumberland", for "Spirou". In 1983 he began a new series, "Les Tours de Bois-Maury", which is set in the Middle Ages and is less focused on action than his other works.
Hermann has also created many non-series graphic novels sometimes together with his son Yves H. One of them, "Lune de Guerre", with a story by Jean Van Hamme, was later filmed as "The Wedding Party" by Dominique Deruddere.
Hermann is characterized by a realistic style and stories that are both somber and angry, with a sense of disillusion with regards to the human character in general, and current society more specifically.
Selected bibliography.
Most of these comics have been published in French and Dutch: other translations are noted in the "remarks" column.

</doc>
<doc id="55345" url="https://en.wikipedia.org/wiki?curid=55345" title="Net present value">
Net present value

In finance, the net present value (NPV) or net present worth (NPW) is defined as the sum of the present values (PVs) of incoming and outgoing cash flows over a period of time. Incoming and outgoing cash flows can also be described as benefit and cost cash flows, respectively.
Time value of money dictates that time affects the value of cash flows. In other words, a lender may give you 99 cents for the promise of receiving $1.00 a month from now, but the promise to receive that same dollar 20 years in the future would be worth much less today to that same person (lender), even if the payback in both cases was equally certain. This decrease in the current value of future cash flows is based on the market dictated rate of return. More technically, cash flows of "nominal" equal value over a time series result in different "effective" value cash flows that makes future cash flows less valuable over time. If for example there exists a time series of identical cash flows, the cash flow in the present is the most valuable, with each future cash flow becoming less valuable than the previous cash flow. A cash flow today is more valuable than an identical cash flow in the future because a present flow can be invested immediately and begin earning returns, while a future flow cannot.
Net present value (NPV) is determined by calculating the costs (negative cash flows) and benefits (positive cash flows) for each period of an investment. The period is typically one year, but could be measured in quarter-years, half-years or months. After the cash flow for each period is calculated, the present value (PV) of each one is achieved by discounting its future value (see Formula) at a periodic rate of return (the rate of return dictated by the market). NPV is the sum of all the discounted future cash flows. Because of its simplicity, NPV is a useful tool to determine whether a project or investment will result in a net profit or a loss. A positive NPV results in profit, while a negative NPV results in a loss. The NPV measures the excess or shortfall of cash flows, in present value terms, above the cost of funds. In a theoretical situation of unlimited capital budgeting a company should pursue every investment with a positive NPV. However, in practical terms a company's capital constraints limit investments to projects with the highest NPV whose cost cash flows, or initial cash investment, do not exceed the company's capital. NPV is a central tool in discounted cash flow (DCF) analysis and is a standard method for using the time value of money to appraise long-term projects. It is widely used throughout economics, finance, and accounting.
In the case when all future cash flows are positive, or incoming (such as the principal and coupon payment of a bond) the only outflow of cash is the purchase price, the NPV is simply the PV of future cash flows minus the purchase price (which is its own PV). NPV can be described as the “difference amount” between the sums of discounted cash inflows and cash outflows. It compares the present value of money today to the present value of money in the future, taking inflation and returns into account.
The NPV of a sequence of cash flows takes as input the cash flows and a discount rate or discount curve and outputs a price. The converse process in DCF analysis — taking a sequence of cash flows and a price as input and inferring as output a discount rate (the discount rate which would yield the given price as NPV) — is called the yield and is more widely used in bond trading.
Formula.
Each cash inflow/outflow is discounted back to its present value (PV). Then they are summed. Therefore, NPV is the sum of all terms,
where
The result of this formula is multiplied with the Annual Net cash in-flows and reduced by Initial Cash outlay the present value but in cases where the cash flows are not equal in amount, then the previous formula will be used to determine the present value of each cash flow separately. Any cash flow within 12 months will not be discounted for NPV purpose, nevertheless the usual initial investments during the first year "R"0 are summed up a negative cash flow.
Given the (period, cash flow) pairs (formula_2, formula_4) where formula_8 is the total number of periods, the net present value formula_9 is given by:
Many computer-based spreadsheet programs have built-in formulae for PV and NPV.
The discount rate.
The rate used to discount future cash flows to the present value is a key variable of this process.
A firm's weighted average cost of capital (after tax) is often used, but many people believe that it is appropriate to use higher discount rates to adjust for risk, opportunity cost, or other factors. A variable discount rate with higher rates applied to cash flows occurring further along the time span might be used to reflect the yield curve premium for long-term debt.
Another approach to choosing the discount rate factor is to decide the rate which the capital needed for the project could return if invested in an alternative venture. If, for example, the capital required for Project A can earn 5% elsewhere, use this discount rate in the NPV calculation to allow a direct comparison to be made between Project A and the alternative. Related to this concept is to use the firm's reinvestment rate. Reinvestment rate can be defined as the rate of return for the firm's investments on average. When analyzing projects in a capital constrained environment, it may be appropriate to use the reinvestment rate rather than the firm's weighted average cost of capital as the discount factor. It reflects opportunity cost of investment, rather than the possibly lower cost of capital.
An NPV calculated using variable discount rates (if they are known for the duration of the investment) may better reflect the situation than one calculated from a constant discount rate for the entire investment duration. Refer to the tutorial article written by Samuel Baker for more detailed relationship between the NPV value and the discount rate.
For some professional investors, their investment funds are committed to target a specified rate of return. In such cases, that rate of return should be selected as the discount rate for the NPV calculation. In this way, a direct comparison can be made between the profitability of the project and the desired rate of return.
To some extent, the selection of the discount rate is dependent on the use to which it will be put. If the intent is simply to determine whether a project will add value to the company, using the firm's weighted average cost of capital may be appropriate. If trying to decide between alternative investments in order to maximize the value of the firm, the corporate reinvestment rate would probably be a better choice.
Using variable rates over time, or discounting "guaranteed" cash flows differently from "at risk" cash flows, may be a superior methodology but is seldom used in practice. Using the discount rate to adjust for risk is often difficult to do in practice (especially internationally) and is difficult to do well. An alternative to using discount factor to adjust for risk is to explicitly correct the cash flows for the risk elements using rNPV or a similar method, then discount at the firm's rate.
Use in decision making.
NPV is an indicator of how much value an investment or project adds to the firm. With a particular project, if formula_4 is a positive value, the project is in the status of positive cash inflow in the time of "t". If formula_4 is a negative value, the project is in the status of discounted cash outflow in the time of "t". Appropriately risked projects with a positive NPV could be accepted. This does not necessarily mean that they should be undertaken since NPV at the cost of capital may not account for opportunity cost, "i.e.," comparison with other available investments. In financial theory, if there is a choice between two mutually exclusive alternatives, the one yielding the higher NPV should be selected.A positive net present value indicates that the projected earnings generated by a project or investment (in present dollars) exceeds the anticipated costs (also in present dollars). Generally, an investment with a positive NPV will be a profitable one and one with a negative NPV will result in a net loss. This concept is the basis for the Net Present Value Rule, which dictates that the only investments that should be made are those with positive NPV values.<br>
Interpretation as integral transform.
The time-discrete formula of the net present value
can also be written in a continuous variation
where
Net present value can be regarded as Laplace- respectively Z-transformed cash flow with the integral operator including the complex number "s" which resembles to the interest rate "i" from the real number space or more precisely "s" = ln(1 + "i").
From this follow simplifications known from cybernetics, control theory and system dynamics. Imaginary parts of the complex number "s" describe the oscillating behaviour (compare with the pork cycle, cobweb theorem, and phase shift between commodity price and supply offer) whereas real parts are responsible for representing the effect of compound interest (compare with damping).
Example.
A corporation must decide whether to introduce a new product line. The company will have immediate costs of 100,000 at "t = 0". Recall, a cost is a negative for outgoing cash flow, thus this cash flow is represented as -100,000. The company assumes the product will provide equal benefits of 10,000 for each of 12 years beginning at "t = 1". For simplicity, assume the company will have no outgoing cash flows after the initial 100,000 cost. This also makes the simplifying assumption that the net cash received or paid is lumped into a single transaction occurring "on the last day" of each year. At the end of the 12 years the product no longer provides any cash flow and is discontinued without any additional costs. Assume that the effective annual discount rate is 10%.
The present value (value at "t = 0") can be calculated for each year:
The total present value of the incoming cash flows is 68,136.92. The total present value of the outgoing cash flows is simply the 100,000 at time "t = 0". 
Thus:
formula_16
Rearranging the formula:
formula_17
In the above example:
formula_18
formula_19
Observe that as "t" increases the present value of each cash flow at "t" decreases. For example, the final incoming cash flow has a future value of 10,000 at "t = 12" but has a present value (at "t = 0") of 3,186.31. The opposite of discounting is compounding. Taking the example in reverse, it is the equivalent of investing 3,186.31 at "t = 0" (the present value) at an interest rate of 10% compounded for 12 years, which results in a cash flow of 10,000 at "t = 12" (the future value).
The importance of NPV becomes clear in this instance. Although the incoming cash flows (10,000 x 12 = 120,000) appear to exceed the outgoing cash flow (100,000), the future cash flows are not adjusted using the discount rate. Thus, the project appears misleadingly profitable. When the cash flows are discounted however, it indicates the project would result in a net loss of 31,863.08. Thus, the NPV calculation indicates that this project should be disregarded because investing in this project is the equivalent of a loss of 31,863.08 at "t = 0". The concept of time value of money indicates that cash flows in different periods of time cannot be accurately compared unless they have been adjusted to reflect their value at the same period of time (in this instance, "t = 0"). It is the present value of each future cash flow that must be determined in order to provide any meaningful comparison between cash flows at different periods of time. There are a few inherent assumptions in this type of discounted cash flow / net present value type analysis:
More realistic problems would also need to consider other factors, generally including: smaller time buckets, the calculation of taxes (including the cash flow timing), inflation, currency exchange fluctuations, hedged or unhedged commodity costs, risks of technical obsolescence, potential future competitive factors, uneven or unpredictable cash flows, and a more realistic salvage value assumption, as well as many others.
A more simple example of the net present value of incoming cash flow over a set period of time, would be winning a Powerball lottery of $500 million. If one does not select the "CASH" option they will be paid $25,000,000 dollars per year for 20 years, a total of $500,000,000, however, if one does select the "CASH" option, they will receive a one-time lump sum payment of approximately $285 million, the NPV of $500,000,000 paid over time. See "other factors" above that could affect the payment amount. Both scenarios are before taxes.
History.
Net present value as a valuation methodology dates at least to the 19th century. Karl Marx refers to NPV as fictitious capital, and the calculation as "capitalising," writing:
In mainstream neo-classical economics, NPV was formalized and popularized by Irving Fisher, in his 1907 "The Rate of Interest" and became included in textbooks from the 1950s onwards, starting in finance texts.

</doc>
<doc id="55347" url="https://en.wikipedia.org/wiki?curid=55347" title="Hierarchical File System">
Hierarchical File System

Hierarchical File System (HFS) is a proprietary file system developed by Apple Inc. for use in computer systems running Mac OS. Originally designed for use on floppy and hard disks, it can also be found on read-only media such as CD-ROMs. HFS is also referred to as Mac OS Standard (or, erroneously, "HFS Standard"), while its successor, HFS Plus, is also called "Mac OS Extended" (or, erroneously, "HFS Extended"). With the introduction of OS X 10.6, Apple dropped support to format or write HFS disks and images, which are only supported as read-only volumes.
History.
HFS was introduced by Apple in September 1985, specifically to support Apple's first hard disk drive for the Macintosh, replacing the Macintosh File System (MFS), the original file system which had been introduced over a year and a half earlier with the first Macintosh computer. HFS drew heavily upon Apple's first hierarchical SOS operating system for the failed Apple III, which also served as the basis for hierarchical filing systems on the Apple IIe and Apple Lisa. HFS was developed by Patrick Dirks and Bill Bruffey. It shared a number of design features with MFS that were not available in other file systems of the time (such as DOS's FAT). Files could have multiple forks (normally a data and a resource fork), which allowed the main data of the file to be stored separately from resources such as icons that might need to be localized. Files were referenced with unique file IDs rather than file names, and file names could be 255 characters long (although the Finder only supported a maximum of 31 characters).
However, MFS had been optimized to be used on very small and slow media, namely floppy disks, so HFS was introduced to overcome some of the performance problems that arrived with the introduction of larger media, notably hard drives. The main concern was the time needed to display the contents of a folder. Under MFS all of the file and directory listing information was stored in a single file, which the system had to search to build a list of the files stored in a particular folder. This worked well with a system with a few hundred kilobytes of storage and perhaps a hundred files, but as the systems grew into megabytes and thousands of files, the performance degraded rapidly.
The solution was to replace MFS's directory structure with one more suitable to larger file systems. HFS replaced the flat table structure with the "Catalog File" which uses a B-tree structure that could be searched very quickly regardless of size. HFS also re-designed various structures to be able to hold larger numbers, 16-bit integers being replaced by 32-bit almost universally. Oddly, one of the few places this "upsizing" did not take place was the file directory itself, which limits HFS to a total of 65,535 files on each logical disk.
While HFS is a proprietary file system-format, it is well-documented, so there are usually solutions available to access HFS formatted disks from most modern operating systems.
Apple introduced HFS out of necessity with its first 20 MB hard disk offering for the Macintosh in September 1985, where it was loaded into RAM from a MFS floppy disk on boot using a patch file ("Hard Disk 20"). However, HFS was not widely introduced until it was included in the 128K ROM that debuted with the Macintosh Plus in January 1986 along with the larger 800 KB floppy disk drive for the Macintosh that also used HFS. The introduction of HFS was the first advancement by Apple to leave a Macintosh computer model behind: the original 128K Macintosh, which lacked sufficient memory to load the HFS code and was promptly discontinued.
In 1998, Apple introduced HFS Plus to address inefficient allocation of disk space in HFS and to add other improvements. HFS is still supported by current versions of Mac OS, but starting with OS X, an HFS volume cannot be used for booting, and beginning with OS X 10.6 (Snow Leopard), HFS volumes are read-only and cannot be created or updated.
Design.
A storage volume is inherently divided into "logical blocks" of 512 bytes. The Hierarchical File System groups these logical blocks into "allocation blocks", which can contain one or more logical blocks, depending on the total size of the volume. HFS uses a 16-bit value to address allocation blocks, limiting the number of allocation blocks to 65,535 (216-1).
Five structures make up an HFS volume:
Limitations.
The Catalog File, which stores all the file and directory records in a single data structure, results in performance problems when the system allows multitasking, as only one program can write to this structure at a time, meaning that many programs may be waiting in queue due to one program "hogging" the system. It is also a serious reliability concern, as damage to this file can destroy the entire file system. This contrasts with other file systems that store file and directory records in separate structures (such as DOS's FAT file system or the Unix File System), where having structure distributed across the disk means that damaging a single directory is generally non-fatal and the data may possibly be re-constructed with data held in the non-damaged portions.
Additionally, the limit of 65,535 allocation blocks resulted in files having a "minimum" size equivalent 1/65,535th the size of the disk. Thus, any given volume, no matter its size, could only store a maximum of 65,535 files. Moreover, any file would be allocated more space than it actually needed, up to the allocation block size. When disks were small, this was of little consequence, because the individual allocation block size was trivial, but as disks started to approach the 1 GB mark, the smallest amount of space that any file could occupy (a single allocation block) became excessively large, wasting significant amounts of disk space. For example, on a 1 GB disk, the allocation block size under HFS is 16 KB, so even a 1 byte file would take up 16 KB of disk space. This situation was less of a problem for users having large files (such as pictures, databases or audio) because these larger files wasted less space as a percentage of their file size. Users with many small files, on the other hand, could lose a copious amount of space due to large allocation block size. This made partitioning disks into smaller logical volumes very appealing for Mac users, because small documents stored on a smaller volume would take up much less space than if they resided on a large partition. The same problem existed in the FAT16 file system.
HFS saves the case of a file that is created or renamed but is case-insensitive in operation.

</doc>
<doc id="55348" url="https://en.wikipedia.org/wiki?curid=55348" title="Elevator music">
Elevator music

Elevator music (also known as Muzak, piped music, weather music, or lift music) refers to a type of popular music, often instrumental, that is commonly played through speakers in elevators, shopping malls, grocery stores, department stores, telephone systems (while the caller is on hold), cruise ships, airliners (during flight after taking off), hotels, airports, business offices, restaurants, bars, hospitals, as well as electronic program guides, weather forecasts, television testcards, and some arcade game venues. The term is also frequently applied as a generic term for any form of easy listening, smooth jazz, or middle of the road music, or to the type of recordings commonly heard on "beautiful music" radio stations.
Elevator music is typically set to a very simple melody so that it can be unobtrusively looped back to the beginning. The dynamic range is also normally reduced, so that the highs and lows do not distract listeners. In a mall or shopping center, elevator music of a specific type has been found to have a psychological effect: slower, more relaxed music tends to make people slow down and browse longer. Elevator music may also be preferred over broadcast radio stations because of the lack of lyrics and commercial interruptions.
This style of music is sometimes used to comedic effect in mass media such as film, where intense or dramatic scenes may be interrupted or interspersed with such anodyne music while characters use an elevator (e.g. "The SpongeBob SquarePants Movie", "The Blues Brothers", "Dawn of the Dead", "Mr. and Mrs. Smith", ', ', "Spider-Man 2", and "Night at the Museum"). Some video games have used elevator music for comedic effect, e.g. ' where a few elevator music-themed tracks are accessible on the in-game iPod, as well as ', which occasionally plays elevator music when in elevators during stages.
Muzak was a major supplier of business background music, and was the best known such supplier for years. Ironically, while its name is commonly associated with elevator music in the public mind, that was never one of the company's offerings. Since 1997, Muzak has used original artists for its music source, except on the Environmental channel.
Notable musicians.
Various songs of popular instrumental musicians are sometimes also used as an elevator music in most public places today.

</doc>
<doc id="55349" url="https://en.wikipedia.org/wiki?curid=55349" title="RAM drive">
RAM drive

A RAM drive (also called a RAM disk) is a block of random-access memory (primary storage or volatile memory) that a computer's software is treating as if the memory were a disk drive (secondary storage). It is sometimes referred to as a virtual RAM drive or software RAM drive to distinguish it from a hardware RAM drive that uses separate hardware containing RAM, which is a type of battery-backed solid-state drive.
Performance.
The performance of a RAM drive is in general orders of magnitude faster than other forms of storage media, such as an SSD, hard drive, tape drive, or optical drive. This performance gain is due to multiple factors, including access time, maximum throughput, and type of file system.
File access time is greatly reduced since a RAM drive is solid state (no mechanical parts). A physical hard drive or optical media, such as CD-ROM, DVD, and Blu-ray must move a head or optical eye into position and tape drives must wind or rewind to a particular position on the media before reading or writing can occur. RAM drives can access data with only the memory address of a given file, with no movement, alignment or positioning necessary.
Second, the maximum throughput of a RAM drive is limited by the speed of the RAM, the data bus, and the CPU of the computer. Other forms of storage media are further limited by the speed of the storage bus, such as IDE (PATA), SATA, USB or Firewire. Compounding this limitation is the speed of the actual mechanics of the drive motors, heads, or eyes.
Third, the file system in use, such as NTFS, HFS, UFS, ext2, etc., uses extra accesses, reads and writes to the drive, which although small, can add up quickly, especially in the event of many small files vs. few larger files (temporary internet folders, web caches, etc.).
Because the storage is in RAM, it is volatile memory, which means it will be lost in the event of power loss, whether intentional (computer reboot or shutdown) or accidental (power failure or system crash). This is, in general, a weakness (the data must periodically be backed up to a persistent-storage medium to avoid loss), but is sometimes desirable: for example, when working with a decrypted copy of an encrypted file.
In many cases, the data stored on the RAM drive is created from data permanently stored elsewhere, for faster access, and is re-created on the RAM drive when the system reboots.
Apart from the risk of data loss, the major limitation of RAM drives is their limited capacity, which is constrained by the amount of RAM within the machine. Multi-terabyte-capacity persistent storage has become commoditized as of 2012, whereas RAM is still measured in gigabytes.
RAM drives use the normal RAM in main memory as if it were a partition on a hard drive rather than actually accessing the data bus normally used for secondary storage. Though RAM drives can often be supported directly from the operating system via special mechanisms in the operating system kernel, it is possible to also create and manage a RAM drive by an application. Usually no battery backup is needed due to the temporary nature of the information stored in the RAM drive, but an uninterrupted power supply can keep the entire system running during a power outage, if necessary.
Some RAM drives use a compressed file system such as cramfs to allow compressed data to be accessed on the fly, without decompressing it first. This is convenient because RAM drives are often small due to the higher price per byte than conventional hard drive storage.
History and operating system specifics.
The first software RAM drive for microcomputers was invented and written by Jerry Karlin in the UK in 1979/80. The software, known as the Silicon Disk System was further developed into a commercial product and marketed by JK Systems Research which became Microcosm Research Ltd when the company was joined by Peter Cheesewright of Microcosm Ltd. The idea was to enable the early microcomputers to use more RAM than the CPU could directly address. Making bank-switched RAM behave like a disk drive was much faster than the disk drives - especially in those days before hard drives were readily available on such machines.
The Silicon Disk was launched in 1980, initially for the CP/M operating system and later for MS-DOS. Due to the limitations in memory addressing on Apple II series and Commodore computers, a RAM drive was also a popular application on Commodore 64 and Commodore 128 systems with RAM Expansion Units and on Apple II series computers with more than 64kB of RAM. Apple Computer supported a software RAM drive natively in ProDOS: on systems with 128kB or more of RAM, ProDOS would automatically allocate a RAM drive named /RAM.
IBM added a RAM drive named VDISK.SYS to PC DOS (version 3.0) in August 1984, which was the first DOS component to use extended memory. VDISK.SYS was not available in Microsoft's MS-DOS as it, unlike most components of early versions of PC DOS, was written by IBM. Microsoft included the similar program RAMDRIVE.SYS in MS-DOS 3.2 (released in 1986), which could also use expanded memory. It was discontinued in Windows 7. DR-DOS and the DR family of multi-user operating systems also came with a RAM disk named VDISK.SYS. In Multiuser DOS, the RAM disk defaults to the drive letter M: (for memory drive). AmigaOS has had a built in RAM drive since the release of version 1.1 in 1985 and still has it in AmigaOS 4.1 (2010). Apple Computer added the functionality to the Apple Macintosh with System 7's Memory control panel in 1991, and kept the feature through the life of Mac OS 9. Mac OS X users can use the hdid, newfs (or newfs hfs) and mount utilities to create, format and mount a RAM drive.
A RAM drive innovation introduced in 1986 but made generally available in 1987 by Perry Kivolowitz for AmigaOS was the ability of the RAM drive to survive most crashes and reboots. Called the ASDG Recoverable Ram Disk, the device survived reboots by allocating memory dynamically in the reverse order of default memory allocation (a feature supported by the underlying OS) so as to reduce memory fragmentation. A "super-block" was written with a unique signature which could be located in memory upon reboot. The super-block, and all other RRD disk "blocks" maintained check sums to enable the invalidation of the disk if corruption was detected. At first, the ASDG RRD was locked to ASDG memory boards and used as a selling feature. Later, the ASDG RRD was made available as shareware carrying a suggested donation of 10 dollars. The shareware version appeared on Fred Fish Disks 58 and 241. AmigaOS itself would gain a Recoverable Ram Disk (called "RAD") in version 1.3.
Many Unix and Unix-like systems provide some form of RAM drive functionality, such as /dev/ram on Linux. RAM drives are particularly useful in high-performance, low-resource applications for which Unix-like operating systems are sometimes configured. There are also a few specialized "ultra-lightweight" Linux distributions which are designed to boot from removable media and stored in a ramdisk for the entire session.

</doc>
<doc id="55351" url="https://en.wikipedia.org/wiki?curid=55351" title="Jeet Kune Do">
Jeet Kune Do

Jeet Kune Do, abbreviated JKD, is an eclectic and hybrid style fighting art heavily influenced by the philosophy of martial artist Bruce Lee, who founded the system on July 9, 1967, referred to it as "non-classical", suggesting that JKD is a form of Chinese Kung Fu, yet without form. Unlike more traditional martial arts, Jeet Kune Do is not fixed or patterned, and is a philosophy with guiding thoughts. It was named for the Wing Chun concept of interception or attacking while one's opponent is about to attack. Jeet Kune Do practitioners believe in minimal movements with maximum effects and extreme speed. The system works by using different "tools" for different situations, where the situations are divided into ranges, which are kicking, punching, trapping, and grappling, where martial artists use techniques to flow smoothly between them.
In the screenplay of the 1973 Warner Brothers film, "Enter the Dragon", when Lee is asked, "What's your style?" Lee replied, "My style?...You can call it the art of fighting without fighting."
The name Jeet Kune Do was often said by Lee to be just a name, and he often referred to it as "the art of expressing the human body" in his writings and in interviews. Through his studies Lee came to believe that styles had become too rigid and unrealistic. He called martial art competitions of the day "dry land swimming". He believed real combat was spontaneous, and a martial artist cannot predict it, but only react to it, and a good martial artist should "be like water"—move fluidly without hesitation.
On January 10-11, 1996, the Bruce Lee Foundation decided to use the name Jun Fan Jeet Kune Do () to refer to the martial arts system which Lee founded; "Jun Fan" being Lee's Chinese given name.
System and philosophy.
Lee's philosophy.
Originally, when Lee began researching various fighting styles, he called it Jun Fan Gung Fu. Not wanting to create another style which would share the limitations that all styles had, he instead described the process which he used to create it:
JKD as it survives since then—if one views it "refined" as a product, not a process—is what was left at the time of Lee's death. It is the result of the lifelong martial arts development process Lee went through. Lee stated his concept does not add more and more things on top of each other to form a system, but rather selects the best thereof. The metaphor Lee borrowed from Chan Buddhism was of constantly filling a cup with water, and then emptying it, used for describing Lee's philosophy of "casting off what is useless". He used the sculptor's mentality of beginning with a lump of clay and removing the material which constituted the "unessentials"; the end result was what he considered to be the bare combat essentials, or JKD. The dominant or strongest hand should be in the lead because it would perform a greater percentage of the work. Lee minimized the use of other stances except when circumstances warranted such actions.
Although the On-Guard position is a formidable overall stance, it is by no means the only one. He acknowledged there were times when other positions should be used. Lee felt the dynamic property of JKD was what enabled its practitioners to adapt to the constant changes and fluctuations of live combat. He believed these decisions should be made within the context of "real combat" and/or "all out sparring" and that it was only in this environment that a practitioner could actually deem a technique worthy of adoption.
Lee believed that real combat was alive and dynamic. Circumstances in a fight change from millisecond to millisecond. Thus, pre-arranged patterns and techniques are not adequate in dealing with such a changing situation. As an antidote to this line of thought, Lee once wrote an epitaph which read: 'In memory of a once fluid man, crammed and distorted by the classical mess.' The "classical mess" in this instance was what Lee thought of the "not too alive way of the classical kung fu styles".
Principles.
The following are principles that Lee incorporated into Jeet Kune Do. Lee felt these were universal combat truths that were self-evident, and would lead to combat success if followed. Familiarity with each of the "Four ranges of combat", in particular, is thought to be instrumental in becoming a "total" martial artist.
JKD teaches that the best defense is a strong offense, hence the principle of an "intercepting fist". For someone to attack another hand-to-hand, the attacker must approach the target. This provides an opportunity for the attacked person to "intercept" the attacking movement. The principle of interception may be applied to more than intercepting physical attacks; non-verbal cues (subtle movements that an opponent may be unaware of) may also be perceived or "intercepted", and thus be used to one's advantage.
The "Five ways of attack", categories which help JKD practitioners organize their fighting repertoire, comprise the offensive teachings of JKD. The concepts of "Stop hits & stop kicks", and "Simultaneous parrying & punching", based on the concept of single fluid motions which attack while defending (in systems such as Épée fencing and Wing Chun), compose the defensive teachings of JKD. These concepts were modified for unarmed combat and implemented into the JKD framework by Lee to complement the principle of interception.
Straight lead.
Lee felt that the straight lead was the most integral part of Jeet Kune Do punching, as he stated, "The leading straight punch is the backbone of all punching in Jeet Kune Do." The straight lead is not a power strike but a strike formulated for speed. The straight lead should always be held loosely with a slight motion, as this adds to its speed and makes it more difficult to see and block. The strike is not only the fastest punch in JKD, but also the most accurate. The speed is attributed to the fact that the fist is held out slightly making it closer to the target and its accuracy is gained from the punch being thrown straight forward from one's centerline. The straight lead should be held and thrown loosely and easily, tightening only upon impact, adding to one's punch. The straight lead punch can be thrown from multiple angles and levels.
Non-telegraphed punch.
Lee felt that explosive attacks with no telegraphing signs of intention were best. He argued that the attacks should catch the opponent off-guard, throwing them off their balance and leaving them unable to defend against further attacks. "The concept behind this is that when you initiate your punch without any forewarning, such as tensing your shoulders or moving your foot or body, the opponent will not have enough time to react," Lee wrote. The key is that one must keep one's body and arms loose, weaving one's arms slightly and only becoming tense upon impact. Lee wanted no wind-up movements or "get ready poses" to prelude any JKD attacks. Lee explained that any twitches or slight movements before striking should be avoided as they will give the opponent signs or hints as to what is being planned and then they will be able to strike first while one is preparing an attack. Consequently, non-telegraphed movement is an essential part of Jeet Kune Do philosophy.
"Be Like Water".
Lee emphasized that every situation, in fighting or in everyday life, is varied. To obtain victory, therefore, it is essential not to be rigid, but to be fluid and able to adapt to any situation. He compared it to being like water: "Empty your mind, be formless, shapeless, like water. If you put water into a cup, it becomes the cup. You put water into a bottle and it becomes the bottle. You put it in a teapot it becomes the teapot. That water can flow, or it can crash. Be water, my friend." Lee's theory behind this was that one must be able to function in any scenario one is thrown into and should react accordingly. One should know when to speed up or slow down, when to expand and when to contract, and when to remain flowing and when to crash. It is the awareness that both life and fighting can be shapeless and ever changing that allows one to be able to adapt to those changes instantaneously and bring forth the appropriate solution. Lee did not believe in "styles" and felt that every person and every situation is different and not everyone fits into a mold; one must remain flexible in order to obtain new knowledge and victory in both life and combat. One must never become stagnant in the mind or method, always evolving and moving towards improving oneself.
Economy of motion.
Jeet Kune Do seeks to waste no time or movement, teaching that the simplest things work best, as in Wing Chun. Economy of motion is the principle by which JKD practitioners achieve:
This is meant to help a practitioner conserve both energy and time, two crucial components in a physical confrontation. Maximized force seeks to end the battle quickly due to the amount of damage inflicted upon the opponent. Rapidity aims to reach the target before the opponent can react, which is half-beat faster timing, as taught in Wing Chun and Western boxing. Learned techniques are utilized in JKD to apply these principles to a variety of situations.
Stop hits.
"When the distance is wide, the attacking opponent requires some sort of preparation. Therefore, attack him on his preparation of attack." "To reach me, you must move to me. Your attack offers me an opportunity to intercept you." This means intercepting an opponent's attack with an attack of one's own instead of simply blocking it. It is for this concept Jeet Kune Do is named. JKD practitioners believe that this is the most difficult defensive skill to develop. This strategy is a feature of some traditional Chinese martial arts as Wing Chun, as well as an essential component of European Épée Fencing. Stop hits and kicks utilize the principle of economy of motion by combining attack and defense into one movement, thus minimizing the "time" element.
Simultaneous parrying and punching.
When confronting an incoming attack, the attack is parried or deflected, and a counterattack is delivered at the same time. This is not as advanced as a stop hit but more effective than blocking and counterattacking in sequence. This is practiced by some Chinese martial arts such as Wing Chun, and it is also known in Krav Maga as "bursting". Simultaneous parrying & punching utilizes the principle of economy of motion by combining attack and defense into one movement, thus minimizing the "time" element and maximising the "energy" element. Efficiency is gained by utilizing a parry rather than a block. By definition a "block" stops an attack, whereas a parry merely re-directs it. Redirection has two advantages, first that it requires less energy to execute and second that it utilizes the opponent's energy against them by creating an imbalance. Efficiency is gained in that the opponent has less time to react to an incoming attack, since they are still nullifying the original attack.
Low kicks.
JKD practitioners believe they should direct their kicks to their opponent's shins, knees, thighs, and midsection, as in Wing Chun. These targets are the closest to the foot, provide more stability and are more difficult to defend against. Maintaining low kicks utilizes the principle of economy of motion by reducing the distance a kick must travel, thus minimizing the "time" element. However, as with all other JKD principles nothing is "written in stone". If a target of opportunity presents itself, even a target above the waist, one could take advantage and not be hampered by this principle.
Four ranges of combat.
Jeet Kune Do students train in each of the aforementioned ranges equally. According to Lee, this range of training serves to differentiate JKD from other martial arts. Lee stated that most but not all traditional martial arts systems specialize in training at one or two ranges. Lee's theories have been especially influential and substantiated in the field of mixed martial arts, as the MMA Phases of Combat are essentially the same concept as the JKD combat ranges. As a historic note, the ranges in JKD have evolved over time. Initially the ranges were categorized as short or close, medium, and long range. These terms proved ambiguous and eventually evolved into their more descriptive forms, although some may still prefer the original three categories.
Five ways of attack.
The original five ways of attack are:
Centerline.
The three guidelines for centerline are:
This notion is closely related to maintaining control of the center squares in the strategic game chess. The concept is naturally present in xiangqi (Chinese chess), where an "X" is drawn on the game board, in front of both players' general and advisors.
Combat realism.
One of the premises that Lee incorporated in Jeet Kune Do was "combat realism". He insisted that martial arts techniques should be incorporated based upon their effectiveness in real combat situations. This would differentiate JKD from other systems where there was an emphasis on "flowery technique", as Lee would put it. Lee claimed that flashy "flowery techniques" would arguably "look good" but were often not practical or would prove ineffective in street survival and self-defense situations. This premise would differentiate JKD from other "sport"-oriented martial arts systems that were geared towards "tournament" or "point systems". Lee felt that these systems were "artificial" and fooled their practitioners into a false sense of true martial skill. Lee felt that because these systems favored a "sports" approach they incorporated too many rule sets that would ultimately handicap a practitioner in self-defense situations. He felt that this approach to martial arts became a "game of tag" which would lead to bad habits such as pulling punches and other attacks; this would again lead to disastrous consequences in real world situations.
Another aspect of realistic martial arts training fundamental to JKD is what Lee referred to as "aliveness". This is the concept of training techniques with an unwilling assistant who offers resistance. Lee made a reference to this concept in his famous quote "Boards don't hit back!" Because of this perspective of realism and aliveness, Lee utilized safety gear from various other contact sports to allow him to spar with opponents "full out". This approach to training allowed practitioners to come as close as possible to real combat situations with a high degree of safety.
Country of Origin.
Jeet Kune Do is and was founded in Seattle, Washington, U.S.A. in 1964 (Jun Fan Gung Fu), until July 9, 1967 the term Jeet Kune Do was formed by both Bruce Lee and Dan Inosanto
Jeet Kune Do practitioners and appearances in popular culture.
Video games.
Many video games characters that utilize Jeet Kune Do or a style based on it:

</doc>
<doc id="55356" url="https://en.wikipedia.org/wiki?curid=55356" title="Asian">
Asian

Asian refers to anything related to the continent of Asia, especially Asian people. 
Asian may also refer to:

</doc>
<doc id="55357" url="https://en.wikipedia.org/wiki?curid=55357" title="Dragon (magazine)">
Dragon (magazine)

Dragon is one of the two official magazines for source material for the "Dungeons & Dragons" role-playing game and associated products; "Dungeon" is the other. TSR, Inc. originally launched the monthly printed magazine in 1976 to succeed the company's earlier publication, "The Strategic Review". The final printed issue was #359 in September 2007. Shortly after the last print issue shipped in mid-August 2007, Wizards of the Coast (part of Hasbro, Inc.), the publication's current intellectual property rightsholder, relaunched "Dragon" as an online magazine, continuing on the numbering of the print edition. The last published issue was No. 430 in December 2013. 
A digital publication called "Dragon+", which replaces the "Dragon" magazine, launched in 2015. It is created by Dialect in collaboration with Wizards of the Coast, and restarted the numbering system for issues at No. 1.
History.
In 1975, TSR, Inc. began publishing "The Strategic Review". At the time, roleplaying games were still seen as a subgenre of the wargaming industry, and the magazine was designed not only to support "Dungeons & Dragons" and TSR's other games, but also to cover wargaming in general. In short order, however, the popularity and growth of "Dungeons & Dragons" made it clear that the game had not only separated itself from its wargaming origins, but had launched an entirely new industry unto itself.
TSR canceled "The Strategic Review" after only seven issues the following year, and replaced it with two magazines, "Little Wars", which covered miniature wargaming, and "The Dragon", which covered role playing games. After twelve issues, "Little Wars" ceased independent publication and issue 13 was published as part of "Dragon" issue 22.
The magazine debuted as The Dragon in June 1976. TSR co-founder Gary Gygax commented years later: "When I decided that "The Strategic Review" was not the right vehicle, hired Tim Kask as a magazine editor for Tactical Studies Rules, and named the new publication he was to produce "The Dragon", I thought we would eventually have a great periodical to serve gaming enthusiasts worldwide... At no time did I ever contemplate so great a success or so long a lifespan."
"Dragon" was the launching point for a number of rules, spells, monsters, magic items, and other ideas that were incorporated into later official products of the "Dungeons & Dragons" game. A prime example is the Forgotten Realms campaign setting, which first became known through a series of "Dragon" articles in the 1980s by its creator Ed Greenwood. It subsequently went on to become one of the primary campaign 'worlds' for official "Dungeons and Dragons" products, starting in 1987. The magazine appeared on the cover as simply "Dragon" from July 1980, later changing its name to Dragon Magazine starting November 1987.
Wizards of the Coast purchased TSR and its intellectual properties, including "Dragon" in 1997. Production was then transferred from Wisconsin to Washington state. In 1999, Wizards of the Coast was itself purchased by Hasbro, Inc. "Dragon" remained published by TSR as a subsidiary of WotC starting September 1997, and until January 2000 when WotC became the listed de facto publisher. The magazine changed its name to "Dragon" starting June 2000.
In 1999 a compilation of the first 250 issues was released in PDF format with a special viewer including an article and keyword search in CD-ROM format. Also included were the 7 issues of "The Strategic Review". This compilation is known as the software title "Dragon Magazine Archive". Because of issues raised with the 2001 ruling in Greenberg v. National Geographic regarding the reprint rights of various comic scripts (such as "Wormy", " What's New with Phil & Dixie", "Snarf Quest", and "Knights of the Dinner Table") that had been printed in "Dragon" over the years and Paizo Publishing's policy that creators of comic retain their copyright, the "Dragon Magazine Archive" is out of print and very hard to find.
In 2002, Paizo Publishing acquired the rights to publish both "Dragon" and "Dungeon" under license from Wizards of the Coast. "Dragon" was published by Paizo starting September 2002. It tied "Dragon" more closely to "Dungeon" by including articles supporting and promoting its major multi-issue adventures such as the "Age of Worms" and "Savage Tide". "Class Acts", monthly one or two-page articles offering ideas for developing specific character classes, were also introduced by Paizo.
On April 18, 2007, Wizards of the Coast announced that it would not be renewing Paizo's licenses for "Dragon" and "Dungeon". Scott Rouse, Senior Brand Manager of "Dungeons & Dragons" at Wizards of the Coast stated, "Today the internet is where people go to get this kind of information. By moving to an online model we are using a delivery system that broadens our reach to fans around the world." Paizo published the last print editions of "Dragon" and "Dungeon" magazines for September 2007.
In August 2007, Wizards of the Coast announced plans for the 4th edition of the "Dungeons & Dragons" game. Part of this announcement was that D&D Insider subscriber content would include the new, online versions of both "Dungeon" and "Dragon" magazines along with tools for building campaigns, managing character sheets and other features. In its online form, "Dragon" continues to publish articles aimed at "Dungeons & Dragons" players, with rules data from these articles feeding the D&D Character Builder and other online tools.
In the September 2013 issue of "Dragon" (#427) an article by Wizards of the Coast game designer and editor Chris Perkins announced that both "Dragon" and its sibling publication "Dungeon" would be going on hiatus starting January 2014 pending the release of "Dungeons & Dragons" 5th edition.
Content.
Although "Dragon" provided coverage of fantasy and roleplaying games in general, it became primarily a house organ for role-playing games produced by TSR (or more recently Wizards of the Coast), with a particular focus on "D&D". Its coverage of games created by other companies is often peripheral.
Most of the magazine's articles provided supplementary material for "D&D" including new prestige classes, races, monsters and many other subjects that could be used to enhance a "Dungeons & Dragons" game. A popular long-running column "Sage Advice" offered official answers to "Dungeons & Dragons" questions submitted by players. Other articles provided tips and suggestions for both players and Dungeon Masters (DMs). It sometimes discussed "meta-gaming" issues, such as getting along with fellow players. At the end of its print run, the magazine also featured four comics; Nodwick, Dork Tower, Zogonia and a specialized version of the webcomic "The Order of the Stick". Previous popular gamer-oriented comic strips include Knights of the Dinner Table, "Fineous Fingers", "What's New with Phil & Dixie", "Wormy", "Yamara" and "SnarfQuest".
Many of the gaming world's most famous writers, game designers and artists have published work in the magazine. Through most of its run the magazine frequently published fantasy fiction, either short stories or novel excerpts. After the 1990s, the appearance of fiction stories became relatively rare. One late example was issue #305's featured excerpt from George R.R. Martin's later Hugo-nominated novel "A Feast for Crows". It also featured book reviews of fantasy and science fiction novels, and occasionally of films of particular interest (such as the TV movie of "Mazes and Monsters).
During various Dungeons & Dragons controversies, "Dragon" featured occasional articles from TSR spokespeople discussing issues from their point of view.
A regular feature of "Dragon" for many years was its "Ecology of ..." articles as sometimes discussed by the fictional sage Elminster, in which a particular D&D monster received an in-depth review, explaining how it found food, reproduced, and so forth. Under Paizo's tenure such ecology articles became heavier in "crunch" (game mechanics) as opposed to "fluff" (narrative and description) than previously. The "Dragon" submissions guidelines explicitly stated that Ecology articles "should have a hunter’s guidebook approach, although it should not be written 'in voice'" and further call out the exact format of Ecology articles, leaving less room for artistic license by the author.
In the early 1980s, almost every issue of "Dragon" would contain a role playing adventure, a simple board game, or some kind of special game supplement (such as a cardboard cut-out castle). For instance, Tom Wham's "Snit's Revenge", "The Awful Green Things From Outer Space" and "File 13" all started as supplements within "The Dragon". These bonus features become infrequent after the 1986 launch of "Dungeon" magazine, which published several new "Dungeons & Dragons" adventures in each issue.
During the 1980s, after TSR had purchased Simulations Publications Inc., the magazine had a subsection called "Ares Magazine", based on SPI's magazine of that name, specializing in science fiction and superhero role playing games, with pages marked by a gray border. The content included write-ups for various characters of the Marvel Universe for TSR's "Marvel Super-Heroes".
Special issues.
As noted above "The Dragon" was preceded by seven issues of "The Strategic Review". In the magazine's early years it also published five "Best of" issues, reprinting highly regarded articles from "The Strategic Review" and "The Dragon".
From 1996 to 2001, "Dragon Magazine" published the "Dragon Annual," a thirteenth issue of all new content.
Other releases.
A collection of Dragon was released as the "Dragon Magazine Archive" in 1999. It was released as a CD-ROM for Windows 95/98 or Windows NT with files in Adobe's PDF format. The "Dragon Magazine Archive" was directed by Rob Voce, and published by TSR/Wizards of the Coast. It was reviewed by the online version of "Pyramid" on November 25, 1999. The reviewer felt that the archive was "worth the price", but noted that it was not Macintosh compatible: "This product fails pretty badly in the Mac world. Because the actual archive is in Adobe's PDF format, the files can be read by anyone with a Macintosh and Adobe Acrobat. Unfortunately, the search utilities that make the archive accessible are not available to Mac users."

</doc>
