<doc id="55929" url="https://en.wikipedia.org/wiki?curid=55929" title="Governor-general">
Governor-general

Governor-general or governor general, in modern usage, is the title of an office-holder appointed to represent the monarch of a sovereign state in the governing of an independent realm. Governors-general have also previously been appointed in respect of major colonial states or other territories held by either a monarchy or republic, such as French Indochina.
Current uses.
In modern usage, the term "governor-general" originated in those British colonies which became self-governing within the British Empire. Before World War I, the title was used only in federated colonies in which each of the previously constituent colonies of these federated colonies already had a governor, namely Canada, Australia, and the Union of South Africa. In these cases, the Crown's representative in the federated Dominion was given the superior title of "governor general". The first exception to this rule was New Zealand, which was granted Dominion status in 1907, but it was not until 28 June 1917 that Arthur Foljambe, 2nd Earl of Liverpool, was appointed the first Governor General of New Zealand. Another non-federal state, Newfoundland, was a Dominion for 16 years with the King's representative retaining the title of governor throughout this time.
Since 2016
, the title "governor-general" has been given to all representatives of the sovereign in independent Commonwealth realms. In these cases, the former office of colonial governor was altered (sometimes for the same incumbent) to become governor-general upon independence, as the nature of the office became an entirely independent constitutional representative of the monarch rather than a symbol of previous colonial rule. In these countries the governor-general acts as the monarch's representative, performing the ceremonial and constitutional functions of a head of state.
The only other nation which uses the governor-general designation is Iran, which has no connection with any monarchy or the Commonwealth. In Iran, the provincial authority is headed by a governor general (Persian: استاندار "ostāndār"), who is appointed by the Minister of the Interior.
British colonialism and the governors-general.
Until the 1920s, governors-general were British subjects, appointed on the advice of the British government, who acted as agents of the British government in each Dominion, as well as being representatives of the monarch. As such they notionally held the prerogative powers of the monarch, and also held the executive power of the country to which they were assigned. The governor-general could be instructed by the colonial secretary on the exercise of some of his functions and duties, such as the use or withholding of the Royal Assent from legislation; history shows many examples of governors-general using their prerogative and executive powers. The monarch or imperial government could overrule any governor-general, though this could often be cumbersome, due to remoteness of the territories from London.
The governor-general was also usually the commander-in-chief of the armed forces in his or her territory and, because of the governor-general's control of the military, the post was as much a military appointment as a civil one. The governors-general are entitled to wear a unique uniform, which are not generally worn today. If of the rank of major general, equivalent or above, they were entitled to wear that military uniform.
Modern Commonwealth.
Commonwealth realms.
Following the Imperial Conference, and subsequent issuing of the Balfour Declaration in 1926, the role and responsibilities of the governor-general began to shift, reflecting the increased independence of the Dominions (which were in 1952 renamed Realms; a term which includes the UK itself). As the sovereign came to be regarded as monarch of each territory independently, and, as such, advised only by the ministers of each country in regard to that country's national affairs (as opposed to a single British monarch ruling all the Dominions as a conglomerate and advised only by an imperial parliament), so too did the governor-general become a direct representative of the national monarch only, who no longer answered to the British government. The report resulting from the 1926 Imperial Conference stated: "...it is an essential consequence of the equality of status existing among the members of the British Commonwealth of Nations that the Governor General of a Dominion is the representative of the Crown, holding in all essential respects the same position in relation to the administration of public affairs in the Dominion as is held by His Majesty the King in Great Britain, and that he is not the representative or agent of His Majesty's Government in Great Britain or of any Department of that Government." These concepts were entrenched in legislation with the enactment of the Statute of Westminster in 1931, and governmental relations with the United Kingdom were placed in the hands of a British High Commissioner in each country.
In other words, the political reality of a self-governing Dominion within the British Empire with a governor-general answerable to the sovereign became clear. British interference in the Dominion was not acceptable and independent country status was clearly displayed. Canada, Australia, and New Zealand were clearly not controlled by the United Kingdom. The monarch of these countries (Elizabeth II) is in law Queen of Canada, Queen of Australia, and Queen of New Zealand and only acts on the advice of the ministers in each country and is in no way influenced by the British government. Today, therefore, in former British colonies which are now independent Commonwealth realms, the governor-general is constitutionally the representative of the monarch in his or her state and may exercise the reserve powers of the monarch according to their own constitutional authority. The governor-general, however, is still appointed by the monarch and takes an oath of allegiance to the monarch of their own country. Executive authority is also vested in the monarch, though much of it can be exercisable only by the governor-general on behalf of the sovereign of the independent realm. Letters of Credence or Letters of Recall are now sometimes received or issued in the name of the monarch, though, in some countries, such as Canada and Australia, the Letters of Credence and Recall are issued in the name of the governor-general alone.
At diplomatic functions where the governor-general is present, the visiting diplomat or head of state toasts "The King" or "The Queen" of the relevant realm, not the governor-general, with any reference to the governor-general being subsidiary in later toasts if featuring at all, and will involve a toast to them by name, not office. (E.g., "Mr. and Mrs. Smith," not "Her Excellency, the Governor-General." Sometimes a toast might be made using name and office, e.g., "Governor-General Smith.")
Except in rare cases, the governor-general only acts in accordance with constitutional convention and upon the advice of the national prime minister. The governor-general is still the local representative of the sovereign and performs the same duties as they carried out historically, though their role is almost purely ceremonial. Rare and controversial exceptions occurred in 1926, when Canadian Governor General the Viscount Byng of Vimy refused Prime Minister Mackenzie King's request for a dissolution of parliament; in 1953 and 1954 when the Governor-General of Pakistan, Ghulam Mohammad, staged a constitutional coup against the Prime Minister and then the Constituent Assembly; and in 1975, when the Governor-General of Australia, Sir John Kerr, dismissed the Prime Minister, Gough Whitlam. In some realms, the monarch could in principle overrule a governor-general, but this has not happened in modern times.
In Australia the present Queen is generally assumed to be head of state, since the governor-general and the state governors are defined as her "representatives". However, since the governor-general performs almost all national regal functions, the governor-general has occasionally been referred to as head of state in political and media discussion. To a lesser extent, uncertainty has been expressed in Canada as to which officeholder—the monarch, the governor general, or both—can be considered the head of state. 
The governor-general is usually a person with a distinguished record of public service, often a retired politician, judge or military commander; but some countries have also appointed prominent academics, members of the clergy, philanthropists, or figures from the news media to the office.
Traditionally, the governor-general's official attire was a unique uniform, but this practice has been abandoned except on occasions when it is appropriate to be worn. In South Africa, the Governor-General of the Union of South Africa nominated by the Afrikaner Nationalist government chose not to wear uniform on any occasion. Most governors-general continue to wear appropriate medals on their clothing when required.
The governor-general's official residence is usually called "Government House". The Governor-General of the Irish Free State resided in the then Viceregal Lodge in Phoenix Park, Dublin, but the government of Éamon de Valera sought to downgrade the office and the last governor-general, Domhnall Ua Buachalla, did not reside there. The office was abolished there in 1936.
In most Commonwealth realms, the flag of the governor-general has been the standard pattern of a blue field with the Royal Crest (a lion standing on a crown) above a scroll with the name of the jurisdiction. In Canada, however, this was replaced with a crowned lion clasping a maple leaf. In the Solomon Islands, the scroll was replaced with a two-headed frigate bird motif, while in Fiji, the former Governor General's flag featured a whale's tooth. In New Zealand, the flag was replaced in 2008 with the shield of the coat of arms of New Zealand surmounted by a crown on a blue field.
Governors-general are accorded the style of "His/Her Excellency". This style is also extended to their spouses, whether female or male.
Appointment.
Until the 1920s, the Governors General were British, and appointed on the advice of the British Government.
Following the changes to the structure of the Commonwealth in the late 1920s, in 1929, the Australian Prime Minister James Scullin established the right of a Dominion prime minister to advise the monarch directly on the appointment of a governor-general, by insisting that his choice (Sir Isaac Isaacs (an Australian) prevail over the recommendation of the British government. The convention was gradually established throughout the Commonwealth that the governor-general would be a citizen of the country concerned, and would be appointed on the advice of the government of that country, with no input from the British government; Governor General of Canada since 1952 and Governor General of New Zealand since 1967. Since 1931 as each former Dominion has patriated its constitution from the UK, the convention has become law and no government of any realm can advise the Monarch on any matter pertaining to another realm, including the appointment of a governor-general. The monarch appoints a governor-general (in Canada: "governor general") as a personal representative only on the advice of the prime minister of each realm. The Governor General of Canada is appointed by the Queen of Canada on the advice of the Canadian prime minister, the Governor-General of Australia is appointed by the Queen of Australia on the advice of the Australian prime minister, and the Governor-General of New Zealand is appointed by the Queen of New Zealand on the advice of the New Zealand prime minister. In Papua New Guinea and the Solomon Islands, the Prime Minister's advice is based on the result of a vote in the national parliament.
The formalities for appointing governors-general are not the same in all the realms. For example: When appointed, a Governor-General of Australia issues a proclamation in his own name, countersigned by the head of government and under the Great Seal of Australia, formally announcing that he has been appointed by the monarch's commission, previously issued also under the Great Seal of Australia. The practice in Canada is to include in the governor general's proclamation of appointment, issued under the Great Seal of Canada, the monarch's commission naming the governor general as Commander-in-Chief of the Canadian Forces Also dissimilar among the realms are the powers of governors-general. The Australian constitution provides that the command-in-chief of the Defence Forces is vested in the governor-general,
Other attributes.
Different realms have different constitutional arrangements governing who acts in place of the governor-general in the event of his or her death, resignation, or incapacity.
Former British colonies.
The title has been used in many British colonial entities that either no longer exist or are now independent countries.
In the Americas.
Dominica became a republic on independence in 1978, with a ceremonial President as head of state.
In Africa.
Zambia and the Seychelles became republics within the Commonwealth on independence.
In Europe.
Cyprus became a republic on independence.
In Oceania.
Kiribati and Vanuatu became republics on independence.
Other colonial and similar usage.
France.
The equivalent word in French is "gouverneur général", used in the following colonies:
Furthermore, in Napoleonic Europe successive French Governors-general were appointed by Napoleon I in:
Greece.
The Balkan Wars of 1912–13 led to the Greek acquisition of the so-called "New Lands" (Epirus, Macedonia, Crete and the islands of the eastern Aegean), almost doubling the country's territory. Instead of fully incorporating these new lands into Greece by dividing them into prefectures, the Ottoman administrative system continued in existence for a while, and Law ΔΡΛΔ΄ of 1913 established five governorates-general (Γενικαὶ Διοικήσεις, sing. Γενική Διοίκησις): Epirus, Macedonia, Crete, Aegean and Samos–Ikaria. The governors-general had wide-ranging authority in their territories, and were almost autonomous of the government in Athens.
Law 524 in 1914 abolished the governorates-general and divided the New Lands into regular prefectures, but in 1918 Law 1149 re-instated them as a superordinate administrative level above the prefectures, with Macedonia now divided in two governorates-general, those of Thessaloniki and Kozani–Florina. The governors-general of Thessaloniki, Crete and Epirus were also given ministerial rank. To these was added the Governorate-General of Thrace in 1920–22, comprising Western Thrace and Eastern Thrace (returned to Turkey in the Armistice of Mudanya in 1922). The extensive but hitherto legally rather undefined powers of the governors-general created friction and confusion with other government branches, until their remit was exactly delineated in 1925. The governorates-general, except for that of Thessaloniki, were abolished in 1928, but re-established in December 1929—for Crete, Epirus, Thrace, and Macedonia—and delegated practically all ministerial authorities for their respective areas. Over the next decade, however, in a see-saw of legislative measures that in turns gave and took away authority, they gradually lost most of their powers in favour of the prefectures and the central government in Athens.
Following liberation from the Axis occupation, in 1945 the Governorate-General of Northern Greece was established, initially with subordinate governorates for West Macedonia, Central Macedonia, East Macedonia, and Thrace, the first three of which were then grouped anew into a new Governorate-General of Macedonia, albeit still subject to the Governorate-General of Northern Greece. This awkward arrangement lasted until 1950, when the administration of Macedonia was streamlined, the junior governorates abolished and only the Governorate-General of Northern Greece retained. Finally, in 1955, the Governorate-General of Northern Greece was transformed into the Ministry of Northern Greece, and all other governorates-general elsewhere in Greece were abolished.
Netherlands.
From 1691 to 1948 the Dutch appointed a gouverneur-generaal ("governor-general") to govern the Netherlands East Indies, now Indonesia.
While in the Caribbean, various other titles were used, Curaçao had three Governors-General between 1816 and 1820:
Portugal.
The equivalent word in Portuguese is "governador-geral". This title was only used for the governors of the major colonies, indicating that they had, under their authority, several subordinate governors. In most of the colonies, lower titles, mainly "governador" (governor) or formerly captain-major ("capitão-mor"), prevailed
Philippines.
The Philippines from the 16th through the 20th century had a series of governors-general during the Spanish and American colonial periods, as well as the Japanese Occupation of the Philippines during World War II
Spain.
From 21 November 1564 the Spanish East Indies had a governor-general, which was under the Viceroy of New Spain based in Mexico. After the successful Mexican War of Independence in 1821, the governor-general reported directly to Spain.
United States of America.
From 1899 to 1935 under the Insular Government, the Philippines was administered by a series of governors-general, first military and then civilian, appointed by the United States Federal Government.
Note.
In Canada the title "Governor General" is always used unhyphenated. In Australia and New Zealand, the term is always hyphenated.

</doc>
<doc id="55931" url="https://en.wikipedia.org/wiki?curid=55931" title="Chronology">
Chronology

Chronology (from Latin "chronologia", from Ancient Greek , "chronos", "time"; and , "-logia") is the science of arranging events in their order of occurrence in time. Consider, for example, the use of a timeline or sequence of events. It is also "the determination of the actual temporal sequence of past events".
Chronology is part of periodization. It is also part of the discipline of history, including earth history, the earth sciences, and study of the geologic time scale.
Related fields.
Chronology is the science of locating historical events in time. It relies upon chronometry, which is also known as timekeeping, and historiography, which examines the writing of history and the use of historical methods. Radiocarbon dating estimates the age of formerly living things by measuring the proportion of carbon-14 isotope in their carbon content. Dendrochronology estimates the age of trees by correlation of the various growth rings in their wood to known year-by-year reference sequences in the region to reflect year-to-year climatic variation. Dendrochronology is used in turn as a calibration reference for radiocarbon dating curves.
Calendar and era.
The familiar terms "calendar" and "era" (within the meaning of a coherent system of numbered calendar years) concern two complementary fundamental concepts of chronology. For example, during eight centuries the calendar belonging to the Christian era, which era was taken in use in the 8th century by Bede, was the Julian calendar, but after the year 1582 it was the Gregorian calendar. Dionysius Exiguus (about the year 500) was the founder of that era, which is nowadays the most widespread dating system on earth. An epoch is the date (year usually) when an era begins.
Ab Urbe condita era.
"Ab Urbe condita" is Latin for "from the founding of the City (Rome)", traditionally set in 753 BC. It was used to identify the Roman year by a few Roman historians. Modern historians use it much more frequently than the Romans themselves did; the dominant method of identifying Roman years was to name the two consuls who held office that year. Before the advent of the modern critical edition of historical Roman works, AUC was indiscriminately added to them by earlier editors, making it appear more widely used than it actually was.
It was used systematically for the first time only about the year 400, by the Iberian historian Orosius. Pope Boniface IV, in about the year 600, seems to have been the first who made a connection between these this era and Anno Domini. (AD 1 = AUC 754.)
Astronomical era.
Dionysius Exiguus’ Anno Domini era (which contains only calendar years "AD") was extended by Bede to the complete Christian era (which contains, in addition all calendar years "BC", but no "year zero"). Ten centuries after Bede, the French astronomers Philippe de la Hire (in the year 1702) and Jacques Cassini (in the year 1740), purely to simplify certain calculations, put the Julian Dating System (proposed in the year 1583 by Joseph Scaliger) and with it an astronomical era into use, which contains a leap year zero, which precedes the year 1 (AD).
Prehistoric chronologies.
While of critical importance to the historian, methods of determining chronology are used in most disciplines of science, especially astronomy, geology, paleontology and archaeology.
In the absence of written history, with its chronicles and king lists, late 19th century archaeologists found that they could develop relative chronologies based on pottery techniques and styles. In the field of Egyptology, William Flinders Petrie pioneered sequence dating to penetrate pre-dynastic Neolithic times, using groups of contemporary artefacts deposited together at a single time in graves and working backwards methodically from the earliest historical phases of Egypt. This method of dating is known as seriation.
Known wares discovered at strata in sometimes quite distant sites, the product of trade, helped extend the network of chronologies. Some cultures have retained the name applied to them in reference to characteristic forms, for lack of an idea of what they called themselves: "The Beaker People" in northern Europe during the 3rd millennium BCE, for example. The study of the means of placing pottery and other cultural artifacts into some kind of order proceeds in two phases, classification and typology: Classification creates categories for the purposes of description, and typology seeks to identify and analyse changes that allow artifacts to be placed into sequences.
Laboratory techniques developed particularly after mid-20th century helped constantly revise and refine the chronologies developed for specific cultural areas. Unrelated dating methods help reinforce a chronology, an axiom of corroborative evidence. Ideally, archaeological materials used for dating a site should complement each other and provide a means of cross-checking. Conclusions drawn from just one unsupported technique are usually regarded as unreliable.
Chronological synchronism.
The fundamental problem of chronology is to synchronize events. By synchronizing an event it becomes possible to relate it to the current time and to compare the event to other events. Among historians, a typical need to is to synchronize the reigns of kings and leaders in order to relate the history of one country or region to that of another. For example, the Chronicon of Eusebius (325 A.D.) is one of the major works of historical synchronism. This work has two sections. The first contains narrative chronicles of nine different kingdoms: Chaldean, Assyrian, Median, Lydian, Persian, Hebrew, Greek, Peloponnesian, Asian, and Roman. The second part is a long table synchronizing the events from each of the nine kingdoms in parallel columns. The image to the right shows two pages from the second section.
By comparing the parallel columns, the reader can determine which events were contemporaneous, or how many years separated two different events. To place all the events on the same time scale, Eusebius used an Anno Mundi (A.M.) era, meaning that events were dated from the supposed beginning of the world as computed from the Book of Genesis in the Hebrew Pentateuch. According to the computation Eusebius used, this occurred in 5199 B.C. The Chronicon of Eusebius was widely used in the medieval world to establish the dates and times of historical events. Subsequent chronographers, such as George Syncellus (died circa 811), analyzed and elaborated on the Chronicon by comparing with other chronologies. The last great chronographer was Joseph Justus Scaliger (1540-1609) who reconstructed the lost Chronicon and synchronized all of ancient history in his two major works, "De emendatione temporum" (1583) and "Thesaurus temporum" (1606). Much of modern historical datings and chronology of the ancient world ultimately derives from these two works. Scaliger invented the concept of the Julian Day which is still used as the standard unified scale of time for both historians and astronomers.
In addition to the literary methods of synchronism used by traditional chronographers such as Eusebius, Syncellus and Scaliger, it is possible to synchronize events by archaeological or astronomical means. For example, the Eclipse of Thales, described in the first book of Herodotus can potentially be used to date the Lydian War because the eclipse took place during the middle of an important battle in that war. Likewise, various eclipses and other astronomical events described in ancient records can be used to astronomically synchronize historical events. Another method to synchronize events is the use of archaeological findings, such as pottery, to do sequence dating.
See also.
Aspects and examples of non-chronological story-telling:

</doc>
<doc id="55934" url="https://en.wikipedia.org/wiki?curid=55934" title="Eric Corley">
Eric Corley

Eric Gordon Corley (born December 16, 1959), also frequently referred to by his pen name of Emmanuel Goldstein, is a figure in the hacker community. He directs the non-profit organization 2600 Enterprises, Inc., and publishes a magazine called "". His pseudonym is derived from the fictional opposition leader in George Orwell's dystopian novel Nineteen Eighty-Four.
Corley has also testified before the United States Congress.
Corley is the editor of "The Best of 2600: A Hacker Odyssey" which was released July, 2008. The book consists of articles from the magazine "" set in chronological order to show the evolution of the internet and technology. A follow-up book, "Dear Hacker. Letters to the Editor of 2600", was published in 2010.
Corley is the host of both the weekly radio programs Off the Hook on WBAI-FM and "Off the Wall" on WUSB-FM. While Off The Hook often includes a panel of guests and is frequently centered on technological topics, Off the Wall is usually narrated by Eric Corley himself and has covered a wide range of topics. Off the Hook has been on the air since 1988.
Corley directed the 2001 film Freedom Downtime, a documentary about the incarcerations of Kevin Mitnick and Bernie S that also examines alleged distortions in mainstream media coverage of Mitnick's case.

</doc>
<doc id="55937" url="https://en.wikipedia.org/wiki?curid=55937" title="Glossary of communication disorders">
Glossary of communication disorders

This is a glossary of medical terms related to communications disorders such as blindness and deafness.

</doc>
<doc id="55940" url="https://en.wikipedia.org/wiki?curid=55940" title="Nulla poena sine lege">
Nulla poena sine lege

Nulla poena sine lege (Latin for "no penalty without a law") is a legal principle, requiring that one cannot be punished for doing something that is not prohibited by law. This principle is accepted and codified in modern democratic states as a basic requirement of the rule of law. 
Requirements.
In modern European criminal law, e.g. of the Constitutional Court of Germany, the principle of "nulla poena sine lege" has been found to consist of four separate requirements:
In common law.
One complexity is the lawmaking power of judges under common law. Even in civil law systems that do not admit judge-made law, it is not always clear when the function of interpretation of the criminal law ends and judicial lawmaking begins.
In English criminal law there are offences of common law origin. For example, murder is still a common law offence and lacks a statutory definition. The Homicide Act 1957 did not include a statutory definition of murder (or any other homicidal offense). There was, consequently, the astonishing spectacle of the definition of murder, still a matter of common law, being the subject of no less than six appeals to the House of Lords within the next 40 years ("Director of Public Prosecutions v. Smith" A.C. 290; "Hyam v. Director of Public Prosecutions" [1975 A.C. 55; "Regina v. Cunningham" A.C. 566; "Regina v. Moloney" [1985 A.C. 905; "Regina v. Hancock" A.C. 455; "Regina v. Woollin" [1998 4 A11 E.R. 103 (H.L.)).
In cases of universal jurisdiction.
The question of jurisdiction may sometimes come to contradict this principle. For example, customary international law allows the prosecution of pirates by any country (applying universal jurisdiction), even if they did not commit crimes at the area that falls under this country's law. A similar principle has appeared in the recent decades with regard to crimes of genocide (see genocide as a crime under domestic law); and UN Security Council Resolution 1674 "reaffirms the provisions of paragraphs 138 and 139 of the 2005 World Summit Outcome Document regarding the responsibility to protect populations from genocide, war crimes, ethnic cleansing and crimes against humanity" even if the State in which the population is being assaulted does not recognise these assaults as a breach of domestic law. However, it seems that universal jurisdiction is not to be expanded substantially to other crimes, so as to satisfy "Nulla poena sine lege".
Since the Nuremberg Trials, penal law is taken to include the prohibitions of international criminal law, in addition to those of domestic law. Thus prosecutions have been possible of such individuals as Nazi war criminals and officials of the German Democratic Republic responsible for the Berlin Wall, even though their deeds may have been allowed or even ordered by domestic law. Also, courts when dealing with such cases will tend to look to the letter of the law at the time, even in regimes where the law as it was written was generally disregarded in practice by its own authors.
However, some legal scholars criticize this, because generally, in the legal systems of Continental Europe where the maxim was first developed, "penal law" was taken to mean statutory penal law, so as to create a guarantee to the individual, considered as a fundamental right, that he would not be prosecuted for an action or omission that was not considered a crime according to the statutes passed by the legislators in force at the time of the action or omission, and that only those penalties that were in place when the infringement took place would be applied. Also, even if one considers that certain actions are prohibited under general principles of international law, critics point out that a prohibition in a general principle does not amount to the establishment of a crime, and that the rules of international law also do not stipulate specific penalties for the violations.
In an attempt to address those criticisms, the statute of the recently established International Criminal Court provides for a system in which crimes and penalties are expressly set out in written law, that shall only be applied to future cases.
This principle is enshrined in several national constitutions, and a number of international instruments. See e.g. European Convention on Human Rights, article 7(1); Rome Statute of the International Criminal Court, articles 22 and 23.

</doc>
<doc id="55941" url="https://en.wikipedia.org/wiki?curid=55941" title="Jabber">
Jabber

Jabber may refer to:

</doc>
<doc id="55942" url="https://en.wikipedia.org/wiki?curid=55942" title="Special Air Service">
Special Air Service

The Special Air Service (SAS) is a special forces unit of the British Army. The SAS was founded in 1941 as a regiment, and later reconstituted as a corps in 1950. The unit undertakes a number of roles including covert reconnaissance, counter-terrorism, direct action, hostage rescue and human intelligence gathering.
The corps presently comprises 22 Special Air Service Regiment, the regular component, under the operational command of United Kingdom Special Forces, and 21 (Artists) Special Air Service Regiment (Reserve) and 23 Special Air Service Regiment (Reserve), which are reserve units under the operational command of 1st Intelligence, Surveillance and Reconnaissance Brigade.
The Special Air Service traces its origins to 1941 and the Second World War, and was reformed as part of the Territorial Army in 1947, named the "21st Special Air Service Regiment (Artists Rifles)". 22 Special Air Service Regiment, part of the regular army, later gained fame and recognition worldwide after successfully assaulting the Iranian Embassy in London and rescuing hostages during the 1980 Iranian Embassy siege, lifting the regiment from obscurity outside the military establishment.
History.
The Special Air Service was a unit of the British Army during the Second World War, formed in July 1941 by David Stirling and originally called "L" Detachment, Special Air Service Brigadethe "L" designation and Air Service name being a tie-in to a British disinformation campaign, trying to deceive the Axis into thinking there was a paratrooper regiment with numerous units operating in the area (the real SAS would "prove" to the Axis that the fake one existed). It was conceived as a commando force to operate behind enemy lines in the North African Campaign and initially consisted of five officers and 60 other ranks. Its first mission, in November 1941, was a parachute drop in support of the Operation Crusader offensive. Due to German resistance and adverse weather conditions, the mission was a disaster: 22 men, a third of the unit, were killed or captured. Its second mission was a success: transported by the Long Range Desert Group, it attacked three airfields in Libya, destroying 60 aircraft with the loss of 2 men and 3 Willys MB. In September 1942 it was renamed 1st SAS, consisting at that time of four British squadrons, one Free French, one Greek, and the Folboat Section.
In January 1943, Stirling was captured in Tunisia and Paddy Mayne replaced him as commander. In April 1943, the 1st SAS was reorganised into the Special Raiding Squadron under Mayne's command and the Special Boat Squadron was placed under the command of George Jellicoe. The Special Raiding Squadron fought in Sicily and Italy along with the 2nd SAS, which had been formed in North Africa in 1943 in part by the renaming of the Small Scale Raiding Force. The Special Boat Squadron fought in the Aegean Islands and Dodecanese until the end of the war. In 1944 the SAS Brigade was formed from the British 1st and 2nd SAS, the French 3rd and 4th SAS and the Belgian 5th SAS. It was tasked with parachute operations behind the German lines in France and carried out operations supporting the Allied advance through Belgium, the Netherlands (Operation Pegasus), and eventually into Germany (Operation Archway). As a result of Hitler's issuing of the Commando Order 18 October 1942, the members of the unit faced the additional danger that they would be summarily executed if ever captured by the Germans. In July 1944, following Operation Bulbasket, 34 captured SAS commandos were summarily executed by the Germans. In October 1944, in the aftermath of Operation Loyton 31 captured SAS commandos were summarily executed by the Germans.
Post war.
At the end of the war the British Government, seeing no further need for the force, disbanded it on 8 October 1945. The following year it was decided there was a need for a long-term deep-penetration commando unit, and a new SAS regiment was to be raised as part of the Territorial Army. Ultimately, the Artists Rifles, raised in 1860 and headquartered at Dukes Road, Euston, took on the SAS mantle as 21st SAS Regiment (V) on 1 January 1947.
In 1950, a 21 SAS squadron was raised to fight in the Korean War. After three months of training in England, it was informed that the squadron would no longer be required in Korea and so it instead volunteered to fight in the Malayan Emergency. Upon arrival in Malaya, it came under the command of Mike Calvert who was forming a new unit called the Malayan Scouts (SAS). Calvert had already formed one squadron from 100 volunteers in the Far East, which became A Squadronthe 21 SAS squadron then became B Squadron; and after a recruitment visit to Rhodesia by Calvert, C Squadron was formed from 1,000 Rhodesian volunteers. The Rhodesians returned home after three years service and were replaced by a New Zealand squadron. By this time, the need for a regular army SAS regiment had been recognised; 22 SAS Regiment was formally added to the army list in 1952 and has been based at Hereford since 1960. In 1959 the third regiment, 23 SAS Regiment, was formed by renaming the Reserve Reconnaissance Unit, which had succeeded MI9 and whose members were experts in escape and evasion.
22 SAS Regiment.
Since serving in Malaya, men from the regular army 22 SAS Regiment have taken part in covert reconnaissance and surveillance by patrols and some larger scale raiding missions in Borneo. An operation against communist guerillas included the Battle of Mirbat in the Oman. They have also taken part in operations in the Aden Emergency, Northern Ireland, and Gambia. Their Special projects team assisted the West German counter-terrorism group GSG 9 at Mogadishu. The SAS counter terrorist wing famously took part in a hostage rescue operation during the Iranian Embassy Siege in London. During the Falklands War B squadron were prepared for Operation Mikado before it was subsequently cancelled whilst D and G squadrons were deployed and participated in the raid on Pebble Island. Operation Flavius was a controversial operation in Gibraltar against the Provisional Irish Republican Army (PIRA). 22 SAS also directed NATO aircraft onto Serb positions and hunted war criminals in Bosnia. They were also involved in the Kosovo War helping KLA guerillas behind Serbian lines. According to Albanian sources one SAS sergeant was killed by Serbian special forces.
The Gulf War, in which A, B and D squadrons deployed, was the largest SAS mobilisation since the Second World War, also notable for the failure of the Bravo Two Zero mission. In Sierra Leone it took part in Operation Barras, a hostage rescue operation, to extract members of the Royal Irish Regiment. In the Iraq War, it formed part of Task Force Black and Task Force Knight, with A Squadron 22 SAS being singled out for exceptional service by General Stanley McChrystal, the American commander of NATO forces: during a six-month tour it carried out 175 combat missions. In 2006, members of the SAS were involved in the operation to free peace activists Norman Kember, James Loney and Harmeet Singh Sooden. The three men had been held hostage in Iraq for 118 days during the Christian Peacemaker hostage crisis. Operations against the Taliban in Afghanistan involved soldiers from 21 and 23 SAS Regiments.
Various British newspapers have speculated on the SAS involvement in Operation Ellamy and the 2011 Libyan civil war, the "Daily Telegraph" reports that "defence sources have confirmed that the SAS has been in Libya for several weeks, and played a key role in coordinating the fall of Tripoli." While "The Guardian" reports "They have been acting as forward air controllers – directing pilots to targets – and communicating with NATO operational commanders. They have also been advising rebels on tactics."
Members of the Special Air Service were deployed to Northern Iraq in late August 2014, and according to former MI6 chief Richard Barrett would also be sent to Syria, tasked with trying to track down the Islamic State terrorist group that the press labeled the Beatles. In October 2014, the SAS began executing raids against ISIS supply lines in western Iraq, using helicopters to drop light vehicles manned by sniper squads. It has been claimed that the SAS have killed up to eight ISIS fighters per day since the raids began.
In recent years SAS officers have risen to the highest ranks in the British Army. General Peter de la Billière was the Commander-in-Chief of the British forces in the 1990 Gulf War. General Michael Rose became commander of the United Nations Protection Force in Bosnia in 1994. In 1997 General Charles Guthrie became Chief of the Defence Staff the head of the British Armed Forces. Lieutenant-General Cedric Delves was appointed Commander of the Field Army and Deputy Commander in Chief NATO Regional Headquarters Allied Forces North in 2002–2003.
Influence on other special forces.
Following the post-war reconstitution of the Special Air Service, other countries in the Commonwealth recognised their need for similar units. The Canadian Special Air Service Company was formed in 1947, being disbanded in 1949. The New Zealand Special Air Service squadron was formed in 1954 to serve with the British SAS in Malaya. Australia formed the 1st SAS Company in July 1957, which became a full regiment of the Australian Special Air Service Regiment (SASR) in August 1964. On its return from Malaya, the C (Rhodesian) Squadron formed the basis for creation of the Rhodesian Special Air Service in 1961. It retained the name "C Squadron (Rhodesian) Special Air Service" within the Rhodesian Security Forces until 1978, when it became 1 (Rhodesian) Special Air Service Regiment.
Non-Commonwealth countries have also formed units based on the SAS. The Belgian Army's Special Forces Group, which wears the same capbadge as the British SAS, traces its ancestry partly from the 5th Special Air Service of the Second World War. The French 1st Marine Infantry Parachute Regiment (1er RPIMa) can trace its origins to the Second World War 3rd and 4th SAS, adopting its "who dares wins" motto. The American unit, 1st Special Forces Operational Detachment-Delta, was formed by Colonel Charles Alvin Beckwith, who served with 22 SAS as an exchange officer, and recognised the need for a similar type of unit in the United States Army. The Israeli Sayeret Matkal has also been modelled after the SAS, sharing its motto. Ireland's Army Ranger Wing (ARW) has also modelled its training on that of the SAS, as well as Delta Force (who in turn have been influenced by the SAS). The Philippine National Police's Special Action Force was formed along the lines of the SAS.
Organisation.
Little publicly verifiable information exists on the SAS, as Her Majesty's Government does not usually comment on special forces matters due to the nature of their work. The Special Air Service comprises three units: one Regular and two Army Reserve (AR) units. The regular army unit is 22 SAS Regiment and the reserve units are 21 Special Air Service Regiment (Artists) (Reserve) (21 SAS(R)) and 23 Special Air Service Regiment (23 SAS (R)), collectively, the Special Air Service (Reserve) (SAS(R)).
Squadrons.
22 SAS Regiment has four operational squadrons: A, B, D and G. Each squadron consists of approximately 65 men commanded by a major, divided into four troops (each troop being commanded by a captain) and a small headquarters section. Troops usually consist of 15 men, and each patrol within a troop consists of four men, with each man possessing a particular skill: signals, demolition, medic or linguist in addition to basic skills learned during the course of his training.
The four troops specialise in four different areas:
In 1980 R Squadron (which has since been renamed L Detachment) was formed; its members are all ex-regular SAS regiment soldiers who have a commitment to reserve service.
Squadron Structure
A Squadron: 1 (Boat) Troop – 2 (Air) Troop – 3 (Mobility) Troop – 4 (Mountain) Troop
B Squadron: 6 (Boat) Troop – 7 (Air) Troop – 8 (Mobility) Troop – 9 (Mountain) Troop
D Squadron: 16 (Air) Troop – 17 (Boat) Troop – 18 (Mobility) Troop – 19 (Mountain) Troop
G Squadron: 21 (Mobility) Troop – 22 (Mountain) Troop – 23 (Boat) Troop – 24 (Air) Troop
Special projects team.
The special projects team is the official name for the Special Air Service anti–hijacking counter–terrorism team. It is trained in Close Quarter Battle (CQB) and sniper techniques and specialises in hostage rescue in buildings or on public transport. The team was formed in the early 1970s after Prime Minister Edward Heath asked the Ministry of Defence to prepare for any possible terrorist attack similar to the massacre at the 1972 Summer Olympics and ordered that the SAS Counter Revolutionary Warfare (CRW) wing be raised.
Once the wing had been established, each squadron rotated on a continual basis through counter–terrorist training including hostage rescue, siege breaking, and live firing exercisesit has been reported that during CRW training each soldier expends as many as 100,000 pistol rounds. Squadrons refresh their training every 16 months, on average. The CRW's first deployment was during the Balcombe Street Siege. The Metropolitan Police had trapped a PIRA unit; it surrendered when it heard on the BBC that the SAS were being sent in.
The first documented action abroad by the CRW wing was assisting the West German counter-terrorism group GSG 9 at Mogadishu. In 1980 the SAS were involved in a hostage rescue during the Iranian Embassy Siege.
Operational command.
Regular.
22 Special Air Service is under the operational command of the Director Special Forces (DSF), a major-general grade post. Previously ranked as a brigadier, the DSF was promoted from brigadier to major-general in recognition of the significant expansion of the United Kingdom Special Forces (UKSF).
Reserve.
During Operation HERRICK the SAS Reserve were responsible for "mentoring" members of the Afghan National Police. Following a review of the unit's operational capability they were withdrawn from this tasking and the task handed over to a regular infantry unit. The report found that the SAS reservists lacked a clearly defined role, and also stated that the reservists lacked the military capability and skillset to serve alongside the regular special forces. and, on 1 September 2014, 21 and 23 SAS left United Kingdom Special Forces and were placed under the command of 1st Intelligence Surveillance and Reconnaissance Brigade alongside the HAC.
Recruitment, selection and training.
The regular elements of United Kingdom Special Forces never recruit directly from the general public. All current members of the UK Armed Forces can apply for special forces selection, but historically the majority of candidates have a Commando or Airborne forces background. Selections are held twice yearly, in summer and winter, in Sennybridge in the Brecon Beacons. Selection lasts for five weeks and normally starts with about 200 potential candidates. On arrival candidates first complete a Personal Fitness Test (PFT) and an Annual Fitness Test (AFT). They then march cross country against the clock, increasing the distances covered each day, culminating in what is known as Endurance: a march with full equipment scaling and descending Pen y Fan in 20 hours. By the end of the hill phase candidates must be able to run in 30 minutes and swim two miles (3.2 km) in 90 minutes.
Following the hill phase is the jungle phase, taking place in Belize, Brunei, or Malaysia. Candidates are taught navigation, patrol formation and movement, and jungle survival skills.
Candidates returning to Hereford finish training in battle plans and foreign weapons and take part in combat survival exercises, the final one being the week-long escape and evasion. Candidates are formed into patrols and, carrying nothing more than a tin can filled with survival equipment, are dressed in old Second World War uniforms and told to head for a point by first light. The final selection test is arguably the most gruelling: resistance to interrogation (RTI), lasting for 36 hours.
Typically, 15–20% of candidates make it through the hill phase selection process. From the approximately 200 candidates, most will drop out within the first few days, and by the end about 30 will remain. Those who complete all phases of selection are rewarded with a transfer to an operational squadron.
Uniform distinctions.
Normal barracks headdress is the sand-coloured beret, its cap badge is a downward pointing Excalibur, wreathed in flames (often incorrectly referred to as a winged dagger) worked into the cloth of a Crusader shield with the motto "Who Dares Wins". SAS pattern parachute wings, designed by Lieutenant Jock Lewes and based on the stylised sacred Ibis wings of Isis of Egyptian iconography depicted in the décor of Shepheard's Hotel in Cairo, are worn on the right shoulder. Its is distinguished by a light blue stripe on the trousers. Its stable belt is a shade of blue similar to the blue stripe on the No 1 dress uniform.
Battle honours.
In the British Army, battle honours are awarded to regiments that have seen active service in a significant engagement or campaign, generally with a victorious outcome. The Special Air Service Regiment has been awarded the following battle honours:
Memorials.
The names of those members of the SAS who have died on duty were inscribed on the regimental clock tower at Stirling Lines. Originally funded by contributions of a day's pay by members of the regiment and a donation from Handley Page in memory of Cpl RK Norry who was killed in a freefall parachuting accident, this was rebuilt at the new barracks at Credenhill. Those whose names are inscribed are said by surviving members to have "failed to beat the clock". At the suggestion of the then Commanding Officer, Dare Wilson, inscribed on the base of the clock is a verse from "The Golden Road to Samarkand" by James Elroy Flecker:
The other main memorial is the SAS and Airborne Forces memorial in the cloisters at Westminster Abbey. The SAS Brigade Memorial at Sennecey-le-Grand in France commemorates the wartime dead of the Belgian, British, and French SAS and recently a memorial plaque was added to the David Stirling Memorial in Scotland. There are other smaller memorials "scattered throughout Europe and in the Far East".
The local church St Martins has part of its graveyard set aside as an SAS memorial, over twenty SAS soldiers are buried there. There is also a wall of remembrance displaying memorial plaques to some who could not be buried, including the 18 SAS men who lost their lives in the Sea King helicopter crash during the Falklands Campaign on 19 May 1982.

</doc>
<doc id="55944" url="https://en.wikipedia.org/wiki?curid=55944" title="Eh">
Eh

Eh ( or ) is a spoken interjection in English that is similar in meaning to "Excuse me?," "Please repeat that", or "huh?". It is also commonly used as a question tag, i.e., method for inciting a reply, as in "It's nice here, eh?" In North America, it is most commonly associated with Canada and Canadian English, and with Michigan's Upper Peninsula. Similar interjections exist in other languages, such as Dutch, Armenian, Hokkien Chinese, Japanese, French, Finnish, Italian, Greek, Hebrew, Malay, Spanish, Persian, Portuguese, Arabic, Turkish, Korean and Catalan.
The spelling of this sound in English is quite different from the common usage of these letters. The vowel is sounded in one of the continental manners, and the letter "h" is used to indicate that it is long, as though the origin of the spelling were German.
It is an invariant question tag, unlike the "is it?" and "have you?" tags that have, with the insertion of "not", different construction in positive and negative questions.
English.
United States.
"Eh" is also used in situations to describe something bad or mediocre, in which case it is often pronounced with a short "e" sound and the "h" may even be noticeable. In addition, many Italian Americans, especially in the New York area, use the term "eh" as a general substitute for such basic greetings, such as "hey" or "hello". This behavior was prominently displayed in the TV show "Happy Days", having its character "The Fonz" constantly use this phrase.
Canada.
The only usage of "eh?" that is exclusive to Canada and some regions of Michigan's Upper Peninsula, northern Wisconsin, and northern Minnesota, according to the "Canadian Oxford Dictionary", is for "ascertaining the comprehension, continued interest, agreement, etc., of the person or persons addressed" as in, "It's four kilometres away, eh, so I have to go by bike." In that case, "eh?" is used to confirm the attention of the listener and to invite a supportive noise such as "Mm" or "Oh" or "Okay". This usage may be paraphrased as "I'm checking to see that you're agreement so I can continue." Grammatically, this usage constitutes an interjection; functionally, it is an implicit request for back-channel communication.
"Eh" can also be added to the end of a declarative sentence to turn it into a question. For example: "The weather is nice." becomes "The weather is nice, eh?" This same phrase could also be taken as "The weather is nice, don't you agree?". In this usage, it is virtually identical to the Dutch "hè?", the Japanese "ne?" or the Mandarin "bā". This usage differs from the French usage of "n'est-ce pas?" ("Is it not?") in that it does not use a (technically double or emphatic) negative.
The usage of "eh" in Canada is occasionally mocked in the United States, where some view its use – along with ', an approximation of a Canadian raising-affected pronunciation of "about" – as a stereotypical Canadianism. Such stereotypes have been reinforced in popular culture, and were famously lampooned in '. Singer Don Freed in his song "Saskatchewan" declares "What is this 'Eh?' nonsense? I wouldn't speak like that if I were paid to." There are many merchandise items on the market today that use this phrase, such as T-shirts and coffee mugs.
It is often joked about by Canadians as well, and is sometimes even a part of the national identity. For example, a Canadian national team is sometimes referred to as "the Eh? team". Likewise, at one of their concerts, a member of the Canadian Brass, referring to their arrangement of the jazz standard "Take the A Train", said that they'd considered calling it "Take the train, eh?". A classic joke illuminating this: "How did they name Canada? The letters were thrown in a bag, and the first one to be picked was 'C' eh?, then 'N' eh? and finally 'D' eh?"
In the Canadian animated faux-reality show "Total Drama Island", one of the 22 teen characters depicted on the show, Ezekiel, is a stereotypical Canadian yokel who uses the term "Eh", usually at the end of a sentence.
New Zealand.
While not as commonly lampooned as the Canadian "eh", there are few features that are 'more eagerly recognized by New Zealanders as a marker of their identity than the tag particle, "eh."'. This commonly used and referenced feature of New Zealand English (NZE) is one of great controversy to many communication scholars as it is both a mark of cultural identity and simultaneously a means to parody those of a lower socioeconomic status. In New Zealand culture, the use of "eh" is frequently linked with two main groups of people, the first being young, working-class, suburban Pākehā women and the second being working-class Māori men. The Pākehā are New Zealanders of British or European descent and Maori are indigenous Polynesians. This slang is not as commonly used in New Zealand compared to other countries, for example, Canada.
A 1994 study by communications scholar Miriam Meyerhoff sought to examine the function of "eh" in New Zealand culture. She hypothesized that "eh" did not function as a clarification device as frequently believed, but instead served as a means of establishing solidarity between individuals of similar ethnic descent. In her research, Meyerhoff analyzed conversations between an interviewer and an interviewee of either Pākehā or Māori descent and calculated the frequency of "eh" in the conversation. In order to yield the most natural speech, Meyerhoff instructed the interviewers to introduce themselves as a "friend of a friend", to their respective interviewees. Her results showed Maori men as the most frequent users of "eh" in their interviews. As Maori are typically of a lower socio-economic status, Meyerhoff proposed that "eh" functioned as a verbal cue that one reciprocated by another individual signified both shared identity and mutual acceptance. Therefore, in the context of Meyerhoff’s research, "eh" can be equated as a device to establish and maintain a group identity. This phenomenon sheds light on the continuous scholarly debate questioning if language determines culture or culture determines language.
Elsewhere.
The usage of the word is widespread throughout much of the UK, particularly in Wales and the north of England, Eastern Scotland, in places such as Yorkshire, Lancashire, and the English Midlands. It is normally used to mean "what?". In Scotland, mainly around the Tayside region, "eh" is also used as a shortened term for "yes". For example, "Are you going to the Disco?" "eh."
"Eh?" used to solicit agreement or confirmation is also heard regularly amongst speakers in Australia and the United Kingdom (where it is sometimes spelled "" on the assumption that "eh" would rhyme with "" or ""). In the Caribbean island of Barbados the word "nuh" acts similarly, as does "noh" in Surinamese Dutch and Sranantongo. The usage in New Zealand is similar, and is more common in the North Island. It is also heard in the United States, especially Minnesota, Wisconsin, the Upper Peninsula of Michigan (although the Scandinavian-based Yooperism "ya" is more common), Oklahoma and the New England region. In New England and Oklahoma, it is also used as a general exclamation as in Scotland and the Channel Islands of Jersey and Guernsey.
It is occasionally used to express indifference, in a similar way to "meh".
Since usage of the word "eh" is not as common in the United States as it is in Canada, it is often used by Americans, and indeed Canadians themselves, to parody Canadian English.
The equivalent in South African English is "hey".
"Eh" is also used in Guernsey English and Jersey English.
"Eh" is very common in the English spoken in the Seychelles.
In Singapore, the use of medium Singlish often includes "Eh" as an interjection, but it is not as popularly used as "lah". An example of a sentence that uses "eh" in its expression is "Dis guy Singlish damn good eh", meaning "this guy's Singlish is very good".

</doc>
<doc id="55951" url="https://en.wikipedia.org/wiki?curid=55951" title="Instant messaging">
Instant messaging

Instant messaging (IM) is a type of online chat which offers real-time text transmission over the Internet. A LAN messenger operates in a similar way over a local area network. Short messages are typically transmitted bi-directionally between two parties, when each user chooses to complete a thought and select "send". Some IM applications can use push technology to provide real-time text, which transmits messages character by character, as they are composed. More advanced instant messaging can add file transfer, clickable hyperlinks, Voice over IP, or video chat.
Non-IM types of chat include multicast transmission, usually referred to as "chat rooms", where participants might be anonymous or might be previously known to each other (for example collaborators on a project that is using chat to facilitate communication). Instant messaging systems tend to facilitate connections between specified known users (often using a contact list also known as a "buddy list" or "friend list"). Depending on the IM protocol, the technical architecture can be peer-to-peer (direct point-to-point transmission) or client-server (a central server retransmits messages from the sender to the communication device).
Overview.
Instant messaging is a set of communication technologies used for text-based communication between two or more participants over the Internet or other types of networks. IM–chat happens in real-time. Of importance is that online chat and instant messaging differ from other technologies such as email due to the perceived quasi-synchrony of the communications by the users. Some systems permit messages to be sent to users not then 'logged on' ("offline messages"), thus removing some differences between IM and email (often done by sending the message to the associated email account).
IM allows effective and efficient communication, allowing immediate receipt of acknowledgment or reply. However IM is basically not necessarily supported by transaction control. In many cases, instant messaging includes added features which can make it even more popular. For example, users may see each other via webcams, or talk directly for free over the Internet using a microphone and headphones or loudspeakers. Many applications allow file transfers, although they are usually limited in the permissible file-size.
It is usually possible to save a text conversation for later reference. Instant messages are often logged in a local message history, making it similar to the persistent nature of emails.
History.
Though the term dates from the 1990s, instant messaging predates the Internet, first appearing on multi-user operating systems like Compatible Time-Sharing System (CTSS) and Multiplexed Information and Computing Service (Multics) in the mid-1960s. Initially, some of these systems were used as notification systems for services like printing, but quickly were used to facilitate communication with other users logged into the same machine. As networks developed, the protocols spread with the networks. Some of these used a peer-to-peer protocol (e.g. talk, ntalk and ytalk), while others required peers to connect to a server (see talker and IRC). The Zephyr Notification Service (still in use at some institutions) was invented at MIT's Project Athena in the 1980s to allow service providers to locate and send messages to users.
Parallel to instant messaging were early online chat facilities, the earliest of which was Talkomatic (1973) on the PLATO system. During the bulletin board system (BBS) phenomenon that peaked during the 1980s, some systems incorporated chat features which were similar to instant messaging; Freelancin' Roundtable was one prime example. The first such general-availability commercial online chat service (as opposed to PLATO, which was educational) was the CompuServe CB Simulator in 1980, created by CompuServe executive Alexander "Sandy" Trevor in Columbus, Ohio.
Early instant messaging programs were primarily real-time text, where characters appeared as they were typed. This includes the Unix "talk" command line program, which was popular in the 1980s and early 1990s. Some BBS chat programs (i.e. Celerity BBS) also used a similar interface. Modern implementations of real-time text also exist in instant messengers, such as AOL's Real-Time IM as an optional feature.
In the latter half of the 1980s and into the early 1990s, the Quantum Link online service for Commodore 64 computers offered user-to-user messages between concurrently connected customers, which they called "On-Line Messages" (or OLM for short), and later "FlashMail." (Quantum Link later became America Online and made AOL Instant Messenger (AIM), discussed later). While the Quantum Link client software ran on a Commodore 64, using only the Commodore's PETSCII text-graphics, the screen was visually divided into sections and OLMs would appear as a yellow bar saying "Message From:" and the name of the sender along with the message across the top of whatever the user was already doing, and presented a list of options for responding. As such, it could be considered a type of graphical user interface (GUI), albeit much more primitive than the later Unix, Windows and Macintosh based GUI IM software. OLMs were what Q-Link called "Plus Services" meaning they charged an extra per-minute fee on top of the monthly Q-Link access costs.
Modern, Internet-wide, GUI-based messaging clients as they are known today, began to take off in the mid-1990s with PowWow, ICQ, and AOL Instant Messenger. Similar functionality was offered by CU-SeeMe in 1992; though primarily an audio/video chat link, users could also send textual messages to each other. AOL later acquired Mirabilis, the authors of ICQ; a few years later ICQ (now owned by AOL) was awarded two patents for instant messaging by the U.S. patent office. Meanwhile, other companies developed their own software; (Excite, MSN, Ubique, and Yahoo!), each with its own proprietary protocol and client; users therefore had to run multiple client applications if they wished to use more than one of these networks. In 1998, IBM released IBM Lotus Sametime, a product based on technology acquired when IBM bought Haifa-based Ubique and Lexington-based Databeam.
In 2000, an open source application and open standards-based protocol called Jabber was launched. The protocol was standardized under the name Extensible Messaging and Presence Protocol (XMPP). XMPP servers could act as gateways to other IM protocols, reducing the need to run multiple clients. Multi-protocol clients can use any of the popular IM protocols by using additional local libraries for each protocol. IBM Lotus Sametime's November 2007 release added IBM Lotus Sametime Gateway support for XMPP.
As of 2010, social networking providers often offer IM abilities. Facebook Chat is a form of instant messaging, and Twitter can be thought of as a Web 2.0 instant messaging system. Similar server-side chat features are part of most dating websites, such as OKCupid or PlentyofFish. The spread of smartphones and similar devices in the late 2000s also caused increased competition with conventional instant messaging, by making text messaging services still more ubiquitous.
Many instant messaging services offer video calling features, voice over IP and web conferencing services. Web conferencing services can integrate both video calling and instant messaging abilities. Some instant messaging companies are also offering desktop sharing, IP radio, and IPTV to the voice and video features.
The term "Instant Messenger" is a service mark of Time Warner and may not be used in software not affiliated with AOL in the United States. For this reason, in April 2007, the instant messaging client formerly named Gaim (or gaim) announced that they would be renamed "Pidgin".
Clients.
Each modern IM service generally provides its own client, either a separately installed piece of software, or a browser-based client. These usually only work with the supplier company's service, although some allow limited function with other services. Third party client software applications exist that will connect with most of the major IM services. Adium, Empathy, Miranda IM, Pidgin, Qnext, Trillian, and Facebook Messenger are a few of the common ones.
Interoperability.
Standard complementary instant messaging applications offer functions like file transfer, contact list(s), the ability to hold several simultaneous conversations, etc. These may be all the functions that a small business needs, but larger organizations will require more sophisticated applications that can work together. The solution to finding applications capable of this is to use enterprise versions of instant messaging applications. These include titles like XMPP, Lotus Sametime, Microsoft Office Communicator, etc., which are often integrated with other enterprise applications such as workflow systems. These enterprise applications, or enterprise application integration (EAI), are built to certain constraints, namely storing data in a common format.
There have been several attempts to create a unified standard for instant messaging: IETF's Session Initiation Protocol (SIP) and SIP for Instant Messaging and Presence Leveraging Extensions (SIMPLE), Application Exchange (APEX), Instant Messaging and Presence Protocol (IMPP), the open XML-based Extensible Messaging and Presence Protocol (XMPP), and Open Mobile Alliance's Instant Messaging and Presence Service developed specifically for mobile devices.
Most attempts at producing a unified standard for the major IM providers (AOL, Yahoo! and Microsoft) have failed, and each continues to use its own proprietary protocol.
However, while discussions at IETF were stalled, Reuters signed the first inter-service provider connectivity agreement on September 2003. This agreement enabled AIM, ICQ and MSN Messenger users to talk with Reuters Messaging counterparts and vice versa. Following this, Microsoft, Yahoo! and AOL agreed to a deal in which Microsoft's Live Communications Server 2005 users would also have the possibility to talk to public instant messaging users. This deal established SIP/SIMPLE as a standard for protocol interoperability and established a connectivity fee for accessing public instant messaging groups or services. Separately, on October 13, 2005, Microsoft and Yahoo! announced that by the 3rd quarter of 2006 they would interoperate using SIP/SIMPLE, which was followed, in December 2005, by the AOL and Google strategic partnership deal in which Google Talk users would be able to communicate with AIM and ICQ users provided they have an AIM account.
There are two ways to combine the many disparate protocols:
Some approaches allow organizations to deploy their own, private instant messaging network by enabling them to restrict access to the server (often with the IM network entirely behind their firewall) and administer user permissions. Other corporate messaging systems allow registered users to also connect from outside the corporation LAN, by using an encrypted, firewall-friendly, HTTPS-based protocol. Usually, a dedicated corporate IM server has several advantages, such as pre-populated contact lists, integrated authentication, and better security and privacy.
Certain networks have made changes to prevent them from being used by such multi-network IM clients. For example, Trillian had to release several revisions and patches to allow its users to access the MSN, AOL, and Yahoo! networks, after changes were made to these networks. The major IM providers usually cite the need for formal agreements, and security concerns as reasons for making these changes.
The use of proprietary protocols has meant that many instant messaging networks have been incompatible and users have been unable to reach users on other networks. This may have allowed social networking with IM-like features and text messaging an opportunity to gain market share at the expense of IM.
IM language.
Users sometimes make use of internet slang or text speak to abbreviate common words or expressions to quicken conversations or reduce keystrokes. The language has become widespread, with well-known expressions such as 'lol' translated over to face-to-face language.
Emotions are often expressed in shorthand, such as the abbreviation LOL, BRB and TTYL; respectively laugh(ing) out loud, be right back, and talk to you later.
Some, however, attempt to be more accurate with emotional expression over IM. Real time reactions such as ("chortle") ("snort") ("guffaw") or ("eye-roll") are becoming more popular. Also there are certain standards that are being introduced into mainstream conversations including, '#' indicates the use of sarcasm in a statement and '*' which indicates a spelling mistake and/or grammatical error in the prior message, followed by a correction.
Business application.
Instant messaging has proven to be similar to personal computers, email, and the World Wide Web, in that its adoption for use as a business communications medium was driven primarily by individual employees using consumer software at work, rather than by formal mandate or provisioning by corporate information technology departments. Tens of millions of the consumer IM accounts in use are being used for business purposes by employees of companies and other organizations.
In response to the demand for business-grade IM and the need to ensure security and legal compliance, a new type of instant messaging, called "Enterprise Instant Messaging" ("EIM") was created when Lotus Software launched IBM Lotus Sametime in 1998. Microsoft followed suit shortly thereafter with Microsoft Exchange Instant Messaging, later created a new platform called Microsoft Office Live Communications Server, and released Office Communications Server 2007 in October 2007. Oracle Corporation has also jumped into the market recently with its Oracle Beehive unified collaboration software. Both IBM Lotus and Microsoft have introduced federation between their EIM systems and some of the public IM networks so that employees may use one interface to both their internal EIM system and their contacts on AOL, MSN, and Yahoo! As of 2010, leading EIM platforms include IBM Lotus Sametime, Microsoft Office Communications Server, Jabber XCP and Cisco Unified Presence. Industry-focused EIM platforms as Reuters Messaging and Bloomberg Messaging also provide enhanced IM abilities to financial services companies.
The adoption of IM across corporate networks outside of the control of IT organizations creates risks and liabilities for companies who do not effectively manage and support IM use. Companies implement specialized IM archiving and security products and services to mitigate these risks and provide safe, secure, productive instant messaging abilities to their employees. IM is increasingly becoming a feature of enterprise software rather than a stand-alone application.
Types of products.
IM products can usually be categorised into two types: Enterprise Instant Messaging (EIM) and Consumer Instant Messaging (CIM). Enterprise solutions use an internal IM server, however this isn't always feasible, particularly for smaller businesses with limited budgets. The second option, using a CIM provides the advantage of being inexpensive to implement and has little need for investing in new hardware or server software.
For corporate use, encryption and conversation archiving are usually regarded as important features due to security concerns. Sometimes the use of different operating systems in organizations requires use of software that supports more than one platform. For example, many software companies use Windows in administration departments but have software developers who use Linux.
Security risks.
Crackers (malicious or black hat hackers) have consistently used IM networks as vectors for delivering phishing attempts, "poison URLs", and virus-laden file attachments from 2004 to the present, with over 1100 discrete attacks listed by the IM Security Center in 2004–2007. Hackers use two methods of delivering malicious code through IM: delivery of viruses, trojan horses, or spyware within an infected file, and the use of "socially engineered" text with a web address that entices the recipient to click on a URL connecting him or her to a website that then downloads malicious code.
Viruses, computer worms, and trojans usually propagate by sending themselves rapidly through the infected user's contact list. An effective attack using a poisoned URL may reach tens of thousands of users in a short period when each user's contact list receives messages appearing to be from a trusted friend. The recipients click on the web address, and the entire cycle starts again. Infections may range from nuisance to criminal, and are becoming more sophisticated each year.
IM connections usually occur in plain text, making them vulnerable to eavesdropping. Also, IM client software often requires the user to expose open UDP ports to the world, raising the threat posed by potential security vulnerabilities.
Compliance risks.
In addition to the malicious code threat, the use of instant messaging at work also creates a risk of non-compliance to laws and regulations governing use of electronic communications in businesses.
In the United States.
In the United States alone there are over 10,000 laws and regulations related to electronic messaging and records retention. The better-known of these include the Sarbanes–Oxley Act, HIPAA, and SEC 17a-3.
Clarification from the Financial Industry Regulatory Authority (FINRA) was issued to member firms in the financial services industry in December, 2007, noting that "electronic communications", "email", and "electronic correspondence" may be used interchangeably and can include such forms of electronic messaging as "instant messaging" and text messaging. Changes to Federal Rules of Civil Procedure, effective December 1, 2006, created a new category for electronic records which may be requested during discovery in legal proceedings.
World-wide.
Most nations also regulate use of electronic messaging and electronic records retention in similar fashion as the United States. The most common regulations related to IM at work involve the need to produce archived business communications to satisfy government or judicial requests under law. Many instant messaging communications fall into the category of business communications that must be archived and retrievable.
Security and archiving.
In the early 2000s, a new class of IT security provider emerged to provide remedies for the risks and liabilities faced by corporations who chose to use IM for business communications. The IM security providers created new products to be installed in corporate networks for the purpose of archiving, content-scanning, and security-scanning IM traffic moving in and out of the corporation. Similar to the e-mail filtering vendors, the IM security providers focus on the risks and liabilities described above.
With rapid adoption of IM in the workplace, demand for IM security products began to grow in the mid-2000s. By 2007, the preferred platform for the purchase of security software had become the "computer appliance", according to IDC, who estimate that by 2008, 80% of network security products will be delivered via an appliance.
By 2014 however, the level of safety offered by instant messengers was still extremely poor. According to the EFF security review, only 7 out of 39 instant messengers had a perfect score (namely ChatSecure, Cryptocat, Signal/RedPhone, Silent Phone, Silent Text and TextSecure), whereas the most popular instant messengers at this time (Whatsapp, Snapchat, Facebook Chat and Hangouts) only attain a score of 2 out of 7. Skype even just attained a number of 1 out of 7. A number of studies have shown that IM services are quite vulnerable for providing user privacy.
User base.
While some numbers are given by the owners of a complete instant messaging system, others are provided by commercial vendors of a part of a distributed system. Some companies may be motivated to inflate their numbers to raise advertising earnings or attract partners, clients, or customers. Importantly, some numbers are reported as the number of "active" users (with no shared standard of that activity), others indicate total user accounts, while others indicate only the users logged in during an instance of peak use.
Since the acquisitions of 2010 and later and with the wide availability of smartphones, the virtual communities of those conglomerates are becoming the user base of most instant messaging services:

</doc>
<doc id="55955" url="https://en.wikipedia.org/wiki?curid=55955" title="Version control">
Version control

A component of software configuration management, version control, also known as revision control or source control, is the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the "revision number", "revision level", or simply "revision". For example, an initial set of files is "revision 1". When the first change is made, the resulting set is "revision 2", and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged.
The need for a logical way to organize and control revisions has existed for almost as long as writing has existed, but revision control became much more important, and complicated, when the era of computing began. The numbering of book editions and of specification revisions are examples that date back to the print-only era. Today, the most capable (as well as complex) revision control systems are those used in software development, where a team of people may change the same files.
Version control systems (VCS) most commonly run as stand-alone applications, but revision control is also embedded in various types of software such as word processors and spreadsheets, e.g., Google Docs and Sheets and in various content management systems, e.g., Wikipedia's . Revision control allows for the ability to revert a document to a previous revision, which is critical for allowing editors to track each other's edits, correct mistakes, and defend against vandalism and spamming.
Software tools for revision control are essential for the organization of multi-developer projects.
Overview.
In computer software engineering, revision control is any kind of practice that tracks and provides control over changes to source code. Software developers sometimes use revision control software to maintain documentation and configuration files as well as source code.
As teams design, develop and deploy software, it is common for multiple versions of the same software to be deployed in different sites and for the software's developers to be working simultaneously on updates. Bugs or features of the software are often only present in certain versions (because of the fixing of some problems and the introduction of others as the program develops). Therefore, for the purposes of locating and fixing bugs, it is vitally important to be able to retrieve and run different versions of the software to determine in which version(s) the problem occurs. It may also be necessary to develop two versions of the software concurrently (for instance, where one version has bugs fixed, but no new features (branch), while the other version is where new features are worked on (trunk).
At the simplest level, developers could simply retain multiple copies of the different versions of the program, and label them appropriately. This simple approach has been used on many large software projects. While this method can work, it is inefficient as many near-identical copies of the program have to be maintained. This requires a lot of self-discipline on the part of developers, and often leads to mistakes. Since the code base is same it also requires granting read-write-execute permission to set of developers, this adds pressure of someone managing permissions so that code base is not compromised which adds to more complexity. Consequently, systems to automate some or all of the revision control process have been developed. This ensures majority of management of version control steps is hidden behind the scenes.
Moreover, in software development, legal and business practice and other environments, it has become increasingly common for a single document or snippet of code to be edited by a team, the members of which may be geographically dispersed and may pursue different and even contrary interests. Sophisticated revision control that tracks and accounts for ownership of changes to documents and code may be extremely helpful or even indispensable in such situations.
Revision control may also track changes to configuration files, such as those typically stored in codice_1 or codice_2 on Unix systems. This gives system administrators another way to easily track changes made and a way to roll back to earlier versions should the need arise.
Structure.
Revision control manages changes to a set of data over time. These changes can be structured in various ways.
Often the data is thought of as a collection of many individual items, such as files or documents, and changes to individual files are tracked. This accords with intuitions about separate files, but causes problems when identity changes, such as during renaming, splitting, or merging of files. Accordingly, some systems, such as git, instead consider changes to the data as a whole, which is less intuitive for simple changes, but simplifies more complex changes.
When data that is under revision control is modified, after being retrieved by "checking out," this is not in general immediately reflected in the revision control system (in the "repository"), but must instead be "checked in" or "committed." A copy outside revision control is known as a "working copy". As a simple example, when editing a computer file, the data stored in memory by the editing program is the working copy, which is committed by saving. Concretely, one may print out a document, edit it by hand, and only later manually input the changes into a computer and save it. For source code control, the working copy is instead a copy of all files in a particular revision, generally stored locally on the developer's computer; in this case saving the file only changes the working copy, and checking into the repository is a separate step.
If multiple people are working on a single data set or document, they are implicitly creating branches of the data (in their working copies), and thus issues of merging arise, as discussed below. For simple collaborative document editing, this can be prevented by using file locking or simply avoiding working on the same document that someone else is working on.
Revision control systems are often centralized, with a single authoritative data store, the "repository," and check-outs and check-ins done with reference to this central repository. Alternatively, in distributed revision control, no single repository is authoritative, and data can be checked out and checked into any repository. When checking into a different repository, this is interpreted as a merge or patch.
Graph structure.
In terms of graph theory, revisions are generally thought of as a line of development (the "trunk") with branches off of this, forming a directed tree, visualized as one or more parallel lines of development (the "mainlines" of the branches) branching off a trunk. In reality the structure is more complicated, forming a directed acyclic graph, but for many purposes "tree with merges" is an adequate approximation.
Revisions occur in sequence over time, and thus can be arranged in order, either by revision number or timestamp. Revisions are based on past revisions, though it is possible to largely or completely replace an earlier revision, such as "delete all existing text, insert new text". In the simplest case, with no branching or undoing, each revision is based on its immediate predecessor alone, and they form a simple line, with a single latest version, the "HEAD" revision or "tip". In graph theory terms, drawing each revision as a point and each "derived revision" relationship as an arrow (conventionally pointing from older to newer, in the same direction as time), this is a linear graph. If there is branching, so multiple future revisions are based on a past revision, or undoing, so a revision can depend on a revision older than its immediate predecessor, then the resulting graph is instead a directed tree (each node can have more than one child), and has multiple tips, corresponding to the revisions without children ("latest revision on each branch"). In principle the resulting tree need not have a preferred tip ("main" latest revision) – just various different revisions – but in practice one tip is generally identified as HEAD. When a new revisions is based on HEAD, it is either identified as the new HEAD, or considered a new branch. The list of revisions from the start to HEAD (in graph theory terms, the unique path in the tree, which forms a linear graph as before) is the "trunk" or "mainline." Conversely, when a revision can be based on more than one previous revision (when a node can have more than one "parent"), the resulting process is called a "merge," and is one of the most complex aspects of revision control. This most often occurs when changes occur in multiple branches (most often two, but more are possible), which are then merged into a single branch incorporating both changes. If these changes overlap, it may be difficult or impossible to merge, and require manual intervention or rewriting.
In the presence of merges, the resulting graph is no longer a tree, as nodes can have multiple parents, but is instead a rooted directed acyclic graph (DAG). The graph is acyclic since parents are always backwards in time, and rooted because there is an oldest version. However, assuming that there is a trunk, merges from branches can be considered as "external" to the tree – the changes in the branch are packaged up as a "patch," which is applied to HEAD (of the trunk), creating a new revision without any explicit reference to the branch, and preserving the tree structure. Thus, while the actual relations between versions form a DAG, this can be considered a tree plus merges, and the trunk itself is a line.
In distributed revision control, in the presence of multiple repositories these may be based on a single original version (a root of the tree), but there need not be an original root, and thus only a separate root (oldest revision) for each repository, for example if two people starting working on a project separately. Similarly in the presence of multiple data sets (multiple projects) that exchange data or merge, there isn’t a single root, though for simplicity one may think of one project as primary and the other as secondary, merged into the first with or without its own revision history.
Specialized strategies.
Engineering revision control developed from formalized processes based on tracking revisions of early blueprints or bluelines. This system of control implicitly allowed returning to any earlier state of the design, for cases in which an engineering dead-end was reached in the development of the design.
A revision table was used to keep track of the changes made. Additionally, the modified areas of the drawing were highlighted using revision clouds.
Version control is also widespread in business and law. Indeed, "contract redline" and "legal blackline" are some of the earliest forms of revision control, and are still employed in business and law with varying degrees of sophistication. An entire industry has emerged to service the document revision control needs of business and other users, and some of the revision control technology employed in these circles is subtle, powerful, and innovative. The most sophisticated techniques are beginning to be used for the electronic tracking of changes to CAD files (see product data management), supplanting the "manual" electronic implementation of traditional revision control.
Source-management models.
Traditional revision control systems use a centralized model where all the revision control functions take place on a shared server. If two developers try to change the same file at the same time, without some method of managing access the developers may end up overwriting each other's work. Centralized revision control systems solve this problem in one of two different "source management models": file locking and version merging.
Atomic operations.
An operation is "atomic" if the system is left in a consistent state even if the operation is interrupted. The "commit" operation is usually the most critical in this sense. Commits tell the revision control system to make a group of changes final, and available to all users. Not all revision control systems have atomic commits; notably, CVS lacks this feature.
File locking.
The simplest method of preventing "concurrent access" problems involves locking files so that only one developer at a time has write access to the central "repository" copies of those files. Once one developer "checks out" a file, others can read that file, but no one else may change that file until that developer "checks in" the updated version (or cancels the checkout).
File locking has both merits and drawbacks. It can provide some protection against difficult merge conflicts when a user is making radical changes to many sections of a large file (or group of files). However, if the files are left exclusively locked for too long, other developers may be tempted to bypass the revision control software and change the files locally, leading to more serious problems.
Version merging.
Most version control systems allow multiple developers to edit the same file at the same time. The first developer to "check in" changes to the central repository always succeeds. The system may provide facilities to merge further changes into the central repository, and preserve the changes from the first developer when other developers check in.
Merging two files can be a very delicate operation, and usually possible only if the data structure is simple, as in text files. The result of a merge of two image files might not result in an image file at all. The second developer checking in code will need to take care with the merge, to make sure that the changes are compatible and that the merge operation does not introduce its own logic errors within the files. These problems limit the availability of automatic or semi-automatic merge operations mainly to simple text based documents, unless a specific merge plugin is available for the file types.
The concept of a "reserved edit" can provide an optional means to explicitly lock a file for exclusive write access, even when a merging capability exists.
Baselines, labels and tags.
Most revision control tools will use only one of these similar terms (baseline, label, tag) to refer to the action of identifying a snapshot ("label the project") or the record of the snapshot ("try it with baseline "X""). Typically only one of the terms "baseline", "label", or "tag" is used in documentation or discussion; they can be considered synonyms.
In most projects some snapshots are more significant than others, such as those used to indicate published releases, branches, or milestones.
When both the term "baseline" and either of "label" or "tag" are used together in the same context, "label" and "tag" usually refer to the mechanism within the tool of identifying or making the record of the snapshot, and "baseline" indicates the increased significance of any given label or tag.
Most formal discussion of configuration management uses the term "baseline".
Distributed revision control.
Distributed revision control systems (DRCS) take a peer-to-peer approach, as opposed to the client-server approach of centralized systems. Rather than a single, central repository on which clients synchronize, each peer's working copy of the codebase is a bona-fide repository.
Distributed revision control conducts synchronization by exchanging patches (change-sets) from peer to peer. This results in some important differences from a centralized system:
Rather, communication is only necessary when pushing or pulling changes to or from other peers.
Integration.
Some of the more advanced revision-control tools offer many other facilities, allowing deeper integration with other tools and software-engineering processes. Plugins are often available for IDEs such as Oracle JDeveloper, IntelliJ IDEA, Eclipse and Visual Studio. Delphi, NetBeans IDE, Xcode and GNU Emacs (via vc.el) come with integrated version control support.
Common vocabulary.
Terminology can vary from system to system, but some terms in common usage include:

</doc>
<doc id="55959" url="https://en.wikipedia.org/wiki?curid=55959" title="Politician">
Politician

A politician (from Classical Greek πόλις, "polis") is a person active in party politics, or a person holding or seeking office in government. In democratic countries, politicians seek elective positions within a government through elections or, at times, temporary appointment to replace politicians who have died, resigned or have been otherwise removed from office. In non-democratic countries, they employ other means of reaching power through appointment, bribery, revolutions and intrigues. Some politicians are experienced in the art or science of government. Politicians propose, support and create laws or policies that govern the land and, by extension, its people. Broadly speaking, a "politician" can be anyone who seeks to achieve political power in any bureaucratic institution.
Identity.
Politicians are people who are politically active, especially in party politics. Positions range from local offices to executive, legislative, and judicial offices of regional and national governments. Some elected law enforcement officers, such as sheriffs, are considered politicians.
Media and rhetoric.
Politicians are known for their rhetoric, as in speeches or campaign advertisements. They are especially known for using common themes that allow them to develop their political positions in terms familiar to the voters. Politicians of necessity become expert users of the media. Politicians in the 19th century made heavy use of newspapers, magazines, and pamphlets, as well as posters. In the 20th century, they branched into radio and television, making television commercials the single most expensive part of an election campaign. In the 21st century, they have become increasingly involved with the social media based on the Internet and smart phones.
Rumor has always played a major role in politics, with negative rumors about an opponent typically more effective than positive rumors about one's own side.
Bureaucracy and spoils.
Once elected, the politician becomes a government official and has to deal with a permanent bureaucracy of non-politicians. Historically, there has been a subtle conflict between the long-term goals of each side. In patronage-based systems, such as the United States and Canada in the 19th century, winning politicians replace the bureaucracy with local politicians who formed their base of support, the "spoils system". Civil service reform was initiated to eliminate the corruption of government services that were involved. However, in many less developed countries, the spoils system is in full-scale operation today.
Careers.
Mattozzi and Merlo argue that there are two main career paths which are typically followed by politicians in modern democracies. First come the career politicians. They are politicians who work in the political sector until retirement. Second are the "political careerists". These are politicians who gain reputation for expertise in controlling certain bureaucracies, then leave politics for a well-paid career in the private sector making use of their political contacts.
Characteristics.
Numerous scholars have studied the characteristics of politicians, comparing those at the local and national levels, and comparing the more liberal or the more conservative ones, and comparing the more successful and less successful in terms of elections. In recent years, special attention has focused on the distinctive career path of women politicians. For example, there are studies of the "Supermadre" model in Latin American politics.
Many politicians have the knack to remember thousands of names and faces and recall personal anecdotes about their constituents—it is an advantage in the job, rather like being seven-foot tall for a basketball player. Presidents George W. Bush and Bill Clinton were renowned for their memories.
Criticism.
Many critics attack politicians for being out of touch with the public. Areas of friction include the manner in which politicians speak, which has been described as being overly formal and filled with many euphemistic and metaphorical expressions and commonly perceived as an attempt to "obscure, mislead, and confuse".
In the popular image, many politicians are corrupt, taking money in exchange for goods or services, rather than working for the general public good.

</doc>
<doc id="55961" url="https://en.wikipedia.org/wiki?curid=55961" title="Chaco War">
Chaco War

The Chaco War (1932–1935) (, Guarani: "Cháko Ñorairõ") was fought between Bolivia and Paraguay over control of the northern part of the Gran Chaco region (known in Spanish as "Chaco Boreal") of South America, which was thought to be rich in oil. It is also referred to as "La Guerra de la Sed" (Spanish for "The War of the Thirst") in literary circles, for being fought in the semi-arid Chaco. It was the bloodiest military conflict fought in South America during the 20th century, between two of its poorest countries, both having previously lost territory to neighbors in 19th century wars.
During the war, both landlocked countries faced difficulties shipping arms and supplies through neighboring countries. Bolivia faced particular external trade problems, coupled with poor internal communications. Although Bolivia had lucrative mining income, and a larger better equipped army, a series of factors turned the tide, and Paraguay came to control most of the disputed zone by war's end.
The ultimate peace treaties granted two-thirds of the disputed territories to Paraguay.
Origins.
The origin of the war is commonly attributed to a conflict between the oil companies Royal Dutch Shell backing Paraguay and Standard Oil supporting Bolivia. The discovery of oil in the Andean foothills sparked speculation that the Chaco might prove a rich source of petroleum, and foreign oil-companies were involved in the exploration. Standard Oil was already producing oil from wells in the high hills of eastern Bolivia, around Villa Montes. However, it is uncertain if the war would have been caused solely by the interests of these companies, and not by aims of Argentina to import oil from the Chaco.
Both Bolivia and Paraguay were landlocked. Though the 600,000 km2 Chaco was sparsely populated, control of the Paraguay River running through it provided access to the Atlantic Ocean. This became especially important to Bolivia, which had lost its Pacific coast to Chile in the 1879 War of the Pacific.
Paraguay had lost almost half of its territory to Brazil and Argentina in the Paraguayan War of 1864-1870. The country was not prepared to surrender its economic viability.
In international arbitration, Bolivia argued that the region had been part of the original Spanish colonial province of Moxos and Chiquitos to which Bolivia was heir. Meanwhile, Paraguay based its case on the occupation of the land. Indeed, both Paraguayan and Argentine planters were already breeding cattle and exploiting quebracho woods in the area, while the small nomadic indigenous population of Guaraní-speaking tribes was related to that country's own Guaraní heritage. As of 1919, Argentine banks owned 400,000 hectares of land in the eastern Chaco, while the Casado family, a powerful part of the Argentine oligarchy, held 141,000. The presence of Mennonite colonies in the Chaco, who settled there in the 1920s under the auspices of the Paraguayan parliament, was another factor in favour of Paraguay's claim.
Prelude to the war.
The first confrontation between the two countries dates back to 1885, when the Bolivian entrepreneur Miguel Araña Suárez founded Puerto Pacheco, a port on the upper Paraguay river, south of Bahía Negra. He assumed that the new settlement was well inside Bolivian territory, but Bolivia had implicitly recognized Bahía Negra as Paraguayan. The Paraguayan government sent in a naval detachment aboard the gunboat "Pirapó", which forcibly evicted the Bolivians from the area in 1888. Two agreements followed – in 1894 and 1907 – which neither the Bolivian nor the Paraguayan parliament ever approved. Meanwhile, in 1905, Bolivia founded two new outposts in the Chaco, Ballivián and Guachalla, this time along the Pilcomayo River. The Bolivian government ignored the half-hearted Paraguayan official protest.
Bolivian penetration in the region went unopposed until 1927, when the first blood was shed over the Chaco Boreal. On 27 February a Paraguayan army foot patrol and its native guides were taken prisoners near the Pilcomayo River and held in the Bolivian outpost of Fortin Sorpresa, where the commander of the Paraguayan platoon, Lt. Adolfo Rojas Silva, was shot and killed in suspicious circumstances. "Fortín" (Spanish for "little fort") was the name used for the small pillbox and trench-like garrisons in the Chaco, although the troops' barracks usually were no more than a few mud huts. While the Bolivian government formally regretted the death of Rojas Silva, the Paraguayan public opinion called it "murder". After the subsequent talks arranged in Buenos Aires failed to produce any agreement and eventually collapsed in January 1928, the dispute grew violent. On 5 December 1928 a Paraguayan cavalry unit overran Fortin Vanguardia, an advance outpost established by the Bolivian army a few miles northwest of Bahía Negra. The Paraguayans captured 21 Bolivian soldiers and burnt the scattered huts to the ground.
The Bolivians retaliated with an air strike on Bahía Negra on 15 December, which didn't cause many casualties or much damage. On 14 December Bolivia seized Fortin Boquerón, which later would be the site of the first major battle of the campaign, at the cost of 15 Paraguayan dead. A return to the "status quo ante" was eventually agreed on 12 September 1929 in Washington, under pressure from the Pan American League, but an arms race had already begun and both countries were on a collision course.
Composition of the armies.
Paraguay had a population only a third as large as that of Bolivia (880,000 vs. 2,150,000), but its innovative style of fighting, centered on rapid marches and flanking encirclements, compared to Bolivia's more conventional strategy, enabled it to take the upper hand. In June 1932 the Paraguayan army totaled about 4,026 men (355 combat officers, 146 surgeons and non-combatant officers, 200 cadets, 690 NCOs and 2,653 soldiers). Both racially and culturally, the Paraguayan army was practically homogeneous. Almost all of its soldiers were European-Guaraní "mestizos". Bolivia's army, however, consisted mostly of Altiplano's aboriginals of Quechua or Aymará descent (90% of the infantry troops), the lower-ranking officers were of Spanish or other European ancestry, and army commander-in-chief Hans Kundt was German. In spite of the fact that the Bolivian army had more manpower, it never mobilized more than 60,000 men, and never more than two-thirds of the army were on the Chaco at any one time. Paraguay, on the other hand, mobilized its entire army. Paraguay's war effort was a total one. Buses were commandeered to transport troops, wedding rings were donated to buy weapons, and by 1935 Paraguay had widened conscription to include 17-year-olds and policemen.
While both armies deployed a significant number of cavalry regiments, these actually served as infantry, since it was soon learned that the Chaco could not provide enough water and forage for horses. Only a relatively few mounted squadrons carried out reconnaissance missions at divisional level. In the course of the conflict, Paraguayan factories developed their own type of hand grenade, the "carumbe'i" (Guaraní for "little turtle") and produced trailers, artillery grenades and aerial bombs. The Paraguayan war effort was centralized and led by the state-owned national dockyards, managed by José Bozzano. The Paraguayan army received the first consignment of "carumbe'i" grenades in January 1933.
The Paraguayans took advantage of their ability to communicate over the radio in Guaraní, a language not spoken by the average Bolivian soldier. Paraguay had little trouble in transporting its army in large barges and gunboats on the Paraguay River to Puerto Casado, and from there directly to the front lines by railway, while the majority of Bolivian troops had to come from the western highlands, some 800 km away and with little or no logistic support. In fact, it took a Bolivian soldier about 14 days to traverse the distance, while a Paraguayan soldier only took about four. The heavy equipment used by Bolivia's army made things even worse. The water supply in the dry climate of the region played a key role during the conflict. There were thousands of non-combat casualties due to dehydration, mostly among Bolivian troops.
Air and naval assets.
The Chaco War is also important historically as the first instance of large-scale aerial warfare to take place in the Americas. Both sides used obsolete single-engined biplane fighter-bombers; the Paraguayans deployed 14 Potez 25s, while the Bolivians made extensive use of at least 20 CW-14 Ospreys. Despite an international arms embargo imposed by the League of Nations, Bolivia in particular went to great lengths in trying to import a small number of Curtiss T-32 Condor II twin-engined bombers disguised as civil transport planes, but they were stopped in Peru before they could be delivered.
The Paraguayan navy played a key role in the conflict by carrying thousands of troops and tons of supplies to the front lines via the Paraguay River, as well as by providing anti-aircraft support to transport ships and port facilities.
Two Italian-built gunboats, the "Humaitá" and "Paraguay" ferried troops to Puerto Casado. On 22 December 1932 three Bolivian Vickers Vespas attacked the Paraguayan riverine outpost of Bahía Negra, on the Paraguay River, killing an army colonel, but one of the aircraft was shot down by the gunboat "Tacuary". The two surviving Vespas met another gunboat, the "Humaitá", while flying downriver. Paraguayan sources claim that one of them was damaged. Conversely, the Bolivian army reported that the "Humaitá" limped back to Asunción seriously damaged. Although the Paraguayan navy admitted that "Humaitá" was struck by machine gun fire from the aircraft, they claimed that her armour shield averted damage.
Shortly before 29 March 1933 a Bolivian Osprey was shot down over the Paraguay River, while on 27 April a strike force of six Ospreys launched a successful mission from their base at Muñoz against the logistic riverine base and town of Puerto Casado, although the strong diplomatic reaction of Argentina prevented any further strategic attacks on targets along the Paraguay River. On 26 November 1934, the Brazilian steamer "Paraguay" was strafed and bombed by mistake by Bolivian aircraft while sailing the Paraguay River near Puerto Mihanovich. The Brazilian government sent 11 naval planes to the area, and its navy begun to convoy shipping on the river.
The Paraguayan navy air service was also very active in the conflict, harassing Bolivian troops deployed along the northern front with flying boats. The aircraft were moored at Bahía Negra Naval Air Base, and consisted of two Macchi M.18s. These seaplanes carried out the first night air attack in South America when they raided the Bolivian outposts of Vitriones and San Juan, on 22 December 1934. Every year since then, the Paraguayan navy celebrates the "Day of the Naval Air Service" on the anniversary of the action.
The Bolivian army deployed at least ten locally-built patrol boats and transport vessels during the conflict, mostly to ship military supplies to the northern Chaco through the Mamoré-Madeira system. The transport ships "Presidente Saavedra" and "Presidente Siles" steamed on the Paraguay River from 1927 until the beginning of the war, when both units were sold to private companies. The 50-ton armed launch "Tahuamanu", based in the Mamoré-Madeira fluvial system, was briefly transferred to Laguna Cáceres to ferry troops downriver from Puerto Suárez, challenging for eight months the Paraguayan naval presence in Bahía Negra. She was withdrawn to the Itenez River in northern Bolivia after Bolivian aerial reconnaissance revealed the actual strength of the Paraguayan navy in the area.
Conflict.
Pitiantuta Lake incident.
On June 15, 1932, a Bolivian detachment captured and burned to the ground the Fortín Carlos Antonio López at Pitiantutá Lake, disobeying explicit orders by Bolivian President Daniel Salamanca to avoid provocations in the Chaco region. One month later, on July 16, a Paraguayan detachment evicted the Bolivian troops from the area. The lake was at and had been discovered by Paraguayan explorers in March 1931, but the Bolivian High Command was unaware of this when one of its aircraft spotted the lake on April 1932.
After the initial incident, Salamanca changed his status quo policy over the disputed area and ordered the outposts of Corrales, Toledo and Boquerón to be captured. The three were soon taken, and in response Paraguay called for a Bolivian withdrawal. Salamanca instead demanded that they be included in a "zone of dispute". On a memorandum directed to President Salamanca on August 30, Bolivian Gen. Filiberto Osorio expressed his concerns over the lack of a plan of operations, and attached a plan of operations focusing on an offensive from the north. At the same time Bolivian Gen. Quintanilla asked for permission to capture two additional Paraguayan garrisons—Nanawa and Rojas Silva. During August Bolivia slowly reinforced its 4,000-men-strong First Bolivian Army, located in the zone of conflict, with 6,000 men.
The breaking of the fragile status quo in the disputed areas of the Chaco by Bolivia convinced Paraguay that a diplomatic solution on agreeable terms was not possible. Paraguay gave its general staff orders to recapture the three forts. During August Paraguay mobilized over 10,000 troops and sent them into the Chaco region. Paraguayan Lieutenant Colonel José Félix Estigarribia prepared for a large offensive before the Bolivians would have mobilized their whole army.
First Paraguayan offensive.
Fortín Boquerón was the first target of the Paraguayan offensive. The Boquerón complex, guarded by 619 Bolivian troops, resisted a 22-day siege by a 5,000-man Paraguayan force. An additional 2,500 Bolivians attempted to relieve the siege from the southwest but were beaten back by 2,200 Paraguayans who defended the accesses to the siege area. A few Bolivian units managed to enter Fortín Boquerón with supplies and the Bolivian Air Force dropped food and ammunition to the besieged soldiers. Having begun on 9 September, the siege ended when Fortín Boquerón finally fell on 29 September 1932.
After the fall of Fortín Boquerón, the Paraguayans continued their offensive and executed a pincer movement, which forced parts of the Bolivian force to surrender. While the Paraguayans had expected to lay a new siege on Fortín Arce, the most advanced Bolivian outpost in the Chaco, when they got there they found it in ruins. The 4,000 Bolivians who defended Arce had retreated to Fortín Alihuatá and Saveedra.
Bolivian offensive.
In December 1932 Bolivian war mobilization had concluded. In terms of weaponry and manpower, its army was ready to virtually overpower the Paraguayans. Gen. Hans Kundt, a former German officer who was a veteran of fighting on the Eastern Front in World War I, was called by President Salamanca to lead the Bolivian counteroffensive. Kundt had intermittently sojourned in Bolivia since the beginning of the century, establishing good relationships with members of the Bolivian political elite. Before the First World War he had been in the service of the Bolivian army as a trainer and counselor, and thus enjoyed great prestige in Bolivia for having, to some extent, shaped the Bolivian army, and also for his services in the army of the German Empire.
The Paraguayan Fortín Nanawa was chosen as the main target of the Bolivian offensive, to be followed by the command centre at Isla Poí. Their capture would allow Bolivia to reach the Paraguay River, putting the Paraguayan city of Concepción in danger. The capture of the fortines of Corrales, Toledo and Fernández by the Bolivian Second Corps were also part of Kundt's offensive plan.
In January 1933 the Bolivian First Corps began its attack on Fortín Nanawa. This stronghold was considered by the Paraguayans to be the backbone of their defenses. It had zig-zag trenches, miles of barbed wire and many machine-gun nests (some embedded in tree trunks). The Bolivian troops had previously stormed the nearby Paraguayan outpost of Mariscal López, isolating Nanawa from the south. On January 20, 1933, Kundt, in personal command of the Bolivian force, launched six to nine aircraft and 6,000 unhorsed cavalry, supported by 12 Vickers machine guns. However, the Bolivians failed to capture the fort and instead formed a defensive amphitheater in front of it. The Second Corps managed to capture Fortín Corrales and Fortín Platanillos but failed to take Fortín Fernández and Fortín Toledo. After a siege that lasted from February 26-March 11, 1933, the Second Corps aborted its attack on Fortín Toledo and withdrew to a defensive line built 15 km from Fortín Corrales.
After the ill-fated attack on Nanawa and the failures at Fernández and Toledo, Kundt ordered an assault on Fortín Alihuatá. The attack on this fortín overwhelmed its few defenders. The capture of Alihuatá allowed the Bolivians to cut the supply route of the Paraguayan First Division. When the Bolivians were informed of the isolation of the First Division, they launched an attack on it. This attack led to the Battle of Campo Jordán, which concluded in the retreat of the Paraguayan First Division to Gondra.
In July 1933 Kundt, still focusing on capturing Nanawa, launched a massive frontal attack on the fortín, in what came to be known as the Second Battle of Nanawa. Kundt had prepared for the second attack in detail, using artillery, airplanes, tanks and flamethrowers to overcome Paraguayan fortifications. The Paraguayans, however, had improved existing fortifications and built new ones since the first battle of Nanawa. While the Bolivian two-pronged attack managed to capture parts of the defensive complex, these were soon retaken by Paraguayan counterattacks made by reserves. The Bolivians lost more than 2,000 men injured and killed in the second battle of Nanawa, while Paraguay lost only 559 men injured and dead. The failure to capture Nanawa and the heavy loss of life led President Salamanca to criticize the Bolivian high command, ordering them to spare more men. The defeat seriously damaged Kundt's prestige. In September he resigned his position as commander in chief, but his resignation was not accepted by the president. Nanawa was a major turning point in the war, because the Paraguayan army regained the strategic initiative that had belonged to the Bolivians since the beginning of 1933.
Second Paraguayan offensive.
In September Paraguay began a new offensive in the form of three separate encirclement movements in the Alihuatá area, which was chosen because Bolivian forces there had been weakened by the transfer of soldiers to attack Fortín Gondra. As a result of the encirclement campaign, the Bolivian regiments Loa and Ballivián, totaling 509 men, surrendered. The Junín regiment suffered the same fate, but the Chacaltaya regiment was able to escape encirclement due to intervention of two other Bolivian regiments.
The success of the Paraguayan army led Paraguayan President Eusebio Ayala to travel to the Chaco to promote José Félix Estigarribia to the rank of general. In that meeting the president approved Estigarribia's new offensive plan. On the other side, the Bolivians gave up their initial plan of reaching the Paraguayan capital of Asunción and moved on to defensive and attrition warfare.
The Paraguayan army executed a large-scale pincer movement against Fortín Alihuatá, repeating the previous success of these operations. Seven thousand Bolivian troops had to evacuate Fortín Alihuatá. On December 10, 1933, the Paraguayans finished the encirclement of the 9th and 4th divisions of the Bolivian army. After unsuccessful attempts to break through Paraguayan lines and having suffered 2,600 dead, 7,500 Bolivian soldiers surrendered. Only 900 Bolivian troops managed to slip away. The Paraguayans obtained 8,000 rifles, 536 machine guns, 25 mortars, two tanks and 20 artillery pieces from the captured Bolivians. The remaining Bolivian troops withdrew to their headquarters at Muñoz, which was set on fire and evacuated on 18 December. General Kundt resigned as chief of staff of the Bolivian army.
Truce.
The massive defeat at Campo de Vía forced the Bolivian troops near Fortín Nanawa to withdraw northwest to form a new defensive line. Paraguayan Col. Rafael Franco proposed to launch a new attack against Ballivián and Villa Montes, but was turned down, as Paraguayan President Eusebio Ayala thought Paraguay had already won the war. A 20-day ceasefire was agreed upon between the warring parties on December 19, 1933. On January 6, 1934, when the armistice expired, Bolivia had reorganized its eroded army, having assembled a larger force than the one involved in its first offensive.
Third Paraguayan offensive.
By the beginning of 1934 Paraguayan Gen. Estigarribia was planning an offensive against the Bolivian garrison at Puerto Suárez, 145 km upriver from Bahía Negra. The Pantanal marshes and the lack of canoes to navigate through them convinced the Paraguayan commander to drop the idea and turn his attention to the main front. After the armistice's end the Paraguayan army continued its advance, capturing the outposts of Platanillos, Loa, Esteros and Jayucubás. After the battle of Campo de Vía in December the Bolivian army built up a defensive line at Magariños-La China. The Magariños-La China line was carefully built and considered to be one of the finest defensive lines of the Chaco War. However, a small Paraguayan attack on February 11, 1934, managed to breach the line, to the surprise of the Paraguayan command, forcing the abandonment of the whole defensive line. A Paraguayan offensive towards Cañada Tarija managed to surround and neutralize 1,000 Bolivian troops on March 27.
In May 1934 the Paraguayans detected a gap in the Bolivian defenses that would allow them to isolate the Bolivian stronghold of Ballivián and force its surrender. The Paraguayans worked at night to open a new route in the forests to make the attack possible. When Bolivian reconnaissance aircraft noticed this new path being opened in the forest, a plan was set up to let the Paraguayans enter halfway up the path and then attack them from the rear. The Bolivian operation resulted in the Battle of Cañada Strongest between May 18 and 25. The Bolivians managed to capture 67 Paraguayan officials and 1,389 soldiers. After their defeat at Cañada Strongest the Paraguayans continued their attempts to capture Ballivián. It was considered a key stronghold by the Bolivians, mostly for its symbolic position as the most southeastern Bolivian position left after the second Paraguayan offensive.
In November 1934 Paraguayan forces once again managed to surround and neutralize two Bolivian divisions at El Carmen. This disaster forced the Bolivians to abandon Ballivián and form a new defensive line at Villa Montes. On November 27, 1934, Bolivian generals confronted President Salamanca while he was visiting their headquarters in Villa Montes and forced him to resign, replacing him with the Vice President, José Luis Tejada. On November 9, 1934, the 12,000-man-strong Bolivian Cavalry Corps managed to capture Yrendagüé and put the Paraguayan army on the run. Yrendagüé was one of the few places with fresh water in that part of the Chaco and, while the Bolivian cavalry was marching towards La Faye from Yrendagüé, a Paraguayan force captured all the wells in Yrendague so that upon their return the exhausted and thirsty Bolivian troops found themselves without water; the already weakened force fell apart. Many were taken prisoner and a great number of those who avoided capture died of thirst and exposure after wandering aimlessly through the hot, dry forest. The Bolivian Cavalry Corps had previously been considered one of the best units of the new army formed after the armistice.
Last battles.
After the collapse of the northern and northeastern fronts, Bolivian defenses focused on the south to avoid the fall of their war headquarters/supply base at Villa Montes. The Paraguayans launched an attack towards Ybybobó, isolating a portion of the Bolivian forces on the Pilcomayo River. The battle began on 28 December 1934 and lasted until the early days of January 1935. The result was that 200 Bolivian troops were killed and 1,200 surrendered, with the Paraguayans losing only a few dozen men. Some fleeing Bolivian soldiers were reported to have jumped into the fast-flowing waters of the Pilcomayo River to avoid capture.
After this defeat the Bolivian army prepared for a last stand at Villa Montes. The loss of that base would allow the Paraguayans to reach the proper Andes. Col. Bernardino Bilbao Rioja and Col. Oscar Moscoso were left in charge of the defenses, after other high-ranking officers declined. On 11 January 1935 the Paraguayans encircled and forced the retreat of two Bolivian regiments. The Paraguayans also managed in January to cut off the road between Villa Montes and Santa Cruz.
Paraguayan commander-in-chief Gen. José Félix Estigarribia decided then to launch a final assault on Villa Montes. On 7 February 1935 some 5,000 Paraguayans attacked the heavily fortified Bolivian lines near Villa Montes, with the aim of capturing the oilfields at Nancarainza, but they were beaten back by the Bolivian First Cavalry Division. The Paraguayans lost 350 men and were forced to withdraw north toward Boyuibé. Estigarribia claimed that the defeat was largely due to the mountainous terrain, conditions in which his forces were not used to fighting. On 6 March, Estigarribia again focused all his efforts on the Bolivian oilfields, this time at Camiri, 130 km north of Villa Montes. The commander of the Paraguayan 3rd Corps, Gen. Franco, found a gap between the Bolivian 1st and 18th Infantry regiments and ordered his troops to attack through it, but they became stuck in a salient with no hope of further progress. The Bolivian Sixth Cavalry forced the hasty retreat of Franco's troops in order to avoid being cut off. The Paraguayans lost 84 troops taken prisoner and more than 500 dead were left behind. The Bolivians lost almost 200 men, although—unlike their exhausted enemies—they could afford a long battle of attrition. On 15 April the Paraguayans punched through the Bolivian lines on the Parapetí River, taking over the city of Charagua. The Bolivian command launched a counter-offensive that forced the Paraguayans back. Although the Bolivian plan fell short of its target of encircling an entire enemy division, they managed to take 475 prisoners on 25 April. On 4 June 1935 a Bolivian regiment was defeated and forced to surrender at Ingavi, in the northern front, after a last attempt at reaching the Paraguay River. On 12 June, the day the ceasefire agreement was signed, Paraguayan troops were entrenched only 15 km from the Bolivian oil fields in Cordillera Province.
While the military conflict ended with a comprehensive Paraguayan victory, from a wider point of view it was a disaster for both sides. Bolivia's Criollo elite forcibly impressed large numbers of the male indigenous population into the army, even though they felt little or no connection to the nation-state, while Paraguay was able to foment nationalist fervor among its predominantly mixed population. On both sides—but more so in the case of Bolivia—soldiers were ill-prepared for the dearth of water and the harsh conditions of terrain and weather they encountered. The effects of the lower-altitude climate had seriously impaired the effectiveness of the Bolivian army: most of its indigenous soldiers lived on the cold Altiplano at altitudes of over . They found themselves at a physical disadvantage when called upon to fight in tropical conditions at almost sea level. In fact, of the war's 100,000 casualties—about 57,000 of them Bolivian—more died from diseases such as malaria and other infections than from combat-related causes. At the same time, the war brought both countries to the brink of economic collapse.
Foreign involvement.
Arms embargo and commerce.
Since both countries were landlocked, imports of arms and other supplies from outside were limited to what the neighboring countries considered convenient or appropriate.
The Bolivian Army was dependent on food supplies that entered southeastern Bolivia from Argentina through Yacuíba. The army had great difficulty importing arms purchased at Vickers, since both Argentina and Chile were reluctant to let war material pass through their ports. The only remaining options were the port of Mollendo in Peru and Puerto Suárez on the Brazilian border. Eventually Bolivia achieved partial success after Vickers managed to persuade the British government to request that Argentina and Chile ease the import restrictions imposed on Bolivia. Internationally, the neighboring countries of Peru, Chile, Brazil and Argentina tried to avoid being accused of fueling the conflict and therefore limited the imports of arms to both Bolivia and Paraguay, although Argentina supported Paraguay behind the neutrality façade. Paraguay received military supplies and daily intelligence from Argentina, which also provided Paraguay with critical economic and military backing throughout the war.
The Argentine Army established a special detachment along the border with Bolivia and Paraguay at Formosa in September 1932, called "Destacamento Mixto Formosa", in order to deal with deserters from both sides trying to cross into Argentine territory and to prevent any boundary crossing by the warring armies, although the cross-border exchange with the Bolivian army was banned only in early 1934, after a formal protest by the Paraguayan government. By the end of the war 15,000 Bolivian soldiers had deserted to Argentina. Some native tribes living on the Argentine bank of the Pilcomayo, like the Wichí and Toba people, were often fired at from the other side of the frontier or strafed by Bolivian aircraft, while a number of members of the Maká tribe from Paraguay, led by deserters who had looted a farm on the border and killed some of its inhabitants, were engaged by Argentine forces in 1933. The Maká had been trained and armed by the Paraguayans for reconnaissance missions. After the defeat of the Bolivian army at Campo Vía, at least one former Bolivian border outpost, Fortin Sorpresa Viejo, was occupied by Argentine troops in December 1933. This led to a minor incident with Paraguayan forces.
Advisers and volunteers.
A number of volunteers and hired personnel from different countries participated in the war on both sides. The high command staff of both countries was at times dominated by Europeans. In Bolivia, Gen. Hans Kundt, a German First World War Eastern Front veteran, was in command from the beginning of the war until December 1933, when he was relieved due to a series of military setbacks. Apart from Kundt, Bolivia had also been advised in the last years of the war from a Czech military mission made of First World War veterans. The Czech military mission assisted the Bolivian military after the defeat of Campo Vía. Paraguay was getting input from 80 former White Russian officers, including two generals, Ern and Belaieff; the latter was part of Gen. Pyotr Wrangel's staff during the Russian Civil War. In the later phase of the war Paraguay would receive training from a large-scale Italian mission. 
Bolivia had more than 107 Chileans fighting on its side. Three died from different causes in the last year of the conflict. The Chileans involved in the war enrolled privately and were mostly military and police officers. They were partly motivated by the unemployment caused by both the Great Depression and the political turbulence in Chile in the early 1930s (after the Chaco War ended some of the Chilean officers went on to fight in the International Brigades during the Spanish Civil War). The arrival of the first group of Chilean combatants in La Paz sparked protests from Paraguay and led the Chilean Congress on 7 September 1934 to approve a law that made it illegal to join the armies of countries at war. This did not, however, stop the enrollment of Chileans in the Bolivian army, and it has been argued that Chilean President Arturo Alessandri Palma secretly approved of the practice in order to get rid of potentially troublesome elements of the military.
The enrollment of Chilean military personnel in the Bolivian army caused surprise in Paraguay, since former Chilean president Gen. Carlos Ibáñez del Campo in 1928 had supported Paraguay after the Bolivian reprisals for the destruction of Fortin Vanguardia. The Paraguayan press denounced the Chilean government as not being neutral and went on to claim that the Chilean soldiers were mercenaries. On 12 August 1934 the Chilean ambassador in Asunción was recalled back to Santiago in response to official Paraguayan support of the accusations against the Chilean government in the press. Early in the war, however, a few Chilean officers had joined the Paraguayan army.
At least two Uruguayan military pilots, Benito Sánchez Leyton and Luis Tuya, volunteered for some of the most daring missions carried out by Paraguayan Air Force Potez 25s, like the resupply of besieged forces during the Battle of Cañada Strongest and the mass air strike on the Bolivian stronghold of Ballivián on 8 July 1934. During the relief mission on Cañada Strongest, Leyton's Potez nº 7 managed to come back home despite having been hit by almost 200 rounds.
Argentina was a source of arms and ammunition for Paraguay. The Argentine military attaché in Asuncion, Col. Schweizer, continued to advise the Paraguayan command well after the start of hostilities. However, the more valuable contribution to the Paraguayan cause came from Argentine military intelligence (G2), led by Col. Esteban Vacareyza, which provided nightly reports on Bolivian movements and supply lines running along the border with Argentina. Argentine First World War veteran pilot Vicente Almandoz Almonacid was appointed Director of Military Aviation from 1932 to 1933.
The open Argentine support for Paraguay was also reflected on the battlefield when a number of Argentine citizens, largely from Corrientes and Entre Ríos, volunteered for the Paraguayan army. Most of them served in the 7th Cavalry Regiment "General San Martín" as infantrymen. They fought against the Bolivian Regiments "Ingavi" and "Warnes" at the outpost of Corrales on 1 January 1933, where they had a narrow escape after being outnumbered by the Bolivians. The commander of the "Warnes" Regiment, Lt. Col. Sánchez, was killed in an ambush set up by the retreating forces, while the volunteers lost seven trucks. The greatest achievement of "San Martín" took place on 10 December 1933, when the First Squadron, led by 2nd Lieutenant Javier Gustavo Schreiber, ambushed and captured the two surviving Bolivian Vickers six-ton tanks on the Alihuatá-Savedra road, in the course of the battle of Campo Vía.
Aftermath.
By the time a ceasefire was negotiated for noon June 10, 1935, Paraguay controlled most of the region. In the last half-hour there was a senseless shootout between the armies. This was recognized in a 1938 truce, signed in Buenos Aires in Argentina and approved in a referendum in Paraguay, by which Paraguay was awarded three-quarters of the Chaco Boreal, . Two Paraguayans and three Bolivians died for every square mile. Bolivia did get the remaining territory that bordered Puerto Busch.
Over the succeeding 77 years, no commercial amounts of oil or gas were discovered in the portion of the Chaco awarded to Paraguay, until 26 November 2012, when Paraguayan President Federico Franco announced the discovery of oil reserves in the area of the Pirity river. According to Franco these oilfields will make Paraguay an oil-producing nation by mid-2013. The President claimed that "in the name of the 30,000 Paraguayans who died in the war" the Chaco will become the richest oil-bearing region in South America. Oil and gas resources extend also from the Villa Montes area and the portion of the Chaco awarded to Bolivia northward along the foothills of the Andes. Today these fields give Bolivia the second largest resources of natural gas in South America after Venezuela.
Paraguay captured 21,000 Bolivian soldiers and 10,000 civilians (1% of the Bolivian population); many chose to stay in Paraguay after the war. In addition, 10,000 Bolivian troops—many of them ill-trained and ill-equipped conscripts—deserted to Argentina or injured or mutilated themselves to avoid combat. Paraguay also captured 2,300 machine guns, 28,000 rifles and ammunition worth $10 million (enough to last 40 years).
Bolivia's stunning military blunders during the Chaco War led to a mass movement known as the "Generación del Chaco", away from the traditional order, which was epitomised by the MNR-led Revolution of 1952.
A final treaty clearly marking the boundaries between the two countries was not signed until April 28, 2009, in Buenos Aires.
Cultural references.
Augusto Céspedes, the Bolivian ambassador to UNESCO, and one of the most important Bolivian writers of the 20th century, has written several books describing different aspects of the conflict. As a war reporter for the newspaper "El Universal" Céspedes had witnessed the penuries of the war, which he described in "Crónicas heroicas de una guerra estúpida" ("Heroic Chronicles of a stupid war") among other books. Several of his fiction works, considered masterworks of the genre, have also the Chaco War conflict as setting. Another diplomat and important figure of Bolivian literature, Adolfo Costa du Rels, has written about the conflict, his novel "Laguna H3" published in 1938 is also set in the Chaco War.
One of the masterpieces of Paraguayan writer Augusto Roa Bastos, the 1960 novel "Hijo de Hombre", describes in one of its chapters the carnage and harsh war conditions during the siege of Boquerón. The author himself took part in the conflict, joining the army medical service at the age of 17. The Argentine film "Hijo de Hombre", directed by Lucas Demare in 1961 is based on this part of the novel.
In Pablo Neruda's poem, "Standard Oil Company", Neruda refers to the Chaco War in the context of the influences that oil companies had on the existence of the war.
Howard Chaykin's TV miniseries "Dominic Fortune" (2009) begins with the title character working as a mercenary pilot in the Chaco War.
The Chaco War inspired Lester Dent to write the Doc Savage adventure novel, "The Dust of Death", also published in 1935.
The Chaco War formed the backdrop for the American film "Storm Over the Andes" (1935) by Christy Cabanne, and the Argentine-Paraguayan movie "Hamaca paraguaya" (2006) by Paz Encina.
Some aspects of the Chaco War are the inspiration for "The Adventures of Tintin" comic "The Broken Ear" by Hergé, which began publication in 1935.
The Chaco War, particularly the brutal battle of Nanawa, plays an important role in the adventure novel "Wings of Fury", by "R.N. Vick."
The Paraguayan polka, ""Regimiento 13 Tuyutí", composed by Ramón Vargas Colman and written in Guaraní by Emiliano R. Fernández, remembers the Paraguayan Fifth Division and its exploits in the battles around Nanawa, in which Fernández fought and was injured. On the other side, the siege of Boquerón inspired ""Boquerón abandonado", a Bolivian "tonada" recorded by Bolivian folk singer and politician Zulma Yugar in 1982.

</doc>
<doc id="55964" url="https://en.wikipedia.org/wiki?curid=55964" title="Lennox Lewis">
Lennox Lewis

Lennox Claudius Lewis, CM, CBE, (born 2 September 1965) is a retired boxer and the last undisputed world heavyweight champion. He holds dual British and Canadian citizenship. As an amateur boxer he won a gold medal representing Canada at the 1988 Olympic Games after defeating future heavyweight champion Riddick Bowe in the final. Lewis became undisputed champion by defeating Evander Holyfield, successfully unifiying his WBC title with Holyfield's WBA and IBF titles. With that victory, he was also awarded the vacant IBO title. Lewis is regarded by many as one of the greatest heavyweight boxers of all time, and also as the greatest British fighter of all time.
Lewis turned professional in 1989, winning his first 21 fights before knocking out Donovan Ruddock in 1992 to take over the number one position in the WBC rankings. He was declared WBC heavyweight champion in December 1992 after Riddick Bowe was stripped of the title. Lewis lost the title to Oliver McCall in 1994, but defeated McCall in a rematch to win the vacant WBC title in 1997. He defended the title four times before becoming the lineal champion when he beat Shannon Briggs by knockout in 1998. Lewis became undisputed champion when he defeated Evander Holyfield in 1999. In 2001 Lewis was knocked out by Hasim Rahman, but again avenged the defeat in a rematch later in the year to regain his titles. After defeating Mike Tyson by knockout in 2002 and stopping Vitali Klitschko in 2003, Lewis retired from boxing in 2004, having beaten every opponent he ever faced in professional competition.
Lewis is tall and has an reach. During his boxing prime, he weighed about . Lewis often referred to himself as "the pugilist specialist."
Early life.
Lewis was born on 2 September 1965, in London, England to parents born in Jamaica. At birth he weighed , and was given the name "Lennox" by the doctor, who said he looked like a Lennox. Lewis moved to Kitchener, Ontario, Canada in 1977 at the age of 12. He attended Cameron Heights Collegiate Institute for high school, where he excelled in Canadian football, soccer and basketball. In the 1982–83 school year, he helped the school's AAA basketball team win the Ontario provincial championship.
Amateur career.
Lewis eventually decided that his favourite sport was boxing. He became a dominant amateur boxer and won the world amateur junior title in 1983.
At the age of 18, Lewis represented Canada as a super heavyweight at the 1984 Summer Olympics in Los Angeles. He advanced to the quarter-finals, where he lost a fight by decision to American Tyrell Biggs, the eventual gold medalist.
Lewis chose not to turn professional after the Olympics, and instead fought four more years as an amateur, hoping for a second chance to win a gold medal. At the 1986 World Amateur Boxing Championships, he lost in the preliminary round to Petar Stoymenov of Bulgaria. Lewis won the Commonwealth Games Gold medal in 1986. After winning several more amateur titles during those years, he travelled to Seoul, South Korea for the 1988 Summer Olympics and achieved his goal. In the gold medal match, Lewis defeated future world champion Riddick Bowe by a second round referee stopped contest (RSC). He was Canada's flag bearer at the Games' closing ceremony.
Professional boxing career.
Having achieved his goal, Lewis declared himself a professional and moved back to his native England. He claimed he had always considered himself British, but many British fans regarded him as "a Canadian at heart and a Briton for convenience." In 2015 Lewis explained "When I turned pro, I had to go to the United Kingdom in order to pursue my career. The infrastructure to develop boxers wasn't in Canada then."
Lewis signed with boxing promoter Frank Maloney and his early professional career was filled with knockouts of journeymen. After he signed with American promoter Main Events, he won the European heavyweight title in 1990 against Frenchman Jean Maurice Chanet. In his next fight in March 1991, Lewis won the British title against undefeated, world-ranked Gary Mason, and in April 1992 won the Commonwealth title against Derek Williams.
Lewis was a top-five world heavyweight. He defeated former WBA heavyweight champion Mike Weaver, 1984 Olympic Gold medalist Tyrell Biggs, former world cruiserweight title holders Glenn McCrory and Osvaldo Ocasio, and journeymen Levi Billups and Mike Dixon.
WBC champion.
On 31 October 1992, Lewis knocked out Canadian Donovan "Razor" Ruddock in two rounds for the number one contender's position in the WBC rankings. It was Lewis' most impressive win to date, and established him as one of the world's best heavyweights. Sportscaster Larry Merchant declared, "We have a great new heavyweight."
The win over Ruddock made Lewis the number one contender for Riddick Bowe's heavyweight championship. Bowe refused to face Lewis, and held a press conference to dump his title in a trash can and relinquish it. On 14 December 1992, the WBC declared Lewis its champion, making him the first world heavyweight titleholder from Britain in the 20th century.
Lewis defended the belt three times, defeating Tony Tucker, whom he knocked down for the first time in Tucker's career, and he followed this with knockout victories over Frank Bruno and Phil Jackson. The Lennox Lewis vs. Frank Bruno fight was the first time two British-born boxers fought for a version of the world heavyweight title in the modern era.
Loss to McCall.
Lewis lost his WBC title to Oliver McCall on 24 September 1994 in a huge upset at the Wembley Arena in London. In the second round, McCall landed a powerful right hook, putting Lewis on his back. Lewis returned to his feet at the count of six, but stumbled forward into the referee in a daze. Referee Jose Guadalupe Garcia felt Lewis was unable to continue and ended the fight, giving McCall the title by technical knockout. Lewis and others argued the stoppage was premature and that a champion should be given the benefit of the doubt. They also contended that Garcia, a Mexican referee working for the Mexican-based WBC, had been persuaded by promoter Don King to end the fight early if the opportunity arose, to bring back the heavyweight title to his promotional stable. In spite of the Lewis camp protests, "Boxing Monthly" editor Glynn Leach pointed out that Lewis "only seemed to recover his senses once the fight was waved off," and that "in the opinions of everyone I spoke to at ringside, the decision was correct." 
After the fight, Lewis decided he needed a new trainer to replace Pepe Correa, who had become increasingly difficult to work with. Correa denounced Lewis in public after being fired. Renowned trainer Emanuel Steward, who had been McCall's trainer during their fight, was Lewis' choice. Even before the fight with McCall, Steward had seen much potential in Lewis and immediately expressed a desire to work with him. He corrected several of Lewis' technical flaws, which included maintaining a more balanced stance, less reliance on his straight right hand, and a focus on using a strong, authoritative jab; the latter of which would become a hallmark of Lewis' style throughout the rest of his career. Their partnership lasted until Lewis' retirement, both having mutual praise and respect for each other to this day.
Regaining the WBC title.
In his first comeback fight, Lewis was given a chance to fight for the mandatory challenger position within the WBC and won it by knocking out American contender Lionel Butler. However, at the behest of promoter Don King, the WBC bypassed him and gave Mike Tyson the first chance at the title recently won by Briton Frank Bruno from Oliver McCall. Bruno had previously lost to both Lewis and Tyson.
Lewis had the number 1 contender's slot in the WBC rankings when he knocked out Australian Justin Fortune, then defeated former WBO Champion Tommy Morrison in October 1995, followed by Olympic gold medalist and former WBO champion Ray Mercer in a close majority decision in May 1996. Lewis successfully sued to force Tyson to make a mandatory defence of the WBC title against him or force him to give up the title, winning a four million dollar settlement from promoter Don King. Rather than fight Lewis, Tyson relinquished the WBC title to fight Evander Holyfield. The WBC title was declared vacant. This set up a rematch between Lewis and McCall, who met on 7 February 1997 in Las Vegas for the WBC title. 
In one of the strangest fights in boxing history, McCall (having lost the first three rounds) refused to box in the fourth and fifth rounds. He then began crying in the ring, forcing the referee to stop the fight and award Lewis the victory and the title.
As newly re-crowned WBC champion, Lewis successfully defended the title during 1997 against fellow Briton and former WBO world champion Henry Akinwande, who was disqualified after five rounds for excessive clinching. Lewis then met Poland's Andrew Golota, whom he knocked out in the first round. Lewis retained the WBC world title in 1998 when he knocked out lineal champion Shannon Briggs in five rounds (Briggs had recently outpointed George Foreman in a controversial fight to win the lineal title), and beat formerly-undefeated European champion Željko Mavrović from Croatia in a 12-round unanimous decision. Lewis stated in 2006 that his fight with Mavrovic was the most awkward win of his career.
Lewis vs. Holyfield.
On 13 March 1999, Lewis faced WBA and IBF title holder Evander Holyfield in New York City in what was supposed to be a heavyweight unification bout. Lewis fought a tactical fight, keeping Holyfield off balance with a long jab and peppering him with combinations almost at will. Although most observers believed Lewis had clearly won the fight, the bout was declared a draw, to much controversy. The raw statistics of the fight suggested the bout belonged to Lewis, who landed 348 punches compared to Holyfield's 130. Lewis also out-jabbed Holyfield 137 to 52. Judge Eugenia Williams, who scored the fight in Holyfield's favour, said she saw Lewis land fewer punches than Holyfield.
Lewis vs. Holyfield II.
The sanctioning bodies ordered a rematch. Eight months later in Las Vegas (13 November 1999), the two men fought again in a more open and entertaining contest than the original fight, with the two boxers having some heavy exchanges from rounds 6 to 9. The punch stats however still clearly favoured Lewis who landed 195 punches to Evander Holyfield's 137 punches, although interestingly Lewis landed 119 power shots and 76 jabs, showing a definite shift in his tactics from the first fight when he focused more on the jab. This time around the 3 Judges did score the fight unanimously (115–113, 116–112 & 117–111) all in favour of Lewis who became undisputed heavyweight champion of the World. The British public voted Lewis the 1999 BBC Sports Personality of the Year.
Reign as Undisputed Champion.
After Lewis defeated Holyfield the WBA ordered Lewis to defend the title against John Ruiz of Puerto Rico, who was then an obscure Don King fighter who had been made the WBA's #1-ranked contender. The WBA gave permission for Lewis to fight his WBC mandatory Michael Grant first if he would fight Ruiz next, to which Lewis agreed. Opposed to this, Ruiz's promoter challenged this decision in court on the basis of a clause in the Lewis-Holyfield rematch contract that said Lewis's first bout as undisputed champion would be against the WBA's number one contender. Lewis was therefore to be stripped of his WBA belt if he fought Grant first. It was because of this that the WBA instated its "Super Champion" title, giving unified titleholders who also hold a WBA belt more time to defend against mandatory challengers.
Lewis proceeded to fight the 6 ft 8 inch American Michael Grant who he considered the best contender available. He successfully defended his WBC, IBO & IBF titles against Grant with a second round knockout victory in Madison Square Garden in April 2000.
Later that same year, Lewis knocked out South African Francois Botha in two rounds in London, before winning a 12-round decision against New Zealander and IBF mandatory opponent, David Tua in Las Vegas.
Lewis vs. Rahman.
On 21 April 2001, Lewis was knocked out by 15-to-1 underdog Hasim Rahman in a bout in South Africa. Prior to the bout, Lewis had a role in the film "Ocean's Eleven" in which he "boxed" against Wladimir Klitschko.
Lewis vs. Rahman II.
Lewis immediately sought a rematch with the new champion; however, Rahman, now being promoted by Don King, tried to secure another opponent for his inaugural title defence. Lewis took Rahman to court to honour the rematch clause in their contract. Rahman was ordered to honour the clause and give Lewis a rematch in his first title defence. While promoting the rematch with Rahman on ESPN's Up Close, the fighters got into a brawl similar to the one between Muhammad Ali and Joe Frazier in front of Howard Cosell on "Wide World of Sports". Lewis regained the title on 17 November by outclassing and then knocking out Hasim Rahman in the fourth round of their rematch.
Lewis vs. Tyson.
On 8 June 2002, Lewis defended his title against Mike Tyson. Ticket sales were slow because they were priced as high as $2,400, but a crowd of 15,327 turned up to see boxing's then biggest event at the Pyramid Arena in Memphis, Tennessee. Tyson also had to pay Lewis $335,000 out of his purse for biting him at the news conference announcing the fight, which was originally scheduled for 6 April 2002 in Las Vegas. Las Vegas, however, rejected the fight because of Tyson's licensing problems and several other states refused Tyson a license before Memphis finally bid $12 million to land it.
By the end of the seventh round Tyson was tired and sluggish, his face swollen and his eyes cut. He was knocked out in the eighth by a right hook. After the fight, George Foreman declared, "He is, no doubt, the best heavyweight of all time. What he's done clearly puts him on top of the heap."
This was the highest-grossing event in pay-per-view history, generating $106.9 million from 1.95 million buys in the US, until it was surpassed by De La Hoya-Mayweather in 2007.
Lewis vs. Klitschko.
In May 2003, Lewis sued boxing promoter Don King for $385 million, claiming that King used threats and bribery to have Tyson pull out of a rematch with Lewis and a fight on the card of a Lewis title defence.
Lewis scheduled a fight with Kirk Johnson for June, but when Johnson suffered an injury in training, Lewis fought Vitali Klitschko, the WBC's No. 1 contender and former WBO champion. Lewis had planned to fight him in December, but since Klitschko had been on the undercard of the Johnson fight anyway, they agreed to square off on 21 June. Lewis entered the ring at a career high 256½ pounds. Lewis was dominated in the early rounds and was wobbled in round two by solid Klitschko punches. Lewis opened a cut above Klitschko's eye with a right cross in the third round and gave a better showing from the fourth round onwards. With both fighters looking tired before the start of round seven, the doctor advised that the fight should be stopped because of a severe cut above Klitschko's left eye, awarding Lewis victory by TKO. Klitschko was leading 58–56 on all three judges' scorecards when the fight was stopped.
Interviewed about the fight by HBO, doctor Paul Wallace explained his decision:
"When he raised his head up, his upper eyelid covered his field of vision. At that point I had no other option but to stop the fight. If he had to move his head to see me, there was no way he could defend his way against a punch."
Klitschko's face required sixty stitches.
Because Klitschko had fought so bravely against Lewis, boxing fans soon began calling for a rematch. The WBC agreed, and kept the Ukrainian as its No. 1 contender. Lewis initially was in favour of a rematch:
"I want the rematch, I enjoyed that fight. It was just a fight. We went at it. You have to play dollars and cents but I'm opting more for the rematch."
Negotiations for the rematch followed but Lewis changed his mind. Instead, Klitschko fought and defeated Kirk Johnson on 6 December in WBC Eliminator, setting up a mandatory rematch with Lewis. Lewis announced his retirement shortly thereafter in February 2004, to pursue other interests, including sports management and music promotion, and vacated the title. Lewis said he would not return to the ring. At his retirement, Lewis's record was 41 wins, 2 losses and 1 draw, with 32 wins by knockout.
Retirement.
Though it was rumoured in an article published by the "Daily Mail" on 24 February that he would return to fight Klitschko once again, Lewis quickly shot down those rumours on his personal website. In 2008 Lewis commented on a possible match up with Riddick Bowe. "He waits until I am in retirement to call out my name," said Lewis. "I will come out of retirement to beat up that guy. I'll beat him up for free."
Along with Gene Tunney and Rocky Marciano, Lewis is one of three world heavyweight champions to have retired with no unavenged defeats.
In 2008, Lewis was inducted into Canada's Sports Hall of Fame. In 2009, in his first year of eligibility, Lewis was inducted into the International Boxing Hall of Fame. He was inducted into the Ontario Sports Hall of Fame in 2012.
Lewis worked as a boxing analyst for HBO on "Boxing After Dark" from 2006 until 2010.
Outside boxing.
In 2000, Lewis appeared on Reflection Eternal's debut album Train of Thought, giving a shout out on the track "Down for the Count."
In 2001, Lewis had a role in the film "Ocean's Eleven" in which he "boxed" against Wladimir Klitschko.
In 2002, Lewis was reportedly offered £5m by World Wrestling Entertainment (WWE) chairman Vince McMahon to take up professional wrestling in his industry. His camp held discussions over a possible match with WWE superstar Brock Lesnar in February 2003, at the No Way Out pay-per-view event. Prior to the offer Lennox was familiar with wrestling; he was part of the famous match held in the old Wembley Stadium between The British Bulldog and Bret "The Hitman" Hart for the Intercontinental Championship at SummerSlam in 1992, representing the Bulldog during his entrance while bearing a Union Flag.
In 2003, Lewis made a brief cameo appearance in the Jennifer Lopez and LL Cool J video "All I Have".
In 2006 he appeared in the movie Johnny Was with Vinnie Jones.
Lewis played in the World Series of Poker in both 2006 and 2007, and was knocked out without winning any money.
Lewis appeared on NBC's "Celebrity Apprentice" in 2008. He came in fourth place (out of 14).
Lewis has also done a public service announcement against domestic violence for Do Something.
In 2011 he was awarded an honorary Doctorate from Wilfrid Laurier University in Waterloo, Ontario.
Lewis is a supporter of his home town football club, West Ham United.
Personal life.
Upon retiring from boxing, Lewis moved to Miami Beach with his wife, Violet Chang, a former Miss Jamaica runner-up. They have four children. Lewis told AventuraUSA.com in 2007 that he is contemplating opening an "international boxing academy" and perhaps one day starting a record label, but he has yet to embark on either endeavour. Lewis has a villa at the Tryall Club in Montego Bay, Jamaica.
Lewis is an avid amateur chess player, and funded an after-school chess programme for disadvantaged youth, one of whom earned a university chess scholarship at Tennessee Tech.
Amateur highlights.
Losses

</doc>
<doc id="55967" url="https://en.wikipedia.org/wiki?curid=55967" title="Communication disorder">
Communication disorder

A communication disorder is any disorder that affects an individual's ability to comprehend, detect, or apply language and speech to engage in discourse effectively with others. The delays and disorders can range from simple sound substitution to the inability to understand or use one's native language.
General definition.
Disorders and tendencies included and excluded under the category of communication disorders may vary by source. For example, the definitions offered by the American Speech–Language–Hearing Association differ from that of the Diagnostic Statistical Manual 4th edition (DSM-IV).
Gleanson (2001) defines a communication disorder as a speech and language disorder which refers to problems in communication and in related areas such as oral motor function. The delays and disorders can range from simple sound substitution to the inability to understand or use their native language.
In general, communications disorders commonly refer to problems in speech (comprehension and/or expression) that significantly interfere with an individual’s achievement and/or quality of life. Knowing the operational definition of the agency performing an assessment or giving a diagnosis may help.
Persons who speak more than one language or are considered to have an accent in their location of residence do not have speech disorders if they are speaking in a manner consistent with their home environment or a blending of their home and foreign environment.
DSM-IV-TR.
According to the DSM-IV-TR, communication disorders are usually first diagnosed in childhood or adolescence though they are not limited as childhood disorders and may persist into adulthood. They may also occur with other disorders.
Diagnosis involves testing and evaluation during which it is determined if the scores/performance are "substantially below" developmental expectations and if they "significantly" interfere with academic achievement, social interactions and daily living. This assessment may also determine if the characteristic is deviant or delayed. Therefore, it may be possible for an individual to have communication challenges but not meet the criteria of being "substantially below" criteria of the DSM IV-TR.
It should also be noted that the DSM diagnoses do not comprise a complete list of all communication disorders, for example, auditory processing disorder is not classified under the DSM or ICD-10.
The following diagnoses are included in the communication disorders:
Changes in DSM-5.
The DSM-5 diagnoses for communication disorders completely rework the ones stated above. The diagnoses are made more general in order to capture the various aspects of communications disorders in a way that emphasizes their childhood onset and differentiate these communications disorders from those associated with other disorders (i.e. autism spectrum disorders)
Examples.
Examples of disorders that may include or create challenges in language and communication and/or may co-occur with the above disorders:
Aphasia.
Aphasia is loss of the ability to produce or comprehend language. There are acute aphasias which result from stroke or brain injury, and primary progressive aphasias caused by progressive illnesses such as dementia.

</doc>
<doc id="55971" url="https://en.wikipedia.org/wiki?curid=55971" title="Inner ear">
Inner ear

The inner ear (internal ear, auris interna) is the innermost part of the vertebrate ear. In vertebrates, the inner ear is mainly responsible for sound detection and balance. In mammals, it consists of the bony labyrinth, a hollow cavity in the temporal bone of the skull with a system of passages comprising two main functional parts:
The inner ear is found in all vertebrates, with substantial variations in form and function. The inner ear is innervated by the eighth cranial nerve in all vertebrates.
Structure.
The labyrinth can be divided by layer or by region.
Bony vs. membranous.
The bony labyrinth, or osseous labyrinth, is the network of passages with bony walls lined with periosteum. The membranous labyrinth runs inside of the bony labyrinth. There is a layer of perilymph fluid between them. The three parts of the bony labyrinth are the vestibule of the ear, the semicircular canals, and the cochlea.
Vestibular vs. cochlear.
In the middle ear, the energy of pressure waves is translated into mechanical vibrations by the three auditory ossicles. Pressure waves move the tympanic membrane which in turns moves the malleus, the first bone of the middle ear. The malleus articulates to incus which connects to the stapes. The footplate of the stapes connects to the oval window, the beginning of the inner ear. When the stapes presses on the oval window, it causes the perilymph, the liquid of the inner ear to move. The middle ear thus serves to convert the energy from sound pressure waves to a force upon the perilymph of the inner ear. The oval window has only approximately 1/18 the area of the tympanic membrane and thus produces a higher pressure. The cochlea propagates these mechanical signals as waves in the fluid and membranes, and then converts them to nerve impulses which are transmitted to the brain.
The vestibular system is the region of the inner ear where the semicircular canals converge, close to the cochlea. The vestibular system works with the visual system to keep objects in view when the head is moved. Joint and muscle receptors are also important in maintaining balance. The brain receives, interprets, and processes the information from all these systems to create the sensation of balance.
The vestibular system of the inner ear is responsible for the sensations of balance and motion. It uses the same kinds of fluids and detection cells (hair cells) as the cochlea uses, and sends information to the brain about the attitude, rotation, and linear motion of the head. The type of motion or attitude detected by a hair cell depends on its associated mechanical structures, such as the curved tube of a semicircular canal or the calcium carbonate crystals (otolith) of the saccule and utricle.
Development.
The human inner ear develops during week 4 of embryonic development from the auditory placode, a thickening of the ectoderm which gives rise to the bipolar neurons of the cochlear and vestibular ganglions. As the auditory placode invaginates towards the embryonic mesoderm, it forms the auditory vesicle or "otocyst".
The auditory vesicle will give rise to the utricluar and saccular components of the membranous labyrinth. They contain the sensory hair cells and otoliths of the macula of utricle and of the saccule, respectively, which respond to linear acceleration and the force of gravity. The utricular division of the auditory vesicle also responds to angular acceleration, as well as the endolymphatic sac and duct that connect the saccule and utricle.
Beginning in the fifth week of development, the auditory vesicle also gives rise to the cochlear duct, which contains the spiral organ of Corti and the endolymph that accumulates in the membranous labyrinth. The vestibular wall will separate the cochlear duct from the perilymphatic scala vestibuli, a cavity inside the cochlea. The basilar membrane separates the cochlear duct from the scala tympani, a cavity within the cochlear labyrinth. The lateral wall of the cochlear duct is formed by the spiral ligament and the stria vascularis, which produces the endolymph. The hair cells develop from the lateral and medial ridges of the cochlear duct, which together with the tectorial membrane make up the organ of Corti.
[[Image:Gray925.png|right|450px|thumbnail|Right human membranous labyrinth, removed from its bony enclosure and viewed from the antero-lateral aspect. "Top image is antero-lateral and bottom image is postero-medial."
Histology.
Rosenthal's canal or the spiral canal of the cochlea is a section of the bony labyrinth of the inner ear that is approximately 30 mm long and makes 2¾ turns about the modiolus.
There are several specialized types of cell in the inner ear. Among these are hair cells, pillar cells, Boettcher's cells, Claudius' cells and Deiters' cells (phalangeal cells). 
The hair cells are the primary auditory receptor cells and they are also known as auditory sensory cells, acoustic hair cells, auditory cells or cells of Corti. The organ of Corti is lined with a single row of inner hair cells and three rows of outer hair cells. The hair cells have a hair bundle at the apical surface of the cell. The hair bundle consists of an array of actin-based stereocilia. Each stereocilium inserts as a rootlet into a dense filamentous actin mesh known as the cuticular plate. Disruption of these bundles results in hearing impairments and balance defects. 
Pillar cells are found in the organ of Corti and act as supporting cells for hair cells. They are divided into two types: inner and outer. Outer pillar cells are unique in that they are free standing cells with no contact to adjacent cells except at the bases and apices. Both types of pillar cell are characterized by the presence of thousands of cross linked microtubules and actin filaments in parallel orientation. They provide mechanical coupling between the basement membrane and the mechanoreceptors on the hair cells. 
Boettcher's cells are found in the organ of Corti where they are present only in the lower turn of the cochlea. They lie on the basilar membrane beneath Claudius' cells and are organized in rows with the number of rows the number of which varies between species. The cells interdigitate with each other, and project microvilli into the intercellular space. They are supporting cells for the auditory hair cells in the organ of Corti. They are named after German pathologist Arthur Böttcher (1831-1889).
Claudius' cells are found in the organ of Corti located above rows of Boettcher's cells. Like Boettcher's cells they are considered supporting cells for the auditory hair cells in the organ of Corti. They contain a variety of aquaporin water channels and appear to be involved in ion transport. They also play a role in sealing off endolymphatic spaces. They are named after the German anatomist Friedrich Matthias Claudius (1822-1869).
Deiters' cells (phalangeal cells) are a type of neuroglial cell found in the organ of Corti and organised in one row of inner phalangeal cells and three rows of outer phalangeal cells. They are the supporting cells of the hair cell area within the cochlea. They are named after the German pathologist Otto Deiters (1834-1863) who described them.
Hensen's cells are high columnar cells that are directly adjacent to the third row of Deiters’ cells.
Hensen's stripe is the section of the tectorial membrane above the inner hair cell.
Nuel's spaces refer to the fluid filled spaces between the outer pillar cells and adjacent hair cells and also the spaces between the outer hair cells.
Hardesty's membrane is the layer of the tectoria closest to the reticular lamina and overlying the outer hair cell region.
Reissner's membrane is composed of two cell layers and separates the scala media from the scala vestibuli.
Huschke's teeth are the tooth shaped ridges on the spiral limbus that are in contact with the tectoria and separated by interdental cells.
Physiology.
Neurons within the ear respond to simple tones, and the brain serves to process other increasingly complex sounds. An average adult is typically able to detect sounds ranging between 20 and 20,000 Hz. The ability to detect higher pitch sounds decreases in older humans. 
The human ear has evolved with two basic tools to encode sound waves; each are separate in detecting high and low frequency sounds. Georg von Békésy (1899-1972) employed the use of a microscope in order to examine the basilar membrane located within the inner-ear of cadavers. He found that movement of the basilar membrane resembles that of a traveling wave; the shape of which varies based on the frequency of the pitch. In low frequency sounds, the tip (apex) of the membrane moves the most, while in high frequency sounds, the base of the membrane moves most.
Clinical significance.
Interference with or infection of the labyrinth can result in a syndrome of ailments called labyrinthitis. The symptoms of labyrinthitis include temporary nausea, disorientation, vertigo, and dizziness. Labyrinthitis can be caused by viral infections, bacterial infections, or physical blockage of the inner ear.
Another condition has come to be known as autoimmune inner ear disease (AIED). It is characterized by idiopathic, rapidly progressive, bilateral sensorineural hearing loss. It is a fairly rare disorder while at the same time, a lack of proper diagnostic testing has meant that its precise incidence cannot be determined.
Comparative anatomy.
Birds have an auditory system similar to that of mammals, including a cochlea. Reptiles, amphibians, and fish do not have cochleas but hear with simpler auditory organs or vestibular organs, which generally detect lower-frequency sounds than the cochlea.
The cochlear system.
In reptiles, sound is transmitted to the inner ear by the stapes (stirrup) bone of the middle ear. This is pressed against the oval window, a membrane-covered opening on the surface of the vestibule. From here, sound waves are conducted through a short perilymphatic duct to a second opening, the round window, which equalizes pressure, allowing the incompressible fluid to move freely. Running parallel with the perilymphatic duct is a separate blind-ending duct, the lagena, filled with endolymph. The lagena is separated from the perilymphatic duct by a basilar membrane, and contains the sensory hair cells that finally translate the vibrations in the fluid into nerve signals. It is attached at one end to the saccule.
In most reptiles the perilymphatic duct and lagena are relatively short, and the sensory cells are confined to a small basilar papilla lying between them. However, in birds, mammals, and crocodilians, these structures become much larger and somewhat more complicated. In birds, crocodilians, and monotremes, the ducts are simply extended, together forming an elongated, more or less straight, tube. The endolymphatic duct is wrapped in a simple loop around the lagena, with the basilar membrane lying along one side. The first half of the duct is now referred to as the scala vestibuli, while the second half, which includes the basilar membrane, is called the scala tympani. As a result of this increase in length, the basilar membrane and papilla are both extended, with the latter developing into the organ of Corti, while the lagena is now called the cochlear duct. All of these structures together constitute the cochlea.
In mammals (other than monotremes), the cochlea is extended still further, becoming a coiled structure in order to accommodate its length within the head. The organ of Corti also has a more complex structure in mammals than it does in other amniotes.
The arrangement of the inner ear in living amphibians is, in most respects, similar to that of reptiles. However, they often lack a basilar papilla, having instead an entirely separate set of sensory cells at the upper edge of the saccule, referred to as the papilla amphibiorum, which appear to have the same function.
Although many fish are capable of hearing, the lagena is, at best, a short diverticulum of the saccule, and appears to have no role in sensation of sound. Various clusters of hair cells within the inner ear may instead be responsible; for example, bony fish contain a sensory cluster called the macula neglecta in the utricle that may have this function. Although fish have neither an outer nor a middle ear, sound may still be transmitted to the inner ear through the bones of the skull, or by the swim bladder, parts of which often lie close by in the body.
The vestibular system.
By comparison with the cochlear system, the vestibular system varies relatively little between the various groups of jawed vertebrates. The central part of the system consists of two chambers, the saccule and utricle, each of which includes one or two small clusters of sensory hair cells. All jawed vertebrates also possess three semicircular canals arising from the utricle, each with an ampulla containing sensory cells at one end.
An endolymphatic duct runs from the saccule up through the head, and ending close to the brain. In cartilaginous fish, this duct actually opens onto the top of the head, and in some teleosts, it is simply blind-ending. In all other species, however, it ends in an endolymphatic sac. In many reptiles, fish, and amphibians this sac may reach considerable size. In amphibians the sacs from either side may fuse into a single structure, which often extends down the length of the body, parallel with the spinal canal.
The primitive lampreys and hagfish, however, have a simpler system. The inner ear in these species consists of a single vestibular chamber, although in lampreys, this is associated with a series of sacs lined by cilia. Lampreys have only two semicircular canals, with the horizontal canal being absent, while hagfish have only a single, vertical, canal.
Equilibrium.
The inner ear is primarily responsible for balance, equilibrium and orientation in three-dimensional space. The inner ear can detect both static and dynamic equilibrium. Three semicircular ducts and two chambers, which contain the saccule and utricle, enable the body to detect any deviation from equilibrium. The macula sacculi detects vertical acceleration while the macula utriculi is responsible for horizontal acceleration. These microscopic structures possess stereocilia and one kinocilium which are located within the gelatinous otolithic membrane. The membrane is further weighted with otoliths. Movement of the stereocilia and kinocilium enable the hair cells of the saccula and utricle to detect motion. The semicircular ducts are responsible for detecting rotational movement. 
References.
Saladin, "Anatomy and Physiology" 6e, print
American Speech-Language-Hearing Association, The Middle Ear, http://www.asha.org/public/hearing/Middle-Ear/

</doc>
<doc id="55972" url="https://en.wikipedia.org/wiki?curid=55972" title="Middle ear">
Middle ear

The middle ear is the portion of the ear internal to the eardrum, and external to the oval window of the inner ear. Also the mammalian middle ear contains three ossicles, which transfer the vibrations of the eardrum into waves in the fluid and membranes of the inner ear. The hollow space of the middle ear is also known as the tympanic cavity. The auditory tube (also known as the Eustachian tube or the pharyngotympanic tube) joins the tympanic cavity with the nasal cavity (nasopharynx), allowing pressure to equalize between the middle ear and throat.
The primary function of the middle ear is to efficiently transfer acoustic energy from compression waves in air to fluid–membrane waves within the cochlea.
Structure.
Ossicles.
The middle ear contains three tiny bones known as the ossicles: "malleus", "incus", and "stapes". The ossicles were given their Latin names for their distinctive shapes; they are also referred to as the "hammer", "anvil", and "stirrup", respectively. The ossicles directly couple sound energy from the ear drum to the oval window of the cochlea. While the stapes is present in all tetrapods, the malleus and incus evolved from lower and upper jaw bones present in reptiles. See Evolution of mammalian auditory ossicles.
The ossicles are classically supposed to mechanically convert the vibrations of the eardrum, into amplified pressure waves in the fluid of the cochlea (or inner ear) with a lever arm factor of 1.3. Since the effective vibratory area of the eardrum is about 14 fold larger than that of the oval window, the sound pressure is concentrated, leading to a pressure gain of at least 18.1. The eardrum is merged to the malleus, which connects to the incus, which in turn connects to the stapes. Vibrations of the stapes footplate introduce pressure waves in the inner ear. There is a steadily increasing body of evidence that shows that the lever arm ratio is actually variable, depending on frequency. Between 0.1 and 1 kHz it is approximately 2, it then rises to around 5 at 2 kHz and then falls off steadily above this frequency. The measurement of this lever arm ratio is also somewhat complicated by the fact that the ratio is generally given in relation to the tip of the malleus (also known as the umbo) and the level of the middle of the stapes. The eardrum is actually attached to the malleus handle over about a 0.5 cm distance. In addition the eardrum itself moves in a very chaotic fashion at frequencies >3 kHz. The linear attachment of the eardrum to the malleus actually smooths out this chaotic motion and allows the ear to respond linearly over a wider frequency range than a point attachment. The auditory ossicles can also reduce sound pressure (the inner ear is very sensitive to overstimulation), by uncoupling each other through particular muscles.
The middle ear efficiency peaks at a frequency of around 1 kHz. The combined transfer function of the outer ear and middle ear gives humans a peak sensitivity to frequencies between 1 kHz and 3 kHz.
Muscles.
The movement of the ossicles may be stiffened by two muscles. The stapedius muscle, the smallest skeletal muscle in the body, connects to the stapes and is controlled by the facial nerve; the tensor tympani muscle connects to the base of the malleus and is under the control of the medial pterygoid nerve which is a branch of the mandibular nerve of the trigeminal nerve. It is also served by the nerve to the stapedius. These muscles contract in response to loud sounds, thereby reducing the transmission of sound to the inner ear. This is called the acoustic reflex or Tympanic reflex.
Nerves.
Of surgical importance are two branches of the facial nerve that also pass through the middle ear space. These are the horizontal portion of the facial nerve and the "chorda tympani". Damage to the horizontal branch during ear surgery can lead to paralysis of the face (same side of the face as the ear). The chorda tympani is the branch of the facial nerve that carries taste to the ipsilateral half of the tongue.
Function.
Sound transfer.
Ordinarily, when sound waves in air strike liquid, most of the energy is reflected off the surface of the liquid. The middle ear allows the impedance matching of sound traveling in air to acoustic waves traveling in a system of fluids and membranes in the inner ear. This system should not be confused, however, with the propagation of sound as compression waves in liquid.
The middle ear couples sound from air to the fluid via the oval window, using the principle of "mechanical advantage" in the form of the "hydraulic principle" and the "lever principle". The vibratory portion of the tympanic membrane (eardrum) is many times the surface area of the footplate of the stapes (the third ossicular bone which attaches to the oval window); furthermore, the shape of the articulated ossicular chain is like a lever, the long arm being the long process of the malleus, the fulcrum being the body of the incus, and the short arm being the lenticular process of the incus. The collected pressure of sound vibration that strikes the tympanic membrane is therefore concentrated down to this much smaller area of the footplate, increasing the force but reducing the velocity and displacement, and thereby coupling the acoustic energy.
The middle ear is able to dampen sound conduction substantially when faced with very loud sound, by noise-induced reflex contraction of the middle-ear muscles.
Clinical significance.
The middle ear is hollow. If the animal moves to a high-altitude environment, or dives into the water, there will be a pressure difference between the middle ear and the outside environment. This pressure will pose a risk of bursting or otherwise damaging the tympanum if it is not relieved. If middle ear pressure remains low, the ear drum may become retracted into the middle ear. One of the functions of the Eustachian tubes that connect the middle ear to the nasopharynx is to help keep middle ear pressure the same as air pressure. The Eustachian tubes are normally pinched off at the nose end, to prevent being clogged with mucus, but they may be opened by lowering and protruding the jaw; this is why yawning or chewing helps relieve the pressure felt in the ears when on board an aircraft.
Otitis media is an inflammation of the middle ear.
Other animals.
The middle ear of tetrapods is homologous with the spiracle of fishes, an opening from the pharynx to the side of the head in front of the main gill slits. In fish embryos, the spiracle forms as a pouch in the pharynx, which grows outward and breaches the skin to form an opening; in most tetrapods, this breach is never quite completed, and the final vestige of tissue separating it from the outside world becomes the eardrum. The inner part of the spiracle, still connected to the pharynx, forms the eustachian tube.
In reptiles, birds, and early fossil tetrapods, there is a single auditory ossicle, the columella ( that is homologous with the stapes, or "stirrup" of mammals). This is connected indirectly with the eardrum via a mostly cartilaginous extracolumella and medially to the inner-ear spaces via a widened footplate in the fenestra ovalis. The columella is an evolutionary derivative of the bone known as the hyomandibula in fish ancestors, a bone that supported the skull and braincase. 
The structure of the middle ear in living amphibians varies considerably, and is often degenerate. In most frogs and toads, it is similar to that of reptiles, but in other amphibians, the middle ear cavity is often absent. In these cases, the stapes either is also missing or, in the absence of an eardrum, connects to the quadrate bone in the skull, although, it is presumed, it still has some ability to transmit vibrations to the inner ear. In many amphibians, there is also a second auditory ossicle, the "operculum" (not to be confused with the structure of the same name in fishes). This is a flat, plate-like bone, overlying the fenestra ovalis, and connecting it either to the stapes or, via a special muscle, to the scapula. It is not found in any other vertebrates.
Mammals are unique in having evolved a three-ossicle middle-ear independently of the various single-ossicle middle ears of other land vertebrates, all during the Triassic period of geological history. Functionally, the mammalian middle ear is very similar to the single-ossicle ear of non-mammals, except that it responds to sounds of higher frequency, because these are better taken up by the inner ear (which also responds to higher frequencies than those of non-mammals). The malleus, or "hammer", evolved from the articular bone of the lower jaw, and the incus, or "anvil", from the quadrate. In other vertebrates, these bones form the primary jaw joint, but the expansion of the dentary bone in mammals led to the evolution of an entirely new jaw joint, freeing up the old joint to become part of the ear. For a period of time, both jaw joints existed together, one medially and one laterally. The evolutionary process leading to a three-ossicle middle ear was thus an "accidental" byproduct of the simultaneous evolution of the new, secondary jaw joint. In many mammals, the middle ear also becomes protected within a cavity, the auditory bulla, not found in other vertebrates. A bulla evolved late in time and independently numerous times in different mammalian clades, and it can be surrounded by membranes, cartilage or bone. The bulla in humans is part of the temporal bone.

</doc>
<doc id="55973" url="https://en.wikipedia.org/wiki?curid=55973" title="Outer ear">
Outer ear

The outer ear is the external portion of the ear, which consists of the auricle (also pinna) and the ear canal. It gathers sound energy and focuses it on the eardrum (tympanic membrane).
Structure.
Auricle.
The visible part is called the auricle, also known as the pinna, especially in other animals. It is composed of a thin plate of yellow elastic cartilage, covered with integument, and connected to the surrounding parts by ligaments and muscles; and to the commencement of the ear canal by fibrous tissue. Many mammals can move the pinna (with the auriculares muscles) in order to focus their hearing in a certain direction in much the same way that they can turn their eyes. Most humans, do not have this ability.
Ear canal.
From the pinna the sound waves move into the ear canal (also known as the "external acoustic meatus") a simple tube running through to the middle ear. This tube leads inward from the bottom of the auricula and conducts the vibrations to the tympanic cavity and amplifies frequencies in the range 3 kHz to 12 kHz.
Muscles.
Intrinsic muscles.
The intrinsic muscles of the external ear are the:
Extrinsic muscles.
The auricular muscles (or extrinsic muscles) are the three muscles surrounding the "auricula" or outer ear:
The superior muscles is the largest of the three, followed by the posterior and the anterior.
In some mammals these muscles can adjust the direction of the pinna. In humans these muscles possess very little action.
The auricularis anterior draws the auricula forward and upward; the Auricularis superior slightly raises it; and the Auricularis posterior draws it backward.
Function.
One consequence of the configuration of the outer ear is selectively to boost the sound pressure 30- to 100-fold for frequencies around 3 kHz. This amplification makes humans most sensitive to frequencies in this range — and also explains why they are particularly prone to acoustical injury and hearing loss near this frequency. Most human speech sounds are also distributed in the bandwidth around 3 kHz.
Clinical significance.
Malformations of the external ear can be a consequence of hereditary disease, or exposure to environmental factors such as radiation, infection. Such defects include:
Surgery.
Usually, malformations are treated with surgery, although artificial prostheses are also sometimes used.
If malformations are accompanied by hearing loss amenable to correction, then the early use of hearing aids may prevent complete hearing loss.

</doc>
<doc id="55974" url="https://en.wikipedia.org/wiki?curid=55974" title="Federal Communications Commission">
Federal Communications Commission

The Federal Communications Commission (FCC) is an independent agency of the United States government, created by Congressional statute (see and ) to regulate interstate communications by radio, television, wire, satellite, and cable in all 50 states, the District of Columbia and U.S. territories. The FCC works towards six goals in the areas of broadband, competition, the spectrum, the media, public safety and homeland security, and modernizing itself.
The FCC was formed by the Communications Act of 1934 to replace the radio regulation functions of the Federal Radio Commission. The FCC took over wire communication regulation from the Interstate Commerce Commission. The FCC's mandated jurisdiction covers the 50 states, the District of Columbia, and Political divisions of the United States. The FCC also provides varied degrees of cooperation, oversight, and leadership for similar communications bodies in other countries of North America. The FCC is funded entirely by regulatory fees. It has an estimated fiscal-2016 budget of US$388 million. It has 1,720 federal employees.
Mission and strategy.
The FCC's mission, specified in Section One of the Communications Act of 1934 and amended by the Telecommunications Act of 1996 (amendment to 47 U.S.C. §151) is to "make available so far as possible, to all the people of the United States, without discrimination on the basis of race, color, religion, national origin, or sex, rapid, efficient, Nationwide, and world-wide wire and radio communication services with adequate facilities at reasonable charges." The Act furthermore provides that the FCC was created "for the purpose of the national defense" and "for the purpose of promoting safety of life and property through the use of wire and radio communications."
Consistent with the objectives of the Act as well as the 1993 Government Performance and Results Act (GPRA), the FCC has identified six goals in its 2006–2011 Strategic Plan. These are:
Organization.
Commissioners.
The FCC is directed by five commissioners appointed by the President of the United States and confirmed by the United States Senate for five-year terms, except when filling an unexpired term. The U.S. President designates one of the commissioners to serve as chairman. Only three commissioners may be members of the same political party. None of them may have a financial interest in any FCC-related business.
Bureaus.
The FCC is organized into seven Bureaus, which process applications for licenses and other filings, analyze complaints, conduct investigations, develop and implement regulations, and participate in hearings.
Offices.
The FCC has eleven Staff Offices.
The FCC's Offices provide support services to the Bureaus. 
History.
Communications Act of 1934.
In 1934, Congress passed the Communications Act, which abolished the Federal Radio Commission and transferred jurisdiction over radio licensing to a new Federal Communications Commission, including in it also the telecommunications jurisdiction previously handled by the Interstate Commerce Commission. Title II of the Communications Act focused on telecommunications using many concepts borrowed from railroad legislation and Title III contained provisions very similar to the Radio Act of 1927.
Report on Chain Broadcasting.
In 1940, the Federal Communications Commission issued the "Report on Chain Broadcasting" which was led by new FCC Chairman James Lawrence Fly. The major point in the report was the breakup of NBC (National Broadcasting Company), which ultimately led to the creation of ABC (American Broadcasting Company), but there were two other important points. One was network option time, the culprit here being CBS. The report limited the amount of time during the day, and what times the networks may broadcast. Previously a network could demand any time it wanted from an affiliate. The second concerned artist bureaus. The networks served as both agents and employers of artists, which was a conflict of interest the report rectified.
Freeze of 1948.
In assigning television stations to various cities after World War II, the FCC found that it placed many stations too close to each other, resulting in interference. At the same time, it became clear that the designated VHF channels, 2 through 13, were inadequate for nationwide television service. As a result, the FCC stopped giving out construction permits for new licenses in October 1948. Most expected this "Freeze" to last six months, but as the allocation of channels to the emerging UHF technology and the eagerly awaited possibilities of color television were debated, the FCC's re-allocation map of stations did not come until April 1952, with July 1, 1952, as the official beginning of licensing new stations.
Other FCC actions hurt the fledgling DuMont and ABC networks. AT&T Corporation forced television coaxial cable users to rent additional radio long lines, discriminating against DuMont, which had no radio network operation. DuMont and ABC protested AT&T's television policies to the FCC, which regulated AT&T's long-line charges, but the commission took no action. The result was that financially marginal DuMont was spending as much in long-line charge as CBS or NBC while using only about 10 to 15 percent of the time and mileage of either larger network.
The FCC's "Sixth Report & Order" ended the Freeze. It would take five years for the U.S. to grow from 108 stations to more than 550. New stations came on line slowly, only five by the end of November 1952. The Sixth Report and Order required some existing TV stations to change channels, but only a few existing VHF stations were required to move to UHF, and a handful of VHF channels were deleted altogether in smaller media markets like Peoria, Fresno, Bakersfield and Fort Wayne, Indiana to create markets which were UHF "islands." The report also set aside a number of channels for the newly emerging field of educational television, which hindered struggling ABC and DuMont's quest for affiliates in the more desirable markets where VHF channels were reserved for non-commercial use.
The Sixth Report and Order also provided for the "intermixture" of VHF and UHF channels in most markets; UHF transmitters in the 1950s were not yet powerful enough, nor receivers sensitive enough (if they included UHF tuners at all - they were not formally required until the 1960s All-Channel Receiver Act), to make UHF viable against entrenched VHF stations. In markets where there were no VHF stations and UHF was the only TV service available, UHF survived. In other markets, which were too small to financially support a television station, too close to VHF outlets in nearby cities, or where UHF was forced to compete with more than one well-established VHF station, UHF had little chance for success.
Denver had been the largest U.S. city without a TV station by 1952. Senator Edwin Johnson (D-Colorado), chair of the Senate's Interstate and Foreign Commerce Committee, had made getting Denver the first post-Freeze station his personal mission. He had pressured the FCC, and proved ultimately successful as the first new station (a VHF station) came on-line a remarkable ten days after the Commission formally announced the first post-Freeze construction permits. KFEL (now KWGN-TV)'s first regular telecast was on July 21, 1952.
Telephone monopoly to competition.
The important relationship of the FCC and the American Telephone and Telegraph (AT&T) Company has evolved over several years. For many years, the FCC and state officials agreed to regulate the telephone systems as a natural monopoly. The FCC controlled telephone rates to limit the profits of AT&T and ensure nondiscriminatory pricing. In the 1960s, the FCC began allowing other long-distance companies, namely MCI, to offer specialized services. In the 1970s, the FCC allowed other companies to expand offerings to the public. A lawsuit in 1982 led by the Justice Department after AT&T underpriced other companies, resulted in the split of the Bells from AT&T. Beginning in 1984, the FCC implemented a new goal that all long-distance companies had equal access to the local phone companies' customers.
Telecommunications Act of 1996.
In 1996, Congress enacted the Telecommunications Act of 1996, in the wake of the break-up of AT&T resulting from the U.S. Justice Department's antitrust suit against AT&T. The legislation attempted to create more competition in local telephone service by requiring Incumbent Local Exchange Carriers to provide access to their facilities for Competitive Local Exchange Carriers. This policy has thus far had limited success and much criticism.
The development of the Internet, cable services and wireless services has raised questions whether new legislative initiates are needed as to competition in what has come to be called 'broadband' services. Congress has monitored developments but as of 2009 has not undertaken a major revision of applicable regulation. The Local Community Radio Act in the 111th Congress has gotten out of committee and will go before the house floor with bi-partisan support, and unanimous support of the FCC.
By passing the Telecommunications Act of 1996, Congress also eliminated the cap on the number of radio stations one entity could own nationwide and substantially loosened local radio station ownership restrictions. Substantial radio consolidation followed. Restrictions on ownership of television stations were also loosened. Public comments to the FCC indicated that the public largely believed that the severe consolidation of media ownership had resulted in harm to diversity, localism, and competition in media, and was harmful to the public interest.
Connection permissivity, indecency crackdowns.
The inauguration of Ronald Reagan as President of the United States in 1981 accelerated an already ongoing shift in the FCC towards a decidedly more market-oriented stance. A number of regulations felt to be outdated were removed, most controversially the Fairness Doctrine in 1987. The FCC also took steps to increase competition to broadcasters, fostering broadcast alternatives such as cable television. In terms of indecency fines, there was no action taken by the FCC from FCC v. Pacifica until 1987, about ten years later.
In the early 2000s, the FCC began stepping up censorship and enforcement of indecency regulations again, most notably following the Janet Jackson "wardrobe malfunction" that occurred during the halftime show of Super Bowl XXXVIII. However, the FCC's regulatory domain with respect to indecency remains restricted to the public airwaves, notably VHF and UHF television and AM/FM radio.
On June 15, 2006, President George W. Bush signed into law the Broadcast Decency Enforcement Act of 2005 sponsored by then-Senator Sam Brownback (now Governor of Kansas), a former broadcaster himself, and endorsed by Congressman Fred Upton of Michigan who authored a similar bill in the United States House of Representatives. The new law stiffens the penalties for each violation of the Act. The Federal Communications Commission will be able to impose fines in the amount of $325,000 for each violation by each station that violates decency standards. The legislation raised the fine ten times over the previous maximum of $32,500 per violation.
Modernization of the FCC's information technology systems.
David A. Bray joined the Commission in 2013 as Chief Information Officer and quickly announced goals of modernizing the FCC's legacy information technology (IT) systems, citing 200 different systems for only 1750 people a situation he found "perplexing". He was named as one of the top 70 Most Social U.S. federal technology professionals in 2014, openly discussing new opportunities with the public as @fcc_cio on Twitter and actively blogging about FCC efforts.
Bray has openly encouraged opportunities to broaden the definition of public service to include citizen-led initiatives and public-private partnerships. He also has emphasized: "As IT accelerates its global distribution and ubiquitous availability, the importance of assuring the integrity of digital communications become paramount" and that communications support "national growth, prosperity, security, safety, and freedom."
Past chairs and notable commissioners.
A complete list of commissioners is available on the FCC website. In addition to chairmen, Frieda B. Hennock (D-NY) was the first female commissioner of the FCC.
Broadcast licensing.
Regulatory powers and enforcement.
The FCC regulates broadcast stations, amateur radio operators, and repeater stations as well as commercial broadcasting operators who operate and repair certain radiotelephone, television, radar, and Morse code radio stations. In recent years it has also licensed people who maintain or operate Global Maritime Distress and Safety System stations. Broadcast licenses are to be renewed if the station meets the "public interest, convenience, or necessity".
The FCC's enforcement powers include fines and broadcast license revocation (see FCC MB Docket 04-232). Burden of proof would be on the complainant in a petition to deny. Fewer than 1% of station renewals are not immediately granted, and only a small fraction of those are ultimately denied.
While the FCC maintains control of the written and Morse testing standards, it no longer administers the exams, having delegated that function to private organizations.
Broadcasting tower database.
An FCC database provides information about the height and year built of broadcasting towers in the US. It does not contain information about the structural types of towers or about the height of towers used by Federal agencies, such as most NDBs, LORAN-C transmission towers or VLF transmission facilities of the US Navy, or about most towers not used for transmission like the BREN Tower. These are instead tracked by the Federal Aviation Administration as obstructions to air navigation.
Internet.
In North America, the FCC made its original Internet policy statement containing four principles "subject to reasonable network management" in 2005. The Commission established the following principles: To encourage broadband deployment and preserve and promote the open and interconnected nature of the public Internet, Consumers are entitled to access the lawful Internet content of their choice; Consumers are entitled to run applications and use services of their choice, subject to the needs of law enforcement; Consumers are entitled to connect their choice of legal devices that do not harm the network; Consumers are entitled to competition among network providers, application and service providers, and content providers.
In December 2010, the FCC revised the principles from the original Internet policy statement and created The Open Internet Order consisting of three rules regarding the Internet: Transparency. Fixed and mobile broadband providers must disclose the network management practices, performance characteristics, and terms and conditions of their broadband services; No blocking. Fixed broadband providers may not block lawful content, applications, services, or non-harmful devices; mobile broadband providers may not block lawful websites, or block applications that compete with their voice or video telephony services; and No unreasonable discrimination.
In April 2014, the FCC created a Notice of Proposed Rulemaking that called for reform regarding The Open Internet Order. In February 2015, the FCC passed new Open Internet Rules through Congress into law.
In August 2015, the FCC said that nearly 55 million Americans did not have access to broadband capable of 
delivering high-quality voice, data, graphics and video offerings.
Controversies.
Unreleased reports.
2003 study of commercial radio concentration.
In 2003, the FCC Media Bureau produced a draft report analyzing the impact of deregulation in the radio industry. The report stated that from March 1996 through March 2003, the number of commercial radio stations on the air rose 5.9 percent while the number of station owners fell 35 percent. The concentration of ownership followed a 1996 rewrite of telecommunications law that eliminated a 40-station national ownership cap.
The report was never made public, nor have any similar analyses followed, despite the fact that radio industry reports were released in 1998, 2001 and 2002. In September 2006, Senator Barbara Boxer, who had received a copy of the report, released it.
2004 study of television media concentration.
In 2004, the FCC ordered its staff to destroy all copies of a draft study by Keith Brown and Peter Alexander, two economists in the FCC's Media Bureau. The two had analyzed a database of 4,078 individual news stories broadcast in 1998, and showed local ownership of television stations adds almost five and one-half minutes of total news to broadcasts and more than three minutes of "on-location" news.
The conclusion of the study was at odds with FCC arguments made when it voted in 2003 to increase the number of television stations a company could own in a single market. (In June 2004, a federal appeals court rejected the agency's reasoning on most of the rules and ordered it to try again.)
In September 2006, Senator Barbara Boxer, who had received a copy of the report "indirectly from someone within the FCC who believed the information should be made public," wrote a letter to FCC Chairman Kevin Martin, asked whether any other commissioners "past or present" knew of the report's existence and why it was never made public. She also asked whether it was "shelved because the outcome was not to the liking of some of the commissioners and/or any outside powerful interests?" Boxer's office said if she does not receive adequate answers to her questions, she will push for an investigation by the FCC inspector general.
Action by FCC Chairman.
In a letter in response to Senator Boxer, FCC Chairman Martin said "I want to assure you that I too am concerned about what happened to these two draft reports." The letter also said "I have asked the inspector general of the FCC to conduct an investigation into what happened to these draft documents and will cooperate fully with him." Martin added that he was not chairman at the time the reports were drafted, and that neither he nor his staff had seen them.
NSA wiretapping.
When it emerged in 2006 that AT&T, BellSouth and Verizon may have broken U.S. laws by aiding the National Security Agency in possible illegal wiretapping of its customers, Congressional representatives called for an FCC investigation into whether or not those companies broke the law. The FCC declined to investigate, however, claiming that it could not investigate due to the classified nature of the program– a move that provoked the criticism of members of Congress.
"Today the watchdog agency that oversees the country's telecommunications industry refused to investigate the nation's largest phone companies' reported disclosure of phone records to the NSA," said Rep. Edward Markey (D-Mass.) in response to the decision. "The FCC, which oversees the protection of consumer privacy under the Communications Act of 1934, has taken a pass at investigating what is estimated to be the nation's largest violation of consumer privacy ever to occur. If the oversight body that monitors our nation's communications is stepping aside then Congress must step in."
Diversity.
With the major demographic shifts occurring in the country in terms of the racial-ethnic composition of the population, the FCC has also been criticized for ignoring the issue of decreasing racial-ethnic diversity of the media. This includes charges that the FCC has been watering down the limited affirmative action regulations it had on the books, including no longer requiring stations to make public their data on their minority staffing and hiring. In the second half of 2006, groups such as the National Hispanic Media Coalition, the National Latino Media Council, the National Association of Hispanic Journalists, the National Institute for Latino Policy, the League of United Latin American Citizens (LULAC) and others held town hall meetings in California, New York and Texas on media diversity as its effects Latinos and minority communities. They documented widespread and deeply felt community concerns about the negative effects of media concentration and consolidation on racial-ethnic diversity in staffing and programming. At these Latino town hall meetings, the issue of the FCC's lax monitoring of obscene and pornographic material in Spanish-language radio and the lack of racial and national-origin diversity among Latino staff in Spanish-language television were other major themes.
President Barack Obama appointed Mark Lloyd to the FCC in the newly created post of Associate General Counsel/Chief Diversity Officer.
Use of white space.
"White spaces" are radio frequencies that went unused after the federally mandated transformation of analog TV signal to digital. On October 15, 2008, FCC Chairman Kevin Martin announced his support for the unlicensed use of white spaces. Martin said he was "hoping to take advantage of utilizing these airwaves for broadband services to allow for unlicensed technologies and new innovations in that space."
Google, Microsoft and other companies are vying for the use of this white-space to support innovation in Wi-Fi technology. Broadcasters and wireless microphone manufacturers fear that the use of white space would "disrupt their broadcasts and the signals used in sports events and concerts." Cell phone providers such as T-Mobile USA have mounted pressure on the FCC to instead offer up the white space for sale to boost competition and market leverage.
On November 4, 2008, the FCC commissioners unanimously agreed to open up unused broadcast TV spectrum for unlicensed use.
Net neutrality.
The FCC has claimed some jurisdiction over the issue of net neutrality and has laid down guideline rules that it expects the telecommunications industry to follow. On February 11, 2008 Rep. Ed Markey and Rep. Chip Pickering introduced HR5353 "To establish broadband policy and direct the Federal Communications Commission to conduct a proceeding and public broadband summit to assess competition, consumer protection, and consumer choice issues relating to broadband Internet access services, and for other purposes." On 1 August 2008 the FCC formally voted 3-to-2 to upholding a complaint against Comcast, the largest cable company in the US, ruling that it had illegally inhibited users of its high-speed Internet service from using file-sharing software. The FCC imposed no fine, but required Comcast to end such blocking in 2008. FCC chairman Kevin J. Martin said the order was meant to set a precedent that Internet providers, and indeed all communications companies, could not prevent customers from using their networks the way they see fit unless there is a good reason. In an interview Martin stated that "We are preserving the open character of the Internet" and "We are saying that network operators can't block people from getting access to any content and any applications." Martin's successor, Julius Genachowski has maintained that the FCC has no plans to regulate the internet, saying: "I've been clear repeatedly that we're not going to regulate the Internet." The Comcast case highlighted broader issues of whether new legislation is needed to force Internet providers to maintain net neutrality, i.e. treat all uses of their networks equally. The legal complaint against Comcast related to BitTorrent, software that is commonly used for downloading larger files. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U. S. Congress H. R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications 
On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept." Net neutrality is supported by 81% of Americans, according to a Washington Post poll. According to the poll, 81% of Democrats and 85% of Republicans said they opposed fast lanes.
On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new "Net Neutrality" regulations.
Proprietary standards.
The FCC has also been criticized for ignoring international open standards, and instead choosing proprietary closed standards, or allowing communications companies to do so and implement the anticompetitive practice of vendor lock-in, thereby preventing a free market.
In the case of digital TV, it chose the ATSC standard, even though DVB was already in use around the world, including DVB-S satellite TV in the U.S. Unlike competing standards, the ATSC system is encumbered by numerous patents, and therefore royalties that make TV sets and DTV converters much more expensive than in the rest of the world. Additionally, the claimed benefit of better reception in rural areas is more than negated in urban areas by multipath interference, which other systems are nearly immune to. It also cannot be received while in motion for this reason, while all other systems can, even without dedicated mobile TV signals or receivers.
For digital radio, the FCC chose proprietary HD Radio, which crowds the existing FM broadcast band and even AM broadcast band with in-band adjacent-channel sidebands, which create noise in other stations. This is in contrast to worldwide DAB, which uses unused TV channels in the VHF band III range. This too has patent fees, while DAB does not. While there has been some effort by iBiquity to lower them, the fees for HD Radio are still an enormous expense when converting each station, and this fee structure presents a potentially high cost barrier to entry for community radio and other non-commercial educational stations when entering the HD Radio market.
Satellite radio (also called SDARS by the FCC) uses two proprietary standards instead of DAB-S, which requires users to change equipment when switching from one provider to the other, and prevents other competitors from offering new choices as stations can do on terrestrial radio. Had the FCC picked DAB-T for terrestrial radio, no separate satellite receiver would have been needed at all, and the only difference from DAB receivers in the rest of the world would be in software, where it would need to tune S band instead of L band.
In mobile telephony, the FCC abandoned the "any lawful device" principle decided against AT&T landlines, and has instead allowed each mobile phone company to dictate what its customers can use.
DTV controversy.
The FCC has been criticized for awarding a digital TV (DTV) channel to each holder of an analog TV station license without an auction, as well as trading auctionable spectrum to Nextel to resolve public safety RF interference problems. Conversely, it has also been criticized for forcing stations to buy and install all new equipment (transmitters, TV antennas, and even entirely new broadcast towers), and operate for years on both channels at once.
After delaying the original deadlines of 2006, 2008, and eventually February 17, 2009, on concerns about elderly and rural folk, on June 12 all full-power analog terrestrial TV licenses in the U.S. were terminated as part of the DTV transition, leaving terrestrial television available only from digital channels and a few low-power LPTV stations. To help U.S. consumers through the conversion, Congress established a federally sponsored DTV Converter Box Coupon Program for two free converters per household.
Local broadcasting.
After being successful in opening the FM band as a superior alternative to the AM band by allowing colleges and other schools to start ten-watt LPFM stations, the FCC banned new ones around 1980.
Numerous controversies have surrounded the city of license concept as the internet has made it possible to broadcast a single signal to every owned station in the nation at once, particularly when Clear Channel, now IHeartMedia, became the largest FM broadcasting corporation in the US after the Telecommunications Act of 1996 became law - owning over 1200 stations at its peak. As part of its license to buy more radio stations, Clear Channel was forced to divest all TV stations.
Public consultation.
As the public interest standard has always been important to the FCC when determining and shaping policy, so too has the relevance of public involvement in U.S. communication policy making. The "FCC Record" is the comprehensive compilation of decisions, reports, public notices, and other documents of the FCC, published since 1986.
History of the issue.
1927 Radio Act.
In the 1927 Radio Act, which was formulated by the predecessor of the FCC (the Federal Radio Commission), section 4(k) stipulated that the commission was authorized to hold hearings for the purpose of developing a greater understanding of the issues for which rules were being crafted. Section 4(k) stated that:
Thus, it is clear that public consultation, or at least consultation with outside bodies was regarded as central to the Commission's job from early on. Though it should not be surprising, the Act also stipulated that the Commission should verbally communicate with those being assigned licenses. Section 11 of the Act noted:
Public hearings.
As early as 1927, there is evidence that public hearings were indeed held; among them, hearings to assess the expansion of the radio broadcast band. At these early hearings, the goal of having a broad range of viewpoints presented was evident, as not only broadcasters, but also radio engineers and manufacturers were in attendance. Numerous groups representing the general public appeared at the hearings as well, including amateur radio operators and inventors as well as representatives of radio listeners' organizations. Interestingly,
Including members of the general public in the discussion was regarded (or at least articulated) as very important to the Commission's deliberations. In fact, FCC Commissioner Bellows noted at the time that "it is the radio listener we must consider above everyone else." Though there were numerous representatives of the general public at the hearing, some expressing their opinions to the commission verbally, overall there was not a great turnout of everyday listeners at the hearings.
Though not a constant fixture of the communications policy-making process, public hearings were occasionally organized as a part of various deliberation processes as the years progressed. For example, seven years after the enactment of the Radio Act, the Communications Act of 1934 was passed, creating the FCC. That year the Federal Government's National Recovery Agency (associated with the New Deal period) held public hearings as a part of its deliberations over the creation of new broadcasting codes.
A few years later , the FCC held hearings to address early cross-ownership issues; specifically, whether newspaper companies owning radio stations was in the public interest. These "newspaper divorcement hearings" were held between 1941 and 1944, though it appears that these hearings were geared mostly towards discussion by industry stakeholders. Around the same time, the Commission held hearings as a part of its evaluation of the national television standard, and in 1958 held additional hearings on the television network broadcasting rules. Though public hearings were organized somewhat infrequently, there was an obvious public appeal. In his now famous "vast wasteland" speech in 1961, FCC Chairman Newton Minow noted that the commission would hold a "well advertised public hearing" in each community to assure broadcasters were serving the public interest, clearly a move to reconnect the Commission with the public interest (at least rhetorically).
Media ownership review 2003.
In September 2002, the FCC issued a Notice of Proposed Rulemaking stating that the Commission would re-evaluate its media ownership rules pursuant to the obligation specified in the Telecommunications Act of 1996. As 2003 was approaching, a battle of words (and perhaps actions) developed between Chairman Powell and Democratic Commissioner Michael Copps. Commissioner Copps felt that the Republican FCC was too focused on the neo-liberal agenda, and not focused enough on hearing the public's voice regarding the issues at hand, noting, "We need a much wider participation … this is not an inside-the-Beltway issue." Copps repeatedly called for the FCC to hold public hearings with time devoted to public input. Powell responded by noting that the public had already taken advantage of the online comment submission process and that no public hearings would be necessary. A spokesman for Powell noted, "if Commissioner Copps thinks something more can be gained from having hearings, he should feel free to do so." In the end, Commissioner Copps and Commissioner Jonathan Adelstein organized a number of "unofficial" FCC hearings.
On January 16, 2003, the FCC held an "unofficial" public hearing on media ownership at Columbia University; Chairman Michael Powell was in attendance. His opening remarks, however, certainly reflected the lack of interest the Commission had displayed towards public hearings in recent years:
The Chief of the Media Bureau and some other associates would be there all day to hear a full report on the event.
Copps remained adamant that all Commissioners should attend an official FCC hearing before any decisions were made. An editorial in Broadcasting and Cable articulated the heated nature of the eventual decision regarding an official hearing (at least from the Republican standpoint). The article is quoted at length as it includes a variety of points that are relevant:
Headquarters.
The FCC leases space in the Portals building in southwest Washington, D.C. Construction of the Portals building was scheduled to begin on March 1, 1996. In January 1996 the General Services Administration signed a lease with the building's owners, agreeing to let the FCC lease of space in Portals for 20 years, at a cost of $17.3 million per year in 1996 dollars. Prior to its current arrangement, the FCC had space in six buildings by 19th Street NW and M Street NW. The FCC first solicited bids for a new headquarters complex in 1989. In 1991 the GSA selected the Portals site. The FCC had wanted to move into a more expensive area along Pennsylvania Avenue.

</doc>
<doc id="55976" url="https://en.wikipedia.org/wiki?curid=55976" title="Makefile">
Makefile

A makefile is a file containing a set of directives used with the "make" build automation tool.
Overview.
Most often, the "makefile" directs "make" on how to compile and link a program. Using C/C++ as an example, when a C/C++ source file is changed, it must be recompiled. If a header file has changed, each C/C++ source file that includes the header file must be recompiled to be safe. Each compilation produces an object file corresponding to the source file. Finally, if any source file has been recompiled, all the object files, whether newly made or saved from previous compilations, must be linked together to produce the new executable program. These instructions with their dependencies are specified in a "makefile". If none of the files that are prerequisites have been changed since the last time the program was compiled, no actions take place. For large software projects, using Makefiles can substantially reduce build times if only a few source files have changed.
Operating system.
Unix-like.
Makefiles originated in Unix like systems and is still the primary software build mechanism.
Microsoft Windows.
Windows supports a variation of makefiles with its "nmake" utility. Standard Unix like makefiles can be executed in Windows in a Cygwin environment.
Contents.
Makefiles contain five kinds of things: "explicit rules", "implicit rules", "variable definitions", "directives", and "comments".
Rules.
A makefile consists of “rules” in the following form:
<syntaxhighlight lang="make">
target: dependencies
</syntaxhighlight>
A target is usually the name of a file that is generated by a program; examples of targets are executable or object files. A target can also be the name of an action to carry out, such as "clean".
A dependency (also called prerequisite) is a file that is used as input to create the target. A target often depends on several files. However, the rule that specifies a recipe for the target need not have any prerequisites. For example, the rule containing the delete command associated with the target "clean" does not have prerequisites.
The system command(s) (also called recipe) is an action that make carries out. A recipe may have more than one command, either on the same line or each on its own line. Note the use of meaningful indentation in specifying commands.
Execution.
A "makefile" is executed with the <syntaxhighlight lang="text" inline>make</syntaxhighlight> command, eg. <syntaxhighlight lang="text" inline>make [target1 target2 ...</syntaxhighlight>.
By default, when make looks for the makefile, if a makefile name was not included as a parameter, it tries the following names, in order: "makefile" and "Makefile".
Example.
Here is a simple makefile that describes the way an executable file called "edit" depends on four object files which, in turn, depend on four C source and two header files.
<syntaxhighlight lang="make">
edit : main.o kbd.o command.o display.o 
main.o : main.c defs.h
kbd.o : kbd.c defs.h command.h
command.o : command.c defs.h command.h
display.o : display.c defs.h
clean :
</syntaxhighlight>
To use this makefile to create the executable file called "edit", type <syntaxhighlight lang="text" inline>make</syntaxhighlight>. To use this makefile to delete the executable file and all the object files from the directory, type <syntaxhighlight lang="text" inline>make clean</syntaxhighlight>.

</doc>
<doc id="55982" url="https://en.wikipedia.org/wiki?curid=55982" title="Dirty bomb">
Dirty bomb

A dirty bomb or radiological dispersal device (RDD) is a speculative radiological weapon that combines radioactive material with conventional explosives. The purpose of the weapon is to contaminate the area around the dispersal agent/conventional explosion with radioactive material, serving primarily as an area denial device against civilians. It is however not to be confused with a nuclear explosion, such as a fission bomb, which by releasing nuclear energy produces blast effects far in excess of what is achievable by the use of conventional explosives.
Though a RDD would be designed to disperse radioactive material over a large area, a bomb that uses conventional explosives and produces a blast wave would be far more lethal to people than the hazard posed by radioactive material that may be mixed with the explosive. At levels created from probable sources, not enough radiation would be present to cause severe illness or death. A test explosion and subsequent calculations done by the United States Department of Energy found that assuming nothing is done to clean up the affected area and everyone stays in the affected area for one year, the radiation exposure would be "fairly high", but not fatal. Recent analysis of the nuclear fallout from the Chernobyl disaster confirms this, showing that the effect on many people in the surrounding area, although not those in close proximity, was almost negligible.
Since a dirty bomb is unlikely to cause many deaths by radiation exposure, many do not consider this to be a weapon of mass destruction. Its purpose would presumably be to create psychological, not physical, harm through ignorance, mass panic, and terror. For this reason dirty bombs are sometimes called "weapons of mass disruption". Additionally, containment and decontamination of thousands of victims, as well as decontamination of the affected area might require considerable time and expense, rendering areas partly unusable and causing economic damage.
Dirty bombs and terrorism.
Since the 9/11 attacks the fear of terrorist groups using dirty bombs has increased significantly, which has been frequently reported in the media. The meaning of terrorism used here, is described by the U.S. Department of Defense's definition, which is "the calculated use of unlawful violence or threat of unlawful violence to inculcate fear; intended to coerce or to intimidate governments or societies in the pursuit of goals that are generally political, religious, or ideological objectives".
There have only ever been two cases of caesium-containing bombs, and neither was detonated. Both involved Chechnya. The first attempt of radiological terror was carried out in November 1995 by a group of Chechen separatists, who buried a caesium-137 source wrapped in explosives at the Izmaylovsky Park in Moscow. A Chechen rebel leader alerted the media, the bomb was never activated, and the incident amounted to a mere publicity stunt.
In December 1998, a second attempt was announced by the Chechen Security Service, who discovered a container filled with radioactive materials attached to an explosive mine. The bomb was hidden near a railway line in the suburban area Argun, ten miles east of the Chechen capital of Grozny. The same Chechen separatist group was suspected to be involved.
Despite the increased fear of a dirty bombing attack, it is hard to assess whether the actual risk of such an event has increased significantly. The following discussions on implications, effects and probability of an attack, as well as indications of terror groups planning such, are based mainly on statistics, qualified guessing and a few comparable scenarios.
Effect of a dirty bomb explosion.
When dealing with the implications of a dirty bomb attack, there are two main areas to be addressed: (i) the civilian impact, not only dealing with immediate casualties and long term health issues, but also the psychological effect and then (ii) the economic impact. With no prior event of a dirty bomb detonation, it is considered difficult to predict the impact. Several analyses have predicted that RDDs will neither sicken nor kill many people.
Accidents with radioactives.
The effects of uncontrolled radioactive contamination have been reported several times.
One example is the radiological accident occurring in Goiânia, Brazil, between September 1987 and March 1988: Two metal scavengers broke into an abandoned radiotherapy clinic and removed a teletherapy source capsule containing powdered caesium-137 with an activity of 50 TBq. They brought it back to the home of one of the men to take it apart and sell as scrap metal. Later that day both men were showing acute signs of radiation illness with vomiting and one of the men had a swollen hand and diarrhea. A few days later one of the men punctured the 1 mm thick window of the capsule, allowing the caesium chloride powder to leak out and when realizing the powder glowed blue in the dark, brought it back home to his family and friends to show it off. After 2 weeks of spread by contact contamination causing an increasing number of adverse health effects, the correct diagnosis of acute radiation sickness was made at a hospital and proper precautions could be put into procedure. By this time 249 people were contaminated, 151 exhibited both external and internal contamination of which 20 people were seriously ill and 5 people died.
The Goiânia incident to some extent predicts the contamination pattern if it is not immediately realized that the explosion spread radioactive material, but also how fatal even very small amounts of ingested radioactive powder can be. This raises worries of terrorists using powdered alpha emitting material, that if ingested can pose a serious health risk, as in the case of deceased former K.G.B. spy Alexander Litvinenko, who either ate, drank or inhaled polonium-210. "Smoky bombs" based on alpha emitters might easily be just as dangerous as beta or gamma emitting dirty bombs.
Public perception of risks.
For the majority involved in an RDD incident, the radiation health risks (i.e. increased probability of developing cancer later in life due to radiation exposure) are comparatively small, comparable to the health risk from smoking five packages of cigarettes on a daily basis. The fear of radiation is not always logical. Although the exposure might be minimal, many people find radiation exposure especially frightening because it is something they cannot see or feel, and it therefore becomes an unknown source of danger. Dealing with public fear may prove the greatest challenge in case of an RDD event. Policy, science and media may inform the public about the real danger and thus reduce the possible psychological and economic effects.
Statements from the U.S. government after 9/11 may have contributed unnecessarily to the public fear of a dirty bomb. When United States Attorney General John Ashcroft on June 10, 2002, announced the arrest of José Padilla, allegedly plotting to detonate such a weapon, he said:
This public fear of radiation also plays a big role in why the costs of an RDD impact on a major metropolitan area (such as lower Manhattan) might be equal to or even larger than that of the 9/11 attacks. Assuming the radiation levels are not too high and the area does not need to be abandoned such as the town of Pripyat near the Chernobyl reactor, an expensive and time consuming cleanup procedure will begin. This will mainly consist of tearing down highly contaminated buildings, digging up contaminated soil and quickly applying sticky substances to remaining surfaces so that radioactive particles adhere before radioactivity penetrates the building materials. These procedures are the current state of the art for radioactive contamination cleanup, but some experts say that a complete cleanup of external surfaces in an urban area to current decontamination limits may not be technically feasible. Loss of working hours will be vast during cleanup, but even after the radiation levels reduce to an acceptable level, there might be residual public fear of the site including possible unwillingness to conduct business as usual in the area. Tourist traffic is likely never to resume.
There is also a psychological warfare element to radioactive substances. Visceral fear is not widely aroused by the daily emissions from coal burning, for example, even though a National Academy of Sciences study found this causes 10,000 premature deaths a year in the US population of 317,413,000. Medical errors leading to death in U.S. hospitals are estimated to be between 44,000 and 98,000. It is "only nuclear radiation that bears a huge psychological burden — for it carries a unique historical legacy".
Constructing and obtaining material for a dirty bomb.
In order for a terrorist organization to construct and detonate a dirty bomb, it must acquire radioactive material by stealing it or buying it through legal or illegal channels. Possible RDD material could come from the millions of radioactive sources used worldwide in the industry, for medical purposes and in academic applications mainly for research. Of these sources, only nine reactor produced isotopes stand out as being suitable for radiological terror: americium-241, californium-252, caesium-137, cobalt-60, iridium-192, plutonium-238, polonium-210, radium-226 and strontium-90, and even from these it is possible that radium-226 and polonium-210 do not pose a significant threat. Of these sources the U.S. Nuclear Regulatory Commission has estimated that within the U.S., approximately one source is lost, abandoned or stolen every day of the year. Within the European Union the annual estimate is 70. There exist thousands of such "orphan" sources scattered throughout the world, but of those reported lost, no more than an estimated 20 percent can be classified as a potential high security concern if used in a RDD. Especially Russia is believed to house thousands of orphan sources, which were lost following the collapse of the Soviet Union. A large but unknown number of these sources probably belong to the high security risk category. Noteworthy are the beta emitting strontium-90 sources used as radioisotope thermoelectric generators for beacons in lighthouses in remote areas of Russia. In December 2001, three Georgian woodcutters stumbled over such a power generator and dragged it back to their camp site to use it as a heat source. Within hours they suffered from acute radiation sickness and sought hospital treatment. The International Atomic Energy Agency (IAEA) later stated that it contained approximately of strontium, equivalent to the amount of radioactivity released immediately after the Chernobyl accident (though the total radioactivity release from Chernobyl was 2500 times greater at around ).
Although a terrorist organization might obtain radioactive material through the "black market", and there has been a steady increase in illicit trafficking of radioactive sources from 1996 to 2004, these recorded trafficking incidents mainly refer to rediscovered orphan sources without any sign of criminal activity, and it has been argued that there is no conclusive evidence for such a market. In addition to the hurdles of obtaining usable radioactive material, there are several conflicting requirements regarding the properties of the material the terrorists need to take into consideration: First, the source should be "sufficiently" radioactive to create direct radiological damage at the explosion or at least to perform societal damage or disruption. Second, the source should be transportable with enough shielding to protect the carrier, but not so much that it will be too heavy to maneuver. Third, the source should be sufficiently dispersible to effectively contaminate the area around the explosion.
An example of a worst-case scenario is a terror organization possessing a source of very highly radioactive material, e.g. a strontium-90 thermal generator, with the ability to create an incident comparable to the Chernobyl accident. Although the detonation of a dirty bomb using such a source might seem terrifying, it would be hard to assemble the bomb and transport it without severe radiation damage and possible death of the perpetrators involved. Shielding the source effectively would make it almost impossible to transport and a lot less effective if detonated.
Due to the three constraints of making a dirty bomb, RDDs might still be defined as "high-tech" weapons and this is probably why they have not been used up to now.
Possibility of terrorist groups using dirty bombs.
The present assessment of the possibility of terrorists using a dirty bomb is based on cases involving Al-Qaeda. This is because the attempts by this group to acquire a dirty bomb are the most well-described in the literature, in part due to the attention this group received for their involvement in the 9/11 attacks.
On 8 May 2002, José Padilla (a.k.a. Abdulla al-Muhajir) was arrested on suspicion that he was an Al-Qaeda terrorist planning to detonate a dirty bomb in the U.S. This suspicion was raised by information obtained from an arrested top Al-Qaeda official in U.S. custody, Abu Zubaydah, who under interrogation revealed that the organization was close to constructing a dirty bomb. Although Padilla had not obtained radioactive material or explosives at the time of arrest, law enforcement authorities uncovered evidence that he was on reconnaissance for usable radioactive material and possible locations for detonation. It has been doubted whether José Padilla was preparing such an attack, and it has been claimed that the arrest was highly politically motivated, given the pre-9/11 security lapses by the CIA and FBI.
Later, these charges against José Padilla were dropped. Although there was no hard evidence for Al-Qaeda possessing a dirty bomb, there is a broad agreement that Al-Qaeda poses a potential dirty bomb attack threat because they need to overcome the alleged image that the U.S. and its allies are winning the war against terror. A further concern is the argument, that "if suicide bombers are prepared to die flying airplanes into building, it is also conceivable that they are prepared to forfeit their lives building dirty bombs". If this would be the case, both the cost and complexity of any protective systems needed to allow the perpetrator to survive long enough to both build the bomb and carry out the attack, would be significantly reduced.
Several other captives were alleged to have played a role in this plot.
Guantanamo captive Binyam Mohammed has alleged he was subjected to extraordinary rendition, and that his confession of a role in the plot was coerced through torture.
He sought access through the American and United Kingdom legal systems to provide evidence he was tortured.
Guantanamo military commission prosecutors continue to maintain the plot was real, and charged Binyam for his alleged role in 2008. However they dropped this charge in October 2008, but maintain they could prove the charge and were only dropping the charge to expedite proceedings.
US District Court Judge Emmet G. Sullivan insisted that the administration still had to hand over the evidence that justified the dirty bomb charge, and admonished United States Department of Justice lawyers that dropping the charge "raises serious questions in this court's mind about whether those allegations were ever true."
In 2006, Dhiren Barot from North London pleaded guilty of conspiring to murder innocent people within the United Kingdom and United States using a radioactive dirty bomb. He planned to target underground car parks within the UK and buildings in the U.S. such as the International Monetary Fund, World Bank buildings in Washington D.C., the New York Stock Exchange, Citigroup buildings and the Prudential Financial buildings in Newark, New Jersey. He also faces 12 other charges including, conspiracy to commit public nuisance, seven charges of making a record of information for terrorist purposes and four charges of possessing a record of information for terrorist purposes. Experts say if the plot to use the dirty bomb was carried out "it would have been unlikely to cause deaths, but was designed to affect about 500 people."
In January 2009, a leaked FBI report described the results of a search of the Maine home of James G. Cummings, a white supremacist who had been shot and killed by his wife. Investigators found four one-gallon containers of 35 percent hydrogen peroxide, uranium, thorium, lithium metal, aluminum powder, beryllium, boron, black iron oxide and magnesium as well as literature on how to build dirty bombs and information about cesium-137, strontium-90 and cobalt-60, radioactive materials. Officials confirmed the veracity of the report but stated that the public was never at risk.
In April 2009, the Security Service of Ukraine announced the arrest of a legislator and two businessmen from the Ternopil Oblast. Seized in the undercover sting operation was 3.7 kilograms of what was claimed by the suspects during the sale as plutonium-239, used mostly in nuclear reactors and nuclear weapons, but was determined by experts to be probably americium, a "widely used" radioactive material which is commonly used in amounts of less than 1 milligram in smoke detectors, but can also be used in a dirty bomb. The suspects reportedly wanted US$ 10 million for the material, which the Security Service determined was produced in Russia during the era of the Soviet Union and smuggled into Ukraine through a neighboring country.
In July 2014, ISIS militants seized 88 pounds of uranium compounds from Mosul University. The material was unenriched and so could not be used to build a conventional fission bomb, but a dirty bomb is a theoretical possibility. However, uranium's relatively low radioactivity makes it a poor candidate for use in a dirty bomb.
Little is known about civil preparedness to respond to a dirty bomb attack. The Boston Marathon appeared to many to be a situation with high potential for use of a dirty bomb as a terrorist weapon. However, the bombing attack that occurred on April 15, 2013 did not involve use of dirty bombs. Any radiological testing or inspections that may have occurred following the attack were either conducted sub rosa or not at all. Also, there was no official dirty bomb "all clear" issued by the Obama administration. Massachusetts General Hospital had, apparently under their own disaster plan, issued instructions to their emergency room to be prepared for incoming radiation poisoning cases.
Dirty bomb tests.
Israel carried out a four-year series of tests on nuclear explosives to measure the effects were “hostile forces” ever to use them against Israel, Israel’s Haaretz daily newspaper reported June 8, 2015.
Other uses of the term.
The term has also been used historically to refer to certain types of nuclear weapons. Due to the inefficiency of early nuclear weapons, only a small amount of the nuclear material would be consumed during the explosion. Little Boy had an efficiency of only 1.4%. Fat Man, which used a different design and a different fissile material, had an efficiency of 14%. Thus, they tended to disperse large amounts of unused fissile material, and the fission products, which are on average much more dangerous, in the form of nuclear fallout. During the 1950s, there was considerable debate over whether "clean" bombs could be produced and these were often contrasted with "dirty" bombs. "Clean" bombs were often a stated goal and scientists and administrators said that high-efficiency nuclear weapon design could create explosions which generated almost all of their energy in the form of nuclear fusion, which does not create harmful fission products.
But the "Castle Bravo" accident of 1954, in which a thermonuclear weapon produced a large amount of fallout which was dispersed among human populations, suggested that this was not what was actually being used in modern thermonuclear weapons, which derive around half of their yield from a final fission stage of the fast fissioning of the uranium tamper of the secondary. While some proposed producing "clean" weapons, other theorists noted that one could make a nuclear weapon intentionally "dirty" by "salting" it with a material, which would generate large amounts of long-lasting fallout when irradiated by the weapon core. These are known as salted bombs; a specific subtype often noted is a cobalt bomb.

</doc>
<doc id="55983" url="https://en.wikipedia.org/wiki?curid=55983" title="Black rat">
Black rat

The black rat ("Rattus rattus", also known as the ship rat, roof rat, house rat, Alexandrine rat, old English rat, and other names) is a common long-tailed rodent of the genus "Rattus" (rats) in the subfamily Murinae (murine rodents). The species originated in tropical Asia and spread through the Near East in Roman times before reaching Europe by the 1st century and spreading with Europeans across the world.
Black rats are generalist omnivores. They are serious pests to farmers as they eat a wide range of agricultural crops.
Taxonomy.
The black rat was one of the many species originally described by Linnaeus in his 18th century work, "Systema Naturae", and it still bears its original tautonym of "Rattus rattus". It is the type species of the genus "Rattus".
Description.
A typical adult black rat is long, including a tail, and weighs . Despite its name, the black rat exhibits several colour forms. It is usually black to light brown in colour with a lighter underside. In England during the 1920s, several variations were bred and shown alongside domesticated brown rats. This included an unusual green tinted variety. The black rat also has a scraggly coat of black fur, and is slightly smaller than the brown (Norway) rat.
Origin of "Rattus rattus".
"Rattus rattus" bone remains that date back to the Norman Period have been discovered in Britain. Evidence also suggests that "R. rattus" existed in prehistoric Europe as well as the Levant during post-glacial periods. The specific origin of the black rat is uncertain due to the rat's disappearance and reintroduction. Evidence such as DNA and bone fragments also suggests that the rats did not originally come from Europe, but migrated from southeast Asia.
Rats are resilient vectors for many diseases because of their ability to hold so many infectious bacteria in their blood. Rats played a primary role in spreading bacteria, such as "Yersinia pestis", which is responsible for the Justinianic plague and bubonic plague. According to epidemiological models, "Yersinia pestis" originated outside of Europe, which indicates that Western and central Europe have never had any natural rodent plagues. A recent study indicates that other Asiatic rodents served as plague reservoirs, from which infections spread as far west as Europe via trade routes, both overland and maritime. Although the black rat was certainly a plague vector in European ports, the spread of the plague beyond areas colonized by rats suggests that the plague was also circulated by humans after reaching Europe.
The modern black rat was probably spread across Europe in the wake of the Roman conquest and arose from an ancestor that originated in southeast Asia, possibly Malaysia. The Mediterranean black rats differ genetically from their southeast Asian ancestors by having 38 instead of 42 chromosomes. Therefore, it seems that speciation could have occurred when the rats colonized southwest India, which was the primary country from which Romans obtained their spices. Because "Rattus rattus" is a passive traveler, they could have easily traveled to Europe during the trading between Rome and southwestern Asian countries. Evidence also suggests that, in 321–331 BC, Egyptian birds were preying on Mediterranean rats, though this is not enough to prove that Egypt was the source of the rats.
Diet.
Black rats are considered omnivores and eat a wide range of foods, including seeds, fruit, stems, leaves, fungi, and a variety of invertebrates and vertebrates. They are generalists, and thus not very specific in their food preferences, which is indicated by their tendency to feed on any meal provided for cows, swine, chickens, cats, and dogs. They are similar to the tree squirrel in their preference of fruits and nuts. They eat about per day and drink about per day. Their diet is high in water content. They are a threat to many natural habitats because they feed on birds and insects. They are also a threat to many farmers, since they feed on a variety of agricultural-based crops, such as cereals, sugar cane, coconuts, cocoa, oranges, and coffee beans.
Distribution and habitat.
The black rat originated in India and Southeast Asia, and spread to the Near East and Egypt, and then throughout the Roman Empire, reaching Great Britain as early as the 1st century. Europeans subsequently spread it throughout the world. The black rat is again largely confined to warmer areas, having been supplanted by the brown rat ("Rattus norvegicus") in cooler regions and urban areas. In addition to being larger and more aggressive, the change from wooden structures and thatched roofs to bricked and tiled buildings favored the burrowing brown rats over the arboreal black rats. In addition, brown rats eat a wider variety of foods, and are more resistant to weather extremes.
Black rat populations can explode under certain circumstances, perhaps having to do with the timing of the fruiting of the bamboo plant, and cause devastation to the plantings of subsistence farmers; this phenomenon is known as "Mautam" in parts of India.
Black rats are thought to have arrived in Australia with the First Fleet, and subsequently spread to many coastal regions in the country.
In New Zealand, black rats have an unusual distribution and importance, in that they are utterly pervasive through native forests, scrublands, and urban parklands. This is typical only of oceanic islands that lack native mammals, especially other rodents. Throughout most of the world, black rats are found only in disturbed habitats near people, mainly near the coast. Black rats are the most frequent predator of small forest birds, invertebrates, and perhaps lizards in New Zealand forests, and are key ecosystem changers. Controlling their abundance on usefully large areas of the New Zealand mainland is a crucial current challenge for conservation managers.
Black rats adapt to a wide range of habitats. In urban areas they are found around warehouses, residential buildings, and other human settlements. They are also found in agricultural areas, such as in barns and crop fields. In urban areas they prefer to live in dry upper levels of buildings, so they are commonly found in wall cavities and false ceilings. In the wild, black rats live in cliffs, rocks, the ground, and trees. They are great climbers and prefer to live in trees, such as pines and palm trees. Their nests are typically spherical and made of shredded material, including sticks, leaves, other vegetation, and cloth. In the absence of trees, they can burrow into the ground. Black rats are also found around fences, ponds, riverbanks, streams, and reservoirs.
The black rat, along with the brown rat, is one of the most widespread rats and animal species in the world.
Home range.
Home range refers to the area in which an animal travels and spends most of its time. It is thought that male and female rats have similar sized home ranges during the winter, but male rats increase the size of their home range during the breeding season. Along with differing between rats of different gender, home range also differs depending on the type of forest in which the black rat inhabits. For example, home ranges in the southern beech forests of the South Island, New Zealand appear to be much larger than the non-beech forests of the North Island. Due to the limited number of rats that are studied in home range studies, the estimated sizes of rat home ranges in different rat demographic groups are inconclusive.
Nesting behavior.
Through the usage of tracking devices such as radio transmitters, rats have been found to occupy dens located in trees, as well as on the ground. In Puketi, a forest in Kauri, New Zealand, rats have been found to form dens together. Rats appear to den and forage in separate areas in their home range depending on the availability of food resources. Research shows that, in New South Wales, the black rat prefers to inhabit lower leaf litter of forest habitat. There is also an apparent correlation between the canopy height and logs and the presence of black rats. This correlation may be a result of the distribution of the abundance of prey as well as available refuges for rats to avoid predators. As found in North Head, New South Wales, there is positive correlation between rat abundance, leaf litter cover, canopy height, and litter depth. All other habitat variables showed little to no correlation. While this species' relative, the brown (Norway) rat prefers to nest near the ground of a building the black rat will prefer the upper floors and roof. Because of this habit they have been given the common name roof rat.
Foraging behavior.
As generalists, black rats express great flexibility in their foraging behavior. They are predatory animals and adapt to different micro-habitats. They often meet and forage together in close proximity within and between sexes. Rats tend to forage after sunset. If the food cannot be eaten quickly, they will search for a place to carry and hoard to eat at a later time. Although black rats eat a broad range of foods, they are highly selective feeders; only a restricted number of the foods they eat are dominant foods. When black rat populations are presented with a wide diversity of foods, they eat only a small sample of each of the available foods. This allows them to monitor the quality of foods that are present year round, such as leaves, as well as seasonal foods, such as herbs and insects. This method of operating on a set of foraging standards ultimately determines the final composition of their meals. Also, by sampling the available food in an area, the rats maintain a dynamic food supply, balance their nutrient intake, and avoid intoxication by secondary compounds.
Ecology.
Black rats (or their ectoparasites) can carry a number of pathogens, of which bubonic plague (via the rat flea), typhus, Weil's disease, toxoplasmosis and trichinosis are the best known. It has been hypothesized that the displacement of black rats by brown rats led to the decline of the Black Death. This theory has, however, been deprecated, as the dates of these displacements do not match the increases and decreases in plague outbreaks.
Predators and diseases.
The black rat is prey to cats and owls in domestic settings. In less urban settings, rats are preyed on by weasels, foxes, and coyotes. These predators have little effect on the control of the black rat population because black rats are agile and fast climbers. In addition to agility, the black rat also uses its keen sense of hearing to detect danger and quickly evade mammalian and avian predators.
Rats serve as outstanding vectors for transmittance of diseases because they can carry bacteria and viruses in their systems. A number of bacterial diseases are common to rats, and these include "Streptococcus pneumoniae", "Corynebacterium kutsheri," "Bacillus piliformis", "Pasteurella pneumotropica", and "Streptobacillus moniliformis", to name a few. All of these bacteria are disease causing agents in humans. In some cases, these diseases are incurable.
"R. rattus" as an invasive species.
Damage caused by "R. rattus".
After "Rattus rattus" was introduced into the northern islands of New Zealand, they fed on the seedlings adversely affecting the ecology of the islands. Even after eradication of "R. rattus", the negative effects may take decades to reverse. When consuming these seabirds and seabird eggs, these rats reduce the pH of the soil. This harms plant species by reducing nutrient availability in soil, thus decreasing the probability of seed germination. For example, research conducted by Hoffman et al. indicates a large impact on 16 indigenous plant species directly preyed on by "R. rattus". These plants displayed a negative correlation in germination and growth in the presence of black rats.
Rats prefer to forage in forest habitats. In the Ogasawara islands, they prey on the indigenous snails and seedlings. Snails that inhabit the leaf litter of these islands showed a significant decline in population on the introduction of "Rattus rattus". The black rat shows a preference for snails with larger shells (greater than 10 mm), and this led to a great decline in the population of snails with larger shells. A lack of prey refuges makes it more difficult for the snail to avoid the rat.
Complex pest.
The black rat is a complex pest, defined as one that influences the environment in both harmful and beneficial ways. In many cases, after the black rat is introduced into a new area, the population size of some native species declines or goes extinct. This is because the black rat is a good generalist with a wide dietary niche and a preference for complex habitats; this causes strong competition for resources among small animals. This has led to the black rat completely displacing many native species in Madagascar, the Galapagos, and the Florida Keys. In a study by Stokes "et al.", habitats suitable for the native bush rat, "Rattus fuscipes", of Australia are often invaded by the black rat and are eventually occupied by only the black rat. When the abundances of these two rat species were compared in different micro-habitats, both were found to be affected by micro-habitat disturbances, but the black rat was most abundant in areas of high disturbance; this indicates it has a better dispersal ability.
Despite the black rat's tendency to displace native species, it can also aid in increasing species population numbers and maintaining species diversity. The bush rat, a common vector for spore dispersal of truffles, has been extirpated from many micro-habitats of Australia. In the absence of a vector, the diversity of truffle species would be expected to decline. In a study in New South Wales, Australia it was found that, although the bush rat consumes a diversity of truffle species, the black rat consumes as much of the diverse fungi as the natives and is an effective vector for spore dispersal. Since the black rat now occupies many of the micro-habitats that were previously inhabited by the bush rat, the black rat plays an important ecological role in the dispersal of fungal spores. By eradicating the black rat populations in Australia, the diversity of fungi would decline, potentially doing more harm than good.
Control methods.
Large-scale rat control programs have been taken to maintain a steady level of the invasive predators in order to conserve the native species in New Zealand such as kokako and mohua. Pesticides, such as pindone and 1080 (sodium fluoroacetate), are commonly distributed via aerial spray by helicopter as a method of mass control on islands infested with invasive rat populations. Bait, such as brodifacoum, is also used along with coloured dyes in order to kill and identify rats for experimental and tracking purposes. Another method to track rats is the use of wired cage traps, which are used along with bait, such as rolled oats and peanut butter, to tag and track rats to determine population sizes through methods like mark-recapture and radio-tracking. Poison control methods are effective in reducing rat populations to nonthreatening sizes, but rat populations often rebound to normal size within months. Besides their highly adaptive foraging behavior and fast reproduction, the exact mechanisms for their rebound is unclear and are still being studied.
In 2010, the Sociedad Ornitológica Puertorriqueña (Puerto Rican Bird Society) and the Ponce Yacht and Fishing Club launched a campaign to eradicate the black rat from the Isla Ratones (Mice Island) and Isla Cardona (Cardona Island) islands off the municipality of Ponce, Puerto Rico.
Endangerment and conservation.
"Rattus rattus" populations were common in Great Britain, but began to decline after the introduction of the brown rat in the 18th century. "R. rattus" populations remained common in seaports and major cities until the late 19th century, but have been decreased due to rodent control and sanitation measures. There is currently only one wild population of "R. rattus" left in Britain, in the Shiant Islands in the Outer Hebrides in Scotland. Although rats pose a threat to native seabird species and their eggs, seabird populations have remained stable. There are no definite studies about their impact on sea birds in the Shiant Islands.

</doc>
<doc id="55986" url="https://en.wikipedia.org/wiki?curid=55986" title="Xanana Gusmão">
Xanana Gusmão

Kay Rala Xanana Gusmão (, born José Alexandre Gusmão, , on 20 June 1946) is an East Timorese politician. A former militant, he was the first President of East Timor, serving from May 2002 to May 2007. He then became the fourth Prime Minister of East Timor, serving from 8 August 2007 to 16 February 2015. He has been Minister of Planning and Strategic Investment since February 2015.
Early life and career.
Gusmão was born to parents of mixed Portuguese-Timorese ancestry, both of whom were school teachers in Manatuto in what was then Portuguese Timor, and attended a Jesuit high school just outside Dili. After leaving high-school for financial reasons at the age of fifteen, he held a variety of unskilled jobs, while continuing his education at night school. In 1965, at the age of 19, he met Emilia Batista, who was later to become his wife. His nickname, "Xanana", was taken from the name of the American rock and roll band Sha Na Na, who in turn were named after a lyric from a doo-wop song of 1957 by the Silhouettes.
In 1966, Gusmão obtained a position with the public service, which allowed him to continue his education. This was interrupted in 1968 when Gusmão was recruited by the Portuguese Army for national service. He served for three years, rising to the rank of corporal. During this time, he married Emilia Batista, with whom he had a son Eugenio, and a daughter Zenilda. He has since divorced Emilia, and in 2000, he married Australian Kirsty Sword, with whom he had three sons: Alexandre, Kay Olok and Daniel. In 1971, Gusmão completed his national service, his son was born, and he became involved with a nationalist organisation headed by José Ramos-Horta. For the next three years he was actively involved in peaceful protests directed at the colonial system.
It was in 1974 that a coup in Portugal resulted in the beginning of decolonisation for Portuguese Timor, and shortly afterwards the Governor Mário Lemos Pires announced plans to grant the colony independence. Plans were drawn up to hold general elections with a view to independence in 1978. During most of 1975 a bitter internal struggle occurred between two rival factions in Portuguese Timor. Gusmão became deeply involved with the FRETILIN faction, and as a result he was arrested and imprisoned by the rival faction the Timorese Democratic Union (UDT) in mid-1975. Taking advantage of the internal disorder, and with an eye to absorbing the colony, Indonesia immediately began a campaign of destabilisation, and frequent raids into Portuguese Timor were staged from Indonesian West Timor. By late 1975 the Fretilin faction had gained control of Portuguese Timor and Gusmão was released from prison. He was given the position of Press Secretary within the FRETILIN organisation. On 28 November 1975, Fretilin declared the independence of Portuguese Timor as "The Democratic Republic of East Timor", and Gusmão was responsible for filming the ceremony. Nine days later, Indonesia invaded East Timor. At the time Gusmão was visiting friends outside of Dili and he witnessed the invasion from the hills. For the next few days he searched for his family.
Indonesian occupation.
After the appointment of the "Provisional Government of East Timor" by Indonesia, Gusmão became heavily involved in resistance activities. Gusmão was largely responsible for the level of organisation that evolved in the resistance, which ultimately led to its success. The early days featured Gusmão walking from village to village to obtain support and recruits. But after FRETILIN suffered some major setbacks in the early 1980s, including a failed 1984 coup attempt against Gusmão led by four senior Falintil officers, including Mauk Moruk. Gusmão left FRETILIN and supported various centrist coalitions, eventually becoming a leading opponent of FRETILIN. By the mid-1980s, he was a major leader. During the early 1990s, Gusmão became deeply involved in diplomacy and media management, and was instrumental in alerting the world to the massacre in Dili that occurred in Santa Cruz on 12 November 1991. Gusmão was interviewed by many major media channels and obtained worldwide attention.
As a result of his high profile, Gusmão became a prime target of the Indonesian government. A campaign for his capture was finally successful in November 1992. In May 1993, Gusmão was tried, convicted and sentenced to life imprisonment by the Indonesian government. He was found guilty under Article 108 of the Indonesian Penal Code (rebellion), Law no. 12 of 1951 (illegal possession of firearms) and Article 106 (attempting to separate part of the territory of Indonesia). He spoke in his own defence and he was appointed with defence lawyers before the commencement of his trial. The sentence was commuted to 20 years by Indonesian President Suharto in August 1993. Although not released until late 1999, Gusmão successfully led the resistance from within prison with the help of Kristy Sword. By the time of his release, he was regularly visited by United Nations representatives, and dignitaries such as Nelson Mandela.
Transition to independence.
On 30 August 1999, a referendum was held in East Timor and an overwhelming majority voted for independence. The Indonesian military commenced a campaign of terror as a result, with terrible consequences. Although the Indonesian government denied ordering this offensive, they were widely condemned for failing to prevent it. As a result of overwhelming diplomatic pressure from the United Nations, promoted by Portugal since the late 1970s and also by the United States and Australia in the 1990s, a UN-sanctioned, Australian-led international peace-keeping force (INTERFET) entered East Timor, and Gusmão was finally released. Upon his return to his native East Timor, he began a campaign of reconciliation and rebuilding.
Gusmão was appointed to a senior role in the UN administration that governed East Timor until 20 May 2002. During this time he continually campaigned for unity and peace within East Timor, and was generally regarded as the "de facto" leader of the emerging nation. Elections were held in late 2001 and Gusmão, endorsed by nine parties but not by Fretilin, ran as an independent and was comfortably elected leader. As a result, he became the first President of East Timor when it became formally independent on 20 May 2002. Gusmão has published an autobiography with selected writings entitled "To Resist Is to Win". He is the main narrator of the film "A Hero's Journey"/"Where the Sun Rises", a 2006 documentary about him and East Timor. According to director Grace Phan, it's an "intimate insight into the personal transformation" of the man who helped shape and liberate East Timor.
Independent East Timor.
Gusmão declined to run for another term in the April 2007 presidential election. In March 2007 he said that he would lead the new National Congress for Timorese Reconstruction (CNRT) into the parliamentary election planned to be held later in the year, and said that he would be willing to become prime minister if his party won the election. He was succeeded as President by José Ramos-Horta on 20 May 2007. The CNRT placed second in the June 2007 parliamentary election, behind FRETILIN, taking 24.10% of the vote and 18 seats. He won a seat in parliament as the first name on the CNRT's candidate list. The CNRT allied with other parties to form a coalition that would hold a majority of seats in parliament. After weeks of dispute between this coalition and FRETILIN over who should form the government, Ramos-Horta announced on 6 August that the CNRT-led coalition would form the government and that Gusmão would become Prime Minister on 8 August. Gusmão was sworn in at the presidential palace in Dili on 8 August.
On 11 February 2008, a motorcade containing Gusmão came under gunfire one hour after President José Ramos-Horta was shot in the stomach. Gusmão's residence was also occupied by rebels. According to the Associated Press, the incidents raised the possibility of a coup attempt; they have also described as possible assassination attempts and kidnap attempts.
Awards and prizes.
While still in prison, in 1993, he was awarded the Great-Cross of the Portuguese Order of Liberty.
In 1999, Gusmão was awarded the Sakharov Prize for Freedom of Thought.
In July 2000 Gusmão was appointed as a Honorary Companion of the New Zealand Order of Merit for "the furtherance of New Zealand–East Timor relations".
In 2000, he was awarded the Sydney Peace Prize for being "Courageous and principled leader for the independence of the East Timorese people".
Also in 2000, he won the first Gwangju Prize for Human Rights, created to honour "individuals, groups or institutions in Korea and abroad that have contributed in promoting and advancing human rights, democracy and peace through their work."
In 2002, he was awarded the North-South Prize by the Council of Europe.
Mr Gusmão is an Eminent Member of the Sergio Vieira de Mello Foundation.
More recently in 2006 he was awarded the Grande-Collar of the Portuguese Order of Prince Henry.

</doc>
<doc id="55988" url="https://en.wikipedia.org/wiki?curid=55988" title="Tell Abu Hureyra">
Tell Abu Hureyra

Tell Abu Hureyra () is an archaeological site located in the Euphrates valley in modern Syria. The remains of the villages within the tell come from over 4,000 years of habitation, spanning the Epipaleolithic and Neolithic periods. Ancient Abu Hureyra was occupied between 13,000 and 9,500 years ago in radio carbon years. The site is significant because the inhabitants of Abu Hureyra started out as hunter-gatherers but gradually moved to farming, making them the earliest known farmers in the world.
History of research.
The site was excavated as a rescue operation before it would be flooded by Lake Assad, the reservoir of the Tabqa Dam which was being built at that time. The site was excavated by Andrew Moore in 1972 and 1973. It was limited to only two seasons of fieldwork, because the site was due to be flooded by Lake Assad. However, despite the limited time frame, a large amount of material was recovered and studied over the following decades. It was one of the first archaeological sites to use modern methods of excavation such as 'flotation,' which preserved even the tiniest and most fragile plant remains. A preliminary report was published in 1983 and a final report in 2000.
Location and description.
Abu Hureyra is a tell, or ancient settlement mound, located in modern-day Ar-Raqqah Governorate in northern Syria. It is located on a plateau near the south bank of the Euphrates, east of Aleppo. The tell is actually a massive accumulation of collapsed houses, debris, and lost objects accumulated over the course of the habitation of the ancient village. The mound is nearly across, deep, and contains over of archaeological deposits. Today the tell is inaccessible, drowned beneath the waters of Lake Assad.
Occupation history.
First occupation.
The village of Abu Hureyra had two separate periods of occupation: An Epipalaeolithic settlement, and a Neolithic settlement. The Epipaleolithic, or Natufian, settlement was established around 13,500 years ago. During the first settlement, c. 13,000 BP, the village consisted of small round huts, cut into the soft sandstone of the terrace. The roofs were supported with wooden posts, and roofed with brushwood and reeds. Huts contained underground storage areas for food. The population was small, housing a few hundred people at most.
The hunter-gatherers of Abu Hureyra obtained food by hunting, fishing, and gathering of wild plants. Gazelle was hunted primarily during the summer, when vast herds passed by the village during their annual migration. Other prey included large wild animals such as onager, sheep, and cattle, and smaller animals such as hare, fox and birds, which were hunted throughout the year. Plant foods were harvested from "wild gardens," with species gathered including wild cereal grasses such as einkorn wheat, emmer wheat, and two varieties of rye.
Depopulation.
After 1300 years the hunter-gatherers of the first occupation mostly abandoned Abu Hureyra, probably because of the Younger Dryas, an intense and relatively abrupt return to glacial climate conditions which lasted over 1,000 years. The drought disrupted the migration of the gazelle, and decimated the forageable plant food sources. It is likely that most of the inhabitants had to give up sedentism and returned to life on the move. Many of them might have gone to Mureybet - just 50 km upstream on the other side of the Euphrates - which expanded dramatically at this time. It seems that a small population managed to hang on at Abu Hureyra, which thereby was continuously inhabited for 4500 years.
Second occupation.
It is from the early part of the Younger Dryas that the first indirect evidence of agriculture was detected in the excavations at Abu Hureyra, although the cereals themselves were still of the wild variety (see PPNA). It was during the intentional sowing of cereals in more favourable refugees like Mureybet that these first farmers developed domesticated strains during the centuries of drought and cold of the Younger Dryas (see the Khiamian). When the climate abated from 9500 B.C. they spread all over the Middle East with this new bio-technology, and Abu Hureyra grew to a large village eventually with several thousand people. The second occupation grew domesticated varieties of rye, wheat and barley, and kept sheep as livestock (see PPNB). The hunting of gazelle decreased sharply, probably due to an overexploitation that eventually left them extinct in the Middle East. At Abu Hureyra they were replaced by meat from domesticated animals.
Transition from foraging to farming.
Some evidence has been found for cultivation of rye from 11,050 BC. Peter Akkermans and Glenn Schwartz found this claim about epipaleolithic rye ""difficult to reconcile with the absence of cultivated cereals at Abu Hureyra and elsewhere for thousands of years afterwards"". It has been suggested that drier climate conditions resulting from the beginning of the Younger Dryas caused wild cereals to become scarce, leading people to begin cultivation as a means of securing a food supply. Results of recent analysis of the rye grains from this level suggest that they may actually have been domesticated during the Epipalaeolithic. It is speculated that the permanent population of the first occupation was fewer than 200 individuals. These individuals occupied several tens of square kilometers, comprising a rich resource base of several different ecosystems (river, forest and steppe). On this land they hunted, harvested food and wood, made charcoal, and may have cultivated cereals and grains for food and fuel.

</doc>
<doc id="55989" url="https://en.wikipedia.org/wiki?curid=55989" title="Radiological weapon">
Radiological weapon

A radiological weapon or radiological dispersion device (RDD) is any weapon that is designed to spread radioactive material with the intent to kill and cause disruption. According to the U.S. Department of Defense, an RDD is "any device, including any weapon or equipment, other than a nuclear explosive device, specifically designed to employ radioactive material by disseminating it to cause destruction, damage, or injury by means of the 
radiation produced by the decay of such material”.
One type of RDD is a "conventional explosive combined with some type of radiological material", also known as a dirty bomb. It is not a true nuclear weapon and does not yield the same explosive power. It uses conventional explosives to spread radioactive material, most commonly the spent fuels from nuclear power plants or radioactive medical waste. "It is not a Weapon of Mass Destruction (WMD), but rather, as researcher Peter Probst 
calls it, a “weapon of mass disruption” (Hughes, 2002). In fact, effective dispersal ranges are 
rather limited. Most deaths (if any) would come from the initial explosion (non-nuclear), but it 
does depend on the type of radiological material used. (Department of Homeland Security 
, 2003)."
Another version is the salted bomb, a true nuclear weapon designed to produce larger amounts of nuclear fallout than a regular nuclear weapon.
Explanation.
Radiological weapons of mass destruction have been suggested as a possible weapon of terrorism used to create panic and casualties in densely populated areas. They could also render a great deal of property uninhabitable for an extended period, unless costly remediation were undertaken. The radiological source and quality greatly impacts the effectiveness of a radiological weapon.
Factors such as: energy and type of radiation, half-life, longevity, availability, shielding, portability, and the role of the environment will determine the effect of the radiological weapon. Radioisotopes that pose the greatest security risk include: , used in radiological medical equipment, , , , , , , , and .
All of these isotopes, except for the final one, are created in nuclear power plants. While the amount of radiation dispersed from the event will likely be minimal, the fact of any radiation may be enough to cause panic and disruption.
History.
The professional history of radioactive weaponry may be traced to a 1940 science fiction story, "Solution Unsatisfactory" by Robert A. Heinlein and a 1943 memo from James Bryant Conant, Arthur Holly Compton and Harold Urey to Brigadier General Leslie Groves, head of the Manhattan Project.
Transmitting a report entitled, "Use of Radioactive Materials as a Military Weapon," the Groves memo states:
The United States, however, chose not to pursue radiological weapons during World War II, though early on in the project considered it as a backup plan in case nuclear fission proved impossible to tame. Some US policymakers and scientists involved in the project felt that radiological weapons would qualify as chemical weapons and thus violate international law.
Deployment.
One possible way of dispersing the material is by using a dirty bomb, a conventional explosive which disperses radioactive material. Dirty bombs are not a type of nuclear weapon, which requires a nuclear chain reaction and the creation of a critical mass. Whereas a nuclear weapon will usually create mass casualties immediately following the blast, a dirty bomb scenario would initially cause only minimal casualties from the conventional explosion.
Means of radiological warfare that do not rely on any specific weapon, but rather on spreading radioactive contamination via a food chain or water table, seem to be more effective in some ways, but share many of the same problems as chemical warfare.
Military uses.
Radiological weapons are widely considered to be militarily useless for a state-sponsored army and are initially not hoped to be used by any military forces. Firstly, the use of such a weapon is of no use to an occupying force, as the target area becomes uninhabitable (due to the fallout caused by radioactive poisoning of the involved environment).
Furthermore, area-denial weapons are generally of limited use to an attacking army, as it slows the rate of advance.
Dirty bombs.
A dirty bomb is a radiological weapon dispersed with conventional explosives.
There is currently (as of 2007) an ongoing debate about the damage that terrorists using such a weapon might inflict. Many experts believe that a dirty bomb such that terrorists might reasonably be able to construct would be unlikely to harm more than a few people and hence it would be no more deadly than a conventional bomb. Furthermore, the casualties would be a result of the initial explosion, because alpha and beta emitting material needs to be inhaled to do damage to the human body. Gamma radiation emitting material is so radioactive that it can't be deployed without wrapping an amount of shielding material around the bomb that would make transport by car or plane impossible without risking detection.
Because of this a dirty bomb with radioactive material around an explosive device would be almost useless, unless said shielding was removed shortly before detonation. This is not only because of the effectiveness but also because this material would be easy to clean up. Furthermore, the possibility of terrorists making a gas or aerosol that is radioactive is very unlikely because of the complex chemical work to achieve this goal.
Hence, this line of argument goes, the objectively dominant effect would be the moral and economic damage due to the massive fear and panic such an incident would spur. On the other hand, some believe that the fatalities and injuries might be in fact much more severe. This point was made by physicist Peter D. Zimmerman (King's College London) who reexamined the Goiânia accident which is arguably comparable. and popularized in a subsequent fictionalized account produced by the BBC and broadcast in the United States by PBS. The latter program showed how shielding might be used to minimize the detection risk.
Salted bomb.
A salted bomb is a theoretical nuclear weapon designed to produce enhanced quantities of radioactive fallout, rendering a large area uninhabitable. As far as is publicly known none have ever been built.

</doc>
<doc id="55992" url="https://en.wikipedia.org/wiki?curid=55992" title="Tommy Cooper">
Tommy Cooper

Thomas Frederick Cooper (19 March 1921 – 15 April 1984) was a British prop comedian and magician. Cooper was a member of the Magic Circle, and respected by traditional magicians. He was famed for his red fez, and his appearance was large and lumbering, at and more than in weight. On 15 April 1984, Cooper collapsed, and died soon afterwards, from a heart attack on live national television.
Biography.
Born at 19 Llwyn Onn Street in Trecenydd, Caerphilly, Glamorgan, Cooper was delivered by the woman who owned the house in which the family were lodging. His parents were Thomas H. Cooper, a recruiting sergeant in the British Army, and Gertrude (née Wright), his English wife from Crediton in Devon.
To escape from the heavily polluted air of Caerphilly, his father accepted the offer of a new job and the family moved to Exeter, Devon, when Cooper was three. It was in Exeter that he acquired the West Country accent that became part of his act. The family lived in the back of Haven Banks, where Cooper attended Mount Radford School for Boys. He also helped his parents run their ice cream van, which attended fairs at weekends. When he was eight an aunt bought him a magic set and he spent hours perfecting the tricks. His brother David (born 1930) opened a magic shop in the 1960s in Slough High Street called D. & Z. Cooper's Magic Shop.
Cooper was influenced by Laurel and Hardy, Max Miller, Bob Hope, and Robert Orben.
Second World War.
After school Cooper became a shipwright in Hythe, Hampshire, and in 1940 he was called up as a trooper in the Royal Horse Guards. He served initially in Montgomery's Desert Rats in Egypt. Cooper became a member of a NAAFI entertainment party and developed an act around his magic tricks interspersed with comedy. One evening in Cairo, during a sketch in which he was supposed to be in a costume that required a pith helmet, having forgotten the prop, Cooper reached out and borrowed a fez from a passing waiter, which got huge laughs.
Development of the act.
When he was demobbed after seven years of military service Cooper took up show business on Christmas Eve, 1947. He later developed a popular monologue about his military experience as "Cooper the Trooper". He worked in variety theatres around the country and at many of London's top night spots, performing as many as 52 shows in one week.
Cooper had developed his conjuring skills and was a member of the Magic Circle, but there are various stories about how and when he developed his delivery of "failed" magic tricks:
To keep the audience on their toes Cooper threw in an occasional trick that worked when it was least expected.
Career.
In 1947 Cooper got his big break with Miff Ferrie, at that time trombonist in a band called the Jackdaws, who booked him to appear as the second-spot comedian in a show starring the sand dance act Marqueeze and the Dance of the Seven Veils. Cooper then began two years of arduous performing, including a tour of Europe and a stint in pantomime, playing one of Cinderella's ugly sisters. The period culminated in a season-long booking at the Windmill Theatre, where he doubled up doing cabaret. In one week he performed 52 shows. Ferrie remained Cooper's sole agent for 37 years, until Cooper's death in 1984. Cooper was supported by a variety of acts, including the vocal percussionist Frank Holder.
Cooper rapidly became a top-liner in variety with his turn as the conjurer whose tricks never succeeded, but it was his television work that raised him to national prominence. After his debut on the BBC talent show "New to You" in March 1948 he started starring in his own shows, and was popular with audiences for nearly 40 years, notably through his work with London Weekend Television from 1968 to 1972 and with Thames Television from 1973 to 1980. Thanks to his many television shows during the mid-1970s, he was one of the most recognisable comedians in the world.
Cooper was a heavy drinker and smoker, and experienced a decline in health during the late 1970s, suffering a heart attack in 1977 while in Rome, where he was performing a show. Three months later he was back on television in "Night Out at the London Casino". By 1980, however, his drinking meant that Thames Television would not give him another starring series, and "Cooper's Half Hour" was his last. He did continue to appear as a guest on other television shows, however, and worked with Eric Sykes on two Thames productions in 1982.
John Fisher writes in his biography of Cooper: "Everyone agrees that he was mean. Quite simply he was acknowledged as the tightest man in show business, with a pathological dread of reaching into his pocket." One of Cooper's stunts was to pay the exact taxi fare and when leaving the cab to slip something into the taxi driver's pocket saying, "Have a drink on me." That something would turn out to be a tea bag.
By the mid-1970s alcohol had started to erode Cooper's professionalism and club owners complained that he turned up late or rushed through his show in five minutes. In addition he suffered from chronic indigestion, lumbago, sciatica, bronchitis and severe circulation problems in his legs. When Cooper realised the extent of his maladies he cut down on his drinking, and the energy and confidence returned to his act. However, he never stopped drinking and could be fallible: on an otherwise triumphant appearance with Michael Parkinson he forgot to set the safety catch on the guillotine illusion into which he had cajoled Parkinson, and only a last-minute intervention by the floor manager saved Parkinson from serious injury or worse.
Death on a live television show.
On 15 April 1984, Cooper collapsed from a heart attack in front of millions of television viewers, midway through his act on the London Weekend Television variety show "Live from Her Majesty's", transmitted live from Her Majesty's Theatre.
An assistant had helped him put on a cloak for his sketch, while Jimmy Tarbuck, the host, was hiding behind the curtain waiting to pass him different props that he would then appear to pull from inside his gown. The assistant smiled at him as he collapsed, believing that it was a part of the act. Likewise, the audience gave "uproarious" laughter as he fell, gasping for air. At this point, Alasdair MacMillan, the director of the television production, cued the orchestra to play music for an unscripted commercial break (noticeable because of several seconds of blank screen while LWT's master control contacted regional stations to start transmitting advertisements) and Tarbuck's manager tried to pull Cooper back through the curtains. It was decided to continue with the show. Dustin Gee and Les Dennis were the act that had to follow Cooper, and other stars proceeded to present their acts in the limited space in front of the stage. While the show continued, efforts were being made backstage to revive Cooper, not made easier by the darkness. It was not until a second commercial break that ambulancemen were able to move his body to Westminster Hospital, where he was pronounced dead on arrival. His death was not officially reported until the next morning, although the incident was the leading item on the news programme that followed the show. Cooper was cremated at Mortlake Crematorium in London.
The video of Tommy Cooper suffering a heart attack on stage has been uploaded to numerous video sharing websites. YouTube drew criticism from a number of sources when footage of the incident was posted on the website in May 2009. John Beyer of the pressure group Mediawatch UK said: "This is very poor taste. That the broadcasters have not repeated the incident shows they have a respect for him and I think that ought to apply also on YouTube." On 28 December 2011 segments of the "Live From Her Majesty's" clip, including Cooper collapsing on stage, were screened on Channel 4 in the UK, on a programme titled "The Untold Tommy Cooper".
Personal life.
From 1967 until his death, Cooper had a relationship with his personal assistant, Mary Fieldhouse. She wrote about it in her book, "For the Love of Tommy" (1986). His son Thomas (a.k.a. Thomas Henty) died in 1988 and his wife, Gwen, died in 2002.
Legacy.
A statue of Cooper was unveiled in his hometown of Caerphilly, Wales, in 2008 by fellow entertainer Sir Anthony Hopkins, who is patron of the Tommy Cooper Society. The statue was sculpted by James Done.
In 2009 for Red Nose Day, a charity Red Nose was put on the statue, but the nose was stolen.
In a 2005 poll "The Comedians' Comedian", Cooper was voted the sixth greatest comedy act ever by fellow comedians and comedy insiders. He has been cited as an influence by Jason Manford and John Lydon.
In 2012 the British Heart Foundation ran a series of adverts featuring Tommy Cooper to raise awareness of heart conditions. These included posters bearing his image together with radio adverts featuring classic Cooper jokes.
Jerome Flynn has toured with his own tribute show to Cooper called "Just Like That". In February 2007, "The Independent" reported that Andy Harries, a producer of "The Queen", was working on a dramatisation about the last week of Tommy Cooper's life. Harries described Cooper's death as "extraordinary" in that the whole thing was broadcast live on national television. The film subsequently went into production over six years later as a television drama for ITV. From a screenplay by Simon Nye, "" was directed by Benjamin Caron and the title role was played by David Threlfall; it was broadcast 21 April 2014.
"Being Tommy Cooper", a new play written by Tom Green and starring Damian Williams, was produced by Franklin Productions and toured the UK in 2013.
Hip-hop duo dan le sac vs Scroobius Pip wrote the song "Tommy C", about Cooper's career and death, which appears on their 2008 album, "Angles".
In 2014, with the support of The Tommy Cooper Estate and Cooper's daughter Victoria, a new tribute show "Just Like That! The Tommy Cooper Show" commemorating 30 years since the comedian's death was produced by "Hambledon Productions". The production moved to the "Museum of Comedy" in Bloomsbury, London from September 2014 and continues to tour extensively throughout the UK.

</doc>
<doc id="55994" url="https://en.wikipedia.org/wiki?curid=55994" title="Ménière's disease">
Ménière's disease

Ménière's disease , is a disorder of the inner ear that usually affects both hearing and balance. It is characterized by episodes of vertigo and by fluctuating or permanent tinnitus and hearing loss. The condition affects people differently. It can range in intensity from being a mild annoyance to a disabling disease.
The condition is named after the French physician Prosper Ménière, who in an article from 1861 described the main symptoms and was the first to suggest as a single cause for all of them a disorder in the combined organ of balance and hearing in the inner ear.
The immediate cause of Ménière's disease is endolymphatic hydrops, an excess of fluid in the inner ear. The possible causes of endolymphatic hydrops, in turn, are not well understood. For this reason a causal treatment of endolymphatic hydrops - and thus also for Ménière's disease - does not exist.
However, episodes of vertigo usually subside as the illness progresses or stabilizes, and most patients learn to manage tinnitus and hearing loss. This even applies to patients who are affected in both ears from some point in the course of their lives (ca. 30%).
Signs and symptoms.
Ménière's is characterized by recurrent episodes of vertigo, hearing loss and tinnitus. Though it often begins with a single symptom, the disease gradually progresses. Not all symptoms must be present to confirm the diagnosis, but experiencing several of the typical symptoms at once greatly increases the likelihood that a diagnosis of Ménière's is correct. The diagnosis of Ménière's disease is made only if patients complain of both episodic vertigo and episodic sensorineural hearing loss. While these symptoms could be related to a variety of ear-related illnesses, Ménière's disease is characterized by the occurrence of 2-3 symptoms at the same time, in discrete "episodes". Conditions with partly similar symptoms - but no connection to Ménière's disease - include syphilis, Cogan's syndrome, autoimmune inner ear disease, dysautonomia, perilymph fistula, multiple sclerosis, acoustic neuroma, and both hypo- and hyperthyroidism.
Ménière's symptoms vary. Not all sufferers experience the same symptoms. However the "classic" presentation of Ménière's has the following three symptoms:
Some patients may also experience a sensation of fullness or pressure to one or both ears. They may also experience additional symptoms related to irregular reactions of the autonomic nervous system. These symptoms are not symptoms of Meniere's disease per se, but rather are side effects resulting from failure of the organ of hearing and balance, and include nausea, vomiting, and sweating—which are typically symptoms of vertigo, and not of Ménière's. Vertigo may induce nystagmus, or uncontrollable rhythmical and jerky eye movements, usually in the horizontal plane, reflecting the essential role of the non-visual input by the organ of balance in coordinating eye movements.
Sudden falls without loss of consciousness (drop attacks, also known as Tumarkin attacks) may be experienced by some people, usually in the later stages of the disease. Less than 10% of people with Ménière's disease tend to experience such attacks. There is typically a sensation of being pushed sharply to the floor from behind (this is thought to be triggered by a sudden mechanical disturbance of the otolithic membrane that activates motoneurons in the vestibulospinal tract). The affected person is able to get up again immediately afterwards.
Cause.
Ménière's disease is linked to endolymphatic hydrops, an excess of fluid in the inner ear. The membranous labyrinth, a system of membranes in the ear, contains a fluid called endolymph. In Ménière's disease, endolymph bursts from its normal channels in the ear and flows into other areas, causing damage. This accumulation of fluid is referred to as "hydrops". The membranes become dilated (stretched thin, like a balloon) when pressure increases and drainage is blocked. This may be related to swelling of the endolymphatic sac or other tissues in the vestibular system of the inner ear, which is responsible for the body's sense of balance.
Ménière's disease affects about 190 people per 100,000. Recent gender predominance studies show that Ménière's tends to affect women more often than men. In three nationwide surveys in Japan on Ménière's disease between 1975 and 1990 the data of 958 definite Ménière cases were analyzed. Onset of the disease peaked in the forties for males and thirties for females. A higher incidence was noted "in people with a nervous and precise character".
Diagnosis.
Doctors establish a diagnosis with complaints and medical history. However, a detailed otolaryngological examination, audiometry, and head MRI scan should be performed to exclude a vestibular schwannoma or superior canal dehiscence, which would cause similar symptoms. Some of the same symptoms also occur with benign paroxysmal positional vertigo (BPPV), and with cervical spondylosis (which can affect blood supply to the brain and cause vertigo).
Ménière's disease is an idiopathic and therefore a diagnosis of exclusion, meaning there is no definitive test for Ménière's; it is only diagnosed when all other possible causes of the patient's symptom have been ruled out.
History.
Ménière's disease had been recognized as early as the 1860s, but the definition was still relatively vague and broad at the time. The American Academy of Otolaryngology-Head and Neck Surgery Committee on Hearing and Equilibrium (AAO HNS CHE) set criteria for diagnosing Ménière's, as well as defining two sub categories of Ménière's: cochlear (without vertigo) and vestibular (without deafness).
In 1972, the academy defined criteria for diagnosing Ménière's disease as:
In 1985, this list changed to alter wording, such as changing "deafness" to "hearing loss associated with tinnitus, characteristically of low frequencies" and requiring more than one attack of vertigo to diagnose. Finally in 1995, the list was again altered to allow for degrees of the disease:
Management.
Several environmental and dietary changes are thought to reduce the frequency or severity of symptom outbreaks. It is believed that since high sodium intake causes water retention, a diet high in salt can lead to an increase (or at least prevent the decrease) of fluid within the inner ear, although the relationship between salt and the inner ear is not fully understood. Thus, a low sodium diet is often prescribed, with sodium intake reduced to one to two grams of sodium per day (equivalent to approximately 2.5 to 5 grams of table salt, or a little more than one third to two thirds of a teaspoon). By comparison, the recommended Upper Limit (UL) for sodium intake is 2.3 grams per day, and most people are recommended to consume less than 1.5 grams, but on average people in the United States consume 3.4 grams per day.
Low-sodium, high-potassium dietary intake has been recommended on the basis of physiological experiments. The WHO generally recommends for adults a daily potassium intake of 3.5 g.
Getting adequate sleep and avoiding stress is an important aspect of managing the symptoms of Ménière's disease. Additionally, patients are advised to avoid alcohol, caffeine, and tobacco, as well as large quantities of chocolate or salt, all of which can aggravate Ménière's symptoms.
Although a causal relation between allergy and Menière's disease is uncertain, it has been recommended that allergy control is part of the treatment for Menière's disease in patients with a history of seasonal or food allergy, childhood or family history of allergy, or a development of symptoms within a short time after exposure to food or inhaled allergen.
In order to reduce nausea and vomiting during an episode, antihistamines such as meclizine are used.
Equalizing pressure in middle and inner ear.
The systems of pressure equalization of the inner ear are coupled in a highly complex way to the permanently ongoing variations of static pressure in the air chamber in the middle ear. Importantly, the inner ear pressure equalization is activated by these middle ear pressure changes. The details of these mechanisms have been described both physiologically and anatomically.
Middle ear pressure is determined primarily by a slow mechanism, the gas exchange with middle ear tissues, and secondarily by a fast mechanism, the transient opening of the Eustachian tube (auditory tube) that links the nasal spaces (nasopharynx) to the middle ear. The fast mechanism is normally triggered automatically, e.g., when chewing or yawning, but it can also be started voluntarily, e.g., when flying or diving (ear clearing).
In Ménière's patients the natural pressure regulation of the middle ear is clearly worse than normal and already in 1988 it was shown that additional ventilation of the middle ear could prevent Ménière's attacks. In 1997 it could even be established experimentally that additional ventilation of the middle ear indeed is effective against the development and progression of endolymphatic hydrops, the pathology preceding and immediately causing Ménière's disease.
The method of choice for voluntary ventilation of the middle ear is the Valsalva maneuver. It can be applied nearly anywhere spontaneously and without tools and is preferably combined with a following equalization by yawning or chewing (soft plopping sound per ear). The Valsalva maneuver, which always needs to be carried out gently, leads to a short-time increase in middle ear pressure that then disappears during the following equalization. Because the procedure activates – secondarily – also the natural pressure equalization in the inner ear, as described above, it generally has positive effects against the symptoms of Ménière's disease.
Coping.
Sufferers tend to have high stress and anxiety, which may be caused directly by the disease and not merely a secondary effect. Vestibular injuries are known to increase levels of anxiety directly by affecting signal processing in the brain, and vice versa, i.e. anxiety negatively affects vestibular signal processing. Some patients benefit from non-specific yoga, t'ai chi, and meditation. Greenberg and Nedzelski recommend education to alleviate feelings of depression or helplessness.
Surgery.
If symptoms do not improve with typical treatment, more permanent surgery is considered. However, because the inner ear deals with both balance and hearing, few surgeries guarantee no hearing loss.
Nondestructive surgeries include procedures that don't actively remove any functionality, but rather aim to improve the way the ear works. Surgery to decompress the endolymphatic sac has shown effective for relief from symptoms. Most patients see a decrease in vertigo occurrence, while their hearing may be unaffected. In a systematic review and meta-analysis from 2014 it was reported that in at least 75% of patients with Ménière's disease it was effective at controlling vertigo in the short term (>1 yearr of follow-up) and long term (>24 months).
Conversely, destructive surgeries are irreversible and involve removing entire functionality of most, if not all, of the affected ear. The inner ear itself can be surgically removed via labyrinthectomy although hearing is always completely lost in the affected ear with this operation. Alternatively, a chemical labyrinthectomy, in which a drug (such as gentamicin) that "kills" parts or most of the vestibular apparatus is injected into the middle ear, can accomplish the same results with a reasonable chance of retaining hearing, at least partly.
In more serious cases surgeons can cut the nerve to the balance portion of the inner ear in a vestibular neurectomy. Hearing is often mostly preserved, however the surgery involves cutting open into the lining of the brain, and a hospital stay of a few days for monitoring would be required. Vertigo (and the associated nausea and vomiting) typically accompany the recovery from destructive surgeries as the brain learns to compensate.
Physical exercise.
Retraining of the balance system after loss of sensory input from the vestibular system in the inner ear is essential. Almost any kind of physical activity is recommended, as soon as the immediate effects of vertigo attacks have subsided.
Prognosis.
Ménière's disease usually starts confined to one ear, but it often extends to involve both ears over time. The number of patients who end up with bilaterial Ménière's was estimated on the basis of temporal bone autopsies to be ca. 30%.
The impact of vertigo on the ability to work has been described as moderate. According to a Swedish study the loss of annual working days was between none and four.
Hearing loss usually fluctuates in the beginning stages and becomes more permanent in later stages, although hearing aids and cochlear implants can help remedy damage. Tinnitus can be unpredictable, but patients usually get used to it over time.
The course of progression of Ménière's disease varies. Attacks can come more frequently and more severely, less frequently and less severely, and anywhere in between. However, Ménière's has a tendency to "burn out". Once much or all of vestibular function in an ear has disappeared, distorted sensory input to the brain is no longer generated and vertigo attacks cease.
While Ménière's disease in general has no known effects on cognitive performance, vertigo is associated with a clearly increased risk of impairment in concentration and memory.

</doc>
<doc id="55995" url="https://en.wikipedia.org/wiki?curid=55995" title="K. M. Peyton">
K. M. Peyton

Kathleen Wendy Herald Peyton, MBE (born 2 August 1929), who writes primarily as K. M. Peyton, is a British author of fiction for children and young adults. 
She has written more than fifty novels including the much loved "Flambards" series of stories about the Russell family which spanned the period before and after the First World War, for which she won both the 1969 Carnegie Medal from the Library Association and the 1970 Guardian Children's Fiction Prize, judged by a panel of British children's writers. In 1979 the Flambards trilogy was adapted by Yorkshire Television as a 13-part TV series, "Flambards", starring Christine McKenna as the heroine Christina Parsons.
Biography.
Kathleen Herald was born in Birmingham, began writing when she was nine, and was first published when she was fifteen. She "never decided to become a writer ... just was one." Growing up in London where she could not have a horse she was obsessed with them: all her early books are about girls who have ponies. After school, she went to Kingston Art School, then Manchester Art School. There she met another student, Mike Peyton, an ex-serviceman who had been a military artist and prisoner of war. He shared her love of walking in the Pennines. They married when she was twenty-one and went travelling around Europe.
When they returned to Britain, Peyton completed a teaching diploma. However, after the birth of her second daughter, she turned to writing full-time: mostly boys' adventure stories that she sold as serials to "The Scout", magazine of The Scout Association, and later published in full. She began writing as 'K. M. Peyton' at this time; 'M' represented her husband Mike who helped create the plots.
The Peytons loved sailing, and her first books were on that subject; soon, however, she returned to her 'first love', horses, and began to write what became the "Flambards" series. When Peyton became involved with horse racing, she used those experiences as further inspiration for writing.
Fidra Books has reissued "Fly-By-Night" and its sequel, "The Team" (Ruth Hollis series). Oxford University Press, Usborne Publishing and David Fickling Books also publish her work.
Writers who cite K M Peyton as an influence include Linda Newbery, whose young adult novel "The Damage Done" (2001, Scholastic) is dedicated "to Kathleen Peyton, who made me want to try".
"Flambards" was published in Italian, German, Finnish, and Swedish-language editions during the 1970s. WorldCat lists eight other languages of publication for her works in all.
Awards.
Peyton won the Guardian Prize for the Flambards trilogy, exceptionally, and won the Carnegie Medal for its second book.
She was also a commended runner-up for the Carnegie Medal six times in eight years during the 1960s. One of the books was the first Flambards book, another was the third Flambards book in competition with the Medal-winning second. The others were "Windfall" (1962), "The Maplin Bird" (1964), "The Plan for Birdmarsh" (1965), and "Thunder in the Sky" (1966).
Peyton was appointed Member of the Order of the British Empire (MBE) in the 2014 New Year Honours for services to children's literature.
Adaptations.
The Flambards trilogy was adapted by Yorkshire Television in 1978 as a TV series comprising 13 episodes broadcast 1979 in the UK, 1980 in the US: "Flambards", starring Christine McKenna as the heroine Christina Parsons. 
"The Right-Hand Man" (1977), a historical novel featuring an English stagecoach driver, was adapted as a feature film shot in Australia during 1985 and released there in 1987.
"Who, Sir? Me, Sir?" (1985) was adapted as a BBC TV series.
Works.
The bibliography of Peyton's "pony books only" by Jane Badger Books includes all nineteen series books and many "other books" (‡) listed here.
Flambards.
Peyton's extension of the trilogy followed its television adaptation and reversed the original ending.
Pennington.
The Pennington series continues the story of Ruth Hollis.
Jonathan Meredith.
See also the Ruth Hollis series: Jonathan Meredith is a minor character in "The Team".
Minna.
Set in Roman Britain.
Other books.
§ By age fifteen, Kathleen Herald had written "about ten more" novels that publishers rejected with "very nice letters".
‡ Jane Badger Books lists these titles among Peyton's "pony books only" – as well as all nineteen series books listed above.

</doc>
<doc id="55998" url="https://en.wikipedia.org/wiki?curid=55998" title="Prosper Ménière">
Prosper Ménière

Prosper Menière (18 June 1799 – 7 February 1862) was a French doctor who first identified a medical condition combining vertigo, hearing loss and tinnitus, which is now known as Menière's disease.
Biography.
Menière was born in Angers, France. During his education he excelled at humanities and classics. He completed his medical studies at Hôtel-Dieu de Paris in 1826, and earned his M.D. in 1828. He then assisted Guillaume Dupuytren.
Menière was originally set to be an assistant professor in faculty, but political tensions disturbed his professorship and he was sent to control the spread of cholera. He received a legion of honor for his work, but never gained professorship. After securing the position of physician-in-chief at the Institute for deaf-mutes, he focused on the diseases of the ear.
Menière's studies at the deaf-mute institute helped formulate his paper, "On a particular kind of hearing loss resulting from lesions of the inner ear" which ultimately led to the recognition of Menière's disease.
There is debate as to how Menière's name is spelled. Prosper himself was known to write his name as "Menière" while his son used the spelling "Ménière." Many people omit the accent marks.

</doc>
<doc id="55999" url="https://en.wikipedia.org/wiki?curid=55999" title="Tongue">
Tongue

The tongue is a muscular organ in the mouth of most vertebrates that manipulates food for mastication, and is used in the act of swallowing. It is of importance in the digestive system and is the primary organ of taste in the gustatory system. The tongue's upper surface (dorsum) is covered in taste buds housed in numerous lingual papillae. It is sensitive and kept moist by saliva, and is richly supplied with nerves and blood vessels. The tongue also serves as a natural means of cleaning the teeth.A major function of the tongue is the enabling of speech in humans and vocalization in other animals. 
The human tongue is divided into two parts, an oral part at the front and a pharyngeal part at the back. The left and right sides are also separated along most of its length by a vertical section of fibrous tissue that results in a groove on the tongue's surface.
Structure.
The tongue is a muscular hydrostat that forms part of the floor of the oral cavity. The left and right sides of the tongue are separated by a vertical section of fibrous tissue known as the lingual septum. This division is along the length of the tongue save for the very back of the pharyngeal part and is visible as a groove called the median sulcus. The human tongue is divided into anterior and posterior parts by the terminal sulcus which is a V-shaped groove.The apex of the terminal sulcus is marked by a blind foramen the foramen cecum which is the remnant of median thyroid diverticulum in early embryonic development. The anterior "oral" part is the visible part situated at the front and makes up roughly two-thirds the length of the tongue. The posterior "pharyngeal" part is the part closest to the throat, roughly one-third of its length. These parts differ in terms of their embryological development and nerve supply. 
The anterior tongue is, at its apex (or tip), thin and narrow, it is directed forward against the lingual surfaces of the lower incisor teeth. The posterior part is, at its root, directed backward, and connected with the hyoid bone by the hyoglossi and genioglossi muscles and the hyoglossal membrane, with the epiglottis by three glossoepiglottic folds of mucous membrane, with the soft palate by the glossopalatine arches, and with the pharynx by the superior pharyngeal constrictor muscle and the mucous membrane. It also forms the anterior wall of the oropharynx.
In phonetics and phonology, a distinction is made between the tip of the tongue and the blade (the portion just behind the tip). Sounds made with the tongue tip are said to be apical, while those made with the tongue blade are said to be laminal.
Muscles.
The eight muscles of the human tongue are classified as either "intrinsic" or "extrinsic". The four intrinsic muscles act to change the shape of the tongue, and are not attached to any bone. The four extrinsic muscles act to change the position of the tongue, and are anchored to bone.
Extrinsic.
The four extrinsic muscles originate from bone and extend to the tongue. Their main functions are altering the tongue's position allowing for protrusion, retraction, and side-to-side movement.
Intrinsic.
Four paired intrinsic muscles of the tongue originate and insert within the tongue, running along its length. These muscles alter the shape of the tongue by: lengthening and shortening it, curling and uncurling its apex and edges, and flattening and rounding its surface. This provides shape, and helps facilitate speech, swallowing, and eating.
They are: the superior longitudinal muscle which runs along the upper surface of the tongue under the mucous membrane, and elevates, assists in retraction of, or deviates the tip of the tongue. It originates near the epiglottis, at the hyoid bone, from the median fibrous septum; the inferior longitudinal muscle that lines the sides of the tongue, and is joined to the styloglossus muscle; the vertical muscle is located in the middle of the tongue, and joins the superior and inferior longitudinal muscles; and the transverse muscle which divides the tongue at the middle, and is attached to the mucous membranes that run along the sides.
Blood supply.
The tongue receives its blood supply primarily from the lingual artery, a branch of the external carotid artery. The lingual veins, drain into the internal jugular vein. The floor of the mouth also receives its blood supply from the lingual artery.There is also a secondary blood supply to the tongue from the tonsillar branch of the facial artery and the ascending pharyngeal artery.
An area in the neck sometimes called Pirogov's triangle is formed by the intermediate tendon of the digastric muscle, the posterior border of the mylohyoid muscle, and the hypoglossal nerve. The lingual artery is a good place to stop severe hemorrage from the tongue.
Innervation.
Innervation of the tongue consists of motor fibers, special sensory fibers for taste, and general sensory fibers for sensation.
Innervation of taste and sensation is different for the anterior and posterior part of the tongue because they are derived from different embryological structures (pharyngeal arch 1 and pharyngeal arch 3 and 4, respectively).
Histology.
The tongue is covered with numerous taste buds, and filiform, fungiform, vallate and foliate, lingual papillae.
Length.
The average length of the human tongue from the oropharynx to the tip is 10 cm in length.
Weight.
The average weight of the human tongue from adult males is 70g and for adult females 60g.
Development.
The anterior tongue is derived primarily from the first pharyngeal arch. The posterior tongue is derived primarily from the third pharyngeal arch. The second arch however has a substantial contribution during fetal development, but this later atrophies. The fourth arch may also contribute, depending upon how the boundaries of the tongue are defined.
The terminal sulcus, which separates the anterior and posterior tongue, is shaped like a V, with the tip of the V situated posteriorly. At the tip of the terminal sulcus is the foramen caecum, which is the point where the embryological thyroid begins to descend.
Function.
Taste.
Chemicals that stimulate taste receptor cells are known as tastants. Once a tastant is dissolved in saliva, it can make contact with the plasma membrane of the gustatory hairs, which are the sites of taste transduction.
The tongue is equipped with many taste buds on its dorsal surface, and each taste bud is equipped with taste receptor cells that can sense particular classes of tastes. Distinct types of taste receptor cells respectively detect substances that are sweet, bitter, salty, sour, spicy, or taste of umami. Umami receptor cells are the least understood and accordingly are the type most intensively under research.
Mastication.
The tongue is also used for crushing food against the hard palate, during mastication. The epithelium on the tongue’s upper, or dorsal surface is keratinised. Consequently, the tongue can grind against the hard palate without being itself damaged or irritated.
Clinical significance.
Disease.
The tongue is prone to several pathologies. Examples of pathological conditions of the tongue include glossitis (e.g. geographic tongue, median rhomboid glossitis), burning mouth syndrome, oral hairy leukoplakia, oral candidiasis and squamous cell carcinoma. Food debris, desquamated epithelial cells and bacteria often form a visible tongue coating. This coating has been identified as a major contributing factor in bad breath (halitosis), which can be managed by brushing the tongue gently with a toothbrush or using special oral hygiene instruments such as tongue scrapers or mouth brushes.
Medical delivery.
The sublingual region underneath the front of the tongue is a location where the oral mucosa is very thin, and underlain by a plexus of veins. This is an ideal location for introducing certain medications to the body. The sublingual route takes advantage of the highly vascular quality of the oral cavity, and allows for the speedy application of medication into the cardiovascular system, bypassing the gastrointestinal tract. This is the only convenient and efficacious route of administration (apart from I.V. administration) of nitroglycerin to a patient suffering chest pain from angina pectoris.
Society and culture.
Figures of speech.
The tongue can be used as a metonym for "language", as in the phrase "mother tongue". Many languages have the same word for "tongue" and "language".
A common temporary failure in word retrieval from memory is referred to as the "tip-of-the-tongue" phenomenon. The expression "tongue in cheek" refers to a statement that is not to be taken entirely seriously – something said or done with subtle ironic or sarcastic humour. A "tongue twister" is a phrase made specifically to be very difficult to pronounce. Aside from being a medical condition, "tongue-tied" means being unable to say what you want to due to confusion or restriction. The phrase "cat got your tongue" refers to when a person is speechless. To "bite one's tongue" is a phrase which describes holding back an opinion to avoid causing offence. A "slip of the tongue" refers to an unintentional utterance, such as a Freudian slip. Speaking in tongues is a common phrase used to describe "glossolalia", which is to make smooth, language-resembling sounds that is no true spoken language itself. A deceptive person is said to have a forked tongue, and a smooth-talking person said to have a .
Gestures.
Sticking one's tongue out at someone is considered a childish gesture of rudeness and/or defiance in many countries; the act may also have sexual connotations, depending on the way in which it is done. However, in Tibet it is considered a greeting. In 2009, a farmer from Fabriano, Italy was convicted and fined by the country's highest court for sticking his tongue out at a neighbor with whom he had been arguing. Proof of the affront had been captured with a cell phone camera. Blowing a raspberry can also be meant as a gesture of derision.
Body art.
Being a cultural custom for long time, tongue piercing and splitting has become quite common in western countries in recent decades, with up to one-fifth of young adults having at least one piece of body art in the tongue.
As food.
The tongues of some animals are consumed and sometimes considered delicacies. Hot tongue sandwiches are frequently found on menus in Kosher delicatessens in America. Taco de lengua ("lengua" being Spanish for tongue) is a taco filled with beef tongue, and is especially popular in Mexican cuisine. As part of Colombian gastronomy, Tongue in Sauce (Lengua en Salsa), is a dish prepared by frying the tongue, adding tomato sauce, onions and salt. Tongue can also be prepared as birria. Pig and beef tongue are consumed in Chinese cuisine. Duck tongues are sometimes employed in Szechuan dishes, while lamb's tongue is occasionally employed in Continental and contemporary American cooking. Fried cod "tongue" is a relatively common part of fish meals in Norway and Newfoundland. In Argentina and Uruguay cow tongue is cooked and served in vinegar ("lengua a la vinagreta"). In the Czech Republic and Poland, a pork tongue is considered a delicacy, and there are many ways of preparing it. In Eastern Slavic countries, pork and beef tongues are commonly consumed, boiled and garnished with horseradish or jelled; beef tongues fetch a significantly higher price and are considered more of a delicacy. In Alaska, cow tongues are among the more common.
Tongues of seals and whales have been eaten, sometimes in large quantities, by sealers and whalers, and in various times and places have been sold for food on shore.
History.
Etymology.
The word tongue derives from the Old English "tunge", which comes from Proto-Germanic *"tungōn". It has cognates in other Germanic languages — for example "tonge" in West Frisian, "tong" in Dutch/Afrikaans, "Zunge" in German, "tunge" in Danish/Norwegian and "tunga" in Icelandic/Faroese/Swedish. The "ue" ending of the word seems to be a fourteenth-century attempt to show "proper pronunciation", but it is "neither etymological nor phonetic". Some used the spelling "tunge" and "tonge" as late as the sixteenth century.
Other animals.
Most vertebrate animals have tongues. In mammals such as dogs and cats, the tongue is often used to clean the fur and body. The tongues of these species have a very rough texture which allows them to remove oils and parasites. A dog's tongue also acts as a heat regulator. As a dog increases its exercise the tongue will increase in size due to greater blood flow. The tongue hangs out of the dog's mouth and the moisture on the tongue will work to cool the bloodflow.
Some animals have tongues that are specially adapted for catching prey. For example, chameleons, frogs, and anteaters have prehensile tongues.
Many species of fish have small folds at the base of their mouths that might informally be called tongues, but they lack a muscular structure like the true tongues found in most tetrapods.
Other animals may have organs that are analogous to tongues, such as a butterfly's proboscis or a radula on a mollusc, but these are not homologous with the tongues found in vertebrates, and often have little resemblance in function, for example, butterflies do not lick with their proboscides; they suck through them, and the proboscis is not a single organ, but two jaws held together to form a tube.

</doc>
<doc id="56001" url="https://en.wikipedia.org/wiki?curid=56001" title="Conformance testing">
Conformance testing

Conformance testing or type testing is testing to determine whether a product or system or just a medium complies with the requirements of a specification, contract or regulation. This may apply to various technical terms as well as to pure formal terms with respect to obligations of the contractors.
Testing is often either logical testing or physical testing. The test procedures may involve other criteria from mathematical testing or chemical testing. Beyond simple conformance other requirements for efficiency, interoperability or compliance may apply. 
To aid in the aim towards a conformance proof, various test procedures and test setups have been developed, either by the standard's maintainers or external auditing organizations, specifically for testing conformance to standards. Conformance testing is performed preferably by independent organizations, which may be the standards body itself, to give sound assurance of compliance. 
Products tested conformance may then become advertised as being certified by the testing organization as complying with the referred technical standard. Service providers, equipment manufacturers, and equipment suppliers rely on such qualified data to ensure Quality of Service (QoS) through this conformance process.
Typical areas of application.
Conformance testing is applied to various areas of application, as e.g.
In all such testing the subject of test is not just the formal conformance in aspects of e.g.
but especially the aspects of e.g.
Hence conformance testing leads to a vast set of documents and files that allow for re-iterating all performed tests.
Software engineering.
In software testing, conformance testing verifies that a product performs according to its specified standards. Compilers, for instance, are extensively tested to determine whether they meet the recognized standard for that language.
Electronic and electrical engineering.
In electronic engineering and electrical engineering, some countries and business environments (such as telecommunication companies) require that an electronic product meet certain requirements before they can be sold. Standards for telecommunication products written by standards organizations such as ANSI, the FCC, and IEC, etc., have certain criteria that a product must meet before compliance is recognized. In countries such as Japan, China, Korea, and some parts of Europe, products cannot be sold unless they are known to meet those requirements specified in the standards. Usually, manufacturers set their own requirements to ensure product quality, sometimes with levels much higher than what the governing bodies require. Compliance is realized after a product passes a series of tests without occurring some specified mode of failure. Failure levels are usually set depending on what environment the product will be sold in. For instance, test on a product for used in an industrial environment will not be as stringent as a product used in a residential area. A failure can include data corruption, loss of communication, and irregular behavior.
Compliance test for electronic devices include emissions tests, immunity tests, and safety tests. Emissions tests ensure that a product will not emit harmful electromagnetic interference in communication and power lines. Immunity tests ensure that a product is immune to common electrical signals and Electromagnetic interference (EMI) that will be found in its operating environment, such as electromagnetic radiation from a local radio station or interference from nearby products. Safety tests ensure that a product will not create a safety risk from situations such as a failed or shorted power supply, blocked cooling vent, and powerline voltage spikes and dips. 
For example, the telecommunications research and development company Telcordia Technologies publishes conformance standards for telecommunication equipment to pass the following tests: 
Telecom and datacom protocols.
In protocol testing, TTCN-3 has been used successfully to deploy a number of test systems, including protocol conformance testers for SIP, WiMAX, and DSRC.
Based on 3GPP and non-3GPP specification, the test equipment vendors develops the test cases and validated by the bodies.

</doc>
<doc id="56045" url="https://en.wikipedia.org/wiki?curid=56045" title="606 BC">
606 BC


</doc>
<doc id="56046" url="https://en.wikipedia.org/wiki?curid=56046" title="538 BC">
538 BC


</doc>
<doc id="56061" url="https://en.wikipedia.org/wiki?curid=56061" title="Discrete space">
Discrete space

In topology, a discrete space is a particularly simple example of a topological space or similar structure, one in which the points form a "discontinuous sequence", meaning they are "isolated" from each other in a certain sense. The discrete topology is the finest topology that can be given on a set, i.e., it defines all subsets as open sets. In particular, each singleton is an open set in the discrete topology.
Definitions.
Given a set "X":
for any formula_3. In this case formula_4 is called a discrete metric space or a space of isolated points.
A metric space formula_16 is said to be "uniformly discrete" if there exists a "packing radius" formula_17 such that, for any formula_18, one has either formula_19 or formula_20. The topology underlying a metric space can be discrete, without the metric being uniformly discrete: for example the usual metric on the set {1, 1/2, 1/4, 1/8, ...} of real numbers.
Let X = {1, 1/2, 1/4, 1/8, ...}, consider this set using the usual metric on the real numbers. Then, X is a discrete space, since for each point 1/2n, we can surround it with the interval (1/2n - ɛ, 1/2n + ɛ), where ɛ = 1/2(1/2n - 1/2n+1) = 1/2n+2. The intersection (1/2n - ɛ, 1/2n + ɛ) ∩ {1/2n} is just the singleton {1/2n}. Since the intersection of two open sets is open, and singletons are open, it follows that X is a discrete space.
However, X cannot be uniformly discrete. To see why, suppose there exists an r>0 such that d(x,y)>r whenever x≠y. It suffices to show that there are at least two points x and y in X that are closer to each other than r. Since the distance between adjacent points 1/2n and 1/2n+1 is 1/2n+1, we need to find an n that satisfies this inequality:
formula_21
formula_22
formula_23
formula_24
formula_25
formula_26
Since there is always an n bigger than any given real number, it follows that there will always be at least two points in X that are closer to each other than any positive r, therefore X is not uniformly discrete.
Properties.
The underlying uniformity on a discrete metric space is the discrete uniformity, and the underlying topology on a discrete uniform space is the discrete topology.
Thus, the different notions of discrete space are compatible with one another.
On the other hand, the underlying topology of a non-discrete uniform or metric space can be discrete; an example is the metric space "X" := {1/"n" : "n" = 1,2,3...} (with metric inherited from the real line and given by d("x","y") = |"x" − "y"|).
Obviously, this is not the discrete metric; also, this space is not complete and hence not discrete as a uniform space.
Nevertheless, it is discrete as a topological space.
We say that "X" is "topologically discrete" but not "uniformly discrete" or "metrically discrete".
Additionally:
Any function from a discrete topological space to another topological space is continuous, and any function from a discrete uniform space to another uniform space is uniformly continuous. That is, the discrete space "X" is free on the set "X" in the category of topological spaces and continuous maps or in the category of uniform spaces and uniformly continuous maps. These facts are examples of a much broader phenomenon, in which discrete structures are usually free on sets.
With metric spaces, things are more complicated, because there are several categories of metric spaces, depending on what is chosen for the morphisms. Certainly the discrete metric space is free when the morphisms are all uniformly continuous maps or all continuous maps, but this says nothing interesting about the metric structure, only the uniform or topological structure. Categories more relevant to the metric structure can be found by limiting the morphisms to Lipschitz continuous maps or to short maps; however, these categories don't have free objects (on more than one element). However, the discrete metric space is free in the category of bounded metric spaces and Lipschitz continuous maps, and it is free in the category of metric spaces bounded by 1 and short maps. That is, any function from a discrete metric space to another bounded metric space is Lipschitz continuous, and any function from a discrete metric space to another metric space bounded by 1 is short.
Going the other direction, a function "f" from a topological space "Y" to a discrete space "X" is continuous if and only if it is "locally constant" in the sense that every point in "Y" has a neighborhood on which "f" is constant.
Uses.
A discrete structure is often used as the "default structure" on a set that doesn't carry any other natural topology, uniformity, or metric; discrete structures can often be used as "extreme" examples to test particular suppositions. For example, any group can be considered as a topological group by giving it the discrete topology, implying that theorems about topological groups apply to all groups. Indeed, analysts may refer to the ordinary, non-topological groups studied by algebraists as "discrete groups" . In some cases, this can be usefully applied, for example in combination with Pontryagin duality. A 0-dimensional manifold (or differentiable or analytical manifold) is nothing but a discrete topological space. We can therefore view any discrete group as a 0-dimensional Lie group.
A product of countably infinite copies of the discrete space of natural numbers is homeomorphic to the space of irrational numbers, with the homeomorphism given by the continued fraction expansion. A product of countably infinite copies of the discrete space {0,1} is homeomorphic to the Cantor set; and in fact uniformly homeomorphic to the Cantor set if we use the product uniformity on the product. Such a homeomorphism is given by using ternary notation of numbers. (See Cantor space.)
In the foundations of mathematics, the study of compactness properties of products of {0,1} is central to the topological approach to the ultrafilter principle, which is a weak form of choice.
Indiscrete spaces.
In some ways, the opposite of the discrete topology is the trivial topology (also called the "indiscrete topology"), which has the fewest possible open sets (just the empty set and the space itself). Where the discrete topology is initial or free, the indiscrete topology is final or cofree: every function "from" a topological space "to" an indiscrete space is continuous, etc.

</doc>
<doc id="56075" url="https://en.wikipedia.org/wiki?curid=56075" title="Northrop Grumman E-2 Hawkeye">
Northrop Grumman E-2 Hawkeye

The Northrop Grumman E-2 Hawkeye is an American all-weather, carrier-capable tactical airborne early warning (AEW) aircraft. This twin-turboprop aircraft was designed and developed during the late 1950s and early 1960s by the Grumman Aircraft Company for the United States Navy as a replacement for the earlier, radial piston-engined E-1 Tracer, which was rapidly becoming obsolete. The aircraft's performance has been upgraded with the E-2B, and E-2C versions, where most of the changes were made to the radar and radio communications due to advances in electronic integrated circuits and other electronics. The fourth version of the Hawkeye is the E-2D, which first flew in 2007. The E-2 was the first aircraft designed specifically for its role, as opposed to a modification of an existing airframe, such as the Boeing E-3 Sentry. Variants of the Hawkeye have been in continuous production since 1960, giving it the longest production run of any carrier-based aircraft.
The E-2 also received the nickname ""Super Fudd"" because it replaced the E-1 Tracer ""Willy Fudd"". In recent decades, the E-2 has been commonly referred to as the "Hummer" because of the distinctive sounds of its turboprop engines, quite unlike that of turbojet and turbofan jet engines. In addition to U.S. Navy service, smaller numbers of E-2s have been sold to the armed forces of Egypt, France, Israel, Japan, Mexico, Singapore and Taiwan.
Development.
Background.
Continual improvements in airborne radars through 1956 led to the construction of AEW airplanes by several different countries and several different armed forces. The functions of command and control and sea & air surveillance were also added. The first carrier-based aircraft to perform these missions for the U.S. Navy and its allies was the Douglas AD Skyraider, which was replaced in US Navy service by the Grumman E-1 Tracer, which was a modified version of the S-2 Tracker twin-engine anti-submarine warfare aircraft, where the radar was carried in an aerofoil-shaped radome carried above the aircraft's fuselage.
E-2A and E-2B Hawkeye.
In 1956, the U.S. Navy developed a requirement for an airborne early warning aircraft where its data could be integrated into the Naval Tactical Data System aboard the Navy's ships, with a design from Grumman being selected to meet this requirement in March 1957. Its design, initially designated W2F-1, but later redesignated the E-2A Hawkeye, was the first carrier plane that had been designed from its wheels up as an AEW and command and control airplane. The problems facing the design engineers at Grumman were immense, and were compounded by having to constrain the design to enable the aircraft to operate from the older modified s. These ‘smaller’ carriers were built during World War II and later modified to allow them to operate jet aircraft. Consequently, various height, weight and length restrictions had to be factored into the E-2A design, resulting in some handling characteristics which were less than ideal. The E-2A actually never operated from the modified Essex class carriers, and it is likely the design would have benefited considerably if this requirement had never been imposed.
The first prototype, acting as an aerodynamic testbed only, flew on 21 October 1960. The first fully equipped aircraft followed it on 19 April 1961, and entered service with the US Navy as the E-2A in January 1964. By 1965 the major development problems delaying the E-2A Hawkeye got so bad that the aircraft was actually cancelled after 59 aircraft had already been built. Particular difficulties were being experienced due to inadequate cooling in the closely packed avionics compartment. Early computer and complex avionics systems generated considerable heat; without proper ventilation this would lead to system failures. These failures continued long after the aircraft entered service and at one point reliability was so bad the entire fleet of aircraft was grounded. The airframe was also prone to corrosion, a serious problem in a carrier based aircraft.
After Navy officials had been forced to explain to Congress why four production contracts had been signed before avionics testing had been completed, action was taken; Grumman and the US Navy scrambled to improve the design. The unreliable rotary drum computer was replaced by a Litton L-304 digital computer and various avionics systems were replaced – the upgraded aircraft were designated E-2Bs. In total, 49 of the 59 E-2As were upgraded to E-2B standard. These aircraft replaced the E-1B Tracers in the various US Navy AEW squadrons and it was the E-2B that was to set a new standard for carrier based AEW aircraft.
E-2C Hawkeye and developments.
Although the upgraded E-2B was a vast improvement on the unreliable E-2A, it was an interim measure. The US Navy knew the design had much greater capability and had yet to achieve the performance and reliability parameters set out in the original 1957 design. In April 1968 a reliability improvement program was instigated. In addition, now that the capabilities of the aircraft were starting to be realized, more were desired; 28 new E-2Cs were ordered to augment the 49 E-2Bs that would be upgraded. Improvements in the new and upgraded aircraft were concentrated in the radar and computer performance.
Two E-2A test machines were modified as E-2C prototypes, the first flying on 20 January 1971. Trials proved satisfactory and the E-2C was ordered into production, the first production machine performed its initial flight on 23 September 1972. The original E-2C, known as Group 0, consisted of 55 aircraft; the first aircraft became operational in 1973 and serving on carriers in the 1980s and 1990s, until they were replaced in first-line service by Group II aircraft. US Navy Reserve used some aircraft for tracking drug smugglers. The type was commonly used in conjunction with Grumman F-14 Tomcat fighters; monitoring airspace and then vectoring Tomcats over the Link-4A datalink to destroy potential threats with long range AIM-54C Phoenix missiles.
The next production run, between 1988 and 1991, saw 18 aircraft built to the Group I standard. Group I aircraft replaced the E-2's older APS-125 radar and T56-A-425 turboprops with their successors, the APS-139 radar system and T56-A-427 turboprops. The first Group I aircraft entered service on August 1981. Upgrading the Group 0 aircraft to Group 1 specifications was considered, but the cost was comparable to a new production aircraft, so upgrades were not conducted. Group 1 aircraft were only flown by the Atlantic fleet squadrons. This version was followed within a few years by the Group II, which had the improved APS-145 radar. A total of 50 Group II aircraft were delivered, 12 being upgraded Group I aircraft. This new version entered service in June 1992 and served with the Pacific and Atlantic Fleet squadrons.
By 1997 the US Navy intended that all front line squadrons would be equipped, for a total of 75 Group II aircraft. Grumman merged with Northrop in 1994 and plans began on the Group II Plus, also known as the Group II / NAV upgrade. This kept the same computer and radar as the Group II while upgrading the pilot avionics, such as replacing the mechanical Inertial Navigation System (INS) with a more reliable and accurate laser Ring Gyroscope-driven INS, installing dual Multifunction Display Units (MFCDUs) (vice one in the Group II), and the integration of GPS into the weapon system. A variant of the Group II with upgrades to the mission computer and CIC workstations is referred to as the MCU/ACIS, these were produced in small numbers due to production of the Hawkeye 2000 soon after its introduction. All Group II aircraft had their 1960s vintage computer processors replaced by a mission computer with the same functionality via modern computer technology, referred to as the GrIIM RePr (Group II Mission Computer Replacement Program, pronounced "grim reaper").
Another upgrade to the Group II was the Hawkeye 2000 which featured the same APS-145 radar but incorporated an upgraded mission computer and CIC (Combat Information Center) workstations (Advanced Control Indicator Set or ACIS), and carries the U.S. Navy’s new CEC (cooperative engagement capability) data-link system. It is also fitted with a larger capacity vapor cycle avionics cooling system. Starting in 2007 a hardware and software upgrade package began to be added to existing Hawkeye 2000 aircraft. This upgrade allows faster processing, double current trackfile capacity and access to satellite information networks. Hawkeye 2000 cockpits being upgraded include solid-state glass displays, and a GPS-approach capability. The remaining Hawkeye Group II NAV Upgrade aircraft received GPS approach capability, but did not get the solid-state glass displays.
In 2004, the E-2C's propeller system was changed; a new eight-bladed propeller system named NP2000 was developed by the Hamilton-Sundstrand company to replace the old four-bladed design. Improvements included reduced vibrations and better maintainability as a result of the ability to remove prop blades individually instead of having to remove the entire prop and hub assembly. The system had previously been used in the C-130 Hercules, which also uses the T-56 engine, to great effect. However one major difference between the C-130J and the E-2C is that the C-130J uses a six-bladed propeller. The E-2C needed to use a new eight-bladed configuration in order to maintain harmonic compatibility with the electronics that were designed for a four-bladed propeller. The propeller blades are of carbon fiber construction with steel leading edge inserts and de-icing boots at the root of the blade.
E-2D Advanced Hawkeye.
Once considered for replacement by the "Common Support Aircraft", this concept was abandoned. The latest E-2 version is the "E-2D Advanced Hawkeye", which features an entirely new avionics suite including the new AN/APY-9 radar, radio suite, mission computer, integrated satellite communications, flight management system, improved T56-A-427A engines, a glass cockpit and later changes should enable aerial refueling by 2020. The APY-9 radar features an active electronically scanned array, which adds electronic scanning to the mechanical rotation of the radar in its radome. The E-2D will include provisions for the copilot to act as a "Tactical 4th Operator" (T4O), who can reconfigure his main cockpit display to show radar, IFF, and Link 16 (JTIDS)/CEC, and access all acquired data. The E-2D's first flight occurred on 3 August 2007. On 8 May 2009, an E-2D used its Cooperative Engagement Capability system to engage an overland cruise missile with a Standard Missile SM-6 fired from another platform in an integrated fire-control system test. These two systems will form the basis of the Naval Integrated Fire Control – Counter Air (NIFC-CA) when fielded in 2015; the USN is investigating adding other systems to the NIFC-CA network in the future.
The APY-9 radar has been suspected of being capable of detecting fighter-sized stealth aircraft, which are typically optimized against high frequencies like Ka, Ku, X, C, and parts of the S-bands. Small aircraft lack the size or weight allowances for all-spectrum low-observable features, leaving a vulnerability to detection by the UHF-band APY-9 radar, potentially detecting fifth-generation fighters like the Russian Sukhoi PAK FA and the Chinese Chengdu J-20 and Shenyang J-31. Historically, UHF radars had resolution and detection issues that made them ineffective for accurate targeting and fire control; Northrop Grumman and Lockheed claim that the APY-9 has solved these shortcomings in the APY-9 using advanced electronic scanning and high digital computing power via space/time adaptive processing. According to the Navy's NIFC-CA concept, the E-2D could guide fleet weapons, such as AIM-120 AMRAAM and SM-6 missiles, onto targets beyond a launch platform's detection range or capabilities.
Deliveries of initial production E-2Ds began in 2010. On 4 February 2010, Delta One conducted the first E-2D carrier landing aboard USS "Harry S. Truman" as a part of carrier suitability testing. On 27 September 2011, an E-2D was successfully launched by the prototype Electromagnetic Aircraft Launch System (EMALS) at Naval Air Engineering Station Lakehurst. On 12 February 2013, the Office of the Secretary of Defense approved the E-2D to enter full-rate production. The Navy plans for an initial operational capability by 2015. In June 2013, the 10th E-2D was delivered to the Navy, with an additional 10 aircraft in various stages of manufacturing and predelivery flight testing. On 18 July 2013, Northrop Grumman was awarded a $113.7 million contract for five full-rate production Lot 2 E-2D Advanced Hawkeye aircraft. On 13 August 2013, Northrop Grumman was awarded a $617 million contract for five E-2Ds until full-rate production Lot 1. On 30 June 2014, Northrop Grumman was awarded a $3.6 billion contract to supply 25 more E-2D, for a total contracted number of 50 aircraft; 13 E-2D models had been delivered by that time.
Design.
The E-2 is a high-wing airplane, with one turboprop engine on each wing and retractable tricycle landing gear. As with most carrier-borne airplanes, the E-2 is equipped with a tail hook for recovery (landing), and the nose gear can attach to a shuttle of the aircraft carrier's catapults for launch (takeoff). A distinguishing feature of the Hawkeye is its 24-foot (7.3 m) diameter rotating dome that is mounted above its fuselage and wings. This carries the E-2's primary antennas for its long-range radar and IFF systems. No other carrier-borne aircraft possesses one of these, and among land-based aircraft, they are mostly seen atop the Boeing E-3 Sentry, a larger AWACS airplane operated by the U.S. Air Force and NATO air forces in large numbers.
The aircraft is operated by a crew of five, with the pilot and co-pilot on the flight deck and the combat information center officer, air control officer and radar operator stations located in the rear fuselage directly beneath the rotodome.
In U.S. service, the E-2 Hawkeye provides all-weather airborne early warning and command and control capabilities for all aircraft-carrier battle groups. In addition, its other purposes include sea and land surveillance, the control of the aircraft carrier's fighter planes for air defense, the control of strike aircraft on offensive missions, the control of search and rescue missions for naval aviators and sailors lost at sea, and for the relay of radio communications, air-to-air and ship-to-air. It can also serve in an air traffic control capacity in emergency situations when land-based ATC is unavailable.
The E-2C and E-2D Hawkeyes use advanced electronic sensors combined with digital computer signal processing, especially its radars, for early warning of enemy aircraft attacks and anti-ship missile attacks, and the control of the carrier's combat air patrol (CAP) fighters, and secondarily for surveillance of the surrounding sea and land for enemy warships and guided-missile launchers, and any other electronic surveillance missions as directed.
Operational history.
US Navy.
The E-2A entered U.S. Navy service on January 1964, and in April 1964 with VAW-11 at NAS North Island. The first deployment was aboard USS "Kitty Hawk" (CVA-63) during 1965.
Since entering combat during the Vietnam War, the E-2 has served the US Navy around the world, acting as the electronic "eyes of the fleet". In August 1981, a Hawkeye from VAW-124 "Bear Aces" directed two F-14 Tomcats from VF-41 "Black Aces" in an intercept mission in the Gulf of Sidra that resulted in the downing of two Libyan Sukhoi Su-22s. Hawkeyes from VAW-123 aboard the aircraft carrier directed a group of F-14 Tomcat fighters flying the Combat Air Patrol during Operation El Dorado Canyon, the joint strike of two Carrier Battle Groups in the Mediterranean Sea against Libyan terrorist targets during 1986. More recently, E-2Cs provided the command and control for both aerial warfare and land-attack missions during the Persian Gulf War. Hawkeyes have supported the U.S. Coast Guard, the U.S. Customs Service, and American federal and state police forces during anti-drug operations.
In the mid-1980s, several E-2Cs were borrowed from the U.S. Navy and given to the U.S. Coast Guard and the U.S. Customs Service for counternarcotics (CN) and maritime interdiction operations (MIO). This also led to the Coast Guard building a small cadre of Naval Flight Officers (NFOs), starting with the recruitment and interservice transfer of Navy flight officers with E-2 flight experience and the flight training of other junior Coast Guard officers as NFOs. A fatal aircraft mishap on 24 August 1990 involving a Coast Guard E-2C at the former Naval Station Roosevelt Roads in Puerto Rico prompted the Coast Guard to discontinue flying E-2Cs and to return its E-2Cs to the Navy. The U.S Customs Service also returned its E-2Cs to the Navy and concentrated on the use of former U.S. Navy P-3 Orion aircraft in the CN role.
E-2C Hawkeye squadrons played a critical role in air operations during Operation Desert Storm. In one instance, a Hawkeye crew provided critical air control direction to two F/A-18 Hornet aircrew, resulting in the shootdown of two Iraqi MiG-21s. During Operations Southern Watch and Desert Fox, Hawkeye crews continued to provide thousands of hours of air coverage, while providing air-to-air and air-to-ground command and control in a number of combat missions.
The E-2 Hawkeye is a crucial component of all U.S. Navy carrier air wings; each carrier is equipped with four Hawkeyes (five in some situations), allowing for continuous 24-hour-a-day operation of at least one E-2 and for one or two to undergo maintenance in the aircraft carrier's hangar deck at all times. Until 2005. the US Navy Hawkeye’s were organised into East and West coast wings, supporting the respective fleets. However, the East coast wing was disestablished, all aircraft were organised into a single wing based at Point Mugu, California. Six E-2C aircraft were deployed by the US Naval Reserve for drug interdiction and homeland security operations until 9 March 2013, when the sole Reserve squadron, VAW-77 'Nightwolves', was decommissioned and its six aircraft sent to other squadrons.
During Operation Enduring Freedom and Operation Iraqi Freedom all ten Regular Navy Hawkeye squadrons flew overland sorties. They provided battle management for attack of enemy ground targets, close-air-support coordination, combat search and rescue control, airspace management, as well as datalink and communication relay for both land and naval forces. During the aftermath of Hurricane Katrina, three Hawkeye squadrons (two Regular Navy and one Navy Reserve) were deployed in support of civilian relief efforts including Air Traffic Control responsibilities spanning three states, and the control of U.S. Army, U.S. Navy, U.S. Air Force, U.S. Marine Corps, U.S. Coast Guard, and Army National Guard and Air National Guard helicopter rescue units.
Hawkeye 2000s first deployed in 2003 aboard with VAW-117, the "Wallbangers", and CVW-11. U.S. Navy E-2C Hawkeyes have been upgraded with eight-bladed propellers as part of the NP2000 program; the first squadron to cruise with the new propellers was VAW-124 "Bear Aces". The Hawkeye 2000 version can track more than 2,000 targets simultaneously (while at the same time, detecting 20,000 simultaneously) to a range greater than and simultaneously guide 40–100 air-to-air intercepts or air-to-surface engagements. In 2014, several E-2C Hawkeyes from the Bear Aces of VAW-124 were deployed from as flying command posts and air traffic controllers over Iraq during Operation Inherent Resolve against the Islamic State.
VAW-120, the E-2C fleet replacement squadron began receiving E-2D Advanced Hawkeyes for training use in July 2010. On 27 March 2014, the first E-2Ds were delivered to the Airborne Early Warning Squadron 125 (VAW-125). The E-2D achieved Initial Operational Capability (IOC) in October 2014 when VAW-125 was certified to have five operational aircraft. This began training on the aircraft for its first operational deployment, scheduled for 2015 aboard . The E-2D will play a larger role than that of the E-2C, with five E-2Ds aboard each carrier instead of the current four C-models, requiring the acquisition of 75 total E-2Ds. On 11 March 2015, the "Theodore Roosevelt" Carrier Strike Group departed Naval Station Norfolk and returned to port on 23 November 2015, concluding the first operational use of the E-2D.
Other operators.
E-2 Hawkeyes have been sold by the U.S. Federal Government under Foreign Military Sales (FMS) procedures to the armed forces of Egypt, France, Israel, Japan, Singapore and Taiwan.
French Naval Aviation.
The French Naval Aviation (Aeronavale) operates three E-2C Hawkeyes and has been the only operator of the E-2 Hawkeye from an aircraft carrier besides the U.S. Navy. The French nuclear-powered carrier, "Charles De Gaulle" (R 91), currently carries two E-2C Hawkeyes on her combat patrols offshore. The third French E-2C Hawkeye have been upgraded with eight-bladed propellers as part of the NP2000 program. In April 2007, France requested the foreign military sale (FMS) of an additional aircraft.
The Flotille 4F of the French Navy's Aeronavale was stood up on 2 July 2000 and flies its E-2C Hawkeyes from its naval air station at Lann-Bihoue or aboard "Charles de Gaulle". They took part in operations in Afghanistan and Libya.
Japan Air Self-Defense Force.
The Japan Air Self-Defense Force bought thirteen E-2C to improve its Early warning capabilities. The E-2C was put into service with the Airborne Early Warning Group (AEWG) at Misawa Air Base in January 1987.
On 6 September 1976, Soviet Air Force pilot Viktor Belenko successfully defected, landing his MiG-25 'Foxbat' at Hakodate Airport, Japan. During this incident, the Japan Self-Defense Forces' radar lost track of the aircraft when Belenko flew his MiG-25 at a low altitude, prompting the JASDF to consider procurement of airborne early warning aircraft.
Initially, the E-3 Sentry airborne warning and control system aircraft was considered to be the prime candidate for the airborne early warning mission by the JASDF. However, the Japanese Defense Agency realized that the E-3 would not be readily available due to USAF needs and opted to procure E-2 Hawkeye aircraft.
On 21 November 2014, the Japanese Ministry of Defense officially decided to procure the E-2D version of the Hawkeye, instead of the Boeing 737 AEW&C design. In June 2015, the Japanese government requested to buy four E-2Ds through a Foreign Military Sale.
Mexico.
In 2004, three former Israel Air Force E-2C aircraft were sold to the Mexican Navy to perform maritime and shore surveillance missions. These aircraft were upgraded locally by IAI. The first Mexican E-2C was rolled out in January 2004.
Singapore.
The Republic of Singapore Air Force acquired four Grumman E-2C Hawkeye airborne early warning aircraft in 1987, which are assigned to the 111 Squadron "Jaeger" based at Tengah Air Base.
In April 2007, it was announced that the four E-2C Hawkeyes were to be replaced with four Gulfstream G550s which would become the primary early warning aircraft of the Singapore Air Force. On 13 April 2012, the newer G550 AEWs officially took over duty from the former.
Israel.
Israel was the first export customer, its four Hawkeyes were delivered during 1981, complete with the folding wings characteristic of carrier-borne aircraft. The four examples were soon put into active service before and during the 1982 Lebanon War during which they won a resounding victory over Syrian air defenses and fighter control. They were central to the Israeli victory in the air battles over the Bekaa Valley during which more than 90 Syrian fighters were downed. The Hawkeyes were also the linchpins of the operation in which the IAF destroyed the SAM array in the Bekaa, coordinating the various stages of the operation, vectoring planes into bombing runs and directing intercepts. Under the constant defense of F-15 Eagles, there were always two Hawkeyes on station off the Lebanese coast, controlling the various assets in the air and detecting any Syrian aircraft upon their takeoff, eliminating any chance of surprise.
The Israeli Air Force (IAF) operated four E-2s for its homeland AEW protection through 1994. The IAF was the first user of the E-2 to install air-to-air refueling equipment.
Three of the four Israeli-owned Hawkeyes were sold to Mexico in 2002 after they had been upgraded with new systems; the remaining example was sent to be displayed in the Israeli Air Force Museum. In 2010, Singapore began retiring its E-2Cs as well. Both Israel and Singapore now employ the IAI "Eitam", a Gulfstream G550-based platform utilizing Elta's EL/W-2085 sensor package (a newer derivative of the airborne Phalcon system) for their national AEW programmes.
Republic of China (Taiwan).
Taiwan acquired four E-2T aircraft from the US on 22 November 1995. On 15 April 2006 Taiwan commissioned two new E-2K Hawkeyes at an official ceremony at the Republic of China Air Force (ROCAF) base in Pingtung Airport in southern Taiwan.
The four E-2Ts were approved for upgrade to Hawkeye 2000 configuration in a 2008 arms deal. The four E-2T aircraft were upgraded to what became known as E-2K standard in two batches, the first batch of two aircraft were sent to the United States in June 2010, arriving home in late 2011; on their return the second batch of two aircraft were sent for upgrade returning, to Taiwan, in March 2013.
Egypt.
Egypt purchased five E-2C Hawkeyes, that entered service in 1987 and were upgraded to Hawkeye 2000 standard. One additional upgraded E-2C was purchased. The first upgraded aircraft was delivered in March 2003 and deliveries were concluded in late 2008. Egypt requested two additional excess E-2C aircraft in October 2007; deliveries began in 2010. They all operate in 601 AEW Brigade, Cairo-West.
Egypt used the E-2C Hawkeye in Libya bombing operation in 2015 against ISIL.
Offers.
In August 2009, the U.S. Navy and Northrop Grumman briefed the Indian Navy on the E-2D Advanced Hawkeye on its potential use to satisfy its current shore-based and future carrier-based Airborne Early Warning and Control (AEW&C) requirements. The Indian Navy has reportedly expressed interest in acquiring up to six Hawkeyes.

</doc>
<doc id="56079" url="https://en.wikipedia.org/wiki?curid=56079" title="Krull dimension">
Krull dimension

In commutative algebra, the Krull dimension of a commutative ring "R", named after Wolfgang Krull, is the supremum of the lengths of all chains of prime ideals. The Krull dimension need not be finite even for a Noetherian ring. More generally the Krull dimension can be defined for modules over possibly non-commutative rings as the deviation of the poset of submodules.
The Krull dimension has been introduced to provide an algebraic definition of the dimension of an algebraic variety: the dimension of the affine variety defined by an ideal "I" in a polynomial ring "R" is the Krull dimension of "R"/"I".
A field "k" has Krull dimension 0; more generally, "k"["x"1, ..., "x""n"] has Krull dimension "n". A principal ideal domain that is not a field has Krull dimension 1. A local ring has Krull dimension 0 if and only if every element of its maximal ideal is nilpotent.
Explanation.
We say that a chain of prime ideals of the form
formula_1
has length n. That is, the length is the number of strict inclusions, not the number of primes; these differ by 1. We define the Krull dimension of formula_2 to be the supremum of the lengths of all chains of prime ideals in formula_2.
Given a prime formula_4 in "R", we define the height of formula_4, written formula_6, to be the supremum of the lengths of all chains of prime ideals contained in formula_4, meaning that formula_8. In other words, the height of formula_4 is the Krull dimension of the localization of "R" at formula_4. A prime ideal has height zero if and only if it is a minimal prime ideal. The Krull dimension of a ring is the supremum of the heights of all maximal ideals, or those of all prime ideals.
In a Noetherian ring, every prime ideal has finite height. Nonetheless,
Nagata gave an example of a Noetherian ring of infinite Krull dimension. A ring is called catenary if any inclusion formula_11 of prime ideals can be extended to a maximal chain of prime ideals between formula_4 and formula_13, and any two maximal chains between formula_4
and formula_13 have the same length. A ring is called universally catenary if any finitely generated algebra over it is catenary. Nagata gave an example of a Noetherian ring which is not catenary.
In a Noetherian ring, Krull's height theorem says that the height of an ideal generated by "n" elements is no greater than "n".
More generally, the height of an ideal I is the infimum of the heights of all prime ideals containing I. In the language of algebraic geometry, this is the codimension of the subvariety of Spec(formula_2) corresponding to I.
Krull dimension and schemes.
It follows readily from the definition of the spectrum of a ring Spec("R"), the space of prime ideals of "R" equipped with the Zariski topology, that the Krull dimension of "R" is equal to the dimension of its spectrum as a topological space, meaning the supremum of the lengths of all chains of irreducible closed subsets. This follows immediately from the Galois connection between ideals of "R" and closed subsets of Spec("R") and the observation that, by the definition of Spec("R"), each prime ideal formula_4 of "R" corresponds to a generic point of the closed subset associated to formula_4 by the Galois connection.
Krull dimension of a module.
If "R" is a commutative ring, and "M" is an "R"-module, we define the Krull dimension of "M" to be the Krull dimension of the quotient of "R" making "M" a faithful module. That is, we define it by the formula:
where Ann"R"("M"), the annihilator, is the kernel of the natural map R → End"R"(M) of "R" into the ring of "R"-linear endomorphisms of "M".
In the language of schemes, finitely generated modules are interpreted as coherent sheaves, or generalized finite rank vector bundles.
Krull dimension for non-commutative rings.
The Krull dimension of a module over a possibly non-commutative ring is defined as the deviation of the poset of submodules ordered by inclusion. For commutative Noetherian rings, this is the same as the definition using chains of prime ideals. The two definitions can be different for commutative rings which are not Noetherian.

</doc>
<doc id="56083" url="https://en.wikipedia.org/wiki?curid=56083" title="Public school">
Public school

Public school may refer to:

</doc>
<doc id="56085" url="https://en.wikipedia.org/wiki?curid=56085" title="Japanese school uniform">
Japanese school uniform

The Japanese school uniform is modeled on European-style naval uniforms and was first used in Japan in the late 19th century. Today, school uniforms are common in many of the Japanese public and private school systems. The Japanese word for this type of uniform is .
History.
An official from Tombow Co., a manufacturer of the sailor fuku (sailor outfits), said that the Japanese took the idea from scaled down sailor suits worn by children of royal European families. The official said "In Japan, they were probably seen as adorable Western-style children’s outfits, rather than navy gear." Sailor suits were adopted in Japan for girls because the uniforms were easy to sew. The sides of the uniform had similarity to existing styles of Japanese dressmaking, and the collar had straight lines. Many home economics classes in Japan up until the 1950s gave sewing sailor outfits as assignments. Girls sewed sailor outfits for younger children in their communities.
In the 1980s "sukeban" gangs began modifying uniforms by making skirts longer and shortening the tops, and so schools began switching to blazer style uniforms to try to combat the effect. As of 2012, 50% of Japanese junior high schools and 20% of senior high schools use sailor suit uniforms.
The "Asahi Shimbun" stated in 2012 that "The sailor suit is changing from adorable and cute, a look that 'appeals to the boys,' to a uniform that 'girls like to wear for themselves.'" As of that year, contemporary sailor suits have front closures with zippers or snaps and more constructed bodices. The "Asahi Shimbun" stated that "he form is snug to enhance the figure—the small collar helps the head look smaller, for better balance".
Usage.
The Japanese junior- and senior-high-school uniform traditionally consists of a military style uniform for boys and a sailor outfit for girls. These uniforms are based on Meiji era formal military dress, themselves modeled on European-style naval uniforms. The sailor outfits replace the undivided hakama (andon bakama 行灯袴) designed by Utako Shimoda between 1920 and 1930. While this style of uniform is still in use, many schools have moved to more Western-pattern catholic school uniform styles. These uniforms consist of a white shirt, tie, blazer with school crest, and tailored trousers (often not of the same color as the blazer) for boys and a white blouse, tie, blazer with school crest, and tartan culottes or skirt for girls.
Regardless of what type of uniform any particular school assigns its students, all schools have a summer version of the uniform (usually consisting of just a white dress shirt and the uniform slacks for boys and a reduced-weight traditional uniform or blouse and tartan skirt with tie for girls) and a sports-activity uniform (a polyester track suit for year-round use and a T-shirt and short pants for summer activities). Depending on the discipline level of any particular school, students may often wear different seasonal and activity uniforms within the same classroom during the day. Individual students may attempt to subvert the system of uniforms by wearing their uniforms incorrectly or by adding prohibited elements such as large loose socks or badges. Girls may shorten their skirts, permanently or by wrapping up the top to decrease length; boys may wear trousers about the hips, omit ties, or keep their shirts unbuttoned.
Since some schools do not have sex-segregated changing- or locker-rooms, students may change for sporting activities in their classrooms. As a result, such students may wear their sports uniforms under their classroom uniforms. Certain schools also regulate student hairstyles, footwear, and book bags; but these particular rules are usually adhered to only on special occasions, such as trimester opening and closing ceremonies and school photo days.
It is normal for uniforms to be worn outside of school areas, however this is going out of fashion and many students wear a casual dress. While not many public elementary schools in Japan require uniforms, many private schools and public schools run by the central government still do so.
Gakuran.
The or the are the uniforms for many middle school and high school boys in Japan. The color is normally black, but some schools use navy blue.
The top has a standing collar buttoning down from top-to-bottom. Buttons are usually decorated with the school emblem to show respect to the school. Pants are straight leg and a black or dark-colored belt is worn with them. Boys usually wear penny loafers or sneakers with this uniform. Some schools may require the students to wear collar-pins representing the school and/or class rank.
The second button from the top of a male's uniform is often given away to a female he is in love with, and is considered a way of confession. The second button is the one closest to the heart and is said to contain the emotions from all three years attendance at the school. This practice was apparently made popular by a scene in a novel by Taijun Takeda.
Traditionally, the gakuran is also worn along with a matching (usually black) student cap, although this custom is less common in modern times.
The gakuran is derived from French Army uniforms. The term is a combination of "gaku" (学) meaning "study" or "student", and "ran" (らん or 蘭) meaning the Netherlands or, historically in Japan, the West in general; thus, "gakuran" translates as "Western student (uniform)". Such clothing was also worn by school children in South Korea and pre-1949 China.
Sailor fuku.
The is a common style of uniform worn by female middle school students, traditionally by high school students, and occasionally, elementary school students. It was introduced as a school uniform in 1920 in and 1921 by the principal of , Elizabeth Lee. It was modeled after the uniform used by the British Royal Navy at the time, which Lee had experienced as an exchange student in the United Kingdom.
Much like the male uniform, the gakuran, the sailor outfits bears a similarity to various military styled naval uniforms. The uniform generally consists of a blouse attached with a sailor-style collar and a pleated skirt. There are seasonal variations for summer and winter: sleeve length and fabric are adjusted accordingly. A ribbon is tied in the front and laced through a loop attached to the blouse. Several variations on the ribbon include neckties, bolo ties, neckerchiefs, and bows. Common colors are navy blue, white, grey, light green and black.
Shoes, socks, and other accessories are sometimes included as part of the uniform. These socks are typically navy or white. The shoes are typically brown or black penny loafers. Although not part of the prescribed uniform, alternate forms of legwear (such as loose socks, knee-length stockings, or similar) are also commonly matched by more fashionable girls with their sailor outfits.
The sailor uniform today is generally associated with junior high schools. A majority of (though by no means all) high schools having changed to more western style plaid skirts or blazers. A large part of the motivation for this change was as a response to the fetishisation of the sailor outfits as well as the desire of modern high school students to differentiate themselves in a more grown up way from junior high students.
Cultural significance.
Various schools are known for their particular uniforms. Uniforms can have a nostalgic characteristic for former students, and are often associated with relatively carefree youth. Uniforms are sometimes modified by students as a means of exhibiting individualism, including lengthening or shortening the skirt, removing the ribbon, hiding patches or badges under the collar, etc. In past decades, brightly coloured variants of the sailor outfits were also adopted by Japanese yankii and Bōsōzoku biker gangs.
Because school uniforms are a popular fetish item, second-hand sailor outfits and other items of school wear are brokered through underground establishments known as burusera, although changes to Japanese law have made such practices difficult. The pop group Onyanko Club had a provocative song called "Don't Strip Off the Sailor Suit!".

</doc>
<doc id="56086" url="https://en.wikipedia.org/wiki?curid=56086" title="Territorial dispute">
Territorial dispute

A territorial dispute is a disagreement over the possession/control of land between two or more territorial entities or over the possession or control of land, usually between a new state and the occupying power.
Context and definitions.
Territorial disputes are often related to the possession of natural resources such as rivers, fertile farmland, mineral or oil resources although the disputes can also be driven by culture, religion and ethnic nationalism. Territorial disputes result often from vague and unclear language in a treaty that set up the original boundary.
Territorial disputes are a major cause of wars and terrorism as states often try to assert their sovereignty over a territory through invasion, and non-state entities try to influence the actions of politicians through terrorism. International law does not support the use of force by one state to annex the territory of another state. The UN Charter says: ""All Members shall refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the Purposes of the United Nations.""
In some cases, where the boundary is not demarcated, such as the Taiwan Strait, and Kashmir, involved parties define a line of control that serves as the "de facto" international border.
Basis in international law.
Territorial disputes have significant meaning in the international society, both because it is related to the fundamental right of states, sovereignty, and also because it is important for international peace. 
International law has significant relations with territorial disputes because territorial disputes tackles the basis of international law; the state territory. International law is based on the 'persons' of international law, which requires a 'defined territory' as mentioned in the Montevideo convention of 1933. 
Article 1 of Montevideo Convention on the Rights and Duty of States
"a person of international law should possess the following qualifications: (a) a permanent population; (b) a defined territory; (c) government; and (d) capacity to enter into relations with other States"
Also, as mentioned in B.T.Sumner's article, "In international law and relations, ownership of territory is significant because sovereignty over land defines what constitutes a state."
Therefore, the breach of a country's borders or territorial disputes pose a threat to a state's very sovereignty and the right as a person of international law. 
In addition, territorial disputes are sometimes brought upon the International Court of Justice, as was the case in Costa Rica and Nicaragua (2005). Territorial disputes cannot be separated from international law, because its basis is on the law of state borders, and because its potential settlement also relies on the international law and court.

</doc>
<doc id="56092" url="https://en.wikipedia.org/wiki?curid=56092" title="Trieste">
Trieste

Trieste (; ) (, ) is a city and seaport in northeastern Italy. It is situated towards the end of a narrow strip of Italian territory lying between the Adriatic Sea and Slovenia, which lies almost immediately south and east of the city. It is also located near Croatia some further south. Trieste is located at the head of the Gulf of Trieste and throughout history it has been influenced by its location at the crossroads of Latin, Slavic, and Germanic cultures. In 2009, it had a population of about 205,000 and it is the capital of the autonomous region Friuli-Venezia Giulia and the Province of Trieste.
Trieste was one of the oldest parts of the Habsburg Monarchy. In the 19th century, it was the most important port of one of the Great Powers of Europe. As a prosperous seaport in the Mediterranean region, Trieste became the fourth largest city of the Austro-Hungarian Empire (after Vienna, Budapest, and Prague). In the "fin-de-siècle" period, it emerged as an important hub for literature and music. It underwent an economic revival during the 1930s, and Trieste was an important spot in the struggle between the Eastern and Western blocs after the Second World War. Today, the city is in one of the richest regions of Italy, and has been a great centre for shipping, through its port (Port of Trieste), shipbuilding and financial services.
In 2012, LonelyPlanet.com listed the city of Trieste as the world's most underrated travel destination.
Name.
The original pre-Roman name of the city, "Tergeste", with the "-est-" suffix typical of Illyrian, is speculated to be derived from a hypothetical Venetic word "*terg-" "market", etymologically related to Old Church Slavonic "" "market" (whence Slovenian and Serbian "trg", "tržnica", and the Scandinavian borrowing "torg"). Roman authors also transliterated the name as "Tergestum". Modern names of the city include: , , , , , , and .
Geography.
Trieste lies in the northernmost part of the high Adriatic in northeastern Italy, near the border with Slovenia. The city lies on the Gulf of Trieste.
It lies on the borders of the Italian geographical region, the Balkan Peninsula, and the Mitteleuropa.
Climate.
The territory of Trieste is composed of several different climate zones depending on the distance from the sea and elevation. The average temperatures are in January and in July. The climatic setting of the city is humid subtropical climate ("Cfa" according to Köppen climate classification) with strong Mediterranean influences. On average, humidity levels are pleasantly low (~65%), while only two months (January & February) receive slightly less than of precipitation. Trieste along with the Istrian peninsula enjoys evenly distributed rainfall above in total; it is noteworthy that no true summer drought occurs. Snow occurs on average 0 – 2 days per year. Temperatures are very mild - lows below zero are somewhat rare (with just 9 days per a year) and highs above similarly can be expected 15 days a year only. Winter maxima are lower than in typical Mediterranean zone (~ 5 - 11 °C) with quite high minima (~2 - 8 °C). Two basic weather patterns interchange - sunny, sometimes windy but often very cold days (max. +7, min. +3; frequently connected to an occurrence of northeast wind called "Bora" ) and rainy days with temperatures about . Summer is very warm with maxima about and lows above , with the hot nights being influenced by the warm sea water. Absolute maximum of the last fifty years is in 2003. Absolute minimum is in 1963. Average year temperature (1971/2000), , is nearly the same as that of Earth.
The Trieste area is divided into 8a-10a zones according to USDA hardiness zoning; Villa Opicina (320 to 420 MSL) with "8a" in upper suburban area down to "10a" in especially shielded and windproof valleys close to the Adriatic sea.
The climate can be severely affected by the "Bora", a very dry and usually cool north-to-northeast katabatic wind that can last for several days and reach speeds of up to , thus sometimes bringing subzero temperatures to the entire city.
City districts.
Trieste is administratively divided in seven districts:
The iconic city center is Piazza Unità d'Italia, which is between the large 19th-century avenues and the old medieval city, composed of many narrow and crooked streets.
History.
Ancient history.
Since the second millennium BC, the location was an inhabited site. Originally an Illyrian settlement, the Veneti entered the region in the 10th-9th c. BC and seem to have given the town its name, "Tergeste", since "terg*" is a Venetic word meaning market (q.v. Oderzo whose ancient name was "Opitergium"). Still later, the town was later captured by the Carni, a tribe of the Eastern Alps, before becoming part of the Roman republic in 177 BC during the Istrian War. Between 52 and 46 BC, it was granted the status of Roman colony under Julius Caesar, who recorded its name as "Tergeste" in "Commentarii de Bello Gallico" (51 BC), his work which recounts events of the Gallic Wars.
In imperial times the border of Roman Italy moved from the Timavo river to Formione (today Risano). Roman Tergeste flourished due to its position on the road from Aquileia, the main Roman city in the area, to Istria, and as a port, some ruins of which are still visible. Emperor Augustus built a line of walls around the city in 33–32 BC, while Trajan built a theatre in the 2nd century. At the same time, the citizens of the town were enrolled in the tribe Pupinia. In 27 BC, Trieste was incorporated in "Regio X" of Augustan "Italia".
In the early Christian era Trieste continued to flourish. Between AD 138 and 161, its territory was enlarged and nearby Carni and Catali were granted Roman citizenship by the Roman Senate and Emperor Antoninus Pius at the pleading of a leading Tergestine citizen, the "quaestor urbanus", Fabius Severus.
Late Antiquity.
The city was witness to the Battle of the Frigidus in Vipava valley in AD 397, in which Theodosius defeated Eugene. Despite the deposition of Romulus Augustulus at Ravenna in 476 and the ascension to power of Odoacer in Italy, Trieste was retained for a time by the Roman Emperor seated at Constantinople, and thus, became a Byzantine military outpost. In 539, the Byzantines annexed it to the Exarchate of Ravenna and despite Trieste's being briefly taken by the Lombards in 567 in the course of their invasion of northern Italy, held it until the time of the coming of the Franks.
Middle Ages.
In 788, Trieste submitted to Charlemagne who placed it under the authority of their count-bishop who in turn was under the Duke of Friùli. From 1081 the city came loosely under the Patriarchate of Aquileia, developing into a free commune by the end of the 12th century.
During the 13th and 14th centuries, Trieste became a maritime trade rival to the Republic of Venice which briefly occupied it in 1283–87, before coming under the patronage of the Patriarchate of Aquileia. After committing a perceived offence against Venice, the Venetian State declared war against Trieste in July 1368 and by November had occupied the city. Venice intended to keep the city and began rebuilding its defenses, but was forced to leave in 1372. By the Peace of Turin in 1381, Venice renounced its claim to Trieste and the leading citizens of Trieste petitioned Leopold III of Habsburg, Duke of Austria, to make Trieste part of his domains. The agreement of voluntary submission ("dedizione") was signed at the castle of Graz on 30 September 1382. The city maintained a high degree of autonomy under the Habsburgs, but was increasingly losing ground as a trade hub, both at the expense of Venice and the Ragusa (Dubrovnik). In 1463, a number of Istrian communities petitioned Venice to attack Trieste. Trieste was saved from utter ruin by the intervention of Pope Pius II who had previously been bishop of Trieste. However, Venice limited Trieste's territory to outside the city. Trieste would be assaulted again in 1468-1469 by Holy Roman Emperor Frederick III. His sack of the city is remembered as the "Destruction of Trieste." Trieste was fortunate to be spared another sack in 1470 by the Ottomans who burned the village of Prosecco, only about from Trieste, while on their way to attack Friuli.
Early modern period.
Following an unsuccessful Habsburg invasion of Venice in the prelude to the 1508–16 War of the League of Cambrai, the Venetians occupied Trieste again in 1508, and were allowed to keep the city under the terms of the peace treaty. However, the Habsburg Empire recovered Trieste a little over one year later, when the conflict resumed. By the 18th century Trieste became an important port and commercial hub for the Austrians. In 1719, it was granted status as a free port within the Habsburg Empire by Emperor Charles VI, and remained a free port until 1 July 1891. The reign of his successor, Maria Theresa of Austria, marked the beginning of a very prosperous era for the city.
19th century.
In the following decades, Trieste was briefly occupied by troops of the French Empire during the Napoleonic Wars on several occasions, in 1797, 1805 and 1809. From 1809 to 1813, Trieste was annexed into Illyrian Provinces, interrupting its status of free port and losing its autonomy. The municipal autonomy was not restored after the return of the city to the Austrian Empire in 1813. Following the Napoleonic Wars, Trieste continued to prosper as the Free Imperial City of Trieste (), a status that granted economic freedom, but limited its political self-government. The city's role as Austria's main trading port and shipbuilding centre was later emphasized with the foundation of the merchant shipping line Austrian Lloyd in 1836, whose headquarters stood at the corner of the Piazza Grande and Sanità (today's Piazza Unità d'Italia). By 1913 Austrian Lloyd had a fleet of 62 ships comprising a total of 236,000 tons. With the introduction of the constitutionalism in the Austrian Empire in 1860, the municipal autonomy of the city was restored, with Trieste becoming capital of the Austrian Littoral crown land ().
In the later part of the 19th century, Pope Leo XIII considered moving his residence to Trieste or Salzburg because of what he considered a hostile anti-Catholic climate in Italy following the 1870 Capture of Rome by the newly established Kingdom of Italy. However, the Austrian monarch, Franz Josef I, rejected the idea. The modern Austro-Hungarian Navy used Trieste as a base and for shipbuilding. The construction of the first major trunk railway in the Empire, the Vienna-Trieste Austrian Southern Railway, was completed in 1857, a valuable asset for trade and the supply of coal.
In 1882 an Irredentist activist, Guglielmo Oberdan, attempted to assassinate Emperor Franz Joseph, who was visiting Trieste. Oberdan was caught, convicted, and executed. He was regarded as a martyr by radical Irredentists, but as a cowardly villain by the supporters of the Austro-Hungarian monarchy. Franz Joseph, who reigned another thirty-five years, never visited Trieste again.
20th century.
At the beginning of the 20th century, Trieste was a bustling cosmopolitan city frequented by artists and philosophers such as James Joyce, Italo Svevo, Sigmund Freud, Dragotin Kette, Ivan Cankar, Scipio Slataper, and Umberto Saba. The city was the major port on the Austrian Riviera, and perhaps the only real enclave of Mitteleuropa (i.e. Central Europe) south of the Alps. Viennese architecture and coffeehouses dominate the streets of Trieste to this day.
World War I, annexation to Italy and the Fascist era.
Italy, in return for entering World War I on the side of the Allied Powers, had been promised substantial territorial gains, which included the former Austrian Littoral and western Inner Carniola. Italy therefore annexed the city of Trieste at the war end, in accordance with the provisions of the 1915 Treaty of London and the Italian-Yugoslav 1920 Treaty of Rapallo. While only a few hundred Italians remained in the newly established South Slavic state, a population of half a million Slavs, of which the annexed Slovenes were cut off from the remaining three-quarters of total Slovene population at the time, were subjected to forced Italianization. Trieste had a large Italian majority, but it had more ethnic Slovene inhabitants than even Slovenia's capital of Ljubljana at the end of 19th century.
The Italian lower middle class—who felt most threatened by the city's Slovene middle class—sought to make Trieste a "città italianissima", committing a series of attacks led by Black Shirts against Slovene-owned shops, libraries, and lawyers' offices, and even the Trieste National Hall, a central building to the Slovene community. By the mid-1930s several thousand Slovenes, especially members of the middle class and the intelligentsia from Trieste, emigrated to the Kingdom of Yugoslavia or to South America. Among the notable Slovene émigrés from Trieste were the author Vladimir Bartol, the legal theorist Boris Furlan and the Argentine architect Viktor Sulčič. The political leadership of the around 70,000 émigrés from the Julian March in Yugoslavia was mostly composed by Trieste Slovenes: Lavo Čermelj, Josip Vilfan and Ivan Marija Čok. Despite the exodus of the Slovene and German speakers, the city's population increased because of the migration of Italians from other parts of Italy. Several thousand ethnic Italians from Dalmatia also moved to Trieste from the newly created Yugoslavia.
In the late 1920s, resistance began with the Slovene militant anti-fascist organization TIGR, which carried out several bomb attacks in the city centre. In 1930 and 1941, two trials of Slovene activists were held in Trieste by the fascist "Special Tribunal for the Security of the State". During the 1920s and 1930s, several monumental building were built in the Fascist architectural style, including the impressive University of Trieste and the almost tall Victory Lighthouse ("Faro della Vittoria"), which became a city landmark. The economy improved in the late 1930s, and several large infrastructure projects were carried out.
The Fascist government encouraged some of the artistic and intellectual subcultures that emerged in the 1920s and the city became home to an important avant-garde movement in visual arts, centered around the futurist Tullio Crali and the constructivist Avgust Černigoj. In the same period, Trieste consolidated its role as one of the centres of modern Italian literature, with authors such as Umberto Saba, Biagio Marin, Giani Stuparich, and Salvatore Satta. Intellectuals frequented the historic Caffè San Marco, still open today. Some non-Italian intellectuals remained in the city, such as the Austrian author Julius Kugy, the Slovene writer and poet Stanko Vuk, the lawyer and human rights activist Josip Ferfolja and the anti-fascist clergyman Jakob Ukmar.
The promulgation of the anti-Jewish racial laws in 1938 was a severe blow to the city's Jewish community, at the time the third largest in Italy. The fascist anti-semitic campaign resulted in a series of attacks on Jewish property and individuals, culminating in July 1942 when the Synagogue of Trieste was raided and devastated by the Fascist Squads and the mob.
World War II and aftermath.
With the annexation of Province of Ljubljana by Italy and the subsequent deportation of 25,000 Slovenes, which equaled 7.5% of the total population of the Province, the operation, one of the most drastic in Europe, filled up Rab concentration camp, Gonars concentration camp, Monigo (Treviso), Renicci d'Anghiari, Chiesanuova, and other Italian concentration camps where altogether 9,000 Slovenes died, World War II came close to Trieste. Following trisection of Slovenia, starting from the winter of 1941, the first Slovene Partisans appeared in Trieste province although the resistance movement did not become active in the city itself until late 1943.
After the Italian armistice in September 1943, the city was occupied by Wehrmacht troops. Trieste became nominally part of the newly constituted Italian Social Republic, but it was de facto ruled by Germany, who created the Operation Zone of the Adriatic Littoral out of former Italian north-eastern regions, with Trieste as the administrative centre. The new administrative entity was headed by Friedrich Rainer. Under German occupation, the only concentration camp with a crematorium on Italian soil was built in a suburb of Trieste, at the Risiera di San Sabba on 4 April 1944. About 3,000 Jews, South Slavs and Italian anti-Fascists died at the "Risiera", while thousands were imprisoned before being transferred to other concentration camps.
The city saw intense Italian and Yugoslav partisan activity and suffered from Allied bombings. The city's Jewish community was deported to extermination camps, where most of them died.
On 30 April 1945, the Italian anti-Fascist National Liberation Committee ("Comitato di Liberazione Nazionale", or CLN) of Marzari and Savio Fonda, made up of approximately 3,500 volunteers, incited a riot against the Nazi occupiers. On May 1, Allied members of the Yugoslav Partisans' 8th Dalmatian Corps took over most of the city, except for the courts and the castle of San Giusto, where the German garrisons refused to surrender to anyone other than New Zealanders.(The Yugoslavs had a reputation for shooting German and Italian prisoners.) The 2nd New Zealand Division continued to advance towards Trieste along Route 14 around the northern coast of the Adriatic sea and arrived in the city the following day (see official histories "The Italian Campaign" and "Through the Venetian Line"). The German forces surrendered on the evening of May 2, but were then turned over to the Yugoslav forces.
The Yugoslavs held full control of the city until June 12, a period known in the Italian historiography as the "forty days of Trieste".
During this period, hundreds of local Italians and anti-Communist Slovenes were arrested by the Yugoslav authorities, and many of them were never seen again.
These included not only former Fascist and German collaborators, but also Italian nationalists and any other real or potential opponents of Yugoslav Communism. Some were interned in Yugoslav concentration camps (in particular at Borovnica, Slovenia), while others were simply murdered and thrown into potholes ("foibe") on the Karst Plateau.
After an agreement between the Yugoslav leader Josip Broz Tito and the British Field Marshal Harold Alexander, the Yugoslav forces withdrew from Trieste, which came under a joint British-U.S. military administration. The Julian March was divided between Anglo-American and Yugoslav military administration until September 1947 when the Paris Peace Treaty established the Free Territory of Trieste.
Zone A of the Free Territory of Trieste (1947–54).
In 1947, Trieste was declared an independent city state under the protection of the United Nations as the Free Territory of Trieste. The territory was divided into two zones, A and B, along the Morgan Line established in 1945.
From 1947 to 1954, the A Zone was governed by the Allied Military Government, composed of the American "Trieste United States Troops" (TRUST), commanded by Major General Bryant E. Moore, the commanding general of the American 88th Infantry Division, and the "British Element Trieste Forces" (BETFOR), commanded by Sir Terence Airey, who were the joint forces commander and also the military governors. Zone A covered almost the same area of the current Italian Province of Trieste, except for four small villages south of Muggia, which were given to Yugoslavia after the dissolution (see London Memorandum of 1954) of the Free Territory in 1954. Zone B, which was under the administration of Miloš Stamatović, then colonel of the Yugoslav People's Army, was composed of the north-westernmost portion of the Istrian peninsula, between the river Mirna and the Debeli Rtič cape.
In 1954, in accordance with the Memorandum of London the vast majority of Zone A, including the city of Trieste, was given as civil administration to Italy. Zone B was given as a civil administration to Yugoslavia along with four villages from Zone A (Plavje, Spodnje Škofije, Hrvatini, and Jelarji), and was divided among the Socialist Republic of Slovenia and Croatia. The final border line with Yugoslavia, and the status of the ethnic minorities in the areas, was settled bilaterally in 1975 with the Treaty of Osimo. This line now constitutes the border between Italy and Slovenia.
Economy.
During the Austro-Hungarian era, Trieste became a leading European city in economy, trade and commerce, and was the fourth-largest and most important centre in the empire, after Vienna, Budapest and Prague. The economy of Trieste, however, fell into a decline after the city's annexation to Italy after World War I. But Fascist Italy promoted a huge development of Trieste in the 1930s, with new manufacturing activities related even to naval and armament industries (like the famous "Cantieri Aeronautici Navali Triestini (CANT)"). Allied bombings during World War II destroyed the industrial section of the city (mainly the shipyards). As a consequence, Trieste was a mainly peripheral city during the Cold War. However, since the 1970s, Trieste has experienced a certain economic revival.
Today, Trieste is a lively and cosmopolitan city, with about 8% of its population hailing from a cultural community, and is a major centre in the EU for trade, politics, culture, shipbuilding, education, transport and commerce. The city is part of the "Corridor 5" project to establish closer transport connections between Western and Eastern Europe, via countries such as Slovenia, Croatia, Hungary, Ukraine and Bosnia.
The Port of Trieste is a trade hub with a significant commercial shipping business, busy container and oil terminals, and steel works.
The oil terminal feeds the Transalpine Pipeline which covers 40% of Germany's energy requirements (100% of the states of Bavaria and Baden-Württemberg), 90% of Austria and more than 30% of the Czech Republic's. The sea highway connecting the ports of Trieste and Istanbul is one of the busiest RO/RO on roll-off routes in the Mediterranean.The port is also Italy's and the Mediterranean's (and one of Europe's) greatest coffee ports, supplying more than 40% of Italy's coffee. The thriving coffee industry in Trieste began under Austria-Hungary, with the Austro-Hungarian government even awarding tax-free status to the city in order to encourage more commerce. Some remnants of Austria-Hungary's coffee-driven economic ambition remain, such as the Hausbrandt Trieste coffee company. As a result, present-day Trieste boasts many cafes, and is still known to this day as "the coffee capital of Italy". Companies active in the coffee sector have given birth to the Trieste Coffee Cluster as their main umbrella organization, but also as an economic actor in its own right.
Two Fortune Global 500 companies have their global or national headquarters in the city, respectively: Assicurazioni Generali and Allianz. Other megacompanies based in Trieste are Fincantieri, one of the world's leading shipbuilding companies and the Italian operations of Wärtsilä. Prominent companies from Trieste include: AcegasApsAmga (Hera Group), Autamarocchi SpA, Banca Generali SpA, Genertel, HERA Trading, Illy, Italia Marittima, Modiano Playing Cards, Nuovo Arsenale Cartubi Srl, Jindal Steel and Power Italia SpA; Pacorini SpA, Siderurgica Triestina (Arvedi Group), TBS Group, Telit (AIM: TCM), and polling and marketing company SWG.
With two main banking institutions, the Zadružna Kraška Banka (ZKB), and a branch of the Nova Ljubljanska Banka the local Slovene community contributes vigorously to the economy.
Demographics.
, there were 204,849 people residing in Trieste, located in the province of Trieste, Friuli-Venezia Giulia, of whom 46.7% were male and 53.3% were female. Trieste had lost roughly ⅓ of its population since the 1970s, due to the crisis of the historical industrial sectors of steel and shipbuilding, a dramatic drop in fertility rates and fast population aging. Minors (children aged 18 and younger) totalled 13.78% of the population compared to pensioners who number 27.9%. This compares with the Italian average of 18.06% (minors) and 19.94% (pensioners). The average age of Trieste residents is 46 compared to the Italian average of 42. In the five years between 2002 and 2007, the population of Trieste declined by 3.5%, while Italy as a whole grew by 3.85%. However, in the last two years the city has shown signs of stabilizing thanks to growing immigration fluxes. The crude birth rate in Trieste is only 7.63 per 1,000, one of the lowest in eastern Italy, while the Italian average is 9.45 births. 
Since the annexation to Italy after World War I, there has been a steady decline in the Trieste's demographic weight compared to other cities. In 1911, Trieste was the 4th largest city in the Austro-Hungarian Empire (3rd largest in the Austrian part of the Monarchy). In 1921, Trieste was the 8th largest city in the country, in 1961 the 12th largest, in 1981 the 14th largest, while in 2011 it dropped to the 15th place.
Language.
The particular Friulian dialect, called "Tergestino", spoken until the beginning of the 19th century, was gradually overcome by the Triestine dialect of Venetian (a language deriving directly from Vulgar Latin) and other languages, including standard Italian, Slovene, and German. While Triestine and Italian were spoken by the largest part of the population, German was the language of the Austrian bureaucracy and Slovene was predominantly spoken in the surrounding villages. From the last decades of the 19th century, the number of speakers of Slovene grew steadily, reaching 25% of the overall population of Trieste municipality in 1911 (30% of the Austro-Hungarian citizens in Trieste).
According to the 1911 census, the proportion of Slovene speakers amounted to 12.6% in the city centre (15.9% counting only Austrian citizens), 47.6% in the suburbs (53% counting only Austrian citizens), and 90.5% in the surroundings. They were the largest ethnic group in 9 of the 19 urban neighbourhoods of Trieste, and represented a majority in 7 of them. The Italian speakers, on the other hand, made up 60.1% of the population in the city center, 38.1% in the suburbs, and 6.0% in the surroundings. They were the largest linguistic group in 10 of the 19 urban neighbourhoods, and represented the majority in 7 of them (including all 6 in the city centre). Of the 11 villages included within the city limits, the Slovene speakers had an overwhelming majority in 10, and the German speakers in one (Miramare).
German speakers amounted to 5% of the city's population, with the highest proportions in the city centre. A small proportion of Trieste's population spoke Croatian (about 1.3% in 1911), and the city also had several other smaller ethnic communities, including Czechs, Istro-Romanians, Serbs, and Greeks, who mostly assimilated either into the Italian or the Slovene-speaking communities.
Today, the dominant local dialect of Trieste is "Triestine" ("Triestin", pronounced ), influenced by a form of Venetian. This dialect and the official Italian language are spoken in the city, while Slovene is spoken in some of the immediate suburbs. There are also small numbers of Serbian, Croatian, German, and Hungarian speakers. 
An estimated 19% of the province's population (49,000 out of 260,000 from the last census) belong to the autochthonous Slovene language community. In total, the city's ethnic Slavic minority makes up about 30 percent of the population.
At the end of 2012, ISTAT estimated that there were 16,279 foreign-born residents in Trieste, representing 7.7% of the total city population. The largest autochthonous minority are Slovenes, but there is also a large immigrant group from Balkan nations (particularly nearby Serbia, Albania and Romania): 4.95%, Asia: 0.52%, and sub-saharan Africa: 0.2%. Serbian community consists of both autochthonous and immigrant groups. Trieste is predominantly Roman Catholic, but also has large numbers of Orthodox Christians, mainly Serbs, due to the city's large migrant population from Eastern Europe and its Balkan influence. 
Main sights.
Castles.
"Castello Miramare" (Miramare Castle).
The "Castello Miramare", or Miramare Castle, on the waterfront from Trieste, was built between 1856 and 1860 from a project by Carl Junker working under Archduke Maximilian. The Castle gardens provide a setting of beauty with a variety of trees, chosen by and planted on the orders of Maximilian, that today make a remarkable collection. Features of particular attraction in the gardens include two ponds, one noted for its swans and the other for lotus flowers, the Castle annexe ("Castelletto"), a bronze statue of Maximilian, and a small chapel where is kept a cross made from the remains of the "Novara", the flagship on which Maximilian, brother of Emperor Franz Josef, set sail to become Emperor of Mexico. Much later, the castle was also the home of Prince Amedeo, Duke of Aosta, the last commander of Italian forces in East Africa during the Second World War. During the period of the application of the Instrument for the Provisional Regime of the Free Territory of Trieste, as establish in the Treaty of Peace with Italy (Paris 10/02/1947), the castle served as headquarters for the United States Army's TRUST force.
"Castel San Giusto" (Castle of San Giusto).
The "Castel San Giusto", or Castle of San Giusto, was designed on the remains of previous castles on the site, and took almost two centuries to build. The stages of the development of the Castle's defensive structures are marked by the central part built under Frederick III (1470-1), the round Venetian bastion (1508-9), the Hoyos-Lalio bastion and the Pomis, or "Bastione fiorito" dated 1630. 
Archaeological remains.
The ruins of the temple dedicated to Zeus are next to the Forum, those of Athena's temple are under the basilica, visitors can see its basement.
Roman theatre.
The Roman theatre lies at the foot of the San Giusto hill, facing the sea. The construction partially exploits the gentle slope of the hill, and much of the theatre is made of stone. The topmost portion of the steps and the stage were supposedly made of wood.
The statues that adorned the theatre, brought to light in the 1930s, are now preserved at the Town Museum. Three inscriptions from the Trajanic period mention a certain Q. Petronius Modestus, someone closely connected to the development of the theatre, which was erected during the second half of the 1st century.
Caves.
In the entire Province of Trieste, there are 10 speleological groups out of 24 in the whole "Friuli-Venezia Giulia" region. The Trieste plateau (Altopiano Triestino), called Kras or the "Carso" and covering an area of about within Italy has approximately 1,500 caves of various sizes (like that of Basovizza, now a monument to the Foibe massacres).
Among the most famous are the Grotta Gigante, the largest tourist cave in the world, with a single cavity large enough to contain St Peter's in Rome, and the "Cave of Trebiciano", deep, at the bottom of which flows the "Timavo River". This river dives underground at Škocjan Caves in Slovenia (they are on UNESCO list and only a few kilometres from Trieste) and flows about before emerging about from the sea in a series of springs near Duino, reputed by the Romans to be an entrance to Hades ("the world of the dead").
Culture.
Trieste has a lively cultural scene with various theatres. Among these are the Opera Teatro Lirico Giuseppe Verdi, Politeama Rossetti, the Teatro La Contrada, the Slovene theatre in Trieste (, since 1902), Teatro Miela, and a several smaller ones.
There are also numerous museums. Among these are:
Two important national monuments:
The "Slovenska gospodarsko-kulturna zveza" - "Unione Economica-Culturale Slovena" is the umbrella organization bringing together cultural and economic associations belonging to the Slovene minority.
Education.
The University of Trieste, founded in 1924, is a medium-size state-supported institution with 12 faculties, and boasts a wide and almost complete range of courses. It currently has about 23,000 students enrolled and 1,000 professors.
Trieste also hosts the Scuola Internazionale Superiore di Studi Avanzati (SISSA), a leading graduate and postgraduate teaching and research institution in the study of mathematics, theoretical physics, and neuroscience, and the MIB School of Management Trieste, a private, ASFOR accredited business school.
There are three international schools offering primary and secondary education programs in English in the greater metropolitan area: the International School of Trieste, the European School of Trieste, and the United World College of the Adriatic. Liceo scientifico statale "France Prešeren" offers public secondary education in the Slovene language.[http://www.preseren.it]
The city hosts numerous national and international scientific research institutions, among which: AREA Science Park, which comprises ELETTRA, a synchrotron particle accelerator with free-electron laser capabilities for research and industrial applications; the International Centre for Theoretical Physics, which operates under a tripartite agreement among the Italian Government, UNESCO, and International Atomic Energy Agency (IAEA); the Trieste Astronomical Observatory; the Istituto Nazionale di Oceanografia e Geofisica Sperimentale (OGS), which carries out research on oceans and geophysics; the International Centre for Genetic Engineering and Biotechnology, a United Nations centre of excellence for research and training in genetic engineering and biotechnology for the benefit of developing countries; ICS-UNIDO, a UNIDO research centre in the areas of renewable energies, biofuels, medicinal plants, food safety and sustainable development; the Carso Center for Advanced Research in Space Optics; and the secretariats of the Third World Academy of Sciences (TWAS) and of the InterAcademy Panel: The Global Network of Science Academies (IAP).
Sports.
The local "calcio" (football) club in Trieste is "Triestina", one of the oldest clubs in Italy. Notably, Triestina was runner-up in the 1947/1948 season of the Italian first division (Serie A), losing the championship to Torino.
Trieste is notable for having had two football clubs participating in the championships of two different nations at the same time during the period of the Free Territory of Trieste, due to the schism within the city and region created by the post-war demarcation. Triestina played in the Italian first division (Serie A). Although it faced relegation after the first season after the Second World War, the FIGC changed the rules to keep it in, as it was seen as important to keep a club of the city in the Italian league, while Yugoslavia had its eye on the city. In the championship of next season the club played its best season with a 3rd-place finish. Meanwhile, Yugoslavia bought A.S.D. Ponziana, a small team in Trieste, which under a new name, "Amatori Ponziana Trst", played in the Yugoslavian league for 3 years. Triestina went bankrupt in the 1990s, but after being re-founded regained a position in the Italian second division (Serie B) in 2002. Ponziana was renamed as "Circolo Sportivo Ponziana 1912" and currently plays in Friuli-Venezia Giulia Group of Promozione, which is 7th level of the Italian league.
Trieste also boasts a famous basketball team, Pallacanestro Trieste, which reached its zenith in the 1990s when, with large financial backing from sponsors Stefanel, it was able to sign players such as Dejan Bodiroga, Fernando Gentile and Gregor Fučka, all stars of European basketball.
Many sailing clubs have roots in the city which contribute to Trieste's strong tradition in that sport. The Barcolana regatta, which had its first edition in 1969, is the world's largest sailing race by number of participants.
Local sporting facilities include the Stadio Nereo Rocco, a UEFA-certified stadium with seating capacity of 32,500; the Palatrieste, an indoor sporting arena sitting 8,000 people, and Piscina Bruno Bianchi, a large olympic size swimming pool.
Film.
Trieste has been portrayed on screen a number of times, with films often shot on location in the area. In 1942 the early neorealist "Alfa Tau!" was filmed partly in the city.
Cinematic interest in Trieste peaked during the height of the "Free Territory" era between 1947 and 1954 with international films such as "Sleeping Car to Trieste" and "Diplomatic Courier" portraying it as a hotbed of espionage. These conveyed an impression of the city as a cosmopolitan place of conflict between Great Powers, a portrayal which resembled that of "Casablanca" (1943). Italian filmmakers, by contrast, portrayed Trieste as unquestionably Italian in a series of patriotic films including "Trieste mia!" and "Ombre su Trieste".
The city hosted in 1963 the first International Festival of Science Fiction Film (Festival internazionale del film di fantascienza), which ran until 1982. Under the name Science Plus Fiction, the festival was brought back in 2000.
Transport.
Maritime transport.
Trieste's maritime location and its former long term status as part of the Austrian and Austro-Hungarian empires made the Port of Trieste the major commercial port for much of the landlocked areas of central Europe. In the 19th century, a new port district known as the "Porto Nuovo" was built northeast to the city centre.
There is significant commercial shipping to the container terminal, steel works and oil terminal, all located to the south of the city centre. After many years of stagnation, a change in the leadership placed the port on a steady growth path, recording a 40% increase in shipping traffic .
Rail transport.
Railways came early to Trieste, due to the importance of its port and the need to transport people and goods inland. The first railroad line to reach Trieste was the "Südbahn", launched by the Austrian government in 1857. This railway stretches for to Lviv, Ukraine, via Ljubljana, Slovenia; Sopron, Hungary; Vienna, Austria; and Kraków, Poland, crossing the backbone of the Alps mountains through the Semmering Pass near Graz. It approaches Trieste through the village of Villa Opicina, a few kilometres from the big city but over higher in elevation. Due to this, the line takes a detour to the north, gradually descending before terminating at the Trieste Centrale railway station.
A second trans-Alpine railway was dedicated in 1906, with the opening of the Transalpina Railway from Vienna, Austria via Jesenice and Nova Gorica. This railway also approached Trieste via Villa Opicina, but it took a rather shorter loop southwards towards Trieste's other main railway station, the , south of the central station. This line no longer operates, and the Campo Marzio station is now a railway museum.
To facilitate freight traffic between the two stations and the nearby dock areas, a temporary railway line known as the "Rivabahn" was built along the waterfront in 1887. This railway survived until 1981, when it was replaced by the "Galleria di Circonvallazione", a railway tunnel route, to the east of the city. Freight services from the dock area include container services to northern Italy and to Budapest, Hungary, together with rolling highway services to Salzburg, Austria and Frankfurt, Germany.
Passenger rail service to Trieste mostly consists of trains to and from Venice, connecting there with high-speed trains to Rome and Milan at Mestre. There are also direct trains to Verona, Turin, Milan, Rome, Florence, Naples and Bologna. These trains reach the Trieste central station bypassing the Gulf of Trieste, connecting with the Südbahn's northern loop. , there are no passenger trains between Italy and Slovenia.
Trieste could in the remote future be connected to the Italian TAV railway network: a fast train route would possibly connect Trieste with Venice. However, this project will not be completed earlier than 2020.
Air transport.
Trieste is served by the Friuli Venezia Giulia Airport (IATA code: TRS), located 30 minutes away from the city, at Ronchi near Monfalcone at the head of the Gulf of Trieste. There are many national and international destinations available.
Local transport.
Local public transport is operated by Trieste Trasporti, which operates a network of around 60 bus routes and two boat services. They also operate the Opicina Tramway, a hybrid between tramway and funicular railway providing a more direct link between the city centre and Opicina.
International relations.
Trieste hosts the Secretariat of the Central European Initiative, an intergovernmental organization among Central and South-Eastern European states.
In recent years, Trieste was chosen to host a number of high level bilateral and multilateral meetings such as: the Italo-Russian Bilateral Summit in 2013 (Letta-Putin) and the Italo-German Bilateral Summit in 2008 (Berlusconi-Merkel); the G8 meetings of Foreign Affairs and Environment Ministers respectively in 2009 and 2001.
Sister cities / Twin towns.
Trieste is twinned with:

</doc>
<doc id="56093" url="https://en.wikipedia.org/wiki?curid=56093" title="Kerchief">
Kerchief

A kerchief (from the French "couvre-chef", "cover the head") also known as a bandana or bandanna, is a triangular or square piece of cloth tied around the head or around the neck for protective or decorative purposes. The popularity of head kerchiefs may vary by culture or religion, as among Orthodox Christian women, Amish women, Orthodox Jewish women and Muslim women.
Handkerchief.
A "handkerchief" or "hanky" primarily refers to a napkin made of cloth, used to dab away perspiration, clear the nostrils, or, in Victorian times, as a means of flirtation. A woman could intentionally drop a dainty square of lacy or embroidered fabric to give a favored man a chance to pick it up as an excuse to speak to her while returning it. Handkerchiefs were sometimes scented to be used like a nosegay or tussy-mussy, a way of protecting those who could afford them from the obnoxious scents in the street.
Society.
Subculture.
The popularity of the bandana and kerchief was at it highest point in the 1970s, 1980s and 1990s depending on one's location. After that its popularity started waning in the west, but some eastern cultures maintained its usage for a while, such as in the Persian Gulf countries. It is largely seen as gender neutral and can be worn by both men and women. Its usage when wrapped up was partially replaced by the headband.
Bandana.
A bandana is a type of large, usually colorful, kerchief, usually worn on the head or around the neck of a person or pet and is not considered to be a hat. Bandanas are frequently printed in a paisley pattern and are most often used to hold hair back, either as a fashionable head accessory, or for practical purposes.
Urban.
Colors, and sometimes designs, can be worn as a means of communication or identification, as with the prominent California criminal gangs, the Bloods, the Crips, the Norteños, and the Sureños. 
In gang subcultures, the bandana could be worn in a pocket or, in some cases, around the leg. In the late 1960s/early 1970s, the Bloods and the Crips wore red or blue paisley bandanas as a signifier of gang affiliation.

</doc>
<doc id="56096" url="https://en.wikipedia.org/wiki?curid=56096" title="Pioneer movement">
Pioneer movement

A pioneer movement is an organization for children operated by a communist party. Typically children enter into the organization in elementary school and continue until adolescence. The adolescents then typically join the Young Communist League. Prior to the 1990s there was a wide cooperation between pioneer and similar movements of about 30 countries, coordinated by the international organization, "International Committee of Children's and Adolescents' Movements" (, CIMEA), founded in 1958, with headquarters in Budapest.
Overview.
Pioneer movements exist in countries where the Communist Party is in power as well as in some countries where the Communist Party is in opposition, if the party is large enough to support a children's organization. In countries ruled by Communist Parties, membership of the pioneer movement is officially optional. However, membership provides many benefits, so the vast majority of children typically join the movement (although at different ages). During the existence of the Soviet Union, thousands of Young Pioneer camps and Young Pioneer Palaces were built exclusively for Young Pioneers, which were free of charge, sponsored by the government and trade unions. There were many newspapers and magazines published for Young Pioneers in millions of copies.
The Pioneer movement was modeled on the Scout movement, but there are some distinct differences. Most notably, the Scout movement is independent of government control and political parties. Some features, however, are reminiscent of the Scout movement. The two movements share some principles like preparedness and promotion of sports and outdoor skills. The pioneer movement also includes teaching of communist principles. Opponents of Communist states claim that this is a form of indoctrination.
A member of the movement is known as a pioneer, and a kerchief or necktie — typically red, but sometimes light blue — is the traditional item of clothing worn by a pioneer. The pioneer organization is often named after a famous party member that is considered a suitable role model for young communists. In the Soviet Union it was Vladimir Lenin; in East Germany, it was Ernst Thälmann. The Thälmann pioneers were taught the slogan "Ernst Thälmann is my role model. We wear our red scarf with pride." Albania, which severed diplomatic relations with the USSR in 1961, also had a variant of Pioneer organization, called Pioneers of Enver, named after the communist ruler of Albania, Enver Hoxha.
Countries with Pioneer movements.
The Pioneer Movement now exists in these countries:
Older children could continue in other communist organizations, but that would typically be done only by a limited number of people.
The communist parties in Russia and other countries continue to run a pioneer organization, but membership tends to be quite limited.

</doc>
<doc id="56097" url="https://en.wikipedia.org/wiki?curid=56097" title="History of Styria">
History of Styria

The history of Styria concerns the region roughly corresponding to the modern Austrian state of Styria and the Slovene region of Styria ("Štajerska") from its settlement by Germans and Slavs in the Dark Ages until the present. This mountainous and scenic region, which became a centre for mountaineering in the 19th century, is often called the "Green March", because half of the area is covered with forests and one quarter with meadows, grasslands, vineyards and orchards. Styria is also rich in minerals, soft coal and iron, which has been mined at Erzberg since the time of the Romans. The Slovenske gorice/Windisch Büheln ("Slovene Hills") is a famous wine-producing district, stretching between Slovenia and Austria. Styria was for long the most densely populated and productive mountain region in Europe.
Styria's population before World War I was 68% German-speaking, 32% Slovene, bordered on (clockwise) Lower Austria, Hungary, Croatia, Carniola, Carinthia, Salzburg, and Upper Austria. In 1918 after World War I the southern, Slovene-speaking third south of the river Mur was incorporated into Slovenia in the Kingdom of Serbs, Croats and Slovenes. The remaining two-thirds became the Austrian federal state of Styria, while the Slovene-speaking third (Lower Styria) formed the informal region of Štajerska in Slovenia, now divided up into the statistical EU regions of Podravska, Savinjska and the major part of Slovenian Carinthia. The capital both of the duchy and the Austrian state has always been Graz, which is now also the residence of the governor and the seat of the administration of the land.
Political history.
Prehistory to Charlemagne.
The Roman history of Styria is as part of Noricum and Pannonia, with the romanized Celtic population of the Taurisci. During the great migrations, various Germanic tribes settled and/or traversed the region using the river valleys and low passes, but about 600 CE the Slavs took possession of the area and settled assimilating the remaining autochthonous romanized population.
When Styria came under the hegemony of Charlemagne as a part of Carantania (Carinthia), erected as a border territory against the Avars and Slavs, there was a large influx of Bavarii and other Christianized Germanic peoples, whom the bishops of Salzburg and the patriarchs of Aquileia kept faithful to Rome. Bishop Vergilius of Salzburg (745-84), was largely instrumental in establishing a church hierarchy in the Duchy and gained for himself the name of "Apostle of Carantania." In 811 Charlemagne made the Drave river the boundary between the Dioceses of Salzburg and Aquileia.
Middle Ages.
The March of Styria was created in the Duchy of Carinthia in the late 10th century as a defence against the Magyars. Long called the Carantanian or Carinthian March it was soon ruled by a margravial dynasty called the Otakars that originated from Steyr in Upper Austria thus giving the land its name: "Steiermark". This march was raised to become a duchy by the Emperor Frederick Barbarossa in 1180 after the fall of Henry the Lion of Bavaria.
With the death of Ottokar the first line of rulers of Styria became extinct; the region fell successively to the Babenberg family, rulers of Austria, as stipulated in the Georgenberg Pact; after their extinction to the control of Hungary (1254–60); to King Ottokar of Bohemia; in 1276 to the Habsburgs, who provided it with Habsburgs for Styrian dukes during the years 1379-1439 and 1564-1619.
At the time of the Ottoman invasions in the 16th and 17th centuries the land suffered severely and was depopulated. The Turks made incursions into Styria nearly twenty times; churches, monasteries, cities, and villages were destroyed and plundered, while the population was either killed or carried away into slavery.
Modern era.
The Semmering Railway, completed in 1854, was a triumph of engineering in its time, the oldest of the great European mountain railways. It was remarkable for its numerous and long tunnels and viaducts spanning mountain valleys, running from Gloggnitz in Lower Austria to Mürzzuschlag in Styria, and passing through some exceedingly beautiful scenery. The railway brought tourists to alpine lake resorts and mineral springs at Rohitsch (today's Rogaška Slatina) and Bad Gleichenberg, the brine springs of Bad Aussee, and the thermal springs of Tuffer (today's Laško), Neuhaus am Klausenbach and Tobelbad.
Following World War I, Styria was divided by the Treaty of Saint Germain. Lower Styria with the cities of Celje and Maribor became part of the Kingdom of Serbs, Croats and Slovenes, while the rest remained with Austria as the State of Styria. Other than in Carinthia, no fighting resulted from this, in spite of a German minority in Slovenia (the larger cities of Lower Styria were largely German-speaking).
Lower Styria was reattached to the Reichsgau Steiermark from 1942 to 1945, whence it was returned to Germany. After World War II, Styria became part of the British occupation zone Austria. The lower third was granted to Yugoslavia and today, it makes up about the eastern third of Slovenia.
Religious history.
The Protestant Reformation made its way into the country about 1530. Duke Karl (ruling 1564-90), whose wife was the Catholic Duchess Maria of Bavaria, introduced the Counter-Reformation into the country; in 1573 he invited the Jesuits into Styria and in 1586 he founded the Catholic University of Graz. In 1598 his son and successor Ferdinand suppressed all Protestant schools and expelled the teachers and preachers: Protestant doctrines were maintained only in a few isolated mountain valleys, as in the valley of the Inn and the valley of the Mur. On a narrow reading of the Peace of Augsburg, 1555, with its principle of "cuius regio, eius religio", only the nobility were not forced to return to the Roman Church; each could have Protestant services privately in his own house.
After Ferdinand had become Holy Roman Emperor in 1619 and had defeated his Protestant opponents in the Battle of White Mountain near Prague in 1620, he forbade all Protestant church services whatsoever (1625). In 1628 he commanded the nobility also to return to the Catholic faith. A large number of noble families, consequently, emigrated from the country. But most of them either returned, or their descendants did so, becoming Catholics and recovering their possessions.
In the second half of the 17th century renewed action against the Protestants in the isolated mountain valleys resulted in the expulsion of Protestant ministers with the peasants who would not give up Protestantism; about 30,000 chose compulsory emigration to Transylvania over conversion. Only an Edict of Toleration issued by Emperor Joseph II as late as 1781 put an end to religious repression. The Protestants then received the right to found parish communities and to exercise their religion in those enclaves undisturbed.
In 1848, all the provinces of the Habsburg Monarchy received complete liberty of religion and of conscience, parity of religions, and the right to the public exercise of religion.
Ecclesiastically the province was historically divided into two Catholic prince-bishoprics, Seckau and Lavant. From the time of their foundation both were suffragans of the Archdiocese of Salzburg. The Prince-Bishopric of Seckau was established in 1218; since 1786 the see of the prince-bishop has been Graz. The Prince-Bishopric of Lavant with its bishop's seat at Sankt Andrä in the Carinthian Lavant valley was founded as a bishopric in 1228 and raised to a prince-bishopric in 1446. In 1847 the bishop's seat was transferred from St. Andrä to "Marburg an der Drau" (Maribor), and after World War I the see's boundaries were adapted to the new political frontiers. A short-lived third Salzburg suffragan diocese of Leoben comprising 157 parishes in the districts of Leoben and Bruck an der Mur existed on Styrian soil from 1786 but was incorporated into the diocese of Graz-Seckau in 1856 Today the see of the bishop of Graz-Seckau is identical in territory with the Austrian State of Styria.

</doc>
<doc id="56098" url="https://en.wikipedia.org/wiki?curid=56098" title="Monte Carlo method">
Monte Carlo method

Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution.
In physics-related problems, Monte Carlo methods are quite useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods.
In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is parameterized, mathematicians often use a Markov Chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler.
In other important problems we are interested in generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depends on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation). In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain. A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and Markov chain Monte Carlo methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the "samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes)" interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.
Introduction.
Monte Carlo methods vary, but tend to follow a particular pattern:
For example, consider a circle inscribed in a unit square. Given that the circle and the square have a ratio of areas that is /4, the value of pi can be approximated using a Monte Carlo method:
In this procedure the domain of inputs is the square that circumscribes our circle. We generate random inputs by scattering grains over the square then perform a computation on each input (test whether it falls within the circle). Finally, we aggregate the results to obtain our final result, the approximation of .
There are two important points to consider here: Firstly, if the grains are not uniformly distributed, then our approximation will be poor. Secondly, there should be a large number of inputs. The approximation is generally poor if only a few grains are randomly dropped into the whole square. On average, the approximation improves as more grains are dropped.
Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling.
History.
Before the Monte Carlo method was developed, simulations tested a previously understood deterministic problem and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a probabilistic analog (see Simulated annealing).
An early variant of the Monte Carlo method can be seen in the Buffon's needle experiment, in which can be estimated by dropping needles on a floor made of parallel and equidistant strips. In the 1930s, Enrico Fermi first experimented with the Monte Carlo method while studying neutron diffusion, but did not publish anything on it.
The modern version of the Markov Chain Monte Carlo method was invented in the late 1940s by Stanislaw Ulam, while he was working on nuclear weapons projects at the Los Alamos National Laboratory. Immediately after Ulam's breakthrough, John von Neumann understood its importance and programmed the ENIAC computer to carry out Monte Carlo calculations. In 1946, physicists at Los Alamos Scientific Laboratory were investigating radiation shielding and the distance that neutrons would likely travel through various materials. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus, and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. Stanislaw Ulam had the idea of using random experiments. He recounts his inspiration as follows:
Being secret, the work of von Neumann and Ulam required a code name. A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name "Monte Carlo", which refers to the Monte Carlo Casino in Monaco where Ulam's uncle would borrow money from relatives to gamble. Using lists of "truly random" random numbers was extremely slow, but von Neumann developed a way to calculate pseudorandom numbers, using the middle-square method. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, and also noted that when it went awry it did so obviously, unlike methods that could be subtly incorrect.
Monte Carlo methods were central to the simulations required for the Manhattan Project, though severely limited by the computational tools at the time. In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb, and became popularized in the fields of physics, physical chemistry, and operations research. The Rand Corporation and the U.S. Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.
The theory of more sophisticated mean field type particle Monte Carlo methods had certainly started by the mid-1960s, with the work of Henry P. McKean Jr. on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics. We also quote an earlier pioneering article by Theodore E. Harris and Herman Kahn, published in 1951, using mean field genetic-type Monte Carlo methods for estimating particle transmission energies. Mean field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms (a.k.a. Metaheuristic) in evolutionary computing. The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of Alan Turing on genetic type mutation-selection learning machines and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton, New Jersey.
Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods can also be interpreted as a mean field particle Monte Carlo approximation of Feynman-Kac path integrals. The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions, but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984 In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth.
The use of Sequential Monte Carlo in advanced Signal processing and Bayesian inference is more recent. It was in 1993, that Gordon et al., published in their seminal work the first application of a Monte Carlo resampling algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related "Monte Carlo filter", and the ones by Pierre Del Moral and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the LAAS-CNRS (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems. These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism.
From 1950 to 1996, all the publications on Sequential Monte Carlo methodologies including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral in 1996. Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons, and by Dan Crisan, Pierre Del Moral and Terry Lyons. Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo.
Definitions.
There is no consensus on how "Monte Carlo" should be defined. For example, Ripley defines most probabilistic modeling as "stochastic simulation", with "Monte Carlo" being reserved for Monte Carlo integration and Monte Carlo statistical tests. Sawilowsky distinguishes between a simulation, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to determine the properties of some phenomenon (or behavior). Examples:
Kalos and Whitlock point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. "Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling."
Monte Carlo and random numbers.
Monte Carlo simulation methods do not always require truly random numbers to be useful — while for some applications, such as primality testing, unpredictability is vital. Many of the most useful techniques use deterministic, pseudorandom sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good simulations is for the pseudo-random sequence to appear "random enough" in a certain sense.
What this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are uniformly distributed or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest, and most common ones. Weak correlations between successive samples is also often desirable/necessary.
Sawilowsky lists the characteristics of a high quality Monte Carlo simulation:
Pseudo-random number sampling algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given probability distribution.
Low-discrepancy sequences are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called quasi-Monte Carlo methods.
Monte Carlo simulation versus "what if" scenarios.
There are ways of using probabilities that are definitely not Monte Carlo simulations — for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a “best guess” estimate. Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.
By contrast, Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring. For example, a comparison of a spreadsheet cost construction model run using traditional “what if” scenarios, and then running the comparison again with Monte Carlo simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the “what if” analysis. This is because the “what if” analysis gives equal weight to all scenarios (see quantifying uncertainty in corporate finance), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called "rare events".
Applications.
Monte Carlo methods are especially useful for simulating phenomena with significant uncertainty in inputs and systems with a large number of coupled degrees of freedom. Areas of application include:
Physical sciences.
Monte Carlo methods are very important in computational physics, physical chemistry, and related applied fields, and have diverse applications from complicated quantum chromodynamics calculations to designing heat shields and aerodynamic forms as well as in modeling radiation transport for radiation dosimetry calculations. In statistical physics Monte Carlo molecular modeling is an alternative to computational molecular dynamics, and Monte Carlo methods are used to compute statistical field theories of simple particle and polymer systems. Quantum Monte Carlo methods solve the many-body problem for quantum systems. In experimental particle physics, Monte Carlo methods are used for designing detectors, understanding their behavior and comparing experimental data to theory. In astrophysics, they are used in such diverse manners as to model both galaxy evolution and microwave radiation transmission through a rough planetary surface. Monte Carlo methods are also used in the ensemble models that form the basis of modern weather forecasting.
Engineering.
Monte Carlo methods are widely used in engineering for sensitivity analysis and quantitative probabilistic analysis in process design. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,
Climate change and radiative forcing.
The Intergovernmental Panel on Climate Change relies on Monte Carlo methods in probability density function analysis of radiative forcing.
Computational biology.
Monte Carlo methods are used in various fields of computational biology, for example for Bayesian inference in phylogeny, or for studying biological systems such as genomes, proteins, or membranes.
The systems can be studied in the coarse-grained or "ab initio" frameworks depending on the desired accuracy. 
Computer simulations allow us to monitor the local environment of a particular molecule to see if some chemical
reaction is happening for instance. In cases where it is not feasible to conduct a physical experiment, thought experiments can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).
Computer graphics.
Path tracing, occasionally referred to as Monte Carlo ray tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the rendering equation, making it one of the most physically accurate 3D graphics rendering methods in existence.
Applied statistics.
In applied statistics, Monte Carlo methods are generally used for two purposes:
Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice – or more frequently—for the efficiency of not having to track which permutations have already been selected).
Artificial intelligence for games.
Monte Carlo methods have been developed into a technique called Monte-Carlo tree search that is useful for searching for the best move in a game. Possible moves are organized in a search tree and a large number of random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.
The Monte Carlo tree search (MCTS) method has four steps:
The net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.
Monte Carlo Tree Search has been used successfully to play games such as Go, Tantrix, Battleship, Havannah, and Arimaa.
Design and visuals.
Monte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in global illumination computations that produce photo-realistic images of virtual 3D models, with applications in video games, architecture, design, computer generated films, and cinematic special effects.
Search and rescue.
The US Coast Guard utilizes Monte Carlo methods within its computer modeling software SAROPS in order to calculate the probable locations of vessels during search and rescue operations. Each simulation can generate as many as ten thousand data points which are randomly distributed based upon provided variables. Search patterns are then generated based upon extrapolations of these data in order to optimize the probability of containment (POC) and the probability of detection (POD), which together will equal an overall probability of success (POS). Ultimately this serves as a practical application of probability distribution in order to provide the swiftest and most expedient method of rescue, saving both lives and resources.
Finance and business.
Monte Carlo methods in finance are often used to evaluate investments in projects at a business unit or corporate level, or to evaluate financial derivatives. They can be used to model project schedules, where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project. Monte Carlo methods are also used in option pricing, default risk analysis.
Use in mathematics.
In general, Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also Random number generation) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically. The most common application of the Monte Carlo method is Monte Carlo integration.
Integration.
Deterministic numerical integration algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then 10100 points are needed for 100 dimensions—far too many to be computed. This is called the curse of dimensionality. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to an iterated integral. 100 dimensions is by no means unusual, since in many physical problems, a "dimension" is equivalent to a degree of freedom.
Monte Carlo methods provide a way out of this exponential increase in computation time. As long as the function in question is reasonably well-behaved, it can be estimated by randomly selecting points in 100-dimensional space, and taking some kind of average of the function values at these points. By the central limit theorem, this method displays formula_1 convergence—i.e., quadrupling the number of sampled points halves the error, regardless of the number of dimensions.
A refinement of this method, known as importance sampling in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling, recursive stratified sampling, adaptive umbrella sampling or the VEGAS algorithm.
A similar approach, the quasi-Monte Carlo method, uses low-discrepancy sequences. These sequences "fill" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.
Another class of methods for sampling points in a volume is to simulate random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis-Hastings algorithm, Gibbs sampling, Wang and Landau algorithm, and interacting type MCMC methodologies such as the sequential Monte Carlo samplers.
Simulation and optimization.
Another powerful and very popular application for random numbers in numerical simulation is in numerical optimization. The problem is to minimize (or maximize) functions of some vector that often has a large number of dimensions. Many problems can be phrased in this way: for example, a computer chess program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the traveling salesman problem the goal is to minimize distance traveled. There are also applications to engineering design, such as multidisciplinary design optimization. It has been applied with quasi-one-dimensional models to solve particle dynamics problems by efficiently exploring large configuration space.
The traveling salesman problem is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.
Inverse problems.
Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines prior information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).
When analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the "a priori" distribution is available.
The best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex "a priori" information and data with an arbitrary noise distribution.
Petroleum reservoir management.
In the context of nonlinear inverse problems, Monte Carlo methods are popular for management of hydrocarbon reservoirs. This includes generating computational models of oil and gas reservoirs for consistency with observed production data. For the goal of decision making and uncertainty assessment, Monte Carlo methods are used for generating multiple geological realizations.

</doc>
<doc id="56099" url="https://en.wikipedia.org/wiki?curid=56099" title="Red dwarf">
Red dwarf

A red dwarf is a small and relatively cool star on the main sequence, either late K or M spectral type. Red dwarfs range in mass from a low of 0.075 solar masses () to about and have a surface temperature of less than 4,000 K.
Red dwarfs are by far the most common type of star in the Milky Way, at least in the neighborhood of the Sun, but because of their low luminosity, individual red dwarfs cannot easily be observed. From Earth, not one is visible to the naked eye. Proxima Centauri, the nearest star to the Sun, is a red dwarf (Type M5, apparent magnitude 11.05), as are twenty of the next thirty nearest stars.
According to some estimates, red dwarfs make up three-quarters of the stars in the Milky Way.
Stellar models indicate that red dwarfs less than are fully convective. Hence the helium produced by the thermonuclear fusion of hydrogen is constantly remixed throughout the star, avoiding a buildup at the core. Red dwarfs therefore develop very slowly, having a constant luminosity and spectral type for, in theory, some trillions of years, until their fuel is depleted. Because of the comparatively short age of the universe, no red dwarfs of advanced evolutionary stages exist.
Description and characteristics.
Red dwarfs are very-low-mass stars. Consequently they have relatively low temperatures in their cores and energy is generated at a slow rate through nuclear fusion of hydrogen into helium by the proton–proton (PP) chain mechanism. Hence these stars emit little light, sometimes as little as that of the Sun. Even the largest red dwarfs (for example HD 179930, HIP 12961 and Lacaille 8760) have only about 10% of the Sun's luminosity. In general, red dwarfs less than transport energy from the core to the surface by convection. Convection occurs because of opacity of the interior, which has a high density compared to the temperature. As a result, energy transfer by radiation is decreased, and instead convection is the main form of energy transport to the surface of the star. Above this mass, a red dwarf will have a region around its core where convection does not occur.
Because low-mass red dwarfs are fully convective, helium does not accumulate at the core and, compared to larger stars such as the Sun, they can burn a larger proportion of their hydrogen before leaving the main sequence. As a result, red dwarfs have estimated lifespans far longer than the present age of the universe, and stars less than have not had time to leave the main sequence. The lower the mass of a red dwarf, the longer the lifespan. It is believed that the lifespan of these stars exceeds the expected 10 billion year lifespan of our Sun by the third or fourth power of the ratio of the solar mass to their masses; thus a red dwarf may continue burning for 10 trillion years. As the proportion of hydrogen in a red dwarf is consumed, the rate of fusion declines and the core starts to contract. The gravitational energy released by this size reduction is converted into heat, which is carried throughout the star by convection.
According to computer simulations, the minimum mass a red dwarf must have in order to become a red giant is ; less massive objects, as they age, increase their surface temperatures and luminosities becoming blue dwarfs and finally become white dwarfs.
The less massive the star, the longer this evolutionary process takes; for example, it has been calculated that a red dwarf (approximately the mass of the nearby Barnard's Star) would stay on the main sequence during 2.5 trillion years that would be followed by five billion years as a blue dwarf, in which the star would have 1/3 of the Sun's luminosity () and a surface temperature of 6,500‒8,500 Kelvin.
The fact that red dwarfs and other low-mass stars still remain on the main sequence when more massive stars have moved off the main sequence allows the age of star clusters to be estimated by finding the mass at which the stars turn off the main sequence. This provides a lower, stellar, age limit to the Universe and also allows formation timescales to be placed upon the structures within the Milky Way, namely the Galactic halo and Galactic disk.
One mystery which has not been solved is the absence of red dwarfs with no metals. (In astronomy, a metal is any element heavier than hydrogen or helium.) The Big Bang model predicts the first generation of stars should have only hydrogen, helium, and trace amounts of lithium. If such stars included red dwarfs, they should still be observable today, but none have yet been identified. The preferred explanation is that without heavy elements only large and not yet observed population III stars can form, and these rapidly burn out, leaving heavy elements which then allow for the formation of red dwarfs. Alternative explanations, such as the idea that zero-metal red dwarfs are dim and could be few in number, are considered much less likely because they seem to conflict with stellar evolution models.
Planets.
Many red dwarfs are orbited by exoplanets but large Jupiter-sized planets are comparatively rare. Doppler surveys around a wide variety of stars indicate about 1 in 6 stars having twice the mass of the Sun are orbited by one or more Jupiter-sized planets, vs. 1 in 16 for Sun-like stars and only 1 in 50 for red dwarfs. On the other hand, microlensing surveys indicate that long-period Neptune-mass planets are found around 1 in 3 red dwarfs.
Observations with HARPS further indicate 40% of red dwarfs have a "super-Earth" class planet orbiting in the habitable zone where liquid water can exist on the surface of the planet.
At least four and possibly up to six exoplanets were discovered orbiting the red dwarf Gliese 581 between 2005–2010. One planet has about the mass of Neptune, or 16 Earth masses (). It orbits just 6 million kilometers (0.04 AU) from its star, and is estimated to have a surface temperature of 150 °C, despite the dimness of its star. In 2006, an even smaller exoplanet (only ) was found orbiting the red dwarf OGLE-2005-BLG-390L; it lies 390 million km (2.6 AU) from the star and its surface temperature is −220 °C (56 K).
In 2007, a new, potentially habitable exoplanet, Gliese 581 c, was found, orbiting Gliese 581. If the minimum mass estimated by its discoverers (a team led by Stephane Udry), namely , is correct, it is the smallest exoplanet revolving around a main-sequence star discovered to date and since then Gliese 581 d, which is also potentially habitable, was discovered. (There are smaller planets known around a neutron star, named PSR B1257+12.) The discoverers estimate its radius to be 1.5 times that of Earth ().
Gliese 581 c and d are within the habitable zone of the host star, and are two of the most likely candidates for habitability of any exoplanets discovered so far. Gliese 581 g, detected September 2010, has a near-circular orbit in the middle of the star's habitable zone. However, the planet's existence is contested.
Habitability.
Planetary habitability of red dwarf systems is subject to some debate. In spite of their great numbers and long lifespans, there are several factors which may make life difficult on planets around a red dwarf. First, planets in the habitable zone of a red dwarf would be so close to the parent star that they would likely be tidally locked. This would mean that one side would be in perpetual daylight and the other in eternal night. This could create enormous temperature variations from one side of the planet to the other. Such conditions would appear to make it difficult for forms of life similar to those on Earth to evolve. And it appears there is a great problem with the atmosphere of such tidally locked planets: the perpetual night zone would be cold enough to freeze the main gases of their atmospheres, leaving the daylight zone nude and dry. On the other hand, recent theories propose that either a thick atmosphere or planetary ocean could potentially circulate heat around such a planet. Alternatively, a moon in orbit around a gas giant may be habitable. It would circumvent the tidal lock problem by becoming tidally locked to its planet. This way there would be a day/night cycle as the moon orbited its primary, and there would be distribution of heat.
In addition, red dwarfs emit most of their radiation as infrared light, while on Earth plants use energy mostly in the visible spectrum. Red dwarfs emit almost no ultraviolet light, which would be a problem, should this kind of light be required for life to exist. Variability in stellar energy output may also have negative impacts on development of life. Red dwarfs are often covered by starspots, reducing stellar output by as much as 40% for months at a time. At other times, some red dwarfs, called flare stars, can emit gigantic flares, doubling their brightness in minutes. This variability may also make it difficult for life to develop and persist near a red dwarf. Gibor Basri of the University of California, Berkeley claims a planet orbiting close to a red dwarf could keep its atmosphere even if the star flares. However, more-recent research suggests that these stars may be the source of constant high energy flares and very large magnetic fields, diminishing the possibility of life as we know it, though whether this is a peculiarity of the star under examination or a feature of the entire class remains to be determined.
Spectral standard stars.
The spectral standards for M-type stars have changed slightly over the years, but settled down somewhat since the early 1990s. Part of this is due to the fact that even the nearest red dwarfs are fairly faint, and the study of mid- to late-M dwarfs has only taken off in the past few decades due to evolution of astronomical techniques, from photographic plates to charged-couple devices (CCDs) to infrared-sensitive arrays.
The revised Yerkes Atlas system (Johnson & Morgan 1953) listed only 2 M-type spectral standard stars: HD 147379 (M0 V)
and HD 95735/Lalande 21185 (M2 V). While HD 147379 was not considered a standard by expert classifiers in later compendia of standards, Lalande 21185 is still a primary standard for M2 V. Robert Garrison does not list any "anchor" standards among the red dwarfs, but Lalande 21185 has survived as a M2 V standard through many compendia. The review on MK classification by Morgan & Keenan (1973) did not contain red dwarf standards. In the mid-1970s, red dwarf standard stars were published by Keenan & McNeil (1976) and Boeshaar (1976), but unfortunately there was little agreement among the standards. As later cooler stars were identified through the 1980s, it was clear that an overhaul of the red dwarf standards was needed. Building primarily upon the Boeshaar standards, a group at Steward Observatory (Kirkpatrick, Henry, & McCarthy 1991) filled in the spectral sequence from K5 V to M9 V. It is these M type dwarf standard stars which have largely survived intact as the main standards to the modern day. There have been negligible changes in the red dwarf spectral sequence since 1991. Additional red dwarf standards were compiled by Henry et al. (2002), and D. Kirkpatrick has recently
reviewed the classification of red dwarfs and standard stars in Gray & Corbally's 2009 monograph. The M-dwarf primary spectral standards are: GJ 270 (M0 V), GJ 229A (M1 V), Lalande 21185 (M2 V), Gliese 581 (M3 V), GJ 402 (M4 V), GJ 51 (M5 V), Wolf 359 (M6 V), Van Biesbroeck 8 (M7 V), VB 10 (M8 V), LHS 2924 (M9 V).

</doc>
<doc id="56100" url="https://en.wikipedia.org/wiki?curid=56100" title="Ouagadougou">
Ouagadougou

Ouagadougou (; Mossi: ) is the capital of Burkina Faso and the administrative, communications, cultural and economic centre of the nation. It is also the country's largest city, with a population of 1,475,223 (""). The city's name is often shortened to "Ouaga". The inhabitants are called "ouagalais". The spelling of the name "Ouagadougou" is derived from the French orthography common in former French African colonies.
Ouagadougou's primary industries are food processing and textiles. It is served by an international airport and it is linked by rail to Abidjan in the Ivory Coast. There is no rail service to Kaya. There is a paved highway to Niamey, Niger, south to Ghana, and Southwest to Ivory Coast. Ouagadougou was the site of Ouagadougou grand market, one of the largest markets in West Africa, which burned in 2003 and has since been reopened. Other attractions include the National Museum of Burkina Faso, the Moro-Naba Palace (site of the Moro-Naba Ceremony), the National Museum of Music, and several craft markets.
History.
The name "Ouagadougou" dates back to the 15th century when the Ninsi tribes inhabited the area. They were in constant conflict until 1441 when Wubri, a Yonyonse hero and an important figure in Burkina Faso's history, led his tribe to victory. He then renamed the area from "Kumbee-Tenga", as the Ninsi had called it, to "Wage sabre soba koumbem tenga", meaning "head war chief's village". "Ouagadougou" is a Francophone spelling of the name.
The city became the capital of the Mossi Empire in 1441 and was the permanent residence of the Mossi emperors (Moro-Naba) from 1681. The Moro-Naba Ceremony is still performed every Friday by the Moro-Naba and his court. The French made Ouagadougou the capital of the Upper Volta territory (basically the same area as contemporary independent Burkina Faso) in 1919. In 1954 the railroad line from Ivory Coast reached the city. The population of Ouagadougou doubled from 1954 to 1960 and has been doubling about every ten years since.
2016 Islamist attacks.
On 15 January 2016, gunmen armed with heavy weapons attacked the Cappuccino restaurant and the Splendid Hotel in the heart of Ouagadougou. 28 people were killed, while at least 56 were wounded; a total of 176 hostages were released after a government counter-attack into the next morning as the siege ended. Three perpetrators were also killed.
Geography.
Ouagadougou, situated on the central plateau (12.4° N 1.5° W), grew around the imperial palace of the Mogho Naaba. Being an administrative center of colonial rule, it became an important urban center in the post-colonial era. First the capital of the Mossi Kingdoms and later of Upper Volta and Burkina Faso, Ouagadougou became a veritable communal center in 1995.
Government.
The first municipal elections were held in 1956.
Ouagadougou is governed by a mayor, who is elected for a five-year term, two senior councillors, and 90 councillors.
The city is divided into five arrondissements, consisting of 30 sectors, which are subdivided into districts. Districts of Ouagadougou include Gounghin, Kamsaoghin, Koulouba, Moemmin, Niogsin, Paspanga, Peuloghin, Bilbalogho, and Tiendpalogo. Seventeen villages comprise the Ouagadougou metropolitan area, which is about .
The city is clean, pleasant, well-organized and relatively safe. There are good restaurants and hotels, clean water, and dependable electricity. Streets are paved, there are stoplights, which most drivers respect. Traffic is congested in some neighborhoods in the mornings and late afternoons.
The population of this area is estimated to be 1,475,000 inhabitants, 48% of which are men and 52% women. The rural population is about 5% and the urban population about 95% of the total, and the density is 6,727 inhabitants per square kilometer, according to 2006 census.
Concerning city management, the communes of Ouagadougou have made the decision to invest in huge projects. This is largely because Ouagadougou constitutes a 'cultural centre' by merit of holding the SIAO (International Arts and Crafts fair) and the FESPACO (Panafrican Film and Television Festival of Ouagadougou). Moreover, the growing affluence of the villages allow for such investment, and the population's rapid growth necessitates it.
Climate.
The climate of Ouagadougou is hot semi-arid (BSh) under Köppen-Geiger classification, that closely borders with tropical wet and dry (Aw). The city is part of the Sudano-Sahelian area, with a rainfall of about per year. The rainy season stretches from May to October, its height from June to September, with a mean average temperature of . The cold season runs from December to January, with a minimum average temperature of . The maximum temperature during the hot season, which runs from March to May, can reach . The harmattan (a dry wind) and the monsoon are the two main factors that determine Ouagadougou's climate. Even though Ouagadougou is farther from the equator, its hottest months' temperatures are slightly hotter than those of Bobo-Dioulasso, the second most populous city.
Social life and education.
Education.
Though literacy in Ouagadougou is not high, there are three universities in the city. The largest college is the state University of Ouagadougou which was founded in 1974. In 2010 it had around 40,000 students (83% of the national population of university students).
The official language in the city is French and the principal local languages are More, Dyula and Fulfulde. The bilingual program in schools (French plus one of the local languages) was established in 1994.
Sport, culture, and leisure.
A wide array of sports, including association football, basketball, and volleyball, is played by Ouagadougou inhabitants. There are sports tournaments and activities organized by the local authorities.
There are a number of cultural and art venues, such as the Maison du Peuple and Salle des Banquets, in addition to performances of many genres of music, including traditional folk music, modern music, and rap.
Art and crafts.
Several international festivals and activities are organized within the municipality, such as FESPACO (Panafrican Film and Television Festival of Ouagadougou), which is Africa's largest festival of this type, SIAO (International Art and Craft Fair), FESPAM (Pan-African Music Festival), FITMO (International Theatre and Marionnette Festival) and FESTIVO.
Health.
Ouagadougou has both state and private hospitals. The two state hospitals in the city are the Centre hospitalier national Yalgado Ouedraogo (CHNYO) and the Centre hospitalier national pédiatrique Charles de Gaulle (CHNP-CDG), but there are also private hospitals. Despite that, the local population still largely can only afford traditional local medicine and the "pharmacopée".
Transport.
Many residents travel on motorcycles and mopeds. The large private vendor of motorcycles JC Megamonde sells 50,000 motorbikes and mopeds every year.
Ouagadougou's citizens also travel in green cabs, which take their passengers anywhere in town for 200 to 400 CFA, but the price is higher after 10:00 pm and can then reach 1000 CFA. 
Air transport.
Ouagadougou Airport (code OUA) serves the area with flights to West Africa and Europe. Air Burkina has its head office in the Air Burkina Storey Building () in Ouagadougou.
Rail.
Ouagadougou is connected by passenger rail service to Bobo-Dioulasso, Koudougou and Ivory Coast. As of June 2014 "Sitarail" operates a passenger train three times a week along the route from Ouagadougou to Abidjan. There are freight services to Kaya in the north of Burkina Faso and in 2014 plans were announced to revive freight services to the Manganese mine at Tambao starting in 2016.
Economy.
The economy of Ouagadougou is based on industry and commerce. Some industrial facilities have relocated from Bobo-Dioulasso to Ouagadougou, which has made the city an important industrial center of Burkina Faso. The industrial areas of Kossodo and Gounghin are home to several processing plants and factories. The industry of Ouagadougou is sector that fuels urban growth, as people move to the city from the countryside to find employment in industry. The Copromof workshop in Ouagadougou sews cotton lingerie for the French label "Atelier Augusti."
Ouagadougou is an important commercial center. It is a center where goods are collected and directed to rural areas. With a large consumer base, large amounts of energy sources, raw materials for buildings, agricultural products and livestock products are imported to the city.
The economy is dominated by the informal sector, which is characterized by petty commodity production, and workers not necessarily having salaries. Traditional, informal trade is widespread and concentrated around markets and major roads, as well as in outlets in neighborhoods. There are also instances of modern economic practices with workplaces having qualified, stable labor forces, or more traditional forms of business such as family businesses.
The tertiary sector is also an important part of the economy. This comprises communications, banking, transport, bars, restaurants, hotels, as well as administrative jobs.
International relations.
Twin towns – Sister cities.
Ouagadougou is twinned with:
Tourism.
Parks.
The Bangr-Weoogo urban park (area: ), before colonialism, belonged to the Mosse chiefs. Considering it a sacred forest, many went there for traditional initiations or for refuge. The French colonists, disregarding its local significance and history, established it as a park in the 1930s. In 1985, renovations were done in the park. In January 2001, the park was renamed "Parc Urbain Bangr-Weoogo", meaning "the urban park of the forest of knowledge".
Another notable park in Ouagadougou is the "L'Unité Pédagogique", which shelters animals in a semi-free state. This botanic garden/biosphere system stretches over and also serves as a museum for the country's history.
"Jardin de l'amitié Ouaga-Loudun" (Garden of Ouaga-Loudun Friendship), with a green space that was renovated in 1996, is a symbol of the twin-city relationship between Ouagadougou and Loudun in France. It is situated in the center of the city, near the "Nation Unies' crossroads".

</doc>
