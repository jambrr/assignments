<doc id="49770" url="https://en.wikipedia.org/wiki?curid=49770" title="High-yield debt">
High-yield debt

In finance, a high-yield bond (non-investment-grade bond, speculative-grade bond, or junk bond) is a bond that is rated below investment grade. These bonds have a higher risk of default or other adverse credit events, but typically pay higher yields than better quality bonds in order to make them attractive to investors. Sometimes the company can provide new bonds as a part of yield which can only be redeemed after its expiry or maturity.
Risk.
The holder of any debt is subject to interest rate risk and credit risk, inflationary risk, currency risk, duration risk, convexity risk, repayment of principal risk, streaming income risk, liquidity risk, default risk, maturity risk, reinvestment risk, market risk, political risk, and taxation adjustment risk. Interest rate risk refers to the risk of the market value of a bond changing due to changes in the structure or level of interest rates or credit spreads or risk premiums. The credit risk of a high-yield bond refers to the probability and probable loss upon a credit event (i.e., the obligor defaults on scheduled payments or files for bankruptcy, or the bond is restructured), or a credit quality change is issued by a rating agency including Fitch, Moody's, or Standard & Poors.
A credit rating agency attempts to describe the risk with a credit rating such as AAA. In North America, the five major agencies are Standard & Poor's, Moody's, Fitch Ratings, Dominion Bond Rating Service and A.M. Best. Bonds in other countries may be rated by US rating agencies or by local credit rating agencies. Rating scales vary; the most popular scale uses (in order of increasing risk) ratings of AAA, AA, A, BBB, BB, B, CCC, CC, C, with the additional rating D for debt already in arrears. Government bonds and bonds issued by government-sponsored enterprises (GSEs) are often considered to be in a zero-risk category above AAA; and categories like AA and A may sometimes be split into finer subdivisions like "AA−" or "AA+".
Bonds rated BBB− and higher are called investment grade bonds. Bonds rated lower than investment grade on their date of issue are called speculative grade bonds, or colloquially as "junk" bonds.
The lower-rated debt typically offers a higher yield, making speculative bonds attractive investment vehicles for certain types of portfolios and strategies. Many pension funds and other investors (banks, insurance companies), however, are prohibited in their by-laws from investing in bonds which have ratings below a particular level. As a result, the lower-rated securities have a different investor base than investment-grade bonds.
The value of speculative bonds is affected to a higher degree than investment grade bonds by the possibility of default. For example, in a recession interest rates may drop, and the drop in interest rates tends to increase the value of investment grade bonds; however, a recession tends to increase the possibility of default in speculative-grade bonds.
Usage.
Corporate debt.
The original speculative grade bonds were bonds that once had been investment grade at time of issue, but where the credit rating of the issuer had slipped and the possibility of default increased significantly. These bonds are called "fallen angels".
The investment banker Michael Milken realized that fallen angels had regularly been valued less than what they were worth. His time with speculative grade bonds started with his investment in these. Only later did he and other investment bankers at Drexel Burnham Lambert, followed by those of competing firms, begin organizing the issue of bonds that were speculative grade from the start. Speculative grade bonds thus became ubiquitous in the 1980s as a financing mechanism in mergers and acquisitions. In a leveraged buyout (LBO) an acquirer would issue speculative grade bonds to help pay for an acquisition and then use the target's cash flow to help pay the debt over time.
In 2005, over 80% of the principal amount of high-yield debt issued by U.S. companies went toward corporate purposes rather than acquisitions or buyouts. 
In emerging markets, such as China and Vietnam, bonds have become increasingly important as term financing options, since access to traditional bank credits has always been proved to be limited, especially if borrowers are non-state corporates. The corporate bond market has been developing in line with the general trend of capital market, and equity market in particular.
Debt repackaging and subprime crisis.
High-yield bonds can also be repackaged into collateralized debt obligations (CDO), thereby raising the credit rating of the senior tranches above the rating of the original debt. The senior tranches of high-yield CDOs can thus meet the minimum credit rating requirements of pension funds and other institutional investors despite the significant risk in the original high-yield debt.
When such CDOs are backed by assets of dubious value, such as subprime mortgage loans, and lose market liquidity, the bonds and their derivatives become what is referred to as "toxic debt". Holding such "toxic" assets led to the demise of several investment banks such as Lehman Brothers and other financial institutions during the subprime mortgage crisis of 2007–09 and led the US Treasury to seek congressional appropriations to buy those assets in September 2008 to prevent a systemic crisis of the banks.
Such assets represent a serious problem for purchasers because of their complexity. Having been repackaged perhaps several times, it is difficult and time-consuming for auditors and accountants to determine their true value. As the recession of 2008–09 bites, their value is decreasing further as more debtors default, so they represent a rapidly depreciating asset. Even those assets that might have gone up in value in the long-term are now depreciating rapidly, quickly becoming "toxic" for the banks that hold them. Toxic assets, by increasing the variance of banks' assets, can turn otherwise healthy institutions into zombies. Potentially insolvent banks have made too few good loans creating a debt overhang problem. Alternatively, potentially insolvent banks with toxic assets will seek out very risky speculative loans to shift risk onto their depositors and other creditors.
On March 23, 2009, U.S. Treasury Secretary Timothy Geithner announced a Public-Private Investment Partnership (PPIP) to buy toxic assets from banks' balance sheets. The major stock market indexes in the United States rallied on the day of the announcement rising by over six percent with the shares of bank stocks leading the way. PPIP has two primary programs. The Legacy Loans Program will attempt to buy residential loans from banks' balance sheets. The Federal Deposit Insurance Corporation will provide non-recourse loan guarantees for up to 85 percent of the purchase price of legacy loans. Private sector asset managers and the U.S. Treasury will provide the remaining assets. The second program is called the legacy securities program which will buy mortgage backed securities (RMBS) that were originally rated AAA and commercial mortgage-backed securities (CMBS) and asset-backed securities (ABS) which are rated AAA. The funds will come in many instances in equal parts from the U.S. Treasury's Troubled Asset Relief Program monies, private investors, and from loans from the Federal Reserve's Term Asset Lending Facility (TALF). The initial size of the Public Private Investment Partnership is projected to be $500 billion. Nobel Prize–winning economist Paul Krugman has been very critical of this program arguing the non-recourse loans lead to a hidden subsidy that will be split by asset managers, banks' shareholders and creditors. Banking analyst Meredith Whitney argues that banks will not sell bad assets at fair market values because they are reluctant to take asset write downs. Removing toxic assets would also reduce the volatility of banks' stock prices. Because stock is akin to a call option on a firm's assets, this lost volatility will hurt the stock price of distressed banks. Therefore, such banks will only sell toxic assets at above market prices.
High-yield bond indices.
High-yield bond indices exist for dedicated investors in the market. Indices for the broad high-yield market include the S&P U.S. Issued High Yield Corporate Bond Index (SPUSCHY), CSFB High Yield II Index (CSHY), Citigroup US High-Yield Market Index, the Merrill Lynch High Yield Master II (H0A0), the Barclays High Yield Index, and the Bear Stearns High Yield Index (BSIX). Some investors, preferring to dedicate themselves to higher-rated and less-risky investments, use an index that only includes BB-rated and B-rated securities, such as the Merrill Lynch Global High Yield BB-B Rated Index (HW40). Other investors focus on the lowest quality debt rated CCC or distressed securities, commonly defined as those yielding 1500 basis points over equivalent government bonds.
EU Member-State Debt Crisis.
On 27 April 2010, the Greek debt rating was decreased to "junk" status by Standard & Poor's amidst fears of default by the Greek Government. They also cut Portugal's credit ratings by two notches to A, over concerns about its state debt and public finances on 28 April.
On 5 July 2011, Portugal's rating was decreased to "junk" status by Moody's (by four notches from Baa1 to Ba2) saying there was a growing risk the country would need a second bail-out before it was ready to borrow money from financial markets again, and private lenders might have to contribute.
On 13 July 2012, Moody's cut Italy's credit rating two notches, to Baa2 (leaving it just above junk). Moody's warned the country it could be cut further.
With the ongoing deleveraging process within the European banking system, many European CFOs are still issuing high-yield bonds. As a result, by the end of September 2012, the total amount of annual primary bond issuances stood at . It is assumed that high-yield bonds are still attractive for companies with a stable funding base, although the ratings have declined continuously for most of those bonds.

</doc>
<doc id="49773" url="https://en.wikipedia.org/wiki?curid=49773" title="Decatur">
Decatur

Decatur is the name of many places in the United States, most of which are named for Stephen Decatur.

</doc>
<doc id="49778" url="https://en.wikipedia.org/wiki?curid=49778" title="Amherst">
Amherst

Amherst may refer to:
Places.
Many named for Jeffrey Amherst, 1st Baron Amherst:

</doc>
<doc id="49779" url="https://en.wikipedia.org/wiki?curid=49779" title="Lawrenceville">
Lawrenceville

Lawrenceville is the name of several places:

</doc>
<doc id="49780" url="https://en.wikipedia.org/wiki?curid=49780" title="Lexington">
Lexington

Lexington may refer to:
Places.
Current.
In the United States:

</doc>
<doc id="49781" url="https://en.wikipedia.org/wiki?curid=49781" title="Mount Vernon (disambiguation)">
Mount Vernon (disambiguation)

Mount Vernon is the Virginia estate of George Washington, the first President of the United States. Mount Vernon or Mont Vernon may also refer to:

</doc>
<doc id="49782" url="https://en.wikipedia.org/wiki?curid=49782" title="Pasadena">
Pasadena

Pasadena may refer to:
Places.
Places in Australia:
Places in Canada:
Places in the United States:

</doc>
<doc id="49783" url="https://en.wikipedia.org/wiki?curid=49783" title="Reading education in the United States">
Reading education in the United States

Reading education is the process by which individuals are taught to derive meaning from text.
Government-funded research on reading and reading instruction in the U.s. began in the 1960s. In the 1970s and 1980s, researchers began publishing findings based on converging evidence from multiple studies. However, these findings have been slow to move into typical classroom practice.
Competencies for proficient reading.
Proficient reading is equally dependent on two critical skills: the ability to understand the language in which the text is written, and the ability to recognize and process printed text. Each of these competencies is likewise dependent on lower level skills and cognitive abilities.
Children who readily understand spoken language and who are able to fluently and easily recognize printed words do not usually have difficulty with reading comprehension. However, students must be proficient in both competencies to read well; difficulty in either domain undermines the overall reading process. At the conclusion of reading, children should be able to retell the story in their own words including characters, setting, and the events of the story. Reading researchers define a skilled reader as one who can understand written text as well as they can understand the same passage if spoken.
There is some debate as to whether print recognition requires the ability to perceive printed text and translate it into spoken language, or rather to translate printed text directly into meaningful symbolic models and relationships. The existence of speed reading, and its typically high comprehension rate would suggest that the translation into verbal form as an intermediate to understanding is not a prerequisite for effective reading comprehension. This aspect of reading is the crux of much of the reading debate.
The purpose of reading is to have access to the literature of a specific language. Reading materials have traditionally been chosen from literary texts that represent 'higher' forms of culture. According to many traditional approaches, the learner's aim is to study vocabulary items, grammar and sentence structures, with a concern for learning the syntax of these 'higher' cultures. These approaches assume that authentic reading material is limited to the work or experience of great authors.
Instructional methods.
A variety of different methods of teaching reading have been advocated in English-speaking countries.
In the United States, the debate is often more political than objective. Parties often divide into two camps which refuse to accept each other's terminology or frame of reference. Despite this both camps often incorporate aspects of the other's methods. Both camps accuse the other of causing failure to learn to read and write. Phonics advocates assert that, to read a large vocabulary of words correctly and fluently requires detailed knowledge of the structure of the English language, particularly spelling-speech patterns. Whole Language advocates assert that students do not need to be able to sound out words, but should look at unknown words and figure them out using context.
Research.
In 2000, the National Reading Panel (NRP) issued a report based on a meta-analysis of published research on effective reading instruction. The report found varying evidence-based support for some common approaches to teaching reading.
Phonemic awareness.
The NRP called phonemic awareness (PA) instruction "impressive":
The report singles out PA instruction based on teaching children to manipulate phonemes with letters as highly effective. Phonemic awareness instruction also improved spelling in grade-level students, although it did not improve spelling in disabled readers.
Lexical reading.
Lexical reading involve acquiring words or phrases without attention to the characters or groups of characters that compose them or by using Whole language learning and teaching methodology. Sometimes argued to be in competition with phonics methods, and that the whole language approach tends to impair learning how to spell.
Historically, the two camps have been called Whole Language and Phonics, although the Whole Language instructional method has also been referred to as "literature-based reading program" and "integrated language arts curriculum". Currently (2007), the differing perspectives are frequently referred to as "balanced reading instruction" (Whole Language) and "scientifically-based reading instruction" (Phonics).
Whole Word.
Whole word, also known as "Sight Word" and "Look and Say", teaches reading skills and strategies in the context of authentic literature. Word recognition accuracy is considered less important than meaning accuracy; therefore, there is an emphasis on comprehension as the ultimate goal.
Students in this method memorize the appearance of words, or learn to recognize words by looking at the first and last letter from rigidly selected vocabularies in progressive texts (such as "The Cat in the Hat"). Often preliminary results show children taught with this method have higher reading levels than children learning phonics, because they learn to automatically recognise a small selection of words. However, later tests demonstrate that literacy development becomes stunted when hit with longer and more complex words later.
Sub-lexical reading.
Sub-lexical reading, involves teaching reading by associating characters or groups of characters with sounds or by using Phonics learning and teaching methodology. Sometimes argued to be in competition with whole language methods.
Phonics.
Phonics refers to an instructional method for teaching children to read. The method teaches sounds to be associated with letters and combinations of letters. "Phonics" is distinct from the linguistics terms "phoneme" and "phonetics", which refer to sounds and the study of sounds respectively.
Varieties of phonics include:
The Orton phonography, originally developed to teach brain-damaged adults to read, is a form of phonics instruction that blends synthetic and analytic components. Orton described 73 "phonograms", or letter combinations, and 23 rules for spelling and pronunciation which Orton claimed would allow the reader to correctly pronounce and spell all but 123 of the 13,000 most common English words.
Pronunciation guides.
In contrast to phonics which teaches the pronunciation rules of English, a new technology Phonetically Intuitive English directly shows English words' pronunciation by adding diacritical marks on them. This solves the problem that pronunciation rules can often be confusing (for example, "ea" has a wide range of diverse pronunciations in "speak", "steak", "bread", "Korea", "reality", "create" and "ocean").
The pronunciation-guide approach has been proven very successful in reading education for languages with very complex orthography such as Chinese. Pinyin is a system of phonetic transcription for Mandarin Chinese and is printed above Chinese characters in children's textbooks as a pronunciation guide, and has enabled China to achieve a high literacy rate for the most difficult language in the world.
Other instructional methods.
Native reading.
Some methods of mix phonics and whole word. Native reading, for example, differs from both in that it emphasizes teaching reading beginning at a very early age, when the human brain is neurodevelopmentally most receptive to learning language. Native readers learn to read as toddlers, starting at the same time they learn to speak, or very soon thereafter.
Reading Workshop.
Reading Workshop is based on the premise that readers need time to read and discuss their reading. Readers need access to a wide variety of reading materials of their choice. Classrooms must acquire a wide variety of reading materials to accommodate this need. Readers need to respond to the text and demonstrate quality literate behaviors. There is not a script to follow but a frame work to guide instruction. Students are exposed to a variety of learning experiences. There is time for student collaboration and a time for engaged reading.
During reading workshop, the teacher models a whole-group strategy lesson and then gives students large blocks of time to read and to practice the strategy. This practice can occur independently, with partners, or in small groups with a book or text chosen by the student. The teacher moves around the room and confers with the students about their reading. The teacher can meet with small, flexible groups to provide additional needs-based instruction. At the end of the workshop the whole groups comes together to share their learning.
Examples of Improving Reading.
An adult or peer reads with the student by modeling fluent reading and then asking the student to read the same passage aloud with encouragement and feedback by the adult or peer.
A student listens to a tape of a fluent reader reading text at the student's independent level at a pace of about 80-100 words a minute. The student listens to the tape the first time and then practices reading along with the tape until the student is able to read fluently.
The student reads with a peer partner. Each partner takes a turn reading to the other. A more fluent reader can be paired with a less fluent reader to model fluent reading. The more fluent reader can provide feedback and encouragement to the less fluent reader. Students of similar reading skills can also be paired, particularly if the teacher has modeled fluent reading and the partner reading involves practice.
The following is a list of the seven important strategies that all readers must be able to apply to text in order to read and understand content. The seven strategies are:
Reading comprehension.
Reading comprehension involves the making sense of text. Done successfully, this allows the readers to gain knowledge, enjoy a story, and make connections with the larger world. Several skills support reading comprehension including making predictions and inferences, monitoring understanding, using text structures, and utilizing prior knowledge. Two of the most important aspects of successful comprehension are activating prior knowledge and metacognition, which are two of the principles of learning identified in the National Research Council's report."
Many studies have identified the importance of prior knowledge in reading comprehension. "Many researchers have shown that having some prior knowledge about the topic of a passage enables both greater comprehension of the text and better memory for it." "if we have prior knowledge about a topic in a text, we construct meaning based on our experience, and we can adjust and change those plans as we go along." Some authors specify two types of prior knowledge necessary for successful comprehension. World knowledge aids in understanding fiction and domain-specific knowledge facilitates comprehension of nonfiction. Students who lack this, request background information, so as to make connections with and within the text.
Another learning principle that greatly influences reading comprehension is the use of metacognition. The 'metacognitive' approach to instruction can help students learn to take control of their own learning by defining learning goals and monitoring their progress in achieving them." A great deal of research indicates that accomplished readers "monitor their comprehension as they read by engag in strategic processing, such as rereading previous text, to resolve comprehension failure." Students who are not able to track their own understanding gain neither information or enjoyment from reading as they do not know how to obtain meaning from the text.
Many strategies have been applied. Many studies point to the success of strategy instruction, particularly for students who are poor comprehenders. Some strategies that have been helpful are summarization, question generation, making predictions and inferences, image making, knowledge and use of text structure, rereading, self-regulation, activation of prior knowledge, questioning the author, and using graphic organizers. The variety of strategies allows the teacher to choose a strategy or strategies to suit the text and the needs of the student.
For an example of a specific intervention incorporating four strategies for comprehension building, utilized reciprocal teaching to model summarizing, questioning, clarifying, and predicting. The authors indicate that they choose these skills due to their dual functions as "comprehension-fostering and comprehension monitoring activities." The reciprocal teaching method, which involves the teacher modeling the designated activities and gradually turning the procedure over to the students themselves, uses Vygotsky's idea of scaffolding. In this process, "children first experience a particular set of cognitive activities in the presence of experts, and only gradually come to perform these functions by themselves. In this study, the students who participated in the reciprocal teaching intervention showed dramatic improvement in comprehension scores and maintained them for at least eight weeks.
Non-traditional approaches.
Programs have been established to provide certified therapy animals, such as dogs, as non-judgmental "listeners" to build motivation and help children build proficiency and gain confidence in their reading ability.
Success rate of reading education in the US.
National literacy rates range from about 10 percent to 99+ percent.
Print exposure.
Print exposure the amount of time a child or person spends being visually aware of the written word (reading)--whether that be through newspapers, magazines, books, journals, scientific papers, or more. Research has shown that the amount of print material that a child accesses has deep cognitive consequences. In addition, the act of reading itself, for the most part irrespective of what is being read, increases the achievement difference among children.
Children who are exposed to large amounts of print often have more success in reading and have a larger vocabulary to draw from than children who see less print. The average conversations among college graduates, spouses or adult friends contain less rare (advanced) words than the average preschool reading book. Other print sources have increasingly higher amounts of rare words, from children's books, to adult books, to popular magazines, newspapers, and scientific articles (listed in increasing level of difficulty). Television, even adult news shows, do not have the same level of rare words that children's books do.
The issue is that oral language is very repetitive. To learn to read effectively a child needs to have a large vocabulary. Without this, when the child does read they stumble over words that they do not know, and have trouble following the idea of the sentence. This leads to frustration and a dislike of reading. When a child is faced with this difficulty he or she is less likely to read, thus further inhibiting the growth of their vocabulary.
Children who enjoy reading do it more frequently and improve their vocabulary. A study of out-of-school reading of fifth graders, found that a student in the 50th percentile read books about 5 minutes a day, while a student in the 20th percentile read books for less than a minute a day. This same study found that the amount of time a child in the 10th percentile spent reading in two days, was the amount of time a child in the 90th percentile spent reading all year.
Print exposure can also be a big factor in learning English as a second language. Book flood experiments are an example of this. The book flood program brought books in English to the classroom. Through focusing their English language learning on reading books instead of endless worksheets the teachers were able to improve the rate at which their students learned English.
Alphabetic principle and English orthography.
Beginning readers must understand the concept of the "alphabetic principle" in order to master basic reading skills. A writing system is said to be "alphabetic" if it uses symbols to represent individual language sounds. In comparison, Logographic writing systems such as Japanese kanji and Chinese hanzi use a symbol to represent a word. And both cultures also use syllabic writing systems such as Japanese kana and Chinese Yi script, there are also many Chinese alphabets.
English is one of several languages using the Latin Alphabet writing system. The orthographic depth of such languages varies. The Italian and Finnish languages have the purest, or shallowest orthographies, and English orthography is the deepest or most complex. In the shallow Spanish orthography; most words are spelled the way they sound, that is, word spellings are almost always regular. English orthography, on the other hand, is far more complex in that it does not have a one-to-one correspondence between symbols and sounds. English has individual sounds that can be represented by more than one symbol or symbol combination. For example, the long |a| sound can be represented by a-consonant-e as in ate, -ay as in hay, -ea as in steak, -ey as in they, -ai as in pain, and -ei as in vein. In addition, there are many words with irregular spelling and many homophones (words that sound the same but have different meanings and often different spellings as well). Pollack Pickeraz (1963) asserted that there are 45 phonemes in the English language, and that the 26 letters of the English alphabet can represent them in about 350 ways.
The irregularity of English spelling is largely an artifact of how the language developed.
English is a West Germanic language with substantial influences and additional vocabulary from Latin, Greek, and French, among others. Imported words usually follow the spelling patterns of their language of origin. Advanced English phonics instruction includes studying words according to their origin, and how to determine the correct spelling of a word using its language of origin.
Clearly, the complexity of English orthography makes it more difficult for children to learn decoding and encoding rules, and more difficult for teachers to teach them. However, effective word recognition relies on the basic understanding that letters represent the sounds of spoken language, that is, word recognition relies on the reader's understanding of the alphabetic principle
Spelling reform.
Attempts to make English spelling behave phonetically have given rise to various campaigns for spelling reform; none have been generally accepted. Opponents of simplified spellings point to the impossibility of phonetic spelling for a language with many diverse accents and dialects. Several distinguished scholars, however, have thoroughly disproven all reasonable objections to spelling reform, including this objection. See, for example, "Dictionary of Simplified American Spelling." Thomas Lounsbury presented a devastating rebuttal to all reasonable objections to spelling reform in 1909 A shorter rebuttal of all the reasonable objections to spelling reform by Bob C Cleckler
Linguists documenting the sounds of speech use various special symbols, of which the International Phonetic Alphabet is the most widely known. Linguistics makes a distinction between a phone and phoneme, and between phonology and phonetics. The study of words and their structure is morphology, and the smallest units of meaning are morphemes. The study of the relationship between words present in the language at one time is synchronic etymology, part of descriptive linguistics, and the study of word origins and evolution is diachronic etymology, part of historical linguistics.
English orthography gives priority first to morphology, then to etymology, and lastly to phonetics. Thus the spelling of a word is dependent principally upon its structure, its relationship to other words, and its language or origin. It is usually necessary to know the meaning of a word in order to spell it correctly, and its meaning will be indicated by the similarity to words of the same meaning and family.
English uses a 26 letter Latin alphabet, but the number of graphemes is expanded by several digraphs, trigraphs, and tetragraphs, while the letter "q" is not used as a grapheme by itself, only in the digraph "qu".
Each grapheme may represent a limited number of phonemes depending on etymology and location in the word. Likewise each phoneme may be represented by a limited number of graphemes. Some letters are not part of any grapheme, but function as etymological markers. Graphemes do not cross morpheme boundaries.
Morphemes are spelt consistently, following rules inflection and word-formation, and allow readers and writers to understand and produce words they have not previously encountered.
Initial teaching alphabet.
This method was designed to overcome the fact that English orthography has a many-to-many relationship between graphemes and phonemes. The method fell into disuse because children still had to learn the Latin alphabet and the conventional English spellings in order to integrate with society outside of school. It also recreated the problem of dialect dependent spelling, which the standardisation of spelling had been created to eliminate.
Augmenting spelling with pronunciation information.
Unlike spelling reforms, we can actually keep a word's original spelling intact but add pronunciation information to it, e.g. using diacritics. Phonetically Intuitive English is a Chrome browser extension that automatically adds such a pronunciation guide to English words on Web pages, for English-speaking children to recognize a written word's pronunciation and therefore map the written word to the mental word in his mind.
Practical application.
In practice, many children are exposed to both "Phonic" and "Whole Language" methods, coupled with reading programs that combine both elements. For example, the extremely popular book, "Teach Your Child to Read in 100 Easy Lessons", by Siegfried Engelman, et al. (ISBN 0-671-63198-5), teaches pronunciation and simple phonics, then supplements it with progressive texts and practice in directed reading. The end result of a mixed method is a casually phonetic student, a much better first-time pronouncer and speller, who still also has look-say acquisition, quick fluency and comprehension. Using an eclectic method, students can select their preferred learning style. This lets all students make progress, yet permits a motivated student to use and recognize the best traits of each method.
Speed reading continues where basic education stops. Usually after some practice, many students' reading speed can be significantly increased. There are various speed-reading techniques.
However, speed reading does not guarantee comprehension or retention of what was read.
Readability indicates the ease of understanding or comprehension due to the style of writing. Reading recovery is a method for helping students learn to read.
History.
In colonial times, reading instruction was simple and straightforward: teach children the code and then let them read. At that time, reading material was not specially written for children but consisted primarily of the Bible and some patriotic essays; the most influential early textbook was "The New England Primer," published late 1680s. There was little consideration for how best to teach children to read or how to assess reading comprehension.
From the 1890s to at least 1910, A. L. Burt of New York and other publishing companies published series of books aimed at young readers, using simple language to retell longer classics. Mrs J. C. Gorham produced three such works, "Gulliver's Travels in words of one syllable" (1896), "Alice's Adventures in Wonderland retold in words of one syllable" (1905), and "Black Beauty retold in words of one syllable" (1905). In the UK, Routledge published a similar series between 1900 and 1910.
The meaning-based curriculum did not dominate reading instruction until the second quarter of the 20th century. Beginning in the 1930s and 1940s, reading programs became very focused on comprehension and taught children to read whole words by sight. Phonics was not to be taught except sparingly and as a tool to be used as a last resort.
In the 1950s Rudolf Flesch wrote a book called "Why Johnny Can't Read", a passionate argument in favor of teaching children to read using phonics. Addressed to the mothers and fathers of America, he also hurled severe criticism at publishers' decisions that he claimed were motivated by profit, and he questioned the honesty and intelligence of experts, schools, and teachers. The book was on the best seller list for 30 weeks and spurred a hue and cry in general population. It also polarized the reading debate among educators, researchers, and parents.
This polarization continues to the present time. In the 1970s an instructional philosophy called whole language (which de-emphasizes teaching phonics out of context) was introduced, and it became the primary method of reading instruction in the 1980s and 1990s. During this time, researchers (such as the National Institute of Health) conducted studies showing that early reading acquisition depends on the understanding of the connection between sounds and letters.
The sight-word (Whole Word) method was invented by Rev. Thomas H. Gallaudet, the director of the American Asylum at Hartford in the 1830s. It was designed for the education of the Deaf by juxtaposing a word, with a picture. In 1830, Gallaudet provided a description of his method to the American Annals of Education which included teaching children to recognize a total of 50 sight words written on cards and by 1837 the method was adopted by the Boston Primary School Committee. Horace Mann the then Secretary of the Board of Education of Massachusetts, USA favored the method and it soon became the dominant method statewide. By 1844 the defects of the new method became so apparent to Boston schoolmasters that they issued an attack against it urging a return to an intensive, systematic phonics. Again Dr. Samuel Orton, a neuropathologist in Iowa in 1929 sought the cause of children's reading problems and concluded that their problems were being caused by the new sight method of teaching reading. (His results were published in the February 1929 issue of the Journal of Educational Psychology, “The Sight Reading Method of Teaching Reading as a Source of Reading Disability.”)

</doc>
<doc id="49784" url="https://en.wikipedia.org/wiki?curid=49784" title="Warren">
Warren

Warren may refer to:

</doc>
<doc id="49785" url="https://en.wikipedia.org/wiki?curid=49785" title="Bayes' rule">
Bayes' rule

In probability theory and applications, Bayes's rule relates the odds of event formula_1 to the odds of event formula_2, before (prior to) and after (posterior to) conditioning on another event formula_3. The odds on formula_1 to event formula_2 is simply the ratio of the probabilities of the two events. The prior odds is the ratio of the unconditional or prior probabilities, the posterior odds is the ratio of conditional or posterior probabilities given the event formula_3. The relationship is expressed in terms of the likelihood ratio or Bayes factor, formula_7. By definition, this is the ratio of the conditional probabilities of the event formula_3 given that formula_1 is the case or that formula_2 is the case, respectively. The rule simply states: posterior odds equals prior odds times Bayes factor (Gelman et al., 2005, Chapter 1).
When arbitrarily many events formula_11 are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, formula_12 where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as formula_11 varies, for fixed or given formula_3 (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005).
Bayes' rule is an equivalent way to formulate Bayes' theorem. If we know the odds for and against formula_11 we also know the probabilities of formula_11. It may be preferred to Bayes' theorem in practice for a number of reasons.
Bayes' rule is widely used in statistics, science and engineering, for instance in model selection, probabilistic expert systems based on Bayes networks, statistical proof in legal proceedings, email spam filters, and so on (Rosenthal, 2005; Bertsch McGrayne, 2012). As an elementary fact from the calculus of probability, Bayes' rule tells us how unconditional and conditional probabilities are related whether we work with a frequentist interpretation of probability or a Bayesian interpretation of probability. Under the Bayesian interpretation it is frequently applied in the situation where formula_1 and formula_2 are competing hypotheses, and formula_3 is some observed evidence. The rule shows how one's judgement on whether formula_1 or formula_2 is true should be updated on observing the evidence formula_3 (Gelman et al., 2003).
The rule.
Single event.
Given events formula_1, formula_2 and formula_3, Bayes' rule states that the conditional odds of formula_26 given formula_3 are equal to the marginal odds of formula_26 multiplied by the Bayes factor or likelihood ratio formula_7:
where the
Here, the odds and conditional odds, also known as prior odds and posterior odds, are defined by
In the special case that formula_34 and formula_35, one writes formula_36, and uses a similar abbreviation for the Bayes factor and for the conditional odds. The odds on formula_11 is by definition the odds for and against formula_11. Bayes' rule can then be written in the abbreviated form
or in words: the posterior odds on formula_11 equals the prior odds on formula_11 times the likelihood ratio for formula_11 given information formula_3. In short, posterior odds equals prior odds times likelihood ratio.
The rule is frequently applied when formula_34 and formula_35 are two competing hypotheses concerning the cause of some event formula_3. The prior odds on formula_11, in other words, the odds between formula_48 and formula_49, expresses our initial beliefs concerning whether or not formula_48 is true. The event formula_3 represents some evidence, information, data, or observations. The likelihood ratio is the ratio of the chances of observing formula_3 under the two hypotheses formula_11 and formula_54. The rule tells us how our prior beliefs concerning whether or not formula_48 is true needs to be updated on receiving the information formula_3.
Many events.
If we think of formula_11 as arbitrary and formula_3 as fixed then we can rewrite Bayes' theorem formula_59 in the form formula_60 where the proportionality symbol means that, as formula_11 varies but keeping formula_3 fixed, the left hand side is equal to a constant times the right hand side.
In words posterior is proportional to prior times likelihood. This version of Bayes' theorem was first called "Bayes' rule" by Cournot (1843). Cournot popularized the earlier work of Laplace (1774) who had independently discovered Bayes' rule. The work of Bayes was published posthumously (1763) but remained more or less unknown till Cournot drew attention to it; see Fienberg (2006).
Bayes' rule may be preferred to the usual statement of Bayes' theorem for a number of reasons. One is that it is intuitively simpler to understand. Another reason is that normalizing probabilities is sometimes unnecessary: one sometimes only needs to know ratios of probabilities. Finally, doing the normalization is often easier to do after simplifying the product of prior and likelihood by deleting any factors which do not depend on formula_11, so we do not need to actually compute the denominator formula_64 in the usual statement of Bayes' theorem formula_65.
In Bayesian statistics, Bayes' rule is often applied with a so-called improper prior, for instance, a uniform probability distribution over all real numbers. In that case, the prior distribution does not exist as a probability measure within conventional probability theory, and Bayes' theorem itself is not available.
Series of events.
Bayes' rule may be applied a number of times. Each time we observe a new event, we update the odds between the events of interest, say formula_1 and formula_2 by taking account of the new information. For two events (information, evidence) formula_3 and formula_69,
where
In the special case of two complementary events formula_11 and formula_54, the equivalent notation is
Derivation.
Consider two instances of Bayes' theorem:
Combining these gives
Now defining
this implies
A similar derivation applies for conditioning on multiple events, using the appropriate extension of Bayes' theorem
Examples.
Frequentist example.
Consider the drug testing example in the article on Bayes' theorem.
The same results may be obtained using Bayes' rule. The prior odds on an individual being a drug-user are 199 to 1 against, as formula_83 and formula_84. The Bayes factor when an individual tests positive is formula_85 in favour of being a drug-user: this is the ratio of the probability of a drug-user testing positive, to the probability of a non-drug user testing positive. The posterior odds on being a drug user are therefore formula_86, which is very close to formula_87. In round numbers, only one in three of those testing positive are actually drug-users.

</doc>
<doc id="49786" url="https://en.wikipedia.org/wiki?curid=49786" title="Microclimate">
Microclimate

A microclimate is a local set of atmospheric conditions that differ from those of the surrounding area. Because climate is a statistic, which implies spatial and temporal variation of the mean values of the describing parametres, it is clear that within a region could occur and persist in time, sets of statiscally distinct conditions, i. e., microclimates. The term may refer to areas as small as a few square meters or square feet (for example a garden bed) or as large as many square kilometers or square miles.
Microclimates exist, for example, near bodies of water which may cool the local atmosphere, or in heav urban areas where brick, concrete, and asphalt absorb the sun's energy, heat up, and reradiate that heat to the ambient air: the resulting urban heat island is a kind of microclimate. Microclimates can be found in most places. Another place this can occur is when the ground is made of tar or concrete; because these are man-made objects, they do not take in much heat, but mainly reradiate it.
Another contributing factor of microclimate is the slope or aspect of an area. South-facing slopes in the Northern Hemisphere and north-facing slopes in the Southern Hemisphere are exposed to more direct sunlight than opposite slopes and are therefore warmer for longer periods of time, giving the slope a warmer microclimate than the areas around the slope.m
Background.
The area in a developed industrial park may vary greatly from a wooded park nearby, as natural flora in parks absorb light and heat in leaves that a building roof or parking lot just radiates back into the air. Advocates of solar energy argue that widespread use of solar collection can mitigate overheating of urban environments by absorbing sunlight and putting it to work instead of heating the foreign surface objects.
A microclimate can offer an opportunity as a small growing region for crops that cannot thrive in the broader area; this concept is often used in permaculture practiced in northern temperate climates. Microclimates can be used to the advantage of gardeners who carefully choose and position their plants. Cities often raise the average temperature by zoning, and a sheltered position can reduce the severity of winter. Roof gardening, however, exposes plants to more extreme temperatures in both summer and winter.
Tall buildings create their own microclimate, both by overshadowing large areas and by channeling strong winds to ground level. Wind effects around tall buildings are assessed as part of a microclimate study.
Microclimates can also refer to purpose-made environments, such as those in a room or other enclosure. Microclimates are commonly created and carefully maintained in museum display and storage environments. This can be done using passive methods, such as silica gel, or with active microclimate control devices.
Usually, if the inland areas have a humid continental climate, the coastal areas stay much milder during winter months, in contrast to the hotter summers. This is the case further north on the American west coast, such as in British Columbia, Canada, where Vancouver has an oceanic wet winter with rare frosts, but inland areas that average several degrees warmer in summer have cold and snowy winters.
Soil types.
The type of soil found in an area can also affect microclimates. For example, soils heavy in clay can act like pavement, moderating the near ground temperature. On the other hand, if soil has many air pockets, then the heat could be trapped underneath the topsoil, resulting in the increased possibility of frost at ground level.
Sources and influences on microclimate.
Two main parameters to define a microclimate within a certain area are temperature and humidity. A source of a drop in temperature and/or humidity can be attributed to different sources or influences.
Often microclimate is shaped by a conglomerate of different influences and is a subject of microscale meteorology.
Cold air pool.
The well known examples of cold air pool (CAP) effect are
Gstettneralm Sinkhole in Austria (lowest recorded temperature -53 C)
The main criterion on the wind speed formula_1 in order to create a warm air flow penetration into a CAP is the following:
where formula_3 is the Froude number, formula_4 --- the Brunt–Väisälä frequency, formula_5 --- depth of the valley, and formula_6 --- Froude number at the threshold wind speed.
Craters.
The presence of permafrost close to the surface in a crater creates a unique microclimate environment.
Caves and lava tubes.
As similar as lava tubes can be to caves which are not formed due to volcanic activity the microclimate within the former is different due to dominant presence of basalt.
Lava tubes and basaltic caves are important astrobiological targets on Earth and Mars (see also Martian lava tube).
Plant microclimate.
As pointed out by Rudolf Geiger in his book not only climate influences the living plant but the opposite effect of the interaction of plants on their environment can also take place, and is known as "plant climate".
Dams.
Artificial reservoirs as well as natural ones create microclimates and often influence the macroscopic climate as well.

</doc>
<doc id="49791" url="https://en.wikipedia.org/wiki?curid=49791" title="Edmund Blunden">
Edmund Blunden

Edmund Charles Blunden, CBE, MC (1 November 1896 – 20 January 1974) was an English poet, author and critic. Like his friend Siegfried Sassoon, he wrote of his experiences in World War I in both verse and prose. For most of his career, Blunden was also a reviewer for English publications and an academic in Tokyo and later Hong Kong. He ended his career as Professor of Poetry at the University of Oxford.
Biography.
Early years and World War I.
Born in London, Blunden was the eldest of the nine children of Charles Edmund Blunden (1871–1951) and his wife, Georgina Margaret "née" Tyler, who were joint-headteachers of Yalding school. Blunden was educated at Christ's Hospital and The Queen's College, Oxford.
In August 1915 Blunden was commissioned as a second lieutenant in the Royal Sussex Regiment and served with them right up to the end of World War I, taking part in the actions at Ypres and the Somme, and receiving the Military Cross in the process. Unusually for a junior infantry officer, Blunden survived nearly two years in the front line without physical injury, but for the rest of his life bore mental scars from his experiences. With characteristic self-deprecation he attributed his survival to his diminutive size: he made "an inconspicuous target". His own account of his frequently traumatic experiences was published in 1928 under the title "Undertones of War".
Career as a writer.
Blunden left the army in 1919 and took up the scholarship at Oxford that he had won while still at school. On the same English Literature course was Robert Graves, and the two were close friends during their time at Oxford together, but Blunden found university life unsatisfactory and left in 1920 to take up a literary career, at first acting as assistant to Middleton Murry on the "Athenaeum". An early supporter was Siegfried Sassoon, who became a lifelong friend. In 1920 Blunden published a collection of poems, "The Waggoner", and with Alan Porter edited the poems of John Clare (mostly from Clare's manuscript).
Blunden's next book of poems, "The Shepherd", published in 1922 won the Hawthornden Prize, but his poetry, though well reviewed, did not provide enough to live on, and in 1924 he accepted the post of Professor of English at the University of Tokyo. He returned to England in 1927, and was literary editor of the "Nation" for a year. In 1927 he published a short book, "On the Poems of Henry Vaughan, Characteristics and Intimations, with his principal Latin poems carefully translated into English verse" (London: H. Cobden-Sanderson, 1927), expanding and revising an essay that he had published in November 1926 in the "London Mercury". In 1931 he returned to Oxford as a Fellow of Merton College, where he was highly regarded as a tutor. During his years in Oxford, Blunden published extensively: several collections of poetry including "Choice or Chance" (1934) and "Shells by a Stream" (1944), prose works on Charles Lamb; Edward Gibbon; Keats's publisher; Percy Bysshe Shelley (""); John Taylor; and Thomas Hardy; and a book about a game he loved, "Cricket Country" (1944). He returned to full-time writing in 1944, becoming assistant editor of "The Times Literary Supplement". In 1947 he returned to Japan as a member of the British liaison mission in Tokyo. In 1953, after three years back in England he accepted the post of Professor of English Literature at the University of Hong Kong.
Blunden retired in 1964 and settled in Suffolk. In 1966 he was nominated for the Oxford Professorship of Poetry in succession to Robert Graves; with some misgivings he agreed to stand and was elected by a large majority over the other candidate, Robert Lowell. However, he now found the strain of public lecturing too much for him, and after two years he resigned.
He died of a heart attack at his home at Long Melford, Suffolk, on 20 January 1974, and is buried in the churchyard of Holy Trinity Church, Long Melford.
Personal life.
Blunden was married three times. While still in the army he met and married Mary Daines in 1918. They had three children, the first of whom died in infancy. They divorced in 1931, and in 1933 Blunden married Sylva Norman, a young novelist and critic. That marriage, which was childless, was dissolved in 1945, and in the same year he married Claire Margaret Poynting (1918-2000), a former pupil of his; they had four daughters. While in Japan in the summer of 1925, he met Aki Hayashi, with whom he began a relationship. When Blunden returned to England in 1927, Aki accompanied him and would become his secretary. The relationship later changed from a romantic one to a platonic friendship, and they remained in contact for the rest of her life.
Blunden's love of cricket, celebrated in his book "Cricket Country", is described by the biographer Philip Ziegler as fanatical. Blunden and his friend Rupert Hart-Davis regularly opened the batting for a publisher's eleven in the 1930s (Blunden insisted on batting without gloves). An affectionate obituary tribute in "The Guardian" commented, "He loved cricket ... and played it ardently and very badly", while in a review of "Cricket Country", George Orwell described him as "the true cricketer":
The test of a true cricketer is that he shall prefer village cricket to 'good' cricket [... Blunden's] friendliest memories are of the informal village game, where everyone plays in braces, where the blacksmith is liable to be called away in mid-innings on an urgent job, and sometimes, about the time when the light begins to fail, a ball driven for four kills a rabbit on the boundary.
In a 2009 appreciation of the book and its author, Bangalore writer Suresh Menon writes,
Any cricket book that talks easily of Henry James and Siegfried Sassoon and Ranji and Grace and Richard Burton (the writer, not the actor) and Coleridge is bound to have a special charm of its own. As Blunden says, "The game which made me write at all, is not terminated at the boundary, but is reflected beyond, is echoed and varied out there among the gardens and the barns, the dells and the thickets, and belongs to some wider field."
Perhaps that is what all books on cricket are trying to say.
Blunden had a robust sense of humour. In Hong Kong he relished linguistic misunderstandings such as those of the restaurant that offered "fried prawn's balls" and the schoolboy who wrote, "In Hong Kong there is a queer at every bus-stop."
His fellow poets' regard for Blunden was illustrated by the contributions to a dinner in his honour for which poems were specially written by Cecil Day-Lewis and William Plomer; T. S. Eliot and Walter de la Mare were guests; and Siegfried Sassoon provided the Burgundy.
Honours.
Blunden's public honours included the C.B.E., 1951; the Queen's Gold Medal for Poetry, 1956; The Royal Society of Literature's Benson Medal; the Order of the Rising Sun, 3rd Class (Japan), 1963; and Honorary Membership of the Japan Academy.
On 11 November 1985, Blunden was among 16 Great War poets commemorated on a slate stone unveiled in Poets' Corner in Westminster Abbey The inscription on the stone was written by fellow Great War poet, Wilfred Owen. It reads: "My subject is War, and the pity of War. The Poetry is in the pity."
Works.
Blunden's output was prolific. To those who thought he published too much he quoted Walter de la Mare's observation that time was the poet's best editor. His books of poetry include "Poems 1913 and 1914" (1914); "Poems Translated from the French" (1914); "Three Poems" (1916); "The Barn" (1916); "The Silver Bird of Herndyke Mill; Stane Street; The Gods of the World Beneath," (1916); "The Harbingers" (1916); "Pastorals" (1916); "The Waggoner and Other Poems" (1920); "The Shepherd, and Other Poems of Peace and War" (1922); "Old Homes" (1922); "To Nature: New Poems" (1923); "Dead Letters" (1923); "Masks of Time: A New Collection of Poems Principally Meditative" (1925); "Japanese Garland" (1928); "Retreat" (1928); "Winter Nights: A Reminiscence" (1928); "Near and Far: New Poems" (1929); "A Summer's Fancy" (1930); "To Themis: Poems on Famous Trials" (1931); "Constantia and Francis: An Autumn Evening," (1931); "Halfway House: A Miscellany of New Poems," (1932); "Choice or Chance: New Poems" (1934); "Verses: To H. R. H. The Duke of Windsor," (1936); "An Elegy and Other Poems" (1937); "On Several Occasions" (1938); "Poems, 1930–1940" (1940); "Shells by a Stream" (1944); "After the Bombing, and Other Short Poems" (1949); "Eastward: A Selection of Verses Original and Translated" (1950); "Records of Friendship" (1950); "A Hong Kong House" (1959); "Poems on Japan" (1967).
"Artists Rifles", an audiobook CD published in 2004, includes a reading of "Concert Party, Busseboom" by Blunden himself, recorded in 1964 by the British Council. Other Great War poets heard on the CD include Siegfried Sassoon, Edgell Rickword, Robert Graves, David Jones and Lawrence Binyon. Blunden can also be heard on "Memorial Tablet", an audiobook of readings by Sassoon issued in 2003.

</doc>
<doc id="49793" url="https://en.wikipedia.org/wiki?curid=49793" title="Richard Aldington">
Richard Aldington

Richard Aldington (8 July 1892 – 27 July 1962), born Edward Godfree Aldington, was an English writer and poet.
Aldington was known best for his World War I poetry, the 1929 novel, "Death of a Hero", and the controversy resulting from his 1955 "Lawrence of Arabia: A Biographical Inquiry". His 1946 biography, "Wellington", was awarded the James Tait Black Memorial Prize.
Early life.
Aldington was born in Portsmouth, the son of a solicitor, and educated at Dover College, and for a year at the University of London. He was unable to complete his degree because of the financial circumstances of his family. He met the poet Hilda Doolittle in 1911 and they married two years later.
Imagism.
Aldington's poetry was associated with the Imagist group, and his poetry forms almost one third of the Imagists' inaugural anthology "Des Imagistes" (1914). Ezra Pound had in fact coined the term "imagistes" for H.D. and Aldington, in 1912. At this time Aldington's poetry was unrhymed free verse, whereas later in his verse the cadences are long and voluptuous, the imagery weighted with ornament. He was one of the poets around the proto-Imagist T. E. Hulme; Robert Ferguson in his life of Hulme portrays Aldington as too squeamish to approve of Hulme's robust approach, particularly to women. However, Aldington shared Hulme's conviction that experimentation with traditional Japanese verse forms could provide a way forward for "avant-garde" literature in English, and went often to the British Museum to examine Nishiki-e prints illustrating such poetry.
He knew Wyndham Lewis well, also, reviewing his work in "The Egoist" at this time, hanging a Lewis portfolio around the room and on a similar note of tension between the domestic and the small circle of London modernists regretting having lent Lewis his razor when the latter announced with hindsight a venereal infection. Going out without a hat, and an interest in Fabian socialism, were perhaps unconventional enough for him.
At this time he was also an associate of Ford Madox Ford, helping him with a hack propaganda volume for a government commission in 1914 and taking dictation for "The Good Soldier" when H.D. found it too harrowing.
In 1915, Aldington and H.D. relocated within London, away from Holland Park very near Ezra Pound and Dorothy, to Hampstead, close to D. H. Lawrence and Frieda. Their relationship became strained by external romantic interests and the stillborn birth of their child. Between 1914 and 1916 he was literary editor of "The Egoist", and columnist there. He was assistant editor with Leonard Compton-Rickett under Dora Marsden.
Harriet Monroe considered 'Choricos'[http://www.bartleby.com/265/11.html], Aldington's finest poem, 'one of the most beautiful death songs in the language' 'a poem of studied and affected gravity'
The gap between the Imagist and Futurist groups was defined partly by Aldington's critical disapproval of the poetry of Filippo Marinetti.
World War I and aftermath.
He joined the army in 1916, was commissioned in the Royal Sussex Regiment during 1917 and was wounded on the Western Front. Aldington never completely recovered from his war experiences, and may have continued to suffer from the then-unrecognised phenomenon of posttraumatic stress disorder.
Aldington and H. D. attempted to mend their marriage in 1919, after the birth of her daughter by a friend of writer D. H. Lawrence, named Cecil Gray, with whom she had become involved and lived with while Aldington was at war. However, she was by this time deeply involved in a lesbian relationship with the wealthy writer Bryher, and she and Aldington formally separated, both becoming romantically involved with other people, but they did not divorce until 1938. They remained friends, however, for the rest of their lives.
Relationship with T. S. Eliot.
He helped T. S. Eliot in a practical way, by persuading Harriet Shaw Weaver to appoint Eliot as his successor at "The Egoist" (helped by Pound), and later in 1919 with an introduction to the editor Bruce Richmond of "The Times Literary Supplement", for which he reviewed French literature. He was on the editorial board, with Conrad Aiken, Eliot, Lewis and Aldous Huxley, of Chaman Lall's London literary quarterly "Coterie" published 1919–1921. With Lady Ottoline Morrell, Leonard Woolf and Harry Norton he took part in Ezra Pound's scheme to "get Eliot out of the bank" (Eliot had a job in the international department of Lloyd's, a London bank, and well-meaning friends wanted him full-time writing poetry). This manoeuvre towards Bloomsbury came to little, with Eliot getting £50 and unwelcome publicity in the "Liverpool Post", but gave Lytton Strachey an opening for mockery.
Aldington made an effort with "A Fool i' the Forest" (1924) to reply to the new style of poetry initiated by "The Waste Land". He was being published at the time, for example in "The Chapbook", but clearly took on too much hack work just to live. He suffered some sort of breakdown in 1925. His interest in poetry waned, and he was straighforwardly envious of Eliot's celebrity.
His attitude towards Eliot shifted, from someone who would mind the Eliots' cat in his cottage (near Reading, Berkshire, during 1921), and to whom Eliot could confide his self-diagnosis of abulia. Aldington became a supporter of Vivienne Eliot in the troubled marriage, and savagely satirized her husband as "Jeremy Cibber" in "Stepping Heavenward" (Florence 1931). He was at this time living with Arabella Yorke (real given name Dorothy), a lover since Mecklenburgh Square days. It was a lengthy and passionate relationship, coming to an end when he went abroad.
Later life.
He went into self-imposed "exile" from England in 1928. He lived in Paris for years, living with Brigit Patmore, and being fascinated by Nancy Cunard whom he met in 1928. After his divorce in 1938 he married Netta, McCullough, previously Brigit's daughter-in-law as Mrs. Michael Patmore.
"Death of a Hero", published in 1929, was his literary response to the war, commended by Lawrence Durrell as "the best war novel of the epoch". It was written while he was living on the island of Port-Cros in Provence as a development of a manuscript from a decade before. Opening with a letter to the playwright Halcott Glover, the book takes a variable but generally satirical, cynical and critical posture, and belabours Victorian and Edwardian cant. He went on to publish several works of fiction.
In 1930, he published a bawdy translation of "The Decameron". In 1933, his novel titled "All Men are Enemies" appeared; it was a romance, as the author chose to term it, and a brighter book than "Death of a Hero", even though Aldington took an anti-war stance again. In 1942, having relocated to the United States with his new wife Netta Patmore, he began to write biographies. The first was one of Wellington ("The Duke: Being an Account of the Life & Achievements of Arthur Wellesley, 1st Duke of Wellington," 1943). It was followed by works on D. H. Lawrence ("Portrait of a Genius, But...," 1950), Robert Louis Stevenson ("Portrait of a Rebel," 1957), and T. E. Lawrence ("Lawrence of Arabia: A Biographical Inquiry," 1955).
Aldington's biography of T. E. Lawrence caused a scandal on its publication, and an immediate backlash. It made many controversial assertions. He was the first to bring to public notice the fact of Lawrence's illegitimacy and also asserted that Lawrence was homosexual. Lawrence lived a celibate life, and the claim was contested by some of his close friends (of whom several were homosexual). He attacked Lawrence as a liar, a charlatan and an "impudent mythomaniac", claims which have coloured Lawrence's reputation ever since. Only later were confidential government files concerning Lawrence's career released, allowing the accuracy of Lawrence's own account to be gauged. Aldington's own reputation has never fully recovered from what came to be seen as a venomous attack upon Lawrence's reputation. Many believed that Aldington's suffering in the bloodbath of Europe during World War I caused him to resent Lawrence's reputation, gained in the Middle Eastern arena.
Aldington died in Sury-en-Vaux, Cher, France on 27 July 1962, shortly after being honoured and feted in Moscow on the occasion of his seventieth birthday and the publication of some of his novels in Russian translation. He did not approve of the Communist "party line", though, and the Russians did not succeed in making him endorse it. His politics had in fact moved far towards the right, but he had felt shut out by the British establishment after his T. E. Lawrence book. He lived in Provence, at Montpellier, Aix-en-Provence and Sury-en-Vaux.
On 11 November 1985, Aldington was among 16 Great War poets commemorated on a slate stone unveiled in Westminster Abbey's Poet's Corner. The inscription on the stone is a quotation from the work of a fellow Great War poet, Wilfred Owen. It reads: "My subject is War, and the pity of War. The Poetry is in the pity."
A savage style and embitterment.
Aldington could be very critical. He accused the Georgian poets of being "regional in their outlook and in love with littleness. They took a little trip for a little weekend to a little cottage where they wrote a little poem on a little theme." He also, however, provided aid and support to other literary figures, even those, such as the alcoholic Harold Monro, whose work he attacked most viciously.
Alec Waugh, who met him through Harold Monro, described him as embittered by the war, and offered Douglas Goldring as comparison; but took it that he worked off his spleen in novels like "The Colonel's Daughter" (1931), rather than letting it poison his life. His novels in fact contained thinly-veiled, disconcerting (at least to the subjects) portraits of some of his friends (Eliot, D. H. Lawrence, Pound in particular), the friendship not always surviving. Lyndall Gordon characterises the sketch of Eliot in the memoirs "Life for Life's Sake" (1941) as 'snide'. As a young man he enjoyed being cutting about William Butler Yeats, but remained on good enough terms to visit him in later years at Rapallo.
His obituary in "The Times" in 1962 described him as "an angry young man of the generation before they became fashionable", and who '" remained something of an angry old man to the end".
"The Religion of Beauty".
"The Religion of Beauty" (subtitle "Selections From the Aesthetes") was a prose and poetry anthology edited by Aldington and published in 1950. Listed below are the authors Aldington included, providing insight into Aldingtons generation and tastes:
Prose.
Aubrey Beardsley – Max Beerbohm – Vernon Lee – Edward MacCurdy – Fiona MacLeod – George Meredith – Alice Meynell – George Moore – William Morris – Frederic W. H. Myers – Walter Pater – Robert Ross – Dante Gabriel Rossetti – John Ruskin – John Addington Symonds – Arthur Symons – Rachel Annand Taylor – James McNeill Whistler
Poetry.
William Allingham – Henry C. Beeching – Oliver Madox Brown – Olive Custance – John Davidson – Austin Dobson – Lord Alfred Douglas – Evelyn Douglas – Edward Dowden – Ernest Dowson – Michael Field – Norman Gale – Edmund Gosse – John Gray – William Ernest Henley – Gerard Manley Hopkins – Herbert P. Horne – Lionel Johnson – Andrew Lang – Eugene Lee-Hamilton – Maurice Hewlett – Edward Cracroft Lefroy – Arran and Isla Leigh – Amy Levy – John William Mackail – Digby Mackworth Dolben – Fiona MacLeod – Frank T. Marzials – Théophile Julius Henry Marzials – George Meredith – Alice Meynell – Cosmo Monkhouse – George Moore – William Morris – Frederic W. H. Myers – Roden Noël – John Payne – Victor Plarr – A. Mary F. Robinson – William Caldwell Roscoe – Christina Rossetti – Dante Gabriel Rossetti – Algernon Charles Swinburne – John Addington Symonds – Arthur Symons – Rachel Annand Taylor – Francis Thompson – John Todhunter – Herbert Trench – John Leicester Warren, Lord de Tabley – Rosamund Marriott Watson – Theodore Watts-Dunton – Oscar Wilde – Margaret L. Woods – Theodore Wratislaw – W. B. Yeats

</doc>
<doc id="49794" url="https://en.wikipedia.org/wiki?curid=49794" title="Rod">
Rod

Rod, Ród, Rőd, Rød, Röd, ROD, or R.O.D. may refer to:

</doc>
<doc id="49796" url="https://en.wikipedia.org/wiki?curid=49796" title="Brighton">
Brighton

Brighton is a seaside resort and the largest part of the City of Brighton and Hove situated in East Sussex, England. Historically in the Rape of Lewes in Sussex, Brighton forms a part of the Brighton/Worthing/Littlehampton conurbation. Brighton is at the heart of the Greater Brighton City Region, a partnership of local authorities and other organisations that signifies Brighton's wider regional economic significance.
Archaeological evidence of settlement in the area dates back to the Bronze Age, Roman and Anglo-Saxon periods. The ancient settlement of "Brighthelmstone" was documented in the "Domesday Book" (1086). The town's importance grew during the Middle Ages as the Old Town developed, but it languished in the early modern period, affected by foreign attacks, storms, a suffering economy and a declining population. During the modern period, Brighton began to attract more visitors following improved road transport to London and becoming a boarding point for boats travelling to France. The town also developed in popularity as a health resort for sea bathing as a purported cure for illnesses.
In the Georgian era, Brighton developed as a fashionable seaside resort, encouraged by the patronage of the Prince Regent (later King George IV), who spent much time in the town and constructed the Royal Pavilion during the early part of his Regency. Brighton continued to grow as a major centre of tourism following the arrival of the railways in 1841, becoming a popular destination for day-trippers from London. Many of the major attractions were built during the Victorian era, including the Grand Hotel, the West Pier, and the Brighton Palace Pier. The town continued to grow into the 20th century, expanding to incorporate more areas into the town's boundaries before joining the town of Hove to form the unitary authority of Brighton and Hove in 1997, which was granted city status in 2000.
Brighton's location has made it a popular destination for tourists, renowned for its diverse communities, quirky shopping areas, large cultural, music and arts scene and its large LGBT population, leading to its reverence as the "gay capital of the UK." Brighton attracts over 8.5 million visitors annually and is the most popular seaside destination in the UK for overseas tourists. Brighton has also been called the UK's "hippest city", and the "the happiest place to live in the UK".
Etymology.
Brighton's earliest name was "Bristelmestune", recorded in the "Domesday Book". Although more than 40 variations have been documented, "Brighthelmstone" (or "Brighthelmston") was the standard rendering between the 14th and 18th centuries.
Brighton was originally an informal shortened form, first seen in 1660; it gradually supplanted the longer name, and was in general use from the late 18th century. "Brighthelmstone" was the town's official name until 1810, though. The name is of Saxon origin. Most scholars believe that it derives from "Beorthelm" + "tūn"—the homestead of Beorthelm, a common Saxon name associated with villages elsewhere in England. The "tūn" element is common in Sussex, especially on the coast, although it occurs infrequently in combination with a personal name. An alternative etymology taken from the Saxon words for "stony valley" is sometimes given but has less acceptance. "Brighthelm" gives its name to, among other things, a church and a pub in Brighton and some halls of residence at the University of Sussex. Writing in 1950, historian Antony Dale noted that unnamed antiquaries had suggested an Old English word "brist" or "briz", meaning "divided", could have contributed the first part of the historic name Brighthelmstone. The town was originally split in half by the Wellesbourne, a winterbourne which was culverted and buried in the 18th century.
Brighton has several nicknames. Poet Horace Smith called it "The Queen of Watering Places", which is still widely used, and "Old Ocean's Bauble". Novelist William Makepeace Thackeray referred to "Doctor Brighton", calling the town "one of the best of Physicians". "London-by-Sea" is well-known, reflecting Brighton's popularity with Londoners as a day-trip resort, a commuter dormitory and a desirable destination for those wanting to move out of the metropolis. "The Queen of Slaughtering Places", a pun on Smith's description, became popular when the Brighton trunk murders came to the public's attention in the 1930s. The mid 19th-century nickname "School Town" referred to the remarkable number of boarding, charity and church schools in the town at the time.
History.
The first settlement in the Brighton area was Whitehawk Camp, a Neolithic encampment on Whitehawk Hill which has been dated to between 3500 BC and 2700 BC. It is one of six causewayed enclosures in Sussex. Archaeologists have only partially explored it, but have found numerous burial mounds, tools and bones, suggesting it was a place of some importance. There was also a Bronze Age settlement at Coldean. Brythonic Celts arrived in Britain in the 7th century BC, and an important Brythonic settlement existed at Hollingbury Camp on Hollingbury Hill. This Celtic Iron Age encampment dates from the 3rd or 2nd century BC and is circumscribed by substantial earthwork outer walls with a diameter of . Cissbury Ring, roughly from Hollingbury, is suggested to have been the tribal "capital".
Later, there was a Roman villa at Preston Village, a Roman road from London ran nearby, and much physical evidence of Roman occupation has been discovered locally. From the 1st century AD, the Romans built a number of villas in Brighton and Romano-British Brythonic Celts formed farming settlements in the area. After the Romans left in the early 4th century AD, the Brighton area returned to the control of the native Celts. Anglo-Saxons then invaded in the late 5th century AD, and the region became part of the Kingdom of Sussex, founded in 477 AD by king Ælle.
Anthony Seldon identified five phases of development in pre-20th century Brighton. The village of "Bristelmestune" was founded by these Anglo-Saxon invaders, probably in the early Saxon period. They were attracted by the easy access for boats, sheltered areas of raised land for building, and better conditions compared to the damp, cold and misty Weald to the north. By the time of the Domesday survey in 1086 it was a fishing and agricultural settlement, a rent of 4,000 herring was established, and its population was about 400. Its importance grew from the Norman era onwards. By the 14th century there was a parish church, a market and rudimentary law enforcement (the first town constable was elected in 1285). Sacked and burnt by French invaders in the early 16th century—the earliest depiction of Brighton, a painting of 1520, shows Admiral Pregent de Bidoux's attack of June 1514—the town recovered strongly based on a thriving mackerel-fishing industry. The grid of streets in the Old Town (the present Lanes area) were well developed and the town grew quickly: the population rose from 1,500 in 1600 to 4,000 in the 1640s. By that time Brighton was Sussex's most populous and important town.
Over the next few decades, though, events severely affected its local and national standing, such that by 1730 "it was a forlorn town decidedly down on its luck". More foreign attacks, storms (especially the devastating Great Storm of 1703), a declining fishing industry, and the emergence of nearby Shoreham as a significant port caused its economy to suffer. By 1708 other parishes in Sussex were charged rates to alleviate poverty in Brighton, and Daniel Defoe wrote that the expected £8,000 cost of providing sea defences was "more than the whole town was worth". The population declined to 2,000 in the early 18th century.
From the 1730s, Brighton entered its second phase of development—one which brought a rapid improvement in its fortunes. The contemporary fad for drinking and bathing in seawater as a purported cure for illnesses was enthusiastically encouraged by Dr Richard Russell from nearby Lewes. He sent many patients to "take the cure" in the sea at Brighton, published a popular treatise on the subject, and moved to the town soon afterwards (the Royal Albion, one of Brighton's early hotels, occupies the site of his house). Others were already visiting the town for recreational purposes before Russell became famous, and his actions coincided with other developments which made Brighton more attractive to visitors. From the 1760s it was a boarding point for boats travelling to France; road transport to London was improved when the main road via Crawley was turnpiked in 1770; and spas and indoor baths were opened by other entrepreneurial physicians such as Sake Dean Mahomed and Anthony Relhan (who also wrote the town's first guidebook).
From 1780, development of the Georgian terraces had started, and the fishing village developed as the fashionable resort of Brighton. Growth of the town was further encouraged by the patronage of the Prince Regent (later King George IV) after his first visit in 1783. He spent much of his leisure time in the town and constructed the Royal Pavilion during the early part of his Regency. In this period the modern form of the name Brighton came into common use.
The arrival of the London and Brighton Railway in 1841 brought Brighton within the reach of day-trippers from London. The population grew from around 7,000 in 1801 to more than 120,000 by 1901. Many of the major attractions were built during the Victorian era, such as the Grand Hotel (1864), the West Pier (1866), and the Palace Pier (1899). Prior to either of these structures, the famous Chain Pier was built, to the designs of Captain Samuel Brown. It lasted from 1823 to 1896, and is featured in paintings by both Turner and Constable.
Because of boundary changes, the land area of Brighton expanded from 1,640 acres (7 km2) in 1854 to 14,347 acres (58 km2) in 1952. New housing estates were established in the acquired areas, including Moulsecoomb, Bevendean, Coldean and Whitehawk. The major expansion of 1928 also incorporated the villages of Patcham, Ovingdean and Rottingdean, and much council housing was built in parts of Woodingdean after the Second World War.
Gentrification since then has made Brighton more fashionable again. Recent housing in North Laine, for instance, has been designed in keeping with the area.
In 1997, Brighton and Hove were joined to form the unitary authority of Brighton and Hove, which was granted city status by Queen Elizabeth II as part of the millennium celebrations in 2000.
Geography and topography.
Brighton lies between the South Downs and the English Channel to the north and south, respectively. The Sussex coast forms a wide, shallow bay between the headlands of Selsey Bill and Beachy Head; Brighton developed near the centre of this bay around a seasonal river, the Wellesbourne (or Whalesbone), which flowed from the South Downs above Patcham. This emptied into the English Channel at the beach near the East Cliff, forming "the natural drainage point for Brighton".
Behind the estuary was a stagnant pond called the Pool or Poole, so named since the medieval era. This was built over with houses and shops from 1793, when the Wellesbourne was culverted to prevent flooding, and only the name of the road (Pool Valley, originally Pool Lane) marks its site. One original house survives from the time of the pool's enclosure. Behind Pool Valley is Old Steine (historically "The Steyne"), originally a flat and marshy area where fishermen dried their nets. The Wellesbourne occasionally reappears during times of prolonged heavy rain; author Mark Antony Lower referred to an early 19th-century drawing of the Royal Pavilion showing "quite a pool of water across the Steyne".
Despite 16th-century writer Andrew Boorde's claim that "Bryght-Hempston among the noble ports and havens of the realm", Brighton never developed as a significant port: rather, it was considered as part of Shoreham. Nevertheless, the descriptions "Port of Brighthelmston" or "Port of Brighton" were sometimes used between the 14th and 19th centuries, as for example in 1766 when its notional limits were defined for customs purposes.
The East Cliff runs for several miles from Pool Valley towards Rottingdean and Saltdean, reaching above sea level. The soil beneath it, a mixture of alluvium and clay with some flint and chalk rubble, has experienced erosion for many years. The cliff itself, like the rest of Brighton's soil, is chalk. Below this are thin layers of Upper and Lower Greensand separated by a thicker band of Gault clay. The land slopes upwards gradually from south to north towards the top of the Downs.
Main transport links developed along the floor of the Wellesbourne valley, from which the land climbs steeply—particularly on the east side. The earliest settlement was by the beach at the bottom of the valley, which was partly protected from erosion by an underwater shale-bar. Changes in sea level affected the foreshore several times: disappeared in the first half of the 14th century, and the Great Storm of 1703 caused widespread destruction. The first sea defences were erected in 1723, and a century later a long sea-wall was built.
Climate.
Brighton has a temperate climate: its is "Cfb". It is characterised by mild, calm weather with high levels of sunshine, sea breezes and a "healthy, bracing air" attributed to the low level of tree cover. Average rainfall levels increase as the land rises: the 1958–1990 mean was on the seafront and about at the top of the South Downs above Brighton. Storms caused serious damage in 1703, 1806, 1824, 1836, 1848, 1850, 1896, 1910 and 1987. Snow is rare, but particularly severe falls were recorded in 1881 and 1967.
Boundaries and areas.
At the time of the Domesday survey in 1086, Brighton was in the Rape of Lewes and the Hundred of Welesmere. The new Hundred of Whalesbone, which covered the parishes of Brighton, West Blatchington, Preston and Hove, was formed in 1296. Parishes moved in and out several times, and by 1801 only Brighton and West Blatchington were included in the Hundred.
Brighton's ecclesiastical and civil parish boundaries were coterminous until 1873. Since then, the latter have changed several times as the urban area has expanded. In its original form, Brighton covered about between the English Channel, Hove, Preston, Ovingdean and Rottingdean. The civil parish was first extended from 31 October 1873, when was annexed from Preston. Its ecclesiastical parish was not affected.
On 1 October 1923, were added to Brighton from Patcham parish: Brighton Corporation was developing the Moulsecoomb council estate there at the time. On 1 April 1928, Brighton became a county borough and grew by nearly five times by adding Ovingdean and Rottingdean parishes in their entirety and parts of Falmer, Patcham and West Blatchington. From 1 April 1952, more of Falmer and part of the adjacent Stanmer parish were added; 20 years later, land and marine territory associated with the new Brighton Marina development also became part of Brighton. Except for a small addition of rural land in 1993 (from Pyecombe parish), Brighton Borough's boundaries remained the same until it was joined to Hove Borough in 1997 to form the unitary authority of Brighton and Hove.
The old boundary between Brighton and Hove is most clearly seen on the seafront, where the King Edward Peace Statue (1912) straddles the border, and in a twitten called Boundary Passage which runs northwards from Western Road to Montpelier Road. There is a Grade II-listed parish boundary marker stone in this passageway. Between Western Road and the seafront, the boundary runs up Little Western Street (pavement on eastern side, in Brighton), but it is not visible. Northwards from Western Road, it runs to the west of Norfolk Road, Norfolk Terrace, Windlesham Road and Windlesham Gardens in the Montpelier area, then along the south side of Davigdor Road to Seven Dials. From there it runs along the west side of Dyke Road as far as Withdean Road in Withdean, at which point it crosses Dyke Road so that the section north of that is part of Hove parish. The boundary continues to follow Dyke Road towards Devil's Dyke on the South Downs.
Governance and politics.
Brighton is covered by two constituencies in the Parliament of the United Kingdom: Brighton Kemptown and Brighton Pavilion. Both are marginal constituencies which were held by Labour from 1997 to 2010. At the 2010 general election, Brighton Kemptown elected the Conservative MP Simon Kirby, while Brighton Pavilion elected Caroline Lucas, the first Green Party MP elected to Westminster. In European elections, Brighton is part of the European Parliament constituency of South-East England.
As of , there are 21 wards in the city of Brighton and Hove, of which 12 are in Brighton. Regency, St Peter's & North Laine, Preston Park, Withdean, Patcham, Hollingdean & Stanmer and Hanover & Elm Grove are part of the Brighton Pavilion constituency; Moulsecoomb & Bevendean, Queen's Park, East Brighton, Woodingdean and Rottingdean Coastal are covered by the Brighton Kemptown constituency.
The newly created Borough of Brighton consisted of six wards in 1854: St Nicholas, St Peter, Pier, Park, Pavilion and West. When the territory was extended to include part of Preston parish in 1873, the new area became a seventh ward named Preston. The seven were split into 14 in 1894: Hanover, Kemp Town (renamed King's Cliff in 1908), Lewes Road, Montpelier, Pavilion, Pier, Preston, Preston Park, Queen's Park, Regency, St John, St Nicholas, St Peter, and West. Preston ward was extended in 1923 to incorporate the area taken into the borough from Patcham parish in 1923 for the construction of the Moulsecoomb estate, and in 1928 the ward was divided into four: Hollingbury, Moulsecoomb, Preston and Preston Park. Elm Grove and Patcham wards were created at the same time, bringing the total to 19. There were further changes in 1952, 1955 and 1983, at which time there were 16 wards. This situation continued until 1 April 1997, when Hove and its wards became part of the new unitary authority of Brighton and Hove.
Brighton Town Hall occupies a large site in The Lanes. Medieval Brighthelmston had a town hall, although it was called the Townhouse and functioned more like a market hall. A later building (1727) known as the Town Hall was principally used as a workhouse. Work on the first purpose-built town hall began in 1830; Thomas Read Kemp laid the first stone, and Thomas Cooper designed it on behalf of the Brighton Town Commissioners (of which he was a member). Brighton Corporation spent £40,000 to extend it in 1897–99 to the Classical design of Brighton Borough Surveyor Francis May. Despite this, the building was too small for municipal requirements by the mid-20th century, and extra council buildings were built in various locations throughout Brighton Borough Council's existence: the most recent, Bartholomew House and Priory House next to the town hall, were finished in 1987. The town hall ceased to be responsible solely for Brighton's affairs when Brighton and Hove were united in 1997, but it is still used by Brighton & Hove City Council—particularly for weddings and civil ceremonies.
The presence of a British subsidiary of the United States arms company EDO Corporation on the Home Farm Industrial Estate in Moulsecoomb has been the cause of protests since 2004. The premises were significantly damaged in January 2009 when protesters broke in.
Economy.
In 1985, the Borough Council described three "myths" about Brighton's economy. Common beliefs were that most of the working population commuted to London every day; that tourism provided most of Brighton's jobs and income; or that the borough's residents were "composed entirely of wealthy theatricals and retired businesspeople" rather than workers. Brighton has been an important centre for commerce and employment since the 18th century. It is home to several major companies, some of which employ thousands of people locally; as a retail centre it is of regional importance; creative, digital and new media businesses are increasingly significant; and, although Brighton was never a major industrial centre, its railway works contributed to Britain's rail industry in the 19th and 20th centuries, particularly in the manufacture of steam locomotives.
Since the amalgamation of Brighton and Hove, economic and retail data has been produced at a citywide level only. Examples of statistics include: Brighton and Hove's tourism industry contributes £380m to the economy and employs 20,000 people directly or indirectly; the city has 9,600 registered companies; and a 2001 report identified it as one of five "supercities for the future". In December 2013, Brighton was the third-highest ranked place on the UK Vitality Index Report, which measures the economic strength of towns and cities in the United Kingdom. It was "among the top performing towns and cities on almost all" of the 20 measures used by the index.
Commerce and industry.
Brighton's largest private sector employer is American Express, whose European headquarters—the Amex House at Carlton Hill—opened in 1977. , 3,500 people worked there. Planning permission to demolish the offices and build a replacement was granted in 2009, and work started in March 2010. The £130 million scheme is expected to support 1,000 jobs in the construction industry. Other major employers include Lloyds Bank, Asda (which has hypermarkets at Hollingbury and Brighton Marina), Brighton & Hove Bus and Coach Company and call-centre operator Inkfish. In 2012, it was reported that about 1,500 of Gatwick Airport's 21,000 workers lived in the city of Brighton and Hove.
Brighton is a popular destination for conferences, exhibitions and trade fairs, and has had a purpose-built conference centre—the Brighton Centre—since 1977. Direct income from the Brighton Centre's 160 events per year is £8 million, and a further £50 million is generated indirectly by visitors spending money during their stay. Events range from political party conferences to concerts.
The Hollingbury Industrial Estate is one of the largest such facilities in Brighton; in its early days about 6,000 people were employed, principally in industrial jobs, but in the late 20th and early 21st centuries its focus has switched to commercial and retail development, limiting Brighton's potential for industrial growth. Brighton Corporation laid out the estate on of land around Crowhurst Road in 1950. By 1956, large-scale employment was provided at a bakery, a typewriter factory and a machine tools manufacturer among others. Most of the large factories closed during the recessions of the 1980s and 1990s, employment fell to 1,000, and structural changes started in the mid-1980s with a move towards small-scale industrial units (the Enterprise Estate was finished in October 1985) and then retail warehouses. Asda's superstore opened in November 1987, MFI followed two years later, and other retail units were built in the 1990s. Two large headquarters buildings were vacated in quick succession when British Bookshops left in March 2011 and "The Argus" newspaper moved out of its headquarters in 2012—although the Brighton & Hove Bus and Coach Company signed a contract to move its 1,250 employees into the latter building.
Brighton has a high density of businesses involved in the media sector, particularly digital or "new media", and since the 1990s has been referred to as "Silicon Beach". By 2007, over 250 new media business had been founded in Brighton. Brandwatch is a social media monitoring company based in offices near Brighton station. Computer game design company Black Rock Studio was founded in 1998 and was taken over by Disney Interactive Studios, who closed it down in 2011. The Gamer Network, whose portfolio of websites relating to computer gaming (including Eurogamer) and creative industries was founded in 1999, is based in Brighton.
By the early 21st century, the market for office accommodation in the city was characterised by fluctuating demand and a lack of supply of high-quality buildings. As an example, the Trafalgar Place development ( 1990), "now considered a prime office location", stood partly empty for a decade. Exion 27 (built in 2001), a high-tech, energy-efficient office development at Hollingbury, remained empty for several years and is still not in commercial use: it houses some administrative departments of the University of Brighton. It was Brighton's first ultramodern commercial property and was intended for mixed commercial and industrial use, but its completion coincided with a slump in demand for high-tech premises.
Retail.
The Lanes form a retail, leisure and residential area near the seafront, characterised by narrow alleyways following the street pattern of the original fishing village. The Lanes contain predominantly clothing stores, jewellers, antique shops, restaurants and pubs. The North Laine area is a retail, leisure and residential area immediately north of the Lanes. Its name derives from the Anglo-Saxon "Laine" meaning "fields", although the misnomer "North Lanes" is often used to describe the area. The North Laine contains a mix of businesses dominated by cafés, independent and avant-garde shops, bars and theatres.
Churchill Square is a shopping centre with a floor space of and over 80 shops, several restaurants and 1,600 car-parking spaces. It was built in the 1960s as an open-air, multi-level pedestrianised shopping centre, but was rebuilt and enlarged in 1998 and is no longer open-air. Further retail areas include Western Road and London Road, the latter of which is currently undergoing extensive regeneration in the form of new housing and commercial properties.
Landmarks.
The Royal Pavilion is a former royal palace built as a home for the Prince Regent during the early 19th century, under the direction of the architect John Nash, and is notable for its Indo-Saracenic architecture and Oriental interior. Other Indo-Saracenic buildings in Brighton include the Sassoon Mausoleum, now, with the bodies reburied elsewhere, in use as a chic supper club.
Brighton Marine Palace and Pier (long known as the Palace Pier) opened in 1899. It features a funfair, restaurants and arcade halls.
The West Pier was built in 1866 and is one of only two Grade I listed piers in the United Kingdom. It has been closed since 1975. For some time it was under consideration for restoration, but two fires in 2003, and other setbacks, led to these plans being abandoned. Plans for a new landmark in its place – the i360, a observation tower designed by London Eye architects Marks Barfield – were announced in June 2006. Plans were approved by the council on 11 October 2006. Construction started in summer 2014 and it is expected to open in summer 2016.
Brighton clocktower, built in 1888 for Queen Victoria's jubilee, stands at the intersection of Brighton's busiest thoroughfares.
Volk's Electric Railway runs along the inland edge of the beach from Brighton Pier to Black Rock and Brighton Marina. It was created in 1883 and is the world's oldest operating electric railway.
The Grand Hotel was built in 1864. The Brighton hotel bombing occurred there. Its nighttime blue lighting is particularly prominent along the foreshore.
The Brighton Wheel opened with some controversy, directly north east of the Brighton Marine Palace and Pier in October 2011 after a previous attempt to locate it in a more central location near the Metropole Hotel, at which time it was to have been the "Brighton O" – a special spokeless design rather than the traditional spoked wheel eventually purchased from its previous home in South Africa.
The Brighton i360 observation tower is expected to be completed in 2016. At 162 metres (531.49 feet) high, and with an observation pod rising to 138 metres (452.75 feet), the i360 will be Britain's highest observation tower outside London – taller even than the London Eye.
Churches and places of worship.
The 11th century (1086) St Nicholas Church is the oldest building in Brighton, commonly known as "The Mother Church". Other notable churches include the very tall brick-built St Bartholomew's (1874) designed by the architect Edmund Scott, St Peter's (1828), and St. Martin's, noted for its decorated interior. Brighton's Quakers run the Friends' Meeting House in the Lanes. There is an active Unitarian community based in a Grade 2 listed building in New Road, and a Spiritualist church in Norfolk Square. There are also a number of New Age outlets and groups.
Brighton has three synagogues: the Middle Street Synagogue is a Grade II-listed building built in 1874–75. It is being gradually restored by English Heritage. There are also several mosques and Buddhist centres.
Brighton has become known as one of the least religious places in the UK, based upon analysis of the 2011 census which revealed that 42 per cent of the population profess no religion, far higher than the national average of 25%. As part of the Jedi census phenomenon, 2.6 per cent claimed their religion was Jedi Knight, the largest percentage in the country.
Beaches.
Brighton has a expanse of shingle beach, part of the unbroken section within the city limits. The seafront has bars, restaurants, nightclubs, sports facilities and amusement arcades, principally along the central section between the West and Palace Piers. This part is the most popular: daily visits exceed 150,000 at weekends in high summer. During a heatwave in October 2011, 200,000 tourists visited in a single weekend and spent about £5 million. Neighbouring Hove is well known for its hundreds of painted timber beach huts, but brick-walled chalets are also available on Brighton seafront, especially towards Rottingdean and Saltdean. Especially east of the Palace Pier, a flat sandy foreshore is exposed at low tide. The Palace Pier section of the beach has been awarded blue flag status. Part of the beach adjoining Madeira Drive, to the east of the city centre, has been redeveloped into a sports complex and opened to the public in March 2007, with courts for pursuits such as beach volleyball and ultimate Frisbee among others.
The city council owns all the beaches, which are divided into named sections by groynes—the first of which were completed in 1724. Eastwards from the Hove boundary, the names are Boundary, Norfolk, Bedford, Metropole, Grand (referring to the four hotels with those names), Centre, King's, Old Ship, Volk's, Albion, Palace Pier, Aquarium, Athina (where the MS "Athina B" ran aground), Paston, Banjo, Duke's, Cliff, Crescent and Black Rock. Cliff Beach is a nudist beach. Beyond Black Rock, the cliffs (part of the Brighton to Newhaven Cliffs Site of Special Scientific Interest) rise to more than and there are three small beaches at Ovingdean Gap, Rottingdean Gap and Saltdean Gap. All are connected by the Undercliff Walk, which has been affected by several cliff falls since 2000.
Since the demolition in 1978 of the Black Rock open-air lido at the eastern end of Brighton's seafront, the area has been developed and now features one of Europe's largest marinas. However, the site of the pool itself remains empty except for a skate park and graffiti wall. Since 2003 a series of developments have been proposed but have come to nothing, including housing, a five-star hotel with a winter garden, and an 11,000-seat sports arena.
The city plans to build a vertical panoramic cable car near the beach. Built near Grenoble in France by Poma, the donut-shaped machine will rise to 531 feet (162 m) and will be in Summer 2016 the highest vertical cable car in the world.
Culture.
Cinema.
Brighton featured in a number of popular movies including "Quadrophenia" (1979), "The End of the Affair" (1999), "Wimbledon" (2004), "MirrorMask" (2005), "Angus, Thongs and Perfect Snogging" (2008), "The Young Victoria" (2009), "Brighton Rock" (2010 and 1947) and "The Boat that Rocked" (2009).
The Duke of York's Picturehouse, dating from 1910, was opened by Mrs Violet Melnotte-Wyatt. It is the country's oldest purpose-built cinema and was Brightons first Electric Bioscope, which still operates as an arthouse cinema. The Duke of Yorks Picturehouse has expanded the cinema range in Brighton to include two more screens at the Komedia Theatre situated in Gardner Street central Brighton. There are two multiplex cinemas.
Festivals and rallies.
Each May the city hosts the Brighton Festival, the second largest arts festival in the UK (after Edinburgh). This includes processions such as the Children's Parade, outdoor spectaculars often involving pyrotechnics, and theatre, music and visual arts in venues throughout the city, some brought into this use exclusively for the festival. The earliest feature of the festival, the Artists' Open Houses, are homes of artists and craftspeople opened to the public as galleries, and usually selling the work of the occupants. Since 2002, these have been organised independently of the official Festival and Fringe.
Brighton Fringe runs alongside Brighton Festival, and has grown to be the second largest fringe festival in the world. Together with the street performers from Brighton Festival's "Streets of Brighton" events, and the Royal Mile-esque outdoor performances that make up "Fringe City", outdoor spectacles and events more than double during May.
Other festivals include The Great Escape, featuring three nights of live music in venues across the city; the Soundwaves Festival in June, which shows classical music composed in the 21st Century, and involves both amateur and professional performers; Paddle Round the Pier; Brighton Live which each September stages a week of free gigs in pubs to show local bands; Burning the Clocks, a winter solstice celebration; and Brighton Pride (see lesbian, gay, bisexual and transgender community, below). For a number of years, Andrew Logan's Alternative Miss World extravaganza was held in the city.
The Kemptown area has its own small annual street festival, the Kemptown Carnival, and the Hanover area similarly has a "Hanover Day".
Local resident Fatboy Slim puts on a "Big Beach Boutique" show most years. An inaugural White Nights (Nuit Blanche) all-night arts festival took place in October 2008 and continued for 4 years until it was postponed in 2012 due to a lack of European funding. 2009 saw the first Brighton Zine Fest celebrating zine and DIY culture within the city.
Brighton is the terminus of a number of London-to-Brighton rides, and runs, such as the veteran car run and bike ride. Transport rallies are also hosted on the seafront. Groups of mods and Rockers still bring their scooters and motorbikes to the town, but their gatherings are now much more sedate than the violent 1960s confrontations depicted in "Quadrophenia".
Food and drink related festivals include the traditional Blessing of the Fisheries, where barbecued mackerel are eaten on the beach and the more recent Fiery Foods Chilli Festival. There is also a twice-yearly general food festival. The main Sussex beer festival is held in nearby Hove, and there is a smaller beer festival in the Hanover area. Foodies Festival also counts Brighton as one of its seven national venues, with the event taking place between 25–27 May at Hove Lawns and including top chefs such as Loyd Grossman.
Brighton is the home of the UK's first Walk of Fame which celebrates the many rich and famous people associated with the city.
Lesbian, gay, bisexual and transgender community.
Brighton is generally agreed to be the unofficial "gay capital" of the UK, and records LGBT history in the city since the 19th century. Many LGBT pubs, clubs, bars, restaurants, cafés and shops are located around Brighton and in particular around St James's Street in Kemptown. Several LGBT charities, publishers, social and support groups are also based in the city. Brighton Pride is the largest Pride event in the UK, celebrated at the start of August and attracting around 160,000 people every year. In a 2014 estimate, 11–15% of the city's population aged 16 or over is thought to be lesbian, gay or bisexual. The city also had the highest percentage of same-sex households in the UK in 2004 and the largest number of civil partnership registrations outside London in 2013.
Museums.
Brighton museums include Brighton Museum & Art Gallery, Preston Manor, Booth Museum of Natural History, Brighton Toy and Model Museum, and Brighton Fishing Museum, the long established social epicentre of the seafront, which includes artefacts from the West Pier. The Royal Pavilion is also open to the public, serving as a museum to the British Regency.
Night-life and popular music.
Brighton has many night-life hotspots and is associated with popular musicians including Fatboy Slim, Kirk Brandon, Tim Booth, Nick Cave, Robert Smith and Jimmy Somerville. Live music venues include the Concorde2, Brighton Centre and the Brighton Dome, where ABBA received a substantial boost to their career when they won the Eurovision Song Contest 1974. Many events and performance companies operate in the city.
Popular alternative rock band The Eighties Matchbox B-Line Disaster, alternative rock duo Blood Red Shoes, indie rock band The Kooks, metalcore band Architects, hip-hop duo Rizzle Kicks and dark cabaret band Birdeatsbaby originated in Brighton.
Alternative blues rock duo Royal Blood formed in Brighton in 2013 and had a UK number 1 album with their debut in 2014.
Indie psych band The Wytches are based in Brighton and signed to Heavenly Recordings in 2014, releasing their debut album Annabel Dream Reader in August which reached the UK top 50. There are over 300 pubs in the town.
Brighton is home to many independent record labels such as FatCat Records and Memorials of Distinction.
Restaurants.
Brighton has about 400 restaurants, more per head than anywhere else in the UK outside London. A wide range of cuisines is available.
Theatre.
Theatres include the Brighton Dome and associated Pavilion Theatre, the expanded Komedia (primarily a comedy and music venue but also a theatre), the Old Market which was renovated and re-opened in 2010 and the Theatre Royal which celebrated its 200th anniversary in 2007. There are also smaller theatres such as the Marlborough Theatre, the New Venture, and the Brighton Little Theatre. The city has the new purpose built Brighton Open Air Theatre, or B•O•A•T, which is due to open for the Brighton Festival in May 2015. It is unique in that its programme will be chosen by lottery to ensure that it remains accessible and open to all comers.
Education.
Brighton & Hove City Council is responsible for 80 schools, of which 54 are in Brighton.
The University of Sussex established in 1961 is a campus university between Stanmer Park and Falmer, four miles (6 km) from the city centre. Served by frequent trains (to Falmer railway station) and 24-hour buses, it has a student population of 12,500 of which 70% are undergraduates. The university is currently ranked 21st in the UK and 110th in the world by the World University Rankings.
The University of Brighton, the former Brighton Polytechnic, has a student population of 20,017 of which 80% are undergraduates. The university is on several sites with additional buildings in Falmer, Moulsecoomb, Eastbourne and Hastings.
In 2003, the universities of Sussex and Brighton formed a medical school, known as Brighton and Sussex Medical School. The school was one of four new medical schools to be created as part of a government programme to increase the number of qualified NHS doctors. The school is based in Falmer and works closely with the Brighton and Sussex University Hospitals NHS Trust.
A range of non-university courses for students over 16, mainly in vocational education subjects, is provided at the further education college, City College Brighton and Hove. More academic subjects can be studied by 16–18-year-olds at Brighton Hove & Sussex Sixth Form College (BHASVIC) in the Seven Dials area. Varndean College in North Brighton occupies a commanding position. The 1920s building is celebrated for its façade and internal quads. The college offers academic A levels, The International Baccalaureate and vocational courses.
There are state schools and some faith schools. Notable state schools include Longhill High School, Varndean School, Patcham High School, Dorothy Stringer High School, Blatchington Mill School and Sixth Form College and Brighton Aldridge Community Academy.
There are a number of private schools, including Brighton College, Roedean School, Steiner School, BHHS and a Montessori School. As with the state schools, some independents are faith-based; Torah Academy, the last Jewish primary school, became a Pre-K/Nursery School at the end of the 2007. The Brighton Institute of Modern Music, a fully accredited music college, opened in 2001 and has since expanded to five locations throughout the UK.
In spring and summer, thousands of students from all over Europe gather to attend language courses at the many language schools.
Sport.
Brighton & Hove Albion Football Club is the city's professional football team. After playing at the Goldstone Ground for 95 years, the club spent two years ground-sharing at Gillingham before returning to the town as tenants of the Withdean Athletics Stadium. However, in 2011 the club moved to a permanent home at Falmer at the start of the 2011–12 season, with the first match in July 2011. The club's notable achievements including winning promotion to the Football League First Division for the first time in 1979, staying there for four seasons, during the last of which they reached the FA Cup Final and took Manchester United to a replay before losing 4–0. Notable former managers of the club include Brian Clough, Peter Taylor (born 1928), Peter Taylor (born 1953), Jimmy Melia, Liam Brady, Jimmy Case, Steve Gritt, Brian Horton, Steve Coppell and Mark McGhee. Notable former players include Gareth Barry, Dave Beasant, Justin Fashanu, Dennis Mortimer, Gordon Smith, Frank Stapleton, Howard Wilkinson and Bobby Zamora.
Whitehawk Football Club is a semi-professional football club based in the Whitehawk suburb of Brighton. Currently, they play in the Conference South having won promotion three times in the space four years between 2009–13. Whitehawk play their games at The Enclosed Ground, beautifully set into the South Downs, close to Brighton Marina. Notable former/current players include Sergio Torres, Jake Robinson, Matthew Lawrence and Darren Freeman.
Brighton and Hove is home to the Sussex County Cricket Club at Eaton Road in Hove.
Brighton Football Club (RFU) is one of the oldest Rugby Clubs in England.
Brighton & Hove Hockey Club is a large hockey club, with a homeground based in Hove. The men's 1XI gained promotion to the England Hockey League system, Conference East, in 2013.
Throughout the year many events take place on Madeira Drive (a piece of roadway on Brighton's seafront), which was constructed to host what is commonly held to be the world's oldest motor race, the Brighton Speed Trials, which has been running since 1905. The event is organised by the Brighton and Hove Motor Club and normally takes place on the second Saturday in September each year.
There is also an from time to time a beach soccer competition in a temporary stadium on imported sand on the beach. The inaugural contest in June 2002 featured football stars such as Eric Cantona and Matt Le Tissier.
Brighton has a horse-racing course, Brighton Racecourse, with the unusual feature that when the full length of the course is to be used, some of the grass turf of the track has to be laid over the tar at the top of Wilson Avenue, a public road, which therefore has to be closed for the races.
There is a greyhound racing circuit – the Brighton & Hove Greyhound Stadium – in Hove, run by Coral, at which Motorcycle speedway racing was staged in 1928.
Basketball team Brighton Bears were in the British Basketball League before dropping out at the end of the 2005/06 season. Their home venue was at The Triangle Leisure Centre in Burgess Hill.
Brighton Ultimate, an ultimate Frisbee team was set up in 1985.
The Brighton and Hove Pétanque Club runs an annual triples, doubles and singles competition, informal KOs, winter and summer league, plus Open competitions with other clubs. The club is affiliated to Sussex Pétanque, the local region of the English Pétanque Association, so they can also play at a Regional and National level. The Peace Statue terrain is the official pétanque terrain situated on the seafront near the West Pier.
There are yachting clubs and other boating activities run from Brighton Marina.
Brighton has two competitive swimming clubs. Brighton SC formed in 1860 claims to be the oldest swimming club in England. Brighton Dolphin SC was formed in 1891 as Brighton Ladies Swimming
Transport.
Brighton has several railway stations, many bus routes, coach services and taxis. A Rapid Transport System has been under consideration for some years. Trolleybuses, trams, ferries and hydrofoil services have operated in the past.
Brighton is connected to the national road network by the A23 (London Road) northwards, and by two east–west routes: the A259 along the coast and the A27 trunk route inland. The A23 joins the M23 motorway at Pease Pottage near Gatwick Airport. The A27 originally ran through the urban area along Old Shoreham Road and Lewes Road, but it now follows the route of the Brighton Bypass (opened in 1990) and the old alignment has become the A270.
A bypass was first proposed in 1932, six routes were submitted for approval in 1973, and the Department of the Environment published its recommended route in 1980. Public enquiries took place in 1983 and 1987, construction started in 1989 and the first section—between London Road at Patcham and the road to Devil's Dyke—opened in summer 1991. By 1985 there were about 5,000 parking spaces in central Brighton. The largest car parks are at London Road, King Street, and the Churchill Square/Regency Road/Russell Road complex. In 1969, a 520-space multi-storey car park was built beneath the central gardens of Regency Square.
Frequent trains operate from Brighton railway station. Many Brighton residents commute to work in London and destinations include London Victoria, London Bridge and St Pancras International. Most trains serve Gatwick Airport, and those operated by Thameslink continue to St Albans, Luton, Luton Airport Parkway and Bedford. The fastest service from London Victoria takes 51 minutes. The West Coastway Line serves stations to Hove, Worthing, Portsmouth and Southampton; and the East Coastway Line runs via Lewes to Newhaven, Eastbourne, Hastings and Ashford, Kent, crossing the landmark London Road viaduct en route and providing "a dramatic high-level view" of Brighton. A wider range of long-distance destinations was served until 2007–08 when rationalisation caused the ending of InterCity services via Kensington (Olympia) and Reading to Birmingham, Manchester and Edinburgh. Twice-daily long-distance services to Bristol and Great Malvern are operated by First Great Western via the West Coastway Line.
Until deregulation in 1986, bus services in Brighton were provided by Southdown Motor Services and Brighton Borough Transport under a joint arrangement called "Brighton Area Transport Services". Southdown were part of the nationalised NBC group and were based at Freshfield Road in the Kemptown area; Brighton Borough Transport were owned by the council and used the former tram depot at Lewes Road as their headquarters. Joint tickets were available and revenue was shared. The Brighton & Hove Bus Company, owned by the Go-Ahead Group since 1993, now runs most bus services in Brighton. Its fleet has about 280 buses. Compass Travel, The Big Lemon, Metrobus, Stagecoach South and The Sussex Bus also operate some services to central Brighton. The city had 1,184 bus stops in 2012, 456 of which had a shelter. Real-time travel information displays are provided at many stops.
The only park and ride facility in Brighton is based at the Withdean Stadium. It does not offer a dedicated shuttle bus service: intending passengers must join the Brighton & Hove Bus Company's route 27 service to Saltdean—which travels via Brighton railway station, the Clock Tower and Old Steine—and pay standard fares. The 20-year City Plan released in January 2013 ruled out an official park-and-ride facility, stating it would be an "inefficient use of public money, particularly in an era of declining car use". Councillors and residents in Woodingdean and Rottingdean have claimed that streets and car parks in those areas have become unofficial park-and-ride sites: drivers park for free and take buses into the city centre.
Shoreham Airport, which offers chartered and scheduled flights using light aircraft, is west of Brighton near the town of Shoreham-by-Sea. In 1971, the borough councils of Worthing, Hove and Brighton bought it and operated it jointly as a municipal airport, but since 2006 it has been privately owned. On 6 March 2013, newly established operator Brighton City Airways started running a scheduled service to Paris Pontoise–Cormeilles Airport in France. Flights were suspended shortly afterwards. The airport was rebranded Brighton (Shoreham) Airport around the same time.
Gatwick Airport, one of Britain's major international airports, is north on the A23; regular coach and rail services operate from Brighton.

</doc>
<doc id="49797" url="https://en.wikipedia.org/wiki?curid=49797" title="Emil Nolde">
Emil Nolde

Emil Nolde (born Emil Hansen; 7 August 1867 – 13 April 1956) was a German-Danish painter and printmaker. He was one of the first Expressionists, a member of Die Brücke, and is considered to be one of the great oil painting and watercolor painters of the 20th century. He is known for his vigorous brushwork and expressive choice of colors. Golden yellows and deep reds appear frequently in his work, giving a luminous quality to otherwise somber tones. His watercolors include vivid, brooding storm-scapes and brilliant florals.
Nolde's intense preoccupation with the subject of flowers reflect his continuing interest in the art of Vincent van Gogh.
Biography.
Emil Nolde was born as Emil Hansen near the village of Nolde (since 1920 part of the municipality of Burkal in Southern Jutland, Denmark), in the Prussian Duchy of Schleswig. He grew up on a farm; his parents, devout Protestants, were Frisian and Danish peasants. He realized his unsuitability for farm life and that he and his three brothers were not at all alike. Between 1884 and 1891, he studied to become a carver and illustrator in Flensburg, and worked in furniture factories as a young adult. He spent his years of travel in Munich, Karlsruhe and Berlin.
In 1889, he gained entrance into the School of Applied Arts in Karlsruhe before becoming a drawing-instructor in Switzerland from 1892 to 1898, eventually leaving this job to finally pursue his dream of becoming an independent artist. As a child he had loved to paint and draw, but he was already 31 by the time he pursued a career as an artist. When he was rejected by the Munich Academy of Fine Arts in 1898, he spent the next three years taking private painting classes, visiting Paris, and becoming familiar with the contemporary impressionist scene that was popular at this time. He married Danish actress Ada Vilstrup in 1902 and moved to Berlin, where he would meet collector Gustav Schiefler and artist Karl Schmidt-Rottluff, both of whom would advocate his work later in life. He spent a brief time between 1906–1907 as a member of the revolutionary expressionist group Die Brücke ("The Bridge"), and as a member of the Berlin Secession in 1908–1910, but he eventually left or was expelled from both of these groups—foreshadowing of the difficulty Nolde had maintaining relationships with the organizations to which he belonged. He had achieved some fame by this time and exhibited with Kandinsky’s Der Blaue Reiter group in 1912, supporting himself through his art. From 1902 he called himself after his birthplace.
Nolde was a supporter of the Nazi party from the early 1920s, having become a member of its Danish section. He expressed negative opinions about Jewish artists, and considered Expressionism to be a distinctively Germanic style. This view was shared by some other members of the Nazi party, notably Joseph Goebbels and Fritz Hippler.
However Hitler rejected all forms of modernism as "degenerate art", and the Nazi regime officially condemned Nolde's work. Until that time he had been held in great prestige in Germany. A total of 1,052 of his works were removed from museums, more than those of any other artist. Some were included in the "Degenerate Art" exhibition of 1937, despite his protests, including (later) a personal appeal to Nazi "gauleiter" Baldur von Schirach in Vienna. He was not allowed to paint—even in private—after 1941. Nevertheless, during this period he created hundreds of watercolors, which he hid. He called them the "Unpainted Pictures".
In 1942 Nolde wrote:
There is silver blue, sky blue and thunder blue. Every color holds within it a soul, which makes me happy or repels me, and which acts as a stimulus. To a person who has no art in him, colors are colors, tones tones...and that is all. All their consequences for the human spirit, which range between heaven to hell, just go unnoticed.
After World War II, Nolde was once again honored, receiving the German Order of Merit, West Germany's highest civilian decoration. He died in Seebüll (now part of Neukirchen).
Apart from paintings, Nolde's work includes many prints, often in color, and watercolor paintings of varied subjects: landscapes, religious images, flowers, stormy seas and scenes from Berlin nightlife. A famous series of paintings covers the German New Guinea Expedition, visiting the South Seas, Moscow, Siberia, Korea, Japan, and China. The Schiefler Catalogue raisonné of his prints describes 231 etchings, 197 woodcuts, 83 lithographs, and 4 hectographs.
Major works.
Nolde's work is exhibited at major museums around the world, including "Portrait of a Young Woman and a Child", "Portrait of a Man" (ca. 1926), and "Portrait of a Young Girl" (1913–1914) at the Hermitage Museum, Saint Petersburg, Russia; and "Prophet" (1921) and "Young Couple" (1913) at the Museum of Modern Art, New York City. His most important print, "The Prophet" (1912), is an icon of 20th-century art.
Among his most important oils are "Lesende junge Frau" (1906), "Blumengarten (ohne Figur)" (1908), and "Blumen und Wolken" (1933).
Emil Nolde's work has become the focus of renewed attention after a painting entitled "Blumengarten (Utenwarf)" from 1917, which now hangs in the art museum Moderna Museet, Stockholm, Sweden and has been valued at US$4,000,000, was discovered to have been looted from Otto Nathan Deutsch, a German-Jewish refugee whose heirs, including a Holocaust survivor, are asking for its return. The Swedish government decided in 2007 that the museum must settle with the heirs. Deutsch was forced to flee Germany before World War II and left for Amsterdam in late 1938 or early 1939. The painting was sold to the Swedish museum at an auction in Switzerland, where it had resurfaced in 1967.
Other important works:
In recent years, Nolde's paintings have achieved prices of several million US dollars, in auctions conducted by the leading international auction houses. On 8 February 2012, "Blumengarten (ohne Figur)" was sold by Sotheby's in London for the amount of US$3,272,673.

</doc>
<doc id="49798" url="https://en.wikipedia.org/wiki?curid=49798" title="Beat">
Beat

Beat or beats may refer to:

</doc>
<doc id="49799" url="https://en.wikipedia.org/wiki?curid=49799" title="Pope Clement I">
Pope Clement I

Pope Clement I (; Greek: ; died 99), also known as Saint Clement of Rome, is listed by Irenaeus and Tertullian as Bishop of Rome, holding office from 88 to his death in 99. He is considered to be the first Apostolic Father of the Church.
Few details are known about Clement's life. According to Tertullian, Clement was consecrated by Saint Peter, and he is known to have been a leading member of the church in Rome in the late 1st century. Early church lists place him as the second or third bishop of Rome after Saint Peter. The "Liber Pontificalis" presents a list that makes Pope Linus the second in the line of bishops of Rome, with Peter as first; but at the same time it states that Peter ordained two bishops, Linus and Pope Cletus, for the priestly service of the community, devoting himself instead to prayer and preaching, and that it was to Clement that he entrusted the Church as a whole, appointing him as his successor. Tertullian considered Clement to be the immediate successor of Peter. In one of his works, Jerome listed Clement as "the fourth bishop of Rome after Peter", and added that "most of the Latins think that Clement was second after the apostle". Clement is put after Linus and Cletus/Anacletus in the earliest (c. 180) account, that of Irenaeus, who is followed by Eusebius of Caesarea.
Clement's only genuine extant writing is his letter to the church at Corinth (1 Clement) in response to a dispute in which certain presbyters of the Corinthian church had been deposed. He asserted the authority of the presbyters as rulers of the church on the ground that the Apostles had appointed such. His letter, which is one of the oldest extant Christian documents outside of the New Testament, was read in church, along with other epistles, some of which later became part of the Christian canon. These works were the first to affirm the apostolic authority of the clergy. A second epistle, 2 Clement, was attributed to Clement, although recent scholarship suggests it to be a homily by another author. In the legendary Clementine Literature, Clement is the intermediary through whom the apostles teach the church. According to tradition, Clement was imprisoned under the Emperor Trajan; during this time he is recorded to have led a ministry among fellow prisoners. Thereafter he was executed by being tied to an anchor and thrown into the sea.
Clement is recognized as a saint in many Christian churches and is considered a patron saint of mariners. He is commemorated on 23 November in the Roman Catholic Church, the Anglican Communion, and the Lutheran Church. In Eastern Orthodox Christianity his feast is kept on 24 or 25 November.
Life.
Starting in the 3rd and 4th century, tradition has identified him as the Clement that Paul mentioned in Philippians , a fellow laborer in Christ.
While in the mid-19th century it was customary to identify him as a freedman of Titus Flavius Clemens, who was consul with his cousin, the Emperor Domitian, this identification, which no ancient sources suggest, then lost support. The 2nd-century "Shepherd of Hermas" mentions a Clement whose office it was to communicate with other churches; most likely, this is a reference to Clement I.
The "Liber Pontificalis", which documents the reigns of popes, states that Clement had known Saint Peter. It also states that he wrote two letters (though the second letter, 2 Clement, is no longer ascribed to him) and that he died in Greece in the third year of Emperor Trajan's reign, or 101 AD.
A large congregation existed in Rome c. 58, when Paul wrote his Epistle to the Romans. Paul arrived in Rome c. 60 (Acts). His Captivity Epistles, as well as Mark, Luke, Acts, and 1 Peter were written here, according to many scholars. Paul and Peter were said to have been martyred here. Nero persecuted Roman Christians after Rome burned in 64, and the congregation may have suffered further persecution under Domitian (81–96). Clement was the first of early Rome's most notable bishops.
Clement is known for his epistle to the church in Corinth (c. 96), in which he asserts the apostolic authority of the bishops/presbyters as rulers of the church. The epistle mentions "episkopoi" (overseers, bishops) or "presbyteroi" (elders, presbyters) as the upper class of minister, served by the deacons, but, since it does not mention himself, it gives no indication of the title or titles used for Clement in Rome. It has been cited as the first work to establish Roman primacy, but most scholars see the epistle as more fraternal than authoritative, and Orthodox scholar John Meyendorff sees it as connected with the Roman church's awareness of its "priority" (rather than "primacy") among local churches.
In the epistle, Clement uses the terms bishop and presbyter interchangeably for the higher order of ministers above deacons. In some congregations, particularly in Egypt, the distinction between bishops and presbyters seems to have become established only later. But by the middle of the second century all the leading Christian centres had bishops. Scholars such as Bart Ehrman treat as significant the fact that, of the seven letters written by Ignatius of Antioch to seven Christian churches shortly after the time of Clement, the only one that does not present the church as headed by a single bishop is that addressed to the church in Rome, although this letter did not refer to a collective priesthood either.
According to apocryphal "acta" dating to the 4th century at earliest, Clement was banished from Rome to the Chersonesus during the reign of the Emperor Trajan and was set to work in a stone quarry. Finding on his arrival that the prisoners were suffering from lack of water, he knelt down in prayer. Looking up, he saw a lamb on a hill, went to where the lamb had stood and struck the ground with his pickaxe, releasing a gushing stream of clear water. This miracle resulted in the conversion of large numbers of the local pagans and his fellow prisoners to Christianity. As punishment, Saint Clement was martyred by being tied to an anchor and thrown from a boat into the Black Sea. The legend recounts that every year a miraculous ebbing of the sea revealed a divinely built shrine containing his bones. However, the oldest sources on Clement's life, Eusebius and Jerome, note nothing of his martyrdom.
The Inkerman Cave Monastery marks the supposed place of Clement's burial in the Crimea. A year or two before his own death in 869, Saint Cyril brought to Rome what he believed to be the relics of Saint Clement, bones he found in the Crimea buried with an anchor on dry land. They are now enshrined in the Basilica di San Clemente. Other relics of Saint Clement, including his head, are claimed by the Kiev Monastery of the Caves in Ukraine.
Early succession lists name Clement as the first, second, or third successor of Saint Peter. However, the meaning of his inclusion in these lists has been very controversial. Some believe there were presbyter-bishops as early as the 1st century, but that there is no evidence for a monarchical episcopacy in Rome at such an early date. There is also, however, no evidence of a change occurring in ecclesiastical organization in the latter half of the 2nd century, which would indicate that a new or newly-monarchical episcopacy was establishing itself. Also Dionysius of Corinth and Irenaeus of Lyon both viewed Clement as a monarchial bishop who intervened in the dispute in the church of Corinth.
Writings.
Epistle of Clement.
Clement's only existing, genuine text is a letter to the Christian congregation in Corinth, often called the First Epistle of Clement or 1 Clement. The history of 1 Clement clearly and continuously shows Clement as the author of this letter. It is considered the earliest authentic Christian document outside of the New Testament.
Clement writes to the troubled congregation in Corinth, where certain "presbyters" or "bishops" have been deposed (the class of clergy above that of deacons is designated indifferently by the two terms). Clement calls for repentance and reinstatement of those who have been deposed, in line with maintenance of order and obedience to church authority, since the apostles established the ministry of "bishops and deacons." He mentions "offering the gifts" as one of the functions of the higher class of clergy. Although one who reads the Epistle will note that when the term "offering the gifts" by the bishops is used, it has no reference to "communion" and or Remembrance of the Lord but that of the gifts of ministering to the church with no actual indication of a specific gift. The epistle offers valuable insight into Church ministry at that time and into the history of the Roman Church. It was highly regarded, and was read in church at Corinth along with the Scriptures c. 170.
Writings formerly attributed to Clement.
Second Epistle of Clement.
The Second Epistle of Clement is a homily, or sermon, likely written in Corinth or Rome, but not by Clement. Early Christian congregations often shared homilies to be read. The homily describes Christian character and repentance. It is possible that the Church from which Clement sent his epistle had included a festal homily to share in one economical post, thus the homily became known as the Second Epistle of Clement.
While 2 Clement has been traditionally ascribed to Clement, most scholars believe that 2 Clement was written in the 2nd century based on the doctrinal themes of the text and a near match between words in 2 Clement and in the Greek Gospel of the Egyptians.
Epistles on Virginity.
Two "Epistles on Virginity" were traditionally attributed to Clement, but now there exists almost universal consensus that Clement was not the author of those two epistles.
False Decretals.
A 9th-century collection of church legislation known as the False Decretals, which was once attributed to Saint Isidore of Seville, is largely composed of forgeries. All of what it presents as letters of pre-Nicene popes, beginning with Clement, are forgeries, as are some of the documents that it attributes to councils; and more than forty falsifications are found in the decretals that it gives as those of post-Nicene popes from Pope Sylvester I (314–335) to Pope Gregory II (715–731). The False Decretals were part of a series of falsifications of past legislation by a party in the Carolingian Empire whose principal aim was to free the church and the bishops from interference by the state and the metropolitan archbishops respectively.
Clement is included among other early Christian popes as authors of the Pseudo-Isidoran (or False) Decretals, a 9th-century forgery. These decrees and letters portray even the early popes as claiming absolute and universal authority. Clement is the earliest pope to whom a text is attributed.
Clementine literature.
St. Clement is also the hero of an early Christian romance or novel that has survived in at least two different versions, known as the Clementine literature, where he is identified with Emperor Domitian's cousin Titus Flavius Clemens. Clementine literature portrays Clement as the Apostles' means of disseminating their teachings to the Church.
Recognition as a saint.
St. Clement's name is in the Roman Canon of the Mass. He is commemorated on 23 November as a Pope and martyr in the Roman Catholic Church as well as within the Anglican Communion and the Lutheran Church. The Syriac Orthodox Church, the Malankara Orthodox Syrian Church and the Greek Orthodox Church, as well as the Syriac Catholic Church, the Syro-Malankara Catholic Church and all Byzantine Rite Eastern Catholic Churches commemorate Saint Clement of Rome (called in Syriac ""Mor Clemis"") on 24 November; the Russian Orthodox Church commemorates St Clement on 25 November.
The St Clement's Church in Moscow is renowned for its glittering Baroque interior and iconostasis, as well as a set of gilded 18th-century railings. The parish was disbanded in 1934 and the original free-standing gate was demolished. The Lenin State Library stored its books in the building throughout the Soviet period. It was not until 2008 that the building reverted to the Russian Orthodox Church.
Saint Clement of Rome is commemorated in the Synaxarium of the Coptic Orthodox Church of Alexandria on the 29th of the month of Hatour November (Julian) – equivalent to 8 December (Gregorian) due to the current 13-day Julian–Gregorian Calendar offset. According to the Coptic Church Synaxarium, he suffered martyrdom in AD 100 during the reign of Emperor Trajan (98–117). He was martyred by tying his neck to an anchor and casting him into the sea. The record of the 29th of the Coptic month of Hatour states that this saint was born in Rome to an honorable father whose name was Fostinus and also states that he was a member of the Roman senate and that his father educated him and taught him Greek literature.
Symbolism.
In works of art, Saint Clement can be recognized by having an anchor at his side or tied to his neck. He is most often depicted wearing papal vestments, including the pallium, and sometimes with a papal tiara but more often with a mitre. He is also sometimes shown with symbols of his office as Pope or Bishop of Rome such as the papal cross and the Keys of Heaven. In reference to his martyrdom, he often holds the palm of martyrdom.
Saint Clement can be seen depicted near a fountain or spring, relating to the incident from his hagiography, or lying in a temple in the sea. The Anchored Cross or Mariner's Cross is also referred to as St. Clement's Cross, in reference to the way he was martyred.

</doc>
<doc id="49801" url="https://en.wikipedia.org/wiki?curid=49801" title="Action">
Action

Action may refer to:

</doc>
<doc id="49802" url="https://en.wikipedia.org/wiki?curid=49802" title="Pope Zephyrinus">
Pope Zephyrinus

Pope Zephyrinus (died 20 December 217) was bishop of Rome or pope from 199 to his death in 217. He was born in Rome. His predecessor was Pope Victor I. Upon his death on 20 December 217, he was succeeded by his principal advisor, Pope Callixtus I.
Papacy.
During the 17-year pontificate of Zephyrinus, the young Church endured severe persecution under the Emperor Severus until his death in the year 211. To quote Alban Butler, "this holy pastor was the support and comfort of the distressed flock". According to St. Optatus, Zephyrinus also combated new heresies and apostasies, chief of which were Marcion, Praxeas, Valentine and the Montanists. Eusebius insists that Zephyrinus fought vigorously against the blasphemies of the two Theodotuses, who in response treated him with contempt, but later called him the greatest defender of the divinity of Christ. Although he was not physically martyred for the faith, his suffering – both mental and spiritual – during his pontificate have earned him the title of martyr.
Conflicts.
During the reign of the Emperor Severus (193–211), relations with the young Christian Church deteriorated, and in 202 or 203 the edict of persecution appeared which forbade conversion to Christianity under the severest penalties.
Zephyrinus's predecessor Pope Victor I had excommunicated Theodotus the Tanner for reviving a heresy that Christ only became God after his resurrection. Theodotus' followers formed a separate heretical community at Rome ruled by another Theodotus, the Money Changer, and Asclepiodotus. Natalius, who was tortured for his faith during the persecution, was persuaded by Asclepiodotus to become a bishop in their sect in exchange for a monthly stipend of 150 denarii. Natalius then reportedly experienced several visions warning him to abandon these heretics. According to an anonymous work entitled "The Little Labyrinth" and quoted by Eusebius, Natalius was whipped a whole night by an angel; the next day he donned sackcloth and ashes, and weeping bitterly threw himself at the feet of Zephyrinus.
Feast day.
A feast of St Zephyrinus, Pope and Martyr, held on 26 August, was inserted in the General Roman Calendar in the 13th century, but was removed in the 1969 revision, since he was not a martyr and 26 August is not the anniversary of his death, which is 20 December, the day under which he is now mentioned in the Roman Martyrology.

</doc>
<doc id="49803" url="https://en.wikipedia.org/wiki?curid=49803" title="IBM PC compatible">
IBM PC compatible

IBM PC compatible computers are those similar to the original IBM PC, XT, and AT, able to run the same software and support the same expansion cards as those. Such computers used to be referred to as PC clones, or IBM clones. They duplicate almost exactly all the significant features of the PC architecture, facilitated by IBM's choice of commodity hardware components and various manufacturers' ability to reverse engineer the BIOS firmware using a "clean room design" technique. Columbia Data Products built the first clone of the IBM personal computer by a clean room implementation of its BIOS.
Early IBM PC compatibles used the same computer bus as the original PC and AT models. The IBM AT compatible bus was later named the Industry Standard Architecture bus by manufacturers of compatible computers. The term "IBM PC compatible" is now a historical description only, since IBM has ended its personal computer sales.
Descendants of the IBM PC compatibles comprise the majority of personal computers on the market presently, although interoperability with the bus structure and peripherals of the original PC architecture may be limited or non-existent.
Origins.
IBM decided in 1980 to market a low-cost single-user computer as quickly as possible in response to Apple Computer's success in the burgeoning microcomputer market. On 12 August 1981, the first IBM PC went on sale. There were three operating systems (OS) available for it. The least expensive and most popular was PC DOS made by Microsoft. In a crucial concession, IBM's agreement allowed Microsoft to sell its own version, MS-DOS, for non-IBM computers. The only component of the original PC architecture exclusive to IBM was the BIOS (Basic Input/Output System).
IBM at first asked developers to avoid writing software that addressed the computer's hardware directly, and to instead make standard calls to BIOS functions that carried out hardware-dependent operations. This software would run on any machine using MS-DOS or PC-DOS. Software that directly addressed the hardware instead of making standard calls was faster, however; this was particularly relevant to games. Software addressing IBM PC hardware in this way would not run on MS-DOS machines with different hardware. The IBM PC was sold in high enough volumes to justify writing software specifically for it, and this encouraged other manufacturers to produce machines which could use the same programs, expansion cards, and peripherals as the PC. The 808x computer marketplace rapidly excluded all machines which were not hardware- and software-compatible with the PC. The 640 kB barrier on "conventional" system memory available to MS-DOS is a legacy of that period; other non-clone machines, while subject to a limit, could exceed 640 kB.
Rumors of "lookalike", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release. By June 1983 "PC Magazine" defined "PC 'clone'" as "a computer can accommodate the user who takes a disk home from an IBM PC, walks across the room, and plugs it into the 'foreign' machine". Because of a shortage of IBM PCs that year, many customers purchased clones instead. Columbia Data Products produced the first computer more or less compatible to the IBM PC standard during June 1982, soon followed by Eagle Computer. Compaq announced its first IBM PC compatible in November 1982, the Compaq Portable. The Compaq was the first sewing machine-sized portable computer that was essentially 100% PC-compatible. The company could not copy the BIOS directly as a result of the court decision in "Apple v. Franklin", but it could reverse-engineer the IBM BIOS and then write its own BIOS using clean room design.
Compatibility issues.
Non-compatible MS-DOS computers.
At the same time, many manufacturers such as Xerox, Hewlett-Packard, Digital Equipment Corporation, Sanyo, Texas Instruments, Tulip, Wang and Olivetti introduced personal computers that supported MS DOS, but were not completely software- or hardware-compatible with the IBM PC.
Like IBM, Microsoft's intention was that application writers would write to the application programming interfaces in MS-DOS or the firmware BIOS, and that this would form what would now be termed a hardware abstraction layer. Each computer would have its own Original Equipment Manufacturer (OEM) version of MS-DOS, customized to its hardware. Any software written for MS-DOS would operate on any MS-DOS computer, despite variations in hardware design.
This expectation seemed reasonable in the computer marketplace of the time. Until then Microsoft was based primarily on computer languages such as BASIC. The established small system operating software was CP/M from Digital Research which was in use both at the hobbyist level and by the more professional of those using microcomputers. To achieve such widespread use, and thus make the product viable economically, the OS had to operate across a range of machines from different vendors that had widely varying hardware. Those customers who needed other applications than the starter programs could reasonably expect publishers to offer their products for a variety of computers, on suitable media for each.
Microsoft's competing OS was intended initially to operate on a similar varied spectrum of hardware, although all based on the 8086 processor. Thus, MS-DOS was for several years sold only as an OEM product. There was no Microsoft-branded MS-DOS: MS-DOS could not be purchased directly from Microsoft, and each OEM release was packaged with the trade dress of the given PC vendor. Malfunctions were to be reported to the OEM, not to Microsoft. However, as machines that were compatible with IBM hardware—thus supporting direct calls to the hardware—became widespread, it soon became clear that the OEM versions of MS-DOS were virtually identical, except perhaps for the provision of a few utility programs.
MS-DOS provided adequate functionality for character-oriented applications such as those that could have been implemented on a text-only terminal. Had the bulk of commercially important software been of this nature, low-level hardware compatibility might not have mattered. However, in order to provide maximum performance and leverage hardware features (or work around hardware bugs), PC applications quickly developed beyond the simple terminal applications that MS-DOS supported directly. Spreadsheets, WYSIWYG word processors, presentation software and remote communication software established new markets that exploited the PC's strengths, but required capabilities beyond what MS-DOS provided. Thus, from very early in the development of the MS-DOS software environment, many significant commercial software products were written directly to the hardware, for a variety of reasons:
Difficulty of "Operationally Compatible".
In May 1983, Future Computing defined four levels of compatibility:
During development, Compaq engineers found that "Microsoft Flight Simulator" would not run because of what subLOGIC's Bruce Artwick described as "a bug in one of Intel's chips", forcing them to make their new computer bug compatible with the IBM PC. At first, few clones other than Compaq's offered truly full compatibility; Columbia University reported in January 1984, for example, that Kermit ran without modification on Compaq and Columbia clones, but not on those from Eagle or Seequa. Other MS-DOS computers also required custom code.
When "PC Magazine" requested samples from computer manufacturers that claimed to produce PC compatibles for an April 1984 review, 14 of 31 declined. Corona Data Systems specified that "Our systems run all software that conforms to IBM PC programming standards. And the most popular software does." When a "BYTE" journalist asked to test Peachtext at the Spring 1983 COMDEX, Corona representatives "hemmed and hawed a bit, but they finally led me ... off in the corner where no one would see it should it fail". The magazine reported that "Their hesitancy was unnecessary. The disk booted up without a problem".
"Creative Computing" in 1985 stated, "we reiterate our standard line regarding the IBM PC compatibles: try the package you want to use before you buy the computer." Companies modified their computers' BIOS to work with newly discovered incompatible applications, and reviewers and users developed stress tests to measure compatibility; by 1984 the ability to operate Lotus 1-2-3 and "Flight Simulator" became the standard.
IBM believed that some companies such as Eagle, Corona, and Handwell infringed on its copyright, and after "Apple Computer, Inc. v. Franklin Computer Corp." successfully forced the clone makers to stop using the BIOS. The Phoenix BIOS in 1984, however, and similar products such as AMI BIOS, permitted computer makers to legally build essentially 100%-compatible clones without having to reverse-engineer the PC BIOS themselves. A September 1985 "InfoWorld" chart listed seven compatibles with 256 KB RAM, two disk drives, and monochrome monitors for $1,495 to $2,320, while the equivalent IBM PC cost $2,820.
The decreasing influence of IBM.
In February 1984 "BYTE" wrote that "IBM's burgeoning influence in the PC community is stifling innovation because so many other companies are mimicking Big Blue", but as the market grew IBM's influence diminished. In November 1985 "PC Magazine" stated "Now that it has created the market, the market doesn't necessarily need IBM for the machines. It may depend on IBM to set standards and to develop higher-performance machines, but IBM had better conform to existing standards so as to not hurt users". In January 1987 Bruce Webster wrote in "BYTE" of rumors that IBM would introduce proprietary personal computers with a proprietary operating system: "Who cares? If IBM does it, they will most likely just isolate themselves from the largest marketplace, in which they really can't compete anymore anyway". The magazine predicted that in 1987 the market "will complete its transition from an IBM standard to an Intel/MS-DOS/expansion bus standard ... Folks aren't so much concerned about IBM compatibility as they are about Lotus 1-2-3 compatibility". In 1988 Gartner Group estimated that the public purchased 1.5 clones for every IBM PC. By 1989 Compaq was so influential that industry executives spoke of "Compaq compatible", with observers stating that customers saw the company as IBM's equal.
After 1987, IBM PC compatibles dominated both the home and business markets of commodity computers, with other notable alternative architectures being used in niche markets, like the Macintosh computers offered by Apple Inc. and used mainly for desktop publishing at the time, the aging 8-bit Commodore 64 which was selling for $150 by this time and became the world's best-selling computer, the 32-bit Commodore Amiga line used for television and video production and the 32-bit Atari ST used by the music industry. However, IBM itself lost the main role in the market for IBM PC compatibles by 1990. A few events in retrospect are important:
Despite popularity of its ThinkPad set of laptop PC's, IBM finally relinquished its role as a PC manufacturer during April 2005, when it sold its consumer PC division to Lenovo for $1.75 billion.
As of October 2007, Hewlett-Packard and Dell have the largest shares of the PC market in North America. They are also successful overseas, with Acer, Lenovo, and Toshiba also notable. Worldwide, a huge number of PCs are "white box" systems assembled by myriad local systems builders. Despite advances of computer technology, all current IBM PC compatibles remain very much compatible with the original IBM PC computers, although most of the components implement the compatibility in special backward compatibility modes used only during a system boot. It is often more practical to run old software on a modern system using an emulator rather than relying on these features.
Expandability.
One of the strengths of the PC compatible design is its modular hardware design. End-users could readily upgrade peripherals and, to some degree, processor and memory without modifying the computer's motherboard or replacing the whole computer, as was the case with many of the microcomputers of the time. However, as processor speed and memory width increased, the limits of the original XT/AT bus design were soon reached, particularly when driving graphics video cards. IBM did introduce an upgraded bus in the IBM PS/2 computer that overcame many of the technical limits of the XT/AT bus, but this was rarely used as the basis for IBM compatible computers since it required licence payments to IBM both for the PS/2 bus and any prior AT-bus designs produced by the company seeking a license. This was unpopular with hardware manufacturers and several competing bus standards were developed by consortiums, with more agreeable license terms. Various attempts to standardize the interfaces were made, but in practice, many of these attempts were either flawed or ignored. Even so, there were many expansion options, and despite the confusion of its users, the PC compatible design advanced much faster than other competing designs of the time, even if only because of its market dominance.
"IBM PC compatible" becomes "Wintel".
During the 1990s, IBM's influence on PC architecture started to decline. An IBM-brand PC became the exception rather than the rule. Instead of placing importance on compatibility with the IBM PC, vendors began to emphasize compatibility with Windows. In 1993, a version of Windows NT was released that could operate on processors other than the x86 set. While it required that applications be recompiled, which most developers did not do, its hardware independence was used for Silicon Graphics (SGI) x86 workstations–thanks to NT's Hardware abstraction layer (HAL), they could operate NT (and its vast application library).
No mass-market personal computer hardware vendor dared to be incompatible with the latest version of Windows, and Microsoft's annual WinHEC conferences provided a setting in which Microsoft could lobby for—and in some cases dictate—the pace and direction of the hardware of the PC industry. Microsoft and Intel had become so important to the ongoing development of PC hardware that industry writers began using the portmanteau word Wintel to refer to the combined hardware-software system.
This terminology itself is becoming a misnomer, as Intel has lost absolute control over the direction of x86 hardware development with AMD's AMD64. Also, non-Windows operating systems like Mac OS X and Linux have established a presence on the x86 architecture.
Design limitations and more compatibility issues.
Although the IBM PC was designed for expandability, the designers could not anticipate the hardware developments of the 1980s, nor the size of the industry they would engender. To make things worse, IBM's choice of the Intel 8088 for the CPU introduced several limitations for developing software for the PC compatible platform. For example, the 8088 processor only had a 20-bit memory addressing space. To expand "PC"s beyond one megabyte, Lotus, Intel, and Microsoft jointly created expanded memory (EMS), a bank-switching scheme to allow more memory provided by add-in hardware, and accessed by a set of four 16-Kilobyte "windows" inside the 20-bit addressing. Later, Intel CPUs had larger address spaces and could directly address 16- Megabytes (MiBs) (80286) or more, causing Microsoft to develop extended memory (XMS) which did not require additional hardware.
"Expanded" and "extended" memory have incompatible interfaces, so anyone writing software that used more than one megabyte had to provide for both systems for the greatest compatibility until MS-DOS began including EMM386, which simulated EMS memory using XMS memory. A protected mode OS can also be written for the 80286, but DOS application compatibility was more difficult than expected, not only because most DOS applications accessed the hardware directly, bypassing BIOS routines intended to ensure compatibility, but also that most BIOS requests were made by the first 32 interrupt vectors, which were marked as "reserved" for protected mode processor exceptions by Intel.
Video cards suffered from their own incompatibilities. Once video cards advanced to SVGA the standard for accessing them was no longer clear. At the time, PC programming used a memory model that had 64 KB memory segments. The most common VGA graphics mode's screen memory fit into a single memory segment. SVGA modes required more memory, so accessing the full screen memory was tricky. Each manufacturer developed their own methods of accessing the screen memory, even going so far as not to number the modes consistently. An attempt at creating a standard named VESA BIOS Extensions (VBE) was made, but not all manufacturers used it.
When the 386 was introduced, again a protected mode OS could be written for it. This time, DOS compatibility was much easier because of virtual 8086 mode. Unfortunately programs could not switch directly between them, so eventually, some new memory-model APIs were developed, VCPI and DPMI, the latter becoming the most popular.
Because of the great number of third-party adapters and no standard for them, programming the PC could be difficult. Professional developers would operate a large test-suite of various known-to-be-popular hardware combinations.
Meanwhile, consumers were overwhelmed by the competing, incompatible standards and many different combinations of hardware on offer. To give them some idea of what sort of PC they would need to operate their software, the Multimedia PC (MPC) standard was set during 1990. A PC that met the minimum MPC standard could be marketed with the MPC logo, giving consumers an easy-to-understand specification to look for. Software that could operate on the most minimally MPC-compliant PC would be guaranteed to operate on any MPC. The MPC level 2 and MPC level 3 standards were set later, but the term "MPC compliant" never became popular. After MPC level 3 during 1996, no further MPC standards were established.
Challenges to Wintel domination.
By the late 1990s, the success of Microsoft Windows had driven rival commercial operating systems into near-extinction, and had ensured that the “IBM PC compatible” computer was the dominant computing platform. This meant that if a developer made their software only for the Wintel platform, they would still be able to reach the vast majority of computer users. By the late 1980s, the only major competitor to Windows with more than a few percentage points of market share was Apple Inc.'s Macintosh. The Mac started out billed as "the computer for the rest of us" but the Mac's high prices and closed architecture meant the DOS/Windows onslaught quickly drove the Macintosh into an education and desktop publishing niche, from which it only emerged in the mid-2000s. By the mid-1990s the Mac's market share had dwindled to around 5% and introducing a new rival operating system had become too risky a commercial venture. Experience had shown that even if an operating system was technically superior to Windows, it would be a failure in the market (BeOS and OS/2 for example). In 1989 Steve Jobs said of his new NeXT system, "It will either be the last new hardware platform to succeed, or the first to fail." Four years later in 1993 NeXT announced it was ending production of the NeXTcube and porting NeXTSTEP to Intel processors.
Very early on in PC history, some companies introduced their own XT-compatible chipsets. For example, Chips and Technologies introduced their 82C100 XT Controller which integrated and replaced six of the original XT circuits: one 8237 DMA controller, one 8253 interrupt timer, one 8255 parallel interface controller, one 8259 interrupt controller, one 8284 clock generator, and one 8288 bus controller. Similar non-Intel chipsets appeared for the AT-compatibles, for example OPTi's 82C206 or 82C495XLC which were found in many 486 and early Pentium systems. The x86 chipset market was very volatile though. In 1993, VLSI Technology had become the dominant market player only to be virtually wiped out by Intel a year later. Intel has been the uncontested leader ever since. As the "Wintel" platform gained dominance Intel gradually abandoned the practice of licensing its technologies to other chipset makers; in 2010 Intel was involved in litigation related to their refusal to license their processor bus and related technologies to other companies like Nvidia.
Companies such as AMD and Cyrix developed alternative CPUs that were functionally compatible with Intel's. Towards the end of the 1990s, AMD was taking an increasing share of the CPU market for PCs. AMD even ended up playing a significant role in directing the develop of the x86 platform when its Athlon line of processors continued to develop the classic x86 architecture as Intel deviated with its Netburst architecture for the Pentium 4 CPUs and the IA-64 architecture for the Itanium set of server CPUs. AMD developed AMD64, the first major extension not created by Intel, which Intel later adopted as x86-64. During 2006 Intel began abandoning Netburst with the release of their set of "Core" processors that represented a development of the earlier Pentium III.
A major alternative to Wintel domination is the rise of mobile computing since the early 2000s, which has been marked as the start of a post-PC era. Tablets and smartphones based on CPUs with the ARM architecture are especially prevalent. A version of Windows, Windows RT, exists for ARM-based computers.
The IBM PC compatible today.
The term "IBM PC compatible" is not commonly used presently because all current mainstream desktop and laptop computers are based on the PC architecture, and IBM no longer makes PCs. The competing hardware architectures have either been discontinued or, like the Amiga, have been relegated to niche, enthusiast markets. In the past, the most successful exception was Apple's Macintosh platform, which used non-Intel processors from its inception. Although Macintosh was initially based on the Motorola 68000 family, then transitioned to the PowerPC architecture, Macintosh computers transitioned to Intel processors beginning in 2006. Today's Macintosh computers share the same system architecture as their Wintel counterparts and can boot Microsoft Windows.
The processor speed and memory capacity of modern PCs are many orders of magnitude greater than they were for the original IBM PC and yet backwards compatibility has been largely maintained – a 32-bit operating system can still operate many of the simpler programs written for the OS of the early 1980s without needing an emulator, though an emulator like DOSBox now has near-native functionality at full speed. Additionally, many modern PCs can still run DOS directly, although special options such as USB legacy mode and SATA-to-PATA emulation may need to be set in the BIOS setup utility. Computers using the Extensible Firmware Interface might need to be set at legacy BIOS mode to be able to boot DOS. However, the BIOS/EFI options in most mass-produced consumer-grade computers are very limited and cannot be configured to truly handle OS's such as the original variants of DOS.
The recent spread of the x86-64 architecture has further distanced current computers' and operating systems' internal similarity with the original IBM PC by introducing yet another processor mode with an instruction set modified for 64-bit addressing, but x86-64 capable processors also retain standard x86 compatibility. Some Android tablets and smartphones using x86 CPUs (most commonly Intel Atom) are also theoretically IBM PC compatibles but aren't practical for running Windows because of the lack of a mouse and keyboard unless connected via USB OTG or bluetooth as well as because of the drivers, however apps for Android that emulate an old x86 machine do benefit from the fact that the real CPU is also x86.

</doc>
<doc id="49804" url="https://en.wikipedia.org/wiki?curid=49804" title="Gary Becker">
Gary Becker

Gary Stanley Becker (December 2, 1930 – May 3, 2014) was an American economist and a professor of economics and sociology at the University of Chicago. Described as “the most important social scientist in the past 50 years” by the "New York Times", Becker was awarded the Nobel Memorial Prize in Economic Sciences in 1992 and received the United States Presidential Medal of Freedom in 2007.
Becker was one of the first economists to branch into what were traditionally considered topics belonging to sociology, including racial discrimination, crime, family organization, and drug addiction (see rational addiction). He was known for arguing that many different types of human behavior can be seen as rational and utility maximizing. His approach included altruistic behavior of human behavior by defining individuals' utility appropriately. He was also among the foremost exponents of the study of human capital. Becker was also credited with the "rotten kid theorem."
Biography.
Born to a Jewish family in Pottsville, Pennsylvania, Becker earned a B.A. at Princeton University in 1951, and a Ph.D. at the University of Chicago in 1955 with thesis titled "The Economics of Racial Discrimination". At Chicago, Becker was influenced by Milton Friedman, whom Becker called "by far the greatest living teacher I have ever had" He taught at Columbia University from 1957 to 1968, and then returned to the University of Chicago. Becker was a founding partner of TGG Group, a business and philanthropy consulting company. Becker won the John Bates Clark Medal in 1967. He was elected a Fellow of the American Academy of Arts and Sciences in 1972, and was a member (and for a time the President) of the Mont Pelerin Society. Becker also received the National Medal of Science in 2000.
A political conservative, he wrote a monthly column for "Business Week" from 1985 to 2004, alternating with liberal Princeton economist Alan Blinder. In December 2004, Becker started a joint weblog with Judge Richard Posner entitled "The Becker-Posner Blog".
Becker's first wife was Doria Slote, from 1954 until her death in 1970. The marriage produced two daughters, Catherine Becker and Judy Becker. In 1980 Becker married Guity Nashat, a historian of the Middle East whose research interests overlapped his own. Becker had two stepsons, Cyrus Claffey and Michael Claffey, from his second marriage.
Becker died in Chicago, Illinois, aged 83, on May 3, 2014, after complications from surgery at Northwestern Memorial Hospital. He was survived by his second wife, two daughters, two stepsons, and four grandchildren.
Nobel Memorial Prize.
Becker received the Nobel Prize in 1992 "for having extended the domain of microeconomic analysis to a wide range of human behaviour and interaction, including nonmarket behaviour".
Discrimination.
Discrimination as defined by Kenneth Arrow is "the valuation in the market place of personal characteristics of the worker that are unrelated to worker productivity." Personal characteristics can be physical features such as sex or race, or other characteristics such as a person's religion, caste, or national origin.
Becker often included a variable of taste for discrimination in explaining behavior. He believes that people often mentally increase the cost of a transaction if it is with a minority against which they discriminate. His theory held that competition decreases discrimination. If firms were able to specialize in employing mainly minorities and offer a better product or service, such a firm could bypass discrepancy in wages between equally productive blacks and whites or equally productive females and males. His research found that when minorities are a very small percentage the cost of discrimination mainly falls on the minorities. However, when minorities represent a larger percentage of society, the cost of discrimination falls on both the minorities and the majority. He also pioneered research on the impact of self-fulfilling prophecies of teachers and employers on minorities. Such attitudes often lead to less investment in productive skills and education of minorities. 
Becker recognized that people (employers, customers, and employees) sometimes do not want to work with minorities because they have preference against the disadvantaged groups. He goes on to say that discrimination increases the cost of the firm because in discriminating against certain workers, the employer would have to pay more so that work can proceed without them. If the employer employs the minority, low wages can be provided, but more people can be employed, and productivity can be increased.
Politics.
Becker is also famous for his economic analysis of democracy. He asked what determines the extent to which an interest group can exploit another. The basis of his analysis was the concept of deadweight loss. As Palda (2013) explains "According to Becker, political equilibrium exists even in non-democratic societies. It arises out of a simple calculation that predatory interest groups and their taxpaying victims make: what return on my investment can I get by lobbying government? Becker’s insight is that the gains to predators are linear, but the losses to prey are exponential, thereby stiffening the resistance of victims as the aggression of predators plods on without similarly increased vigor. Think of a gang of robbers taking half the crop from peasants. They then return for the second half. The gain to the gang of the second half cut is the same as in their first extortion. Yet for peasants to lose the last half of their crops means possible starvation and the certain loss of seed corn. They can be expected to resist violently, as they did in the Hollywood movie The Magnificent Seven and in the Japanese movie on which it was based, The Seven Samurai."
Becker's insight was to recognize that deadweight losses put a brake on predation. He took the well-known insight that deadweight losses are proportional to the square of the tax, and used it to argue that a linear increase in takings by a predatory interest group will provoke a non-linear increase in the deadweight losses its victim suffers. These rapidly increasing losses will prod victims to invest equivalent sums in resisting attempts on their wealth. The advance of predators, fueled by linear incentives slows before the stiffening resistance of prey outraged by non-linear damages.
Crime and punishment.
Jurist Richard Posner has stressed the enormous influence of Becker's work "has turned out to be a fount of economic writing on crime and its control."
Becker's interest in criminology arose when he was rushed for time one day. He had to weigh the cost and benefits of legally parking in an inconvenient garage versus in an illegal but convenient spot. After roughly calculating the probability of getting caught and potential punishment, Becker rationally opted for the crime. Becker surmised that other criminals make such rational decisions. However, such a premise went against conventional thought that crime was a result of mental illness and social oppression.
While Becker acknowledged that many people operate under a high moral and ethical constraint, criminals rationally see that the benefits of their crime outweigh the cost such as the probability of apprehension, conviction, and punishment, and their current set of opportunities. From the public policy perspective, since the cost of increasing the fine is trivial in comparison to the cost of increasing surveillance, one can conclude that the best policy is to maximize the fine and minimize surveillance. However, this conclusion has limits, not the least of which include ethical considerations.
One of the main differences between this theory and Jeremy Bentham's rational choice theory, which had been abandoned in criminology, is that if Bentham considered it possible to annihilate crime completely (through the panopticon), Becker's theory acknowledged that a society could not eradicate crime beneath a certain level. For example, if 25% of a supermarket's products were stolen, it would be very easy to reduce this rate to 15%, quite easy to reduce it until 5%, difficult to reduce it under 3%, and nearly impossible to reduce it to 0% (a feat that would be so costly to the supermarket that it would outweigh the benefit, if it is even possible).
Human capital.
Becker's research was fundamental in arguing for the augmentability of human capital. When his research was first introduced it was considered very controversial as some considered it debasing. However, he was able to convince many that individuals make choices of investing in human capital based on rational benefits and cost that include a return on investment as well as a cultural aspect.
His research included the impact of positive and negative habits such as punctuality and alcoholism on human capital. He explored the different rates of return for different people and the resulting macroeconomic implications. He also distinguished between general to specific education and their influence on job-lock and promotions.
Families.
Becker has done research on the family, including analyses of marriage, divorce, fertility, and social security. He first analyzed fertility starting in 1960.
In the 1960s he and Jacob Mincer developed the New Home Economics, of which Becker's theory of allocation of time is a centerpiece. Becker argued that such decisions are made in a marginal-cost and marginal-benefit framework and that marriage markets affect allocation into couples and individual well-being. His research examined the impact of higher real wages in increasing the value of time and therefore the cost of home production such as childrearing. As women increase investment in human capital and enter the workforce, the opportunity cost of childcare rises. Additionally, the increased rate of return to education raises the desire to provide children with formal and costly education. Coupled together, the impact is to lower fertility rates. His theory of marriage was published in 1973 and 1974. Among its many insights are that (1) sex ratios (the ratio of men to women in marriage markets) are positively related with wives' relative access to consumption in marriages and (2) men with higher incomes are more likely to be polygamous. He published a paper on divorce in 1977, with his students Robert T. Michael and Elizabeth Landes, hypothesizing that divorces are more likely when there are unexpected changes in income. Many of these insights on fertility, marriage, and divorce were included in Becker's "A Treatise on the Family", first published in 1981 by Harvard University Press.
In April 2013, in response to the lack of women in top positions in the United States, Becker told "Wall Street Journal" reporter David Wessel, "A lot of barriers women and blacks have been broken down. That's all for the good. It's much less clear what we see today is the result of such artificial barriers. Going home to take care of the kids when the man doesn't: Is that a waste of a woman's time? There's no evidence that it is." This view was then criticized by economist Charles Jones, who stated, "Productivity could be 9% to 15% higher, potentially, if all barriers were eliminated."
Organ markets.
An article by Gary Becker and Julio Elias entitled "Introducing Incentives in the market for Live and Cadaveric Organ Donations" posited that a free market could help solve the problem of a scarcity in organ transplants. Their economic modeling was able to estimate the price tag for human kidneys ($15,000) and human livers ($32,000). It is argued by critics that this particular market would exploit the underprivileged donors from the developing world.

</doc>
<doc id="49808" url="https://en.wikipedia.org/wiki?curid=49808" title="Measures of national income and output">
Measures of national income and output

A variety of measures of national income and output are used in economics to estimate total economic activity in a country or region, including gross domestic product (GDP), gross national product (GNP), net national income (NNI), and adjusted national income (NNI* adjusted for natural resource depletion). All are specially concerned with counting the total amount of goods and services produced within some "boundary". The boundary is usually defined by geography or citizenship, and may also restrict the goods and services that are counted. For instance, some measures count only goods & services that are exchanged for money, excluding bartered goods, while other measures may attempt to include bartered goods by "imputing" monetary values to them. 
National accounts.
Arriving at a figure for the total production of goods and services in a large region like a country entails a large amount of data-collection and calculation.
Although some attempts were made to estimate national incomes as long ago as the 17th century,
the systematic keeping of national accounts, of which these figures are a part, only began in the 1930s, in the United States and some European countries. The impetus for that major statistical effort was the Great Depression and the rise of Keynesian economics, which prescribed a greater role for the government in managing an economy, and made it necessary for governments to obtain accurate information so that their interventions into the economy could proceed as well-informed as possible.
Market value.
In order to count a good or service, it is necessary to assign value to it. The value that the measures of national income and output assign to a good or service is its market value – the price it fetches when bought or sold. The actual usefulness of a product (its use-value) is not measured – assuming the use-value to be any different from its market value.
Three strategies have been used to obtain the market values of all the goods and services produced: the product (or output) method, the expenditure method, and the income method. The product method looks at the economy on an industry-by-industry basis. The total output of the economy is the sum of the outputs of every industry. However, since an output of one industry may be used by another industry and become part of the output of that second industry, to avoid counting the item twice we use not the value output by each industry, but the value-added; that is, the difference between the value of what it puts out and what it takes in. The total value produced by the economy is the sum of the values-added by every industry.
The expenditure method is based on the idea that all products are bought by somebody or some organisation. Therefore, we sum up the total amount of money people and organisations spend in buying things. This amount must equal the value of everything produced. Usually expenditures by private individuals, expenditures by businesses, and expenditures by government are calculated separately and then summed to give the total expenditure. Also, a correction term must be introduced to account for imports and exports outside the boundary.
The income method works by summing the incomes of all producers within the boundary. Since what they are paid is just the market value of their product, their total income must be the total value of the product. Wages, proprietor's incomes, and corporate profits are the major subdivisions of income.
Methods of measuring National Income.
Output.
The output approach focuses on finding the total output of a nation by directly finding the total value of all goods and services a nation produces.
Because of the complication of the multiple stages in the production of a good or service, only the final value of a good or service is included in the total output. This avoids an issue often called 'double counting', wherein the total value of a good is included several times in national output, by counting it repeatedly in several stages of production. In the example of meat production, the value of the good from the farm may be $10, then $30 from the butchers, and then $60 from the supermarket. The value that should be included in final national output should be $60, not the sum of all those numbers, $100. The values added at each stage of production over the previous stage are respectively $10, $20, and $30. Their sum gives an alternative way of calculating the value of final output.
Formulae:
GDP(gross domestic product) at market price = value of output in an economy in the particular year - intermediate consumption
at factor cost = GDP at market price - depreciation + NFIA "(net factor income from abroad)" - net indirect taxes
NDP at factor cost = Compensation of employees + Net interest + Rental & royalty income + Profit of incorporated and unincorporated NDP at factor cost
Expenditure.
The expenditure approach is basically an output accounting method. It focuses on finding the total output of a nation by finding the total amount of money spent. This is acceptable to economists, because like income, the total value of all goods is equal to the total amount of money spent on goods. The basic formula for domestic output takes all the different areas in which money is spent within the region, and then combines them to find the total output.
formula_1
Where:
C = household consumption expenditures / personal consumption expenditures
I = gross private domestic investment
G = government consumption and gross investment expenditures
X = gross exports of goods and services
M = gross imports of goods and services
Note: (X - M) is often written as XN or less commonly as NX, both stand for "net exports"
The names of the measures consist of one of the words "Gross" or "Net", followed by one of the words "National" or "Domestic", followed by one of the words "Product", "Income", or "Expenditure". All of these terms can be explained separately.
Note that all three counting methods should in theory give the same final figure.
However, in practice minor differences are obtained from the three methods for several reasons, including changes in inventory levels and errors in the statistics. One problem for instance is that goods in inventory have been produced (therefore included in Product), but not yet sold (therefore not yet included in Expenditure). Similar timing issues can also cause a slight discrepancy between the value of goods produced (Product) and the payments to the factors that produced the goods (Income), particularly if inputs are purchased on credit, and also because wages are collected often after a period of production.
Gross domestic product and gross national product.
Gross domestic product (GDP) is defined as "the value of all final goods and services produced in a country in 1 year".
Gross National Product (GNP) is defined as "the market value of all goods and services produced in one year by labour and property supplied by the residents of a country."
As an example, the table below shows some GDP and GNP, and NNI data for the United States:
National income and welfare.
GDP per capita (per person) is often used as a measure of a person's welfare. Countries with higher GDP may be more likely to also score high on other measures of welfare, such as life expectancy. However, there are serious limitations to the usefulness of GDP as a measure of welfare:
Because of this, other measures of welfare such as the Human Development Index (HDI), Index of Sustainable Economic Welfare (ISEW), Genuine Progress Indicator (GPI), gross national happiness (GNH), and sustainable national income (SNI) are used.
Difficulties in Measurement of National Income.
There are many difficulties when it comes to measuring national income, however these can be grouped into conceptual difficulties and practical difficulties:
Bibliography.
Australian Bureau of Statistics, "Australian National Accounts: Concepts, Sources and Methods", 2000. This fairly large document has a wealth of information on the meaning of the national income and output measures and how they are obtained.

</doc>
<doc id="49810" url="https://en.wikipedia.org/wiki?curid=49810" title="USS Oregon (BB-3)">
USS Oregon (BB-3)

USS "Oregon" (BB-3) was a pre-dreadnought of the United States Navy. Her construction was authorized on 30 June 1890, and the contract to build her was awarded to Union Iron Works of San Francisco, California on 19 November 1890. Her keel was laid exactly one year later. She was launched on 26 October 1893, sponsored by Miss Daisy Ainsworth (daughter of Oregon steamboat magnate John C. Ainsworth), delivered to the Navy on 26 June 1896, and commissioned on 15 July 1896 with Captain H.L. Howison in command. Later she was commanded by Captains Albert S. Barker and Alexander H. McCormick. Captain Charles E. Clark assumed command 17 March 1898 throughout the Spanish–American War.
"Oregon" served for a short time with the Pacific Squadron before being ordered on a voyage around South America to the East Coast in March 1898 in preparation for war with Spain. She departed from San Francisco on 19 March, and reached Jupiter Inlet 66 days later, a journey of 14,000 nautical miles (26,000 km; 16,000 mi). This was considered a remarkable achievement at the time. The journey popularized the ship with the American public and demonstrated the need for a shorter route, which led to construction of the Panama Canal. After completing her journey "Oregon" was ordered to join the blockade at Santiago as part of the North Atlantic Squadron under Rear Admiral Sampson. She took part in the Battle of Santiago de Cuba, where she and the cruiser were the only ships fast enough to chase down the Spanish cruiser "Cristóbal Colón", forcing its surrender. Around this time she received the nickname "Bulldog of the Navy", most likely because of her high bow wave—known as "having a bone in her teeth" in nautical slang—and perseverance during the cruise around South America and the battle of Santiago.
After the war "Oregon" was refitted and sent back to the Pacific. She served for a year in the Philippines during the Philippine–American War and then spent a year in China at Wusong during the Boxer Rebellion before returning to the United States for an overhaul. In March 1903 "Oregon" returned to Asiatic waters and stayed there for three years, decommissioning in April 1906. "Oregon" was recommissioned in August 1911, but saw little activity and was officially placed on reserve status in 1914. After the United States joined World War I in 1917 "Oregon" acted as one of the escorts for transport ships during the Siberian Intervention. In October 1919, she was decommissioned for the final time. As a result of the Washington Naval Treaty, "Oregon" was declared "incapable of further warlike service" in January 1924. In June 1925 she was loaned to the State of Oregon, who used her as a floating monument and museum in Portland.
In February 1941, "Oregon" was redesignated IX–22. Due to the outbreak of World War II it was decided that the scrap value of the ship was more important than her historical value, so she was sold. Her stripped hulk was later returned to the Navy and used as an ammunition barge during the battle of Guam, where she remained for several years. "USCGC Tupelo (WLB-303)" assisted towing "Oregon" to Guam. During a typhoon in November 1948, she broke loose and drifted out to sea. She was located 500 miles southeast of Guam and towed back. She was sold on 15 March 1956 and scrapped in Japan.
Design and construction.
"Oregon" was constructed from a modified version of a design drawn up by a policy board in 1889 for a short-range battleship. The original design was part of an ambitious naval construction plan to build 33 battleships and 167 smaller ships. The United States Congress saw the plan as an attempt to end the U.S. policy of isolationism and did not approve it, but a year later approved funding for three coast defense battleships, which would become "Oregon" and her sister ships and . The ships were limited to coastal defense due to their moderate endurance, relatively small displacement and low freeboard, or distance from the deck to the water, which limited seagoing capability. They were however heavily armed and armored; "Conway's All The World's Fighting Ships" describes their design as "attempting too much on a very limited displacement."
Construction of the ships was authorized on 30 June 1890 and the contracts for "Indiana" and "Massachusetts" were awarded to William Cramp & Sons of Philadelphia. They also offered to build "Oregon", but the Senate specified one of the ships had to be built on the West Coast of the United States. Therefore, the contract for "Oregon"—not including guns and armor—was awarded to Union Iron Works in San Francisco for $3,180,000. The total cost of the ship was over twice as high, approximately $6,500,000. Her keel was laid down on 19 November 1891 and she was launched two years later on 26 October 1893, a ceremony attended by thousands of people. The construction was slowed due to delays in armor deliveries, so the ship was not completed until March 1896. Her sea trial was on 14 May 1896, during which she achieved a speed of , a significant improvement over the design speed of and superior to her sister ships.
Service history.
Journey around South America.
"Oregon" was commissioned on 16 July 1896 under the command of Captain H.L. Howison as the first United States battleship on the Pacific Coast. In the winter of 1897–1898 she was put in drydock where bilge keels were installed to improve her stability. She left dock on 16 February 1898 after receiving news that the had blown up in Havana harbor. While she went to San Francisco to load ammunition, relations between Spain and the United States rapidly deteriorated. In San Francisco the captain fell ill and was replaced by Captain Charles Edgar Clark. Because of the impending threat of war "Oregon" was ordered to reinforce the North Atlantic Squadron on the East Coast. To do so the ship would have to make a journey of roughly around South America.
On 19 March "Oregon" started on the first leg of her journey, departing from San Francisco and steaming to Callao, Peru. She arrived in Callao on 4 April and replenished her coal. Her next stop would have been Valparaíso, Chile, but Clark decided to press on. "Oregon" entered the Strait of Magellan on 16 April, where she encountered a severe storm. She was forced to anchor on a rocky shelf during the night and proceed the next day through the narrow passage and to Punta Arenas, Chile. While refueling she was joined by the gunboat , which was also sailing to the East Coast. The ships left Punta Arenas together and steamed on to Rio de Janeiro, where they arrived on 30 April and heard the United States and Spain were now officially at war. "Oregon" stopped very briefly in Salvador, Brazil, and then proceeded to Barbados for a final coal resupply. She arrived off the Florida coast on 24 May, completing a mile journey in 66 days, a remarkable achievement at the time.
The "Dictionary of American Naval Fighting Ships" describes the effect of the journey on the American public and government as follows: "On one hand the feat had demonstrated the many capabilities of a heavy battleship in all conditions of wind and sea. On the other it swept away all opposition for the construction of the Panama Canal, for it was then made clear that the country could not afford to take two months to send warships from one coast to the other each time an emergency arose." The extensive press coverage of the journey also increased the popularity of the ship with the American public.
Spanish–American War.
"Oregon" proceeded to the naval base at Key West, where she was attached to the North Atlantic Squadron under Rear Admiral William T. Sampson. They had just received word that Commodore Winfield Scott Schley's Flying Squadron had found the Spanish fleet and was blockading them in the port of Santiago de Cuba. Sampson reinforced the blockade on 1 June and assumed overall command.
In an attempt to break the stalemate, it was decided to attack Santiago from land. An expeditionary force, under the command of Major General William Rufus Shafter, landed east of the city and attacked it on 1 July. The Spanish commander, Admiral Pascual Cervera y Topete, saw that his situation was desperate and attempted to break through the blockade on 3 July 1898, resulting in the battle of Santiago de Cuba. The cruisers and and battleship had left the day before to load coal in Guantanamo Bay. Admiral Sampson's flagship, the cruiser , had also sailed east earlier that morning for a meeting with General Shafter, leaving Commodore Schley in command. This left the blockade weakened and unbalanced on the day of the battle, as three modern battleships ("Oregon", and ) and the armed yacht guarded the east, while the west was only defended by the second-class battleship , cruiser and armed yacht .
When the Spanish fleet steamed out of the harbor at 9:00 on 3 July, they immediately turned westwards and tried to outrun the blockade ships. "Oregon" took the lead in the ensuing chase as she was the only large American ship which had good steam pressure when the battle began. The cruiser "Brooklyn" had uncoupled two of her four engines, but could still achieve and was right behind her. Cervera's flagship, the , took heavy damage and Cervera ordered her driven ashore at 10:15 to prevent her from sinking. shared her fate 15 minutes later and had to beach herself at 11:15. Only the , which had a lead at that point, was still running westward. She was trapped inshore of the American vessels and would need to make a large detour around Cape Cruz westward. Schley ordered "Oregon" to keep up the chase and directed the "Brooklyn" directly to the point of the cape. The American ships were slowly catching up and started firing when "Cristóbal Colón" came within range of their forward guns. "Cristóbal Colon"—who was ordered to Cuba before her main guns could be installed—had nothing to return fire with. She struck her flag at 13:20 and was scuttled in the mouth of Tarquino River to prevent capture by the Americans.
The battle of Santiago de Cuba was a complete victory for the United States and left Spain without a major naval force in the Caribbean. Santiago capitulated on 17 July and the war itself ended less than a month later on 12 August. "Oregon" went to New York for a refit and departed for the Pacific in October 1898 under the command of her new Captain Albert S. Barker. By now she had received the nickname "Bulldog of the Navy", most likely because of her high bow wave—known as "having a bone in her teeth" in nautical slang—and perseverance during the cruise around South America and the battle of Santiago.
Asiatic Station.
After the war the United States annexed the Spanish colonies of Guam, Puerto Rico and the Philippines. However, in the Philippines revolutionary forces under Emilio Aguinaldo had ousted the Spanish colonial government, declared independence and established the First Philippine Republic. The United States did not recognize the republic, which led to the Philippine–American War. "Oregon" was sent to the Philippines to be used for gunboat diplomacy and arrived in Manila on 18 March 1899. Over the next year she functioned as a station ship, took part in the capture of Vigan, and performed blockades. She left the Philippines on 13 February 1900 and cruised in Japanese waters for a few months until heading for Hong Kong in May. She then left Hong Kong on 23 June for Taku, China to take part in the Boxer Rebellion, but grounded on a rock near the Changshan Islands on 28 June. She took significant damage and a forward compartment was flooded. After several days she was successfully re-floated and headed towards Kure, Japan, where she arrive on 17 July and was put into drydock for repairs.
On 29 August she steamed again for China, this time to serve as a station ship at Woosung, the port town of Shanghai. She stayed there until 5 May 1901, when she departed for the United States to be overhauled in the Puget Sound Navy Yard. Extensive repairs were made to her bottom and deck, to repair damage caused by her grounding in June 1900. She stayed in the navy yard for over a year and left for San Francisco on 13 September 1902. She then returned to the far east, arriving in Hong Kong on 18 March 1903. From there she returned to Woosung, where she helped quell a mutiny on a civilian ship. She remained in Asiatic waters for the next three years to support United States interests there. During that time she visited various ports in China, Japan and the Philippines and went to Honolulu during an Asiatic Fleet winter cruise. Early 1906 she was ordered back to United States to be modernized, for which a budget of a million dollar was approved (adjusted for inflation, approximately $23 million in 2010 dollars). She officially decommissioned on 27 April 1906 in the Puget Sound navy yard.
Second commission.
In 1911 a reserve fleet on the Pacific coast was formed, for which "Oregon" recommissioned on 29 August 1911 She remained in reserve until October, when she sailed to San Diego. The following years were ones of relative inactivity for the aging veteran, as she operated out of West Coast ports. On 9 April 1913, she was placed in ordinary at Bremerton, Washington and on 16 September 1914 went into a reserve status, although she remained in commission. On 2 January 1915, she was again in full commission and sailed to San Francisco for the Panama-Pacific International Exposition. The ship visited Portland for the Rose Festival in 1916, arriving on 6 June. The sailors wrote the Portland mayor on 12 June, especially thanking the Portland Railway & Light streetcar company for giving the sailors free rides.
From 11 February 1916 – 7 April 1917, she was placed in commission in reserve, this time at San Francisco. Returned to full commission again on the latter date, "Oregon" remained first on the West Coast, then acted as one of the escorts for transports of the Siberian Intervention. On 12 June 1919 she was decommissioned at Bremerton. From 21 August – 4 October, she was recommissioned briefly and was the reviewing ship for President of the United States Woodrow Wilson during the arrival of the Pacific Fleet at Seattle.
Inter-war period.
With the adoption of ship classification symbols on 17 July 1920, "Oregon" was redesignated BB-3.
In 1921, a movement was begun to preserve the battleship as an object of historic and sentimental interest, and to lay her up permanently at some port in the state of Oregon.
In accordance with the Washington Naval Treaty, "Oregon" was rendered incapable of further warlike service on 4 January 1924, and was retained on the Naval Vessel Register as a naval relic with a classification of "unclassified". In June 1925, she was loaned to the state of Oregon, restored, and moored at Portland, Oregon, as a floating monument and museum.
On 17 February 1941, when identifying numbers were assigned to unclassified vessels, "Oregon" was redesignated IX-22.
Fate.
With the outbreak of World War II, it was deemed that the ship should be scrapped. Accordingly, she was struck from the Naval Vessel Register on 2 November 1942 and sold on 7 December. On that day, one year after the attack on Pearl Harbor, a parade commemorating the ship marched through the streets of downtown Portland. Congressman Lyndon B. Johnson delivered the keynote speech, and the front page of "The Oregonian" included a fifteen-stanza ode to the ship by Ben Hur Lampman. The poem ended:
She was sold for $35,000 and towed to Kalama, Washington in March 1943 for dismantling. The scrap company removed her superstructure and turned her stripped hulk into a barge, hoping to sell it for $150,000, but the War Shipping Administration wanted it. The ownership of the barge went to the United States Court of Claims, was reinstated by the military and towed to Guam to be used as a munitions barge during the Battle of Guam.
The hulk of the old battleship remained at Guam for several years. During a typhoon on 14–15 November 1948, she broke her moorings and drifted out to sea. On 8 December 1948, she was located by search planes southeast of Guam and towed back. She was sold for $208,000 on 15 March 1956 to the Massey Supply Corporation, a large salvage operation owned by Lester M. Dean, Sr., of Kansas City, Missouri. After salvaging and retaining much of the teak wood from the decks and officers' quarters of the USS "Oregon", the ship, with all its steel plate, was resold to the Iwai Sanggo Company, and finally towed by Dean to Kawasaki, Japan, and scrapped.
Surviving pieces.
Her mast survives as a memorial located in Tom McCall Waterfront Park in Portland, Oregon. On 4 July 1976, a time capsule was sealed in the base of the memorial. The time capsule is scheduled to be opened on 5 July 2076.
"Oregon"s two funnels were preserved for a time at a separate location, in Portland's Liberty Ship Memorial Park. In 2006, the funnels were put into storage and the park (always on private property) became part of the Waterfront Pearl condominium development.
During the dismantling of the ship during World War II, small 3" by 3" sections of the ship's wooden decking and interior were used as incentives for selling war bonds. Purchase of a certain number of bonds would allow the buyer to claim a small piece of the ship. This program was highly successful and resulted in a large number of bonds being sold in the Portland area. Other items from the ship's interior were also auctioned for war bonds at this time, such as the pool table from the officers' mess.
The teak wood salvaged from the ship by Lester Dean, Sr. has been fashioned, at the direction of son, Lester Dean, Jr., into a conference room table, reception table and armoire. The pieces are housed, along with one of the portholes of the Oregon, in a board room dedicated to the battleship and entrepreneurial spirit of once owner of the Oregon, Dean, Sr. The office suite is managed by the real estate company founded by Lester Dean, Sr., Dean Realty Co., in Kansas City, Missouri.
Notes.
Footnotes
Citations
Bibliography.
Print references
Dictionary of American Naval Fighting Ships
The New York Times
Other

</doc>
<doc id="49812" url="https://en.wikipedia.org/wiki?curid=49812" title="Avro">
Avro

Avro was a British aircraft manufacturer founded in 1910 whose designs include the Avro 504 used as a trainer in the First World War, the Avro Lancaster, one of the pre-eminent bombers of the Second World War, and the delta wing Avro Vulcan, a stalwart of the Cold War.
Avro was founded on 1 January 1910 by Alliott Verdon Roe at the Brownsfield Mill on Great Ancoats Street in Manchester. The company remained based primarily in Lancashire throughout its 53 years of existence with key developmental and manufacturing sites in Alexandra Park, Chadderton, Trafford Park and Woodford.
History.
Early history.
One of the world's first aircraft builders, A.V. Roe and Company was established at Brownsfield Mill, Great Ancoats Street, Manchester, by Alliott Verdon Roe and his brother Humphrey Verdon Roe on 1 January 1910. Humphrey's contribution was chiefly financial and organizational; funding it from the earnings of the family webbing business and acting as managing director until he joined the RFC in 1917. Alliot had already constructed a successful aircraft, the Roe I Triplane, named "The Bullseye" after a brand of braces manufactured by Humphrey. The first Avro aircraft to be produced in any quantity was the Avro E or Avro 500, first flown in March 1912, of which 18 were manufactured, most for the newly formed RFC. The company also built the world's first aircraft with enclosed crew accommodation in 1912, the monoplane Type F and the biplane Avro Type G in 1912, neither progressing beyond the prototype stage. The Type 500 was developed into the Avro 504, first flown in September 1913. A small number were bought by the War Office before the outbreak of the First World War and the type saw some front-line service in the early months of the war, but is best known as a training aircraft, serving in this role until 1933. Production lasted 20 years and totalled 8,340 at several factories: Hamble, Failsworth, Miles Platting and Newton Heath.
The inter-war years.
After the boom in orders during the First World War, the lack of new work in peacetime caused severe financial problems and in August 1920, 68.5% of the company's shares were acquired by nearby Crossley Motors which had an urgent need for more factory space for automotive vehicle body building. In 1924, the Company left Alexandra Park Aerodrome in south Manchester where test flying had taken place during the period since 1918 and the site was taken over by a mixture of recreation and housing development. A rural site to the south of the growing city was found at New Hall Farm, Woodford in Cheshire, which continued to serve aviation builders BAE Systems until March 2011, (the site has now been earmarked for a mixed use development). In 1928, Crossley Motors sold AVRO to Armstrong Siddeley Holdings Ltd. In 1928, A.V.Roe resigned from the company he had founded and formed the Saunders-Roe company that after World War II developed several radical designs for combat jets, and, eventually, a range of powerful hovercraft. In 1935, Avro became a subsidiary of Hawker Siddeley.
The Second World War.
Maintaining their skills in designing trainer aircraft, the company built a more robust biplane called the Avro Tutor in the 1930s which the Royal Air Force (RAF) also bought in quantity. A twin piston-engined airliner called the Anson followed but as tensions rose again in Europe the firm's emphasis returned to combat aircraft. The Avro Manchester, Lancaster and Lincoln were particularly famous Avro designs. More than 7,000 Lancasters were built and their bombing capabilities led to their use in the famous "Dam Busters" raid. Of the total, nearly half were built at Avro's Woodford (Stockport) and Chadderton (Oldham) sites, with some 700 Lancasters built at the Avro "shadow" factory next to Leeds Bradford Airport (formerly Yeadon Aerodrome), northwest Leeds. This factory employed 17,500 workers at a time when the population of Yeadon was just 10,000. It was the largest building in Europe at the time, at 1.5 million square feet, and its roof was disguised by the addition of fields and hedges to hide it from enemy planes. The old taxiway from the factory to the runway is still evident.
The Avro Lancaster carried the heaviest bomb loads of the war, including the Grand Slam bomb.
Postwar developments.
The civilian Lancastrian and maritime reconnaissance Shackleton were derived from the successful Lancaster design. The Tudor was a pressurised but problematic post-war Avro airliner which faced strong competition from designs by Bristol, Canadair, Douglas, Handley Page, and Lockheed. With the same wings and engines as the Lincoln, it achieved only a short (34 completed) production run following a first flight in June 1945 and the cancellation of an order from BOAC. The older Avro York was somewhat more successful in both the RAF and in commercial service, being distinguished by a fuselage square in cross-section. Both Tudors and Yorks played an important humanitarian part in the Berlin Airlift.
The postwar Vulcan bomber, originally designed as a nuclear-strike aircraft, was used to maintain the British nuclear deterrent, armed with the Avro Blue Steel stand-off nuclear bomb. The Vulcan saw service as a conventional bomber during the British campaign to recapture the Falkland Islands in 1982. Recently, Vulcan XH558 flew again after several years of refurbishment, and several are prized as museum exhibits.
A twin turboprop airliner, the Avro 748, was developed during the 1950s and sold widely across the globe, powered by two Rolls-Royce Dart engines. The Royal Flight bought a few and a variant with a rear-loading ramp and a "kneeling" main undercarriage was sold to the RAF and several members of the Commonwealth as the Andover.
Avro regional jets.
The Avro name would subsequently be resurrected by British Aerospace when this aircraft manufacturer renamed its BAe 146 family of regional jetliners as Avro regional jets (Avro RJ). Three differently sized versions of the four engine jetliner were produced: the Avro RJ70, the Avro RJ85 the Avro RJ100.
The largest example of the family being the Avro RJ 115
Avro Canada.
In 1945, Hawker Siddeley Group purchased the former Victory Aircraft firm in Malton, Ontario, and renamed the operation A.V. Roe Canada Limited. Commonly known as Avro Canada, it was actually a subsidiary of the Hawker Siddeley Group and used the Avro name for trading purposes.
Car production.
Avro also built motor vehicles in the immediate post-World War 1 era. Avro produced the three-wheeler Harper Runabout, and also their own light car, which was powered by a 1,330 cc 4-cylinder engine. Wood and aluminium were used in an integral construction, similar to an aircraft. Approximately 100 were built.
In 1927 Alliott Verdon-Roe designed a two-wheeler car powered by a 350cc Villiers air-cooled engine. An outrigger wheel kept the car upright when stationary. The "Mobile" did not go into production.
Football.
AVRO F.C. was founded at the Chadderton factory and still exists today.

</doc>
<doc id="49813" url="https://en.wikipedia.org/wiki?curid=49813" title="Amherst, Massachusetts">
Amherst, Massachusetts

Amherst () is a town in Hampshire County, Massachusetts, United States in the Connecticut River valley. As of the 2010 census, the population was 37,819, making it the largest community in Hampshire County (although the county seat is Northampton). The town is home to Amherst College, Hampshire College, and the University of Massachusetts Amherst, three of the Five Colleges. The name of the town is pronounced without the "h" ("AM-erst"), giving rise to the local saying, "only the 'h' is silent", in reference both to the pronunciation and to the town's politically active populace.
The communities of Amherst Center, North Amherst, and South Amherst are census-designated places.
Amherst is part of the Springfield, Massachusetts Metropolitan Statistical Area. Lying 18 miles (28.9 km) northeast of the city of Springfield, Amherst is considered the northernmost town in the Hartford-Springfield Knowledge Corridor Metropolitan Region.
History.
The earliest known document of the lands now comprising Amherst is the deed of purchase dated December 1658 between John Pynchon of Springfield and three native inhabitants, referred to as Umpanchla, Quonquont, and Chickwalopp. According to the deed, "ye Indians of Nolwotogg (Norwottuck) upon ye River of Quinecticott (Connecticut)" sold the entire area in exchange for "two Hundred fatham of Wampam & Twenty fatham, and one large Coate at Eight fatham wch Chickwollop set of, of trusts, besides severall small giftes" . Amherst celebrated its 250th anniversary in 2009. The Amherst 250th Anniversary Celebration Committee was established to oversee the creation and implementation of townwide activities throughout 2009. The Amherst Historical Society also organized events, including a book published by them and written by Elizabeth M. Sharpe, "Amherst A to Z".
When the first permanent English settlements arrived in 1727, this land and the surrounding area (including present-day South Hadley and Granby) belonged to the town of Hadley. It gained precinct status in 1734 and eventually township in 1759.
When it incorporated, the colonial governor assigned the town the name Amherst after Jeffery Amherst, 1st Baron Amherst. Many colonial governors at the time scattered his name amidst the influx of new town applications, which is why several towns in the Northeast bear the name. Amherst was a hero of the French and Indian War who, according to popular legend, singlehandedly won Canada for the British and banished France from North America. Popular belief has it that he supported the American side in the Revolutionary war and resigned his commission rather than fight for the British. Baron Amherst actually remained in the service of the Crown during the war—albeit in Great Britain rather than North America—where he organized the defense against the proposed Franco-Spanish Armada of 1779. Nonetheless, his previous service in the French and Indian War meant he remained popular in New England. Amherst is also infamous for recommending, in a letter to a subordinate, the use of smallpox-covered blankets in warfare against the Native Americans along with any "other method that can serve to Extirpate this Execrable Race". For this reason, there have been occasional ad hoc movements to rename the town. Suggested new names have included "Emily", after Emily Dickinson.
Geography and climate.
According to the United States Census Bureau, Amherst has a total area of , of which is land and (0.14%) is water. The town is bordered by Hadley to the west, Sunderland and Leverett to the north, Shutesbury, Pelham, and Belchertown to the east, and Granby and South Hadley to the south. The highest point in the town is on the northern shoulder of Mount Norwottuck; the peak is in Granby but the town's high point is a few yards away and is about 1100 feet. The town is nearly equidistant from both the northern and southern state lines. For interactive mapping provided by the Town of Amherst, see External Links on this page.
Amherst's ZIP code of 01002 is the second-lowest number in the continental United States after Agawam (not counting codes used for specific government buildings such as the IRS).
Demographics.
As of the 2008 U.S. Census, there were 35,564 people, 9,174 households, and 4,550 families residing in the town. The population density was 1,283.4 people per square mile (485.7/km²). There were 9,427 housing units at an average density of 340.1 per square mile (131.3/km²). The racial makeup of the town was 76.7% White, 5.10% Black or African American, 0.21% Native American, 9.02% Asian, 0.09% Pacific Islander, 2.89% from other races, and 3.35% from two or more races. 6.19% of the population were Hispanic or Latino of any race.
There were 9,174 households out of which 27.0% had children under the age of 18 living with them, 36.4% were married couples living together, 10.8% had a female householder with no husband present, and 50.4% were non-families. Of all households 28.6% were made up of individuals and 8.6% had someone living alone who was 65 years of age or older. The average household size was 2.45 and the average family size was 2.97.
In the town the population was spread out with 12.8% under the age of 18, 50.0% from 18 to 24, 17.2% from 25 to 44, 13.4% from 45 to 64, and 6.6% who were 65 years of age or older. The median age was 22 years. For every 100 females there were 91.6 males. For every 100 females age 18 and over, there were 88.8 males.
The median income for a household in the town was $40,017, and the median income for a family was $61,237. Males had a median income of $44,795 versus $32,672 for females. The per capita income for the town was $17,427. About 7.2% of families and 20.2% of the population were below the poverty line, including 9.3% of those under age 18 and 3.5% of those age 65 or over. The reason for the large population living below the poverty line is the large number of students that live in Amherst.
According to the 2010 5-year American Community Survey estimates, occupied housing units had a median household income of $50,063, which includes both renter and owner-occupied units. More specifically, owner-occupied units had a median income of $100,208, while renter-occupied housing units had a median income of $23,925. Large disparities in income between the two groups could explain the high poverty rate and lower median income, as students are the primary tenants of renter-occupied units within Amherst.
Of residents 25 years old or older, 41.7% have a graduate or professional degree, and only 4.9% did not graduate from high school. The largest industry is education, health, and social services, in which 51.9% of employed persons work.
These statistics given above include some but not all of the large student population, roughly 30,000 in 2010, many of whom only reside in the town part of the year. Amherst is home to thousands of part-time and full-time residents associated with the University of Massachusetts Amherst, Amherst College, and Hampshire College and many of those students are involved with the liberal politics of the town.
Income.
Data is from the 2009–2013 American Community Survey 5-Year Estimates.
Government.
Amherst is among relatively few towns of its size in Massachusetts without a mayor-council or council-manager form of government. Instead, it has maintained its traditional system, with a representative town meeting for the legislative branch and a select board for the executive. The Select Board hires a town manager for daily administrative issues.
By a special state law, Amherst's town meeting is modified from the traditional, by designating ten precincts, each with 24 elected representatives. With an additional 14 "ex officio" members (the 5 Select Board members; the 5 School Committee members; President of the Library Trustees; Chair of the Finance Committee; Town Manager; and a Moderator), the total membership of Town Meeting is 254.
In recent years, some have sought to abolish the 254-member Town Meeting with a new charter that would create a directly elected mayor and a nine-member Town Council. The charter was rejected by voters in spring 2003 by fourteen votes and defeated again on March 29, 2005, by 252 votes.
State and federal representation.
In the Massachusetts General Court, Amherst is in the "Hampshire, Franklin and Worcester" Senatorial District, represented by State Senator Stanley Rosenberg, a Democrat. Representative Ellen Story, also a Democrat, represents Amherst for the 3rd Hampshire District in the Massachusetts House of Representatives.
Amherst is represented at the federal level by an all-Democratic delegation, including Senators Elizabeth Warren and Ed Markey, and by Representative Jim McGovern of the Second Congressional District of Massachusetts.
Transportation.
The Pioneer Valley Transit Authority, funded by local governments and the Five Colleges, provides public transportation in the area, operated by University of Massachusetts Transportation Services. Service runs well into the early morning hours on weekends when school is in session. Students attending any colleges in the Five Colleges Consortium have a fee included in their tuition bills (service fee for UMass Amherst students and student activity fees for the other colleges) for each semester that prepays their bus fares for the semester. UMass Transit buses operate via a proof-of-payment system, in which there are random checks of student identification cards and bus passes and transfers.
There is a Peter Pan Bus terminal with services to Springfield, Boston, and other locations in New England. MAX provides intercity bus service to Worcester, Fitchburg, and Northampton with intermediate stops. Megabus provides service between New York City, Amherst, and Burlington, Vermont.
Amtrak rail service is available in nearby Northampton (NHT) on the "Vermonter" service between Washington D.C. and St. Albans, Vermont. More frequent Amtrak service to New York City and Washington, D.C. is available from Union Station in Springfield.
The closest major domestic and limited international air service is available through Bradley International Airport (BDL) in Windsor Locks, Connecticut. Bradley is located approximately one hour's driving time from Amherst. Major international service is available through Logan International Airport (BOS) in Boston, 90 miles away.
General aviation service is close by, at Northampton Airport (7B2), Westover Metropolitan Airport (CEF), and Turners Falls Airport (0B5).
Economy.
Major employers in Amherst include University of Massachusetts Amherst, Amherst College, William D. Mullins Memorial Center, Hampshire College, and Amherst-Pelham Regional School District.

</doc>
<doc id="49814" url="https://en.wikipedia.org/wiki?curid=49814" title="USS Salt Lake City">
USS Salt Lake City

Two ships of the United States Navy have borne the name USS Salt Lake City, in honor of the city in Utah which has served successively as the capital of the Provisional State of Deseret, the Utah Territory, and the 45th state. See Salt Lake City, Utah.

</doc>
<doc id="49816" url="https://en.wikipedia.org/wiki?curid=49816" title="896">
896

__NOTOC__
Year 896 (DCCCXCVI) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="49819" url="https://en.wikipedia.org/wiki?curid=49819" title="Gross world product">
Gross world product

The gross world product (GWP) is the combined gross national product of all the countries in the world. Because imports and exports balance exactly when considering the whole world, this also equals the total global gross domestic product (GDP). In 2014, according to the CIA's "World Factbook", the GWP totalled approximately US$107.5 trillion in terms of purchasing power parity (PPP), and around US$78.28 trillion in nominal terms. The per capita PPP GWP in 2014 was approximately US$16,100 according to the "World Factbook". According to the World Bank, the 2013 nominal GWP was approximately US$75.59 trillion.
Recent GWP growth.
The table below gives regional percentage values for overall GWP growth from 2006 through 2015, as well as an estimate for 2016, according to the International Monetary Fund (IMF)'s World Economic Outlook database. Data is given in terms of constant year-on-year prices.
Historical and prehistorical estimates.
In 1998, J. Bradford DeLong of the Department of Economics, U.C. Berkeley, estimated the total GWP in 1990 U.S. dollars for main years between one million years BCE and 2000 CE, as shown below. Nominal GWP estimates from 2005 onwards are also shown in contemporary U.S. dollars, according to estimates from the "CIA World Factbook" and the World Bank. "Billion" in the table below refers to the short scale usage of the term, where 1 billion = 1,000 million = 109.

</doc>
<doc id="49820" url="https://en.wikipedia.org/wiki?curid=49820" title="904">
904

__NOTOC__
Year 904 (CMIV) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="49821" url="https://en.wikipedia.org/wiki?curid=49821" title="Multivariate random variable">
Multivariate random variable

In mathematics, probability, and statistics, a multivariate random variable or random vector is a list of mathematical variables each of whose value is unknown, either because the value has not yet occurred or because there is imperfect knowledge of its value. The individual variables in a random vector are grouped together because there may be correlations among them — often they represent different properties of an individual statistical unit. For example, while a given person has a specific age, height and weight, the representation of "any person" from within a group would be a random vector. Normally each element of a random vector is a real number. 
Random vectors are often used as the underlying implementation of various types of aggregate random variables, e.g. a random matrix, random tree, random sequence, stochastic process, etc.
More formally, a multivariate random variable is a column vector formula_1 (or its transpose, which is a row vector) whose components are scalar-valued random variables on the same probability space formula_2, where formula_3 is the sample space, formula_4 is the sigma-algebra (the collection of all events), and formula_5 is the probability measure (a function returning each event's probability).
Probability distribution.
Every random vector gives rise to a probability measure on formula_6 with the Borel algebra as the underlying sigma-algebra. This measure is also known as the joint probability distribution, the joint distribution, or the multivariate distribution of the random vector.
The distributions of each of the component random variables formula_7 are called marginal distributions. The conditional probability distribution of formula_7 given formula_9 is the probability distribution of formula_7 when formula_9 is known to be a particular value.
Operations on random vectors.
Random vectors can be subjected to the same kinds of algebraic operations as can non-random vectors: addition, subtraction, multiplication by a scalar, and the taking of inner products.
Similarly, a new random vector formula_12 can be defined by applying an affine transformation formula_13 to a random vector formula_14:
If formula_16 is invertible and the probability density of formula_21 is formula_22, then the probability density of formula_12 is
Expected value, covariance, and cross-covariance.
The expected value or mean of a random vector formula_14 is a fixed vector formula_26 whose elements are the expected values of the respective random variables.
The covariance matrix (also called the variance-covariance matrix) of an formula_27 random vector is an formula_28 matrix whose formula_29 element is the covariance between the formula_30 and the formula_31 random variables. The covariance matrix is the expected value, element by element, of the formula_28 matrix computed as formula_33, where the superscript T refers to the transpose of the indicated vector:
By extension, the cross-covariance matrix between two random vectors formula_14 and formula_12 (formula_14 having formula_38 elements and formula_12 having formula_40 elements) is the formula_41 matrix
where again the indicated matrix expectation is taken element-by-element in the matrix. The cross-covariance matrix formula_43 is simply the transpose of the matrix formula_44.
Further properties.
Expectation of a quadratic form.
One can take the expectation of a quadratic form in the random vector "X" as follows:
where "C" is the covariance matrix of "X" and tr refers to the trace of a matrix — that is, to the sum of the elements on its main diagonal (from upper left to lower right). Since the quadratic form is a scalar, so is its expectation.
Proof: Let formula_46 be an formula_47 random vector with formula_48 and formula_49 and let formula_50 be an formula_51 non-stochastic matrix.
Based on the formula of the covariance, then if we call formula_52 and formula_53, we see that:
Hence
which leaves us to show that
This is true based on the fact that one can cyclically permute matrices when taking a trace without changing the end result (e.g.: trace("AB") = trace("BA")).
We see that
And since
is a fixed number, then
trivially. Using the permutation we get:
and by plugging this into the original formula we get:
Expectation of the product of two different quadratic forms.
One can take the expectation of the product of two different quadratic forms in a zero-mean Gaussian random vector "X" as follows:
where again "C" is the covariance matrix of "X". Again, since both quadratic forms are scalars and hence their product is a scalar, the expectation of their product is also a scalar.
Applications.
Portfolio theory.
In portfolio theory in finance, an objective often is to choose a portfolio of risky assets such that the distribution of the random portfolio return has desirable properties. For example, one might want to choose the portfolio return having the lowest variance for a given expected value. Here the random vector is the vector "r" of random returns on the individual assets, and the portfolio return "p" (a random scalar) is the inner product of the vector of random returns with a vector "w" of portfolio weights — the fractions of the portfolio placed in the respective assets. Since "p" = "w"T"r", the expected value of the portfolio return is "w"TE("r") and the variance of the portfolio return can be shown to be "w"TC"w", where C is the covariance matrix of "r".
Regression theory.
In linear regression theory, we have data on "n" observations on a dependent variable "y" and "n" observations on each of "k" independent variables "xj". The observations on the dependent variable are stacked into a column vector "y"; the observations on each independent variable are also stacked into column vectors, and these latter column vectors are combined into a matrix "X" of observations on the independent variables. Then the following regression equation is postulated as a description of the process that generated the data:
where β is a postulated fixed but unknown vector of "k" response coefficients, and "e" is an unknown random vector reflecting random influences on the dependent variable. By some chosen technique such as ordinary least squares, a vector formula_64 is chosen as an estimate of β, and the estimate of the vector "e", denoted formula_65, is computed as
Then the statistician must analyze the properties of formula_64 and formula_65, which are viewed as random vectors since a randomly different selection of "n" cases to observe would have resulted in different values for them.

</doc>
<doc id="49823" url="https://en.wikipedia.org/wiki?curid=49823" title="Creole language">
Creole language

A creole language, or simply creole, is a stable natural language that has developed from a pidgin (i.e. a simplified language or simplified mixture of languages used by non-native speakers) becoming nativized by children as their first language, with the accompanying effect of a fully developed vocabulary and system of grammar.
The precise number of creoles is not known, particularly as these are poorly attested, but about one hundred creole languages have arisen since 1500, predominantly based on European languages, due to the Age of Discovery and the Atlantic slave trade, though there are creoles based on other languages, including Arabic, Chinese, and Malay. The creole with the largest number of speakers is Haitian Creole, with about ten million native speakers.
The lexicon of a creole language is largely supplied by the parent languages, particularly that of the most dominant group in the social context of the creole's construction, though there are often clear phonetic and semantic shifts. On the other hand, the grammar often has original features that may differ substantially from those of the parent languages.
Overview.
A creole is believed to arise when a pidgin, developed by adults for use as a second language, becomes the native and primary language of their children — a process known as nativization. The pidgin-creole life cycle was studied by Hall in the 1960s.
Creoles share more grammatical similarities with each other than with the languages from which they are phylogenetically derived. However, there is no widely accepted theory that would account for those perceived similarities. Moreover, no grammatical feature has been shown to be specific to creoles, although it is generally acknowledged that creoles have simpler and less sophisticated grammar than longer-established languages.
Many of the creoles known today arose in the last 500 years, as a result of the worldwide expansion of European maritime power and trade in the Age of Discovery, which led to extensive European colonial empires. Like most non-official and minority languages, creoles have generally been regarded in popular opinion as degenerate variants or dialects of their parent languages. Because of that prejudice, many of the creoles that arose in the European colonies, having been stigmatized, have become extinct. However, political and academic changes in recent decades have improved the status of creoles, both as living languages and as object of linguistic study. Some creoles have even been granted the status of official or semi-official languages of particular political territories.
Linguists now recognize that creole formation is a universal phenomenon, not limited to the European colonial period, and an important aspect of language evolution (see ). For example, in 1933 Sigmund Feist postulated a creole origin for the Germanic languages.
Other scholars, such as Salikoko Mufwene, argue that pidgins and creoles arise independently under different circumstances, and that a pidgin need not always precede a creole nor a creole evolve from a pidgin. Pidgins, according to Mufwene, emerged in trade colonies among "users who preserved their native vernaculars for their day-to-day interactions." Creoles, meanwhile, developed in settlement colonies in which speakers of a European language, often indentured servants whose language would be far from the standard in the first place, interacted extensively with non-European slaves, absorbing certain words and features from the slaves' non-European native languages, resulting in a heavily basilectalized version of the original language. These servants and slaves would come to use the creole as an everyday vernacular, rather than merely in situations in which contact with a speaker of the superstrate was necessary.
History.
Origin.
The English term "creole" comes from French "créole", which is cognate with the Spanish term "criollo" and Portuguese "crioulo", all descending from the verb "criar" ('to breed' or 'to raise'), all coming from Latin "creare" ('to produce, create'). The specific sense of the term was coined in the 16th and 17th century, during the great expansion in European maritime power and trade that led to the establishment of European colonies in other continents.
The terms "criollo" and "crioulo" were originally qualifiers used throughout the Spanish and Portuguese colonies to distinguish the members of an ethnic group who were born and raised locally from those who immigrated as adults. They were most commonly applied to nationals of the colonial power, e.g. to distinguish "españoles criollos" (people born in the colonies from Spanish ancestors) from "españoles peninsulares" (those born in the Iberian Peninsula, i.e. Spain). However, in Brazil the term was also used to distinguish between "negros crioulos" (blacks born in Brazil from African slave ancestors) and "negros africanos" (born in Africa). Over time, the term and its derivatives (Creole, Kréol, Kreyol, Kreyòl, Kriol, Krio, etc.) lost the generic meaning and became the proper name of many distinct ethnic groups that developed locally from immigrant communities. Originally, therefore, the term "creole language" meant the speech of any of those creole peoples.
Geographic distribution.
As a consequence of colonial European trade patterns, most of the known European-based creole languages arose in coastal areas in the equatorial belt around the world, including the Americas, western Africa, Goa along the west of India, and along Southeast Asia up to Indonesia, Singapore, Macau, Hong Kong, the Philippines, Malaysia, Seychelles and Oceania.
Many of those creoles are now extinct, but others still survive in the Caribbean, the north and east coasts of South America (The Guyanas), western Africa, Australia (see Australian Kriol language), and in the Indian Ocean.
Atlantic Creole languages are based on European languages with elements from African and possibly Amerindian languages. Indian Ocean Creole languages are based on European languages with elements from Malagasy and possibly other Asian languages. There are, however, creoles like Nubi and Sango that are derived solely from non-European languages.
Social and political status.
Because of the generally low status of the Creole peoples in the eyes of prior European colonial powers, creole languages have generally been regarded as "degenerate" languages, or at best as rudimentary "dialects" of the politically dominant parent languages. Because of this, the word "creole" was generally used by linguists in opposition to "language", rather than as a qualifier for it.
Another factor that may have contributed to the relative neglect of creole languages in linguistics is that they do not fit the 19th-century neogrammarian "tree model" for the evolution of languages, and its postulated regularity of sound changes (these critics including the earliest advocates of the wave model, Johannes Schmidt and Hugo Schuchardt, the forerunners of modern sociolinguistics). This controversy of the late 19th century profoundly shaped modern approaches to the comparative method in historical linguistics and in creolistics.
Because of social, political, and academic changes brought on by decolonization in the second half of the 20th century, creole languages have experienced revivals in the past few decades. They are increasingly being used in print and film, and in many cases, their community prestige has improved dramatically. In fact, some have been standardized, and are used in local schools and universities around the world. At the same time, linguists have begun to come to the realization that creole languages are in no way inferior to other languages. They now use the term "creole" or "creole language" for any language suspected to have undergone creolization, terms that now imply no geographic restrictions nor ethnic prejudices. 
Classification of creoles.
Historic classification.
According to their external history, four types of creoles have been distinguished: plantation creoles, fort creoles, maroon creoles, and creolized pidgins. By the very nature of a creole language, the phylogenetic classification of a particular creole usually is a matter of dispute; especially when the pidgin precursor and its parent tongues (which may have been other creoles or pidgins) have disappeared before they could be documented.
Phylogenetic classification traditionally relies on inheritance of the lexicon, especially of "core" terms, and of the grammar structure. However, in creoles, the core lexicon often has mixed origin, and the grammar is largely original. For these reasons, the issue of which language is "the" parent of a creole — that is, whether a language should be classified as a "Portuguese creole" or "English creole", etc. — often has no definitive answer, and can become the topic of long-lasting controversies, where social prejudices and political considerations may interfere with scientific discussion.
Substrate and superstrate.
The terms substrate and superstrate are often used when two languages interact. However, the meaning of these terms is reasonably well-defined only in second language acquisition or language replacement events, when the native speakers of a certain source language (the substrate) are somehow compelled to abandon it for another target language (the superstrate). The outcome of such an event is that erstwhile speakers of the substrate will use some version of the superstrate, at least in more formal contexts. The substrate may survive as a second language for informal conversation. As demonstrated by the fate of many replaced European languages (such as Etruscan, Breton, and Venetian), the influence of the substrate on the official speech is often limited to pronunciation and a modest number of loanwords. The substrate might even disappear altogether without leaving any trace.
However, there is dispute over the extent to which the terms "substrate" and "superstrate" are applicable to the genesis or the description of creole languages. The language replacement model may not be appropriate in creole formation contexts, where the emerging language is derived from multiple languages without any one of them being imposed as a replacement for any other. The substratum-superstratum distinction becomes awkward when multiple superstrata must be assumed (such as in Papiamentu), when the substratum cannot be identified, or when the presence or the survival of substratal evidence is inferred from mere typological analogies. On the other hand, the distinction may be meaningful when the contributions of each parent language to the resulting creole can be shown to be very unequal, in a scientifically meaningful way. In the literature on Atlantic Creoles, "superstrate" usually means European and "substrate" non-European or African.
Decreolization.
Since creole languages rarely attain official status, the speakers of a fully formed creole may eventually feel compelled to conform their speech to one of the parent languages. This decreolization process typically brings about a post-creole speech continuum characterized by large scale variation and hypercorrection in the language.
It is generally acknowledged that creoles have a simpler grammar and more internal variability than older, more established languages. However, these notions are occasionally challenged. (See also language complexity.)
Phylogenetic or typological comparisons of creole languages have led to divergent conclusions. Similarities are usually higher among creoles derived from related languages, such as the languages of Europe, than among broader groups that include also creoles based on non-Indo-European languages (like Nubi or Sango). French-based creoles in turn are more similar to each other (and to varieties of French) than to other European-based creoles. It was observed, in particular, that definite articles are mostly prenominal in English-based creole languages and English whereas they are generally postnominal in French creoles and in the variety of French that was exported to the colonies in the 17th and 18th century. Moreover, the European languages which gave rise to the creole languages of European colonies all belong to the same subgroup of Western Indo-European and have highly convergent grammars; to the point that Whorf joined them into a single Standard Average European language group. French and English are particularly close, since English, through extensive borrowing, is typologically closer to French than to other Germanic languages. Thus the claimed similarities between creoles may be mere consequences of similar parentage, rather than characteristic features of all creoles.
Creole genesis.
There are a variety of theories on the origin of creole languages, all of which attempt to explain the similarities among them. outline a fourfold classification of explanations regarding creole genesis:
Theories focusing on European input.
Monogenetic theory of pidgins and creoles.
The monogenetic theory of pidgins and creoles hypothesizes that they are all derived from a single Mediterranean Lingua Franca, via a West African Pidgin Portuguese of the 17th century, relexified in the so-called "slave factories" of Western Africa that were the source of the Atlantic slave trade. This theory was originally formulated by Hugo Schuchardt in the late 19th century and popularized in the late 1950s and early 1960s by Taylor, Whinnom, Thompson, and Stewart. However, this hypothesis is no longer actively investigated, and creoles such as the Hezhou language obviously have nothing to do with the Mediterranean Lingua Franca.
Domestic origin hypothesis.
Proposed by for the origin of English-based creoles of the West Indies, the Domestic Origin Hypothesis argues that, towards the end of the 16th century, English-speaking traders began to settle in the Gambia and Sierra Leone rivers as well as in neighboring areas such as the Bullom and Sherbro coasts. These settlers intermarried with the local population leading to mixed populations, and, as a result of this intermarriage, an English pidgin was created. This pidgin was learned by slaves in slave depots, who later on took it to the West Indies and formed one component of the emerging English creoles.
European dialect origin hypothesis.
The French creoles are the foremost candidates to being the outcome of "normal" linguistic change and their creoleness to be sociohistoric in nature and relative to their colonial origin. Within this theoretical framework, a French creole is a language phylogenetically based on the French language, more specifically on a 17th-century koiné French extant in Paris, the French Atlantic harbours, and the nascent French colonies. Supporters of this hypothesis suggest that the non-Creole French dialects still spoken in many parts of the Americas share mutual descent from this single koiné. These dialects are found in Canada (mostly in Québec and among the Acadian people of the Eastern Maritime provinces), the Prairies, Louisiana, Saint-Barthélemy (leeward portion of the island) and as isolates in other parts of the Americas. Approaches under this hypothesis are compatible with gradualism in change and models of imperfect language transmission in koiné genesis.
Foreigner talk and baby talk.
The Foreigner Talk (FT) hypothesis argues that a pidgin or creole language forms when native speakers attempt to simplify their language in order to address speakers who do not know their language at all. Because of the similarities found in this type of speech and speech directed to a small child, it is also sometimes called baby talk.
This could explain why creole languages have much in common, while avoiding a monogenetic model. However, , in analyzing German Foreigner Talk, claims that it is too inconsistent and unpredictable to provide any model for language learning.
While the simplification of input was supposed to account for creoles' simple grammar, commentators have raised a number of criticisms of this explanation:
Another problem with the FT explanation is its potential circularity. points out that FT is often based on the imitation of the incorrect speech of the non-natives, that is the pidgin. Therefore, one may be mistaken in assuming that the former gave rise to the latter.
Imperfect L2 learning.
The imperfect L2 (second language) learning hypothesis claims that pidgins are primarily the result of the imperfect L2 learning of the dominant lexifier language by the slaves. Research on naturalistic L2 processes has revealed a number of features of "interlanguage systems" that are also seen in pidgins and creoles:
Imperfect L2 learning is compatible with other approaches, notably the European dialect origin hypothesis and the universalist models of language transmission.
Theories focusing on non-European input.
Theories focusing on the substrate, or non-European, languages attribute similarities amongst creoles to the similarities of African substrate languages. These features are often assumed to be transferred from the substrate language to the creole or to be preserved invariant from the substrate language in the creole through a process of relexification: the substrate language replaces the native lexical items with lexical material from the superstrate language while retaining the native grammatical categories. The problem with this explanation is that the postulated substrate languages differ amongst themselves and with creoles in meaningful ways. argues that the number and diversity of African languages and the paucity of a historical record on creole genesis makes determining lexical correspondences a matter of chance. coined the term "cafeteria principle" to refer to the practice of arbitrarily attributing features of creoles to the influence of substrate African languages or assorted substandard dialects of European languages.
For a representative debate on this issue, see the contributions to ; for a more recent view, .
Because of the sociohistoric similarities amongst many (but by no means all) of the creoles, the Atlantic slave trade and the plantation system of the European colonies have been emphasized as factors by linguists such as .
Gradualist and developmental hypotheses.
One class of creoles might start as pidgins, rudimentary second languages improvised for use between speakers of two or more non-intelligible native languages. Keith Whinnom (in ) suggests that pidgins need three languages to form, with one (the superstrate) being clearly dominant over the others. The lexicon of a pidgin is usually small and drawn from the vocabularies of its speakers, in varying proportions. Morphological details like word inflections, which usually take years to learn, are omitted; the syntax is kept very simple, usually based on strict word order. In this initial stage, all aspects of the speech — syntax, lexicon, and pronunciation — tend to be quite variable, especially with regard to the speaker's background.
If a pidgin manages to be learned by the children of a community as a native language, it may become fixed and acquire a more complex grammar, with fixed phonology, syntax, morphology, and syntactic embedding. Pidgins can become full languages in only a single generation. "Creolization" is this second stage where the pidgin language develops into a fully developed native language. The vocabulary, too, will develop to contain more and more items according to a rationale of lexical enrichment.
Universalist approaches.
Universalist models stress the intervention of specific general processes during the transmission of language from generation to generation and from speaker to speaker. The process invoked varies: a general tendency towards semantic transparency, first language learning driven by universal process, or general process of discourse organization. The main universalist theory is still Bickerton's language bioprogram theory, proposed in the 1980s. Bickerton claims that creoles are inventions of the children growing up on newly founded plantations. Around them, they only heard pidgins spoken, without enough structure to function as natural languages; and the children used their own innate linguistic capacities to transform the pidgin input into a full-fledged language. The alleged common features of all creoles would then be the consequence of those innate abilities being universal.
Recent study.
The last decade has seen the emergence of some new questions about the nature of creoles: in particular, the question of how complex creoles are and the question of whether creoles are indeed "exceptional" languages.
Creole prototype.
Some features that distinguish creole languages from noncreoles have been proposed (by Bickerton, for example).
John McWhorter has proposed the following list of features to indicate a creole prototype:
McWhorter hypothesizes that these three properties exactly characterize a creole. However, the creole prototype hypothesis has been disputed:
Exceptionalism.
Building up on this discussion, McWhorter proposed that "the world's simplest grammars are Creole grammars", claiming that every noncreole language's grammar is at least as complex as any creole language's grammar. Gil has replied that Riau Indonesian has a simpler grammar than Saramaccan, the language McWhorter uses as a showcase for his theory. The same objections were raised by Wittmann in his 1999 debate with McWhorter.
The lack of progress made in defining creoles in terms of their morphology and syntax has led scholars such as Robert Chaudenson, Salikoko Mufwene, Michel DeGraff, and Henri Wittmann to question the value of "creole" as a typological class; they argue that creoles are structurally no different from any other language, and that "creole" is a sociohistoric concept — not a linguistic one — encompassing displaced populations and slavery.
Given these objections to "creole" as a concept, articles such as "Against Creole Exceptionalism" by DeGraff and texts like "Deconstructing Creole" by Ansaldo and Matthews have arisen which question the idea that creoles are exceptional in any meaningful way. Additionally, argues that some Romance languages are potential creoles but that they are not considered as such by linguists because of a historical bias against such a view.

</doc>
<doc id="49824" url="https://en.wikipedia.org/wiki?curid=49824" title="Grand Canal (China)">
Grand Canal (China)

The Grand Canal (also known as the Beijing-Hangzhou Grand Canal), a UNESCO World Heritage Site, is the longest canal or artificial river in the world and a famous tourist destination. Starting at Beijing, it passes through Tianjin and the provinces of Hebei, Shandong, Jiangsu and Zhejiang to the city of Hangzhou, linking the Yellow River and Yangtze River. The oldest parts of the canal date back to the 5th century BC, although the various sections were finally combined during the Sui dynasty (581–618 AD).
The total length of the Grand Canal is . Its greatest height is reached in the mountains of Shandong, at a summit of 42 m (138 ft). Ships in Chinese canals did not have trouble reaching higher elevations after the pound lock was invented in the 10th century, during the Song dynasty (960–1279), by the government official and engineer Qiao Weiyue. The canal has been admired by many throughout history including Japanese monk Ennin (794–864), Persian historian Rashid al-Din (1247–1318), Korean official Choe Bu (1454–1504), and Italian missionary Matteo Ricci (1552–1610).
Historically, periodic flooding of the adjacent Yellow River threatened the safety and functioning of the canal. During wartime the high dikes of the Yellow River were sometimes deliberately broken in order to flood advancing enemy troops. This caused disaster and prolonged economic hardships. Despite temporary periods of desolation and disuse, the Grand Canal furthered an indigenous and growing economic market in China's urban centers since the Sui period. It has allowed faster trading and has improved China's economy. In the southern portion it is in heavy constant use to the present day. 
History.
Early history.
In the late Spring and Autumn period (722–481 BC), King Fuchai of Wu, ruler of the State of Wu (present-day Suzhou), ventured north to conquer the neighboring State of Qi. He ordered a canal be constructed for trading purposes, as well as a means to ship ample supplies north in case his forces should engage the northern states of Song and Lu. This canal became known as the Han Gou (邗沟, "Han Conduit"). Work began in 486 BC, from south of Yangzhou to north of Huai'an in Jiangsu, and within three years the Han Gou had connected the Yangtze River to the Huai River by means of existing waterways, lakes, and marshes.
Han Gou is known as the second oldest section of the later Grand Canal since the Hong Gou (鴻溝, "Canal of the Flying Geese" or "Far-Flung Canal") most likely preceded it. It linked the Yellow River near Kaifeng to the Si and Bian rivers and became the model for the shape of the Grand Canal in the north. The exact date of the Hong Gou's construction is uncertain; it is first mentioned by the diplomat Su Qin in 330 BC when discussing state boundaries. The historian Sima Qian (145–90 BC) dated it much earlier than the 4th century BC, attributing it to the work of Yu the Great; modern scholars now consider it to belong to the 6th century BC.
Grand Canal in the Sui dynasty.
The sections of the Grand Canal today in Zhejiang and southern Jiangsu provinces were in large part a creation of the Sui dynasty (581-618), a result of the migration of China’s core economic and agricultural region away from the Yellow River valley in the north and toward the southern provinces. Its main role throughout its history was the transport of grain to the capital. The institution of the Grand Canal by the Sui also obviated the need for the army to become self-sufficient farmers while posted at the northern frontier, as food supplies could now easily be shipped from south to north over the pass.
By the year 600, there were major build ups of silt on the bottom of the Hong Gou canal, obstructing river barges whose drafts were too deep for its waters. The chief engineer of the Sui dynasty, Yuwen Kai, advised the dredging of a new canal that would run parallel to the existing canal, diverging from it at Chenliu (Yanzhou). The new canal was to pass not Xuzhou but Suzhou, to avoid connecting with the Si River, and instead make a direct connection with the Huai River just west of Lake Hongze. With the recorded labor of five million men and women under the supervision of Ma Shumou, the first major section of the Grand Canal was completed in the year 605—called the Bian Qu. The Grand Canal was fully completed under the second Sui emperor, from the years 604 to 609, first by linking Luoyang to the Yangzhou (and the Yangzi valley), then expanding it to Hangzhou (south), and to Beijing (north). This allowed the southern area to provide grain to the northern province, particularly to troops stationed there. Running alongside and parallel to the canal was an imperial roadway and post offices supporting a courier system. The government also planted an enormous line of trees. The history of the canal's construction is handed down in the book "Kaiheji" ('Record of the Opening of the Canal').
The earlier dyke-building project in 587 along the Yellow River—overseen by engineer Liang Rui—established canal lock gates to regulate water levels for the canal. Double slipways were installed to haul boats over when the difference in water levels were too great for the flash lock to operate.
Between 604 and 609, Emperor Yang Guang (or Sui Yangdi) of the Sui dynasty ordered a number of canals be dug in a ‘Y’ shape, from Hangzhou in the south to termini in (modern) Beijing and in the capital region along the Yellow River valley. When the canal was completed it linked the systems of the Qiantang River, the Yangtze River, the Huai River, the Yellow River, the Wei River and the Hai River. Its southern section, running between Hangzhou and the Yangtze, was named the Jiangnan River (the river ‘South of the Yangtze’). The canal’s central portions stretched from Yangzhou to Luoyang; the section between the Yangtze and the Huai continued to the Shanyang River; and the next section connected the Huai to the Yellow River and was called the Tongji Channel. The northernmost portion, linking Beijing and Luoyang, was named the Yongji Channel. This portion of the canal was used to transport troops to what is now the North Korean border region during the Goguryeo-Sui Wars (598–614). After the canal's completion in 609, Emperor Yang led a recorded long naval flotilla of boats from the north down to his southern capital at Yangzhou.
The Grand Canal at this time was not a continuous, man-made canal but a collection of often non-contiguous artificial cuts and canalised or natural rivers.
Grand Canal from Tang to Yuan.
Although the Tang dynasty (618–907) capital at Chang'an was the most thriving metropolis of China in its day, it was the city of Yangzhou—in proximity to the Grand Canal—that was the economic hub of the Tang era. Besides being the headquarters for the government salt monopoly and the largest pre-modern industrial production center of the empire, Yangzhou was also the geographical midpoint along the north-south trade axis, and so became the major center for southern goods shipped north. One of the greatest benefits of the canal system in the Tang dynasty—and subsequent dynasties—was that it reduced the cost of shipping grain that had been collected in taxes from the Yangtze River Delta to northern China. Minor additions to the canal were made after the Sui period to cut down on travel time, but overall no fundamental differences existed between the Sui Grand Canal and the Tang Grand Canal.
By the year 735, it was recorded that about of grain were shipped annually along the canal. The Tang government oversaw canal lock efficiency and built granaries along route in case a flood or other disaster impeded the path of shipment. To ensure smooth travel of grain shipments, Transport Commissioner Liu Yan (in office from 763 to 779) had special river barge ships designed and constructed to fit the depths of each section of the entire canal.
After the An Shi Rebellion (755–763), the economy of northern China was greatly damaged and never recovered due to wars and to constant flooding of the Yellow River. Such a case occurred in the year 858 when an enormous flood along the Grand Canal inundated thousands of acres of farmland and killed tens of thousands of people in the North China Plain. Such an unfortunate event could reduce the legitimacy of a ruling dynasty by causing others to perceive it as having lost the Mandate of Heaven; this was a good reason for dynastic authorities to maintain a smooth and efficient canal system.
The city of Kaifeng grew to be a major hub, later becoming the capital of the Song dynasty (960–1279). Although the Tang and Song dynasty international seaports—the greatest being Guangzhou and Quanzhou, respectively—and maritime foreign trade brought merchants great fortune, it was the Grand Canal within China that spurred the greatest amount of economic activity and commercial profit. During the Song and earlier periods, barge ships occasionally crashed and wrecked along the Shanyang Yundao section of the Grand Canal while passing the double slipways, and more often than not those were then robbed of the tax grain by local bandits. This prompted Qiao Weiyue, an Assistant Commissioner of Transport for Huainan, to invent a double-gate system known as the pound lock in the year 984. This allowed ships to wait within a gated space while the water could be drained to appropriate levels; the Chinese also built roofed hangars over the space to add further protection for the ships.
Much of the Grand Canal south of the Yellow River was ruined for several years after 1128, when Du Chong decided to break the dykes and dams holding back the waters of the Yellow River in order to decimate the oncoming Jurchen invaders during the Jin–Song wars. The Jurchen Jin dynasty continually battled with the Song in the region between the Huai River and the Yellow River; this warfare led to the dilapidation of the canal until the Mongols invaded in the 13th century AD and began necessary repairs.
During the Mongol Yuan dynasty (1271–1368) the capital of China was moved to Beijing, eliminating the need for the canal arm flowing west to Kaifeng or Luoyang. A summit section was dug across the foothills of the Shandong massif during the 1280s, shortening the overall length by as much as 700 km (making the total length about 1800 km) and linking Hangzhou and Beijing with a direct north-south waterway for the first time. As in the Song and Jin era, the canal fell into disuse and dilapidation during the Yuan dynasty's decline.
Ming dynasty restoration.
The Grand Canal was renovated almost in its entirety between 1411 and 1415 during the Ming dynasty (1368–1644). A magistrate of Jining, Shandong sent a memorandum to the throne of the Yongle Emperor protesting the current inefficient means of transporting 4,000,000 "dan" (428,000,000 liters) of grain a year by means of transferring it along several different rivers and canals in barge types that went from deep to shallow after the Huai River, and then transferred back onto deep barges once the shipment of grain reached the Yellow River. Chinese engineers built a dam to divert the Wen River to the southwest in order to feed 60% of its water north into the Grand Canal, with the remainder going south. They dug four large reservoirs in Shandong to regulate water levels, which allowed them to avoid pumping water from local sources and water tables. Between 1411 and 1415 a total of 165,000 laborers dredged the canal bed in Shandong and built new channels, embankments, and canal locks.
The Yongle Emperor moved the Ming capital from Nanjing to Beijing in 1403. This move deprived Nanjing of its status as chief political center of China. The reopening of the Grand Canal also benefited Suzhou over Nanjing since the former was in a better position on the main artery of the Grand Canal, and so it became Ming China's greatest economic center. The only other viable contender with Suzhou in the Jiangnan region was Hangzhou, but it was located further down the Grand Canal and away from the main delta. Even the shipwrecked Korean Choe Bu (1454–1504)—while traveling for five months throughout China in 1488—acknowledged that Hangzhou served not as a competitor but as an economic feeder into the greater Suzhou market. Therefore, the Grand Canal served to make or break the economic fortunes of certain cities along its route, and served as the economic lifeline of indigenous trade within China.
The scholar Gu Yanwu of the early Qing dynasty (1644–1912) estimated that the previous Ming dynasty had to employ 47,004 full-time laborers recruited by the "lijia" corvée system in order to maintain the entire canal system. It is known that 121,500 soldiers and officers were needed simply to operate the 11,775 government grain barges in the mid-15th century.
Besides its function as a grain shipment route and major vein of river borne indigenous trade in China, the Grand Canal had long been a government-operated courier route as well. In the Ming dynasty, official courier stations were placed at intervals of 35 to 45 km. Each courier station was assigned a different name, all of which were popularized in travel songs of the period.
Qing dynasty and 20th century China.
The Manchus invaded China in the mid-17th century, allowed through the northern passes by the Chinese general Wu Sangui once the Ming capital at Beijing had fallen into the hands of a rebel army. The Manchus established the Qing dynasty (1644–1912), and under their leadership the Grand Canal was overseen and maintained just as in earlier times.
In 1855, the Yellow River flooded and changed its course, severing the course of the canal in Shandong. This was foreseen by a Chinese official in 1447, who remarked that the flood-prone Yellow River made the Grand Canal like a throat that could be easily strangled (leading some officials to request restarting the grain shipments through the East China Sea). Because of various factors – the difficulty of crossing the Yellow River, the increased development of an alternative sea route for grain-ships, and the opening of the Tianjin-Pukou Railway and the Beijing-Hankou Railway – the canal languished and for decades the northern and southern parts remained separate. Many of the canal sections fell into disrepair, and some parts were returned to flat fields. Even today, the Grand Canal has not fully recovered from this disaster. After the founding of the People's Republic of China in 1949, the need for economic development led the authorities to order heavy reconstruction work.
The economic importance of the canal likely will continue. The governments of the Shandong, Jiangsu and Zhejiang Provinces planned dredging meant to increase shipping capacity by 40 percent by 2012.
On June 22, 2014, The Grand Canal was listed as a World Heritage Site, at the 2014 Conference on World Heritage.
Historical sections.
As well as its present-day course, fourteen centuries of canal-building have left the Grand Canal with a number of historical sections. Some of these have disappeared, others are still partially extant, and others form the basis for the modern canal. The following are the most important, but do not form an exhaustive list.
Jia Canal.
In 12 BC, to solve the problem of the Grand Canal having to use of the perilous course of the Yellow River in Northern Jiangsu, a man named Li Hualong opened the Jia Canal. Named after the Jia River whose course it followed, it ran from Xiazhen (modern Weishan) on the shore of Shandong's Weishan Lake to Suqian in Jiangsu. The construction of the Jia Canal left only of Yellow River navigation on the Grand Canal, from Suqian to Huai'an, which by 1688 had been removed by the construction of the Middle Canal by Jin Fu.
Nanyang New Canal.
In 1566, to escape the problems caused by flooding of the Yellow River around Yutai (now on the western shore of Weishan Lake), the Nanyang New Canal was opened. It ran for from Nanyang (now Nanyang Town in the centre of Weishan Lake) to the small settlement of Liucheng (in the vicinity of modern Gaolou Village, Weishan County, Shandong) north of Xuzhou City. This change in effect moved the Grand Canal from the low-lying and flood-prone land west of Weishan Lake onto the marginally higher land to its east. It was fed by rivers flowing east-west from the borders of the Shandong massif.
Huitong Canal.
North of the Jizhou Canal summit section, the Huitong Canal ran downhill, fed principally by the River Wen, to join the Wei River at the city of Linqing. In 1289, a geological survey preceded its one-year construction. The Huitong Canal, built by an engineer called Ma Zhizhen, ran across sharply sloping ground and the high concentration of locks gave it the nicknames "chahe" or "zhahe", i.e. 'the river of locks'. Its great number of feeder springs (between two and four hundred, depending on the counting method and season of the year) also led to it being called the "quanhe" or 'river of springs'.
Jizhou Canal.
This, the Grand Canal’s first true summit section, was engineered by the Mongol Oqruqči in 1238 to connect Jining to the southern end of the Huitong Canal. It rose to a height of 138 feet above the Yangtze, but environmental and technical factors left it with chronic water shortages until it was re-engineered in 1411 by Song Li of the Ming. Song Li's improvements, recommended by a local man named Bai Ying, included damming the rivers Wen and Guang and drawing lateral canals from them to feed reservoir lakes at the very summit, at a small town called Nanwang.
Duke Huan's Conduit.
In 369 AD, General Huan Wen of the Eastern Jin dynasty connected the shallow river valleys of the Huai and the Yellow. He achieved this by joining two of these rivers' tributaries, the Si and the Ji respectively, at their closest point, across a low watershed of the Shandong massif. Huan Wen’s primitive summit canal became a model for the engineers of the Jizhou Canal.
Yilou Canal.
The Shanyang Canal originally opened onto the Yangtze a short distance south of Yangzhou. As the north shore of the Yangtze gradually silted up to create the sandbank island of Guazhou, it became necessary for boats crossing to and from the Jiangnan Canal to sail the long way around the eastern edge of that island. After a particularly rough crossing of the Yangtze from Zhenjiang, the local prefect realised that a canal dug directly across Guazhou would slash the journey time and so make the crossing safer. The Yilou Canal was opened in 738 AD and still exists, though not as part of the modern Grand Canal route.
Modern course.
The Grand Canal nominally runs between Beijing and Hangzhou over a total length of , however, only the section from Hangzhou to Jining is currently navigable. Its course is today divided into seven sections. From south to north these are the Jiangnan Canal, the Li Canal, the Zhong Canal, the Lu Canal, the South Canal, the North Canal, and the Tonghui River.
Jiangnan Canal.
This southernmost section of the canal runs from Hangzhou in Zhejiang, where the canal connects with the Qiantang River, to Zhenjiang in Jiangsu, where it meets the Yangtze. After leaving Hangzhou the canal passes around the eastern border of Lake Tai, through the major cities of Jiaxing, Suzhou, Wuxi and Changzhou before reaching Zhenjiang. The Jiangnan (or ‘South of the Yangtze’) Canal is very heavily used by barge traffic bringing coal and construction materials to the booming delta. It is generally a minimum of 100 metres wide in the congested city centres, and often two or three times this width in the countryside beyond. In recent years, broad bypass canals have been dug around the major cities to reduce ‘traffic jams’.
Li Canal.
This ‘Inner Canal’ runs between the Yangtze and Huai'an in Jiangsu, skirting the Shaobo, Gaoyou and Hongze lakes of central Jiangsu. Here the land lying to the west of the canal is higher than its bed while the land to the east is lower. Traditionally the Shanghe region west of the canal has been prone to frequent flooding, while the Xiahe region to its east has been hit by less frequent but immensely damaging inundations caused by failure of the Grand Canal levees. Recent works have allowed floodwaters from Shanghe to be diverted safely out to sea.
Zhong Canal.
This ‘Middle Canal’ section runs from Huai'an to Weishan Lake, passing through Luoma Lake and following more than one course, the result of the impact of centuries of Yellow River flooding. After Pizhou, a northerly course passes through Tai'erzhuang to enter Weishan Lake at Hanzhuang bound for Nanyang and Jining (this course is the remnant of the New Nanyang Canal of 1566 – see below). A southerly course passes close by Xuzhou and enters Weishan Lake near Peixian. This latter course is less used today.
Lu Canal.
At Weishan Lake, both courses enter Shandong province. From here to Linqing, the canal is called the Lu or ‘Shandong’ Canal. It crosses a series of lakes – Zhaoyang, Dushan and Nanyang – which nominally form a continuous body of water. At present, diversions of water mean that the lakes are often largely dry land. North of the northernmost Nanyang Lake is the city of Jining. Further on, about 30 km north of Jining, the highest elevation of the canal (38.5 m above sea level) is reached at the town of Nanwang. In the 1950s a new canal was dug to the south of the old summit section. The old summit section is now dry, while the new canal holds too little water to be navigable. About 50 km further north, passing close by Dongping Lake, the canal reaches the Yellow River. By this point waterless, it no longer communicates with the river. It reappears again in Liaocheng City on the north bank where, intermittently flowing through a renovated stone channel, it reaches the city of Linqing on the Shandong – Hebei border.
Southern Canal.
The fifth section of the canal extends for a distance of from Linqing to Tianjin along the course of the canalised Wei River. Though one of the northernmost sections, its name derives from its position relative to Tianjin. The Wei River at this point is heavily polluted while drought and industrial water extraction have left it too low to be navigable. The canal, now in Hebei province, passes through the cities of Dezhou and Cangzhou. Although to spectators the canal appears to be a deep waterway in these city centres, its depth is maintained by weirs and the canal is all but dry where it passes through the surrounding countryside. At its end the canal joins the Hai River in the centre of Tianjin City before turning north-west.
Northern Canal and Tonghui River.
In Tianjin the canal heads northwest, for a short time following the course of the Yongding, a tributary of the Hai River, before branching off toward Tongzhou on the edge of the municipality of Beijing. It is here that the modern canal stops and that a Grand Canal Cultural Park has been built. During the Yuan dynasty a further canal, the Tonghui River, connected Tongzhou with a wharf called the Houhai or "rear sea" in central Beijing. In the Ming and Qing dynasties, however, the water level in the Tonghui River dropped and it was impossible for ships to travel from Tongzhou to Beijing. Tongzhou became the northern shipping terminus of the canal. Cargoes were unloaded at Tongzhou and transported to Beijing by land. The Tonghui river still exists as a wide, concrete lined storm-channel and drain for the suburbs of Beijing.
Elevations.
Though the canal nominally crosses the watersheds of five river systems, in reality the variation between these is so low that it has only a single summit section. The elevation of the canal bed varies from 1 m below sea level at Hangzhou to 38.5 m above at its summit. At Beijing it reaches 27 m, fed by streams flowing downhill from the mountains to the west. The water flows from Beijing toward Tianjin, from Nanwang north toward Tianjin, and from Nanwang south toward Yangzhou. The water level in the Jiangnan Canal remains scarcely above sea level (the Zhenjiang ridge is 12 meters higher than the Yangzi River).
Uses.
Transportation.
From the Tang to Qing dynasties, the Grand Canal served as the main artery between northern and southern China and was essential for the transport of grain to Beijing. Although it was mainly used for shipping grain, it also transported other commodities and the corridor along the canal developed into an important economic belt. Records show that, at its height, every year more than 8,000 boats transported four to six million dan (240,000–360,000 metric tons) of grain. The convenience of transport also enabled rulers to lead inspection tours to southern China. In the Qing dynasty, the Kangxi and Qianlong emperors made twelve trips to the south, on all occasions but one reaching Hangzhou.
The Grand Canal also enabled cultural exchange and political integration to mature between the north and south of China. The canal even made a distinct impression on some of China's early European visitors. Marco Polo recounted the Grand Canal's arched bridges as well as the warehouses and prosperous trade of its cities in the 13th century. The famous Roman Catholic missionary Matteo Ricci travelled from Nanjing to Beijing on the canal at the end of the 16th century.
Since the founding of the People's Republic of China in 1949, the canal has been used primarily to transport vast amounts of bulk goods such as bricks, gravel, sand, diesel and coal. The Jianbi shiplocks on the Yangtze are currently handling some 75,000,000 tons each year, and the Li Canal is forecast to reach 100,000,000 tons in the next few years.
South-North Water Transfer Project.
The Grand Canal is currently being upgraded to serve as the Eastern Route of the South-North Water Transfer Project. Additional amounts of water from the Yangtze will be drawn into the canal in Jiangdu City, where a giant pumping station was already built in the 1980s, and is then fed uphill by pumping stations along the route and through a tunnel under the Yellow River, from where it can flow downhill to reservoirs near Tianjin. Construction on the Eastern Route officially began on December 27, 2002, and water was supposed to reach Tianjin by 2012. However, water pollution has affected the viability of this project.
Notable travellers.
In 1169, with China divided between the Jurchen-led Jin dynasty in the north and the Southern Song dynasty in the south, the Chinese emperor sent a delegation to the Jurchen to wish their ruler well for the New Year. A scholar-official named Lou Yue, secretary to the delegation, recorded the journey, much of which was made upon the Grand Canal, and submitted his "Diary of a Journey to the North" to the emperor on his return.
In 1170 the poet, politician and historian Lu You travelled upon the Grand Canal from Shaoxing to the river Yangtze, recording his progress in a diary.
In 1345 Arab traveler Ibn Battuta traveled China and journeyed through the Abe Hayat river (Grand Canal) up to the capital Khanbalik (Beijing).
In 1488, the shipwrecked Korean scholar Choe Bu travelled the length of the Grand Canal on his way from Zhejiang to Beijing (and on to Korea), and left a detailed account of his trip.
In 1600, Matteo Ricci, a famous Italian Christian missionary, traveled to Beijing from Nanjing via the Grand Canal waterway to try to get the support of Emperor of Ming Dynasty with the help of Wang Zhongde, the Director of the Board of Rites in the central government of China at the time.
In 1793, after a largely fruitless diplomatic mission to Jehol, a large part of Lord Macartney's embassy returned south to the Yangtze delta on the Grand Canal.

</doc>
<doc id="49827" url="https://en.wikipedia.org/wiki?curid=49827" title="Arcade game">
Arcade game

An arcade game or coin-op is a coin-operated entertainment machine typically installed in public businesses such as restaurants, bars and amusement arcades. Most arcade games are video games, pinball machines, electro-mechanical games, redemption games or merchandisers.
While exact dates are debated, the golden age of arcade video games is usually defined as a period beginning sometime in the late 1970s and ending sometime in the mid-1980s. Excluding a brief resurgence in the early 1990s, the arcade industry subsequently declined in the Western hemisphere as competing home-based video game consoles increased in capability and decreased in cost.
Arcade action games.
The term "arcade game" is also used to refer to an action video game that was designed to play similarly to an arcade game with frantic, addictive gameplay. The focus of arcade action games is on the user's reflexes, and the games usually feature very little puzzle-solving, complex thinking, or strategy skills. Games with complex thinking are called strategy video games or puzzle video games.
History.
The first popular "arcade games" included early amusement-park midway games such as shooting galleries, ball-toss games, and the earliest coin-operated machines, such as those that claimed to tell a person's fortune or that played mechanical music. The old midways of 1920s-era amusement parks (such as Coney Island in New York) provided the inspiration and atmosphere of later arcade games.
In the 1930s the first coin-operated pinball machines emerged. These early amusement machines differed from their later electronic cousins in that they were made of wood. They lacked plungers or lit-up bonus surfaces on the playing field, and used mechanical instead of electronic scoring-readouts. By around 1977 most pinball machines in production switched to using solid-state electronics both for operation and for scoring.
Electro-mechanical games.
In 1966, Sega introduced an electro-mechanical game called "Periscope". It was an early submarine simulator and light gun shooter, which used lights and plastic waves to simulate sinking ships from a submarine. It became an instant success in Japan, Europe, and North America, where it was the first arcade game to cost a quarter per play, which would remain the standard price for arcade games for many years to come. In 1967, Taito released an electro-mechanical arcade game of their own, "Crown Soccer Special", a two-player sports game that simulated association football, using various electronic components, including electronic versions of pinball flippers.
The company Sega later produced gun games which resemble first-person shooter video games, but were in fact electro-mechanical games that used rear image projection in a manner similar to the ancient zoetrope to produce moving animations on a screen. The first of these was the light gun game "Duck Hunt", which Nintendo released in 1969; it featured animated moving targets on a screen, printed out the player's score on a ticket, and had sound effects that were volume controllable. That same year, Sega released an electro-mechanical arcade racing game "Grand Prix", which had a first-person view, electronic sound, a dashboard with a racing wheel and accelerator, and a forward-scrolling road projected on a screen. Another Sega release that year was "Missile", a shooter and vehicle combat simulation that featured electronic sound and a moving film strip to represent the targets on a projection screen. It was also the earliest known arcade game to feature a joystick with a fire button, which was used as part of an early dual-control scheme, where two directional buttons are used to move the player's tank and a two-way joystick is used to shoot and steer the missile onto oncoming planes displayed on the screen; when a plane is hit, an explosion is animated on screen along with an explosion sound. In 1970, the game was released in North America as "S.A.M.I." by Midway. That same year, Sega released "Jet Rocket", a combat flight simulator featuring cockpit controls that could move the player aircraft around a landscape displayed on a screen and shoot missiles onto targets that explode when hit.
Throughout the 1970s, electro-mechanical arcade games were gradually replaced by electronic video games, following the release of "Pong" in 1972. In 1972, Sega released an electro-mechanical game called "Killer Shark", a first-person light gun shooter known for appearing in the 1975 film "Jaws". In 1974, Nintendo released "Wild Gunman", a light gun shooter that used full-motion video projection from 16 mm film to display live-action cowboy opponents on the screen. One of the last successful electro-mechanical arcade games was "F-1", a racing game developed by Namco and distributed by Atari in 1976; the game was shown in the films "Dawn of the Dead" (1978) and "Midnight Madness" (1980), as was Sega's "Jet Rocket" in the latter film. The 1978 video game "Space Invaders", however, dealt a yet more powerful blow to the popularity of electro-mechanical games.
Arcade video games.
In 1971 students at Stanford University set up the "Galaxy Game", a coin-operated version of the Spacewar video game. This ranks as the earliest known instance of a coin-operated video game. Later in the same year, Nolan Bushnell created the first mass-manufactured game, "Computer Space", for Nutting Associates.
In 1972, Atari was formed by Nolan Bushnell and Ted Dabney. Atari essentially created the coin-operated video game industry with the game "Pong", the first successful electronic ping pong video game. "Pong" proved to be popular, but imitators helped keep Atari from dominating the fledgling coin-operated video game market.
Golden age.
Taito's "Space Invaders", in 1978, proved to be the first blockbuster arcade video game. Its success marked the beginning of the golden age of arcade video games. Video game arcades sprang up in shopping malls, and small "corner arcades" appeared in restaurants, grocery stores, bars and movie theaters all over the United States, Japan and other countries during the late 1970s and early 1980s. "Space Invaders" (1978), "Galaxian" (1979), "Pac-Man" (1980), "Battlezone" (1980), "Defender" (1980), and "Bosconian" (1981) were especially popular. By 1981, the arcade video game industry was worth $8 billion ($ in ).
During the late 1970s and 1980s, chains such as Chuck E. Cheese's, Ground Round, Dave and Busters, ShowBiz Pizza Place and Gatti's Pizza combined the traditional restaurant and/or bar environment with arcades. By the late 1980s, the arcade video game craze was beginning to fade due to advances in home video game console technology. By 1991, US arcade video game revenues had fallen to $2.1 billion.
Late 1980s.
Sega AM2's "Hang-On", designed by Yu Suzuki and running on the Sega Space Harrier hardware, was the first of Sega's "Super Scaler" arcade system boards that allowed pseudo-3D sprite-scaling at high frame rates. The pseudo-3D sprite/tile scaling was handled in a similar manner to textures in later texture-mapped polygonal 3D games of the 1990s. Designed by Sega AM2's Yu Suzuki, he stated that his "designs were always 3D from the beginning. All the calculations in the system were 3D, even from Hang-On. I calculated the position, scale, and zoom rate in 3D and converted it backwards to 2D. So I was always thinking in 3D." It was controlled using a video game arcade cabinet resembling a motorbike, which the player moves with their body. This began the "Taikan" trend, the use of motion-controlled hydraulic arcade cabinets in many arcade games of the late 1980s, two decades before motion controls became popular on video game consoles.
Renaissance.
In the early 1990s, the arcades experienced a major resurgence with the 1991 release of Capcom's "Street Fighter II", which popularized competitive fighting games and revived the arcade industry to a level of popularity not seen since the days of "Pac-Man", setting off a renaissance for the arcade game industry in the early 1990s. Its success led to a wave of other popular games which mostly were in the fighting genre, such as "Pit-Fighter" (1990) by Atari, "Mortal Kombat" by Midway Games, "" (1992) by SNK, "Virtua Fighter" (1993) by SEGA, "Killer Instinct" (1994) by Rare, and "The King of Fighters" (1994–2005) by SNK. In 1993, "Electronic Games" noted that when "historians look back at the world of coin-op during the early 1990s, one of the defining highlights of the video game art form will undoubtedly focus on fighting/martial arts themes" which it described as "the backbone of the industry" at the time.
3D polygon graphics were popularized by the Sega Model 1 games "Virtua Racing" (1992) and "Virtua Fighter" (1993), followed by racing games like the Namco System 22 title "Ridge Racer" (1993) and Sega Model 2 title "Daytona USA", and light gun shooters like Sega's "Virtua Cop" (1994) and Mesa Logic's "Area 51" (1995), gaining considerable popularity in the arcades. By 1994, arcade games in the United States were generating revenues of $7 billion in quarters (equivalent to $ in ), in comparison to home console game sales of $6 billion, with many of the best-selling home video games in the early 1990s often being arcade ports. Combined, total US arcade and console game revenues of $13 billion in 1994 ($ in ) was nearly two and a half times the $5 billion revenue grossed by movies in the United States at the time.
Around the mid-1990s, the fifth-generation home consoles, Sega Saturn, PlayStation, and Nintendo 64, began offering true 3D graphics. By 1995, personal computers followed, with 3D accelerator cards. While arcade systems such as the Sega Model 3 remained considerably more advanced than home systems in the late 1990s, the technological advantage that arcade games had, in their ability to customize and use the latest graphics and sound chips, slowly began narrowing, and the convenience of home games eventually caused a decline in arcade gaming. Sega's sixth generation console, the Dreamcast, could produce 3D graphics comparable to the Sega NAOMI arcade system in 1998, after which Sega produced more powerful arcade systems such as the Sega NAOMI Multiboard and Sega Hikaru in 1999 and the Sega NAOMI 2 in 2000, before Sega eventually stopped manufacturing expensive proprietary arcade system boards, with their subsequent arcade boards being based on more affordable commercial console or PC components.
Decline.
Arcade video games had declined in popularity so much by the late 1990s, that revenues in the United States dropped to $1.33 billion in 1999, and reached a low of $866 million in 2004. Furthermore, by the early 2000s, networked gaming via computers and then consoles across the Internet had also appeared, replacing the venue of head-to-head competition and social atmosphere once provided solely by arcades.
The arcades also lost their status as the forefront of new game releases. Given the choice between playing a game at an arcade three or four times (perhaps 15 minutes of play for a typical arcade game), and renting, at about the same price, exactly the same game—for a video game console—the console became the preferred choice. Fighting games were the most attractive feature for arcades, since they offered the prospect of face-to-face competition and tournaments, which correspondingly led players to practice more (and spend more money in the arcade), but they could not support the business all by themselves.
To remain viable, arcades added other elements to complement the video games such as redemption games, merchandisers, and food service. Referred to as "fun centers" or "family fun centers", some of the longstanding chains such as Chuck E. Cheese's and Gatti's Pizza ("GattiTowns") also changed to this format. Many old video game arcades have long since closed, and classic coin-operated games have become largely the province of dedicated hobbyists.
Today.
Today's arcades have found a niche in games that use special controllers largely inaccessible to home users. An alternative interpretation (one that includes fighting games, which continue to thrive and require no special controller) is that the arcade game is now a more socially-oriented hangout, with games that focus on an individual's performance, rather than the game's content, as the primary form of novelty. Examples of today's popular genres are rhythm games such as "Dance Dance Revolution" (1998) and "DrumMania" (1999), and rail shooters such as "Virtua Cop" (1994), "Time Crisis" (1995) and "House of the Dead" (1996).
In the Western world, the arcade video game industry still exists today but in a greatly reduced form. Video arcade game hardware is often based on home game consoles to reduce development costs; there are video arcade versions of Dreamcast (NAOMI, Atomiswave), PlayStation 2 (System 246), Nintendo GameCube (Triforce), and Microsoft Xbox (Chihiro) home consoles and PC (e.g. Taito Type X). Some arcades have survived by expanding into ticket-based prize redemption and more physical games with no home console equivalent, such as skee ball and Whac-A-Mole. Some genres, particularly dancing and rhythm games (such as Konami's "Dance Dance Revolution"), continue to be popular in arcades.
Worldwide, arcade game revenues gradually increased from $1.8 billion in 1998 to $3.2 billion in 2002, rivalling PC game sales of $3.2 billion that same year. In particular, arcade video games are a thriving industry in China, where arcades are widespread across the country. The US market has also experienced a slight resurgence, with the number of video game arcades across the nation increasing from 2,500 in 2003 to 3,500 in 2008, though this is significantly less than the 10,000 arcades in the early 1980s. As of 2009, a successful arcade game usually sells around 4000 to 6000 units worldwide.
The relative simplicity yet solid gameplay of many of these early games has inspired a new generation of fans who can play them on mobile phones or with emulators such as MAME. Some classic arcade games are reappearing in commercial settings, such as Namco's "Ms. Pac-Man 20 Year Reunion / Galaga Class of 1981" two-in-one game, or integrated directly into controller hardware (joysticks) with replaceable flash drives storing game ROMs. Arcade classics have also been reappearing as mobile games, with "Pac-Man" in particular selling over 30 million downloads in the United States by 2010. Arcade classics have also begun to appear on multi-game arcade machines for home users.
Japan.
In the Japanese gaming industry, arcades have remained popular through to the present day. As of 2009, out of Japan's $20 billion gaming market, $6 billion of that amount is generated from arcades, which represent the largest sector of the Japanese video game market, followed by home console games and mobile games at $3.5 billion and $2 billion, respectively. In 2005, arcade ownership and operation accounted for a majority of Namco's for example. With considerable withdrawal from the arcade market from companies such as Capcom, Sega became the strongest player in the arcade market with 60% marketshare in 2006. However, due to the country's economic recession, the Japanese arcade industry has also been steadily declining, from ¥702.9 billion (US$8.7 billion) in 2007 to ¥504.3 billion ($6.2 billion) in 2010. In 2013, estimation of revenue is ¥470 billion.
In the Japanese market, network and card features introduced by "Virtua Fighter 4" and "World Club Champion Football", and novelty cabinets such as machines have caused revitalizations in arcade profitability in Japan. The reason for the continued popularity of arcades in comparison to the west, are heavy population density and an infrastructure similar to casino facilities.
Former rivals in the Japanese arcade industry, Konami, Taito, Bandai Namco and Sega, are now working together to keep the arcade industry vibrant. This is evidenced in the sharing of arcade networks, and venues having games from all major companies rather than only games from their own company.
Technology.
Virtually all modern arcade games (other than the very traditional midway-type games at county fairs) make extensive use of solid state electronics and integrated circuits. In the past, coin-operated arcade video games generally used custom per-game hardware often with multiple CPUs, highly specialized sound and graphics chips, and the latest in expensive computer graphics display technology. This allowed arcade system boards to produce more complex graphics and sound than what was then possible on video game consoles or personal computers, which is no longer the case today. Recent arcade game hardware is often based on modified video game console hardware or high-end PC components.
Arcade games frequently have more immersive and realistic game controls than either PC or console games, including specialized ambiance or control accessories: fully enclosed dynamic cabinets with force feedback controls, dedicated lightguns, rear-projection displays, reproductions of automobile or airplane cockpits, motorcycle or horse-shaped controllers, or highly dedicated controllers such as dancing mats and fishing rods. These accessories are usually what set modern video games apart from other games, as they are usually too bulky, expensive, and specialized to be used with typical home PCs and consoles.
Arcade genre.
Arcade games often have short levels, simple and intuitive control schemes, and rapidly increasing difficulty. This is due to the environment of the Arcade, where the player is essentially renting the game for as long as their in-game avatar can stay alive (or until they run out of tokens).
Games on consoles or PCs can be referred to as "arcade games" if they share these qualities or are direct ports of arcade titles. Many independent developers are now producing games in the arcade genre that are designed specifically for use on the Internet. These games are usually designed with Flash/Java/DHTML and run directly in web-browsers.
Arcade racing games have a simplified physics engine and do not require much learning time when compared with racing simulators. Cars can turn sharply without braking or understeer, and the AI rivals are sometimes programmed so they are always near the player (rubberband effect).
Arcade flight games also use simplified physics and controls in comparison to flight simulators. These are meant to have an easy learning curve, in order to preserve their action component. Increasing numbers of console flight video games, from to Ace Combat and Secret Weapons Over Normandy indicate the falling of manual-heavy flight sim popularity in favor of instant arcade flight action.
Other types of arcade-style games include fighting games (often played with an arcade controller), beat 'em up games (including fast-paced hack and slash games), light gun rail shooters and "bullet hell" shooters (intuitive controls and rapidly increasing difficulty), music games (particularly rhythm games), and mobile/casual games (intuitive controls and often played in short sessions).
Emulation.
Emulators such as MAME, which can be run on modern computers and a number of other devices, aim to preserve the games of the past.
Legitimate emulated titles started to appear on the Sony PlayStation and Sega Saturn, with CD-ROM compilations such as , and on the PlayStation 2 and GameCube with DVD-ROM titles such as Midway Arcade Treasures.
Arcade games are currently being downloaded and emulated through the Nintendo Wii Virtual Console Service starting in 2009 with "Gaplus", "Mappy", "Space Harrier", "Star Force", "The Tower of Druaga", "Tecmo Bowl", "Altered Beast" and many more. Other classic arcade games such as "Asteroids", "Tron", "Discs of Tron", "Yie Ar Kung-Fu", "Pac-Man", "Joust", "Battlezone", "Dig Dug", "", and "Missile Command" are emulated on PlayStation Network and Xbox Live Arcade.
Locations.
In addition to restaurants and video arcades, arcade games are also found in bowling alleys, college campuses, dormitories, laundromats, movie theaters, supermarkets, shopping malls, airports, ice rinks, corner shops, truck stops, bar/pubs, hotels, and even bakeries. In short, arcade games are popular in places open to the public where people are likely to have free time.
List of highest-grossing arcade video games.
For arcade games, success was usually judged by either the number of arcade hardware units sold to operators, or the amount of revenue generated, from the number of coins (such as quarters or 100 yen coins) inserted into machines, and/or the hardware sales (with arcade hardware prices often ranging from $1000 to $4000 or more). This list only includes arcade games that have either sold more than 1000 hardware units or generated a revenue of more than US$1 million. Most of the games in this list date back to the golden age of arcade video games, though some are also from before and after the golden age.
Best-selling arcade video game franchises.
These are the combined hardware sales of at least two or more arcade games that are part of the same franchise. This list only includes franchises that have sold at least 5,000 hardware units or grossed at least $10 million revenues.

</doc>
<doc id="49828" url="https://en.wikipedia.org/wiki?curid=49828" title="Otto von Guericke">
Otto von Guericke

Otto von Guericke (originally spelled "Gericke," ) (November 20, 1602 – May 11, 1686 (Julian calendar); November 30, 1602 – May 21, 1686 (Gregorian calendar)) was a German scientist, inventor, and politician. His major scientific achievements were the establishment of the physics of vacuums, the discovery of an experimental method for clearly demonstrating electrostatic repulsion, and his advocacy of the reality of "action at a distance" and of "absolute space".
Biography.
Otto von Guericke was born to a patrician family of Magdeburg, Germany. In 1617 he became a student at the Leipzig University. Owing to the outbreak of the Thirty Years War his studies at Leipzig were disrupted and subsequently he studied at the Academia Julia in Helmstedt and the universities of Jena and Leyden. At the last of these he attended courses on mathematics, physics and fortification engineering. His education was completed by a nine-month-long trip to France and England. On his return to Magdeburg in 1626 he married Margarethe Alemann and became a member of the Rats Collegium of Magdeburg. He was to remain a member of this body until old age.
Von Guericke was personally distrustful of the city's enthusiasm for the cause of Gustavus Adolphus but was nonetheless a victim of the fall of Magdeburg to von Tilly's troops in May 1631. Destitute, but fortunate to escape with his life, he was an Imperial prisoner at a camp in Fermersleben until, through the good offices of Ludwig of Anhalt-Cothen, a ransom of three hundred thalers had been paid. Following a period of employment as engineer in the service of Gustavus Adolphus he and his family returned to Magdeburg in February 1632. For the next decade he was occupied rebuilding his own and the city's fortunes from the ruins of the fire of 1631. Under the Swedish and subsequently Saxon authorities he remained involved in the civic affairs of the city, becoming in 1641 a Kammerer and in 1646 Burgomaster, a position he was to hold for thirty years. His first diplomatic mission on behalf of the city, in September 1642, was to the court of the Elector of Saxony at Dresden to seek some mitigation of the harshness with which the Saxon military commander treated Magdeburg. Diplomatic missions, often dangerous as well as tedious, occupied much of his time for the next twenty years. A private scientific life, of which much remains unclear, was developing in parallel.
His scientific and diplomatic pursuits finally intersected when, at the Reichstag in Regensburg in 1654, he was invited to demonstrate his experiments on the vacuum before the highest dignitaries of the Holy Roman Empire. One of them, the Archbishop Elector Johann Philip von Schonborn, bought von Guericke's apparatus from him and had it sent to his Jesuit College at Wurzburg. One of the professors at the College, Fr. Gaspar Schott, entered into friendly correspondence with von Guericke and thus it was that, at the age of 55, von Guericke's work was first published as an Appendix to a book by Fr. Schott - Mechanica Hydraulico-pneumatica - published in 1657. This book came to the attention of Robert Boyle who, stimulated by it, embarked on his own experiments on air pressure and the vacuum, and in 1660 published "New Experiments Physico-Mechanical touching the Spring of Air and its Effects". The following year this was translated into Latin and, made aware of it in correspondence with Fr. Schott, von Guericke acquired a copy.
In the decade following the first publication of his own work von Guericke, in addition to his diplomatic and administrative commitments, was scientifically very active. He embarked upon his magnum opus — "Ottonis de Guericke Experimenta Nova (ut vocantur) Magdeburgica de Vacuo Spatio" — which as well as a detailed account of his experiments on the vacuum, contains his pioneering electrostatic experiments in which electrostatic repulsion was demonstrated for the first time and sets out his theologically based view of the nature of Space. In the Preface to the Reader he claims to have finished the book on March 14, 1663 though publication was delayed for another nine years until 1672. In 1664, his work again appeared in print, again through the good offices of Fr. Schott, the first section of whose book "Technica Curiosa", entitled Mirabilia Magdeburgica, was dedicated to von Guericke's work. The earliest reference to the celebrated Magdeburg hemispheres experiment is on p. 39 of the Technica Curiosa where Fr. Schott notes that von Guericke had mentioned them in a letter of July 22, 1656. Fr. Schott goes on to quote a subsequent letter of von Guericke of August 4, 1657 in which he states that he now had carried out the experiment, at considerable cost, with 12 horses.
The 1660s saw the final collapse of Magdeburg's aim, to which von Guericke had devoted some twenty years of diplomatic effort, of achieving the status of a Free City within the Holy Roman Empire. On behalf of Magdeburg, he was the first signatory to the Treaty of Klosterberg (1666) whereby Magdeburg accepted a garrison of Brandenburg troops and the obligation to pay dues to the Great Elector, Friedrich Wilhelm I of Brandenburg. Despite the Elector's crushing of Magdeburg's political aspirations, the personal relationship of von Guericke and Friedrich Wilhelm remained warm. The Great Elector was a patron of scientific scholarship; he had employed von Guericke's son, Hans Otto, as his Resident in Hamburg and in 1666 had named Otto himself to the Brandenburg Rat. When the Experimenta Nova finally appeared it was prefaced with a fulsome dedication to Friedrich Wilhelm. The year 1666 also saw von Guericke's ennoblement by Leopold I, Holy Roman Emperor when he changed the spelling of his name from "Gericke" to "Guericke" and when he became entitled to the prefix "von". Schimank p. 69 reproduces von Guericke's petition to Leopold requesting the prefix "von" and the change of spelling.
In 1677 von Guericke, after repeated requests, was reluctantly permitted to step down from his civic responsibilities. In January 1681, as a precaution against an outbreak of plague then affecting Magdeburg, he and his second wife Dorothea moved to the home of his son Hans Otto in Hamburg. There he died peacefully on May 11 (Julian) 1686, 55 years to the day after he had fled the flames in 1631. His body was returned to Magdeburg for interment in the Ulrichskirche on May 23 (Julian) (Schneider p. 144). The Otto von Guericke University of Magdeburg is named after him.
There are only three important contemporary sources describing Von Guericke's scientific work - Fr. Schott's Mechanica Hydraulico-pneumatica and Technica Curiosa of 1657 and 1664 and his own Experimenta Nova of 1672. His scientific concerns may be divided into three areas, to each of which a Book of the Experimenta Nova is dedicated as follows:
Nature of Space and the Possibility of the Void.
Book II of the Experimenta Nova is an extended philosophical essay in which von Guericke puts forward a view of the nature of Space similar to that later espoused by Newton. He is explicitly critical of the plenist views of Aristotle and of their adoption by his younger contemporary Descartes. A particular and repeated target of his criticism is the manner in which the "nature abhors a vacuum" principle had migrated from simply a matter of experiment to a high principle of physics which could be invoked to explain phenomena such as suction but which itself was above question. In setting out his own view, von Guericke, while acknowledging the influence of previous philosophers such as Lessius (but not Gassendi), makes it clear that he considers his thinking on this topic to be original and new. There is no evidence that von Guericke was aware of the "Nouvelles Experiences touchant le vide" of Blaise Pascal published in 1647. In the Experimenta Nova Book III Ch. 34 he relates how he first became aware of Torricelli's mercury tube experiment from Valerianus Magnus at Regensburg in 1654. Pascal's work built upon reports of the mercury tube experiment which had reached Paris via Marin Mersenne in 1644. An indication of the unresolved status of the "nature abhors a vacuum" principle at that time may be taken from Pascal's opinion, expressed in the conclusion of the Nouvelles Experiences, when he writes: "I hold for true the maxims set out below : (a) that all bodies possess a repugnance to being separated one from another and from admitting a vacuum in the interval between them - that is to say that nature abhors a void." Pascal goes on to claim that this abhorrence of a void is however a limited force and thus that the creation of a vacuum is possible.
There were three broad currents of opinion from which von Guericke dissented. Firstly there was the Aristotelian view that there simply was no void and that everything that exists objectively is in the category of substance. The general plenist position lost credibility in the 17th century, owing primarily to the success of Newtonian mechanics. It was revived again in the 19th century as a theory of an all pervading aether and again lost plausibility with the success of Special Relativity. Secondly, there was the Augustinian position of an intimate relation between space, time, and matter; all three, according to St. Augustine in the Confessions and the City of God,(Confessions Ch. XI,City of God Book XI Ch. VI) came into being as a unity and ways of speaking that purport to separate them - such as "outside the universe" or "before the beginning of the universe" are, in fact, meaningless. Augustine's way of thinking is also attractive to many and seems to have a strong resonance with General Relativity. The third view, which von Guericke discusses at length, but does not attribute to any individual, is that Space is a creation of the human Imagination. Thus it is not truly objective in the sense that matter is objective. The later theories of Leibniz and Kant seem inspired by this general outlook, but the denial of the objectivity of Space has not been scientifically fruitful.
Von Guericke sidestepped the vexed question of the meaning of "nothing" by asserting that all objective reality fell into one of two categories - the created and the uncreated. Space and Time were objectively real but were uncreated, whereas matter was created. In this way he created a new fundamental category alongside Aristotle's category of substance, that of the uncreated. His understanding of Space is theological and similar to that expressed by Newton in the Scholium to the Principia. For instance, von Guericke writes (Book II Chapter VII) "For God cannot be contained in any location, nor in any vacuum, nor in any space, for He Himself is, of His nature, location and vacuum."
Air pressure and the vacuum.
In 1654 von Guericke invented a vacuum pump consisting of a piston and an air gun cylinder with two-way flaps designed to pull air out of whatever vessel it was connected to, and used it to investigate the properties of the vacuum in many experiments. This pump is described in Chapters II and III of Book III of the Experimenta Nova and in the Mechanica Hydraulico-pneumatica (p. 445-6). Guericke demonstrated the force of air pressure with dramatic experiments.
In 1657, he machined two 20-inch diameter hemispheres and pumped all the air out of them, locking them together with a vacuum seal. The air pressure outside held the halves together so tightly that sixteen horses, eight harnessed to each side of the globe, could not pull the halves apart. It would have required more than 4,000 pounds of force to separate them.
With his experiments Guericke disproved the hypothesis of "horror vacui", that nature abhors a vacuum. Aristotle in e.g. Physics IV 6-9 had argued against the existence of the void and his views commanded near universal endorsement by philosophers and scientists up to the 17th century. Guericke showed that substances were not pulled by a vacuum, but were pushed by the pressure of the surrounding fluids.
All of von Guericke's work on the vacuum and air pressure is described in Book III of the Experimenta Nova (1672). As regards the more detailed chronology of his work we have, in addition to the Experimenta Nova's description of his demonstrations at Regensburg in 1654, the two accounts published by Fr.Schott in 1657 and 1663.
In Chapter 27 he alludes to what transpired at Regensburg in 1654. The first experiment he explicitly records as having been demonstrated was the crushing of a non-spherical vessel as the air was withdrawn from it. He did not use a vacuum pump directly on the vessel but allowed the air in it to expand into a previously evacuated receiver.
The second was an experiment in which a number of men proved able to pull an airtight piston only about half way up a cylindrical copper vessel. Von Guericke then attached his evacuated Receiver to the space below the piston and succeeded in drawing the piston back down again against the force of the men pulling it up. In a letter to Fr. Schott of June 1656, reproduced in Mechanica Hydraulico-pneumatica (p. 454-455), von Guericke gives a short account of his experiences at Regensburg. Based on this, Schimank gives a list of ten experiments which he considers likely to have been carried out at Regensburg. In addition to the above two, these included the extraction of air using a vacuum pump, the extinction of a flame in a sealed vessel, the raising of water by suction, a demonstration that air has weight, and a demonstration of how fog and mist can be produced in a sealed vessel. The Mechanica Hydraulico-pneumatica also provides the earliest drawing of von Guericke's vacuum pump. This corresponds to the description in the opening chapters of Book III of the Experimenta Nova of the first version of his pump.
Stimulated by the interest taken in his work von Guericke was scientifically very active in the decade after 1654. In June 1656 we find him writing to Fr. Schott (Mechanica Hydraulico-pneumatica p. 444) "Since the time when I produced the exhibition for the said eminent Elector, I have a better and clearer grasp of all these matters and many other topics as well." The celebrated hemispheres experiment was, as noted in the biographical section above, carried out between July 1656 and August 1657. In Chapter IV of Book III he describes a new and much improved design of vacuum pump and attributes its invention to the need for a more easily transportable machine with which he could demonstrate his experiments to Frederick William who had expressed the desire to see them. The new pump is also described on p. 67 of the Technica Curiosa. The demonstration in the Elector's Library at Cölln an der Spree took place in November 1663 and was recorded by a tutor to the Elector's sons. (Schneider p. 113.) A number of experiments, such as the rather cruel testing of the effect of a vacuum on birds and fish (Experimenta Nova Book III Chapter XVI), are not described in the Technica Curiosa. Although the Experimenta Nova does contain correspondence from 1665, there is no reason to doubt von Guericke's assertion that the work was essentially finished by March 1663.
Throughout Books II and III he returns again and again to the theme of there being no abhorrence of a vacuum and that all the phenomena explained by this supposed principle are in fact attributable to the pressure of the atmosphere in conjunction with various incorporeal potencies which he held to be acting. Thus the Earth's "conservative potency" (virtus conservativa) provided the explanation for the fact that the Earth retains its atmosphere though travelling through space. In countering the objection of a Dr. Deusing that the weight of the atmosphere would simply crush the bodies of all living things, he shows explicit awareness of the key property of a fluid - that it exerts pressure equally across all planes. In Chapter XXX of Book III he writes: "Dr. Deusing ought to have borne in mind that the air does not just press on our heads but flows all around us. Just as it presses from above on the head, it likewise presses on the soles of the feet from below and simultaneously on all parts of the body from all directions."
Other research.
In the Experimenta Nova Book III Chapter 20 von Guericke reports on a barometer he had constructed and its application to weather forecasting. The earliest reference to his barometer is in a letter to Fr. Schott of November 1661 (Technica Curiosa p. 37) where he writes: "I have observed the variation in the weight of the air by using a little man (i.e. a statue in the form of one) who hangs from a wall in my hypocaust where it floats on air in a glass tube and uses a finger to show the weight or lightness of the air. At the same time it indicates whether or not it is raining in nearby localities or whether there is unusually stormy weather at sea." In a subsequent letter of December 30, 1661 (Technica Curiosa p. 52) he gives a somewhat amplified account. His barometer thus prepared the way for meteorology. His later works focused on electricity. He invented the first electrostatic generator, the "Elektrisiermaschine", of which a version is illustrated in the engraving by Hubert-François Gravelot, c. 1750.
Electrostatic investigations.
Von Guericke thought of the capacity of body to exert an influence beyond its immediate boundaries in terms of "corporeal and incorporeal potencies". Examples of "corporeal potencies" were the giving off of fumes, smells, gases etc. by bodies. An example of an "incorporeal potencies" was the Earth's "conservative potency" whereby it retained its atmosphere and caused the return of objects thrown upwards to the Earth's surface. The Earth also possessed an "expulsive potency" which was deemed to explain why objects that fall bounce back up again. The notion of an "incorporeal potency" is similar to that of "action at a distance" except the former notion remained purely qualitative and there is no inkling of the fundamental "action and reaction" principle.
Von Guericke describes his work on electrostatics in Chapter 15 of Book IV of the "Experimenta Nova". In a letter of November 1661 to Fr. Schott, reproduced in the Technica Curiosa, he notes that the then projected Book IV would be concerned with "cosmic potencies" (virtutes mundanae). Accepting the claim of the preface to the "Experimenta Nova" that the entire work had been essentially completed before March 1663, von Guericke can be fairly credited with inventing a primitive form of frictional electrical machine before 1663. He used a sulphur globe that could be rubbed by hand.
In Chapter 6 of Book IV von Guericke writes: "It seems reasonable to suppose that if the Earth has a fitting and appropriate attractive potency it will also have a potency of repelling things that might be dangerous or disagreeable to it. This is to be seen in the case of the sulphur sphere described below in Chapter 15. When that sphere is stroked or rubbed not only does it attract all light objects, but it sometimes arbitrarily also repels them before attracting them again. Sometimes indeed it doesn't even attract them again." Von Guericke was aware of both Gilbert's book "On the Magnet and Magnetic bodies and on the great magnet the Earth" published in 1600 and of the Jesuit Niccolo Cabeo's "Philosophia Magnetica" (1629). He does not explicitly acknowledge any anticipation of his demonstration of electrostatic repulsion by the latter but,as he quotes a passage from the same page, could not have been unaware that, in a discussion of the nature of electrical attraction, Cabeo had written (Philosophia Magnetica p. 192): " When we see that small bodies (corpuscula) are lifted (sublevari et attolli) above the amber and also fall back to the motionless amber, it cannot be said that such erratic behaviour (talem matum - but if "matum" is taken as a misprint for "motum", then the translation is simply "such motion") is an attraction by the gravity of the attracting body." In Book IV Chapter 8 of the Experimenta Nova von Guericke is at pains to point out the difference between his own "incorporeal potency" views and Cabeo's more Aristotelian conclusions. He writes : "Writers who have written on magnetism, always confuse it with electrical attraction, although there is a great difference. In particular, Gilbert in his book De Magnete claims that electrical attraction is caused by the effluence of a humour, that the humid seeks the humid and this is the cause of the attraction. Moreover in Philosophia Magnetica, Book 2 Chapter 21, Cabeo criticises Gilbert but does admit that this attraction is created by the agency of an effluent. Humidity does not play any role but the attraction is brought about purely by the agency of an effluent, by which the air is disturbed.After the initial impulse, the air returns to the amber again taking with it little particles. He (Cabeo) concludes by saying :'I say therefore that from amber or any other electrically attracting body, a very rarefied effluent is emitted which dispels and attenuates the air, extremely agitating it. Then the agitated and attenuated air returns to the amber body sweeping along with it whatever dust or small bodies are in its way'. We however, who in the previous chapter, take the attraction of the sulphur ball as electrical in nature and operating through a conservative potency, cannot admit that the air plays a role in producing the attraction. Experiment visibly shows that this sulphur globe (once it has been rubbed) also exercises its potency through a linen cord up to a range of a cubit and more and can attract at that distance."
The key Chapter 15 is entitled "On an experiment, in which these potencies, listed above,can be evoked by the rubbing of a sulphur ball." In Section 3 of this chapter he describes how light bodies are repelled from a sulphur sphere which has been rubbed with a dry hand, and are not again attracted until they have touched another body. Oldenburg's review of the Experimenta Nova (November 1672) in the Proceeding of the Royal Society sceptically observes: "How far this globe may be confided in, the Tryals and Consideration of some ingenious person here may perhaps inform us hereafter." In fact, Robert Boyle repeated von Guericke's experiments for the Royal Society in November 1672 and February 1673. (Schneider p. 127)

</doc>
<doc id="49829" url="https://en.wikipedia.org/wiki?curid=49829" title="Material dispersion coefficient">
Material dispersion coefficient

In an optical fiber, the material dispersion coefficient, "M"(λ), characterizes the amount of pulse broadening by material dispersion per unit length of fiber and per unit of spectral width. It is usually expressed in picoseconds per (nanometre·kilometre). 
For many optical fiber materials, "M"(λ) approaches zero at a specific wavelength λ0 between 1.3 and 1.5 μm. At wavelengths shorter than λ0, "M"(λ) is negative and increases with wavelength; at wavelengths longer than λ0, "M"(λ) is positive and decreases with wavelength. 
Pulse broadening caused by material dispersion in a unit length of optical fiber is given by the product of "M"(λ) and spectral width (Δλ).

</doc>
<doc id="49831" url="https://en.wikipedia.org/wiki?curid=49831" title="Light pen">
Light pen

A light pen is a computer input device in the form of a light-sensitive wand used in conjunction with a computer's CRT display. 
It allows the user to point to displayed objects or draw on the screen in a similar way to a touchscreen but with greater positional accuracy. A light pen can work with any CRT-based display and other display technologies, but its ability to be used with LCDs was unclear (though Toshiba and Hitachi displayed a similar idea at the "Display 2006" show in Japan).
A light pen detects a change of brightness of nearby screen pixels when scanned by cathode ray tube electron beam and communicates the timing of this event to the computer. Since a CRT scans the entire screen one pixel at a time, the computer can keep track of the expected time of scanning various locations on screen by the beam and infer the pen's position from the latest timestamp.
History.
The first light pen was created around 1955 as part of the Whirlwind project at MIT.
During the 1960s light pens were common on graphics terminals such as the IBM 2250, and were also available for the IBM 3270 text-only terminal.
The light pen was used in the early 1980s. It was notable for its use in the Fairlight CMI, and the BBC Micro. IBM PC compatible CGA, HGC and some EGA graphics cards featured a connector for a light pen as well. Even some consumer products were given light pens, such as the Thomson MO5 computer family as well as the Atari 8-bit and Commodore 8-bit home computers. 
Because the user was required to hold his arm in front of the screen for long periods of time or to use a desk that tilts the monitor, the light pen fell out of use as a general purpose input device.

</doc>
<doc id="49836" url="https://en.wikipedia.org/wiki?curid=49836" title="N-ary code">
N-ary code

In telecommunication, a "n"-ary code is a code that has "n" significant conditions, where "n" is a positive integer greater than 1. The integer substituted for "n" indicates the specific number of significant conditions, "i.e.", quantization states, in the code. For example, an 8-ary code has eight significant conditions and can convey three bits per code symbol. A prefix that indicates an integer, "e.g.", "bin", "tern," or "quatern", may be used in lieu of a numeral, to produce "binary", "ternary", or "quaternary" (2, 3, and 4 states respectively).
See also.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="49838" url="https://en.wikipedia.org/wiki?curid=49838" title="OGG">
OGG

Ogg may refer to:

</doc>
<doc id="49839" url="https://en.wikipedia.org/wiki?curid=49839" title="RealMedia">
RealMedia

RealMedia is a proprietary multimedia container format created by RealNetworks. Its extension is ".rm". It is typically used in conjunction with RealVideo and RealAudio and is used for streaming content over the Internet. Typically these streams are in CBR (constant bitrate), but a container for VBR (variable bitrate) streams, named RMVB (RealMedia variable bitrate), has been developed.
Support of RealMedia is available in a few multimedia players for different architectures/platforms, including RealPlayer and the Real Alternative codec pack. The open source DAP firmware Rockbox has added support for reading RM containers as of 6 July 2009, although only certain formats used in RM files are playable.

</doc>
<doc id="49842" url="https://en.wikipedia.org/wiki?curid=49842" title="Keyboard instrument">
Keyboard instrument

A keyboard instrument is a musical instrument played using a keyboard. The most common of these are the piano, organ, and various electronic keyboards, including synthesizers and digital pianos. Other keyboard instruments include celestas, which are struck idiophones operated by a keyboard, and carillons, which are usually housed in bell towers or belfries of churches or municipal buildings.
Today, the term "keyboard" often refers to keyboard-style synthesizers. Under the fingers of a sensitive performer, the keyboard may also be used to control dynamics, phrasing, shading, articulation, and other elements of expression—depending on the design and inherent capabilities of the instrument.
Another important use of the word "keyboard" is in historical musicology, where it means an instrument whose identity cannot be firmly established. Particularly in the 18th century, the harpsichord, the clavichord, and the early piano were in competition, and the same piece might be played on more than one. Hence in a phrase like "Mozart excelled as a keyboard player" the word "keyboard" is usefully noncommittal.
History.
The earliest keyboard instruments include the pipe organ, hurdy-gurdy, clavichord, and harpsichord. The organ is the oldest of these, appearing in the third century BC—though this early instrument, called hydraulis, did not use a keyboard in the modern sense. From its invention until the fourteenth century, the organ remained the only keyboard instrument. Often, the organ did not feature a keyboard at all, but rather buttons or large levers operated by a whole hand. Almost every keyboard until the fifteenth century had seven naturals to each octave.
The clavichord and the harpsichord appeared during the 14th century—the clavichord probably being earlier. The harpsichord and clavichord were both common until widespread adoption of the piano in the 18th century, after which their popularity decreased. The piano was revolutionary, because a pianist could vary the volume (or dynamics) of the sound by varying the vigor with which each key was struck. The piano's full name is "gravicèmbalo con piano e forte" meaning "harpsichord with soft and loud" but can be shortened to "piano-forte", which means "soft-loud" in Italian. In its current form, the piano is a product of the late 19th century, and is far removed in both sound and appearance from the "pianos" known to Mozart, Haydn, and Beethoven. In fact, the modern piano is significantly different from even the 19th-century pianos used by Liszt, Chopin, and Brahms. "See Piano history and musical performance".
Keyboard instruments were further developed in the early twentieth century. Early electromechanical instruments, such as the Ondes Martenot, appeared early in the century. This was a very important contribution to the keyboard's history.
Modern keyboards.
Much effort has gone into creating an instrument that sounds like the piano but lacks its size and weight. The electric piano and electronic piano were early efforts that, while useful instruments in their own right, did not convincingly reproduce the timbre of the piano. Electric and electronic organs were developed during the same period. More recent electronic keyboard designs strive to emulate the sound of specific make and model pianos using digital samples and computer models.

</doc>
<doc id="49852" url="https://en.wikipedia.org/wiki?curid=49852" title="Local exchange carrier">
Local exchange carrier

Local Exchange Carrier (LEC) is a regulatory term in telecommunications for the local telephone company.
In the United States, wireline telephone companies are divided into two large categories: long distance (interexchange carrier, or IXCs) and local (local exchange carrier, or LECs). This structure is a result of 1984 divestiture of then-regulated monopoly carrier American Telephone & Telegraph. Local telephone companies at the time of the divestiture are also known as Incumbent Local Exchange Carriers (ILEC).
The divestiture created local exchange carriers for the management of local telephone lines and switches, and provisioning of local phone services within their business area, as well as the long distance calls originating or terminating in their business area. The vast majority of the United States are served by LECs called Baby Bells, or RBOCs (Regional Bell Operating Companies). The rest of the United States, most commonly in rural or outlying suburban areas, are served by independent LECs, known in the industry simply as the "independents." Although independent companies typically serve these areas, RBOC LECs still have vast territories of low population density regions of the country. Therefore, independents generally exist as pockets of territory within a greater RBOC region. Popular independents are Frontier Communications, and Windstream Communications.
Local calls are defined as calls originating and terminating within a local access and transport area (LATA) which is defined by the Federal Communications Commission. All the Baby Bells, as well as other LECs, typically operate businesses in more than one LATA yet their services of local telephone calls are still defined by LATA boundaries, not their business areas.
Residential Local Exchange Services.
The following information applied to residential local telephone service in the Detroit, Michigan area during the 1970s and 1980s. Much about this subject has changed dramatically since that time, and continues to do so.
A local exchange carrier is a carrier of telephone calls and other communication services carried by telephone lines. A local exchange is generally either an exchange within one's own LATA or in an immediately adjacent LATA. A call that is neither local nor long distance is called a local toll call. A local exchange carrier normally sells package deals that include local and local toll calls. Local calls are customarily billed in by the call, or in blocks of calls. Residential local exchange carrier service typically charges formula_1, where formula_2 is the monthly minimum and covers the first formula_3 calls, formula_4 is the price per local call, and formula_5 is the total quantity of calls consumed. Local toll calls are each billed at formula_6, where formula_2 is minimum charge for a local toll call, formula_4 is the per-minute charge, and formula_9 is the duration of the call in minutes. Local toll calls are traditionally grouped into two price ranges, called "near zone" and "far zone", with values of formula_2 and formula_4 higher for far zone.
Duties.
Generally, the local exchange carrier has the following duties: 

</doc>
<doc id="49853" url="https://en.wikipedia.org/wiki?curid=49853" title="Lilliput">
Lilliput

Lilliput may refer to:

</doc>
<doc id="49855" url="https://en.wikipedia.org/wiki?curid=49855" title="Umayyad Caliphate">
Umayyad Caliphate

The Umayyad Caliphate (, trans. "Al-Khilāfat al-ʾumawiyya") was the second of the four major Islamic caliphates established after the death of Muhammad. This caliphate was centered on the Umayyad dynasty (, "al-ʾUmawiyyūn", or , "Banū ʾUmayya", "Sons of Umayya"), hailing from Mecca. The Umayyad family had first come to power under the third caliph, Uthman ibn Affan (r. 644–656), but the Umayyad regime was founded by Muawiya ibn Abi Sufyan, long-time governor of Syria, after the end of the First Muslim Civil War in 661 CE/41 AH. Syria remained the Umayyads' main power base thereafter, and Damascus was their capital. The Umayyads continued the Muslim conquests, incorporating the Caucasus, Transoxiana, Sindh, the Maghreb and the Iberian Peninsula (Al-Andalus) into the Muslim world. At its greatest extent, the Umayyad Caliphate covered 15 million km2 (5.79 million square miles) and 62 million people (29% of the world's population), making it the fifth largest empire in history in both area and proportion of the world's population.
At the time, the Umayyad taxation and administrative practice were perceived as unjust by some Muslims. The Christian and Jewish population still had autonomy; their judicial matters were dealt with in accordance with their own laws and by their own religious heads or their appointees, although they did pay a poll tax for policing to the central state. Muhammad had stated explicitly during his lifetime that abrahamic religious groups (still a majority in times of the Umayyad Caliphate), should be allowed to practice their own religion, provided that they paid the jizya taxation. The welfare state of both the Muslim and the non-Muslim poor started by Umar ibn al Khattab had also continued. Muawiya's wife Maysum (Yazid's mother) was also a Christian. The relations between the Muslims and the Christians in the state were stable in this time. The Umayyads were involved in frequent battles with the Christian Byzantines without being concerned with protecting themselves in Syria, which had remained largely Christian like many other parts of the empire. Prominent positions were held by Christians, some of whom belonged to families that had served in Byzantine governments. The employment of Christians was part of a broader policy of religious assimilation that was necessitated by the presence of large Christian populations in the conquered provinces, as in Syria. This policy also boosted Muawiya's popularity and solidified Syria as his power base.
The rivalries between the Arab tribes had caused unrest in the provinces outside Syria, most notably in the Second Muslim Civil War of 680–692 CE and the Berber Revolt of 740–743 CE. During the Second Civil War, leadership of the Umayyad clan shifted from the Sufyanid branch of the family to the Marwanid branch. As the constant campaigning exhausted the resources and manpower of the state, the Umayyads, weakened by the Third Muslim Civil War of 744–747 CE, were finally toppled by the Abbasid Revolution in 750 CE/132 AH. A branch of the family fled across North Africa to Al-Andalus, where they established the Caliphate of Córdoba, which lasted until 1031 before falling due to the Fitna of al-Ándalus.
Origins.
According to tradition, the Umayyad family "(also known as the Banu Abd-Shams)" and Muhammad both descended from a common ancestor, Abd Manaf ibn Qusai, and they originally came from the city of Mecca. Muhammad descended from Abd Manāf via his son Hashim, while the Umayyads descended from Abd Manaf via a different son, Abd-Shams, whose son was Umayya. The two families are therefore considered to be different clans (those of Hashim and of Umayya, respectively) of the same tribe (that of the Quraish). However Muslim Shia historians suspect that Umayya was an adopted son of Abd Shams so he was not a blood relative of Abd Manaf ibn Qusai. Umayya was later discarded from the noble family. Sunni historians disagree with this and view Shia claims as nothing more than outright polemics due to their hostility to the Umayyad family in general. They point to the fact that the grand sons of Uthman, Zaid bin amr bin uthman bin affan and Abdullah bin Amr bin Uthman got married to the Sukaina and Fatima the daughters of Hussein son of Ali to show closeness of Banu hashem and Bani Ummayah.
While the Umayyads and the Hashimites may have had bitterness between the two clans before Muhammad, the rivalry turned into a severe case of tribal animosity after the Battle of Badr. The battle saw three top leaders of the Umayyad clan (Utba ibn Rabi'ah, Walid ibn Utbah and Shaybah) killed by Hashimites (Ali, Hamza ibn ‘Abd al-Muttalib and Ubaydah ibn al-Harith) in a three-on-three melee. This fueled the opposition of Abu Sufyan ibn Harb, the grandson of Umayya, to Muhammad and to Islam. Abu Sufyan sought to exterminate the adherents of the new religion by waging another battle with Muslims based in Medina only a year after the Battle of Badr. He did this to avenge the defeat at Badr. The Battle of Uhud is generally believed by scholars to be the first defeat for the Muslims, as they had incurred greater losses than the Meccans. After the battle, Abu Sufyan's wife Hind, who was also the daughter of Utba ibn Rabi'ah, is reported to have cut open the corpse of Hamza, taking out his liver which she then attempted to eat. Within five years after his defeat in the Battle of Uhud, however, Muhammad took control of Mecca and announced a general amnesty for all. Abu Sufyan and his wife Hind embraced Islam on the eve of the conquest of Mecca, as did their son (the future caliph Muawiyah I).
Most historians consider Caliph Muawiyah (661–80) to have been the second ruler of the Umayyad dynasty, even though he was the first to assert the Umayyads' right to rule on a dynastic principle. It was really the caliphate of Uthman Ibn Affan (644–656), a member of Umayyad clan himself, that witnessed the revival and then the ascendancy of the Umayyad clan to the corridors of power. Uthman placed some of the trusted members of his clan at prominent and strong positions throughout the state. Most notable was the appointment of Marwan ibn al-Hakam, Uthman's first cousin, as his top advisor, which created a stir among the Hashimite companions of Muhammad, as Marwan along with his father Al-Hakam ibn Abi al-'As had been permanently exiled from Medina by Muhammad during his lifetime. Uthman also appointed as governor of Kufa his half-brother, Walid ibn Uqba, who was accused by Hashmites of leading prayer while under the influence of alcohol. Uthman also consolidated Muawiyah's governorship of Syria by granting him control over a larger area and appointed his foster brother Abdullah ibn Saad as the Governor of Egypt. However, since Uthman never named an heir, he cannot be considered the founder of a dynasty.
In 639, Muawiyah I was appointed as the governor of Syria after the previous governor Abu Ubaidah ibn al-Jarrah died in a plague along with 25,000 other people. To stop the Byzantine harassment from the sea during the Arab-Byzantine Wars, in 649 Muawiyah I set up a navy manned by Monophysite Christian, Copt and Jacobite Syrian Christian sailors and Muslim troops. This resulted in the defeat of the Byzantine navy at the Battle of the Masts in 655, opening up the Mediterranean.
Muawiyah I was a very successful governor and built up a very loyal and disciplined army from the old Roman Syrian army. He also befriended Amr ibn al-As who had conquered Egypt but was removed by Uthman ibn al-Affan.
The Quran and Muhammad talked about racial equality and justice as in The Farewell Sermon. Tribal and nationalistic differences were discouraged. But after Muhammad's passing, the old tribal differences between the Arabs started to resurface. Following the Roman–Persian Wars and the Byzantine–Sassanid Wars, deep rooted differences between Iraq, formally under the Persian Sassanid Empire, and Syria, formally under the Byzantine Empire, also existed. Each wanted the capital of the newly established Islamic State to be in their area. Previously, the second caliph Umar ibn Al-Khattab was very firm on the governors and his spies kept an eye on them. If he felt that a governor or a commander was becoming attracted to wealth, he had him removed from his position.
Early Muslim armies stayed in encampments away from cities because Umar ibn Al-Khattab feared that they might get attracted to wealth and luxury. In the process, they might turn away from the worship of God and start accumulating wealth and establishing dynasties. When Uthman ibn al-Affan became very old, Marwan I, a relative of Muawiyah I, slipped into the vacuum, became his secretary, slowly assumed more control and relaxed some of these restrictions. Marwan I had previously been excluded from positions of responsibility. In 656, Muhammad ibn Abi Bakr, the son of Abu Bakr, the adopted son of Ali ibn Abi Talib, and the great grandfather of Ja'far al-Sadiq, showed some Egyptians the house of Uthman ibn al-Affan. Later the Egyptians ended up killing Uthman ibn al-Affan.
After the assassination of Uthman in 656, Ali, a member of the Quraysh tribe and the cousin and son-in-law of Muhammad, was elected as the caliph. He soon met with resistance from several factions, owing to his relative political inexperience. Ali moved his capital from Medina to Kufa. The resulting conflict, which lasted from 656 until 661, is known as the First Fitna ("civil war"). Muawiyah I, the governor of Syria, a relative of Uthman ibn al-Affan and Marwan I, wanted the culprits arrested. Marwan I manipulated everyone and created conflict. Aisha, the wife of Muhammad, and Talhah and Al-Zubayr, two of the companions of Muhammad, went to Basra to tell Ali to arrest the culprits who murdered Uthman. Marwan I and other people who wanted conflict manipulated everyone to fight. The two sides clashed at the Battle of the Camel in 656, where Ali won a decisive victory.
Following this battle, Ali fought a battle against Muawiyah, known as the Battle of Siffin. The battle was stopped before either side had achieved victory, and the two parties agreed to arbitrate their dispute. After the battle Amr ibn al-As was appointed by Muawiyah as an arbitrator, and Ali appointed Abu Musa Ashaari. Seven months later, in February 658, the two arbitrators met at Adhruh, about 10 miles north west of Maan in Jordon. Amr ibn al-As convinced Abu Musa Ashaari that both Ali and Muawiyah should step down and a new Caliph be elected. Ali and his supporters were stunned by the decision which had lowered the Caliph to the status of the rebellious Muawiyah I. Ali was therefore outwitted by Muawiyah and Amr. Ali refused to accept the verdict and found himself technically in breach of his pledge to abide by the arbitration. This put Ali in a weak position even amongst his own supporters. The most vociferous opponents in Ali's camp were the very same people who had forced Ali into the ceasefire. They broke away from Ali's force, rallying under the slogan, "arbitration belongs to God alone." This group came to be known as the Kharijites ("those who leave"). In 659 Ali's forces and the Kharijites met in the Battle of Nahrawan. Although Ali won the battle, the constant conflict had begun to affect his standing, and in the following years some Syrians seem to have acclaimed Muawiyah as a rival caliph.
Ali was assassinated in 661 by a Kharijite partisan. Six months later in the same year, in the interest of peace, Hasan ibn Ali, highly regarded for his wisdom and as a peacemaker, and the Second Imam for the Shias, and the grandson of Muhammad, made a peace treaty with Muawiyah I. In the Hasan-Muawiya treaty, Hasan ibn Ali handed over power to Muawiya on the condition that he be just to the people and keep them safe and secure, and after his death he not establish a dynasty. This brought to an end the era of the Rightly Guided Caliphs for the Sunnis, and Hasan ibn Ali was also the last Imam for the Shias to be a Caliph. Following this, Mu'awiyah broke the conditions of the agreement and began the Umayyad dynasty, with its capital in Damascus.
After Mu'awiyah's death in 680, conflict over succession broke out again in a civil war known as the "Second Fitna". After making every one else fight, the Umayyad dynasty later fell into the hands of Marwan I, who was also an Umayyad.
Syria would remain the base of Umayyad power until the end of the dynasty in 750. However, this Dynasty became reborn in Cordoba (Al Andalus, today's Portugal and Spain) in the form of an Emirate and then a Caliphate, lasting until 1031 AD. Muslim rule continued in Iberia for another 500 years in several forms: Taifas, Berber kingdoms, and under the Kingdom of Granada until the 16th century.
In the year 712, Muhammad bin Qasim, an Umayyad general, sailed from the Persian Gulf into Sindh in Pakistan and conquered both the Sindh and the Punjab regions along the Indus river. The conquest of Sindh and Punjab, in modern-day Pakistan, although costly, were major gains for the Umayyad Caliphate. However, further gains were halted by Hindu kingdoms in India in the battle of Rajasthan. The Arabs tried to invade India but they were defeated by the north Indian king Nagabhata of the Gurjara Pratihara Dynasty and by the south Indian Emperor Vikramaditya II of the Chalukya dynasty in the early 8th century. After this the Arab chroniclers admit that the Caliph Mahdi "gave up the project of conquering any part of India."
During the later period of its existence and particularly from 1031 CE under the Ta'ifa system of Islamic Emirates (Princedoms) in the southern half of Iberia, the Emirate/Sultanate of Granada maintained its independence largely due to the payment of Tributes to the northern Christian Kingdoms, which from 1031 began to gradually expand south at its expense.
Muslim rule in Iberia came to an end on January 2, 1492 with the conquest of the Nasrid kingdom of Granada. The last Muslim ruler of Granada, Muhammad XII, better known as Boabdil, surrendered his kingdom to Ferdinand II of Aragon and Isabella I of Castile, the Catholic Monarchs, los Reyes Católicos.
History.
Sufyanids.
Muawiyah's personal dynasty, the "Sufyanids" (descendants of Abu Sufyan), reigned from 661 to 684, until his grandson Muawiya II. The reign of Muawiyah I was marked by internal security and external expansion. On the internal front, only one major rebellion is recorded, that of Hujr ibn Adi in Kufa. Hujr ibn Adi supported the claims of the descendants of Ali to the caliphate, but his movement was easily suppressed by the governor of Iraq, Ziyad ibn Abi Sufyan.
Muawiyah also encouraged peaceful coexistence with the Christian communities of Syria, granting his reign with "peace and prosperity for Christians and Arabs alike", and one of his closest advisers was Sarjun, the father of John of Damascus. At the same time, he waged unceasing war against the Byzantine Roman Empire. During his reign, Rhodes and Crete were occupied, and several assaults were launched against Constantinople. After their failure, and faced with a large-scale Christian uprising in the form of the Mardaites, Muawiyah concluded a peace with Byzantium. Muawiyah also oversaw military expansion in North Africa (the foundation of Kairouan) and in Central Asia (the conquest of Kabul, Bukhara, and Samarkand).
Following Muawiyah's death in 680, he was succeeded by his son, Yazid I. The hereditary accession of Yazid was opposed by a number of prominent Muslims, most notably Abd-Allah ibn al-Zubayr, son of one of the companions of Muhammad, and Husayn ibn Ali, grandson of Muhammad and younger son of Ali. The resulting conflict is known as the Second Fitna.
In 680 Ibn al-Zubayr fled Medina for Mecca. Hearing about Husayn's opposition to Yazid I, the people of Kufa sent to Husayn asking him to take over with their support. Al-Husayn sent his cousin Muslim bin Aqeel to verify if they would rally behind him. When the news reached Yazid I, he sent Ubayd-Allah bin Ziyad, ruler of Basrah, with the instruction to prevent the people of Kufa rallying behind Al-Husayn. Ubayd-Allah bin Ziyad managed to disperse the crowd that gathered around Muslim bin Aqeel and captured him. Realizing Ubayd-Allah bin Ziyad had been instructed to prevent Husayn from establishing support in Kufa, Muslim bin Aqeel requested a message to be sent to Husayn to prevent his immigration to Kufa. The request was denied and Ubayd-Allah bin Ziyad killed Muslim bin Aqeell. While Ibn al-Zubayr would stay in Mecca until his death, Husayn decided to travel on to Kufa with his family, unaware of the lack of support there. Husayn and his family were intercepted by Yazid I's forces led by Amru bin Saad, Shamar bin Thi Al-Joshan, and Hussain bin Tamim, who fought Al-Husayn and his male family members until they were killed. There were 200 people in Husayn's caravan, many of whom were women, including his sisters, wives, daughters and their children. The women and children from Husayn's camp were taken as prisoners of war and led back to Damascus to be presented to Yazid I. They remained imprisoned until public opinion turned against him as word of Husayn's death and his family's capture spread. They were then granted passage back to Medina. The sole adult male survivor from the caravan was Ali ibn Husayn who was with fever too ill to fight when the caravan was attacked.
Following the death of Husayn, Ibn al-Zubayr, although remaining in Mecca, was associated with two opposition movements, one centered in Medina and the other around Kharijites in Basra and Arabia. Because Medina had been home to Muhammad and his family, including Husayn, word of his death and the imprisonment of his family led to a large opposition movement. In 683, Yazid dispatched an army to subdue both movements. The army suppressed the Medinese opposition at the Battle of al-Harrah. The Grand Mosque in Medina was severely damaged and widespread pillaging caused deep-seated dissent. Yazid's army continued on and laid siege to Mecca. At some point during the siege, the Kaaba was badly damaged in a fire. The destruction of the Kaaba and Grand Mosque became a major cause for censure of the Umayyads in later histories of the period.
Yazid died while the siege was still in progress, and the Umayyad army returned to Damascus, leaving Ibn al-Zubayr in control of Mecca. Yazid's son Muawiya II (683–84) initially succeeded him but seems to have never been recognized as caliph outside of Syria. Two factions developed within Syria: the Confederation of Qays, who supported Ibn al-Zubayr, and the Quda'a, who supported Marwan, a descendant of Umayya via Wa'il ibn Umayyah. The partisans of Marwan triumphed at a battle at Marj Rahit, near Damascus, in 684, and Marwan became caliph shortly thereafter.
First Marwanids.
Marwan's first task was to assert his authority against the rival claims of Ibn al-Zubayr, who was at this time recognized as caliph throughout most of the Islamic world. Marwan recaptured Egypt for the Umayyads, but died in 685, having reigned for only nine months.
Marwan was succeeded by his son, Abd al-Malik (685–705), who reconsolidated Umayyad control of the caliphate. The early reign of Abd al-Malik was marked by the revolt of Al-Mukhtar, which was based in Kufa. Al-Mukhtar hoped to elevate Muhammad ibn al-Hanafiyyah, another son of Ali, to the caliphate, although Ibn al-Hanafiyyah himself may have had no connection to the revolt. The troops of al-Mukhtar engaged in battles both with the Umayyads in 686, defeating them at the river Khazir near Mosul, and with Ibn al-Zubayr in 687, at which time the revolt of al-Mukhtar was crushed. In 691, Umayyad troops reconquered Iraq, and in 692 the same army captured Mecca. Ibn al-Zubayr was killed in the attack.
The second major event of the early reign of Abd al-Malik was the construction of the Dome of the Rock in Jerusalem. Although the chronology remains somewhat uncertain, the building seems to have been completed in 692, which means that it was under construction during the conflict with Ibn al-Zubayr. This had led some historians, both medieval and modern, to suggest that the Dome of the Rock was built as a destination for pilgrimage to rival the Kaaba, which was under the control of Ibn al-Zubayr.
Abd al-Malik is credited with centralizing the administration of the Caliphate and with establishing Arabic as its official language. He also introduced a uniquely Muslim coinage, marked by its aniconic decoration, which supplanted the Byzantine and Sasanian coins that had previously been in use. Abd al-Malik also recommenced offensive warfare against Byzantium, defeating the Byzantines at Sebastopolis and recovering control over Armenia and Caucasian Iberia.
Following Abd al-Malik's death, his son, Al-Walid I (705–15), became caliph. Al-Walid was also active as a builder, sponsoring the construction of Al-Masjid al-Nabawi in Medina and the Great Mosque of Damascus.
A major figure during the reigns of both al-Walid and Abd al-Malik was the Umayyad governor of Iraq, Al-Hajjaj bin Yousef. Many Iraqis remained resistant to Umayyad rule, and to maintain order al-Hajjaj imported Syrian troops, which he housed in a new garrison town, Wasit. These troops became crucial in the suppression of a revolt led by an Iraqi general, Ibn al-Ash'ath, in the early eighth century.
Al-Walid was succeeded by his brother, Sulayman (715–17), whose reign was dominated by a protracted siege of Constantinople. The failure of the siege marked the end of serious Arab ambitions against the Byzantine capital. However, the first two decades of the eighth century witnessed the continuing expansion of the Caliphate, which pushed into the Iberian Peninsula in the west, and into Transoxiana (under Qutayba ibn Muslim) and northern India in the east.
Sulayman was succeeded by his cousin, Umar ibn Abd al-Aziz (717–20), whose position among the Umayyad caliphs is somewhat unique. He is the only Umayyad ruler to have been recognized by subsequent Islamic tradition as a genuine caliph ("khalifa") and not merely as a worldly king ("malik").
Umar is honored for his attempt to resolve the fiscal problems attendant upon conversion to Islam. During the Umayyad period, the majority of people living within the caliphate were not Muslim, but Christian, Jewish, Zoroastrian, or members of other small groups. These religious communities were not forced to convert to Islam, but were subject to a tax ("jizyah") which was not imposed upon Muslims. This situation may actually have made widespread conversion to Islam undesirable from the point of view of state revenue, and there are reports that provincial governors actively discouraged such conversions. It is not clear how Umar attempted to resolve this situation, but the sources portray him as having insisted on like treatment of Arab and non-Arab ("mawali") Muslims, and on the removal of obstacles to the conversion of non-Arabs to Islam.
After the death of Umar, another son of Abd al-Malik, Yazid II (720–24) became caliph. Yazid is best known for his "iconoclastic edict", which ordered the destruction of Christian images within the territory of the Caliphate. In 720, another major revolt arose in Iraq, this time led by Yazid ibn al-Muhallab.
Hisham and the limits of military expansion.
The final son of Abd al-Malik to become caliph was Hisham (724–43), whose long and eventful reign was above all marked by the curtailment of military expansion. Hisham established his court at Resafa in northern Syria, which was closer to the Byzantine border than Damascus, and resumed hostilities against the Byzantines, which had lapsed following the failure of the last siege of Constantinople. The new campaigns resulted in a number of successful raids into Anatolia, but also in a major defeat (the Battle of Akroinon), and did not lead to any significant territorial expansion.
From the caliphate's north-western African bases, a series of raids on coastal areas of the Visigothic Kingdom paved the way to the permanent occupation of most of Iberia by the Umayyads (starting in 711), and on into south-eastern Gaul (last stronghold at Narbonne in 759). Hisham's reign witnessed the end of expansion in the west, following the defeat of the Arab army by the Franks at the Battle of Tours in 732. In 739 a major Berber Revolt broke out in North Africa, which was subdued only with difficulty, but it was followed by the collapse of Umayyad authority in al-Andalus. In India the Arab armies were defeated by the south Indian Chalukya dynasty and by the north Indian Pratiharas Dynasty in the 8th century and the Arabs were driven out of India. In the Caucasus, the confrontation with the Khazars peaked under Hisham: the Arabs established Derbent as a major military base and launched several invasions of the northern Caucasus, but failed to subdue the nomadic Khazars. The conflict was arduous and bloody, and the Arab army even suffered a major defeat at the Battle of Marj Ardabil in 730. Marwan ibn Muhammad, the future Marwan II, finally ended the war in 737 with a massive invasion that is reported to have reached as far as the Volga, but the Khazars remained unsubdued.
Hisham suffered still worse defeats in the east, where his armies attempted to subdue both Tokharistan, with its center at Balkh, and Transoxiana, with its center at Samarkand. Both areas had already been partially conquered, but remained difficult to govern. Once again, a particular difficulty concerned the question of the conversion of non-Arabs, especially the Sogdians of Transoxiana. Following the Umayyad defeat in the "Day of Thirst" in 724, Ashras ibn 'Abd Allah al-Sulami, governor of Khurasan, promised tax relief to those Sogdians who converted to Islam, but went back on his offer when it proved too popular and threatened to reduce tax revenues. Discontent among the Khurasani Arabs rose sharply after the losses suffered in the Battle of the Defile in 731, and in 734, al-Harith ibn Surayj led a revolt that received broad backing from Arabs and natives alike, capturing Balkh but failing to take Merv. After this defeat, al-Harith's movement seems to have been dissolved, but the problem of the rights of non-Arab Muslims would continue to plague the Umayyads.
Third Fitna.
Hisham was succeeded by Al-Walid II (743–44), the son of Yazid II. Al-Walid is reported to have been more interested in earthly pleasures than in religion, a reputation that may be confirmed by the decoration of the so-called "desert palaces" (including Qusayr Amra and Khirbat al-Mafjar) that have been attributed to him. He quickly attracted the enmity of many, both by executing a number of those who had opposed his accession, and by persecuting the Qadariyya.
In 744, Yazid III, a son of al-Walid I, was proclaimed caliph in Damascus, and his army tracked down and killed al-Walid II. Yazid III has received a certain reputation for piety, and may have been sympathetic to the Qadariyya. He died a mere six months into his reign.
Yazid had appointed his brother, Ibrahim, as his successor, but Marwan II (744–50), the grandson of Marwan I, led an army from the northern frontier and entered Damascus in December 744, where he was proclaimed caliph. Marwan immediately moved the capital north to Harran, in present-day Turkey. A rebellion soon broke out in Syria, perhaps due to resentment over the relocation of the capital, and in 746 Marwan razed the walls of Homs and Damascus in retaliation.
Marwan also faced significant opposition from Kharijites in Iraq and Iran, who put forth first Dahhak ibn Qays and then Abu Dulaf as rival caliphs. In 747, Marwan managed to reestablish control of Iraq, but by this time a more serious threat had arisen in Khorasan.
Abbasid Revolution.
The Hashimiyya movement (a sub-sect of the Kaysanites Shia), led by the Abbasid family, overthrew the Umayyad caliphate. The Abbasids were members of the Hashim clan, rivals of the Umayyads, but the word "Hashimiyya" seems to refer specifically to Abu Hashim, a grandson of Ali and son of Muhammad ibn al-Hanafiyya. According to certain traditions, Abu Hashim died in 717 in Humeima in the house of Muhammad ibn Ali, the head of the Abbasid family, and before dying named Muhammad ibn Ali as his successor. This tradition allowed the Abbasids to rally the supporters of the failed revolt of Mukhtar, who had represented themselves as the supporters of Muhammad ibn al-Hanafiyya.
Beginning around 719, Hashimiyya missions began to seek adherents in Khurasan. Their campaign was framed as one of proselytism (dawah). They sought support for a "member of the family" of Muhammad, without making explicit mention of the Abbasids. These missions met with success both among Arabs and non-Arabs (mawali), although the latter may have played a particularly important role in the growth of the movement.
Around 746, Abu Muslim assumed leadership of the Hashimiyya in Khurasan. In 747, he successfully initiated an open revolt against Umayyad rule, which was carried out under the sign of the black flag. He soon established control of Khurasan, expelling its Umayyad governor, Nasr ibn Sayyar, and dispatched an army westwards. Kufa fell to the Hashimiyya in 749, the last Umayyad stronghold in Iraq, Wasit, was placed under siege, and in November of the same year Abu al-Abbas was recognized as the new caliph in the mosque at Kufa. At this point Marwan mobilized his troops from Harran and advanced toward Iraq. In January 750 the two forces met in the Battle of the Zab, and the Umayyads were defeated. Damascus fell to the Abbasids in April, and in August, Marwan was killed in Egypt.
The victors desecrated the tombs of the Umayyads in Syria, sparing only that of Umar II, and most of the remaining members of the Umayyad family were tracked down and killed. When Abbasids declared amnesty for members of the Umayyad family, eighty gathered to receive pardons, and all were massacred. One grandson of Hisham, Abd al-Rahman I, survived and established a kingdom in Al-Andalus (Moorish Iberia), proclaiming his family to be the Umayyad Caliphate revived.
Previté-Orton argues that the reasons for the decline of the Umayyads was the rapid expansion of Islam. During Umayyad period, mass conversions brought Persians, Berbers, Copts, and Aramaics to Islam. These "mawalis" (enslaved) were often better educated and more civilised than their Arab invaders. The new converts, on the basis of equality of all Muslims, transformed the political landscape. Previté-Orton also argues that the feud between Syria and Iraq further weakened the empire.
Umayyad Administration.
One of Muawiya's first tasks was to create a stable administration for the empire. He followed the main ideas of the Byzantine Empire which had ruled the same region previously, and had three main governmental branches: political and military affairs, tax collection, and religious administration. Each of these was further subdivided into more branches, offices, and departments.
Provinces.
Geographically, the empire was divided into several provinces, the borders of which changed numerous times during the Umayyad reign. Each province had a governor appointed by the khalifah. The governor was in charge of the religious officials, army leaders, police, and civil administrators in his province. Local expenses were paid for by taxes coming from that province, with the remainder each year being sent to the central government in Damascus. As the central power of the Umayyad rulers waned in the later years of the dynasty, some governors neglected to send the extra tax revenue to Damascus and created great personal fortunes.
Government workers.
As the empire grew, the number of qualified Arab workers was too small to keep up with the rapid expansion of the empire. Therefore, Muawiya allowed many of the local government workers in conquered provinces to keep their jobs under the new Umayyad government. Thus, much of the local government's work was recorded in Greek, Coptic, and Persian. It was only during the reign of Abd al-Malik that government work began to be regularly recorded in Arabic.
Currency.
The Byzantine and Sassanid Empires relied on money economies before the Muslim conquest, and that system remained in effect during the Umayyad period. Pre-existing coins remained in use, but with phrases from the Quran stamped on them. In addition to this, the Umayyad government began to mint its own coins in Damascus (which were similar to pre-existing coins), the first coins minted by a Muslim government in history. Gold coins were called dinars while silver coins were called dirhams.
Central diwans.
To assist the Caliph in administration there were six Boards at the Centre: Diwan al-Kharaj (the Board of Revenue), Diwan al-Rasa'il (the Board of Correspondence), Diwan al-Khatam (the Board of Signet), Diwan al-Barid (the Board of Posts), Diwan al-Qudat (the Board of Justice) and Diwan al-Jund (the Military Board)
Diwan al-Kharaj.
The Central Board of Revenue administered the entire finances of the empire. It also imposed and collected taxes and disbursed revenue.
Diwan al-Rasa'il.
A regular Board of Correspondence was established under the Umayyads. It issued state missives and circulars to the Central and Provincial Officers. It co-ordinated the work of all Boards and dealt with all correspondence as the chief secretariat.
Diwan al-Khatam.
In order to check forgery, Diwan al-Khatam (Bureau of Registry), a kind of state chancellery, was instituted by Mu'awiyah. It used to make and preserve a copy of each official document before sealing and despatching the original to its destination. Thus in the course of time a state archive developed in Damascus by the Umayyads under Abd al-Malik. This department survived till the middle of the Abbasid period.
Diwan al-Barid.
Mu'awiyah introduced postal service, Abd al-Malik extended it throughout his empire, and Walid made full use of it. The Umayyad Caliph Abd al-Malik developed a regular postal service. Umar bin Abdul-Aziz developed it further by building caravanserais at stages along the Khurasan highway. Relays of horses were used for the conveyance of dispatches between the caliph and his agents and officials posted in the provinces. The main highways were divided into stages of each and each stage had horses, donkeys or camels ready to carry the post. Primarily the service met the needs of Government officials, but travellers and their important dispatches were also benefitted by the system. The postal carriages were also used for the swift transport of troops. They were able to carry fifty to a hundred men at a time. Under Governor Yusuf bin Umar, the postal department of Iraq cost 4,000,000 dirhams a year.
Diwan al-Qudat.
In the early period of Islam, justice was administered by Muhammad and the orthodox Caliphs in person. After the expansion of the Islamic State, Umar al-Faruq had to separate judiciary from the general administration and appointed the first qadi in Egypt as early as 23H/643AD. After 661AD a series of judges succeeded one after another in Egypt under the Umayyad Caliphs, Hisham and Walid II.
Diwan al-Jund.
The Diwan of Umar, assigning annuities to all Arabs and to the Muslim soldiers of other races, underwent a change in the hands of the Umayyads. The Umayyads meddled with the register and the recipients regarded pensions as the subsistence allowance even without being in active service. Hisham reformed it and paid only to those who participated in battle.
On the pattern of the Byzantine system the Umayyads reformed their army organization in general and divided it into five corps: the centre, two wings, vanguards and rearguards, following the same formation while on march or on a battle field. Marwan II (740–50) abandoned the old division and introduced Kurdus (cohort), a small compact body.
The Umayyad troops were divided into three divisions: infantry, cavalry and artillery. Arab troops were dressed and armed in Greek fashion. The Umayyad cavalry used plain and round saddles. The artillery used arradah (ballista), manjaniq (the mangonel) and dabbabah or kabsh (the battering ram). The heavy engines, siege machines and baggage were carried on camels behind the army.
Social Organization.
The Umayyad Caliphate exhibited four main social classes:
The Muslim Arabs were at the top of the society and saw it as their duty to rule over the conquered areas. Despite the fact that Islam teaches the equality of all Muslims, the Arab Muslims held themselves in higher esteem than Muslim non-Arabs and generally did not mix with other Muslims.
The inequality of Muslims in the empire led to social unrest. As Islam spread, more and more of the Muslim population was constituted of non-Arabs. This caused tension as the new converts were not given the same rights as Muslim Arabs. Also, as conversions increased, tax revenues from non-Muslims decreased to dangerous lows. These issues continued to grow until they helped cause the Abbasid Revolt in the 740s.
Non-Muslims.
Non-Muslim groups in the Umayyad Caliphate, which included Christians, Jews, Zoroastrians, and pagan Berbers, were called dhimmis. They were given a legally protected status as second-class citizens as long as they accepted and acknowledged the political supremacy of the ruling Muslims. They were allowed to have their own courts, and were given freedom of their religion within the empire. Although they could not hold the highest public offices in the empire, they had many bureaucratic positions within the government. Christians and Jews still continued to produce great theological thinkers within their communities, but as time wore on, many of the intellectuals converted to Islam, leading to a lack of great thinkers in the non-Muslim communities.
Legacy.
Currently many Sunni scholars agree that Muawiyah's family, including his progenitors, Abu Sufyan ibn Harb and Hind bint Utbah, were originally opponents of Islam and particularly of Muhammad until the Conquest of Mecca.
However many early history books like the Islamic Conquest of Syria Fatuhusham by al-Imam al-Waqidi state that after the conversion to Islam Muawiyah's father Abu Sufyan ibn Harb and his brothers Yazid ibn Abi Sufyan were appointed as commanders in the Muslim armies by Muhammad. Muawiyah, Abu Sufyan ibn Harb, Yazid ibn Abi Sufyan and Hind bint Utbah fought in the Battle of Yarmouk. The defeat of the Byzantine Emperor Heraclius at the Battle of Yarmouk opened the way for the Muslim expansion into Jerusalem and Syria.
In 639, Muawiyah was appointed as the governor of Syria by the second caliph Umar after his brother the previous governor Yazid ibn Abi Sufyan and the governor before him Abu Ubaidah ibn al-Jarrah died in a plague along with 25,000 other people. 'Amr ibn al-'As was sent to take on the Roman Army in Egypt. Fearing an attack by the Romans, Umar asked Muawiyah to defend against a Roman attack.
With limited resources Muawiyah went about creating allies. Muawiyah married Maysum the daughter of the chief of the Kalb tribe, that was a large Jacobite Christian Arab tribe in Syria. His marriage to Maysum was politically motivated. The Kalb tribe had remained largely neutral when the Muslims first went into Syria. After the plague that killed much of the Muslim Army in Syria, by marrying Maysum, Muawiyah started to use the Jacobite Christians, against the Romans. Muawiya's wife Maysum (Yazid's mother) was also a Jacobite Christian. With limited resources and the Byzantine just over the border, Muawiyah worked in cooperation with the local Christian population. To stop Byzantine harassment from the sea during the Arab-Byzantine Wars, in 649 Muawiyah set up a navy; manned by Monophysitise Christians, Copts and Jacobite Syrian Christians sailors and Muslim troops.
Muawiya was one of the first to realize the full importance of having a navy; as long as the Byzantine fleet could sail the Mediterranean unopposed, the coast line of Syria, Palestine and Egypt would never be safe. Muawiyah along with Adbullah ibn Sa'd the new governor of Egypt successfully persuaded Uthman to give them permission to construct a large fleet in the dockyards of Egypt and Syria
The first real naval engagement between the Muslim and the Byzantine navy was the so-called Battle of the Masts (Dhat al-sawari) or battle of Phoenix off the Lycian coast in 655. This resulted in the defeat of the Byzantine navy at the Battle of the Masts in 655, opening up the Mediterranean.
Muawiyah came to power after the death of Ali and established a dynasty.
Historical significance.
The Umayyad caliphate was marked both by territorial expansion and by the administrative and cultural problems that such expansion created. Despite some notable exceptions, the Umayyads tended to favor the rights of the old Arab families, and in particular their own, over those of newly converted Muslims (mawali). Therefore, they held to a less universalist conception of Islam than did many of their rivals. As G.R. Hawting has written, "Islam was in fact regarded as the property of the conquering aristocracy."
During the period of the Umayyads, Arabic became the administrative language. State documents and currency were issued in the language. Mass conversions brought a large influx of Muslims to the caliphate. The Umayyads also constructed famous buildings such as the Dome of the Rock at Jerusalem, and the Umayyad Mosque at Damascus.
According to one common view, the Umayyads transformed the caliphate from a religious institution (during the rashidun) to a dynastic one. However, the Umayyad caliphs do seem to have understood themselves as the representatives of God on earth, and to have been responsible for the "definition and elaboration of God's ordinances, or in other words the definition or elaboration of Islamic law."
The Umayyads have met with a largely negative reception from later Islamic historians, who have accused them of promoting a kingship ("mulk", a term with connotations of tyranny) instead of a true caliphate ("khilafa"). In this respect it is notable that the Umayyad caliphs referred to themselves not as "khalifat rasul Allah" ("successor of the messenger of God", the title preferred by the tradition), but rather as "khalifat Allah" ("deputy of God"). The distinction seems to indicate that the Umayyads "regarded themselves as God's representatives at the head of the community and saw no need to share their religious power with, or delegate it to, the emergent class of religious scholars." In fact, it was precisely this class of scholars, based largely in Iraq, that was responsible for collecting and recording the traditions that form the primary source material for the history of the Umayyad period. In reconstructing this history, therefore, it is necessary to rely mainly on sources, such as the histories of Tabari and Baladhuri, that were written in the Abbasid court at Baghdad.
Modern Arab nationalism regards the period of the Umayyads as part of the Arab Golden Age which it sought to emulate and restore.
This is particularly true of Syrian nationalists and the present-day state of Syria, centered like that of the Umayyads on Damascus.
White, one of the four Pan-Arab colors which appear in various combinations on the flags of most Arab countries, is considered as representing the Umayyads.
Theological opinions concerning the Umayyads.
Sunni opinions.
Many Muslims criticized the Umayyads for having too many non-Muslim, former Roman administrators in their government. St John of Damascus was also a high administrator in the Umayyad administration.
Later when Umar ibn Abd al-Aziz came to power, he reduced these taxes. He is therefore praised as one of the greatest Muslim rulers after the four Rightly Guided Caliphs. Imam Abu Muhammad Adbullah ibn Abdul Hakam who lived in 829 and wrote a biography on Umar Ibn Adbul Aziz stated that the reduction in these taxes stimulated the economy and created wealth but it also reduced the government budget and this then led to a reduction in the defense budget.
Only Umayyad ruler (Caliphs of Damascus), Umar ibn Abd al-Aziz, is unanimously praised by Sunni sources for his devout piety and justice. In his efforts to spread Islam he established liberties for the "Mawali" by abolishing the jizya tax for converts to Islam. Imam Abu Muhammad Adbullah ibn Abdul Hakam stated that Umar ibn Abd al-Aziz also stopped the personal allowance offered to his relatives stating that he could only give them an allowance if he gave an allowance to everyone else in the empire. Umar ibn Abd al-Aziz was later poisoned in the year 720. When successive governments tried to reverse Umar ibn Abd al-Aziz's tax policies it created rebellion.
Early literature.
The book Al Muwatta by Imam Malik's was written in the early Abbasid period in Madina. It does not contain any anti Umayyad content because it was more concerned with what the Quran and what Muhammad said and was not a history book on the Umayyads.
Even the earliest pro-Shia accounts of al-Masudi are more balanced. al-Masudi in Ibn Hisham is the earliest Shia account of Muawiyah and he recount that Muawiyah spent a great deal of time in prayer, in spite of the burden of managing a large empire.
Az-Zuhri stated that Muawiya led the Hajj Pilgrimage with the people twice during his era as caliph.
Books written in the early Abbasid period like al-baladhuri "The Origins of the Islamic State" provide a more accurate and balanced history. Ibn Hisham also wrote about these events.
Much of the anti Umayyad literature started to appear in the later Abbasid period in Persia.
After killing off most of the Umayyads and destroying the graves of the Umayyad rulers apart from Muawiyah and Umar Ibn Adbul Aziz, the history books written during the later Abbasid period are more anti Umayyad The Abbasids justified their rule by saying that their ancestor Abd Allah ibn Abbas was a cousin of Muhammad.
The books written later in the Abbasid period in Iran are more anti Umayyad. Iran was Sunni at the time. There was much anti Arab feeling in Iran after the fall of the Persian empire. This anti Arab feeling also influenced the books on Islamic history. Al-Tabri was also written in Iran during that period. Al-Tabri was a huge collection including all the text that he could find, from all the sources. It was a collection preserving everything for future generations to codify and for future generations to judge if it was true or false.
Shi'a opinions.
The negative view of the Umayyads by Shias is briefly expressed in the Shi'a book "Sulh al-Hasan". According to some sources Ali described them as the worst Fitna.
Bahá'í standpoint.
Asked for an explanation of the prophecies in the Book of Revelation (12:3), `Abdu'l-Bahá suggests in Some Answered Questions that the "great red dragon, having seven heads and ten horns, and seven crowns upon his heads," refers to the Umayyad caliphs who "rose against the religion of Prophet Muhammad and against the reality of Ali".
The seven heads of the dragon is symbolic of the seven provinces of the lands dominated by the Umayyads; Damascus, Persia, Arabia, Egypt, Africa, Andalusia, and Transoxania. The ten horns represent the ten names of the leaders of the Umayyad dynasty; Abu Sufyan, Muawiya, Yazid, Marwan, Abd al-Malik, Walid, Sulayman, Umar, Hisham, and Ibrahim. Some names were re-used, as in the case of Yazid II and Yazid III, which were not counted for this interpretation.

</doc>
<doc id="49856" url="https://en.wikipedia.org/wiki?curid=49856" title="Abbasid Caliphate">
Abbasid Caliphate

The Abbasid Caliphate ( or "") was the third of the Islamic caliphates to succeed the Islamic prophet Muhammad. The Abbasid dynasty descended from Muhammad's youngest uncle, Abbas ibn Abd al-Muttalib (566–653 CE), from whom the dynasty takes its name. They ruled as caliphs, for most of their period from their capital in Baghdad in modern-day Iraq, after assuming authority over the Muslim empire from the Umayyads in 750 CE (132 AH).
The Abbasid caliphate first centered its government in Kufa, but in 762 the caliph Al-Mansur founded the city of Baghdad, north of the Sasanian capital city of Ctesiphon. The choice of a capital so close to Persia proper reflected a growing reliance on Persian bureaucrats, most notably of the Barmakid family, to govern the territories conquered by Arab Muslims, as well as an increasing inclusion of non-Arab Muslims in the ummah. Despite this initial cooperation, the Abbasids of the late 8th century had alienated both Arab "mawali" and Iranian bureaucrats, and were forced to cede authority over Al-Andalus and Maghreb to the Umayyads, Morocco to the Idrisid dynasty, Ifriqiya to the Aghlabids, and Egypt to the Shi'ite Caliphate of the Fatimids. The political power of the caliphs largely ended with the rise of the Buyids and the Seljuq Turks. Although Abbasid leadership over the vast Islamic empire was gradually reduced to a ceremonial religious function, the dynasty retained control over its Mesopotamian demesne. The capital city of Baghdad became a center of science, culture, philosophy and invention during the Golden Age of Islam.
This period of cultural fruition ended in 1258 with the sack of Baghdad by the Mongols under Hulagu Khan. The Abbasid line of rulers, and Muslim culture in general, recentered themselves in the Mamluk capital of Cairo in 1261. Though lacking in political power, the dynasty continued to claim authority in religious matters until after the Ottoman conquest of Egypt (1517).
History.
Abbasid Revolution (750–751).
The Abbasid caliphs were Arabs descended from Abbas ibn Abd al-Muttalib, one of the youngest uncles of Muhammad and of the same Banu Hashim clan. The Abbasids claimed to be the true successors of Muhammad in replacing the Umayyad descendants of Banu Umayya by virtue of their closer bloodline to Muhammad.
The Abbasids also distinguished themselves from the Umayyads by attacking their moral character and administration in general. According to Ira Lapidus, "The Abbasid revolt was supported largely by Arabs, mainly the aggrieved settlers of Marw with the addition of the Yemeni faction and their Mawali".
The Abbasids also appealed to non-Arab Muslims, known as "mawali", who remained outside the kinship-based society of the Arabs and were perceived as a lower class within the Umayyad empire. Muhammad ibn 'Ali, a great-grandson of Abbas, began to campaign for the return of power to the family of Muhammad, the Hashimites, in Persia during the reign of Umar II.
During the reign of Marwan II, this opposition culminated in the rebellion of Ibrahim the Imam, the fourth in descent from Abbas. Supported by the province of Khorasan, Iran, even though the governor opposed them, and the Shi'i Arabs, he achieved considerable success, but was captured in the year 747 and died, possibly assassinated, in prison.
On 9 June 747 (15 Ramadan AH 129), Abu Muslim successfully initiated an open revolt against Umayyad rule, which was carried out under the sign of the Black Standard. Close to 10,000 soldiers were under Abu Muslim's command when the hostilities officially began in Merv. General Qahtaba followed the fleeing governor Nasr ibn Sayyar west defeating the Umayyads at the Battle of Nishapur, the Battle of Gorgan, Battle of Nahāvand and finally in the Battle of Karbala.
The quarrel was taken up by Ibrahim's brother Abdallah, known by the name of Abu al-'Abbas as-Saffah, who defeated the Umayyads in 750 in the Battle of the Zab near the Great Zab and was subsequently proclaimed caliph. After this loss, Marwan fled to Egypt, where he was subsequently assassinated. The remainder of his family, barring one male, were also eliminated.
Immediately after their victory, As-Saffah sent his forces to Central Asia, where his forces fought against Tang expansion during the Battle of Talas. Barmakids, who were instrumental in building Baghdad; introduced the world's first recorded paper mill in Baghdad, thus beginning a new era of intellectual rebirth in the Abbasid domain. As-Saffah focuses on putting down numerous rebellions in Syria and Mesopotamia. The Byzantines conduct raids during these early distractions.
Power (752–775).
The first change the Abbasids, under Al-Mansur, made was to move the empire's capital from Damascus, in Syria, to Baghdad in Iraq. This was to both appease as well to be closer to the Persian "mawali" support base that existed in this region more influenced by Persian history and culture, and part of the Persian mawali demand for less Arab dominance in the empire. Baghdad was established on the Tigris River in 762. A new position, that of the vizier, was also established to delegate central authority, and even greater authority was delegated to local emirs. Eventually, this meant that many Abbasid caliphs were relegated to a more ceremonial role than under the Umayyads, as the viziers began to exert greater influence, and the role of the old Arab aristocracy was slowly replaced by a Persian bureaucracy. During Al-Mansur's time control of Al-Andalus was lost, and the Shiites revolted and were defeated a year later at the Battle of Bakhamra.
The Abbasids had depended heavily on the support of Persians in their overthrow of the Umayyads. Abu al-'Abbas' successor, Al-Mansur welcomed non-Arab Muslims to his court. While this helped integrate Arab and Persian cultures, it alienated many of their Arab supporters, particularly the Khorasanian Arabs who had supported them in their battles against the Umayyads.
These fissures in their support led to immediate problems. The Umayyads, while out of power, were not destroyed. The only surviving member of the Umayyad royal family, which had been all but annihilated, ultimately made his way to Spain where he established himself as an independent Emir (Abd ar-Rahman I, 756). In 929, Abd ar-Rahman III assumed the title of Caliph, establishing Al Andalus from Córdoba as a rival to Baghdad as the legitimate capital of the Islamic Empire.
In 756, the Abbasid Caliph Al-Mansur sent over 4,000 Arab mercenaries to assist the Chinese Tang dynasty in the An Shi Rebellion against An Lushan. The Abbasides or "Black Flags," as they were commonly called, were known in Tang dynasty chronicles as the "hēiyī Dàshí", " The Black-robed Tazi", (黑衣大食) ("Tazi" being a Tang dynasty borrowing from Persian to denote 'Arabs'). Al-Rashid sent embassies to the Chinese Tang dynasty and established good relations with them. After the war, these embassies remained in China with Caliph Harun al-Rashid establishing an alliance with China. Several embassies from the Abbasid Caliphs to the Chinese court have been recorded in the T'ang Annals, the most important of these being those of Abul Abbas al-Saffah, the founder of the Abbasid dynasty, Abu Jafar and Harun al-Rashid.
Abbasid Golden Age (775–861).
The Abbasid leadership had to work hard in the last half of the 8th century (750–800), under several competent caliphs and their viziers to overcome the political challenges created by the far flung nature of the empire, and the limited communication across it and usher in the administrative changes needed to keep order. It was also during this early period of the dynasty, in particular during the governance of al-Mansur, Harun al-Rashid, and al-Ma'mun, that the reputation and power of the dynasty was created. Al-Mahdi restarted the fighting with the Byzantines and his sons continued the conflict until Empress Irene pushed for peace. After several years of peace, Nikephoros I broke the treaty, then fended off multiple incursions during the first decade of the 9th century. These attacks pushed into the Taurus Mountains culminating with a victory at the Battle of Krasos and the massive invasion of 806, led by Rashid himself. Rashid's navy also proved successful as he took Cyprus. Eventually, the momentum turned and much of the land gained was lost. Rashid decided to focus on the rebellion of Rafi ibn al-Layth in Khorasan and died while there. While the Byzantine Empire was fighting Abbasid rule in Syria and Anatolia, military operations during this period were minimal, as the caliphate focused on internal matters, its governors exerting greater autonomy and using their increasing power to make their positions hereditary.
At the same time, the Abbasids faced challenges closer to home. Harun al-Rashid turned on the Barmakids, a Persian family that had grown significantly in power within the administration of the state and killed most of the family. During the same period, several factions began either to leave the empire for other lands or to take control of distant parts of the empire away from the Abbasids. The reign of al-Rashid and his sons were considered to be the apex of the Abbasids. After Rashid's death, the empire was split by a civil war between the caliph al-Amin and his brother al-Ma'mun who had the support of Khorasan. This war ended with a two-year siege of Baghdad and the eventual death of al-Amin in 813. Al-Ma'mun ruled for 20 years of relative calm interspersed with a rebellion supported by the Byzantines in Azerbaijan by the Khurramites. Al-Ma'mun was also responsible for the creation of an autonomous Khorasan, and continued repulsing of Byzantine forays. Al-Mu'tasim gained power in 833 and his rule marked the end of the strong caliphs. He strengthened his personal army with Turkish mercenaries and promptly restarted the war with the Byzantines. His military excursions were generally successful culminating with a resounding victory in the Sack of Amorium. His attempt at seizing Constantinople failed when his fleet was destroyed by a storm. The Byzantines restarted the fighting by sacking Damietta in Egypt. Al-Mutawakkil responded by sending his troops into Anatolia again, sacking and marauding until they were eventually annihilated in 863.
Fracture to autonomous dynasties (861–945).
Even by 820, the Samanids had begun the process of exercising independent authority in Transoxiana and Greater Khorasan, as had the Shia Hamdanids in Northern Syria, and the succeeding Tahirid and Saffarid dynasties of Iran. The Saffarids, from Khorasan, nearly seized Baghdad in 876, and the Tulunids took control of most of Syria. The trend of weakening of the central power and strengthening of the minor caliphates on the periphery continued, except for the 10-year period of Al-Mu'tadid's rule. He brought parts of Egypt, Syria, and Khorasan back into the Abbasid's control. Especially after the "Anarchy at Samarra", the Abbasid central government was weakened and centrifugal tendencies became more prominent in the Caliphate's provinces. By the early 10th century, the Abbasids almost lost control of Iraq to various amirs, and the caliph al-Radi was forced to acknowledge their power by creating the position of "Prince of Princes" ("amir al-umara"). Al-Mustakfi had a short reign from 944-946, and it was during this period that the Persian faction known as the Buyids from Daylam swept into power and assumed control over the bureaucracy in Baghdad. According to the history of Miskawayh, they began distributing iqtas (fiefs in the form of tax farms) to their supporters. This period of localized secular control was to last nearly 100 years. The loss of Abbasid power to the Buyids would shift as the Seljuks would take over from the Persians.
At the end of the eighth century the Abbasids found they could no longer keep a huge polity larger than that of Rome together from Baghdad. In 793 the Shi'ite dynasty of Idrisids set up a state from Fez in Morocco, while a family of governors under the Abbasids became increasingly independent until they founded the Aghlabid Emirate from the 830s. Al-Mu'tasim started the downward slide by utilizing non-Muslim mercenaries in his personal army. Also during this period officers started assassinating superiors with which they disagree, in particular the caliphs. By the 870s Egypt became autonomous under Ahmad ibn Tulun. In the East as well, governors decreased their ties to the center. The Saffarids of Herat and the Samanids of Bukhara had broken away from the 870s, cultivating a much more Persianate culture and statecraft. By this time only the central lands of Mesopotamia were under direct Abbasid control, with Palestine and the Hijaz often managed by the Tulunids. Byzantium, for its part, had begun to push Arab Muslims farther east in Anatolia.
By the 920s, the situation had changed further, as North Africa was lost to the Abbasids. A Shi'ite sect only recognizing the first five Imams and tracing its roots to Muhammad's daughter Fatima took control of Idrisi and then Aghlabid domains. Called the Fatimid dynasty, they had advanced to Egypt in 969, establishing their capital near Fustat in Cairo, which they built as a bastion of Shi'ite learning and politics. By 1000 they had become the chief political and ideological challenge to Sunni Islam in the form of the Abbasids. By this time the latter state had fragmented into several governorships that, while recognizing caliphal authority from Baghdad, did mostly as they wanted, fighting with each other. The Caliph himself was under 'protection' of the Buyid Emirs who possessed all of Iraq and western Iran, and were quietly Shi'ite in their sympathies.
Outside Iraq, all the autonomous provinces slowly took on the characteristic of de facto states with hereditary rulers, armies, and revenues and operated under only nominal caliph suzerainty, which may not necessarily be reflected by any contribution to the treasury, such as the Soomro Emirs that had gained control of Sindh and ruled the entire province from their capital of Mansura. Mahmud of Ghazni took the title of sultan, as opposed to the "amir" that had been in more common usage, signifying the Ghaznavid Empire's independence from caliphal authority, despite Mahmud's ostentatious displays of Sunni orthodoxy and ritual submission to the caliph. In the 11th century, the loss of respect for the caliphs continued, as some Islamic rulers no longer mentioned the caliph's name in the Friday khutba, or struck it off their coinage.
The Ismaili Fatimid dynasty of Cairo contested the Abbasids for even the titular authority of the Islamic ummah. They commanded some support in the Shia sections of Baghdad (such as Karkh), although Baghdad was the city most closely connected to the caliphate, even in the Buyid and Seljuq eras. The Fatimids' green banners contrasted with Abbasids' black, and the challenge of the Fatimids only ended with their downfall in the 12th century.
Buyid and Seljuq military control (945–1118).
Despite the power of the Buyid amirs, the Abbasids retained a highly ritualized court in Baghdad, as described by the Buyid bureaucrat Hilal al-Sabi', and they retained a certain influence over Baghdad as well as religious life. As Buyid power waned after the death of Baha' al-Daula, the caliphate was able to regain some measure of strength. The caliph al-Qadir, for example, led the ideological struggle against the Shia with writings such as the "Baghdad Manifesto". The caliphs kept order in Baghdad itself, attempting to prevent the outbreak of fitnas in the capital, often contending with the "ayyarun".
With the Buyid dynasty on the wane, a vacuum was created that was eventually filled by the dynasty of Oghuz Turks known as the Seljuqs. By 1055, the Seljuqs had wrested control from the Buyids and Abbasids, and took any remaining temporal power. When the amir and former slave Basasiri took up the Shia Fatimid banner in Baghdad in 1058, the caliph al-Qa'im was unable to defeat him without outside help. Toghril Beg, the Seljuq sultan, restored Baghdad to Sunni rule and took Iraq for his dynasty. Once again, the Abbasids were forced to deal with a military power that they could not match, though the Abbasid caliph remained the titular head of the Islamic community. The succeeding sultans Alp Arslan and Malikshah, as well as their vizier Nizam al-Mulk, took up residence in Persia, but held power over the Abbasids in Baghdad. When the dynasty began to weaken in the 12th century, the Abbasids gained greater independence once again.
Revival of military strength (1118–1206).
While the Caliph al-Mustarshid was the first caliph to build an army capable of meeting a Seljuk army in battle, he was nonetheless defeated in 1135 and assassinated. The Caliph al-Muqtafi was the first Abbasid Caliph to regain the full military independence of the Caliphate, with the help of his vizier Ibn Hubayra. After nearly 250 years of subjection to foreign dynasties, he successfully defended Baghdad against the Seljuqs in the siege of Baghdad (1157), thus securing Iraq for the Abbasids. The reign of al-Nasir (d. 1225) brought the caliphate back into power throughout Iraq, based in large part on the Sufi futuwwa organizations that the caliph headed. Al-Mustansir built the Mustansiriya School, in an attempt to eclipse the Seljuq-era Nizamiyya built by Nizam al-Mulk.
Mongol invasion (1206–1258).
In 1206, Genghis Khan established a powerful dynasty among the Mongols of central Asia. During the 13th century, this Mongol Empire conquered most of the Eurasian land mass, including both China in the east and much of the old Islamic caliphate (as well as Kievan Rus') in the west. Hulagu Khan's destruction of Baghdad in 1258 is traditionally seen as the approximate end of the Golden Age. Mongols feared that a supernatural disaster would strike if the blood of Al-Musta'sim, a direct descendant of Muhammad's uncle Al-‘Abbas ibn ‘Abd al-Muttalib, and the last reigning Abbasid caliph in Baghdad, was spilled. The Shiites of Persia stated that no such calamity had happened after the deaths of Husayn ibn Ali; nevertheless, as a precaution and in accordance with a Mongol taboo which forbade spilling royal blood, Hulagu had Al-Musta'sim wrapped in a carpet and trampled to death by horses on 20 February 1258. The Caliph's immediate family was also executed, with the lone exceptions of his youngest son who was sent to Mongolia, and a daughter who became a slave in the harem of Hulagu.
Abbasid Caliphate of Cairo (1261–1517).
In the 9th century, the Abbasids created an army loyal only to their caliphate, composed of non-Arab origin people, known as Mamluks. This force, created in the reign of al-Ma'mun (813–33) and his brother and successor al-Mu'tasim (833–42), prevented the further disintegration of the empire. The Mamluk army, though often viewed negatively, both helped and hurt the caliphate. Early on, it provided the government with a stable force to address domestic and foreign problems. However, creation of this foreign army and al-Mu'tasim's transfer of the capital from Baghdad to Samarra created a division between the caliphate and the peoples they claimed to rule. In addition, the power of the Mamluks steadily grew until al-Radi (934–41) was constrained to hand over most of the royal functions to Muhammad ibn Ra'iq.
The Mamluks eventually came to power in Egypt. In 1261, following the devastation of Baghdad by the Mongols, the Mamluk rulers of Egypt re-established the Abbasid caliphate in Cairo. The first Abbasid caliph of Cairo was Al-Mustansir. The Abbasid caliphs in Egypt continued to maintain the presence of authority, but it was confined to religious matters. The Abbasid caliphate of Cairo lasted until the time of Al-Mutawakkil III, who was taken away as a prisoner by Selim I to Constantinople where he had a ceremonial role. He died in 1543, following his return to Cairo.
Culture.
Islamic Golden Age.
The Abbasid historical period lasting to the Mongol conquest of Baghdad in 1258 CE is considered the Islamic Golden Age. The Islamic Golden Age was inaugurated by the middle of the 8th century by the ascension of the Abbasid Caliphate and the transfer of the capital from Damascus to Baghdad. The Abbassids were influenced by the Qur'anic injunctions and hadith such as "the ink of a scholar is more holy than the blood of a martyr" stressing the value of knowledge. During this period the Muslim world became an intellectual center for science, philosophy, medicine and education as the Abbasids championed the cause of knowledge and established the House of Wisdom in Baghdad; where both Muslim and non-Muslim scholars sought to translate and gather all the world's knowledge into Arabic. Many classic works of antiquity that would otherwise have been lost were translated into Arabic and Persian and later in turn translated into Turkish, Hebrew and Latin. During this period the Muslim world was a cauldron of cultures which collected, synthesized and significantly advanced the knowledge gained from the ancient Roman, Chinese, Indian, Persian, Egyptian, North African, Greek and Byzantine civilizations. "In virtually every field of endeavor — in astronomy, alchemy, mathematics, medicine, optics and so forth — the Caliphate's scientists were in the forefront of scientific advance."
Science.
The reigns of Harun al-Rashid (786–809) and his successors fostered an age of great intellectual achievement. In large part, this was the result of the schismatic forces that had undermined the Umayyad regime, which relied on the assertion of the superiority of Arab culture as part of its claim to legitimacy, and the Abbasids' welcoming of support from non-Arab Muslims. It is well established that the Abbasid caliphs modeled their administration on that of the Sassanids. Harun al-Rashid's son, Al-Ma'mun (whose mother was Persian), is even quoted as saying:
A number of medieval thinkers and scientists living under Islamic rule played a role in transmitting Islamic science to the Christian West. These people greatly contributed to making Aristotle known in Christian Europe. In addition, the period saw the recovery of much of the Alexandrian mathematical, geometric and astronomical knowledge, such as that of Euclid and Claudius Ptolemy. These recovered mathematical methods were later enhanced and developed by other Islamic scholars, notably by Persian scientists Al-Biruni and Abu Nasr Mansur.
Christians (particularly Nestorian Christians) contributed to the Arab Islamic Civilization during the Ummayads and the Abbasids by translating works of Greek philosophers to Syriac and afterwards to Arabic. Nestorians played a prominent role in the formation of Arab culture, with the Jundishapur school being prominent in the late Sassanid, Umayyad and early Abbasid periods. Notably, eight generations of the Nestorian Bukhtishu family served as private doctors to caliphs and sultans between the eighth and eleventh centuries.
Algebra was significantly developed by Persian scientist Muhammad ibn Mūsā al-Khwārizmī during this time in his landmark text, "Kitab al-Jabr wa-l-Muqabala", from which the term "algebra" is derived. He is thus considered to be the father of algebra by some, although the Greek mathematician Diophantus has also been given this title. The terms algorism and algorithm are derived from the name of al-Khwarizmi, who was also responsible for introducing the Arabic numerals and Hindu-Arabic numeral system beyond the Indian subcontinent.
Ibn al-Haytham (Alhazen) developed an early scientific method in his "Book of Optics" (1021). The most important development of the scientific method was the use of experiments to distinguish between competing scientific theories set within a generally empirical orientation, which began among Muslim scientists. Ibn al-Haytham's empirical proof of the intromission theory of light (that is, that light rays entered the eyes rather than being emitted by them) was particularly important. Alhazen was significant in the history of scientific method, particularly in his approach to experimentation, and has been referred to as the "world’s first true scientist".
Medicine in medieval Islam was an area of science that advanced particularly during the Abbasids' reign. During the 9th century, Baghdad contained over 800 doctors, and great discoveries in the understanding of anatomy and diseases were made. The clinical distinction between measles and smallpox was described during this time. Famous Persian scientist Ibn Sina (known to the West as Avicenna) produced treatises and works that summarized the vast amount of knowledge that scientists had accumulated, and was very influential through his encyclopedias, "The Canon of Medicine" and "The Book of Healing". The work of him and many others directly influenced the research of European scientists during the Renaissance.
Astronomy in medieval Islam was advanced by Al-Battani, who improved the precision of the measurement of the precession of the Earth's axis. The corrections made to the geocentric model by al-Battani, Averroes, Nasir al-Din al-Tusi, Mo'ayyeduddin Urdi and Ibn al-Shatir were later incorporated into the Copernican heliocentric model. The astrolabe, though originally developed by the Greeks, was developed further by Islamic astronomers and engineers, and subsequently brought to medieval Europe.
Muslim alchemists influenced medieval European alchemists, particularly the writings attributed to Jābir ibn Hayyān (Geber). A number of chemical processes such as distillation techniques were developed in the Muslim world and then spread to Europe.
Literature.
The best known fiction from the Islamic world is "The Book of One Thousand and One Nights", a collection of fantastical folk tales, legends and parables compiled primarily during the Abbassid era. The collection is recorded as having originated from an Arabic translation of a Sassanian era Persian prototype, with likely origins in Indian literary traditions. Stories from Arabic, Persian, Mesopotamian, and Egyptian folklore and literature were later incorporated. The epic is believed to have taken shape in the 10th century and reached its final form by the 14th century; the number and type of tales have varied from one manuscript to another. All Arabian fantasy tales were often called "Arabian Nights" when translated into English, regardless of whether they appeared in "The Book of One Thousand and One Nights". This epic has been influential in the West since it was translated in the 18th century, first by Antoine Galland. Many imitations were written, especially in France. Various characters from this epic have themselves become cultural icons in Western culture, such as Aladdin, Sinbad and Ali Baba.
A famous example of Islamic poetry on romance was "Layla and Majnun", which further developed mainly by Iranian, Azerbaijani and other poets in Persian, Azerbaijani, Turkish, and other Turk languages dating back to the Umayyad era in the 7th century. It is a tragic story of undying love much like the later "Romeo and Juliet".
Arabic poetry reached its greatest heights in the Abbasid era, especially before the loss of central authority and the rise of the Persianate dynasties. Writers like Abu Tammam and Abu Nuwas were closely connected to the caliphal court in Baghdad during the early 9th century, while others such as al-Mutanabbi received their patronage from regional courts.
Philosophy.
One of the common definitions for "Islamic philosophy" is "the style of philosophy produced within the framework of Islamic culture." Islamic philosophy, in this definition is neither necessarily concerned with religious issues, nor is exclusively produced by Muslims. Their works on Aristotle was a key step in the transmission of learning from ancient Greeks to the Islamic world and the West. They often corrected the philosopher, encouraging a lively debate in the spirit of ijtihad. They also wrote influential original philosophical works, and their thinking was incorporated into Christian philosophy during the Middle Ages, notably by Thomas Aquinas.
Three speculative thinkers, al-Kindi, al-Farabi, and Avicenna, combined Aristotelianism and Neoplatonism with other ideas introduced through Islam, and Avicennism was later established as a result. Other influential Muslim philosophers in the Caliphates include al-Jahiz, and Ibn al-Haytham (Alhacen).
Architecture.
As the power shifted from the Umayyads to the Abbasids, the architecture styles changed also. The Christian styles evolved into a style based more on the Sassanian empire utilizing mud bricks and baked bricks with carved stucco. Another major developments was the creation or vast enlargement of cities as they were turned into the capital of the empire. First, starting with the creation of Baghdad, starting in 762, which was planned as a walled city with a mosque and palace in the center. The walls were to have four gates to exit the city. Al-Mansur, who was responsible for the creation of Baghdad, also planned the city of Raqqa, along the Euphrates. Finally, in 836, al-Mu'tasim moved the capital to a new site that he created along the Tigris, called Samarra. This city saw 60 years of work, with race-courses and game preserves to add to the atmosphere. Due to the dry remote nature of the environment, some of the palaces built in this era, were isolated havens. Al-Ukhaidir Fortress is a fine example of this type of building which has stables, living quarters, and a mosque, all surrounding inner courtyards. Other mosques of this era, such as the Mosque of Ibn Tulun, in Cairo, and the Great Mosque of Kairouan in Tunisia while ultimately built during the Umayyad dynasty, it was substantially renovated in the 9th century. This renovation was so extensive as to ostensibly be a rebuild, was in the furthest reaches of the Muslim world, in an area that the Aghlabids controlled; however the styles utilized were mainly of the Abbasids. Mesopotamia only has one surviving mausoleum from this era, in Samarra. This octagonal dome is the final resting place of al-Muntasir. Other architectural innovations and styles were few, such as the four-centered arch, and a dome erected on squinches. Unfortunately, much was lost due to the ephemeral nature of the stucco and luster tiles.
Glass and crystal.
The Near East has, since Roman times, been recognized as a center of quality glassware and crystal. 9th century finds from Samarra show styles similar to Sassanian forms. The types of objects made were bottles, flasks, vases, and cups utilized for domestic use. Decorations on these domestic items include molded flutes, honeycomb patters, and inscriptions. Other styles seen that may not have come from the Sassanians were stamped items. These were typically round stamps, such as medallions or disks with animals, birds, or Kufic inscriptions. Colored lead glass, typically blue or green, have been found in Nishapur, along with prismatic perfume bottles. Finally, cut glass may have been the high point of Abbasid glass-working, decorated with floral and animal designs.
Painting.
Early Islamic painting has not survived in great quantities, and sometimes harder to differentiate; however Samarra is a good example as it was built by the Abbasids and abandoned 56 years later. The walls of the principal rooms of the palace that has been excavated show wall paintings and lively carved stucco dadoes. The style is obviously adopted with little variation from Sassanian art, as not only the styles is similar with harems, animals, and dancing people, all enclosed in scrollwork, but also the garments are Persian. Nishapur had its own school of painting. Excavations at Nishapur show artwork both monochrome and polychrome from the 8th and 9th centuries. One famous piece of art consists of hunting nobles with falcons and on horseback, in full regalia, The clothing places him as Tahirid, which was again, a sub-dynasty of the Abbasids. Other styles are of vegetation, and fruit in nice colors on a four foot high dedo.
Pottery.
Whereas painting and architecture were not areas of strength for the Abbasid dynasty, pottery was a different story. The Islamic culture as a whole and the Abbasid's, in particular, were at the forefront of new ideas and techniques. Some examples of their work were pieces engraved with decorations and then colored with yellow-brown, green, and purple glazes. Designs were diverse with geometric patterns, Kufic lettering, arabesque scrollwork, along with rosettes, animals, birds, and humans. Abbasid pottery from the 8th and 9th centuries have been found throughout the region, as far as Cairo. These were generally made with a yellow clay and fired multiple times with separate glazes to produce metallic luster in shades of gold, brown, or red. By the 9th century, the potters had mastered their techniques and their decorative designs could be divided into two styles. The Persian style would show animals, birds, humans, along with Kufic lettering in gold. Pieces excavated from Samarra exceed in vibrancy and beauty any from later periods. These predominantly being made for the Caliphs use. Tiles were also made utilizing this same technique to create both monochromic and polychromic luster tiles.
Textiles.
Egypt being a center of the textile industry was part of the Abbasid cultural advancement. Copts were employed in the textile industry and produced linens and silks. Tinnis was famous for its factories and had over 5,000 looms. Kasab, a fine linen for turbans and badana for garments of the upper class to name a couple. In a town named Tuna near Tinnis, was made the kiswah for the kaaba in Mecca. Fine silk was also made in Dabik, and Damietta. Of particular interest is the stamped and inscribed fabrics. Not only did they utilize inks but also liquid gold. Some of the finer pieces were colored in such a manner as to require six separate stamps to achieve the proper design and color. This technology spread to Europe eventually.
Technology.
In technology, the Muslim world adopted papermaking from China. The use of paper spread from China into the Muslim world in the 8th century CE, arriving in Spain (and then the rest of Europe) in the 10th century. It was easier to manufacture than parchment, less likely to crack than papyrus, and could absorb ink, making it ideal for making records and making copies of the Koran. "Islamic paper makers devised assembly-line methods of hand-copying manuscripts to turn out editions far larger than any available in Europe for centuries." It was from Islam that the rest of the world learned to make paper from linen. The knowledge of gunpowder was also transmitted from China via Islamic countries, where the formulas for pure potassium nitrate and an explosive gunpowder effect were first developed.
Advances were made in irrigation and farming, using new technology such as the windmill. Crops such as almonds and citrus fruit were brought to Europe through al-Andalus, and sugar cultivation was gradually adopted by the Europeans. Apart from the Nile, Tigris and Euphrates, navigable rivers were uncommon, so transport by sea was very important. Navigational sciences were highly developed, making use of a rudimentary sextant (known as a kamal). When combined with detailed maps of the period, sailors were able to sail across oceans rather than skirt along the coast. Muslim sailors were also responsible for reintroducing large three masted merchant vessels to the Mediterranean. The name caravel may derive from an earlier Arab ship known as the "qārib". Arab merchants dominated trade in the Indian Ocean until the arrival of the Portuguese in the 16th century. Hormuz was an important center for this trade. There was also a dense network of trade routes in the Mediterranean, along which Muslim countries traded with each other and with European powers such as Venice, Genoa and Catalonia. The Silk Road crossing Central Asia passed through Muslim states between China and Europe.
Muslim engineers in the Islamic world made a number of innovative industrial uses of hydropower, and early industrial uses of tidal power, wind power, and petroleum (notably by distillation into kerosene). The industrial uses of watermills in the Islamic world date back to the 7th century, while horizontal-wheeled and vertical-wheeled water mills were both in widespread use since at least the 9th century. By the time of the Crusades, every province throughout the Islamic world had mills in operation, from al-Andalus and North Africa to the Middle East and Central Asia. These mills performed a variety of agricultural and industrial tasks. Muslim engineers also developed machines (such as pumps) incorporating crankshafts, employed gears in mills and water-raising machines, and used dams to provide additional power to watermills and water-raising machines. Such advances made it possible for many industrial tasks that were previously driven by manual labour in ancient times to be mechanized and driven by machinery instead in the medieval Islamic world. It has been argued that the industrial use of waterpower had spread from Islamic to Christian Spain, where fulling mills, paper mills, and forge mills were recorded for the first time in Catalonia.
A number of industries were generated during the Arab Agricultural Revolution, including early industries for textiles, sugar, rope-making, matting, silk, and paper. Latin translations of the 12th century passed on knowledge of chemistry and instrument making in particular. The agricultural and handicraft industries also experienced high levels of growth during this period.
Status of Women.
As the Arab Muslims conquered and moved the centers of power from the Arabian Peninsula to what is now Iraq, during the time of the Abbasid Caliphate, the status of women notably declined. In fact, to quote Harvard Divinity scholar Leila Ahmed "In Abbasid society women were conspicuous for their absence from all arenas of the community's central affairs." While their Muslim forbearers led men into battle, started rebellions, and played an active role in community life, as demonstrated in the Hadith literature, Abbasid women were ideally kept in seclusion. Conquests had brought enormous wealth and large numbers of slaves to the Muslim elite. The majority of the slaves were women and children, many of whom had been dependents or harem-members of the defeated Sassanian upper classes. In the wake of the conquests an elite man could potentially own a thousand slaves, and ordinary soldiers could have ten people serving them.
Nabia Abbott, preeminent historian of elite women of the Abbasid Caliphate, describes the lives of harem women as follows.
The choicest women were imprisoned behind heavy curtains and locked doors, the strings and keys of which were entrusted into the hands of that pitiable creature – the eunuch. As the size of the harem grew, men indulged to satiety. Satiety within the individual harem meant boredom for the one man and neglect for the many women. Under these conditions ... satisfaction by perverse and unnatural means crept into society, particularly in its upper classes.
Due to the fact that the marketing of human beings, particularly women, as objects for sexual use meant that elite men owned the vast majority of women they interacted with, and related to them as would masters to slaves. Being a slave meant relative lack of autonomy during this time period, and belonging to a harem caused a wife and her children to have little insurance of stability and continued support due to the volatile politics of harem life.
Elite men expressed in literature the horror they felt for the humiliation and degradation of their daughters and female relatives. For example, the verses addressed to Hasan ibn al-Firat on the death of his daughter read:
Historical evidence suggests that many of these practices originated not in the culture of the Early Islamic dynasties centered in Mecca and Medina, rather, they were learned from the conquered empires, the declining Sassanian and Persian societies. It is argued that many of the stereotypes and dogmas about Islamic gender roles date from the Abbasid period.
Even so, courtesans and princesses produced prestigious and important poetry. Enough survives to give us access to women's historical experiences, and reveals some vivacious and powerful figures, such as the Sufi mystic Raabi'a al-Adwiyya (714–801 CE), the princess and poet 'Ulayya bint al-Mahdi (777–825 CE), and the singing-girls Shāriyah (c. c. 815-70 CE), Fadl Ashsha'ira (d. 871 CE) and Arib al-Ma'muniyya (797–890 CE).
Evolution of Islamic identity.
While the Abbasids originally gained power by exploiting the social inequalities against non-Arabs in the Umayyad Empire, ironically during Abbasid rule the empire rapidly Arabized. As knowledge was shared in the Arabic language throughout the empire, people of different nationalities and religions began to speak Arabic in their everyday lives. Resources from other languages began to be translated into Arabic, and a unique Islamic identity began to form that fused previous cultures with Arab culture, creating a level of civilization and knowledge that was considered a marvel in Europe.
Decline of the empire.
Abbasids found themselves at odds with the Shia Muslims, most of whom had supported their war against the Umayyads, since the Abbasids and the Shias claimed legitimacy by their familial connection to Muhammad. Once in power, the Abbasids embraced Sunni Islam and disavowed any support for Shi'a beliefs. Shortly thereafter, Berber Kharijites set up an independent state in North Africa in 801. Within 50 years the Idrisids in the Maghreb and Aghlabids of Ifriqiya and a little later the Tulunids and Ikshidids of Misr were effectively independent in Africa. The Abbasid authority began to deteriorate during the reign of al-Radi when their Turkic Army generals, who already had de facto independence, stopped paying the Caliphate. Even provinces close to Baghdad began to seek local dynastic rule. Also, the Abbasids found themselves to often be at conflict with the Umayyads in Spain.
The Abbasid financial position weakened as well, with tax revenues from the Sawād decreasing in the 9th and 10th centuries.
Separatist dynasties and their successors.
The Abbasid Caliphate differed from others in that it did not have the same borders and extent as Islam. Particularly, in the west of the Caliphate, there were multiple smaller caliphates that existed in relative peace with them. This list represents the succession of Islamic dynasties that emerged from the fractured Abbasid empire by their general geographic location. Dynasties often overlap, where a vassal emir revolted from and later conquered his lord. Gaps appear during periods of contest where the dominating power was unclear. Except for the Fatimid Caliphate in Egypt, recognizing a Shi'ite succession through Ali, and the Andalusian Caliphates of the Umayyads and Almohads, every Muslim dynasty at least acknowledged the nominal suzerainty of the Abbasids as Caliph and Commander of the Faithful.
Abbasid Khanate of Bastak.
In 656 AH/1258 CE, the year of the fall of Baghdad, and following the sack of the city, a few surviving members of the Abbasid dynastic family led by the eldest amongst them, Ismail II son of Hamza son of Ahmed son of Mohamed, made their way into the region of Fars in Southern Persia. They settled in the city of Khonj, then a great centre for learning and scholarship. Shaikh Abdulsalam Khonji (b. 661 AH – d. 746 AH) son of Abbas son of Ismail II was born in Khonj only five years after the fall of Baghdad and the arrival of his grandfather in the city. He became a great religious scholar and Sufi saint, held in high esteem by the local populace. His tomb still stands in Khonj and is a site visited by people from near and far.
The descendants of Shaikh Abdulsalam Khonji were religious scholars and figures of great respect and repute for generation after generation. One such scholar and direct descendant of Shaikh Abdulsalam Khonji in the male line, Shaikh Mohamed (d. around 905 AH) son of Shaikh Jaber son of Shaikh Ismail IV, moved to Bastak. His grandson, Shaikh Mohamed the Elder (d. 950 or 975 AH) son of Shaikh Nasser al-Din Ahmed son of Shaikh Mohamed, settled in Khonj for a time. But in 938 AH, in response to growing Safavid power, Shaikh Mohamed the Elder moved permanently to Bastak as his grandfather had done. His own grandson, Shaikh Hassan (d. 1084 AH) (also called Mulla Hassan) son of Shaikh Mohamed the Younger son of Shaikh Mohamed the Elder, is the common ancestor of all the Abbasids of Bastak and its neighbouring areas.
Shaikh Hassan’s grandsons, Shaikh Mohamed Saeed (b. 1096 AH – d. 1152 AH) and Shaikh Mohamed Khan (b. 1113 AH – d. 1197 AH) son of Shaikh Abdulqader son of Shaikh Hassan, became the first two Abbasid rulers of the region. In 1137 AH, Shaikh Mohamed Saeed began gathering support for an armed force. Following the capture of Lar, he ruled the city and its dependencies for 12 or 14 years before dying in 1152 AH.
Shaikh Mohamed Khan Bastaki, his brother, was meanwhile the ruler of Bastak and the region of Jahangiriyeh. In 1161 AH, Shaikh Mohamed Khan Bastaki departed for Didehban Fortress, leaving Bastak and its dependencies in the hands of his eldest son Shaikh Mohamed Sadeq and his cousin Agha Hassan Khan son of Mulla Ismail. Shaikh Mohamed Khan ruled Jahangiriyeh from Didehban Fortress for a period of roughly 20 to 24 years, for which reason he has been referred to as Shaikh Mohamed "Didehban". He eventually returned to Bastak and continued to reign from there up to the time of his death. At the height of his rule, the Khanate of Bastak included not only the region of Jahangiriyeh, but its power also extended to Lar and Bandar Abbas as well as their dependencies, not to mention several islands in the Persian Gulf.
Shaikh Mohamed Khan Bastaki was the first Abbasid ruler of Bastak to hold the title of "Khan" (Persian: خان, Arabic: الحاكم), meaning "ruler" or "king", which was bestowed upon him by Karim Khan Zand. The title then became that of all the subsequent Abbasid rulers of Bastak and Jahangiriyeh, and also collectively refers in plural form – i.e., "Khans" (Persian: خوانين) - to the descendants of Shaikh Mohamed Khan Bastaki.
The last Abbasid ruler of Bastak and Jahangiriyeh was Mohamed A’zam Khan Baniabbassian son of Mohamed Reza Khan "Satvat al-Mamalek" Baniabbasi. He authored the book "Tarikh-e Jahangiriyeh va Baniabbassian-e Bastak" (1960), in which is recounted the history of the region and the Abbasid family that ruled it. Mohamed A’zam Khan Baniabbassian died in 1967, a year regarded as marking the end of the Abbasid reign in Bastak.

</doc>
<doc id="49860" url="https://en.wikipedia.org/wiki?curid=49860" title="Fauna of Australia">
Fauna of Australia

The fauna of Australia consists of a huge variety of animals; some 83% of mammals, 89% of reptiles, 24% of fish and insects and 93% of amphibians that inhabit the continent are endemic to Australia. This high level of endemism can be attributed to the continent's long geographic isolation, tectonic stability, and the effects of an unusual pattern of climate change on the soil and flora over geological time. A unique feature of Australia's fauna is the relative scarcity of native placental mammals. Consequently, the marsupials—a group of mammals that raise their young in a pouch, including the macropods, possums and dasyuromorphs—occupy many of the ecological niches placental animals occupy elsewhere in the world. Australia is home to two of the five known extant species of monotremes and has numerous venomous species, which include the platypus, spiders, scorpions, octopus, jellyfish, molluscs, stonefish, and stingrays. Uniquely, Australia has more venomous than non-venomous species of snakes.
The settlement of Australia by Indigenous Australians between 48,000 and 70,000 years ago (research in 2011 using DNA suggesting an arrival around 50,000 years ago), and by Europeans from 1788, has significantly affected the fauna. Hunting, the introduction of non-native species, and land-management practices involving the modification or destruction of habitats have led to numerous extinctions. Some examples include the paradise parrot, pig-footed bandicoot and the broad-faced potoroo. Unsustainable land use still threatens the survival of many species. To target threats to the survival of its fauna, Australia has passed wide-ranging federal and state legislation and established numerous protected areas.
Origins.
Both geologic and climatic events helped to make Australia's fauna unique. Australia was once part of the southern supercontinent Gondwana, which also included South America, Africa, India and Antarctica. Gondwana began to break up 140 million years ago (MYA); 50 MYA Australia separated from Antarctica and was relatively isolated until the collision of the Indo-Australian Plate with Asia in the Miocene era 5.3 MYA. The establishment and evolution of the present-day fauna was apparently shaped by the unique climate and the geology of the continent. As Australia drifted, it was, to some extent, isolated from the effects of global climate change. The unique fauna that originated in Gondwana, such as the marsupials, survived and adapted in Australia.
After the Miocene, fauna of Asian origin were able to establish themselves in Australia. The Wallace Line—the hypothetical line separating the zoogeographical regions of Asia and Australasia—marks the tectonic boundary between the Eurasian and Indo-Australian plates. This continental boundary prevented the formation of land bridges and resulted in a distinct zoological distribution, with limited overlap, of most Asian and Australian fauna, with the exception of birds. Following the emergence of the circumpolar current in the mid-Oligocene era (some 15 MYA), the Australian climate became increasingly arid, giving rise to a diverse group of arid-specialised organisms, just as the wet tropical and seasonally wet areas gave rise to their own uniquely adapted species.
Mammals.
Australia has a rich mammalian fossil history, as well as a variety of extant mammalian species, dominated by the marsupials,currently however there is limited taxonomic research into Australia’s mammals. The fossil record shows that monotremes have been present in Australia since the Early Cretaceous 145–99 MYA, and that marsupials and placental mammals date from the Eocene 56–34 MYA, when modern mammals first appeared in the fossil record. Although marsupials and placental mammals did coexist in Australia in the Eocene, only marsupials have survived to the present. The placental mammals made their reappearance in Australia in the Miocene, when Australia moved closer to Indonesia, and bats and rodents started to appear reliably in the fossil record. The marsupials evolved to fill specific ecological niches, and in many cases they are physically similar to the placental mammals in Eurasia and North America that occupy similar niches, a phenomenon known as convergent evolution. For example, the top predator in Australia, the Tasmanian tiger, bore a striking resemblance to canids such as the gray wolf; gliding possums and flying squirrels have similar adaptations enabling their arboreal lifestyle; and the numbat and anteaters are both digging insectivores. For the most part, mammals are not a highly visible part of the faunal landscape, as most species are nocturnal and many arboreal. Furthermore, there are few extant large ground-dwelling species.
Monotremes and marsupials.
Two of the five living species of monotreme occur in Australia: the platypus and the short-beaked echidna. The monotremes differ from other mammals in their methods of reproduction; in particular, they lay eggs instead of giving birth to live young. The platypus—a venomous, egg-laying, duck-billed amphibious mammal—is considered to be one of the strangest creatures in the animal kingdom. When it was first presented by Joseph Banks to English naturalists it was thought to be so strange that it was a cleverly created hoax. The short-beaked echidna is similarly strange, covered in hairy spikes with a tubular snout in the place of a mouth, and a tongue that can move in and out of the snout about 100 times a minute to capture termites.
Australia has the world's largest and most diverse range of marsupials. Marsupials are characterised by the presence of a pouch in which they rear their young. The carnivorous marsupials—order Dasyuromorphia—are represented by two surviving families: the Dasyuridae with 51 members, and the Myrmecobiidae with the numbat as its sole surviving member. The Tasmanian tiger was the largest Dasyuromorphia and the last living specimen of the family Thylacinidae died in captivity in 1936. The world's largest surviving carnivorous marsupial is the Tasmanian devil; it is the size of a small dog and can hunt, although it is mainly a scavenger. It became extinct on the mainland some 600 years ago, and is now found only in Tasmania. There are four species of quoll, or "native cat", all of which are threatened species. The remainder of the Dasyuridae are referred to as "marsupial mice"; most weigh less than 100 g. There are two species of marsupial mole—order Notoryctemorphia—that inhabit the deserts of Western Australia. These rare, blind and earless carnivorous creatures spend most of their time underground; little is known about them.
The bandicoots and bilbies—order Peramelemorphia—are marsupial omnivores. There are seven species in Australia, most of which are endangered. These small creatures share several characteristic physical features: a plump, arch-backed body with a long, delicately tapering snout, large upright ears, long, thin legs, and a thin tail. The evolutionary origin of this group is unclear, because they share characteristics from both carnivorous and herbivorous marsupials.
Herbivorous marsupials are classified in the order Diprotodontia, and further into the suborders Vombatiformes and Phalangerida. The Vombatiformes include the koala and the three species of wombat. One of Australia's best-known marsupials, the koala is an arboreal species that feeds on the leaves of various species of eucalyptus. Wombats, on the other hand, live on the ground and feed on grasses, sedges and roots. Wombats use their rodent-like front teeth and powerful claws to dig extensive burrow systems; they are mainly crepuscular and nocturnal.
The Phalangerida includes six families and 26 species of possum and three families with 53 species of macropod. The possums are a diverse group of arboreal marsupials and vary in size from the little pygmy possum, weighing just 7 g, to the cat-sized common ringtail and brushtail possums. The sugar and squirrel gliders are common species of gliding possum, found in the eucalypt forests of eastern Australia, while the feathertail glider is the smallest glider species. The gliding possums have membranes called "patagiums" that extend from the fifth finger of their forelimb back to the first toe of their hind foot. These membranes, when outstretched, allow them to glide between trees.
The macropods are divided into three families: the Hypsiprymnodontidae, with the musky rat-kangaroo as its only member; the Potoroidae, with 11 species; and the Macropodidae, with 45 species. Macropods are found in all Australian environments except alpine areas. The Potoroidae include the bettongs, potaroos and rat-kangaroos, small species that make nests and carry plant material with their tails. The Macropodiae include kangaroos, wallabies and associated species; size varies widely within this family. Most macropods have large hind legs and long, narrow hind feet, with a distinctive arrangement of four toes, and powerfully muscled tails, which they use to hop around. The musky rat-kangaroo is the smallest macropod and the only species that is quadrupedal not bipedal, while the male red kangaroo is the largest, reaching a height of about 2 m and weighing up to 85 kg.
Placental mammals.
Australia has indigenous placental mammals from two orders: the bats—order Chiroptera—represented by six families; and the mice and rats—order Rodentia, family Muridae. Bats and rodents are relatively recent arrivals to Australia; bats are present in the fossil record only from as recently as 15 MYA, and probably arrived from Asia. There are only two endemic genera of bats, although 7% of the world's bats species live in Australia. Rodents first arrived in Australia 5–10 MYA, undergoing a wide radiation to produce the species collectively known as the "old endemic" rodents. The old endemics are represented by 14 extant genera. A million years ago, the rat entered Australia from New Guinea and evolved into seven species of "Rattus", collectively called the "new endemics".
Since human settlement many placental mammals have been introduced to Australia and are now feral. The first animal introduced to Australia was the dingo. Fossil evidence suggests that people from the north brought the dingo to Australia about 5000 years ago. When Europeans settled Australia they intentionally released many species into the wild including the red fox, brown hare, and the European rabbit. Other domestic species have escaped and over time have produced wild populations including the cat, fallow deer, red deer, sambar deer, rusa deer, chital, hog deer, horse, donkey, pig, goat, water buffalo, and the camel. Only three species of placental mammal were not deliberately introduced to Australia, the house mouse, black rat and the brown rat.
Forty-six marine mammals from the order Cetacea are found in Australian coastal waters. Since the majority of these species have global distribution, some authors do not consider them to be Australian species. There are nine species of baleen whale present, including the humpback whale. There are 37 species of toothed whale, which include all six genera of the family Ziphiidae, and 21 species of oceanic dolphin, including the Australian snubfin dolphin, a species first described in 2005. Some oceanic dolphins, such as the orca, can be found in all waters around the continent; others, such as the Irrawaddy dolphin, are confined to the warm northern waters. The dugong is an endangered marine species that inhabits the waters of north-eastern and north-western Australia, particularly the Torres Strait. It can grow up to 3 m long and weigh as much as 400 kg. The dugong is the only herbivorous marine mammal in Australia, feeding on sea grass in coastal areas. The destruction of sea grass beds is a threat to the survival of this species. Eleven species of seal—family Pinnipedia—live off the southern coast.
Birds.
Australia and its territories are home to around 800 species of bird; about 350 of these are endemic to the zoogeographic region that covers Australia, New Guinea and New Zealand. The fossil record of birds in Australia is patchy; however, there are records of the ancestors of contemporary species as early as the Late Oligocene. Birds with a Gondwanan history include the flightless ratites (the emu and southern cassowary), megapodes (the malleefowl and Australian brush-turkey), and a huge group of endemic parrots, order Psittaciformes. Australian parrots comprise a sixth of the world's parrots, including many cockatoos and galahs. The kookaburra is the largest species of the kingfisher family, known for its call, which sounds uncannily like loud, echoing human laughter.
The passerines of Australia, also known as songbirds or perching birds, include wrens, robins, the magpie group, thornbills, pardalotes, the huge honeyeater family, treecreepers, lyrebirds, birds of paradise and bowerbirds. The satin bowerbird has attracted the interest of evolutionary psychologists; it has a complex courtship ritual in which the male creates a bower filled with blue, shiny items to woo mates.
Relatively recent colonists from Eurasia are swallows, larks, thrushes, cisticolas, sunbirds, and some raptors, including the large wedge-tailed eagle. A number of bird species have been introduced by humans; some, like the European goldfinch and greenfinch, coexist happily with Australian species, while others, such as the common starling, common blackbird, house sparrow and Indian mynah, are destructive of some native bird species and thus destabilise the native ecosystem.
About 200 species of seabird live on the Australian coast, including many species of migratory seabird. Australia is at the southern end of the East Asian-Australasian Flyway for migratory water birds, which extends from Far-East Russia and Alaska through Southeast Asia to Australia and New Zealand. About two million birds travel this route to and from Australia each year. One very common large seabird is the Australian pelican, which can be found in most waterways in Australia. The little penguin is the only species of penguin that breeds on mainland Australia.
Amphibians and reptiles.
Australia has four families of native frogs and one introduced toad, the cane toad. In 1935 the cane toad was introduced to Australia in a failed attempt to control pests in sugarcane crops. It has since become a devastating pest, spreading across northern Australia. As well as competing with native insectivores for food, the cane toad produces a venom that is toxic to native fauna, as well as to humans. The Myobatrachidae, or southern frogs, are Australia's largest group of frogs, with 112 species classified into anywhere from 17 to 22 genera. A notable member of this group is the colourful and endangered Corroboree frog. The tree frogs, from family Hylidae, are common in high rainfall areas on the north and east coasts; there are 77 Australian species from three genera. The 18 species from two genera of the Microhylidae frogs are restricted to the rainforests of northern Australia and nearby habitats; the smallest species, the scanty frog, is from this family. There is a single species from the world's dominant frog group, family Ranidae — the Australian wood frog — which only occurs in the Queensland rainforests. As elsewhere, there has been a precipitous decline in Australia's frog populations in recent years. Although the full reasons for the decline are uncertain, it can be at least partly attributed to the fatal amphibian fungal disease chytridiomycosis. Another theory for the decline might be, as research shows, that species from the Southern Hemisphere are on average 4.6 million years old, compared to an average 2.9 million years old for the Northern Hemisphere: Researchers believe this age difference is because of the history of severe ice ages in the Northern Hemisphere, which may drive older species to extinction.
Australia has two species of crocodile. The saltwater crocodile, known colloquially as the "salty", is the largest living crocodile species; reaching over 7 m, and weighing over 1,000 kg, they can and do kill people. They live on the coast and in the freshwater rivers and wetlands of northern Australia, and they are farmed for their meat and leather. Freshwater crocodiles, found only in northern Australia, are not considered dangerous to humans.
The Australian coast is visited by six species of sea turtle: the flatback, green sea, hawksbill, olive ridley, loggerhead and the leatherback sea turtles; all are protected in Australian waters. There are 35 species of Australian freshwater turtles from eight genera of the family Chelidae. The pig-nosed turtle is the only Australian turtle not of that family. Australia is the only continent without any living species of land tortoise.
Australia is the only continent where venomous snakes outnumber their non-venomous cousins. Australian snakes belong to seven families. Of these, the most venomous species, including the fierce snake, eastern brown snake, taipan and eastern tiger snake are from the family Elapidae. Of the 200 species of elapid, 86 are found only in Australia. Thirty-three sea snakes from family Hydrophiidae inhabit Australia's northern waters; many are extremely venomous. Two species of sea snake from the Acrochordidae also occur in Australian waters. Australia has only 11 species from the world's most significant snake family Colubridae; none are endemic, and they are considered to be relatively recent arrivals from Asia. There are 15 python species and 31 species of insectivorous blind snake.
There are more than 700 species of lizards in Australia with representatives of five families. There are over 130 species in 20 genera of gecko found throughout the Australian continent. The Pygopodidae is a family of limbless lizards endemic to the Australian region; all 39 species from seven genera occur in Australia. The Agamidae or dragon lizards are represented by 70 species in 14 genera, including the thorny devil, bearded dragon and frill-necked lizard. There are 30 species of monitor lizard, family Varanidae, in Australia, where they are commonly known as goannas. The largest Australian monitor is the perentie, which can reach up to 2 m in length. There are about 450 species of skink from more than 40 genera, comprising more than 50% of the total Australian lizard fauna; this group includes the blue-tongued lizards.
Fish.
More than 5000 species of fish inhabit Australia's waterways; of these, 24% are endemic. However, because of the relative scarcity of freshwater waterways, Australia has only about 300 species of freshwater fish. Two families of freshwater fish have ancient origins: the arowana or bonytongues, and the Queensland lungfish. The Queensland lungfish is the most primitive of the lungfish, having evolved before Australia separated from Gondwana. One of the smallest freshwater fish, peculiar to the southwest of Western Australia, is the salamanderfish, which can survive desiccation in the dry season by burrowing into mud. Other families with a potentially Gondwanan origin include the Retropinnidae, Galaxiidae, Aplochitonidae and Percichthyidae. Apart from the ancient freshwater species, 70% of Australia's freshwater fish have affinities with tropical Indo-Pacific marine species that have adapted to freshwater. These species include freshwater lampreys, herrings, catfish, rainbowfish, and some 50 species of gudgeon, including the sleepy cod. Native freshwater game fish include the barramundi, Murray cod, and golden perch. Two species of endangered freshwater shark are found in the Northern Territory.
Several exotic freshwater fish species, including brown, brook and rainbow trout, Atlantic and Chinook salmon, redfin perch, common carp, and mosquitofish, have been introduced to Australian waterways. The mosquitofish is a particularly aggressive species known for harassing and nipping the fins of other fish. It has been linked to declines and localised extirpations of several small native fish species. The introduced trout species have had serious negative impacts on a number of upland native fish species including trout cod, Macquarie perch and mountain galaxias species as well as other upland fauna such as the spotted tree frog. The common carp is strongly implicated in the dramatic loss in waterweed, decline of small native fish species and permanently elevated levels of turbidity in the Murray-Darling Basin of south west Australia.
Most of Australia's fish species are marine, and 75% live in tropical marine environments. This is partly due to Australia's huge marine territory, covering 9 million km2. Groups of interest include the moray eels and squirrelfish, as well as the pipefish and seahorses, whose males incubate their partner's eggs in a specialised pouch. There are 80 species of grouper in Australian waters, including one of the world's biggest bony fish, the giant grouper, which can grow as large as 2.7 m and weigh up to 400 kg. The trevally, a group of 50 species of silver schooling fish, and the snappers are popular species for commercial fishing. The Great Barrier Reef supports a huge variety of small- and medium-sized reef fish, including the damselfish, butterflyfish, angelfish, gobies, cardinalfish, wrassees, triggerfish and surgeonfish. There are several venomous fish, among them several species of stonefish and pufferfish and the red lionfish, all of which have toxins that can kill humans. There are 11 venomous species of stingray, the largest of which is the smooth stingray. The barracudas are one of the reef's largest species. However, large reef fish should not be eaten for fear of ciguatera poisoning.
Sharks inhabit all the coastal waters and estuarine habitats of Australia's coast. There are 166 species, including 30 species of requiem shark, 32 of catshark, six of wobbegong shark, and 40 of dogfish shark. There are three species from the family Heterodontidae: the Port Jackson shark, the zebra bullhead shark and the crested bullhead shark. In 2004, there were 12 unprovoked shark attacks in Australia, of which two were fatal. Only 3 species of shark pose a significant threat to humans: the bull shark, the tiger shark and the great white shark. Some popular beaches in Queensland and New South Wales are protected by shark netting, a method that has reduced the population of both dangerous and harmless shark species through accidental entanglement. The overfishing of sharks has also significantly reduced shark numbers in Australian waters, and several species are now endangered. A megamouth shark was found on a Perth beach in 1988; very little is known about this species, but this discovery may indicate the presence of the species in Australian coastal waters.
Invertebrates.
Of the estimated 200,000 animal species in Australia, about 96% are invertebrates. While the full extent of invertebrate diversity is uncertain, 90% of insects and molluscs are considered endemic. Invertebrates occupy many ecological niches and are important in all ecosystems as decomposers, pollinators, and food sources. The largest group of invertebrates is the insects, comprising 75% of Australia's known species of animals. The most diverse insect orders are the Coleoptera, with 28,200 species of beetles and weevils, the Lepidoptera with 20,816 species including butterflies and moths, and around 14,800 species of Hymenoptera, including the ants, bees and wasps. Order Diptera, which includes the flies and mosquitoes,comprises 7,786 species. Order Hemiptera, including bugs, aphids and hoppers, comprises 5,650 species; and there are 2,827 species of order Orthoptera, including grasshoppers, crickets and katydids. Introduced species that pose a significant threat to native species include the European wasp, the red fire ant, the yellow crazy ant and feral honeybees which compete with native bees.
Australia has a wide variety of arachnids, including 135 species of spider familiar enough to have common names. There are numerous highly venomous species, including the notorious Sydney funnel-web and redback spiders, whose bites can be deadly. There are thousands of species of mites and ticks from the subclass Acari. Australia also has eight species of pseudoscorpion and nine scorpion species.
In the Annelida (sub)class Oligochaeta there are many families of aquatic worms, and for native terrestrial worms: the Enchytraeidae (pot worms) and the "true" earthworms in families Acanthodrilidae, Octochaetidae and Megascolecidae. The latter includes the world's largest earthworm, the giant Gippsland earthworm, found only in Gippsland, Victoria. On average they reach 80 cm in length, but specimens up to 3.7 m in length have been found.
The large family Parastacidae includes 124 species of Australian freshwater crayfish. These include the world's smallest crayfish, the swamp crayfish, which does not exceed 30 mm in length, and the world's largest crayfish, the Tasmanian giant freshwater crayfish, measuring up to 76 cm long and weighing 4.5 kg. The crayfish genus "Cherax" includes the common yabby, in addition to the farmed species marron and Queensland red claw. Species from the genus "Engaeus", commonly known as the land crayfish, are also found in Australia. "Engaeus" species are not entirely aquatic, because they spend most of their lives living in burrows. Australia has seven species of freshwater crab from the genus "Austrothelphusa". These crabs live burrowed into the banks of waterways and can plug their burrows, surviving through several years of drought. The extremely primitive freshwater mountain shrimp, found only in Tasmania, are a unique group, resembling species found in the fossil record from 200 MYA.
A huge variety of marine invertebrates are found in Australian waters, with the Great Barrier Reef an important source of this diversity. Families include the Porifera or sea sponges, the Cnidaria (includes the jellyfish, corals and sea anemones, comb jellies), the Echinodermata (includes the sea urchins, sea stars, brittle stars, sea cucumbers, the lamp shells) and the Mollusca (includes snails, slugs, limpets, squid, octopus, cockles, oysters, clams, and chitons). Venomous invertebrates include the box jellyfish, the blue-ringed octopus, and ten species of cone snail, which can cause respiratory failure and death in humans. The crown-of-thorns starfish usually inhabits the Reef at low densities. However, under conditions that are not yet well understood, they can reproduce to reach an unsustainable population density when coral is devoured at a rate faster than it can regenerate. This presents a serious reef management issue. Other problematic marine invertebrates include the native species purple sea urchin and the white urchin, which have been able to take over marine habitats and form urchin barrens due to the over harvesting of their natural predators which include abalone and rock lobster. Introduced invertebrate pests include the Asian mussel, New Zealand green-lipped mussel, black-striped mussel and the Northern Pacific seastar, all of which displace native shellfish.
There are many unique marine crustaceans in Australian waters. The best-known class, to which all the edible species of crustacean belong, is Malacostraca. The warm waters of northern Australia are home to many species of decapod crustaceans, including crabs, false crabs, hermit crabs, lobsters, shrimps, and prawns. The peracarids, including the amphipods and isopods, are more diverse in the colder waters of southern Australia. Less-well-known marine groups include the classes Remipedia, Cephalocarida, Branchiopoda, Maxillopoda (which includes the barnacles, copepods and fish lice), and the Ostracoda. Notable species include the Tasmanian giant crab, the second largest crab species in the world, found in deep water, and weighing up to 13 kg, and the Australian spiny lobsters, such as the western rock lobster, which are distinct from other lobster species as they do not have claws.
Invasive species.
Introduction of exotic fauna in Australia by design, accident and natural processes has led to a considerable number of invasive, feral and pest species which have flourished and now impact the environment adversely. Introduced organisms affect the environment in a number of ways. Rabbits render land economically useless by eating everything. Red foxes affect local endemic fauna by predation while the cane toad poisons the predators by being eaten. The invasive species include birds (Indian mynah) and fish (common carp), insects (red imported fire ant) and molluscs (Asian mussel). The problem is compounded by invasive exotic flora as well as introduced diseases, fungi and parasites.
Costly, laborious and time-consuming efforts at control of these species has met with little success and this continues to be a major problem area in the conservation of Australia's biodiversity.
Human impact and conservation.
For at least 40,000 years, Australia's fauna played an integral role in the traditional lifestyles of Indigenous Australians, who exploited many species as a source of food and skins. Vertebrates commonly harvested included macropods, opossums, seals, fish and the short-tailed shearwater, most commonly known as the muttonbird. Invertebrates used as food included insects such as the Bogong moth and larvae collectively called witchetty grubs and molluscs. The use of fire-stick farming, in which large swathes of bushland were burnt to facilitate hunting, modified both flora and fauna — and are thought to have contributed to the extinction of large herbivores with a specialised diet, such as the flightless birds from the genus "Genyornis". The role of hunting and landscape modification by aboriginal people in the extinction of the Australian megafauna is debated.
The impact of Aborigines on native species populations is considered to be less significant than that of the European settlers, whose impact on the landscape has been on a relatively large scale. Since European settlement, direct exploitation of native fauna, habitat destruction and the introduction of exotic predators and competitive herbivores has led to the extinction of some 27 mammal, 23 bird and 4 frog species. Much of Australia's fauna is protected by legislation. The federal Environment Protection and Biodiversity Conservation Act 1999 was created to meet Australia's obligations as a signatory to the 1992 Convention on Biological Diversity. This act protects all native fauna and provides for the identification and protection of threatened species. In each state and territory, there is statutory listing of threatened species. At present, 380 animal species are classified as either endangered or threatened under the EPBC Act, and other species are protected under state and territory legislation. More broadly, a complete cataloguing of all the species within Australia has been undertaken, a key step in the conservation of Australian fauna and biodiversity. In 1973, the federal government established the Australian Biological Resources Study (ABRS), which coordinates research in the taxonomy, identification, classification and distribution of flora and fauna. The ABRS maintains free online databases cataloguing much of the described Australian flora and fauna. Impacts such as the illegal setting of traps in rivers affect animals such as the Australian platypus, along with lack of awareness each year an average of 2–5 Australians lose their lives to what is presumed a safe creature. The key is understanding of Australia's diverse wildlife and fauna; what seems safe is often deadly.
Australia is a member of the International Whaling Commission and is strongly opposed to commercial whaling—all cetacean species are protected in Australian waters. Australia is also a signatory to the CITES agreement and prohibits the export of endangered species. Protected areas have been created in every state and territory to protect and preserve the country's unique ecosystems. These protected areas include national parks and other reserves, as well as 64 wetlands registered under the Ramsar Convention and 16 World Heritage Sites. As of 2002, 10.8% (774,619.51 km²) of the total land area of Australia is within protected areas. Protected marine zones have been created in many areas to preserve marine biodiversity; as of 2002, these areas cover about 7% (646,000 km²) of Australia's marine jurisdiction. The Great Barrier Reef is managed by the Great Barrier Reef Marine Park Authority under specific federal and state legislation. Some of Australia's fisheries are already overexploited, and quotas have been set for the sustainable harvest of many marine species.
The "State of the Environment Report, 2001", prepared by independent researchers for the federal government, concluded that the condition of the environment and environmental management in Australia had worsened since the previous report in 1996. Of particular relevance to wildlife conservation, the report indicated that many processes—such as salinity, changing hydrological conditions, land clearing, fragmentation of ecosystems, poor management of the coastal environment, and invasive species—pose major problems for protecting Australia's biodiversity.

</doc>
