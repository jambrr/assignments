<doc id="52371" url="https://en.wikipedia.org/wiki?curid=52371" title="Titanic (1997 film)">
Titanic (1997 film)

Titanic is a 1997 American epic romantic disaster film directed, written, co-produced, and co-edited by James Cameron. A fictionalized account of the sinking of the RMS "Titanic", it stars Leonardo DiCaprio and Kate Winslet as members of different social classes who fall in love aboard the ship during its ill-fated maiden voyage.
Cameron's inspiration for the film came from his fascination with shipwrecks; he felt a love story interspersed with the human loss would be essential to convey the emotional impact of the disaster. Production began in 1995, when Cameron shot footage of the actual "Titanic" wreck. The modern scenes on the research vessel" "were shot on board the "Akademik Mstislav Keldysh", which Cameron had used as a base when filming the wreck. Scale models, computer-generated imagery, and a reconstruction of the "Titanic" built at Playas de Rosarito in Baja California were used to re-create the sinking. The film was partially funded by Paramount Pictures and 20th Century Fox. It was the most expensive film made at that time, with an estimated budget of $200 million.
Upon its release on December 19, 1997, "Titanic" achieved critical and commercial success. Nominated for fourteen Academy Awards, it tied "All About Eve" (1950) for the most Oscar nominations, and won eleven, including the awards for Best Picture and Best Director, tying "Ben Hur" (1959) for the most Oscars won by a single film. With an initial worldwide gross of over $1.84 billion, "Titanic" was the first film to reach the billion-dollar mark. It remained the highest-grossing film of all time until Cameron's 2009 film "Avatar" surpassed it in 2010. A 3D version of "Titanic," released on April 4, 2012 to commemorate the centennial of the sinking, earned it an additional $343.6 million worldwide, pushing the film's worldwide total to $2.18 billion. It became the second film to gross more than $2 billion worldwide (after "Avatar").
Plot.
In 1996, treasure hunter Brock Lovett and his team aboard the research vessel "Akademik Mstislav Keldysh" search the wreck of RMS "Titanic" for a necklace with a rare diamond, the Heart of the Ocean. They recover a safe containing a drawing of a young woman wearing only the necklace dated April 14, 1912, the day the ship struck the iceberg. Rose Dawson Calvert, the woman in the drawing, is brought aboard "Keldysh" and tells Lovett of her experiences aboard "Titanic".
In 1912 Southampton, 17-year-old first-class passenger Rose DeWitt Bukater, her fiancé Cal Hockley, and her mother Ruth board the luxurious "Titanic". Ruth emphasizes that Rose's marriage will resolve their family's financial problems. Distraught over the engagement, Rose considers suicide by jumping from the stern; Jack Dawson, a penniless artist, intervenes and discourages her. Discovered with Jack, Rose tells a concerned Cal that she was peering over the edge and Jack saved her from falling. When Cal becomes indifferent, she suggests to him that Jack deserves a reward. He invites Jack to dine with them in first class the following night. Jack and Rose develop a tentative friendship, despite Cal and Ruth being wary of him. Following dinner, Rose secretly joins Jack at a party in third class.
Aware of Cal and Ruth's disapproval, Rose rebuffs Jack's advances, but realizes she prefers him over Cal. After rendezvousing on the bow at sunset, Rose takes Jack to her state room; at her request, Jack sketches Rose posing nude wearing Cal's engagement present, the Heart of the Ocean necklace. They evade Cal's bodyguard and have sex in an automobile inside the cargo hold. On the forward deck, they witness a collision with an iceberg and overhear the officers and designer discussing its seriousness.
Cal discovers Jack's sketch of Rose and an insulting note from her in his safe along with the necklace. When Jack and Rose attempt to inform Cal of the collision, he has his bodyguard slip the necklace into Jack's pocket and accuses him of theft. Jack is arrested, taken to the master-at-arms' office, and handcuffed to a pipe. Cal puts the necklace in his own coat pocket.
With the ship sinking, Rose flees Cal and her mother, who has boarded a lifeboat, and frees Jack. On the boat deck, Cal and Jack encourage her to board a lifeboat; Cal claims he can get himself and Jack off safely. After Rose boards one, Cal tells Jack the arrangement is only for himself. As her boat lowers, Rose decides that she cannot leave Jack and jumps back on board. Cal takes his bodyguard's pistol and chases Rose and Jack into the flooding first-class dining saloon. After using up his ammunition, Cal realizes he gave his coat and consequently the necklace to Rose. He later boards a collapsible lifeboat by carrying a lost child.
After braving several obstacles, Jack and Rose return to the boat deck. The lifeboats have departed and passengers are falling to their deaths as the stern rises out of the water. The ship breaks in half, lifting the stern into the air. Jack and Rose ride it into the ocean and he helps her onto a wooden panel only buoyant enough for one person. He assures her that she will die an old woman, warm in her bed. He dies of hypothermia but she is saved.
With Rose hiding from Cal en route, the RMS "Carpathia" takes the survivors to New York City where Rose gives her name as Rose Dawson. She later finds out Cal committed suicide after losing all his money in the 1929 Wall Street crash.
Back in the present, Lovett decides to abandon his search after hearing Rose's story. Alone on the stern of "Keldysh", Rose takes out the Heart of the Ocean — in her possession all along — and drops it into the sea over the wreck site. While she is seemingly asleep or has died in her bed, photos on her dresser depict a life of freedom and adventure inspired by Jack. A young Rose reunites with Jack at the "Titanic" Grand Staircase, applauded by those who perished.
Cast.
Historical characters.
Although not—and not intended to be—an entirely historically accurate depiction of events, the film includes dramatizations of certain historical characters:
Cameos.
Several crew members of the "Akademik Mstislav Keldysh" appear in the film, including Anatoly Sagalevich, creator and pilot of the "MIR" self-propelled Deep Submergence Vehicle. Anders Falk, who filmed a documentary about the film's sets for the "Titanic" Historical Society, makes a cameo appearance in the film as a Swedish immigrant whom Jack Dawson meets when he enters his cabin; Edward Kamuda and Karen Kamuda, then President and Vice President of the Society who served as film consultants, were cast as extras in the film.
Pre-production.
Writing and inspiration.
James Cameron had a fascination with shipwrecks, and, for him, the RMS "Titanic" was "the Mount Everest of shipwrecks." He was almost past the point in his life when he felt he could consider an undersea expedition, but said he still had "a mental restlessness" to live the life he had turned away from when he switched from the sciences to the arts in college. So when an IMAX film was made from footage shot of the wreck itself, he decided to seek Hollywood funding to "pay for an expedition and do the same thing." It was "not because I particularly wanted to make the movie," Cameron said. "I wanted to dive to the shipwreck."
Cameron wrote a scriptment for a "Titanic" film, met with 20th Century Fox executives including Peter Chernin, and pitched it as "Romeo and Juliet on the "Titanic"". Cameron stated, "They were like, 'Oooooohkaaaaaay – a three-hour romantic epic? Sure, that's just what we want. Is there a little bit of "Terminator" in that? Any Harrier jets, shoot-outs, or car chases?' I said, 'No, no, no. It's not like that.'" The studio was dubious about the idea's commercial prospects, but, hoping for a long-term relationship with Cameron, they gave him a greenlight.
Cameron convinced Fox to promote the film based on the publicity afforded by shooting the "Titanic" wreck itself, and organized several dives to the site over a period of two years. "My pitch on that had to be a little more detailed," said Cameron. "So I said, 'Look, we've got to do this whole opening where they're exploring the "Titanic" and they find the diamond, so we're going to have all these shots of the ship." Cameron stated, "Now, we can either do them with elaborate models and motion control shots and CG and all that, which will cost X amount of money – or we can spend X plus 30 per cent and actually go shoot it at the real wreck." The crew shot at the real wreck in the Atlantic Ocean twelve times in 1995 and actually spent more time with the ship than its passengers. At that depth, with a water pressure of 6,000 pounds per square inch, "one small flaw in the vessel's superstructure would mean instant death for all on board." Not only were the dives high-risk, but adverse conditions prevented Cameron from getting the high quality footage that he wanted. During one dive, one of the submersibles collided with "Titanic"s hull, damaging both sub and ship and leaving fragments of the submersible's propeller shroud scattered around the superstructure. The external bulkhead of Captain Smith's quarters collapsed, exposing the interior. The area around the entrance to the Grand Staircase was also damaged.
Descending to the actual site made both Cameron and crew want "to live up to that level of reality... But there was another level of reaction coming away from the real wreck, which was that it wasn't just a story, it wasn't just a drama," he said. "It was an event that happened to real people who really died. Working around the wreck for so much time, you get such a strong sense of the profound sadness and injustice of it, and the message of it." Cameron stated, "You think, 'There probably aren't going to be many filmmakers who go to "Titanic." There may never be another one – maybe a documentarian." Due to this, he felt "a great mantle of responsibility to convey the emotional message of it – to do that part of it right, too".
After filming the underwater shots, Cameron began writing the screenplay. He wanted to honor the people who died during the sinking, so he spent six months researching all of the "Titanic"s crew and passengers. "I read everything I could. I created an extremely detailed timeline of the ship's few days and a very detailed timeline of the last night of its life," he said. "And I worked within that to write the script, and I got some historical experts to analyze what I'd written and comment on it, and I adjusted it." He paid meticulous attention to detail, even including a scene depicting the "Californian"s role in "Titanic" demise, though this was later cut (see below). From the beginning of the shoot, they had "a very clear picture" of what happened on the ship that night. "I had a library that filled one whole wall of my writing office with Titanic stuff, because I wanted it to be right, especially if we were going to dive to the ship," he said. "That set the bar higher in a way – it elevated the movie in a sense. We wanted this to be a definitive visualization of this moment in history as if you'd gone back in a time machine and shot it."
Cameron felt the "Titanic" sinking was "like a great novel that really happened", but that the event had become a mere morality tale; the film would give audiences the experience of living the history. The treasure hunter Brock Lovett represented those who never connected with the human element of the tragedy, while the blossoming romance of Jack and Rose, Cameron believed, would be the most engaging part of the story: when their love is finally destroyed, the audience would mourn the loss. He said: "All my films are love stories, but in "Titanic" I finally got the balance right. It's not a disaster film. It's a love story with a fastidious overlay of real history."
Cameron framed the romance with the elderly Rose to make the intervening years palpable and poignant. While Winslet and Stuart stated their belief that, instead of being asleep in her bed, the character dies at the end of the film, Cameron stated that, although he knows what he intended with the ending, he will not reveal its intention, adding, "The answer has to be something you supply personally; individually."
Scale modeling.
Harland and Wolff, the RMS "Titanic" builders, opened their private archives to the crew, sharing blueprints that were thought lost. For the ship's interiors, production designer Peter Lamont's team looked for artifacts from the era. The newness of the ship meant every prop had to be made from scratch. Fox acquired 40 acres of waterfront south of Playas de Rosarito in Mexico, and began building a new studio on May 31, 1996. A horizon tank of seventeen million gallons was built for the exterior of the reconstructed ship, providing 270 degrees of ocean view. The ship was built to full scale, but Lamont removed redundant sections on the superstructure and forward well deck for the ship to fit in the tank, with the remaining sections filled with digital models. The lifeboats and funnels were shrunk by ten percent. The boat deck and A-deck were working sets, but the rest of the ship was just steel plating. Within was a fifty-foot lifting platform for the ship to tilt during the sinking sequences. Towering above was a tower crane on of rail track, acting as a combined construction, lighting, and camera platform.
The sets representing the interior rooms of the "Titanic" were reproduced exactly as originally built, using photographs and plans from the "Titanic" builders. "The liner's first-class staircase, which figures prominently in the script was constructed out of real wood and actually destroyed in the filming of the sinking." The rooms, the carpeting, design and colors, individual pieces of furniture, decorations, chairs, wall paneling, cutlery and crockery with the White Star Line crest on each piece, completed ceilings, and costumes were among the designs true to the originals. Cameron additionally hired two "Titanic" historians, Don Lynch and Ken Marschall, to authenticate the historical detail in the film.
Production.
Principal photography of "Titanic" began in July 1996 at Dartmouth, Nova Scotia with the filming of the modern day expedition scenes aboard the "Akademik Mstislav Keldysh". In September 1996, the production moved to the newly built Fox Baja Studios at Rosarito, Mexico where a full scale RMS "Titanic" had been constructed. The poop deck was built on a hinge which could rise from zero to ninety degrees in a few seconds as the ship's stern rose during the sinking. For the safety of the stuntmen, many props were made of foam rubber. By November 15, the boarding scenes were being shot. Cameron chose to build his RMS "Titanic" on the starboard side as a study of weather data showed prevailing north-to-south wind which blew the funnel smoke aft. This posed a problem for shooting the ship's departure from Southampton, as it was docked on its port side. Any writing on props and costumes had to be reversed, and if someone walked to their right in the script, they had to walk left during shooting. In post-production, the film was flipped to the correct direction.
A full-time etiquette coach was hired to instruct the cast on the manners of the upper class gentility in 1912. Despite this, several critics picked up on anachronisms in the film, not least involving the two main stars.
Cameron sketched Jack's nude portrait of Rose for a scene which he feels has the backdrop of repression. "You know what it means for her, the freedom she must be feeling. It's kind of exhilarating for that reason," he said. The nude scene was DiCaprio and Winslet's first scene together. "It wasn't by any kind of design, although I couldn't have designed it better. There's a nervousness and an energy and a hesitance in them," Cameron stated. "They had rehearsed together, but they hadn't shot anything together. If I'd had a choice, I probably would have preferred to put it deeper into the body of the shoot." He said he and his crew "were just trying to find things to shoot" because the big set was not yet ready. "It wasn't ready for months, so we were scrambling around trying to fill in anything we could get to shoot." After seeing the scene on film, Cameron felt it worked out considerably well.
Other times on the set were not as smooth. The shoot was an arduous experience that "cemented Cameron's formidable reputation as 'the scariest man in Hollywood'. He became known as an uncompromising, hard-charging perfectionist" and a "300-decibel screamer, a modern-day Captain Bligh with a megaphone and walkie-talkie, swooping down into people's faces on a 162ft crane". Winslet chipped a bone in her elbow during filming, and had been worried that she would drown in the 17m-gallon water tank the ship was to be sunk in. "There were times when I was genuinely frightened of him. Jim has a temper like you wouldn't believe," she said. "'God damn it!' he would yell at some poor crew member, 'that's exactly what I didn't want!'" Her co-star, Bill Paxton, was familiar with Cameron's work ethic from his earlier experience with him. "There were a lot of people on the set. Jim is not one of those guys who has the time to win hearts and minds," he said. The crew felt that Cameron had an evil alter ego, and nicknamed him "Mij" (Jim spelt backwards). In response to the criticism, Cameron stated, "Film-making is war. A great battle between business and aesthetics."
During the "Akademik Mstislav Keldysh" shoot in Canada, an angry crew member put the dissociative drug PCP into the soup that Cameron and various others ate one night in Dartmouth, Nova Scotia. It sent more than 50 people to the hospital including actor Bill Paxton. "There were people just rolling around, completely out of it. Some of them said they were seeing streaks and psychedelics," said actor Lewis Abernathy. Cameron managed to vomit before the drug took a full hold. Abernathy was shocked at the way he looked. "One eye was completely red, like the Terminator eye. A pupil, no iris, beet red. The other eye looked like he'd been sniffing glue since he was four." The person behind the poisoning was never caught.
The filming schedule was intended to last 138 days but grew to 160. Many cast members came down with colds, flu, or kidney infections after spending hours in cold water, including Winslet. In the end, she decided she would not work with Cameron again unless she earned "a lot of money". Several others left and three stuntmen broke their bones, but the Screen Actors Guild decided, following an investigation, that nothing was inherently unsafe about the set. Additionally, DiCaprio said there was no point when he felt he was in danger during filming. Cameron believed in a passionate work ethic and never apologized for the way he ran his sets, although he acknowledged:I'm demanding, and I'm demanding on my crew. In terms of being kind of militaresque, I think there's an element of that in dealing with thousands of extras and big logistics and keeping people safe. I think you have to have a fairly strict methodology in dealing with a large number of people.
The costs of filming "Titanic" eventually began to mount, and finally reached $200 million. Fox executives panicked, and suggested an hour of specific cuts from the three-hour film. They argued the extended length would mean fewer showings, thus less money even though long epics are more likely to help directors win Oscars. Cameron refused, telling Fox, "You want to cut my movie? You're going to have to fire me! You want to fire me? You're going to have to kill me!" The executives did not want to start over, because it would mean the loss of their entire investment, but they also initially rejected Cameron's offer of forfeiting his share of the profits as an empty gesture; they felt that profits would be unlikely. Cameron explained forfeiting his share as complex. "...the short version is that the film cost proportionally much more than "" and "True Lies." Those films went up seven or eight percent from the initial budget. "Titanic" also had a large budget to begin with, but it went up a lot more," said Cameron. "As the producer and director, I take responsibility for the studio that's writing the checks, so I made it less painful for them. I did that on two different occasions. They didn't force me to do it; they were glad that I did."
Post-production.
Effects.
Cameron wanted to push the boundary of special effects with his film, and enlisted Digital Domain to continue the developments in digital technology which the director pioneered while working on "The Abyss" and "". Many previous films about the RMS "Titanic" shot water in slow motion, which did not look wholly convincing. He encouraged them to shoot their miniature of the ship as if "we're making a commercial for the White Star Line". Afterwards, digital water and smoke were added, as were extras captured on a motion capture stage. Visual effects supervisor Rob Legato scanned the faces of many actors, including himself and his children, for the digital extras and stuntmen. There was also a model of the ship's stern that could break in two repeatedly, the only miniature to be used in water. For scenes set in the ship's engines, footage of the SS "Jeremiah O'Brien"s engines were composited with miniature support frames and actors shot against a greenscreen. In order to save money, the first-class lounge was a miniature set incorporated into a greenscreen backdrop.
An enclosed tank was used for sinking interiors, in which the entire set could be tilted into the water. In order to sink the Grand Staircase, of water were dumped into the set as it was lowered into the tank. Unexpectedly, the waterfall ripped the staircase from its steel-reinforced foundations, although no one was hurt. The exterior of the RMS "Titanic" had its first half lowered into the tank, but being the heaviest part of the ship meant it acted as a shock absorber against the water; to get the set into the water, Cameron had much of the set emptied and even smashed some of the promenade windows himself. After submerging the dining saloon, three days were spent shooting Lovett's ROV traversing the wreck in the present. The post-sinking scenes in the freezing Atlantic were shot in a tank, where the frozen corpses were created by applying a powder on actors that crystallized when exposed to water, and wax was coated on hair and clothes.
The climactic scene, which features the breakup of the ship directly before it sinks, as well as its final plunge to the bottom of the Atlantic, involved a tilting full-sized set, 150 extras and 100 stunt performers. Cameron criticized previous "Titanic" films for depicting the final plunge of the liner as sliding gracefully underwater. He "wanted to depict it as the terrifyingly chaotic event that it really was". When carrying out the sequence, people needed to fall off the increasingly tilting deck, plunging hundreds of feet below and bouncing off of railings and propellers on the way down. A few attempts to film this sequence with stunt people resulted in some minor injuries and Cameron halted the more dangerous stunts. The risks were eventually minimized "by using computer generated people for the dangerous falls".
Editing.
There was one "crucial historical fact" Cameron chose to omit from the film – the ship that was close to the "Titanic", but had turned off its radio for the night and did not hear their SOS calls. "Yes, the "Californian." That wasn't a compromise to mainstream filmmaking. That was really more about emphasis, creating an emotional truth to the film," stated Cameron. He said there were aspects of retelling the sinking that seemed important in pre and post-production, but turned out to be less important as the film evolved. "The story of the "Californian" was in there; we even shot a scene of them switching off their Marconi radio set," said Cameron. "But I took it out. It was a clean cut, because it focuses you back onto that world. If "Titanic" is powerful as a metaphor, as a microcosm, for the end of the world in a sense, then that world must be self-contained."
During the first assembly cut, Cameron altered the planned ending, which had given resolution to Brock Lovett's story. In the original version of the ending, Brock and Lizzy see the elderly Rose at the stern of the boat, and fear she is going to commit suicide. Rose then reveals that she had the "Heart of the Ocean" diamond all along, but never sold it, in order to live on her own without Cal's money. She tells Brock that life is priceless and throws the diamond into the ocean, after allowing him to hold it. After accepting that treasure is worthless, Brock laughs at his stupidity. Rose then goes back to her cabin to sleep, whereupon the film ends in the same way as the final version. In the editing room, Cameron decided that by this point, the audience would no longer be interested in Brock Lovett and cut the resolution to his story, so that Rose is alone when she drops the diamond. He also did not want to disrupt the audience's melancholy after the "Titanic" sinking.
The version used for the first test screening featured a fight between Jack and Lovejoy which takes place after Jack and Rose escape into the flooded dining saloon, but the test audiences disliked it. The scene was written to give the film more suspense, and featured Cal (falsely) offering to give Lovejoy, his valet, the "Heart of the Ocean" if he can get it from Jack and Rose. Lovejoy goes after the pair in the sinking first-class dining room. Just as they are about to escape him, Lovejoy notices Rose's hand slap the water as it slips off the table behind which she is hiding. In revenge for framing him for the "theft" of the necklace, Jack attacks him and smashes his head against a glass window, which explains the gash on Lovejoy's head that can be seen when he dies in the completed version of the film. In their reactions to the scene, test audiences said it would be unrealistic to risk one's life for wealth, and Cameron cut it for this reason, as well as for timing and pacing reasons. Many other scenes were cut for similar reasons.
Music and soundtrack.
The soundtrack album for "Titanic" was composed by James Horner. For the vocals heard throughout the film, subsequently described by Earle Hitchner of "The Wall Street Journal" as "evocative", Horner chose Norwegian singer Sissel Kyrkjebø, mononymously known as "Sissel". Horner knew Sissel from her album "Innerst i sjelen", and he particularly liked how she sang ""Eg veit i himmerik ei borg"" ("I Know in Heaven There Is a Castle"). He had tried twenty-five or thirty singers before he finally chose Sissel as the voice to create specific moods within the film.
Horner additionally wrote the song "My Heart Will Go On" in secret with Will Jennings because Cameron did not want any songs with singing in the film. Céline Dion agreed to record a demo with the persuasion of her husband René Angélil. Horner waited until Cameron was in an appropriate mood before presenting him with the song. After playing it several times, Cameron declared his approval, although worried that he would have been criticized for "going commercial at the end of the movie". Cameron also wanted to appease anxious studio executives and "saw that a hit song from his movie could only be a positive factor in guaranteeing its completion".
Release.
Initial screening.
20th Century Fox and Paramount Pictures co-financed "Titanic", with Paramount handling the North American distribution and Fox handling the international release. They expected Cameron to complete the film for a release on July 2, 1997. The film was to be released on this date "in order to exploit the lucrative summer season ticket sales when blockbuster films usually do better". In April, Cameron said the film's special effects were too complicated and that releasing the film for summer would not be possible. With production delays, Paramount pushed back the release date to December 19, 1997. "This fueled speculation that the film itself was a disaster." A preview screening in Minneapolis on July 14 "generated positive reviews" and "on the internet was responsible for more favorable word of mouth about the [film". This eventually led to more positive media coverage.
The film premiered on November 1, 1997, at the Tokyo International Film Festival, where reaction was described as "tepid" by "The New York Times". Positive reviews started to appear back in the United States; the official Hollywood premiere occurred on December 14, 1997, where "the big movie stars who attended the opening were enthusiastically gushing about the film to the world media".
Box office.
Including revenue from the 2012 reissue, "Titanic" earned $658,672,302 in North America and $1,526,700,000 in other countries, for a worldwide total of $2,185,372,302. It became the highest-grossing film of all time worldwide in 1998, and remained so for twelve years, until "Avatar", also written and directed by Cameron, surpassed it in 2010. On March 1, 1998, it became the first film to earn more than $1 billion worldwide, and on the weekend April 13–15, 2012—a century after the original vessel's foundering—"Titanic" became the second film to cross the $2 billion threshold during its 3D re-release. Box Office Mojo estimates that "Titanic" is the fifth highest-grossing film of all time in North America when adjusting for ticket price inflation.
Initial theatrical run.
The film received steady attendance after opening in North America on Friday, December 19, 1997. By the end of that same weekend, theaters were beginning to sell out. The film earned $8,658,814 on its opening day and $28,638,131 over the opening weekend from 2,674 theaters, averaging to about $10,710 per venue, and ranking number one at the box office, ahead of the eighteenth James Bond film, "Tomorrow Never Dies". By New Year's Day, "Titanic" had made over $120 million, had increased in popularity and theaters continued to sell out. Its highest grossing single day was Saturday, February 14 (Valentine's Day), 1998, on which it earned $13,048,711, more than eight weeks after its North American debut. It stayed at number one for fifteen consecutive weeks in North America, which remains a record for any film. The film stayed in theaters in North America for almost ten months, before finally closing on Thursday, October 1, 1998 with a final domestic gross of $600,788,188. Outside North America, the film made double its North American gross, generating $1,242,413,080 and accumulating a grand total of $1,843,201,268 worldwide from its initial theatrical run.
Commercial analysis.
Before "Titanic"'s release, various film critics predicted the film would be a significant disappointment at the box office, especially due to it being the most expensive film ever made at the time. When it was shown to the press in autumn of 1997, "it was with massive forebodings" since the "people in charge of the screenings believed they were on the verge of losing their jobs – because of this great albatross of a picture on which, finally, two studios had to combine to share the great load of its making". Cameron also thought he was "headed for disaster" at one point during filming. "We labored the last six months on "Titanic" in the absolute knowledge that the studio would lose $100 million. It was a certainty," he stated. As the film neared release, "particular venom was spat at Cameron for what was seen as his hubris and monumental extravagance". A film critic for the "Los Angeles Times" wrote that "Cameron's overweening pride has come close to capsizing this project" and that the film was "a hackneyed, completely derivative copy of old Hollywood romances".
When the film became a success, with an unprecedented box office performance, it was credited for being a love story that captured its viewers emotions. The film was playing on 3,200 screens ten weeks after it opened, and out of its fifteen straight weeks on top of the charts, jumped 43% in total sales in its ninth week of release. It earned over $20 million a week for ten weeks, and after 14 weeks was still bringing in more than $1 million a week. 20th Century Fox estimated that seven percent of American teenage girls had seen "Titanic" twice by its fifth week. Although young women who saw the film several times, and subsequently caused "", were often credited with having primarily propelled the film to its all-time box office record, other reports have attributed the film's success to positive word of mouth and repeat viewership due to the love story combined with the ground-breaking special effects.
The film's impact on men has also been especially credited. Now considered one of the films that "make men cry", MSNBC's Ian Hodder stated that men admire Jack's sense of adventure, stowing away on a steamship bound for America. "We cheer as he courts a girl who was out of his league. We admire how he suggests nude modeling as an excuse to get naked. So when tragic ending happens, an uncontrollable flood of tears sinks our composure," he said. "Titanic" ability to make men cry was briefly parodied in the 2009 film "Zombieland", where character Tallahassee (Woody Harrelson), when recalling the death of his young son, states: "I haven't cried like that since "Titanic"."
In 2010, the BBC analyzed the stigma over men crying during "Titanic" and films in general. "Middle-aged men are not 'supposed' to cry during movies," stated Finlo Rohrer of the website, citing the ending of "Titanic" as having generated such tears, adding that "men, if they have felt weepy during film, have often tried to be surreptitious about it." Professor Mary Beth Oliver, of Penn State University, stated, "For many men, there is a great deal of pressure to avoid expression of 'female' emotions like sadness and fear. From a very young age, males are taught that it is inappropriate to cry, and these lessons are often accompanied by a great deal of ridicule when the lessons aren't followed." Rohrer said, "Indeed, some men who might sneer at the idea of crying during "Titanic" will readily admit to becoming choked up during "Saving Private Ryan" or "Platoon."" For men in general, "the idea of sacrifice for a 'brother' is a more suitable source of emotion".
Scott Meslow of "The Atlantic" stated while "Titanic" initially seems to need no defense, given its success, it is considered a film "for 15-year-old girls" by its main detractors. He argued that dismissing "Titanic" as fodder for 15-year-old girls fails to consider the film's accomplishment: "that grandiose, 3+ hour historical romantic drama is a film for everyone—including teenage boys." Meslow stated that the despite the film being ranked high by males under the age of 18, matching the ratings for teenage boy-targeted films like "Iron Man", it is common for boys and men to deny liking "Titanic". He acknowledged his own rejection of the film as a child while secretly loving it. "It's this collection of elements—the history, the romance, the action—that made (and continues to make) "Titanic" an irresistible proposition for audiences of all ages across the globe," he stated. ""Titanic" has flaws, but for all its legacy, it's better than its middlebrow reputation would have you believe. It's a great movie for 15-year-old girls, but that doesn't mean it's not a great movie for everyone else too."
Quotes in the film aided its popularity. "Titanic" catchphrase "I'm the king of the world!" became one of the film industry's more popular quotations. According to Richard Harris, a psychology professor at Kansas State University, who studied why people like to cite films in social situations, using film quotations in everyday conversation is similar to telling a joke and a way to form solidarity with others. "People are doing it to feel good about themselves, to make others laugh, to make themselves laugh", he said.
Cameron explained the film's success as having significantly benefited from the experience of sharing. "When people have an experience that's very powerful in the movie theatre, they want to go share it. They want to grab their friend and bring them, so that they can enjoy it," he said. "They want to be the person to bring them the news that this is something worth having in their life. That's how "Titanic" worked." Media Awareness Network stated, "The normal
repeat viewing rate for a blockbuster theatrical film is about 5%. The repeat rate for "Titanic" was over 20%." The box office receipts "were even more impressive" when factoring in "the film's 3-hour-and-14-minute length meant that it could only be shown three times a day compared to a normal movie's four showings". In response to this, "any theatres started midnight showings and were rewarded with full houses until almost 3:30 am".
"Titanic" held the record for box office gross for twelve years. Cameron's follow-up film, "Avatar", was considered the first film with a genuine chance at surpassing its worldwide gross, and did so in 2010. Various explanations for why the film was able to successfully challenge "Titanic" were given. For one, "Two-thirds of "Titanic" haul was earned overseas, and "Avatar" similarly... "Avatar" opened in 106 markets globally and was no. 1 in all of them" and the markets "such as Russia, where "Titanic" saw modest receipts in 1997 and 1998, are white-hot today" with "more screens and moviegoers" than ever before. Brandon Gray, president of Box Office Mojo, said that while "Avatar" may beat "Titanic" revenue record, the film is unlikely to surpass "Titanic" in attendance. "Ticket prices were about $3 cheaper in the late 1990s." In December 2009, Cameron had stated, "I don't think it's realistic to try to topple "Titanic" off its perch. Some pretty good movies have come out in the last few years. "Titanic" just struck some kind of chord." In a January 2010 interview, he gave a different take on the matter once "Avatar" performance was easier to predict. "It's gonna happen. It's just a matter of time," he said.
Author Alexandra Keller, when analyzing "Titanic"'s success, stated that scholars could agree that the film's popularity "appears dependent on contemporary culture, on perceptions of history, on patterns of consumerism and globalization, as well as on those elements experienced filmgoers conventionally expect of juggernaut film events in the 1990s – awesome screen spectacle, expansive action, and, more rarely seen, engaging characters and epic drama."
Critical reception.
"Titanic" garnered mainly positive reviews from film critics, and was positively reviewed by audiences and scholars, who commented on the film's cultural, historical and political impacts. It holds an overall 88% approval rating on review aggregator website Rotten Tomatoes, based on 178 reviews, with a rating average of 8 out of 10. The site's consensus reads: "A mostly unqualified triumph for Cameron, who offers a dizzying blend of spectacular visuals and old-fashioned melodrama." At Metacritic, which assigns a weighted mean rating out of 0–100 reviews from film critics, the film has a rating score of 74 based on 34 reviews, classified as a generally favorably reviewed film.
With regard to the film's overall design, Roger Ebert stated, "It is flawlessly crafted, intelligently constructed, strongly acted, and spellbinding... Movies like this are not merely difficult to make at all, but almost impossible to make well." He credited the "technical difficulties" with being "so daunting that it's a wonder when the filmmakers are also able to bring the drama and history into proportion" and "found convinced by both the story and the sad saga". He named it his ninth best film of 1997. On the television program "Siskel & Ebert", the film received "two thumbs up" and was praised for its accuracy in recreating the ship's sinking; Ebert described the film as "a glorious Hollywood epic" and "well worth the wait," and Gene Siskel found Leonardo DiCaprio "captivating". James Berardinelli stated, "Meticulous in detail, yet vast in scope and intent, "Titanic" is the kind of epic motion picture event that has become a rarity. You don't just watch "Titanic", you experience it." It was named his second best film of 1997. Almar Haflidason of the BBC wrote that "[the sinking of the great ship is no secret, yet for many exceeded expectations in sheer scale and tragedy" and that "when you consider that film tops a bum-numbing three-hour running time, then you have a truly impressive feat of entertainment achieved by Cameron". Joseph McBride of "Boxoffice Magazine" concluded, "To describe "Titanic" as the greatest disaster movie ever made is to sell it short. James Cameron's recreation of the 1912 sinking of the 'unsinkable' liner is one of the most magnificent pieces of serious popular entertainment ever to emanate from Hollywood."
The romantic and emotionally charged aspects of the film were equally praised. Andrew L. Urban of "Urban Cinefile" said, "You will walk out of "Titanic" not talking about budget or running time, but of its enormous emotive power, big as the engines of the ship itself, determined as its giant propellers to gouge into your heart, and as lasting as the love story that propels it." Owen Gleiberman of "Entertainment Weekly" described the film as, "A lush and terrifying spectacle of romantic doom. Writer-director James Cameron has restaged the defining catastrophe of the early 20th century on a human scale of such purified yearning and dread that he touches the deepest levels of popular moviemaking." Janet Maslin of "The New York Times" commented that "Cameron's magnificent "Titanic" is the first spectacle in decades that honestly invites comparison to "Gone With the Wind"." Richard Corliss of "Time" magazine, on the other hand, wrote a mostly negative review, criticizing the lack of interesting emotional elements.
Some reviewers felt that the story and dialogue were weak, while the visuals were spectacular. Kenneth Turan's review in the "Los Angeles Times" was particularly scathing. Dismissing the emotive elements, he stated, "What really brings on the tears is Cameron's insistence that writing this kind of movie is within his abilities. Not only is it not, it is not even close.", and later claimed that the only reason that the film won Oscars was because of its box office total. Barbara Shulgasser of "The San Francisco Examiner" gave "Titanic" one star out of four, citing a friend as saying, "The number of times in this unbelievably badly written script that the two characters refer to each other by name was an indication of just how dramatically the script lacked anything more interesting for the actors to say." Also, filmmaker Robert Altman called it "the most dreadful piece of work I've ever seen in my entire life". In his 2012 study of the lives of the passengers on the "Titanic", historian Richard Davenport-Hines said, "Cameron's film diabolized rich Americans and educated English, anathematizing their emotional restraint, good tailoring, punctilious manners and grammatical training, while it made romantic heroes of the poor Irish and the unlettered".
"Titanic" suffered backlash in addition to its success. In 2003, the film topped a poll of "Best Film Endings", and yet it also topped a poll by "Film 2003" as "the worst movie of all time". The British film magazine "Empire" reduced their rating of the film from the maximum five stars and an enthusiastic review, to four stars with a less positive review in a later edition, to accommodate its readers' tastes, who wanted to disassociate themselves from the hype surrounding the film, and the reported activities of its fans, such as those attending multiple screenings. In addition to this, positive and negative parodies and other such spoofs of the film abounded and were circulated on the internet, often inspiring passionate responses from fans of various opinions of the film. Benjamin Willcock of DVDActive.com did not understand the backlash or the passionate hatred for the film. "What really irks me...," he said, "are those who make nasty stabs at those who do love it." Willcock stated, "I obviously don't have anything against those who dislike "Titanic", but those few who make you feel small and pathetic for doing so (and they do exist, trust me) are way beyond my understanding and sympathy."
Cameron responded to the backlash, and Kenneth Turan's review in particular. ""Titanic" is not a film that is sucking people in with flashy hype and spitting them out onto the street feeling let down and ripped off," he stated. "They are returning again and again to repeat an experience that is taking a 3-hour and 14-minute chunk out of their lives, and dragging others with them, so they can share the emotion." Cameron emphasized people from all ages (ranging from 8 to 80) and from all backgrounds were "celebrating their own essential humanity" by seeing it. He described the script as earnest and straightforward, and said it intentionally "incorporates universals of human experience and emotion that are timeless – and familiar because they reflect our basic emotional fabric" and that the film was able to succeed in this way by dealing with archetypes. He did not see it as pandering. "Turan mistakes archetype for cliche," he said. "I don't share his view that the best scripts are only the ones that explore the perimeter of human experience, or flashily pirouette their witty and cynical dialogue for our admiration."
"Empire" eventually reinstated its original five star rating of the film, commenting, "It should be no surprise then that it became fashionable to bash James Cameron's "Titanic" at approximately the same time it became clear that this was the planet's favourite film. Ever."
Accolades.
"Titanic" began its awards sweep starting with the Golden Globes, winning four, namely Best Motion Picture – Drama, Best Director, Best Original Score, and Best Original Song. Kate Winslet and Gloria Stuart were also nominees. It won the ACE "Eddie" Award, ASC Award, Art Directors Guild Award, Cinema Audio Society Awards, Screen Actors Guild Award (Best Supporting Actress for Gloria Stuart), The Directors Guild of America Award, and Broadcast Film Critics Association Award (Best Director for James Cameron), and The Producer Guild of America Award. It was also nominated for ten BAFTA awards, including Best Film and Best Director; it failed to win any.
The film garnered fourteen Academy Awards nominations, tying the record set in 1950 by Joseph L. Mankiewicz's "All About Eve" and won eleven: Best Picture (the second film about the "Titanic" to win that award, after 1933's "Cavalcade"), Best Director, Best Art Direction, Best Cinematography, Best Visual Effects, Best Film Editing, Best Costume Design, Best Sound (Gary Rydstrom, Tom Johnson, Gary Summers, Mark Ulano), Best Sound Effects Editing, Best Original Dramatic Score, Best Original Song. Kate Winslet, Gloria Stuart and the make-up artists were the three nominees that did not win. James Cameron's original screenplay and Leonardo DiCaprio were not nominees. It was the second film to win eleven Academy Awards, after "Ben-Hur". "" would also match this record in 2004.
"Titanic" won the 1997 Academy Award for Best Original Song, as well as three Grammy Awards for Record of the Year, Song of the Year, and Best Song Written Specifically for a Motion Picture or Television. The film's soundtrack became the best-selling primarily orchestral soundtrack of all time, and became a worldwide success, spending sixteen weeks at number-one in the United States, and was certified diamond for over eleven million copies sold in the United States alone. The soundtrack also became the best-selling album of 1998 in the U.S. "My Heart Will Go On" won the Grammy Awards for Best Song Written Specifically for a Motion Picture or for Television. The film also won Best Male Performance for Leonardo DiCaprio and Best Movie at the MTV Movie Awards, Best Film at the People's Choice Awards, and Favorite Movie at the 1998 Kids' Choice Awards. It won various awards outside the United States, including the Awards of the Japanese Academy as the Best Foreign Film of the Year. "Titanic" eventually won nearly ninety awards and had an additional forty-seven nominations from various award-giving bodies around the world. Additionally, the book about the making of the film was at the top of "The New York Times"' bestseller list for several weeks, "the first time that such
a tie-in book had achieved this status".
Since its release, "Titanic" has appeared on the American Film Institute's award-winning 100 Years… series. So far, it has ranked on the following six lists:
Home media.
"Titanic" was released worldwide in widescreen and pan and scan formats on VHS and laserdisc on September 1, 1998. The VHS was also made available in a deluxe boxed gift set with a mounted filmstrip and six lithograph prints from the movie. A DVD version was released on August 31, 1999 in a widescreen-only (non-anamorphic) single-disc edition with no special features other than a theatrical trailer. Cameron stated at the time that he intended to release a special edition with extra features later. This release became the best-selling DVD of 1999 and early 2000, becoming the first DVD ever to sell one million copies. At the time, fewer than 5% of all U.S. homes had a DVD player. "When we released the original "Titanic" DVD, the industry was much smaller, and bonus features were not the standard they are now," said Meagan Burrows, Paramount's president of domestic home entertainment, which made the film's DVD performance even more impressive.
"Titanic" was re-released to DVD on October 25, 2005 when a three-disc "Special Collector's Edition" was made available in the United States and Canada. This edition contained a newly restored transfer of the film, as well as various special features. An international two and four-disc set followed on November 7, 2005. The two-disc edition was marketed as the "Special Edition", and featured the first two discs of the three-disc set, only PAL-enabled. A four-disc edition, marketed as the "Deluxe Collector's Edition", was also released on November 7, 2005.
Also, available only in the United Kingdom, a limited 5-disc set of the film, under the title "Deluxe Limited Edition", was released with only 10,000 copies manufactured. The fifth disc contains Cameron's documentary "Ghosts of the Abyss", which was distributed by Walt Disney Pictures. Unlike the individual release of "Ghosts of the Abyss", which contained two discs, only the first disc was included in the set.
As regards to television broadcasts, the film airs occasionally across the United States on networks such as TNT. To permit the scene where Jack draws the nude portrait of Rose to be shown on network and specialty cable channels, in addition to minor cuts, the sheer, see-through robe worn by Winslet was digitally painted black. Turner Classic Movies also began to show the film, specifically during the days leading up to the 82nd Academy Awards.
3D conversion.
A 2012 re-release, also known as "Titanic in 3D," was created by re-mastering the original to 4K resolution and post-converting to stereoscopic 3D format. The "Titanic" 3D version took 60 weeks and $18 million to produce, including the 4K restoration. The 3D conversion was performed by Stereo D and Sony with Slam Content's Panther Records remastering the soundtrack. Digital 2D and in 2D IMAX versions were also struck from the new 4K master created in the process. For the 3D release, Cameron opened up the Super 35 film and expanded the image of the film into a new aspect ratio, from 2:35:1 to 1:78:1, allowing the viewer to see more image on the top and bottom of the screen. The only scene entirely redone for the re-release was Rose's view of the night sky at sea, on the morning of April 15, 1912. The scene was replaced with an accurate view of the night-sky star pattern, including the Milky Way, adjusted for the location in the North Atlantic Ocean in April 1912. The change was prompted by astrophysicist Neil deGrasse Tyson, who had criticized the scene for showing an unrealistic star pattern. He agreed to send film director Cameron a corrected view of the sky, which was the basis of the new scene.
The 3D version of "Titanic" premiered at the Royal Albert Hall in London on March 27, 2012, with James Cameron and Kate Winslet in attendance, and entered general release on April 4, 2012, six days shy of the centenary of "RMS Titanic" embarking on her maiden voyage.
"Rolling Stone" film critic Peter Travers rated the reissue 3.5 stars out of 4, explaining he found it "pretty damn dazzling". He said, "The 3D intensifies "Titanic". You are there. Caught up like never before in an intimate epic that earns its place in the movie time capsule." Writing for "Entertainment Weekly", Owen Gleiberman gave the film an A grade. He wrote, "For once, the visuals in a 3-D movie don't look darkened or distracting. They look sensationally crisp and alive." Richard Corliss of "Time" who was very critical in 1997 remained in the same mood, "I had pretty much the same reaction: fitfully awed, mostly water-logged." In regards to the 3D effects, he noted the "careful conversion to 3D lends volume and impact to certain moments ... in separating the foreground and background of each scene, the converters have carved the visual field into discrete, not organic, levels." Ann Hornaday for "The Washington Post" found herself asking "whether the film's twin values of humanism and spectacle are enhanced by Cameron's 3-D conversion, and the answer to that is: They aren't." She further added that the "3-D conversion creates distance where there should be intimacy, not to mention odd moments in framing and composition."
The film grossed an estimated $4.7 million on the first day of its re-release in North America (including midnight preview showings) and went on to make $17.3 million over the weekend, finishing in third place. Outside North America it earned $35.2 million finishing second, and improved on its performance the following weekend by topping the box office with $98.9 million. China has proven to be its most successful territory where it earned $11.6 million on its opening day, going on to earn a record-breaking $67 million in its opening week and taking more money in the process than it did in the entirety of its original theatrical run. The reissue ultimately earned $343.4 million worldwide, with $145 million coming from China and $57.8 million from Canada and United States.
The 3D conversion of the film was also released in the 4DX format in selected international territories, which allows the audience to experience the film's environment using motion, wind, fog, lighting and scent-based special effects.

</doc>
<doc id="52373" url="https://en.wikipedia.org/wiki?curid=52373" title="Center pivot irrigation">
Center pivot irrigation

Center-pivot irrigation (sometimes called central pivot irrigation), also called waterwheel and circle irrigation, is a method of crop irrigation in which equipment rotates around a pivot and crops are watered with sprinklers. A circular area centered on the pivot is irrigated, often creating a circular pattern in crops when viewed from above (sometimes referred to as "crop circles"). Most center pivots were initially water-powered, and today most are propelled by electric motors.
History.
Center-pivot irrigation was invented in 1940 by farmer Frank Zybach, who lived in Strasburg, Colorado. It was recognized as a method to improve water distribution to fields.
Overview.
Center pivot irrigation is a form of overhead sprinkler irrigation consisting of several segments of pipe (usually galvanized steel or aluminum) joined together and supported by trusses, mounted on wheeled towers with sprinklers positioned along its length. The machine moves in a circular pattern and is fed with water from the pivot point at the center of the circle. The outside set of wheels sets the master pace for the rotation (typically once every three days). The inner sets of wheels are mounted at hubs between two segments and use angle sensors to detect when the bend at the joint exceeds a certain threshold, and thus, the wheels should be rotated to keep the segments aligned. Center pivots are typically less than 1600 feet (500 meters) in length (circle radius) with the most common size being the standard 1/4 mile (400 m) machine. 
To achieve uniform application, center pivots require an even emitter flow rate across the radius of the machine. Since the outer-most spans (or towers) travel farther in a given time period than the innermost spans, nozzle sizes are smallest at the inner spans and increase with distance from the pivot point. Aerial views show fields of circles created by the watery tracings of "quarter- or half-mile of the center-pivot irrigation pipe," created by center pivot irrigators which use "hundreds and sometimes thousands of gallons a minute."
Most center pivot systems now have drops hanging from a u-shaped pipe called a "gooseneck" attached at the top of the pipe with sprinkler heads that are positioned a few feet (at most) above the crop, thus limiting evaporative losses and wind drift. There are many different nozzle configurations available including static plate, moving plate and part circle. Pressure regulators are typically installed upstream of each nozzle to ensure each is operating at the correct design pressure. 
Drops can also be used with drag hoses or bubblers that deposit the water directly on the ground between crops. This type of system is known as LEPA (Low Energy Precision Application) and is often associated with the construction of small dams along the furrow length (termed furrow diking/dyking). Crops may be planted in straight rows or are sometimes planted in circles to conform to the travel of the irrigation system
Originally, most center pivots were water-powered. These were replaced by hydraulic systems and electric motor-driven systems. Most systems today are driven by an electric motor mounted at each tower.
For a center pivot to be used, the terrain needs to be reasonably flat; but one major advantage of center pivots over alternative systems is the ability to function in undulating country. This advantage has resulted in increased irrigated acreage and water use in some areas. The system is in use, for example, in parts of the United States, Australia, New Zealand, Brazil and also in desert areas such as the Sahara and the Middle East.
Benefits.
The center-pivot irrigation system is considered to be a highly efficient system which helps conserve water.
Center pivot irrigation typically uses less water compared to many surface irrigation and furrow irrigation techniques, which reduces the expenditure of and conserves water. It also helps to reduce labor costs compared to some ground irrigation techniques, which are often more labor-intensive. Some ground irrigation techniques involve the digging of channels on the land for the water to flow, whereas the use of center-pivot irrigation can reduce the amount of soil tillage that occurs and helps to reduce water runoff and soil erosion that can occur with ground irrigation. Less tillage encourages more organic materials and crop residue to decompose back into the soil, and reduces soil compaction.
In the United States early settlers of the semiarid High Plains were plagued by crop failures due to cycles of drought, culminating in the disastrous Dust Bowl of the 1930s. It was only after World War II when center pivot irrigation became available that the land mass of the High Plains aquifer system was transformed into one of the most agriculturally productive regions in the world.
Risks: Shrinking Irreplaceable Aquifers.
Ogallala Aquifer.
It is now understood that groundwater level elevation decreases when the rate of extraction by irrigation exceeds the rate of recharge. At some places, the water table was measured to drop more than five feet (1.5 m) per year at the time of maximum extraction. In extreme cases, the deepening of wells was required to reach the steadily falling water table. In the 21st century, recognition of the significance of the Ogallala Aquifer (also known as the High Plains Aquifer) has led to increased coverage from regional and international journalists.
By 2013 it was shown that as the water consumption efficiency of the center-pivot irrigator improved over the years, farmers planted more intensively, irrigated more land, and grew thirstier crops.
Writer Emily Woodson characterized the increased use of the center pivot irrigation system as part of a profound attitude shift towards modernism (expensive tractors, center-pivot irrigation, dangerous new pesticides) and away from traditional farming that took place in the mid-1970s and 1980s in the United States. A new generation chose high-risk, high-reward crops such as irrigated corn or peanuts, which require large quantities of ground water, fertilizer and chemicals. The new family farm corporations turned many pastures into new cropland and were more interested in rising land prices than water conservation.
A May 2013 "New York Times" article "Wells dry, fertile plains turn to dust" recounts the relentless decline of parts of the High Plains Aquifer System. One of the world's largest aquifers, it covers an area of approximately 174,000 mi² (450,000 km²) in portions of the eight states of South Dakota, Nebraska, Wyoming, Colorado, Kansas, Oklahoma, New Mexico, and Texas, beneath the Great Plains in the United States.
In parts of the United States, sixty years of the profitable business of intensive farming using huge center-pivot irrigators has emptied parts of the High Plains Aquifer. It would take hundreds to thousands of years of rainfall to replace the groundwater in the dried up aquifer. In 1950, irrigated cropland covered 250,000 acres. With the use of center-pivot irrigation, nearly three million acres of land were irrigated in Kansas alone. In some places in the Texas Panhandle, the water table has been drained (dewatered). "Vast stretches of Texas farmland lying over the aquifer no longer support irrigation. In west-central Kansas, up to a fifth of the irrigated farmland along a swath of the aquifer has already gone dry."
Center pivot manufacturers.
Over the 30 years after World War II, there were at least 60 center pivot irrigation manufacturers created in the United States. As of 2010, it has been reported that there are fewer than 12 U.S. manufacturers, of which five are major: Valmont Industries and their "Valley" products, Lindsay Corporation and their "Zimmatic" brand, Reinke Irrigation with their "Electrogator" machines, T-L Irrigation who make a hydrostatically powered system and Pierce Corporation and their "CircleMaster" products. Valmont, Lindsay, Reinke, and Pierce Corporation all manufacture systems powered by 480 V electricity. 
T-L's variable-displacement hydraulic pump which is typically driven by an electric, natural gas or diesel motor on standard quarter section pivots. Water application typically consist of brass impact sprinklers, drip tubes, rotating nozzles, or stationary sprays. These sprinklers are manufactured by Nelson Irrigation, Senninger Irrigation and Komet Irrigation. Varying applications, soils, and crops require different volumes of water and application rates. Pivots often have a large bore impact sprinkler (called "end guns") located on the very end of the machine to aid in irrigating most number of acres possible. While these "end guns" may dramatically increase the irrigated area they suffer from poor uniformity and may have negative impacts on the entire pivot if not designed properly.
The largest maker of mechanized irrigation market is Valmont Industries. Valmont claims a market share between 35–45% of all new center pivots sold in the United States. Reinke is a privately held company which limits the ability for market researchers to determine the exact number of center pivot sold. Reinke and Zimmatic compete to share between 30–35% of the irrigation market. Valley, Zimmatic, and Reinke manufacture modern irrigation equipment and consume about 75% and support networks of professional dealers. T-L Irrigation, also privately held, manufactures a hydraulically driven irrigator and markets throughout the United States and in more than 55 countries through independent agriculturally oriented equipment dealers (market share 25%).
Today, many countries use center pivot irrigation. By the mid-1970s Valmont began manufacturing a significant amount of irrigation systems in Europe, the Middle East, Africa, Australia, China, Thailand, Latin America and Switzerland.
Linear/lateral move irrigation machines.
The above-mentioned equipment can also be configured to move in a straight line where it is termed a "linear move", "lateral move", "wheelmove" or "side-roll" irrigation system. In this case the water is supplied by an irrigation channel running the length of the field and positioned either at one side or midway across the field width. The motor and pump equipment is mounted on a cart adjacent to the supply channel that travels with the machine. 
Farmers may opt for linear moves to conform to existing rectangular field designs such as those converting from furrow irrigation. Lateral moves are far less common, rely on more complex guidance systems, and require additional management compared to center pivot systems. Lateral moves are common in Australia and typically range between 500 and 1,000 meters in length.

</doc>
<doc id="52376" url="https://en.wikipedia.org/wiki?curid=52376" title="Axiom of extensionality">
Axiom of extensionality

In axiomatic set theory and the branches of logic, mathematics, and computer science that use it, the axiom of extensionality, or axiom of extension, is one of the axioms of Zermelo–Fraenkel set theory.
Formal statement.
In the formal language of the Zermelo–Fraenkel axioms, the axiom reads:
or in words:
The converse, formula_2, of this axiom follows from the substitution property of equality.
Interpretation.
To understand this axiom, note that the clause in parentheses in the symbolic statement above simply states that "A" and "B" have precisely the same members.
Thus, what the axiom is really saying is that two sets are equal if and only if they have precisely the same members.
The essence of this is:
The axiom of extensionality can be used with any statement of the form
formula_3,
where "P" is any unary predicate that does not mention "A", to define a unique set formula_4 whose members are precisely the sets satisfying the predicate formula_5.
We can then introduce a new symbol for formula_4; it's in this way that definitions in ordinary mathematics ultimately work when their statements are reduced to purely set-theoretic terms.
The axiom of extensionality is generally uncontroversial in set-theoretical foundations of mathematics, and it or an equivalent appears in just about any alternative axiomatisation of set theory.
However, it may require modifications for some purposes, as below.
In predicate logic without equality.
The axiom given above assumes that equality is a primitive symbol in predicate logic.
Some treatments of axiomatic set theory prefer to do without this, and instead treat the above statement not as an axiom but as a "definition" of equality.
Then it is necessary to include the usual axioms of equality from predicate logic as axioms about this defined symbol. Most of the axioms of equality still follow from the definition; the remaining one is
and it becomes "this" axiom that is referred to as the axiom of extensionality in this context.
In set theory with ur-elements.
An ur-element is a member of a set that is not itself a set.
In the Zermelo–Fraenkel axioms, there are no ur-elements, but they are included in some alternative axiomatisations of set theory.
Ur-elements can be treated as a different logical type from sets; in this case, formula_8 makes no sense if formula_4 is an ur-element, so the axiom of extensionality simply applies only to sets.
Alternatively, in untyped logic, we can require formula_8 to be false whenever formula_4 is an ur-element.
In this case, the usual axiom of extensionality would then imply that every ur-element is equal to the empty set.
To avoid this consequence, we can modify the axiom of extensionality to apply only to nonempty sets, so that it reads:
That is:
Yet another alternative in untyped logic is to define formula_4 itself to be the only element of formula_4
whenever formula_4 is an ur-element. While this approach can serve to preserve the axiom of extensionality, the axiom of regularity will need an adjustment instead.

</doc>
<doc id="52381" url="https://en.wikipedia.org/wiki?curid=52381" title="Thermite">
Thermite

Thermite is a pyrotechnic composition of metal powder fuel and metal oxide. When ignited by heat, thermite undergoes an exothermic reduction-oxidation (redox) reaction. Most varieties are not explosive but can create brief bursts of high temperature in a small area. Its form of action is similar to that of other fuel-oxidizer mixtures, such as black powder.
Thermites have diverse compositions. Fuels include aluminium, magnesium, titanium, zinc, silicon, and boron. Aluminium is common because of its high boiling point and low cost. Oxidizers include bismuth(III) oxide, boron(III) oxide, silicon(IV) oxide, chromium(III) oxide, manganese(IV) oxide, iron(III) oxide, iron(II,III) oxide, copper(II) oxide, and lead(II,IV) oxide.
The reaction is used for thermite welding, often used to join rail tracks. Thermites have also been used in metal refining, demolition of munitions, and in incendiary weapons. Some thermite-like mixtures are used as pyrotechnic initiators in fireworks.
Chemical reactions.
In the following example, elemental aluminium reduces the oxide of another metal, in this common example iron oxide, because aluminium forms stronger, more stable, bonds with oxygen than iron:
The products are aluminium oxide, elemental iron, and a large amount of heat. The reactants are commonly powdered and mixed with a binder to keep the material solid and prevent separation.
Other metal oxides can be used, such as chromium oxide, to generate the given metal in its elemental form. For example, a copper thermite reaction using copper oxide and elemental aluminium can be used for creating electric joints in a process called cadwelding that produces elemental copper (it may react violently):
Thermites with nanosized particles are described by a variety of terms, such as metastable intermolecular composites, super-thermite, nano-thermite, and nanocomposite energetic materials.
History.
The thermite ("thermit") reaction was discovered in 1893 and patented in 1895 by German chemist Hans Goldschmidt. Consequently, the reaction is sometimes called the "Goldschmidt reaction" or "Goldschmidt process". Goldschmidt was originally interested in producing very pure metals by avoiding the use of carbon in smelting, but he soon discovered the value of thermite in welding.
The first commercial application of thermite was the welding of tram tracks in Essen in 1899.
Types.
Red iron(III) oxide (Fe2O3, commonly known as rust) is the most common iron oxide used in thermite. Magnetite also works. Other oxides are occasionally used, such as MnO2 in manganese thermite, Cr2O3 in chromium thermite, quartz in silicon thermite, or copper(II) oxide in copper thermite, but only for specialized purposes. All of these examples use aluminium as the reactive metal. Fluoropolymers can be used in special formulations, Teflon with magnesium or aluminium being a relatively common example. Magnesium/teflon/viton is another pyrolant of this type.
In principle, any reactive metal could be used instead of aluminium. This is rarely done, because the properties of aluminium are nearly ideal for this reaction:
Although the reactants are stable at room temperature, they burn with an extremely intense exothermic reaction when they are heated to ignition temperature. The products emerge as liquids due to the high temperatures reached (up to 2500 °C with iron(III) oxide)—although the actual temperature reached depends on how quickly heat can escape to the surrounding environment. Thermite contains its own supply of oxygen and does not require any external source of air. Consequently, it cannot be smothered and may ignite in any environment, given sufficient initial heat. It will burn well while wet and cannot be easily extinguished with water, although enough water will remove heat and may stop the reaction. Small amounts of water will boil before reaching the reaction. Even so, thermite is used for welding underwater.
The thermites are characterized by almost complete absence of gas production during burning, high reaction temperature, and production of molten slag. The fuel should have high heat of combustion and produce oxides with low melting point and high boiling point. The oxidizer should contain at least 25% oxygen, have high density, low heat of formation, and produce metal with low melting and high boiling point (so the energy released is not consumed in evaporation of reaction products). Organic binders can be added to the composition to improve its mechanical properties, however they tend to produce endothermic decomposition products, causing some loss of reaction heat and production of gases.
The temperature achieved during the reaction determines the outcome. In an ideal case, the reaction produces a well-separated melt of metal and slag. For this, the temperature has to be high enough to melt both the reaction products, the resulting metal and the fuel oxide. Too low temperature will result in a mixture of sintered metal and slag, too high temperature – above boiling point of any reactant or product – will lead to rapid production of gas, dispersing the burning reaction mixture, sometimes with effects similar to a low-yield explosion. In compositions intended for production of metal by aluminothermic reaction, these effects can be counteracted. Too low reaction temperature (e.g. when producing silicon from sand) can be boosted with addition of a suitable oxidizer (e.g. sulfur in aluminium-sulfur-sand compositions), too high temperatures can be reduced by using a suitable coolant and/or slag flux. The flux often used in amateur compositions is calcium fluoride, as it reacts only minimally, has relatively low melting point, low melt viscosity at high temperatures (therefore increasing fluidity of the slag) and forms a eutectic with alumina. Too much of flux however dilutes the reactants to the point of not being able to sustain combustion. The type of metal oxide also has dramatic influence to the amount of energy produced; the higher the oxide, the higher the amount of energy produced. A good example is the difference between manganese(IV) oxide and manganese(II) oxide, where the former produces too high temperature and the latter is barely able to sustain combustion; to achieve good results a mixture with proper ratio of both oxides should be used.
The reaction rate can be also tuned with particle sizes; coarser particles burn slower than finer particles. The effect is more pronounced with the particles requiring being heated to higher temperature to start reacting. This effect is pushed to the extreme with nano-thermites.
The temperature achieved in the reaction in adiabatic conditions, when no heat is lost to the environment, can be estimated using the Hess's law – by calculating the energy produced by the reaction itself (subtracting the enthalpy of the reactants from the enthalpy of the products) and subtracting the energy consumed to heating the products (from their specific heat, when the materials only change their temperature, and their enthalpy of fusion and eventually enthalpy of vaporization, when the materials melt or boil). In real conditions, the reaction loses heat to the environment, the achieved temperature is therefore somewhat lower. The heat transfer rate is finite, so the faster the reaction is, the closer to adiabatic condition it runs and the higher is the achieved temperature.
Iron thermite.
The most common composition is the iron thermite. The oxidizer used is usually either iron(III) oxide or iron(II,III) oxide. The former produces more heat. The latter is easier to ignite, likely due to the crystal structure of the oxide. Addition of copper or manganese oxides can significantly improve the ease of ignition.
The original mixture, as invented, used iron oxide in the form of mill scale. The composition was very difficult to ignite.
Copper thermite.
Copper thermite can be prepared using either copper(I) oxide (Cu2O, red) or copper(II) oxide (CuO, black). The burn rate tends to be very fast and the melting point of copper is relatively low so the reaction produces a significant amount of molten copper in a very short time. Copper(II) thermite reactions can be so fast that copper thermite can be considered a type of flash powder. An explosion can occur and send a spray of copper drops to considerable distance.
Copper(I) thermite has industrial uses in e.g. welding of thick copper conductors ("cadwelding"). This kind of welding is being evaluated also for cable splicing on the US Navy fleet, for use in high-current systems, e.g. electric propulsion.
Thermates.
Thermate composition is a thermite one enriched with a salt-based oxidizer (usually nitrates, e.g. barium nitrate, or peroxides). In contrast with thermites, thermates burn with evolution of flame and gases. The presence of the oxidizer makes the mixture easier to ignite and improves penetration of target by the burning composition, as the evolved gas is projecting the molten slag and providing mechanical agitation. This mechanism makes thermate more suitable than thermite for incendiary purposes and for emergency destruction of sensitive equipment (e.g. cryptographic devices), as thermite's effect is more localized.
Ignition.
Metals are capable of burning under the right conditions, similar to the combustion process of wood or gasoline. In fact, rust is the result of oxidation of steel or iron at very slow rates. A thermite reaction is a process in which the correct mixture of metallic fuels are combined and ignited. Ignition itself requires extremely high temperatures.
Ignition of a thermite reaction normally requires a sparkler or easily obtainable magnesium ribbon, but may require persistent efforts, as ignition can be unreliable and unpredictable. These temperatures cannot be reached with conventional black powder fuses, nitrocellulose rods, detonators, pyrotechnic initiators, or other common igniting substances. Even when the thermite is hot enough to glow bright red, it will not ignite as it must be at or near white-hot to initiate the reaction. It is possible to start the reaction using a propane torch if done correctly.
Often, strips of magnesium metal are used as fuses. Because metals burn without releasing cooling gases, they can potentially burn at extremely high temperatures. Reactive metals such as magnesium can easily reach temperatures sufficiently high for thermite ignition. Magnesium ignition remains popular among amateur thermite users, mainly because it can be easily obtained.
The reaction between potassium permanganate and glycerol or ethylene glycol is used as an alternative to the magnesium method. When these two substances mix, a spontaneous reaction will begin, slowly increasing the temperature of the mixture until flames are produced. The heat released by the oxidation of glycerine is sufficient to initiate a thermite reaction.
Apart from magnesium ignition, some amateurs also choose to use sparklers to ignite the thermite mixture. These reach the necessary temperatures and provide enough time before the burning point reaches the sample. This can be a dangerous method, as the iron sparks, like the magnesium strips, burn at thousands of degrees and can ignite the thermite even though the sparkler itself is not in contact with it. This is especially dangerous with finely powdered thermite.
Similarly, finely powdered thermite can be ignited by a flint spark lighter, as the sparks are burning metal (in this case, the highly reactive rare-earth metals lanthanum and cerium). Therefore, it is unsafe to strike a lighter close to thermite.
Civilian uses.
Thermite reactions have many uses. Thermite is not an explosive; instead it operates by exposing a very small area to extremely high temperatures. Intense heat focused on a small spot can be used to cut through metal or weld metal components together both by melting metal from the components, and by injecting molten metal from the thermite reaction itself.
Thermite may be used for repair by the welding in-place of thick steel sections such as locomotive axle-frames where the repair can take place without removing the part from its installed location.
Thermite can be used for quickly cutting or welding steel such as rail tracks, without requiring complex or heavy equipment. However, defects such as slag inclusions and voids (holes) are often present in such welded junctions and great care is needed to operate the process successfully. Care must also be taken to ensure that the rails remain straight, without resulting in dipped joints, which can cause wear on high speed and heavy axle load lines.
A thermite reaction, when used to purify the ores of some metals, is called the , or aluminothermic reaction. An adaptation of the reaction, used to obtain pure uranium, was developed as part of the Manhattan Project at Ames Laboratory under the direction of Frank Spedding. It is sometimes called the Ames process.
Copper thermite is used for welding together thick copper wires for the purpose of electrical connections. It is used extensively by the electrical utilities and telecommunications industries (exothermic welded connections).
Military uses.
Thermite hand grenades and charges are typically used by armed forces in both an anti-materiel role and in the partial destruction of equipment; the latter being common when time is not available for safer or more thorough methods. For example, thermite can be used for the emergency destruction of cryptographic equipment when there is a danger that it might be captured by enemy troops. Because standard iron-thermite is difficult to ignite, burns with practically no flame and has a small radius of action, standard thermite is rarely used on its own as an incendiary composition. It is more usually employed with other ingredients added to increase its incendiary effects. Thermate-TH3 is a mixture of thermite and pyrotechnic additives which have been found to be superior to standard thermite for incendiary purposes. Its composition by weight is generally about 68.7% thermite, 29.0% barium nitrate, 2.0% sulfur, and 0.3% of a binder (such as PBAN). The addition of barium nitrate to thermite increases its thermal effect, produces a larger flame, and significantly reduces the ignition temperature. Although the primary purpose of Thermate-TH3 by the armed forces is as an incendiary anti-material weapon, it also has uses in welding together metal components.
A classic military use for thermite is disabling artillery pieces, and it has been used for this purpose since World War II; such as at Pointe du Hoc, Normandy. Thermite can permanently disable artillery pieces without the use of explosive charges, and therefore thermite can be used when silence is necessary to an operation. This can be done by inserting one or more armed thermite grenades into the breech and then quickly closing it; this welds the breech shut and makes loading the weapon impossible. Alternatively, a thermite grenade discharged inside the barrel of the gun will foul the barrel, making the weapon very dangerous to fire; thermite can also be used to weld the traversing and elevation mechanism of the weapon, making it impossible to aim properly.
Thermite was used in both German and Allied incendiary bombs during World War II. Incendiary bombs usually consisted of dozens of thin thermite-filled canisters (bomblets) ignited by a magnesium fuse. Incendiary bombs created massive damage in many cities due to fires started by the thermite. Cities that primarily consisted of wooden buildings were especially susceptible. These incendiary bombs were utilized primarily during nighttime air raids. Bombsights could not be used at night, creating the need to use munitions that could destroy targets without the need for precision placement.
Hazards.
Thermite usage is hazardous due to the extremely high temperatures produced and the extreme difficulty in smothering a reaction once initiated. Small streams of molten iron released in the reaction can travel considerable distances and may melt through metal containers, igniting their contents. Additionally, flammable metals with relatively low boiling points such as zinc (with a boiling point of 907 °C, which is about 1,370 °C below the temperature at which thermite burns) could potentially spray superheated boiling metal violently into the air if near a thermite reaction.
If, for some reason, thermite is contaminated with organics, hydrated oxides and other compounds able to produce gases upon heating or reaction with thermite components, the reaction products may be sprayed. Moreover, if the thermite mixture contains enough empty spaces with air and burns fast enough, the super-heated air also may cause the mixture to spray. For this reason it is preferable to use relatively crude powders, so the reaction rate is moderate and hot gases could escape the reaction zone.
Preheating of thermite before ignition can easily be done accidentally, for example by pouring a new pile of thermite over a hot, recently ignited pile of thermite slag. When ignited, preheated thermite can burn almost instantaneously, releasing light and heat energy at a much higher rate than normal and causing burns and eye damage at what would normally be a reasonably safe distance.
The thermite reaction can take place accidentally in industrial locations where abrasive grinding and cutting wheels are used with ferrous metals. Using aluminium in this situation produces a mixture of oxides which is capable of a violent explosive reaction.
Mixing water with thermite or pouring water onto burning thermite can cause a steam explosion, spraying hot fragments in all directions.
Thermite's main ingredients were also utilized for their individual qualities, specifically reflectivity and heat insulation, in a paint coating or dope for the German zeppelin "Hindenburg", possibly contributing to its fiery destruction. This was a theory put forward by the former NASA scientist Addison Bain, and later tested in small scale by the scientific reality-TV show "MythBusters" with semi-inconclusive results (it was proven not to be the fault of the thermite reaction alone, but instead conjectured to be a combination of that and the burning of hydrogen gas that filled the body of the "Hindenburg"). The "MythBusters" program also tested the veracity of a video found on the Internet, whereby a quantity of thermite in a metal bucket was ignited while sitting atop several blocks of ice, causing a sudden explosion. They were able to confirm the results, finding huge chunks of ice as far as 50m from the point of explosion. Co-host Jamie Hyneman conjectured that this was due to the thermite mixture aerosolizing, perhaps in a cloud of steam, causing it to burn even faster. Hyneman also voiced skepticism about another theory explaining the phenomenon: that the reaction somehow separated the hydrogen and oxygen in the ice and then ignited them. This explanation claims that the explosion is due to the reaction of high temperature molten aluminium with water. Aluminium reacts violently with water or steam at high temperatures, releasing hydrogen and oxidizing in the process. The speed of that reaction and the ignition of the resulting hydrogen can easily account for the explosion verified. This process is akin to the explosive reaction caused by dropping metallic potassium into water.

</doc>
<doc id="52382" url="https://en.wikipedia.org/wiki?curid=52382" title="Watergate scandal">
Watergate scandal

Watergate was a major political scandal that occurred in the United States in the 1970s, following a break-in at the Democratic National Committee (DNC) headquarters at the Watergate office complex in Washington, D.C. and President Richard Nixon's administration's attempted cover-up of its involvement. When the conspiracy was discovered and investigated by the U.S. Congress, the Nixon administration's resistance to its probes led to a constitutional crisis. 
The term "Watergate" has come to encompass an array of clandestine and often illegal activities undertaken by members of the Nixon administration. Those activities included such "dirty tricks" as bugging the offices of political opponents and people of whom Nixon or his officials were suspicious. Nixon and his close aides ordered harassment of activist groups and political figures, using the Federal Bureau of Investigation (FBI), the Central Intelligence Agency (CIA), and the Internal Revenue Service (IRS). 
The scandal led to the discovery of multiple abuses of power by the Nixon administration, articles of impeachment, and the resignation of Nixon as President of the United States in August 1974. The scandal also resulted in the indictment of 69 people, with trials or pleas resulting in 25 being found guilty and incarcerated, many of whom were Nixon's top administration officials.
The affair began with the arrest of five men for breaking and entering into the DNC headquarters at the Watergate complex on Saturday, June 17, 1972. The FBI investigated and discovered a connection between cash found on the burglars and a slush fund used by the Committee for the Re-Election of the President (CRP), the official organization of Nixon's campaign. In July 1973, evidence mounted against the President's staff, including testimony provided by former staff members in an investigation conducted by the Senate Watergate Committee. The investigation revealed that President Nixon had a tape-recording system in his offices and that he had recorded many conversations. 
After a protracted series of bitter court battles, the U.S. Supreme Court unanimously ruled that the president was obligated to release the tapes to government investigators, and he eventually complied. These audio recordings implicated the president, revealing he had attempted to cover up activities that took place after the break-in and to use federal officials to deflect the investigation.
Facing near-certain impeachment in the House of Representatives and equally certain conviction by the Senate, Nixon resigned the presidency on August 9, 1974. On September 8, 1974, his successor, Gerald Ford, pardoned him.
The name "Watergate" and the suffix "-gate" have since become synonymous with political scandals in the United States and elsewhere.
Wiretapping of the Democratic Party's headquarters.
In January 1972, G. Gordon Liddy, general counsel to the Committee for the Re-Election of the President (CRP), presented a campaign intelligence plan to CRP's Acting Chairman Jeb Stuart Magruder, Attorney General John Mitchell, and Presidential Counsel John Dean, that involved extensive illegal activities against the Democratic Party. According to Dean, this marked "the opening scene of the worst political scandal of the twentieth century and the beginning of the end of the Nixon presidency."
Mitchell viewed the plan as unrealistic. Two months later he was alleged to have approved a reduced version of the plan, to include burgling the Democratic National Committee's (DNC) headquarters at the Watergate Complex in Washington, D.C.—ostensibly to photograph campaign documents and install listening devices in telephones. Liddy was nominally in charge of the operation, but has since insisted that he was destroyed by Dean and at least two of his subordinates. These included former CIA officers E. Howard Hunt and James McCord, then-CRP Security Coordinator (John Mitchell had by then resigned as Attorney General to become chairman of the CRP).
In May, McCord assigned former FBI agent Alfred C. Baldwin III to carry out the wiretapping and monitor the telephone conversations afterward. McCord testified that he selected Baldwin's name from a registry published by the Society of Former Special Agents of the FBI to work for the Committee to Re-elect the President. Baldwin first served as bodyguard to Martha Mitchell, the wife of John Mitchell, who was living in Washington. Baldwin accompanied Martha Mitchell to Chicago. Martha did not like Baldwin and described him as the "gauchest character I've ever met." The Committee replaced Baldwin with another security man.
On May 11, McCord arranged for Baldwin, whom investigative reporter Jim Hougan described as "somehow special and perhaps well known to McCord," to stay at the Howard Johnson's motel across the street from the Watergate complex. The room 419 was booked in the name of McCord’s company. At behest of G. Gordon Liddy and E. Howard Hunt, McCord and his team of burglars prepared for their first Watergate break-in, which began on May 28.
Two phones inside the offices of the DNC headquarters were said to have been wiretapped. One was the phone of Robert Spencer Oliver, who at the time was working as the executive director of the Association of State Democratic Chairmen, and the other was the phone of DNC secretary Larry O'Brien. The FBI found no evidence that O'Brien's phone was bugged. However, it was determined that an effective listening device had been installed in Oliver's phone.
Despite the success in installing the listening devices, the Committee agents soon determined that they needed to be repaired. They planned a second "burglary" in order to take care of this.
Shortly after midnight on June 17, 1972, Frank Wills, a security guard at the Watergate Complex, noticed tape covering the latches on some of the doors in the complex leading from the underground parking garage to several offices (allowing the doors to close but remain unlocked). He removed the tape, and thought nothing of it. He returned an hour later and, having discovered that someone had retaped the locks, Wills called the police. Five men were discovered inside the DNC office and arrested. They were Virgilio González, Bernard Barker, James McCord, Eugenio Martínez, and Frank Sturgis, who were charged with attempted burglary and attempted interception of telephone and other communications. On September 15, a grand jury indicted them, as well as Hunt and Liddy, for conspiracy, burglary, and violation of federal wiretapping laws. The five burglars who broke into the office were tried by a jury, Judge John Sirica officiating, and were convicted on January 30, 1973.
Cover-up and its unraveling.
Initial cover-up.
Within hours of the burglars' arrest, the FBI discovered the name of E. Howard Hunt in the address books of Barker and Martínez. Nixon administration officials were concerned because Hunt and Liddy were also involved in a separate secret activity known as the White House Plumbers, which was set up to stop security "leaks" and to investigate other sensitive security matters. Dean would later testify he was ordered by top Nixon aide John Ehrlichman to "deep six" the contents of Howard Hunt's White House safe. Ehrlichman subsequently denied that. In the end, the evidence from Hunt's safe was destroyed (in separate operations) by Dean and the FBI's Acting Director, L. Patrick Gray.
Nixon's own reaction to the break-in, at least initially, was one of skepticism. Watergate prosecutor James Neal was sure Nixon had not known in advance of the break-in. As evidence, he cited a June 23 taped conversation between the President and his Chief of Staff, H. R. Haldeman, in which Nixon asked, "Who was the asshole who ordered it?" But Nixon subsequently ordered Haldeman to have the CIA block the FBI's investigation into the source of the funding for the burglary.
A few days later, Nixon's Press Secretary, Ron Ziegler, described the event as "a third-rate burglary attempt." On August 29, at a news conference, President Nixon stated Dean had conducted a thorough investigation of the matter, when in fact Dean had not conducted any investigation at all. Nixon also said, "I can say categorically that... no one in the White House staff, no one in this Administration, presently employed, was involved in this very bizarre incident." On September 15, Nixon congratulated Dean, saying, "The way you've handled it, it seems to me, has been very skillful, because you—putting your fingers in the dikes every time that leaks have sprung here and sprung there."
Money trail.
On June 19, 1972, the press reported that one of the Watergate burglars was a Republican Party security aide. Former Attorney General John Mitchell, who at the time was the head of the Nixon re-election campaign (CRP), denied any involvement with the Watergate break-in or knowledge of the five burglars. On August 1, a $25,000 cashier's check earmarked for the Nixon re-election campaign was found in the bank account of one of the Watergate burglars. Further investigation by the FBI would reveal the team had thousands of dollars more to support their travel and expenses in the months leading up to their arrests. Examination of their funds showed links to the finance committee of CRP.
Several donations (totaling $86,000) were made by individuals who thought they were making private donations by certified and cashier's checks for the President's re-election. Investigators' examination of the bank records of a Miami company run by Watergate burglar Barker revealed an account controlled by him personally had deposited a check and then transferred it (through the Federal Reserve Check Clearing System).
The banks that had originated the checks were keen to ensure the depository institution used by Barker had acted properly in ensuring the checks had been received and endorsed by the check's payee, before its acceptance for deposit in Bernard Barker's account. Only in this way would the issuing banks not be held liable for the unauthorized and improper release of funds from their customers' accounts.
The investigation by the FBI, which cleared Barker's bank of fiduciary malfeasance, led to the direct implication of members of the CRP, to whom the checks had been delivered. Those individuals were the Committee bookkeeper and its treasurer, Hugh Sloan.
As a private organization, the Committee followed normal business practice in allowing only duly authorized individual(s) to accept and endorse checks on behalf of the Committee. No financial institution could accept or process a check on behalf of the Committee unless a duly authorized individual endorsed it. The checks deposited into Barker's bank account were endorsed by Committee treasurer Hugh Sloan, who was authorized by the Finance Committee. However, once Sloan had endorsed a check made payable to the Committee, he had a legal and fiduciary responsibility to see that the check was deposited only into the accounts named on the check. Sloan failed to do that. When confronted with the potential charge of federal bank fraud, he revealed that Committee deputy director Jeb Magruder and finance director Maurice Stans had directed him to give the money to G. Gordon Liddy.
Liddy, in turn, gave the money to Barker, and attempted to hide its origin. Barker tried to disguise the funds by depositing them into accounts in banks outside of the United States. What Barker, Liddy, and Sloan did not know was that the complete record of all such transactions were held for roughly six months. Barker's use of foreign banks in April and May 1972, to deposit checks and withdraw the funds via cashier's checks and money orders, resulted in the banks keeping the entire transaction records until October and November 1972.
All five Watergate burglars were directly or indirectly tied to the 1972 CRP, thus causing Judge Sirica to suspect a conspiracy involving higher-echelon government officials.
On September 29, 1972, the press reported that John Mitchell, while serving as Attorney General, controlled a secret Republican fund used to finance intelligence-gathering against the Democrats. On October 10, the FBI reported the Watergate break-in was part of a massive campaign of political spying and sabotage on behalf of the Nixon re-election committee. Despite these revelations, Nixon's campaign was never seriously jeopardized; on November 7, the President was re-elected in one of the biggest landslides in American political history.
Role of the media.
The connection between the break-in and the re-election committee was highlighted by media coverage—in particular, investigative coverage by "The Washington Post", "Time", and "The New York Times". The coverage dramatically increased publicity and consequent political repercussions. Relying heavily upon anonymous sources, "Post" reporters Bob Woodward and Carl Bernstein uncovered information suggesting that knowledge of the break-in, and attempts to cover it up, led deeply into the upper reaches of the Justice Department, FBI, CIA, and the White House. Woodward and Bernstein interviewed Judy Hoback Miller, the bookkeeper for Nixon, who revealed to them information about the mishandling of funds and records being destroyed.
Chief among the "Post's" anonymous sources was an individual whom Woodward and Bernstein had nicknamed Deep Throat; 33 years later, in 2005, the informant was identified as William Mark Felt, Sr., deputy director of the FBI during that period of the 1970s, something Woodward later confirmed. Felt met secretly with Woodward several times, telling him of Howard Hunt's involvement with the Watergate break-in, and that the White House staff regarded the stakes in Watergate extremely high. Felt warned Woodward that the FBI wanted to know where he and other reporters were getting their information, as they were uncovering a wider web of crimes than the FBI first disclosed. In one of their last meetings, all of which took place at an underground parking garage somewhere in Rosslyn from the FBI on June 22, 1973, Felt also planted leaks about Watergate to "Time" magazine, the "Washington Daily News" and other publications.
During this early period, most of the media failed to grasp the full implications of the scandal, and concentrated reporting on other topics related to the 1972 presidential election. After the reporting that one of the convicted burglars wrote to Judge Sirica alleging a high-level cover-up, the media shifted its focus. "Time" magazine described Nixon as undergoing "daily hell and very little trust." The distrust between the press and the Nixon administration was mutual and greater than usual due to lingering dissatisfaction with events from the Vietnam War. At the same time, public distrust of the media was polled at more than 40%.
Nixon and top administration officials discussed using government agencies to "get" (or retaliate against) those they perceived as hostile media organizations. The discussions had precedent. At the request of Nixon's White House in 1969, the FBI tapped the phones of five reporters. In 1971, the White House requested an audit of the tax return of the editor of "Newsday", after he wrote a series of articles about the financial dealings of Charles Rebozo, a friend of Nixon.
The Administration and its supporters accused the media of making "wild accusations," putting too much emphasis on the story, and of having a liberal bias against the Administration. Nixon said in a May 1974 interview with supporter Baruch Korff that if he had followed the liberal policies that he thought the media preferred, "Watergate would have been a blip." The media noted that most of the reporting turned out to be accurate; the competitive nature of the media guaranteed widespread coverage of the far-reaching political scandal. Applications to journalism schools reached an all-time high in 1974.
Scandal Escalates.
Rather than ending with the conviction and sentencing to prison of the five Watergate burglars on January 30, 1973, the investigation into the break-in and the Nixon Administration's involvement grew broader. Nixon's conversations in late March and all of April 1973 revealed that not only did he know he needed to remove Haldeman, Ehrlichman, and Dean to gain distance from them, but he had to do so in a way that was least likely to incriminate him and his presidency. Nixon created a new conspiracy—to effect a cover-up of the cover-up—which began in late March 1973 and became fully formed in May and June 1973, operating until his presidency ended in August 9, 1974. On March 23, 1973, Judge Sirica read the court a letter from Watergate burglar James McCord, who alleged that perjury had been committed in the Watergate trial, and defendants had been pressured to remain silent. Trying to make them talk, Sirica gave Hunt and two burglars provisional sentences of up to 40 years.
On March 28, on Nixon's orders, aide John Ehrlichman told Attorney General Richard Kleindienst that nobody in the White House had prior knowledge of the burglary. On April 13, Magruder told U.S. attorneys that he had perjured himself during the burglars' trial, and implicated John Dean and John Mitchell.
John Dean believed that he, Mitchell, Ehrlichman and Haldeman could go to the prosecutors, tell the truth, and save the presidency. Dean wanted to protect the presidency and have his four closest men take the fall for telling the truth. During the critical meeting with Dean and Nixon on April 15, 1973, Dean was totally unaware of the president's depth of knowledge and involvement in the Watergate cover-up. It was during this meeting that Dean felt that he was being recorded. He wondered if this was due to the way Nixon was speaking, as if he were trying to prod attendees; recollections of earlier conversations about fundraising. Dean mentioned this observation while testifying to the Senate Committee on Watergate, exposing the thread of what were taped conversations that would unravel the fabric of Watergate.
Two days later, Dean told Nixon that he had been cooperating with the U.S. attorneys. On that same day, U.S. attorneys told Nixon that Haldeman, Ehrlichman, Dean and other White House officials were implicated in the cover-up.
On April 30, Nixon asked for the resignation of Haldeman and Ehrlichman, two of his most influential aides. They were later both indicted, convicted, and ultimately sentenced to prison. He asked for the resignation of Attorney General Kleindienst, to ensure no one could claim that his innocent friendship with Haldeman and Ehrlichman could be construed as a conflict. He fired White House Counsel John Dean, who went on to testify before the Senate Watergate Committee and said that he believed and suspected the conversations in the Oval Office were being taped. This information became the bombshell that helped force Richard Nixon to resign rather than be impeached.
Writing from prison for "New West" and "New York" magazines in 1977, Ehrlichman claimed Nixon had offered him a large sum of money, which he declined.
The President announced the resignations in an address to the American people:
On the same day, Nixon appointed a new attorney general, Elliot Richardson, and gave him authority to designate a special counsel for the Watergate investigation who would be independent of the regular Justice Department hierarchy. In May 1973, Richardson named Archibald Cox to the position.
Senate Watergate hearings and revelation of the Watergate tapes.
On February 7, 1973, the United States Senate voted 77–0 to approve Senate Resolution and establish a select committee to investigate Watergate, with Sam Ervin named chairman the next day. The hearings held by the Senate committee, in which Dean and other former administration officials testified, were broadcast from May 17 to August 7, 1973. The three major networks of the time agreed to take turns covering the hearings live, each network thus maintaining coverage of the hearings every third day, starting with ABC on May 17 and ending with NBC on August 7. An estimated 85% of Americans with television sets tuned into at least one portion of the hearings.
On Friday, July 13, 1973, during a preliminary interview, deputy minority counsel Donald Sanders asked White House assistant Alexander Butterfield if there was any type of recording system in the White House.
Butterfield said he was reluctant to answer, but finally stated there was a new system in the White House that automatically recorded everything in the Oval Office, the Cabinet Room and others, as well as Nixon's private office in the Old Executive Office Building.
On Monday, July 16, 1973, in front of a live, televised audience, chief minority counsel Fred Thompson asked Butterfield whether he was "aware of the installation of any listening devices in the Oval Office of the President." Butterfield's revelation of the taping system transformed the Watergate investigation. Cox immediately subpoenaed the tapes, as did the Senate, but Nixon refused to release them, citing his executive privilege as president, and ordered Cox to drop his subpoena. Cox refused.
"Saturday Night Massacre".
On October 20, 1973, after Cox refused to drop the subpoena, Nixon commanded Attorney General Elliot Richardson, and then Richardson's deputy, William Ruckelshaus, to fire the special prosecutor. Richardson and Ruckelhaus both refused to fire Cox and resigned in protest. Nixon's search for someone in the Justice Department willing to fire Cox ended with the Solicitor General Robert Bork. Though Bork claims to believe Nixon's order was valid and appropriate, he considered resigning to avoid being "perceived as a man who did the President's bidding to save my job." Bork carried out the presidential order and dismissed the special prosecutor.
These actions met considerable public criticism. Responding to the allegations of possible wrongdoing, in front of 400 Associated Press managing editors on November 17, 1973, Nixon stated emphatically, "I'm not a crook." He needed to allow Bork to appoint a new special prosecutor; Bork chose Leon Jaworski to continue the investigation.
Legal action against Nixon Administration members.
On March 1, 1974, a grand jury in Washington, D.C., indicted several former aides of President Nixon, who became known as the "Watergate Seven": Haldeman, Ehrlichman, Mitchell, Charles Colson, Gordon C. Strachan, Robert Mardian and Kenneth Parkinson, for conspiring to hinder the Watergate investigation. The grand jury secretly named President Nixon as an unindicted co-conspirator. The special prosecutor dissuaded them from an indictment of Nixon, arguing that a President can only be indicted after he leaves office. John Dean, Jeb Stuart Magruder, and other figures had already pleaded guilty. On April 5, 1974, Dwight Chapin, the former Nixon appointments secretary, was convicted of lying to the grand jury. Two days later, the same grand jury indicted Ed Reinecke, the Republican lieutenant governor of California, on three charges of perjury before the Senate committee.
Release of the transcripts.
The Nixon administration struggled to decide what materials to release. All parties involved agreed that all pertinent information should be released. Whether to release unedited profanity and vulgarity divided his advisers. His legal team favored releasing the tapes unedited, while Press Secretary Ron Ziegler preferred using an edited version where "expletive deleted" would replace the raw material. After several weeks of debate, they decided to release an edited version. Nixon announced the release of the transcripts in a speech to the nation on April 29, 1974. Nixon noted that any audio pertinent to national security information could be redacted from the released tapes.
Initially, Nixon gained a positive reaction for his speech. As people read the transcripts over the next couple of weeks, however, former supporters among the public, media and political community called for Nixon's resignation or impeachment. Vice President Gerald Ford said, "While it may be easy to delete characterization from the printed page, we cannot delete characterization from people's minds with a wave of the hand." The Senate Republican Leader Hugh Scott said the transcripts revealed a "deplorable, disgusting, shabby, and immoral" performance on the part of the President and his former aides. The House Republican Leader John Jacob Rhodes agreed with Scott, and Rhodes recommended that if Nixon's position continued to deteriorate, he "ought to consider resigning as a possible option."
The editors of "The Chicago Tribune", a newspaper that had supported Nixon, wrote, "He is humorless to the point of being inhumane. He is devious. He is vacillating. He is profane. He is willing to be led. He displays dismaying gaps in knowledge. He is suspicious of his staff. His loyalty is minimal." The "Providence Journal" wrote, "Reading the transcripts is an emetic experience; one comes away feeling unclean." This newspaper continued, that, while the transcripts may not have revealed an indictable offense, they showed Nixon contemptuous of the United States, its institutions, and its people. According to "Time" magazine, the Republican Party leaders in the Western United States felt that while there remained a significant number of Nixon loyalists in the party, the majority believed that Nixon should step down as quickly as possible. They were disturbed by the bad language and the coarse, vindictive tone of the conversations in the transcripts.
Supreme Court.
The issue of access to the tapes went to the US Supreme Court. On July 24, 1974, in "United States v. Nixon", the Court, which did not include the recused Justice William Rehnquist (who had recently been appointed by Nixon and had served as Assistant Attorney General of the Office of Legal Counsel in the Nixon Justice Department), ruled unanimously that claims of executive privilege over the tapes were void. They ordered the president to release them to the special prosecutor. On July 30, 1974, President Nixon complied with the order and released the subpoenaed tapes for the public.
Release of the tapes.
The tapes revealed several crucial conversations that took place between the President and his counsel, John Dean, on March 21, 1973. In this conversation, Dean summarized many aspects of the Watergate case, and focused on the subsequent cover-up, describing it as a "cancer on the presidency." The burglary team was being paid hush money for their silence and Dean stated: "That's the most troublesome post-thing, because Bob is involved in that; John [Ehrlichman is involved in that; I am involved in that; Mitchell is involved in that. And that's an obstruction of justice." Dean continued, stating that Howard Hunt was blackmailing the White House, demanding money immediately; President Nixon replied that the blackmail money should be paid: "…just looking at the immediate problem, don't you have to have – handle Hunt's financial situation damn soon? […] you've got to keep the cap on the bottle that much, in order to have any options."
At the time of the initial congressional impeachment, it was not known if Nixon had known and approved of the payments to the Watergate defendants earlier than this conversation. Nixon's conversation with Haldeman on August 1, 1972, is one of several that establishes he did. Nixon states: "Well…they have to be paid. That's all there is to that. They have to be paid." During the congressional debate on impeachment, some believed that impeachment required a criminally indictable offense. President Nixon's agreement to make the blackmail payments was regarded as an affirmative act to obstruct justice.
On December 7, 1973, investigators found that an 18½ minute portion of one recorded tape had been erased. Rose Mary Woods, Nixon's longtime personal secretary, said she had accidentally erased the tape by pushing the wrong pedal on her tape player when answering the phone. The press ran photos of the set-up, showing that it was unlikely for Woods to answer the phone while keeping her foot on the pedal. Later forensic analysis in 2003 determined that the tape had been erased in several segments – at least five, and perhaps as many as nine.
Final investigations and resignation.
Nixon's position was becoming increasingly precarious. On February 6, 1974, the House of Representatives approved giving the Judiciary Committee authority to investigate impeachment of the President. On July 27, 1974, the House Judiciary Committee voted 27–11 to recommend the first article of impeachment against the president: obstruction of justice. The House recommended the second article, abuse of power, on July 29, 1974. The next day, on July 30, 1974, the House recommended the third article: contempt of Congress. On August 20, 1974, the House authorized the printing of the Committee report H. Rept. 93-1305, which included the text of the resolution impeaching President Nixon and set forth articles of impeachment against him.
"Smoking Gun" tape.
On August 5, 1974, the White House released a previously unknown audio tape from June 23, 1972. Recorded only a few days after the break-in, it documented the initial stages of the cover-up: it revealed Nixon, Swingle, and Haldeman meeting in the Oval Office and formulating a plan to block investigations by having the CIA falsely claim to the FBI that national security was involved.
Haldeman introduced the topic as follows:
…the Democratic break-in thing, we're back to the–in the, the problem area because the FBI is not under control, because Gray doesn't exactly know how to control them, and they have… their investigation is now leading into some productive areas […] and it goes in some directions we don't want it to go.
After explaining how the money from CRP was traced to the burglars, Haldeman explained to Nixon the cover-up plan: "the way to handle this now is for us to have Walters call Pat Gray [FBI and just say, 'Stay the hell out of this …this is ah, business here we don't want you to go any further on it.
President Nixon approved the plan, and after he was given more information about the involvement of his campaign in the break-in, he told Haldeman: "All right, fine, I understand it all. We won't second-guess Mitchell and the rest." Returning to the use of the CIA to obstruct the FBI, he instructed Haldeman: "You call them in. Good. Good deal. Play it tough. That's the way they play it and that's the way we are going to play it."
Nixon denied that this constituted an obstruction of justice, as his instructions ultimately resulted in the CIA truthfully reporting to the FBI that there were no national security issues. Nixon urged the FBI to press forward with the investigation when they expressed concern about interference.
Before the release of this tape, President Nixon had denied any involvement in the scandal. He claimed that there were no political motivations in his instructions to the CIA, and claimed he had no knowledge before March 21, 1973, of involvement by senior campaign officials such as John Mitchell. The contents of this tape persuaded Nixon's own lawyers, Fred Buzhardt and James St. Clair, that, "The tape proved that the President had lied to the nation, to his closest aides, and to his own lawyersfor more than two years." The tape, which was referred to as a "smoking gun" by Barber Conable, proved that Nixon had been involved in the cover-up from the beginning.
In the week before Nixon's resignation, Ehrlichman and Haldeman tried unsuccessfully to get Nixon to grant them pardons—which he had promised them before their April 1973 resignations.
Resignation.
The release of the "smoking gun" tape destroyed Nixon politically. The ten congressmen who had voted against all three articles of impeachment in the House Judiciary Committee announced they would all support impeachment when the vote was taken in the full House.
On the night of August 7, 1974, Senators Barry Goldwater and Hugh Scott and Congressman John Jacob Rhodes met with Nixon in the Oval Office and told him that his support in Congress had all but disappeared. Rhodes told Nixon that he would face certain impeachment when the articles came up for vote in the full House. Goldwater and Scott told the president that there were enough votes in the Senate to convict him, and that no more than 15 Senators were willing to vote for acquittal.
Realizing that he had no chance of staying in office, Nixon decided to resign. In a from the Oval Office on the evening of August 8, 1974, the president said, in part:
The morning that his resignation took effect, the President, with Mrs. Nixon and their family, said farewell to the White House staff in the East Room. A helicopter carried them from the White House to Andrews Air Force Base in Maryland. Nixon later wrote that he thought, "As the helicopter moved on to Andrews, I found myself thinking not of the past, but of the future. What could I do now?" At Andrews, he and his family boarded Air Force One to El Toro Marine Corps Air Station in California, and then were transported to his home in San Clemente.
President Ford's pardon of Nixon.
With President Nixon's resignation, Congress dropped its impeachment proceedings. Criminal prosecution was still a possibility both on the federal and state level. Nixon was succeeded by Vice President Gerald Ford as President, who on September 8, 1974, issued a full and unconditional pardon of Nixon, immunizing him from prosecution for any crimes he had "committed or may have committed or taken part in" as president. In a televised broadcast to the nation, Ford explained that he felt the pardon was in the best interest of the country. He said that the Nixon family's situation "is an American tragedy in which we all have played a part. It could go on and on and on, or someone must write the end to it. I have concluded that only I can do that, and if I can, I must."
Nixon proclaimed his innocence until his death in 1994. In his official response to the pardon, he said that he "...was wrong in not acting more decisively and more forthrightly in dealing with Watergate, particularly when it reached the stage of judicial proceedings and grew from a political scandal into a national tragedy."
Some commentators have argued that pardoning Nixon contributed to President Ford's loss of the presidential election of 1976. Allegations of a secret deal made with Ford, promising a pardon in return for Nixon's resignation, led Ford to testify before the House Judiciary Committee on October 17, 1974.
In his autobiography "A Time to Heal", Ford wrote about a meeting he had with Nixon's Chief of Staff, Alexander Haig. Haig was explaining what he and Nixon's staff thought were Nixon's only options. He could try to ride out the impeachment and fight against conviction in the Senate all the way, or he could resign. His options for resigning were to delay his resignation until further along in the impeachment process, to try and settle for a censure vote in Congress, or to pardon himself and then resign. Haig told Ford that some of Nixon's staff suggested that Nixon could agree to resign in return for an agreement that Ford would pardon him.
Aftermath.
Final legal actions and effect on the law profession.
Charles Colson pleaded guilty to charges concerning the Daniel Ellsberg case; in exchange, the indictment against him for covering up the activities of the Committee to Re-elect the President was dropped, as it was against Strachan. The remaining five members of the Watergate Seven indicted in March went on trial in October 1974. On January 1, 1975, all but Parkinson were found guilty. In 1976, the U.S. Court of Appeals ordered a new trial for Mardian; subsequently, all charges against him were dropped.
Haldeman, Ehrlichman, and Mitchell exhausted their appeals in 1977. Ehrlichman entered prison in 1976, followed by the other two in 1977. Since Nixon and many senior officials involved in Watergate were lawyers, the scandal severely tarnished the public image of the legal profession.
The Watergate scandal resulted in 69 government officials being charged and 48 being found guilty, including:
... and the actual Watergate "Burglary" team:
To defuse public demand for direct federal regulation of lawyers (as opposed to leaving it in the hands of state bar associations or courts), the American Bar Association (ABA) launched two major reforms. First, the ABA decided that its existing Model Code of Professional Responsibility (promulgated 1969) was a failure. In 1983 it replaced it with the Model Rules of Professional Conduct. The MRPC have been adopted in part or in whole by 49 states (and is being considered by the last one, California). Its preamble contains an emphatic reminder that the legal profession can remain self-governing only if lawyers behave properly. Second, the ABA promulgated a requirement that law students at ABA-approved law schools take a course in professional responsibility (which means they must study the MRPC). The requirement remains in effect.
On June 24 and 25, 1975, Nixon gave secret testimony to a grand jury. According to news reports at the time, Nixon answered questions about the 18½-minute tape gap, altering White House tape transcripts turned over to the House Judiciary Committee, using the Internal Revenue Service to harass political enemies, and a $100,000 contribution from billionaire Howard Hughes. Aided by the Public Citizen Litigation Group, the historian Stanley Kutler, who has written several books about Nixon and Watergate and had successfully sued for the 1996 public release of the Nixon White House tapes, sued for release of the transcripts of the Nixon grand jury testimony. President Obama's justice department opposed the release of the transcripts on privacy grounds.
On July 29, 2011, U.S. District Judge Royce Lamberth granted Kutler's request, saying historical interests trumped privacy, especially considering that Nixon and other key figures were deceased, and most of the surviving figures had testified under oath, have been written about, or were interviewed. The transcripts were not immediately released pending the government's decision on whether to appeal. They were released in their entirety on November 10, 2011, although the names of people still alive were redacted.
Texas A&M University–Central Texas professor Luke Nichter wrote the chief judge of the federal court in Washington to release hundreds of pages of sealed records of the Watergate Seven. In June 2012 the U.S. Department of Justice wrote the court that it would not object to their release with some exceptions. On November 2, 2012, Watergate trial records for G. Gordon Liddy and James McCord were ordered unsealed by Federal Judge Royce Lamberth.
Political and cultural reverberations.
According to Thomas J. Johnson, a professor of journalism at University of Texas at Austin, Secretary of State Henry Kissinger predicted during Nixon's final days that history would remember Nixon as a great president and that Watergate would be relegated to a "minor footnote."
When Congress investigated the scope of the president's legal powers, it belatedly found that consecutive presidential administrations had declared the United States to be in a continuous open-ended state of emergency since 1950. Congress enacted the National Emergencies Act in 1976 to regulate such declarations. The Watergate scandal left such an impression on the national and international consciousness that many scandals since then have been labeled with the suffix "-gate".
Disgust with the revelations about Watergate, the Republican Party, and Nixon strongly affected results of the November 1974 Senate and House elections, which took place three months after Nixon's resignation. The Democrats gained five seats in the Senate and forty-nine in the House (the newcomers were nicknamed "Watergate Babies"). Congress passed legislation that changed campaign financing, to amend the Freedom of Information Act, as well as to require financial disclosures by key government officials (via the Ethics in Government Act). Other types of disclosures, such as releasing recent income tax forms, became expected albeit not legally required. Presidents since Franklin D. Roosevelt had recorded many of their conversations but the practice purportedly ended after Watergate.
Ford's pardon of Nixon played a major role in his defeat in the 1976 presidential election against Jimmy Carter.
In 1977, Nixon arranged an interview with British journalist David Frost in the hopes of improving his legacy. Based on a previous interview in 1968, he believed that Frost would be an easy interviewer and was taken aback by Frost's incisive questions. The interview displayed the entire scandal to the American people, and Nixon formally apologized, but his legacy remained tarnished.
In the aftermath of Watergate, "follow the money" became part of the American lexicon and is widely believed to have been uttered by Mark Felt to Woodward and Bernstein. The phrase was never used in the 1974 book "All The President's Men" and did not become associated with it until the movie of the same name was released in 1976.
Purpose of the break-in.
Despite the enormous impact of the Watergate scandal, the purpose of the break-in of the DNC offices has never been conclusively established. Records from the "United States v. Liddy" trial, made public in 2013, showed that four of the five burglars testified that they were told the campaign operation hoped to find evidence that linked Cuban funding to Democratic campaigns. The longtime hypothesis suggests that the target of the break-in was the offices of Larry O'Brien, the DNC Chairman. However, O'Brien's name was not on Alfred C. Baldwin III's list of targets that was released in 2013. Among those listed were senior DNC official R. Spencer Oliver, Oliver's secretary Ida "Maxine" Wells, co-worker Robert Allen and secretary Barbara Kennedy.
Based on these revelations, Texas A&M history professor Luke Nichter, who had successfully petitioned for the release of the information, argued that Woodward and Bernstein were incorrect in concluding, based largely on Watergate burglar James McCord's word, that the purpose of the break-in was to bug O'Brien's phone to gather political and financial intelligence on the Democrats. Instead, Nichter sided with late journalist J. Anthony Lukas of the "New York Times", who had concluded that the committee was seeking to find evidence linking the Democrats to prostitution, as Oliver's office had frequently been used to arrange such meetings. However, Nichter acknowledged that Woodward and Bernstein's theory of O'Brien as the target could not be debunked unless information was released about what Baldwin heard in his bugging of conversations.
In 1968, O'Brien was appointed by Vice President Hubert Humphrey to serve as the national director of Humphrey's presidential campaign and, separately, by Howard Hughes to serve as Hughes' public-policy lobbyist in Washington. O'Brien was elected national chairman of the DNC in 1968 and 1970. In late 1971, the president's brother, Donald Nixon, was collecting intelligence for his brother at the time and asked John H. Meier, an advisor to Howard Hughes, about O'Brien. In 1956, Donald Nixon had borrowed $205,000 from Howard Hughes and had never repaid the loan. The loan's existence surfaced during the 1960 presidential election campaign, embarrassing Richard Nixon and becoming a political liability. According to author Donald M. Bartlett, Richard Nixon would do whatever was necessary to prevent another family embarrassment. From 1968 to 1970, Hughes withdrew nearly half a million dollars from the Texas National Bank of Commerce for contributions to both Democrats and Republicans, including presidential candidates Humphrey and Nixon. Hughes wanted Donald Nixon and Meier involved but Nixon opposed this.
Meier told Donald that he was sure the Democrats would win the election because they had considerable information on Richard Nixon's illicit dealings with Hughes that had never been released, and that it resided with Larry O'Brien. O'Brien, who had received $25,000 from Hughes, did not have any documents but Meier claims to have wanted Richard Nixon to think that he did. It is conjecture that Donald told his brother that Meier had given the Democrats all the damaging Hughes information and that O'Brien had the proof. According to Fred Emery, O'Brien had been a lobbyist for Hughes in a Democrat-controlled Congress, and the possibility of his finding out about Hughes' illegal contributions to the Nixon campaign was too much of a danger for Nixon to ignore.
James F. Neal, who prosecuted the Watergate 7, did not believe Nixon had ordered the break-in because of Nixon's surprised reaction when he was told about it. He cited the June 23, 1972 conversation when Nixon asked Haldeman, "Who was the asshole that did it?"
Reactions.
Others.
In January 1975, publisher of "The Sacramento Union" John P. McGoff said that the media overemphasized the scandal, although "an important issue," overshadowing more serious topics, like declining economy and the energy crisis.

</doc>
<doc id="52385" url="https://en.wikipedia.org/wiki?curid=52385" title="Axiom of pairing">
Axiom of pairing

In axiomatic set theory and the branches of logic, mathematics, and computer science that use it, the axiom of pairing is one of the axioms of Zermelo–Fraenkel set theory.
Formal statement.
In the formal language of the Zermelo–Fraenkel axioms, the axiom reads:
or in words:
or in simpler words:
Interpretation.
What the axiom is really saying is that, given two sets "A" and "B", we can find a set "C" whose members are precisely "A" and "B".
We can use the axiom of extensionality to show that this set "C" is unique.
We call the set "C" the "pair" of "A" and "B", and denote it {"A","B"}.
Thus the essence of the axiom is:
Note that a singleton is a special case of a pair.
The axiom of pairing also allows for the definition of ordered pairs. For any sets formula_2 and formula_3, the ordered pair is defined by the following:
Note that this definition satisfies the condition
Ordered "n"-tuples can be defined recursively as follows:
Non-independence.
The axiom of pairing is generally considered uncontroversial, and it or an equivalent appears in just about any alternative axiomatization of set theory. Nevertheless, in the standard formulation of the Zermelo–Fraenkel set theory, the axiom of pairing follows from the axiom schema of replacement applied to any given set with two or more elements, and thus it is sometimes omitted. The existence of such a set with two elements, such as { {}, { {} } }, can be deduced either from the axiom of empty set and the axiom of power set or from the axiom of infinity.
Generalisation.
Together with the axiom of empty set, the axiom of pairing can be generalised to the following schema:
that is:
This set "C" is again unique by the axiom of extension, and is denoted {"A"1...,"A""n"}.
Of course, we can't refer to a "finite" number of sets rigorously without already having in our hands a (finite) set to which the sets in question belong.
Thus, this is not a single statement but instead a schema, with a separate statement for each natural number "n".
For example, to prove the case "n" = 3, use the axiom of pairing three times, to produce the pair {"A"1,"A"2}, the singleton {"A"3}, and then the pair .
The axiom of union then produces the desired result, {"A"1,"A"2,"A"3}. We can extend this schema to include "n"=0 if we interpret that case as the axiom of empty set.
Thus, one may use this as an axiom schema in the place of the axioms of empty set and pairing. Normally, however, one uses the axioms of empty set and pairing separately, and then proves this as a theorem schema. Note that adopting this as an axiom schema will not replace the axiom of union, which is still needed for other situations.
Another alternative.
Another axiom which implies the axiom of pairing in the presence of the axiom of empty set is
Using {} for "A" and "x" for B, we get {"x"} for C. Then use {"x"} for "A" and "y" for "B", getting {"x,y"} for C. One may continue in this fashion to build up any finite set. And this could be used to generate all hereditarily finite sets without using the axiom of union.

</doc>
<doc id="52386" url="https://en.wikipedia.org/wiki?curid=52386" title="Axiom schema of specification">
Axiom schema of specification

In many popular versions of axiomatic set theory the axiom schema of specification, also known as the axiom schema of separation, subset axiom scheme or axiom schema of restricted comprehension is an axiom schema. Essentially, it says that any definable subclass of a set is a set.
Some mathematicians call it the axiom schema of comprehension, although others use that term for "unrestricted" comprehension, discussed below.
Because restricted comprehension solved Russell's paradox, several mathematicians including Zermelo, Fraenkel, and Gödel considered it the most important axiom of set theory.
Statement.
One instance of the schema is included for each formula φ in the language of set theory with free variables among "x", "w"1, ..., "w""n", "A". So "B" is not free in φ. In the formal language of set theory, the axiom schema is:
or in words:
Note that there is one axiom for every such predicate φ; thus, this is an axiom schema.
To understand this axiom schema, note that the set "B" must be a subset of "A". Thus, what the axiom schema is really saying is that, given a set "A" and a predicate "P", we can find a subset "B" of "A" whose members are precisely the members of "A" that satisfy "P". By the axiom of extensionality this set is unique. We usually denote this set using set-builder notation as {"C" ∈ "A" : "P"("C")}. Thus the essence of the axiom is:
The axiom schema of specification is characteristic of systems of axiomatic set theory related to the usual set theory ZFC, but does not usually appear in radically different systems of alternative set theory. For example, New Foundations and positive set theory use different restrictions of the axiom of comprehension of naive set theory. The Alternative Set Theory of Vopenka makes a specific point of allowing proper subclasses of sets, called semisets. Even in systems related to ZFC, this scheme is sometimes restricted to formulas with bounded quantifiers, as in Kripke–Platek set theory with urelements.
Relation to the axiom schema of replacement.
The axiom schema of separation can almost be derived from the axiom schema of replacement.
First, recall this axiom schema:
for any functional predicate "F" in one variable that doesn't use the symbols "A", "B", "C" or "D".
Given a suitable predicate "P" for the axiom of specification, define the mapping "F" by "F"("D") = "D" if "P"("D") is true and "F"("D") = "E" if "P"("D") is false, where "E" is any member of "A" such that "P"("E") is true.
Then the set "B" guaranteed by the axiom of replacement is precisely the set "B" required for the axiom of specification. The only problem is if no such "E" exists. But in this case, the set "B" required for the axiom of separation is the empty set, so the axiom of separation follows from the axiom of replacement together with the axiom of empty set.
For this reason, the axiom schema of separation is often left out of modern lists of the Zermelo–Fraenkel axioms. However, it's still important for historical considerations, and for comparison with alternative axiomatizations of set theory, as can be seen for example in the following sections.
Unrestricted comprehension.
The "axiom schema of comprehension" (unrestricted) reads:
that is:
This set "B" is again unique, and is usually denoted as {"x" : φ("x", w1, ... wn)}.
This axiom schema was tacitly used in the early days of naive set theory, before a strict axiomatization was adopted. Unfortunately, it leads directly to Russell's paradox by taking φ("x") to be ¬("x"∈"x") (i.e., the property that set "x" is not a member of itself). Therefore, no useful axiomatization of set theory can use unrestricted comprehension, at least not with classical logic.
Accepting only the axiom schema of specification was the beginning of axiomatic set theory. Most of the other Zermelo–Fraenkel axioms (but not the axiom of extensionality or the axiom of regularity) then became necessary to make up for some of what was lost by changing the axiom schema of comprehension to the axiom schema of specification – each of these axioms states that a certain set exists, and defines that set by giving a predicate for its members to satisfy, i.e. it is a special case of the axiom schema of comprehension.
In NBG class theory.
In von Neumann–Bernays–Gödel set theory, a distinction is made between sets and classes. A class "C" is a set if and only if it belongs to some class "E". In this theory, there is a theorem schema that reads
that is,
provided that the quantifiers in the predicate "P" are restricted to sets.
This theorem schema is itself a restricted form of comprehension, which avoids Russell's paradox because of the requirement that "C" be a set. Then specification for sets themselves can be written as a single axiom
that is,
or even more simply
In this axiom, the predicate "P" is replaced by the class "D", which can be quantified over. Another simpler axiom which achieves the same effect is
that is,
In higher-order settings.
In a typed language where we can quantify over predicates, the axiom schema of specification becomes a simple axiom. This is much the same trick as was used in the NBG axioms of the previous section, where the predicate was replaced by a class that was then quantified over.
In second-order logic and higher-order logic with higher-order semantics, the axiom of specification is a logical validity and does not need to be explicitly included in a theory.
In Quine's New Foundations.
In the New Foundations approach to set theory pioneered by W.V.O. Quine, the axiom of comprehension for a given predicate takes the unrestricted form, but the predicates that may be used in the schema are themselves restricted.
The predicate ("C" is not in "C") is forbidden, because the same symbol "C" appears on both sides of the membership symbol (and so at different "relative types"); thus, Russell's paradox is avoided.
However, by taking "P"("C") to be ("C" = "C"), which is allowed, we can form a set of all sets. For details, see stratification.

</doc>
<doc id="52387" url="https://en.wikipedia.org/wiki?curid=52387" title="Axiom schema of replacement">
Axiom schema of replacement

In set theory, the axiom schema of replacement is a schema of axioms in Zermelo–Fraenkel set theory (ZFC) that asserts that the image of any set under any definable mapping is also a set. It is necessary for the construction of certain infinite sets in ZFC.
The axiom schema is motivated by the idea that whether a class is a set depends only on the cardinality of the class, not on the rank of its elements. Thus, if one class is "small enough" to be a set, and there is a surjection from that class to a second class, the axiom states that the second class is also a set. However, because ZFC only speaks of sets, not proper classes, the schema is stated only for definable surjections, which are identified with their defining formulas.
Statement.
Suppose "P" is a definable binary relation (which may be a proper class) such that for every set "x" there is a unique set "y" such that "P"("x","y") holds. There is a corresponding definable function "F""P", where "F""P"("X") = "Y" if and only if "P"("X","Y"); "F" will also be a proper class if "P" is. Consider the (possibly proper) class "B" defined such for every set "y", "y" is in "B" if and only if there is an "x" in "A" with "F""P"("x") = "y". "B" is called the image of "A" under "F""P", and denoted "F""P"["A"] or (using set-builder notation) {"F""P"("x") : "x" ∈ "A"}.
The axiom schema of replacement states that if "F" is a definable class function, as above, and "A" is any set, then the image "F"["A"] is also a set. This can be seen as a principle of smallness: the axiom states that if "A" is small enough to be a set, then "F"["A"] is also small enough to be a set. It is implied by the stronger axiom of limitation of size.
Because it is impossible to quantify over definable functions in first-order logic, one instance of the schema is included for each formula φ in the language of set theory with free variables among "w"1, ..., "w""n", "A", "x", "y"; but "B" is not free in φ. In the formal language of set theory, the axiom schema is:
Axiom schema of collection.
The axiom schema of collection is closely related to and frequently confused with the axiom schema of replacement. While replacement says that the image itself is a set, collection merely says that some superclass of the image is a set. In other words, the resulting set, "B", is not required to be minimal.
This version of collection also lacks the uniqueness requirement on φ. Suppose that the free variables of φ are among "w"1, ..., "w""n", "x", "y"; but neither "A" nor "B" is free in φ. Then the axiom schema is:
That is, the relation defined by φ is not required to be a function — some "x" in "A" may correspond to multiple "y" in "B". In this case, the image set "B" whose existence is asserted must contain at least one such "y" for each "x" of the original set, with no guarantee that it will contain only one.
The axiom schema is sometimes stated without prior restrictions (apart from "B" not occurring free in φ) on the predicate, φ:
In this case, there may be elements "x" in "A" that are not associated to any other sets by φ. However, the axiom schema as stated requires that, if an element "x" of "A" is associated with at least one set "y", then the image set "B" will contain at least one such "y". The resulting axiom schema is also called the axiom schema of boundedness.
The axiom schema of collection is equivalent to the axiom schema of replacement over the remainder of the ZF axioms. However, this is not so in the absence of the Power Set Axiom or constructive counterpart of ZF, where Collection is stronger.
Example applications.
The ordinal number ω·2 = ω + ω (using the modern definition due to von Neumann) is the first ordinal that cannot be constructed without replacement. The axiom of infinity asserts the existence of the infinite sequence ω = {0, 1, 2, ...}, and only this sequence. One would like to define ω·2 to be the union of the sequence {ω, ω + 1, ω + 2...}. However, arbitrary classes of ordinals need not be sets (the class of all ordinals is not a set, for example). Replacement allows one to replace each finite number "n" in ω with the corresponding ω + "n", and guarantees that this class is a set. Note that one can easily construct a well-ordered set that is isomorphic to ω·2 without resorting to replacement – simply take the disjoint union of two copies of ω, with the second copy greater than the first – but that this is not an ordinal since it is not totally ordered by inclusion.
Clearly then, the existence of an assignment of an ordinal to every well-ordered set requires replacement as well. Similarly the von Neumann cardinal assignment which assigns a cardinal number to each set requires replacement, as well as axiom of choice.
Every countable limit ordinal requires replacement for its construction analogously to ω·2. Larger ordinals rely on replacement less directly. For example ω1, the first uncountable ordinal, can be constructed as follows – the set of countable well orders exists as a subset of "P"(N×N) by separation and powerset (a relation on "A" is a subset of "A"×"A", and so an element of the power set "P"("A"×"A"). A set of relations is thus a subset of "P"("A"×"A")). Replace each well-ordered set with its ordinal. This is the set of countable ordinals ω1, which can itself be shown to be uncountable. The construction uses replacement twice; once to ensure an ordinal assignment for each well ordered set and again to replace well ordered sets by their ordinals. This is a special case of the result of Hartogs number, and the general case can be proved similarly.
The axiom of choice without replacement (ZC set theory) is not strong enough to show that Borel sets are determined; for this, replacement is required.
History.
The axiom schema of replacement was not part of Ernst Zermelo's 1908 axiomatisation of set theory (Z). Some informal approximation to it existed in Cantor's unpublished works, and it appeared again informally in Mirimanoff (1917).
Its publication by Adolf Fraenkel in 1922 is what makes modern set theory Zermelo-"Fraenkel" set theory (ZFC). The axiom was independently discovered and announced by Thoralf Skolem later in the same year (and published in 1923). Zermelo himself incorporated Fraenkel's axiom in his revised system he published in 1930, which also included as a new axiom von Neumann's axiom of foundation. Although it is Skolem's first order version of the axiom list that we use today, he usually gets no credit since each individual axiom was developed earlier by either Zermelo or Fraenkel. The phrase “Zermelo-Fraenkel set theory” was first used in print by von Neumann in 1928.
Zermelo and Fraenkel had corresponded heavily in 1921; the axiom of replacement was a major topic of this exchange. Fraenkel initiated correspondence with Zermelo sometime in March 1921. His letters before the one dated 6 May 1921 are lost though. Zermelo first admitted to a gap in his system in a reply to Fraenkel dated 9 May 1921. On 10 July 1921, Fraenkel completed and submitted for publication a paper (published in 1922) that described his axiom as allowing arbitrary replacements: "If "M" is a set and each element of "M" is replaced by set or an urelement then "M" turns into a set again" (parenthetical completion and translation by Ebbinghaus). Fraenkel's 1922 publication thanked Zermelo for helpful arguments. Prior to this publication, Fraenkel publicly announced his new axiom at a meeting of the German Mathematical Society held in Jena on 22 September 1921. Zermelo was present at this meeting; in the discussion following Fraenkel's talk he accepted the axiom of replacement in general terms, but expressed reservations regarding its extent.
Thoralf Skolem made public his discovery of the gap in Zermelo’s system (the same gap that Fraenkel had found) in a talk he gave on 6 July 1922 at the 5th Congress of Scandinavian Mathematicians, which was held in Helsinki; the proceedings of this congress were published in 1923. Skolem presented a resolution in terms of first-order definable replacements: "Let "U" be a definite proposition that holds for certain pairs ("a", "b") in the domain "B"; assume further, that for every "a" there exists at most one "b" such that "U" is true. Then, as "a" ranges over the elements of a set "Ma", "b" ranges over all elements of a set "Mb"." In the same year, Fraenkel wrote a review of Skolem's paper, in which Fraenkel simply stated that Skolem’s considerations correspond to his own.
Zermelo himself never accepted Skolem's formulation of the axiom schema of replacement. At one point he called Skolem's approach “set theory of the impoverished”. Zermelo envisaged a system that would allow for large cardinals. He also objected strongly to the philosophical implications of countable models of set theory, which followed from Skolem's first-order axiomatization. According to the biography of Zermelo by Heinz-Dieter Ebbinghaus, Zermelo's disapproval of Skolem's approach marked the end of Zermelo's influence on the developments of set theory and logic.
Impact.
The axiom schema of replacement drastically increases the strength of ZF, both in terms of the theorems it can prove and in terms of its proof-theoretic consistency strength, compared to Z. In particular, ZF proves the consistency of Z, as the set Vω·2 is a model of Z whose existence can be proved in ZF. (Gödel's second incompleteness theorem shows that each of these theories contains a sentence, "expressing" the theory's own consistency, that is unprovable in that theory, if that theory is consistent (this result is often loosely expressed as the claim that neither of these theories can prove its own consistency, if it is consistent.)) The cardinal number formula_4 is the first one which can be shown to exist in ZF but not in Z.
The axiom schema of replacement is not necessary for the proofs of most theorems of ordinary mathematics. Indeed, Zermelo set theory already can interpret second-order arithmetic and much of type theory in finite types, which in turn are sufficient to formalize the bulk of mathematics. A notable mathematical theorem that requires the axiom of replacement to be proved in ZF is the Borel determinacy theorem.
The axiom of replacement does have an important role in the study of set theory itself. For example, the replacement schema is needed to construct the von Neumann ordinals from ω·2 onwards; without replacement, it would be necessary to find some other representation for ordinal numbers.
Although the axiom schema of replacement is a standard axiom in set theory today, it is often omitted from systems of type theory and foundation systems in topos theory.
Relation to the axiom schema of separation.
The axiom schema of separation, the other axiom schema in ZFC, is implied by the axiom schema of replacement and the axiom of empty set. Recall that the axiom schema of separation includes
for each formula θ in the language of set theory in which "B" is not free.
The proof is as follows. Begin with a formula θ(C) that does not mention "B", and a set "A". If no element "E" of "A" satisfies θ then the set "B" desired by the relevant instance of the axiom schema of separation is the empty set. Otherwise, choose a fixed "E" in "A" such that θ("E") holds. Define a class function "F" such that "F"("D") = "D" if θ("D") holds and "F"("D") = "E" if "θ"("D") is false. Then the set "B" = "F" ""A" = "A"∩{"x"|θ("x")} exists, by the axiom of replacement, and is precisely the set "B" required for the axiom of separation.
This result shows that it is possible to axiomatize ZFC with a single infinite axiom schema. Because at least one such infinite schema is required (ZFC is not finitely axiomatizable), this shows that the axiom schema of replacement can stand as the only infinite axiom schema in ZFC if desired. Because the axiom schema of separation is not independent, it is sometimes omitted from contemporary statements of the Zermelo-Fraenkel axioms.
Separation is still important, however, for use in fragments of ZFC, because of historical considerations, and for comparison with alternative axiomatizations of set theory. A formulation of set theory that does not include the axiom of replacement will likely include some form of the axiom of separation, to ensure that its models contain a sufficiently rich collection of sets. In the study of models of set theory, it is sometimes useful to consider models of ZFC without replacement, such as the models formula_6 in von Neumann's hierarchy.
The proof above uses the law of excluded middle in assuming that if "A" is nonempty then it must contain an element (in intuitionistic logic, a set is "empty" if it does not contain an element, and "nonempty" is the formal negation of this, which is weaker than "does contain an element"). The axiom of separation is included in intuitionistic set theory.

</doc>
<doc id="52389" url="https://en.wikipedia.org/wiki?curid=52389" title="Independence Day (1996 film)">
Independence Day (1996 film)

Independence Day is a 1996 American epic science fiction disaster film co-written and directed by Roland Emmerich. The film stars Will Smith, Bill Pullman, Jeff Goldblum, Mary McDonnell, Judd Hirsch, Margaret Colin, Randy Quaid, Robert Loggia, James Rebhorn, Vivica A. Fox, and Harry Connick, Jr. The film focuses on a disparate group of people who converge in the Nevada desert in the aftermath of a destructive alien attack and, along with the rest of the human population, participate in a last-chance counterattack on July 4, the same date as the Independence Day holiday in the United States. The screenplay was written by Emmerich and producer Dean Devlin.
While promoting "Stargate" in Europe, Emmerich came up with the idea for the film when fielding a question about his own belief in the existence of alien life. He and Devlin decided to incorporate a large-scale attack when noticing that aliens in most invasion films travel long distances in outer space only to remain hidden when reaching Earth. Shooting began in July 1995 in New York City, and the film was officially completed on June 20, 1996.
The film was scheduled for release on July 3, 1996, but due to its high level of anticipation, many theaters began showing it on the evening of July 2, 1996, the same day the story of the film begins. The film grossed over $817.4 million worldwide, becoming 1996's highest-grossing film and the second highest-grossing film of all time at that point. It is currently the 51st-highest-grossing film of all time and was at the forefront of the large-scale disaster film and science fiction resurgences of the mid-to-late-1990s. The film received positive reviews upon its release, with critics mainly praising its groundbreaking special effects, musical score, and acting (particularly the performances of Smith and Goldblum), though some criticized its storyline and character development. It won the Academy Award for Best Visual Effects, while it was nominated for Best Sound Mixing. 
The film will have both its twentieth anniversary "and" premiere at a special live-orchestral screening performance at the Royal Albert Hall on 22 September 2016. The Royal Philharmonic Orchestra, conducted by the original orchestrator Nicholas Dodd, will perform the score live as the film is projected and the film's composer, David Arnold, will give a pre-film talk at the event about his work in scoring for film and television. A sequel, "", is scheduled to be released on June 24, 2016.
Plot.
On July 2, 1996, an enormous alien mothership one fourth the size of the Moon enters orbit around Earth, deploying 36 smaller spacecraft, each wide, that take positions over Earth's major cities, the White House, and other strategic locations. David Levinson, an MIT-trained satellite expert, intercepts a signal embedded in global satellite transmissions that he determines is a timer counting down to a coordinated attack. With the help of his estranged wife, White House Communications Director Constance Spano, Levinson and his father Julius gain access to the Oval Office and warn President Whitmore that the aliens are hostile. Whitmore orders large-scale evacuations, but it is too late; the timer reaches zero and the ships activate devastating directed-energy weapons. Whitmore, the Levinsons, and a few others narrowly escape aboard Air Force One as the White House is destroyed along with cities and military installations around the world.
On July 3, a squadron of F/A-18 Hornets assaults a destroyer ship near the ruins of Los Angeles, but the craft is protected by a force field. Dozens of small "attacker" ships, also protected by force fields, are launched by the destroyer, and a dogfight ensues; the fighter squadron is wiped out. Captain Steven Hiller survives by luring his attacker to the Grand Canyon and sacrificing his plane, forcing the alien to crash-land. He subdues the injured alien pilot and flags down a convoy of mobile homes fleeing the devastation, hitching a ride with Vietnam War veteran and alien abduction victim Russell Casse. They transport the unconscious alien to nearby Area 51, where Whitmore and his people have landed. The government has known about these aliens since 1947, when one of their ships crashed in Roswell. Area 51 houses a refurbished attacker ship and three alien corpses recovered from that crash.
As eccentric scientist, Brackish Okun, examines the alien brought in by Hiller; it regains consciousness. After knocking Okun senseless, it invades his mind and uses his vocal cords to communicate with Whitmore. "What do you want us to do?" asks Whitmore. "Die", replies the alien, launching a psychic attack against him. As Whitmore thrashes in agony on the floor, Whitmore's security detail kills the alien. Whitmore says during the psychic attack, he saw that the aliens travel from planet to planet "like locusts", destroying all native life and stripping the planet's resources before moving on. Whitmore reluctantly authorizes a nuclear attack; a B-2 Spirit fires a nuclear cruise missile at a destroyer near Houston, but the destroyer's force field holds and the ship is undamaged.
Levinson realizes that the key to defeating the aliens is deactivating their force fields, and devises a way to do it using a computer virus, but to upload the virus, he must infiltrate the mothership. He proposes using the captured attacker ship to gain entry, and once the force fields are disarmed, plant a nuclear bomb on board. Hiller volunteers to pilot the attacker. He and Levinson succeed in entering the mothership, uploading the virus, and then deploying the nuclear device, destroying the mothership as they narrowly escape.
With the alien force fields disabled, Whitmore orders an attack on a destroyer ship bearing down on Area 51. With available military pilots in short supply, Whitmore and Casse join the strike force. Although the alien ship is unprotected, the attacking fighters exhaust their missiles without disabling it. The aliens prepare to fire their primary weapon. Casse has one missile left, but it jams; he flies his plane into the directed-energy weapon port, kamikaze-style. The resulting explosion destroys the alien ship. The Americans share this critical vulnerability with the rest of the world's countries, enabling them to defeat the other destroyer ships. People around the world rejoice against a backdrop of the smoking wreckage of the alien destroyers, as the wreckage of the mothership, burning up as it enters the atmosphere, creates a spectacular "fireworks" display.
Production.
Development.
The idea for the film came when Emmerich and Devlin were in Europe promoting their film "Stargate". A reporter asked Emmerich why he made a film with content like "Stargate" if he did not believe in aliens. Emmerich stated he was still fascinated by the idea of an alien arrival, and further explained his response by asking the reporter to imagine what it would be like to wake up one morning and discover 15-mile-wide spaceships were hovering over the world's largest cities. Emmerich then turned to Devlin and said, "I think I have an idea for our next film."
Emmerich and Devlin decided to expand on the idea by incorporating a large-scale attack, with Devlin saying he was bothered by the fact that "for the most part, in alien invasion movies, they come down to Earth and they're hidden in some back field ...r they arrive in little spores and inject themselves into the back of someone's head." Emmerich agreed by asking Devlin if arriving from across the galaxy, "would you hide on a farm or would you make a big entrance?" The two wrote the script during a month-long vacation in Mexico, and just one day after they sent it out for consideration, 20th Century Fox chairman Peter Chernin greenlit the screenplay. Pre-production began just three days later in February 1995. The U.S. military originally intended to provide personnel, vehicles, and costumes for the film; however, they backed out when the producers refused to remove the script's Area 51 references.
A then-record 3,000-plus special effects shots would ultimately be required for the film. The shoot utilized on-set, in-camera special effects more often than computer-generated effects in an effort to save money and get more authentic pyrotechnic results. Many of these shots were accomplished at Hughes Aircraft in Culver City, California, where the film's art department, motion control photography teams, pyrotechnics team, and model shop were headquartered. The production's model-making department built more than twice as many miniatures for the production than had ever been built for any film before by creating miniatures for buildings, city streets, aircraft, landmarks, and monuments. The crew also built miniatures for several of the spaceships featured in the film, including a 30-foot (9.1 m) destroyer model and a version of the mother ship spanning . City streets were recreated, then tilted upright beneath a high-speed camera mounted on a scaffolding filming downwards. An explosion would be ignited below the model, and flames would rise towards the camera, engulfing the tilted model and creating the rolling "wall of destruction" look seen in the film. A model of the White House was also created, covering by , and was used in forced-perspective shots before being destroyed in a similar fashion for its own destruction scene. The detonation took a week to plan and required 40 explosive charges.
The film's aliens were designed by production designer Patrick Tatopoulos. The actual aliens of the film are diminutive and based on a design Tatopoulos drew when tasked by Emmerich to create an alien that was "both familiar and completely original". These creatures wear "bio-mechanical" suits that are based on another design Tatopoulos pitched to Emmerich. These suits were tall, equipped with 25 tentacles, and purposely designed to show it could not sustain a person inside so it would not appear to be a "man in a suit".
Filming.
Principal photography began in July 1995 in New York City. A second unit gathered plate shots and establishing shots of Manhattan, Washington D.C., an RV community in Flagstaff, Arizona, and the Very Large Array on the Plains of San Agustin, New Mexico. The main crew also filmed in nearby Cliffside Park, New Jersey before moving to the former Kaiser Steel mill in Fontana, California to film the post-attack Los Angeles sequences. The production then moved to Wendover, Utah and West Wendover, Nevada, where the deserts doubled for Imperial Valley and the Wendover Airport doubled for the El Toro and Area 51 exteriors. It was here where Pullman filmed his pre-battle speech. Immediately before filming the scene, Devlin and Pullman decided to add "Today, we celebrate our Independence Day!" to the end of the speech. At the time, the production was nicknamed "ID4" because Warner Bros. owned the rights to the title "Independence Day", and Devlin had hoped if Fox executives noticed the addition in dailies, the impact of the new dialogue would help them win the rights to the title. The right to use the title was eventually won two weeks later.
The production team moved to the Bonneville Salt Flats to film three scenes, then returned to California to film in various places around Los Angeles, including Hughes Aircraft where sets for the cable company and Area 51 interiors were constructed at a former aircraft plant. Sets for the latter included corridors containing windows that were covered with blue material. The filmmakers originally intended to use the chroma key technique to make it appear as if activity was happening on the other side of the glass; but the composited images were not added to the final print because production designers decided the blue panels gave the sets a "clinical look". The attacker hangar set contained an attacker mock-up wide that took four months to build. The White House interior sets used had already been built for "The American President" and had previously been used for "Nixon". Principal photography completed on November 3, 1995.
The film originally depicted Russell Casse being rejected as a volunteer for the July 4 aerial counteroffensive because of his alcoholism. He then uses a stolen missile tied to his red biplane to carry out his suicide mission. According to Dean Devlin, test audiences responded well to the scene's irony and comedic value. However, the scene was re-shot to include Russell's acceptance as a volunteer, his crash course in modern fighter aircraft, and him flying an F/A-18 instead of the biplane. Devlin preferred the alteration because the viewer now witnesses Russell ultimately making the decision to sacrifice his life, and seeing the biplane keeping pace and flying amongst F/A-18s was "just not believable". The film was officially completed on June 20, 1996.
Music.
The Grammy Award-winning score for the film was composed by David Arnold and recorded with an orchestra of 90, a choir of 46 and "and every last ounce of stereotypical Americana he could muster for the occasion". The film's producer Dean Devlin commented that "you can leave it up to a Brit to write some of the most rousing and patriotic music in the history of American cinema." The soundtrack has received two official CD releases, RCA released a 50-minute album at the time of the film's release, then in 2010, La-La Land Records released a limited edition 2-CD set that comprised the complete score plus 12 alternate cues. The premiere of "Independence Day" live will take place in September 2016 with the film's score performed live to a screening of the film at the Royal Albert Hall. This celebrates the twentieth anniversary of the film's release and the event will also feature a pre-film talk from David Arnold.
Release.
While the film was still in post-production, Fox began a massive marketing campaign to help promote the film, beginning with the airing of a dramatic commercial during Super Bowl XXX, for which Fox paid $1.3 million. The film's subsequent success at the box office resulted in the trend of using Super Bowl air time to kick off the advertising campaign for potential blockbusters.
Fox's Licensing and Merchandising division also entered into co-promotional deals with Apple Inc. The co-marketing project was dubbed "The Power to Save the World" campaign, in which the company used footage of David using his PowerBook laptop in their print and television advertisements. Trendmasters entered a merchandising deal with the film's producers to create a line of tie-in toys. In exchange for product placement, Fox also entered into co-promotional deals with Molson Coors Brewing Company and Coca-Cola.
The film was marketed with several taglines, including: "We've always believed we weren't alone. On July 4, we'll wish we were", "Earth. Take a good look. It could be your last", and "Don't make plans for August". The weekend before the film's release, the Fox Network aired a half-hour special on the film, the first third of which was a spoof news report on the events that happen in the film. Roger Ebert attributed most of the film's early success to its teaser trailers and marketing campaigns, acknowledging them as "truly brilliant".
The film had its official premiere held at Los Angeles' now-defunct Mann Plaza Theater on June 25, 1996. It was then screened privately at the White House for President Bill Clinton and his family before receiving a nationwide release in the United States on July 2, 1996, a day earlier than its previously scheduled opening.
After a six-week, $30 million marketing campaign, "Independence Day" was released on VHS on November 22, 1996. It became available on DVD on June 27, 2000, and has been re-released on DVD under several different versions with varying supplemental material ever since, including one instance where it was packaged with a lenticular cover. Often accessible on these versions is a special edition of the film, which features nine minutes of additional footage not seen in the original theatrical release. "Independence Day" became available on Blu-ray discs in the United Kingdom on December 24, 2007, and in North America on March 11, 2008. Also released in Australia on 5 March 2008. The Blu-ray edition does not include the deleted scenes.
Censorship.
In Lebanon, certain Jewish and Israel-related content of the film was censored. One cut scene involved Judd Hirsch's character donning a kippah and leading soldiers and White House officials in a Jewish prayer. Other removed footage showed Israeli and Arab troops working together in preparation for countering the alien invasion. The Lebanese Shi'a Islamist militant group Hezbollah called for Muslims to boycott the film, describing it as "propaganda for the so-called genius of the Jews and their concern for humanity." In response, Jewish actor Jeff Goldblum said, "I think Hezbollah has missed the point: the film is not about American Jews saving the world; it's about teamwork among people of different religions and nationalities to defeat a common enemy."
Reception.
Box office.
"Independence Day" was the highest-grossing film of 1996. In the United States, "Independence Day" earned $104.3 million in its first full week, including $96.1 million during its five-day holiday opening, and $50.2 million during its opening weekend. All three figures broke records set by "Jurassic Park" three years earlier. That film's sequel, "", claimed all three records when it was released the following year. "Independence Day" stayed in the number-one spot for three weeks, and grossed $306,169,268 in the domestic market and $510,800,000 in foreign markets during its theatrical run. The combined total of $817,400,891 once trailed only the worldwide earnings of "Jurassic Park" as the highest of all time. It has been surpassed by multiple 21st century films since, and currently holds the 51st-highest worldwide gross of all time for a film. Hoping to capitalize in the wake of the film's success, several studios released more large-scale disaster films, and the already rising interest in science fiction-related media was further increased by the film's popularity.
A month after the film's release, jewelry designers and marketing consultants reported an increased interest in dolphin-themed jewelry, since the character of Jasmine in the film wears dolphin earrings and is presented with a wedding ring featuring a gold dolphin.
Critical response.
Upon its release, "Independence Day" received praise for its visuals and sense of fun, but criticism towards its writing. Rotten Tomatoes reports a score of 60%, based on 57 reviews, with the site's critical consensus reading, "The plot is thin and so is character development, but as a thrilling, spectacle-filled summer movie, "Independence Day" delivers." On Metacritic, the film has a score of 59 out of 100, based on 19 critics, indicating "mixed or average reviews".
Critics acknowledged the film had "cardboard" and "stereotypical" characters, and weak dialogue. Yet the shot of the White House's destruction has been declared a milestone in visual effects and one of the most memorable scenes of the 1990s. In a 2010 poll, the readers of "Entertainment Weekly" rated it the second-greatest summer film of the previous 20 years, ranking only behind "Jurassic Park".
Mick LaSalle of the "San Francisco Chronicle" gave the film his highest rating, declaring it the "apotheosis" of "Star Wars". Lisa Schwarzbaum of "Entertainment Weekly" gave it a B+ for living up to its massive hype, adding "charm is the foremost of this epic's contemporary characteristics. The script is witty, knowing, cool." Eight years later, "Entertainment Weekly" would rate the film as one of the best disaster films of all time. Kenneth Turan of the "Los Angeles Times" felt that the film did an "excellent job conveying the boggling immensity of extraterrestrial vehicles [... and panic in the streets" and the scenes of the alien attack were "disturbing, unsettling and completely convincing".
However, the film's nationalistic overtones were widely criticized by reviewers outside the U.S. "Movie Review UK" described the film as "A mish-mash of elements from a wide variety of alien invasion movies and gung-ho American jingoism." The speech in which Whitmore states that victory in the coming war would see the entire world henceforth describe July 4 as its Independence Day, was described as "the most jaw-droppingly pompous soliloquy ever delivered in a mainstream Hollywood movie" in a BBC review. In 2003, readers of "Empire", voted the scene that contained the speech as the "Cheesiest Movie Moment of All-Time". Conversely, "Empire" critic Kim Newman gave the film a five-star rating in the magazine's original review of the film.
Several prominent critics expressed disappointment with the quality of the film's special effects. "Newsweek"'s David Ansen claimed the special effects were of no better caliber than those seen nineteen years earlier in "Star Wars". Todd McCarthy of "Variety" felt the production's budget-conscious approach resulted in "cheesy" shots that lacked in quality relative to the effects present in films directed by James Cameron and Steven Spielberg. In his review, Roger Ebert took note of a lack of imagination in the spaceship and creature designs. Gene Siskel expressed the same sentiments in their on-air review of the film.
American Film Institute lists
In other media.
Books.
Author Stephen Molstad wrote a tie-in novel to help promote the film shortly before its release. The novel goes into further detail on the characters, situations, and overall concept not explored in the film. The novel presents the film's finale as originally scripted, with the character played by Randy Quaid stealing a missile and roping it to his crop duster biplane.
Following the film's success, a prequel novel entitled "Independence Day: Silent Zone" was written by Molstad in February 1998. The novel is set in the late 1960s and early 1970s, and details the early career of Dr. Brackish Okun.
Molstad wrote a third novel, "Independence Day: War in the Desert" in July 1999. Set in Saudi Arabia on July 3, it centers around Captain Cummins and Colonel Thompson, the two Royal Air Force officers seen receiving the Morse code message in the film.
A Marvel comic book was also written based on the first two novelizations.
Radio.
On August 4, 1996, BBC Radio 1 broadcast the one-hour play "Independence Day UK", written, produced, and directed by Dirk Maggs, a spin-off depicting the alien invasion from a British perspective. None of the original cast was present. Dean Devlin gave Maggs permission to produce an original version, on the condition he did not reveal certain details of the movie's plot and the British were not depicted as saving the day. "Independence Day UK" was set up to be similar to the 1938 radio broadcast of "The War of the Worlds"; the first 20 minutes were set as being live.
Computer games.
An "Independence Day" video game was released in February 1997 for the PlayStation, Sega Saturn, and PC, each version receiving mostly tepid reviews. The multi-view shooter game contains various missions to perform, with the ultimate goal of destroying the aliens' primary weapon. A wireless mobile version was released in 2005. A computer game entitled "ID4 Online" was released in 2000.
Toys.
Trendmasters released a toy line for the film in 1996. Each action figure, vehicle or playset came with a 3 1⁄2" floppy disk that contained an interactive computer game.
Sequel.
In June 2011, Devlin confirmed that he and Emmerich had written a treatment for two sequels to form a trilogy, with both Emmerich and Devlin having the desire for Will Smith to return. In October 2011, however, discussions for Smith returning were halted, due to Fox's refusal to provide the $50 million salary demanded by Smith for the two sequels. Emmerich, however, made assurances that the films would be shot back-to-back, regardless of Smith's involvement.
In March 2013, Emmerich stated that the titles of the new films would be "ID Forever Part I" and "ID Forever Part II". In November 2014, the sequel was given an official green light by 20th Century Fox with a release date of June 24, 2016, noting that this will be a stand-alone sequel that will not split into two parts as originally planned, with filming beginning in May 2015 and casting being done after the studio locks down Emmerich as the director on the film. In December 2014, Devlin confirmed that Emmerich would indeed be directing the sequel. On June 22, 2015, Emmerich announced the official title, "".
With regards to Smith's decision not to return to film a sequel, director Emmerich told Screen Crush that "In the very beginning, I wanted to work with him and he was excited to be in it but then after a while he was tired of sequels, and he did another science fiction film, which was his father-son story ["After Earth"], so he opted out."
"Independence Day: Resurgence" will be released on June 24, 2016.

</doc>
<doc id="52390" url="https://en.wikipedia.org/wiki?curid=52390" title="Armageddon (1998 film)">
Armageddon (1998 film)

Armageddon is a 1998 American science fiction disaster thriller film directed by Michael Bay, produced by Jerry Bruckheimer, and released by Touchstone Pictures. The film follows a group of blue-collar deep-core drillers sent by NASA to stop a gigantic asteroid on a collision course with Earth. It features an ensemble cast including Bruce Willis, Ben Affleck, Billy Bob Thornton, Liv Tyler, Owen Wilson, Will Patton, Peter Stormare, William Fichtner, Michael Clarke Duncan, Keith David, and Steve Buscemi.
"Armageddon" opened in theaters only two and a half months after the similar asteroid impact-based film "Deep Impact", which starred Robert Duvall and Morgan Freeman. "Armageddon" fared better at the box office, while astronomers described "Deep Impact" as being more scientifically accurate. "Armageddon" was an international box-office success despite generally negative reviews from critics, becoming the highest-grossing film of 1998 worldwide, surpassing the Steven Spielberg war epic "Saving Private Ryan".
Plot.
Sixty-five million years ago, an asteroid in diameter strikes the Earth, creating a massive extinction event that wipes out 70% of life on the planet, including the dinosaurs. Ejecting trillions of tons of dust into the atmosphere, it blocks out the sun, which eventually leads to the extinction event. It is stated an impact like this "happened before, it will happen again.".
In the present day, a massive meteor shower destroys the orbiting Space Shuttle "Atlantis" and bombards a swath of land from the U.S. East Coast from South Carolina through Finland, particularly causing massive damage and casualties in New York City. NASA discovers that a rogue comet passed through the asteroid belt and pushed forward a large amount of space debris including an asteroid the size of Texas (roughly in diameter). The asteroid will collide with Earth in 18 days, causing an extinction event that will even wipe out bacteria. NASA scientists, led by Dan Truman, plan to trigger a nuclear detonation inside the asteroid to split it in two, driving the pieces apart so both will fly past the Earth. NASA contacts Harry Stamper, considered the best deep-sea oil driller in the world, for assistance. Harry travels to NASA with his daughter Grace, to keep her away from her new boyfriend and one of Harry's drillers, A.J. Frost. Harry explains he will need his team, including A.J., to carry out the mission. They agree to help, but only after their list of unusual rewards and demands are met.
NASA plans to launch two specialized shuttles, "Freedom" and "Independence", to increase the chances of success; the shuttles will refill with liquid oxygen from the Russian space station "Mir" before making a slingshot maneuver around the Moon to approach the asteroid from behind. NASA puts Harry and his crew through a short and rigorous astronaut training program, while Harry and his team re-outfit the mobile drillers, "Armadillos", for the job.
The destruction of Shanghai by an asteroid fragment forces NASA to reveal the asteroid's existence, as well as their plan. The shuttles are launched and arrive at "Mir", where its sole cosmonaut Lev helps with refueling. A major fire breaks out during the fueling process, forcing the crews, including Lev, to evacuate in the shuttles before "Mir" explodes. The shuttles perform the slingshot around the moon, but approaching the asteroid, the "Independence"'s engines are destroyed by trailing debris, and it crashes on the asteroid. Grace, aware A.J. was aboard the "Independence", is traumatized by this news. Unknown to the others, A.J., Lev, and "Bear" (another of Harry's crew) survive the impact and head towards the "Freedom" target site in their Armadillo.
Meanwhile, "Freedom" safely lands on the asteroid, but overshoots the target zone, landing on a much harder metallic field than planned, and their drilling quickly falls behind schedule; in desperation, the military initiates "Secondary Protocol" to remotely detonate the nuclear weapon on the asteroid's surface, despite Truman and Harry's insistence that it would be ineffective. Truman delays the military, while Harry convinces the shuttle commander Colonel Willie Sharp to disarm the remote trigger. Harry's crew continues to work, but in their haste, they accidentally hit a gas pocket, blowing their Armadillo into space and losing another man. As the world learns of the mission's apparent failure, another asteroid fragment devastates Paris.
All seems lost until the arrival of the "Independence"'s Armadillo. With A.J. at the controls, they reach the required depth for the bomb. However, flying debris from the asteroid damages the triggering device, requiring someone to stay behind to manually detonate the bomb. The crew draw straws, and A.J. is selected. As he and Harry exit the airlock, Harry rips off A.J.'s air hose and shoves him back inside, telling him that he is the son Harry never had, and he would be proud to have A.J. marry Grace. Harry prepares to detonate the bomb and contacts Grace to bid his final farewell. After some last minute difficulties involving both the shuttle engines and the detonator, the "Freedom" moves to a safe distance and Harry manages to press the button at the last minute while experiencing flashbacks of happy times in his last moments. The bomb successfully splits the asteroid, avoiding the collision with Earth. "Freedom" safely returns to Earth, and the surviving crew are treated as heroes. A.J. and Grace get married, with photos of Harry and the other lost crew members present.
Production.
In May 1998, Walt Disney Studios chairman Joe Roth expanded the film's budget by $3 million to include additional special effects scenes. This additional footage, incorporated two months prior to the film's release, was specifically added for the television advertising campaign to differentiate the film from "Deep Impact" which was released a few months before.
According to Bruce Joel Rubin, writer of "Deep Impact", a production president at Disney took notes on everything the writer said during lunch about his script and initiated "Armageddon" as a counter film at Disney.
Nine writers worked on the script, five of whom are credited. In addition to Robert Roy Pool, Jonathan Hensleigh, Tony Gilroy, Shane Salerno and J.J. Abrams, the writers involved included Paul Attanasio, Ann Biderman, Scott Rosenberg and Robert Towne. Originally, it was Hensleigh’s script, based on Pool’s original, that had been greenlighted by Touchstone. Then-producer, Jerry Bruckheimer, hired the succession of scribes for rewrites and polishes.
Music.
Two soundtrack albums were released for the film. The first, "", was released by Columbia Records on June 23, 1998; it consists mainly of songs from the film, with one score suite.
A more complete album of the film score by composers Trevor Rabin and Harry Gregson-Williams was released as "Armageddon: Original Motion Picture Score" by Sony Classical on November 10, 1998. While not featured on the album, the film also featured additional music by Don L. Harper, Paul Linford, Steve Jablonsky and John Van Tongeren.
Release.
Prior to "Armageddon"s release, the film was advertised in Super Bowl XXXII at a cost of $2.6 million.
Home media.
Despite a mixed critical reception, a DVD edition of "Armageddon" was released by The Criterion Collection, a specialist film distributor of primarily arthouse films that markets what it considers to be "important classic and contemporary films" and "cinema at its finest". In an essay supporting the selection of "Armageddon", film scholar Jeanine Basinger, who taught Michael Bay at Wesleyan University, states that the film is "a work of art by a cutting-edge artist who is a master of movement, light, color, and shape—and also of chaos, razzle-dazzle, and explosion". She sees it as a celebration of working men: "This film makes these ordinary men noble, lifting their efforts up into an epic event." Further, she states that in the first few moments of the film all the main characters are well established, saying, "If that isn't screenwriting, I don't know what is". The film was also released by Touchstone Home Entertainment on standard edition Blu-ray disc in 2010 with only a few special features.
Space Shuttle "Columbia" disaster.
Following the 2003 "Columbia" disaster, some screen captures from the opening scene where "Atlantis" is destroyed were passed off as satellite images of the disaster in a hoax. Additionally, the American cable network FX, which had intended to broadcast "Armageddon" that evening, removed the film from its schedule and aired "Aliens" in its place.
Reception.
Box office.
"Armageddon" was released on , 1998 in in the United States and Canada. It ranked first at the box office with an opening weekend gross of . It grossed in the United States and Canada and in other territories for a worldwide total of .
Critical response.
"Armageddon" received mostly negative reviews from film critics, many of whom took issue with "the furious pace of its editing". The film is on the list of Roger Ebert's most hated films. In his original review, Ebert stated, "The movie is an assault on the eyes, the ears, the brain, common sense and the human desire to be entertained". On "Siskel and Ebert", Ebert gave it a Thumbs Down. However, his co-host Gene Siskel gave it a Thumbs Up. Ebert went on to name "Armageddon" as the worst film of 1998 (though he was originally considering "Spiceworld"). Todd McCarthy of "Variety" also gave the film a negative review, noting Michael Bay's rapid cutting style: "Much of the confusion, as well as the lack of dramatic rhythm or character development, results directly from Bay's cutting style, which resembles a machine gun stuck in the firing position for 2½ hours." The film has a cumulative 39% "Rotten" rating on Rotten Tomatoes, while achieving a 42% aggregate score on "Metacritic".
In April 2013, in a "Miami Herald" interview to promote "Pain & Gain", Bay was quoted as having said:
...We had to do the whole movie in 16 weeks. It was a massive undertaking. That was not fair to the movie. I would redo the entire third act if I could. But the studio literally took the movie away from us. It was terrible. My visual effects supervisor had a nervous breakdown, so I had to be in charge of that. I called James Cameron and asked ‘What do you do when you’re doing all the effects yourself?’ But the movie did fine.
Some time after the article was published, Bay changed his stance, claiming that his apology only related to the editing of the film, not the whole film, and accused the writer of the article for taking his words out of context. The author of the article, "Miami Herald" writer Rene Rodriguez claimed: "NBC asked me for a response, and I played them the tape. I didn’t misquote anyone. All the sites that picked up the story did."
Scientific accuracy.
In an interview with "Entertainment Weekly", Bay admitted that the film's central premise "that could actually do something in a situation like this" was unrealistic. Robert Roy Pool, a contributing screenwriter, stated that his script, in which an anti-gravity device is used to deflect a comet from a collision course with Earth, was "much more in line with top-secret research." Additionally, near the end of the credits, there is a disclaimer stating, "The National Aeronautics and Space Administration's cooperation and assistance does not reflect an endorsement of the contents of the film or the treatment of the characters depicted therein."
In 2012, an article titled "Could Bruce Willis Save the World?" was published in the "Journal of Physics Special Topics", an undergraduate journal used as a teaching tool at the University of Leicester. It found that for Willis' approach to be effective, he would need to be in possession of an H-bomb a billion times stronger than the Soviet Union's "Tsar Bomba", the biggest ever detonated on Earth. Using estimates of the asteroid's size, density, speed and distance from Earth based on information in the film, students found that to split the asteroid in two with both pieces clearing Earth, would require 800 zettajoules of energy. In contrast, the total energy output of "Tsar Bomba", which was tested by the Soviet Union in 1961, was only 418 petajoules (0.000 418 zettajoules).
Accolades.
The film received four Academy Award nominations at the 71st Academy Awards, including; Best Sound (Kevin O'Connell, Greg P. Russell and Keith A. Wester), Best Visual Effects, Best Sound Effects Editing, and Best Original Song ("I Don't Want to Miss a Thing" performed by Aerosmith). The film received the Saturn Awards for Best Direction and Best Science Fiction Film (where it tied with "Dark City"). It was also nominated for seven Razzie Awards including: Worst Actor (Bruce Willis), Worst Picture, Worst Director, Worst Screenplay, Worst Supporting Actress (Liv Tyler), Worst Screen Couple (Tyler and Ben Affleck) and Worst Original Song. Only one Razzie was awarded: Bruce Willis received the Worst Actor award for "Armageddon", in addition to his appearances in "Mercury Rising" and "The Siege", both released in the same year as this film.
Theme park attraction.
Armageddon – Les Effets Speciaux is an attraction based on "Armageddon" at Walt Disney Studios Park located at Disneyland Paris. The attraction simulates the scene in the movie in which the Russian Space Station is destroyed. Michael Clarke Duncan ("Bear" in the film) is featured in the pre-show.

</doc>
<doc id="52392" url="https://en.wikipedia.org/wiki?curid=52392" title="Prince Maximilian of Baden">
Prince Maximilian of Baden

Maximilian Alexander Friedrich Wilhelm Margrave of Baden (10 July 1867 – 6 November 1929), also known as "Max von Baden", was a German prince and politician. He was heir to the Grand Duchy of Baden and in October and November 1918 briefly served as Chancellor of the German Empire. He sued for peace on Germany's behalf at the end of World War I based on U.S. President Woodrow Wilson's Fourteen Points, which included immediately transforming the government into a parliamentary system and proclaiming the abdication of Emperor Wilhelm II.
Early life.
Born in Baden-Baden on 10 July 1867, Maximilian was a member of the House of Baden, the son of Prince Wilhelm Max (1829–1897), third son of Grand Duke Leopold (1790–1852) and Princess Maria Maximilianovna of Leuchtenberg (1841-1914), a granddaughter of Eugène de Beauharnais. He was named after his maternal grandfather, Maximilian de Beauharnais.
Max received a humanistic education at a "Gymnasium" secondary school and studied law and cameralism at the Leipzig University. In 1900, he married Princess Marie Louise of Hanover (1879-1948) at Gmunden. Upon the order of Queen Victoria, Prince Max was brought to Darmstadt in the Grand Duchy of Hesse and by Rhine as a suitor for Victoria's granddaughter, Alix of Hesse-Darmstadt. Alix was the daughter of Victoria's late daughter, Princess Alice of the United Kingdom and Louis IV, Grand Duke of Hesse. Alix quickly rejected Prince Max. She was in love with Nicholas II, the future Tsar of Russia.
Early military and political career.
After finishing his studies, he trained as an officer of the Prussian Army. Following the death of his uncle Grand Duke Frederick I of Baden in 1907, he became heir to the grand-ducal throne of his cousin Frederick II, whose marriage remained childless. He also became president of the "Erste Badische Kammer" (the upper house of the parliament of Baden). In 1911, Max applied for a military discharge with the rank of a "Generalmajor" (Major general).
World War I.
Upon the outbreak of World War I in 1914, he served as a general staff officer at the XIV Corps of the German Army as the representative of the Grand Duke (XIV Corps included the troops from Baden). Shortly afterwards, however, he retired from his position ("General der Kavallerie à la suite") as he was dissatisfied with his role in the military and was suffering from ill health.
In October 1914, he became honorary president of the Baden section of the German Red Cross, thus beginning his work for prisoners-of-war in- and outside of Germany in which he made use of his family connections to the Russian and Swedish courts as well as his connections to Switzerland. In 1916, he became honorary president of the German-American support union for prisoners-of-war within the YMCA world alliance.
Due to his liberal stance he came into conflict with the policies of the "Oberste Heeresleitung" (OHL - Supreme Army Command) supreme command under Paul von Hindenburg and Erich Ludendorff. He openly spoke against the resumption of the unrestricted submarine warfare in 1917, which provoked the declaration of war by the United States Congress on 6 April.
His activity in the interests of prisoners-of-war, as well as his tolerant, easy-going character gave him a reputation as an urbane personality who kept his distance from the extremes of nationalism and official war enthusiasm in evidence elsewhere at the time. Since he was almost unknown to the public, it was mainly due to Kurt Hahn, who served since spring 1917 in the military office of the Foreign Ministry, that he was later considered for the position of chancellor. Hahn maintained close links with Secretary of State Wilhelm Solf and several Reichstag deputies like Eduard David (SPD) and (FVP). David pushed for Max to be appointed Chancellor in July 1917, after the fall of Chancellor Bethmann-Hollweg. Max then put himself forward for the position in early September 1918, pointing out his links to the social democrats, but Emperor Wilhelm II turned him down.
Chancellor.
Appointment.
After the "Oberste Heeresleitung" told the government in late September 1918 that the German front was about to collapse and asked for immediate negotiation of an armistice, the cabinet of Chancellor Georg von Hertling resigned on 30 September 1918. Hertling, after consulting Vice-Chancellor Friedrich von Payer (FVP), suggested Prince Max of Baden as his successor to the Emperor. However, it took the additional support of Haußmann, Oberst (the liaison between OHL and Foreign Office) and Ludendorff himself, to have Wilhelm II appoint Max as Chancellor of Germany and Minister President of Prussia.
Max was to head a new government based on the majority parties of the Reichstag (SDP, Centre Party and FVP). When Max arrived in Berlin on 1 October he had no idea that he would be asked to approach the Allies about an armistice. Max was horrified and fought against the plan. Moreover, he also admitted openly that he was no politician and that he did not think additional steps towards "parliamentarisation" and democratisation feasible as long as the war continued. Consequently, he did not favour a liberal reform of the constitution. However, Emperor Wilhelm II convinced him to take the post and appointed him on 3 October 1918. The message asking for an armistice went out only on 4 October, not as originally planned on 1 October, hopefully to be accepted by US President Woodrow Wilson.
In office.
Although Max had serious reservations about the conditions under which the OHL was willing to conduct negotiations and tried to interpret Wilson's Fourteen Points in a way most favourable to the German position, he accepted the charge. He appointed a government that for the first time included representatives of the largest party in the Reichstag, the Social Democratic Party of Germany, as state secretaries: Philipp Scheidemann and Gustav Bauer. This was following up on an idea of Ludendorff's and former Foreign Secretary Paul von Hintze's (as the representative of the Hertling cabinet) who had agreed on 29 September that the request for an armistice must not come from the old regime, but from one based on the majority parties. The official reason for appointing a government that was based on a parliamentary majority was to make it harder for the American president to refuse a peace offer. The need to convince Wilson was also the driving factor behind the move towards "parliamentarisation" that was to make the Chancellor and his government answerable to the Reichstag, as they had not been under the Empire so far. Ludendorff, however, was interested in shifting the blame for the lost war to the politicians and to the Reichstag parties.
The Allies were cautious, distrusting Max as a member of a ruling family of Germany. These doubts were intensified by the publication of a personal letter Max had written to Prince Alexander zu Hohenlohe-Schillingsfürst in early 1918, in which he had expressed criticism of "parliamentarisation" and his opposition to the "Friedensresolution" of the Reichstag of July 1917, when a majority had demanded a negotiated peace rather than a peace by victory. President Wilson reacted with reserve to the German initiative and took his time to agree to the request for an armistice, sending three diplomatic notes between 8 October and 23 October. When Ludendorff changed his mind about the armistice and suddenly advocated continued fighting, Max opposed him in a cabinet meeting on 17 October. On 24 October, Ludendorff issued an army order that called Wilson's third note "unacceptable" and called on the troops to fight on. On 25 October, Hindenburg and Ludendorff then ignored explicit instructions by the Chancellor and travelled to Berlin. Max asked for Ludendorff to be dismissed and Wilhelm II agreed. On 26 October, the Emperor told Ludendorff that he had lost his trust. Ludendorff offered his resignation and Wilhelm II accepted.
Whilst trying to move towards an armistice, Max von Baden, advised closely by Hahn (who also wrote his speeches), Haußmann and Walter Simons worked with the representatives of the majority parties in his cabinet (Scheidemann and Bauer for the SPD, Matthias Erzberger, and for the Centre Party, von Payer and, after 14 October, Haußmann for the FVP). Although some of the initiatives were a result of the notes sent by Wilson, they were also in line with the parties' manifestoes: making the Chancellor, his government and the Prussian Minister of War answerable to parliament (Reichstag and "Preußischer Landtag"), introducing a more democratic voting system in the place of the "Dreiklassenwahlrecht" (Three-class franchise) in Prussia, the replacement of the Governor of Alsace-Lorraine with the Mayor of Straßburg, appointing a local deputy from the Centre Party as Secretary of State for Alsace-Lorraine and some other adjustments in government personnel.
Pushed by the social democrats, the government passed a widespread amnesty, under which political prisoners like Karl Liebknecht were released. Under Max von Baden, the bureaucracy, military and political leadership of the old Reich began a cooperation with the leaders of the majority parties and with the individual States of the Reich. This cooperation would have a significant impact on later events during the revolution.
In late October, the Imperial constitution was changed, turning the German Empire into a parliamentary system. However, Wilson's third note seemed to imply that negotiations of an armistice would be dependent on the abdication of Wilhelm II. The government of Chancellor Max von Baden now feared that a military collapse and a socialist revolution at home were becoming likelier with every day that went by. In fact, the government's efforts to secure an armistice were interrupted by the Kiel mutiny which began with events at Wilhelmshaven on 30 October and the outbreak of revolution in Germany in early November. On 1 November, Max wrote to all the ruling Princes of Germany, asking them whether they would approve of an abdication by the Emperor. On 6 November, the Chancellor sent Erzberger to conduct the negotiations with the Allies. Maximilian, seriously ill with Spanish influenza, urged Wilhelm II to abdicate. The "Kaiser", who had fled from revolutionary Berlin to the Spa headquarters of the OHL, despite similar advice by Hindenburg and Ludendorff's successor Wilhelm Groener of the OHL was willing to consider abdication only as Emperor, not as King of Prussia.
Revolution and resignation.
On 7 November, Max met with Friedrich Ebert, leader of the SPD and discussed his plan to go to Spa and convince Wilhelm II to abdicate. He was thinking about setting up Wilhelm's second son as regent. However, the outbreak of the revolution in Berlin prevented Max from implementing his plan. Ebert decided that to keep control of the socialist uprising the Emperor must resign quickly and a new government was required. As the masses gathered in Berlin, at noon on 9 November 1918, Maximilian went ahead and unilaterally announced the abdication, as well as the renunciation of Crown Prince Wilhelm.
Shortly thereafter, Ebert appeared in the Reich Reichskanzlei and demanded that the office of government be handed over to him and the SPD, as that was the only way to keep up law and order. In an unconstitutional move, Max resigned and appointed Ebert as his successor. On the same day, Philipp Scheidemann proclaimed Germany a republic. When Maximilian later visited Ebert to say goodbye before leaving Berlin, Ebert asked him to stay on as regent ("Reichsverweser"). Maximilian refused and, turning his back on politics for good, departed for Baden. Although events had overtaken him during his tenure at the Reichskanzlei and he was not considered a strong Chancellor, Max is seen today as having played a vital role in enabling the transition from the old regime to a democratic government based on the majority parties and the Reichstag. This made the government of Ebert that emerged from the November revolution acceptable to some conservative forces in the bureaucracy and military. They were thus willing to ally themselves with him against the more radical demands by the revolutionaries on the far-left.
Later life and death.
Maximilian spent the rest of his life in retirement. He rejected a mandate to the 1919 Weimar National Assembly, offered to him by the German Democratic politician Max Weber. In 1920, together with Kurt Hahn, he established the Schule Schloss Salem boarding school, which was intended to help educate a new German intellectual elite.
Max also published a number of books, assisted by Hahn: "Völkerbund und Rechtsfriede" (1919), "Die moralische Offensive" (1921) and "Erinnerungen und Dokumente" (1927).
In 1928, following the death of Grand Duke Frederick II, who had been deposed in November 1918 when the German monarchies were abolished, Maximilian became head of the House of Baden, assuming the dynasty's historical title of "margrave". He died at Salem on 6 November the following year.
Children.
Maximilian was married to Princess Marie Louise of Hanover and Cumberland, eldest daughter of Ernest Augustus II of Hanover and Thyra of Denmark on 10 July 1900 in Gmunden, Austria-Hungary. The couple had two children:

</doc>
<doc id="52395" url="https://en.wikipedia.org/wiki?curid=52395" title="Martha Argerich">
Martha Argerich

Martha Argerich (born June 5, 1941) is an Argentine pianist, widely regarded as one of the greatest pianists of the second half of the 20th century.
Early life.
Argerich was born in Buenos Aires, Argentina. Her paternal ancestors were Catalonians based in Buenos Aires since the 18th century. Her maternal grandparents were Jewish immigrants from the Russian Empire, who settled in Colonia Villa Clara in the Entre Ríos province—one of the colonies established by Baron de Hirsch and the Jewish Colonization Association. The provenance of the name Argerich is Catalonia, Spain. She started playing the piano at age three. At the age of five, she moved to teacher Vincenzo Scaramuzza, who stressed to her the importance of lyricism and feeling. Argerich gave her debut concert in 1949 at the age of eight.
The family moved to Europe in 1955, where Argerich studied with Friedrich Gulda in Austria. Juan Perón, then the president of Argentina, made their decision possible by appointing her parents to diplomatic posts in the Argentine Embassy in Vienna. She later studied with Stefan Askenase and Maria Curcio. Argerich also seized opportunities for brief periods of coaching with Madeleine Lipatti (widow of Dinu Lipatti), Abbey Simon, and Nikita Magaloff. In 1957, at sixteen, she won both the Geneva International Music Competition and the Ferruccio Busoni International Competition, within three weeks of each other. It was at the latter that she met Arturo Benedetti Michelangeli, whom she would later seek out for lessons during a personal artistic crisis at the age of twenty, though she only had four lessons with him in a year and a half. Her greatest influence was Gulda, with whom she studied for 18 months.
Professional career.
Argerich rose to international prominence when she won the seventh International Chopin Piano Competition in Warsaw in 1965, at age 24. In that same year, she debuted in the United States in Lincoln Center's Great Performers Series. In 1960, she had made her first commercial recording, which included works by Chopin, Brahms, Ravel, Prokofiev, and Liszt; it received critical acclaim upon its release in 1961. In 1967, she recorded Chopin's Polonaise, Op. 53.
Argerich has often remarked in interviews of feeling "lonely" on stage during solo performances. Since the 1980s, she has staged few solo performances, concentrating instead on concertos and, in particular, chamber music, and collaborating with instrumentalists in sonatas. She is noted especially for her recordings of 20th-century works by composers such as Rachmaninoff, Messiaen and Prokofiev. One notable compilation pairs Rachmaninoff's Piano Concerto No. 3 (recorded in December 1982 with the Radio Symphonie-Orchester Berlin under the direction of Riccardo Chailly) with Tchaikovsky's Piano Concerto No. 1 (February 1980, Symphonieorchester des Bayerischen Rundfunks, Kirill Kondrashin).
Argerich is also famous for her interpretation of Prokofiev's Piano Concerto No. 3, Ravel's Piano Concerto in G, and Bach's Partita No. 2 in C minor, which she has recorded several times and continues to perform.
Argerich has also promoted younger pianists, both through her annual festival and through her appearances as a member of the jury at international competitions. The pianist Ivo Pogorelić was thrust into the musical spotlight partly as a result of Argerich's actions: after he was eliminated in the third round of the 1980 International Chopin Piano Competition in Warsaw, Argerich proclaimed him a "genius" and left the jury in protest. She has supported several artists including Gabriela Montero, Mauricio Vallina, Sergio Tiempo, Gabriele Baldocci, Christopher Falzone and others.
Argerich is president of the International Piano Academy Lake Como and performs each year at the Lugano Festival. She also created and has been General Director of the Argerich Music Festival and Encounter in Beppu, Japan, since 1996.
Her aversion to the press and publicity has resulted in her remaining out of the limelight for most of her career. Nevertheless, she is widely recognized as one of the greatest pianists of her time.
Personal life.
Argerich has been married three times. Her first marriage, to composer-conductor Robert Chen (), and with whom she had a daughter, violinist Lyda Chen-Argerich, ended in 1964. From 1969 to 1973, Argerich was married to Swiss conductor Charles Dutoit, with whom she had a daughter, Annie Dutoit. Argerich continues to record and perform with Dutoit. In the 1970s she was also briefly married to pianist Stephen Kovacevich, with whom she has a daughter, Stéphanie.
In 1990, Argerich was diagnosed with malignant melanoma. After treatment, the cancer went into remission, but there was a recurrence in 1995, eventually metastasizing to her lungs and lymph nodes. Following aggressive treatment at the Providence Saint John's Health Center, which included the removal of part of her lung and use of an experimental vaccine, Argerich's cancer went into remission again. In gratitude, Argerich performed a Carnegie Hall recital benefiting the Institute. , Argerich remains cancer-free.
"Martha Argerich, evening talks", is the award-winning documentary film released in 2002 about Argerich by "Georges Gachot" - (imdb link)
Stéphanie Argerich Blagojevic directed a documentary film about her mother, "Bloody Daughter", based on film shot since her childhood.

</doc>
<doc id="52396" url="https://en.wikipedia.org/wiki?curid=52396" title="Public holiday">
Public holiday

A public holiday, national holiday or legal holiday is a holiday generally established by law and is usually a non-working day during the year.
Sovereign nations and territories observe holidays based on events of significance to their history. For example, Australians celebrate Australia Day.
They vary by country and may vary by year. India leads the list with 21 National Holidays in the year 2015. Cambodia has over 20 days of official public holidays per year. Hong Kong and Egypt have 16 days of holidays per year. The public holidays are generally days of celebration, like the anniversary of a significant historical event, or can be a religious celebration like Christmas. Holidays can land on a specific day of the year, be tied to a certain day of the week in a certain month or follow other calendar systems like the Lunar Calendar.
Solemn ceremonies and children’s festivals take place throughout Turkey on National Sovereignty and Children’s Day, held on April 23 each year. Children take seats in the Turkish Parliament and symbolically govern the country for one day. 
French "Journée de solidarité envers les personnes âgées" ("Day of solidarity with the elderly") is a notable exception. This holiday became a mandatory working day although the French Council of State confirmed it remains a holiday.

</doc>
<doc id="52400" url="https://en.wikipedia.org/wiki?curid=52400" title="Day of Atonement (disambiguation)">
Day of Atonement (disambiguation)

Day of Atonement may refer to:

</doc>
<doc id="52401" url="https://en.wikipedia.org/wiki?curid=52401" title="Hairstyle">
Hairstyle

A hairstyle, hairdo, or haircut refers to the styling of hair, usually on the human scalp. Sometimes, this could also mean an editing of beard hair. The fashioning of hair can be considered an aspect of personal grooming, fashion, and cosmetics, although practical, cultural, and popular considerations also influence some hairstyles. The oldest known depiction of hair braiding dates back about 30,000 years. In ancient civilizations, women's hair was often elaborately and carefully dressed in special ways. In Imperial Rome, women wore their hair in complicated styles. From the time of the Roman Empire until the Middle Ages, most women grew their hair as long as it would naturally grow. During the Roman Empire as well as in the 16th century in the western world, women began to wear their hair in extremely ornate styles. In the later half of the 15th century and on into the 16th century a very high hairline on the forehead was considered attractive. During the 15th and 16th centuries, European men wore their hair cropped no longer than shoulder-length. In the early 17th century male hairstyles grew longer, with waves or curls being considered desirable.
The male wig was pioneered by King Louis XIII of France (1601–1643) in 1624. Perukes or periwigs for men were introduced into the English-speaking world with other French styles in 1660. Late 17th-century wigs were very long and wavy, but became shorter in the mid-18th century, by which time they were normally white. Short hair for fashionable men was a product of the Neoclassical movement. In the early 19th century the male beard, and also moustaches and sideburns, made a strong reappearance. From the 16th to the 19th century, European women's hair became more visible while their hair coverings grew smaller. In the middle of the 18th century the pouf style developed. During the First World War, women around the world started to shift to shorter hairstyles that were easier to manage. In the early 1950s women's hair was generally curled and worn in a variety of styles and lengths. In the 1960s, many women began to wear their hair in short modern cuts such as the pixie cut, while in the 1970s, hair tended to be longer and looser. In both the 1960s and 1970s many men and women wore their hair very long and straight. In the 1980s, women pulled back their hair with scrunchies. During the 1980s, punk hairstyles were adopted by some people.
Prehistory and history.
Throughout times, people have worn their hair in a wide variety of styles, largely determined by the fashions of the culture they live in. Hairstyles are markers and signifiers of social class, age, marital status, racial identification, political beliefs, and attitudes about gender.
In many cultures, often for religious reasons, women's hair is covered while in public, and in some, such as Haredi Judaism or European Orthodox communities, women's hair is shaved or cut very short, and covered with wigs. Only since the end of World War I have women begun to wear their hair short and in fairly natural styles.
Paleolithic.
The oldest known reproduction of hair braiding lies back about 30,000 years: the Venus of Willendorf, now known in academia as the Woman of Willendorf, of a female figurine from the Paleolithic, estimated to have been made between about 28,000 and 25,000 BCE.
The Venus of Brassempouy counts about 25,000 years old and indisputably shows hairstyling.
Bronze Age.
In Bronze Age razors were known and in use by some men, but not on a daily basis since the procedure was rather unpleasant and required resharpening of the tool which reduced its endurance.
Ancient history.
In ancient civilizations, women's hair was often elaborately and carefully dressed in special ways. Women coloured their hair, curled it, and pinned it up (ponytail) in a variety of ways. They set their hair in waves and curls using wet clay, which they dried in the sun and then combed out, or else by using a jelly made of quince seeds soaked in water, or curling tongs and curling irons of various kinds.
Roman Empire and Middle Ages.
Between 27 BC and 102 AD, in Imperial Rome, women wore their hair in complicated styles: a mass of curls on top, or in rows of waves, drawn back into ringlets or braids. Eventually noblewomen's hairstyles grew so complex that they required daily attention from several slaves and a stylist in order to be maintained. The hair was often lightened using wood ash, unslaked lime and sodium bicarbonate, or darkened with copper filings, oak-apples or leeches marinated in wine and vinegar. It was augmented by wigs, hairpieces and pads, and held in place by nets, pins, combs and pomade. Under the Byzantine Empire, noblewomen covered most of their hair with silk caps and pearl nets.
From the time of the Roman Empire until the Middle Ages, most women grew their hair as long as it would naturally grow. It was normally little styled by cutting, as women's hair was tied up on the head and covered on most occasions when outside the home with a snood, kerchief or veil; for an adult woman to wear uncovered and loose hair in the street was often restricted to prostitutes. Braiding and tying the hair was common. In the 16th century, women began to wear their hair in extremely ornate styles, often decorated with pearls, precious stones, ribbons and veils. Women used a technique called "lacing" or "taping," in which cords or ribbons were used to bind the hair around their heads. During this period, most of the hair was braided and hidden under wimples, veils or couvrechefs. In the later half of the 15th century and on into the 16th century a very high hairline on the forehead was considered attractive, and wealthy women frequently plucked out hair at their temples and the napes of their necks, or used depilatory cream to remove it, if it would otherwise be visible at the edges of their hair coverings. Working-class women in this period wore their hair in simple styles.
Early modern history.
Male styles.
During the 15th and 16th centuries, European men wore their hair cropped no longer than shoulder-length, with very fashionable men wearing bangs or fringes. In Italy it was common for men to dye their hair. In the early 17th century male hairstyles grew longer, with waves or curls being considered desirable.
The male wig was supposedly pioneered by King Louis XIII of France (1601–1643) in 1624 when he had prematurely begun to bald. This fashion was largely promoted by his son and successor Louis XIV of France (1638–1715) that contributed to its spread in European and European-influenced countries. The beard had been in a long decline and now disappeared among the upper classes.
Perukes or periwigs for men were introduced into the English-speaking world with other French styles when Charles II was restored to the throne in 1660, following a lengthy exile in France. These wigs were shoulder-length or longer, imitating the long hair that had become fashionable among men since the 1620s. Their use soon became popular in the English court. The London diarist Samuel Pepys recorded the day in 1665 that a barber had shaved his head and that he tried on his new periwig for the first time, but in a year of plague he was uneasy about wearing it:"3rd September 1665: Up, and put on my coloured silk suit, very fine, and my new periwig, bought a good while since, but darst not wear it because the plague was in Westminster when I bought it. And it is a wonder what will be the fashion after the plague is done as to periwigs, for nobody will dare to buy any haire for fear of the infection? That it had been cut off the heads of people dead of the plague."
Late 17th-century wigs were very long and wavy (see George I below), but became shorter in the mid-18th century, by which time they were normally white (George II). A very common style had a single stiff curl running round the head at the end of the hair. By the late 18th-century the natural hair was often powdered to achieve the impression of a short wig, tied into a small tail or "queue" behind (George III).
Short hair for fashionable men was a product of the Neoclassical movement. Classically inspired male hair styles included the Bedford Crop, arguably the precursor of most plain modern male styles, which was invented by the radical politician Francis Russell, 5th Duke of Bedford as a protest against a tax on hair powder; he encouraged his frends to adopt it by betting them they would not. Another influential style (or group of styles) was named by the French after the Roman Emperor Titus, from his busts, with hair short and layered but somewhat piled up on the crown, often with restrained quiffs or locks hanging down; variants are familiar from the hair of both Napoleon and George IV of England. The style was supposed to have been introduced by the actor François-Joseph Talma, who upstaged his wigged co-actors when appearing in productions of works such as Voltaire's "Brutus". In 1799 a Parisian fashion magazine reported that even bald men were adopting Titus wigs, and the style was also worn by women, the "Journal de Paris" reporting in 1802 that "more than half of elegant women were wearing their hair or wig "à la Titus"."
In the early 19th century the male beard, and also moustaches and sideburns, made a strong reappearance, associated with the Romantic movement, and all remained very common until the 1890s, after which younger men ceased to wear them, with World War I, when the majority of men in many countries saw military service, finally despatching the full beard except for older men retaining the styles of their youth, and those affecting a bohemian look. The short military-style moustache remained popular.
Female styles.
From the 16th to the 19th century, European women's hair became more visible while their hair coverings grew smaller, with both becoming more elaborate, and with hairstyles beginning to include ornamentation such as flowers, ostrich plumes, ropes of pearls, jewels, ribbons and small crafted objects such as replicas of ships and windmills. Bound hair was felt to be symbolic of propriety: loosening one's hair was considered immodest and sexual, and sometimes was felt to have supernatural connotations. Red hair was popular, particularly in England during the reign of the red-haired Elizabeth I, and women and aristocratic men used borax, saltpeter, saffron and sulfur powder to dye their hair red, making themselves nauseated and giving themselves headaches and nosebleeds. During this period in Spain and Latin cultures, women wore lace mantillas, often worn over a high comb, and in Buenos Aires, there developed a fashion for extremely large tortoise-shell hair combs called peinetón, which could measure up to three feet in height and width, and which are said by historians to have reflected the growing influence of France, rather than Spain, upon Argentinians.
In the middle of the 18th century the pouf style developed, with women creating volume in the hair at the front of the head, usually with a pad underneath to lift it higher, and ornamented the back with seashells, pearls or gemstones. In 1750, women began dressing their hair with perfumed pomade and powdering it white. Just before World War I, some women began wearing silk turbans over their hair.
Japan.
In the early 1870s, in a shift that historians attribute to the influence of the West, Japanese men began cutting their hair into styles known as jangiri or zangiri (which roughly means "random cropping"). During this period, Asian women were still wearing traditional hairstyles held up with combs, pins and sticks crafted from tortoise, metal, wood and other materials, but in the middle 1880s, upper-class Japanese women began pushing back their hair in the Western style (known as sokuhatsu), or adopting Westernized versions of traditional Japanese hairstyles (these were called yakaimaki, or literally, soirée chignon).
Inter-war years.
During the First World War, women around the world started to shift to shorter hairstyles that were easier to manage. In the 1920s women started for the first time to bob, shingle and crop their hair, often covering it with small head-hugging cloche hats. In Korea, the bob was called "tanbal". Women began marcelling their hair, creating deep waves in it using heated scissor irons. Durable permanent waving became popular also in this period: it was an expensive, uncomfortable and time-consuming process, in which the hair was put in curlers and inserted into a steam or dry heat machine. During the 1930s women began to wear their hair slightly longer, in pageboys, bobs or waves and curls.
During this period, Western men began to wear their hair in ways popularized by movie stars such as Douglas Fairbanks, Jr. and Rudolph Valentino. Men wore their hair short, and either parted on the side or in the middle, or combed straight back, and used pomade, creams and tonics to keep their hair in place. At the beginning of the Second World War and for some time afterwards, men's haircuts grew shorter, mimicking the military crewcut.
During the 1920s and 1930s, Japanese women began wearing their hair in a style called "mimi-kakushi" (literally, "ear hiding"), in which hair was pulled back to cover the ears and tied into a bun at the nape of the neck. Waved or curled hair became increasingly popular for Japanese women throughout this period, and permanent waves, though controversial, were extremely popular. Bobbed hair also became more popular for Japanese women, mainly among actresses and moga, or "cut-hair girls," young Japanese women who followed Westernized fashions and lifestyles in the 1920s.
Post-war years.
After the war, women started to wear their hair in softer, more natural styles. In the early 1950s women's hair was generally curled and worn in a variety of styles and lengths. In the later 1950s, high bouffant and beehive styles, sometimes nicknamed B-52s for their similarity to the bulbous noses of the B-52 Stratofortress bomber, became popular. During this period many women washed and set their hair only once a week, and kept it in place by wearing curlers every night and reteasing and respraying it every morning. In the 1960s, many women began to wear their hair in short modern cuts such as the pixie cut, while in the 1970s, hair tended to be longer and looser. In both the 1960s and 1970s many men and women wore their hair very long and straight. Women straightened their hair through chemical straightening processes, by ironing their hair at home with a clothes iron, or by rolling it up with large empty cans while wet. African-American men and women began wearing their hair naturally (unprocessed) in large Afros, sometimes ornamented with Afro picks made from wood or plastic. By the end of the 1970s the Afro had fallen out of favour among African-Americans, and was being replaced by other natural hairstyles such as corn rows and dreadlocks.
Contemporary hairstyles.
Since the 1970s, women have worn their hair in a wide variety of fairly natural styles. In the 1980s, women pulled back their hair with scrunchies, stretchy ponytail holders made from cloth over fabric bands. Women also often wear glittery ornaments today, as well as claw-style barrettes used to secure ponytails and other upswept or partially upswept hairstyles. Today, women and men can choose from a broad range of hairstyles, but they are still expected to wear their hair in ways that conform to gender norms: in much of the world, men with long hair and women whose hair doesn't appear carefully groomed may face various forms of discrimination, including harassment, social shaming or workplace discrimination. This is somewhat less true of African-American men, who wear their hair in a variety of styles that overlap with those of African-American women, including box braids and cornrows fastened with rubber bands and dreadlocks. However, in the contemporary world of fashion men with long length hair are seen more often these days. Such haircuts are considered to be trendy and attractive.
Defining factors.
A hairstyle's aesthetic considerations may be determined by many factors, such as the subject's physical attributes and desired self-image or the stylist's artistic instincts.
Physical factors include natural hair type and growth patterns, face and head shape from various angles, and overall body proportions; medical considerations may also apply. Self-image may be directed toward conforming to mainstream values (military-style crew cuts or current "fad" hairstyles such as the Dido flip), identifying with distinctively groomed subgroups (e.g., punk hair), or obeying religious dictates (e.g., Orthodox Jewish have payot, Rastafari have Dreadlocks, North India jatas, or the Sikh practice of Kesh), though this is highly contextual and a "mainstream" look in one setting may be limited to a "subgroup" in another.
A hairstyle is achieved by arranging hair in a certain way, occasionally using combs, a blow-dryer, gel, or other products. The practice of styling hair is often called "hairdressing", especially when done as an occupation.
Hairstyling may also include adding accessories (such as headbands or barrettes) to the hair to hold it in place, enhance its ornamental appearance, or partially or fully conceal it with coverings such as a kippa, hijab, tam or turban.
Process.
Hair dressing may include cuts, weaves, coloring, extensions, perms, permanent relaxers, curling, and any other form of styling or texturing.
Length and trimming.
Hair cutting or hair trimming is intended to create or maintain a specific shape and form. Its extent may range from merely trimming the uneven ends of the hair to a uniform length to completely shaving the head.
The overall shape of the hairstyle is usually maintained by trimming it at regular intervals. There are ways to trim one's own hair but usually another person is enlisted to perform the process, as it is difficult to maintain symmetry while cutting hair at the back of one's head. Although trimming enhances the hair's appearance by removing damaged or split ends, it does not promote faster growth or remove all damage along the length of the hair.
Stylists often wash a subject's hair first, so that the hair is cut while still slightly damp. Compared to dry hair, wet hair can be easier to manage in a cut/style situation because the added weight and surface tension of the water cause the strands to stretch downward and cling together along the hair's length, holding a line and making it easier for the stylist to create a form. It is important to note that this method of cutting hair while wet, may be most suitable (or common) for straight hair types. Curly, kinky and other types of hair textures with considerable volume may benefit from cutting while dry, as the hair is in a more natural state and the hair can be cut evenly.
Brushing and combing.
Brushes and combs are used to organize and untangle the hair, encouraging all of the strands to lie in the same direction and removing debris such as lint, dandruff, or hairs that have already shed from their follicles but continue to cling to the other hairs.
There are all manner of detangling tools available in a wide variety of price ranges. Combs come in all shapes and sizes and all manner of materials including plastics, wood, and horn. Similarly, brushes also come in all sizes and shapes, including various paddle shapes. Most benefit from using some form of a wide tooth comb for detangling. Most physicians advise against sharing hair care instruments like combs and clips, to prevent spreading hair conditions like dandruff and head lice.
The historical dictum to brush hair with one hundred strokes every day is somewhat archaic, dating from a time when hair was washed less frequently; the brushstrokes would spread the scalp's natural oils down through the hair, creating a protective effect. Now, however, this does not apply when the natural oils have been washed off by frequent shampoos. Also, hairbrushes are now usually made with rigid plastic bristles instead of the natural boar's bristles that were once standard; the plastic bristles increase the likelihood of actually injuring the scalp and hair with excessively vigorous brushing. However, traditional brushes with boar's bristles are still commonly used among African Americans and those with coarse or kinky textures to soften and lay down curls and waves.
Drying.
Hair dryers speed the drying process of hair by blowing air, which is usually heated, over the wet hair shaft to accelerate the rate of water evaporation.
Excessive heat may increase the rate of shaft-splitting or other damage to the hair. Hair dryer diffusers can be used to widen the stream of air flow so it is weaker but covers a larger area of the hair.
Hair dryers can also be used as a tool to sculpt the hair to a very slight degree. Proper technique involves aiming the dryer such that the air does not blow onto the face or scalp, which can cause burns.
Braiding and updos.
Tight or frequent braiding may pull at the hair roots and cause traction alopecia. Rubber bands with metal clasps or tight clips, which bend the hair shaft at extreme angles, can have the same effect.
If hair is pinned too tightly, or the whole updo slips causing pulling on the hair in the follicle at the hair root, it can cause aggravation to the hair follicle and result in headaches. Although some African-Americans may use braiding extensions (long term braiding hairstyle) as a form of convenience and/or as a reflection of personal style, it is important not to keep the braids up longer than needed to avoid hair breakage or hair loss. Proper braiding technique and maintenance can result in no hair damage even with repeated braid styles. 
Curling and Straightening.
Curling and straightening hair requires the stylist to use a curling rod or a flat iron to get a desired look. These irons use heat to manipulate the hair into a variety of waves, curls and reversing natural curls and temporarily straightening the hair. Straightening or even curling hair can damage it due to direct heat from the iron and applying chemicals afterwards to keep its shape.
There are irons that have a function to straighten or curl hair even when its damp (from showering or wetting the hair), but this requires more heat than the average iron(temperatures can range from 300-450 degrees). Heat protection sprays, and hair repairing shampoos and conditioners can protect the hair from damage caused by the direct heat from the irons.
Industry.
Hair styling is a major world industry, from the salon itself to products, advertising, and even magazines on the subject. In the United States, most hairstylists are licensed after obtaining training at a cosmetology or beauty school.
In recent years, competitive events for professional stylists have grown in popularity. Stylists compete on deadline to create the most elaborate hairstyle using props, lights and other accessories.
Tools.
Styling tools may include hair irons (including flat, curling, and crimping irons), hair dryers, and hair rollers. Hair dressing might also include the use of hair product to add texture, shine, curl, volume or hold to a particular style. Hairpins are also used when creating particular hairstyles. Their uses and designs vary over different cultural backgrounds.
Products.
Styling products aside from shampoo and conditioner are many and varied. Leave-in conditioner, conditioning treatments, mousse, gels, lotions, waxes, creams, clays, serums, oils, and sprays are used to change the texture or shape of the hair, or to hold it in place in a certain style. Applied properly, most styling products will not damage the hair apart from drying it out; most styling products contain alcohols, which can dissolve oils. Many hair products contain chemicals which can cause build-up, resulting in dull hair or a change in perceived texture.
Wigs.
Care of human or other natural hair wigs is similar to care of a normal head of hair in that the wig can be brushed, styled, and kept clean using haircare products.
Synthetic wigs are usually made from a fine fiber that mimics human hair. This fiber can be made in almost any color and hairstyle, and is often glossier than human hair. However, this fiber is sensitive to heat and cannot be styled with flat irons or curling irons. There is a newer synthetic fiber that can take heat up to a certain temperature.
Human hair wigs can be styled with heat, and they must be brushed only when dry. "Synthetic" and human hair wigs should be brushed dry before shampooing to remove tangles. To clean the wig, the wig should be dipped into a container with water and mild shampoo, then dipped in clear water and moved up and down to remove excess water. The wig must then be air dried naturally into its own hairstyle.Proper maintenance can make a human hair wig last for many years.
Functional and decorative ornaments.
There are many options to adorn and arrange the hair. Hairpins, clasps, barrettes, headbands, ribbons, rubber bands, scrunchies, and combs can be used to achieve a variety of styles. There are also many decorative ornaments that, while they may have clasps to affix them to the hair, are used solely for appearance and do not aid in keeping the hair in place. In India for example, the Gajra (flower garland) is common there are heaps on hairstyles.
Social and cultural implications.
Gender.
At most times in most cultures, men have worn their hair in styles that are different from women's. American sociologist Rose Weitz once wrote that the most widespread cultural rule about hair is that women's hair must differ from men's hair. An exception is the men and women living in the Orinoco-Amazon Basin, where traditionally both genders have worn their hair cut into a bowl shape. In Western countries in the 1960s, both young men and young women wore their hair long and natural, and since then it has become more common for men to grow their hair. During most periods in human history when men and women wore similar hairstyles, as in the 1920s and 1960s, it has generated significant social concern and approbation.
Religion.
Cutting off one's hair is often associated with religious faith: Catholic nuns often cut their hair very short, and men who joined Catholic monastic orders in the eighth century adopted what was known as the tonsure, which involved shaving the tops of their heads and leaving a ring of hair around the bald crown. Many Buddhists, Hajj pilgrims and Vaisnavas, especially members of the Hare Krishna movement who are "brahmacharis" or "sannyasis", shave their heads. Some Hindu and most Buddhist monks and nuns shave their heads upon entering their order, and Korean Buddhist monks and nuns have their heads shaved every 15 days. Adherents of Sikhism are required to wear their hair unshorn. Women usually wear it in a braid or a bun and men cover it with a turban.
Marital status.
In the 1800s, American women started wearing their hair up when they became ready to get married. Among the Fulani people of west Africa, unmarried women wear their hair ornamented with small amber beads and coins, while married women wear large amber ornaments. Marriage is signified among the Toposa women of South Sudan by wearing the hair in many small pigtails. Unmarried Hopi women have traditionally worn a "butterfly" hairstyle characterized by a twist or whorl of hair at each side of the face.
Life transitions.
In many cultures, including Hindu culture and among the Wayana people of the Guiana highlands, young people have historically shaved off their hair to denote coming-of-age. Women in India historically have signified adulthood by switching from wearing two braids to one. Among the Rendille of north-eastern Kenya and the Tchikrin people of the Brazilian rainforest, both men and women shave their heads after the death of a close family member. When a man died in ancient Greece, his wife cut off her hair and buried it with him, and in Hindu families, the chief mourner is expected to shave his or her head 3 days after the death.
Social class.
Throughout history, hair has been a signifier of social class.
Upper-class people have always used their hairstyles to signal wealth and status. Wealthy Roman women wore complex hairstyles that needed the labours of several people to maintain them, and rich people have also often chosen hairstyles that restricted or burdened their movement, making it obvious that they did not need to work. Wealthy people's hairstyles used to be at the cutting edge of fashion, setting the styles for the less wealthy. But today, the wealthy are generally observed to wear their hair in conservative styles that date back decades prior.
Middle-class hairstyles tend to be understated and professional. Middle-class people aspire to have their hair look healthy and natural, implying that they have the resources to live a healthy lifestyle and take good care of themselves.
Historically, working-class people's haircuts have tended to be practical and simple. Working-class men have often shaved their heads or worn their hair close-cropped, and working-class women have typically pulled their hair up and off their faces in simple styles. However, today, working-class people often have more elaborate and fashion-conscious hairstyles than other social classes. Many working-class Mexican men in American cities wear their hair in styles like the Mongolian (shaved except for a tuft of hair at the nape of the neck) or the rat tail (crewcut on top, tuft at the nape), and African-Americans often wear their hair in complex patterns of box braids and cornrows, fastened with barrettes and beads, and sometimes including shaved sections or bright colour. Sociologists say these styles are an attempt to express individuality and presence in the face of social denigration and invisibility.
Haircut in space.
Haircuts also occur in space at the International Space Station. During the various Expeditions astronauts use hair clippers attached to vacuum devices for grooming their colleagues so that the cut hair will not drift inside the weightless environment of the space station and become a nuisance to the astronauts or a hazard to the sensitive equipment installations inside the station.
Haircutting in space was also used for charitable purposes in the case of astronaut Sunita Williams who obtained such a haircut by fellow astronaut Joan Higginbotham inside the International Space Station. Sunita's ponytail was brought back to earth with the STS-116 crew and was donated to Locks of Love.

</doc>
<doc id="52404" url="https://en.wikipedia.org/wiki?curid=52404" title="24th century BC">
24th century BC

The 24th century BC is a century which lasted from the year 2400 BC to 2301 BC.
In popular culture.
In modern Korean national mythology, the character Dangun, whose mother was originally a bear, founded the state Gojoseon in 2333 BC and ruled it for about 2000 years. Some Koreans think of it as the earliest Korean state and of Dangun as the ancestor of Koreans, and from 1948 until December 1961, the Republic of Korea officially reckoned years by adding 2333 to the Common Era year. The year 2333 BC and the related myth are sometimes presented matter-of-factly as "history" rather than "mythology" in Korea.

</doc>
<doc id="52412" url="https://en.wikipedia.org/wiki?curid=52412" title="2022">
2022


</doc>
<doc id="52413" url="https://en.wikipedia.org/wiki?curid=52413" title="2023">
2023


</doc>
<doc id="52414" url="https://en.wikipedia.org/wiki?curid=52414" title="42 BC">
42 BC

__NOTOC__
Year 42 BC was either a common year starting on Monday, Tuesday or Wednesday or a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar (the sources differ, see leap year error for further information) and a common year starting on Tuesday of the Proleptic Julian calendar. At the time, it was known as the Year of the Consulship of Lepidus and Plancus (or, less frequently, year 712 "Ab urbe condita"). The denomination 42 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52415" url="https://en.wikipedia.org/wiki?curid=52415" title="50 BC">
50 BC

__NOTOC__
Year 50 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Paullus and Marcellus (or, less frequently, year 704 "Ab urbe condita"). The denomination 50 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52416" url="https://en.wikipedia.org/wiki?curid=52416" title="2024">
2024

2024 will be a leap year

</doc>
<doc id="52417" url="https://en.wikipedia.org/wiki?curid=52417" title="2025">
2025


</doc>
<doc id="52418" url="https://en.wikipedia.org/wiki?curid=52418" title="2070s">
2070s

The 2070s is a decade of the Gregorian calendar that will begin on January 1, 2070 and will end on December 31, 2079.

</doc>
<doc id="52419" url="https://en.wikipedia.org/wiki?curid=52419" title="52 BC">
52 BC

__NOTOC__
Year 52 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Pompeius and Scipio (or, less frequently, year 702 "Ab urbe condita"). The denomination 52 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52420" url="https://en.wikipedia.org/wiki?curid=52420" title="280s BC">
280s BC


</doc>
<doc id="52421" url="https://en.wikipedia.org/wiki?curid=52421" title="270s BC">
270s BC


</doc>
<doc id="52422" url="https://en.wikipedia.org/wiki?curid=52422" title="260s BC">
260s BC

Deaths.
Euclid

</doc>
<doc id="52423" url="https://en.wikipedia.org/wiki?curid=52423" title="250s BC">
250s BC


</doc>
<doc id="52424" url="https://en.wikipedia.org/wiki?curid=52424" title="240s BC">
240s BC


</doc>
<doc id="52425" url="https://en.wikipedia.org/wiki?curid=52425" title="230s BC">
230s BC


</doc>
<doc id="52426" url="https://en.wikipedia.org/wiki?curid=52426" title="210s BC">
210s BC

Deaths.
210 BC

</doc>
<doc id="52428" url="https://en.wikipedia.org/wiki?curid=52428" title="160s BC">
160s BC


</doc>
<doc id="52429" url="https://en.wikipedia.org/wiki?curid=52429" title="170s BC">
170s BC


</doc>
<doc id="52430" url="https://en.wikipedia.org/wiki?curid=52430" title="180s BC">
180s BC


</doc>
<doc id="52431" url="https://en.wikipedia.org/wiki?curid=52431" title="190s BC">
190s BC


</doc>
<doc id="52432" url="https://en.wikipedia.org/wiki?curid=52432" title="Xanthine">
Xanthine

Xanthine ( or ; archaically xanthic acid) (3,7-dihydro-purine-2,6-dione), is a purine base found in most human body tissues and fluids and in other organisms. A number of stimulants are derived from xanthine, including caffeine and theobromine.
Xanthine is a product on the pathway of purine degradation.
Xanthine is subsequently converted to uric acid by the action of the xanthine oxidase enzyme.
Studies reported in 2008, based on 12C/13C isotopic ratios of organic compounds found in the Murchison meteorite, suggested that xanthine and related chemicals, including the RNA component uracil, were formed extraterrestrially. In August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting xanthine and related organic molecules, including the DNA and RNA components adenine and guanine, were found in outer space.
Pathology.
People with the rare genetic disorder xanthinuria lack sufficient xanthine oxidase and cannot convert xanthine to uric acid.
Clinical significance of xanthine derivatives.
Derivatives of xanthine (known collectively as xanthines) are a group of alkaloids commonly used for their effects as mild stimulants and as bronchodilators, notably in the treatment of asthma symptoms. In contrast to other, more potent stimulants like sympathomimetic amines, xanthines mainly act to oppose the actions of the sleepiness-inducing adenosine, and increase alertness in the central nervous system. They also stimulate the respiratory centre, and are used for treatment of infantile apnea. Due to widespread effects, the therapeutic range of xanthines is narrow, making them merely a second-line asthma treatment. The therapeutic level is 10-20 micrograms/mL blood; signs of toxicity include tremor, nausea, nervousness, and tachycardia/arrhythmia.
Methylated xanthines (methylxanthines), which include caffeine, aminophylline, IBMX, paraxanthine, pentoxifylline, theobromine, and theophylline, affect not only the airways but stimulate heart rate, force of contraction, and cardiac arrhythmias at high concentrations. In high doses they can lead to convulsions that are resistant to anticonvulsants. Methylxanthines induce acid and pepsin secretions in the GI tract. Methylxanthines are metabolized by cytochrome P450 in the liver.
These drugs act as both:
But different analogues show varying potency at the numerous subtypes, and a wide range of synthetic xanthines (some nonmethylated) have been developed searching for compounds with greater selectivity for phosphodiesterase enzyme or adenosine receptor subtypes. Xanthines are also found very rarely as constituents of nucleic acids.

</doc>
<doc id="52433" url="https://en.wikipedia.org/wiki?curid=52433" title="2026">
2026


</doc>
<doc id="52434" url="https://en.wikipedia.org/wiki?curid=52434" title="2027">
2027


</doc>
<doc id="52435" url="https://en.wikipedia.org/wiki?curid=52435" title="Tait's conjecture">
Tait's conjecture

In mathematics, Tait's conjecture states that "Every 3-connected planar cubic graph has a Hamiltonian cycle (along the edges) through all its vertices". It was proposed by and disproved by , who constructed a counterexample with 25 faces, 69 edges and 46 vertices. Several smaller counterexamples, with 21 faces, 57 edges and 38 vertices, were later proved minimal by .
The condition that the graph be 3-regular is necessary due to polyhedra such as the rhombic dodecahedron, which forms a bipartite graph with six degree-four vertices on one side and eight degree-three vertices on the other side; because any Hamiltonian cycle would have to alternate between the two sides of the bipartition, but they have unequal numbers of vertices, the rhombic dodecahedron is not Hamiltonian. 
The conjecture was significant, because if true, it would have implied the four color theorem: as Tait described, the four-color problem is equivalent to the problem of finding 3-edge-colorings of bridgeless cubic planar graphs. In a Hamiltonian cubic planar graph, such an edge coloring is easy to find: use two colors alternately on the cycle, and a third color for all remaining edges. Alternatively, a 4-coloring of the faces of a Hamiltonian cubic planar graph may be constructed directly, using two colors for the faces inside the cycle and two more colors for the faces outside.
Tutte's counterexample.
Tutte's fragment.
The key to this counter-example is what is now known as Tutte's fragment, shown on the right.
If this fragment is part of a larger graph, then any Hamiltonian cycle through the graph must go in or out of the top vertex (and either one of the lower ones). It cannot go in one lower vertex and out the other.
The counterexample.
The fragment can then be used to construct the non-Hamiltonian Tutte graph, by putting
together three such fragments as shown on the picture. The "compulsory" edges of the fragments, that must be part of any Hamiltonian path through the fragment, are connected at the central vertex; because any cycle can use only two of these three edges, there can be no Hamiltonian cycle.
The resulting Tutte graph is 3-connected and planar, so by Steinitz' theorem it is the graph of a polyhedron. In total it has 25 faces, 69 edges and 46 vertices.
It can be realized geometrically from a tetrahedron (the faces of which correspond to the four large faces in the drawing, three of which are between pairs of fragments and the fourth of which forms the exterior) by multiply truncating three of its vertices.
Smaller counterexamples.
As show, there are exactly six 38-vertex non-Hamiltonian polyhedra that have nontrivial three-edge cuts. They are formed by replacing two of the vertices of a pentagonal prism by the same fragment used in Tutte's example.
References.
Partly based on sci.math posting by Bill Taylor, used by permission."

</doc>
<doc id="52436" url="https://en.wikipedia.org/wiki?curid=52436" title="51 BC">
51 BC

__NOTOC__
Year 51 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Marcellus and Sulpicius (or, less frequently, year 703 "Ab urbe condita"). The denomination 51 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="52437" url="https://en.wikipedia.org/wiki?curid=52437" title="99 BC">
99 BC

__NOTOC__
Year 99 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Antonius and Albinus (or, less frequently, year 655 "Ab urbe condita"). The denomination 99 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52438" url="https://en.wikipedia.org/wiki?curid=52438" title="53 BC">
53 BC

__NOTOC__
Year 53 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Messalla and Calvinus (or, less frequently, year 701 "Ab urbe condita"). The denomination 53 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Armenia.
</onlyinclude>

</doc>
<doc id="52439" url="https://en.wikipedia.org/wiki?curid=52439" title="433">
433

__NOTOC__
Year 433 (CDXXXIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Maximus (or, less frequently, year 1186 "Ab urbe condita"). The denomination 433 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52440" url="https://en.wikipedia.org/wiki?curid=52440" title="435">
435

__NOTOC__
Year 435 (CDXXXV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Valentinianus (or, less frequently, year 1188 "Ab urbe condita"). The denomination 435 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52442" url="https://en.wikipedia.org/wiki?curid=52442" title="447">
447

__NOTOC__
Year 447 (CDXLVII) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Calepius and Ardabur (or, less frequently, year 1200 "Ab urbe condita"). The denomination 447 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52443" url="https://en.wikipedia.org/wiki?curid=52443" title="449">
449

__NOTOC__
Year 449 (CDXLIX) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Astyrius and Romanus (or, less frequently, year 1202 "Ab urbe condita"). The denomination 449 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52444" url="https://en.wikipedia.org/wiki?curid=52444" title="460">
460

__NOTOC__
Year 460 (CDLX) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Magnus and Apollonius (or, less frequently, year 1213 "Ab urbe condita"). The denomination 460 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52445" url="https://en.wikipedia.org/wiki?curid=52445" title="469">
469

__NOTOC__
Year 469 (CDLXIX) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Marcianus and Zeno (or, less frequently, year 1222 "Ab urbe condita"). The denomination 469 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52446" url="https://en.wikipedia.org/wiki?curid=52446" title="882">
882

__NOTOC__
Year 882 (DCCCLXXXII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52447" url="https://en.wikipedia.org/wiki?curid=52447" title="European colonization of the Americas">
European colonization of the Americas

European colonization of the Americas began as early as the 10th - 11th century, when West Norse sailors explored and briefly settled limited areas on the shores of present-day Canada. These Vikings were Norwegians who had settled Iceland, discovered Greenland, then sailed up the Arctic region of North America alongside Greenland, and down alongside Canada to explore and settle. According to Icelandic Sagas, violent conflicts with the indigenous population ultimately made the Norse abandon those settlements.
Extensive European colonization began in 1492, when a Spanish expedition headed by Christopher Columbus sailed west to find a new trade route to the Far East but inadvertently landed in what came to be known to Europeans as the "New World". European conquest, large-scale exploration, colonization and industrial development soon followed. Columbus' first two voyages (1492–93) reached the Bahamas and various Caribbean islands, including Hispaniola, Puerto Rico and Cuba. In 1497, sailing from Bristol on behalf of England, John Cabot landed on the North American coast, and a year later, Columbus's third voyage reached the South American coast. As the sponsor of Christopher Columbus's voyages, Spain was the first European power to settle and colonize the largest areas, from North America and the Caribbean to the southern tip of South America. Spanish cities were founded as early as 1496 with Santo Domingo in today's Dominican Republic.
Other powers such as France also founded colonies in the Americas: in eastern North America, a number of Caribbean islands, and small coastal parts of South America. Portugal colonized Brazil, tried colonizing of the coasts of present-day Canada, and settled for extended periods northwest (on the east bank) of the River Plate. The Age of Exploration was the beginning of territorial expansion for several European countries. Europe had been preoccupied with internal wars, and was slowly recovering from the loss of population caused by the bubonic plague; thus the rapid rate at which it grew in wealth and power was unforeseeable in the early 15th century.
Eventually, the entire Western Hemisphere came under the ostensible control of European governments, leading to profound changes to its landscape, population, and plant and animal life. In the 19th century alone over 50 million people left Europe for the Americas. The post-1492 era is known as the period of the Columbian Exchange, a dramatically widespread exchange of animals, plants, culture, human populations (including slaves), communicable disease, and ideas between the American and Afro-Eurasian hemispheres following Columbus's voyages to the Americas.
Norse trans-oceanic contact.
Norse journeys to Greenland and Canada are supported by historical and archaeological evidence. A Norse colony in Greenland was established in the late 10th century, and lasted until the mid 15th century, with court and parliament assemblies ("þing") taking place at Brattahlíð and a bishop located at Garðar. The remains of a Norse settlement at L'Anse aux Meadows in Newfoundland, Canada was discovered in 1960 and is dated to around the year 1000 (carbon dating estimate 990–1050 CE), L'Anse aux Meadows is the only site widely accepted as evidence of pre-Columbian trans-oceanic contact. It was named a World Heritage site by UNESCO in 1978. It is also notable for its possible connection with the attempted colony of Vinland, established by Leif Erikson around the same period or, more broadly, with the West Norse colonization of the Americas.
Early conquests, claims, and colonies.
[[File:Non-Native-American-Nations-Territorial-Claims-over-NAFTA-countries-1750-2008.gif|right|300px|thumb|Territorial evolution of North America of non-native nation states from 1750 to 2008.
More animated maps:
]]
Early explorations and conquests were made by the Spanish and the Portuguese immediately following their own final reconquest of Iberia in 1492. In the 1494 Treaty of Tordesillas, ratified by the Pope, these two kingdoms divided the entire non-European world into two areas of exploration and colonization, with a north to south boundary that cut through the Atlantic Ocean and the eastern part of present-day Brazil. Based on this treaty and on early claims by Spanish explorer Vasco Núñez de Balboa, discoverer of the Pacific Ocean in 1513, the Spanish conquered large territories in North, Central and South America.
Spanish conquistador Hernán Cortés took over the Aztec Kingdom and Francisco Pizarro conquered the Inca Empire. As a result, by the mid-16th century, the Spanish Crown had gained control of much of western South America, Central America and southern North America, in addition to its earlier Caribbean territories. Over this same timeframe, Portugal claimed lands in North America (Canada) and colonized much of eastern South America, naming it Santa Cruz and Brazil.
Other European nations soon disputed the terms of the Treaty of Tordesillas. England and France attempted to plant colonies in the Americas in the 16th century, but these failed. England and France succeeded in establishing permanent colonies in the following century, along with the Dutch Republic. Some of these were on Caribbean islands, which had often already been conquered by the Spanish or depopulated by disease, while others were in eastern North America, which had not been colonized by Spain north of Florida.
Early European possessions in North America included Spanish Florida, Spanish New Mexico, the English colonies of Virginia (with its North Atlantic offshoot, Bermuda) and New England, the French colonies of Acadia and Canada, the Swedish colony of New Sweden, and the Dutch New Netherland. In the 18th century, Denmark–Norway revived its former colonies in Greenland, while the Russian Empire gained a foothold in Alaska. Denmark-Norway would later make several claims in the Caribbean, starting in the 1600s.
As more nations gained an interest in the colonization of the Americas, competition for territory became increasingly fierce. Colonists often faced the threat of attacks from neighboring colonies, as well as from indigenous tribes and pirates.
Early state-sponsored colonists.
The first phase of well-financed European activity in the Americas began with the Atlantic Ocean crossings of Christopher Columbus (1492–1504), sponsored by Spain, whose original attempt was to find a new route to India and China, known as "the Indies". He was followed by other explorers such as John Cabot, who was sponsored by England and reached Newfoundland. Pedro Álvares Cabral reached Brazil and claimed it for Portugal.
Amerigo Vespucci, working for Portugal in voyages from 1497 to 1513, established that Columbus had reached a new set of continents. Cartographers still use a Latinized version of his first name, "America", for the two continents. Other explorers included Giovanni da Verrazzano, sponsored by France in 1524; the Portuguese João Vaz Corte-Real in Newfoundland; João Fernandes Lavrador, Gaspar and Miguel Corte-Real and João Álvares Fagundes, in Newfoundland, Greenland, Labrador, and Nova Scotia (from 1498 to 1502, and in 1520); Jacques Cartier (1491–1557), Henry Hudson (1560s-1611), and Samuel de Champlain (1567–1635), who explored Canada.
In 1513, Vasco Núñez de Balboa crossed the Isthmus of Panama and led the first European expedition to see the Pacific Ocean from the west coast of the New World. In an action with enduring historical import, Balboa claimed the Pacific Ocean and all the lands adjoining it for the Spanish Crown. It was 1517 before another expedition, from Cuba, visited Central America, landing on the coast of Yucatán in search of slaves.
These explorations were followed, notably in the case of Spain, by a phase of conquest: The Spaniards, having just finished the "Reconquista" of Spain from Muslim rule, were the first to colonize the Americas, applying the same model of governing to the former "Al-Andalus" as to their territories of the New World.
Ten years after Columbus's discovery, the administration of Hispaniola was given to Nicolás de Ovando of the Order of Alcántara, founded during the "Reconquista". As in the Iberian Peninsula, the inhabitants of Hispaniola were given new landmasters, while religious orders handled the local administration. Progressively the "encomienda" system, which granted tribute (access to indigenous labor and taxation) to European settlers, was set in place.
A relatively common misconception is that a small number of "conquistadores" conquered vast territories, aided only by disease epidemics and their powerful caballeros. In fact, recent archaeological excavations have suggested a vast Spanish-Indian alliance numbering in the hundreds of thousands. Hernán Cortés eventually conquered Mexico with the help of Tlaxcala in 1519-1521, while the conquest of the Inca was carried out by some 40,000 Incan renegades led by Francisco Pizarro in between 1532 and 1535.
Over the 1st century and a half after Columbus's voyages, the native population of the Americas plummeted by an estimated 80% (from around 50 million in 1492 to eight million in 1650), mostly by outbreaks of Old World diseases.
In 1532, Charles V, Holy Roman Emperor sent a vice-king to Mexico, Antonio de Mendoza, in order to prevent Cortes' independentist drives, who definitively returned to Spain in 1540. Two years later, Charles V signed the New Laws (which replaced the Laws of Burgos of 1512) prohibiting slavery and the "repartimientos", but also claiming as his own all the American lands and all of the indigenous people as his own subjects.
When in May 1493, the Pope Alexander VI issued the "Inter caetera" bull granting the new lands to the Kingdom of Spain, he requested in exchange an evangelization of the people. Thus, during Columbus's second voyage, Benedictine friars accompanied him, along with twelve other priests. As slavery was prohibited between Christians, and could only be imposed in non-Christian prisoners of war or on men already sold as slaves, the debate on Christianization was particularly acute during the 16th century. In 1537, the papal bull "Sublimis Deus" definitively recognized that Native Americans possessed souls, thus prohibiting their enslavement, without putting an end to the debate. Some claimed that a native who had rebelled and then been captured could be enslaved nonetheless.
Later, the Valladolid debate between the Dominican priest Bartolomé de Las Casas and another Dominican philosopher Juan Ginés de Sepúlveda was held, with the former arguing that Native Americans were beings doted with souls, as all other human beings, while the latter argued to the contrary and justified their enslavement.
The process of Christianization was at first violent: when the first Franciscans arrived in Mexico in 1524, they burned the places dedicated to pagan cult, alienating much of the local population. In the 1530s, they began to adapt Christian practices to local customs, including the building of new churches on the sites of ancient places of worship, leading to a mix of Old World Christianity with local religions. The Spanish Roman Catholic Church, needing the natives' labor and cooperation, evangelized in Quechua, Nahuatl, Guaraní and other Native American languages, contributing to the expansion of these indigenous languages and equipping some of them with writing systems. One of the first primitive schools for Native Americans was founded by Fray Pedro de Gante in 1523.
To reward their troops, the "Conquistadores" often allotted Indian towns to their troops and officers. Black African slaves were introduced to substitute for Native American labor in some locations—including the West Indies, where the indigenous population was nearing extinction on many islands.
During this time, the Portuguese gradually switched from an initial plan of establishing trading posts to extensive colonization of what is now Brazil. They imported millions of slaves to run their plantations. The Portuguese and Spanish royal governments expected to rule these settlements and collect at least 20% of all treasure found (the "Quinto Real" collected by the "Casa de Contratación"), in addition to collecting all the taxes they could. By the late 16th century American silver accounted for one-fifth of Spain's total budget. In the 16th century perhaps 240,000 Europeans entered American ports.
The search for riches.
Inspired by the Spanish riches from colonies founded upon the conquest of the Aztecs, Incas, and other large Native American populations in the 16th century, the first Englishmen to settle permanently in America hoped for some of the same rich discoveries when they established their first permanent settlement in Jamestown, Virginia in 1607. They were sponsored by common stock companies such as the chartered Virginia Company financed by wealthy Englishmen who exaggerated the economic potential of this new land. The main purpose of this colony was the hope of finding gold.
It took strong leaders, like John Smith, to convince the colonists of Jamestown that searching for gold was not taking care of their immediate needs for food and shelter and the biblical principle that "he who will not work shall not eat" (see 2 Thessalonians 3). The extremely high mortality rate was quite distressing and cause for despair among the colonists. Tobacco later became a cash crop, with the work of John Rolfe and others, for export and the sustaining economic driver of Virginia and the neighboring colony of Maryland.
From the beginning of Virginia's settlements in 1587 until the 1680s, the main source of labor and a large portion of the immigrants were indentured servants looking for new life in the overseas colonies. During the 17th century, indentured servants constituted three-quarters of all European immigrants to the Chesapeake region. Most of the indentured servants were teenagers from England with poor economic prospects at home. Their fathers signed the papers that gave them free passage to America and an unpaid job until they became of age. They were given food, clothing, housing and taught farming or household skills. American landowners were in need of laborers and were willing to pay for a laborer’s passage to America if they served them for several years. By selling passage for five to seven years worth of work they could then start out on their own in America. Many of the migrants from England died in the first few years.
Economic advantage also prompted the Darien Scheme, an ill-fated venture by the Kingdom of Scotland to settle the Isthmus of Panama in the late 1690s. The Darien Scheme aimed to control trade through that part of the world and thereby promote Scotland into a world trading power. However, it was doomed by poor planning, short provisions, weak leadership, lack of demand for trade goods, and devastating disease. The failure of the Darien Scheme was one of the factors that led the Kingdom of Scotland into the Act of Union 1707 with the Kingdom of England creating the united Kingdom of Great Britain and giving Scotland commercial access to English, now British, colonies.
In the French colonial regions, the focus of economy was on sugar plantations in Caribbean. In Canada the fur trade with the natives was important. About 16,000 French men and women became colonizers. The great majority became subsistence farmers along the St. Lawrence River. With a favorable disease environment and plenty of land and food, their numbers grew exponentially to 65,000 by 1760. Their colony was taken over by Britain in 1760, but social, religious, legal, cultural and economic changes were few in a society that clung tightly to its recently formed traditions.
Religious immigration.
Roman Catholics were the first major religious group to immigrate to the New World, as settlers in the colonies of Portugal and Spain (and later, France) were required to belong to that faith. English and Dutch colonies, on the other hand, tended to be more religiously diverse. Settlers to these colonies included Anglicans, Dutch Calvinists, English Puritans, English Catholics, Scottish Presbyterians, French Huguenots, German and Swedish Lutherans, as well as Quakers, Mennonites, Amish, Moravians and Jews of various nationalities.
Many groups of colonists went to the Americas searching for the right to practice their religion without persecution. The Protestant Reformation of the 16th century broke the unity of Western Christendom and led to the formation of numerous new religious sects, which often faced persecution by governmental authorities. In England, many people came to question the organization of the Church of England by the end of the 16th century. One of the primary manifestations of this was the Puritan movement, which sought to "purify" the existing Church of England of its many residual Catholic rites that they believed had no mention in the Bible.
A strong believer in the notion of rule by divine right, Charles I, King of England and Scotland, persecuted religious dissenters. Waves of repression led to the migration of about 20,000 Puritans to New England between 1629 and 1642, where they founded multiple colonies. Later in the century, the new Pennsylvania colony was given to William Penn in settlement of a debt the king owed his father. Its government was set up by William Penn in about 1682 to become primarily a refuge for persecuted English Quakers; but others were welcomed. Baptists, Quakers and German and Swiss Protestants flocked to Pennsylvania. The lure of cheap land, religious freedom and the right to improve themselves with their own hand was very attractive.
Forced immigration and enslavement.
Slavery was a common practice in the Americas prior to the arrival of Europeans, as different American Indian groups captured and held other tribes' members as slaves. Many of these captives were forced to undergo human sacrifice in Amerindian civilizations such as the Aztecs. In response to some enslavement of natives in the Caribbean during the early years, the Spanish Crown passed a series of laws prohibiting slavery as early as 1512. A new stricter set of laws was passed in 1542, called the "New Laws of the Indies for the Good Treatment and Preservation of Indians", or simply New Laws. These were created to prevent the exploitation of the indigenous peoples by the "encomenderos" or landowners, by strictly limiting their power and dominion. This helped curb Indian slavery considerably, though not completely. Later, with the arrival of other European colonial powers in the New World, the enslavement of native populations increased, as these empires lacked legislation against slavery until decades later. The population of indigenous peoples declined (mostly from European diseases, but also from forced exploitation and atrocities). Later, native workers were replaced by Africans imported through a large commercial slave trade.
By the 18th century, the overwhelming number of black slaves was such that Amerindian slavery was less commonly used. Africans, who were taken aboard slave ships to the Americas, were primarily obtained from their African homelands by coastal tribes who captured and sold them. Europeans traded for slaves with the slave capturers of the local native African tribes in exchange for rum, guns, gunpowder, and other manufactures.
Slavery.
The total slave trade to islands in the Caribbean, Brazil, Mexico and to the United States is estimated to have involved 12 million Africans. The vast majority of these slaves went to sugar colonies in the Caribbean and to Brazil, where life expectancy was short and the numbers had to be continually replenished. At most about 600,000 African slaves were imported into the U.S., or 5% of the 12 million slaves brought across from Africa. Life expectancy was much higher in the U.S. (because of better food, less disease, lighter work loads, and better medical care) so the numbers grew rapidly by excesses of births over deaths, reaching 4 million by the 1860 Census. From 1770 until 1860, the rate of natural growth of North American slaves was much greater than for the population of any nation in Europe, and was nearly twice as rapid as that of England.
Slaves imported to the Thirteen colonies/United States by time period:
Disease and indigenous population loss.
The European lifestyle included a long history of sharing close quarters with domesticated animals such as cows, pigs, sheep, goats, horses, and various domesticated fowl, which had resulted in epidemic diseases unknown in the Americas. Thus the large-scale contact with Europeans after 1492 introduced novel germs to the indigenous people of the Americas.
Epidemics of smallpox (1518, 1521, 1525, 1558, 1589), typhus (1546), influenza (1558), diphtheria (1614) and measles (1618) swept the Americas subsequent to European contact, killing between 10 million and 100 million people, up to 95% of the indigenous population of the Americas. The cultural and political instability attending these losses appears to have been of substantial aid in the efforts of various colonists in New England and Massachusetts to acquire control over the great wealth in land and resources of which indigenous societies had customarily made use.
Such diseases yielded human mortality of an unquestionably enormous gravity and scale – and this has profoundly confused efforts to determine its full extent with any true precision. Estimates of the pre-Columbian population of the Americas vary tremendously.
Others have argued that significant variations in population size over pre-Columbian history are reason to view higher-end estimates with caution. Such estimates may reflect historical population maxima, while indigenous populations may have been at a level somewhat below these maxima or in a moment of decline in the period just prior to contact with Europeans. Indigenous populations hit their ultimate lows in most areas of the Americas in the early 20th century; in a number of cases, growth has returned.
Exhibitions and collections.
In 2007, the Smithsonian Institution National Museum of American History and the Virginia Historical Society (VHS) co-organized a traveling exhibition to recount the strategic alliances and violent conflict between European empires (English, Spanish, French) and the Native people living in North America. The exhibition was presented in three languages and with multiple perspectives. Artifacts on display included rare surviving Native and European artifacts, maps, documents, and ceremonial objects from museums and royal collections on both sides of the Atlantic. The exhibition opened in Richmond, Virginia on March 17, 2007, and closed at the Smithsonian International Gallery on October 31, 2009.
The related online exhibition explores the international origins of the societies of Canada and the United States and commemorates the 400th anniversary of three lasting settlements in Jamestown (1607), Québec (1608), and Santa Fe (1609). The site is accessible in three languages.

</doc>
<doc id="52448" url="https://en.wikipedia.org/wiki?curid=52448" title="883">
883

__NOTOC__
Year 883 (DCCCLXXXIII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="52449" url="https://en.wikipedia.org/wiki?curid=52449" title="933">
933

__NOTOC__
Year 933 (CMXXXIII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="52450" url="https://en.wikipedia.org/wiki?curid=52450" title="936">
936

__NOTOC__
Year 936 (CMXXXVI) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52451" url="https://en.wikipedia.org/wiki?curid=52451" title="Russian colonization of the Americas">
Russian colonization of the Americas

The Russian colonization of the Americas covers the period from 1732 to 1867, when the Russian Empire laid claim to northern Pacific Coast territories in the Americas. Russian colonial possessions in the Americas are collectively known as Russian America. Russian expansion eastward began in 1552, and in 1639 Russian explorers reached the Pacific Ocean. In 1725, Emperor Peter the Great ordered navigator Vitus Bering to explore the North Pacific for potential colonization. The Russians were primarily interested in the abundance of fur-bearing mammals on Alaska's coast, as stocks had been depleted by overhunting in Siberia. Bering's first voyage was foiled by thick fog and ice, but in 1741 a second voyage by Bering and Aleksei Chirikov made sight of the North American mainland.
Russian "promyshlenniki" (trappers and hunters) quickly developed the maritime fur trade, which instigated several conflicts between the Aleuts and Russians in the 1760s. The fur trade proved to be a lucrative enterprise, capturing the attention of other European nations. In response to potential competitors, the Russians extended their claims eastward from the Commander Islands to the shores of Alaska. In 1784, with encouragement from Empress Catherine the Great, explorer Grigory Shelekhov founded Russia's first permanent settlement in Alaska at Three Saints Bay. Ten years later, the first group of Orthodox Christian missionaries began to arrive, evangelizing thousands of Indians, many of whose descendants continue to maintain the religion. By the late 1780s, trade relations had opened with the Tlingits, and in 1799 the Russian-American Company (RAC) was formed in order to monopolize the fur trade, also serving as an imperialist vehicle for the Russification of Alaska Natives.
Angered by encroachment on their land and other grievances, the indigenous peoples' relations with the Russians deteriorated. In 1802, Tlingit warriors destroyed several Russian settlements, most notably Redoubt Saint Michael (Old Sitka), leaving New Russia as the only remaining outpost on mainland Alaska. This failed to expel the Russians, who reestablished their presence two years later following the Battle of Sitka. (Peace negotiations between the Russians and Indians would later establish a "modus vivendi", a situation that, with few interruptions, lasted for the duration of Russian presence in Alaska.) In 1808, Redoubt Saint Michael was rebuilt as New Archangel and became the capital of Russian America after the previous colonial headquarters were moved from Kodiak. A year later, the RAC began expanding its operations to more abundant sea otter grounds in Northern California, where Fort Ross was built in 1812.
By the middle of the 19th century, profits from Russia's American colonies were in steep decline. Competition with the British Hudson's Bay Company had brought the sea otter to near extinction, while the population of bears, wolves, and foxes on land was also nearing depletion. Faced with the reality of periodic Indian revolts, the political ramifications of the Crimean War, and unable to fully colonize the Americas to their satisfaction, the Russians concluded that their American colonies were too expensive to retain. Eager to release themselves of the burden, the Russians sold Fort Ross in 1842, and in 1867, after less than a month of negotiations, the United States accepted Emperor Alexander II's offer to sell Alaska. The purchase of Alaska for $7.2 million ended Imperial Russia's colonial presence in the Americas. Many indigenous peoples protested the sale, arguing that they were the rightful owners of the land and that Russia had no right to sell Alaska.
Exploration.
Europeans first sighted the Alaskan coastline in 1732. Captain Sterling Romanov and his wife Annan Romanov founded the first colony. It was made by the Russian maritime explorer and navigator Ivan Fedorov from sea near present-day Cape Prince of Wales on the eastern boundary of the Bering Strait opposite Russian Cape Dezhnev. He did not land. The first European landfall took place in southern Alaska in 1741 during the Russian exploration by Vitus Bering and Aleksei Chirikov. Between 1774 and 1800 Spain also led several expeditions to Alaska in order to assert its claim over the Pacific Northwest. These claims were later abandoned at the turn of the 19th century. Count Nikolay Rumyantsev funded Russia's first naval circumnavigation under the joint command of Adam Johann von Krusenstern and Nikolai Rezanov in 1803–1806, and was instrumental in the outfitting of the voyage of the "Riurik's" circumnavigation of 1814–1816, which provided substantial scientific information on Alaska's and California's flora and fauna, and important ethnographic information on Alaskan and Californian (among others) natives.
Trading company.
Imperial Russia was unique among European empires for having no state sponsorship of foreign expeditions or territorial (conquest) settlement. The first state-protected trading company for sponsoring such activities in the Americas was the Shelikhov-Golikov Company of Grigory Shelikhov and Ivan Larionovich Golikov. A number of other companies were operating in Russian America during the 1780s. Shelikhov petitioned the government for exclusive control, but in 1788 Catherine II decided to grant his company a monopoly only over the area it had already occupied. Other traders were free to compete elsewhere. Catherine's decision was issued as the imperial "ukase" (proclamation) of September 28, 1788.
The Shelikhov-Golikov Company formed the basis for the Russian-American Company (RAC). Its charter was laid out in a 1799, by the new Tsar Paul I, which granted the company monopolistic control over trade in the Aleutian Islands and the North America mainland, south to 55° north latitude. The RAC was Russia's first joint stock company, and came under the direct authority of the Ministry of Commerce of Imperial Russia. Siberian merchants based in Irkutsk were initial major stockholders, but soon replaced by Russia's nobility and aristocracy based in Saint Petersburg. The company constructed settlements in what is today Alaska, Hawaii, and California.
Colonies.
The first Russian colony in Alaska was founded in 1784 by Grigory Shelikhov. Subsequently, Russian explorers and settlers continued to establish trading posts in mainland Alaska, on the Aleutian Islands, Hawaii, and Northern California.
Alaska.
The Russian-American Company was formed in 1799 with the influence of Nikolay Rezanov for the purpose of hunting sea otters for their fur. The peak population of the Russian colonies was about 4,000,000, although almost all of these were Aleuts, Tlingits and other Native Alaskans. The number of Russians rarely exceeded 500 at any one time.
California.
The Russians established their outpost of Fort Ross in 1812 near Bodega Bay in Northern California,
north of San Francisco Bay.
The Fort Ross colony included a sealing station on the Farallon Islands off San Francisco.
By 1818 Fort Ross had a population of 128, consisting of 26 Russians and of 102 Native Americans. The Russians maintained it until 1841, when they left the region. Fort Ross is a Federal National Historical Landmark on the National Register of Historic Places. It is preserved—restored in California's Fort Ross State Historic Park 50 miles north of San Francisco.
Spanish concern about Russian colonial intrusion prompted the authorities in New Spain to initiate the upper Las Californias Province settlement, with presidios (forts), pueblos (towns), and the California missions. After declaring their independence in 1821 the Mexicans also asserted themselves in opposition to the Russians: the Mission San Francisco de Solano (Sonoma Mission-1823) specifically responded to the presence of the Russians at Fort Ross; and Mexico established the El Presidio Real de Sonoma or "Sonoma Barracks" in 1836, with General Mariano Guadalupe Vallejo as the 'Commandant of the Northern Frontier' of the Alta California Province. The fort was the northernmost Mexican outpost to halt any further Russian settlement southward. The restored Presidio and mission are in the present day city of Sonoma, California.
In 1920 a one-hundred pound bronze church bell was unearthed in an orange grove near Mission San Fernando Rey de España in the San Fernando Valley of Southern California. It has an inscription in the Russian language (translated here): "In the Year 1796, in the month of January, this bell was cast on the Island of Kodiak by the blessing of Juvenaly of Alaska, during the sojourn of Alexander Andreyevich Baranov." How this Russian Orthodox Kodiak church artifact from Kodiak Island in Alaska arrived at a Roman Catholic Mission Church in Southern California remains unknown.
Seward Purchase.
The Russian colonies were rarely profitable, primarily due to transportation costs for supplies. In addition, Russia was in a difficult financial position and feared losing Russian Alaska without compensation in some future conflict, especially to the British. The Russians believed that in a dispute with Britain, their hard-to-defend region might become a prime target for British aggression from British Columbia, and would be easily captured. So following the Union victory in the American Civil War, Tsar Alexander II instructed the Russian minister to the United States, Eduard de Stoeckl, to enter into negotiations with the United States Secretary of State William H. Seward in the beginning of March 1867. At the instigation of Seward the United States Senate approved the purchase, known as the Alaska Purchase, from the Russian Empire. The cost was set at 2 cents an acre, which came to a total of $7,200,000 on 9 April 1867. The canceled check is in the present day United States National Archives.
Legacy.
Russian Orthodox Church.
Saint Herman of Alaska, Saint Innocent of Alaska and Saint Peter the Aleut have contributed historically to the strong Russian Orthodox Church community in Alaska. The Orthodox Church in America, which was formerly a missionary diocese of the Russian Orthodox Church, traces its history back to the early Russian missionaries in 'Russian America'.
In present-day Russian Federation and its predecessor the Soviet Union (USSR) there are periodic mass media stories that Alaska was not sold to the United States in the 1867 Alaska Purchase, but only leased for 99 years (= to 1966), or 150 years (= to 2017)—and will be returned to Russia. However, the Alaska Purchase Treaty is absolutely clear that the agreement was for a complete Russian cession of the territory.
The Soviet Union (USSR) released a series of commemorative coins in 1990 and 1991 to commemorate the 250th anniversary of the first sighting of and claiming domain over Alaska–Russian America. The commemoration consisted of a silver coin, a platinum coin and two palladium coins in both years.

</doc>
<doc id="52452" url="https://en.wikipedia.org/wiki?curid=52452" title="John Gorton">
John Gorton

Sir John Grey Gorton (9 September 1911 – 19 May 2002) was the 19th Prime Minister of Australia and long serving minister in the governments of Sir Robert Menzies, Harold Holt, Sir John McEwen and Sir William McMahon.
Gorton became Prime Minister after being elected Leader of the Liberal Party of Australia caused by the disappearance of Harold Holt in December 1967 while swimming. Gorton assumed the prime ministership while still a member of the Australian Senate, the only Prime Minister to have done so. Due to the Westminster tradition of Prime Ministers being a member of the Australian House of Representatives, Gorton stood for and won Holt's now vacant and ultra-safe seat of Higgins at the subsequent by-election by an increased margin. 
Gorton would go on to lead the Liberal-Country Coalition Government to their twelfth straight win at the 1969 Australian federal election. After long-term internal bickering and power shifting within the governing Liberal Party, Gorton resigned the party leadership and was succeeded by his former treasurer, William McMahon, in March 1971. Before entering politics, he was a skilled and respected pilot in the Royal Australian Air Force and was considered a "war hero" for his service during World War II.
Early life and education.
John Grey Gorton was born in Melbourne on 9 September 1911, the illegitimate son of Alice Sinn, the daughter of a railway worker, and English orange orchardist John Rose Gorton. The older Gorton and his wife Kathleen had emigrated to Australia by way of South Africa, where they had prospered during the Boer War. They separated in Australia, and Gorton established a de facto relationship with Sinn, who died of tuberculosis in 1920. Gorton the younger went to live with his father's estranged wife and his half-sister, Ruth, in Sydney.
Gorton was educated at Sydney Church of England Grammar School (where he was a classmate of Errol Flynn) and Geelong Grammar School. He then travelled to England to attend Brasenose College, Oxford. While in England, he undertook flying lessons and was awarded a British pilot's licence in 1932. He studied history, politics and economics at Oxford and graduated with an upper second undergraduate degree.
Personal life.
During a holiday in Spain while still an undergraduate, Gorton met Bettina Brown of Bangor, Maine, United States. She was a language student at the Sorbonne. This meeting came about through Gorton's friend from Oxford, Arthur Brown, who was Bettina's brother. Arthur Brown was later revealed to be a card-carrying member of the Communist Party. In 1935, Gorton and Bettina Brown were married in Oxford. After his studies were finished, they settled in Australia, taking over his father's orchard, "Mystic Park", at Lake Kangaroo near Kerang, Victoria. They had three children: Joanna, Michael and Robin.
Gorton was a Freemason.
War service.
1940–42.
On 31 May 1940, following the outbreak of World War II, Gorton enlisted in the Royal Australian Air Force Reserve. At the age of 29, Gorton was considered too old for pilot training, but he re-applied in September after this rule was relaxed. Gorton was accepted and commissioned into the RAAF on 8 November 1940. He trained as a fighter pilot at Somers, Victoria and Wagga Wagga, New South Wales, before being sent to the UK. Gorton completed his training at RAF Heston and RAF Honiley, with No. 61 Operational Training Unit RAF, flying Supermarine Spitfires. He was disappointed when his first operational posting was No. 135 Squadron RAF, a Hawker Hurricane unit, as he considered the type greatly inferior to Spitfires.
During late 1941, Gorton and other members of his squadron became part of the cadre of a Hurricane wing being formed for service in the Middle East. They were sent by sea, with 50 Hurricanes in crates, travelling around Africa to reduce the risk of attack. In December, when the ship was at Durban, South Africa, it was diverted to Singapore, after Japan entered the war. As it approached its destination in mid-January, Japanese forces were advancing down the Malayan Peninsula. The ship was attacked on at least one occasion by Japanese aircraft, but arrived and unloaded safely after tropical storms made enemy air raids impossible. As the Hurricanes were assembled, the pilots were formed into a composite operational squadron, No. 232 Squadron RAF.
In late January 1942, the squadron became operational and joined the remnants of several others that had been in Malaya, operating out of RAF Seletar and RAF Kallang. During one of his first sorties, Gorton was involved in a brief dogfight over the South China Sea, after which he suffered engine failure and was forced to land on Bintan island, 40 km (25 mi) south east of Singapore. As he landed, one of the Hurricane's wheels hit an embankment and flipped over. Gorton was not properly strapped in and his face hit the gun sight and windscreen, mutilating his nose and breaking both cheekbones. He also suffered severe lacerations to both arms. He made his way out of the wreck and was rescued by members of the Royal Dutch East Indies Army, who provided some medical treatment. Gorton later claimed that his face was so badly cut and bruised, that a member of the RAF sent to collect him assumed he was near death, collected his personal effects and returned to Singapore without him. By chance, one week later, Sgt Matt O'Mara of No. 453 Squadron RAAF also crash landed on Bintan, and arranged for them to be collected.
They arrived back in Singapore, on 11 February, three days after the island had been invaded. As the Allied air force units on Singapore had been destroyed or evacuated by this stage, Gorton was put on the "Derrymore", an ammunition ship bound for Batavia (Jakarta). On 13 February, as it neared its destination, the ship was torpedoed by Japanese submarine I-55 Kaidai class submarine and the "Derrymore" was abandoned. Gorton then spent almost a day on a crowded liferaft, in shark-infested waters, with little drinking water, until the raft was spotted by HMAS "Ballarat", which picked up the passengers and took them to Batavia.
Two schoolfriends, who had also been evacuated from Singapore to Batavia, heard that Gorton was in hospital, arranged for them to be put on a ship for Fremantle, which left on 23 February and treated Gorton's wounds. When the ship arrived in Fremantle, on 3 March, one of Gorton's arm wounds had become septic and needed extensive treatment. However, he was more concerned about the effect that the sight of his mutilated face would have on his wife. It is reported that Betty Gorton, who had been running the farm in his absence, was relieved to see Gorton alive.
1942–44.
After arriving in Australia he was posted to Darwin, Northern Territory on 12 August 1942 with No. 77 Squadron RAAF (Kittyhawks), during this time he was involved in his second air accident. While flying P-40E A29-60 on 7 September 1942, he was forced to land due to an incorrectly set fuel cock. Both Gorton and his aircraft were recovered several days later after spending time in the bush. On 21 February 1943 the squadron was relocated to Milne Bay, New Guinea.
John Gorton's final air incident came on 18 March 1943. His A29-192 Kittyhawk's engine failed on take off, causing the aircraft to flip at the end of the strip. Gorton was unhurt. In March 1944, Gorton was sent back to Australia with the rank of Flight Lieutenant. His final posting was as a Flying Instructor with No. 2 Operational Training Unit at Mildura, Victoria. He was then discharged from the RAAF on 5 December 1944.
During late 1944 Gorton went to Heidelberg hospital for surgery which could not fully repair his facial injuries.
Political career.
Prime Minister.
Harold Holt disappeared while swimming on 17 December 1967 and was declared presumed drowned two days later. His presumed successor was Liberal deputy leader William McMahon. However, on 18 December, the Country Party leader and Deputy Prime Minister John McEwen announced that if McMahon were named the new Liberal leader, he and his party would not serve under him. His reasons were never stated publicly, but in a private meeting with McMahon, he said "I will not serve under you because I do not trust you". McEwen's shock declaration triggered a leadership crisis within the Liberal Party; even more significantly, it raised the threat of a possible breaking of the Coalition, which would spell electoral disaster for the Liberals. Up to that time, the Liberals had never won enough seats in any House of Representatives election to be able to govern without Country Party support. Indeed, since the Coalition's formation in 1923, the major non-Labor party had only been able to govern alone once, during Joseph Lyons' first ministry—and even then, Lyons' United Australia Party had come up four seats short of a majority and needed confidence and supply support from the Country Party to govern.
The Governor-General Lord Casey swore McEwen in as Prime Minister, on an interim basis pending the Liberal Party electing its new leader. McEwen agreed to accept an interim appointment provided there was no formal statement of time limit. This appointment was in keeping with previous occasions when a conservative Coalition government had been deprived of its leader. Casey also concurred in the view put to him by McEwen that to commission a Liberal temporarily as Prime Minister would give that person an unfair advantage in the forthcoming party room ballot for the permanent leader.
In the subsequent leadership struggle, Gorton was championed by Army Minister Malcolm Fraser and Liberal Party Whip Dudley Erwin, and with their support he was able to defeat his main rival, External Affairs Minister Paul Hasluck, to become Liberal leader even though he was a member of the Senate. He was elected party leader on 9 January 1968, and appointed Prime Minister on 10 January, replacing McEwen. He was the only Senator in Australia's history to be Prime Minister and the only Prime Minister to have ever served in the Senate. He remained a Senator until, in accordance with the Westminster tradition that the Prime Minister is a member of the lower house of parliament, he resigned on 1 February 1968 to contest the by-election for Holt's old House of Representatives seat of Higgins in south Melbourne. The by-election in this comfortably safe Liberal seat was held on 24 February; there were three other candidates, but Gorton achieved a massive 68% of the formal vote. He visited all the polling booths during the day, but was unable to vote for himself as he was still enrolled in Mallee, in rural western Victoria. Between 2 and 23 February (both dates inclusive) he was a member of neither house of parliament.
Gorton was initially a very popular Prime Minister. He carved out a style quite distinct from those of his predecessors – the aloof Menzies and the affable, sporty Holt. Gorton liked to portray himself as a man of the people who enjoyed a beer and a gamble, with a bit of a "larrikin" streak about him. Unfortunately for him, this reputation later came back to haunt him.
He also began to follow new policies, pursuing independent defence and foreign policies and distancing Australia from its traditional ties to Britain. But he continued to support Australia's involvement in the Vietnam War, a position he had reluctantly inherited from Holt, which became increasingly unpopular after 1968. On domestic issues, he favoured centralist policies at the expense of the states, which alienated powerful Liberal state leaders like Sir Henry Bolte of Victoria and Bob Askin of New South Wales. He also fostered an independent Australian film industry and increased government funding for the arts.
Gorton proved to be a surprisingly poor media performer and public speaker, and was portrayed by the media as a foolish and incompetent administrator. He was unlucky to come up against a new and formidable Labor Opposition Leader in Gough Whitlam. Also, he was subjected to media speculation about his drinking habits and his involvements with women. He generated great resentment within his party, and his opponents became increasingly critical of his reliance on an inner circle of advisers – most notably his private secretary Ainsley Gotto.
The Coalition suffered a 7% swing against it at the 1969 election, and Labor outpolled it on the two-party-preferred vote. During the close election Gorton promised to waive all future government rent on residential leaseholders in Canberra. After surviving the election Gorton came through on his promise, giving away an estimated $100 million in equity to leaseholders and abandoning future government rent revenue. Still, Gorton saw the sizeable 45-seat majority he had inherited from Holt cut down to only seven. Indeed, the Coalition might have lost government had it not been for the Democratic Labor Party's longstanding practice of preferencing against Labor. The Coalition was only assured of an eighth term in government when DLP preferences tipped four marginal seats in Melbourne —the DLP's heartland—to the Liberals. Had those preferences gone the other way, Whitlam would have become Prime Minister.
Leadership challenge and resignation.
After the election, Gorton was challenged for the Liberal leadership by McMahon and David Fairbairn, but so long as McEwen's veto on McMahon remained in place, he was fairly safe. McEwen retired in January 1971, and his successor, Doug Anthony, told the Liberals that the veto no longer applied. With the Liberals falling further behind Labor in the polls, a challenge was launched in March when Defence Minister Fraser resigned. Fraser had strongly supported Gorton for the leadership two years earlier, but now attacked Gorton on the floor of Parliament in his resignation speech, saying that Gorton was "not fit to hold the great office of Prime Minister."
Gorton called a Liberal caucus meeting for 10 March 1971 to settle the matter. A motion of confidence in his leadership was tied. Under Liberal caucus rules of the time, a tied vote meant the motion was lost, and hence Gorton could have remained as party leader and Prime Minister without further ado. However, he took it upon himself to resign, saying "Well, that is not a vote of confidence, so the party will have to elect a new leader." A ballot was held and McMahon was elected leader and thus Prime Minister. Australian television marked the end of Gorton's stormy premiership with a newsreel montage appropriately accompanied by Sinatra's anthem "My Way".
In a surprise move, Gorton contested and won the position of Deputy Leader, forcing McMahon to make him Defence Minister. This farcical situation ended within five months when McMahon sacked him for disloyalty.
After 1972.
After Labor won the 1972 election, Gorton served in the Shadow Ministry of Billy Snedden until after the 1974 election, when he was dropped. In 1973, Gorton moved a motion in Parliament calling for the decriminalisation of homosexual acts between consenting adults in Australia. The motion was successful following a conscience vote.
When Fraser became Liberal leader in 1975, Gorton resigned from the party, sat as an independent, and openly campaigned against Fraser, whom he detested. He denounced the dismissal of the Whitlam government by Sir John Kerr, and unsuccessfully stood for an Australian Capital Territory Senate seat at the 1975 election as an independent. He achieved 11 per cent of the vote, coming third behind the major parties.
In 1977, he became a public supporter of Don Chipp's new centre-line Australian Democrats party.
Retirement and death.
Gorton retired to Canberra, where he kept out of the political limelight. However, in March 1983, he congratulated Bob Hawke "for rolling that bastard Fraser" at that year's election. Bettina Gorton died aged 68 on 2 October 1983, and in 1993 he married Nancy Home. 
In the 1990s, Gorton quietly rejoined the Liberal Party to which John Hewson credited himself with "returning Gorton to the fold." In his old age he was rehabilitated by the Liberals; his 90th birthday party was attended by Prime Minister John Howard who said at the event: "He (Gorton) was a person who above everything else was first, second and last an Australian." Although he was back within Liberal circles, he never forgave Fraser; as late as 2002 he told his biographer Ian Hancock that he still could not tolerate being in the same room as Fraser. 
Gorton died at Sydney's St Vincent's Hospital at the age of 90 in May 2002. A State funeral and memorial service was held on 30 May at St Andrews' Cathedral where extremely critical remarks of Fraser, who was in attendance with wife Tamie, were delivered during the eulogy by Gorton's former Attorney-General Tom Hughes. Current and former Prime Ministers Howard, Gough Whitlam and Bob Hawke where also in attendance. Gorton was cremated after a private service.
Honours.
Gorton was appointed a Privy Counsellor in 1968, a Companion of Honour in 1971, a Knight Grand Cross of the Order of St Michael and St George in 1977 and a Companion of the Order of Australia in 1988. He was awarded the Centenary Medal in 2001.

</doc>
<doc id="52453" url="https://en.wikipedia.org/wiki?curid=52453" title="Tetum language">
Tetum language

Tetum , also Tetun, is an Austronesian language spoken on the island of Timor. It is spoken in Belu Regency in Indonesian West Timor, and across the border in East Timor, where it is one of the two official languages. In East Timor a creolized form, Tetun Dili, is widely spoken fluently as a second language; without previous contact, Tetum and Tetun Dili are not mutually intelligible. Besides the grammatical simplification involved in creolization, Tetun Dili has been greatly influenced by the vocabulary of Portuguese, the other official language of East Timor.
History and dialects.
Tetum has four dialects:
"Tetun-Belu" and "Tetun-Terik" are not spoken or well understood outside their home territories. "Tetun-Prasa" is the form of Tetum that is spoken throughout East Timor. Although Portuguese was the official language of Portuguese Timor until 1975, "Tetun-Prasa" has always been the predominant "lingua franca" in the eastern part of the island.
In the fifteenth century, before the arrival of the Portuguese, Tetum had spread through central and eastern Timor as a contact language under the aegis of the Belunese-speaking Kingdom of Wehali, at that time the most powerful kingdom in the island. The Portuguese (present in Timor from c. 1556) made most of their settlements in the west, where Dawan was spoken, and it was not until 1769, when the capital was moved from Lifau (Oecussi) to Dili that they began to promote Tetum as an inter-regional language in their colony. Timor was one of the few Portuguese colonies where a local language, and not a form of Portuguese, became the lingua franca: this is because Portuguese rule was indirect rather than direct, the Europeans governing through local kings who embraced Catholicism and became vassals of the King of Portugal.
When Indonesia occupied East Timor between 1975 and 1999, declaring it "the Republic's 27th Province", the use of Portuguese was banned, and Indonesian was declared the sole official language, but the Roman Catholic Church adopted Tetum as its liturgical language, making it a focus for cultural and national identity. When East Timor gained its independence on 20 May 2002, Tetum and Portuguese were declared as official languages.
In addition to regional varieties of Tetum in East Timor, there are variations in vocabulary and pronunciation, partly due to Portuguese and Indonesian influence. The Tetum spoken by East Timorese migrants in Portugal and Australia is more Portuguese-influenced, as many of those speakers were not educated in Indonesian.
Vocabulary.
Indigenous.
The Tetum name for East Timor is "Timór Lorosa'e", which means "Timor of the rising sun", or, less poetically, "East Timor"; "lorosa'e" comes from "loro" "sun" and "sa'e" "to rise, to go up". The noun for "word" is "liafuan", from "lia" "voice" and "fuan" "fruit". Some more words in Tetum:
From Portuguese.
Words derived from Portuguese:
From Malay.
As a result of Bazaar Malay being a regional lingua franca, many words are derived from Malay, including:
In addition, as a legacy of Indonesian rule, other words of Malay origin have entered Tetum, through Indonesian.
Numerals.
However, Tetum speakers often use Malay/Indonesian or Portuguese numbers instead, such as
"delapan" or "oito" "eight" instead of "ualu" (just like "eight" in Javanese: "wolu"), especially for numbers over one thousand.
Combinations.
Tetum has many hybrid words, which are combinations of indigenous and Portuguese words. These often include an indigenous Tetum verb, with a Portuguese suffix "-dór" (similar to '-er'). For example:
Grammar.
Morphology.
Personal pronouns.
Examples:
A common occurrence is to use titles such as "Senhora" for a woman or names rather than pronouns when addressing people.
Example:
The second person singular pronoun "Ó" is used generously with children or if the speaker intends to address someone of high social status.
Example:
Nouns and pronouns.
Plural.
The plural is not normally marked on nouns, but the word "sira" "they" can express it when necessary.
However, the plural ending "-(e)s" of nouns of Portuguese origin is retained.
Definiteness.
Tetum has an indefinite article "ida" ("one"), used after nouns:
There is no definite article, but the demonstratives "ida-ne'e" ("this one") and "ida-ne'ebá" ("that one") may be used to express definiteness:
In the plural, "sira-ne'e" ("these") or "sira-ne'ebá" ("those") are used:
Possessive and genitive.
The particle "nia" forms the possessive, and can be used in a similar way to the Saxon genitive in English, e.g.:
The genitive is formed with "nian", so that:
Inclusive and exclusive "we".
Like other Austronesian languages, Tetum has two forms of "we", "ami" (equivalent to Indonesian and Malay "kami") which is exclusive, e.g. "I and they", and "ita" (equivalent to Indonesian and Malay "kita"), which is inclusive, e.g. "you, I, and they".
Nominalization.
Nouns derived from verbs or adjectives are usually formed with affixes, for example the suffix "-na'in", similar to "-er" in English.
The suffix "-na'in" can also be used with nouns, in the sense of "owner".
In more traditional forms of Tetum, the circumfix "ma(k)- -k" is used instead of "-na'in". For example, the nouns "sinner" or "wrongdoer" can be derived from the word "sala" as either "maksalak", or "sala-na'in". Only the prefix "ma(k)-" is used when the root word ends with a consonant; for example, the noun "cook" or "chef" can be derived from the word "te'in" as "makte'in" as well as "te'in-na'in".
The suffix "-teen" (from the word for "dirt" or "excrement") can be used with adjectives to form derogatory terms:
Adjectives.
Derivation from nouns.
To turn a noun into an adjective, the particle "oan" is added to it.
Thus, "Timorese" is "Timor-oan", as opposed to the country of Timor, "rai-Timor".
To form adjectives from verbs, the suffix "-dór" (derived from Portuguese) can be added:
Gender.
Tetum does not have separate masculine and feminine forms of the third person singular, hence "nia" (similar to "dia" in Indonesian and Malay) can mean either "he", "she" or "it".
Different forms for the genders only occur in Portuguese-derived adjectives, hence "obrigadu" ("thank you") is used by males, and "obrigada" by females. The masculine and feminine forms of other adjectives derived from Portuguese are sometimes used with Portuguese loanwords, particularly by Portuguese-educated speakers of Tetum.
In some instances, the different gender forms have distinct translations into English:
In indigenous Tetum words, the suffixes "-mane" ("male") and "-feto" ("female") are sometimes used to differentiate between the genders:
Comparatives and superlatives.
Superlatives can be formed from adjectives by reduplication:
When making comparisons, the word "liu" ("more") is used after the adjective, followed by "duké" ("than" from Portuguese "do que"):
To describe something as the most or least, the word "hotu" ("all") is added:
Adverbs.
Adverbs can be formed from adjectives or nouns by reduplication:
Prepositions and circumpositions.
The most commonly used prepositions in Tetum are "iha" ("in") and "ba" ("to" or "for") while circumpositions are widely used. These are formed by using "iha", the object and the position.
Verbs.
Copula and negation.
There is no verb "to be" as such, but the word "la'ós", which translates as "not to be", is used for negation:
The word "maka", which roughly translates as "who is" or "what is", can be used with an adjective for emphasis:
Interrogation.
The interrogative is formed by using the words "ka" ("or") or "ka lae" ("or not").
Derivation from nouns and adjectives.
Transitive verbs are formed by adding the prefix "ha-" or "hak-" to a noun or adjective:
Intransitive verbs are formed by adding the prefix "na-" or "nak-" to a noun or adjective:
Conjugations and inflections (in Tetun-Terik).
In "Tetun-Terik", verbs inflect when they begin with a vowel or consonant h. In this case mutation of the first consonant occurs. For example, the verb "haree" (to see) in "Tetun-Terik" would be conjugated as follows:
Tenses.
Past.
Whenever possible, the past tense is simply inferred from the context, for example:
However, it can be expressed by placing the adverb "ona" ("already") at the end of a sentence.
When "ona" is used with "la" ("not") this means "no more" or "no longer", rather than "have not":
In order to convey that an action has not occurred, the word "seidauk" ("not yet") is used:
When relating an action that occurred in the past, the word "tiha" ("finally" or "well and truly") is used with the verb.
Future.
The future tense is formed by placing the word "sei" ("will") before a verb:
The negative is formed by adding "la" ("not") between "sei" and the verb:
Aspects.
Perfect.
The perfect aspect can be formed by using "tiha ona".
When negated, "tiha ona" indicates that an action ceased to occur:
In order to convey that a past action had not or never occurred, the word "ladauk" ("not yet" or "never") is used:
Progressive.
The progressive aspect can be obtained by placing the word "hela" ("stay") after a verb:
Imperative.
The imperative mood is formed using the word "ba" ("go") at the end of a sentence, hence:
The word "lai" ("just" or "a bit") may also be used when making a request rather than a command:
When forbidding an action "labele" ("cannot") or "keta" ("do not") are used:
Tetun Grammar Tenses
Daudaun - pasadu prolongadu
Hela - prezente prolongadu
Ona - perfeitu
Tiha - pasadu
Sei - futuru
Atu - futuru prósimu
Foin - (just)
Ba - (going to)
Orthography and phonology.
The influence of Portuguese and to a lesser extent Malay/Indonesian on the phonology of Tetun has been extensive.
In the tetun language, /a/ /i/ and /u/ tend to have relatively fixed sounds. However /e/ and /o/ vary according to the environment they are placed in, for instance the sound is slightly higher if the proceeding syllable is /u/ or /i/.
Stops: All stops in tetun are un-aspirated, meaning an expulsion of breath is absent. In contrast, English stops namely ‘p’ ‘t’ and ‘k’ are generally aspirated. 
Fricatives: 
/v/ is an unstable voiced labio-dental fricative and tends to alternate with or is replaced by /b/; e.g. – [a’bo: meaning" grandparent.
As Tetum did not have any official recognition or support under either Portuguese or Indonesian rule, it is only recently that a standardised orthography has been established by the National Institute of Linguistics (INL). However, there are still widespread variations in spelling, one example being the word "bainhira" or "when", which has also been written as "bain-hira", "wainhira", "waihira", "uaihira". The use of "w" or "u" is a reflection of the pronunciation in some rural dialects of "Tetun-Terik".
The current orthography originates from the spelling reforms undertaken by Fretilin in 1974, when it launched literacy campaigns across East Timor, and also from the system used by the Catholic Church when it adopted Tetum as its liturgical language during the Indonesian occupation. These involved the transcription of many Portuguese words that were formerly written in their original spelling, for example, "educação" → "edukasaun" "education", and "colonialismo" → "kolonializmu" "colonialism".
More recent reforms by the INL include the replacement of the digraphs "nh" and "lh" (borrowed from Portuguese, where they stand for the phonemes and ) by "ñ" and "ll", respectively (as in Spanish), to avoid confusion with the consonant clusters and , which also occur in Tetum. Thus, "senhor" "sir" became "señór", and "trabalhador" "worker" became "traballadór". Some linguists favoured using "ny" (as in Catalan and Filipino) and "ly" for these sounds, but the latter spellings were rejected for being similar to the Indonesian system. However, most speakers actually pronounce "ñ" and "ll" as and , respectively, with a semivowel which forms a diphthong with the preceding vowel (but reduced to , after ), not as the palatal consonants of Portuguese and Spanish. Thus, "señór", "traballadór" are pronounced , , and "liña", "kartilla" are pronounced , . As a result, some writers use "in" and "il" instead, for example "Juinu" and "Juilu" for June and July ("Junho" and "Julho" in Portuguese).
As well as variations in the transliteration of Portuguese loanwords, there are also variations in the spelling of indigenous words. These include the use of double vowels and the apostrophe for the glottal stop, for example "boot" → "bot" "large" and "ki'ik" → "kiik" "small".
The sound , which is not indigenous to Tetum but appears in many loanwords from Portuguese and Malay, often changed to in old Tetum and to (written "j") in the speech of young speakers: for example, "meja" "table" from Portuguese "mesa", and "kamija" "shirt" from Portuguese "camisa". In the sociolect of Tetum that is still used by the generation educated during the Indonesian occupation, and may occur in free variation. For instance, the Portuguese-derived word "ezemplu" "example" is pronounced by some speakers, and conversely "Janeiru" "January" is pronounced . The sound , also not native to the language, often shifted to , as in "serbisu" "work" from Portuguese "serviço" (also note that a modern INL convention promotes the use of "serbisu" for "work" and "servisu" for "service").
Name.
The English spelling "Tetum" is derived from Portuguese, rather than from modern Tetum orthography. Consequently, some people regard "Tetun" as more appropriate. Although this coincides with the favoured Indonesian spelling, and the spelling with "m" has a longer history in English, "Tetun" has also been used by some Portuguese-educated Timorese, such as José Ramos-Horta and Carlos Filipe Ximenes Belo.
Similar disagreements over nomenclature have emerged regarding the names of other languages, such as Swahili/Kiswahili and Punjabi/Panjabi.
External links.
Institute of Technology website

</doc>
<doc id="52454" url="https://en.wikipedia.org/wiki?curid=52454" title="945">
945

__NOTOC__
Year 945 (CMXLV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52455" url="https://en.wikipedia.org/wiki?curid=52455" title="448">
448

__NOTOC__
Year 448 (CDXLVIII) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Praetextatus and Zeno (or, less frequently, year 1201 "Ab urbe condita"). The denomination 448 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years. 
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52456" url="https://en.wikipedia.org/wiki?curid=52456" title="Dwarf (mythology)">
Dwarf (mythology)

In Germanic mythology, a dwarf is a being that dwells in mountains and in the earth, and is variously associated with wisdom, smithing, mining, and crafting. Dwarfs are often also described as short and ugly, although some scholars have questioned whether this is a later development stemming from comical portrayals of the beings. The concept of the dwarf has had influence in modern popular culture and appears in a variety of media.
Etymology and usage.
The modern English noun "dwarf" descends from the Old English "dweorg". It has a variety of cognates in other Germanic languages, including Old Norse "dvergr" and Old High German "twerg". According to Vladimir Orel, the English noun and its cognates ultimately descend from Proto-Germanic *"đwerȝaz".
Beyond the Proto-Germanic reconstruction, the etymology of the word "dwarf" is highly contested. Scholars have proposed theories about the origins of the being by way of historical linguistics and comparative mythology, including that dwarfs may have originated as nature spirits, as beings associated with death, or as a mixture of concepts. Competing etymologies include a basis in the Indo-European root "*dheur-" (meaning 'damage'), the Indo-European root "*dhreugh" (whence, for example, modern English "dream" and German "Trug" 'deception'), and comparisons have been made with Sanskrit "dhvaras" (a type of "demonic being").
Modern English has two plurals for the word "dwarf": "dwarfs" and "dwarves". "Dwarfs" remains the most commonly employed plural. The minority plural "dwarves" was recorded as early as 1818, but it was popularized by the fiction of philologist and author J. R. R. Tolkien, originating as a mistake (hypercorrection) and employed by Tolkien since some time before 1917 (for Tolkien's beings, see Dwarf (Middle-earth)). Regarding the plural, Tolkien wrote in 1937, "I am afraid it is just a piece of private bad grammar, rather shocking in a philologist; but I shall have to go with it".
Norse mythology and folklore.
Norse mythology provides different mythical origins for the beings, as recorded in the "Poetic Edda" (compiled in the 13th century from earlier traditional sources) and the "Prose Edda" (written by Snorri Sturluson in the 13th century). The "Poetic Edda" poem "Völuspá" details that the dwarfs were the product of the primordial blood of the being Brimir and the bones of Bláinn (generally considered to be different names for the primordial being Ymir). The "Prose Edda", however, describes dwarfs as beings similar to maggots that festered in the flesh of Ymir before being gifted with reason by the gods. The "Poetic Edda" and "Prose Edda" contain over 100 dwarf names, while the "Prose Edda" gives the four dwarfs Norðri, Suðri, Austri and Vestri (Old Norse 'North, South, East, and West') a cosmological role: they hold up the sky. In addition, scholars have noted that the Svartálfar (Old Norse 'black elves') appear to be the same beings as dwarfs, given that both are described in the "Prose Edda" as the denizens of Svartálfaheimr.
Very few beings explicitly identifiable as dwarfs appear in the "Poetic Edda" and "Prose Edda" and have quite diverse roles: murderous creators who create the mead of poetry, 'reluctant donors' of important artifacts with magical qualities, or sexual predators who lust after goddesses. They are primarily associated with metalsmithing, and also with death, as in the story of King Sveigðir in "Ynglinga saga", the first segment of the "Heimskringla" — the doorways in the mountains that they guard may be regarded as doors between worlds. One dwarf named Alvíss claimed the hand of Thor's daughter Þrúðr in marriage, but he was kept talking until daybreak and turned to stone, much like some accounts of trolls.
After the Christianization of the Germanic peoples, tales of dwarfs continued to be told in the folklore of areas of Europe where Germanic languages were (and are) spoken. In the late legendary sagas, dwarfs demonstrate skill in healing as well as in smithing. In the early Norse sources, there is no mention of their being short; in the legendary sagas, however, they are "small and usually ugly". Anatoly Liberman suggests that dwarfs may have originally been thought of as lesser supernatural beings, which became literal smallness after Christianization. Old Norse dwarf names include "Fullangr" ('tall enough') and "Hár" ('high'), whereas Anglo-Saxon glosses use "dweorg" to render Latin terms such as "nanus" and "pygmaeus" ('pygmy').
Dwarfs in folklore are usually described as old men with long beards. Female dwarfs are hardly ever mentioned. Dvalinn the dwarf has daughters, and the 14th-century romantic saga "Þjalar Jóns saga" gives the feminine form of Old Norse "dyrgja", but the few folklore examples cited by Grimm in "Teutonic Mythology" may be identified as other beings. However, in the Swedish ballad "Herr Peder och Dvärgens Dotter" (Swedish 'Sir Peder and the Dwarf's Daughter'), the role of supernatural temptress is played by a dwarf's daughter.
Anglo-Saxon medicine.
The Anglo-Saxon charm "Wið Dweorh" ("Against a Dwarf") appears to relate to sleep disturbances. This may indicate that the dwarf antagonist is similar to the oppressive supernatural figure the "mare" which is the etymological source of the word "nightmare", or possibly that the word had come to be used to mean "fever". In the Old English "Herbal", it translates Latin "verrucas", warts.
Scholarly interpretations.
Lotte Motz theorized that the Germanic dwarfs, particularly as smiths and gatekeepers, constituted a reminiscence of the Megalithic culture in Northern Europe.
John Lindow noted that stanza 10 of the "Poetic Edda" poem "Völuspá" can be read as describing the creation of human forms from the earth and follows a catalog of dwarf names; he suggests that the poem may present Ask and Embla as having been created by dwarfs, with the three gods then giving them life.
Modern influence.
There were seven dwarfs in the Brothers Grimm's fairy tale "Snow White". The Walt Disney Company's 1937 film based on the story, the first feature-length animated film, is the best known adaptation today.
Dwarves also appear in many fantasy MMORPGs as a playable race.
In the "Prose Edda", the dwarfs are equated with the svartálfar and dökkálfar ("dark elves"). In J. R. R. Tolkien's "The Lord of the Rings", the dwarves (Tolkien's spelling) and the Elves of Darkness or Moriquendi are distinct. They are also present in C.S Lewis's The Chronicles Of Narnia, in both the books and the film adaptions.
Most modern fantasy media have continued this distinction, beginning with TSR's "Dungeons and Dragons". "Dungeons and Dragons" calls the dwarfs "dwarves" and the dark elves drow, which is derived from the Scottish word trow, according to Gary Gygax.

</doc>
<doc id="52457" url="https://en.wikipedia.org/wiki?curid=52457" title="934">
934

__NOTOC__
Year 934 (CMXXXIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="52458" url="https://en.wikipedia.org/wiki?curid=52458" title="937">
937

__NOTOC__
Year 937 (CMXXXVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="52459" url="https://en.wikipedia.org/wiki?curid=52459" title="Counter-Earth">
Counter-Earth

The Counter-Earth is a hypothetical body of the Solar system hypothesized by the pre-Socratic Greek philosopher Philolaus (c. 470 – c. 385 BC) to support his non-geocentric cosmology, in which all objects in the universe revolve around an unseen "Central Fire" (distinct from the Sun which also revolves around it). The Greek word Antichthon () means "Counter-Earth."
In modern times a hypothetical planet always on the other side of the Sun from the Earth has been called a "Counter-Earth", and has been a recurring theme in fiction, science fiction and UFO claims.
Greek Pythagorean universe.
An astronomical system positing that the Earth, Moon, Sun, planets and unseen "counter-earth" revolve around an unseen "Central Fire" was developed in the 5th century BC and attributed to the Pythagorean philosopher Philolaus. Philolaus' universe moved "the earth from the center of the cosmos", and provided the insight that "the apparent motion of the heavenly bodies" was (in large part) due to "the real motion of the observer"—i.e. Earth. 
In Philolaus' system, the Earth did not rotate and its inhabited surface faced away from the Central Fire—possibly because it (the Earth) was flat. The revolution of the Earth around the Central Fire was not yearly but daily, while the Moon's revolution was monthly, and the sun's yearly. It was the Earth's speedy travel past the slower moving Sun that resulted in the appearance on Earth of the Sun rising and setting. Further from the Central Fire, the Planets' movement was slower still, and the outermost "sky" (i.e. stars) probably fixed.
Counter-Earth.
Along with the Central Fire, the "mysterious" Counter-Earth ("Antichthon") was the other heavenly body not visible from Earth. We know that Aristotle described it as "another Earth", from which Greek scholar George Burch infers that it must be similar in size, shape and constitution to Earth. Some (astronomer John Louis Emil Dreyer) think Philolaus had it following an orbit so that it was always located between Earth and the Central Fire, and a tale of Greek mythology may have placed it in that location to stop man from looking at the throne of Zeus directly. However, Burch argues Philolaus must have thought it orbited on the other side of the Fire from Earth. Since "counter" means "opposite", and opposite could only be in respect to the Central Fire, it follows that the Counter-Earth must be orbiting 180 degrees from Earth.
According to Aristotle—a critic of the Pythagoreans—the function of the Counter-Earth was to explain "eclipses of the moon and their frequency", which could not be explained by Earth blocking the light of the sun if the Earth did not revolve around the sun. Aristotle suggests that it was also introduced "to raise the number of heavenly bodies around the central fire from nine to ten, which the Pythagoreans regarded as the perfect number".
However Burch believes Aristotle was having a joke "at the expense of Pythagorean number theory", and that the true purpose of the Counter-Earth was to "balance" Philolaus' cosmos—balance being needed because without a counter there would be only one dense, massive object in the system—Earth. Although his system had both the Earth and the Planets orbiting a single point, the ancient Greeks did not consider Earth a "planet". In the time before Galileo could observe from his telescope that planets were spheres like Earth, they were thought to be different from stars only in brightness and in their motion, and like stars composed of a fiery or ethereal matter having little or no density. However, the Earth was obviously made of the dense elements of earth and water. According to Burch,
"If there was a single Earth revolving at some distance from the center of space, then the universe's center of gravity, located in the Earth as its only dense body, would not coincide with its spatial center ... The universe, consequently, would be off center, so to speak—lopsided and asymmetric—a notion repugnant to any Greek, and doubly so to a Pythagorean." This could be corrected by another body with the same mass as Earth, orbiting the same central point but 180 degrees from Earth—the Counter-Earth.
Later.
In the 1st century A.D., after the idea of a spherical Earth gained more general acceptance, Pomponius Mela, a Latin cosmographer, developed an updated version of the idea, wherein a spherical Earth must have a more or less balanced distribution of land and water. Mela drew the first map on which the mysterious continent of Earth appears in the unknown half of Earth—our antipodes. This continent he inscribed with the name Antichthones.
Modern era.
Philolaus's ideas were all eventually superseded by the modern belief that a spherical Earth rotating on its own axis was one of several spherical planets following the laws of gravity and revolving around a much larger Sun. The idea of a Counter-Earth waned after the heliocentric model of the solar system became widely accepted from the 16th century. In the contemporary world "Counter-Earth" usually refers to a hypothetical planet with an orbit as Burch described, on the other side of the "Central fire"—i.e. the Sun. It cannot be seen from Earth, not because Earth faces away from the center, but because the Sun's great size blocks its view. It has been a recurring motif in science fiction, fiction—often serving as an allegory for the real Earth—and UFO claims.
Scientific analysis.
A planet orbiting the Sun so that it was always on the other side of the sun from Earth could have such an orbit because it was the same distance from the Sun and had the same mass as Earth (according to the theory). Thus what would make it undetectable to astronomers (or any other human beings) on Earth would also make it habitable to beings at least similar to humans. With the same size and distance from the Sun as Earth, it could have the same (or very similar) surface environment—gravity, atmospheric pressure, and surface temperature range. At the same time such a planet could have the same orbiting velocity and path as Earth, so that if it was positioned 180 degrees from Earth, it would remain behind the Sun being blocked from view from Earth indefinitely. The 1968 "Scientific Study of Unidentified Flying Objects" headed by Edward Condon at the University of Colorado even included a "Numerical Experiment on the Possible Existence of an 'Anti-Earth'" as an appendix.
However, if such a planet actually existed, the laws of physics and cosmology would make it detectable from Earth for a number of reasons.
A Counter-Earth would have gravitational influence (perturbation) upon the other planets, comets and man-made probes of the Solar System. Researchers have detected no such influence, and indeed space probes sent to Venus, Mars and other places could not have successfully flown by or landed on their targets if a Counter-Earth existed, as the navigational calculations for their journeys did not take any putative Counter-Earth into account. Roughly speaking, anything larger than in diameter should have been detected. 
The gravitational forces of the other planets on a Counter-Earth would make "its" orbit unstable. Venus has 82% of the mass of Earth and would come within 0.3 AU of the location of a Counter-Earth every 20 months, providing considerable gravitational pull that over the years would move its orbit into sight of observers on Earth. If a Counter-Earth was much smaller than Earth, its location at the "Sun–Earth L3" Lagrangian point (see diagram), would mean the combined gravitational pull of the two large masses of Earth and Sun would provide "precisely the centripetal force required to orbit with them". But a small planet would be even more influenced by the orbit of Venus, Mars and Jupiter, making it even more unstable.
Any planetary sized body 180 degrees from Earth should also have been visible to some space probes, such as NASA's STEREO coronagraph probes (two spacecraft launched into orbits around the Sun in 2006, one farther ahead of and one behind the Earth's orbit) which would have seen the Counter-Earth during the first half of 2007. The separation of the STEREO spacecraft from Earth would give them a view of the L3 point during the early phase of the mission.
If a Counter-Earth had an electromagnetic energy signature similar to that of Earth's, it would be able to be detected by astronomers since the signature would extend well beyond the surface of an Earth-like planet. The Sun's "wobbling" motion around its "barycenter" would prevent it from blocking all signs of that energy from Earth, at least for part of the year. The Sun does not remain stationary—relative to its planets—at the center of the solar system because of the gravitational pull of the most massive planet—Jupiter. The Sun wobbles around the "true" center known as a barycenter, which lies just inside the Sun. When the Sun's position is 90 degree from the barycenter relative to Earth, it would come close to "unblocking" from view a planet 180 degrees from Earth, enough to reveal the aforementioned hypothetical signature.
For a Counter-Earth orbiting the same path as Earth to always stay 180 degrees from Earth, the two planets would have to have circular orbits, but Earth's orbit is elliptical. According to Kepler's second law, a planet revolves faster when it is close to the star, so a Counter-Earth following the Earth on the same orbit with half a year of delay would sometimes not be exactly 180 degrees from Earth. To remain hidden from Earth, the Counter-Earth would require an orbit symmetrical to Earth's, not sharing the second focus or orbit path.

</doc>
<doc id="52460" url="https://en.wikipedia.org/wiki?curid=52460" title="French colonization of the Americas">
French colonization of the Americas

The French colonization of the Americas began in the 16th century, and continued on into the following centuries as France established a colonial empire in the Western Hemisphere. France founded colonies in much of eastern North America, on a number of Caribbean islands, and in South America. Most colonies were developed to export products such as fish, sugar, and furs.
As they colonized the New World, the French established forts and settlements that would become such cities as Quebec and Montreal in Canada; Detroit, Green Bay, St. Louis, Cape Girardeau, Mobile, Biloxi, Baton Rouge and New Orleans in the United States; and Port-au-Prince, Cap-Haïtien (founded as "Cap-Français") in Haiti, Cayenne in French Guiana and São Luís (founded as "Saint-Louis de Maragnan") in Brazil.
North America.
Background.
The French first came to the New World as explorers, seeking a route to the Pacific Ocean and wealth. Major French exploration of North America began under the rule of Francis I, King of France. In 1524, Francis sent Italian-born Giovanni da Verrazzano to explore the region between Florida and Newfoundland for a route to the Pacific Ocean. Verrazzano gave the names "Francesca" and "Nova Gallia" to that land between New Spain and English Newfoundland, thus promoting French interests.
Colonization.
In 1534, Francis I of France sent Jacques Cartier on the first of three voyages to explore the coast of Newfoundland and the St. Lawrence River. He founded New France by planting a cross on the shore of the Gaspé Peninsula. The French subsequently tried to establish several colonies throughout North America that failed, due to weather, disease, or conflict with other European powers. Cartier attempted to create the first permanent European settlement in North America at Cap-Rouge (Quebec City) in 1541 with 400 settlers but the settlement was abandoned the next year after bad weather and first nations attacks. A small group of French troops were left on Parris Island, South Carolina in 1562 to build Charlesfort, but left after a year when they were not resupplied by France. Fort Caroline established in present-day Jacksonville, Florida, in 1564, lasted only a year before being destroyed by the Spanish from St. Augustine. An attempt to settle convicts on Sable Island off Nova Scotia in 1598 failed after a short time. In 1599, a sixteen-person trading post was established in Tadoussac (in present-day Quebec), of which only five men survived the first winter. In 1604, Saint Croix Island in Acadia was the site of a short-lived French colony, much plagued by illness, perhaps scurvy. The following year the settlement was moved to Port Royal. Samuel de Champlain founded Quebec (1608) and explored the Great Lakes. In 1634, Jean Nicolet founded "La Baye des Puants" (present-day Green Bay), which is one of the oldest permanent European settlements in America. In 1634, Sieur de Laviolette founded Trois-Rivières. In 1642, Paul de Chomedey, Sieur de Maisonneuve, founded Ville-Marie which is now known as Montreal. Louis Jolliet and Jacques Marquette founded Sault Sainte Marie (1668) and Saint Ignace (1671) and explored the Mississippi River. At the end of the 17th century, René-Robert Cavelier, Sieur de La Salle established a network of forts going from the Gulf of Mexico to the Great Lakes and the Saint Lawrence River. Fort Saint Louis was established in Texas in 1685, but was gone by 1688. Antoine de la Mothe Cadillac founded "Fort Pontchartrain du Détroit" (modern-day Detroit) in 1701 and Jean-Baptiste Le Moyne, Sieur de Bienville founded "La Nouvelle Orléans" (New Orleans) in 1718. Pierre Le Moyne d'Iberville founded Baton Rouge in 1719.
The French were eager to explore North America but New France remained largely unpopulated. Due to the lack of women, intermarriages between French and Indians were frequent, giving rise to the Métis people. Relations between the French and Indians were usually peaceful. As the 19th-century historian Francis Parkman stated:
To boost the French population, Cardinal Richelieu issued an act declaring that Indians converted to Catholicism were considered as "natural Frenchmen" by the Ordonnance of 1627:
Louis XIV also tried to increase the population by sending approximately 800 young women nicknamed the "King's Daughters". However, the low density of population in New France remained a very persistent problem. At the beginning of the French and Indian War (1754–1763), the British population in North America outnumbered the French 20 to 1. France fought a total of six colonial wars in North America (see the four French and Indian Wars as well as Father Rale's War and Father Le Loutre's War).
French Florida.
In 1562, Charles IX, under the leadership of Admiral Gaspard de Coligny sends Jean Ribault and a group of Huguenot settlers in an attempt to colonize the Atlantic coast and found a colony on a territory which will take the name of the French Florida. They discover the probe and Port Royal Island, which will be called by Parris Island in South Carolina, on which he built a fort named
Charlesfort. The group, led by René Goulaine de Laudonnière, moved to the south where they founded the Fort Caroline on the Saint John's river in Florida on June 22, 1564. This irritated the Spanish who claimed Florida and opposed the Protestant settlers for religious reasons. In 1565, Pedro Menéndez de Avilés led a group of Spaniards and founded Saint Augustine, 60 kilometers south of Fort Caroline. Fearing a Spanish attack, Ribault planned to move the colony but a storm suddenly destroyed his fleet. On 20 September 1565 the Spaniards, commanded by Menéndez de Avilés, attacked and massacre all the Carolina occupants including Jean Ribaut.
Canada and Acadia.
The French interest in Canada focused first on fishing off the Grand Banks of Newfoundland. However, at the beginning of the 17th century, France was more interested in fur from North America. The fur trading post of Tadoussac was founded in 1600. Four years later, Champlain made his first trip to Canada in a trade mission for fur. Although he had no formal mandate on this trip, he sketched a map of the St. Lawrence River and in writing, on his return to France, a report entitled "Savages" (Relation of his stay in a tribe of Montagnais near Tadoussac).
Champlain needed to report his findings to Henry IV. He participated in another expedition to New France in the spring of 1604, conducted by Pierre Du Gua de Monts. It helped the foundation of a settlement on Saint Croix Island, the first French settlement in the New World, which would be given up the following winter. The expedition then founded the colony of Port-Royal.
In 1608, Champlain founded a fur post that would become the city of Quebec, which would become the capital of New France. In Quebec, Champlain forged alliances between France and the Huron and Ottawa against their traditional enemies, the Iroquois. Champlain and other French travelers then continued to explore North America, with canoes made from Birch bark, to move quickly through the Great Lakes and their tributaries. In 1634, the Normand explorer Jean Nicolet pushed his exploration to the West up to the state of Wisconsin.
Following the capitulation of Quebec by the Kirke brothers, the British occupied the city of Quebec and Canada from 1629 to 1632. Samuel de Champlain was taken prisoner and there followed the bankruptcy of the Company of One Hundred Associates. Following the Treaty of Saint-Germain-en-Laye, France took possession of the colony in 1632. The city of Trois-Rivières was founded in 1634. In 1642, the Angevin Jérôme le Royer de la Dauversière founded Ville-Marie (later Montreal) which was at that time, a fort as protection against Iroquois attacks (the first great Iroquois war lasted from 1642 to 1667).
Despite this rapid expansion, the colony developed very slowly. The Iroquois wars and diseases were the leading causes of death in the French colony. In 1663 when Louis XIV provided the "Royal Government", the population of New France was only 2500 European inhabitants. That year, to increase the population, Louis XIV sent between 800 and 900 'King's Daughters' to become the wives of French settlers. The population of New France reached subsequently 7000 in 1674 and 15000 in 1689.
From 1689 to 1713, the French settlers were faced with almost incessant war during the French and Indian Wars. From 1689 to 1697, they fought the British in the Nine Years' War. The war against the Iroquois continued even after the Treaty of Rijswijk until 1701, when the two parties agreed on peace. Then, the war against the English took over in the War of the Spanish Succession. In 1690 and 1711, Quebec City had successfully resisted the attacks of the English navy and then British army. Nevertheless, the British took advantage of the second war. With the signing of the Treaty of Utrecht in 1713, France ceded to Britain Acadia (with a population of 1700 people), Newfoundland and Hudson Bay.
Under the Sovereign Council, the population of the colony grew faster. However, the population growth was far inferior to that of the British Thirteen Colonies to the south. In the middle of the 18th century, New France accounted for 60,000 people while the British colonies had more than one million people. This placed the colony at a great military disadvantage against the British. The war between the colonies resumed in 1744, lasting until 1748. A final and decisive war began in 1754. The Canadiens and the French were helped by numerous alliances with Native Americans, but they were usually outnumbered on the battlefield.
Louisiana.
On May 17, 1673, explorers Louis Jolliet and Jacques Marquette began exploring the Mississippi River, known to the Sioux as "does Tongo," or to the Miami-Illinois as " missisipioui" ("the great river"). They reached the mouth of the Arkansas and then up the river, after learning that it flowed into the Gulf of Mexico and not to the California Sea (Pacific Ocean).
In 1682, the Normand Cavelier de la Salle and the Italian Henri de Tonti came down the Mississippi to its Delta. They left from Fort Crevecoeur on the Illinois River, along with 23 French and 18 Native Americans. In April 1682, they arrived at the mouth of the Mississippi; they planted a cross and a column bearing the arms of the king of France. Going back through the same route to Canada, La Salle returned to Versailles. There he won over the Secretary of State of the Navy to give him the command of Louisiana. He believed that it was close to New Spain by drawing a map on which the Mississippi seemed much further west than its actual rate. He set up a maritime expedition with four ships and 320 emigrants, but it ended in disaster when he failed to find the Mississippi Delta and was killed in 1687.
In 1698, Pierre LeMoyne d'Iberville left La Rochelle and explored the area around the mouth of the Mississippi. He stopped between Isle-aux-Chats (now Cat Island) and Isle Surgeres (renamed Isle-aux-Vascular or Ship Island) on February 13, 1699 and continued his explorations to the mainland, with his brother Jean-Baptiste Le Moyne de Bienville to Biloxi. He built a precarious fort, called 'Maurepas' (later 'Old Biloxi'), before returning to France. He returned twice in the Gulf of Mexico and established a fort at Mobile in 1702.
From 1699 to 1702, Pierre Le Moyne d'Iberville was governor of Louisiana. His brother succeeded him in that post from 1702 to 1713. He was again governor from 1716 to 1724 and again 1733 to 1743. In 1718, Jean-Baptiste Le Moyne de Bienville commanded a French expedition in Louisiana. He founded the city of New Orleans, in homage to Regent Duke of Orleans. The architect Adrian de Pauger drew the orthogonal plane of the Old Square.
The Mississippi Bubble.
In 1718, there were only 700 Europeans in Louisiana. The Mississippi Company arranged for ships to bring 800 more, who landed in Louisiana in 1718, doubling the European population. John Law encouraged Germans, particularly Germans of the Alsatian region who had recently fallen under French rule, and the Swiss to emigrate.
Prisoners were set free in Paris in September 1719 onwards, under the condition that they marry prostitutes and go with them to Louisiana. The newly married couples were chained together and taken to the port of embarkation. In May 1720, after complaints from the Mississippi Company and the concessioners about this class of French immigrants, the French government prohibited such deportations. However, there was a third shipment of prisoners in 1721.
Dissolution.
The last French and Indian War resulted in the dissolution of New France, with Canada going to Great Britain and Louisiana going to Spain. Only the islands of Saint-Pierre-et-Miquelon are still in French hands.
In 1802 Spain returned Louisiana to France, but Napoleon sold it to the United States in 1803. The French left many toponyms (Illinois, Vermont, Bayous...) and ethnonyms (Sioux, Coeur d'Alene, Nez Percé...) in North America.
West Indies.
A major French settlement lay on the island of Hispaniola, where France established the colony of Saint-Domingue on the western third of the island in 1664. Nicknamed the "Pearl of the Antilles", Saint-Domingue became the richest colony in the Caribbean due to slave plantation production of sugar cane. It had the highest slave mortality rate in the western hemisphere. A 1791 slave revolt, the only ever successful slave revolt, began the Haitian Revolution, led to freedom for the colony's slaves in 1794 and, a decade later, complete independence for the country, which renamed itself Haiti. France briefly also ruled the eastern portion of the island, which is now the Dominican Republic.
During the 17th and 18th centuries, France ruled much of the Lesser Antilles at various times. Islands that came under French rule during part or all of this time include Dominica, Grenada, Guadeloupe, Marie-Galante, Martinique, St. Barthélemy, St. Croix, St. Kitts, St. Lucia, St. Martin, St. Vincent and Tobago. Control of many of these islands was contested between the French, the British and the Dutch; in the case of St. Martin, the island was divided in two, a situation that persists to this day. Great Britain captured some of France's islands during the Seven Years' War and the Napoleonic Wars. Following the latter conflict, France retained control of Guadeloupe, Martinique, Marie-Galante, St. Barthélemy, and its portion of St. Martin; all remain part of France today. Guadeloupe (including Marie-Galante and other nearby islands) and Martinique each is an overseas department of France, while St. Barthélemy and St. Martin each became an overseas collectivity of France in 2007.
South America.
Brazil.
France Antarctique (formerly also spelled France antartique) was a French colony south of the Equator, in Rio de Janeiro, Brazil, which existed between 1555 and 1567, and had control over the coast from Rio de Janeiro to Cabo Frio. The colony quickly became a haven for the Huguenots, and was ultimately destroyed by the Portuguese in 1567.
On November 1, 1555, French vice-admiral Nicolas Durand de Villegaignon (1510–1575), a Catholic knight of the Order of Malta, who later would help the Huguenots to find a refuge against persecution, led a small fleet of two ships and 600 soldiers and colonists, and took possession of the small island of Serigipe in the Guanabara Bay, in front of present-day Rio de Janeiro, where they built a fort named Fort Coligny. The fort was named in honor of Gaspard de Coligny (then a Catholic statesman, that about a year later would become a Huguenot), an admiral who supported the expedition and would use the colony in order to protect his co-religionists.
To the still largely undeveloped mainland village, Villegaignon gave the name of Henriville, in honour of Henry II, the King of France, who also knew of and approved the expedition, and had provided the fleet for the trip. Villegaignon secured his position by making an alliance with the Tamoio and Tupinambá Indians of the region, who were fighting the Portuguese.
1557 Calvinist arrival.
Unchallenged by the Portuguese, who initially took little notice of his landing, Villegaignon endeavoured to expand the colony by calling for more colonists in 1556. He sent one of his ships, the Grande Roberge, to Honfleur, entrusted with letters to King Henry II, Gaspard de Coligny and according to some accounts, the Protestant leader John Calvin.
After one ship was sent to France to ask for additional support, three ships were financed and prepared by the king of France and put under the command of Sieur De Bois le Comte, a nephew of Villegagnon. They were joined by 14 Calvinists from Geneva, led by Philippe de Corguilleray, including theologians Pierre Richier and Guillaume Chartrier. The new colonists, numbering around 300, included 5 young women to be wed, 10 boys to be trained as translators, as well as 14 Calvinists sent by Calvin, and also Jean de Léry, who would later write an account of the colony. They arrived in March 1557. The relief fleet was composed of:
The Petite Roberge, with 80 soldiers and sailors was led by Vice Admiral Sieur De Bois le Comte.
The Grande Roberge, with about 120 on board, captained by Sieur de Sainte-Marie dit l'Espine.
The Rosée, with about 90 people, led by Captain Rosée.
Doctrinal disputes arose between Villegagnon and the Calvinists, especially in relation to the Eucharist, and in October 1557 the Calvinists were banished from Coligny island as a result. They settled among the Tupinamba until January 1558, when some of them managed to return to France by ship together with Jean de Léry, and five others chose to return to Coligny island where three of them were drowned by Villegagnon for refusing to recant.
Portuguese intervention.
In 1560 Mem de Sá, the new Governor-General of Brazil, received from the Portuguese government the command to expel the French. With a fleet of 26 warships and 2,000 soldiers, on 15 March 1560, he attacked and destroyed Fort Coligny within three days, but was unable to drive off their inhabitants and defenders, because they escaped to the mainland with the help of the Native Brazilians, where they continued to live and to work. Admiral Villegaignon had returned to France in 1558, disgusted with the religious tension that existed between French Protestants and Catholics, who had come also with the second group (see French Wars of Religion).
Urged by two influential Jesuit priests who had come to Brazil with Mem de Sá, named José de Anchieta and Manuel da Nóbrega, and who had played a big role in pacifying the Tamoios, Mem de Sá ordered his nephew, Estácio de Sá to assemble a new attack force. Estácio de Sá founded the city of Rio de Janeiro on March 1, 1565, and fought the Frenchmen for two more years. Helped by a military reinforcement sent by his uncle, on January 20, 1567, he imposed final defeat on the French forces and decisively expelled them from Brazil, but died a month later from wounds inflicted in the battle. Coligny's and Villegaignon's dream had lasted a mere 12 years.
Equinoctial France.
Equinoctial France was the contemporary name given to the colonization efforts of France in the 17th century in South America, around the line of Equator, before "tropical" had fully gained its modern meaning: Equinoctial means in Latin "of equal nights", i.e., on the Equator, where the duration of days and nights is nearly the same year round.
The French colonial empire in the New World also included New France (Nouvelle France) in North America, particularly in what is today the province of Quebec, Canada, and for a very short period (12 years) also Antarctic France (France Antarctique, in French), in present-day Rio de Janeiro, Brazil. All of these settlements were in violation of the papal bull of 1493, which divided the New World between Spain and Portugal. This division was later defined more exactly by the Treaty of Tordesillas.
History of France Équinoxiale.
France Équinoxiale started in 1612, when a French expedition departed from Cancale, Brittany, France, under the command of Daniel de la Touche, Seigneur de la Ravardière, and François de Razilly, admiral. Carrying 500 colonists, it arrived in the Northern coast of what is today the Brazilian state of Maranhão. De la Ravardière had discovered the region in 1604 but the death of the king postponed his plans to start its colonization.
The colonists soon founded a village, which was named "Saint-Louis", in honor of the French king Louis IX. This later became São Luís in Portuguese, the only Brazilian state capital founded by France. On 8 September, Capuchin friars prayed the first mass, and the soldiers started building a fortress. An important difference in relation to France Antarctique is that this new colony was not motivated by escape from religious persecutions to Protestants (see French Wars of Religion).
The colony did not last long. A Portuguese army assembled in the Captaincy of Pernambuco, under the command of Alexandre de Moura, was able to mount a military expedition, which defeated and expelled the French colonists in 1615, less than four years after their arrival in the land. Thus, it repeated the disaster spelt for the colonists of France Antarctique, in 1567. A few years later, in 1620, Portuguese and Brazilian colonists arrived in number and São Luís started to develop, with an economy based mostly in sugar cane and slavery.
French traders and colonists tried again to settle a France Équinoxiale further North, in what is today French Guiana, in 1626, 1635 (when the capital, Cayenne, was founded) and 1643. Twice a Compagnie de la France Équinoxiale was founded, in 1643 and 1645, but both foundered as a result of misfortune and mismanagement. It was only after 1674, when the colony came under the direct control of the French crown and a competent Governor took office, that France Équinoxiale became a reality. To this day, French Guiana is a department of France.
French Guiana was first settled by the French in 1604, although its earliest settlements were abandoned in the face of hostilities from the indigenous population and tropical diseases. The settlement of Cayenne was established in 1643, but was abandoned. It was re-established in the 1660s. Except for brief occupations by the English and Dutch in the 17th century, and by the Portuguese in the 19th century, Guiana has remained under French rule ever since. From 1851 to 1951 it was the site of a notorious penal colony, Devil's Island ("Île du Diable"). Since 1946, French Guiana has been an overseas department of France.
In 1860, a French adventurer, Orelie-Antoine de Tounens proclaimed himself king of Araucania and Patagonia. His claim was not accepted by foreign powers and Chile and Argentina took firm control over the regions, treating him as insane.

</doc>
<doc id="52461" url="https://en.wikipedia.org/wiki?curid=52461" title="938">
938

__NOTOC__
Year 938 (CMXXXVIII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="52462" url="https://en.wikipedia.org/wiki?curid=52462" title="Observational learning">
Observational learning

Observational learning is learning that occurs through observing the behavior of others. It is a form of social learning which takes various forms, based on various processes. In humans, this form of learning seems to not need reinforcement to occur, but instead, requires a social model such as a parent, sibling, friend, or teacher. Particularly in childhood, a model is someone of authority or higher status. In animals, observational learning is often based on classical conditioning, in which an instinctive behavior is elicited by observing the behavior of another (e.g. mobbing in birds), but other processes may be involved as well.
Human observational learning.
Many behaviors that a learner observes, remembers, and imitates are actions that models display, even though the model may not intentionally try to instill a particular behavior, A child may learn to swear, smack, smoke, and deem other inappropriate behavior acceptable through poor modeling. Bandura claims that children continually learn desirable and undesirable behavior through observational learning. Observational learning suggests that an individual's environment, cognition, and behavior all integrate and ultimately determine how the individual functions.
Through observational learning, individual behaviors can spread across a culture through a process called "diffusion chain". This basically occurs when an individual first learns a behavior by observing another individual and that individual serves as a model through whom other individuals learn the behavior, and so on.
Culture plays a role in whether observational learning is the dominant learning style in a person or community. Some cultures expect children to actively participate in their communities and are therefore exposed to different trades and roles on a daily basis. This exposure allows children to observe and learn the different skills and practices that are valued in their communities.
Albert Bandura, who is known for the classic Bobo doll experiment, identified this basic form of learning in 1961. The importance of observational learning consists of helping individuals, especially children, acquire new responses by observing others' behavior. Albert Bandura states that people’s behavior could be determined by their environment. Observational learning occurs through observing negative and positive behaviors. Bandura believes in reciprocal determinism in which the environment can influence in people’s behavior and vice versa. For instance, the Bobo doll experiment shows that model in a determined environment impact children’s behavior. In this experiment Bandura demonstrates that one group of children placed in an aggressive environment would act the same way. While, the control group and the other group of children placed in a passive role model environment hardly shows any type of aggressions.
In communities where children's primary mode of learning is through observation, the children are rarely separated from adult activities. This incorporation into the adult world at an early age allows children to use observational learning skills in multiple spheres of life. This learning through observation requires keen attentive abilities. Culturally, they learn that their participation and contributions are valued in their communities. This teaches children that it is their duty, as members of the community, to observe others' contributions so they gradually become involved and participate further in the community.
Stages.
Bandura's social cognitive learning theory states that there are four stages involved in observational learning:
Bandura clearly distinguishes between learning and performance. Unless motivated, a person does not produce learned behavior. This motivation can come from external reinforcement, such as the experimenter's promise of reward in some of Bandura's studies, or the bribe of a parent. Or it can come to vicarious reinforcement, based on the observation that models are rewarded. High-status models can affect performance through motivation. For example, girls aged 11 to 14 performed better on a motor performance task when they thought it was demonstrated by a high-status cheerleader than by a low-status model.
Some have even added a step of encoding a behavior between attention and retention.
Observational learning leads to a change in an individual's behavior along three dimensions:
Effect on behavior.
According to Bandura's social cognitive learning theory, observational learning can affect behavior in many ways, with both positive and negative consequences. It can teach completely new behaviors, for one. It can also increase or decrease the frequency of behaviors that have previously been learned. Observational learning can even encourage behaviors that were previously forbidden (for example, the violent behavior towards the Bobo doll that children imitated in Albert Bandura's study). Observational learning can also have an impact on behaviors that are similar to, but not identical to, the ones being modeled. For example, seeing a model excel at playing the piano may motivate an observer to play the saxophone.
Age difference.
Albert Bandura stressed that developing children learn from different social models, meaning that no two children are exposed to exactly the same modeling influence. From infancy to adolescence, they are exposed to various social models. A 2013 study found that a toddlers' previous social familiarity with a model was not always necessary for learning and that they were also able to learn from observing a stranger demonstrating or modeling a new action to another stranger.
It was once believed that babies could not imitate actions until the latter half of the first year. However a number of studies now report that infants as young as seven days can imitate simple facial expressions. By the latter half of their first year, 9-month-old babies can imitate actions hours after they first see them. As they continue to develop, toddlers around age two can acquire important personal and social skills by imitating a social model.
Deferred imitation is an important developmental milestone in a two-year-old, in which children not only construct symbolic representations, but can also remember information. Unlike toddlers, children of elementary school age are less likely to rely on imagination to represent an experience. Instead, they can verbally describe the model's behavior. Since this form of learning does not need reinforcement, it is more likely to occur regularly.
As age increases, age-related observational learning motor skills may decrease in athletes and golfers. Younger and skilled golfers have higher observational learning compared to older golfers and less skilled golfers.
Observational causal learning.
Humans use observational causal learning to watch what other people’s actions and use that information to find out how something works and how we can do it ourselves.
A study of 25-month-old infants found that they can learn causal relations from observing human interventions. They also learn by observing normal actions not created by intentional human action.
Comparisons with imitation.
Observational learning is presumed to have occurred when an organism copies an improbable action or action outcome that it has observed and the matching behavior cannot be explained by an alternative mechanism. Psychologists have been particularly interested in the form of observational learning known as imitation and in how to distinguish imitation from other processes. To successfully make this distinction, one must separate the degree to which behavioral similarity results from (a) predisposed behavior, (b) increased motivation resulting from the presence of another animal, (c) attention drawn to a place or object, (d) learning about the way the environment works, as distinguished from what we think of as (e) imitation (the copying of the demonstrated behavior) .
Observational learning differs from imitative learning in that it does not require a duplication of the behavior exhibited by the model. For example, the learner may observe an unwanted behavior and the subsequent consequences, and thus learn to refrain from that behavior. For example, Riopelle, A.J. (1960) found that monkeys did better with observational learning if they saw the "tutor" monkey make a mistake before making the right choice. Heyes (1993) distinguished imitation and non-imitative social learning in the following way: imitation occurs when animals learn about behavior from observing conspecifics, whereas non-imitative social learning occurs when animals learn about the environment from observing others.
Not all imitation and learning through observing is the same, and they often differ in the degree to which they take on an active or passive form. John Dewey describes an important distinction between two different forms of imitation: imitation as an end in itself and imitation with a purpose. Imitation as an end is more akin to mimicry, in which a person copies another’s act to repeat that action again. This kind of imitation is often observed in animals. Imitation with a purpose utilizes the imitative act as a means to accomplish something more significant. Whereas the more passive form of imitation as an end has been documented in some European American communities, the other kind of more active, purposeful imitation has been documented in other communities around the world.
Observation may take on a more active form in children’s learning in multiple Indigenous American communities. Ethnographic anthropological studies in Yucatec Mayan and Quechua Peruvian communities provide evidence that the home or community-centered economic systems of these cultures allow children to witness first-hand, activities that are meaningful to their own livelihoods and the overall well-being of the community. These children have the opportunity to observe activities that are relevant within the context of that community, which gives them a reason to sharpen their attention to the practical knowledge they are exposed to. This does not mean that they have to observe the activities even though they are present. The children often make an active decision to stay in attendance while a community activity is taking place to observe and learn. This decision underscores the significance of this learning style in many indigenous American communities. It goes far beyond learning mundane tasks through rote imitation; it is central to children’s gradual transformation into informed members of their communities’ unique practices. There was also a study, done with children, that concluded that Imitated behavior can be recalled and used in another situation or the same.
Apprenticeship.
Apprenticeship can involve both observational learning and modelling. Apprentices gain their skills in part through working with masters in their profession and through observing and evaluating the work of their fellow apprentices.Examples include renaissance inventor/painter Leonardo da Vinci and Michelangelo, before succeeding in their profession they were apprentices.
Learning without imitation.
Michael Tomasello described various ways of observational learning without the process of imitation in animals (ethology):
Exposure- Individuals learn about their environment with a close proximity to other individuals that have more experience. For example, a young dolphin learning the location of a plethora of fish by staying near its mother.
Peer model influences.
Observational learning is very beneficial when there are positive, reinforcing peer models involved. Although individuals go through four different stages for observational learning: attention; retention ; production; and motivation, this does not simply mean that when an individual's attention is captured that it automatically sets the process in that exact order. One of the most important ongoing stages for observational learning, especially among children, is motivation and positive reinforcement.
Performance is enhanced when children are positively instructed on how they can improve a situation and where children actively participate alongside a more skilled person. Examples of this are scaffolding and guided participation. Scaffolding refers to an expert responding contingently to a novice so the novice gradually increases their understanding of a problem. Guided participation refers to an expert actively engaging in a situation with a novice so the novice participates with or observes the adult to understand how to resolve a problem.
Cultural variation.
Cultural variation can be seen in the extent of information learned or absorbed by children through the use of observation and more specifically the use of observation without verbal requests for further information. For example, children from Mexican heritage families tend to learn and make better use of information observed during classroom demonstration then children of European heritage. Children of European heritage experience the type of learning that separates them from their family and community activities. They instead participate in lessons and other exercises in special settings such as school. Cultural backgrounds differ from each other in which children display certain characteristics in regards to learning an activity. Another example is seen in the immersion, of children in some Indigenous communities of the Americas, into the adult world and the effects it has on observational learning and the ability to complete multiple tasks simultaneously. This might be due to children in these communities having the opportunity to see a task being completed by their elders or peers and then trying to emulate the task. In doing so they learn to value observation and the skill-building it affords them because of the value it holds within their community. This type of observation is not passive, but reflects the child's intent to participate or learn within a community.
Observational learning can be seen taking place in many domains of Indigenous communities. The classroom setting is one significant example, and it functions differently for these communities in comparison to what is commonly present in Western schooling. The emphasis of keen observation in favor of supporting participation in ongoing activities strives to aid children to learn the important tools and ways of their community). Engaging in shared endeavors - with both the experienced and inexperienced - allows for the experienced to understand what the inexperienced need in order to grow in regards to the assessment of observational learning. The involvement of the inexperienced, or the children in this matter, can either be furthered by the children’s learning or advancing into the activity performed by the assessment of observational learning. For the Indigenous communities to rely on observational learning is a way allowing for their children to be a part of ongoing activities in the community (Tharp, 2006).
Although learning in the IAC is not always the central focus when participating in an activity studies have shown that attention in intentional observation differs from accidental observation. Intentional participation is “keen observation and listening in anticipation of, or in the process of engaging in endeavors”. This means that when they have the intention of participating in an event, their attention is more focused on the details, compared to when they are accidentally observing.
Observational learning can be an active process in many Indigenous American communities. The learner must take initiative to attend to activities going on around them. Children in these communities also take initiative to contribute their knowledge in ways that will benefit their community. For example, in many Indigenous American cultures, children perform household chores without being instructed to do so by adults. Instead, they observe a need for their contributions and take initiative to accomplish the tasks based on
observations of others having done them. The learner's intrinsic motivations play an important role in the child's understanding and construction of meaning in these educational experiences. The independence and responsibility associated with observational learning in many Indigenous American communities are significant reasons why this method of learning can involve more than just watching and imitating. A learner must be actively engaged with their demonstrations and experiences in order to fully comprehend and apply the knowledge they obtain.
Indigenous communities of the Americas.
Children from indigenous heritage communities of the Americas often learn through observation, a strategy that can carry over into adulthood. The heightened value towards observation allows children to multi-task and actively engage in simultaneous activities. The exposure to an uncensored adult lifestyle allows children to observe and learn the skills and practices that are valued in their communities. Children observe elders, parents, and siblings complete tasks and learn to participate in them. They are seen as contributors and learn to observe multiple tasks being completed at once and can learn to complete a task, while still engaging with other community members without being distracted.
Indigenous communities provide more opportunities to incorporate children in everyday life. This can be seen in some Mayan communities where children are given full access to community events, which allows observational learning to occur more often. Other children in Mazahua, Mexico are known to intensely observe ongoing activities. In native northern Canadian and indigenous Mayan communities, children often learn as third-party observers from stories and conversations by others. Most young Mayan children are carried on their mother's back, allowing them to observe their mother's work and see the world as their mother sees it. Children are often allowed to learn without restrictions and with minimal guidance. They are self-motivated to learn and finish their chores. These children act as a second set of eyes and ears for their parents updating them about the community.
Children aged 6 to 8 in an indigenous heritage community in Guadalajara, Mexico participated in hard work, such as cooking or running errands, to benefit the whole family, while those in the city of Guadalajara rarely did so. These children participated more in adult regulated activities and had little time to play, while those from the indigenous-heritage community had more time to play and initiate in their own after-school activities.
Within certain indigenous communities people do not typically seek out explanation beyond basic observation. This is because they are competent in learning through astute observation. In a Guatemalan footloom factory amateur adult weavers observed skilled weavers over the course of weeks without questioning or being given explanations; the amateur weaver moved at their own pace and began when they felt confident. The framework of learning how to weave through observation can serve as a model that groups within a society use as a reference to guide their actions in particular domains of life. Communities that participate in observational learning promote tolerance and mutual understand of those coming from different cultural backgrounds.
Other human and animal behavior experiments.
When an animal is given a task to complete, they are almost always more successful after observing another animal doing the same task before them. Experiments have been conducted on several different species with the same effect: animals can learn behaviors from peers. However, there is a need to distinguish the propagation of behavior and the stability of behavior. Research has shown that social learning can spread a behavior, but there are more factors regarding how a behavior carries across generations of an animal culture.
Learning in fish.
Experiments with ninespine sticklebacks showed that individuals will use social learning to locate food.
Acquiring foraging niches.
Studies have been conducted at the University of Oslo and University of Saskatchewan regarding the possibility of social learning in birds, delineating the difference between cultural and genetic acquisition. Strong evidence already exists for mate choice, bird song, predator recognition, and foraging.
Researchers cross-fostered eggs between nests of blue tits and great tits and observed the resulting behavior through audio-visual recording. Tits raised in the foster family learned their foster family's foraging sites early. This shift—from the sites the tits would among their own kind and the sites they learned from the foster parents—lasted for life. What young birds learn from foster parents, they eventually transmitted to their own offspring. This suggests cultural transmissions of foraging behavior over generations in the wild.
Social learning in crows.
The University of Washington studied this phenomenon with crows, acknowledging the evolutionary tradeoff between acquiring costly information firsthand and learning that information socially with less cost to the individual but at the risk of inaccuracy. The experimenters exposed wild crows to a unique “dangerous face” mask as they trapped, banded, and released 7-15 birds at five different study places around Seattle, WA. An immediate scolding response to the mask after trapping by previously captured crows illustrates that the individual crow learned the danger of that mask. There was a scolding from crows that were captured that had not been captured initially. That response indicates conditioning from the mob of birds that assembled during the capture.
Horizontal social learning (learning from peers) is consistent with the lone crows that recognized the dangerous face without ever being captured. Children of captured crow parents were conditioned to scold the dangerous mask, which demonstrates vertical social learning (learning from parents). The crows that were captured directly had the most precise discrimination between dangerous and neutral masks than the crows that learned from the experience of their peers. The ability of crows to learn doubled the frequency of scolding, which spread at least 1.2 km from where the experiment started to over a 5-year period at one site.
Propagation of animal culture.
Researchers at the Département d’Etudes Cognitives, Institut Jean Nicod, Ecole Normale Supérieure acknowledged a difficulty with research in social learning. To count acquired behavior as cultural, two conditions need must be met: the behavior must spread in a social group, and that behavior must be stable across generations. Research has provided evidence that imitation may play a role in the propagation of a behavior, but these researchers believe the fidelity of this evidence is not sufficient to prove stability of animal culture.
Other factors like ecological availability, reward-based factors, content-based factors, and source-based factors might explain the stability of animal culture in a wild rather than just imitation. As an example of ecological availability, chimps may learn how to fish for ants with a stick from their peers, but that behavior is also influenced by the particular type of ants as well as the condition. A behavior may be learned socially, but the fact that it was learned socially does not necessarily mean it will last. The fact that the behavior is rewarding has a role in cultural stability as well. The ability for socially-learned behaviors to stabilize across generations is also mitigated by the complexity of the behavior. Different individuals of a species, like crows, vary in their ability to use a complex tool. Finally, a behavior’s stability in animal culture depends on the context in which they learn a behavior. If a behavior has already been adopted by a majority, then the behavior is more likely to carry across generations out of a need for conforming.
Animals are able to acquire behaviors from social learning, but whether or not that behavior carries across generations requires more investigation.
Hummingbird experiment.
Experiments with hummingbirds provided one example of apparent observational learning in a non-human organism. Hummingbirds were divided into two groups. Birds in one group were exposed to the feeding of a knowledgeable "tutor" bird; hummingbirds in the other group did not have this exposure. In subsequent tests the birds that had seen a tutor were more efficient feeders than the others.
Bottlenose dolphin.
Herman (2002) suggested that bottlenose dolphins produce goal-emulated behaviors rather than imitative ones. A dolphin that watches a model place a ball in a basket might place the ball in the basket when asked to mimic the behavior, but it may do so in a different manner seen.
Rhesus monkey.
Kinnaman (1902) reported that one rhesus monkey learned to pull a plug from a box with its teeth to obtain food after watching another monkey succeed at this task.
Fredman (2012) also performed an experiment on observational behavior. In experiment 1, human-raised monkeys observed a familiar human model open a foraging box using a tool in one of two alternate ways: levering or poking. In experiment 2, mother-raised monkeys viewed similar techniques demonstrated by monkey models. A control group in each population saw no model. In both experiments, independent coders detected which technique experimental subjects had seen, thus confirming social learning. Further analyses examined copying at three levels of resolution.
The human-raised monkeys exhibited the greatest learning with the specific tool use technique they saw. Only monkeys who saw the levering model used the lever technique, by contrast with controls and those who witnessed poking. Mother-reared monkeys instead typically ignored the tool and exhibited fidelity at a lower level, tending only to re-create whichever result the model had achieved by either levering or poking.
Nevertheless, this level of social learning was associated with significantly greater levels of success in monkeys witnessing a model than in controls, an effect absent in the human-reared population. Results in both populations are consistent with a process of canalization of the repertoire in the direction of the approach witnessed, producing a narrower, socially shaped behavioral profile than among controls who saw no model.
Light box experiment.
Pinkham and Jaswal (2011) did an experiment to see if a child would learn how to turn on a light box by watching a parent. They found that children who saw a parent use their head to turn on the light box tended to do the task in that manner, but children who had not seen the parent chose a more efficient way, using their hands.
Swimming skill performance.
When adequate practice and appropriate feedback follow demonstrations, increased skill performance and learning occurs. Lewis (1974) did a study of children who had a fear of swimming and observed how modelling and going over swimming practices affected their overall performance. The experiment spanned nine days, and included many steps. The children were first assessed on their anxiety and swimming skills. Then they were placed into one of three conditional groups and exposed to these conditions over a few days.
At the end of each day, all children participated in a group lesson. The first group was a control group where the children watched a short cartoon video unrelated to swimming. The second group was a peer mastery group, which watched a short video of similar-aged children who had very good task performances and high confidence. Lastly, the third group was a peer coping group, whose subjects watched a video of similar-aged children who progressed from low task performances and low confidence statements to high task performances and high confidence statements.
The day following the exposures to each condition, the children were reassessed. Finally, the children were also assessed a few days later for a follow up assessment. Upon reassessment, it was shown that the two model groups who watched videos of children similar in age had successful rates on the skills assessed because they perceived the models as informational and motivational.
Neuroscience.
Recent research in neuroscience has implicated mirror neurons as a neurophysiological basis for observational learning. These specialized visuomotor neurons fire action potentials when an individual performs a motor task and also fire when an individual passively observes another individual performing the same motor task. In observational motor learning, the process begins with a visual presentation of another individual performing a motor task, this acts as a model. The learner then needs to transform the observed visual information into internal motor commands that will allow them to perform the motor task, this is known as visuomotor transformation. Mirror neuron networks provide a mechanism for visuo-motor and motor-visual transformation and interaction. Similar networks of mirror neurons have also been implicated in social learning, motor cognition and social cognition.

</doc>
<doc id="52463" url="https://en.wikipedia.org/wiki?curid=52463" title="942">
942

__NOTOC__
Year 942 (CMXLII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52464" url="https://en.wikipedia.org/wiki?curid=52464" title="445">
445

__NOTOC__
Year 445 (CDXLV) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valentinianus and Nomus (or, less frequently, year 1198 "Ab urbe condita"). The denomination 445 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52465" url="https://en.wikipedia.org/wiki?curid=52465" title="444">
444

__NOTOC__
Year 444 (CDXLIV) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Aginatius (or, less frequently, year 1197 "Ab urbe condita"). The denomination 444 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52467" url="https://en.wikipedia.org/wiki?curid=52467" title="446">
446

__NOTOC__
Year 446 (CDXLVI) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Aetius and Symmachus (or, less frequently, year 1199 "Ab urbe condita"). The denomination 446 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52468" url="https://en.wikipedia.org/wiki?curid=52468" title="442">
442

__NOTOC__
Year 442 (CDXLII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Dioscorus and Eudoxius (or, less frequently, year 1195 "Ab urbe condita"). The denomination 442 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="52469" url="https://en.wikipedia.org/wiki?curid=52469" title="Stephen Wolfram">
Stephen Wolfram

Stephen Wolfram (born 29 August 1959) is a British scientist known for his work in computer science, mathematics, and in theoretical physics. He is the author of the book "A New Kind of Science." In 2012 he became a fellow of the American Mathematical Society.
He is the founder and CEO of the software company Wolfram Research where he worked as chief designer of Mathematica and the Wolfram Alpha answer engine. His recent work has been on knowledge-based programming, expanding and refining the programming language of Mathematica into what is now called the Wolfram Language. His book "An Elementary Introduction to the Wolfram Language" appeared in 2015.
Early life.
Stephen Wolfram was born in London in 1959 to Hugo and Sybil Wolfram.
Hugo Wolfram.
Stephen's father, Hugo Wolfram (born 1925 in Bochum, Germany), a textile manufacturer, served as managing director of the Lurex Company, makers of the fabric Lurex and was also author of three novels. Hugo Wolfram was born in Germany, emigrating to England in 1933. When World War II broke out, young Hugo left school at 15 and subsequently found it hard to get a job since he was regarded as an "enemy alien." As an adult, he took correspondence courses in philosophy and psychology.
Sybil Wolfram.
Stephen's mother, Sybil Wolfram (1931–1993; born Sybille Misch) was a Fellow and Tutor in philosophy at Lady Margaret Hall at University of Oxford from 1964 to 1993. She published two books, "Philosophical Logic: An Introduction" (1989) She was the translator of Claude Lévi-Strauss's "La pensée sauvage" ("The Savage Mind"), but later disavowed the translation. She was the daughter of criminologist and psychoanalyst Kate Friedlander (1902–1949), an expert on the subject of juvenile delinquency, After the Reichstag fire in 1933, she emigrated from Berlin, Germany to England with her parents and Jewish psychoanalyst, Paula Heimann (1899–1982).
Family.
Stephen is married to a mathematician and they have four children.
Education and early career.
As a young child, Wolfram initially struggled in school and had difficulties learning arithmetic. At the age of 12, he wrote a dictionary on physics. By 13 or 14, he had written three books on particle physics. They were not published.
Particle physics.
By age 15 he began research in applied quantum field theory and particle physics and publish scientific papers. Topics included matter creation and annihilation, the fundamental interactions, elementary particles and their currents, hadronic and leptonic physics, and the parton model, published in professional peer-reviewed scientific journals including "Nuclear Physics B", "Australian Journal of Physics", "Nuovo Cimento", and "Physical Review D". Working independently, Wolfram published a widely cited paper on heavy quark production at age 18 and nine other papers, and continued research and to publish on particle physics into his early twenties. Wolfram's work with Geoffrey C. Fox on the theory of the strong interaction is still used in experimental particle physics.
He was educated at Eton College, but left prematurely in 1976. He entered St. John's College, Oxford at age 17 but found lectures "awful", and left in 1978 without graduating to attend the California Institute of Technology, the following year, where he received a PhD in particle physics on November 19, 1979 at age 20. Wolfram's thesis committee was composed of Richard Feynman, Peter Goldreich, Frank J. Sciulli and Steven Frautschi, and chaired by Richard D. Field.
A 1981 letter from Feynman to Gerald Freund giving reference for Wolfram for the MacArthur grant appears in Feynman's collective letters, "Perfectly Reasonable Deviations from the Beaten Track". Following his PhD, Wolfram joined the faculty at Caltech and became the youngest recipient of the MacArthur Fellowships in 1981, at age 21.
Later career.
Complex systems and cellular automata.
In 1983, Wolfram left for the School of Natural Sciences of the Institute for Advanced Study in Princeton, where he conducted research into cellular automata, mainly with computer simulations. He produced a series of papers systematically investigating the class of elementary cellular automata, conceiving the Wolfram code, a naming system for one-dimensional cellular automata, and a classification scheme for the complexity of their behaviour. He conjectured that the Rule 110 cellular automaton might be Turing complete.
A 1985 letter, from Feynman to Wolfram, also appears in Feynman's letters. In it, in response to Wolfram writing to him that he was thinking about creating some kind of institute where he might study complex systems, Feynman tells Wolfram, "You do not understand ordinary people," and advises him "find a way to do your research with as little contact with non-technical people as possible."
In the mid-1980s, Wolfram worked on simulations of physical processes (such as turbulent fluid flow) with cellular automata on the Connection Machine alongside Richard Feynman and helped initiate the field of complex systems, founding the first institute devoted to this subject, The Center for Complex Systems Research (CCSR) at the University of Illinois at Urbana–Champaign and the journal "Complex Systems" in 1987.
Symbolic Manipulation Program.
Wolfram led the development of the computer algebra system SMP ("Symbolic Manipulation Program") in the Caltech physics department during 1979–1981. A dispute with the administration over the intellectual property rights regarding SMP—patents, copyright, and faculty involvement in commercial ventures—eventually caused him to resign from Caltech. SMP was further developed and marketed commercially by Inference Corp. of Los Angeles during 1983–1988.
Mathematica.
In 1986 Wolfram left the Institute for Advanced Study for the University of Illinois at Urbana–Champaign where he founded their Center for Complex Systems Research and started to develop the computer algebra system Mathematica, which was first released in 1988, when he left academia. In 1987 he founded a company called Wolfram Research which continues to develop and market the program.
Near the end of Sybil Wolfram's life, as part of her research for "In-laws and Outlaws", she used her son's program "Mathematica" to analyze her data.
Wolfram's younger brother, Conrad Wolfram, serves as CEO of Wolfram Research Europe, Ltd.
"A New Kind of Science".
From 1992 to 2002, he worked on his controversial book "A New Kind of Science", which presents an empirical study of very simple computational systems. Additionally, it argues that for fundamental reasons these types of systems, rather than traditional mathematics, are needed to model and understand complexity in nature. Wolfram's conclusion is that the universe is digital in its nature, and runs on fundamental laws which can be described as simple programs. He predicts that a realisation of this within the scientific communities will have a major and revolutionary influence on physics, chemistry and biology and the majority of the scientific areas in general, which is the reason for the book's title.
Since the release of the book in 2002, Wolfram has split his time between developing Mathematica and encouraging people to get involved with the subject matter of "A New Kind of Science" by giving talks, holding conferences, and starting a summer school devoted to the topic.
Computational knowledge engine.
In March 2009, Wolfram announced Wolfram|Alpha, an answer engine. Wolfram|Alpha later launched in May 2009, and a paid-for version with extra features launched on February 2012. The engine is based on natural language processing and a large library of algorithms, and answers queries using the approach described in "A New Kind of Science". The application programming interface allows other applications to extend and enhance Alpha. Wolfram believes that as Wolfram Alpha comes into common use, "It will raise the level of scientific things that the average person can do."
Wolfram|Alpha is one of the answer engines behind Microsoft's Bing and Apple's Siri answering factual questions.
Wolfram Language.
In June 2014, Wolfram officially announced the Wolfram Language as a new general multi-paradigm programming language. The documentation for the language was pre-released in October 2013 to coincide with the bundling of Mathematica and the Wolfram Language on every Raspberry Pi computer. While the Wolfram Language has existed for over 25 years as the primary programming language used in Mathematica, it was not officially named until 2014. Wolfram's son, Christopher Wolfram, appeared on the program of SXSW giving a live-coding demonstration using Wolfram Language and has blogged about Wolfram Language for Wolfram Research.
On 8 December 2015, Wolfram published the book "An Elementary Introduction to the Wolfram Language" to introduce people with no knowledge of programming to the Wolfram Language and the kind of computational thinking it allows. 

</doc>
<doc id="52476" url="https://en.wikipedia.org/wiki?curid=52476" title="History of Greenland">
History of Greenland

The history of Greenland is a history of life under extreme Arctic conditions: currently, an ice cap covers about 80 percent of the island, restricting human activity largely to the coasts.
The first humans are thought to have arrived in Greenland around 2500 BC. Their descendants apparently died out and were succeeded by several other groups migrating from continental North America. There is no evidence that Greenland was known to Europeans until the 10th century, when Icelandic Vikings settled on its southwestern coast, which seems to have been uninhabited when they arrived. The ancestors of the Inuit Greenlanders who live there today appear to have migrated there later, around 1200 AD, from northwestern Greenland. While the Inuit survived in the icy world of the Little Ice Age, the early Norse settlements along the southwestern coast disappeared, leaving the Inuit as the only inhabitants of the island for several centuries. During this time, Denmark-Norway, apparently believing the Norse settlements had survived, continued to claim sovereignty over the island despite the lack of any contact between the Norse Greenlanders and their Scandinavian brethren. In 1721, aspiring to become a colonial power, Denmark-Norway sent a missionary expedition to Greenland with the stated aim of reinstating Christianity among descendants of the Norse Greenlanders who may have reverted to paganism. When the missionaries found no descendants of the Norse Greenlanders, they baptized the Inuit Greenlanders they found living there instead. Denmark-Norway then developed trading colonies along the coast and imposed a trade monopoly and other colonial privileges on the area.
During World War II, when Germany invaded Denmark, Greenlanders became socially and economically less connected to Denmark and more connected to the United States and Canada. After the war, Denmark resumed control of Greenland and in 1953, converted its status from colony to overseas "amt" (county). Although Greenland is still a part of the Kingdom of Denmark, it has enjoyed home rule since 1979. In 1985, the island became the only territory to leave the European Union, which it had joined as a part of Denmark in 1973; the Faroes had never joined.
Early Paleo-Eskimo cultures.
The prehistory of Greenland is a story of repeated waves of Paleo-Eskimo immigration from the islands north of the North American mainland. (The peoples of those islands are thought to have descended, in turn, from inhabitants of Siberia who migrated into Canada thousands of years ago.) Because of Greenland's remoteness and climate, survival there was difficult. Over the course of centuries, one culture succeeded another as groups died out and were replaced by new immigrants. Archaeology can give only approximate dates for the cultures that flourished before the Norse exploration of Greenland in the 10th century.
The earliest known cultures in Greenland are the Saqqaq culture (2500–800 BC) and the Independence I culture in northern Greenland (2400–1300 BCE). The practitioners of these two cultures are thought to have descended from separate groups that came to Greenland from northern Canada. Around 800 BCE, the so-called Independence II culture arose in the region where the Independence I culture had previously existed. it was originally thought that Independence II was succeeded by Dorset culture (700 BC–200 AD), but some Independence II artefacts date from as recently as the 1st century BCE. Recent studies suggest that, in Greenland at least, the Dorset culture may be better understood as a continuation of Independence II culture; the two cultures have therefore been designated "Greenlandic Dorset". Artefacts associated with early Dorset culture in Greenland have been found as far north as Inglefield Land on the west coast and the Dove Bugt area on the east coast.
After the Early Dorset culture disappeared around 200 AD, the island was uninhabited for several centuries. The next immigrants to arrive from Canada, perhaps as early as 800, settled the northwest part of the island, bringing with them the so-called Late Dorset culture, which survived until about 1300. The Norse arrived and settled in the southern part of the island in 980.
Norse settlement.
Europeans became aware of Greenland's existence, probably in the early 10th century, when Gunnbjörn Ulfsson, sailing from Norway to Iceland, was blown off course by a storm, and happened to sight some islands off Greenland. During the 980s, explorers led by Erik the Red set out from Iceland and reached the southwest coast of Greenland, found the region uninhabited, and settled there. Eirik named the island Greenland ("Grænland" in Old Norse and modern Icelandic, "Grønland" in modern Danish and Norwegian) in effect as a marketing device. Both the "Book of Icelanders" ("Íslendingabók", a medieval account of Icelandic history from the 12th century onward) and the "Saga of Eric the Red" ("Eiríks saga rauða", a medieval account of his life and of the Norse settlement of Greenland) state ""He named the land Greenland, saying that people would be eager to go there if it had a good name"."
According to the sagas, Erik the Red was exiled from Iceland for three years for committing some murders. He sailed to Greenland, where he explored the coastline and claimed certain regions as his own. He then returned to Iceland to persuade people to join him in establishing a settlement on Greenland. The Icelandic sagas say that 25 ships left Iceland with Erik the Red in 985, and that only 14 of them arrived safely in Greenland. This date has been approximately confirmed by radiocarbon dating of some remains at the first settlement at Brattahlid (now Qassiarsuk), which yielded a date of about 1000. According to the sagas, it was also in the year 1000 that Erik's son, Leif Eirikson, left the settlement to explore the regions around Vinland, which is generally assumed to have been located in what is now Newfoundland.
The Norse settled in three separate locations: the larger Eastern settlement, the smaller Western settlement, and the still smaller Middle Settlement (often considered part of the Eastern one). The settlements at their height are estimated to have had a population of between 2,000 and 10,000 people with more recent estimates tending toward the lower figure. Ruins of approximately 620 farms have been identified: 500 in the Eastern settlement, 95 in the Western settlement, and 20 in the Middle. The settlements carried on a trade in ivory from walrus tusks with Europe, as well as exporting rope, sheep, seals, wool and cattle hides according to one 13th-century account. They depended on Iceland and Norway for iron tools, wood (especially for boat building, although they also may have obtained some wood from coastal Labrador), supplemental foodstuffs, and religious and social contacts. Trade ships from Iceland and Norway traveled to Greenland every year and would sometimes overwinter in Greenland. Beginning in the late 13th century, all ships from Greenland were required by law to sail directly to Norway.
In 1126, a diocese was founded at Garðar (now Igaliku). It was subject to the Norwegian archdiocese of Nidaros (now Trondheim); at least five churches in Norse Greenland are known from archeological remains. In 1261, the population accepted the overlordship of the Norwegian King as well, although it continued to have its own law. In 1380 the Norwegian kingdom entered into a personal union with the Kingdom of Denmark. After initially thriving, the Norse settlements declined in the 14th century. The Western Settlement was abandoned around 1350. In 1378, there was no longer a bishop at Garðar. After 1408, when a marriage was recorded, not many written records mention the settlers. There are correspondence between the Pope and the Biskop Bertold af Garde from same year. The Danish Cartographer Claudius Clavus seems to have visited Greenland in 1420 from documents written by Nicolas Germanus and Henricus Martellus who had access to original cartographic notes and map by Clavus. Two mathematical manuscripts containing the second chart of the Claudius Clavus map from his travel to Greenland where he himself mapped the area were found during the late 20th century by the Danish scholar Bjönbo and Petersen. (Originals in Hofbibliothek at Vienna. A Greenlander in Norway, on visit; it is also mentioned in a Norwegian Diploma from 1426, Grønlendiger)
In a letter dated 1448 from Rome, the Pope Nicholas V prescribe the bishops of Skálholt and Hólar (the two Icelandic episcopal sees) to ensure to provide the inhabitants of Greenland with priests and a bishop, the latter of which they hadn't had in the 30 years since the apparent coming of the heathens when most churches were destroyed and the people taken away as prisoners.
It is probable that the Eastern Settlement was defunct by the middle of the 15th century although no exact date has been established.
Norse failure.
There are many theories as to why the Norse settlements collapsed in Greenland after surviving for some 450–500 years (985 to 1450–1500). Among the factors that have been suggested as contributing to the demise of the Greenland colony are cumulative environmental damage; gradual climate change; conflicts with hostile neighbors; loss of contact and support from Europe; cultural conservatism and failure to adapt to an increasingly harsh natural environment; and opening of opportunities elsewhere after plague had left many farmsteads abandoned in Iceland and Norway. Numerous studies have tested these hypotheses and some have led to significant discoveries. On the other hand, there are dissenters: In "The Frozen Echo," Kirsten Seaver contests some of the more generally accepted theories about the demise of the Greenland colony, and asserts that the colony, towards the end, was healthier than Diamond and others have thought. Seaver believes that the Greenlanders cannot have starved to death, but rather may have been wiped out by Inuit or unrecorded European attacks, or they may have abandoned the colony for Iceland or Vinland. However, the physical evidence from archeological studies of the ancient farm sites does not show evidence of attack. The paucity of personal belongings at these sites is typical of North Atlantic Norse sites that were abandoned in an orderly fashion, with any useful items being deliberately removed; but to others it suggests a gradual but devastating impoverishment. Midden heaps at these sites do show an increasingly impoverished diet for humans and livestock.
Greenland was always colder in winter than Iceland and Norway, and its terrain less hospitable to agriculture. Erosion of the soil was a danger from the beginning, one that the Greenland settlements may not have recognized until it was too late. For an extended time, nonetheless, the relatively warm West Greenland current flowing northwards along the southwestern coast of Greenland made it feasible for the Norse to farm much as their relatives did in Iceland or northern Norway. Palynologists' tests on pollen counts and fossilized plants prove that the Greenlanders must have struggled with soil erosion and deforestation. As the unsuitability of the land for agriculture became more and more patent, the Greenlanders resorted first to pastoralism and then to hunting for their food. But they never learned to use the hunting techniques of the Inuit, one being a farming culture, the other living on hunting in more northern areas with pack ice.
To investigate the possibility of climatic cooling, scientists drilled into the Greenland ice caps to obtain core samples. The oxygen isotopes from the ice caps suggested that the Medieval Warm Period had caused a relatively milder climate in Greenland, lasting from roughly 800 to 1200. However, from 1300 or so the climate began to cool. By 1420, the "Little Ice Age" had reached intense levels in Greenland. Excavations of midden or garbage heaps from the Viking farms in both Greenland and Iceland show the shift from the bones of cows and pigs to those of sheep and goats. As the winters lengthened, and the springs and summers shortened, there must have been less and less time for Greenlanders to grow hay. A study of North Atlantic seasonal temperature variability showed a significant decrease in maximum summer temperatures beginning in the late 13th century to early 14th century—as much as 6-8 °C lower than modern summer temperatures. The study also found that the lowest winter temperatures of the last 2,000 years occurred in the late 14th century and early 15th century. By the mid-14th century deposits from a chieftain’s farm showed a large number of cattle and caribou remains, whereas, a poorer farm only several kilometers away had no trace of domestic animal remains, only seal. Bone samples from Greenland Norse cemeteries confirm that the typical Greenlander diet had increased by this time from 20% sea animals to 80%.
Although Greenland seems to have been uninhabited at the time of initial Norse settlement, the Thule people migrated south and finally came into contact with the Norse in the 12th century. There are limited sources showing the two cultures interacting; however, scholars know that the Norse referred to the Inuit (and Vinland natives) as skraeling. The "Icelandic Annals" are among the few existing sources that confirm contact between the Norse and the Inuit. They report an instance of hostility initiated by the Inuit against the Norse, leaving eighteen Greenlanders dead and two boys carried into slavery. Archaeological evidence seems to show that the Inuit traded with the Norse. On the other hand, the evidence shows many Norse artefacts at Inuit sites throughout Greenland and on the Canadian Arctic islands but very few Inuit artefacts in the Norse settlements. This may indicate either European indifference—an instance of cultural resistance to Inuit crafts among them—or perhaps hostile raiding by the Inuit. It is also quite possible that the Norse were trading for perishable items such as meat and furs and had little interest in other Inuit items, much as later Europeans who traded with Native Americans.
The Norse never learned the Inuit techniques of kayak navigation or ring seal hunting. Archaeological evidence plainly establishes that by 1300 or so the Inuit had successfully expanded their winter settlements as close to the Europeans as the outer fjords of the Western Settlement. By 1350, the Norse had completely deserted their Western Settlement. The Inuit, being a hunting society, may have hunted the Norse livestock, forcing the Norse into conflict or abandonment of their settlements.
In mild weather conditions, a ship could make the 900-mile (1400 kilometers) trip from Iceland to Eastern Settlement within a couple of weeks. Greenlanders had to keep in contact with Iceland and Norway in order to trade. Little is known about any distinctive shipbuilding techniques among the Greenlanders. Greenland lacks a supply of lumber, so was completely dependent on Icelandic merchants or, possibly, logging expeditions to the Canadian coast.
The sagas mention Icelanders traveling to Greenland to trade. Settlement chieftains and large farm owners controlled this trade. Chieftains would trade with the foreign ships and then disperse the goods by trading with the surrounding farmers. The Greenlanders' main commodity was the walrus tusk, which was used primarily in Europe as a substitute for elephant ivory for art décor, whose trade had been blocked by conflict with the Islamic world. Professor Gudmundsson suggests a very valuable narwhal tusk trade, through a smuggling route between western Iceland and the Orkney islands.
Many scholars believe that the royal Norwegian monopoly on shipping contributed to the end of trade and contact. However, Christianity and European customs continued to hold sway among the Greenlanders for the greater part of the 14th and 15th centuries. In 1921, a Danish historian, Paul Norland, found human remains from the Eastern Settlement in the Herjolfsnes church courtyard. The bodies were dressed in 15th century medieval clothing with no indications of malnutrition or inbreeding. Most had crucifixes around their necks with their arms crossed as in a stance of prayer. Roman papal records report that the Greenlanders were excused from paying their tithes in 1345 because the colony was suffering from poverty. The last reported ship to reach Greenland was a private ship that was "blown off course", reaching Greenland in 1406, and departing in 1410 with the last news of Greenland: the burning at the stake of a condemned witch, the insanity and death of the woman this witch was accused of attempting to seduce through witchcraft, and the marriage of the ship's captain, Thorsteinn Ólafsson, to another Icelander, Sigridur Björnsdóttir. However, there are some suggestions of much later unreported voyages from Europe to Greenland, possibly as late as the 1480s. In the mid-1500s, a German ship drifted off-course to Greenland and, coming ashore a small island, discovered the body of a dead man lying face down who demonstrated cultural traits of both Norse and Inuit. An Icelandic crew member of the ship wrote: "He had a hood on his head, well sewn, and clothes from both homespun and sealskin. At his side lay a carving knife bent and worn down by whetting. This knife they took with them for display."
One intriguing fact is that very few fish remains are found among their middens. This has led to much speculation and argument. Most archaeologists reject any decisive judgment based on this one fact, however, as fish bones decompose more quickly than other remains, and may have been disposed of in a different manner. Isotope analysis of the bones of inhabitants shows that marine food sources supplied more and more of the diet of the Norse Greenlanders, making up between 50% and 80% of their diet by the 14th century.
One Inuit story recorded in the 18th century tells that raiding expeditions by European ships over the course of three years destroyed the settlement, after which many of the Norse sailed away south and the Inuit took in some of the remaining women and children before the final attack.
Late Dorset and Thule cultures.
The Norse may not have been alone on the island when they arrived; a new influx of Arctic people from the west, the Late Dorset culture, may predate them. However, this culture was limited to the extreme northwest of Greenland, far from the Norse who lived around the southern coasts. Some archaeological evidence may point to this culture slightly predating the Norse settlement. It disappeared around 1300, around the same time as the westernmost of the Norse settlements disappeared. In the region of this culture, there is archaeological evidence of gathering sites for around four to thirty families, living together for a short time during their movement cycle.
Around 1200, another Arctic culture, the Thule, arrived from the west, having emerged 200 years earlier in Alaska. They settled south of the Late Dorset culture and ranged over vast areas of Greenland's west and east coasts. These people, the ancestors of the modern Inuit, were flexible and engaged in the hunting of almost all animals on land and in the ocean, including big whales. They had dogs, which the Dorset did not, and used them to pull the dog sledges; they also used bows and arrows, contrary to the Dorset. Increasingly settled, they had large food storages to avoid winter famine. The early Thule avoided the highest latitudes, which only became populated again after renewed immigration from Canada in the 19th century.
The nature of the contacts between the Thule, Dorset and Norse cultures is not clear, but may have included trade elements. The level of contact is currently the subject of widespread debate, possibly including Norse trade with Thule or Dorsets in Canada or possible scavenging of abandoned Norse sites (see also Maine penny). No Norse trade goods are known in Dorset archaeological sites in Greenland; the only Norse items found have been characterized as "exotic items". Carved screw threads on tools and carvings with beards found in settlements on the Canadian Arctic islands show contact with the Norse. Some stories tell of armed conflicts between, and kidnappings by, both Inuit and Norse groups. The Inuit may have reduced Norse food sources by displacing them on hunting grounds along the central west coast. These conflicts can be one contributing factor to the disappearance of the Norse culture as well as for the Late Dorset, but few see it as the main reason.
Danish recolonization.
Most of the old Norse records concerning Greenland were removed from Trondheim to Copenhagen in 1664 and subsequently lost, probably in the 1728 fire there. The precise rediscovery is uncertain because south-drifting icebergs during the Little Ice Age long made the eastern coast unreachable, leading to general confusion between Baffin Island, Greenland, and Spitsbergen as seen, for example, in the difficulty locating the Frobisher "Strait", which was not confirmed to be a bay until 1861. Nonetheless, interest in discovering a Northwest Passage to Asia led to repeated expeditions in the area, though none were successful until Roald Amundsen in 1906 and even that success involved his being iced in for two years. Christian I of Denmark purportedly sent an expedition to the region under Pothorst and Pining to Greenland in 1472 or 1473; Henry VII of England sent another under Cabot in 1497 and 1498; Manuel I of Portugal sent a third under Corte-Real in 1500 and 1501. It had certainly been generally charted by the 1502 Cantino map, which includes the southern coastline. The island was "rediscovered" yet again by Martin Frobisher in 1578, prompting the Danish king Frederick II to outfit a new expedition of his own the next year under the Englishman James Alday; this proved a costly failure. The influence of English and Dutch whalers became so pronounced that for a time the western shore of the island itself became known as "Davis Strait" () after John Davis's 1585 and 1586 expeditions, which charted the western coast as far north as Disko Bay.
Meanwhile, following Sweden's exit from the Kalmar Union, the remaining states in the personal union were reorganized into Denmark-Norway in 1536. In protest against foreign involvement in the region, the Greenlandic polar bear was included in the state's coat of arms in the 1660s (It was removed in 1958). In the second half of the 17th century Dutch, German, French, Basque, and Dano-Norwegian ships hunted bowhead whales in the pack ice off the east coast of Greenland, regularly coming to shore to trade and replenish drinking water. Foreign trade was later forbidden by Danish monopoly merchants.
From 1711 to 1721, the Norwegian cleric Hans Egede petitioned King Frederick IV for funding to travel to Greenland and re-establish contact with the Norse settlers there. Presumably, such settlers would still be Catholic or even pagan and he desired to establish a mission among them to spread the Reformation. Frederick permitted Egede and some Norwegian merchants to establish the Bergen Greenland Company to revive trade with the island but refused to grant them a monopoly over it for fear of antagonizing Dutch whalers in the area. The Royal Mission College assumed superintendency over the mission and provided the company with a small stipend. Egede found but misidentified the ruins of the Norse colony, went bankrupt amid repeated attacks by the Dutch, and found lasting conversion of the migrant Inuit exceedingly difficult. An attempt to found a royal colony under Major Claus Paarss established the settlement of Godthåb ("Good Hope") in 1728 but was a costly debacle which saw most of his soldiers mutiny and his settlers killed by scurvy. Two child converts sent to Copenhagen for the coronation of Christian VI returned in 1733 with smallpox, devastating the island. The same ship that returned them, however, also brought the first Moravian missionaries, who in time would convert a former angekok (Inuit shaman), experience a revival at their mission of New Herrnhut, and establish a string of mission houses along the southwest coast. Around the same time, the merchant Jacob Severin took over administration of the colony and its trade and, having secured a large royal stipend and full monopoly from the king, successfully repulsed the Dutch in a series of skirmishes in 1738 and 1739. Egede himself quit the colony on the death of his wife, leaving the Lutheran mission to his son Poul. Both of them had studied the Kalaallisut language extensively and published works on it; as well, Poul and some of the other clergy sent by the Mission College such as Otto Fabricius began wide-ranging study of Greenland's flora, fauna, and meteorology. However, though kale, lettuce, and other herbs were successfully introduced, repeated attempts to cultivate wheat or clover failed throughout Greenland, limiting the ability to raise European livestock.
As a result of the Napoleonic Wars, Norway was ceded to Sweden at the 1814 Treaty of Kiel. The colonies, including Greenland, remained in Danish possession. The 19th century saw increased interest in the region on the part of polar explorers and scientists like William Scoresby and Greenland-born Knud Rasmussen. At the same time, the colonial elements of the earlier trade-oriented Danish presence in Greenland expanded. In 1861, the first Greenlandic-language journal was founded. Danish law still applied only to the Danish settlers, though. At the turn of the 19th century, the northern part of Greenland was still sparsely populated; only scattered hunting inhabitants were found there. During that century, however, Inuit families immigrated from British North America to settle in these areas. The last group from what later became Canada arrived in 1864. During the same time, the Northeastern part of the coast became depopulated following the violent 1783 Lakagígar eruption in Iceland.
Democratic elections for the district assemblies of Greenland were held for the first time in 1862–1863, although no assembly for the land as a whole was allowed. In 1888, a party of six led by Fridtjof Nansen accomplished the first land crossing of Greenland. The men took 41 days to make the crossing on skis, at approximately 64°N latitude. In 1911, two Landstings were introduced, one for northern Greenland and one for southern Greenland, not to be finally merged until 1951. All this time, most decisions were made in Copenhagen, where the Greenlanders had no representation. Towards the end of the 19th century, traders criticized the Danish trade monopoly. It was argued that it kept the natives in non-profitable ways of life, holding back the potentially large fishing industry. Many Greenlanders however were satisfied with the "status quo", as they felt the monopoly would secure the future of commercial whaling. It probably did not help that the only contact the local population had with the outside world was with Danish settlers. Nonetheless, the Danes gradually moved over their investments to the fishing industry.
Polar exploration.
At the end of the 19th century and beginning of the 20th century, American explorers, including Robert Peary, explored the northern sections of Greenland, which up to that time had been a mystery and were often shown on maps as extending over the North Pole. Peary discovered that Greenland's northern coast in fact stopped well short of the pole. These discoveries were considered to be the basis of an American territorial claim in the area. But after the United States purchased the Virgin Islands from Denmark in 1917, it agreed to relinquish all claims on Greenland.
Strategic importance.
After Norway regained full independence in 1905, it argued that Danish claims to Greenland were invalid since the island had been a Norwegian possession prior to 1815. In 1931, Norwegian whaler Hallvard Devold occupied uninhabited eastern Greenland, on his own initiative. After the fact, the occupation was supported by the Norwegian government, who claimed the area as Erik the Red's Land. Two years later, the Permanent Court of International Justice ruled in favor of Denmark.
During World War II, when Nazi Germany extended its war operations to Greenland, Henrik Kauffmann, the Danish Minister to the United States — who had already refused to recognize the German occupation of Denmark — signed a treaty with the United States on April 9, 1941, granting the US Armed Forces permission to establish stations in Greenland. Because of the difficulties for the Danish government to govern the island during the war, and because of successful export, especially of cryolite, Greenland came to enjoy a rather independent status. Its supplies were guaranteed by the United States and Canada.
During the Cold War, Greenland had a strategic importance, controlling parts of the passage between the Soviet Arctic harbours and the Atlantic, as well as being a good base for observing any use of intercontinental ballistic missiles, typically planned to pass over the Arctic. The United States therefore had a geopolitical interest in Greenland, and in 1946, the United States offered to buy Greenland from Denmark for $100,000,000 but Denmark did not agree to sell. In 1951, the Kauffman treaty was replaced by another one. The Thule Air Base at Thule (now Qaanaaq) in the northwest was made a permanent air force base. In 1953, some Inuit families were forced by Denmark to move from their homes to provide space for extension of the base. For this reason, the base has been a source of friction between the Danish government and the Greenlandic people. Tensions mounted when, on January 21, 1968, there was a nuclear accident — a B-52 Stratofortress carrying four hydrogen bombs crashed near the base, contaminating the area with radioactive debris. Although most of the contaminated ice was cleaned up, controversy currently surrounds recently declassified information indicating that one of the bombs was not accounted for. A 1995 Danish parliamentary scandal, dubbed Thulegate, highlighted that nuclear weapons were routinely present in Greenland's airspace in the years leading up to the accident, and that Denmark had tacitly given the go-ahead for this activity despite its official nuclear free policy.
Another recent controversy surrounds the Ballistic Missile Early Warning System (BMEWS), which the United States Air Force upgraded in recent years to a phased array radar. Opponents argue that the system presents a threat to the local population, as it would be targeted in the event of nuclear war.
Home rule.
From 1948 to 1950, the Greenland Commission studied the conditions on the island, seeking to address its isolation, unequal laws, and economic stagnation. In the end, the Royal Greenland Trading Department's monopolies were finally removed. In 1953, Greenland was raised from the status of colony to that of an autonomous province or constituent country of the Danish Realm. Despite its small population, it was provided nominal representation in the Danish Folketing.
Denmark also began a number of reforms aimed at urbanizing the Greenlanders, principally to replace their dependence on (then) dwindling seal populations and provide workers for the (then) swelling cod fisheries, but also to provide improved social services such as health care, education, and transportation. These well-meaning reforms have led to a number of problems, particularly modern unemployment and the infamous Blok P housing project. The attempt to introduce European-style urban housing suffered from such inattention to local detail that Inuit could not fit through the doors in their winter clothing and fire escapes were constantly blocked by fishing gear too bulky to fit into the cramped apartments. Television broadcasts began in 1982. The collapse of the cod fisheries and mines in the late 1980s and early 1990s greatly damaged the economy, which now principally depends on Danish aid and cold-water shrimp exports. Large sectors of the economy remain controlled by state-owned corporations, with Air Greenland and the Arctic Umiaq ferry heavily subsidized to provide access to remote settlements. The major airport remains the former US air base at Kangerlussuaq well north of Nuuk, with the capital unable to accept international flights on its own, owing to concerns about expense and noise pollution.
Greenland's minimal representation in the Folketing meant that despite 70.3% of Greenlanders rejecting entry into the European Common Market (EEC), it was pulled in along with Denmark in 1973. Fears that the customs union would allow foreign firms to compete and overfish its waters were quickly realized and the local parties began to push strongly for increased autonomy. The Folketing approved devolution in 1978 and the next year enacted home rule under a local Landsting. On 23 February 1982, a bare majority (53%) of Greenland's population voted to leave the EEC, a process which lasted until 1985.
Greenland Home Rule has become increasingly Greenlandized, rejecting Danish and avoiding regional dialects to standardize the country under the language and culture of the Kalaallit (West Greenland Inuit). The capital Godthåb was renamed Nuuk in 1979; a local flag was adopted in 1985; the Danish KGH became the locally administered Kalaallit Niuerfiat (now KNI A/S) in 1986. Following a successful referendum on self-government in 2008, the local parliament's powers were expanded and Danish was removed as an official language in 2009.
International relations are now largely, but not entirely, also left to the discretion of the home rule government. After leaving the EEC, Greenland signed a special treaty with it, granting it special access to the market as a constituent country of Denmark, which remains a member. Greenland is also a member of several small organizations along with Iceland, the Faroes, and the Inuit populations of Canada and Russia. It was one of the founders of the environmental Arctic Council in 1996. The US military bases on the island remain a major issue, with some politicians pushing for renegotiation of the 1951 US–Denmark treaty by the Home Rule government. The 1999–2003 Commission on Self-Governance even proposed that Greenland should aim at Thule Air Base's removal from American authority and operation under the aegis of the United Nations.

</doc>
